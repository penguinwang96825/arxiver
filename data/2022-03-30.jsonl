{"title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models", "abstract": "Pre-trained language models have been prevailed in natural language\nprocessing and become the backbones of many NLP tasks, but the demands for\ncomputational resources have limited their applications. In this paper, we\nintroduce TextPruner, an open-source model pruning toolkit designed for\npre-trained language models, targeting fast and easy model compression.\nTextPruner offers structured post-training pruning methods, including\nvocabulary pruning and transformer pruning, and can be applied to various\nmodels and tasks. We also propose a self-supervised pruning method that can be\napplied without the labeled data. Our experiments with several NLP tasks\ndemonstrate the ability of TextPruner to reduce the model size without\nre-training the model.", "published": "2022-03-30 02:10:33", "link": "http://arxiv.org/abs/2203.15996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clozer: Adaptable Data Augmentation for Cloze-style Reading\n  Comprehension", "abstract": "Task-adaptive pre-training (TAPT) alleviates the lack of labelled data and\nprovides performance lift by adapting unlabelled data to downstream task.\nUnfortunately, existing adaptations mainly involve deterministic rules that\ncannot generalize well. Here, we propose Clozer, a sequence-tagging based cloze\nanswer extraction method used in TAPT that is extendable for adaptation on any\ncloze-style machine reading comprehension (MRC) downstream tasks. We experiment\non multiple-choice cloze-style MRC tasks, and show that Clozer performs\nsignificantly better compared to the oracle and state-of-the-art in escalating\nTAPT effectiveness in lifting model performance, and prove that Clozer is able\nto recognize the gold answers independently of any heuristics.", "published": "2022-03-30 03:21:35", "link": "http://arxiv.org/abs/2203.16027v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Graph Convolutional Networks for Text Classification", "abstract": "Graph Convolutional Networks (GCN) have been effective at tasks that have\nrich relational structure and can preserve global structure information of a\ndataset in graph embeddings. Recently, many researchers focused on examining\nwhether GCNs could handle different Natural Language Processing tasks,\nespecially text classification. While applying GCNs to text classification is\nwell-studied, its graph construction techniques, such as node/edge selection\nand their feature representation, and the optimal GCN learning mechanism in\ntext classification is rather neglected. In this paper, we conduct a\ncomprehensive analysis of the role of node and edge embeddings in a graph and\nits GCN learning techniques in text classification. Our analysis is the first\nof its kind and provides useful insights into the importance of each graph\nnode/edge construction mechanism when applied at the GCN training/testing in\ndifferent text classification benchmarks, as well as under its semi-supervised\nenvironment.", "published": "2022-03-30 05:14:31", "link": "http://arxiv.org/abs/2203.16060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Overview of Indian Language Datasets used for Text Summarization", "abstract": "In this paper, we survey Text Summarization (TS) datasets in Indian Languages\n(ILs), which are also low-resource languages (LRLs). We seek to answer one\nprimary question: is the pool of Indian Language Text Summarization (ILTS)\ndataset growing or is there a resource poverty? To an-swer the primary\nquestion, we pose two sub-questions that we seek about ILTS datasets: first,\nwhat characteristics: format and domain do ILTS datasets have? Second, how\ndifferent are those characteristics of ILTS datasets from high-resource\nlanguages (HRLs) particularly English. We focus on datasets reported in\npublished ILTS research works during 2012-2022. The survey of ILTS and English\ndatasets reveals two similarities and one contrast. The two similarities are:\nfirst, the domain of dataset commonly is news (Hermann et al., 2015). The\nsecond similarity is the format of the dataset which is both extractive and\nabstractive. The contrast is in how the research in dataset development has\nprogressed. ILs face a slow speed of development and public release of datasets\nas compared with English. We argue that the relatively lower number of ILTS\ndatasets is because of two reasons: first, absence of a dedicated forum for\ndeveloping TS tools and resources; and second, lack of shareable standard\ndatasets in the public domain.", "published": "2022-03-30 08:09:36", "link": "http://arxiv.org/abs/2203.16127v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and\n  Approaches to Modeling", "abstract": "This work presents a new resource for borrowing identification and analyzes\nthe performance and errors of several models on this task. We introduce a new\nannotated corpus of Spanish newswire rich in unassimilated lexical borrowings\n-- words from one language that are introduced into another without\northographic adaptation -- and use it to evaluate how several sequence labeling\nmodels (CRF, BiLSTM-CRF, and Transformer-based models) perform. The corpus\ncontains 370,000 tokens and is larger, more borrowing-dense, OOV-rich, and\ntopic-varied than previous corpora available for this task. Our results show\nthat a BiLSTM-CRF model fed with subword embeddings along with either\nTransformer-based embeddings pretrained on codeswitched data or a combination\nof contextualized word embeddings outperforms results obtained by a\nmultilingual BERT-based model.", "published": "2022-03-30 09:46:51", "link": "http://arxiv.org/abs/2203.16169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Auto-MLM: Improved Contrastive Learning for Self-supervised\n  Multi-lingual Knowledge Retrieval", "abstract": "Contrastive learning (CL) has become a ubiquitous approach for several\nnatural language processing (NLP) downstream tasks, especially for question\nanswering (QA). However, the major challenge, how to efficiently train the\nknowledge retrieval model in an unsupervised manner, is still unresolved.\nRecently the commonly used methods are composed of CL and masked language model\n(MLM). Unexpectedly, MLM ignores the sentence-level training, and CL also\nneglects extraction of the internal info from the query. To optimize the CL\nhardly obtain internal information from the original query, we introduce a\njoint training method by combining CL and Auto-MLM for self-supervised\nmulti-lingual knowledge retrieval. First, we acquire the fixed dimensional\nsentence vector. Then, mask some words among the original sentences with random\nstrategy. Finally, we generate a new token representation for predicting the\nmasked tokens. Experimental results show that our proposed approach\nconsistently outperforms all the previous SOTA methods on both AliExpress $\\&$\nLAZADA service corpus and openly available corpora in 8 languages.", "published": "2022-03-30 10:13:57", "link": "http://arxiv.org/abs/2203.16187v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DePA: Improving Non-autoregressive Machine Translation with\n  Dependency-Aware Decoder", "abstract": "Non-autoregressive machine translation (NAT) models have lower translation\nquality than autoregressive translation (AT) models because NAT decoders do not\ndepend on previous target tokens in the decoder input. We propose a novel and\ngeneral Dependency-Aware Decoder (DePA) to enhance target dependency modeling\nin the decoder of fully NAT models from two perspectives: decoder\nself-attention and decoder input. First, we propose an autoregressive\nforward-backward pre-training phase before NAT training, which enables the NAT\ndecoder to gradually learn bidirectional target dependencies for the final NAT\ntraining. Second, we transform the decoder input from the source language\nrepresentation space to the target language representation space through a\nnovel attentive transformation process, which enables the decoder to better\ncapture target dependencies. DePA can be applied to any fully NAT models.\nExtensive experiments show that DePA consistently improves highly competitive\nand state-of-the-art fully NAT models on widely used WMT and IWSLT benchmarks\nby up to 1.88 BLEU gain, while maintaining the inference latency comparable to\nother fully NAT models.", "published": "2022-03-30 12:53:20", "link": "http://arxiv.org/abs/2203.16266v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Pipeline for Zero-Shot Data-to-Text Generation", "abstract": "In data-to-text (D2T) generation, training on in-domain data leads to\noverfitting to the data representation and repeating training data noise. We\nexamine how to avoid finetuning pretrained language models (PLMs) on D2T\ngeneration datasets while still taking advantage of surface realization\ncapabilities of PLMs. Inspired by pipeline approaches, we propose to generate\ntext by transforming single-item descriptions with a sequence of modules\ntrained on general-domain text-based operations: ordering, aggregation, and\nparagraph compression. We train PLMs for performing these operations on a\nsynthetic corpus WikiFluent which we build from English Wikipedia. Our\nexperiments on two major triple-to-text datasets -- WebNLG and E2E -- show that\nour approach enables D2T generation from RDF triples in zero-shot settings.", "published": "2022-03-30 13:14:35", "link": "http://arxiv.org/abs/2203.16279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Table Question Answering via Retrieval-Augmented Generation", "abstract": "Most existing end-to-end Table Question Answering (Table QA) models consist\nof a two-stage framework with a retriever to select relevant table candidates\nfrom a corpus and a reader to locate the correct answers from table candidates.\nEven though the accuracy of the reader models is significantly improved with\nthe recent transformer-based approaches, the overall performance of such\nframeworks still suffers from the poor accuracy of using traditional\ninformation retrieval techniques as retrievers. To alleviate this problem, we\nintroduce T-RAG, an end-to-end Table QA model, where a non-parametric dense\nvector index is fine-tuned jointly with BART, a parametric sequence-to-sequence\nmodel to generate answer tokens. Given any natural language question, T-RAG\nutilizes a unified pipeline to automatically search through a table corpus to\ndirectly locate the correct answer from the table cells. We apply T-RAG to\nrecent open-domain Table QA benchmarks and demonstrate that the fine-tuned\nT-RAG model is able to achieve state-of-the-art performance in both the\nend-to-end Table QA and the table retrieval tasks.", "published": "2022-03-30 23:30:16", "link": "http://arxiv.org/abs/2203.16714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-driven Fact-aware Abstractive Summarization of Biomedical\n  Literature", "abstract": "As part of the large number of scientific articles being published every\nyear, the publication rate of biomedical literature has been increasing.\nConsequently, there has been considerable effort to harness and summarize the\nmassive amount of biomedical research articles. While transformer-based\nencoder-decoder models in a vanilla source document-to-summary setting have\nbeen extensively studied for abstractive summarization in different domains,\ntheir major limitations continue to be entity hallucination (a phenomenon where\ngenerated summaries constitute entities not related to or present in source\narticle(s)) and factual inconsistency. This problem is exacerbated in a\nbiomedical setting where named entities and their semantics (which can be\ncaptured through a knowledge base) constitute the essence of an article. The\nuse of named entities and facts mined from background knowledge bases\npertaining to the named entities to guide abstractive summarization has not\nbeen studied in biomedical article summarization literature. In this paper, we\npropose an entity-driven fact-aware framework for training end-to-end\ntransformer-based encoder-decoder models for abstractive summarization of\nbiomedical articles. We call the proposed approach, whose building block is a\ntransformer-based model, EFAS, Entity-driven Fact-aware Abstractive\nSummarization. We conduct experiments using five state-of-the-art\ntransformer-based models (two of which are specifically designed for long\ndocument summarization) and demonstrate that injecting knowledge into the\ntraining/inference phase of these models enables the models to achieve\nsignificantly better performance than the standard source document-to-summary\nsetting in terms of entity-level factual accuracy, N-gram novelty, and semantic\nequivalence while performing comparably on ROUGE metrics. The proposed approach\nis evaluated on ICD-11-Summ-1000, and PubMed-50k.", "published": "2022-03-30 00:34:56", "link": "http://arxiv.org/abs/2203.15959v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-scale Speaker Diarization with Dynamic Scale Weighting", "abstract": "Speaker diarization systems are challenged by a trade-off between the\ntemporal resolution and the fidelity of the speaker representation. By\nobtaining a superior temporal resolution with an enhanced accuracy, a\nmulti-scale approach is a way to cope with such a trade-off. In this paper, we\npropose a more advanced multi-scale diarization system based on a multi-scale\ndiarization decoder. There are two main contributions in this study that\nsignificantly improve the diarization performance. First, we use multi-scale\nclustering as an initialization to estimate the number of speakers and obtain\nthe average speaker representation vector for each speaker and each scale.\nNext, we propose the use of 1-D convolutional neural networks that dynamically\ndetermine the importance of each scale at each time step. To handle a variable\nnumber of speakers and overlapping speech, the proposed system can estimate the\nnumber of existing speakers. Our proposed system achieves a state-of-art\nperformance on the CALLHOME and AMI MixHeadset datasets, with 3.92% and 1.05%\ndiarization error rates, respectively.", "published": "2022-03-30 01:26:31", "link": "http://arxiv.org/abs/2203.15974v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Evaluation of semantic relations impact in query expansion-based\n  retrieval systems", "abstract": "With the increasing demand of intelligent systems capable of operating in\ndifferent contexts (e.g. users on the move) the correct interpretation of the\nuser-need by such systems has become crucial to give consistent answers to the\nuser questions. The most effective applications addressing such task are in the\nfields of natural language processing and semantic expansion of terms. These\ntechniques are aimed at estimating the goal of an input query reformulating it\nas an intent, commonly relying on textual resources built exploiting different\nsemantic relations like \\emph{synonymy}, \\emph{antonymy} and many others. The\naim of this paper is to generate such resources using the labels of a given\ntaxonomy as source of information. The obtained resources are integrated into a\nplain classifier for reformulating a set of input queries as intents and\ntracking the effect of each relation, in order to quantify the impact of each\nsemantic relation on the classification. As an extension to this, the best\ntradeoff between improvement and noise introduction when combining such\nrelations is evaluated. The assessment is made generating the resources and\ntheir combinations and using them for tuning the classifier which is used to\nreformulate the user questions as labels. The evaluation employs a wide and\nvaried taxonomy as a use-case, exploiting its labels as basis for the semantic\nexpansion and producing several corpora with the purpose of enhancing the\npseudo-queries estimation.", "published": "2022-03-30 12:06:32", "link": "http://arxiv.org/abs/2203.16230v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Incorporating Dynamic Semantics into Pre-Trained Language Model for\n  Aspect-based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a\nspecific aspect in the given sentence. While pre-trained language models such\nas BERT have achieved great success, incorporating dynamic semantic changes\ninto ABSA remains challenging. To this end, in this paper, we propose to\naddress this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method\ndesigned to learn dynamic aspect-oriented semantics for ABSA. Specifically, we\nfirst take the Stack-BERT layers as a primary encoder to grasp the overall\nsemantic of the sentence and then fine-tune it by incorporating a lightweight\nDynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention\nto a small region of the sentences at each step and re-weigh the vitally\nimportant words for better aspect-aware sentiment understanding. Finally,\nexperimental results on three benchmark datasets demonstrate the effectiveness\nand the rationality of our proposed model and provide good interpretable\ninsights for future semantic modeling.", "published": "2022-03-30 14:48:46", "link": "http://arxiv.org/abs/2203.16369v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual\n  Transformer Models", "abstract": "Eye tracking data during reading is a useful source of information to\nunderstand the cognitive processes that take place during language\ncomprehension processes. Different languages account for different brain\ntriggers , however there seems to be some uniform indicators. In this paper, we\ndescribe our submission to the CMCL 2022 shared task on predicting human\nreading patterns for multi-lingual dataset. Our model uses text representations\nfrom transformers and some hand engineered features with a regression layer on\ntop to predict statistical measures of mean and standard deviation for 2 main\neye-tracking features. We train an end to end model to extract meaningful\ninformation from different languages and test our model on two seperate\ndatasets. We compare different transformer models and show ablation studies\naffecting model performance. Our final submission ranked 4th place for\nSubTask-1 and 1st place for SubTask-2 for the shared task.", "published": "2022-03-30 17:11:48", "link": "http://arxiv.org/abs/2203.16474v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speculative Decoding: Exploiting Speculative Execution for Accelerating\n  Seq2seq Generation", "abstract": "We propose Speculative Decoding (SpecDec), for the first time ever, to\nformally study exploiting the idea of speculative execution to accelerate\nautoregressive (AR) decoding. Speculative Decoding has two innovations:\nSpec-Drafter -- an independent model specially optimized for efficient and\naccurate drafting -- and Spec-Verification -- a reliable method for verifying\nthe drafted tokens efficiently in the decoding paradigm. Experimental results\non various seq2seq tasks including machine translation and abstractive\nsummarization show our approach can achieve around $5\\times$ speedup for the\npopular Transformer architectures with comparable generation quality to beam\nsearch decoding, refreshing the impression that the draft-then-verify paradigm\nintroduces only $1.4\\times$$\\sim$$2\\times$ speedup. In addition to the\nremarkable speedup, we also demonstrate 3 additional advantages of SpecDec,\nrevealing its practical value for accelerating generative models in real-world\napplications. Our models and codes are available at\nhttps://github.com/hemingkx/SpecDec.", "published": "2022-03-30 17:27:09", "link": "http://arxiv.org/abs/2203.16487v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vakyansh: ASR Toolkit for Low Resource Indic languages", "abstract": "We present Vakyansh, an end to end toolkit for Speech Recognition in Indic\nlanguages. India is home to almost 121 languages and around 125 crore speakers.\nYet most of the languages are low resource in terms of data and pretrained\nmodels. Through Vakyansh, we introduce automatic data pipelines for data\ncreation, model training, model evaluation and deployment. We create 14,000\nhours of speech data in 23 Indic languages and train wav2vec 2.0 based\npretrained models. These pretrained models are then finetuned to create state\nof the art speech recognition models for 18 Indic languages which are followed\nby language models and punctuation restoration models. We open source all these\nresources with a mission that this will inspire the speech community to develop\nspeech first applications using our ASR models in Indic languages.", "published": "2022-03-30 17:50:18", "link": "http://arxiv.org/abs/2203.16512v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Graph Refinement for Coreference Resolution", "abstract": "The state-of-the-art models for coreference resolution are based on\nindependent mention pair-wise decisions. We propose a modelling approach that\nlearns coreference at the document-level and takes global decisions. For this\npurpose, we model coreference links in a graph structure where the nodes are\ntokens in the text, and the edges represent the relationship between them. Our\nmodel predicts the graph in a non-autoregressive manner, then iteratively\nrefines it based on previous predictions, allowing global dependencies between\ndecisions. The experimental results show improvements over various baselines,\nreinforcing the hypothesis that document-level information improves conference\nresolution.", "published": "2022-03-30 18:03:54", "link": "http://arxiv.org/abs/2203.16574v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Code Switched and Code Mixed Speech Recognition for Indic languages", "abstract": "Training multilingual automatic speech recognition (ASR) systems is\nchallenging because acoustic and lexical information is typically language\nspecific. Training multilingual system for Indic languages is even more tougher\ndue to lack of open source datasets and results on different approaches. We\ncompare the performance of end to end multilingual speech recognition system to\nthe performance of monolingual models conditioned on language identification\n(LID). The decoding information from a multilingual model is used for language\nidentification and then combined with monolingual models to get an improvement\nof 50% WER across languages. We also propose a similar technique to solve the\nCode Switched problem and achieve a WER of 21.77 and 28.27 over Hindi-English\nand Bengali-English respectively. Our work talks on how transformer based ASR\nespecially wav2vec 2.0 can be applied in developing multilingual ASR and code\nswitched ASR for Indic languages.", "published": "2022-03-30 18:09:28", "link": "http://arxiv.org/abs/2203.16578v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Speech Recognition for Indic Languages using Language Model", "abstract": "We study the effect of applying a language model (LM) on the output of\nAutomatic Speech Recognition (ASR) systems for Indic languages. We fine-tune\nwav2vec $2.0$ models for $18$ Indic languages and adjust the results with\nlanguage models trained on text derived from a variety of sources. Our findings\ndemonstrate that the average Character Error Rate (CER) decreases by over $28$\n\\% and the average Word Error Rate (WER) decreases by about $36$ \\% after\ndecoding with LM. We show that a large LM may not provide a substantial\nimprovement as compared to a diverse one. We also demonstrate that high quality\ntranscriptions can be obtained on domain-specific data without retraining the\nASR model and show results on biomedical domain.", "published": "2022-03-30 18:22:12", "link": "http://arxiv.org/abs/2203.16595v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Is Word Error Rate a good evaluation metric for Speech Recognition in\n  Indic Languages?", "abstract": "We propose a new method for the calculation of error rates in Automatic\nSpeech Recognition (ASR). This new metric is for languages that contain half\ncharacters and where the same character can be written in different forms. We\nimplement our methodology in Hindi which is one of the main languages from\nIndic context and we think this approach is scalable to other similar languages\ncontaining a large character set. We call our metrics Alternate Word Error Rate\n(AWER) and Alternate Character Error Rate (ACER).\n  We train our ASR models using wav2vec 2.0\\cite{baevski2020wav2vec} for Indic\nlanguages. Additionally we use language models to improve our model\nperformance. Our results show a significant improvement in analyzing the error\nrates at word and character level and the interpretability of the ASR system is\nimproved upto $3$\\% in AWER and $7$\\% in ACER for Hindi. Our experiments\nsuggest that in languages which have complex pronunciation, there are multiple\nways of writing words without changing their meaning. In such cases AWER and\nACER will be more useful rather than WER and CER as metrics. Further, we open\nsource a new benchmarking dataset of 21 hours for Hindi with the new metric\nscripts.", "published": "2022-03-30 18:32:08", "link": "http://arxiv.org/abs/2203.16601v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Reproducibility Issues for BERT-based Evaluation Metrics", "abstract": "Reproducibility is of utmost concern in machine learning and natural language\nprocessing (NLP). In the field of natural language generation (especially\nmachine translation), the seminal paper of Post (2018) has pointed out problems\nof reproducibility of the dominant metric, BLEU, at the time of publication.\nNowadays, BERT-based evaluation metrics considerably outperform BLEU. In this\npaper, we ask whether results and claims from four recent BERT-based metrics\ncan be reproduced. We find that reproduction of claims and results often fails\nbecause of (i) heavy undocumented preprocessing involved in the metrics, (ii)\nmissing code and (iii) reporting weaker results for the baseline metrics. (iv)\nIn one case, the problem stems from correlating not to human scores but to a\nwrong column in the csv file, inflating scores by 5 points. Motivated by the\nimpact of preprocessing, we then conduct a second study where we examine its\neffects more closely (for one of the metrics). We find that preprocessing can\nhave large effects, especially for highly inflectional languages. In this case,\nthe effect of preprocessing may be larger than the effect of the aggregation\nmechanism (e.g., greedy alignment vs. Word Mover Distance).", "published": "2022-03-30 20:35:37", "link": "http://arxiv.org/abs/2204.00004v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Position-based Prompting for Health Outcome Generation", "abstract": "Probing Pre-trained Language Models (PLMs) using prompts has indirectly\nimplied that language models (LMs) can be treated as knowledge bases. To this\nend, this phenomena has been effective especially when these LMs are fine-tuned\ntowards not just data of a specific domain, but also to the style or linguistic\npattern of the prompts themselves. We observe that, satisfying a particular\nlinguistic pattern in prompts is an unsustainable constraint that unnecessarily\nlengthens the probing task, especially because, they are often manually\ndesigned and the range of possible prompt template patterns can vary depending\non the prompting objective and domain. We therefore explore an idea of using a\nposition-attention mechanism to capture positional information of each word in\na prompt relative to the mask to be filled, hence avoiding the need to\nre-construct prompts when the prompts linguistic pattern changes. Using our\napproach, we demonstrate the ability of eliciting answers to rare prompt\ntemplates (in a case study on health outcome generation) such as Postfix and\nMixed patterns whose missing information is respectively at the start and in\nmultiple random places of the prompt. More so, using various biomedical PLMs,\nour approach consistently outperforms a baseline in which the default mask\nlanguage model (MLM) representation is used to predict masked tokens.", "published": "2022-03-30 16:44:04", "link": "http://arxiv.org/abs/2204.03489v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Federated Domain Adaptation for ASR with Full Self-Supervision", "abstract": "Cross-device federated learning (FL) protects user privacy by collaboratively\ntraining a model on user devices, therefore eliminating the need for\ncollecting, storing, and manually labeling user data. While important topics\nsuch as the FL training algorithm, non-IID-ness, and Differential Privacy have\nbeen well studied in the literature, this paper focuses on two challenges of\npractical importance for improving on-device ASR: the lack of ground-truth\ntranscriptions and the scarcity of compute resource and network bandwidth on\nedge devices. First, we propose a FL system for on-device ASR domain adaptation\nwith full self-supervision, which uses self-labeling together with data\naugmentation and filtering techniques. The system can improve a strong\nEmformer-Transducer based ASR model pretrained on out-of-domain data, using\nin-domain audio without any ground-truth transcriptions. Second, to reduce the\ntraining cost, we propose a self-restricted RNN Transducer (SR-RNN-T) loss, a\nvariant of alignment-restricted RNN-T that uses Viterbi alignments from\nself-supervision. To further reduce the compute and network cost, we\nsystematically explore adapting only a subset of weights in the\nEmformer-Transducer. Our best training recipe achieves a $12.9\\%$ relative WER\nreduction over the strong out-of-domain baseline, which equals $70\\%$ of the\nreduction achievable with full human supervision and centralized training.", "published": "2022-03-30 00:50:16", "link": "http://arxiv.org/abs/2203.15966v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Span Classification with Structured Information for Disfluency Detection\n  in Spoken Utterances", "abstract": "Existing approaches in disfluency detection focus on solving a token-level\nclassification task for identifying and removing disfluencies in text.\nMoreover, most works focus on leveraging only contextual information captured\nby the linear sequences in text, thus ignoring the structured information in\ntext which is efficiently captured by dependency trees. In this paper, building\non the span classification paradigm of entity recognition, we propose a novel\narchitecture for detecting disfluencies in transcripts from spoken utterances,\nincorporating both contextual information through transformers and\nlong-distance structured information captured by dependency trees, through\ngraph convolutional networks (GCNs). Experimental results show that our\nproposed model achieves state-of-the-art results on the widely used English\nSwitchboard for disfluency detection and outperforms prior-art by a significant\nmargin. We make all our codes publicly available on GitHub\n(https://github.com/Sreyan88/Disfluency-Detection-with-Span-Classification)", "published": "2022-03-30 03:22:29", "link": "http://arxiv.org/abs/2203.16028v2", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Rainbow Keywords: Efficient Incremental Learning for Online Spoken\n  Keyword Spotting", "abstract": "Catastrophic forgetting is a thorny challenge when updating keyword spotting\n(KWS) models after deployment. This problem will be more challenging if KWS\nmodels are further required for edge devices due to their limited memory. To\nalleviate such an issue, we propose a novel diversity-aware incremental\nlearning method named Rainbow Keywords (RK). Specifically, the proposed RK\napproach introduces a diversity-aware sampler to select a diverse set from\nhistorical and incoming keywords by calculating classification uncertainty. As\na result, the RK approach can incrementally learn new tasks without forgetting\nprior knowledge. Besides, the RK approach also proposes data augmentation and\nknowledge distillation loss function for efficient memory management on the\nedge device. Experimental results show that the proposed RK approach achieves\n4.2% absolute improvement in terms of average accuracy over the best baseline\non Google Speech Command dataset with less required memory. The scripts are\navailable on GitHub.", "published": "2022-03-30 14:39:21", "link": "http://arxiv.org/abs/2203.16361v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TubeDETR: Spatio-Temporal Video Grounding with Transformers", "abstract": "We consider the problem of localizing a spatio-temporal tube in a video\ncorresponding to a given text query. This is a challenging task that requires\nthe joint and efficient modeling of temporal, spatial and multi-modal\ninteractions. To address this task, we propose TubeDETR, a transformer-based\narchitecture inspired by the recent success of such models for text-conditioned\nobject detection. Our model notably includes: (i) an efficient video and text\nencoder that models spatial multi-modal interactions over sparsely sampled\nframes and (ii) a space-time decoder that jointly performs spatio-temporal\nlocalization. We demonstrate the advantage of our proposed components through\nan extensive ablation study. We also evaluate our full approach on the\nspatio-temporal video grounding task and demonstrate improvements over the\nstate of the art on the challenging VidSTG and HC-STVG benchmarks. Code and\ntrained models are publicly available at\nhttps://antoyang.github.io/tubedetr.html.", "published": "2022-03-30 16:31:49", "link": "http://arxiv.org/abs/2203.16434v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Generative Spoken Dialogue Language Modeling", "abstract": "We introduce dGSLM, the first \"textless\" model able to generate audio samples\nof naturalistic spoken dialogues. It uses recent work on unsupervised spoken\nunit discovery coupled with a dual-tower transformer architecture with\ncross-attention trained on 2000 hours of two-channel raw conversational audio\n(Fisher dataset) without any text or labels. We show that our model is able to\ngenerate speech, laughter and other paralinguistic signals in the two channels\nsimultaneously and reproduces more naturalistic and fluid turn-taking compared\nto a text-based cascaded model.", "published": "2022-03-30 17:39:45", "link": "http://arxiv.org/abs/2203.16502v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Transformer Language Models without Positional Encodings Still Learn\n  Positional Information", "abstract": "Causal transformer language models (LMs), such as GPT-3, typically require\nsome form of positional encoding, such as positional embeddings. However, we\nshow that LMs without any explicit positional encoding are still competitive\nwith standard models, and that this phenomenon is robust across different\ndatasets, model sizes, and sequence lengths. Probing experiments reveal that\nsuch models acquire an implicit notion of absolute positions throughout the\nnetwork, effectively compensating for the missing information. We conjecture\nthat causal attention enables the model to infer the number of predecessors\nthat each token can attend to, thereby approximating its absolute position. Our\nfindings indicate that causal LMs might derive positional awareness not only\nfrom the explicit positioning mechanism, but also from the effects of the\ncausal mask.", "published": "2022-03-30 19:37:07", "link": "http://arxiv.org/abs/2203.16634v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic\n  descriptions, and Conceptual Relations", "abstract": "We present a meta-learning framework for learning new visual concepts\nquickly, from just one or a few examples, guided by multiple naturally\noccurring data streams: simultaneously looking at images, reading sentences\nthat describe the objects in the scene, and interpreting supplemental sentences\nthat relate the novel concept with other concepts. The learned concepts support\ndownstream applications, such as answering questions by reasoning about unseen\nimages. Our model, namely FALCON, represents individual visual concepts, such\nas colors and shapes, as axis-aligned boxes in a high-dimensional space (the\n\"box embedding space\"). Given an input image and its paired sentence, our model\nfirst resolves the referential expression in the sentence and associates the\nnovel concept with particular objects in the scene. Next, our model interprets\nsupplemental sentences to relate the novel concept with other known concepts,\nsuch as \"X has property Y\" or \"X is a kind of Y\". Finally, it infers an optimal\nbox embedding for the novel concept that jointly 1) maximizes the likelihood of\nthe observed instances in the image, and 2) satisfies the relationships between\nthe novel concepts and the known ones. We demonstrate the effectiveness of our\nmodel on both synthetic and real-world datasets.", "published": "2022-03-30 19:45:00", "link": "http://arxiv.org/abs/2203.16639v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo", "abstract": "We present a debiased dataset for the Person-centric Visual Grounding (PCVG)\ntask first proposed by Cui et al. (2021) in the Who's Waldo dataset. Given an\nimage and a caption, PCVG requires pairing up a person's name mentioned in a\ncaption with a bounding box that points to the person in the image. We find\nthat the original Who's Waldo dataset compiled for this task contains a large\nnumber of biased samples that are solvable simply by heuristic methods; for\ninstance, in many cases the first name in the sentence corresponds to the\nlargest bounding box, or the sequence of names in the sentence corresponds to\nan exact left-to-right order in the image. Naturally, models trained on these\nbiased data lead to over-estimation of performance on the benchmark. To enforce\nmodels being correct for the correct reasons, we design automated tools to\nfilter and debias the original dataset by ruling out all examples of\ninsufficient context, such as those with no verb or with a long chain of\nconjunct names in their captions. Our experiments show that our new sub-sampled\ndataset contains less bias with much lowered heuristic performances and widened\ngaps between heuristic and supervised methods. We also demonstrate the same\nbenchmark model trained on our debiased training set outperforms that trained\non the original biased (and larger) training set on our debiased test set. We\nargue our debiased dataset offers the PCVG task a more practical baseline for\nreliable benchmarking and future improvements.", "published": "2022-03-30 21:35:53", "link": "http://arxiv.org/abs/2203.16682v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings", "abstract": "This paper presents a streaming speaker-attributed automatic speech\nrecognition (SA-ASR) model that can recognize ``who spoke what'' with low\nlatency even when multiple people are speaking simultaneously. Our model is\nbased on token-level serialized output training (t-SOT) which was recently\nproposed to transcribe multi-talker speech in a streaming fashion. To further\nrecognize speaker identities, we propose an encoder-decoder based speaker\nembedding extractor that can estimate a speaker representation for each\nrecognized token not only from non-overlapping speech but also from overlapping\nspeech. The proposed speaker embedding, named t-vector, is extracted\nsynchronously with the t-SOT ASR model, enabling joint execution of speaker\nidentification (SID) or speaker diarization (SD) with the multi-talker\ntranscription with low latency. We evaluate the proposed model for a joint task\nof ASR and SID/SD by using LibriSpeechMix and LibriCSS corpora. The proposed\nmodel achieves substantially better accuracy than a prior streaming model and\nshows comparable or sometimes even superior results to the state-of-the-art\noffline SA-ASR model.", "published": "2022-03-30 21:42:00", "link": "http://arxiv.org/abs/2203.16685v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Disentangled Variational Speech Representation Learning for\n  Zero-shot Voice Conversion", "abstract": "Traditional studies on voice conversion (VC) have made progress with parallel\ntraining data and known speakers. Good voice conversion quality is obtained by\nexploring better alignment modules or expressive mapping functions. In this\nstudy, we investigate zero-shot VC from a novel perspective of self-supervised\ndisentangled speech representation learning. Specifically, we achieve the\ndisentanglement by balancing the information flow between global speaker\nrepresentation and time-varying content representation in a sequential\nvariational autoencoder (VAE). A zero-shot voice conversion is performed by\nfeeding an arbitrary speaker embedding and content embeddings to the VAE\ndecoder. Besides that, an on-the-fly data augmentation training strategy is\napplied to make the learned representation noise invariant. On TIMIT and VCTK\ndatasets, we achieve state-of-the-art performance on both objective evaluation,\ni.e., speaker verification (SV) on speaker embedding and content embedding, and\nsubjective evaluation, i.e., voice naturalness and similarity, and remains to\nbe robust even with noisy source/target utterances.", "published": "2022-03-30 23:03:19", "link": "http://arxiv.org/abs/2203.16705v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "eess.SP"], "primary_category": "eess.AS"}
{"title": "VL-InterpreT: An Interactive Visualization Tool for Interpreting\n  Vision-Language Transformers", "abstract": "Breakthroughs in transformer-based models have revolutionized not only the\nNLP field, but also vision and multimodal systems. However, although\nvisualization and interpretability tools have become available for NLP models,\ninternal mechanisms of vision and multimodal transformers remain largely\nopaque. With the success of these transformers, it is increasingly critical to\nunderstand their inner workings, as unraveling these black-boxes will lead to\nmore capable and trustworthy models. To contribute to this quest, we propose\nVL-InterpreT, which provides novel interactive visualizations for interpreting\nthe attentions and hidden representations in multimodal transformers.\nVL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety\nof statistics in attention heads throughout all layers for both vision and\nlanguage components, (2) visualizes cross-modal and intra-modal attentions\nthrough easily readable heatmaps, and (3) plots the hidden representations of\nvision and language tokens as they pass through the transformer layers. In this\npaper, we demonstrate the functionalities of VL-InterpreT through the analysis\nof KD-VLP, an end-to-end pretraining vision-language multimodal\ntransformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and\nWebQA, two visual question answering benchmarks. Furthermore, we also present a\nfew interesting findings about multimodal transformer behaviors that were\nlearned through our tool.", "published": "2022-03-30 05:25:35", "link": "http://arxiv.org/abs/2203.17247v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards Few-shot Entity Recognition in Document Images: A Label-aware\n  Sequence-to-Sequence Framework", "abstract": "Entity recognition is a fundamental task in understanding document images.\nTraditional sequence labeling frameworks treat the entity types as class IDs\nand rely on extensive data and high-quality annotations to learn semantics\nwhich are typically expensive in practice. In this paper, we aim to build an\nentity recognition model requiring only a few shots of annotated document\nimages. To overcome the data limitation, we propose to leverage the label\nsurface names to better inform the model of the target entity type semantics\nand also embed the labels into the spatial embedding space to capture the\nspatial correspondence between regions and labels. Specifically, we go beyond\nsequence labeling and develop a novel label-aware seq2seq framework, LASER. The\nproposed model follows a new labeling scheme that generates the label surface\nnames word-by-word explicitly after generating the entities. During training,\nLASER refines the label semantics by updating the label surface name\nrepresentations and also strengthens the label-region correlation. In this way,\nLASER recognizes the entities from document images through both semantic and\nlayout correspondence. Extensive experiments on two benchmark datasets\ndemonstrate the superiority of LASER under the few-shot setting.", "published": "2022-03-30 18:30:42", "link": "http://arxiv.org/abs/2204.05819v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Disentangling the Impacts of Language and Channel Variability on Speech\n  Separation Networks", "abstract": "Because the performance of speech separation is excellent for speech in which\ntwo speakers completely overlap, research attention has been shifted to dealing\nwith more realistic scenarios. However, domain mismatch between training/test\nsituations due to factors, such as speaker, content, channel, and environment,\nremains a severe problem for speech separation. Speaker and environment\nmismatches have been studied in the existing literature. Nevertheless, there\nare few studies on speech content and channel mismatches. Moreover, the impacts\nof language and channel in these studies are mostly tangled. In this study, we\ncreate several datasets for various experiments. The results show that the\nimpacts of different languages are small enough to be ignored compared to the\nimpacts of different channels. In our experiments, training on data recorded by\nAndroid phones leads to the best generalizability. Moreover, we provide a new\nsolution for channel mismatch by evaluating projection, where the channel\nsimilarity can be measured and used to effectively select additional training\ndata to improve the performance of in-the-wild test data.", "published": "2022-03-30 04:07:23", "link": "http://arxiv.org/abs/2203.16040v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MAE-AST: Masked Autoencoding Audio Spectrogram Transformer", "abstract": "In this paper, we propose a simple yet powerful improvement over the recent\nSelf-Supervised Audio Spectrogram Transformer (SSAST) model for speech and\naudio classification. Specifically, we leverage the insight that the SSAST uses\na very high masking ratio (75%) during pretraining, meaning that the vast\nmajority of self-attention compute is performed on mask tokens. We address this\nby integrating the encoder-decoder architecture from Masked Autoencoders are\nScalable Vision Learners (MAE) into the SSAST, where a deep encoder operates on\nonly unmasked input, and a shallow decoder operates on encoder outputs and mask\ntokens. We find that MAE-like pretraining can provide a 3x speedup and 2x\nmemory usage reduction over the vanilla SSAST using current audio pretraining\nstrategies with ordinary model and input sizes. When fine-tuning on downstream\ntasks, which only uses the encoder, we find that our approach outperforms the\nSSAST on a variety of downstream tasks. We further conduct comprehensive\nevaluations into different strategies of pretraining and explore differences in\nMAE-style pretraining between the visual and audio domains.", "published": "2022-03-30 22:06:13", "link": "http://arxiv.org/abs/2203.16691v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generation of Speaker Representations Using Heterogeneous Training Batch\n  Assembly", "abstract": "In traditional speaker diarization systems, a well-trained speaker model is a\nkey component to extract representations from consecutive and partially\noverlapping segments in a long speech session. To be more consistent with the\nback-end segmentation and clustering, we propose a new CNN-based speaker\nmodeling scheme, which takes into account the heterogeneity of the speakers in\neach training segment and batch. We randomly and synthetically augment the\ntraining data into a set of segments, each of which contains more than one\nspeaker and some overlapping parts. A soft label is imposed on each segment\nbased on its speaker occupation ratio, and the standard cross entropy loss is\nimplemented in model training. In this way, the speaker model should have the\nability to generate a geometrically meaningful embedding for each multi-speaker\nsegment. Experimental results show that our system is superior to the baseline\nsystem using x-vectors in two speaker diarization tasks. In the CALLHOME task\ntrained on the NIST SRE and Switchboard datasets, our system achieves a\nrelative reduction of 12.93% in DER. In Track 2 of CHiME-6, our system provides\n13.24%, 12.60%, and 5.65% relative reductions in DER, JER, and WER,\nrespectively.", "published": "2022-03-30 19:59:05", "link": "http://arxiv.org/abs/2203.16646v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Using Adapters to Overcome Catastrophic Forgetting in End-to-End\n  Automatic Speech Recognition", "abstract": "Learning a set of tasks in sequence remains a challenge for artificial neural\nnetworks, which, in such scenarios, tend to suffer from Catastrophic Forgetting\n(CF). The same applies to End-to-End (E2E) Automatic Speech Recognition (ASR)\nmodels, even for monolingual tasks. In this paper, we aim to overcome CF for\nE2E ASR by inserting adapters, small architectures of few parameters which\nallow a general model to be fine-tuned to a specific task, into our model. We\nmake these adapters task-specific, while regularizing the parameters of the\nmodel shared by all tasks, thus stimulating the model to fully exploit the\nadapters while keeping the shared parameters to work well for all tasks. Our\nmethod outperforms all baselines on two monolingual experiments while being\nmore storage efficient and without requiring the storage of data from previous\ntasks.", "published": "2022-03-30 06:22:27", "link": "http://arxiv.org/abs/2203.16082v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multiple Narrow-band signals Direction Finding with TMLA by Nonuniform\n  Period Modulation", "abstract": "A new array signal reconstruction and signal-channel DOA estimation method\nbased on TMLA by nonuniform period modulation are proposed. By using\nnon-uniform period modulation, the harmonic component produced by different\nelements could be separated. Therefore, the conventional snapshot could be\nreconstructed by analyzing the spectrum of the combined signal. Then spatial\nspectrum estimation method is used to implement DOA estimation. Numerical\nsimulations are provided to verify the feasibility and accuracy of the proposed\nmethod. Since the duration of the signal in the frequency domain analysis\nprocessed in a single time is very short, this method is also applicable to\nnarrowband signals. Another highlight is that this method can simultaneously\nmeasure the number of the elements-1 angle of incident signals.", "published": "2022-03-30 03:13:26", "link": "http://arxiv.org/abs/2203.16025v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "ConferencingSpeech 2022 Challenge: Non-intrusive Objective Speech\n  Quality Assessment (NISQA) Challenge for Online Conferencing Applications", "abstract": "With the advances in speech communication systems such as online conferencing\napplications, we can seamlessly work with people regardless of where they are.\nHowever, during online meetings, speech quality can be significantly affected\nby background noise, reverberation, packet loss, network jitter, etc. Because\nof its nature, speech quality is traditionally assessed in subjective tests in\nlaboratories and lately also in crowdsourcing following the international\nstandards from ITU-T Rec. P.800 series. However, those approaches are costly\nand cannot be applied to customer data. Therefore, an effective objective\nassessment approach is needed to evaluate or monitor the speech quality of the\nongoing conversation. The ConferencingSpeech 2022 challenge targets the\nnon-intrusive deep neural network models for the speech quality assessment\ntask. We open-sourced a training corpus with more than 86K speech clips in\ndifferent languages, with a wide range of synthesized and live degradations and\ntheir corresponding subjective quality scores through crowdsourcing. 18 teams\nsubmitted their models for evaluation in this challenge. The blind test sets\nincluded about 4300 clips from wide ranges of degradations. This paper\ndescribes the challenge, the datasets, and the evaluation methods and reports\nthe final results.", "published": "2022-03-30 03:32:25", "link": "http://arxiv.org/abs/2203.16032v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optimizing Shoulder to Shoulder: A Coordinated Sub-Band Fusion Model for\n  Real-Time Full-Band Speech Enhancement", "abstract": "Due to the high computational complexity to model more frequency bands, it is\nstill intractable to conduct real-time full-band speech enhancement based on\ndeep neural networks. Recent studies typically utilize the compressed\nperceptually motivated features with relatively low frequency resolution to\nfilter the full-band spectrum by one-stage networks, leading to limited speech\nquality improvements. In this paper, we propose a coordinated sub-band fusion\nnetwork for full-band speech enhancement, which aims to recover the low- (0-8\nkHz), middle- (8-16 kHz), and high-band (16-24 kHz) in a step-wise manner.\nSpecifically, a dual-stream network is first pretrained to recover the low-band\ncomplex spectrum, and another two sub-networks are designed as the middle- and\nhigh-band noise suppressors in the magnitude-only domain. To fully capitalize\non the information intercommunication, we employ a sub-band interaction module\nto provide external knowledge guidance across different frequency bands.\nExtensive experiments show that the proposed method yields consistent\nperformance advantages over state-of-the-art full-band baselines.", "published": "2022-03-30 03:35:22", "link": "http://arxiv.org/abs/2203.16033v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Asymmetric Proxy Loss for Multi-View Acoustic Word Embeddings", "abstract": "Acoustic word embeddings (AWEs) are discriminative representations of speech\nsegments, and learned embedding space reflects the phonetic similarity between\nwords. With multi-view learning, where text labels are considered as\nsupplementary input, AWEs are jointly trained with acoustically grounded word\nembeddings (AGWEs). In this paper, we expand the multi-view approach into a\nproxy-based framework for deep metric learning by equating AGWEs with proxies.\nA simple modification in computing the similarity matrix allows the general\npair weighting to formulate the data-to-proxy relationship. Under the\nsystematized framework, we propose an asymmetric-proxy loss that combines\ndifferent parts of loss functions asymmetrically while keeping their merits. It\nfollows the assumptions that the optimal function for anchor-positive pairs may\ndiffer from one for anchor-negative pairs, and a proxy may have a different\nimpact when it substitutes for different positions in the triplet. We present\ncomparative experiments with various proxy-based losses including our\nasymmetric-proxy loss, and evaluate AWEs and AGWEs for word discrimination\ntasks on WSJ corpus. The results demonstrate the effectiveness of the proposed\nmethod.", "published": "2022-03-30 06:17:59", "link": "http://arxiv.org/abs/2203.16080v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Combination of Time-domain, Frequency-domain, and Cepstral-domain\n  Acoustic Features for Speech Commands Classification", "abstract": "In speech-related classification tasks, frequency-domain acoustic features\nsuch as logarithmic Mel-filter bank coefficients (FBANK) and cepstral-domain\nacoustic features such as Mel-frequency cepstral coefficients (MFCC) are often\nused. However, time-domain features perform more effectively in some sound\nclassification tasks which contain non-vocal or weakly speech-related sounds.\nWe previously proposed a feature called bit sequence representation (BSR),\nwhich is a time-domain binary acoustic feature based on the raw waveform.\nCompared with MFCC, BSR performed better in environmental sound detection and\nshowed comparable accuracy performance in limited-vocabulary speech recognition\ntasks. In this paper, we propose a novel improvement BSR feature called\nBSR-float16 to represent floating-point values more precisely. We\nexperimentally demonstrated the complementarity among time-domain,\nfrequency-domain, and cepstral-domain features using a dataset called Speech\nCommands proposed by Google. Therefore, we used a simple back-end score fusion\nmethod to improve the final classification accuracy. The fusion results also\nshowed better noise robustness.", "published": "2022-03-30 06:24:42", "link": "http://arxiv.org/abs/2203.16085v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Probing phoneme, language and speaker information in unsupervised speech\n  representations", "abstract": "Unsupervised models of representations based on Contrastive Predictive Coding\n(CPC)[1] are primarily used in spoken language modelling in that they encode\nphonetic information. In this study, we ask what other types of information are\npresent in CPC speech representations. We focus on three categories: phone\nclass, gender and language, and compare monolingual and bilingual models. Using\nqualitative and quantitative tools, we find that both gender and phone class\ninformation are present in both types of models. Language information, however,\nis very salient in the bilingual model only, suggesting CPC models learn to\ndiscriminate languages when trained on multiple languages. Some language\ninformation can also be retrieved from monolingual models, but it is more\ndiffused across all features. These patterns hold when analyses are carried on\nthe discrete units from a downstream clustering model. However, although there\nis no effect of the number of target clusters on phone class and language\ninformation, more gender information is encoded with more clusters. Finally, we\nfind that there is some cost to being exposed to two languages on a downstream\nphoneme discrimination task.", "published": "2022-03-30 10:27:30", "link": "http://arxiv.org/abs/2203.16193v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learn2Sing 2.0: Diffusion and Mutual Information-Based Target Speaker\n  SVS by Learning from Singing Teacher", "abstract": "Building a high-quality singing corpus for a person who is not good at\nsinging is non-trivial, thus making it challenging to create a singing voice\nsynthesizer for this person. Learn2Sing is dedicated to synthesizing the\nsinging voice of a speaker without his or her singing data by learning from\ndata recorded by others, i.e., the singing teacher. Inspired by the fact that\npitch is the key style factor to distinguish singing from speaking voice, the\nproposed Learn2Sing 2.0 first generates the preliminary acoustic feature with\naveraged pitch value in the phone level, which allows the training of this\nprocess for different styles, i.e., speaking or singing, share same conditions\nexcept for the speaker information. Then, conditioned on the specific style, a\ndiffusion decoder, which is accelerated by a fast sampling algorithm during the\ninference stage, is adopted to gradually restore the final acoustic feature.\nDuring the training, to avoid the information confusion of the speaker\nembedding and the style embedding, mutual information is employed to restrain\nthe learning of speaker embedding and style embedding. Experiments show that\nthe proposed approach is capable of synthesizing high-quality singing voice for\nthe target speaker without singing data with 10 decoding steps.", "published": "2022-03-30 15:48:44", "link": "http://arxiv.org/abs/2203.16408v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Joint domain adaptation and speech bandwidth extension using time-domain\n  GANs for speaker verification", "abstract": "Speech systems developed for a particular choice of acoustic domain and\nsampling frequency do not translate easily to others. The usual practice is to\nlearn domain adaptation and bandwidth extension models independently. Contrary\nto this, we propose to learn both tasks together. Particularly, we learn to map\nnarrowband conversational telephone speech to wideband microphone speech. We\ndeveloped parallel and non-parallel learning solutions which utilize both\npaired and unpaired data. First, we first discuss joint and disjoint training\nof multiple generative models for our tasks. Then, we propose a two-stage\nlearning solution where we use a pre-trained domain adaptation system for\npre-processing in bandwidth extension training. We evaluated our schemes on a\nSpeaker Verification downstream task. We used the JHU-MIT experimental setup\nfor NIST SRE21, which comprises SRE16, SRE-CTS Superset and SRE21. Our results\nprovide the first evidence that learning both tasks is better than learning\njust one. On SRE16, our best system achieves 22% relative improvement in Equal\nError Rate w.r.t. a direct learning baseline and 8% w.r.t. a strong bandwidth\nexpansion system.", "published": "2022-03-30 18:51:20", "link": "http://arxiv.org/abs/2203.16614v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using Active Speaker Faces for Diarization in TV shows", "abstract": "Speaker diarization is one of the critical components of computational media\nintelligence as it enables a character-level analysis of story portrayals and\nmedia content understanding. Automated audio-based speaker diarization of\nentertainment media poses challenges due to the diverse acoustic conditions\npresent in media content, be it background music, overlapping speakers, or\nsound effects. At the same time, speaking faces in the visual modality provide\ncomplementary information and not prone to the errors seen in the audio\nmodality. In this paper, we address the problem of speaker diarization in TV\nshows using the active speaker faces. We perform face clustering on the active\nspeaker faces and show superior speaker diarization performance compared to the\nstate-of-the-art audio-based diarization methods. We additionally report a\nsystematic analysis of the impact of active speaker face detection quality on\nthe diarization performance. We also observe that a moderately well-performing\nactive speaker system could outperform the audio-based diarization systems.", "published": "2022-03-30 00:37:19", "link": "http://arxiv.org/abs/2203.15961v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Device-Directed Speech Detection: Regularization via Distillation for\n  Weakly-Supervised Models", "abstract": "We address the problem of detecting speech directed to a device that does not\ncontain a specific wake-word. Specifically, we focus on audio coming from a\ntouch-based invocation. Mitigating virtual assistants (VAs) activation due to\naccidental button presses is critical for user experience. While the majority\nof approaches to false trigger mitigation (FTM) are designed to detect the\npresence of a target keyword, inferring user intent in absence of keyword is\ndifficult. This also poses a challenge when creating the training/evaluation\ndata for such systems due to inherent ambiguity in the user's data. To this\nend, we propose a novel FTM approach that uses weakly-labeled training data\nobtained with a newly introduced data sampling strategy. While this sampling\nstrategy reduces data annotation efforts, the data labels are noisy as the data\nare not annotated manually. We use these data to train an acoustics-only model\nfor the FTM task by regularizing its loss function via knowledge distillation\nfrom an ASR-based (LatticeRNN) model. This improves the model decisions,\nresulting in 66% gain in accuracy, as measured by equal-error-rate (EER), over\nthe base acoustics-only model. We also show that the ensemble of the LatticeRNN\nand acoustic-distilled models brings further accuracy improvement of 20%.", "published": "2022-03-30 01:27:39", "link": "http://arxiv.org/abs/2203.15975v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Sound of Bounding-Boxes", "abstract": "In the task of audio-visual sound source separation, which leverages visual\ninformation for sound source separation, identifying objects in an image is a\ncrucial step prior to separating the sound source. However, existing methods\nthat assign sound on detected bounding boxes suffer from a problem that their\napproach heavily relies on pre-trained object detectors. Specifically, when\nusing these existing methods, it is required to predetermine all the possible\ncategories of objects that can produce sound and use an object detector\napplicable to all such categories. To tackle this problem, we propose a fully\nunsupervised method that learns to detect objects in an image and separate\nsound source simultaneously. As our method does not rely on any pre-trained\ndetector, our method is applicable to arbitrary categories without any\nadditional annotation. Furthermore, although being fully unsupervised, we found\nthat our method performs comparably in separation accuracy.", "published": "2022-03-30 01:58:52", "link": "http://arxiv.org/abs/2203.15991v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Multi-target Extractor and Detector for Unknown-number Speaker\n  Diarization", "abstract": "Strong representations of target speakers can help extract important\ninformation about speakers and detect corresponding temporal regions in\nmulti-speaker conversations. In this study, we propose a neural architecture\nthat simultaneously extracts speaker representations consistent with the\nspeaker diarization objective and detects the presence of each speaker on a\nframe-by-frame basis regardless of the number of speakers in a conversation. A\nspeaker representation (called z-vector) extractor and a time-speaker\ncontextualizer, implemented by a residual network and processing data in both\ntemporal and speaker dimensions, are integrated into a unified framework. Tests\non the CALLHOME corpus show that our model outperforms most of the methods\nproposed so far. Evaluations in a more challenging case with simultaneous\nspeakers ranging from 2 to 7 show that our model achieves 6.4% to 30.9%\nrelative diarization error rate reductions over several typical baselines.", "published": "2022-03-30 02:30:20", "link": "http://arxiv.org/abs/2203.16007v4", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Zero-Shot Many to Many Voice Conversion with Self-Attention\n  VAE", "abstract": "Variational auto-encoder (VAE) is an effective neural network architecture to\ndisentangle a speech utterance into speaker identity and linguistic content\nlatent embeddings, then generate an utterance for a target speaker from that of\na source speaker. This is possible by concatenating the identity embedding of\nthe target speaker and the content embedding of the source speaker uttering a\ndesired sentence. In this work, we propose to improve VAE models with\nself-attention and structural regularization (RGSM). Specifically, we found a\nsuitable location of VAE's decoder to add a self-attention layer for\nincorporating non-local information in generating a converted utterance and\nhiding the source speaker's identity. We applied relaxed group-wise splitting\nmethod (RGSM) to regularize network weights and remarkably enhance\ngeneralization performance.\n  In experiments of zero-shot many-to-many voice conversion task on VCTK data\nset, with the self-attention layer and relaxed group-wise splitting method, our\nmodel achieves a gain of speaker classification accuracy on unseen speakers by\n28.3\\% while slightly improved conversion voice quality in terms of MOSNet\nscores. Our encouraging findings point to future research on integrating more\nvariety of attention structures in VAE framework while controlling model size\nand overfitting for advancing zero-shot many-to-many voice conversions.", "published": "2022-03-30 03:52:42", "link": "http://arxiv.org/abs/2203.16037v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Coarse-to-Fine Recursive Speech Separation for Unknown Number of\n  Speakers", "abstract": "The vast majority of speech separation methods assume that the number of\nspeakers is known in advance, hence they are specific to the number of\nspeakers. By contrast, a more realistic and challenging task is to separate a\nmixture in which the number of speakers is unknown. This paper formulates the\nspeech separation with the unknown number of speakers as a multi-pass source\nextraction problem and proposes a coarse-to-fine recursive speech separation\nmethod. This method comprises two stages, namely, recursive cue extraction and\ntarget speaker extraction. The recursive cue extraction stage determines how\nmany computational iterations need to be performed and outputs a coarse cue\nspeech by monitoring statistics in the mixture. As the number of recursive\niterations increases, the accumulation of distortion eventually comes into the\nextracted speech and reminder. Therefore, in the second stage, we use a target\nspeaker extraction network to extract a fine speech based on the coarse target\ncue and the original distortionless mixture. Experiments show that the proposed\nmethod archived state-of-the-art performance on the WSJ0 dataset with a\ndifferent number of speakers. Furthermore, it generalizes well to an unseen\nlarge number of speakers.", "published": "2022-03-30 04:45:34", "link": "http://arxiv.org/abs/2203.16054v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Distortion Robustness of Self-supervised Speech Processing\n  Tasks with Domain Adaptation", "abstract": "Speech distortions are a long-standing problem that degrades the performance\nof supervisely trained speech processing models. It is high time that we\nenhance the robustness of speech processing models to obtain good performance\nwhen encountering speech distortions while not hurting the original performance\non clean speech. In this work, we propose to improve the robustness of speech\nprocessing models by domain adversarial training (DAT). We conducted\nexperiments based on the SUPERB framework on five different speech processing\ntasks. In case we do not always have knowledge of the distortion types for\nspeech data, we analyzed the binary-domain and multi-domain settings, where the\nformer treats all distorted speech as one domain, and the latter views\ndifferent distortions as different domains. In contrast to supervised training\nmethods, we obtained promising results in target domains where speech data is\ndistorted with different distortions including new unseen distortions\nintroduced during testing.", "published": "2022-03-30 07:25:52", "link": "http://arxiv.org/abs/2203.16104v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Example-based Explanations with Adversarial Attacks for Respiratory\n  Sound Analysis", "abstract": "Respiratory sound classification is an important tool for remote screening of\nrespiratory-related diseases such as pneumonia, asthma, and COVID-19. To\nfacilitate the interpretability of classification results, especially ones\nbased on deep learning, many explanation methods have been proposed using\nprototypes. However, existing explanation techniques often assume that the data\nis non-biased and the prediction results can be explained by a set of\nprototypical examples. In this work, we develop a unified example-based\nexplanation method for selecting both representative data (prototypes) and\noutliers (criticisms). In particular, we propose a novel application of\nadversarial attacks to generate an explanation spectrum of data instances via\nan iterative fast gradient sign method. Such unified explanation can avoid\nover-generalisation and bias by allowing human experts to assess the model\nmistakes case by case. We performed a wide range of quantitative and\nqualitative evaluations to show that our approach generates effective and\nunderstandable explanation and is robust with many deep learning models", "published": "2022-03-30 08:28:48", "link": "http://arxiv.org/abs/2203.16141v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Symbolic music generation conditioned on continuous-valued emotions", "abstract": "In this paper we present a new approach for the generation of\nmulti-instrument symbolic music driven by musical emotion. The principal\nnovelty of our approach centres on conditioning a state-of-the-art transformer\nbased on continuous-valued valence and arousal labels. In addition, we provide\na new large-scale dataset of symbolic music paired with emotion labels in terms\nof valence and arousal. We evaluate our approach in a quantitative manner in\ntwo ways, first by measuring its note prediction accuracy, and second via a\nregression task in the valence-arousal plane. Our results demonstrate that our\nproposed approaches outperform conditioning using control tokens which is\nrepresentative of the current state of the art.", "published": "2022-03-30 09:38:09", "link": "http://arxiv.org/abs/2203.16165v2", "categories": ["eess.AS", "cs.AI", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Phase-Aware Deep Speech Enhancement: It's All About The Frame Length", "abstract": "Algorithmic latency in speech processing is dominated by the frame length\nused for Fourier analysis, which in turn limits the achievable performance of\nmagnitude-centric approaches. As previous studies suggest the importance of\nphase grows with decreasing frame length, this work presents a systematical\nstudy on the contribution of phase and magnitude in modern Deep Neural Network\n(DNN)-based speech enhancement at different frame lengths. Results indicate\nthat DNNs can successfully estimate phase when using short frames, with similar\nor better overall performance compared to using longer frames. Thus,\ninterestingly, modern phase-aware DNNs allow for low-latency speech enhancement\nat high quality.", "published": "2022-03-30 11:51:30", "link": "http://arxiv.org/abs/2203.16222v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End to End Lip Synchronization with a Temporal AutoEncoder", "abstract": "We study the problem of syncing the lip movement in a video with the audio\nstream. Our solution finds an optimal alignment using a dual-domain recurrent\nneural network that is trained on synthetic data we generate by dropping and\nduplicating video frames. Once the alignment is found, we modify the video in\norder to sync the two sources. Our method is shown to greatly outperform the\nliterature methods on a variety of existing and new benchmarks. As an\napplication, we demonstrate our ability to robustly align text-to-speech\ngenerated audio with an existing video stream. Our code and samples are\navailable at\nhttps://github.com/itsyoavshalev/End-to-End-Lip-Synchronization-with-a-Temporal-AutoEncoder.", "published": "2022-03-30 12:00:18", "link": "http://arxiv.org/abs/2203.16224v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Does Audio Deepfake Detection Generalize?", "abstract": "Current text-to-speech algorithms produce realistic fakes of human voices,\nmaking deepfake detection a much-needed area of research. While researchers\nhave presented various techniques for detecting audio spoofs, it is often\nunclear exactly why these architectures are successful: Preprocessing steps,\nhyperparameter settings, and the degree of fine-tuning are not consistent\nacross related work. Which factors contribute to success, and which are\naccidental? In this work, we address this problem: We systematize audio\nspoofing detection by re-implementing and uniformly evaluating architectures\nfrom related work. We identify overarching features for successful audio\ndeepfake detection, such as using cqtspec or logspec features instead of\nmelspec features, which improves performance by 37% EER on average, all other\nfactors constant. Additionally, we evaluate generalization capabilities: We\ncollect and publish a new dataset consisting of 37.9 hours of found audio\nrecordings of celebrities and politicians, of which 17.2 hours are deepfakes.\nWe find that related work performs poorly on such real-world data (performance\ndegradation of up to one thousand percent). This may suggest that the community\nhas tailored its solutions too closely to the prevailing ASVSpoof benchmark and\nthat deepfakes are much harder to detect outside the lab than previously\nthought.", "published": "2022-03-30 12:48:22", "link": "http://arxiv.org/abs/2203.16263v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustics-specific Piano Velocity Estimation", "abstract": "Motivated by the state-of-art psychological research, we note that a piano\nperformance transcribed with existing Automatic Music Transcription (AMT)\nmethods cannot be successfully resynthesized without affecting the artistic\ncontent of the performance. This is due to 1) the different mappings between\nMIDI parameters used by different instruments, and 2) the fact that musicians\nadapt their way of playing to the surrounding acoustic environment. To face\nthis issue, we propose a methodology to build acoustics-specific AMT systems\nthat are able to model the adaptations that musicians apply to convey their\ninterpretation. Specifically, we train models tailored for virtual instruments\nin a modular architecture that takes as input an audio recording and the\nrelative aligned music score, and outputs the acoustics-specific velocities of\neach note. We test different model shapes and show that the proposed\nmethodology generally outperforms the usual AMT pipeline which does not\nconsider specificities of the instrument and of the acoustic environment.\nInterestingly, such a methodology is extensible in a straightforward way since\nonly slight efforts are required to train models for the inference of other\npiano parameters, such as pedaling.", "published": "2022-03-30 13:36:17", "link": "http://arxiv.org/abs/2203.16294v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Forensic Analysis and Localization of Multiply Compressed MP3 Audio\n  Using Transformers", "abstract": "Audio signals are often stored and transmitted in compressed formats. Among\nthe many available audio compression schemes, MPEG-1 Audio Layer III (MP3) is\nvery popular and widely used. Since MP3 is lossy it leaves characteristic\ntraces in the compressed audio which can be used forensically to expose the\npast history of an audio file. In this paper, we consider the scenario of audio\nsignal manipulation done by temporal splicing of compressed and uncompressed\naudio signals. We propose a method to find the temporal location of the splices\nbased on transformer networks. Our method identifies which temporal portions of\na audio signal have undergone single or multiple compression at the temporal\nframe level, which is the smallest temporal unit of MP3 compression. We tested\nour method on a dataset of 486,743 MP3 audio clips. Our method achieved higher\nperformance and demonstrated robustness with respect to different MP3 data when\ncompared with existing methods.", "published": "2022-03-30 17:32:37", "link": "http://arxiv.org/abs/2203.16499v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hybrid Handcrafted and Learnable Audio Representation for Analysis of\n  Speech Under Cognitive and Physical Load", "abstract": "As a neurophysiological response to threat or adverse conditions, stress can\naffect cognition, emotion and behaviour with potentially detrimental effects on\nhealth in the case of sustained exposure. Since the affective content of speech\nis inherently modulated by an individual's physical and mental state, a\nsubstantial body of research has been devoted to the study of paralinguistic\ncorrelates of stress-inducing task load. Historically, voice stress analysis\n(VSA) has been conducted using conventional digital signal processing (DSP)\ntechniques. Despite the development of modern methods based on deep neural\nnetworks (DNNs), accurately detecting stress in speech remains difficult due to\nthe wide variety of stressors and considerable variability in the individual\nstress perception. To that end, we introduce a set of five datasets for task\nload detection in speech. The voice recordings were collected as either\ncognitive or physical stress was induced in the cohort of volunteers, with a\ncumulative number of more than a hundred speakers. We used the datasets to\ndesign and evaluate a novel self-supervised audio representation that leverages\nthe effectiveness of handcrafted features (DSP-based) and the complexity of\ndata-driven DNN representations. Notably, the proposed approach outperformed\nboth extensive handcrafted feature sets and novel DNN-based audio\nrepresentation learning approaches.", "published": "2022-03-30 19:43:21", "link": "http://arxiv.org/abs/2203.16637v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Detection of Expressed Emotion from Five-Minute Speech\n  Samples: Challenges and Opportunities", "abstract": "We present a novel feasibility study on the automatic recognition of\nExpressed Emotion (EE), a family environment concept based on caregivers\nspeaking freely about their relative/family member. We describe an automated\napproach for determining the \\textit{degree of warmth}, a key component of EE,\nfrom acoustic and text features acquired from a sample of 37 recorded\ninterviews. These recordings, collected over 20 years ago, are derived from a\nnationally representative birth cohort of 2,232 British twin children and were\nmanually coded for EE. We outline the core steps of extracting usable\ninformation from recordings with highly variable audio quality and assess the\nefficacy of four machine learning approaches trained with different\ncombinations of acoustic and text features. Despite the challenges of working\nwith this legacy data, we demonstrated that the degree of warmth can be\npredicted with an $F_{1}$-score of \\textbf{61.5\\%}. In this paper, we summarise\nour learning and provide recommendations for future work using real-world\nspeech samples.", "published": "2022-03-30 16:26:31", "link": "http://arxiv.org/abs/2203.17242v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech and the n-Back task as a lens into depression. How combining both\n  may allow us to isolate different core symptoms of depression", "abstract": "Embedded in any speech signal is a rich combination of cognitive,\nneuromuscular and physiological information. This richness makes speech a\npowerful signal in relation to a range of different health conditions,\nincluding major depressive disorders (MDD). One pivotal issue in\nspeech-depression research is the assumption that depressive severity is the\ndominant measurable effect. However, given the heterogeneous clinical profile\nof MDD, it may actually be the case that speech alterations are more strongly\nassociated with subsets of key depression symptoms. This paper presents strong\nevidence in support of this argument. First, we present a novel large,\ncross-sectional, multi-modal dataset collected at Thymia. We then present a set\nof machine learning experiments that demonstrate that combining speech with\nfeatures from an n-Back working memory assessment improves classifier\nperformance when predicting the popular eight-item Patient Health Questionnaire\ndepression scale (PHQ-8). Finally, we present a set of experiments that\nhighlight the association between different speech and n-Back markers at the\nPHQ-8 item level. Specifically, we observe that somatic and psychomotor\nsymptoms are more strongly associated with n-Back performance scores, whilst\nthe other items: anhedonia, depressed mood, change in appetite, feelings of\nworthlessness and trouble concentrating are more strongly associated with\nspeech changes.", "published": "2022-03-30 09:12:59", "link": "http://arxiv.org/abs/2204.00088v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
