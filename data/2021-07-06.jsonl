{"title": "Probabilistic Graph Reasoning for Natural Proof Generation", "abstract": "In this paper, we investigate the problem of reasoning over natural language\nstatements. Prior neural based approaches do not explicitly consider the\ninter-dependency among answers and their proofs. In this paper, we propose\nPRobr, a novel approach for joint answer prediction and proof generation. PRobr\ndefines a joint probabilistic distribution over all possible proof graphs and\nanswers via an induced graphical model. We then optimize the model using\nvariational approximation on top of neural textual representation. Experiments\non multiple datasets under diverse settings (fully supervised, few-shot and\nzero-shot evaluation) verify the effectiveness of PRobr, e.g., achieving\n10%-30% improvement on QA accuracy in few/zero-shot evaluation. Our codes and\nmodels can be found at https://github.com/changzhisun/PRobr/.", "published": "2021-07-06 06:34:41", "link": "http://arxiv.org/abs/2107.02418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An NLG pipeline for a legal expert system: a work in progress", "abstract": "We present the NLG component for L4, a prototype domain-specific language\n(DSL) for drafting laws and contracts. As a concrete use case, we describe a\npipeline for a legal expert system created from L4 code. The NLG component is\nused in two steps. The first step is to create an interview, whose answers are\nprocessed into a query for an automated reasoner. The second step is to render\nthe answers of the reasoner in natural language.", "published": "2021-07-06 06:42:38", "link": "http://arxiv.org/abs/2107.02421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The NiuTrans End-to-End Speech Translation System for IWSLT 2021 Offline\n  Task", "abstract": "This paper describes the submission of the NiuTrans end-to-end speech\ntranslation system for the IWSLT 2021 offline task, which translates from the\nEnglish audio to German text directly without intermediate transcription. We\nuse the Transformer-based model architecture and enhance it by Conformer,\nrelative position encoding, and stacked acoustic and textual encoding. To\naugment the training data, the English transcriptions are translated to German\ntranslations. Finally, we employ ensemble decoding to integrate the predictions\nfrom several models trained with the different datasets. Combining these\ntechniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which\nshows the enormous potential of the end-to-end model.", "published": "2021-07-06 07:45:23", "link": "http://arxiv.org/abs/2107.02444v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Improving Results on Russian Sentiment Datasets", "abstract": "In this study, we test transfer learning approach on Russian sentiment\nbenchmark datasets using additional train sample created with distant\nsupervision technique. We compare several variants of combining additional data\nwith benchmark train samples. The best results were achieved using three-step\napproach of sequential training on general, thematic and original train\nsamples. For most datasets, the results were improved by more than 3% to the\ncurrent state-of-the-art methods. The BERT-NLI model treating sentiment\nclassification problem as a natural language inference task reached the human\nlevel of sentiment analysis on one of the datasets.", "published": "2021-07-06 09:31:36", "link": "http://arxiv.org/abs/2107.02499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Modeling in the Voynich Manuscript", "abstract": "This article presents the results of investigations using topic modeling of\nthe Voynich Manuscript (Beinecke MS408). Topic modeling is a set of\ncomputational methods which are used to identify clusters of subjects within\ntext. We use latent dirichlet allocation, latent semantic analysis, and\nnonnegative matrix factorization to cluster Voynich pages into `topics'. We\nthen compare the topics derived from the computational models to clusters\nderived from the Voynich illustrations and from paleographic analysis. We find\nthat computationally derived clusters match closely to a conjunction of scribe\nand subject matter (as per the illustrations), providing further evidence that\nthe Voynich Manuscript contains meaningful text.", "published": "2021-07-06 19:50:03", "link": "http://arxiv.org/abs/2107.02858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Kosp2e: Korean Speech to English Translation Corpus", "abstract": "Most speech-to-text (S2T) translation studies use English speech as a source,\nwhich makes it difficult for non-English speakers to take advantage of the S2T\ntechnologies. For some languages, this problem was tackled through corpus\nconstruction, but the farther linguistically from English or the more\nunder-resourced, this deficiency and underrepresentedness becomes more\nsignificant. In this paper, we introduce kosp2e (read as `kospi'), a corpus\nthat allows Korean speech to be translated into English text in an end-to-end\nmanner. We adopt open license speech recognition corpus, translation corpus,\nand spoken language corpora to make our dataset freely available to the public,\nand check the performance through the pipeline and training-based approaches.\nUsing pipeline and various end-to-end schemes, we obtain the highest BLEU of\n21.3 and 18.0 for each based on the English hypothesis, validating the\nfeasibility of our data. We plan to supplement annotations for other target\nlanguages through community contributions in the future.", "published": "2021-07-06 20:34:06", "link": "http://arxiv.org/abs/2107.02875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying negativity factors from social media text corpus using\n  sentiment analysis method", "abstract": "Automatic sentiment analysis play vital role in decision making. Many\norganizations spend a lot of budget to understand their customer satisfaction\nby manually going over their feedback/comments or tweets. Automatic sentiment\nanalysis can give overall picture of the comments received against any event,\nproduct, or activity. Usually, the comments/tweets are classified into two main\nclasses that are negative or positive. However, the negative comments are too\nabstract to understand the basic reason or the context. organizations are\ninterested to identify the exact reason for the negativity. In this research\nstudy, we hierarchically goes down into negative comments, and link them with\nmore classes. Tweets are extracted from social media sites such as Twitter and\nFacebook. If the sentiment analysis classifies any tweet into negative class,\nthen we further try to associates that negative comments with more possible\nnegative classes. Based on expert opinions, the negative comments/tweets are\nfurther classified into 8 classes. Different machine learning algorithms are\nevaluated and their accuracy are reported.", "published": "2021-07-06 10:15:31", "link": "http://arxiv.org/abs/2107.02175v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhanced Universal Dependency Parsing with Automated Concatenation of\n  Embeddings", "abstract": "This paper describes the system used in submission from SHANGHAITECH team to\nthe IWPT 2021 Shared Task. Our system is a graph-based parser with the\ntechnique of Automated Concatenation of Embeddings (ACE). Because recent work\nfound that better word representations can be obtained by concatenating\ndifferent types of embeddings, we use ACE to automatically find the better\nconcatenation of embeddings for the task of enhanced universal dependencies.\nAccording to official results averaged on 17 languages, our system ranks 2nd\nover 9 teams.", "published": "2021-07-06 06:33:42", "link": "http://arxiv.org/abs/2107.02416v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sangrahaka: A Tool for Annotating and Querying Knowledge Graphs", "abstract": "In this work, we present a web-based annotation and querying tool Sangrahaka.\nIt annotates entities and relationships from text corpora and constructs a\nknowledge graph (KG). The KG is queried using templatized natural language\nqueries. The application is language and corpus agnostic, but can be tuned for\nspecial needs of a specific language or a corpus. A customized version of the\nframework has been used in two annotation tasks. The application is available\nfor download and installation. Besides having a user-friendly interface, it is\nfast, supports customization, and is fault tolerant on both client and server\nside. The code is available at https://github.com/hrishikeshrt/sangrahaka and\nthe presentation with a demo is available at https://youtu.be/nw9GFLVZMMo.", "published": "2021-07-06 17:44:34", "link": "http://arxiv.org/abs/2107.02782v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Question Answering over Knowledge Graphs with Neural Machine Translation\n  and Entity Linking", "abstract": "The goal of Question Answering over Knowledge Graphs (KGQA) is to find\nanswers for natural language questions over a knowledge graph. Recent KGQA\napproaches adopt a neural machine translation (NMT) approach, where the natural\nlanguage question is translated into a structured query language. However, NMT\nsuffers from the out-of-vocabulary problem, where terms in a question may not\nhave been seen during training, impeding their translation. This issue is\nparticularly problematic for the millions of entities that large knowledge\ngraphs describe. We rather propose a KGQA approach that delegates the\nprocessing of entities to entity linking (EL) systems. NMT is then used to\ncreate a query template with placeholders that are filled by entities\nidentified in an EL phase. Slot filling is used to decide which entity fills\nwhich placeholder. Experiments for QA over Wikidata show that our approach\noutperforms pure NMT: while there remains a strong dependence on having seen\nsimilar query templates during training, errors relating to entities are\ngreatly reduced.", "published": "2021-07-06 19:57:02", "link": "http://arxiv.org/abs/2107.02865v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "PhotoChat: A Human-Human Dialogue Dataset with Photo Sharing Behavior\n  for Joint Image-Text Modeling", "abstract": "We present a new human-human dialogue dataset - PhotoChat, the first dataset\nthat casts light on the photo sharing behavior in onlin emessaging. PhotoChat\ncontains 12k dialogues, each of which is paired with a user photo that is\nshared during the conversation. Based on this dataset, we propose two tasks to\nfacilitate research on image-text modeling: a photo-sharing intent prediction\ntask that predicts whether one intends to share a photo in the next\nconversation turn, and a photo retrieval task that retrieves the most relevant\nphoto according to the dialogue context. In addition, for both tasks, we\nprovide baseline models using the state-of-the-art models and report their\nbenchmark performances. The best image retrieval model achieves 10.4% recall@1\n(out of 1000 candidates) and the best photo intent prediction model achieves\n58.1% F1 score, indicating that the dataset presents interesting yet\nchallenging real-world problems. We are releasing PhotoChat to facilitate\nfuture research work among the community.", "published": "2021-07-06 14:03:04", "link": "http://arxiv.org/abs/2108.01453v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on\n  Active Learning for Visual Question Answering", "abstract": "Active learning promises to alleviate the massive data needs of supervised\nmachine learning: it has successfully improved sample efficiency by an order of\nmagnitude on traditional tasks like topic classification and object\nrecognition. However, we uncover a striking contrast to this promise: across 5\nmodels and 4 datasets on the task of visual question answering, a wide variety\nof active learning approaches fail to outperform random selection. To\nunderstand this discrepancy, we profile 8 active learning methods on a\nper-example basis, and identify the problem as collective outliers -- groups of\nexamples that active learning methods prefer to acquire but models fail to\nlearn (e.g., questions that ask about text in images or require external\nknowledge). Through systematic ablation experiments and qualitative\nvisualizations, we verify that collective outliers are a general phenomenon\nresponsible for degrading pool-based active learning. Notably, we show that\nactive learning sample efficiency increases significantly as the number of\ncollective outliers in the active learning pool decreases. We conclude with a\ndiscussion and prescriptive recommendations for mitigating the effects of these\noutliers in future work.", "published": "2021-07-06 00:52:11", "link": "http://arxiv.org/abs/2107.02331v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Empowering NGOs in Countering Online Hate Messages", "abstract": "Studies on online hate speech have mostly focused on the automated detection\nof harmful messages. Little attention has been devoted so far to the\ndevelopment of effective strategies to fight hate speech, in particular through\nthe creation of counter-messages. While existing manual scrutiny and\nintervention strategies are time-consuming and not scalable, advances in\nnatural language processing have the potential to provide a systematic approach\nto hatred management. In this paper, we introduce a novel ICT platform that NGO\noperators can use to monitor and analyze social media data, along with a\ncounter-narrative suggestion tool. Our platform aims at increasing the\nefficiency and effectiveness of operators' activities against islamophobia. We\ntest the platform with more than one hundred NGO operators in three countries\nthrough qualitative and quantitative evaluation. Results show that NGOs favor\nthe platform solution with the suggestion tool, and that the time required to\nproduce counter-narratives significantly decreases.", "published": "2021-07-06 08:36:24", "link": "http://arxiv.org/abs/2107.02472v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Location, Location: Enhancing the Evaluation of Text-to-Speech Synthesis\n  Using the Rapid Prosody Transcription Paradigm", "abstract": "Text-to-Speech synthesis systems are generally evaluated using Mean Opinion\nScore (MOS) tests, where listeners score samples of synthetic speech on a\nLikert scale. A major drawback of MOS tests is that they only offer a general\nmeasure of overall quality-i.e., the naturalness of an utterance-and so cannot\ntell us where exactly synthesis errors occur. This can make evaluation of the\nappropriateness of prosodic variation within utterances inconclusive. To\naddress this, we propose a novel evaluation method based on the Rapid Prosody\nTranscription paradigm. This allows listeners to mark the locations of errors\nin an utterance in real-time, providing a probabilistic representation of the\nperceptual errors that occur in the synthetic signal. We conduct experiments\nthat confirm that the fine-grained evaluation can be mapped to system rankings\nof standard MOS tests, but the error marking gives a much more comprehensive\nassessment of synthesized prosody. In particular, for standard audiobook test\nset samples, we see that error marks consistently cluster around words at major\nprosodic boundaries indicated by punctuation. However, for question-answer\nbased stimuli, where we control information structure, we see differences\nemerge in the ability of neural TTS systems to generate context-appropriate\nprosodic prominence.", "published": "2021-07-06 10:36:40", "link": "http://arxiv.org/abs/2107.02527v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style", "abstract": "While recent text to speech (TTS) models perform very well in synthesizing\nreading-style (e.g., audiobook) speech, it is still challenging to synthesize\nspontaneous-style speech (e.g., podcast or conversation), mainly because of two\nreasons: 1) the lack of training data for spontaneous speech; 2) the difficulty\nin modeling the filled pauses (um and uh) and diverse rhythms in spontaneous\nspeech. In this paper, we develop AdaSpeech 3, an adaptive TTS system that\nfine-tunes a well-trained reading-style TTS model for spontaneous-style speech.\nSpecifically, 1) to insert filled pauses (FP) in the text sequence\nappropriately, we introduce an FP predictor to the TTS model; 2) to model the\nvarying rhythms, we introduce a duration predictor based on mixture of experts\n(MoE), which contains three experts responsible for the generation of fast,\nmedium and slow speech respectively, and fine-tune it as well as the pitch\npredictor for rhythm adaptation; 3) to adapt to other speaker timbre, we\nfine-tune some parameters in the decoder with few speech data. To address the\nchallenge of lack of training data, we mine a spontaneous speech dataset to\nsupport our research this work and facilitate future research on spontaneous\nTTS. Experiments show that AdaSpeech 3 synthesizes speech with natural FP and\nrhythms in spontaneous styles, and achieves much better MOS and SMOS scores\nthan previous adaptive TTS systems.", "published": "2021-07-06 10:40:45", "link": "http://arxiv.org/abs/2107.02530v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VidLanKD: Improving Language Understanding via Video-Distilled Knowledge\n  Transfer", "abstract": "Since visual perception can give rich information beyond text descriptions\nfor world understanding, there has been increasing interest in leveraging\nvisual grounding for language learning. Recently, vokenization (Tan and Bansal,\n2020) has attracted attention by using the predictions of a text-to-image\nretrieval model as labels for language model supervision. Despite its success,\nthe method suffers from approximation error of using finite image labels and\nthe lack of vocabulary diversity of a small image-text dataset. To overcome\nthese limitations, we present VidLanKD, a video-language knowledge distillation\nmethod for improving language understanding. We train a multi-modal teacher\nmodel on a video-text dataset, and then transfer its knowledge to a student\nlanguage model with a text dataset. To avoid approximation error, we propose to\nuse different knowledge distillation objectives. In addition, the use of a\nlarge-scale video-text dataset helps learn diverse and richer vocabularies. In\nour experiments, VidLanKD achieves consistent improvements over text-only\nlanguage models and vokenization models, on several downstream language\nunderstanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the\nimproved world knowledge, physical reasoning, and temporal reasoning\ncapabilities of our model by evaluating on the GLUE-diagnostics, PIQA, and\nTRACIE datasets. Lastly, we present comprehensive ablation studies as well as\nvisualizations of the learned text-to-video grounding results of our teacher\nand student language models. Our code and models are available at:\nhttps://github.com/zinengtang/VidLanKD", "published": "2021-07-06 15:41:32", "link": "http://arxiv.org/abs/2107.02681v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Coherence and Consistency in Neural Sequence Models with\n  Dual-System, Neuro-Symbolic Reasoning", "abstract": "Human reasoning can often be understood as an interplay between two systems:\nthe intuitive and associative (\"System 1\") and the deliberative and logical\n(\"System 2\"). Neural sequence models -- which have been increasingly successful\nat performing complex, structured tasks -- exhibit the advantages and failure\nmodes of System 1: they are fast and learn patterns from data, but are often\ninconsistent and incoherent. In this work, we seek a lightweight, training-free\nmeans of improving existing System 1-like sequence models by adding System\n2-inspired logical reasoning. We explore several variations on this theme in\nwhich candidate generations from a neural sequence model are examined for\nlogical consistency by a symbolic reasoning module, which can either accept or\nreject the generations. Our approach uses neural inference to mediate between\nthe neural System 1 and the logical System 2. Results in robust story\ngeneration and grounded instruction-following show that this approach can\nincrease the coherence and accuracy of neurally-based generations.", "published": "2021-07-06 17:59:49", "link": "http://arxiv.org/abs/2107.02794v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Comparative Study of Modular and Joint Approaches for\n  Speaker-Attributed ASR on Monaural Long-Form Audio", "abstract": "Speaker-attributed automatic speech recognition (SA-ASR) is a task to\nrecognize \"who spoke what\" from multi-talker recordings. An SA-ASR system\nusually consists of multiple modules such as speech separation, speaker\ndiarization and ASR. On the other hand, considering the joint optimization, an\nend-to-end (E2E) SA-ASR model has recently been proposed with promising results\non simulation data. In this paper, we present our recent study on the\ncomparison of such modular and joint approaches towards SA-ASR on real monaural\nrecordings. We develop state-of-the-art SA-ASR systems for both modular and\njoint approaches by leveraging large-scale training data, including 75 thousand\nhours of ASR training data and the VoxCeleb corpus for speaker representation\nlearning. We also propose a new pipeline that performs the E2E SA-ASR model\nafter speaker clustering. Our evaluation on the AMI meeting corpus reveals that\nafter fine-tuning with a small real data, the joint system performs 8.9--29.9%\nbetter in accuracy compared to the best modular system while the modular system\nperforms better before such fine-tuning. We also conduct various error analyses\nto show the remaining issues for the monaural SA-ASR.", "published": "2021-07-06 19:36:48", "link": "http://arxiv.org/abs/2107.02852v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploiting Single-Channel Speech For Multi-channel End-to-end Speech\n  Recognition", "abstract": "Recently, the end-to-end training approach for neural beamformer-supported\nmulti-channel ASR has shown its effectiveness in multi-channel speech\nrecognition. However, the integration of multiple modules makes it more\ndifficult to perform end-to-end training, particularly given that the\nmulti-channel speech corpus recorded in real environments with a sizeable data\nscale is relatively limited. This paper explores the usage of single-channel\ndata to improve the multi-channel end-to-end speech recognition system.\nSpecifically, we design three schemes to exploit the single-channel data,\nnamely pre-training, data scheduling, and data simulation. Extensive\nexperiments on CHiME4 and AISHELL-4 datasets demonstrate that all three methods\nimprove the multi-channel end-to-end training stability and speech recognition\nperformance, while the data scheduling approach keeps a much simpler pipeline\n(vs. pre-training) and less computation cost (vs. data simulation). Moreover,\nwe give a thorough analysis of our systems, including how the performance is\naffected by the choice of front-end, the data augmentation, training strategy,\nand single-channel data size.", "published": "2021-07-06 15:21:41", "link": "http://arxiv.org/abs/2107.02670v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Separation Guided Speaker Diarization in Realistic Mismatched Conditions", "abstract": "We propose a separation guided speaker diarization (SGSD) approach by fully\nutilizing a complementarity of speech separation and speaker clustering. Since\nthe conventional clustering-based speaker diarization (CSD) approach cannot\nwell handle overlapping speech segments, we investigate, in this study,\nseparation-based speaker diarization (SSD) which inherently has the potential\nto handle the speaker overlap regions. Our preliminary analysis shows that the\nstate-of-the-art Conv-TasNet based speech separation, which works quite well on\nthe simulation data, is unstable in realistic conversational speech due to the\nhigh mismatch speaking styles in simulated training speech and read speech. In\ndoing so, separation-based processing can assist CSD in handling the\noverlapping speech segments under the realistic mismatched conditions.\nSpecifically, several strategies are designed to select between the results of\nSSD and CSD systems based on an analysis of the instability of the SSD system\nperformances. Experiments on the conversational telephone speech (CTS) data\nfrom DIHARD-III Challenge show that the proposed SGSD system can significantly\nimprove the performance of state-of-the-art CSD systems, yielding relative\ndiarization error rate reductions of 20.2% and 20.8% on the development set and\nevaluation set, respectively.", "published": "2021-07-06 02:39:32", "link": "http://arxiv.org/abs/2107.02357v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-training with noisy student model and semi-supervised loss function\n  for dcase 2021 challenge task 4", "abstract": "This report proposes a polyphonic sound event detection (SED) method for the\nDCASE 2021 Challenge Task 4. The proposed SED model consists of two stages: a\nmean-teacher model for providing target labels regarding weakly labeled or\nunlabeled data and a self-training-based noisy student model for predicting\nstrong labels for sound events. The mean-teacher model, which is based on the\nresidual convolutional recurrent neural network (RCRNN) for the teacher and\nstudent model, is first trained using all the training data from a weakly\nlabeled dataset, an unlabeled dataset, and a strongly labeled synthetic\ndataset. Then, the trained mean-teacher model predicts the strong label to each\nof the weakly labeled and unlabeled datasets, which is brought to the noisy\nstudent model in the second stage of the proposed SED model. Here, the\nstructure of the noisy student model is identical to the RCRNN-based student\nmodel of the mean-teacher model in the first stage. Then, it is self-trained by\nadding feature noises, such as time-frequency shift, mixup, SpecAugment, and\ndropout-based model noise. In addition, a semi-supervised loss function is\napplied to train the noisy student model, which acts as label noise injection.\nThe performance of the proposed SED model is evaluated on the validation set of\nthe DCASE 2021 Challenge Task 4, and then, several ensemble models that combine\nfive-fold validation models with different hyperparameters of the\nsemi-supervised loss function are finally selected as our final models.", "published": "2021-07-06 12:11:16", "link": "http://arxiv.org/abs/2107.02569v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Energy Consumption of Deep Generative Audio Models", "abstract": "In most scientific domains, the deep learning community has largely focused\non the quality of deep generative models, resulting in highly accurate and\nsuccessful solutions. However, this race for quality comes at a tremendous\ncomputational cost, which incurs vast energy consumption and greenhouse gas\nemissions. At the heart of this problem are the measures that we use as a\nscientific community to evaluate our work. In this paper, we suggest relying on\na multi-objective measure based on Pareto optimality, which takes into account\nboth the quality of the model and its energy consumption. By applying our\nmeasure on the current state-of-the-art in generative audio models, we show\nthat it can drastically change the significance of the results. We believe that\nthis type of metric can be widely used by the community to evaluate their work,\nwhile putting computational cost -- and in fine energy consumption -- in the\nspotlight of deep learning research.", "published": "2021-07-06 13:52:27", "link": "http://arxiv.org/abs/2107.02621v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
