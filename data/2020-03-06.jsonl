{"title": "A Framework for the Computational Linguistic Analysis of Dehumanization", "abstract": "Dehumanization is a pernicious psychological process that often leads to\nextreme intergroup bias, hate speech, and violence aimed at targeted social\ngroups. Despite these serious consequences and the wealth of available data,\ndehumanization has not yet been computationally studied on a large scale.\nDrawing upon social psychology research, we create a computational linguistic\nframework for analyzing dehumanizing language by identifying linguistic\ncorrelates of salient components of dehumanization. We then apply this\nframework to analyze discussions of LGBTQ people in the New York Times from\n1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ\npeople over time. However, we find that the label homosexual has emerged to be\nmuch more strongly associated with dehumanizing attitudes than other labels,\nsuch as gay. Our proposed techniques highlight processes of linguistic\nvariation and change in discourses surrounding marginalized groups.\nFurthermore, the ability to analyze dehumanizing language at a large scale has\nimplications for automatically detecting and understanding media bias as well\nas abusive language online.", "published": "2020-03-06 03:02:12", "link": "http://arxiv.org/abs/2003.03014v2", "categories": ["cs.CL", "J.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Sensitive Data Detection and Classification in Spanish Clinical Text:\n  Experiments with BERT", "abstract": "Massive digital data processing provides a wide range of opportunities and\nbenefits, but at the cost of endangering personal data privacy. Anonymisation\nconsists in removing or replacing sensitive information from data, enabling its\nexploitation for different purposes while preserving the privacy of\nindividuals. Over the years, a lot of automatic anonymisation systems have been\nproposed; however, depending on the type of data, the target language or the\navailability of training documents, the task remains challenging still. The\nemergence of novel deep-learning models during the last two years has brought\nlarge improvements to the state of the art in the field of Natural Language\nProcessing. These advancements have been most noticeably led by BERT, a model\nproposed by Google in 2018, and the shared language models pre-trained on\nmillions of documents. In this paper, we use a BERT-based sequence labelling\nmodel to conduct a series of anonymisation experiments on several clinical\ndatasets in Spanish. We also compare BERT to other algorithms. The experiments\nshow that a simple BERT-based model with general-domain pre-training obtains\nhighly competitive results without any domain specific feature engineering.", "published": "2020-03-06 09:46:51", "link": "http://arxiv.org/abs/2003.03106v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morfessor EM+Prune: Improved Subword Segmentation with Expectation\n  Maximization and Pruning", "abstract": "Data-driven segmentation of words into subword units has been used in various\nnatural language processing applications such as automatic speech recognition\nand statistical machine translation for almost 20 years. Recently it has became\nmore widely adopted, as models based on deep neural networks often benefit from\nsubword units even for morphologically simpler languages. In this paper, we\ndiscuss and compare training algorithms for a unigram subword model, based on\nthe Expectation Maximization algorithm and lexicon pruning. Using English,\nFinnish, North Sami, and Turkish data sets, we show that this approach is able\nto find better solutions to the optimization problem defined by the Morfessor\nBaseline model than its original recursive training algorithm. The improved\noptimization also leads to higher morphological segmentation accuracy when\ncompared to a linguistic gold standard. We publish implementations of the new\nalgorithms in the widely-used Morfessor software package.", "published": "2020-03-06 10:58:59", "link": "http://arxiv.org/abs/2003.03131v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Practical Annotation Strategies for Question Answering Datasets", "abstract": "Annotating datasets for question answering (QA) tasks is very costly, as it\nrequires intensive manual labor and often domain-specific knowledge. Yet\nstrategies for annotating QA datasets in a cost-effective manner are scarce. To\nprovide a remedy for practitioners, our objective is to develop heuristic rules\nfor annotating a subset of questions, so that the annotation cost is reduced\nwhile maintaining both in- and out-of-domain performance. For this, we conduct\na large-scale analysis in order to derive practical recommendations. First, we\ndemonstrate experimentally that more training samples contribute often only to\na higher in-domain test-set performance, but do not help the model in\ngeneralizing to unseen datasets. Second, we develop a model-guided annotation\nstrategy: it makes a recommendation with regard to which subset of samples\nshould be annotated. Its effectiveness is demonstrated in a case study based on\ndomain customization of QA to a clinical setting. Here, remarkably, annotating\na stratified subset with only 1.2% of the original training set achieves 97.7%\nof the performance as if the complete dataset was annotated. Hence, the\nlabeling effort can be reduced immensely. Altogether, our work fulfills a\ndemand in practice when labeling budgets are limited and where thus\nrecommendations are needed for annotating QA datasets more cost-effectively.", "published": "2020-03-06 14:25:50", "link": "http://arxiv.org/abs/2003.03235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NYTWIT: A Dataset of Novel Words in the New York Times", "abstract": "We present the New York Times Word Innovation Types dataset, or NYTWIT, a\ncollection of over 2,500 novel English words published in the New York Times\nbetween November 2017 and March 2019, manually annotated for their class of\nnovelty (such as lexical derivation, dialectal variation, blending, or\ncompounding). We present baseline results for both uncontextual and contextual\nprediction of novelty class, showing that there is room for improvement even\nfor state-of-the-art NLP systems. We hope this resource will prove useful for\nlinguists and NLP practitioners by providing a real-world environment of novel\nword appearance.", "published": "2020-03-06 21:19:44", "link": "http://arxiv.org/abs/2003.03444v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parsing Thai Social Data: A New Challenge for Thai NLP", "abstract": "Dependency parsing (DP) is a task that analyzes text for syntactic structure\nand relationship between words. DP is widely used to improve natural language\nprocessing (NLP) applications in many languages such as English. Previous works\non DP are generally applicable to formally written languages. However, they do\nnot apply to informal languages such as the ones used in social networks.\nTherefore, DP has to be researched and explored with such social network data.\nIn this paper, we explore and identify a DP model that is suitable for Thai\nsocial network data. After that, we will identify the appropriate linguistic\nunit as an input. The result showed that, the transition based model called,\nimprove Elkared dependency parser outperform the others at UAS of 81.42%.", "published": "2020-03-06 08:18:13", "link": "http://arxiv.org/abs/2003.03069v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Neural Named Entity Recognition with Gazetteers", "abstract": "The goal of this work is to improve the performance of a neural named entity\nrecognition system by adding input features that indicate a word is part of a\nname included in a gazetteer. This article describes how to generate gazetteers\nfrom the Wikidata knowledge graph as well as how to integrate the information\ninto a neural NER system. Experiments reveal that the approach yields\nperformance gains in two distinct languages: a high-resource, word-based\nlanguage, English and a high-resource, character-based language, Chinese.\nExperiments were also performed in a low-resource language, Russian on a newly\nannotated Russian NER corpus from Reddit tagged with four core types and twelve\nextended types. This article reports a baseline score. It is a longer version\nof a paper in the 33rd FLAIRS conference (Song et al. 2020).", "published": "2020-03-06 08:29:37", "link": "http://arxiv.org/abs/2003.03072v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is POS Tagging Necessary or Even Helpful for Neural Dependency Parsing?", "abstract": "In the pre deep learning era, part-of-speech tags have been considered as\nindispensable ingredients for feature engineering in dependency parsing. But\nquite a few works focus on joint tagging and parsing models to avoid error\npropagation. In contrast, recent studies suggest that POS tagging becomes much\nless important or even useless for neural parsing, especially when using\ncharacter-based word representations. Yet there are not enough investigations\nfocusing on this issue, both empirically and linguistically. To answer this, we\ndesign and compare three typical multi-task learning framework, i.e.,\nShare-Loose, Share-Tight, and Stack, for joint tagging and parsing based on the\nstate-of-the-art biaffine parser. Considering that it is much cheaper to\nannotate POS tags than parse trees, we also investigate the utilization of\nlarge-scale heterogeneous POS tag data. We conduct experiments on both English\nand Chinese datasets, and the results clearly show that POS tagging (both\nhomogeneous and heterogeneous) can still significantly improve parsing\nperformance when using the Stack joint framework. We conduct detailed analysis\nand gain more insights from the linguistic aspect.", "published": "2020-03-06 13:47:30", "link": "http://arxiv.org/abs/2003.03204v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Role of Conceptualization in Commonsense Knowledge Graph\n  Construction", "abstract": "Commonsense knowledge graphs (CKGs) like Atomic and ASER are substantially\ndifferent from conventional KGs as they consist of much larger number of nodes\nformed by loosely-structured text, which, though, enables them to handle highly\ndiverse queries in natural language related to commonsense, leads to unique\nchallenges for automatic KG construction methods. Besides identifying relations\nabsent from the KG between nodes, such methods are also expected to explore\nabsent nodes represented by text, in which different real-world things, or\nentities, may appear. To deal with the innumerable entities involved with\ncommonsense in the real world, we introduce to CKG construction methods\nconceptualization, i.e., to view entities mentioned in text as instances of\nspecific concepts or vice versa. We build synthetic triples by\nconceptualization, and further formulate the task as triple classification,\nhandled by a discriminatory model with knowledge transferred from pretrained\nlanguage models and fine-tuned by negative sampling. Experiments demonstrate\nthat our methods can effectively identify plausible triples and expand the KG\nby triples of both new nodes and edges of high diversity and novelty.", "published": "2020-03-06 14:35:20", "link": "http://arxiv.org/abs/2003.03239v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distributional semantic modeling: a revised technique to train term/word\n  vector space models applying the ontology-related approach", "abstract": "We design a new technique for the distributional semantic modeling with a\nneural network-based approach to learn distributed term representations (or\nterm embeddings) - term vector space models as a result, inspired by the recent\nontology-related approach (using different types of contextual knowledge such\nas syntactic knowledge, terminological knowledge, semantic knowledge, etc.) to\nthe identification of terms (term extraction) and relations between them\n(relation extraction) called semantic pre-processing technology - SPT. Our\nmethod relies on automatic term extraction from the natural language texts and\nsubsequent formation of the problem-oriented or application-oriented (also\ndeeply annotated) text corpora where the fundamental entity is the term\n(includes non-compositional and compositional terms). This gives us an\nopportunity to changeover from distributed word representations (or word\nembeddings) to distributed term representations (or term embeddings). This\ntransition will allow to generate more accurate semantic maps of different\nsubject domains (also, of relations between input terms - it is useful to\nexplore clusters and oppositions, or to test your hypotheses about them). The\nsemantic map can be represented as a graph using Vec2graph - a Python library\nfor visualizing word embeddings (term embeddings in our case) as dynamic and\ninteractive graphs. The Vec2graph library coupled with term embeddings will not\nonly improve accuracy in solving standard NLP tasks, but also update the\nconventional concept of automated ontology development. The main practical\nresult of our work is the development kit (set of toolkits represented as web\nservice APIs and web application), which provides all necessary routines for\nthe basic linguistic pre-processing and the semantic pre-processing of the\nnatural language texts in Ukrainian for future training of term vector space\nmodels.", "published": "2020-03-06 18:27:39", "link": "http://arxiv.org/abs/2003.03350v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "S-APIR: News-based Business Sentiment Index", "abstract": "This paper describes our work on developing a new business sentiment index\nusing daily newspaper articles. We adopt a recurrent neural network (RNN) with\nGated Recurrent Units to predict the business sentiment of a given text. An RNN\nis initially trained on Economy Watchers Survey and then fine-tuned on news\ntexts for domain adaptation. Also, a one-class support vector machine is\napplied to filter out texts deemed irrelevant to business sentiment. Moreover,\nwe propose a simple approach to temporally analyzing how much and when any\ngiven factor influences the predicted business sentiment. The validity and\nutility of the proposed approaches are empirically demonstrated through a\nseries of experiments on Nikkei Newspaper articles published from 2013 to 2018.", "published": "2020-03-06 00:18:50", "link": "http://arxiv.org/abs/2003.02973v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Corpus for Detecting High-Context Medical Conditions in Intensive Care\n  Patient Notes Focusing on Frequently Readmitted Patients", "abstract": "A crucial step within secondary analysis of electronic health records (EHRs)\nis to identify the patient cohort under investigation. While EHRs contain\nmedical billing codes that aim to represent the conditions and treatments\npatients may have, much of the information is only present in the patient\nnotes. Therefore, it is critical to develop robust algorithms to infer\npatients' conditions and treatments from their written notes. In this paper, we\nintroduce a dataset for patient phenotyping, a task that is defined as the\nidentification of whether a patient has a given medical condition (also\nreferred to as clinical indication or phenotype) based on their patient note.\nNursing Progress Notes and Discharge Summaries from the Intensive Care Unit of\na large tertiary care hospital were manually annotated for the presence of\nseveral high-context phenotypes relevant to treatment and risk of\nre-hospitalization. This dataset contains 1102 Discharge Summaries and 1000\nNursing Progress Notes. Each Discharge Summary and Progress Note has been\nannotated by at least two expert human annotators (one clinical researcher and\none resident physician). Annotated phenotypes include treatment non-adherence,\nchronic pain, advanced/metastatic cancer, as well as 10 other phenotypes. This\ndataset can be utilized for academic and industrial research in medicine and\ncomputer science, particularly within the field of medical natural language\nprocessing.", "published": "2020-03-06 05:56:49", "link": "http://arxiv.org/abs/2003.03044v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Information Extraction with Limited Data", "abstract": "This paper presents a practical approach to fine-grained information\nextraction. Through plenty of experiences of authors in practically applying\ninformation extraction to business process automation, there can be found a\ncouple of fundamental technical challenges: (i) the availability of labeled\ndata is usually limited and (ii) highly detailed classification is required.\nThe main idea of our proposal is to leverage the concept of transfer learning,\nwhich is to reuse the pre-trained model of deep neural networks, with a\ncombination of common statistical classifiers to determine the class of each\nextracted term. To do that, we first exploit BERT to deal with the limitation\nof training data in real scenarios, then stack BERT with Convolutional Neural\nNetworks to learn hidden representation for classification. To validate our\napproach, we applied our model to an actual case of document processing, which\nis a process of competitive bids for government projects in Japan. We used 100\ndocuments for training and testing and confirmed that the model enables to\nextract fine-grained named entities with a detailed level of information\npreciseness specialized in the targeted business process, such as a department\nname of application receivers.", "published": "2020-03-06 08:08:20", "link": "http://arxiv.org/abs/2003.03064v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Quality of Word Embeddings on Sentiment Analysis Tasks", "abstract": "Word embeddings or distributed representations of words are being used in\nvarious applications like machine translation, sentiment analysis, topic\nidentification etc. Quality of word embeddings and performance of their\napplications depends on several factors like training method, corpus size and\nrelevance etc. In this study we compare performance of a dozen of pretrained\nword embedding models on lyrics sentiment analysis and movie review polarity\ntasks. According to our results, Twitter Tweets is the best on lyrics sentiment\nanalysis, whereas Google News and Common Crawl are the top performers on movie\npolarity analysis. Glove trained models slightly outrun those trained with\nSkipgram. Also, factors like topic relevance and size of corpus significantly\nimpact the quality of the models. When medium or large-sized text sets are\navailable, obtaining word embeddings from same training dataset is usually the\nbest choice.", "published": "2020-03-06 15:03:08", "link": "http://arxiv.org/abs/2003.03264v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language QA Approaches using Reasoning with External Knowledge", "abstract": "Question answering (QA) in natural language (NL) has been an important aspect\nof AI from its early days. Winograd's ``councilmen'' example in his 1972 paper\nand McCarthy's Mr. Hug example of 1976 highlights the role of external\nknowledge in NL understanding. While Machine Learning has been the go-to\napproach in NL processing as well as NL question answering (NLQA) for the last\n30 years, recently there has been an increasingly emphasized thread on NLQA\nwhere external knowledge plays an important role. The challenges inspired by\nWinograd's councilmen example, and recent developments such as the Rebooting AI\nbook, various NLQA datasets, research on knowledge acquisition in the NLQA\ncontext, and their use in various NLQA models have brought the issue of NLQA\nusing ``reasoning'' with external knowledge to the forefront. In this paper, we\npresent a survey of the recent work on them. We believe our survey will help\nestablish a bridge between multiple fields of AI, especially between (a) the\ntraditional fields of knowledge representation and reasoning and (b) the field\nof NL understanding and NLQA.", "published": "2020-03-06 21:28:44", "link": "http://arxiv.org/abs/2003.03446v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Brazilian Lyrics-Based Music Genre Classification Using a BLSTM Network", "abstract": "Organize songs, albums, and artists in groups with shared similarity could be\ndone with the help of genre labels. In this paper, we present a novel approach\nfor automatic classifying musical genre in Brazilian music using only the song\nlyrics. This kind of classification remains a challenge in the field of Natural\nLanguage Processing. We construct a dataset of 138,368 Brazilian song lyrics\ndistributed in 14 genres. We apply SVM, Random Forest and a Bidirectional Long\nShort-Term Memory (BLSTM) network combined with different word embeddings\ntechniques to address this classification task. Our experiments show that the\nBLSTM method outperforms the other models with an F1-score average of $0.48$.\nSome genres like \"gospel\", \"funk-carioca\" and \"sertanejo\", which obtained 0.89,\n0.70 and 0.69 of F1-score, respectively, can be defined as the most distinct\nand easy to classify in the Brazilian musical genres context.", "published": "2020-03-06 05:39:21", "link": "http://arxiv.org/abs/2003.05377v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML", "68T50(Primary), 68T05 (Secondary)", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "A Neural Network Based Framework for Archetypical Sound Synthesis", "abstract": "This paper describes a preliminary approach to algorithmically reproduce the\narchetypical structure adopted by humans to classify sounds. In particular, we\npropose an approach to predict the human perceived chaos/order level in a sound\nand synthesize new timbres that present the desired amount of this feature. We\nadopted a Neural Network based method, in order to exploit its inner\npredisposition to model perceptive and abstract features. We finally discuss\nthe obtained accuracy and possible implications in creative contexts.", "published": "2020-03-06 12:38:58", "link": "http://arxiv.org/abs/2003.03160v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Wavelet-based spatial audio framework", "abstract": "Ambisonics is a complete theory for spatial audio whose building blocks are\nthe spherical harmonics. Some of the drawbacks of low order Ambisonics, like\npoor source directivity and small sweet-spot, are directly related to the\nproperties of spherical harmonics. In this thesis we illustrate a novel spatial\naudio framework similar in spirit to Ambisonics that replaces the spherical\nharmonics by an alternative set of functions with compact support: the\nspherical wavelets. We develop a complete audio chain from encoding to\ndecoding, using discrete spherical wavelets built on a multiresolution mesh. We\nshow how the wavelet family and the decoding matrices to loudspeakers can be\ngenerated via numerical optimization. In particular, we present a decoding\nalgorithm optimizing acoustic and psychoacoustic parameters that can generate\ndecoding matrices to irregular layouts for both Ambisonics and the new wavelet\nformat. This audio workflow is directly compared with Ambisonics.", "published": "2020-03-06 15:48:26", "link": "http://arxiv.org/abs/2003.03287v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lightweight Speaker Verification for Online Identification of New\n  Speakers with Short Segments", "abstract": "Verifying if two audio segments belong to the same speaker has been recently\nput forward as a flexible way to carry out speaker identification, since it\ndoes not require to be re-trained when new speakers appear on the auditory\nscene. Although many of the current techniques have achieved high performances,\nthey require a considerably high amount of memory, and a specific minimum\nlength for their input audio segments. These requirements limit the\napplicability of these techniques in scenarios such as service robots, internet\nof things and virtual assistants, where computational resources are limited and\nthe users tend to speak in short segments. In this work we propose a\nBLSTM-based model that reaches a level of performance comparable to the current\nstate of the art when using short input audio segments, while requiring a\nconsiderably less amount of memory. Further, as far as we know, a complete\nspeaker identification system has not been reported using this verification\nparadigm. Thus, we present a complete online speaker identifier, based on a\nsimple voting system, that shows that the proposed BLSTM-based model achieves a\nsimilar performance at identifying speakers online compared to the current\nstate of the art.", "published": "2020-03-06 20:45:28", "link": "http://arxiv.org/abs/2003.03432v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Defense against adversarial attacks on spoofing countermeasures of ASV", "abstract": "Various forefront countermeasure methods for automatic speaker verification\n(ASV) with considerable performance in anti-spoofing are proposed in the\nASVspoof 2019 challenge. However, previous work has shown that countermeasure\nmodels are vulnerable to adversarial examples indistinguishable from natural\ndata. A good countermeasure model should not only be robust against spoofing\naudio, including synthetic, converted, and replayed audios; but counteract\ndeliberately generated examples by malicious adversaries. In this work, we\nintroduce a passive defense method, spatial smoothing, and a proactive defense\nmethod, adversarial training, to mitigate the vulnerability of ASV spoofing\ncountermeasure models against adversarial examples. This paper is among the\nfirst to use defense methods to improve the robustness of ASV spoofing\ncountermeasure models under adversarial attacks. The experimental results show\nthat these two defense methods positively help spoofing countermeasure models\ncounter adversarial examples.", "published": "2020-03-06 08:08:54", "link": "http://arxiv.org/abs/2003.03065v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-supervised Development of ASR Systems for Multilingual\n  Code-switched Speech in Under-resourced Languages", "abstract": "This paper reports on the semi-supervised development of acoustic and\nlanguage models for under-resourced, code-switched speech in five South African\nlanguages. Two approaches are considered. The first constructs four separate\nbilingual automatic speech recognisers (ASRs) corresponding to four different\nlanguage pairs between which speakers switch frequently. The second uses a\nsingle, unified, five-lingual ASR system that represents all the languages\n(English, isiZulu, isiXhosa, Setswana and Sesotho). We evaluate the\neffectiveness of these two approaches when used to add additional data to our\nextremely sparse training sets. Results indicate that batch-wise\nsemi-supervised training yields better results than a non-batch-wise approach.\nFurthermore, while the separate bilingual systems achieved better recognition\nperformance than the unified system, they benefited more from pseudo-labels\ngenerated by the five-lingual system than from those generated by the bilingual\nsystems.", "published": "2020-03-06 11:08:38", "link": "http://arxiv.org/abs/2003.03135v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "F.2.2; I.2.7"], "primary_category": "eess.AS"}
{"title": "Multi-Time-Scale Convolution for Emotion Recognition from Speech Audio\n  Signals", "abstract": "Robustness against temporal variations is important for emotion recognition\nfrom speech audio, since emotion is ex-pressed through complex spectral\npatterns that can exhibit significant local dilation and compression on the\ntime axis depending on speaker and context. To address this and potentially\nother tasks, we introduce the multi-time-scale (MTS) method to create\nflexibility towards temporal variations when analyzing time-frequency\nrepresentations of audio data. MTS extends convolutional neural networks with\nconvolution kernels that are scaled and re-sampled along the time axis, to\nincrease temporal flexibility without increasing the number of trainable\nparameters compared to standard convolutional layers. We evaluate MTS and\nstandard convolutional layers in different architectures for emotion\nrecognition from speech audio, using 4 datasets of different sizes. The results\nshow that the use of MTS layers consistently improves the generalization of\nnetworks of different capacity and depth, compared to standard convolution,\nespecially on smaller datasets", "published": "2020-03-06 12:28:04", "link": "http://arxiv.org/abs/2003.03375v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
