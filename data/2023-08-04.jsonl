{"title": "Prompt2Gaussia: Uncertain Prompt-learning for Script Event Prediction", "abstract": "Script Event Prediction (SEP) aims to predict the subsequent event for a\ngiven event chain from a candidate list. Prior research has achieved great\nsuccess by integrating external knowledge to enhance the semantics, but it is\nlaborious to acquisite the appropriate knowledge resources and retrieve the\nscript-related knowledge. In this paper, we regard public pre-trained language\nmodels as knowledge bases and automatically mine the script-related knowledge\nvia prompt-learning. Still, the scenario-diversity and label-ambiguity in\nscripts make it uncertain to construct the most functional prompt and label\ntoken in prompt learning, i.e., prompt-uncertainty and verbalizer-uncertainty.\nConsidering the innate ability of Gaussian distribution to express uncertainty,\nwe deploy the prompt tokens and label tokens as random variables following\nGaussian distributions, where a prompt estimator and a verbalizer estimator are\nproposed to estimate their probabilistic representations instead of\ndeterministic representations. We take the lead to explore prompt-learning in\nSEP and provide a fresh perspective to enrich the script semantics. Our method\nis evaluated on the most widely used benchmark and a newly proposed large-scale\none. Experiments show that our method, which benefits from knowledge evoked\nfrom pre-trained language models, outperforms prior baselines by 1.46\\% and\n1.05\\% on two benchmarks, respectively.", "published": "2023-08-04 01:34:46", "link": "http://arxiv.org/abs/2308.02103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Financial Text Emotion Mining: GCGTS -- A Character\n  Relationship-based Approach for Simultaneous Aspect-Opinion Pair Extraction", "abstract": "Aspect-Opinion Pair Extraction (AOPE) from Chinese financial texts is a\nspecialized task in fine-grained text sentiment analysis. The main objective is\nto extract aspect terms and opinion terms simultaneously from a diverse range\nof financial texts. Previous studies have mainly focused on developing grid\nannotation schemes within grid-based models to facilitate this extraction\nprocess. However, these methods often rely on character-level (token-level)\nfeature encoding, which may overlook the logical relationships between Chinese\ncharacters within words. To address this limitation, we propose a novel method\ncalled Graph-based Character-level Grid Tagging Scheme (GCGTS). The GCGTS\nmethod explicitly incorporates syntactic structure using Graph Convolutional\nNetworks (GCN) and unifies the encoding of characters within the same syntactic\nsemantic unit (Chinese word level). Additionally, we introduce an image\nconvolutional structure into the grid model to better capture the local\nrelationships between characters within evaluation units. This innovative\nstructure reduces the excessive reliance on pre-trained language models and\nemphasizes the modeling of structure and local relationships, thereby improving\nthe performance of the model on Chinese financial texts. Through comparative\nexperiments with advanced models such as Synchronous Double-channel Recurrent\nNetwork (SDRN) and Grid Tagging Scheme (GTS), the proposed GCGTS model\ndemonstrates significant improvements in performance.", "published": "2023-08-04 02:20:56", "link": "http://arxiv.org/abs/2308.02113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "You talk what you read: Understanding News Comment Behavior by\n  Dispositional and Situational Attribution", "abstract": "Many news comment mining studies are based on the assumption that comment is\nexplicitly linked to the corresponding news. In this paper, we observed that\nusers' comments are also heavily influenced by their individual characteristics\nembodied by the interaction history. Therefore, we position to understand news\ncomment behavior by considering both the dispositional factors from news\ninteraction history, and the situational factors from corresponding news. A\nthree-part encoder-decoder framework is proposed to model the generative\nprocess of news comment. The resultant dispositional and situational\nattribution contributes to understanding user focus and opinions, which are\nvalidated in applications of reader-aware news summarization and news\naspect-opinion forecasting.", "published": "2023-08-04 07:10:15", "link": "http://arxiv.org/abs/2308.02168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Fake to Hyperpartisan News Detection Using Domain Adaptation", "abstract": "Unsupervised Domain Adaptation (UDA) is a popular technique that aims to\nreduce the domain shift between two data distributions. It was successfully\napplied in computer vision and natural language processing. In the current\nwork, we explore the effects of various unsupervised domain adaptation\ntechniques between two text classification tasks: fake and hyperpartisan news\ndetection. We investigate the knowledge transfer from fake to hyperpartisan\nnews detection without involving target labels during training. Thus, we\nevaluate UDA, cluster alignment with a teacher, and cross-domain contrastive\nlearning. Extensive experiments show that these techniques improve performance,\nwhile including data augmentation further enhances the results. In addition, we\ncombine clustering and topic modeling algorithms with UDA, resulting in\nimproved performances compared to the initial UDA setup.", "published": "2023-08-04 07:58:48", "link": "http://arxiv.org/abs/2308.02185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ESRL: Efficient Sampling-based Reinforcement Learning for Sequence\n  Generation", "abstract": "Applying Reinforcement Learning (RL) to sequence generation models enables\nthe direct optimization of long-term rewards (\\textit{e.g.,} BLEU and human\nfeedback), but typically requires large-scale sampling over a space of action\nsequences. This is a computational challenge as presented by the practice of\nsequence generation problems, such as machine translation, where we often deal\nwith a large action space (\\textit{e.g.,} a vocabulary) and a long action\nsequence (\\textit{e.g.,} a translation). In this work, we introduce two-stage\nsampling and dynamic sampling approaches to improve the sampling efficiency\nduring training sequence generation models via RL. We experiment with our\napproaches on the traditional sequence generation tasks, including machine\ntranslation and abstractive summarization. Furthermore, we evaluate our\napproaches in RL from human feedback (RLHF) through training a large language\nmodel using the reward model. Experimental results show that the efficient\nsampling-based RL, referred to as ESRL, can outperform all baselines in terms\nof both training efficiency and memory consumption. Notably, ESRL yields\nconsistent performance gains over the strong REINFORCE, minimum risk training,\nand proximal policy optimization methods.", "published": "2023-08-04 09:35:45", "link": "http://arxiv.org/abs/2308.02223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Paraphrase Sentences to Different Complexity Levels", "abstract": "While sentence simplification is an active research topic in NLP, its\nadjacent tasks of sentence complexification and same-level paraphrasing are\nnot. To train models on all three tasks, we present two new unsupervised\ndatasets. We compare these datasets, one labeled by a weak classifier and the\nother by a rule-based approach, with a single supervised dataset. Using these\nthree datasets for training, we perform extensive experiments on both\nmultitasking and prompting strategies. Compared to other systems trained on\nunsupervised parallel data, models trained on our weak classifier labeled\ndataset achieve state-of-the-art performance on the ASSET simplification\nbenchmark. Our models also outperform previous work on sentence level\ntargeting. Finally, we establish how a handful of Large Language Models perform\non these tasks under a zero-shot setting.", "published": "2023-08-04 09:43:37", "link": "http://arxiv.org/abs/2308.02226v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sinhala-English Parallel Word Dictionary Dataset", "abstract": "Parallel datasets are vital for performing and evaluating any kind of\nmultilingual task. However, in the cases where one of the considered language\npairs is a low-resource language, the existing top-down parallel data such as\ncorpora are lacking in both tally and quality due to the dearth of human\nannotation. Therefore, for low-resource languages, it is more feasible to move\nin the bottom-up direction where finer granular pairs such as dictionary\ndatasets are developed first. They may then be used for mid-level tasks such as\nsupervised multilingual word embedding alignment. These in turn can later guide\nhigher-level tasks in the order of aligning sentence or paragraph text corpora\nused for Machine Translation (MT). Even though more approachable than\ngenerating and aligning a massive corpus for a low-resource language, for the\nsame reason of apathy from larger research entities, even these finer granular\ndata sets are lacking for some low-resource languages. We have observed that\nthere is no free and open dictionary data set for the low-resource language,\nSinhala. Thus, in this work, we introduce three parallel English-Sinhala word\ndictionaries (En-Si-dict-large, En-Si-dict-filtered, En-Si-dict-FastText) which\nhelp in multilingual Natural Language Processing (NLP) tasks related to English\nand Sinhala languages. In this paper, we explain the dataset creation pipeline\nas well as the experimental results of the tests we have carried out to verify\nthe quality of the data sets. The data sets and the related scripts are\navailable at https://github.com/kasunw22/sinhala-para-dict.", "published": "2023-08-04 10:21:35", "link": "http://arxiv.org/abs/2308.02234v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Redundancy Aware Multi-Reference Based Gainwise Evaluation of Extractive\n  Summarization", "abstract": "The ROUGE metric is commonly used to evaluate extractive summarization task,\nbut it has been criticized for its lack of semantic awareness and its ignorance\nabout the ranking quality of the extractive summarizer. Previous research has\nintroduced a gain-based automated metric called Sem-nCG that addresses these\nissues, as it is both rank and semantic aware. However, it does not consider\nthe amount of redundancy present in a model summary and currently does not\nsupport evaluation with multiple reference summaries. It is essential to have a\nmodel summary that balances importance and diversity, but finding a metric that\ncaptures both of these aspects is challenging. In this paper, we propose a\nredundancy-aware Sem-nCG metric and demonstrate how the revised Sem-nCG metric\ncan be used to evaluate model summaries against multiple references as well\nwhich was missing in previous research. Experimental results demonstrate that\nthe revised Sem-nCG metric has a stronger correlation with human judgments\ncompared to the previous Sem-nCG metric and traditional ROUGE and BERTScore\nmetric for both single and multiple reference scenarios.", "published": "2023-08-04 11:47:19", "link": "http://arxiv.org/abs/2308.02270v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dataflow Dialogue Generation", "abstract": "We demonstrate task-oriented dialogue generation within the dataflow dialogue\nparadigm. We show an example of agenda driven dialogue generation for the\nMultiWOZ domain, and an example of generation without an agenda for the\nSMCalFlow domain, where we show an improvement in the accuracy of the\ntranslation of user requests to dataflow expressions when the generated\ndialogues are used to augment the translation training dataset.", "published": "2023-08-04 13:40:54", "link": "http://arxiv.org/abs/2308.02323v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting the NICT-JLE Corpus for Disfluency Detection Models", "abstract": "The detection of disfluencies such as hesitations, repetitions and false\nstarts commonly found in speech is a widely studied area of research. With a\nstandardised process for evaluation using the Switchboard Corpus, model\nperformance can be easily compared across approaches. This is not the case for\ndisfluency detection research on learner speech, however, where such datasets\nhave restricted access policies, making comparison and subsequent development\nof improved models more challenging. To address this issue, this paper\ndescribes the adaptation of the NICT-JLE corpus, containing approximately 300\nhours of English learners' oral proficiency tests, to a format that is suitable\nfor disfluency detection model training and evaluation. Points of difference\nbetween the NICT-JLE and Switchboard corpora are explored, followed by a\ndetailed overview of adaptations to the tag set and meta-features of the\nNICT-JLE corpus. The result of this work provides a standardised train, heldout\nand test set for use in future research on disfluency detection for learner\nspeech.", "published": "2023-08-04 17:54:52", "link": "http://arxiv.org/abs/2308.02482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Good Are SOTA Fake News Detectors", "abstract": "Automatic fake news detection with machine learning can prevent the\ndissemination of false statements before they gain many views. Several datasets\nlabeling statements as legitimate or false have been created since the 2016\nUnited States presidential election for the prospect of training machine\nlearning models. We evaluate the robustness of both traditional and deep\nstate-of-the-art models to gauge how well they may perform in the real world.\nWe find that traditional models tend to generalize better to data outside the\ndistribution it was trained on compared to more recently-developed large\nlanguage models, though the best model to use may depend on the specific task\nat hand.", "published": "2023-08-04 22:14:19", "link": "http://arxiv.org/abs/2308.02727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Zero-Shot Instruction Following", "abstract": "This work proposes a challenging yet more realistic setting for zero-shot\ncross-task generalization: zero-shot instruction following, presuming the\nexistence of a paragraph-style task definition while no demonstrations exist.\nTo better learn the task supervision from the definition, we propose two\nstrategies: first, to automatically find out the critical sentences in the\ndefinition; second, a ranking objective to force the model to generate the gold\noutputs with higher probabilities when those critical parts are highlighted in\nthe definition. The joint efforts of the two strategies yield state-of-the-art\nperformance on the Super-NaturalInstructions. Our code is available on GitHub.", "published": "2023-08-04 21:44:16", "link": "http://arxiv.org/abs/2308.03795v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Legal Summarisation through LLMs: The PRODIGIT Project", "abstract": "We present some initial results of a large-scale Italian project called\nPRODIGIT which aims to support tax judges and lawyers through digital\ntechnology, focusing on AI. We have focused on generation of summaries of\njudicial decisions and on the extraction of related information, such as the\nidentification of legal issues and decision-making criteria, and the\nspecification of keywords. To this end, we have deployed and evaluated\ndifferent tools and approaches to extractive and abstractive summarisation. We\nhave applied LLMs, and particularly on GPT4, which has enabled us to obtain\nresults that proved satisfactory, according to an evaluation by expert tax\njudges and lawyers. On this basis, a prototype application is being built which\nwill be made publicly available.", "published": "2023-08-04 16:59:48", "link": "http://arxiv.org/abs/2308.04416v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned\n  Samples in NLP", "abstract": "Backdoor attacks have emerged as a prominent threat to natural language\nprocessing (NLP) models, where the presence of specific triggers in the input\ncan lead poisoned models to misclassify these inputs to predetermined target\nclasses. Current detection mechanisms are limited by their inability to address\nmore covert backdoor strategies, such as style-based attacks. In this work, we\npropose an innovative test-time poisoned sample detection framework that hinges\non the interpretability of model predictions, grounded in the semantic meaning\nof inputs. We contend that triggers (e.g., infrequent words) are not supposed\nto fundamentally alter the underlying semantic meanings of poisoned samples as\nthey want to stay stealthy. Based on this observation, we hypothesize that\nwhile the model's predictions for paraphrased clean samples should remain\nstable, predictions for poisoned samples should revert to their true labels\nupon the mutations applied to triggers during the paraphrasing process. We\nemploy ChatGPT, a state-of-the-art large language model, as our paraphraser and\nformulate the trigger-removal task as a prompt engineering problem. We adopt\nfuzzing, a technique commonly used for unearthing software vulnerabilities, to\ndiscover optimal paraphrase prompts that can effectively eliminate triggers\nwhile concurrently maintaining input semantics. Experiments on 4 types of\nbackdoor attacks, including the subtle style backdoors, and 4 distinct datasets\ndemonstrate that our approach surpasses baseline methods, including STRIP, RAP,\nand ONION, in precision and recall.", "published": "2023-08-04 03:48:28", "link": "http://arxiv.org/abs/2308.02122v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Tweet Insights: A Visualization Platform to Extract Temporal Insights\n  from Twitter", "abstract": "This paper introduces a large collection of time series data derived from\nTwitter, postprocessed using word embedding techniques, as well as specialized\nfine-tuned language models. This data comprises the past five years and\ncaptures changes in n-gram frequency, similarity, sentiment and topic\ndistribution. The interface built on top of this data enables temporal analysis\nfor detecting and characterizing shifts in meaning, including complementary\ninformation to trending metrics, such as sentiment and topic association over\ntime. We release an online demo for easy experimentation, and we share code and\nthe underlying aggregated data for future work. In this paper, we also discuss\nthree case studies unlocked thanks to our platform, showcasing its potential\nfor temporal linguistic analysis.", "published": "2023-08-04 05:39:26", "link": "http://arxiv.org/abs/2308.02142v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Retroformer: Retrospective Large Language Agents with Policy Gradient\n  Optimization", "abstract": "Recent months have seen the emergence of a powerful new trend in which large\nlanguage models (LLMs) are augmented to become autonomous language agents\ncapable of performing objective oriented multi-step tasks on their own, rather\nthan merely responding to queries from human users. Most existing language\nagents, however, are not optimized using environment-specific rewards. Although\nsome agents enable iterative refinement through verbal feedback, they do not\nreason and plan in ways that are compatible with gradient-based learning from\nrewards. This paper introduces a principled framework for reinforcing large\nlanguage agents by learning a retrospective model, which automatically tunes\nthe language agent prompts from environment feedback through policy gradient.\nSpecifically, our proposed agent architecture learns from rewards across\nmultiple environments and tasks, for fine-tuning a pre-trained language model\nwhich refines the language agent prompt by summarizing the root cause of prior\nfailed attempts and proposing action plans. Experimental results on various\ntasks demonstrate that the language agents improve over time and that our\napproach considerably outperforms baselines that do not properly leverage\ngradients from the environment. This demonstrates that using policy gradient\noptimization to improve language agents, for which we believe our work is one\nof the first, seems promising and can be applied to optimize other models in\nthe agent architecture to enhance agent performances over time.", "published": "2023-08-04 06:14:23", "link": "http://arxiv.org/abs/2308.02151v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speaker Diarization of Scripted Audiovisual Content", "abstract": "The media localization industry usually requires a verbatim script of the\nfinal film or TV production in order to create subtitles or dubbing scripts in\na foreign language. In particular, the verbatim script (i.e. as-broadcast\nscript) must be structured into a sequence of dialogue lines each including\ntime codes, speaker name and transcript. Current speech recognition technology\nalleviates the transcription step. However, state-of-the-art speaker\ndiarization models still fall short on TV shows for two main reasons: (i) their\ninability to track a large number of speakers, (ii) their low accuracy in\ndetecting frequent speaker changes. To mitigate this problem, we present a\nnovel approach to leverage production scripts used during the shooting process,\nto extract pseudo-labeled data for the speaker diarization task. We propose a\nnovel semi-supervised approach and demonstrate improvements of 51.7% relative\nto two unsupervised baseline models on our metrics on a 66 show test set.", "published": "2023-08-04 06:37:34", "link": "http://arxiv.org/abs/2308.02160v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Clinical Trial Matching Using Large Language Models: A Case\n  Study in Oncology", "abstract": "Clinical trial matching is a key process in health delivery and discovery. In\npractice, it is plagued by overwhelming unstructured data and unscalable manual\nprocessing. In this paper, we conduct a systematic study on scaling clinical\ntrial matching using large language models (LLMs), with oncology as the focus\narea. Our study is grounded in a clinical trial matching system currently in\ntest deployment at a large U.S. health network. Initial findings are promising:\nout of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate\neligibility criteria of clinical trials and extract complex matching logic\n(e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially\noutperform prior strong baselines and may serve as a preliminary solution to\nhelp triage patient-trial candidates with humans in the loop. Our study also\nreveals a few significant growth areas for applying LLMs to end-to-end clinical\ntrial matching, such as context limitation and accuracy, especially in\nstructuring patient information from longitudinal medical records.", "published": "2023-08-04 07:51:15", "link": "http://arxiv.org/abs/2308.02180v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explaining Relation Classification Models with Semantic Extents", "abstract": "In recent years, the development of large pretrained language models, such as\nBERT and GPT, significantly improved information extraction systems on various\ntasks, including relation classification. State-of-the-art systems are highly\naccurate on scientific benchmarks. A lack of explainability is currently a\ncomplicating factor in many real-world applications. Comprehensible systems are\nnecessary to prevent biased, counterintuitive, or harmful decisions.\n  We introduce semantic extents, a concept to analyze decision patterns for the\nrelation classification task. Semantic extents are the most influential parts\nof texts concerning classification decisions. Our definition allows similar\nprocedures to determine semantic extents for humans and models. We provide an\nannotation tool and a software framework to determine semantic extents for\nhumans and models conveniently and reproducibly. Comparing both reveals that\nmodels tend to learn shortcut patterns from data. These patterns are hard to\ndetect with current interpretability methods, such as input reductions. Our\napproach can help detect and eliminate spurious decision patterns during model\ndevelopment. Semantic extents can increase the reliability and security of\nnatural language processing systems. Semantic extents are an essential step in\nenabling applications in critical areas like healthcare or finance. Moreover,\nour work opens new research directions for developing methods to explain deep\nlearning models.", "published": "2023-08-04 08:17:52", "link": "http://arxiv.org/abs/2308.02193v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation\n  from Text", "abstract": "The recent advances in large language models (LLM) and foundation models with\nemergent capabilities have been shown to improve the performance of many NLP\ntasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs\ncan be used for KG construction or completion while existing KGs can be used\nfor different tasks such as making LLM outputs explainable or fact-checking in\nNeuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to\nevaluate the capabilities of language models to generate KGs from natural\nlanguage text guided by an ontology. Given an input ontology and a set of\nsentences, the task is to extract facts from the text while complying with the\ngiven ontology (concepts, relations, domain/range constraints) and being\nfaithful to the input sentences. We provide two datasets (i) Wikidata-TekGen\nwith 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19\nontologies and 4,860 sentences. We define seven evaluation metrics to measure\nfact extraction performance, ontology conformance, and hallucinations by LLMs.\nFurthermore, we provide results for two baseline models, Vicuna-13B and\nAlpaca-LoRA-13B using automatic prompt generation from test cases. The baseline\nresults show that there is room for improvement using both Semantic Web and\nNatural Language Processing techniques.", "published": "2023-08-04 14:47:15", "link": "http://arxiv.org/abs/2308.02357v1", "categories": ["cs.CL", "cs.AI", "68", "I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Generalist Foundation Model for Radiology by Leveraging\n  Web-scale 2D&3D Medical Data", "abstract": "In this study, we aim to initiate the development of Radiology Foundation\nModel, termed as RadFM. We consider the construction of foundational models\nfrom three perspectives, namely, dataset construction, model design, and\nthorough evaluation. Our contribution can be concluded as follows: (i), we\nconstruct a large-scale Medical Multi-modal Dataset, MedMD, which consists of\n16M 2D and 3D medical scans with high-quality text descriptions or reports\nacross various data formats, modalities, and tasks, covering over 5000 distinct\ndiseases. To the best of our knowledge, this is the first large-scale,\nhigh-quality, medical visual-language dataset, with both 2D and 3D scans; (ii),\nwe propose an architecture that enables visually conditioned generative\npre-training, i.e., allowing for integration of text input with 2D or 3D\nmedical scans, and generate responses for diverse radiologic tasks. The model\nwas initially pre-trained on MedMD and subsequently fine-tuned on the\ndomain-specific dataset, which is a radiologic cleaned version of MedMD,\ncontaining 3M radiologic visual-language pairs, termed as RadMD; (iii), we\npropose a new evaluation benchmark, RadBench, that comprises five tasks,\nincluding modality recognition, disease diagnosis, visual question answering,\nreport generation and rationale diagnosis, aiming to comprehensively assess the\ncapability of foundation models in handling practical clinical problems. We\nconduct both automatic and human evaluation on RadBench, in both cases, RadFM\noutperforms existing multi-modal foundation models, that are publicaly\naccessible, including Openflamingo, MedFlamingo, MedVInT and GPT-4V.\nAdditionally, we also adapt RadFM for different public benchmarks, surpassing\nexisting SOTAs on diverse datasets. All codes, data, and model checkpoint will\nall be made publicly available to promote further research and development in\nthe field.", "published": "2023-08-04 17:00:38", "link": "http://arxiv.org/abs/2308.02463v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for\n  Domain Adaptation on Text Classification", "abstract": "Text classification is a fundamental task for natural language processing,\nand adapting text classification models across domains has broad applications.\nSelf-training generates pseudo-examples from the model's predictions and\niteratively trains on the pseudo-examples, i.e., minimizes the loss on the\nsource domain and the Gibbs entropy on the target domain. However, Gibbs\nentropy is sensitive to prediction errors, and thus, self-training tends to\nfail when the domain shift is large. In this paper, we propose Meta-Tsallis\nEntropy minimization (MTEM), which applies a meta-learning algorithm to\noptimize the instance adaptive Tsallis entropy on the target domain. To reduce\nthe computation cost of MTEM, we propose an approximation technique to\napproximate the Second-order derivation involved in the meta-learning. To\nefficiently generate pseudo labels, we propose an annealing sampling mechanism\nfor exploring the model's prediction probability. Theoretically, we prove the\nconvergence of the meta-learning algorithm in MTEM and analyze the\neffectiveness of MTEM in achieving domain adaptation. Experimentally, MTEM\nimproves the adaptation performance of BERT with an average of 4 percent on the\nbenchmark dataset.", "published": "2023-08-04 23:50:58", "link": "http://arxiv.org/abs/2308.02746v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "N-gram Boosting: Improving Contextual Biasing with Normalized N-gram\n  Targets", "abstract": "Accurate transcription of proper names and technical terms is particularly\nimportant in speech-to-text applications for business conversations. These\nwords, which are essential to understanding the conversation, are often rare\nand therefore likely to be under-represented in text and audio training data,\ncreating a significant challenge in this domain. We present a two-step keyword\nboosting mechanism that successfully works on normalized unigrams and n-grams\nrather than just single tokens, which eliminates missing hits issues with\nboosting raw targets. In addition, we show how adjusting the boosting weight\nlogic avoids over-boosting multi-token keywords. This improves our keyword\nrecognition rate by 26% relative on our proprietary in-domain dataset and 2% on\nLibriSpeech. This method is particularly useful on targets that involve\nnon-alphabetic characters or have non-standard pronunciations.", "published": "2023-08-04 00:23:14", "link": "http://arxiv.org/abs/2308.02092v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus\n  Speech Emotion Recognition", "abstract": "Cross-corpus speech emotion recognition (SER) seeks to generalize the ability\nof inferring speech emotion from a well-labeled corpus to an unlabeled one,\nwhich is a rather challenging task due to the significant discrepancy between\ntwo corpora. Existing methods, typically based on unsupervised domain\nadaptation (UDA), struggle to learn corpus-invariant features by global\ndistribution alignment, but unfortunately, the resulting features are mixed\nwith corpus-specific features or not class-discriminative. To tackle these\nchallenges, we propose a novel Emotion Decoupling aNd Alignment learning\nframework (EMO-DNA) for cross-corpus SER, a novel UDA method to learn\nemotion-relevant corpus-invariant features. The novelties of EMO-DNA are\ntwo-fold: contrastive emotion decoupling and dual-level emotion alignment. On\none hand, our contrastive emotion decoupling achieves decoupling learning via a\ncontrastive decoupling loss to strengthen the separability of emotion-relevant\nfeatures from corpus-specific ones. On the other hand, our dual-level emotion\nalignment introduces an adaptive threshold pseudo-labeling to select confident\ntarget samples for class-level alignment, and performs corpus-level alignment\nto jointly guide model for learning class-discriminative corpus-invariant\nfeatures across corpora. Extensive experimental results demonstrate the\nsuperior performance of EMO-DNA over the state-of-the-art methods in several\ncross-corpus scenarios. Source code is available at\nhttps://github.com/Jiaxin-Ye/Emo-DNA.", "published": "2023-08-04 08:15:17", "link": "http://arxiv.org/abs/2308.02190v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Survey of Spanish Clinical Language Models", "abstract": "This survey focuses in encoder Language Models for solving tasks in the\nclinical domain in the Spanish language. We review the contributions of 17\ncorpora focused mainly in clinical tasks, then list the most relevant Spanish\nLanguage Models and Spanish Clinical Language models. We perform a thorough\ncomparison of these models by benchmarking them over a curated subset of the\navailable corpora, in order to find the best-performing ones; in total more\nthan 3000 models were fine-tuned for this study. All the tested corpora and the\nbest models are made publically available in an accessible way, so that the\nresults can be reproduced by independent teams or challenged in the future when\nnew Spanish Clinical Language models are created.", "published": "2023-08-04 08:33:07", "link": "http://arxiv.org/abs/2308.02199v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Monaural Speech Enhancement using Spectrum Attention Fusion", "abstract": "Speech enhancement is a demanding task in automated speech processing\npipelines, focusing on separating clean speech from noisy channels. Transformer\nbased models have recently bested RNN and CNN models in speech enhancement,\nhowever at the same time they are much more computationally expensive and\nrequire much more high quality training data, which is always hard to come by.\nIn this paper, we present an improvement for speech enhancement models that\nmaintains the expressiveness of self-attention while significantly reducing\nmodel complexity, which we have termed Spectrum Attention Fusion. We carefully\nconstruct a convolutional module to replace several self-attention layers in a\nspeech Transformer, allowing the model to more efficiently fuse spectral\nfeatures. Our proposed model is able to achieve comparable or better results\nagainst SOTA models but with significantly smaller parameters (0.58M) on the\nVoice Bank + DEMAND dataset.", "published": "2023-08-04 11:39:29", "link": "http://arxiv.org/abs/2308.02263v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Select the Relevant History Turns in Conversational Question\n  Answering", "abstract": "The increasing demand for the web-based digital assistants has given a rapid\nrise in the interest of the Information Retrieval (IR) community towards the\nfield of conversational question answering (ConvQA). However, one of the\ncritical aspects of ConvQA is the effective selection of conversational history\nturns to answer the question at hand. The dependency between relevant history\nselection and correct answer prediction is an intriguing but under-explored\narea. The selected relevant context can better guide the system so as to where\nexactly in the passage to look for an answer. Irrelevant context, on the other\nhand, brings noise to the system, thereby resulting in a decline in the model's\nperformance. In this paper, we propose a framework, DHS-ConvQA (Dynamic History\nSelection in Conversational Question Answering), that first generates the\ncontext and question entities for all the history turns, which are then pruned\non the basis of similarity they share in common with the question at hand. We\nalso propose an attention-based mechanism to re-rank the pruned terms based on\ntheir calculated weights of how useful they are in answering the question. In\nthe end, we further aid the model by highlighting the terms in the re-ranked\nconversational history using a binary classification task and keeping the\nuseful terms (predicted as 1) and ignoring the irrelevant terms (predicted as\n0). We demonstrate the efficacy of our proposed framework with extensive\nexperimental results on CANARD and QuAC -- the two popularly utilized datasets\nin ConvQA. We demonstrate that selecting relevant turns works better than\nrewriting the original question. We also investigate how adding the irrelevant\nhistory turns negatively impacts the model's performance and discuss the\nresearch challenges that demand more attention from the IR community.", "published": "2023-08-04 12:59:39", "link": "http://arxiv.org/abs/2308.02294v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "From Military to Healthcare: Adopting and Expanding Ethical Principles\n  for Generative Artificial Intelligence", "abstract": "In 2020, the U.S. Department of Defense officially disclosed a set of ethical\nprinciples to guide the use of Artificial Intelligence (AI) technologies on\nfuture battlefields. Despite stark differences, there are core similarities\nbetween the military and medical service. Warriors on battlefields often face\nlife-altering circumstances that require quick decision-making. Medical\nproviders experience similar challenges in a rapidly changing healthcare\nenvironment, such as in the emergency department or during surgery treating a\nlife-threatening condition. Generative AI, an emerging technology designed to\nefficiently generate valuable information, holds great promise. As computing\npower becomes more accessible and the abundance of health data, such as\nelectronic health records, electrocardiograms, and medical images, increases,\nit is inevitable that healthcare will be revolutionized by this technology.\nRecently, generative AI has captivated the research community, leading to\ndebates about its application in healthcare, mainly due to concerns about\ntransparency and related issues. Meanwhile, concerns about the potential\nexacerbation of health disparities due to modeling biases have raised notable\nethical concerns regarding the use of this technology in healthcare. However,\nthe ethical principles for generative AI in healthcare have been understudied,\nand decision-makers often fail to consider the significance of generative AI.\nIn this paper, we propose GREAT PLEA ethical principles, encompassing\ngovernance, reliability, equity, accountability, traceability, privacy,\nlawfulness, empathy, and autonomy, for generative AI in healthcare. We aim to\nproactively address the ethical dilemmas and challenges posed by the\nintegration of generative AI in healthcare.", "published": "2023-08-04 16:22:06", "link": "http://arxiv.org/abs/2308.02448v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities", "abstract": "We propose MM-Vet, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown various\nintriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual\njokes. Rapid model advancements pose challenges to evaluation benchmark\ndevelopment. Problems include: (1) How to systematically structure and evaluate\nthe complicated multimodal tasks; (2) How to design evaluation metrics that\nwork well across question and answer types; and (3) How to give model insights\nbeyond a simple performance ranking. To this end, we present MM-Vet, designed\nbased on the insight that the intriguing ability to solve complicated tasks is\noften achieved by a generalist model being able to integrate different core\nvision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and\nexamines the 16 integrations of interest derived from the capability\ncombination. For evaluation metrics, we propose an LLM-based evaluator for\nopen-ended outputs. The evaluator enables the evaluation across different\nquestion types and answer styles, resulting in a unified scoring metric. We\nevaluate representative LMMs on MM-Vet, providing insights into the\ncapabilities of different LMM system paradigms and models.", "published": "2023-08-04 17:59:47", "link": "http://arxiv.org/abs/2308.02490v4", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "ChatGPT for GTFS: Benchmarking LLMs on GTFS Understanding and Retrieval", "abstract": "The General Transit Feed Specification (GTFS) standard for publishing transit\ndata is ubiquitous. GTFS being tabular data, with information spread across\ndifferent files, necessitates specialized tools or packages to retrieve\ninformation. Concurrently, the use of Large Language Models(LLMs) for text and\ninformation retrieval is growing. The idea of this research is to see if the\ncurrent widely adopted LLMs (ChatGPT) are able to understand GTFS and retrieve\ninformation from GTFS using natural language instructions without explicitly\nproviding information. In this research, we benchmark OpenAI's GPT-3.5-Turbo\nand GPT-4 LLMs which are the backbone of ChatGPT. ChatGPT demonstrates a\nreasonable understanding of GTFS by answering 59.7% (GPT-3.5-Turbo) and 73.3%\n(GPT-4) of our multiple-choice questions (MCQ) correctly. Furthermore, we\nevaluated the LLMs on information extraction tasks using a filtered GTFS feed\ncontaining four routes. We found that program synthesis techniques outperformed\nzero-shot approaches, achieving up to 93% (90%) accuracy for simple queries and\n61% (41%) for complex ones using GPT-4 (GPT-3.5-Turbo).", "published": "2023-08-04 14:50:37", "link": "http://arxiv.org/abs/2308.02618v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Capturing Spectral and Long-term Contextual Information for Speech\n  Emotion Recognition Using Deep Learning Techniques", "abstract": "Traditional approaches in speech emotion recognition, such as LSTM, CNN, RNN,\nSVM, and MLP, have limitations such as difficulty capturing long-term\ndependencies in sequential data, capturing the temporal dynamics, and\nstruggling to capture complex patterns and relationships in multimodal data.\nThis research addresses these shortcomings by proposing an ensemble model that\ncombines Graph Convolutional Networks (GCN) for processing textual data and the\nHuBERT transformer for analyzing audio signals. We found that GCNs excel at\ncapturing Long-term contextual dependencies and relationships within textual\ndata by leveraging graph-based representations of text and thus detecting the\ncontextual meaning and semantic relationships between words. On the other hand,\nHuBERT utilizes self-attention mechanisms to capture long-range dependencies,\nenabling the modeling of temporal dynamics present in speech and capturing\nsubtle nuances and variations that contribute to emotion recognition. By\ncombining GCN and HuBERT, our ensemble model can leverage the strengths of both\napproaches. This allows for the simultaneous analysis of multimodal data, and\nthe fusion of these modalities enables the extraction of complementary\ninformation, enhancing the discriminative power of the emotion recognition\nsystem. The results indicate that the combined model can overcome the\nlimitations of traditional methods, leading to enhanced accuracy in recognizing\nemotions from speech.", "published": "2023-08-04 06:20:42", "link": "http://arxiv.org/abs/2308.04517v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song", "abstract": "In this paper, we introduce a computational analysis of the field recording\ndataset of approximately 700 hours of Korean folk songs, which were recorded\naround 1980-90s. Because most of the songs were sung by non-expert musicians\nwithout accompaniment, the dataset provides several challenges. To address this\nchallenge, we utilized self-supervised learning with convolutional neural\nnetwork based on pitch contour, then analyzed how the musical concept of tori,\na classification system defined by a specific scale, ornamental notes, and an\nidiomatic melodic contour, is captured by the model. The experimental result\nshows that our approach can better capture the characteristics of tori compared\nto traditional pitch histograms. Using our approaches, we have examined how\nmusical discussions proposed in existing academia manifest in the actual field\nrecordings of Korean folk songs.", "published": "2023-08-04 11:13:15", "link": "http://arxiv.org/abs/2308.02249v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Improving Harmonic Sensitivity and Prediction Stability for\n  Singing Melody Extraction", "abstract": "In deep learning research, many melody extraction models rely on redesigning\nneural network architectures to improve performance. In this paper, we propose\nan input feature modification and a training objective modification based on\ntwo assumptions. First, harmonics in the spectrograms of audio data decay\nrapidly along the frequency axis. To enhance the model's sensitivity on the\ntrailing harmonics, we modify the Combined Frequency and Periodicity (CFP)\nrepresentation using discrete z-transform. Second, the vocal and non-vocal\nsegments with extremely short duration are uncommon. To ensure a more stable\nmelody contour, we design a differentiable loss function that prevents the\nmodel from predicting such segments. We apply these modifications to several\nmodels, including MSNet, FTANet, and a newly introduced model, PianoNet,\nmodified from a piano transcription network. Our experimental results\ndemonstrate that the proposed modifications are empirically effective for\nsinging melody extraction.", "published": "2023-08-04 21:59:40", "link": "http://arxiv.org/abs/2308.02723v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
