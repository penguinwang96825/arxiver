{"title": "Risk measures on incomplete markets: a new non-solid paradigm", "abstract": "We study risk measures $\\varphi:E\\longrightarrow\\mathbb{R}\\cup\\{\\infty\\}$,\nwhere $E$ is a vector space of random variables which a priori has no lattice\nstructure$\\unicode{x2014}$a blind spot of the existing risk measures\nliterature. In particular, we address when $\\varphi$ admits a tractable dual\nrepresentation (one which does not contain non-$\\sigma$-additive signed\nmeasures), and whether one can extend $\\varphi$ to a solid superspace of $E$.\nThe existence of a tractable dual representation is shown to be equivalent,\nmodulo certain technicalities, to a Fatou-like property, while extension\ntheorems are established under the existence of a sufficiently regular lift, a\npotentially non-linear mechanism of assigning random variable extensions to\ncertain linear functionals on $E$. Our motivation is broadening the theory of\nrisk measures to spaces without a lattice structure, which are ubiquitous in\nfinancial economics, especially when markets are incomplete.", "published": "2024-09-08 19:12:52", "link": "http://arxiv.org/abs/2409.05194v2", "categories": ["q-fin.RM", "math.FA", "math.PR", "q-fin.MF", "46E30, 46N30, 91G70"], "primary_category": "q-fin.RM"}
{"title": "Bellwether Trades: Characteristics of Trades influential in Predicting Future Price Movements in Markets", "abstract": "In this study, we leverage powerful non-linear machine learning methods to\nidentify the characteristics of trades that contain valuable information.\nFirst, we demonstrate the effectiveness of our optimized neural network\npredictor in accurately predicting future market movements. Then, we utilize\nthe information from this successful neural network predictor to pinpoint the\nindividual trades within each data point (trading window) that had the most\nimpact on the optimized neural network's prediction of future price movements.\nThis approach helps us uncover important insights about the heterogeneity in\ninformation content provided by trades of different sizes, venues, trading\ncontexts, and over time.", "published": "2024-09-08 18:59:52", "link": "http://arxiv.org/abs/2409.05192v1", "categories": ["q-fin.TR", "cs.LG", "econ.EM", "stat.ML"], "primary_category": "q-fin.TR"}
{"title": "Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text\n  against Neural Machine Translation", "abstract": "While neural machine translation (NMT) models achieve success in our daily\nlives, they show vulnerability to adversarial attacks. Despite being harmful,\nthese attacks also offer benefits for interpreting and enhancing NMT models,\nthus drawing increased research attention. However, existing studies on\nadversarial attacks are insufficient in both attacking ability and human\nimperceptibility due to their sole focus on the scope of language. This paper\nproposes a novel vision-fused attack (VFA) framework to acquire powerful\nadversarial text, i.e., more aggressive and stealthy. Regarding the attacking\nability, we design the vision-merged solution space enhancement strategy to\nenlarge the limited semantic solution space, which enables us to search for\nadversarial candidates with higher attacking ability. For human\nimperceptibility, we propose the perception-retained adversarial text selection\nstrategy to align the human text-reading mechanism. Thus, the finally selected\nadversarial text could be more deceptive. Extensive experiments on various\nmodels, including large language models (LLMs) like LLaMA and GPT-3.5, strongly\nsupport that VFA outperforms the comparisons by large margins (up to 81%/14%\nimprovements on ASR/SSIM).", "published": "2024-09-08 08:22:17", "link": "http://arxiv.org/abs/2409.05021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in\n  Large Documents", "abstract": "Watermarking algorithms for large language models (LLMs) have attained high\naccuracy in detecting LLM-generated text. However, existing methods primarily\nfocus on distinguishing fully watermarked text from non-watermarked text,\noverlooking real-world scenarios where LLMs generate only small sections within\nlarge documents. In this scenario, balancing time complexity and detection\nperformance poses significant challenges. This paper presents WaterSeeker, a\nnovel approach to efficiently detect and locate watermarked segments amid\nextensive natural text. It first applies an efficient anomaly extraction method\nto preliminarily locate suspicious watermarked regions. Following this, it\nconducts a local traversal and performs full-text detection for more precise\nverification. Theoretical analysis and experimental results demonstrate that\nWaterSeeker achieves a superior balance between detection accuracy and\ncomputational efficiency. Moreover, its localization capability lays the\nfoundation for building interpretable AI detection systems. Our code is\navailable at https://github.com/THU-BPM/WaterSeeker.", "published": "2024-09-08 14:45:47", "link": "http://arxiv.org/abs/2409.05112v6", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Hate Content Detection via Novel Pre-Processing Sequencing and Ensemble\n  Methods", "abstract": "Social media, particularly Twitter, has seen a significant increase in\nincidents like trolling and hate speech. Thus, identifying hate speech is the\nneed of the hour. This paper introduces a computational framework to curb the\nhate content on the web. Specifically, this study presents an exhaustive study\nof pre-processing approaches by studying the impact of changing the sequence of\ntext pre-processing operations for the identification of hate content. The\nbest-performing pre-processing sequence, when implemented with popular\nclassification approaches like Support Vector Machine, Random Forest, Decision\nTree, Logistic Regression and K-Neighbor provides a considerable boost in\nperformance. Additionally, the best pre-processing sequence is used in\nconjunction with different ensemble methods, such as bagging, boosting and\nstacking to improve the performance further. Three publicly available benchmark\ndatasets (WZ-LS, DT, and FOUNTA), were used to evaluate the proposed approach\nfor hate speech identification. The proposed approach achieves a maximum\naccuracy of 95.14% highlighting the effectiveness of the unique pre-processing\napproach along with an ensemble classifier.", "published": "2024-09-08 15:32:17", "link": "http://arxiv.org/abs/2409.05134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MHS-STMA: Multimodal Hate Speech Detection via Scalable\n  Transformer-Based Multilevel Attention Framework", "abstract": "Social media has a significant impact on people's lives. Hate speech on\nsocial media has emerged as one of society's most serious issues in recent\nyears. Text and pictures are two forms of multimodal data that are distributed\nwithin articles. Unimodal analysis has been the primary emphasis of earlier\napproaches. Additionally, when doing multimodal analysis, researchers neglect\nto preserve the distinctive qualities associated with each modality. To address\nthese shortcomings, the present article suggests a scalable architecture for\nmultimodal hate content detection called transformer-based multilevel attention\n(STMA). This architecture consists of three main parts: a combined\nattention-based deep learning mechanism, a vision attention-mechanism encoder,\nand a caption attention-mechanism encoder. To identify hate content, each\ncomponent uses various attention processes and handles multimodal data in a\nunique way. Several studies employing multiple assessment criteria on three\nhate speech datasets such as Hateful memes, MultiOff, and MMHS150K, validate\nthe suggested architecture's efficacy. The outcomes demonstrate that on all\nthree datasets, the suggested strategy performs better than the baseline\napproaches.", "published": "2024-09-08 15:42:18", "link": "http://arxiv.org/abs/2409.05136v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interactive Machine Teaching by Labeling Rules and Instances", "abstract": "Weakly supervised learning aims to reduce the cost of labeling data by using\nexpert-designed labeling rules. However, existing methods require experts to\ndesign effective rules in a single shot, which is difficult in the absence of\nproper guidance and tooling. Therefore, it is still an open question whether\nexperts should spend their limited time writing rules or instead providing\ninstance labels via active learning. In this paper, we investigate how to\nexploit an expert's limited time to create effective supervision. First, to\ndevelop practical guidelines for rule creation, we conduct an exploratory\nanalysis of diverse collections of existing expert-designed rules and find that\nrule precision is more important than coverage across datasets. Second, we\ncompare rule creation to individual instance labeling via active learning and\ndemonstrate the importance of both across 6 datasets. Third, we propose an\ninteractive learning framework, INTERVAL, that achieves efficiency by\nautomatically extracting candidate rules based on rich patterns (e.g., by\nprompting a language model), and effectiveness by soliciting expert feedback on\nboth candidate rules and individual instances. Across 6 datasets, INTERVAL\noutperforms state-of-the-art weakly supervised approaches by 7% in F1.\nFurthermore, it requires as few as 10 queries for expert feedback to reach F1\nvalues that existing active learning methods cannot match even with 100\nqueries.", "published": "2024-09-08 19:24:14", "link": "http://arxiv.org/abs/2409.05199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Intrinsic Language-specific Subspaces in Fine-tuning\n  Multilingual Neural Machine Translation", "abstract": "Multilingual neural machine translation models support fine-tuning hundreds\nof languages simultaneously. However, fine-tuning on full parameters solely is\ninefficient potentially leading to negative interactions among languages. In\nthis work, we demonstrate that the fine-tuning for a language occurs in its\nintrinsic language-specific subspace with a tiny fraction of entire parameters.\nThus, we propose language-specific LoRA to isolate intrinsic language-specific\nsubspaces. Furthermore, we propose architecture learning techniques and\nintroduce a gradual pruning schedule during fine-tuning to exhaustively explore\nthe optimal setting and the minimal intrinsic subspaces for each language,\nresulting in a lightweight yet effective fine-tuning procedure. The\nexperimental results on a 12-language subset and a 30-language subset of\nFLORES-101 show that our methods not only outperform full-parameter fine-tuning\nup to 2.25 spBLEU scores but also reduce trainable parameters to $0.4\\%$ for\nhigh and medium-resource languages and $1.6\\%$ for low-resource ones.", "published": "2024-09-08 21:40:44", "link": "http://arxiv.org/abs/2409.05224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Socially Responsible Data for Large Multilingual Language Models", "abstract": "Large Language Models (LLMs) have rapidly increased in size and apparent\ncapabilities in the last three years, but their training data is largely\nEnglish text. There is growing interest in multilingual LLMs, and various\nefforts are striving for models to accommodate languages of communities outside\nof the Global North, which include many languages that have been historically\nunderrepresented in digital realms. These languages have been coined as \"low\nresource languages\" or \"long-tail languages\", and LLMs performance on these\nlanguages is generally poor. While expanding the use of LLMs to more languages\nmay bring many potential benefits, such as assisting cross-community\ncommunication and language preservation, great care must be taken to ensure\nthat data collection on these languages is not extractive and that it does not\nreproduce exploitative practices of the past. Collecting data from languages\nspoken by previously colonized people, indigenous people, and non-Western\nlanguages raises many complex sociopolitical and ethical questions, e.g.,\naround consent, cultural safety, and data sovereignty. Furthermore, linguistic\ncomplexity and cultural nuances are often lost in LLMs. This position paper\nbuilds on recent scholarship, and our own work, and outlines several relevant\nsocial, cultural, and ethical considerations and potential ways to mitigate\nthem through qualitative research, community partnerships, and participatory\ndesign approaches. We provide twelve recommendations for consideration when\ncollecting language data on underrepresented language communities outside of\nthe Global North.", "published": "2024-09-08 23:51:04", "link": "http://arxiv.org/abs/2409.05247v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models with Tests of Spanish as a Foreign\n  Language: Pass or Fail?", "abstract": "Large Language Models (LLMs) have been profusely evaluated on their ability\nto answer questions on many topics and their performance on different natural\nlanguage understanding tasks. Those tests are usually conducted in English, but\nmost LLM users are not native English speakers. Therefore, it is of interest to\nanalyze how LLMs understand other languages at different levels: from\nparagraphs to morphems. In this paper, we evaluate the performance of\nstate-of-the-art LLMs in TELEIA, a recently released benchmark with similar\nquestions to those of Spanish exams for foreign students, covering topics such\nas reading comprehension, word formation, meaning and compositional semantics,\nand grammar. The results show that LLMs perform well at understanding Spanish\nbut are still far from achieving the level of a native speaker in terms of\ngrammatical competence.", "published": "2024-09-08 11:30:03", "link": "http://arxiv.org/abs/2409.15334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Google Translate for Mandarin Chinese translation using\n  sentiment and semantic analysis", "abstract": "Machine translation using large language models (LLMs) is having a\nsignificant global impact, making communication easier. Mandarin Chinese is the\nofficial language used for communication by the government and media in China.\nIn this study, we provide an automated assessment of translation quality of\nGoogle Translate with human experts using sentiment and semantic analysis. In\norder to demonstrate our framework, we select the classic early\ntwentieth-century novel 'The True Story of Ah Q' with selected Mandarin Chinese\nto English translations. We use Google Translate to translate the given text\ninto English and then conduct a chapter-wise sentiment analysis and semantic\nanalysis to compare the extracted sentiments across the different translations.\nOur results indicate that the precision of Google Translate differs both in\nterms of semantic and sentiment analysis when compared to human expert\ntranslations. We find that Google Translate is unable to translate some of the\nspecific words or phrases in Chinese, such as Chinese traditional allusions.\nThe mistranslations may be due to lack of contextual significance and\nhistorical knowledge of China.", "published": "2024-09-08 04:03:55", "link": "http://arxiv.org/abs/2409.04964v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference", "abstract": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.", "published": "2024-09-08 06:06:44", "link": "http://arxiv.org/abs/2409.04992v1", "categories": ["cs.AR", "cs.CL"], "primary_category": "cs.AR"}
{"title": "Towards Patronizing and Condescending Language in Chinese Videos: A\n  Multimodal Dataset and Detector", "abstract": "Patronizing and Condescending Language (PCL) is a form of discriminatory\ntoxic speech targeting vulnerable groups, threatening both online and offline\nsafety. While toxic speech research has mainly focused on overt toxicity, such\nas hate speech, microaggressions in the form of PCL remain underexplored.\nAdditionally, dominant groups' discriminatory facial expressions and attitudes\ntoward vulnerable communities can be more impactful than verbal cues, yet these\nframe features are often overlooked. In this paper, we introduce the PCLMM\ndataset, the first Chinese multimodal dataset for PCL, consisting of 715\nannotated videos from Bilibili, with high-quality PCL facial frame spans. We\nalso propose the MultiPCL detector, featuring a facial expression detection\nmodule for PCL recognition, demonstrating the effectiveness of modality\ncomplementarity in this challenging task. Our work makes an important\ncontribution to advancing microaggression detection within the domain of toxic\nspeech.", "published": "2024-09-08 07:26:13", "link": "http://arxiv.org/abs/2409.05005v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LLM-based Abstraction and Concretization for GUI Test Migration", "abstract": "GUI test migration aims to produce test cases with events and assertions to\ntest specific functionalities of a target app. Existing migration approaches\ntypically focus on the widget-mapping paradigm that maps widgets from source\napps to target apps. However, since different apps may implement the same\nfunctionality in different ways, direct mapping may result in incomplete or\nbuggy test cases, thus significantly impacting the effectiveness of testing\ntarget functionality and the practical applicability.\n  In this paper, we propose a new migration paradigm (i.e.,\nabstraction-concretization paradigm) that first abstracts the test logic for\nthe target functionality and then utilizes this logic to generate the concrete\nGUI test case. Furthermore, we introduce MACdroid, the first approach that\nmigrates GUI test cases based on this paradigm. Specifically, we propose an\nabstraction technique that utilizes source test cases from source apps\ntargeting the same functionality to extract a general test logic for that\nfunctionality. Then, we propose a concretization technique that utilizes the\ngeneral test logic to guide an LLM in generating the corresponding GUI test\ncase (including events and assertions) for the target app. We evaluate MACdroid\non two widely-used datasets (including 31 apps, 34 functionalities, and 123\ntest cases). On the FrUITeR dataset, the test cases generated by MACdroid\nsuccessfully test 64% of the target functionalities, improving the baselines by\n191%. On the Lin dataset, MACdroid successfully tests 75% of the target\nfunctionalities, outperforming the baselines by 42%. These results underscore\nthe effectiveness of MACdroid in GUI test migration.", "published": "2024-09-08 08:46:05", "link": "http://arxiv.org/abs/2409.05028v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "EdaCSC: Two Easy Data Augmentation Methods for Chinese Spelling\n  Correction", "abstract": "Chinese Spelling Correction (CSC) aims to detect and correct spelling errors\nin Chinese sentences caused by phonetic or visual similarities. While current\nCSC models integrate pinyin or glyph features and have shown significant\nprogress,they still face challenges when dealing with sentences containing\nmultiple typos and are susceptible to overcorrection in real-world scenarios.\nIn contrast to existing model-centric approaches, we propose two data\naugmentation methods to address these limitations. Firstly, we augment the\ndataset by either splitting long sentences into shorter ones or reducing typos\nin sentences with multiple typos. Subsequently, we employ different training\nprocesses to select the optimal model. Experimental evaluations on the SIGHAN\nbenchmarks demonstrate the superiority of our approach over most existing\nmodels, achieving state-of-the-art performance on the SIGHAN15 test set.", "published": "2024-09-08 14:29:10", "link": "http://arxiv.org/abs/2409.05105v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "READoc: A Unified Benchmark for Realistic Document Structured Extraction", "abstract": "Document Structured Extraction (DSE) aims to extract structured content from\nraw documents. Despite the emergence of numerous DSE systems, their unified\nevaluation remains inadequate, significantly hindering the field's advancement.\nThis problem is largely attributed to existing benchmark paradigms, which\nexhibit fragmented and localized characteristics. To address these limitations\nand offer a thorough evaluation of DSE systems, we introduce a novel benchmark\nnamed READoc, which defines DSE as a realistic task of converting unstructured\nPDFs into semantically rich Markdown. The READoc dataset is derived from 2,233\ndiverse and real-world documents from arXiv and GitHub. In addition, we develop\na DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring\nmodules, to conduct a unified evaluation of state-of-the-art DSE approaches. By\nevaluating a range of pipeline tools, expert visual models, and general VLMs,\nwe identify the gap between current work and the unified, realistic DSE\nobjective for the first time. We aspire that READoc will catalyze future\nresearch in DSE, fostering more comprehensive and practical solutions.", "published": "2024-09-08 15:42:48", "link": "http://arxiv.org/abs/2409.05137v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?", "abstract": "State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.", "published": "2024-09-08 19:22:58", "link": "http://arxiv.org/abs/2409.05197v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Better Spanish Emotion Recognition In-the-wild: Bringing Attention to\n  Deep Spectrum Voice Analysis", "abstract": "Within the context of creating new Socially Assistive Robots, emotion\nrecognition has become a key development factor, as it allows the robot to\nadapt to the user's emotional state in the wild. In this work, we focused on\nthe analysis of two voice recording Spanish datasets: ELRA-S0329 and\nEmoMatchSpanishDB. Specifically, we centered our work in the paralanguage,\ne.~g. the vocal characteristics that go along with the message and clarifies\nthe meaning. We proposed the use of the DeepSpectrum method, which consists of\nextracting a visual representation of the audio tracks and feeding them to a\npretrained CNN model. For the classification task, DeepSpectrum is often paired\nwith a Support Vector Classifier --DS-SVC--, or a Fully-Connected deep-learning\nclassifier --DS-FC--. We compared the results of the DS-SVC and DS-FC\narchitectures with the state-of-the-art (SOTA) for ELRA-S0329 and\nEmoMatchSpanishDB. Moreover, we proposed our own classifier based upon\nAttention Mechanisms, namely DS-AM. We trained all models against both\ndatasets, and we found that our DS-AM model outperforms the SOTA models for the\ndatasets and the SOTA DeepSpectrum architectures. Finally, we trained our DS-AM\nmodel in one dataset and tested it in the other, to simulate real-world\nconditions on how biased is the model to the dataset.", "published": "2024-09-08 16:25:38", "link": "http://arxiv.org/abs/2409.05148v1", "categories": ["cs.SD", "cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs", "abstract": "Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation.", "published": "2024-09-08 16:35:19", "link": "http://arxiv.org/abs/2409.05152v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Disentangling the Prosody and Semantic Information with Pre-trained\n  Model for In-Context Learning based Zero-Shot Voice Conversion", "abstract": "Voice conversion (VC) aims to modify the speaker's timbre while retaining\nspeech content. Previous approaches have tokenized the outputs from\nself-supervised into semantic tokens, facilitating disentanglement of speech\ncontent information. Recently, in-context learning (ICL) has emerged in\ntext-to-speech (TTS) systems for effectively modeling specific characteristics\nsuch as timbre through context conditioning. This paper proposes an ICL\ncapability enhanced VC system (ICL-VC) employing a mask and reconstruction\ntraining strategy based on flow-matching generative models. Augmented with\nsemantic tokens, our experiments on the LibriTTS dataset demonstrate that\nICL-VC improves speaker similarity. Additionally, we find that k-means is a\nversatile tokenization method applicable to various pre-trained models.\nHowever, the ICL-VC system faces challenges in preserving the prosody of the\nsource speech. To mitigate this issue, we propose incorporating prosody\nembeddings extracted from a pre-trained emotion recognition model into our\nsystem. Integration of prosody embeddings notably enhances the system's\ncapability to preserve source speech prosody, as validated on the Emotional\nSpeech Database.", "published": "2024-09-08 07:24:03", "link": "http://arxiv.org/abs/2409.05004v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TF-Mamba: A Time-Frequency Network for Sound Source Localization", "abstract": "Sound source localization (SSL) determines the position of sound sources\nusing multi-channel audio data. It is commonly used to improve speech\nenhancement and separation. Extracting spatial features is crucial for SSL,\nespecially in challenging acoustic environments. Previous studies performed\nwell based on long short-term memory models. Recently, a novel scalable SSM\nreferred to as Mamba demonstrated notable performance across various\nsequence-based modalities, including audio and speech. This study introduces\nthe Mamba for SSL tasks. We consider the Mamba-based model to analyze spatial\nfeatures from speech signals by fusing both time and frequency features, and we\ndevelop an SSL system called TF-Mamba. This system integrates time and\nfrequency fusion, with Bidirectional Mamba managing both time-wise and\nfrequency-wise processing. We conduct the experiments on the simulated dataset\nand the LOCATA dataset. Experiments show that TF-Mamba significantly\noutperforms other advanced methods on simulated and real-world data.", "published": "2024-09-08 09:20:04", "link": "http://arxiv.org/abs/2409.05034v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Diffusion-based Speech Enhancement with Schr\u00f6dinger Bridge and\n  Symmetric Noise Schedule", "abstract": "Recently, diffusion-based generative models have demonstrated remarkable\nperformance in speech enhancement tasks. However, these methods still encounter\nchallenges, including the lack of structural information and poor performance\nin low Signal-to-Noise Ratio (SNR) scenarios. To overcome these challenges, we\npropose the Schr\\\"oodinger Bridge-based Speech Enhancement (SBSE) method, which\nlearns the diffusion processes directly between the noisy input and the clean\ndistribution, unlike conventional diffusion-based speech enhancement systems\nthat learn data to Gaussian distributions. To enhance performance in extremely\nnoisy conditions, we introduce a two-stage system incorporating ratio mask\ninformation into the diffusion-based generative model. Our experimental results\nshow that our proposed SBSE method outperforms all the baseline models and\nachieves state-of-the-art performance, especially in low SNR conditions.\nImportantly, only a few inference steps are required to achieve the best\nresult.", "published": "2024-09-08 14:56:25", "link": "http://arxiv.org/abs/2409.05116v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SS-BRPE: Self-Supervised Blind Room Parameter Estimation Using Attention\n  Mechanisms", "abstract": "In recent years, dynamic parameterization of acoustic environments has\ngarnered attention in audio processing. This focus includes room volume and\nreverberation time (RT60), which define local acoustics independent of sound\nsource and receiver orientation. Previous studies show that purely\nattention-based models can achieve advanced results in room parameter\nestimation. However, their success relies on supervised pretrainings that\nrequire a large amount of labeled true values for room parameters and complex\ntraining pipelines. In light of this, we propose a novel Self-Supervised Blind\nRoom Parameter Estimation (SS-BRPE) system. This system combines a purely\nattention-based model with self-supervised learning to estimate room acoustic\nparameters, from single-channel noisy speech signals. By utilizing unlabeled\naudio data for pretraining, the proposed system significantly reduces\ndependencies on costly labeled datasets. Our model also incorporates dynamic\nfeature augmentation during fine-tuning to enhance adaptability and\ngeneralizability. Experimental results demonstrate that the SS-BRPE system not\nonly achieves more superior performance in estimating room parameters than\nstate-of-the-art (SOTA) methods but also effectively maintains high accuracy\nunder conditions with limited labeled data. Code available at\nhttps://github.com/bjut-chunxiwang/SS-BRPE.", "published": "2024-09-08 20:04:52", "link": "http://arxiv.org/abs/2409.05212v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient learning-based sound propagation for virtual and real-world\n  audio processing applications", "abstract": "Sound propagation is the process by which sound energy travels through a\nmedium, such as air, to the surrounding environment as sound waves. The room\nimpulse response (RIR) describes this process and is influenced by the\npositions of the source and listener, the room's geometry, and its materials.\nPhysics-based acoustic simulators have been used for decades to compute\naccurate RIRs for specific acoustic environments. However, we have encountered\nlimitations with existing acoustic simulators. To address these limitations, we\npropose three novel solutions. First, we introduce a learning-based RIR\ngenerator that is two orders of magnitude faster than an interactive\nray-tracing simulator. Our approach can be trained to input both statistical\nand traditional parameters directly, and it can generate both monaural and\nbinaural RIRs for both reconstructed and synthetic 3D scenes. Our generated\nRIRs outperform interactive ray-tracing simulators in speech-processing\napplications, including ASR, Speech Enhancement, and Speech Separation.\nSecondly, we propose estimating RIRs from reverberant speech signals and visual\ncues without a 3D representation of the environment. By estimating RIRs from\nreverberant speech, we can augment training data to match test data, improving\nthe word error rate of the ASR system. Our estimated RIRs achieve a 6.9%\nimprovement over previous learning-based RIR estimators in far-field ASR tasks.\nWe demonstrate that our audio-visual RIR estimator aids tasks like visual\nacoustic matching, novel-view acoustic synthesis, and voice dubbing, validated\nthrough perceptual evaluation. Finally, we introduce IR-GAN to augment accurate\nRIRs using real RIRs. IR-GAN parametrically controls acoustic parameters\nlearned from real RIRs to generate new RIRs that imitate different acoustic\nenvironments, outperforming Ray-tracing simulators on the far-field ASR\nbenchmark by 8.95%.", "published": "2024-09-08 14:20:02", "link": "http://arxiv.org/abs/2409.15335v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Guided Fusion Techniques for Multimodal Emotion Analysis", "abstract": "In this paper, we propose a solution for the semi-supervised learning track\n(MER-SEMI) in MER2024. First, in order to enhance the performance of the\nfeature extractor on sentiment classification tasks,we fine-tuned video and\ntext feature extractors, specifically CLIP-vit-large and Baichuan-13B, using\nlabeled data. This approach effectively preserves the original emotional\ninformation conveyed in the videos. Second, we propose an Audio-Guided\nTransformer (AGT) fusion mechanism, which leverages the robustness of\nHubert-large, showing superior effectiveness in fusing both inter-channel and\nintra-channel information. Third, To enhance the accuracy of the model, we\niteratively apply self-supervised learning by using high-confidence unlabeled\ndata as pseudo-labels. Finally, through black-box probing, we discovered an\nimbalanced data distribution between the training and test sets. Therefore, We\nadopt a prior-knowledge-based voting mechanism. The results demonstrate the\neffectiveness of our strategy, ultimately earning us third place in the\nMER-SEMI track.", "published": "2024-09-08 07:28:27", "link": "http://arxiv.org/abs/2409.05007v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Multimodal Emotion Recognition by Leveraging Acoustic\n  Adaptation and Visual Alignment", "abstract": "Multimodal Emotion Recognition (MER) aims to automatically identify and\nunderstand human emotional states by integrating information from various\nmodalities. However, the scarcity of annotated multimodal data significantly\nhinders the advancement of this research field. This paper presents our\nsolution for the MER-SEMI sub-challenge of MER 2024. First, to better adapt\nacoustic modality features for the MER task, we experimentally evaluate the\ncontributions of different layers of the pre-trained speech model HuBERT in\nemotion recognition. Based on these observations, we perform\nParameter-Efficient Fine-Tuning (PEFT) on the layers identified as most\neffective for emotion recognition tasks, thereby achieving optimal adaptation\nfor emotion recognition with a minimal number of learnable parameters. Second,\nleveraging the strengths of the acoustic modality, we propose a feature\nalignment pre-training method. This approach uses large-scale unlabeled data to\ntrain a visual encoder, thereby promoting the semantic alignment of visual\nfeatures within the acoustic feature space. Finally, using the adapted acoustic\nfeatures, aligned visual features, and lexical features, we employ an attention\nmechanism for feature fusion. On the MER2024-SEMI test set, the proposed method\nachieves a weighted F1 score of 88.90%, ranking fourth among all participating\nteams, validating the effectiveness of our approach.", "published": "2024-09-08 07:56:51", "link": "http://arxiv.org/abs/2409.05015v2", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Exploring WavLM Back-ends for Speech Spoofing and Deepfake Detection", "abstract": "This paper describes our submitted systems to the ASVspoof 5 Challenge Track\n1: Speech Deepfake Detection - Open Condition, which consists of a stand-alone\nspeech deepfake (bonafide vs spoof) detection task. Recently, large-scale\nself-supervised models become a standard in Automatic Speech Recognition (ASR)\nand other speech processing tasks. Thus, we leverage a pre-trained WavLM as a\nfront-end model and pool its representations with different back-end\ntechniques. The complete framework is fine-tuned using only the trained dataset\nof the challenge, similar to the close condition. Besides, we adopt\ndata-augmentation by adding noise and reverberation using MUSAN noise and RIR\ndatasets. We also experiment with codec augmentations to increase the\nperformance of our method. Ultimately, we use the Bosaris toolkit for score\ncalibration and system fusion to get better Cllr scores. Our fused system\nachieves 0.0937 minDCF, 3.42% EER, 0.1927 Cllr, and 0.1375 actDCF.", "published": "2024-09-08 08:54:36", "link": "http://arxiv.org/abs/2409.05032v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Generic Representations for Domain-Generalized Anomalous Sound\n  Detection", "abstract": "Developing a reliable anomalous sound detection (ASD) system requires\nrobustness to noise, adaptation to domain shifts, and effective performance\nwith limited training data. Current leading methods rely on extensive labeled\ndata for each target machine type to train feature extractors using\nOutlier-Exposure (OE) techniques, yet their performance on the target domain\nremains sub-optimal. In this paper, we present \\textit{GenRep}, which utilizes\ngeneric feature representations from a robust, large-scale pre-trained feature\nextractor combined with kNN for domain-generalized ASD, without the need for\nfine-tuning. \\textit{GenRep} incorporates MemMixup, a simple approach for\naugmenting the target memory bank using nearest source samples, paired with a\ndomain normalization technique to address the imbalance between source and\ntarget domains. \\textit{GenRep} outperforms the best OE-based approach without\na need for labeled data with an Official Score of 73.79\\% on the DCASE2023T2\nEval set and demonstrates robustness under limited data scenarios. The code is\navailable open-source.", "published": "2024-09-08 09:20:30", "link": "http://arxiv.org/abs/2409.05035v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The first Cadenza challenges: using machine learning competitions to\n  improve music for listeners with a hearing loss", "abstract": "It is well established that listening to music is an issue for those with\nhearing loss, and hearing aids are not a universal solution. How can machine\nlearning be used to address this? This paper details the first application of\nthe open challenge methodology to use machine learning to improve audio quality\nof music for those with hearing loss. The first challenge was a stand-alone\ncompetition (CAD1) and had 9 entrants. The second was an 2024 ICASSP grand\nchallenge (ICASSP24) and attracted 17 entrants. The challenge tasks concerned\ndemixing and remixing pop/rock music to allow a personalised rebalancing of the\ninstruments in the mix, along with amplification to correct for raised hearing\nthresholds. The software baselines provided for entrants to build upon used two\nstate-of-the-art demix algorithms: Hybrid Demucs and Open-Unmix. Evaluation of\nsystems was done using the objective metric HAAQI, the Hearing-Aid Audio\nQuality Index. No entrants improved on the best baseline in CAD1 because there\nwas insufficient room for improvement. Consequently, for ICASSP24 the scenario\nwas made more difficult by using loudspeaker reproduction and specified gains\nto be applied before remixing. This also made the scenario more useful for\nlistening through hearing aids. 9 entrants scored better than the the best\nICASSP24 baseline. Most entrants used a refined version of Hybrid Demucs and\nNAL-R amplification. The highest scoring system combined the outputs of several\ndemixing algorithms in an ensemble approach. These challenges are now open\nbenchmarks for future research with the software and data being freely\navailable.", "published": "2024-09-08 13:45:45", "link": "http://arxiv.org/abs/2409.05095v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention-Based Efficient Breath Sound Removal in Studio Audio\n  Recordings", "abstract": "In this research, we present an innovative, parameter-efficient model that\nutilizes the attention U-Net architecture for the automatic detection and\neradication of non-speech vocal sounds, specifically breath sounds, in vocal\nrecordings. This task is of paramount importance in the field of sound\nengineering, despite being relatively under-explored. The conventional manual\nprocess for detecting and eliminating these sounds requires significant\nexpertise and is extremely time-intensive. Existing automated detection and\nremoval methods often fall short in terms of efficiency and precision. Our\nproposed model addresses these limitations by offering a streamlined process\nand superior accuracy, achieved through the application of advanced deep\nlearning techniques. A unique dataset, derived from Device and Produced Speech\n(DAPS), was employed for this purpose. The training phase of the model\nemphasizes a log spectrogram and integrates an early stopping mechanism to\nprevent overfitting. Our model not only conserves precious time for sound\nengineers but also enhances the quality and consistency of audio production.\nThis constitutes a significant breakthrough, as evidenced by its comparative\nefficiency, necessitating only 1.9M parameters and a training duration of 3.2\nhours - markedly less than the top-performing models in this domain. The model\nis capable of generating identical outputs as previous models with drastically\nimproved precision, making it an optimal choice.", "published": "2024-09-08 02:11:33", "link": "http://arxiv.org/abs/2409.04949v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
