{"title": "Por Qu\u00e9 N\u00e3o Utiliser Alla Spr\u00e5k? Mixed Training with Gradient\n  Optimization in Few-Shot Cross-Lingual Transfer", "abstract": "The current state-of-the-art for few-shot cross-lingual transfer learning\nfirst trains on abundant labeled data in the source language and then\nfine-tunes with a few examples on the target language, termed target-adapting.\nThough this has been demonstrated to work on a variety of tasks, in this paper\nwe show some deficiencies of this approach and propose a one-step mixed\ntraining method that trains on both source and target data with\n\\textit{stochastic gradient surgery}, a novel gradient-level optimization.\nUnlike the previous studies that focus on one language at a time when\ntarget-adapting, we use one model to handle all target languages simultaneously\nto avoid excessively language-specific models. Moreover, we discuss the\nunreality of utilizing large target development sets for model selection in\nprevious literature. We further show that our method is both development-free\nfor target languages, and is also able to escape from overfitting issues. We\nconduct a large-scale experiment on 4 diverse NLP tasks across up to 48\nlanguages. Our proposed method achieves state-of-the-art performance on all\ntasks and outperforms target-adapting by a large margin, especially for\nlanguages that are linguistically distant from the source language, e.g., 7.36%\nF1 absolute gain on average for the NER task, up to 17.60% on Punjabi.", "published": "2022-04-29 04:05:02", "link": "http://arxiv.org/abs/2204.13869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OA-Mine: Open-World Attribute Mining for E-Commerce Products with Weak\n  Supervision", "abstract": "Automatic extraction of product attributes from their textual descriptions is\nessential for online shopper experience. One inherent challenge of this task is\nthe emerging nature of e-commerce products -- we see new types of products with\ntheir unique set of new attributes constantly. Most prior works on this matter\nmine new values for a set of known attributes but cannot handle new attributes\nthat arose from constantly changing data. In this work, we study the attribute\nmining problem in an open-world setting to extract novel attributes and their\nvalues. Instead of providing comprehensive training data, the user only needs\nto provide a few examples for a few known attribute types as weak supervision.\nWe propose a principled framework that first generates attribute value\ncandidates and then groups them into clusters of attributes. The candidate\ngeneration step probes a pre-trained language model to extract phrases from\nproduct titles. Then, an attribute-aware fine-tuning method optimizes a\nmultitask objective and shapes the language model representation to be\nattribute-discriminative. Finally, we discover new attributes and values\nthrough the self-ensemble of our framework, which handles the open-world\nchallenge. We run extensive experiments on a large distantly annotated\ndevelopment set and a gold standard human-annotated test set that we collected.\nOur model significantly outperforms strong baselines and can generalize to\nunseen attributes and product types.", "published": "2022-04-29 04:16:04", "link": "http://arxiv.org/abs/2204.13874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Czech Dataset for Cross-lingual Subjectivity Classification", "abstract": "In this paper, we introduce a new Czech subjectivity dataset of 10k manually\nannotated subjective and objective sentences from movie reviews and\ndescriptions. Our prime motivation is to provide a reliable dataset that can be\nused with the existing English dataset as a benchmark to test the ability of\npre-trained multilingual models to transfer knowledge between Czech and English\nand vice versa. Two annotators annotated the dataset reaching 0.83 of the\nCohen's \\k{appa} inter-annotator agreement. To the best of our knowledge, this\nis the first subjectivity dataset for the Czech language. We also created an\nadditional dataset that consists of 200k automatically labeled sentences. Both\ndatasets are freely available for research purposes. Furthermore, we fine-tune\nfive pre-trained BERT-like models to set a monolingual baseline for the new\ndataset and we achieve 93.56% of accuracy. We fine-tune models on the existing\nEnglish dataset for which we obtained results that are on par with the current\nstate-of-the-art results. Finally, we perform zero-shot cross-lingual\nsubjectivity classification between Czech and English to verify the usability\nof our dataset as the cross-lingual benchmark. We compare and discuss the\ncross-lingual and monolingual results and the ability of multilingual models to\ntransfer knowledge between languages.", "published": "2022-04-29 07:31:46", "link": "http://arxiv.org/abs/2204.13915v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QRelScore: Better Evaluating Generated Questions with Deeper\n  Understanding of Context-aware Relevance", "abstract": "Existing metrics for assessing question generation not only require costly\nhuman reference but also fail to take into account the input context of\ngeneration, rendering the lack of deep understanding of the relevance between\nthe generated questions and input contexts. As a result, they may wrongly\npenalize a legitimate and reasonable candidate question when it (i) involves\ncomplicated reasoning with the context or (ii) can be grounded by multiple\nevidences in the context. In this paper, we propose $\\textbf{QRelScore}$, a\ncontext-aware $\\underline{\\textbf{Rel}}$evance evaluation metric for\n$\\underline{\\textbf{Q}}$uestion Generation. Based on off-the-shelf language\nmodels such as BERT and GPT2, QRelScore employs both word-level hierarchical\nmatching and sentence-level prompt-based generation to cope with the\ncomplicated reasoning and diverse generation from multiple evidences,\nrespectively. Compared with existing metrics, our experiments demonstrate that\nQRelScore is able to achieve a higher correlation with human judgments while\nbeing much more robust to adversarial samples.", "published": "2022-04-29 07:39:53", "link": "http://arxiv.org/abs/2204.13921v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"My nose is running.\"\"Are you also coughing?\": Building A Medical\n  Diagnosis Agent with Interpretable Inquiry Logics", "abstract": "With the rise of telemedicine, the task of developing Dialogue Systems for\nMedical Diagnosis (DSMD) has received much attention in recent years. Different\nfrom early researches that needed to rely on extra human resources and\nexpertise to help construct the system, recent researches focused on how to\nbuild DSMD in a purely data-driven manner. However, the previous data-driven\nDSMD methods largely overlooked the system interpretability, which is critical\nfor a medical application, and they also suffered from the data sparsity issue\nat the same time. In this paper, we explore how to bring interpretability to\ndata-driven DSMD. Specifically, we propose a more interpretable decision\nprocess to implement the dialogue manager of DSMD by reasonably mimicking real\ndoctors' inquiry logics, and we devise a model with highly transparent\ncomponents to conduct the inference. Moreover, we collect a new DSMD dataset,\nwhich has a much larger scale, more diverse patterns and is of higher quality\nthan the existing ones. The experiments show that our method obtains 7.7%,\n10.0%, 3.0% absolute improvement in diagnosis accuracy respectively on three\ndatasets, demonstrating the effectiveness of its rational decision process and\nmodel design. Our codes and the GMD-12 dataset are available at\nhttps://github.com/lwgkzl/BR-Agent.", "published": "2022-04-29 09:02:23", "link": "http://arxiv.org/abs/2204.13953v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ExaASC: A General Target-Based Stance Detection Corpus in Arabic\n  Language", "abstract": "Target-based Stance Detection is the task of finding a stance toward a\ntarget. Twitter is one of the primary sources of political discussions in\nsocial media and one of the best resources to analyze Stance toward entities.\nThis work proposes a new method toward Target-based Stance detection by using\nthe stance of replies toward a most important and arguing target in source\ntweet. This target is detected with respect to the source tweet itself and not\nlimited to a set of pre-defined targets which is the usual approach of the\ncurrent state-of-the-art methods. Our proposed new attitude resulted in a new\ncorpus called ExaASC for the Arabic Language, one of the low resource languages\nin this field. In the end, we used BERT to evaluate our corpus and reached a\n70.69 Macro F-score. This shows that our data and model can work in a general\nTarget-base Stance Detection system. The corpus is publicly available1.", "published": "2022-04-29 10:03:51", "link": "http://arxiv.org/abs/2204.13979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Climate and Weather: Inspecting Depression Detection via Emotion\n  Recognition", "abstract": "Automatic depression detection has attracted increasing amount of attention\nbut remains a challenging task. Psychological research suggests that depressive\nmood is closely related with emotion expression and perception, which motivates\nthe investigation of whether knowledge of emotion recognition can be\ntransferred for depression detection. This paper uses pretrained features\nextracted from the emotion recognition model for depression detection, further\nfuses emotion modality with audio and text to form multimodal depression\ndetection. The proposed emotion transfer improves depression detection\nperformance on DAIC-WOZ as well as increases the training stability. The\nanalysis of how the emotion expressed by depressed individuals is further\nperceived provides clues for further understanding of the relationship between\ndepression and emotion.", "published": "2022-04-29 13:44:22", "link": "http://arxiv.org/abs/2204.14099v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developmental Negation Processing in Transformer Language Models", "abstract": "Reasoning using negation is known to be difficult for transformer-based\nlanguage models. While previous studies have used the tools of\npsycholinguistics to probe a transformer's ability to reason over negation,\nnone have focused on the types of negation studied in developmental psychology.\nWe explore how well transformers can process such categories of negation, by\nframing the problem as a natural language inference (NLI) task. We curate a set\nof diagnostic questions for our target categories from popular NLI datasets and\nevaluate how well a suite of models reason over them. We find that models\nperform consistently better only on certain categories, suggesting clear\ndistinctions in how they are processed.", "published": "2022-04-29 14:07:34", "link": "http://arxiv.org/abs/2204.14114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OPERA:Operation-Pivoted Discrete Reasoning over Text", "abstract": "Machine reading comprehension (MRC) that requires discrete reasoning\ninvolving symbolic operations, e.g., addition, sorting, and counting, is a\nchallenging task. According to this nature, semantic parsing-based methods\npredict interpretable but complex logical forms. However, logical form\ngeneration is nontrivial and even a little perturbation in a logical form will\nlead to wrong answers. To alleviate this issue, multi-predictor -based methods\nare proposed to directly predict different types of answers and achieve\nimprovements. However, they ignore the utilization of symbolic operations and\nencounter a lack of reasoning ability and interpretability. To inherit the\nadvantages of these two types of methods, we propose OPERA, an\noperation-pivoted discrete reasoning framework, where lightweight symbolic\noperations (compared with logical forms) as neural modules are utilized to\nfacilitate the reasoning ability and interpretability. Specifically, operations\nare first selected and then softly executed to simulate the answer reasoning\nprocedure. Extensive experiments on both DROP and RACENum datasets show the\nreasoning ability of OPERA. Moreover, further analysis verifies its\ninterpretability.", "published": "2022-04-29 15:41:47", "link": "http://arxiv.org/abs/2204.14166v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TemporalWiki: A Lifelong Benchmark for Training and Evaluating\n  Ever-Evolving Language Models", "abstract": "Language Models (LMs) become outdated as the world changes; they often fail\nto perform tasks requiring recent factual information which was absent or\ndifferent during training, a phenomenon called temporal misalignment. This is\nespecially a challenging problem because the research community still lacks a\ncoherent dataset for assessing the adaptability of LMs to frequently-updated\nknowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a\nlifelong benchmark for ever-evolving LMs that utilizes the difference between\nconsecutive snapshots of English Wikipedia and English Wikidata for training\nand evaluation, respectively. The benchmark hence allows researchers to\nperiodically track an LM's ability to retain previous knowledge and acquire\nupdated/new knowledge at each point in time. We also find that training an LM\non the diff data through continual learning methods achieves similar or better\nperplexity than on the entire snapshot in our benchmark with 12 times less\ncomputational cost, which verifies that factual knowledge in LMs can be safely\nupdated with minimal training data via continual learning. The dataset and the\ncode are available at https://github.com/joeljang/temporalwiki.", "published": "2022-04-29 16:40:07", "link": "http://arxiv.org/abs/2204.14211v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Naturalized Semantic Parsers with Very Little Data", "abstract": "Semantic parsing is an important NLP problem, particularly for voice\nassistants such as Alexa and Google Assistant. State-of-the-art (SOTA) semantic\nparsers are seq2seq architectures based on large language models that have been\npretrained on vast amounts of text. To better leverage that pretraining, recent\nwork has explored a reformulation of semantic parsing whereby the output\nsequences are themselves natural language sentences, but in a controlled\nfragment of natural language. This approach delivers strong results,\nparticularly for few-shot semantic parsing, which is of key importance in\npractice and the focus of our paper. We push this line of work forward by\nintroducing an automated methodology that delivers very significant additional\nimprovements by utilizing modest amounts of unannotated data, which is\ntypically easy to obtain. Our method is based on a novel synthesis of four\ntechniques: joint training with auxiliary unsupervised tasks; constrained\ndecoding; self-training; and paraphrasing. We show that this method delivers\nnew SOTA few-shot performance on the Overnight dataset, particularly in very\nlow-resource settings, and very compelling few-shot results on a new semantic\nparsing dataset.", "published": "2022-04-29 17:14:54", "link": "http://arxiv.org/abs/2204.14243v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Handling and Presenting Harmful Text in NLP Research", "abstract": "Text data can pose a risk of harm. However, the risks are not fully\nunderstood, and how to handle, present, and discuss harmful text in a safe way\nremains an unresolved issue in the NLP community. We provide an analytical\nframework categorising harms on three axes: (1) the harm type (e.g.,\nmisinformation, hate speech or racial stereotypes); (2) whether a harm is\n\\textit{sought} as a feature of the research design if explicitly studying\nharmful content (e.g., training a hate speech classifier), versus\n\\textit{unsought} if harmful content is encountered when working on unrelated\nproblems (e.g., language generation or part-of-speech tagging); and (3) who it\naffects, from people (mis)represented in the data to those handling the data\nand those publishing on the data. We provide advice for practitioners, with\nconcrete steps for mitigating harm in research and in publication. To assist\nimplementation we introduce \\textsc{HarmCheck} -- a documentation standard for\nhandling and presenting harmful text in research.", "published": "2022-04-29 17:34:12", "link": "http://arxiv.org/abs/2204.14256v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Polyglot Prompt: Multilingual Multitask PrompTraining", "abstract": "This paper aims for a potential architectural improvement for multilingual\nlearning and asks: Can different tasks from different languages be modeled in a\nmonolithic framework, i.e. without any task/language-specific module? The\nbenefit of achieving this could open new doors for future multilingual\nresearch, including allowing systems trained on low resources to be further\nassisted by other languages as well as other tasks. We approach this goal by\ndeveloping a learning framework named Polyglot Prompting to exploit prompting\nmethods for learning a unified semantic space for different languages and tasks\nwith multilingual prompt engineering. We performed a comprehensive evaluation\nof 6 tasks, namely topic classification, sentiment classification, named entity\nrecognition, question answering, natural language inference, and summarization,\ncovering 24 datasets and 49 languages. The experimental results demonstrated\nthe efficacy of multilingual multitask prompt-based learning and led to\ninspiring observations. We also present an interpretable multilingual\nevaluation methodology and show how the proposed framework, multilingual\nmultitask prompt training, works. We release all datasets prompted in the best\nsetting and code.", "published": "2022-04-29 17:40:50", "link": "http://arxiv.org/abs/2204.14264v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answer Consolidation: Formulation and Benchmarking", "abstract": "Current question answering (QA) systems primarily consider the single-answer\nscenario, where each question is assumed to be paired with one correct answer.\nHowever, in many real-world QA applications, multiple answer scenarios arise\nwhere consolidating answers into a comprehensive and non-redundant set of\nanswers is a more efficient user interface. In this paper, we formulate the\nproblem of answer consolidation, where answers are partitioned into multiple\ngroups, each representing different aspects of the answer set. Then, given this\npartitioning, a comprehensive and non-redundant set of answers can be\nconstructed by picking one answer from each group. To initiate research on\nanswer consolidation, we construct a dataset consisting of 4,699 questions and\n24,006 sentences and evaluate multiple models. Despite a promising performance\nachieved by the best-performing supervised models, we still believe this task\nhas room for further improvements.", "published": "2022-04-29 18:57:23", "link": "http://arxiv.org/abs/2205.00042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Textual Adversarial Examples Based on Distributional\n  Characteristics of Data Representations", "abstract": "Although deep neural networks have achieved state-of-the-art performance in\nvarious machine learning tasks, adversarial examples, constructed by adding\nsmall non-random perturbations to correctly classified inputs, successfully\nfool highly expressive deep classifiers into incorrect predictions. Approaches\nto adversarial attacks in natural language tasks have boomed in the last five\nyears using character-level, word-level, phrase-level, or sentence-level\ntextual perturbations. While there is some work in NLP on defending against\nsuch attacks through proactive methods, like adversarial training, there is to\nour knowledge no effective general reactive approaches to defence via detection\nof textual adversarial examples such as is found in the image processing\nliterature. In this paper, we propose two new reactive methods for NLP to fill\nthis gap, which unlike the few limited application baselines from NLP are based\nentirely on distribution characteristics of learned representations: we adapt\none from the image processing literature (Local Intrinsic Dimensionality\n(LID)), and propose a novel one (MultiDistance Representation Ensemble Method\n(MDRE)). Adapted LID and MDRE obtain state-of-the-art results on\ncharacter-level, word-level, and phrase-level attacks on the IMDB dataset as\nwell as on the later two with respect to the MultiNLI dataset. For future\nresearch, we publish our code.", "published": "2022-04-29 02:32:02", "link": "http://arxiv.org/abs/2204.13853v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leaner and Faster: Two-Stage Model Compression for Lightweight\n  Text-Image Retrieval", "abstract": "Current text-image approaches (e.g., CLIP) typically adopt dual-encoder\narchitecture using pre-trained vision-language representation. However, these\nmodels still pose non-trivial memory requirements and substantial incremental\nindexing time, which makes them less practical on mobile devices. In this\npaper, we present an effective two-stage framework to compress large\npre-trained dual-encoder for lightweight text-image retrieval. The resulting\nmodel is smaller (39% of the original), faster (1.6x/2.9x for processing\nimage/text respectively), yet performs on par with or better than the original\nfull model on Flickr30K and MSCOCO benchmarks. We also open-source an\naccompanying realistic mobile image search application.", "published": "2022-04-29 07:29:06", "link": "http://arxiv.org/abs/2204.13913v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "KERMIT -- A Transformer-Based Approach for Knowledge Graph Matching", "abstract": "One of the strongest signals for automated matching of knowledge graphs and\nontologies are textual concept descriptions. With the rise of transformer-based\nlanguage models, text comparison based on meaning (rather than lexical\nfeatures) is available to researchers. However, performing pairwise comparisons\nof all textual descriptions of concepts in two knowledge graphs is expensive\nand scales quadratically (or even worse if concepts have more than one\ndescription). To overcome this problem, we follow a two-step approach: we first\ngenerate matching candidates using a pre-trained sentence transformer (so\ncalled bi-encoder). In a second step, we use fine-tuned transformer\ncross-encoders to generate the best candidates. We evaluate our approach on\nmultiple datasets and show that it is feasible and produces competitive\nresults.", "published": "2022-04-29 08:07:17", "link": "http://arxiv.org/abs/2204.13931v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PIE: a Parameter and Inference Efficient Solution for Large Scale\n  Knowledge Graph Embedding Reasoning", "abstract": "Knowledge graph (KG) embedding methods which map entities and relations to\nunique embeddings in the KG have shown promising results on many reasoning\ntasks. However, the same embedding dimension for both dense entities and sparse\nentities will cause either over parameterization (sparse entities) or under\nfitting (dense entities). Normally, a large dimension is set to get better\nperformance. Meanwhile, the inference time grows log-linearly with the number\nof entities for all entities are traversed and compared. Both the parameter and\ninference become challenges when working with huge amounts of entities. Thus,\nwe propose PIE, a \\textbf{p}arameter and \\textbf{i}nference \\textbf{e}fficient\nsolution. Inspired from tensor decomposition methods, we find that decompose\nentity embedding matrix into low rank matrices can reduce more than half of the\nparameters while maintaining comparable performance. To accelerate model\ninference, we propose a self-supervised auxiliary task, which can be seen as\nfine-grained entity typing. By randomly masking and recovering entities'\nconnected relations, the task learns the co-occurrence of entity and relations.\nUtilizing the fine grained typing, we can filter unrelated entities during\ninference and get targets with possibly sub-linear time requirement.\nExperiments on link prediction benchmarks demonstrate the proposed key\ncapabilities. Moreover, we prove effectiveness of the proposed solution on the\nOpen Graph Benchmark large scale challenge dataset WikiKG90Mv2 and achieve the\nstate of the art performance.", "published": "2022-04-29 09:06:56", "link": "http://arxiv.org/abs/2204.13957v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Robust is Neural Machine Translation to Language Imbalance in\n  Multilingual Tokenizer Training?", "abstract": "A multilingual tokenizer is a fundamental component of multilingual neural\nmachine translation. It is trained from a multilingual corpus. Since a skewed\ndata distribution is considered to be harmful, a sampling strategy is usually\nused to balance languages in the corpus. However, few works have systematically\nanswered how language imbalance in tokenizer training affects downstream\nperformance. In this work, we analyze how translation performance changes as\nthe data ratios among languages vary in the tokenizer training corpus. We find\nthat while relatively better performance is often observed when languages are\nmore equally sampled, the downstream performance is more robust to language\nimbalance than we usually expected. Two features, UNK rate and closeness to the\ncharacter level, can warn of poor downstream performance before performing the\ntask. We also distinguish language sampling for tokenizer training from\nsampling for model training and show that the model is more sensitive to the\nlatter.", "published": "2022-04-29 17:50:36", "link": "http://arxiv.org/abs/2204.14268v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What do we Really Know about State of the Art NER?", "abstract": "Named Entity Recognition (NER) is a well researched NLP task and is widely\nused in real world NLP scenarios. NER research typically focuses on the\ncreation of new ways of training NER, with relatively less emphasis on\nresources and evaluation. Further, state of the art (SOTA) NER models, trained\non standard datasets, typically report only a single performance measure\n(F-score) and we don't really know how well they do for different entity types\nand genres of text, or how robust are they to new, unseen entities. In this\npaper, we perform a broad evaluation of NER using a popular dataset, that takes\ninto consideration various text genres and sources constituting the dataset at\nhand. Additionally, we generate six new adversarial test sets through small\nperturbations in the original test set, replacing select entities while\nretaining the context. We also train and test our models on randomly generated\ntrain/dev/test splits followed by an experiment where the models are trained on\na select set of genres but tested genres not seen in training. These\ncomprehensive evaluation strategies were performed using three SOTA NER models.\nBased on our results, we recommend some useful reporting practices for NER\nresearchers, that could help in providing a better understanding of a SOTA\nmodel's performance in future.", "published": "2022-04-29 18:35:53", "link": "http://arxiv.org/abs/2205.00034v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt Consistency for Zero-Shot Task Generalization", "abstract": "One of the most impressive results of recent NLP history is the ability of\npre-trained language models to solve new tasks in a zero-shot setting. To\nachieve this, NLP tasks are framed as natural language prompts, generating a\nresponse indicating the predicted output. Nonetheless, the performance in such\nsettings often lags far behind its supervised counterpart, suggesting a large\nspace for potential improvement. In this paper, we explore methods to utilize\nunlabeled data to improve zero-shot performance. Specifically, we take\nadvantage of the fact that multiple prompts can be used to specify a single\ntask, and propose to regularize prompt consistency, encouraging consistent\npredictions over this diverse set of prompts. Our method makes it possible to\nfine-tune the model either with extra unlabeled training data, or directly on\ntest input at inference time in an unsupervised manner. In experiments, our\napproach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,\n2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points\nin terms of accuracy. The gains are often attained with a small number of\nunlabeled examples.", "published": "2022-04-29 19:18:37", "link": "http://arxiv.org/abs/2205.00049v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Plagiarism in Introductory Programming Course Assignments", "abstract": "Measuring plagiarism in programming assignments is an essential task to the\neducational procedure. This paper discusses the methods of plagiarism and its\ndetection in introductory programming course assignments written in C++. A\nsmall corpus of assignments is made publically available. A general framework\nto compute the similarity between a solution pair is developed that uses the\nthree token-based similarity methods as features and predicts if the solution\nis plagiarized. The importance of each feature is also measured, which in\nreturn ranks the effectiveness of each method in use. Finally, the artificially\ngenerated dataset improves the results compared to the original data. We\nachieved an F1 score of 0.955 and 0.971 on original and synthetic datasets.", "published": "2022-04-29 17:06:26", "link": "http://arxiv.org/abs/2205.08520v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Repro: An Open-Source Library for Improving the Reproducibility and\n  Usability of Publicly Available Research Code", "abstract": "We introduce Repro, an open-source library which aims at improving the\nreproducibility and usability of research code. The library provides a\nlightweight Python API for running software released by researchers within\nDocker containers which contain the exact required runtime configuration and\ndependencies for the code. Because the environment setup for each package is\nhandled by Docker, users do not have to do any configuration themselves. Once\nRepro is installed, users can run the code for the 30+ papers currently\nsupported by the library. We hope researchers see the value provided to others\nby including their research code in Repro and consider adding support for their\nown research code.", "published": "2022-04-29 01:54:54", "link": "http://arxiv.org/abs/2204.13848v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Vision-Language Pre-Training for Boosting Scene Text Detectors", "abstract": "Recently, vision-language joint representation learning has proven to be\nhighly effective in various scenarios. In this paper, we specifically adapt\nvision-language joint learning for scene text detection, a task that\nintrinsically involves cross-modal interaction between the two modalities:\nvision and language, since text is the written form of language. Concretely, we\npropose to learn contextualized, joint representations through vision-language\npre-training, for the sake of enhancing the performance of scene text\ndetectors. Towards this end, we devise a pre-training architecture with an\nimage encoder, a text encoder and a cross-modal encoder, as well as three\npretext tasks: image-text contrastive learning (ITC), masked language modeling\n(MLM) and word-in-image prediction (WIP). The pre-trained model is able to\nproduce more informative representations with richer semantics, which could\nreadily benefit existing scene text detectors (such as EAST and PSENet) in the\ndown-stream text detection task. Extensive experiments on standard benchmarks\ndemonstrate that the proposed paradigm can significantly improve the\nperformance of various representative text detectors, outperforming previous\npre-training approaches. The code and pre-trained models will be publicly\nreleased.", "published": "2022-04-29 03:53:54", "link": "http://arxiv.org/abs/2204.13867v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "User Experience Design for Automatic Credibility Assessment of News\n  Content About COVID-19", "abstract": "The increasingly rapid spread of information about COVID-19 on the web calls\nfor automatic measures of quality assurance. In that context, we check the\ncredibility of news content using selected linguistic features. We present two\nempirical studies to evaluate the usability of graphical interfaces that offer\nsuch credibility assessment. In a moderated qualitative interview with six\nparticipants, we identify rating scale, sub-criteria and algorithm authorship\nas important predictors of the usability. A subsequent quantitative online\nsurvey with 50 participants reveals a conflict between transparency and\nconciseness in the interface design, as well as a perceived hierarchy of\nmetadata: the authorship of a news text is more important than the authorship\nof the credibility algorithm used to assess the content quality. Finally, we\nmake suggestions for future research, such as proactively documenting\ncredibility-related metadata for Natural Language Processing and Language\nTechnology services and establishing an explicit hierarchical taxonomy of\nusability predictors for automatic credibility assessment.", "published": "2022-04-29 08:38:45", "link": "http://arxiv.org/abs/2204.13943v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "68-04", "H.5.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Making sense of violence risk predictions using clinical notes", "abstract": "Violence risk assessment in psychiatric institutions enables interventions to\navoid violence incidents. Clinical notes written by practitioners and available\nin electronic health records (EHR) are valuable resources that are seldom used\nto their full potential. Previous studies have attempted to assess violence\nrisk in psychiatric patients using such notes, with acceptable performance.\nHowever, they do not explain why classification works and how it can be\nimproved. We explore two methods to better understand the quality of a\nclassifier in the context of clinical note analysis: random forests using topic\nmodels, and choice of evaluation metric. These methods allow us to understand\nboth our data and our methodology more profoundly, setting up the groundwork to\nwork on improved models that build upon this understanding. This is\nparticularly important when it comes to the generalizability of evaluated\nclassifiers to new data, a trustworthiness problem that is of great interest\ndue to the increased availability of new data in electronic format.", "published": "2022-04-29 10:00:07", "link": "http://arxiv.org/abs/2204.13976v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient\n  Ensembling", "abstract": "Recent advances in federated learning have demonstrated its promising\ncapability to learn on decentralized datasets. However, a considerable amount\nof work has raised concerns due to the potential risks of adversaries\nparticipating in the framework to poison the global model for an adversarial\npurpose. This paper investigates the feasibility of model poisoning for\nbackdoor attacks through rare word embeddings of NLP models. In text\nclassification, less than 1% of adversary clients suffices to manipulate the\nmodel output without any drop in the performance on clean sentences. For a less\ncomplex dataset, a mere 0.1% of adversary clients is enough to poison the\nglobal model effectively. We also propose a technique specialized in the\nfederated learning scheme called Gradient Ensemble, which enhances the backdoor\nperformance in all our experimental settings.", "published": "2022-04-29 11:17:05", "link": "http://arxiv.org/abs/2204.14017v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Training Language Models with Language Feedback", "abstract": "Pretrained language models often do not perform tasks in ways that are in\nline with our preferences, e.g., generating offensive text or factually\nincorrect summaries. Recent work approaches the above issue by learning from a\nsimple form of human evaluation: comparisons between pairs of model-generated\ntask outputs. Comparison feedback conveys limited information about human\npreferences per human evaluation. Here, we propose to learn from natural\nlanguage feedback, which conveys more information per human evaluation. We\nlearn from language feedback on model outputs using a three-step learning\nalgorithm. First, we condition the language model on the initial output and\nfeedback to generate many refinements. Second, we choose the refinement with\nthe highest similarity to the feedback. Third, we finetune a language model to\nmaximize the likelihood of the chosen refinement given the input. In synthetic\nexperiments, we first evaluate whether language models accurately incorporate\nfeedback to produce refinements, finding that only large language models (175B\nparameters) do so. Using only 100 samples of human-written feedback, our\nlearning algorithm finetunes a GPT-3 model to roughly human-level summarization\nability.", "published": "2022-04-29 15:06:58", "link": "http://arxiv.org/abs/2204.14146v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-end Spoken Conversational Question Answering: Task, Dataset and\n  Model", "abstract": "In spoken question answering, the systems are designed to answer questions\nfrom contiguous text spans within the related speech transcripts. However, the\nmost natural way that human seek or test their knowledge is via human\nconversations. Therefore, we propose a new Spoken Conversational Question\nAnswering task (SCQA), aiming at enabling the systems to model complex dialogue\nflows given the speech documents. In this task, our main objective is to build\nthe system to deal with conversational questions based on the audio recordings,\nand to explore the plausibility of providing more cues from different\nmodalities with systems in information gathering. To this end, instead of\ndirectly adopting automatically generated speech transcripts with highly noisy\ndata, we propose a novel unified data distillation approach, DDNet, which\neffectively ingests cross-modal information to achieve fine-grained\nrepresentations of the speech and language modalities. Moreover, we propose a\nsimple and novel mechanism, termed Dual Attention, by encouraging better\nalignments between audio and text to ease the process of knowledge transfer. To\nevaluate the capacity of SCQA systems in a dialogue-style interaction, we\nassemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with\nmore than 40k question-answer pairs from 4k conversations. The performance of\nthe existing state-of-the-art methods significantly degrade on our dataset,\nhence demonstrating the necessity of cross-modal information integration. Our\nexperimental results demonstrate that our proposed method achieves superior\nperformance in spoken conversational question answering tasks.", "published": "2022-04-29 17:56:59", "link": "http://arxiv.org/abs/2204.14272v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Aware Feedback-Based Self-Learning in Large-Scale Conversational AI", "abstract": "Self-learning paradigms in large-scale conversational AI agents tend to\nleverage user feedback in bridging between what they say and what they mean.\nHowever, such learning, particularly in Markov-based query rewriting systems\nhave far from addressed the impact of these models on future training where\nsuccessive feedback is inevitably contingent on the rewrite itself, especially\nin a continually updating environment. In this paper, we explore the\nconsequences of this inherent lack of self-awareness towards impairing the\nmodel performance, ultimately resulting in both Type I and II errors over time.\nTo that end, we propose augmenting the Markov Graph construction with a\nsuperposition-based adjacency matrix. Here, our method leverages an induced\nstochasticity to reactively learn a locally-adaptive decision boundary based on\nthe performance of the individual rewrites in a bi-variate beta setting. We\nalso surface a data augmentation strategy that leverages template-based\ngeneration in abridging complex conversation hierarchies of dialogs so as to\nsimplify the learning process. All in all, we demonstrate that our self-aware\nmodel improves the overall PR-AUC by 27.45%, achieves a relative defect\nreduction of up to 31.22%, and is able to adapt quicker to changes in global\npreferences across a large number of customers.", "published": "2022-04-29 18:18:40", "link": "http://arxiv.org/abs/2205.00029v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Logically Consistent Adversarial Attacks for Soft Theorem Provers", "abstract": "Recent efforts within the AI community have yielded impressive results\ntowards \"soft theorem proving\" over natural language sentences using language\nmodels. We propose a novel, generative adversarial framework for probing and\nimproving these models' reasoning capabilities. Adversarial attacks in this\ndomain suffer from the logical inconsistency problem, whereby perturbations to\nthe input may alter the label. Our Logically consistent AdVersarial Attacker,\nLAVA, addresses this by combining a structured generative process with a\nsymbolic solver, guaranteeing logical consistency. Our framework successfully\ngenerates adversarial attacks and identifies global weaknesses common across\nmultiple target models. Our analyses reveal naive heuristics and\nvulnerabilities in these models' reasoning capabilities, exposing an incomplete\ngrasp of logical deduction under logic programs. Finally, in addition to\neffective probing of these models, we show that training on the generated\nsamples improves the target model's performance.", "published": "2022-04-29 19:10:12", "link": "http://arxiv.org/abs/2205.00047v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Autonomous In-Situ Soundscape Augmentation via Joint Selection of Masker\n  and Gain", "abstract": "The selection of maskers and playback gain levels in a soundscape\naugmentation system is crucial to its effectiveness in improving the overall\nacoustic comfort of a given environment. Traditionally, the selection of\nappropriate maskers and gain levels has been informed by expert opinion, which\nmay not representative of the target population, or by listening tests, which\ncan be time-consuming and labour-intensive. Furthermore, the resulting static\nchoices of masker and gain are often inflexible to the dynamic nature of\nreal-world soundscapes. In this work, we utilized a deep learning model to\nperform joint selection of the optimal masker and its gain level for a given\nsoundscape. The proposed model was designed with highly modular building\nblocks, allowing for an optimized inference process that can quickly search\nthrough a large number of masker and gain combinations. In addition, we\nintroduced the use of feature-domain soundscape augmentation conditioned on the\ndigital gain level, eliminating the computationally expensive waveform-domain\nmixing process during inference time, as well as the tedious pre-calibration\nprocess required for new maskers. The proposed system was validated on a\nlarge-scale dataset of subjective responses to augmented soundscapes with more\nthan 440 participants, ensuring the ability of the model to predict combined\neffect of the masker and its gain level on the perceptual pleasantness level.", "published": "2022-04-29 04:59:56", "link": "http://arxiv.org/abs/2204.13883v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deployment of an IoT System for Adaptive In-Situ Soundscape Augmentation", "abstract": "Soundscape augmentation is an emerging approach for noise mitigation by\nintroducing additional sounds known as \"maskers\" to increase acoustic comfort.\nTraditionally, the choice of maskers is often predicated on expert guidance or\npost-hoc analysis which can be time-consuming and sometimes arbitrary.\nMoreover, this often results in a static set of maskers that are inflexible to\nthe dynamic nature of real-world acoustic environments. Overcoming the\ninflexibility of traditional soundscape augmentation is twofold. First, given a\nsnapshot of a soundscape, the system must be able to select an optimal masker\nwithout human supervision. Second, the system must also be able to react to\nchanges in the acoustic environment with near real-time latency. In this work,\nwe harness the combined prowess of cloud computing and the Internet of Things\n(IoT) to allow in-situ listening and playback using microcontrollers while\ndelegating computationally expensive inference tasks to the cloud. In\nparticular, a serverless cloud architecture was used for inference, ensuring\nnear real-time latency and scalability without the need to provision computing\nresources. A working prototype of the system is currently being deployed in a\npublic area experiencing high traffic noise, as well as undergoing public\nevaluation for future improvements.", "published": "2022-04-29 05:34:50", "link": "http://arxiv.org/abs/2204.13890v1", "categories": ["eess.AS", "cs.SD", "cs.SY", "eess.SY"], "primary_category": "eess.AS"}
