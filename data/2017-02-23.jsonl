{"title": "LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and\n  Vector Representations", "abstract": "Topic models have been widely used in discovering latent topics which are\nshared across documents in text mining. Vector representations, word embeddings\nand topic embeddings, map words and topics into a low-dimensional and dense\nreal-value vector space, which have obtained high performance in NLP tasks.\nHowever, most of the existing models assume the result trained by one of them\nare perfect correct and used as prior knowledge for improving the other model.\nSome other models use the information trained from external large corpus to\nhelp improving smaller corpus. In this paper, we aim to build such an algorithm\nframework that makes topic models and vector representations mutually improve\neach other within the same corpus. An EM-style algorithm framework is employed\nto iteratively optimize both topic model and vector representations.\nExperimental results show that our model outperforms state-of-art methods on\nvarious NLP tasks.", "published": "2017-02-23 07:16:03", "link": "http://arxiv.org/abs/1702.07117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Utilizing Lexical Similarity between Related, Low-resource Languages for\n  Pivot-based SMT", "abstract": "We investigate pivot-based translation between related languages in a low\nresource, phrase-based SMT setting. We show that a subword-level pivot-based\nSMT model using a related pivot language is substantially better than word and\nmorpheme-level pivot models. It is also highly competitive with the best direct\ntranslation model, which is encouraging as no direct source-target training\ncorpus is used. We also show that combining multiple related language pivot\nmodels can rival a direct translation model. Thus, the use of subwords as\ntranslation units coupled with multiple related pivot languages can compensate\nfor the lack of a direct parallel corpus.", "published": "2017-02-23 13:13:53", "link": "http://arxiv.org/abs/1702.07203v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Emojis Predictable?", "abstract": "Emojis are ideograms which are naturally combined with plain text to visually\ncomplement or condense the meaning of a message. Despite being widely used in\nsocial media, their underlying semantics have received little attention from a\nNatural Language Processing standpoint. In this paper, we investigate the\nrelation between words and emojis, studying the novel task of predicting which\nemojis are evoked by text-based tweet messages. We train several models based\non Long Short-Term Memory networks (LSTMs) in this task. Our experimental\nresults show that our neural model outperforms two baselines as well as humans\nsolving the same task, suggesting that computational models are able to better\ncapture the underlying semantics of emojis.", "published": "2017-02-23 16:47:01", "link": "http://arxiv.org/abs/1702.07285v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inherent Biases of Recurrent Neural Networks for Phonological\n  Assimilation and Dissimilation", "abstract": "A recurrent neural network model of phonological pattern learning is\nproposed. The model is a relatively simple neural network with one recurrent\nlayer, and displays biases in learning that mimic observed biases in human\nlearning. Single-feature patterns are learned faster than two-feature patterns,\nand vowel or consonant-only patterns are learned faster than patterns involving\nvowels and consonants, mimicking the results of laboratory learning\nexperiments. In non-recurrent models, capturing these biases requires the use\nof alpha features or some other representation of repeated features, but with a\nrecurrent neural network, these elaborations are not necessary.", "published": "2017-02-23 18:19:35", "link": "http://arxiv.org/abs/1702.07324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pronunciation recognition of English phonemes /\\textipa{@}/, /\u00e6/,\n  /\\textipa{A}:/ and /\\textipa{2}/ using Formants and Mel Frequency Cepstral\n  Coefficients", "abstract": "The Vocal Joystick Vowel Corpus, by Washington University, was used to study\nmonophthongs pronounced by native English speakers. The objective of this study\nwas to quantitatively measure the extent at which speech recognition methods\ncan distinguish between similar sounding vowels. In particular, the phonemes\n/\\textipa{@}/, /{\\ae}/, /\\textipa{A}:/ and /\\textipa{2}/ were analysed. 748\nsound files from the corpus were used and subjected to Linear Predictive Coding\n(LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients\n(MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree\nClassifier was used to build a predictive model that learnt the patterns of the\ntwo first formants measured in the data set, as well as the patterns of the 13\ncepstral coefficients. An accuracy of 70\\% was achieved using formants for the\nmentioned phonemes. For the MFCC analysis an accuracy of 52 \\% was achieved and\nan accuracy of 71\\% when /\\textipa{@}/ was ignored. The results obtained show\nthat the studied algorithms are far from mimicking the ability of\ndistinguishing subtle differences in sounds like human hearing does.", "published": "2017-02-23 02:31:03", "link": "http://arxiv.org/abs/1702.07071v1", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "A Neural Attention Model for Categorizing Patient Safety Events", "abstract": "Medical errors are leading causes of death in the US and as such, prevention\nof these errors is paramount to promoting health care. Patient Safety Event\nreports are narratives describing potential adverse events to the patients and\nare important in identifying and preventing medical errors. We present a neural\nnetwork architecture for identifying the type of safety events which is the\nfirst step in understanding these narratives. Our proposed model is based on a\nsoft neural attention model to improve the effectiveness of encoding long\nsequences. Empirical results on two large-scale real-world datasets of patient\nsafety reports demonstrate the effectiveness of our method with significant\nimprovements over existing methods.", "published": "2017-02-23 04:27:49", "link": "http://arxiv.org/abs/1702.07092v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Stability of Topic Modeling via Matrix Factorization", "abstract": "Topic models can provide us with an insight into the underlying latent\nstructure of a large corpus of documents. A range of methods have been proposed\nin the literature, including probabilistic topic models and techniques based on\nmatrix factorization. However, in both cases, standard implementations rely on\nstochastic elements in their initialization phase, which can potentially lead\nto different results being generated on the same corpus when using the same\nparameter values. This corresponds to the concept of \"instability\" which has\npreviously been studied in the context of $k$-means clustering. In many\napplications of topic modeling, this problem of instability is not considered\nand topic models are treated as being definitive, even though the results may\nchange considerably if the initialization process is altered. In this paper we\ndemonstrate the inherent instability of popular topic modeling approaches,\nusing a number of new measures to assess stability. To address this issue in\nthe context of matrix factorization for topic modeling, we propose the use of\nensemble learning strategies. Based on experiments performed on annotated text\ncorpora, we show that a K-Fold ensemble strategy, combining both ensembles and\nstructured initialization, can significantly reduce instability, while\nsimultaneously yielding more accurate topic models.", "published": "2017-02-23 12:00:10", "link": "http://arxiv.org/abs/1702.07186v2", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
