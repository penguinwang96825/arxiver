{"title": "Improved Transition-Based Parsing by Modeling Characters instead of\n  Words with LSTMs", "abstract": "We present extensions to a continuous-state dependency parsing method that\nmakes it applicable to morphologically rich languages. Starting with a\nhigh-performance transition-based parser that uses long short-term memory\n(LSTM) recurrent neural networks to learn representations of the parser state,\nwe replace lookup-based word representations with representations constructed\nfrom the orthographic representations of the words, also using LSTMs. This\nallows statistical sharing across word forms that are similar on the surface.\nExperiments for morphologically rich languages show that the parsing model\nbenefits from incorporating the character-based encodings of words.", "published": "2015-08-04 04:36:36", "link": "http://arxiv.org/abs/1508.00657v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Modal Bayesian Embeddings for Learning Social Knowledge Graphs", "abstract": "We study the extent to which online social networks can be connected to open\nknowledge bases. The problem is referred to as learning social knowledge\ngraphs. We propose a multi-modal Bayesian embedding model, GenVector, to learn\nlatent topics that generate word and network embeddings. GenVector leverages\nlarge-scale unlabeled data with embeddings and represents data of two\nmodalities---i.e., social network users and knowledge concepts---in a shared\nlatent topic space. Experiments on three datasets show that the proposed method\nclearly outperforms state-of-the-art methods. We then deploy the method on\nAMiner, a large-scale online academic search system with a network of\n38,049,189 researchers with a knowledge base with 35,415,011 concepts. Our\nmethod significantly decreases the error rate in an online A/B test with live\nusers.", "published": "2015-08-04 09:34:22", "link": "http://arxiv.org/abs/1508.00715v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
