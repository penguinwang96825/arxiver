{"title": "OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset\n  with Visual Contexts", "abstract": "In order to better simulate the real human conversation process, models need\nto generate dialogue utterances based on not only preceding textual contexts\nbut also visual contexts. However, with the development of multi-modal dialogue\nlearning, the dataset scale gradually becomes a bottleneck. In this report, we\nrelease OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset\ncompared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a\ntotal number of 5.6 million dialogue turns extracted from either movies or TV\nseries from different resources, and each dialogue turn is paired with its\ncorresponding visual context. We hope this large-scale dataset can help\nfacilitate future researches on open-domain multi-modal dialog generation,\ne.g., multi-modal pretraining for dialogue generation.", "published": "2021-09-27 02:10:29", "link": "http://arxiv.org/abs/2109.12761v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rumour Detection via Zero-shot Cross-lingual Transfer Learning", "abstract": "Most rumour detection models for social media are designed for one specific\nlanguage (mostly English). There are over 40 languages on Twitter and most\nlanguages lack annotated resources to build rumour detection models. In this\npaper we propose a zero-shot cross-lingual transfer learning framework that can\nadapt a rumour detection model trained for a source language to another target\nlanguage. Our framework utilises pretrained multilingual language models (e.g.\\\nmultilingual BERT) and a self-training loop to iteratively bootstrap the\ncreation of ''silver labels'' in the target language to adapt the model from\nthe source language to the target language. We evaluate our methodology on\nEnglish and Chinese rumour datasets and demonstrate that our model\nsubstantially outperforms competitive benchmarks in both source and target\nlanguage rumour detection.", "published": "2021-09-27 03:07:25", "link": "http://arxiv.org/abs/2109.12773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Non-local Features for Neural Constituency Parsing", "abstract": "Thanks to the strong representation power of neural encoders, neural\nchart-based parsers have achieved highly competitive performance by using local\nfeatures. Recently, it has been shown that non-local features in CRF structures\nlead to improvements. In this paper, we investigate injecting non-local\nfeatures into the training process of a local span-based parser, by predicting\nconstituent n-gram non-local patterns and ensuring consistency between\nnon-local patterns and local constituents. Results show that our simple method\ngives better results than the self-attentive parser on both PTB and CTB.\nBesides, our method achieves state-of-the-art BERT-based performance on PTB\n(95.92 F1) and strong performance on CTB (92.31 F1). Our parser also achieves\nbetter or competitive performance in multilingual and zero-shot cross-domain\nsettings compared with the baseline.", "published": "2021-09-27 06:14:30", "link": "http://arxiv.org/abs/2109.12814v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MFAQ: a Multilingual FAQ Dataset", "abstract": "In this paper, we present the first multilingual FAQ dataset publicly\navailable. We collected around 6M FAQ pairs from the web, in 21 different\nlanguages. Although this is significantly larger than existing FAQ retrieval\ndatasets, it comes with its own challenges: duplication of content and uneven\ndistribution of topics. We adopt a similar setup as Dense Passage Retrieval\n(DPR) and test various bi-encoders on this dataset. Our experiments reveal that\na multilingual model based on XLM-RoBERTa achieves the best results, except for\nEnglish. Lower resources languages seem to learn from one another as a\nmultilingual model achieves a higher MRR than language-specific ones. Our\nqualitative analysis reveals the brittleness of the model on simple word\nchanges. We publicly release our dataset, model and training script.", "published": "2021-09-27 08:43:25", "link": "http://arxiv.org/abs/2109.12870v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small data problems in political research: a critical replication study", "abstract": "In an often-cited 2019 paper on the use of machine learning in political\nresearch, Anastasopoulos & Whitford (A&W) propose a text classification method\nfor tweets related to organizational reputation. The aim of their paper was to\nprovide a 'guide to practice' for public administration scholars and\npractitioners on the use of machine learning. In the current paper we follow up\non that work with a replication of A&W's experiments and additional analyses on\nmodel stability and the effects of preprocessing, both in relation to the small\ndata size. We show that (1) the small data causes the classification model to\nbe highly sensitive to variations in the random train-test split, and that (2)\nthe applied preprocessing causes the data to be extremely sparse, with the\nmajority of items in the data having at most two non-zero lexical features.\nWith additional experiments in which we vary the steps of the preprocessing\npipeline, we show that the small data size keeps causing problems, irrespective\nof the preprocessing choices. Based on our findings, we argue that A&W's\nconclusions regarding the automated classification of organizational reputation\ntweets -- either substantive or methodological -- can not be maintained and\nrequire a larger data set for training and more careful validation.", "published": "2021-09-27 09:55:58", "link": "http://arxiv.org/abs/2109.12911v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The JDDC 2.0 Corpus: A Large-Scale Multimodal Multi-Turn Chinese\n  Dialogue Dataset for E-commerce Customer Service", "abstract": "With the development of the Internet, more and more people get accustomed to\nonline shopping. When communicating with customer service, users may express\ntheir requirements by means of text, images, and videos, which precipitates the\nneed for understanding these multimodal information for automatic customer\nservice systems. Images usually act as discriminators for product models, or\nindicators of product failures, which play important roles in the E-commerce\nscenario. On the other hand, detailed information provided by the images is\nlimited, and typically, customer service systems cannot understand the intents\nof users without the input text. Thus, bridging the gap of the image and text\nis crucial for the multimodal dialogue task. To handle this problem, we\nconstruct JDDC 2.0, a large-scale multimodal multi-turn dialogue dataset\ncollected from a mainstream Chinese E-commerce platform (JD.com), containing\nabout 246 thousand dialogue sessions, 3 million utterances, and 507 thousand\nimages, along with product knowledge bases and image category annotations. We\npresent the solutions of top-5 teams participating in the JDDC multimodal\ndialogue challenge based on this dataset, which provides valuable insights for\nfurther researches on the multimodal dialogue task.", "published": "2021-09-27 09:57:44", "link": "http://arxiv.org/abs/2109.12913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake News Detection: Experiments and Approaches beyond Linguistic\n  Features", "abstract": "Easier access to the internet and social media has made disseminating\ninformation through online sources very easy. Sources like Facebook, Twitter,\nonline news sites and personal blogs of self-proclaimed journalists have become\nsignificant players in providing news content. The sheer amount of information\nand the speed at which it is generated online makes it practically beyond the\nscope of human verification. There is, hence, a pressing need to develop\ntechnologies that can assist humans with automatic fact-checking and reliable\nidentification of fake news. This paper summarizes the multiple approaches that\nwere undertaken and the experiments that were carried out for the task.\nCredibility information and metadata associated with the news article have been\nused for improved results. The experiments also show how modelling\njustification or evidence can lead to improved results. Additionally, the use\nof visual features in addition to linguistic features is demonstrated. A\ndetailed comparison of the results showing that our models perform\nsignificantly well when compared to robust baselines as well as\nstate-of-the-art models are presented.", "published": "2021-09-27 10:00:44", "link": "http://arxiv.org/abs/2109.12914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrated Training for Sequence-to-Sequence Models Using\n  Non-Autoregressive Transformer", "abstract": "Complex natural language applications such as speech translation or pivot\ntranslation traditionally rely on cascaded models. However, cascaded models are\nknown to be prone to error propagation and model discrepancy problems.\nFurthermore, there is no possibility of using end-to-end training data in\nconventional cascaded systems, meaning that the training data most suited for\nthe task cannot be used. Previous studies suggested several approaches for\nintegrated end-to-end training to overcome those problems, however they mostly\nrely on (synthetic or natural) three-way data. We propose a cascaded model\nbased on the non-autoregressive Transformer that enables end-to-end training\nwithout the need for an explicit intermediate representation. This new\narchitecture (i) avoids unnecessary early decisions that can cause errors which\nare then propagated throughout the cascaded models and (ii) utilizes the\nend-to-end training data directly. We conduct an evaluation on two pivot-based\nmachine translation tasks, namely French-German and German-Czech. Our\nexperimental results show that the proposed architecture yields an improvement\nof more than 2 BLEU for French-German over the cascaded baseline.", "published": "2021-09-27 11:04:09", "link": "http://arxiv.org/abs/2109.12950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pragmatic competence of pre-trained language models through the lens of\n  discourse connectives", "abstract": "As pre-trained language models (LMs) continue to dominate NLP, it is\nincreasingly important that we understand the depth of language capabilities in\nthese models. In this paper, we target pre-trained LMs' competence in\npragmatics, with a focus on pragmatics relating to discourse connectives. We\nformulate cloze-style tests using a combination of naturally-occurring data and\ncontrolled inputs drawn from psycholinguistics. We focus on testing models'\nability to use pragmatic cues to predict discourse connectives, models' ability\nto understand implicatures relating to connectives, and the extent to which\nmodels show humanlike preferences regarding temporal dynamics of connectives.\nWe find that although models predict connectives reasonably well in the context\nof naturally-occurring data, when we control contexts to isolate high-level\npragmatic cues, model sensitivity is much lower. Models also do not show\nsubstantial humanlike temporal preferences. Overall, the findings suggest that\nat present, dominant pre-training paradigms do not result in substantial\npragmatic competence in our models.", "published": "2021-09-27 11:04:41", "link": "http://arxiv.org/abs/2109.12951v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Context-guided Triple Matching for Multiple Choice Question Answering", "abstract": "The task of multiple choice question answering (MCQA) refers to identifying a\nsuitable answer from multiple candidates, by estimating the matching score\namong the triple of the passage, question and answer. Despite the general\nresearch interest in this regard, existing methods decouple the process into\nseveral pair-wise or dual matching steps, that limited the ability of assessing\ncases with multiple evidence sentences. To alleviate this issue, this paper\nintroduces a novel Context-guided Triple Matching algorithm, which is achieved\nby integrating a Triple Matching (TM) module and a Contrastive Regularization\n(CR). The former is designed to enumerate one component from the triple as the\nbackground context, and estimate its semantic matching with the other two.\nAdditionally, the contrastive term is further proposed to capture the\ndissimilarity between the correct answer and distractive ones. We validate the\nproposed algorithm on several benchmarking MCQA datasets, which exhibits\ncompetitive performances against state-of-the-arts.", "published": "2021-09-27 12:30:39", "link": "http://arxiv.org/abs/2109.12996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Enhanced Span-based Decomposition Method for Few-Shot Sequence\n  Labeling", "abstract": "Few-Shot Sequence Labeling (FSSL) is a canonical paradigm for the tagging\nmodels, e.g., named entity recognition and slot filling, to generalize on an\nemerging, resource-scarce domain. Recently, the metric-based meta-learning\nframework has been recognized as a promising approach for FSSL. However, most\nprior works assign a label to each token based on the token-level similarities,\nwhich ignores the integrality of named entities or slots. To this end, in this\npaper, we propose ESD, an Enhanced Span-based Decomposition method for FSSL.\nESD formulates FSSL as a span-level matching problem between test query and\nsupporting instances. Specifically, ESD decomposes the span matching problem\ninto a series of span-level procedures, mainly including enhanced span\nrepresentation, class prototype aggregation and span conflicts resolution.\nExtensive experiments show that ESD achieves the new state-of-the-art results\non two popular FSSL benchmarks, FewNERD and SNIPS, and is proven to be more\nrobust in the nested and noisy tagging scenarios. Our code is available at\nhttps://github.com/Wangpeiyi9979/ESD.", "published": "2021-09-27 12:59:48", "link": "http://arxiv.org/abs/2109.13023v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Patterns of Lexical Ambiguity in Contextualised Language Models", "abstract": "One of the central aspects of contextualised language models is that they\nshould be able to distinguish the meaning of lexically ambiguous words by their\ncontexts. In this paper we investigate the extent to which the contextualised\nembeddings of word forms that display multiplicity of sense reflect traditional\ndistinctions of polysemy and homonymy. To this end, we introduce an extended,\nhuman-annotated dataset of graded word sense similarity and co-predication\nacceptability, and evaluate how well the similarity of embeddings predicts\nsimilarity in meaning. Both types of human judgements indicate that the\nsimilarity of polysemic interpretations falls in a continuum between identity\nof meaning and homonymy. However, we also observe significant differences\nwithin the similarity ratings of polysemes, forming consistent patterns for\ndifferent types of polysemic sense alternation. Our dataset thus appears to\ncapture a substantial part of the complexity of lexical ambiguity, and can\nprovide a realistic test bed for contextualised embeddings. Among the tested\nmodels, BERT Large shows the strongest correlation with the collected word\nsense similarity ratings, but struggles to consistently replicate the observed\nsimilarity patterns. When clustering ambiguous word forms based on their\nembeddings, the model displays high confidence in discerning homonyms and some\ntypes of polysemic alternations, but consistently fails for others.", "published": "2021-09-27 13:11:44", "link": "http://arxiv.org/abs/2109.13032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Invariant Properties in Natural Language Processing", "abstract": "Meaning is context-dependent, but many properties of language (should) remain\nthe same even if we transform the context. For example, sentiment, entailment,\nor speaker properties should be the same in a translation and original of a\ntext. We introduce language invariant properties: i.e., properties that should\nnot change when we transform text, and how they can be used to quantitatively\nevaluate the robustness of transformation algorithms. We use translation and\nparaphrasing as transformation examples, but our findings apply more broadly to\nany transformation. Our results indicate that many NLP transformations change\nproperties like author characteristics, i.e., make them sound more male. We\nbelieve that studying these properties will allow NLP to address both social\nfactors and pragmatic aspects of language. We also release an application suite\nthat can be used to evaluate the invariance of transformation applications.", "published": "2021-09-27 13:23:05", "link": "http://arxiv.org/abs/2109.13037v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task and Multi-Corpora Training Strategies to Enhance\n  Argumentative Sentence Linking Performance", "abstract": "Argumentative structure prediction aims to establish links between textual\nunits and label the relationship between them, forming a structured\nrepresentation for a given input text. The former task, linking, has been\nidentified by earlier works as particularly challenging, as it requires finding\nthe most appropriate structure out of a very large search space of possible\nlink combinations. In this paper, we improve a state-of-the-art linking model\nby using multi-task and multi-corpora training strategies. Our auxiliary tasks\nhelp the model to learn the role of each sentence in the argumentative\nstructure. Combining multi-corpora training with a selective sampling strategy\nincreases the training data size while ensuring that the model still learns the\ndesired target distribution well. Experiments on essays written by\nEnglish-as-a-foreign-language learners show that both strategies significantly\nimprove the model's performance; for instance, we observe a 15.8% increase in\nthe F1-macro for individual link predictions.", "published": "2021-09-27 14:17:40", "link": "http://arxiv.org/abs/2109.13067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Neural Dialogue Summarization with Personal Named Entity\n  Planning", "abstract": "In this paper, we propose a controllable neural generation framework that can\nflexibly guide dialogue summarization with personal named entity planning. The\nconditional sequences are modulated to decide what types of information or what\nperspective to focus on when forming summaries to tackle the under-constrained\nproblem in summarization tasks. This framework supports two types of use cases:\n(1) Comprehensive Perspective, which is a general-purpose case with no\nuser-preference specified, considering summary points from all conversational\ninterlocutors and all mentioned persons; (2) Focus Perspective, positioning the\nsummary based on a user-specified personal named entity, which could be one of\nthe interlocutors or one of the persons mentioned in the conversation. During\ntraining, we exploit occurrence planning of personal named entities and\ncoreference information to improve temporal coherence and to minimize\nhallucination in neural generation. Experimental results show that our proposed\nframework generates fluent and factually consistent summaries under various\nplanning controls using both objective metrics and human evaluations.", "published": "2021-09-27 14:19:32", "link": "http://arxiv.org/abs/2109.13070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Reinforcement Learning for Pivot-based Neural Machine\n  Translation with Non-autoregressive Transformer", "abstract": "Pivot-based neural machine translation (NMT) is commonly used in low-resource\nsetups, especially for translation between non-English language pairs. It\nbenefits from using high resource source-pivot and pivot-target language pairs\nand an individual system is trained for both sub-tasks. However, these models\nhave no connection during training, and the source-pivot model is not optimized\nto produce the best translation for the source-target task. In this work, we\npropose to train a pivot-based NMT system with the reinforcement learning (RL)\napproach, which has been investigated for various text generation tasks,\nincluding machine translation (MT). We utilize a non-autoregressive transformer\nand present an end-to-end pivot-based integrated model, enabling training on\nsource-target data.", "published": "2021-09-27 14:49:35", "link": "http://arxiv.org/abs/2109.13097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does referent predictability affect the choice of referential form? A\n  computational approach using masked coreference resolution", "abstract": "It is often posited that more predictable parts of a speaker's meaning tend\nto be made less explicit, for instance using shorter, less informative words.\nStudying these dynamics in the domain of referring expressions has proven\ndifficult, with existing studies, both psycholinguistic and corpus-based,\nproviding contradictory results. We test the hypothesis that speakers produce\nless informative referring expressions (e.g., pronouns vs. full noun phrases)\nwhen the context is more informative about the referent, using novel\ncomputational estimates of referent predictability. We obtain these estimates\ntraining an existing coreference resolution system for English on a new task,\nmasked coreference resolution, giving us a probability distribution over\nreferents that is conditioned on the context but not the referring expression.\nThe resulting system retains standard coreference resolution performance while\nyielding a better estimate of human-derived referent predictability than\nprevious attempts. A statistical analysis of the relationship between model\noutput and mention form supports the hypothesis that predictability affects the\nform of a mention, both its morphosyntactic type and its length.", "published": "2021-09-27 14:54:46", "link": "http://arxiv.org/abs/2109.13105v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of Word Problems for Academic Education via Natural\n  Language Processing (NLP)", "abstract": "Digital learning platforms enable students to learn on a flexible and\nindividual schedule as well as providing instant feedback mechanisms. The field\nof STEM education requires students to solve numerous training exercises to\ngrasp underlying concepts. It is apparent that there are restrictions in\ncurrent online education in terms of exercise diversity and individuality. Many\nexercises show little variance in structure and content, hindering the adoption\nof abstraction capabilities by students. This thesis proposes an approach to\ngenerate diverse, context rich word problems. In addition to requiring the\ngenerated language to be grammatically correct, the nature of word problems\nimplies additional constraints on the validity of contents. The proposed\napproach is proven to be effective in generating valid word problems for\nmathematical statistics. The experimental results present a tradeoff between\ngeneration time and exercise validity. The system can easily be parametrized to\nhandle this tradeoff according to the requirements of specific use cases.", "published": "2021-09-27 15:36:19", "link": "http://arxiv.org/abs/2109.13123v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Aware Neural Networks for Medical Forum Question\n  Classification", "abstract": "Online medical forums have become a predominant platform for answering\nhealth-related information needs of consumers. However, with a significant rise\nin the number of queries and the limited availability of experts, it is\nnecessary to automatically classify medical queries based on a consumer's\nintention, so that these questions may be directed to the right set of medical\nexperts. Here, we develop a novel medical knowledge-aware BERT-based model\n(MedBERT) that explicitly gives more weightage to medical concept-bearing\nwords, and utilize domain-specific side information obtained from a popular\nmedical knowledge base. We also contribute a multi-label dataset for the\nMedical Forum Question Classification (MFQC) task. MedBERT achieves\nstate-of-the-art performance on two benchmark datasets and performs very well\nin low resource settings.", "published": "2021-09-27 15:57:21", "link": "http://arxiv.org/abs/2109.13141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TURINGBENCH: A Benchmark Environment for Turing Test in the Age of\n  Neural Text Generation", "abstract": "Recent progress in generative language models has enabled machines to\ngenerate astonishingly realistic texts. While there are many legitimate\napplications of such models, there is also a rising need to distinguish\nmachine-generated texts from human-written ones (e.g., fake news detection).\nHowever, to our best knowledge, there is currently no benchmark environment\nwith datasets and tasks to systematically study the so-called \"Turing Test\"\nproblem for neural text generation methods. In this work, we present the\nTuringBench benchmark environment, which is comprised of (1) a dataset with\n200K human- or machine-generated samples across 20 labels {Human, GPT-1,\nGPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3,\nGROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large,\nFAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two\nbenchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and\n(3) a website with leaderboards. Our preliminary experimental results using\nTuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all\nlanguage models tested, in generating the most human-like indistinguishable\ntexts with the lowest F1 score by five state-of-the-art TT detection models.\nThe TuringBench is available at: https://turingbench.ist.psu.edu/", "published": "2021-09-27 18:35:33", "link": "http://arxiv.org/abs/2109.13296v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Isotropy Calibration of Transformers", "abstract": "Different studies of the embedding space of transformer models suggest that\nthe distribution of contextual representations is highly anisotropic - the\nembeddings are distributed in a narrow cone. Meanwhile, static word\nrepresentations (e.g., Word2Vec or GloVe) have been shown to benefit from\nisotropic spaces. Therefore, previous work has developed methods to calibrate\nthe embedding space of transformers in order to ensure isotropy. However, a\nrecent study (Cai et al. 2021) shows that the embedding space of transformers\nis locally isotropic, which suggests that these models are already capable of\nexploiting the expressive capacity of their embedding space. In this work, we\nconduct an empirical evaluation of state-of-the-art methods for isotropy\ncalibration on transformers and find that they do not provide consistent\nimprovements across models and tasks. These results support the thesis that,\ngiven the local isotropy, transformers do not benefit from additional isotropy\ncalibration.", "published": "2021-09-27 18:54:10", "link": "http://arxiv.org/abs/2109.13304v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COVID-19 India Dataset: Parsing COVID-19 Data in Daily Health Bulletins\n  from States in India", "abstract": "While India has been one of the hotspots of COVID-19, data about the pandemic\nfrom the country has proved to be largely inaccessible at scale. Much of the\ndata exists in unstructured form on the web, and limited aspects of such data\nare available through public APIs maintained manually through volunteer effort.\nThis has proved to be difficult both in terms of ease of access to detailed\ndata and with regards to the maintenance of manual data-keeping over time. This\npaper reports on our effort at automating the extraction of such data from\npublic health bulletins with the help of a combination of classical PDF parsers\nand state-of-the-art machine learning techniques. In this paper, we will\ndescribe the automated data-extraction technique, the nature of the generated\ndata, and exciting avenues of ongoing work.", "published": "2021-09-27 16:05:03", "link": "http://arxiv.org/abs/2110.02311v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural\n  Language Understanding", "abstract": "The few-shot natural language understanding (NLU) task has attracted much\nrecent attention. However, prior methods have been evaluated under a disparate\nset of protocols, which hinders fair comparison and measuring progress of the\nfield. To address this issue, we introduce an evaluation framework that\nimproves previous evaluation procedures in three key aspects, i.e., test\nperformance, dev-test correlation, and stability. Under this new evaluation\nframework, we re-evaluate several state-of-the-art few-shot methods for NLU\ntasks. Our framework reveals new insights: (1) both the absolute performance\nand relative gap of the methods were not accurately estimated in prior\nliterature; (2) no single method dominates most tasks with consistent\nperformance; (3) improvements of some methods diminish with a larger pretrained\nmodel; and (4) gains from different methods are often complementary and the\nbest combined model performs close to a strong fully-supervised baseline. We\nopen-source our toolkit, FewNLU, that implements our evaluation framework along\nwith a number of state-of-the-art methods.", "published": "2021-09-27 00:57:30", "link": "http://arxiv.org/abs/2109.12742v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text to Insight: Accelerating Organic Materials Knowledge Extraction via\n  Deep Learning", "abstract": "Scientific literature is one of the most significant resources for sharing\nknowledge. Researchers turn to scientific literature as a first step in\ndesigning an experiment. Given the extensive and growing volume of literature,\nthe common approach of reading and manually extracting knowledge is too time\nconsuming, creating a bottleneck in the research cycle. This challenge spans\nnearly every scientific domain. For the materials science, experimental data\ndistributed across millions of publications are extremely helpful for\npredicting materials properties and the design of novel materials. However,\nonly recently researchers have explored computational approaches for knowledge\nextraction primarily for inorganic materials. This study aims to explore\nknowledge extraction for organic materials. We built a research dataset\ncomposed of 855 annotated and 708,376 unannotated sentences drawn from 92,667\nabstracts. We used named-entity-recognition (NER) with BiLSTM-CNN-CRF deep\nlearning model to automatically extract key knowledge from literature.\nEarly-phase results show a high potential for automated knowledge extraction.\nThe paper presents our findings and a framework for supervised knowledge\nextraction that can be adapted to other scientific domains.", "published": "2021-09-27 01:58:35", "link": "http://arxiv.org/abs/2109.12758v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ReINTEL Challenge 2020: A Comparative Study of Hybrid Deep Neural\n  Network for Reliable Intelligence Identification on Vietnamese SNSs", "abstract": "The overwhelming abundance of data has created a misinformation crisis.\nUnverified sensationalism that is designed to grab the readers' short attention\nspan, when crafted with malice, has caused irreparable damage to our society's\nstructure. As a result, determining the reliability of an article has become a\ncrucial task. After various ablation studies, we propose a multi-input model\nthat can effectively leverage both tabular metadata and post content for the\ntask. Applying state-of-the-art finetuning techniques for the pretrained\ncomponent and training strategies for our complete model, we have achieved a\n0.9462 ROC-score on the VLSP private test set.", "published": "2021-09-27 03:40:28", "link": "http://arxiv.org/abs/2109.12777v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Effective Use of Graph Convolution Network and Contextual Sub-Tree\n  forCommodity News Event Extraction", "abstract": "Event extraction in commodity news is a less researched area as compared to\ngeneric event extraction. However, accurate event extraction from commodity\nnews is useful in abroad range of applications such as under-standing event\nchains and learning event-event relations, which can then be used for commodity\nprice prediction. The events found in commodity news exhibit characteristics\ndifferent from generic events, hence posing a unique challenge in event\nextraction using existing methods. This paper proposes an effective use of\nGraph Convolutional Networks(GCN) with a pruned dependency parse tree, termed\ncontextual sub-tree, for better event ex-traction in commodity news. The event\nex-traction model is trained using feature embed-dings from ComBERT, a\nBERT-based masked language model that was produced through domain-adaptive\npre-training on a commodity news corpus. Experimental results show the\nefficiency of the proposed solution, which out-performs existing methods with\nF1 scores as high as 0.90. Furthermore, our pre-trained language model\noutperforms GloVe by 23%, and BERT and RoBERTa by 7% in terms of argument roles\nclassification. For the goal of re-producibility, the code and trained models\nare made publicly available1.", "published": "2021-09-27 03:57:17", "link": "http://arxiv.org/abs/2109.12781v1", "categories": ["cs.CL", "cs.AI", "I.7; H.4; H.5"], "primary_category": "cs.CL"}
{"title": "Multiplicative Position-aware Transformer Models for Language\n  Understanding", "abstract": "Transformer models, which leverage architectural improvements like\nself-attention, perform remarkably well on Natural Language Processing (NLP)\ntasks. The self-attention mechanism is position agnostic. In order to capture\npositional ordering information, various flavors of absolute and relative\nposition embeddings have been proposed. However, there is no systematic\nanalysis on their contributions and a comprehensive comparison of these methods\nis missing in the literature. In this paper, we review major existing position\nembedding methods and compare their accuracy on downstream NLP tasks, using our\nown implementations. We also propose a novel multiplicative embedding method\nwhich leads to superior accuracy when compared to existing methods. Finally, we\nshow that our proposed embedding method, served as a drop-in replacement of the\ndefault absolute position embedding, can improve the RoBERTa-base and\nRoBERTa-large models on SQuAD1.1 and SQuAD2.0 datasets.", "published": "2021-09-27 04:18:32", "link": "http://arxiv.org/abs/2109.12788v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Challenging the Semi-Supervised VAE Framework for Text Classification", "abstract": "Semi-Supervised Variational Autoencoders (SSVAEs) are widely used models for\ndata efficient learning. In this paper, we question the adequacy of the\nstandard design of sequence SSVAEs for the task of text classification as we\nexhibit two sources of overcomplexity for which we provide simplifications.\nThese simplifications to SSVAEs preserve their theoretical soundness while\nproviding a number of practical advantages in the semi-supervised setup where\nthe result of training is a text classifier. These simplifications are the\nremoval of (i) the Kullback-Liebler divergence from its objective and (ii) the\nfully unobserved latent variable from its probabilistic model. These changes\nrelieve users from choosing a prior for their latent variables, make the model\nsmaller and faster, and allow for a better flow of information into the latent\nvariables. We compare the simplified versions to standard SSVAEs on 4 text\nclassification tasks. On top of the above-mentioned simplification, experiments\nshow a speed-up of 26%, while keeping equivalent classification scores. The\ncode to reproduce our experiments is public.", "published": "2021-09-27 11:46:32", "link": "http://arxiv.org/abs/2109.12969v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Every time I fire a conversational designer, the performance of the\n  dialog system goes down", "abstract": "Incorporating explicit domain knowledge into neural-based task-oriented\ndialogue systems is an effective way to reduce the need of large sets of\nannotated dialogues. In this paper, we investigate how the use of explicit\ndomain knowledge of conversational designers affects the performance of\nneural-based dialogue systems. To support this investigation, we propose the\nConversational-Logic-Injection-in-Neural-Network system (CLINN) where explicit\nknowledge is coded in semi-logical rules. By using CLINN, we evaluated\nsemi-logical rules produced by a team of differently skilled conversational\ndesigners. We experimented with the Restaurant topic of the MultiWOZ dataset.\nResults show that external knowledge is extremely important for reducing the\nneed of annotated examples for conversational systems. In fact, rules from\nconversational designers used in CLINN significantly outperform a\nstate-of-the-art neural-based dialogue system.", "published": "2021-09-27 13:05:31", "link": "http://arxiv.org/abs/2109.13029v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recall and Learn: A Memory-augmented Solver for Math Word Problems", "abstract": "In this article, we tackle the math word problem, namely, automatically\nanswering a mathematical problem according to its textual description. Although\nrecent methods have demonstrated their promising results, most of these methods\nare based on template-based generation scheme which results in limited\ngeneralization capability. To this end, we propose a novel human-like\nanalogical learning method in a recall and learn manner. Our proposed framework\nis composed of modules of memory, representation, analogy, and reasoning, which\nare designed to make a new exercise by referring to the exercises learned in\nthe past. Specifically, given a math word problem, the model first retrieves\nsimilar questions by a memory module and then encodes the unsolved problem and\neach retrieved question using a representation module. Moreover, to solve the\nproblem in a way of analogy, an analogy module and a reasoning module with a\ncopy mechanism are proposed to model the interrelationship between the problem\nand each retrieved question. Extensive experiments on two well-known datasets\nshow the superiority of our proposed algorithm as compared to other\nstate-of-the-art competitors from both overall performance comparison and\nmicro-scope studies.", "published": "2021-09-27 14:59:08", "link": "http://arxiv.org/abs/2109.13112v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual\n  Question Answering", "abstract": "We present VQA-MHUG - a novel 49-participant dataset of multimodal human gaze\non both images and questions during visual question answering (VQA) collected\nusing a high-speed eye tracker. We use our dataset to analyze the similarity\nbetween human and neural attentive strategies learned by five state-of-the-art\nVQA models: Modular Co-Attention Network (MCAN) with either grid or region\nfeatures, Pythia, Bilinear Attention Network (BAN), and the Multimodal\nFactorized Bilinear Pooling Network (MFB). While prior work has focused on\nstudying the image modality, our analyses show - for the first time - that for\nall models, higher correlation with human attention on text is a significant\npredictor of VQA performance. This finding points at a potential for improving\nVQA performance and, at the same time, calls for further research on neural\ntext attention mechanisms and their integration into architectures for vision\nand language tasks, including but potentially also beyond VQA.", "published": "2021-09-27 15:06:10", "link": "http://arxiv.org/abs/2109.13116v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mitigating Racial Biases in Toxic Language Detection with an\n  Equity-Based Ensemble Framework", "abstract": "Recent research has demonstrated how racial biases against users who write\nAfrican American English exists in popular toxic language datasets. While\nprevious work has focused on a single fairness criteria, we propose to use\nadditional descriptive fairness metrics to better understand the source of\nthese biases. We demonstrate that different benchmark classifiers, as well as\ntwo in-process bias-remediation techniques, propagate racial biases even in a\nlarger corpus. We then propose a novel ensemble-framework that uses a\nspecialized classifier that is fine-tuned to the African American English\ndialect. We show that our proposed framework substantially reduces the racial\nbiases that the model learns from these datasets. We demonstrate how the\nensemble framework improves fairness metrics across all sample datasets with\nminimal impact on the classification performance, and provide empirical\nevidence in its ability to unlearn the annotation biases towards authors who\nuse African American English.\n  ** Please note that this work may contain examples of offensive words and\nphrases.", "published": "2021-09-27 15:54:05", "link": "http://arxiv.org/abs/2109.13137v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Integration of Human-Like Attention in Visual Question\n  Answering", "abstract": "Human-like attention as a supervisory signal to guide neural attention has\nshown significant promise but is currently limited to uni-modal integration -\neven for inherently multimodal tasks such as visual question answering (VQA).\nWe present the Multimodal Human-like Attention Network (MULAN) - the first\nmethod for multimodal integration of human-like attention on image and text\nduring training of VQA models. MULAN integrates attention predictions from two\nstate-of-the-art text and image saliency models into neural self-attention\nlayers of a recent transformer-based VQA model. Through evaluations on the\nchallenging VQAv2 dataset, we show that MULAN achieves a new state-of-the-art\nperformance of 73.98% accuracy on test-std and 73.72% on test-dev and, at the\nsame time, has approximately 80% fewer trainable parameters than prior work.\nOverall, our work underlines the potential of integrating multimodal human-like\nand neural attention for VQA", "published": "2021-09-27 15:56:54", "link": "http://arxiv.org/abs/2109.13139v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Discovering Drug-Target Interaction Knowledge from Biomedical Literature", "abstract": "The Interaction between Drugs and Targets (DTI) in human body plays a crucial\nrole in biomedical science and applications. As millions of papers come out\nevery year in the biomedical domain, automatically discovering DTI knowledge\nfrom biomedical literature, which are usually triplets about drugs, targets and\ntheir interaction, becomes an urgent demand in the industry. Existing methods\nof discovering biological knowledge are mainly extractive approaches that often\nrequire detailed annotations (e.g., all mentions of biological entities,\nrelations between every two entity mentions, etc.). However, it is difficult\nand costly to obtain sufficient annotations due to the requirement of expert\nknowledge from biomedical domains. To overcome these difficulties, we explore\nthe first end-to-end solution for this task by using generative approaches. We\nregard the DTI triplets as a sequence and use a Transformer-based model to\ndirectly generate them without using the detailed annotations of entities and\nrelations. Further, we propose a semi-supervised method, which leverages the\naforementioned end-to-end model to filter unlabeled literature and label them.\nExperimental results show that our method significantly outperforms extractive\nbaselines on DTI discovery. We also create a dataset, KD-DTI, to advance this\ntask and will release it to the community.", "published": "2021-09-27 17:00:14", "link": "http://arxiv.org/abs/2109.13187v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Pause Information for More Accurate Entity Recognition", "abstract": "Entity tags in human-machine dialog are integral to natural language\nunderstanding (NLU) tasks in conversational assistants. However, current\nsystems struggle to accurately parse spoken queries with the typical use of\ntext input alone, and often fail to understand the user intent. Previous work\nin linguistics has identified a cross-language tendency for longer speech\npauses surrounding nouns as compared to verbs. We demonstrate that the\nlinguistic observation on pauses can be used to improve accuracy in\nmachine-learnt language understanding tasks. Analysis of pauses in French and\nEnglish utterances from a commercial voice assistant shows the statistically\nsignificant difference in pause duration around multi-token entity span\nboundaries compared to within entity spans. Additionally, in contrast to\ntext-based NLU, we apply pause duration to enrich contextual embeddings to\nimprove shallow parsing of entities. Results show that our proposed novel\nembeddings improve the relative error rate by up to 8% consistently across\nthree domains for French, without any added annotation or alignment costs to\nthe parser.", "published": "2021-09-27 17:47:21", "link": "http://arxiv.org/abs/2109.13222v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Factorized Neural Transducer for Efficient Language Model Adaptation", "abstract": "In recent years, end-to-end (E2E) based automatic speech recognition (ASR)\nsystems have achieved great success due to their simplicity and promising\nperformance. Neural Transducer based models are increasingly popular in\nstreaming E2E based ASR systems and have been reported to outperform the\ntraditional hybrid system in some scenarios. However, the joint optimization of\nacoustic model, lexicon and language model in neural Transducer also brings\nabout challenges to utilize pure text for language model adaptation. This\ndrawback might prevent their potential applications in practice. In order to\naddress this issue, in this paper, we propose a novel model, factorized neural\nTransducer, by factorizing the blank and vocabulary prediction, and adopting a\nstandalone language model for the vocabulary prediction. It is expected that\nthis factorization can transfer the improvement of the standalone language\nmodel to the Transducer for speech recognition, which allows various language\nmodel adaptation techniques to be applied. We demonstrate that the proposed\nfactorized neural Transducer yields 15% to 20% WER improvements when\nout-of-domain text data is used for language model adaptation, at the cost of a\nminor degradation in WER on a general test set.", "published": "2021-09-27 15:04:00", "link": "http://arxiv.org/abs/2110.01500v5", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fast-MD: Fast Multi-Decoder End-to-End Speech Translation with\n  Non-Autoregressive Hidden Intermediates", "abstract": "The multi-decoder (MD) end-to-end speech translation model has demonstrated\nhigh translation quality by searching for better intermediate automatic speech\nrecognition (ASR) decoder states as hidden intermediates (HI). It is a two-pass\ndecoding model decomposing the overall task into ASR and machine translation\nsub-tasks. However, the decoding speed is not fast enough for real-world\napplications because it conducts beam search for both sub-tasks during\ninference. We propose Fast-MD, a fast MD model that generates HI by\nnon-autoregressive (NAR) decoding based on connectionist temporal\nclassification (CTC) outputs followed by an ASR decoder. We investigated two\ntypes of NAR HI: (1) parallel HI by using an autoregressive Transformer ASR\ndecoder and (2) masked HI by using Mask-CTC, which combines CTC and the\nconditional masked language model. To reduce a mismatch in the ASR decoder\nbetween teacher-forcing during training and conditioning on CTC outputs during\ntesting, we also propose sampling CTC outputs during training. Experimental\nevaluations on three corpora show that Fast-MD achieved about 2x and 4x faster\ndecoding speed than that of the na\\\"ive MD model on GPU and CPU with comparable\ntranslation quality. Adopting the Conformer encoder and intermediate CTC loss\nfurther boosts its quality without sacrificing decoding speed.", "published": "2021-09-27 05:21:30", "link": "http://arxiv.org/abs/2109.12804v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Classifying Dyads for Militarized Conflict Analysis", "abstract": "Understanding the origins of militarized conflict is a complex, yet important\nundertaking. Existing research seeks to build this understanding by considering\nbi-lateral relationships between entity pairs (dyadic causes) and multi-lateral\nrelationships among multiple entities (systemic causes). The aim of this work\nis to compare these two causes in terms of how they correlate with conflict\nbetween two entities. We do this by devising a set of textual and graph-based\nfeatures which represent each of the causes. The features are extracted from\nWikipedia and modeled as a large graph. Nodes in this graph represent entities\nconnected by labeled edges representing ally or enemy-relationships. This\nallows casting the problem as an edge classification task, which we term dyad\nclassification. We propose and evaluate classifiers to determine if a\nparticular pair of entities are allies or enemies. Our results suggest that our\nsystemic features might be slightly better correlates of conflict. Further, we\nfind that Wikipedia articles of allies are semantically more similar than\nenemies.", "published": "2021-09-27 08:19:41", "link": "http://arxiv.org/abs/2109.12860v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "PicTalky: Augmentative and Alternative Communication Software for\n  Language Developmental Disabilities", "abstract": "Augmentative and alternative communication (AAC) is a practical means of\ncommunication for people with language disabilities. In this study, we propose\nPicTalky, which is an AI-based AAC system that helps children with language\ndevelopmental disabilities to improve their communication skills and language\ncomprehension abilities. PicTalky can process both text and pictograms more\naccurately by connecting a series of neural-based NLP modules. Moreover, we\nperform quantitative and qualitative analyses on the essential features of\nPicTalky. It is expected that those suffering from language problems will be\nable to express their intentions or desires more easily and improve their\nquality of life by using this service. We have made the models freely available\nalongside a demonstration of the Web interface. Furthermore, we implemented\nrobotics AAC for the first time by applying PicTalky to the NAO robot.", "published": "2021-09-27 10:46:14", "link": "http://arxiv.org/abs/2109.12941v2", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Understanding and Overcoming the Challenges of Efficient Transformer\n  Quantization", "abstract": "Transformer-based architectures have become the de-facto standard models for\na wide range of Natural Language Processing tasks. However, their memory\nfootprint and high latency are prohibitive for efficient deployment and\ninference on resource-limited devices. In this work, we explore quantization\nfor transformers. We show that transformers have unique quantization challenges\n-- namely, high dynamic activation ranges that are difficult to represent with\na low bit fixed-point format. We establish that these activations contain\nstructured outliers in the residual connections that encourage specific\nattention patterns, such as attending to the special separator token. To combat\nthese challenges, we present three solutions based on post-training\nquantization and quantization-aware training, each with a different set of\ncompromises for accuracy, model size, and ease of use. In particular, we\nintroduce a novel quantization scheme -- per-embedding-group quantization. We\ndemonstrate the effectiveness of our methods on the GLUE benchmark using BERT,\nestablishing state-of-the-art results for post-training quantization. Finally,\nwe show that transformer weights and embeddings can be quantized to ultra-low\nbit-widths, leading to significant memory savings with a minimum accuracy loss.\nOur source code is available\nat~\\url{https://github.com/qualcomm-ai-research/transformer-quantization}.", "published": "2021-09-27 10:57:18", "link": "http://arxiv.org/abs/2109.12948v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Spread of Propaganda by Coordinated Communities on Social Media", "abstract": "Large-scale manipulations on social media have two important characteristics:\n(i) use of propaganda to influence others, and (ii) adoption of coordinated\nbehavior to spread it and to amplify its impact. Despite the connection between\nthem, these two characteristics have so far been considered in isolation. Here\nwe aim to bridge this gap. In particular, we analyze the spread of propaganda\nand its interplay with coordinated behavior on a large Twitter dataset about\nthe 2019 UK general election. We first propose and evaluate several metrics for\nmeasuring the use of propaganda on Twitter. Then, we investigate the use of\npropaganda by different coordinated communities that participated in the online\ndebate. The combination of the use of propaganda and coordinated behavior\nallows us to uncover the authenticity and harmfulness of the different\ncommunities. Finally, we compare our measures of propaganda and coordination\nwith automation (i.e., bot) scores and Twitter suspensions, revealing\ninteresting trends. From a theoretical viewpoint, we introduce a methodology\nfor analyzing several important dimensions of online behavior that are seldom\nconjointly considered. From a practical viewpoint, we provide new insights into\nauthentic and inauthentic online activities during the 2019 UK general\nelection.", "published": "2021-09-27 13:39:10", "link": "http://arxiv.org/abs/2109.13046v3", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Trans-Encoder: Unsupervised sentence-pair modelling through self- and\n  mutual-distillations", "abstract": "In NLP, a large volume of tasks involve pairwise comparison between two\nsequences (e.g. sentence similarity and paraphrase identification).\nPredominantly, two formulations are used for sentence-pair tasks: bi-encoders\nand cross-encoders. Bi-encoders produce fixed-dimensional sentence\nrepresentations and are computationally efficient, however, they usually\nunderperform cross-encoders. Cross-encoders can leverage their attention heads\nto exploit inter-sentence interactions for better performance but they require\ntask fine-tuning and are computationally more expensive. In this paper, we\npresent a completely unsupervised sentence representation model termed as\nTrans-Encoder that combines the two learning paradigms into an iterative joint\nframework to simultaneously learn enhanced bi- and cross-encoders.\nSpecifically, on top of a pre-trained Language Model (PLM), we start with\nconverting it to an unsupervised bi-encoder, and then alternate between the bi-\nand cross-encoder task formulations. In each alternation, one task formulation\nwill produce pseudo-labels which are used as learning signals for the other\ntask formulation. We then propose an extension to conduct such\nself-distillation approach on multiple PLMs in parallel and use the average of\ntheir pseudo-labels for mutual-distillation. Trans-Encoder creates, to the best\nof our knowledge, the first completely unsupervised cross-encoder and also a\nstate-of-the-art unsupervised bi-encoder for sentence similarity. Both the\nbi-encoder and cross-encoder formulations of Trans-Encoder outperform recently\nproposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT\nand SimCSE by up to 5% on the sentence similarity benchmarks.", "published": "2021-09-27 14:06:47", "link": "http://arxiv.org/abs/2109.13059v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Stack Overflow question title generation with copying enhanced\n  CodeBERT model and bi-modal information", "abstract": "Context: Stack Overflow is very helpful for software developers who are\nseeking answers to programming problems. Previous studies have shown that a\ngrowing number of questions are of low quality and thus obtain less attention\nfrom potential answerers. Gao et al. proposed an LSTM-based model (i.e.,\nBiLSTM-CC) to automatically generate question titles from the code snippets to\nimprove the question quality. However, only using the code snippets in the\nquestion body cannot provide sufficient information for title generation, and\nLSTMs cannot capture the long-range dependencies between tokens. Objective:\nThis paper proposes CCBERT, a deep learning based novel model to enhance the\nperformance of question title generation by making full use of the bi-modal\ninformation of the entire question body. Method: CCBERT follows the\nencoder-decoder paradigm and uses CodeBERT to encode the question body into\nhidden representations, a stacked Transformer decoder to generate predicted\ntokens, and an additional copy attention layer to refine the output\ndistribution. Both the encoder and decoder perform the multi-head\nself-attention operation to better capture the long-range dependencies. This\npaper builds a dataset containing around 200,000 high-quality questions\nfiltered from the data officially published by Stack Overflow to verify the\neffectiveness of the CCBERT model. Results: CCBERT outperforms all the baseline\nmodels on the dataset. Experiments on both code-only and low-resource datasets\nshow the superiority of CCBERT with less performance degradation. The human\nevaluation also shows the excellent performance of CCBERT concerning both\nreadability and correlation criteria.", "published": "2021-09-27 14:23:13", "link": "http://arxiv.org/abs/2109.13073v2", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "FQuAD2.0: French Question Answering and knowing that you know nothing", "abstract": "Question Answering, including Reading Comprehension, is one of the NLP\nresearch areas that has seen significant scientific breakthroughs over the past\nfew years, thanks to the concomitant advances in Language Modeling. Most of\nthese breakthroughs, however, are centered on the English language. In 2020, as\na first strong initiative to bridge the gap to the French language, Illuin\nTechnology introduced FQuAD1.1, a French Native Reading Comprehension dataset\ncomposed of 60,000+ questions and answers samples extracted from Wikipedia\narticles. Nonetheless, Question Answering models trained on this dataset have a\nmajor drawback: they are not able to predict when a given question has no\nanswer in the paragraph of interest, therefore making unreliable predictions in\nvarious industrial use-cases. In the present work, we introduce FQuAD2.0, which\nextends FQuAD with 17,000+ unanswerable questions, annotated adversarially, in\norder to be similar to answerable ones. This new dataset, comprising a total of\nalmost 80,000 questions, makes it possible to train French Question Answering\nmodels with the ability of distinguishing unanswerable questions from\nanswerable ones. We benchmark several models with this dataset: our best model,\na fine-tuned CamemBERT-large, achieves a F1 score of 82.3% on this\nclassification task, and a F1 score of 83% on the Reading Comprehension task.", "published": "2021-09-27 17:30:46", "link": "http://arxiv.org/abs/2109.13209v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Challenges and Opportunities of Speech Recognition for Bengali Language", "abstract": "Speech recognition is a fascinating process that offers the opportunity to\ninteract and command the machine in the field of human-computer interactions.\nSpeech recognition is a language-dependent system constructed directly based on\nthe linguistic and textual properties of any language. Automatic Speech\nRecognition (ASR) systems are currently being used to translate speech to text\nflawlessly. Although ASR systems are being strongly executed in international\nlanguages, ASR systems' implementation in the Bengali language has not reached\nan acceptable state. In this research work, we sedulously disclose the current\nstatus of the Bengali ASR system's research endeavors. In what follows, we\nacquaint the challenges that are mostly encountered while constructing a\nBengali ASR system. We split the challenges into language-dependent and\nlanguage-independent challenges and guide how the particular complications may\nbe overhauled. Following a rigorous investigation and highlighting the\nchallenges, we conclude that Bengali ASR systems require specific construction\nof ASR architectures based on the Bengali language's grammatical and phonetic\nstructure.", "published": "2021-09-27 17:38:26", "link": "http://arxiv.org/abs/2109.13217v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning\n  for Automatic Speech Recognition", "abstract": "We summarize the results of a host of efforts using giant automatic speech\nrecognition (ASR) models pre-trained using large, diverse unlabeled datasets\ncontaining approximately a million hours of audio. We find that the combination\nof pre-training, self-training and scaling up model size greatly increases data\nefficiency, even for extremely large tasks with tens of thousands of hours of\nlabeled data. In particular, on an ASR task with 34k hours of labeled data, by\nfine-tuning an 8 billion parameter pre-trained Conformer model we can match\nstate-of-the-art (SoTA) performance with only 3% of the training data and\nsignificantly improve SoTA with the full training set. We also report on the\nuniversal benefits gained from using big pre-trained and self-trained models\nfor a large set of downstream tasks that cover a wide range of speech domains\nand span multiple orders of magnitudes of dataset sizes, including obtaining\nSoTA performance on many public benchmarks. In addition, we utilize the learned\nrepresentation of pre-trained networks to achieve SoTA results on non-ASR\ntasks.", "published": "2021-09-27 17:59:19", "link": "http://arxiv.org/abs/2109.13226v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analyzing the Use of Character-Level Translation with Sparse and Noisy\n  Datasets", "abstract": "This paper provides an analysis of character-level machine translation models\nused in pivot-based translation when applied to sparse and noisy datasets, such\nas crowdsourced movie subtitles. In our experiments, we find that such\ncharacter-level models cut the number of untranslated words by over 40% and are\nespecially competitive (improvements of 2-3 BLEU points) in the case of limited\ntraining data. We explore the impact of character alignment, phrase table\nfiltering, bitext size and the choice of pivot language on translation quality.\nWe further compare cascaded translation models to the use of synthetic training\ndata via multiple pivots, and we find that the latter works significantly\nbetter. Finally, we demonstrate that neither word-nor character-BLEU correlate\nperfectly with human judgments, due to BLEU's sensitivity to length.", "published": "2021-09-27 07:35:47", "link": "http://arxiv.org/abs/2109.13723v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Translating from Morphologically Complex Languages: A Paraphrase-Based\n  Approach", "abstract": "We propose a novel approach to translating from a morphologically complex\nlanguage. Unlike previous research, which has targeted word inflections and\nconcatenations, we focus on the pairwise relationship between morphologically\nrelated words, which we treat as potential paraphrases and handle using\nparaphrasing techniques at the word, phrase, and sentence level. An important\nadvantage of this framework is that it can cope with derivational morphology,\nwhich has so far remained largely beyond the capabilities of statistical\nmachine translation systems. Our experiments translating from Malay, whose\nmorphology is mostly derivational, into English show significant improvements\nover rivaling approaches based on five automatic evaluation measures (for\n320,000 sentence pairs; 9.5 million English word tokens).", "published": "2021-09-27 07:02:19", "link": "http://arxiv.org/abs/2109.13724v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Evaluation of Non-Negative Matrix Factorization and n-stage Latent\n  Dirichlet Allocation for Emotion Analysis in Turkish Tweets", "abstract": "With the development of technology, the use of social media has become quite\ncommon. Analyzing comments on social media in areas such as media and\nadvertising plays an important role today. For this reason, new and traditional\nnatural language processing methods are used to detect the emotion of these\nshares. In this paper, the Latent Dirichlet Allocation, namely LDA, and\nNon-Negative Matrix Factorization methods in topic modeling were used to\ndetermine which emotion the Turkish tweets posted via Twitter. In addition, the\naccuracy of a proposed n-level method based on LDA was analyzed. Dataset\nconsists of 5 emotions, namely angry, fear, happy, sad and confused. NMF was\nthe most successful method among all topic modeling methods in this study.\nThen, the F1-measure of Random Forest, Naive Bayes and Support Vector Machine\nmethods was analyzed by obtaining a file suitable for Weka by using the word\nweights and class labels of the topics. Among the Weka results, the most\nsuccessful method was n-stage LDA, and the most successful algorithm was Random\nForest.", "published": "2021-09-27 18:43:52", "link": "http://arxiv.org/abs/2110.00418v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "H.3.3; I.2.7; I.7.0"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis in Twitter for Macedonian", "abstract": "We present work on sentiment analysis in Twitter for Macedonian. As this is\npioneering work for this combination of language and genre, we created suitable\nresources for training and evaluating a system for sentiment analysis of\nMacedonian tweets. In particular, we developed a corpus of tweets annotated\nwith tweet-level sentiment polarity (positive, negative, and neutral), as well\nas with phrase-level sentiment, which we made freely available for research\npurposes. We further bootstrapped several large-scale sentiment lexicons for\nMacedonian, motivated by previous work for English. The impact of several\ndifferent pre-processing steps as well as of various features is shown in\nexperiments that represent the first attempt to build a system for sentiment\nanalysis in Twitter for the morphologically rich Macedonian language. Overall,\nour experimental results show an F1-score of 92.16, which is very strong and is\non par with the best results for English, which were achieved in recent SemEval\ncompetitions.", "published": "2021-09-27 06:53:02", "link": "http://arxiv.org/abs/2109.13725v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.SI", "math.IT", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Emotional Speech Synthesis for Companion Robot to Imitate Professional\n  Caregiver Speech", "abstract": "When people try to influence others to do something, they subconsciously\nadjust their speech to include appropriate emotional information. In order for\na robot to influence people in the same way, the robot should be able to\nimitate the range of human emotions when speaking. To achieve this, we propose\na speech synthesis method for imitating the emotional states in human speech.\nIn contrast to previous methods, the advantage of our method is that it\nrequires less manual effort to adjust the emotion of the synthesized speech.\nOur synthesizer receives an emotion vector to characterize the emotion of\nsynthesized speech. The vector is automatically obtained from human utterances\nby using a speech emotion recognizer. We evaluated our method in a scenario\nwhen a robot tries to regulate an elderly person's circadian rhythm by speaking\nto the person using appropriate emotional states. For the target speech to\nimitate, we collected utterances from professional caregivers when they speak\nto elderly people at different times of the day. Then we conducted a subjective\nevaluation where the elderly participants listened to the speech samples\ngenerated by our method. The results showed that listening to the samples made\nthe participants feel more active in the early morning and calmer in the middle\nof the night. This suggests that the robot may be able to adjust the\nparticipants' circadian rhythm and that the robot can potentially exert\ninfluence similarly to a person.", "published": "2021-09-27 04:12:53", "link": "http://arxiv.org/abs/2109.12787v1", "categories": ["cs.RO", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Estimating Angle of Arrival (AoA) of multiple Echoes in a Steering\n  Vector Space", "abstract": "Consider a microphone array, such as those present in Amazon Echos,\nconference phones, or self-driving cars. One of the goals of these arrays is to\ndecode the angles in which acoustic signals arrive at them. This paper\nconsiders the problem of estimating K angle of arrivals (AoA), i.e., the direct\npath's AoA and the AoA of subsequent echoes. Significant progress has been made\non this problem, however, solutions remain elusive when the source signal is\nunknown (such as human voice) and the channel is strongly correlated (such as\nin multipath settings). Today's algorithms reliably estimate the\ndirect-path-AoA, but the subsequent AoAs diverge in noisy real-world\nconditions.\n  We design SubAoA, an algorithm that improves on the current body of work. Our\ncore idea models signal in a new AoA sub-space, and employs a cancellation\napproach that successively cancels each AoA to decode the next. We explain the\nbehavior and complexity of the algorithm from the first principles, simulate\nthe performance across a range of parameters, and present results from\nreal-world experiments. Comparison against multiple existing algorithms like\nGCC-PHAT, MUSIC, and VoLoc shows increasing gains for the latter AoAs, while\nour computation complexity allows real-time operation. We believe progress in\nmulti-AoA estimation is a fundamental building block to various acoustic and RF\napplications, including human or vehicle localization, multi-user separation,\nand even (blind) channel estimation.", "published": "2021-09-27 14:21:46", "link": "http://arxiv.org/abs/2109.13072v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Inferring Facing Direction from Voice Signals", "abstract": "Consider a home or office where multiple devices are running voice assistants\n(e.g., TVs, lights, ovens, refrigerators, etc.). A human user turns to a\nparticular device and gives a voice command, such as ``Alexa, can you ...''.\nThis paper focuses on the problem of detecting which device the user was\nfacing, and therefore, enabling only that device to respond to the command. Our\ncore intuition emerges from the fact that human voice exhibits a directional\nradiation pattern, and the orientation of this pattern should influence the\nsignal received at each device. Unfortunately, indoor multipath, unknown user\nlocation, and unknown voice signals pose as critical hurdles. Through a new\nalgorithm that estimates the line-of-sight (LoS) power from a given signal, and\ncombined with beamforming and triangulation, we design a functional solution\ncalled CoDIR. Results from $500+$ configurations, across $5$ rooms and $9$\ndifferent users, are encouraging. While improvements are necessary, we believe\nthis is an important step forward in a challenging but urgent problem space.", "published": "2021-09-27 14:48:47", "link": "http://arxiv.org/abs/2109.13094v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-to-Image Cross-Modal Generation", "abstract": "Cross-modal representation learning allows to integrate information from\ndifferent modalities into one representation. At the same time, research on\ngenerative models tends to focus on the visual domain with less emphasis on\nother domains, such as audio or text, potentially missing the benefits of\nshared representations. Studies successfully linking more than one modality in\nthe generative setting are rare. In this context, we verify the possibility to\ntrain variational autoencoders (VAEs) to reconstruct image archetypes from\naudio data. Specifically, we consider VAEs in an adversarial training framework\nin order to ensure more variability in the generated data and find that there\nis a trade-off between the consistency and diversity of the generated images -\nthis trade-off can be governed by scaling the reconstruction loss up or down,\nrespectively. Our results further suggest that even in the case when the\ngenerated images are relatively inconsistent (diverse), features that are\ncritical for proper image classification are preserved.", "published": "2021-09-27 21:25:31", "link": "http://arxiv.org/abs/2109.13354v1", "categories": ["cs.MM", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "FlowVocoder: A small Footprint Neural Vocoder based Normalizing flow for\n  Speech Synthesis", "abstract": "Recently, autoregressive neural vocoders have provided remarkable performance\nin generating high-fidelity speech and have been able to produce synthetic\nspeech in real-time. However, autoregressive neural vocoders such as WaveFlow\nare capable of modeling waveform signals from mel-spectrogram, its number of\nparameters is significant to deploy on edge devices. Though NanoFlow, which has\na small number of parameters, is a state-of-the-art autoregressive neural\nvocoder, the performance of NanoFlow is marginally lower than WaveFlow.\nTherefore, we propose a new type of autoregressive neural vocoder called\nFlowVocoder, which has a small memory footprint and is capable of generating\nhigh-fidelity audio in real-time. Our proposed model improves the density\nestimation of flow blocks by utilizing a mixture of Cumulative Distribution\nFunctions (CDF) for bipartite transformation. Hence, the proposed model is\ncapable of modeling waveform signals, while its memory footprint is much\nsmaller than WaveFlow. As shown in experiments, FlowVocoder achieves\ncompetitive results with baseline methods in terms of both subjective and\nobjective evaluation, also, it is more suitable for real-time text-to-speech\napplications.", "published": "2021-09-27 06:52:55", "link": "http://arxiv.org/abs/2109.13675v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
