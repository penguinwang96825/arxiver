{"title": "Improved Neural Machine Translation with a Syntax-Aware Encoder and\n  Decoder", "abstract": "Most neural machine translation (NMT) models are based on the sequential\nencoder-decoder framework, which makes no use of syntactic information. In this\npaper, we improve this model by explicitly incorporating source-side syntactic\ntrees. More specifically, we propose (1) a bidirectional tree encoder which\nlearns both sequential and tree structured representations; (2) a tree-coverage\nmodel that lets the attention depend on the source-side syntax. Experiments on\nChinese-English translation demonstrate that our proposed models outperform the\nsequential attentional model as well as a stronger baseline with a bottom-up\ntree encoder and word coverage.", "published": "2017-07-18 01:53:58", "link": "http://arxiv.org/abs/1707.05436v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Top-Rank Enhanced Listwise Optimization for Statistical Machine\n  Translation", "abstract": "Pairwise ranking methods are the basis of many widely used discriminative\ntraining approaches for structure prediction problems in natural language\nprocessing(NLP). Decomposing the problem of ranking hypotheses into pairwise\ncomparisons enables simple and efficient solutions. However, neglecting the\nglobal ordering of the hypothesis list may hinder learning. We propose a\nlistwise learning framework for structure prediction problems such as machine\ntranslation. Our framework directly models the entire translation list's\nordering to learn parameters which may better fit the given listwise samples.\nFurthermore, we propose top-rank enhanced loss functions, which are more\nsensitive to ranking errors at higher positions. Experiments on a large-scale\nChinese-English translation task show that both our listwise learning framework\nand top-rank enhanced listwise losses lead to significant improvements in\ntranslation quality.", "published": "2017-07-18 02:04:37", "link": "http://arxiv.org/abs/1707.05438v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Intentional Lexical Ambiguity in English Puns", "abstract": "The article describes a model of automatic analysis of puns, where a word is\nintentionally used in two meanings at the same time (the target word). We\nemploy Roget's Thesaurus to discover two groups of words which, in a pun, form\naround two abstract bits of meaning (semes). They become a semantic vector,\nbased on which an SVM classifier learns to recognize puns, reaching a score\n0.73 for F-measure. We apply several rule-based methods to locate intentionally\nambiguous (target) words, based on structural and semantic criteria. It appears\nthat the structural criterion is more effective, although it possibly\ncharacterizes only the tested dataset. The results we get correlate with the\nresults of other teams at SemEval-2017 competition (Task 7 Detection and\nInterpretation of English Puns) considering effects of using supervised\nlearning models and word statistics.", "published": "2017-07-18 05:04:03", "link": "http://arxiv.org/abs/1707.05468v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PunFields at SemEval-2017 Task 7: Employing Roget's Thesaurus in\n  Automatic Pun Recognition and Interpretation", "abstract": "The article describes a model of automatic interpretation of English puns,\nbased on Roget's Thesaurus, and its implementation, PunFields. In a pun, the\nalgorithm discovers two groups of words that belong to two main semantic\nfields. The fields become a semantic vector based on which an SVM classifier\nlearns to recognize puns. A rule-based model is then applied for recognition of\nintentionally ambiguous (target) words and their definitions. In SemEval Task 7\nPunFields shows a considerably good result in pun classification, but requires\nimprovement in searching for the target word and its definition.", "published": "2017-07-18 05:40:18", "link": "http://arxiv.org/abs/1707.05479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Story Generation from Sequence of Independent Short Descriptions", "abstract": "Existing Natural Language Generation (NLG) systems are weak AI systems and\nexhibit limited capabilities when language generation tasks demand higher\nlevels of creativity, originality and brevity. Effective solutions or, at least\nevaluations of modern NLG paradigms for such creative tasks have been elusive,\nunfortunately. This paper introduces and addresses the task of coherent story\ngeneration from independent descriptions, describing a scene or an event.\nTowards this, we explore along two popular text-generation paradigms -- (1)\nStatistical Machine Translation (SMT), posing story generation as a translation\nproblem and (2) Deep Learning, posing story generation as a sequence to\nsequence learning problem. In SMT, we chose two popular methods such as phrase\nbased SMT (PB-SMT) and syntax based SMT (SYNTAX-SMT) to `translate' the\nincoherent input text into stories. We then implement a deep recurrent neural\nnetwork (RNN) architecture that encodes sequence of variable length input\ndescriptions to corresponding latent representations and decodes them to\nproduce well formed comprehensive story like summaries. The efficacy of the\nsuggested approaches is demonstrated on a publicly available dataset with the\nhelp of popular machine translation and summarization evaluation metrics.", "published": "2017-07-18 07:08:31", "link": "http://arxiv.org/abs/1707.05501v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the State of the Art of Evaluation in Neural Language Models", "abstract": "Ongoing innovations in recurrent neural network architectures have provided a\nsteady influx of apparently state-of-the-art results on language modelling\nbenchmarks. However, these have been evaluated using differing code bases and\nlimited computational resources, which represent uncontrolled sources of\nexperimental variation. We reevaluate several popular architectures and\nregularisation methods with large-scale automatic black-box hyperparameter\ntuning and arrive at the somewhat surprising conclusion that standard LSTM\narchitectures, when properly regularised, outperform more recent models. We\nestablish a new state of the art on the Penn Treebank and Wikitext-2 corpora,\nas well as strong baselines on the Hutter Prize dataset.", "published": "2017-07-18 12:35:53", "link": "http://arxiv.org/abs/1707.05589v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spherical Paragraph Model", "abstract": "Representing texts as fixed-length vectors is central to many language\nprocessing tasks. Most traditional methods build text representations based on\nthe simple Bag-of-Words (BoW) representation, which loses the rich semantic\nrelations between words. Recent advances in natural language processing have\nshown that semantically meaningful representations of words can be efficiently\nacquired by distributed models, making it possible to build text\nrepresentations based on a better foundation called the Bag-of-Word-Embedding\n(BoWE) representation. However, existing text representation methods using BoWE\noften lack sound probabilistic foundations or cannot well capture the semantic\nrelatedness encoded in word vectors. To address these problems, we introduce\nthe Spherical Paragraph Model (SPM), a probabilistic generative model based on\nBoWE, for text representation. SPM has good probabilistic interpretability and\ncan fully leverage the rich semantics of words, the word co-occurrence\ninformation as well as the corpus-wide information to help the representation\nlearning of texts. Experimental results on topical classification and sentiment\nanalysis demonstrate that SPM can achieve new state-of-the-art performances on\nseveral benchmark datasets.", "published": "2017-07-18 14:19:50", "link": "http://arxiv.org/abs/1707.05635v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Short Survey of Biomedical Relation Extraction Techniques", "abstract": "Biomedical information is growing rapidly in the recent years and retrieving\nuseful data through information extraction system is getting more attention. In\nthe current research, we focus on different aspects of relation extraction\ntechniques in biomedical domain and briefly describe the state-of-the-art for\nrelation extraction between a variety of biological elements.", "published": "2017-07-18 20:38:42", "link": "http://arxiv.org/abs/1707.05850v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Encoding Word Confusion Networks with Recurrent Neural Networks for\n  Dialog State Tracking", "abstract": "This paper presents our novel method to encode word confusion networks, which\ncan represent a rich hypothesis space of automatic speech recognition systems,\nvia recurrent neural networks. We demonstrate the utility of our approach for\nthe task of dialog state tracking in spoken dialog systems that relies on\nautomatic speech recognition output. Encoding confusion networks outperforms\nencoding the best hypothesis of the automatic speech recognition in a neural\nsystem for dialog state tracking on the well-known second Dialog State Tracking\nChallenge dataset.", "published": "2017-07-18 20:47:06", "link": "http://arxiv.org/abs/1707.05853v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering topics in text datasets by visualizing relevant words", "abstract": "When dealing with large collections of documents, it is imperative to quickly\nget an overview of the texts' contents. In this paper we show how this can be\nachieved by using a clustering algorithm to identify topics in the dataset and\nthen selecting and visualizing relevant words, which distinguish a group of\ndocuments from the rest of the texts, to summarize the contents of the\ndocuments belonging to each topic. We demonstrate our approach by discovering\ntrending topics in a collection of New York Times article snippets.", "published": "2017-07-18 15:46:47", "link": "http://arxiv.org/abs/1707.06100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Analysis of Social Network Pages by Interests of Their\n  Followers", "abstract": "Being a matter of cognition, user interests should be apt to classification\nindependent of the language of users, social network and content of interest\nitself. To prove it, we analyze a collection of English and Russian Twitter and\nVkontakte community pages by interests of their followers. First, we create a\nmodel of Major Interests (MaIs) with the help of expert analysis and then\nclassify a set of pages using machine learning algorithms (SVM, Neural Network,\nNaive Bayes, and some other). We take three interest domains that are typical\nof both English and Russian-speaking communities: football, rock music,\nvegetarianism. The results of classification show a greater correlation between\nRussian-Vkontakte and Russian-Twitter pages while English-Twitterpages appear\nto provide the highest score.", "published": "2017-07-18 05:49:30", "link": "http://arxiv.org/abs/1707.05481v2", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives", "abstract": "We present a new technique for learning visual-semantic embeddings for\ncross-modal retrieval. Inspired by hard negative mining, the use of hard\nnegatives in structured prediction, and ranking loss functions, we introduce a\nsimple change to common loss functions used for multi-modal embeddings. That,\ncombined with fine-tuning and use of augmented data, yields significant gains\nin retrieval performance. We showcase our approach, VSE++, on MS-COCO and\nFlickr30K datasets, using ablation studies and comparisons with existing\nmethods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8%\nin caption retrieval and 11.3% in image retrieval (at R@1).", "published": "2017-07-18 13:51:32", "link": "http://arxiv.org/abs/1707.05612v4", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Grounding Spatio-Semantic Referring Expressions for Human-Robot\n  Interaction", "abstract": "The human language is one of the most natural interfaces for humans to\ninteract with robots. This paper presents a robot system that retrieves\neveryday objects with unconstrained natural language descriptions. A core issue\nfor the system is semantic and spatial grounding, which is to infer objects and\ntheir spatial relationships from images and natural language expressions. We\nintroduce a two-stage neural-network grounding pipeline that maps natural\nlanguage referring expressions directly to objects in the images. The first\nstage uses visual descriptions in the referring expressions to generate a\ncandidate set of relevant objects. The second stage examines all pairwise\nrelationships between the candidates and predicts the most likely referred\nobject according to the spatial descriptions in the referring expressions. A\nkey feature of our system is that by leveraging a large dataset of images\nlabeled with text descriptions, it allows unrestricted object types and natural\nlanguage referring expressions. Preliminary results indicate that our system\noutperforms a near state-of-the-art object comprehension system on standard\nbenchmark datasets. We also present a robot system that follows voice commands\nto pick and place previously unseen objects.", "published": "2017-07-18 16:02:05", "link": "http://arxiv.org/abs/1707.05720v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
