{"title": "A High-Quality Multilingual Dataset for Structured Documentation\n  Translation", "abstract": "This paper presents a high-quality multilingual dataset for the documentation\ndomain to advance research on localization of structured text. Unlike\nwidely-used datasets for translation of plain text, we collect XML-structured\nparallel text segments from the online documentation for an enterprise software\nplatform. These Web pages have been professionally translated from English into\n16 languages and maintained by domain experts, and around 100,000 text segments\nare available for each language pair. We build and evaluate translation models\nfor seven target languages from English, with several different copy mechanisms\nand an XML-constrained beam search. We also experiment with a non-English pair\nto show that our dataset has the potential to explicitly enable $17 \\times 16$\ntranslation settings. Our experiments show that learning to translate with the\nXML tags improves translation accuracy, and the beam search accurately\ngenerates XML structures. We also discuss trade-offs of using the copy\nmechanisms by focusing on translation of numerical words and named entities. We\nfurther provide a detailed human analysis of gaps between the model output and\nhuman translations for real-world applications, including suitability for\npost-editing.", "published": "2020-06-24 02:08:44", "link": "http://arxiv.org/abs/2006.13425v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XREF: Entity Linking for Chinese News Comments with Supplementary\n  Article Reference", "abstract": "Automatic identification of mentioned entities in social media posts\nfacilitates quick digestion of trending topics and popular opinions.\nNonetheless, this remains a challenging task due to limited context and diverse\nname variations. In this paper, we study the problem of entity linking for\nChinese news comments given mentions' spans. We hypothesize that comments often\nrefer to entities in the corresponding news article, as well as topics\ninvolving the entities. We therefore propose a novel model, XREF, that\nleverages attention mechanisms to (1) pinpoint relevant context within\ncomments, and (2) detect supporting entities from the news article. To improve\ntraining, we make two contributions: (a) we propose a supervised attention loss\nin addition to the standard cross entropy, and (b) we develop a weakly\nsupervised training scheme to utilize the large-scale unlabeled corpus. Two new\ndatasets in entertainment and product domains are collected and annotated for\nexperiments. Our proposed method outperforms previous methods on both datasets.", "published": "2020-06-24 19:42:54", "link": "http://arxiv.org/abs/2006.14017v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Analyzing Annotation Consistency in Online Abusive Behavior Datasets", "abstract": "Online abusive behavior is an important issue that breaks the cohesiveness of\nonline social communities and even raises public safety concerns in our\nsocieties. Motivated by this rising issue, researchers have proposed,\ncollected, and annotated online abusive content datasets. These datasets play a\ncritical role in facilitating the research on online hate speech and abusive\nbehaviors. However, the annotation of such datasets is a difficult task; it is\noften contentious on what should be the true label of a given text as the\nsemantic difference of the labels may be blurred (e.g., abusive and hate) and\noften subjective. In this study, we proposed an analytical framework to study\nthe annotation consistency in online hate and abusive content datasets. We\napplied our proposed framework to evaluate the consistency of the annotation in\nthree popular datasets that are widely used in online hate speech and abusive\nbehavior studies. We found that there is still a substantial amount of\nannotation inconsistency in the existing datasets, particularly when the labels\nare semantically similar.", "published": "2020-06-24 06:34:25", "link": "http://arxiv.org/abs/2006.13507v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Efficient Constituency Parsing by Pointing", "abstract": "We propose a novel constituency parsing model that casts the parsing problem\ninto a series of pointing tasks. Specifically, our model estimates the\nlikelihood of a span being a legitimate tree constituent via the pointing score\ncorresponding to the boundary words of the span. Our parsing model supports\nefficient top-down decoding and our learning objective is able to enforce\nstructural consistency without resorting to the expensive CKY inference. The\nexperiments on the standard English Penn Treebank parsing task show that our\nmethod achieves 92.78 F1 without using pre-trained models, which is higher than\nall the existing methods with similar time complexity. Using pre-trained BERT,\nour model achieves 95.48 F1, which is competitive with the state-of-the-art\nwhile being faster. Our approach also establishes new state-of-the-art in\nBasque and Swedish in the SPMRL shared tasks on multilingual constituency\nparsing.", "published": "2020-06-24 08:29:09", "link": "http://arxiv.org/abs/2006.13557v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings", "abstract": "Much of biomedical and healthcare data is encoded in discrete, symbolic form\nsuch as text and medical codes. There is a wealth of expert-curated biomedical\ndomain knowledge stored in knowledge bases and ontologies, but the lack of\nreliable methods for learning knowledge representation has limited their\nusefulness in machine learning applications. While text-based representation\nlearning has significantly improved in recent years through advances in natural\nlanguage processing, attempts to learn biomedical concept embeddings so far\nhave been lacking. A recent family of models called knowledge graph embeddings\nhave shown promising results on general domain knowledge graphs, and we explore\ntheir capabilities in the biomedical domain. We train several state-of-the-art\nknowledge graph embedding models on the SNOMED-CT knowledge graph, provide a\nbenchmark with comparison to existing methods and in-depth discussion on best\npractices, and make a case for the importance of leveraging the\nmulti-relational nature of knowledge graphs for learning biomedical knowledge\nrepresentation. The embeddings, code, and materials will be made available to\nthe communitY.", "published": "2020-06-24 14:47:33", "link": "http://arxiv.org/abs/2006.13774v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Accelerated Large Batch Optimization of BERT Pretraining in 54 minutes", "abstract": "BERT has recently attracted a lot of attention in natural language\nunderstanding (NLU) and achieved state-of-the-art results in various NLU tasks.\nHowever, its success requires large deep neural networks and huge amount of\ndata, which result in long training time and impede development progress. Using\nstochastic gradient methods with large mini-batch has been advocated as an\nefficient tool to reduce the training time. Along this line of research, LAMB\nis a prominent example that reduces the training time of BERT from 3 days to 76\nminutes on a TPUv3 Pod. In this paper, we propose an accelerated gradient\nmethod called LANS to improve the efficiency of using large mini-batches for\ntraining. As the learning rate is theoretically upper bounded by the inverse of\nthe Lipschitz constant of the function, one cannot always reduce the number of\noptimization iterations by selecting a larger learning rate. In order to use\nlarger mini-batch size without accuracy loss, we develop a new learning rate\nscheduler that overcomes the difficulty of using large learning rate. Using the\nproposed LANS method and the learning rate scheme, we scaled up the mini-batch\nsizes to 96K and 33K in phases 1 and 2 of BERT pretraining, respectively. It\ntakes 54 minutes on 192 AWS EC2 P3dn.24xlarge instances to achieve a target F1\nscore of 90.5 or higher on SQuAD v1.1, achieving the fastest BERT training time\nin the cloud.", "published": "2020-06-24 05:00:41", "link": "http://arxiv.org/abs/2006.13484v2", "categories": ["cs.LG", "cs.CL", "cs.DC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Black-box Adaptation of ASR for Accented Speech", "abstract": "We introduce the problem of adapting a black-box, cloud-based ASR system to\nspeech from a target accent. While leading online ASR services obtain\nimpressive performance on main-stream accents, they perform poorly on\nsub-populations - we observed that the word error rate (WER) achieved by\nGoogle's ASR API on Indian accents is almost twice the WER on US accents.\nExisting adaptation methods either require access to model parameters or\noverlay an error-correcting module on output transcripts. We highlight the need\nfor correlating outputs with the original speech to fix accent errors.\nAccordingly, we propose a novel coupling of an open-source accent-tuned local\nmodel with the black-box service where the output from the service guides\nframe-level inference in the local model. Our fine-grained merging algorithm is\nbetter at fixing accent errors than existing word-level combination strategies.\nExperiments on Indian and Australian accents with three leading ASR models as\nservice, show that we achieve as much as 28% relative reduction in WER over\nboth the local and service models.", "published": "2020-06-24 07:07:49", "link": "http://arxiv.org/abs/2006.13519v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Crossmodal Language Grounding in an Embodied Neurocognitive Model", "abstract": "Human infants are able to acquire natural language seemingly easily at an\nearly age. Their language learning seems to occur simultaneously with learning\nother cognitive functions as well as with playful interactions with the\nenvironment and caregivers. From a neuroscientific perspective, natural\nlanguage is embodied, grounded in most, if not all, sensory and sensorimotor\nmodalities, and acquired by means of crossmodal integration. However,\ncharacterising the underlying mechanisms in the brain is difficult and\nexplaining the grounding of language in crossmodal perception and action\nremains challenging. In this paper, we present a neurocognitive model for\nlanguage grounding which reflects bio-inspired mechanisms such as an implicit\nadaptation of timescales as well as end-to-end multimodal abstraction. It\naddresses developmental robotic interaction and extends its learning\ncapabilities using larger-scale knowledge-based data. In our scenario, we\nutilise the humanoid robot NICO in obtaining the EMIL data collection, in which\nthe cognitive robot interacts with objects in a children's playground\nenvironment while receiving linguistic labels from a caregiver. The model\nanalysis shows that crossmodally integrated representations are sufficient for\nacquiring language merely from sensory input through interaction with objects\nin an environment. The representations self-organise hierarchically and embed\ntemporal and spatial information through composition and decomposition. This\nmodel can also provide the basis for further crossmodal integration of\nperceptually grounded cognitive representations.", "published": "2020-06-24 08:12:09", "link": "http://arxiv.org/abs/2006.13546v2", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Differentiable Window for Dynamic Local Attention", "abstract": "We propose Differentiable Window, a new neural module and general purpose\ncomponent for dynamic window selection. While universally applicable, we\ndemonstrate a compelling use case of utilizing Differentiable Window to improve\nstandard attention modules by enabling more focused attentions over the input\nregions. We propose two variants of Differentiable Window, and integrate them\nwithin the Transformer architecture in two novel ways. We evaluate our proposed\napproach on a myriad of NLP tasks, including machine translation, sentiment\nanalysis, subject-verb agreement and language modeling. Our experimental\nresults demonstrate consistent and sizable improvements across all tasks.", "published": "2020-06-24 08:47:26", "link": "http://arxiv.org/abs/2006.13561v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Unsupervised Cross-lingual Representation Learning for Speech\n  Recognition", "abstract": "This paper presents XLSR which learns cross-lingual speech representations by\npretraining a single model from the raw waveform of speech in multiple\nlanguages. We build on wav2vec 2.0 which is trained by solving a contrastive\ntask over masked latent speech representations and jointly learns a\nquantization of the latents shared across languages. The resulting model is\nfine-tuned on labeled data and experiments show that cross-lingual pretraining\nsignificantly outperforms monolingual pretraining. On the CommonVoice\nbenchmark, XLSR shows a relative phoneme error rate reduction of 72% compared\nto the best known results. On BABEL, our approach improves word error rate by\n16% relative compared to a comparable system. Our approach enables a single\nmultilingual speech recognition model which is competitive to strong individual\nmodels. Analysis shows that the latent discrete speech representations are\nshared across languages with increased sharing for related languages. We hope\nto catalyze research in low-resource speech understanding by releasing XLSR-53,\na large model pretrained in 53 languages.", "published": "2020-06-24 18:25:05", "link": "http://arxiv.org/abs/2006.13979v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multilingual Jointly Trained Acoustic and Written Word Embeddings", "abstract": "Acoustic word embeddings (AWEs) are vector representations of spoken word\nsegments. AWEs can be learned jointly with embeddings of character sequences,\nto generate phonetically meaningful embeddings of written words, or\nacoustically grounded word embeddings (AGWEs). Such embeddings have been used\nto improve speech retrieval, recognition, and spoken term discovery. In this\nwork, we extend this idea to multiple low-resource languages. We jointly train\nan AWE model and an AGWE model, using phonetically transcribed data from\nmultiple languages. The pre-trained models can then be used for unseen\nzero-resource languages, or fine-tuned on data from low-resource languages. We\nalso investigate distinctive features, as an alternative to phone labels, to\nbetter share cross-lingual information. We test our models on word\ndiscrimination tasks for twelve languages. When trained on eleven languages and\ntested on the remaining unseen language, our model outperforms traditional\nunsupervised approaches like dynamic time warping. After fine-tuning the\npre-trained models on one hour or even ten minutes of data from a new language,\nperformance is typically much better than training on only the target-language\ndata. We also find that phonetic supervision improves performance over\ncharacter sequences, and that distinctive feature supervision is helpful in\nhandling unseen phones in the target language.", "published": "2020-06-24 19:16:02", "link": "http://arxiv.org/abs/2006.14007v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bounds of the sum of edge lengths in linear arrangements of trees", "abstract": "A fundamental problem in network science is the normalization of the\ntopological or physical distance between vertices, that requires understanding\nthe range of variation of the unnormalized distances. Here we investigate the\nlimits of the variation of the physical distance in linear arrangements of the\nvertices of trees. In particular, we investigate various problems on the sum of\nedge lengths in trees of a fixed size: the minimum and the maximum value of the\nsum for specific trees, the minimum and the maximum in classes of trees (bistar\ntrees and caterpillar trees) and finally the minimum and the maximum for any\ntree. We establish some foundations for research on optimality scores for\nspatial networks in one dimension.", "published": "2020-06-24 21:53:39", "link": "http://arxiv.org/abs/2006.14069v3", "categories": ["cs.DM", "cs.CL", "physics.soc-ph"], "primary_category": "cs.DM"}
{"title": "The NetHack Learning Environment", "abstract": "Progress in Reinforcement Learning (RL) algorithms goes hand-in-hand with the\ndevelopment of challenging environments that test the limits of current\nmethods. While existing RL environments are either sufficiently complex or\nbased on fast simulation, they are rarely both. Here, we present the NetHack\nLearning Environment (NLE), a scalable, procedurally generated, stochastic,\nrich, and challenging environment for RL research based on the popular\nsingle-player terminal-based roguelike game, NetHack. We argue that NetHack is\nsufficiently complex to drive long-term research on problems such as\nexploration, planning, skill acquisition, and language-conditioned RL, while\ndramatically reducing the computational resources required to gather a large\namount of experience. We compare NLE and its task suite to existing\nalternatives, and discuss why it is an ideal medium for testing the robustness\nand systematic generalization of RL agents. We demonstrate empirical success\nfor early stages of the game using a distributed Deep RL baseline and Random\nNetwork Distillation exploration, alongside qualitative analysis of various\nagents trained in the environment. NLE is open source at\nhttps://github.com/facebookresearch/nle.", "published": "2020-06-24 14:12:56", "link": "http://arxiv.org/abs/2006.13760v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Compositional Explanations of Neurons", "abstract": "We describe a procedure for explaining neurons in deep representations by\nidentifying compositional logical concepts that closely approximate neuron\nbehavior. Compared to prior work that uses atomic labels as explanations,\nanalyzing neurons compositionally allows us to more precisely and expressively\ncharacterize their behavior. We use this procedure to answer several questions\non interpretability in models for vision and natural language processing.\nFirst, we examine the kinds of abstractions learned by neurons. In image\nclassification, we find that many neurons learn highly abstract but\nsemantically coherent visual concepts, while other polysemantic neurons detect\nmultiple unrelated features; in natural language inference (NLI), neurons learn\nshallow lexical heuristics from dataset biases. Second, we see whether\ncompositional explanations give us insight into model performance: vision\nneurons that detect human-interpretable concepts are positively correlated with\ntask performance, while NLI neurons that fire for shallow heuristics are\nnegatively correlated with task performance. Finally, we show how compositional\nexplanations provide an accessible way for end users to produce simple\n\"copy-paste\" adversarial examples that change model behavior in predictable\nways.", "published": "2020-06-24 20:37:05", "link": "http://arxiv.org/abs/2006.14032v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multi-path RNN for hierarchical modeling of long sequential data and its\n  application to speaker stream separation", "abstract": "Recently, the source separation performance was greatly improved by\ntime-domain audio source separation based on dual-path recurrent neural network\n(DPRNN). DPRNN is a simple but effective model for a long sequential data.\nWhile DPRNN is quite efficient in modeling a sequential data of the length of\nan utterance, i.e., about 5 to 10 second data, it is harder to apply it to\nlonger sequences such as whole conversations consisting of multiple utterances.\nIt is simply because, in such a case, the number of time steps consumed by its\ninternal module called inter-chunk RNN becomes extremely large. To mitigate\nthis problem, this paper proposes a multi-path RNN (MPRNN), a generalized\nversion of DPRNN, that models the input data in a hierarchical manner. In the\nMPRNN framework, the input data is represented at several (>3)\ntime-resolutions, each of which is modeled by a specific RNN sub-module. For\nexample, the RNN sub-module that deals with the finest resolution may model\ntemporal relationship only within a phoneme, while the RNN sub-module handling\nthe most coarse resolution may capture only the relationship between utterances\nsuch as speaker information. We perform experiments using simulated\ndialogue-like mixtures and show that MPRNN has greater model capacity, and it\noutperforms the current state-of-the-art DPRNN framework especially in online\nprocessing scenarios.", "published": "2020-06-24 09:45:11", "link": "http://arxiv.org/abs/2006.13579v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Neural Network based Distance Estimation for Geometry Calibration\n  in Acoustic Sensor Networks", "abstract": "We present an approach to deep neural network based (DNN-based) distance\nestimation in reverberant rooms for supporting geometry calibration tasks in\nwireless acoustic sensor networks. Signal diffuseness information from acoustic\nsignals is aggregated via the coherent-to-diffuse power ratio to obtain a\ndistance-related feature, which is mapped to a source-to-microphone distance\nestimate by means of a DNN. This information is then combined with\ndirection-of-arrival estimates from compact microphone arrays to infer the\ngeometry of the sensor network. Unlike many other approaches to geometry\ncalibration, the proposed scheme does only require that the sampling clocks of\nthe sensor nodes are roughly synchronized. In simulations we show that the\nproposed DNN-based distance estimator generalizes to unseen acoustic\nenvironments and that precise estimates of the sensor node positions are\nobtained.", "published": "2020-06-24 14:33:15", "link": "http://arxiv.org/abs/2006.13769v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Face-to-Music Translation Using a Distance-Preserving Generative\n  Adversarial Network with an Auxiliary Discriminator", "abstract": "Learning a mapping between two unrelated domains-such as image and audio,\nwithout any supervision is a challenging task. In this work, we propose a\ndistance-preserving generative adversarial model to translate images of human\nfaces into an audio domain. The audio domain is defined by a collection of\nmusical note sounds recorded by 10 different instrument families (NSynth\n\\cite{nsynth2017}) and a distance metric where the instrument family class\ninformation is incorporated together with a mel-frequency cepstral coefficients\n(MFCCs) feature. To enforce distance-preservation, a loss term that penalizes\ndifference between pairwise distances of the faces and the translated audio\nsamples is used. Further, we discover that the distance preservation constraint\nin the generative adversarial model leads to reduced diversity in the\ntranslated audio samples, and propose the use of an auxiliary discriminator to\nenhance the diversity of the translations while using the distance preservation\nconstraint. We also provide a visual demonstration of the results and numerical\nanalysis of the fidelity of the translations. A video demo of our proposed\nmodel's learned translation is available in\nhttps://www.dropbox.com/s/the176w9obq8465/face_to_musical_note.mov?dl=0.", "published": "2020-06-24 04:17:40", "link": "http://arxiv.org/abs/2006.13469v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Gamma Boltzmann Machine for Simultaneously Modeling Linear- and\n  Log-amplitude Spectra", "abstract": "In audio applications, one of the most important representations of audio\nsignals is the amplitude spectrogram. It is utilized in many\nmachine-learning-based information processing methods including the ones using\nthe restricted Boltzmann machines (RBM). However, the ordinary\nGaussian-Bernoulli RBM (the most popular RBM among its variations) cannot\ndirectly handle amplitude spectra because the Gaussian distribution is a\nsymmetric model allowing negative values which never appear in the amplitude.\nIn this paper, after proposing a general gamma Boltzmann machine, we propose a\npractical model called the gamma-Bernoulli RBM that simultaneously handles both\nlinear- and log-amplitude spectrograms. Its conditional distribution of the\nobservable data is given by the gamma distribution, and thus the proposed RBM\ncan naturally handle the data represented by positive numbers as the amplitude\nspectra. It can also treat amplitude in the logarithmic scale which is\nimportant for audio signals from the perceptual point of view. The advantage of\nthe proposed model compared to the ordinary Gaussian-Bernoulli RBM was\nconfirmed by PESQ and MSE in the experiment of representing the amplitude\nspectrograms of speech signals.", "published": "2020-06-24 10:09:58", "link": "http://arxiv.org/abs/2006.13590v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
