{"title": "Every Graph is Essential to Large Treewidth", "abstract": "We show that for every graph $H$, there is a hereditary weakly sparse graph\nclass $\\mathcal C_H$ of unbounded treewidth such that the $H$-free (i.e.,\nexcluding $H$ as an induced subgraph) graphs of $\\mathcal C_H$ have bounded\ntreewidth. This refutes several conjectures and critically thwarts the quest\nfor the unavoidable induced subgraphs in classes of unbounded treewidth, a\nwished-for counterpart of the Grid Minor theorem. We actually show a stronger\nresult: For every positive integer $t$, there is a hereditary graph class\n$\\mathcal C_t$ of unbounded treewidth such that for any graph $H$ of treewidth\nat most $t$, the $H$-free graphs of $\\mathcal C_t$ have bounded treewidth. Our\nconstruction is a variant of so-called layered wheels. We also introduce a\nframework of abstract layered wheels, based on their most salient properties.\nIn particular, we streamline and extend key lemmas previously shown on\nindividual layered wheels. We believe that this should greatly help develop\nthis topic, which appears to be a very strong yet underexploited source of\ncounterexamples.", "published": "2025-02-20 17:59:36", "link": "http://arxiv.org/abs/2502.14775v3", "categories": ["math.CO", "cs.DM", "05C75", "G.2.2"], "primary_category": "math.CO"}
{"title": "Enumerating minimal dominating sets and variants in chordal bipartite graphs", "abstract": "Enumerating minimal dominating sets with polynomial delay in bipartite graphs\nis a long-standing open problem. To date, even the subcase of chordal bipartite\ngraphs is open, with the best known algorithm due to Golovach, Heggernes,\nKant\\'e, Kratsch, Saether, and Villanger running in incremental-polynomial\ntime. We improve on this result by providing a polynomial delay and space\nalgorithm enumerating minimal dominating sets in chordal bipartite graphs.\nAdditionally, we show that the total and connected variants admit polynomial\nand incremental-polynomial delay algorithms, respectively, within the same\nclass. This provides an alternative proof of a result by Golovach et al. for\ntotal dominating sets, and answers an open question for the connected variant.\nFinally, we give evidence that the techniques used in this paper cannot be\ngeneralized to bipartite graphs for (total) minimal dominating sets, unless P =\nNP, and show that enumerating minimal connected dominating sets in bipartite\ngraphs is harder than enumerating minimal transversals in general hypergraphs.", "published": "2025-02-20 14:51:41", "link": "http://arxiv.org/abs/2502.14611v2", "categories": ["cs.DS", "cs.DM", "math.CO"], "primary_category": "cs.DS"}
{"title": "Temporal Connectivity Augmentation", "abstract": "Connectivity in temporal graphs relies on the notion of temporal paths, in\nwhich edges follow a chronological order (either strict or non-strict). In this\nwork, we investigate the question of how to make a temporal graph connected.\nMore precisely, we tackle the problem of finding, among a set of proposed\ntemporal edges, the smallest subset such that its addition makes the graph\ntemporally connected (TCA). We study the complexity of this problem and\nvariants, under restricted lifespan of the graph, i.e. the maximum time step in\nthe graph. Our main result on TCA is that for any fixed lifespan at least 2, it\nis NP-complete in both the strict and non-strict setting. We additionally\nprovide a set of restrictions in the non-strict setting which makes the problem\nsolvable in polynomial time and design an algorithm achieving this complexity.\nInterestingly, we prove that the source variant (making a given vertex a source\nin the augmented graph) is as difficult as TCA. On the opposite, we prove that\nthe version where a list of connectivity demands has to be satisfied is\nsolvable in polynomial time, when the size of the list is fixed. Finally, we\nhighlight a variant of the previous case for which even with two pairs the\nproblem is already NP-hard.", "published": "2025-02-20 13:17:08", "link": "http://arxiv.org/abs/2502.14540v1", "categories": ["cs.DM", "G.2.2"], "primary_category": "cs.DM"}
{"title": "A Parallel Hierarchical Approach for Community Detection on Large-scale Dynamic Networks", "abstract": "In this paper, we propose a novel parallel hierarchical Leiden-based\nalgorithm for dynamic community detection. The algorithm, for a given batch\nupdate of edge insertions and deletions, partitions the network into\ncommunities using only a local neighborhood of the affected nodes. It also uses\nthe inner hierarchical graph-based structure, which is updated incrementally in\nthe process of optimizing the modularity of the partitioning. The algorithm has\nbeen extensively tested on various networks. The results demonstrate promising\nimprovements in performance and scalability while maintaining the modularity of\nthe partitioning.", "published": "2025-02-20 09:53:13", "link": "http://arxiv.org/abs/2502.18497v1", "categories": ["cs.SI", "cs.DC", "cs.DM"], "primary_category": "cs.SI"}
{"title": "Pursuing Top Growth with Novel Loss Function", "abstract": "Making consistently profitable financial decisions in a continuously evolving\nand volatile stock market has always been a difficult task. Professionals from\ndifferent disciplines have developed foundational theories to anticipate price\nmovement and evaluate securities such as the famed Capital Asset Pricing Model\n(CAPM). In recent years, the role of artificial intelligence (AI) in asset\npricing has been growing. Although the black-box nature of deep learning models\nlacks interpretability, they have continued to solidify their position in the\nfinancial industry. We aim to further enhance AI's potential and utility by\nintroducing a return-weighted loss function that will drive top growth while\nproviding the ML models a limited amount of information. Using only publicly\naccessible stock data (open/close/high/low, trading volume, sector information)\nand several technical indicators constructed from them, we propose an efficient\ndaily trading system that detects top growth opportunities. Our best models\nachieve 61.73% annual return on daily rebalancing with an annualized Sharpe\nRatio of 1.18 over 1340 testing days from 2019 to 2024, and 37.61% annual\nreturn with an annualized Sharpe Ratio of 0.97 over 1360 testing days from 2005\nto 2010. The main drivers for success, especially independent of any domain\nknowledge, are the novel return-weighted loss function, the integration of\ncategorical and continuous data, and the ML model architecture. We also\ndemonstrate the superiority of our novel loss function over traditional loss\nfunctions via several performance metrics and statistical evidence.", "published": "2025-02-20 21:43:51", "link": "http://arxiv.org/abs/2502.17493v1", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "I.2.1; I.2.6"], "primary_category": "cs.LG"}
{"title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis", "abstract": "We propose a structural default model for portfolio-wide valuation\nadjustments (xVAs) and represent it as a system of coupled backward stochastic\ndifferential equations. The framework is divided into four layers, each\ncapturing a key component: (i) clean values, (ii) initial margin and Collateral\nValuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments\n(CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding\nValuation Adjustment (FVA). Because these layers depend on one another through\ncollateral and default effects, a naive Monte Carlo approach would require\ndeeply nested simulations, making the problem computationally intractable.\n  To address this challenge, we use an iterative deep BSDE approach, handling\neach layer sequentially so that earlier outputs serve as inputs to the\nsubsequent layers. Initial margin is computed via deep quantile regression to\nreflect margin requirements over the Margin Period of Risk. We also adopt a\nchange-of-measure method that highlights rare but significant defaults of the\nbank or counterparty, ensuring that these events are accurately captured in the\ntraining process.\n  We further extend Han and Long's (2020) a posteriori error analysis to BSDEs\non bounded domains. Due to the random exit from the domain, we obtain an order\nof convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual\n$\\mathcal{O}(h^{1/2})$.\n  Numerical experiments illustrate that this method drastically reduces\ncomputational demands and successfully scales to high-dimensional,\nnon-symmetric portfolios. The results confirm its effectiveness and accuracy,\noffering a practical alternative to nested Monte Carlo simulations in\nmulti-counterparty xVA analyses.", "published": "2025-02-20 17:41:55", "link": "http://arxiv.org/abs/2502.14766v2", "categories": ["q-fin.CP", "91G40, 91G20, 91G60 65C30, 60G40"], "primary_category": "q-fin.CP"}
{"title": "Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework", "abstract": "The lifetime behaviour of loans is notoriously difficult to model, which can\ncompromise a bank's financial reserves against future losses, if modelled\npoorly. Therefore, we present a data-driven comparative study amongst three\ntechniques in modelling a series of default risk estimates over the lifetime of\neach loan, i.e., its term-structure. The behaviour of loans can be described\nusing a nonstationary and time-dependent semi-Markov model, though we model its\nelements using a multistate regression-based approach. As such, the transition\nprobabilities are explicitly modelled as a function of a rich set of input\nvariables, including macroeconomic and loan-level inputs. Our modelling\ntechniques are deliberately chosen in ascending order of complexity: 1) a\nMarkov chain; 2) beta regression; and 3) multinomial logistic regression. Using\nresidential mortgage data, our results show that each successive model\noutperforms the previous, likely as a result of greater sophistication. This\nfinding required devising a novel suite of simple model diagnostics, which can\nitself be reused in assessing sampling representativeness and the performance\nof other modelling techniques. These contributions surely advance the current\npractice within banking when conducting multistate modelling. Consequently, we\nbelieve that the estimation of loss reserves will be more timeous and accurate\nunder IFRS 9.", "published": "2025-02-20 11:53:20", "link": "http://arxiv.org/abs/2502.14479v1", "categories": ["q-fin.RM", "q-fin.ST", "stat.AP"], "primary_category": "q-fin.RM"}
{"title": "Causality Analysis of COVID-19 Induced Crashes in Stock and Commodity Markets: A Topological Perspective", "abstract": "The paper presents a comprehensive causality analysis of the US stock and\ncommodity markets during the COVID-19 crash. The dynamics of different sectors\nare also compared. We use Topological Data Analysis (TDA) on multidimensional\ntime-series to identify crashes in stock and commodity markets. The Wasserstein\nDistance WD shows distinct spikes signaling the crash for both stock and\ncommodity markets. We then compare the persistence diagrams of stock and\ncommodity markets using the WD metric. A significant spike in the $WD$ between\nstock and commodity markets is observed during the crisis, suggesting\nsignificant topological differences between the markets. Similar spikes are\nobserved between the sectors of the US market as well. Spikes obtained may be\ndue to either a difference in the magnitude of crashes in the two markets (or\nsectors), or from the temporal lag between the two markets suggesting\ninformation flow. We study the Granger-causality between stock and commodity\nmarkets and also between different sectors. The results show a bidirectional\nGranger-causality between commodity and stock during the crash period,\ndemonstrating the greater interdependence of financial markets during the\ncrash. However, the overall analysis shows that the causal direction is from\nstock to commodity. A pairwise Granger-causal analysis between US sectors is\nalso conducted. There is a significant increase in the interdependence between\nthe sectors during the crash period. TDA combined with Granger-causality\neffectively analyzes the interdependence and sensitivity of different markets\nand sectors.", "published": "2025-02-20 10:29:58", "link": "http://arxiv.org/abs/2502.14431v1", "categories": ["q-fin.ST", "math.AT", "physics.data-an"], "primary_category": "q-fin.ST"}
{"title": "Financial fraud detection system based on improved random forest and gradient boosting machine (GBM)", "abstract": "This paper proposes a financial fraud detection system based on improved\nRandom Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the\nsystem introduces a novel model architecture called GBM-SSRF (Gradient Boosting\nMachine with Simplified and Strengthened Random Forest), which cleverly\ncombines the powerful optimization capabilities of the gradient boosting\nmachine (GBM) with improved randomization. The computational efficiency and\nfeature extraction capabilities of the Simplified and Strengthened Random\nForest (SSRF) forest significantly improve the performance of financial fraud\ndetection. Although the traditional random forest model has good classification\ncapabilities, it has high computational complexity when faced with large-scale\ndata and has certain limitations in feature selection. As a commonly used\nensemble learning method, the GBM model has significant advantages in\noptimizing performance and handling nonlinear problems. However, GBM takes a\nlong time to train and is prone to overfitting problems when data samples are\nunbalanced. In response to these limitations, this paper optimizes the random\nforest based on the structure, reducing the computational complexity and\nimproving the feature selection ability through the structural simplification\nand enhancement of the random forest. In addition, the optimized random forest\nis embedded into the GBM framework, and the model can maintain efficiency and\nstability with the help of GBM's gradient optimization capability. Experiments\nshow that the GBM-SSRF model not only has good performance, but also has good\nrobustness and generalization capabilities, providing an efficient and reliable\nsolution for financial fraud detection.", "published": "2025-02-20 03:27:57", "link": "http://arxiv.org/abs/2502.15822v1", "categories": ["q-fin.ST", "cs.LG", "q-fin.GN", "stat.AP", "stat.ML"], "primary_category": "q-fin.ST"}
{"title": "Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs,\n  Desires, and Intentions for Human-Like Interaction", "abstract": "Natural language interaction with agentic Artificial Intelligence (AI),\ndriven by Large Language Models (LLMs), is expected to remain a dominant\nparadigm in the near future. While humans instinctively align their\ncommunication with mental states -- an ability known as Theory of Mind (ToM),\ncurrent LLM powered systems exhibit significant limitations in this regard.\nThis study examines the extent to which open source language models (LLaMA) can\ncapture and preserve ToM related information and how effectively it contributes\nto consistent ToM reasoning in generated responses. We further investigate\nwhether explicit manipulation of ToM related components, such as beliefs,\ndesires, and intentions, can enhance response alignment. Experiments on two\nLLaMA 3 variants demonstrate that incorporating ToM informed alignment improves\nresponse quality, achieving win rates of 67 and 63 percent for the 3B and 8B\nmodels, respectively. These findings highlight the potential of ToM driven\nstrategies to improve alignment in LLM based conversational agents.", "published": "2025-02-20 00:39:05", "link": "http://arxiv.org/abs/2502.14171v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare\n  Text Multi-Label Classification", "abstract": "The escalating volume of collected healthcare textual data presents a unique\nchallenge for automated Multi-Label Text Classification (MLTC), which is\nprimarily due to the scarcity of annotated texts for training and their nuanced\nnature. Traditional machine learning models often fail to fully capture the\narray of expressed topics. However, Large Language Models (LLMs) have\ndemonstrated remarkable effectiveness across numerous Natural Language\nProcessing (NLP) tasks in various domains, which show impressive computational\nefficiency and suitability for unsupervised learning through prompt\nengineering. Consequently, these LLMs promise an effective MLTC of medical\nnarratives. However, when dealing with various labels, different prompts can be\nrelevant depending on the topic. To address these challenges, the proposed\napproach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,\nPEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which\nBERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and\nBART provides topics' assignment probabilities, which results in four\nclassifications, all in a 0-shot setting. The outputs are then combined using\nensemble learning and processed through a meta-classifier to produce the final\nMLTC result. The approach is evaluated using three samples of annotated texts,\nwhich contrast it with traditional and single-model methods. The results show\nsignificant improvements across the majority of the topics in the\nclassification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and\n80.16% with standard deviations of 0.025 and 0.011, respectively). This\nresearch advances MLTC using LLMs and provides an efficient and scalable\nsolution to rapidly categorize healthcare-related text data without further\ntraining.", "published": "2025-02-20 01:46:12", "link": "http://arxiv.org/abs/2502.14189v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer-Prompting: Enhancing Cross-Task Adaptation in Large Language\n  Models via Dual-Stage Prompts Optimization", "abstract": "Large language models (LLMs) face significant challenges when balancing\nmultiple high-level objectives, such as generating coherent, relevant, and\nhigh-quality responses while maintaining efficient task adaptation across\ndiverse tasks. To address these challenges, we introduce Transfer-Prompting, a\nnovel two-stage framework designed to enhance cross-task adaptation in prompt\ngeneration. The framework comprises two key components: (1) source prompt\nconstruction, which refines the original prompts on source task datasets to\ngenerate source prompts with enhanced generalization ability, and (2) target\nprompt generation, which enhances cross-task adaptation of target prompts by\nfine-tuning a set of high-scored source prompts on task-specific datasets. In\neach optimization cycle, a reference LLM generates candidate prompts based on\nhistorical prompt-score pairs and task descriptions in our designed reference\nprompt. These candidate prompts are refined iteratively, while a scorer LLM\nevaluates their effectiveness using the multi-dimensional metrics designed in\nthe objective prompts evaluator-a novel contribution in this work that provides\na holistic evaluation of prompt quality and task performance. This feedback\nloop facilitates continuous refinement, optimizing both prompt quality and\ntask-specific outcomes. We validate Transfer-Prompting through extensive\nexperiments across 25 LLMs, including 7 foundational models and 18 specialized\nmodels, evaluated on 9 diverse datasets. The results demonstrate that\nTransfer-Prompting significantly improves task-specific performance,\nhighlighting its potential for enhancing cross-task adaptation in LLMs. The\ncode is available at https://github.com/llm172/Transfer-Prompting.", "published": "2025-02-20 02:47:04", "link": "http://arxiv.org/abs/2502.14211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop\n  Question Answering", "abstract": "In this paper, we identify a critical problem, \"lost-in-retrieval\", in\nretrieval-augmented multi-hop question answering (QA): the key entities are\nmissed in LLMs' sub-question decomposition. \"Lost-in-retrieval\" significantly\ndegrades the retrieval performance, which disrupts the reasoning chain and\nleads to the incorrect answers. To resolve this problem, we propose a\nprogressive retrieval and rewriting method, namely ChainRAG, which sequentially\nhandles each sub-question by completing missing key entities and retrieving\nrelevant sentences from a sentence graph for answer generation. Each step in\nour retrieval and rewriting process builds upon the previous one, creating a\nseamless chain that leads to accurate retrieval and answers. Finally, all\nretrieved sentences and sub-question answers are integrated to generate a\ncomprehensive answer to the original question. We evaluate ChainRAG on three\nmulti-hop QA datasets$\\unicode{x2013}$MuSiQue, 2Wiki, and\nHotpotQA$\\unicode{x2013}$using three large language models: GPT4o-mini,\nQwen2.5-72B, and GLM-4-Plus. Empirical results demonstrate that ChainRAG\nconsistently outperforms baselines in both effectiveness and efficiency.", "published": "2025-02-20 04:19:05", "link": "http://arxiv.org/abs/2502.14245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PaperHelper: Knowledge-Based LLM QA Paper Reading Assistant", "abstract": "In the paper, we introduce a paper reading assistant, PaperHelper, a potent\ntool designed to enhance the capabilities of researchers in efficiently\nbrowsing and understanding scientific literature. Utilizing the\nRetrieval-Augmented Generation (RAG) framework, PaperHelper effectively\nminimizes hallucinations commonly encountered in large language models (LLMs),\noptimizing the extraction of accurate, high-quality knowledge. The\nimplementation of advanced technologies such as RAFT and RAG Fusion\nsignificantly boosts the performance, accuracy, and reliability of the\nLLMs-based literature review process. Additionally, PaperHelper features a\nuser-friendly interface that facilitates the batch downloading of documents and\nuses the Mermaid format to illustrate structural relationships between\ndocuments. Experimental results demonstrate that PaperHelper, based on a\nfine-tuned GPT-4 API, achieves an F1 Score of 60.04, with a latency of only 5.8\nseconds, outperforming the basic RAG model by 7\\% in F1 Score.", "published": "2025-02-20 05:18:00", "link": "http://arxiv.org/abs/2502.14271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vulnerability of Text-to-Image Models to Prompt Template Stealing: A\n  Differential Evolution Approach", "abstract": "Prompt trading has emerged as a significant intellectual property concern in\nrecent years, where vendors entice users by showcasing sample images before\nselling prompt templates that can generate similar images. This work\ninvestigates a critical security vulnerability: attackers can steal prompt\ntemplates using only a limited number of sample images. To investigate this\nthreat, we introduce Prism, a prompt-stealing benchmark consisting of 50\ntemplates and 450 images, organized into Easy and Hard difficulty levels. To\nidentify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a\nnovel template stealing method that operates without model fine-tuning by\nleveraging differential evolution algorithms. The system first initializes\npopulation sets using multimodal large language models (MLLMs) based on\npredefined patterns, then iteratively generates enhanced offspring through\nMLLMs. During evolution, EvoStealer identifies common features across offspring\nto derive generalized templates. Our comprehensive evaluation conducted across\nopen-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini)\ndemonstrates that EvoStealer's stolen templates can reproduce images highly\nsimilar to originals and effectively generalize to other subjects,\nsignificantly outperforming baseline methods with an average improvement of\nover 10%. Moreover, our cost analysis reveals that EvoStealer achieves template\nstealing with negligible computational expenses. Our code and dataset are\navailable at https://github.com/whitepagewu/evostealer.", "published": "2025-02-20 05:52:10", "link": "http://arxiv.org/abs/2502.14285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Drift: Decoding-time Personalized Alignments with Implicit User\n  Preferences", "abstract": "Personalized alignments for individual users have been a long-standing goal\nin large language models (LLMs). We introduce Drift, a novel framework that\npersonalizes LLMs at decoding time with implicit user preferences. Traditional\nReinforcement Learning from Human Feedback (RLHF) requires thousands of\nannotated examples and expensive gradient updates. In contrast, Drift\npersonalizes LLMs in a training-free manner, using only a few dozen examples to\nsteer a frozen model through efficient preference modeling. Our approach models\nuser preferences as a composition of predefined, interpretable attributes and\naligns them at decoding time to enable personalized generation. Experiments on\nboth a synthetic persona dataset (Perspective) and a real human-annotated\ndataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines\nwhile using only 50-100 examples. Our results and analysis show that Drift is\nboth computationally efficient and interpretable.", "published": "2025-02-20 06:05:29", "link": "http://arxiv.org/abs/2502.14289v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in\n  Procedural Text Comprehension", "abstract": "Despite the impressive performance of multilingual large language models\n(mLLMs) in various natural language processing tasks, their ability to\nunderstand procedural texts, particularly those with culture-specific content,\nremains largely unexplored. Texts describing cultural procedures, including\nrituals, traditional craftsmanship, and social etiquette, require an inherent\nunderstanding of cultural context, presenting a significant challenge for\nmLLMs. In this work, we introduce CAPTex, a benchmark designed to evaluate\nmLLMs' ability to process and reason about culturally diverse procedural texts\nacross multiple languages using various methodologies to assess their\nperformance. Our findings indicate that (1) mLLMs face difficulties with\nculturally contextualized procedural texts, showing notable performance\ndeclines in low-resource languages, (2) model performance fluctuates across\ncultural domains, with some areas presenting greater difficulties, and (3)\nlanguage models exhibit better performance on multiple-choice tasks within\nconversational frameworks compared to direct questioning. These results\nunderscore the current limitations of mLLMs in handling culturally nuanced\nprocedural texts and highlight the need for culturally aware benchmarks like\nCAPTex to enhance their adaptability and comprehension across diverse\nlinguistic and cultural landscapes.", "published": "2025-02-20 07:01:08", "link": "http://arxiv.org/abs/2502.14315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation", "abstract": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.", "published": "2025-02-20 07:10:43", "link": "http://arxiv.org/abs/2502.14317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Types in Product Reviews", "abstract": "Information in text is communicated in a way that supports a goal for its\nreader. Product reviews, for example, contain opinions, tips, product\ndescriptions, and many other types of information that provide both direct\ninsights, as well as unexpected signals for downstream applications. We devise\na typology of 24 communicative goals in sentences from the product review\ndomain, and employ a zero-shot multi-label classifier that facilitates\nlarge-scale analyses of review data. In our experiments, we find that the\ncombination of classes in the typology forecasts helpfulness and sentiment of\nreviews, while supplying explanations for these decisions. In addition, our\ntypology enables analysis of review intent, effectiveness and rhetorical\nstructure. Characterizing the types of information in reviews unlocks many\nopportunities for more effective consumption of this genre.", "published": "2025-02-20 07:44:04", "link": "http://arxiv.org/abs/2502.14335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Earlier Tokens Contribute More: Learning Direct Preference Optimization\n  From Temporal Decay Perspective", "abstract": "Direct Preference Optimization (DPO) has gained attention as an efficient\nalternative to reinforcement learning from human feedback (RLHF) for aligning\nlarge language models (LLMs) with human preferences. Despite its advantages,\nDPO suffers from a length bias, generating responses longer than those from the\nreference model. Existing solutions like SimPO and SamPO address this issue but\nuniformly treat the contribution of rewards across sequences, overlooking\ntemporal dynamics. To this end, we propose an enhanced preference optimization\nmethod that incorporates a temporal decay factor controlled by a gamma\nparameter. This dynamic weighting mechanism adjusts the influence of each\nreward based on its position in the sequence, prioritizing earlier tokens that\nare more critical for alignment. By adaptively focusing on more relevant\nfeedback, our approach mitigates overfitting to less pertinent data and remains\nresponsive to evolving human preferences. Experimental results on several\nbenchmarks show that our approach consistently outperforms vanilla DPO by\n5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across\ndifferent model architectures and sizes. Furthermore, additional experiments on\nmathematical and reasoning benchmarks (MMLU, GSM8K, and MATH) confirm that our\nmethod enhances performance without compromising general capabilities. Our\ncodebase would be available at \\url{https://github.com/LotuSrc/D2PO}.", "published": "2025-02-20 07:53:11", "link": "http://arxiv.org/abs/2502.14340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SR-LLM: Rethinking the Structured Representation in Large Language Model", "abstract": "Structured representations, exemplified by Abstract Meaning Representation\n(AMR), have long been pivotal in computational linguistics. However, their role\nremains ambiguous in the Large Language Models (LLMs) era. Initial attempts to\nintegrate structured representation into LLMs via a zero-shot setting yielded\ninferior performance. We hypothesize that such a decline stems from the\nstructure information being passed into LLMs in a code format unfamiliar to\nLLMs' training corpora. Consequently, we propose SR-LLM, an innovative\nframework with two settings to explore a superior way of integrating structured\nrepresentation with LLMs from training-free and training-dependent\nperspectives. The former integrates structural information through natural\nlanguage descriptions in LLM prompts, whereas its counterpart augments the\nmodel's inference capability through fine-tuning on linguistically described\nstructured representations. Performance improvements were observed in widely\ndownstream datasets, with particularly notable gains of 3.17% and 12.38% in\nPAWS. To the best of our knowledge, this work represents the pioneering\ndemonstration that leveraging structural representations can substantially\nenhance LLMs' inference capability. We hope that our work sheds light and\nencourages future research to enhance the reasoning and interoperability of\nLLMs by structure data.", "published": "2025-02-20 08:17:56", "link": "http://arxiv.org/abs/2502.14352v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Full-Step-DPO: Self-Supervised Preference Optimization with Step-wise\n  Rewards for Mathematical Reasoning", "abstract": "Direct Preference Optimization (DPO) often struggles with long-chain\nmathematical reasoning. Existing approaches, such as Step-DPO, typically\nimprove this by focusing on the first erroneous step in the reasoning chain.\nHowever, they overlook all other steps and rely heavily on humans or GPT-4 to\nidentify erroneous steps. To address these issues, we propose Full-Step-DPO, a\nnovel DPO framework tailored for mathematical reasoning. Instead of optimizing\nonly the first erroneous step, it leverages step-wise rewards from the entire\nreasoning chain. This is achieved by training a self-supervised process reward\nmodel, which automatically scores each step, providing rewards while avoiding\nreliance on external signals. Furthermore, we introduce a novel step-wise DPO\nloss, which dynamically updates gradients based on these step-wise rewards.\nThis endows stronger reasoning capabilities to language models. Extensive\nevaluations on both in-domain and out-of-domain mathematical reasoning\nbenchmarks across various base language models, demonstrate that Full-Step-DPO\nachieves superior performance compared to state-of-the-art baselines.", "published": "2025-02-20 08:28:11", "link": "http://arxiv.org/abs/2502.14356v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive\n  Tests", "abstract": "We examine three evaluation paradigms: large question-answering benchmarks\n(e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and\ncognitive tests (e.g., for working memory or theory of mind). First, we\ninvestigate which of the former two-benchmarks or games-is most effective at\ndiscriminating LLMs of varying quality. Then, inspired by human cognitive\nassessments, we compile a suite of targeted tests that measure cognitive\nabilities deemed essential for effective language use, and we investigate their\ncorrelation with model performance in benchmarks and games. Our analyses reveal\nthat interactive games are superior to standard benchmarks in discriminating\nmodels. Causal and logical reasoning correlate with both static and interactive\ntests, while differences emerge regarding core executive functions and\nsocial/emotional skills, which correlate more with games. We advocate the\ndevelopment of new interactive benchmarks and targeted cognitive tasks inspired\nby assessing human abilities but designed specifically for LLMs.", "published": "2025-02-20 08:36:58", "link": "http://arxiv.org/abs/2502.14359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rumor Detection by Multi-task Suffix Learning based on Time-series Dual\n  Sentiments", "abstract": "The widespread dissemination of rumors on social media has a significant\nimpact on people's lives, potentially leading to public panic and fear. Rumors\noften evoke specific sentiments, resonating with readers and prompting sharing.\nTo effectively detect and track rumors, it is essential to observe the\nfine-grained sentiments of both source and response message pairs as the rumor\nevolves over time. However, current rumor detection methods fail to account for\nthis aspect. In this paper, we propose MSuf, the first multi-task suffix\nlearning framework for rumor detection and tracking using time series dual\n(coupled) sentiments. MSuf includes three modules: (1) an LLM to extract\nsentiment intensity features and sort them chronologically; (2) a module that\nfuses the sorted sentiment features with their source text word embeddings to\nobtain an aligned embedding; (3) two hard prompts are combined with the aligned\nvector to perform rumor detection and sentiment analysis using one frozen LLM.\nMSuf effectively enhances the performance of LLMs for rumor detection with only\nminimal parameter fine-tuning. Evaluating MSuf on four rumor detection\nbenchmarks, we find significant improvements compared to other emotion-based\nmethods.", "published": "2025-02-20 09:20:32", "link": "http://arxiv.org/abs/2502.14383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tradutor: Building a Variety Specific Translation Model", "abstract": "Language models have become foundational to many widely used systems.\nHowever, these seemingly advantageous models are double-edged swords. While\nthey excel in tasks related to resource-rich languages like English, they often\nlose the fine nuances of language forms, dialects, and varieties that are\ninherent to languages spoken in multiple regions of the world. Languages like\nEuropean Portuguese are neglected in favor of their more popular counterpart,\nBrazilian Portuguese, leading to suboptimal performance in various linguistic\ntasks. To address this gap, we introduce the first open-source translation\nmodel specifically tailored for European Portuguese, along with a novel dataset\nspecifically designed for this task. Results from automatic evaluations on two\nbenchmark datasets demonstrate that our best model surpasses existing\nopen-source translation systems for Portuguese and approaches the performance\nof industry-leading closed-source systems for European Portuguese. By making\nour dataset, models, and code publicly available, we aim to support and\nencourage further research, fostering advancements in the representation of\nunderrepresented language varieties.", "published": "2025-02-20 09:20:59", "link": "http://arxiv.org/abs/2502.14385v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Portuguese Variety Identification with Cross-Domain Approaches", "abstract": "Recent advances in natural language processing have raised expectations for\ngenerative models to produce coherent text across diverse language varieties.\nIn the particular case of the Portuguese language, the predominance of\nBrazilian Portuguese corpora online introduces linguistic biases in these\nmodels, limiting their applicability outside of Brazil. To address this gap and\npromote the creation of European Portuguese resources, we developed a\ncross-domain language variety identifier (LVI) to discriminate between European\nand Brazilian Portuguese. Motivated by the findings of our literature review,\nwe compiled the PtBrVarId corpus, a cross-domain LVI dataset, and study the\neffectiveness of transformer-based LVI classifiers for cross-domain scenarios.\nAlthough this research focuses on two Portuguese varieties, our contribution\ncan be extended to other varieties and languages. We open source the code,\ncorpus, and models to foster further research in this task.", "published": "2025-02-20 09:31:48", "link": "http://arxiv.org/abs/2502.14394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Data Contamination for Large Language Models", "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated\nsignificant progress in various areas, such as text generation and code\nsynthesis. However, the reliability of performance evaluation has come under\nscrutiny due to data contamination-the unintended overlap between training and\ntest datasets. This overlap has the potential to artificially inflate model\nperformance, as LLMs are typically trained on extensive datasets scraped from\npublicly available sources. These datasets often inadvertently overlap with the\nbenchmarks used for evaluation, leading to an overestimation of the models'\ntrue generalization capabilities. In this paper, we first examine the\ndefinition and impacts of data contamination. Secondly, we review methods for\ncontamination-free evaluation, focusing on three strategies: data\nupdating-based methods, data rewriting-based methods, and prevention-based\nmethods. Specifically, we highlight dynamic benchmarks and LLM-driven\nevaluation methods. Finally, we categorize contamination detecting methods\nbased on model information dependency: white-Box, gray-Box, and black-Box\ndetection approaches. Our survey highlights the requirements for more rigorous\nevaluation protocols and proposes future directions for addressing data\ncontamination challenges.", "published": "2025-02-20 10:23:27", "link": "http://arxiv.org/abs/2502.14425v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Token-Level Density-Based Uncertainty Quantification Methods for\n  Eliciting Truthfulness of Large Language Models", "abstract": "Uncertainty quantification (UQ) is a prominent approach for eliciting\ntruthful answers from large language models (LLMs). To date, information-based\nand consistency-based UQ have been the dominant UQ methods for text generation\nvia LLMs. Density-based methods, despite being very effective for UQ in text\nclassification with encoder-based models, have not been very successful with\ngenerative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a\nwell-established UQ technique in classification tasks - for text generation and\nintroduce a new supervised UQ method. Our method extracts token embeddings from\nmultiple layers of LLMs, computes MD scores for each token, and uses linear\nregression trained on these features to provide robust uncertainty scores.\nThrough extensive experiments on eleven datasets, we demonstrate that our\napproach substantially improves over existing UQ methods, providing accurate\nand computationally efficient uncertainty scores for both sequence-level\nselective generation and claim-level fact-checking tasks. Our method also\nexhibits strong generalization to out-of-domain data, making it suitable for a\nwide range of LLM-based applications.", "published": "2025-02-20 10:25:13", "link": "http://arxiv.org/abs/2502.14427v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Early-Exit and Instant Confidence Translation Quality Estimation", "abstract": "Quality estimation is omnipresent in machine translation, for both evaluation\nand generation. Unfortunately, quality estimation models are often opaque and\ncomputationally expensive, making them impractical to be part of large-scale\npipelines. In this work, we tackle two connected challenges: (1) reducing the\ncost of quality estimation at scale, and (2) developing an inexpensive\nuncertainty estimation method for quality estimation. To address the latter, we\nintroduce Instant Confidence COMET, an uncertainty-aware quality estimation\nmodel that matches the performance of previous approaches at a fraction of\ntheir costs. We extend this to Early-Exit COMET, a quality estimation model\nthat can compute quality scores and associated confidences already at early\nmodel layers, allowing us to early-exit computations and reduce evaluation\ncosts. We also apply our model to machine translation reranking. We combine\nEarly-Exit COMET with an upper confidence bound bandit algorithm to find the\nbest candidate from a large pool without having to run the full evaluation\nmodel on all candidates. In both cases (evaluation and reranking) our methods\nreduce the required compute by 50% with very little degradation in performance.", "published": "2025-02-20 10:27:13", "link": "http://arxiv.org/abs/2502.14429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Generation", "abstract": "This book provides a broad overview of Natural Language Generation (NLG),\nincluding technology, user requirements, evaluation, and real-world\napplications. The focus is on concepts and insights which hopefully will remain\nrelevant for many years, not on the latest LLM innovations. It draws on decades\nof work by the author and others on NLG.\n  The book has the following chapters: Introduction to NLG; Rule-Based NLG;\nMachine Learning and Neural NLG; Requirements; Evaluation; Safety, Maintenance,\nand Testing; and Applications. All chapters include examples and anecdotes from\nthe author's personal experiences, and end with a Further Reading section.\n  The book should be especially useful to people working on applied NLG,\nincluding NLG researchers, people in other fields who want to use NLG, and\ncommercial developers. It will not however be useful to people who want to\nunderstand the latest LLM technology.\n  There is a companion site with more information at\nhttps://ehudreiter.com/book/", "published": "2025-02-20 10:41:34", "link": "http://arxiv.org/abs/2502.14437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Enhancement of Jiang, Z., et al.s Compression-Based Classification\n  Algorithm Applied to News Article Categorization", "abstract": "This study enhances Jiang et al.'s compression-based classification algorithm\nby addressing its limitations in detecting semantic similarities between text\ndocuments. The proposed improvements focus on unigram extraction and optimized\nconcatenation, eliminating reliance on entire document compression. By\ncompressing extracted unigrams, the algorithm mitigates sliding window\nlimitations inherent to gzip, improving compression efficiency and similarity\ndetection. The optimized concatenation strategy replaces direct concatenation\nwith the union of unigrams, reducing redundancy and enhancing the accuracy of\nNormalized Compression Distance (NCD) calculations. Experimental results across\ndatasets of varying sizes and complexities demonstrate an average accuracy\nimprovement of 5.73%, with gains of up to 11% on datasets containing longer\ndocuments. Notably, these improvements are more pronounced in datasets with\nhigh-label diversity and complex text structures. The methodology achieves\nthese results while maintaining computational efficiency, making it suitable\nfor resource-constrained environments. This study provides a robust, scalable\nsolution for text classification, emphasizing lightweight preprocessing\ntechniques to achieve efficient compression, which in turn enables more\naccurate classification.", "published": "2025-02-20 10:50:59", "link": "http://arxiv.org/abs/2502.14444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimal word order for non-causal text generation with Large Language\n  Models: the Spanish case", "abstract": "Natural Language Generation (NLG) popularity has increased owing to the\nprogress in Large Language Models (LLMs), with zero-shot inference\ncapabilities. However, most neural systems utilize decoder-only causal\n(unidirectional) transformer models, which are effective for English but may\nreduce the richness of languages with less strict word order, subject omission,\nor different relative clause attachment preferences. This is the first work\nthat analytically addresses optimal text generation order for non-causal\nlanguage models. We present a novel Viterbi algorithm-based methodology for\nmaximum likelihood word order estimation. We analyze the non-causal\nmost-likelihood order probability for NLG in Spanish and, then, the probability\nof generating the same phrases with Spanish causal NLG. This comparative\nanalysis reveals that causal NLG prefers English-like SVO structures. We also\nanalyze the relationship between optimal generation order and causal\nleft-to-right generation order using Spearman's rank correlation. Our results\ndemonstrate that the ideal order predicted by the maximum likelihood estimator\nis not closely related to the causal order and may be influenced by the\nsyntactic structure of the target sentence.", "published": "2025-02-20 11:06:16", "link": "http://arxiv.org/abs/2502.14451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Argument-Based Comparative Question Answering Evaluation Benchmark", "abstract": "In this paper, we aim to solve the problems standing in the way of automatic\ncomparative question answering. To this end, we propose an evaluation framework\nto assess the quality of comparative question answering summaries. We formulate\n15 criteria for assessing comparative answers created using manual annotation\nand annotation from 6 large language models and two comparative question\nasnwering datasets. We perform our tests using several LLMs and manual\nannotation under different settings and demonstrate the constituency of both\nevaluations. Our results demonstrate that the Llama-3 70B Instruct model\ndemonstrates the best results for summary evaluation, while GPT-4 is the best\nfor answering comparative questions. All used data, code, and evaluation\nresults are publicly\navailable\\footnote{\\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}.", "published": "2025-02-20 11:52:26", "link": "http://arxiv.org/abs/2502.14476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unshackling Context Length: An Efficient Selective Attention Approach\n  through Query-Key Compression", "abstract": "Handling long-context sequences efficiently remains a significant challenge\nin large language models (LLMs). Existing methods for token selection in\nsequence extrapolation either employ a permanent eviction strategy or select\ntokens by chunk, which may lead to the loss of critical information. We propose\nEfficient Selective Attention (ESA), a novel approach that extends context\nlength by efficiently selecting the most critical tokens at the token level to\ncompute attention. ESA reduces the computational complexity of token selection\nby compressing query and key vectors into lower-dimensional representations. We\nevaluate ESA on long sequence benchmarks with maximum lengths up to 256k using\nopen-source LLMs with context lengths of 8k and 32k. ESA outperforms other\nselective attention methods, especially in tasks requiring the retrieval of\nmultiple pieces of information, achieving comparable performance to\nfull-attention extrapolation methods across various tasks, with superior\nresults in certain tasks.", "published": "2025-02-20 11:52:36", "link": "http://arxiv.org/abs/2502.14477v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLoRA: Nystr\u00f6m-Initiated Low-Rank Adaptation for Large Language Models", "abstract": "Parameter-efficient fine-tuning (PEFT) is essential for adapting large\nlanguage models (LLMs), with low-rank adaptation (LoRA) being the most popular\napproach. However, LoRA suffers from slow convergence, and some recent LoRA\nvariants, such as PiSSA, primarily rely on Singular Value Decomposition (SVD)\nfor initialization, leading to expensive computation. To mitigate these\nproblems, we use the Nystr\\\"om method, which follows a three-matrix\nmanipulation. We first introduce StructuredLoRA (SLoRA), which investigates\nadding a small intermediate matrix between the low-rank matrices A and B.\nSecondly, we propose Nystr\\\"omLoRA (NLoRA), which leverages Nystr\\\"om-based\ninitialization for SLoRA to improve its effectiveness and efficiency. Finally,\nwe propose IntermediateTune (IntTune), which explores fine-tuning exclusively\non the intermediate matrix of NLoRA to further boost LLM efficiency. We\nevaluate our methods on five natural language generation (NLG) tasks and eight\nnatural language understanding (NLU) tasks. On GSM8K, SLoRA and NLoRA achieve\naccuracies of 56.48% and 57.70%, surpassing LoRA by 33.52% and 36.41%, with\nonly 3.67 million additional trainable parameters. IntTune improves average NLG\nperformance over LoRA by 7.45% while using only 1.25% of its parameters. These\nresults demonstrate the efficiency and effectiveness of our approach in\nenhancing model performance with minimal parameter overhead.", "published": "2025-02-20 12:01:11", "link": "http://arxiv.org/abs/2502.14482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction\n  Following", "abstract": "Multi-turn instruction following capability constitutes a core competency of\nlarge language models (LLMs) in real-world applications. Existing evaluation\nbenchmarks predominantly focus on fine-grained constraint satisfaction and\ndomain-specific capability assessment, yet overlook the crucial structural\ndependency between dialogue turns that distinguishes multi-turn from\nsingle-turn interactions. This structural dependency not only reflects user\nintent but also establishes a second dimension for instruction following\nevaluation beyond constraint satisfaction. To address this gap, we propose\nStructFlowBench, a multi-turn instruction following benchmark with structural\nflow modeling. The benchmark innovatively defines a structural flow framework\ncomprising six fundamental inter-turn relationships, which not only introduces\nnovel structural constraints for model evaluation but also serves as generation\nparameters for creating customized dialogue flows tailored to specific\nscenarios. Adopting established LLM-based automatic evaluation methodologies,\nwe conduct systematic evaluations of 13 leading open-source and closed-source\nLLMs. Experimental results reveal significant deficiencies in current models'\ncomprehension of multi-turn dialogue structures. The code is available at\n\\url{https://github.com/MLGroupJLU/StructFlowBench}.", "published": "2025-02-20 12:22:18", "link": "http://arxiv.org/abs/2502.14494v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Language Multi-Agent Learning with Multi-Agent Credit\n  Re-Assignment for Interactive Environment Generalization", "abstract": "LLM-based agents have made significant advancements in interactive\nenvironments, such as mobile operations and web browsing, and other domains\nbeyond computer using. Current multi-agent systems universally excel in\nperformance, compared to single agents, but struggle with generalization across\nenvironments due to predefined roles and inadequate strategies for generalizing\nlanguage agents. The challenge of achieving both strong performance and good\ngeneralization has hindered the progress of multi-agent systems for interactive\nenvironments. To address these issues, we propose CollabUIAgents, a multi-agent\nreinforcement learning framework with a novel multi-agent credit re-assignment\n(CR) strategy, assigning process rewards with LLMs rather than\nenvironment-specific rewards and learning with synthesized preference data, in\norder to foster generalizable, collaborative behaviors among the role-free\nagents' policies. Empirical results show that our framework improves both\nperformance and cross-environment generalizability of multi-agent systems.\nMoreover, our 7B-parameter system achieves results on par with or exceed strong\nclosed-source models, and the LLM that guides the CR. We also provide insights\nin using granular CR rewards effectively for environment generalization, and\naccommodating trained LLMs in multi-agent systems.", "published": "2025-02-20 12:26:15", "link": "http://arxiv.org/abs/2502.14496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Perspectivist Turn in Argument Quality Assessment", "abstract": "The assessment of argument quality depends on well-established logical,\nrhetorical, and dialectical properties that are unavoidably subjective:\nmultiple valid assessments may exist, there is no unequivocal ground truth.\nThis aligns with recent paths in machine learning, which embrace the\nco-existence of different perspectives. However, this potential remains largely\nunexplored in NLP research on argument quality. One crucial reason seems to be\nthe yet unexplored availability of suitable datasets. We fill this gap by\nconducting a systematic review of argument quality datasets. We assign them to\na multi-layered categorization targeting two aspects: (a) What has been\nannotated: we collect the quality dimensions covered in datasets and\nconsolidate them in an overarching taxonomy, increasing dataset comparability\nand interoperability. (b) Who annotated: we survey what information is given\nabout annotators, enabling perspectivist research and grounding our\nrecommendations for future actions. To this end, we discuss datasets suitable\nfor developing perspectivist models (i.e., those containing individual,\nnon-aggregated annotations), and we showcase the importance of a controlled\nselection of annotators in a pilot study.", "published": "2025-02-20 12:30:26", "link": "http://arxiv.org/abs/2502.14501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?", "abstract": "The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities.", "published": "2025-02-20 12:31:03", "link": "http://arxiv.org/abs/2502.14502v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis\n  of L1-Dependent Biases", "abstract": "This study evaluates Large Language Models' (LLMs) ability to simulate\nnon-native-like English use observed in human second language (L2) learners\ninterfered with by their native first language (L1). In dialogue-based\ninterviews, we prompt LLMs to mimic L2 English learners with specific L1s\n(e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to\nreal L2 learner data. Our analysis examines L1-driven linguistic biases, such\nas reference word usage and avoidance behaviors, using information-theoretic\nand distributional density measures. Results show that modern LLMs (e.g.,\nQwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed\nin human L2 data, with distinct influences from various languages (e.g.,\nJapanese, Korean, and Mandarin significantly affect tense agreement, and Urdu\ninfluences noun-verb collocations). Our results reveal the potential of LLMs\nfor L2 dialogue generation and evaluation for future educational applications.", "published": "2025-02-20 12:34:46", "link": "http://arxiv.org/abs/2502.14507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiSlav: Using Cross-Lingual Knowledge Transfer to Combat the Curse of\n  Multilinguality", "abstract": "Does multilingual Neural Machine Translation (NMT) lead to The Curse of the\nMultlinguality or provides the Cross-lingual Knowledge Transfer within a\nlanguage family? In this study, we explore multiple approaches for extending\nthe available data-regime in NMT and we prove cross-lingual benefits even in\n0-shot translation regime for low-resource languages. With this paper, we\nprovide state-of-the-art open-source NMT models for translating between\nselected Slavic languages. We released our models on the HuggingFace Hub\n(https://hf.co/collections/allegro/multislav-6793d6b6419e5963e759a683) under\nthe CC BY 4.0 license. Slavic language family comprises morphologically rich\nCentral and Eastern European languages. Although counting hundreds of millions\nof native speakers, Slavic Neural Machine Translation is under-studied in our\nopinion. Recently, most NMT research focuses either on: high-resource languages\nlike English, Spanish, and German - in WMT23 General Translation Task 7 out of\n8 task directions are from or to English; massively multilingual models\ncovering multiple language groups; or evaluation techniques.", "published": "2025-02-20 12:35:25", "link": "http://arxiv.org/abs/2502.14509v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via\n  Gradient-Guided Perturbation Optimization", "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing, but their full fine-tuning remains resource-intensive.\nParameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), have emerged as a practical solution by approximating parameter updates\nwith low-rank matrices. However, LoRA often exhibits a \"double descent\"\nphenomenon during fine-tuning, where model performance degrades due to\noverfitting and limited expressiveness caused by low-rank constraints. To\naddress this issue, we propose LoRA-GGPO (Gradient-Guided Perturbation\nOptimization), a novel method that leverages gradient and weight norms to\ngenerate targeted perturbations. By optimizing the sharpness of the loss\nlandscape, LoRA-GGPO guides the model toward flatter minima, mitigating the\ndouble descent problem and improving generalization. Extensive experiments on\nnatural language understanding (NLU) and generation (NLG) tasks demonstrate\nthat LoRA-GGPO outperforms LoRA and its state-of-the-art variants. Furthermore,\nextended experiments specifically designed to analyze the double descent\nphenomenon confirm that LoRA-GGPO effectively alleviates this issue, producing\nmore robust and generalizable models. Our work provides a robust and efficient\nsolution for fine-tuning LLMs, with broad applicability in real-world\nscenarios. The code is available at https://github.com/llm172/LoRA-GGPO.", "published": "2025-02-20 13:14:41", "link": "http://arxiv.org/abs/2502.14538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-based User Profile Management for Recommender System", "abstract": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations.", "published": "2025-02-20 13:20:19", "link": "http://arxiv.org/abs/2502.14541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Behavioral Analysis of Information Salience in Large Language Models", "abstract": "Large Language Models (LLMs) excel at text summarization, a task that\nrequires models to select content based on its importance. However, the exact\nnotion of salience that LLMs have internalized remains unclear. To bridge this\ngap, we introduce an explainable framework to systematically derive and\ninvestigate information salience in LLMs through their summarization behavior.\nUsing length-controlled summarization as a behavioral probe into the content\nselection process, and tracing the answerability of Questions Under Discussion\nthroughout, we derive a proxy for how models prioritize information. Our\nexperiments on 13 models across four datasets reveal that LLMs have a nuanced,\nhierarchical notion of salience, generally consistent across model families and\nsizes. While models show highly consistent behavior and hence salience\npatterns, this notion of salience cannot be accessed through introspection, and\nonly weakly correlates with human perceptions of information salience.", "published": "2025-02-20 14:52:23", "link": "http://arxiv.org/abs/2502.14613v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FIND: Fine-grained Information Density Guided Adaptive\n  Retrieval-Augmented Generation for Disease Diagnosis", "abstract": "Retrieval-Augmented Large Language Models (LLMs), which integrate external\nknowledge into LLMs, have shown remarkable performance in various medical\ndomains, including clinical diagnosis. However, existing RAG methods struggle\nto effectively assess task difficulty to make retrieval decisions, thereby\nfailing to meet the clinical requirements for balancing efficiency and\naccuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained\n\\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework\nthat improves the reliability of RAG in disease diagnosis scenarios. FIND\nincorporates a fine-grained adaptive control module to determine whether\nretrieval is necessary based on the information density of the input. By\noptimizing the retrieval process and implementing a knowledge filtering module,\nFIND ensures that the retrieval is better suited to clinical scenarios.\nExperiments on three Chinese electronic medical record datasets demonstrate\nthat FIND significantly outperforms various baseline methods, highlighting its\neffectiveness in clinical diagnosis tasks.", "published": "2025-02-20 14:52:36", "link": "http://arxiv.org/abs/2502.14614v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Far are LLMs from Being Our Digital Twins? A Benchmark for\n  Persona-Based Behavior Chain Simulation", "abstract": "Recently, LLMs have garnered increasing attention across academic disciplines\nfor their potential as human digital twins, virtual proxies designed to\nreplicate individuals and autonomously perform tasks such as decision-making,\nproblem-solving, and reasoning on their behalf. However, current evaluations of\nLLMs primarily emphasize dialogue simulation while overlooking human behavior\nsimulation, which is crucial for digital twins. To address this gap, we\nintroduce BehaviorChain, the first benchmark for evaluating LLMs' ability to\nsimulate continuous human behavior. BehaviorChain comprises diverse,\nhigh-quality, persona-based behavior chains, totaling 15,846 distinct behaviors\nacross 1,001 unique personas, each with detailed history and profile metadata.\nFor evaluation, we integrate persona metadata into LLMs and employ them to\niteratively infer contextually appropriate behaviors within dynamic scenarios\nprovided by BehaviorChain. Comprehensive evaluation results demonstrated that\neven state-of-the-art models struggle with accurately simulating continuous\nhuman behavior.", "published": "2025-02-20 15:29:32", "link": "http://arxiv.org/abs/2502.14642v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Length-Controlled Margin-Based Preference Optimization without Reference\n  Model", "abstract": "Direct Preference Optimization (DPO) is a widely adopted offline algorithm\nfor preference-based reinforcement learning from human feedback (RLHF),\ndesigned to improve training simplicity and stability by redefining reward\nfunctions. However, DPO is hindered by several limitations, including length\nbias, memory inefficiency, and probability degradation. To address these\nchallenges, we propose Length-Controlled Margin-Based Preference Optimization\n(LMPO), a more efficient and robust alternative. LMPO introduces a uniform\nreference model as an upper bound for the DPO loss, enabling a more accurate\napproximation of the original optimization objective. Additionally, an average\nlog-probability optimization strategy is employed to minimize discrepancies\nbetween training and inference phases. A key innovation of LMPO lies in its\nLength-Controlled Margin-Based loss function, integrated within the\nBradley-Terry framework. This loss function regulates response length while\nsimultaneously widening the margin between preferred and rejected outputs. By\ndoing so, it mitigates probability degradation for both accepted and discarded\nresponses, addressing a significant limitation of existing methods. We evaluate\nLMPO against state-of-the-art preference optimization techniques on two\nopen-ended large language models, Mistral and LLaMA3, across six conditional\nbenchmarks. Our experimental results demonstrate that LMPO effectively controls\nresponse length, reduces probability degradation, and outperforms existing\napproaches. The code is available at \\url{https://github.com/gengxuli/LMPO}.", "published": "2025-02-20 15:30:27", "link": "http://arxiv.org/abs/2502.14643v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIFT: Improving Long Context Understanding of Large Language Models\n  through Long Input Fine-Tuning", "abstract": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research.", "published": "2025-02-20 15:32:24", "link": "http://arxiv.org/abs/2502.14644v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning.", "published": "2025-02-20 16:05:18", "link": "http://arxiv.org/abs/2502.14669v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Get Your LLM to Generate Challenging Problems for Evaluation", "abstract": "The pace of evolution of Large Language Models (LLMs) necessitates new\napproaches for rigorous and comprehensive evaluation. Traditional human\nannotation is increasingly impracticable due to the complexities and costs\ninvolved in generating high-quality, challenging problems. In this work, we\nintroduce CHASE, a unified framework to synthetically generate challenging\nproblems using LLMs without human involvement. For a given task, our approach\nbuilds a hard problem in a bottom-up manner from simpler components. Moreover,\nour framework decomposes the generation process into independently verifiable\nsub-tasks, thereby ensuring a high level of quality and correctness. We\nimplement CHASE to create evaluation benchmarks across three diverse domains:\n(1) document-based question answering, (2) repository-level code completion,\nand (3) math reasoning. The performance of state-of-the-art LLMs on these\nsynthetic benchmarks lies in the range of 40-60% accuracy, thereby\ndemonstrating the effectiveness of our framework at generating challenging\nproblems. We publicly release our benchmarks and code.", "published": "2025-02-20 16:09:55", "link": "http://arxiv.org/abs/2502.14678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap: Transforming Natural Language Questions into SQL\n  Queries via Abstract Query Pattern and Contextual Schema Markup", "abstract": "Large language models have demonstrated excellent performance in many tasks,\nincluding Text-to-SQL, due to their powerful in-context learning capabilities.\nThey are becoming the mainstream approach for Text-to-SQL. However, these\nmethods still have a significant gap compared to human performance, especially\non complex questions. As the complexity of questions increases, the gap between\nquestions and SQLs increases. We identify two important gaps: the structural\nmapping gap and the lexical mapping gap. To tackle these two gaps, we propose\nPAS-SQL, an efficient SQL generation pipeline based on LLMs, which alleviates\ngaps through Abstract Query Pattern (AQP) and Contextual Schema Markup (CSM).\nAQP aims to obtain the structural pattern of the question by removing\ndatabase-related information, which enables us to find structurally similar\ndemonstrations. CSM aims to associate database-related text span in the\nquestion with specific tables or columns in the database, which alleviates the\nlexical mapping gap. Experimental results on the Spider and BIRD datasets\ndemonstrate the effectiveness of our proposed method. Specifically, PAS-SQL +\nGPT-4o sets a new state-of-the-art on the Spider benchmark with an execution\naccuracy of 87.9\\%, and achieves leading results on the BIRD dataset with an\nexecution accuracy of 64.67\\%.", "published": "2025-02-20 16:11:27", "link": "http://arxiv.org/abs/2502.14682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree\n  Search", "abstract": "Recent advancements in large language models (LLMs) have shown remarkable\npotential in automating machine learning tasks. However, existing LLM-based\nagents often struggle with low-diversity and suboptimal code generation. While\nrecent work has introduced Monte Carlo Tree Search (MCTS) to address these\nissues, limitations persist in the quality and diversity of thoughts generated,\nas well as in the scalar value feedback mechanisms used for node selection. In\nthis study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a\nnovel approach that iteratively expands tree nodes through an introspective\nprocess that meticulously analyzes solutions and results from parent and\nsibling nodes. This facilitates a continuous refinement of the node in the\nsearch tree, thereby enhancing the overall decision-making process.\nFurthermore, we integrate a Large Language Model (LLM)-based value model to\nfacilitate direct evaluation of each node's solution prior to conducting\ncomprehensive computational rollouts. A hybrid rewarding mechanism is\nimplemented to seamlessly transition the Q-value from LLM-estimated scores to\nactual performance scores. This allows higher-quality nodes to be traversed\nearlier. Applied to the various ML tasks, our approach demonstrates a 6%\nabsolute improvement in performance compared to the strong open-source AutoML\nagents, showcasing its effectiveness in enhancing agentic AutoML systems.\nResource available at https://github.com/jokieleung/I-MCTS", "published": "2025-02-20 16:19:09", "link": "http://arxiv.org/abs/2502.14693v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Framing and Role Portrayal in the News", "abstract": "We introduce a novel multilingual hierarchical corpus annotated for entity\nframing and role portrayal in news articles. The dataset uses a unique taxonomy\ninspired by storytelling elements, comprising 22 fine-grained roles, or\narchetypes, nested within three main categories: protagonist, antagonist, and\ninnocent. Each archetype is carefully defined, capturing nuanced portrayals of\nentities such as guardian, martyr, and underdog for protagonists; tyrant,\ndeceiver, and bigot for antagonists; and victim, scapegoat, and exploited for\ninnocents. The dataset includes 1,378 recent news articles in five languages\n(Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two\ncritical domains of global significance: the Ukraine-Russia War and Climate\nChange. Over 5,800 entity mentions have been annotated with role labels. This\ndataset serves as a valuable resource for research into role portrayal and has\nbroader implications for news analysis. We describe the characteristics of the\ndataset and the annotation process, and we report evaluation results on\nfine-tuned state-of-the-art multilingual transformers and hierarchical\nzero-shot learning using LLMs at the level of a document, a paragraph, and a\nsentence.", "published": "2025-02-20 16:44:46", "link": "http://arxiv.org/abs/2502.14718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Smith: Formally Controllable Text Transformation and its\n  Application to Evaluation of Text Embedding Models", "abstract": "We propose the Sentence Smith framework that enables controlled and specified\nmanipulation of text meaning. It consists of three main steps: 1. Parsing a\nsentence into a semantic graph, 2. Applying human-designed semantic\nmanipulation rules, and 3. Generating text from the manipulated graph. A final\nfiltering step (4.) ensures the validity of the applied transformation. To\ndemonstrate the utility of Sentence Smith in an application study, we use it to\ngenerate hard negative pairs that challenge text embedding models. Since the\ncontrollable generation makes it possible to clearly isolate different types of\nsemantic shifts, we can gain deeper insights into the specific strengths and\nweaknesses of widely used text embedding models, also addressing an issue in\ncurrent benchmarking where linguistic phenomena remain opaque. Human validation\nconfirms that the generations produced by Sentence Smith are highly accurate.", "published": "2025-02-20 17:00:19", "link": "http://arxiv.org/abs/2502.14734v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines", "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.", "published": "2025-02-20 17:05:58", "link": "http://arxiv.org/abs/2502.14739v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language\n  Models via Monitoring Hidden States", "abstract": "The integration of additional modalities increases the susceptibility of\nlarge vision-language models (LVLMs) to safety risks, such as jailbreak\nattacks, compared to their language-only counterparts. While existing research\nprimarily focuses on post-hoc alignment techniques, the underlying safety\nmechanisms within LVLMs remain largely unexplored. In this work , we\ninvestigate whether LVLMs inherently encode safety-relevant signals within\ntheir internal activations during inference. Our findings reveal that LVLMs\nexhibit distinct activation patterns when processing unsafe prompts, which can\nbe leveraged to detect and mitigate adversarial inputs without requiring\nextensive fine-tuning. Building on this insight, we introduce HiddenDetect, a\nnovel tuning-free framework that harnesses internal model activations to\nenhance safety. Experimental results show that {HiddenDetect} surpasses\nstate-of-the-art methods in detecting jailbreak attacks against LVLMs. By\nutilizing intrinsic safety-aware patterns, our method provides an efficient and\nscalable solution for strengthening LVLM robustness against multimodal threats.\nOur code will be released publicly at\nhttps://github.com/leigest519/HiddenDetect.", "published": "2025-02-20 17:14:34", "link": "http://arxiv.org/abs/2502.14744v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Struggle to Describe the Haystack without Human\n  Help: Human-in-the-loop Evaluation of LLMs", "abstract": "A common use of NLP is to facilitate the understanding of large document\ncollections, with a shift from using traditional topic models to Large Language\nModels. Yet the effectiveness of using LLM for large corpus understanding in\nreal-world applications remains under-explored. This study measures the\nknowledge users acquire with unsupervised, supervised LLM-based exploratory\napproaches or traditional topic models on two datasets. While LLM-based methods\ngenerate more human-readable topics and show higher average win probabilities\nthan traditional models for data exploration, they produce overly generic\ntopics for domain-specific datasets that do not easily allow users to learn\nmuch about the documents. Adding human supervision to the LLM generation\nprocess improves data exploration by mitigating hallucination and\nover-genericity but requires greater human effort. In contrast, traditional.\nmodels like Latent Dirichlet Allocation (LDA) remain effective for exploration\nbut are less user-friendly. We show that LLMs struggle to describe the haystack\nof large corpora without human help, particularly domain-specific data, and\nface scaling and hallucination limitations due to context length constraints.\nDataset available at https://huggingface. co/datasets/zli12321/Bills.", "published": "2025-02-20 17:19:41", "link": "http://arxiv.org/abs/2502.14748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SurveyX: Academic Survey Automation via Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn", "published": "2025-02-20 17:59:45", "link": "http://arxiv.org/abs/2502.14776v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Faithfulness of Chains of Thought by Unlearning Reasoning\n  Steps", "abstract": "When prompted to think step-by-step, language models (LMs) produce a chain of\nthought (CoT), a sequence of reasoning steps that the model supposedly used to\nproduce its prediction. However, despite much work on CoT prompting, it is\nunclear if CoT reasoning is faithful to the models' parameteric beliefs. We\nintroduce a framework for measuring parametric faithfulness of generated\nreasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an\ninstance of this framework. FUR erases information contained in reasoning steps\nfrom model parameters. We perform experiments unlearning CoTs of four LMs\nprompted on four multi-choice question answering (MCQA) datasets. Our\nexperiments show that FUR is frequently able to change the underlying models'\nprediction by unlearning key steps, indicating when a CoT is parametrically\nfaithful. Further analysis shows that CoTs generated by models post-unlearning\nsupport different answers, hinting at a deeper effect of unlearning.\nImportantly, CoT steps identified as important by FUR do not align well with\nhuman notions of plausbility, emphasizing the need for specialized alignment", "published": "2025-02-20 18:45:05", "link": "http://arxiv.org/abs/2502.14829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks", "abstract": "Large Language Models (LLMs) have shown great promise in tool-making, yet\nexisting frameworks often struggle to efficiently construct reliable toolsets\nand are limited to single-task settings. To address these challenges, we\npropose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that\ndynamically constructs and evolves a hierarchical graph of reusable tools\nacross multiple scenarios. We evaluate GATE on open-ended tasks (Minecraft),\nagent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date,\nTabMWP). Our results show that GATE achieves up to 4.3x faster milestone\ncompletion in Minecraft compared to the previous SOTA, and provides an average\nimprovement of 9.23% over existing tool-making methods in code generation tasks\nand 10.03% in agent tasks. GATE demonstrates the power of adaptive evolution,\nbalancing tool quantity, complexity, and functionality while maintaining high\nefficiency. Code and data are available at\n\\url{https://github.com/ayanami2003/GATE}.", "published": "2025-02-20 18:56:03", "link": "http://arxiv.org/abs/2502.14848v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CLIPPER: Compression enables long-context synthetic data generation", "abstract": "LLM developers are increasingly reliant on synthetic data, but generating\nhigh-quality data for complex long-context reasoning tasks remains challenging.\nWe introduce CLIPPER, a compression-based approach for generating synthetic\ndata tailored to narrative claim verification - a task that requires reasoning\nover a book to verify a given claim. Instead of generating claims directly from\nthe raw text of the book, which results in artifact-riddled claims, CLIPPER\nfirst compresses the book into chapter outlines and book summaries and then\nuses these intermediate representations to generate complex claims and\ncorresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces\nclaims that are more valid, grounded, and complex. Using CLIPPER, we construct\na dataset of 19K synthetic book claims paired with their source texts and\nchain-of-thought reasoning, and use it to fine-tune three open-weight models.\nOur best model achieves breakthrough results on narrative claim verification\n(from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for\nsub-10B models on the NoCha leaderboard. Further analysis shows that our models\ngenerate more detailed and grounded chain-of-thought reasoning while also\nimproving performance on other narrative understanding tasks (e.g.,\nNarrativeQA).", "published": "2025-02-20 18:58:03", "link": "http://arxiv.org/abs/2502.14854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning", "abstract": "Large language models (LLMs) often fail to ask effective questions under\nuncertainty, making them unreliable in domains where proactive\ninformation-gathering is essential for decisionmaking. We present ALFA, a\nframework that improves LLM question-asking by (i) decomposing the notion of a\n\"good\" question into a set of theory-grounded attributes (e.g., clarity,\nrelevance), (ii) controllably synthesizing attribute-specific question\nvariations, and (iii) aligning models via preference-based optimization to\nexplicitly learn to ask better questions along these fine-grained attributes.\nFocusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs\ndataset, composed of 17k real-world clinical interactions augmented with 80k\nattribute-specific preference pairs of follow-up questions, as well as a novel\nexpert-annotated interactive healthcare QA task to evaluate question-asking\nabilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on\nMediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level\nwin-rate of 64.4% and strong generalizability. Our findings suggest that\nexplicitly guiding question-asking with structured, fine-grained attributes\noffers a scalable path to improve LLMs, especially in expert application\ndomains.", "published": "2025-02-20 18:59:31", "link": "http://arxiv.org/abs/2502.14860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Retrieve and Reason on Knowledge Graph through Active\n  Self-Reflection", "abstract": "Extensive research has investigated the integration of large language models\n(LLMs) with knowledge graphs to enhance the reasoning process. However,\nunderstanding how models perform reasoning utilizing structured graph knowledge\nremains underexplored. Most existing approaches rely on LLMs or retrievers to\nmake binary judgments regarding the utilization of knowledge, which is too\ncoarse. Meanwhile, there is still a lack of feedback mechanisms for reflection\nand correction throughout the entire reasoning path. This paper proposes an\nActive self-Reflection framework for knowledge Graph reasoning ARG, introducing\nfor the first time an end-to-end training approach to achieve iterative\nreasoning grounded on structured graphs. Within the framework, the model\nleverages special tokens to \\textit{actively} determine whether knowledge\nretrieval is necessary, performs \\textit{reflective} critique based on the\nretrieved knowledge, and iteratively reasons over the knowledge graph. The\nreasoning paths generated by the model exhibit high interpretability, enabling\ndeeper exploration of the model's understanding of structured knowledge.\nUltimately, the proposed model achieves outstanding results compared to\nexisting baselines in knowledge graph reasoning tasks.", "published": "2025-02-20 06:38:48", "link": "http://arxiv.org/abs/2502.14932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in Space: Optimizing Tokens for Grammar-Constrained Decoding", "abstract": "General-purpose language models are trained to produce varied natural\nlanguage outputs, but for some tasks like annotation or classification we need\nmore specific output formats. LLM systems increasingly support structured\noutput, sampling tokens according to a grammar, which enforces a format but\nwhich can also reduce performance. We ask whether there are systematic\ndifferences between grammars that appear semantically similar to humans. To\nanswer this question, we test four popular model families with five token\nformats on four NLP benchmarks. All models perform most accurately when\ninstructed to classify with real numbers. Performance also improves by 5%-10%\nwhen models are instructed to return tokens incorporating leading whitespace,\nwhich we find can help models avoid structural deficiencies in subword token\nrepresentations. Format-based differences are largest for smaller models that\nare often used for local laptop-scale inference. We present best practices for\nresearchers using language models as zero-shot classifiers with structured\noutput.", "published": "2025-02-20 19:06:18", "link": "http://arxiv.org/abs/2502.14969v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualizing Search Queries In-Context Learning for Conversational\n  Rewriting with LLMs", "abstract": "Conversational query rewriting is crucial for effective conversational\nsearch, yet traditional supervised methods require substantial labeled data,\nwhich is scarce in low-resource settings. This paper introduces Prompt-Guided\nIn-Context Learning, a novel approach that leverages the in-context learning\ncapabilities of Large Language Models (LLMs) for few-shot conversational query\nrewriting. Our method employs carefully designed prompts, incorporating task\ndescriptions, input/output format specifications, and a small set of\nillustrative examples, to guide pre-trained LLMs to generate\ncontext-independent queries without explicit fine-tuning. Extensive experiments\non benchmark datasets, TREC and Taskmaster-1, demonstrate that our approach\nsignificantly outperforms strong baselines, including supervised models and\ncontrastive co-training methods, across various evaluation metrics such as\nBLEU, ROUGE-L, Success Rate, and MRR. Ablation studies confirm the importance\nof in-context examples, and human evaluations further validate the superior\nfluency, relevance, and context utilization of our generated rewrites. The\nresults highlight the potential of prompt-guided in-context learning as an\nefficient and effective paradigm for low-resource conversational query\nrewriting, reducing the reliance on extensive labeled data and complex training\nprocedures.", "published": "2025-02-20 20:02:42", "link": "http://arxiv.org/abs/2502.15009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using tournaments to calculate AUROC for zero-shot classification with\n  LLMs", "abstract": "Large language models perform surprisingly well on many zero-shot\nclassification tasks, but are difficult to fairly compare to supervised\nclassifiers due to the lack of a modifiable decision boundary. In this work, we\npropose and evaluate a method that converts binary classification tasks into\npairwise comparison tasks, obtaining relative rankings from LLMs. Repeated\npairwise comparisons can be used to score instances using the Elo rating system\n(used in chess and other competitions), inducing a confidence ordering over\ninstances in a dataset. We evaluate scheduling algorithms for their ability to\nminimize comparisons, and show that our proposed algorithm leads to improved\nclassification performance, while also providing more information than\ntraditional zero-shot classification.", "published": "2025-02-20 20:13:20", "link": "http://arxiv.org/abs/2502.15018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Meta-Evaluation of Style and Attribute Transfer Metrics", "abstract": "LLMs make it easy to rewrite text in any style, be it more polite,\npersuasive, or more positive. We present a large-scale study of evaluation\nmetrics for style and attribute transfer with a focus on content preservation;\nmeaning content not attributed to the style shift is preserved. The de facto\nevaluation approach uses lexical or semantic similarity metrics often between\nsource sentences and rewrites. While these metrics are not designed to\ndistinguish between style or content differences, empirical meta-evaluation\nshows a reasonable correlation to human judgment. In fact, recent works find\nthat LLMs prompted as evaluators are only comparable to semantic similarity\nmetrics, even though intuitively, the LLM approach should better fit the task.\nTo investigate this discrepancy, we benchmark 8 metrics for evaluating content\npreservation on existing datasets and additionally construct a new test set\nthat better aligns with the meta-evaluation aim. Indeed, we then find that the\nempirical conclusion aligns with the intuition: content preservation metrics\nfor style/attribute transfer must be conditional on the style shift. To support\nthis, we propose a new efficient zero-shot evaluation method using the\nlikelihood of the next token. We hope our meta-evaluation can foster more\nresearch on evaluating content preservation metrics, and also to ensure fair\nevaluation of methods for conducting style transfer.", "published": "2025-02-20 20:16:34", "link": "http://arxiv.org/abs/2502.15022v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of\n  Large Language Models", "abstract": "As the use of large language model (LLM) agents continues to grow, their\nsafety vulnerabilities have become increasingly evident. Extensive benchmarks\nevaluate various aspects of LLM safety by defining the safety relying heavily\non general standards, overlooking user-specific standards. However, safety\nstandards for LLM may vary based on a user-specific profiles rather than being\nuniversally consistent across all users. This raises a critical research\nquestion: Do LLM agents act safely when considering user-specific safety\nstandards? Despite its importance for safe LLM use, no benchmark datasets\ncurrently exist to evaluate the user-specific safety of LLMs. To address this\ngap, we introduce U-SAFEBENCH, the first benchmark designed to assess\nuser-specific aspect of LLM safety. Our evaluation of 18 widely used LLMs\nreveals current LLMs fail to act safely when considering user-specific safety\nstandards, marking a new discovery in this field. To address this\nvulnerability, we propose a simple remedy based on chain-of-thought,\ndemonstrating its effectiveness in improving user-specific safety. Our\nbenchmark and code are available at https://github.com/yeonjun-in/U-SafeBench.", "published": "2025-02-20 22:58:44", "link": "http://arxiv.org/abs/2502.15086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Singular Spectrum for Large Language Model Compression", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, yet\nprohibitive parameter complexity often hinders their deployment. Existing\nsingular value decomposition (SVD) based compression methods simply deem\nsingular values as importance scores of decomposed components. However, this\nimportance ordered by singular values does not necessarily correlate with the\nperformance of a downstream task. In this work, we introduce SoCo (Singular\nspectrum optimization for large language model Compression), a novel\ncompression framework that learns to rescale the decomposed components of SVD\nin a data-driven manner. Concretely, we employ a learnable diagonal matrix to\nassign importance scores for singular spectrum and develop a three-stage\ntraining process that progressively refines these scores from initial coarse\ncompression to fine-grained sparsification-thereby striking an effective\nbalance between aggressive model compression and performance preservation.\nThanks to the learnable singular spectrum, SoCo adaptively prunes components\naccording to the sparsified importance scores, rather than relying on the fixed\norder of singular values. More importantly, the remaining components with\namplified importance scores can compensate for the loss of the pruned ones.\nExperimental evaluations across multiple LLMs and benchmarks demonstrate that\nSoCo surpasses the state-of-the-art methods in model compression.", "published": "2025-02-20 23:18:39", "link": "http://arxiv.org/abs/2502.15092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the logical skills of large language models: evaluations using\n  arbitrarily complex first-order logic problems", "abstract": "We present a method of generating first-order logic statements whose\ncomplexity can be controlled along multiple dimensions. We use this method to\nautomatically create several datasets consisting of questions asking for the\ntruth or falsity of first-order logic statements in Zermelo-Fraenkel set\ntheory. While the resolution of these questions does not require any knowledge\nbeyond basic notation of first-order logic and set theory, it does require a\ndegree of planning and logical reasoning, which can be controlled up to\narbitrarily high difficulty by the complexity of the generated statements.\nFurthermore, we do extensive evaluations of the performance of various large\nlanguage models, including recent models such as DeepSeek-R1 and OpenAI's\no3-mini, on these datasets. All of the datasets along with the code used for\ngenerating them, as well as all data from the evaluations is publicly available\nat https://github.com/bkuckuck/logical-skills-of-llms.", "published": "2025-02-20 01:18:24", "link": "http://arxiv.org/abs/2502.14180v1", "categories": ["cs.LG", "cs.CL", "I.2.6"], "primary_category": "cs.LG"}
{"title": "Federated Fine-Tuning of Large Language Models: Kahneman-Tversky vs.\n  Direct Preference Optimization", "abstract": "We evaluate Kahneman-Tversky Optimization (KTO) as a fine-tuning method for\nlarge language models (LLMs) in federated learning (FL) settings, comparing it\nagainst Direct Preference Optimization (DPO). Using Alpaca-7B as the base\nmodel, we fine-tune on a realistic dataset under both methods and evaluate\nperformance using MT-Bench-1, Vicuna, and AdvBench benchmarks. Additionally, we\nintroduce a redistributed dataset setup, where only KTO is applicable due to\nits ability to handle single-response feedback, unlike DPO's reliance on paired\nresponses. Our results demonstrate that KTO, in both its original (KTOO) and\nredistributed (KTOR) configurations, consistently outperforms DPO across all\nbenchmarks. In the redistributed setup, KTO further validates its flexibility\nand resilience by maintaining superior performance in scenarios where DPO\ncannot be applied. These findings establish KTO as a robust and scalable\nfine-tuning method for FL, motivating its adoption for privacy-preserving,\ndecentralized, and heterogeneous environments.", "published": "2025-02-20 01:44:21", "link": "http://arxiv.org/abs/2502.14187v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on\n  LLM", "abstract": "Large language models (LLMs) have been widely applied in question answering\nover scientific research papers. To enhance the professionalism and accuracy of\nresponses, many studies employ external knowledge augmentation. However,\nexisting structures of external knowledge in scientific literature often focus\nsolely on either paper entities or domain concepts, neglecting the intrinsic\nconnections between papers through shared domain concepts. This results in less\ncomprehensive and specific answers when addressing questions that combine\npapers and concepts. To address this, we propose a novel knowledge graph\nframework that captures deep conceptual relations between academic papers,\nconstructing a relational network via intra-paper semantic elements and\ninter-paper citation relations. Using a few-shot knowledge graph construction\nmethod based on LLM, we develop NLP-AKG, an academic knowledge graph for the\nNLP domain, by extracting 620,353 entities and 2,271,584 relations from 60,826\npapers in ACL Anthology. Based on this, we propose a 'sub-graph community\nsummary' method and validate its effectiveness on three NLP scientific\nliterature question answering datasets.", "published": "2025-02-20 01:48:46", "link": "http://arxiv.org/abs/2502.14192v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "On-the-fly Preference Alignment via Principle-Guided Decoding", "abstract": "With the rapidly expanding landscape of large language models, aligning model\ngenerations with human values and preferences is becoming increasingly\nimportant. Popular alignment methods, such as Reinforcement Learning from Human\nFeedback, have shown significant success in guiding models with greater\ncontrol. However, these methods require considerable computational resources,\nwhich is inefficient, and substantial collection of training data to\naccommodate the diverse and pluralistic nature of human preferences, which is\nimpractical. These limitations significantly constrain the scope and efficacy\nof both task-specific and general preference alignment methods. In this work,\nwe introduce On-the-fly Preference Alignment via Principle-Guided Decoding\n(OPAD) to directly align model outputs with human preferences during inference,\neliminating the need for fine-tuning. Our approach involves first curating a\nsurrogate solution to an otherwise infeasible optimization problem and then\ndesigning a principle-guided reward function based on this surrogate. The final\naligned policy is derived by maximizing this customized reward, which exploits\nthe discrepancy between the constrained policy and its unconstrained\ncounterpart. OPAD directly modifies the model's predictions during inference,\nensuring principle adherence without incurring the computational overhead of\nretraining or fine-tuning. Experiments show that OPAD achieves competitive or\nsuperior performance in both general and personalized alignment tasks,\ndemonstrating its efficiency and effectiveness compared to state-of-the-art\nbaselines.", "published": "2025-02-20 02:23:09", "link": "http://arxiv.org/abs/2502.14204v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall\n  Time-specific Information", "abstract": "While the ability of language models to elicit facts has been widely\ninvestigated, how they handle temporally changing facts remains underexplored.\nWe discover Temporal Heads, specific attention heads primarily responsible for\nprocessing temporal knowledge through circuit analysis. We confirm that these\nheads are present across multiple models, though their specific locations may\nvary, and their responses differ depending on the type of knowledge and its\ncorresponding years. Disabling these heads degrades the model's ability to\nrecall time-specific knowledge while maintaining its general capabilities\nwithout compromising time-invariant and question-answering performances.\nMoreover, the heads are activated not only numeric conditions (\"In 2004\") but\nalso textual aliases (\"In the year ...\"), indicating that they encode a\ntemporal dimension beyond simple numerical representation. Furthermore, we\nexpand the potential of our findings by demonstrating how temporal knowledge\ncan be edited by adjusting the values of these heads.", "published": "2025-02-20 04:52:05", "link": "http://arxiv.org/abs/2502.14258v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MCQA-Eval: Efficient Confidence Evaluation in NLG with Gold-Standard\n  Correctness Labels", "abstract": "Large Language Models (LLMs) require robust confidence estimation,\nparticularly in critical domains like healthcare and law where unreliable\noutputs can lead to significant consequences. Despite much recent work in\nconfidence estimation, current evaluation frameworks rely on correctness\nfunctions -- various heuristics that are often noisy, expensive, and possibly\nintroduce systematic biases. These methodological weaknesses tend to distort\nevaluation metrics and thus the comparative ranking of confidence measures. We\nintroduce MCQA-Eval, an evaluation framework for assessing confidence measures\nin Natural Language Generation (NLG) that eliminates dependence on an explicit\ncorrectness function by leveraging gold-standard correctness labels from\nmultiple-choice datasets. MCQA-Eval enables systematic comparison of both\ninternal state-based white-box (e.g. logit-based) and consistency-based\nblack-box confidence measures, providing a unified evaluation methodology\nacross different approaches. Through extensive experiments on multiple LLMs and\nwidely used QA datasets, we report that MCQA-Eval provides efficient and more\nreliable assessments of confidence estimation methods than existing approaches.", "published": "2025-02-20 05:09:29", "link": "http://arxiv.org/abs/2502.14268v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Capturing Nuanced Preferences: Preference-Aligned Distillation for Small\n  Language Models", "abstract": "Aligning small language models (SLMs) with human values typically involves\ndistilling preference knowledge from large language models (LLMs). However,\nexisting distillation methods model preference knowledge in teacher LLMs by\ncomparing pairwise responses, overlooking the extent of difference between\nresponses. This limitation hinders student SLMs from capturing the nuanced\npreferences for multiple responses. In this paper, we propose a\nPreference-Aligned Distillation (PAD) framework, which models teacher's\npreference knowledge as a probability distribution over all potential\npreferences, thereby providing more nuanced supervisory signals. Our insight in\ndeveloping PAD is rooted in the demonstration that language models can serve as\nreward functions, reflecting their intrinsic preferences. Based on this, PAD\ncomprises three key steps: (1) sampling diverse responses using\nhigh-temperature; (2) computing rewards for both teacher and student to\nconstruct their intrinsic preference; and (3) training the student's intrinsic\npreference distribution to align with the teacher's. Experiments on four\nmainstream alignment benchmarks demonstrate that PAD consistently and\nsignificantly outperforms existing approaches, achieving over 20\\% improvement\non AlpacaEval 2 and Arena-Hard, indicating superior alignment with human\npreferences. Notably, on MT-Bench, using the \\textsc{Gemma} model family, the\nstudent trained by PAD surpasses its teacher, further validating the\neffectiveness of our PAD.", "published": "2025-02-20 05:18:23", "link": "http://arxiv.org/abs/2502.14272v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fact or Guesswork? Evaluating Large Language Model's Medical Knowledge\n  with Structured One-Hop Judgment", "abstract": "Large language models (LLMs) have been widely adopted in various downstream\ntask domains. However, their ability to directly recall and apply factual\nmedical knowledge remains under-explored. Most existing medical QA benchmarks\nassess complex reasoning or multi-hop inference, making it difficult to isolate\nLLMs' inherent medical knowledge from their reasoning capabilities. Given the\nhigh-stakes nature of medical applications, where incorrect information can\nhave critical consequences, it is essential to evaluate how well LLMs encode,\nretain, and recall fundamental medical facts.\n  To bridge this gap, we introduce the Medical Knowledge Judgment, a dataset\nspecifically designed to measure LLMs' one-hop factual medical knowledge. MKJ\nis constructed from the Unified Medical Language System (UMLS), a large-scale\nrepository of standardized biomedical vocabularies and knowledge graphs. We\nframe knowledge assessment as a binary judgment task, requiring LLMs to verify\nthe correctness of medical statements extracted from reliable and structured\nknowledge sources.\n  Our experiments reveal that LLMs struggle with factual medical knowledge\nretention, exhibiting significant performance variance across different\nsemantic categories, particularly for rare medical conditions. Furthermore,\nLLMs show poor calibration, often being overconfident in incorrect answers. To\nmitigate these issues, we explore retrieval-augmented generation, demonstrating\nits effectiveness in improving factual accuracy and reducing uncertainty in\nmedical decision-making.", "published": "2025-02-20 05:27:51", "link": "http://arxiv.org/abs/2502.14275v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts", "abstract": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.", "published": "2025-02-20 05:41:15", "link": "http://arxiv.org/abs/2502.14280v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models", "abstract": "With the rapid emergence of novel capabilities in Large Language Models\n(LLMs), the need for rigorous multilingual and multicultural benchmarks that\nare integrated has become more pronounced. Though existing LLM benchmarks are\ncapable of evaluating specific capabilities of LLMs in English as well as in\nvarious mid- to low-resource languages, including those in the Southeast Asian\n(SEA) region, a comprehensive and authentic evaluation suite for the SEA\nlanguages has not been developed thus far. Here, we present SEA-HELM, a\nholistic linguistic and cultural LLM evaluation suite that emphasizes SEA\nlanguages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics,\n(3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports\nFilipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the\nSEA-HELM leaderboard, which allows users to understand models' multilingual and\nmulticultural performance in a systematic and user-friendly manner.", "published": "2025-02-20 06:32:45", "link": "http://arxiv.org/abs/2502.14301v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Self-Talk: A Communication-Centric Survey of LLM-Based\n  Multi-Agent Systems", "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable\ncapabilities in reasoning, planning, and decision-making. Building upon these\nstrengths, researchers have begun incorporating LLMs into multi-agent systems\n(MAS), where agents collaborate or compete through natural language\ninteractions to tackle tasks beyond the scope of single-agent setups. In this\nsurvey, we present a communication-centric perspective on LLM-based multi-agent\nsystems, examining key system-level features such as architecture design and\ncommunication goals, as well as internal mechanisms like communication\nstrategies, paradigms, objects and content. We illustrate how these\ncommunication elements interplay to enable collective intelligence and flexible\ncollaboration. Furthermore, we discuss prominent challenges, including\nscalability, security, and multimodal integration, and propose directions for\nfuture work to advance research in this emerging domain. Ultimately, this\nsurvey serves as a catalyst for further innovation, fostering more robust,\nscalable, and intelligent multi-agent systems across diverse application\ndomains.", "published": "2025-02-20 07:18:34", "link": "http://arxiv.org/abs/2502.14321v1", "categories": ["cs.MA", "cs.CL"], "primary_category": "cs.MA"}
{"title": "A Survey on Feedback-based Multi-step Reasoning for Large Language\n  Models on Mathematics", "abstract": "Recent progress in large language models (LLM) found chain-of-thought\nprompting strategies to improve the reasoning ability of LLMs by encouraging\nproblem solving through multiple steps. Therefore, subsequent research aimed to\nintegrate the multi-step reasoning process into the LLM itself through process\nrewards as feedback and achieved improvements over prompting strategies. Due to\nthe cost of step-level annotation, some turn to outcome rewards as feedback.\nAside from these training-based approaches, training-free techniques leverage\nfrozen LLMs or external tools for feedback at each step to enhance the\nreasoning process. With the abundance of work in mathematics due to its logical\nnature, we present a survey of strategies utilizing feedback at the step and\noutcome levels to enhance multi-step math reasoning for LLMs. As multi-step\nreasoning emerges a crucial component in scaling LLMs, we hope to establish its\nfoundation for easier understanding and empower further research.", "published": "2025-02-20 07:31:00", "link": "http://arxiv.org/abs/2502.14333v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "English Please: Evaluating Machine Translation for Multilingual Bug\n  Reports", "abstract": "Accurate translation of bug reports is critical for efficient collaboration\nin global software development. In this study, we conduct the first\ncomprehensive evaluation of machine translation (MT) performance on bug\nreports, analyzing the capabilities of DeepL, AWS Translate, and ChatGPT using\ndata from the Visual Studio Code GitHub repository, specifically focusing on\nreports labeled with the english-please tag. To thoroughly assess the accuracy\nand effectiveness of each system, we employ multiple machine translation\nmetrics, including BLEU, BERTScore, COMET, METEOR, and ROUGE. Our findings\nindicate that DeepL consistently outperforms the other systems across most\nautomatic metrics, demonstrating strong lexical and semantic alignment. AWS\nTranslate performs competitively, particularly in METEOR, while ChatGPT lags in\nkey metrics. This study underscores the importance of domain adaptation for\ntranslating technical texts and offers guidance for integrating automated\ntranslation into bug-triaging workflows. Moreover, our results establish a\nfoundation for future research to refine machine translation solutions for\nspecialized engineering contexts. The code and dataset for this paper are\navailable at GitHub: https://github.com/av9ash/gitbugs/tree/main/multilingual.", "published": "2025-02-20 07:47:03", "link": "http://arxiv.org/abs/2502.14338v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Self-Improvement Towards Pareto Optimality: Mitigating Preference\n  Conflicts in Multi-Objective Alignment", "abstract": "Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple\nhuman preference objectives, with Direct Preference Optimization (DPO) emerging\nas a prominent approach. However, we find that DPO-based MOA approaches suffer\nfrom widespread preference conflicts in the data, where different objectives\nfavor different responses. This results in conflicting optimization directions,\nhindering the optimization on the Pareto Front. To address this, we propose to\nconstruct Pareto-optimal responses to resolve preference conflicts. To\nefficiently obtain and utilize such responses, we propose a self-improving DPO\nframework that enables LLMs to self-generate and select Pareto-optimal\nresponses for self-supervised preference alignment. Extensive experiments on\ntwo datasets demonstrate the superior Pareto Front achieved by our framework\ncompared to various baselines. Code is available at\n\\url{https://github.com/zyttt-coder/SIPO}.", "published": "2025-02-20 08:27:00", "link": "http://arxiv.org/abs/2502.14354v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Entropy-UID: A Method for Optimizing Information Density", "abstract": "Balanced and efficient information flow is essential for optimizing language\ngeneration models. In this work, we propose Entropy-UID, a new token selection\nmethod that balances entropy and Uniform Information Density (UID) principles\nfor enhanced efficiency of text generation. Our approach adaptively adjusts\ntoken selection by jointly minimizing entropy and surprisal, promoting more\neven information distribution across generated sequences. Theoretical\nvalidation demonstrates that Entropy-UID optimally reduces information spikes\nwhile maintaining fluency and coherence. The method has been evulated using\ninformation-theoretic metrics on multiple benchmark datasets, including\nWikiText-2, OpenWebText, and WMT. Experimental results show that Entropy-UID\nachieves lower surprisal and entropy variance compared to standard GPT-2 and\nalternative heuristics, leading to more balanced and human-like text\ngeneration. Our findings point towards the potential of leveraging\ninformation-theoretic constraints to refine token selection strategies in\nautoregressive language models.", "published": "2025-02-20 08:42:47", "link": "http://arxiv.org/abs/2502.14366v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Similarity Paradigm Through Textual Regularization Without Forgetting", "abstract": "Prompt learning has emerged as a promising method for adapting pre-trained\nvisual-language models (VLMs) to a range of downstream tasks. While optimizing\nthe context can be effective for improving performance on specific tasks, it\ncan often lead to poor generalization performance on unseen classes or datasets\nsampled from different distributions. It may be attributed to the fact that\ntextual prompts tend to overfit downstream data distributions, leading to the\nforgetting of generalized knowledge derived from hand-crafted prompts. In this\npaper, we propose a novel method called Similarity Paradigm with Textual\nRegularization (SPTR) for prompt learning without forgetting. SPTR is a\ntwo-pronged design based on hand-crafted prompts that is an inseparable\nframework. 1) To avoid forgetting general textual knowledge, we introduce the\noptimal transport as a textual regularization to finely ensure approximation\nwith hand-crafted features and tuning textual features. 2) In order to\ncontinuously unleash the general ability of multiple hand-crafted prompts, we\npropose a similarity paradigm for natural alignment score and adversarial\nalignment score to improve model robustness for generalization. Both modules\nshare a common objective in addressing generalization issues, aiming to\nmaximize the generalization capability derived from multiple hand-crafted\nprompts. Four representative tasks (i.e., non-generalization few-shot learning,\nbase-to-novel generalization, cross-dataset generalization, domain\ngeneralization) across 11 datasets demonstrate that SPTR outperforms existing\nprompt learning methods.", "published": "2025-02-20 09:06:44", "link": "http://arxiv.org/abs/2502.14376v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Leveraging Small LLMs for Argument Mining in Education: Argument\n  Component Identification, Classification, and Assessment", "abstract": "Argument mining algorithms analyze the argumentative structure of essays,\nmaking them a valuable tool for enhancing education by providing targeted\nfeedback on the students' argumentation skills. While current methods often use\nencoder or encoder-decoder deep learning architectures, decoder-only models\nremain largely unexplored, offering a promising research direction.\n  This paper proposes leveraging open-source, small Large Language Models\n(LLMs) for argument mining through few-shot prompting and fine-tuning. These\nmodels' small size and open-source nature ensure accessibility, privacy, and\ncomputational efficiency, enabling schools and educators to adopt and deploy\nthem locally. Specifically, we perform three tasks: segmentation of student\nessays into arguments, classification of the arguments by type, and assessment\nof their quality. We empirically evaluate the models on the Feedback Prize -\nPredicting Effective Arguments dataset of grade 6-12 students essays and\ndemonstrate how fine-tuned small LLMs outperform baseline methods in segmenting\nthe essays and determining the argument types while few-shot prompting yields\ncomparable performance to that of the baselines in assessing quality. This work\nhighlights the educational potential of small, open-source LLMs to provide\nreal-time, personalized feedback, enhancing independent learning and writing\nskills while ensuring low computational cost and privacy.", "published": "2025-02-20 09:23:40", "link": "http://arxiv.org/abs/2502.14389v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization", "abstract": "Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query. Extracting and properly citing\nevidence spans could help improve the transparency and reliability of these\nsummaries. At the same time, LLMs suffer from positional biases in terms of\nwhich information they understand and attend to, which could affect evidence\ncitation. Whereas previous work has focused on evidence citation with\npredefined levels of granularity (e.g. sentence, paragraph, document, etc.), we\npropose the task of long-context query focused summarization with unstructured\nevidence citation. We show how existing systems struggle to generate and\nproperly cite unstructured evidence from their context, and that evidence tends\nto be \"lost-in-the-middle\". To help mitigate this, we create the Summaries with\nUnstructured Evidence Text dataset (SUnsET), a synthetic dataset generated\nusing a novel domain-agnostic pipeline which can be used as supervision to\nadapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4\ndatasets with varying document types and lengths that LLMs adapted with SUnsET\ndata generate more relevant and factually consistent evidence than their base\nmodels, extract evidence from more diverse locations in their context, and can\ngenerate more relevant and consistent summaries.", "published": "2025-02-20 09:57:42", "link": "http://arxiv.org/abs/2502.14409v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Generative adversarial networks vs large language models: a comparative\n  study on synthetic tabular data generation", "abstract": "We propose a new framework for zero-shot generation of synthetic tabular\ndata. Using the large language model (LLM) GPT-4o and plain-language prompting,\nwe demonstrate the ability to generate high-fidelity tabular data without\ntask-specific fine-tuning or access to real-world data (RWD) for pre-training.\nTo benchmark GPT-4o, we compared the fidelity and privacy of LLM-generated\nsynthetic data against data generated with the conditional tabular generative\nadversarial network (CTGAN), across three open-access datasets: Iris, Fish\nMeasurements, and Real Estate Valuation. Despite the zero-shot approach, GPT-4o\noutperformed CTGAN in preserving means, 95% confidence intervals, bivariate\ncorrelations, and data privacy of RWD, even at amplified sample sizes. Notably,\ncorrelations between parameters were consistently preserved with appropriate\ndirection and strength. However, refinement is necessary to better retain\ndistributional characteristics. These findings highlight the potential of LLMs\nin tabular data synthesis, offering an accessible alternative to generative\nadversarial networks and variational autoencoders.", "published": "2025-02-20 12:56:16", "link": "http://arxiv.org/abs/2502.14523v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems\n  Based on Large Language Models", "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated\nremarkable real-world capabilities, effectively collaborating to complete\ncomplex tasks. While these systems are designed with safety mechanisms, such as\nrejecting harmful instructions through alignment, their security remains\nlargely unexplored. This gap leaves LLM-MASs vulnerable to targeted\ndisruptions. In this paper, we introduce Contagious Recursive Blocking Attacks\n(Corba), a novel and simple yet highly effective attack that disrupts\ninteractions between agents within an LLM-MAS. Corba leverages two key\nproperties: its contagious nature allows it to propagate across arbitrary\nnetwork topologies, while its recursive property enables sustained depletion of\ncomputational resources. Notably, these blocking attacks often involve\nseemingly benign instructions, making them particularly challenging to mitigate\nusing conventional alignment methods. We evaluate Corba on two widely-used\nLLM-MASs, namely, AutoGen and Camel across various topologies and commercial\nmodels. Additionally, we conduct more extensive experiments in open-ended\ninteractive LLM-MASs, demonstrating the effectiveness of Corba in complex\ntopology structures and open-source models. Our code is available at:\nhttps://github.com/zhrli324/Corba.", "published": "2025-02-20 13:02:00", "link": "http://arxiv.org/abs/2502.14529v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context\n  Learning and Fine-tuning on Open LLMs", "abstract": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches that rely on pre-trained models like SciBERT, which\nrequire extensive domain-specific pretraining and specialized architectures, we\ndemonstrate that general-purpose LLMs can be adapted to this task with minimal\ntask-specific data. We evaluate twelve model variations across five prominent\nopen LLM families using zero, one, few, and many-shot prompting to assess\nperformance across scenarios. Our experimental study identifies the\ntop-performing model through extensive experimentation of in-context\nlearning-related parameters, which we fine-tune to further enhance task\nperformance. The results highlight the strengths and limitations of LLMs in\nrecognizing citation intents, providing valuable insights for model selection\nand prompt engineering. Additionally, we make our end-to-end evaluation\nframework and models openly available for future use.", "published": "2025-02-20 13:45:42", "link": "http://arxiv.org/abs/2502.14561v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification", "abstract": "Self-awareness, i.e., the ability to assess and correct one's own generation,\nis a fundamental aspect of human intelligence, making its replication in large\nlanguage models (LLMs) an important yet challenging task. Previous works tackle\nthis by employing extensive reinforcement learning or rather relying on large\nexternal verifiers. In this work, we propose Refine via Intrinsic\nSelf-Verification (ReVISE), an efficient and effective framework that enables\nLLMs to self-correct their outputs through self-verification. The core idea of\nReVISE is to enable LLMs to verify their reasoning processes and continually\nrethink reasoning trajectories based on its verification. We introduce a\nstructured curriculum based upon online preference learning to implement this\nefficiently. Specifically, as ReVISE involves two challenging tasks (i.e.,\nself-verification and reasoning correction), we tackle each task sequentially\nusing curriculum learning, collecting both failed and successful reasoning\npaths to construct preference pairs for efficient training. During inference,\nour approach enjoys natural test-time scaling by integrating self-verification\nand correction capabilities, further enhanced by our proposed confidence-aware\ndecoding mechanism. Our experiments on various reasoning tasks demonstrate that\nReVISE achieves efficient self-correction and significantly improves reasoning\nperformance.", "published": "2025-02-20 13:50:02", "link": "http://arxiv.org/abs/2502.14565v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline\n  Comparison for Semantic Similarity", "abstract": "This paper investigates the efficacy of RWKV, a novel language model\narchitecture known for its linear attention mechanism, for generating sentence\nembeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate\nthe semantic similarity captured by embeddings from different hidden layers of\na pre-trained RWKV model. The performance is assessed on the Microsoft Research\nParaphrase Corpus (MRPC) dataset using Spearman correlation and compared\nagainst a GloVe-based baseline. My results indicate that while RWKV embeddings\ncapture some semantic relatedness, they underperform compared to the GloVe\nbaseline in terms of Spearman correlation. I also analyze the inference time\nand GPU memory usage, highlighting the computational trade-offs associated with\nRWKV embeddings. The findings suggest that while RWKV offers potential\nadvantages in terms of linear scaling, its zero-shot sentence embedding quality\nfor semantic similarity tasks requires further investigation and potential\ntask-specific fine-tuning to match or exceed simpler baselines.", "published": "2025-02-20 14:58:37", "link": "http://arxiv.org/abs/2502.14620v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.7.3"], "primary_category": "cs.CL"}
{"title": "Multi-Record Web Page Information Extraction From News Websites", "abstract": "In this paper, we focused on the problem of extracting information from web\npages containing many records, a task of growing importance in the era of\nmassive web data. Recently, the development of neural network methods has\nimproved the quality of information extraction from web pages. Nevertheless,\nmost of the research and datasets are aimed at studying detailed pages. This\nhas left multi-record \"list pages\" relatively understudied, despite their\nwidespread presence and practical significance.\n  To address this gap, we created a large-scale, open-access dataset\nspecifically designed for list pages. This is the first dataset for this task\nin the Russian language. Our dataset contains 13,120 web pages with news lists,\nsignificantly exceeding existing datasets in both scale and complexity. Our\ndataset contains attributes of various types, including optional and\nmulti-valued, providing a realistic representation of real-world list pages.\nThese features make our dataset a valuable resource for studying information\nextraction from pages containing many records.\n  Furthermore, we proposed our own multi-stage information extraction methods.\nIn this work, we explore and demonstrate several strategies for applying\nMarkupLM to the specific challenges of multi-record web pages. Our experiments\nvalidate the advantages of our methods.\n  By releasing our dataset to the public, we aim to advance the field of\ninformation extraction from multi-record pages.", "published": "2025-02-20 15:05:00", "link": "http://arxiv.org/abs/2502.14625v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "PEARL: Towards Permutation-Resilient LLMs", "abstract": "The in-context learning (ICL) capability of large language models (LLMs)\nenables them to perform challenging tasks using provided demonstrations.\nHowever, ICL is highly sensitive to the ordering of demonstrations, leading to\ninstability in predictions. This paper shows that this vulnerability can be\nexploited to design a natural attack - difficult for model providers to detect\n- that achieves nearly 80% success rate on LLaMA-3 by simply permuting the\ndemonstrations. Existing mitigation methods primarily rely on post-processing\nand fail to enhance the model's inherent robustness to input permutations,\nraising concerns about safety and reliability of LLMs. To address this issue,\nwe propose Permutation-resilient learning (PEARL), a novel framework based on\ndistributionally robust optimization (DRO), which optimizes model performance\nagainst the worst-case input permutation. Specifically, PEARL consists of a\npermutation-proposal network (P-Net) and the LLM. The P-Net generates the most\nchallenging permutations by treating it as an optimal transport problem, which\nis solved using an entropy-constrained Sinkhorn algorithm. Through minimax\noptimization, the P-Net and the LLM iteratively optimize against each other,\nprogressively improving the LLM's robustness. Experiments on synthetic\npre-training and real-world instruction tuning tasks demonstrate that PEARL\neffectively mitigates permutation attacks and enhances performance. Notably,\ndespite being trained on fewer shots and shorter contexts, PEARL achieves\nperformance gains of up to 40% when scaled to many-shot and long-context\nscenarios, highlighting its efficiency and generalization capabilities.", "published": "2025-02-20 15:07:02", "link": "http://arxiv.org/abs/2502.14628v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "NAVIG: Natural Language-guided Analysis with Vision Language Models for\n  Image Geo-localization", "abstract": "Image geo-localization is the task of predicting the specific location of an\nimage and requires complex reasoning across visual, geographical, and cultural\ncontexts. While prior Vision Language Models (VLMs) have the best accuracy at\nthis task, there is a dearth of high-quality datasets and models for analytical\nreasoning. We first create NaviClues, a high-quality dataset derived from\nGeoGuessr, a popular geography game, to supply examples of expert reasoning\nfrom language. Using this dataset, we present Navig, a comprehensive image\ngeo-localization framework integrating global and fine-grained image\ninformation. By reasoning with language, Navig reduces the average distance\nerror by 14% compared to previous state-of-the-art models while requiring fewer\nthan 1000 training samples. Our dataset and code are available at\nhttps://github.com/SparrowZheyuan18/Navig/.", "published": "2025-02-20 15:21:35", "link": "http://arxiv.org/abs/2502.14638v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual\n  Knowledge Synchronization in LLMs", "abstract": "Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings.", "published": "2025-02-20 15:32:31", "link": "http://arxiv.org/abs/2502.14645v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InstructAgent: Building User Controllable Recommender via LLM Agent", "abstract": "Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure. To this end, we first construct four recommendation\ndatasets, denoted as $\\dataset$, along with user instructions for each record.", "published": "2025-02-20 15:58:25", "link": "http://arxiv.org/abs/2502.14662v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Data-Constrained Synthesis of Training Data for De-Identification", "abstract": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data.", "published": "2025-02-20 16:09:27", "link": "http://arxiv.org/abs/2502.14677v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data-Efficient Pretraining with Group-Level Data Influence Modeling", "abstract": "Data-efficient pretraining has shown tremendous potential to elevate scaling\nlaws. This paper argues that effective pretraining data should be curated at\nthe group level, treating a set of data points as a whole rather than as\nindependent contributors. To achieve that, we propose Group-Level Data\nInfluence Modeling (Group-MATES), a novel data-efficient pretraining method\nthat captures and optimizes group-level data utility. Specifically, Group-MATES\ncollects oracle group-level influences by locally probing the pretraining model\nwith data sets. It then fine-tunes a relational data influence model to\napproximate oracles as relationship-weighted aggregations of individual\ninfluences. The fine-tuned model selects the data subset by maximizing its\ngroup-level influence prediction, with influence-aware clustering to enable\nefficient inference. Experiments on the DCLM benchmark demonstrate that\nGroup-MATES achieves a 10% relative core score improvement on 22 downstream\ntasks over DCLM-Baseline and 5% over individual-influence-based methods,\nestablishing a new state-of-the-art. Further analyses highlight the\neffectiveness of relational data influence models in capturing intricate\ninteractions between data points.", "published": "2025-02-20 16:34:46", "link": "http://arxiv.org/abs/2502.14709v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TritonBench: Benchmarking Large Language Model Capabilities for\n  Generating Triton Operators", "abstract": "Triton, a high-level Python-like language designed for building efficient GPU\nkernels, is widely adopted in deep learning frameworks due to its portability,\nflexibility, and accessibility. However, programming and parallel optimization\nstill require considerable trial and error from Triton developers. Despite\nadvances in large language models (LLMs) for conventional code generation,\nthese models struggle to generate accurate, performance-optimized Triton code,\nas they lack awareness of its specifications and the complexities of GPU\nprogramming. More critically, there is an urgent need for systematic\nevaluations tailored to Triton. In this work, we introduce TritonBench, the\nfirst comprehensive benchmark for Triton operator generation. TritonBench\nfeatures two evaluation channels: a curated set of 184 real-world operators\nfrom GitHub and a collection of operators aligned with PyTorch interfaces.\nUnlike conventional code benchmarks prioritizing functional correctness,\nTritonBench also profiles efficiency performance on widely deployed GPUs\naligned with industry applications. Our study reveals that current\nstate-of-the-art code LLMs struggle to generate efficient Triton operators,\nhighlighting a significant gap in high-performance code generation. TritonBench\nwill be available at https://github.com/thunlp/TritonBench.", "published": "2025-02-20 17:21:27", "link": "http://arxiv.org/abs/2502.14752v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Influence of Context Size and Model Choice in Retrieval-Augmented\n  Generation Systems", "abstract": "Retrieval-augmented generation (RAG) has emerged as an approach to augment\nlarge language models (LLMs) by reducing their reliance on static knowledge and\nimproving answer factuality. RAG retrieves relevant context snippets and\ngenerates an answer based on them. Despite its increasing industrial adoption,\nsystematic exploration of RAG components is lacking, particularly regarding the\nideal size of provided context, and the choice of base LLM and retrieval\nmethod. To help guide development of robust RAG systems, we evaluate various\ncontext sizes, BM25 and semantic search as retrievers, and eight base LLMs.\nMoving away from the usual RAG evaluation with short answers, we explore the\nmore challenging long-form question answering in two domains, where a good\nanswer has to utilize the entire context. Our findings indicate that final QA\nperformance improves steadily with up to 15 snippets but stagnates or declines\nbeyond that. Finally, we show that different general-purpose LLMs excel in the\nbiomedical domain than the encyclopedic one, and that open-domain evidence\nretrieval in large corpora is challenging.", "published": "2025-02-20 17:34:34", "link": "http://arxiv.org/abs/2502.14759v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Step-by-Step Fact Verification System for Medical Claims with\n  Explainable Reasoning", "abstract": "Fact verification (FV) aims to assess the veracity of a claim based on\nrelevant evidence. The traditional approach for automated FV includes a\nthree-part pipeline relying on short evidence snippets and encoder-only\ninference models. More recent approaches leverage the multi-turn nature of LLMs\nto address FV as a step-by-step problem where questions inquiring additional\ncontext are generated and answered until there is enough information to make a\ndecision. This iterative method makes the verification process rational and\nexplainable. While these methods have been tested for encyclopedic claims,\nexploration on domain-specific and realistic claims is missing. In this work,\nwe apply an iterative FV system on three medical fact-checking datasets and\nevaluate it with multiple settings, including different LLMs, external web\nsearch, and structured reasoning using logic predicates. We demonstrate\nimprovements in the final performance over traditional approaches and the high\npotential of step-by-step FV systems for domain-specific claims.", "published": "2025-02-20 17:40:21", "link": "http://arxiv.org/abs/2502.14765v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for\n  Scientific Comparative Analysis", "abstract": "With the exponential growth of research facilitated by modern technology and\nimproved accessibility, scientific discoveries have become increasingly\nfragmented within and across fields. This makes it challenging to assess the\nsignificance, novelty, incremental findings, and equivalent ideas between\nrelated works, particularly those from different research communities. Large\nlanguage models (LLMs) have recently demonstrated strong quantitative and\nqualitative reasoning abilities, and multi-agent LLM debates have shown promise\nin handling complex reasoning tasks by exploring diverse perspectives and\nreasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a\nframework which converts scientific papers into LLM personas that debate their\nrespective novelties. To emphasize structured, critical reasoning rather than\nfocusing solely on outcomes, ToD dynamically constructs a debate tree, enabling\nfine-grained analysis of independent novelty arguments within scholarly\narticles. Through experiments on scientific literature across various domains,\nevaluated by expert researchers, we demonstrate that ToD generates informative\narguments, effectively contrasts papers, and supports researchers in their\nliterature review.", "published": "2025-02-20 17:43:40", "link": "http://arxiv.org/abs/2502.14767v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement\n  Learning", "abstract": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in large reasoning models. To analyze\nreasoning dynamics, we use synthetic logic puzzles as training data due to\ntheir controllable complexity and straightforward answer verification. We make\nsome key technical contributions that lead to effective and stable RL training:\na system prompt that emphasizes the thinking and answering process, a stringent\nformat reward function that penalizes outputs for taking shortcuts, and a\nstraightforward training recipe that achieves stable convergence. Our 7B model\ndevelops advanced reasoning skills-such as reflection, verification, and\nsummarization-that are absent from the logic corpus. Remarkably, after training\non just 5K logic problems, it demonstrates generalization abilities to the\nchallenging math benchmarks AIME and AMC.", "published": "2025-02-20 17:49:26", "link": "http://arxiv.org/abs/2502.14768v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language\n  Models", "abstract": "Our ability to continuously acquire, organize, and leverage knowledge is a\nkey feature of human intelligence that AI systems must approximate to unlock\ntheir full potential. Given the challenges in continual learning with large\nlanguage models (LLMs), retrieval-augmented generation (RAG) has become the\ndominant way to introduce new information. However, its reliance on vector\nretrieval hinders its ability to mimic the dynamic and interconnected nature of\nhuman long-term memory. Recent RAG approaches augment vector embeddings with\nvarious structures like knowledge graphs to address some of these gaps, namely\nsense-making and associativity. However, their performance on more basic\nfactual memory tasks drops considerably below standard RAG. We address this\nunintended deterioration and propose HippoRAG 2, a framework that outperforms\nstandard RAG comprehensively on factual, sense-making, and associative memory\ntasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in\nHippoRAG and enhances it with deeper passage integration and more effective\nonline use of an LLM. This combination pushes this RAG system closer to the\neffectiveness of human long-term memory, achieving a 7% improvement in\nassociative memory tasks over the state-of-the-art embedding model while also\nexhibiting superior factual knowledge and sense-making memory capabilities.\nThis work paves the way for non-parametric continual learning for LLMs. Our\ncode and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.", "published": "2025-02-20 18:26:02", "link": "http://arxiv.org/abs/2502.14802v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in\n  Fine-Tuned LLMs", "abstract": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).", "published": "2025-02-20 18:45:43", "link": "http://arxiv.org/abs/2502.14830v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs", "abstract": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.", "published": "2025-02-20 18:50:42", "link": "http://arxiv.org/abs/2502.14837v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revealing and Mitigating Over-Attention in Knowledge Editing", "abstract": "Large Language Models have demonstrated superior performance across a wide\nrange of tasks, but they still exhibit undesirable errors due to incorrect\nknowledge learned from the training data. To avoid this, knowledge editing\nmethods emerged to precisely edit the specific model knowledge via efficiently\nmodifying a very small percentage of parameters. % However, those methods can\nlead to the problem of Specificity Failure: when the content related to the\nedited knowledge occurs in the context, it can inadvertently corrupt other\npre-existing knowledge. However, those methods can lead to the problem of\nSpecificity Failure, where the existing knowledge and capabilities are severely\ndegraded due to editing. Our preliminary indicates that Specificity Failure\nprimarily stems from the model's attention heads assigning excessive attention\nscores to entities related to the edited knowledge, thereby unduly focusing on\nspecific snippets within the context, which we denote as the Attention Drift\nphenomenon. To mitigate such Attention Drift issue, we introduce a simple yet\neffective method Selective Attention Drift Restriction}(SADR), which introduces\nan additional regularization term during the knowledge editing process to\nrestrict changes in the attention weight distribution, thereby preventing undue\nfocus on the edited entity. Experiments on five frequently used strong LLMs\ndemonstrate the effectiveness of our method, where SADR can significantly\nmitigate Specificity Failure in the predominant knowledge editing tasks.", "published": "2025-02-20 18:51:12", "link": "http://arxiv.org/abs/2502.14838v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation", "abstract": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.", "published": "2025-02-20 18:55:30", "link": "http://arxiv.org/abs/2502.14846v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Prompt-to-Leaderboard", "abstract": "Large language model (LLM) evaluations typically rely on aggregated metrics\nlike accuracy or human preference, averaging across users and prompts. This\naveraging obscures user- and prompt-specific variations in model performance.\nTo address this, we propose Prompt-to-Leaderboard (P2L), a method that produces\nleaderboards specific to a prompt. The core idea is to train an LLM taking\nnatural language prompts as input to output a vector of Bradley-Terry\ncoefficients which are then used to predict the human preference vote. The\nresulting prompt-dependent leaderboards allow for unsupervised task-specific\nevaluation, optimal routing of queries to models, personalization, and\nautomated evaluation of model strengths and weaknesses. Data from Chatbot Arena\nsuggest that P2L better captures the nuanced landscape of language model\nperformance than the averaged leaderboard. Furthermore, our findings suggest\nthat P2L's ability to produce prompt-specific evaluations follows a power law\nscaling similar to that observed in LLMs themselves. In January 2025, the\nrouter we trained based on this methodology achieved the #1 spot on the Chatbot\nArena leaderboard. Our code is available on GitHub at\nhttps://github.com/lmarena/p2l.", "published": "2025-02-20 18:58:07", "link": "http://arxiv.org/abs/2502.14855v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GenAI vs. Human Fact-Checkers: Accurate Ratings, Flawed Rationales", "abstract": "Despite recent advances in understanding the capabilities and limits of\ngenerative artificial intelligence (GenAI) models, we are just beginning to\nunderstand their capacity to assess and reason about the veracity of content.\nWe evaluate multiple GenAI models across tasks that involve the rating of, and\nperceived reasoning about, the credibility of information. The information in\nour experiments comes from content that subnational U.S. politicians post to\nFacebook. We find that GPT-4o, one of the most used AI models in consumer\napplications, outperforms other models, but all models exhibit only moderate\nagreement with human coders. Importantly, even when GenAI models accurately\nidentify low-credibility content, their reasoning relies heavily on linguistic\nfeatures and ``hard'' criteria, such as the level of detail, source\nreliability, and language formality, rather than an understanding of veracity.\nWe also assess the effectiveness of summarized versus full content inputs,\nfinding that summarized content holds promise for improving efficiency without\nsacrificing accuracy. While GenAI has the potential to support human\nfact-checkers in scaling misinformation detection, our results caution against\nrelying solely on these models.", "published": "2025-02-20 17:47:40", "link": "http://arxiv.org/abs/2502.14943v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Beyond No: Quantifying AI Over-Refusal and Emotional Attachment\n  Boundaries", "abstract": "We present an open-source benchmark and evaluation framework for assessing\nemotional boundary handling in Large Language Models (LLMs). Using a dataset of\n1156 prompts across six languages, we evaluated three leading LLMs (GPT-4o,\nClaude-3.5 Sonnet, and Mistral-large) on their ability to maintain appropriate\nemotional boundaries through pattern-matched response analysis. Our framework\nquantifies responses across seven key patterns: direct refusal, apology,\nexplanation, deflection, acknowledgment, boundary setting, and emotional\nawareness. Results demonstrate significant variation in boundary-handling\napproaches, with Claude-3.5 achieving the highest overall score (8.69/10) and\nproducing longer, more nuanced responses (86.51 words on average). We\nidentified a substantial performance gap between English (average score 25.62)\nand non-English interactions (< 0.22), with English responses showing markedly\nhigher refusal rates (43.20% vs. < 1% for non-English). Pattern analysis\nrevealed model-specific strategies, such as Mistral's preference for deflection\n(4.2%) and consistently low empathy scores across all models (< 0.06).\nLimitations include potential oversimplification through pattern matching, lack\nof contextual understanding in response analysis, and binary classification of\ncomplex emotional responses. Future work should explore more nuanced scoring\nmethods, expand language coverage, and investigate cultural variations in\nemotional boundary expectations. Our benchmark and methodology provide a\nfoundation for systematic evaluation of LLM emotional intelligence and\nboundary-setting capabilities.", "published": "2025-02-20 19:09:40", "link": "http://arxiv.org/abs/2502.14975v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context\n  Memory of Transformers", "abstract": "We introduce methods to quantify how Large Language Models (LLMs) encode and\nstore contextual information, revealing that tokens often seen as minor (e.g.,\ndeterminers, punctuation) carry surprisingly high context. Notably, removing\nthese tokens -- especially stopwords, articles, and commas -- consistently\ndegrades performance on MMLU and BABILong-4k, even if removing only irrelevant\ntokens. Our analysis also shows a strong correlation between contextualization\nand linearity, where linearity measures how closely the transformation from one\nlayer's embeddings to the next can be approximated by a single linear mapping.\nThese findings underscore the hidden importance of filler tokens in maintaining\ncontext. For further exploration, we present LLM-Microscope, an open-source\ntoolkit that assesses token-level nonlinearity, evaluates contextual memory,\nvisualizes intermediate layer contributions (via an adapted Logit Lens), and\nmeasures the intrinsic dimensionality of representations. This toolkit\nilluminates how seemingly trivial tokens can be critical for long-range\nunderstanding.", "published": "2025-02-20 19:59:35", "link": "http://arxiv.org/abs/2502.15007v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reducing Hallucinations of Medical Multimodal Large Language Models with\n  Visual Retrieval-Augmented Generation", "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive performance in\nvision and text tasks. However, hallucination remains a major challenge,\nespecially in fields like healthcare where details are critical. In this work,\nwe show how MLLMs may be enhanced to support Visual RAG (V-RAG), a\nretrieval-augmented generation framework that incorporates both text and visual\ndata from retrieved images. On the MIMIC-CXR chest X-ray report generation and\nMulticare medical image caption generation datasets, we show that Visual RAG\nimproves the accuracy of entity probing, which asks whether a medical entities\nis grounded by an image. We show that the improvements extend both to frequent\nand rare entities, the latter of which may have less positive training data.\nDownstream, we apply V-RAG with entity probing to correct hallucinations and\ngenerate more clinically accurate X-ray reports, obtaining a higher RadGraph-F1\nscore.", "published": "2025-02-20 20:55:34", "link": "http://arxiv.org/abs/2502.15040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rare Disease Differential Diagnosis with Large Language Models at Scale:\n  From Abdominal Actinomycosis to Wilson's Disease", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\ndisease diagnosis. However, their effectiveness in identifying rarer diseases,\nwhich are inherently more challenging to diagnose, remains an open question.\nRare disease performance is critical with the increasing use of LLMs in\nhealthcare settings. This is especially true if a primary care physician needs\nto make a rarer prognosis from only a patient conversation so that they can\ntake the appropriate next step. To that end, several clinical decision support\nsystems are designed to support providers in rare disease identification. Yet\ntheir utility is limited due to their lack of knowledge of common disorders and\ndifficulty of use.\n  In this paper, we propose RareScale to combine the knowledge LLMs with expert\nsystems. We use jointly use an expert system and LLM to simulate rare disease\nchats. This data is used to train a rare disease candidate predictor model.\nCandidates from this smaller model are then used as additional inputs to\nblack-box LLM to make the final differential diagnosis. Thus, RareScale allows\nfor a balance between rare and common diagnoses. We present results on over 575\nrare diseases, beginning with Abdominal Actinomycosis and ending with Wilson's\nDisease. Our approach significantly improves the baseline performance of\nblack-box LLMs by over 17% in Top-5 accuracy. We also find that our candidate\ngeneration performance is high (e.g. 88.8% on gpt-4o generated chats).", "published": "2025-02-20 22:02:52", "link": "http://arxiv.org/abs/2502.15069v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyze the Neurons, not the Embeddings: Understanding When and Where\n  LLM Representations Align with Humans", "abstract": "Modern large language models (LLMs) achieve impressive performance on some\ntasks, while exhibiting distinctly non-human-like behaviors on others. This\nraises the question of how well the LLM's learned representations align with\nhuman representations. In this work, we introduce a novel approach to the study\nof representation alignment: we adopt a method from research on activation\nsteering to identify neurons responsible for specific concepts (e.g., 'cat')\nand then analyze the corresponding activation patterns. Our findings reveal\nthat LLM representations closely align with human representations inferred from\nbehavioral data. Notably, this alignment surpasses that of word embeddings,\nwhich have been center stage in prior work on human and model alignment.\nAdditionally, our approach enables a more granular view of how LLMs represent\nconcepts. Specifically, we show that LLMs organize concepts in a way that\nreflects hierarchical relationships interpretable to humans (e.g.,\n'animal'-'dog').", "published": "2025-02-20 23:08:03", "link": "http://arxiv.org/abs/2502.15090v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Judging It, Washing It: Scoring and Greenwashing Corporate Climate\n  Disclosures using Large Language Models", "abstract": "We study the use of large language models (LLMs) to both evaluate and\ngreenwash corporate climate disclosures. First, we investigate the use of the\nLLM-as-a-Judge (LLMJ) methodology for scoring company-submitted reports on\nemissions reduction targets and progress. Second, we probe the behavior of an\nLLM when it is prompted to greenwash a response subject to accuracy and length\nconstraints. Finally, we test the robustness of the LLMJ methodology against\nresponses that may be greenwashed using an LLM. We find that two LLMJ scoring\nsystems, numerical rating and pairwise comparison, are effective in\ndistinguishing high-performing companies from others, with the pairwise\ncomparison system showing greater robustness against LLM-greenwashed responses.", "published": "2025-02-20 23:23:45", "link": "http://arxiv.org/abs/2502.15094v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "LUME: LLM Unlearning with Multitask Evaluations", "abstract": "Unlearning aims to remove copyrighted, sensitive, or private content from\nlarge language models (LLMs) without a full retraining. In this work, we\ndevelop a multi-task unlearning benchmark (LUME) which features three tasks:\n(1) unlearn synthetically generated creative short novels, (2) unlearn\nsynthetic biographies with sensitive information, and (3) unlearn a collection\nof public biographies. We further release two fine-tuned LLMs of 1B and 7B\nparameter sizes as the target models. We conduct detailed evaluations of\nseveral recently proposed unlearning algorithms and present results on\ncarefully crafted metrics to understand their behavior and limitations.", "published": "2025-02-20 23:30:45", "link": "http://arxiv.org/abs/2502.15097v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action\n  Analysis with Cross-Category Generalization", "abstract": "Sustainability reports are key for evaluating companies' environmental,\nsocial and governance, ESG performance, but their content is increasingly\nobscured by greenwashing - sustainability claims that are misleading,\nexaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack\nrobustness against greenwashing risks, often extracting insights that reflect\nmisleading or exaggerated sustainability claims rather than objective ESG\nperformance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis\nwith Cross-Category Generalization, as a novel dataset to improve the\nrobustness of ESG analysis amid the prevalence of greenwashing. By explicitly\nlinking sustainability aspects with their associated actions, A3CG facilitates\na more fine-grained and transparent evaluation of sustainability claims,\nensuring that insights are grounded in verifiable actions rather than vague or\nmisleading rhetoric. Additionally, A3CG emphasizes cross-category\ngeneralization. This ensures robust model performance in aspect-action analysis\neven when companies change their reports to selectively favor certain\nsustainability areas. Through experiments on A3CG, we analyze state-of-the-art\nsupervised models and LLMs, uncovering their limitations and outlining key\ndirections for future research.", "published": "2025-02-20 03:01:08", "link": "http://arxiv.org/abs/2502.15821v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing", "abstract": "Large language models (LLMs) often retain outdated or incorrect information\nfrom pre-training, which undermines their reliability. While model editing\nmethods have been developed to address such errors without full re-training,\nthey frequently suffer from knowledge conflicts, where outdated information\ninterferes with new knowledge. In this work, we propose Conflict-free Model\nEditing (CoME), a novel framework that enhances the accuracy of knowledge\nupdates in LLMs by selectively removing outdated knowledge. CoME leverages\nunlearning to mitigate knowledge interference, allowing new information to be\nintegrated without compromising relevant linguistic features. Through\nexperiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we\ndemonstrate that CoME improves both editing accuracy and model reliability when\napplied to existing editing methods. Our results highlight that the targeted\nremoval of outdated knowledge is crucial for enhancing model editing\neffectiveness and maintaining the model's generative performance.", "published": "2025-02-20 04:55:38", "link": "http://arxiv.org/abs/2502.15826v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language\n  Models", "abstract": "Large language models (LLMs) have become increasingly popular. Their emergent\ncapabilities can be attributed to their massive training datasets. However,\nthese datasets often contain undesirable or inappropriate content, e.g.,\nharmful texts, personal information, and copyrighted material. This has\npromoted research into machine unlearning that aims to remove information from\ntrained models. In particular, approximate unlearning seeks to achieve\ninformation removal by strategically editing the model rather than complete\nmodel retraining.\n  Recent work has shown that soft token attacks (STA) can successfully extract\npurportedly unlearned information from LLMs, thereby exposing limitations in\ncurrent unlearning methodologies. In this work, we reveal that STAs are an\ninadequate tool for auditing unlearning. Through systematic evaluation on\ncommon unlearning benchmarks (Who Is Harry Potter? and TOFU), we demonstrate\nthat such attacks can elicit any information from the LLM, regardless of (1)\nthe deployed unlearning algorithm, and (2) whether the queried content was\noriginally present in the training corpus. Furthermore, we show that STA with\njust a few soft tokens (1-10) can elicit random strings over 400-characters\nlong. Thus showing that STAs are too powerful, and misrepresent the\neffectiveness of the unlearning methods.\n  Our work highlights the need for better evaluation baselines, and more\nappropriate auditing tools for assessing the effectiveness of unlearning in\nLLMs.", "published": "2025-02-20 13:22:33", "link": "http://arxiv.org/abs/2502.15836v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hallucination Detection in Large Language Models with Metamorphic\n  Relations", "abstract": "Large Language Models (LLMs) are prone to hallucinations, e.g., factually\nincorrect information, in their responses. These hallucinations present\nchallenges for LLM-based applications that demand high factual accuracy.\nExisting hallucination detection methods primarily depend on external\nresources, which can suffer from issues such as low availability, incomplete\ncoverage, privacy concerns, high latency, low reliability, and poor\nscalability. There are also methods depending on output probabilities, which\nare often inaccessible for closed-source LLMs like GPT models. This paper\npresents MetaQA, a self-contained hallucination detection approach that\nleverages metamorphic relation and prompt mutation. Unlike existing methods,\nMetaQA operates without any external resources and is compatible with both\nopen-source and closed-source LLMs. MetaQA is based on the hypothesis that if\nan LLM's response is a hallucination, the designed metamorphic relations will\nbe violated. We compare MetaQA with the state-of-the-art zero-resource\nhallucination detection method, SelfCheckGPT, across multiple datasets, and on\ntwo open-source and two closed-source LLMs. Our results reveal that MetaQA\noutperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the\nfour LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin\nranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and\n0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an\naverage F1-score of 0.435, compared to SelfCheckGPT's F1-score of 0.205,\nrepresenting an improvement rate of 112.2%. MetaQA also demonstrates\nsuperiority across all different categories of questions.", "published": "2025-02-20 19:44:33", "link": "http://arxiv.org/abs/2502.15844v2", "categories": ["cs.CL", "cs.LG", "68N30 (Primary) 68T50 (Secondary)", "I.2.7; D.2.5"], "primary_category": "cs.CL"}
{"title": "Verify when Uncertain: Beyond Self-Consistency in Black Box\n  Hallucination Detection", "abstract": "Large Language Models (LLMs) suffer from hallucination problems, which hinder\ntheir reliability in sensitive applications. In the black-box setting, several\nself-consistency-based techniques have been proposed for hallucination\ndetection. We empirically study these techniques and show that they achieve\nperformance close to that of a supervised (still black-box) oracle, suggesting\nlittle room for improvement within this paradigm. To address this limitation,\nwe explore cross-model consistency checking between the target model and an\nadditional verifier LLM. With this extra information, we observe improved\noracle performance compared to purely self-consistency-based methods. We then\npropose a budget-friendly, two-stage detection algorithm that calls the\nverifier model only for a subset of cases. It dynamically switches between\nself-consistency and cross-consistency based on an uncertainty interval of the\nself-consistency classifier. We provide a geometric interpretation of\nconsistency-based hallucination detection methods through the lens of kernel\nmean embeddings, offering deeper theoretical insights. Extensive experiments\nshow that this approach maintains high detection performance while\nsignificantly reducing computational cost.", "published": "2025-02-20 21:06:08", "link": "http://arxiv.org/abs/2502.15845v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs Consider Security? An Empirical Study on Responses to\n  Programming Questions", "abstract": "The widespread adoption of conversational LLMs for software development has\nraised new security concerns regarding the safety of LLM-generated content. Our\nmotivational study outlines ChatGPT's potential in volunteering\ncontext-specific information to the developers, promoting safe coding\npractices. Motivated by this finding, we conduct a study to evaluate the degree\nof security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and\nLlama 3. We prompt these LLMs with Stack Overflow questions that contain\nvulnerable code to evaluate whether they merely provide answers to the\nquestions or if they also warn users about the insecure code, thereby\ndemonstrating a degree of security awareness. Further, we assess whether LLM\nresponses provide information about the causes, exploits, and the potential\nfixes of the vulnerability, to help raise users' awareness. Our findings show\nthat all three models struggle to accurately detect and warn users about\nvulnerabilities, achieving a detection rate of only 12.6% to 40% across our\ndatasets. We also observe that the LLMs tend to identify certain types of\nvulnerabilities related to sensitive information exposure and improper input\nneutralization much more frequently than other types, such as those involving\nexternal control of file names or paths. Furthermore, when LLMs do issue\nsecurity warnings, they often provide more information on the causes, exploits,\nand fixes of vulnerabilities compared to Stack Overflow responses. Finally, we\nprovide an in-depth discussion on the implications of our findings and present\na CLI-based prompting tool that can be used to generate significantly more\nsecure LLM responses.", "published": "2025-02-20 02:20:06", "link": "http://arxiv.org/abs/2502.14202v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Effects of Prompt Length on Domain-specific Tasks for Large Language\n  Models", "abstract": "In recent years, Large Language Models have garnered significant attention\nfor their strong performance in various natural language tasks, such as machine\ntranslation and question answering. These models demonstrate an impressive\nability to generalize across diverse tasks. However, their effectiveness in\ntackling domain-specific tasks, such as financial sentiment analysis and\nmonetary policy understanding, remains a topic of debate, as these tasks often\nrequire specialized knowledge and precise reasoning. To address such\nchallenges, researchers design various prompts to unlock the models' abilities.\nBy carefully crafting input prompts, researchers can guide these models to\nproduce more accurate responses. Consequently, prompt engineering has become a\nkey focus of study. Despite the advancements in both models and prompt\nengineering, the relationship between the two-specifically, how prompt design\nimpacts models' ability to perform domain-specific tasks-remains underexplored.\nThis paper aims to bridge this research gap.", "published": "2025-02-20 04:42:06", "link": "http://arxiv.org/abs/2502.14255v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "primary_category": "cs.CL"}
{"title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning", "abstract": "Large language model (LLM)-based agents have shown promise in tackling\ncomplex tasks by interacting dynamically with the environment. Existing work\nprimarily focuses on behavior cloning from expert demonstrations and preference\nlearning through exploratory trajectory sampling. However, these methods often\nstruggle in long-horizon tasks, where suboptimal actions accumulate step by\nstep, causing agents to deviate from correct task trajectories. To address\nthis, we highlight the importance of timely calibration and the need to\nautomatically construct calibration trajectories for training agents. We\npropose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM\nagent learning. Specifically, STeCa identifies suboptimal actions through a\nstep-level reward comparison during exploration. It constructs calibrated\ntrajectories using LLM-driven reflection, enabling agents to learn from\nimproved decision-making processes. These calibrated trajectories, together\nwith successful trajectory data, are utilized for reinforced training.\nExtensive experiments demonstrate that STeCa significantly outperforms existing\nmethods. Further analysis highlights that step-level calibration enables agents\nto complete tasks with greater robustness. Our code and data are available at\nhttps://github.com/WangHanLinHenry/STeCa.", "published": "2025-02-20 05:28:44", "link": "http://arxiv.org/abs/2502.14276v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations\n  in Large Language Models", "abstract": "Advancements in Large Language Models (LLMs) and their increasing use in\nmedical question-answering necessitate rigorous evaluation of their\nreliability. A critical challenge lies in hallucination, where models generate\nplausible yet factually incorrect outputs. In the medical domain, this poses\nserious risks to patient safety and clinical decision-making. To address this,\nwe introduce MedHallu, the first benchmark specifically designed for medical\nhallucination detection. MedHallu comprises 10,000 high-quality question-answer\npairs derived from PubMedQA, with hallucinated answers systematically generated\nthrough a controlled pipeline. Our experiments show that state-of-the-art LLMs,\nincluding GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical,\nstruggle with this binary hallucination detection task, with the best model\nachieving an F1 score as low as 0.625 for detecting \"hard\" category\nhallucinations. Using bidirectional entailment clustering, we show that\nharder-to-detect hallucinations are semantically closer to ground truth.\nThrough experiments, we also show incorporating domain-specific knowledge and\nintroducing a \"not sure\" category as one of the answer categories improves the\nprecision and F1 scores by up to 38% relative to baselines.", "published": "2025-02-20 06:33:23", "link": "http://arxiv.org/abs/2502.14302v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Impact and Feasibility of Self-Confidence Shaping for AI-Assisted\n  Decision-Making", "abstract": "In AI-assisted decision-making, it is crucial but challenging for humans to\nappropriately rely on AI, especially in high-stakes domains such as finance and\nhealthcare. This paper addresses this problem from a human-centered perspective\nby presenting an intervention for self-confidence shaping, designed to\ncalibrate self-confidence at a targeted level. We first demonstrate the impact\nof self-confidence shaping by quantifying the upper-bound improvement in\nhuman-AI team performance. Our behavioral experiments with 121 participants\nshow that self-confidence shaping can improve human-AI team performance by\nnearly 50% by mitigating both over- and under-reliance on AI. We then introduce\na self-confidence prediction task to identify when our intervention is needed.\nOur results show that simple machine-learning models achieve 67% accuracy in\npredicting self-confidence. We further illustrate the feasibility of such\ninterventions. The observed relationship between sentiment and self-confidence\nsuggests that modifying sentiment could be a viable strategy for shaping\nself-confidence. Finally, we outline future research directions to support the\ndeployment of self-confidence shaping in a real-world scenario for effective\nhuman-AI collaboration.", "published": "2025-02-20 06:55:41", "link": "http://arxiv.org/abs/2502.14311v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Line Goes Up? Inherent Limitations of Benchmarks for Evaluating Large\n  Language Models", "abstract": "Large language models (LLMs) regularly demonstrate new and impressive\nperformance on a wide range of language, knowledge, and reasoning benchmarks.\nSuch rapid progress has led many commentators to argue that LLM general\ncognitive capabilities have likewise rapidly improved, with the implication\nthat such models are becoming progressively more capable on various real-world\ntasks. Here I summarise theoretical and empirical considerations to challenge\nthis narrative. I argue that inherent limitations with the benchmarking\nparadigm, along with specific limitations of existing benchmarks, render\nbenchmark performance highly unsuitable as a metric for generalisable\ncompetence over cognitive tasks. I also contend that alternative methods for\nassessing LLM capabilities, including adversarial stimuli and interpretability\ntechniques, have shown that LLMs do not have robust competence in many language\nand reasoning tasks, and often fail to learn representations which facilitate\ngeneralisable inferences. I conclude that benchmark performance should not be\nused as a reliable indicator of general LLM cognitive capabilities.", "published": "2025-02-20 07:13:29", "link": "http://arxiv.org/abs/2502.14318v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Affinity and Diversity: A Unified Metric for Demonstration Selection via\n  Internal Representations", "abstract": "The performance of In-Context Learning (ICL) is highly sensitive to the\nselected demonstrations. Existing approaches to demonstration selection\noptimize different objectives, yielding inconsistent results. To address this,\nwe propose a unified metric--affinity and diversity--that leverages ICL model's\ninternal representations. Our experiments show that both affinity and diversity\nstrongly correlate with test accuracies, indicating their effectiveness for\ndemonstration selection. Moreover, we show that our proposed metrics align well\nwith various previous works to unify the inconsistency.", "published": "2025-02-20 09:12:51", "link": "http://arxiv.org/abs/2502.14380v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Macro- and Micro-Hierarchical Transfer Learning Framework for\n  Cross-Domain Fake News Detection", "abstract": "Cross-domain fake news detection aims to mitigate domain shift and improve\ndetection performance by transferring knowledge across domains. Existing\napproaches transfer knowledge based on news content and user engagements from a\nsource domain to a target domain. However, these approaches face two main\nlimitations, hindering effective knowledge transfer and optimal fake news\ndetection performance. Firstly, from a micro perspective, they neglect the\nnegative impact of veracity-irrelevant features in news content when\ntransferring domain-shared features across domains. Secondly, from a macro\nperspective, existing approaches ignore the relationship between user\nengagement and news content, which reveals shared behaviors of common users\nacross domains and can facilitate more effective knowledge transfer. To address\nthese limitations, we propose a novel macro- and micro- hierarchical transfer\nlearning framework (MMHT) for cross-domain fake news detection. Firstly, we\npropose a micro-hierarchical disentangling module to disentangle\nveracity-relevant and veracity-irrelevant features from news content in the\nsource domain for improving fake news detection performance in the target\ndomain. Secondly, we propose a macro-hierarchical transfer learning module to\ngenerate engagement features based on common users' shared behaviors in\ndifferent domains for improving effectiveness of knowledge transfer. Extensive\nexperiments on real-world datasets demonstrate that our framework significantly\noutperforms the state-of-the-art baselines.", "published": "2025-02-20 09:39:44", "link": "http://arxiv.org/abs/2502.14403v2", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "PredictaBoard: Benchmarking LLM Score Predictability", "abstract": "Despite possessing impressive skills, Large Language Models (LLMs) often fail\nunpredictably, demonstrating inconsistent success in even basic common sense\nreasoning tasks. This unpredictability poses a significant challenge to\nensuring their safe deployment, as identifying and operating within a reliable\n\"safe zone\" is essential for mitigating risks. To address this, we present\nPredictaBoard, a novel collaborative benchmarking framework designed to\nevaluate the ability of score predictors (referred to as assessors) to\nanticipate LLM errors on specific task instances (i.e., prompts) from existing\ndatasets. PredictaBoard evaluates pairs of LLMs and assessors by considering\nthe rejection rate at different tolerance errors. As such, PredictaBoard\nstimulates research into developing better assessors and making LLMs more\npredictable, not only with a higher average performance. We conduct\nillustrative experiments using baseline assessors and state-of-the-art LLMs.\nPredictaBoard highlights the critical need to evaluate predictability alongside\nperformance, paving the way for safer AI systems where errors are not only\nminimised but also anticipated and effectively mitigated. Code for our\nbenchmark can be found at\nhttps://github.com/Kinds-of-Intelligence-CFI/PredictaBoard", "published": "2025-02-20 10:52:38", "link": "http://arxiv.org/abs/2502.14445v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Enhancing Smart Environments with Context-Aware Chatbots using Large\n  Language Models", "abstract": "This work presents a novel architecture for context-aware interactions within\nsmart environments, leveraging Large Language Models (LLMs) to enhance user\nexperiences. Our system integrates user location data obtained through UWB tags\nand sensor-equipped smart homes with real-time human activity recognition (HAR)\nto provide a comprehensive understanding of user context. This contextual\ninformation is then fed to an LLM-powered chatbot, enabling it to generate\npersonalised interactions and recommendations based on the user's current\nactivity and environment. This approach moves beyond traditional static chatbot\ninteractions by dynamically adapting to the user's real-time situation. A case\nstudy conducted from a real-world dataset demonstrates the feasibility and\neffectiveness of our proposed architecture, showcasing its potential to create\nmore intuitive and helpful interactions within smart homes. The results\nhighlight the significant benefits of integrating LLM with real-time activity\nand location data to deliver personalised and contextually relevant user\nexperiences.", "published": "2025-02-20 11:46:51", "link": "http://arxiv.org/abs/2502.14469v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation", "abstract": "Jailbreak attacks, where harmful prompts bypass generative models' built-in\nsafety, raise serious concerns about model vulnerability. While many defense\nmethods have been proposed, the trade-offs between safety and helpfulness, and\ntheir application to Large Vision-Language Models (LVLMs), are not well\nunderstood. This paper systematically examines jailbreak defenses by reframing\nthe standard generation task as a binary classification problem to assess model\nrefusal tendencies for both harmful and benign queries. We identify two key\ndefense mechanisms: safety shift, which increases refusal rates across all\nqueries, and harmfulness discrimination, which improves the model's ability to\ndistinguish between harmful and benign inputs. Using these mechanisms, we\ndevelop two ensemble defense strategies-inter-mechanism ensembles and\nintra-mechanism ensembles-to balance safety and helpfulness. Experiments on the\nMM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these\nstrategies effectively improve model safety or optimize the trade-off between\nsafety and helpfulness.", "published": "2025-02-20 12:07:40", "link": "http://arxiv.org/abs/2502.14486v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Stories that (are) Move(d by) Markets: A Causal Exploration of Market\n  Shocks and Semantic Shifts across Different Partisan Groups", "abstract": "Macroeconomic fluctuations and the narratives that shape them form a mutually\nreinforcing cycle: public discourse can spur behavioural changes leading to\neconomic shifts, which then result in changes in the stories that propagate. We\nshow that shifts in semantic embedding space can be causally linked to\nfinancial market shocks -- deviations from the expected market behaviour.\nFurthermore, we show how partisanship can influence the predictive power of\ntext for market fluctuations and shape reactions to those same shocks. We also\nprovide some evidence that text-based signals are particularly salient during\nunexpected events such as COVID-19, highlighting the value of language data as\nan exogenous variable in economic forecasting. Our findings underscore the\nbidirectional relationship between news outlets and market shocks, offering a\nnovel empirical approach to studying their effect on each other.", "published": "2025-02-20 12:26:56", "link": "http://arxiv.org/abs/2502.14497v1", "categories": ["cs.CL", "cs.CE", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents", "abstract": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for\nevaluating and developing LLM agents on AI research tasks. This is the first\nGym environment for machine learning (ML) tasks, enabling research on\nreinforcement learning (RL) algorithms for training such agents. MLGym-bench\nconsists of 13 diverse and open-ended AI research tasks from diverse domains\nsuch as computer vision, natural language processing, reinforcement learning,\nand game theory. Solving these tasks requires real-world AI research skills\nsuch as generating new ideas and hypotheses, creating and processing data,\nimplementing ML methods, training models, running experiments, analyzing the\nresults, and iterating through this process to improve on a given task. We\nevaluate a number of frontier large language models (LLMs) on our benchmarks\nsuch as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5\nPro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate\nmodels or agents, generate synthetic data at scale, as well as develop new\nlearning algorithms for training agents on AI research tasks. We find that\ncurrent frontier models can improve on the given baselines, usually by finding\nbetter hyperparameters, but do not generate novel hypotheses, algorithms,\narchitectures, or substantial improvements. We open-source our framework and\nbenchmark to facilitate future research in advancing the AI research\ncapabilities of LLM agents.", "published": "2025-02-20 12:28:23", "link": "http://arxiv.org/abs/2502.14499v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multiscale Byte Language Models -- A Hierarchical Architecture for\n  Causal Million-Length Sequence Modeling", "abstract": "Bytes form the basis of the digital world and thus are a promising building\nblock for multimodal foundation models. Recently, Byte Language Models (BLMs)\nhave emerged to overcome tokenization, yet the excessive length of bytestreams\nrequires new architectural paradigms. Therefore, we present the Multiscale Byte\nLanguage Model (MBLM), a model-agnostic hierarchical decoder stack that allows\ntraining with context windows of $5$M bytes on single GPU in full model\nprecision. We thoroughly examine MBLM's performance with Transformer and Mamba\nblocks on both unimodal and multimodal tasks. Our experiments demonstrate that\nhybrid architectures are efficient in handling extremely long byte sequences\nduring training while achieving near-linear generational efficiency. To the\nbest of our knowledge, we present the first evaluation of BLMs on visual Q\\&A\ntasks and find that, despite serializing images and the absence of an encoder,\na MBLM with pure next token prediction can match custom CNN-LSTM architectures\nwith designated classification heads. We show that MBLMs exhibit strong\nadaptability in integrating diverse data representations, including pixel and\nimage filestream bytes, underlining their potential toward omnimodal foundation\nmodels. Source code is publicly available at:\nhttps://github.com/ai4sd/multiscale-byte-lm", "published": "2025-02-20 13:31:50", "link": "http://arxiv.org/abs/2502.14553v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Less is More: Improving LLM Alignment via Preference Data Selection", "abstract": "Direct Preference Optimization (DPO) has emerged as a promising approach for\naligning large language models with human preferences. While prior work mainly\nextends DPO from the aspect of the objective function, we instead improve DPO\nfrom the largely overlooked but critical aspect of data selection.\nSpecifically, we address the issue of parameter shrinkage caused by noisy data\nby proposing a novel margin-maximization principle for dataset curation in DPO\ntraining. To accurately estimate margins for data selection, we propose a\ndual-margin guided approach that considers both external reward margins and\nimplicit DPO reward margins. Extensive experiments demonstrate that our method\nreduces computational cost dramatically while improving performance.\nRemarkably, by using just 10\\% of the Ultrafeedback dataset, our approach\nachieves 3\\% to 8\\% improvements across various Llama and Mistral series models\non the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends\nto iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data,\nwhile further reducing training time. These results highlight the potential of\ndata selection strategies for advancing preference optimization.", "published": "2025-02-20 13:45:17", "link": "http://arxiv.org/abs/2502.14560v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Statistical Case Against Empirical Human-AI Alignment", "abstract": "Empirical human-AI alignment aims to make AI systems act in line with\nobserved human behavior. While noble in its goals, we argue that empirical\nalignment can inadvertently introduce statistical biases that warrant caution.\nThis position paper thus advocates against naive empirical alignment, offering\nprescriptive alignment and a posteriori empirical alignment as alternatives. We\nsubstantiate our principled argument by tangible examples like human-centric\ndecoding of language models.", "published": "2025-02-20 14:12:18", "link": "http://arxiv.org/abs/2502.14581v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.OT"], "primary_category": "cs.AI"}
{"title": "Reward Models Identify Consistency, Not Causality", "abstract": "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation.", "published": "2025-02-20 14:57:14", "link": "http://arxiv.org/abs/2502.14619v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Explanations of Large Language Models Explain Language Representations\n  in the Brain", "abstract": "Large language models (LLMs) not only exhibit human-like performance but also\nshare computational principles with the brain's language processing mechanisms.\nWhile prior research has focused on mapping LLMs' internal representations to\nneural activity, we propose a novel approach using explainable AI (XAI) to\nstrengthen this link. Applying attribution methods, we quantify the influence\nof preceding words on LLMs' next-word predictions and use these explanations to\npredict fMRI data from participants listening to narratives. We find that\nattribution methods robustly predict brain activity across the language\nnetwork, revealing a hierarchical pattern: explanations from early layers align\nwith the brain's initial language processing stages, while later layers\ncorrespond to more advanced stages. Additionally, layers with greater influence\non next-word prediction$\\unicode{x2014}$reflected in higher attribution\nscores$\\unicode{x2014}$demonstrate stronger brain alignment. These results\nunderscore XAI's potential for exploring the neural basis of language and\nsuggest brain alignment for assessing the biological plausibility of\nexplanation methods.", "published": "2025-02-20 16:05:45", "link": "http://arxiv.org/abs/2502.14671v3", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "From Knowledge Generation to Knowledge Verification: Examining the\n  BioMedical Generative Capabilities of ChatGPT", "abstract": "The generative capabilities of LLM models offer opportunities for\naccelerating tasks but raise concerns about the authenticity of the knowledge\nthey produce. To address these concerns, we present a computational approach\nthat evaluates the factual accuracy of biomedical knowledge generated by an\nLLM. Our approach consists of two processes: generating disease-centric\nassociations and verifying these associations using the semantic framework of\nbiomedical ontologies. Using ChatGPT as the selected LLM, we designed\nprompt-engineering processes to establish linkages between diseases and related\ndrugs, symptoms, and genes, and assessed consistency across multiple ChatGPT\nmodels (e.g., GPT-turbo, GPT-4, etc.). Experimental results demonstrate high\naccuracy in identifying disease terms (88%-97%), drug names (90%-91%), and\ngenetic information (88%-98%). However, symptom term identification was notably\nlower (49%-61%), due to the informal and verbose nature of symptom\ndescriptions, which hindered effective semantic matching with the formal\nlanguage of specialized ontologies. Verification of associations reveals\nliterature coverage rates of 89%-91% for disease-drug and disease-gene pairs,\nwhile symptom-related associations exhibit lower coverage (49%-62%).", "published": "2025-02-20 16:39:57", "link": "http://arxiv.org/abs/2502.14714v2", "categories": ["cs.AI", "cs.CL", "cs.IR", "I.2; I.2.4; I.2.7"], "primary_category": "cs.AI"}
{"title": "Harnessing PDF Data for Improving Japanese Large Multimodal Models", "abstract": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nEnglish, but their effectiveness in Japanese remains limited due to the lack of\nhigh-quality training data. Current Japanese LMMs often rely on translated\nEnglish datasets, restricting their ability to capture Japan-specific cultural\nknowledge. To address this, we explore the potential of Japanese PDF data as a\ntraining resource, an area that remains largely underutilized. We introduce a\nfully automated pipeline that leverages pretrained models to extract image-text\npairs from PDFs through layout analysis, OCR, and vision-language pairing,\nremoving the need for manual annotation. Additionally, we construct instruction\ndata from extracted image-text pairs to enrich the training data. To evaluate\nthe effectiveness of PDF-derived data, we train Japanese LMMs and assess their\nperformance on the Japanese LMM Benchmark. Our results demonstrate substantial\nimprovements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench.\nFurther analysis highlights the impact of PDF-derived data on various factors,\nsuch as model size and language models, reinforcing its value as a multimodal\nresource for Japanese LMMs. We plan to make the source code and data publicly\navailable upon acceptance.", "published": "2025-02-20 17:59:59", "link": "http://arxiv.org/abs/2502.14778v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ReVision: A Dataset and Baseline VLM for Privacy-Preserving\n  Task-Oriented Visual Instruction Rewriting", "abstract": "Efficient and privacy-preserving multimodal interaction is essential as AR,\nVR, and modern smartphones with powerful cameras become primary interfaces for\nhuman-computer communication. Existing powerful large vision-language models\n(VLMs) enabling multimodal interaction often rely on cloud-based processing,\nraising significant concerns about (1) visual privacy by transmitting sensitive\nvision data to servers, and (2) their limited real-time, on-device usability.\nThis paper explores Visual Instruction Rewriting, a novel approach that\ntransforms multimodal instructions into text-only commands, allowing seamless\nintegration of lightweight on-device instruction rewriter VLMs (250M\nparameters) with existing conversational AI systems, enhancing vision data\nprivacy. To achieve this, we present a dataset of over 39,000 examples across\n14 domains and develop a compact VLM, pretrained on image captioning datasets\nand fine-tuned for instruction rewriting. Experimental results, evaluated\nthrough NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic\nparsing analysis, demonstrate that even a quantized version of the model\n(<500MB storage footprint) can achieve effective instruction rewriting, thus\nenabling privacy-focused, multimodal AI applications.", "published": "2025-02-20 18:01:41", "link": "http://arxiv.org/abs/2502.14780v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Rapid Word Learning Through Meta In-Context Learning", "abstract": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.", "published": "2025-02-20 18:11:38", "link": "http://arxiv.org/abs/2502.14791v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimizing Model Selection for Compound AI Systems", "abstract": "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.", "published": "2025-02-20 18:36:25", "link": "http://arxiv.org/abs/2502.14815v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables", "abstract": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndiverse domains, yet their application in e-commerce remains underexplored due\nto a lack of domain-specific datasets. To address this gap, we introduce\neC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce,\nincluding detailed product attributes and user-specific queries. Leveraging\neC-Tab2Text, we focus on text generation from product tables, enabling LLMs to\nproduce high-quality, attribute-specific product reviews from structured\ntabular data. Fine-tuned models were rigorously evaluated using standard\nTable2Text metrics, alongside correctness, faithfulness, and fluency\nassessments. Our results demonstrate substantial improvements in generating\ncontextually accurate reviews, highlighting the transformative potential of\ntailored datasets and fine-tuning methodologies in optimizing e-commerce\nworkflows. This work highlights the potential of LLMs in e-commerce workflows\nand the essential role of domain-specific datasets in tailoring them to\nindustry-specific challenges.", "published": "2025-02-20 18:41:48", "link": "http://arxiv.org/abs/2502.14820v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in\n  Vision-Language Models", "abstract": "Existing Large Vision-Language Models (LVLMs) can process inputs with context\nlengths up to 128k visual and text tokens, yet they struggle to generate\ncoherent outputs beyond 1,000 words. We find that the primary limitation is the\nabsence of long output examples during supervised fine-tuning (SFT). To tackle\nthis issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158\nexamples, each with multiple input images, an instruction, and corresponding\noutputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that\nmaintain high-fidelity to the input images, we employ Direct Preference\nOptimization (DPO) to the SFT model. Given the high cost of collecting human\nfeedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which\nbreaks long outputs into segments and uses iterative corrections to form\npreference pairs with the original outputs. Additionally, we develop\nMMLongBench-Write, a benchmark featuring six tasks to evaluate the\nlong-generation capabilities of VLMs. Our 7B parameter model, trained with\nLongWriter-V-22k and IterDPO, achieves impressive performance on this\nbenchmark, outperforming larger proprietary models like GPT-4o. Code and data:\nhttps://github.com/THU-KEG/LongWriter-V", "published": "2025-02-20 18:47:36", "link": "http://arxiv.org/abs/2502.14834v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FR-Spec: Accelerating Large-Vocabulary Language Models via\n  Frequency-Ranked Speculative Sampling", "abstract": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2. Code available at\nhttps://github.com/thunlp/FR-Spec.", "published": "2025-02-20 18:58:10", "link": "http://arxiv.org/abs/2502.14856v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpretable Text Embeddings and Text Similarity Explanation: A Primer", "abstract": "Text embeddings and text embedding models are a backbone of many AI and NLP\nsystems, particularly those involving search. However, interpretability\nchallenges persist, especially in explaining obtained similarity scores, which\nis crucial for applications requiring transparency. In this paper, we give a\nstructured overview of interpretability methods specializing in explaining\nthose similarity scores, an emerging research area. We study the methods'\nindividual ideas and techniques, evaluating their potential for improving\ninterpretability of text embeddings and explaining predicted similarities.", "published": "2025-02-20 18:59:34", "link": "http://arxiv.org/abs/2502.14862v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Socratic RAG Approach to Connect Natural Language Queries on Research\n  Topics with Knowledge Organization Systems", "abstract": "In this paper, we propose a Retrieval Augmented Generation (RAG) agent that\nmaps natural language queries about research topics to precise,\nmachine-interpretable semantic entities. Our approach combines RAG with\nSocratic dialogue to align a user's intuitive understanding of research topics\nwith established Knowledge Organization Systems (KOSs). The proposed approach\nwill effectively bridge \"little semantics\" (domain-specific KOS structures)\nwith \"big semantics\" (broad bibliometric repositories), making complex academic\ntaxonomies more accessible. Such agents have the potential for broad use. We\nillustrate with a sample application called CollabNext, which is a\nperson-centric knowledge graph connecting people, organizations, and research\ntopics. We further describe how the application design has an intentional focus\non HBCUs and emerging researchers to raise visibility of people historically\nrendered invisible in the current science system.", "published": "2025-02-20 19:58:59", "link": "http://arxiv.org/abs/2502.15005v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.7; F.4.1"], "primary_category": "cs.CL"}
{"title": "Obliviate: Efficient Unmemorization for Protecting Intellectual Property\n  in Large Language Models", "abstract": "Recent copyright agreements between AI companies and content creators have\nhighlighted the need for precise control over language models' ability to\nreproduce copyrighted content. While existing approaches rely on either\ncomplete concept removal through unlearning or simple output filtering, we\npropose Obliviate, a novel post-training technique that selectively prevents\nverbatim reproduction of specific text while preserving semantic understanding.\n  Obliviate operates by selecting tokens within memorized sequences and\nmodifying the model's probability distribution to prevent exact reproduction\nwhile maintaining contextual understanding. We evaluate Obliviate on multiple\nlarge language models (LLaMA-3.1 8B, LLaMA-3.1-instruct 8B, Qwen-2.5-7B, and\nYi-1.5 6B) across both synthetic memorization tasks and organic copyright\ncontent. Our results demonstrate that Obliviate achieves orders of magnitude\nreduction, e.g., 100x, in verbatim memorization while maintaining model\nperformance within 1% of baseline on standard benchmarks (HellaSwag, MMLU,\nTruthfulQA, and Winogrande). This makes Obliviate particularly suitable for\npractical deployment scenarios where companies need to efficiently address\ncopyright concerns in pretrained models without compromising their general\ncapabilities.", "published": "2025-02-20 20:02:56", "link": "http://arxiv.org/abs/2502.15010v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal\n  Models via Human Feedback", "abstract": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their\ninteractive intelligence with human users, which is vital for developing\ngeneral-purpose AI assistants. We design InterFeedback, an interactive\nframework, which can be applied to any LMM and dataset to assess this ability\nautonomously. On top of this, we introduce InterFeedback-Bench which evaluates\ninteractive intelligence using two representative datasets, MMMU-Pro and\nMathVerse, to test 10 different open-source LMMs. Additionally, we present\nInterFeedback-Human, a newly collected dataset of 120 cases designed for\nmanually testing interactive performance in leading models such as OpenAI-o1\nand Claude-3.5-Sonnet. Our evaluation results indicate that even the\nstate-of-the-art LMM, OpenAI-o1, struggles to refine its responses based on\nhuman feedback, achieving an average score of less than 50%. Our findings point\nto the need for methods that can enhance LMMs' capabilities to interpret and\nbenefit from feedback.", "published": "2025-02-20 20:27:06", "link": "http://arxiv.org/abs/2502.15027v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Can Hallucination Correction Improve Video-Language Alignment?", "abstract": "Large Vision-Language Models often generate hallucinated content that is not\ngrounded in its visual inputs. While prior work focuses on mitigating\nhallucinations, we instead explore leveraging hallucination correction as a\ntraining objective to improve video-language alignment. We introduce HACA, a\nself-training framework learning to correct hallucinations in descriptions that\ndo not align with the video content. By identifying and correcting\ninconsistencies, HACA enhances the model's ability to align video and textual\nrepresentations for spatio-temporal reasoning. Our experimental results show\nconsistent gains in video-caption binding and text-to-video retrieval tasks,\ndemonstrating that hallucination correction-inspired tasks serve as an\neffective strategy for improving vision and language alignment.", "published": "2025-02-20 22:43:22", "link": "http://arxiv.org/abs/2502.15079v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning", "abstract": "User specifications or legal frameworks often require information to be\nremoved from pretrained models, including large language models (LLMs). This\nrequires deleting or \"forgetting\" a set of data points from an already-trained\nmodel, which typically degrades its performance on other data points. Thus, a\nbalance must be struck between removing information and keeping the model's\nother abilities intact, with a failure to balance this trade-off leading to\npoor deletion or an unusable model. To this end, we propose UPCORE\n(Utility-Preserving Coreset Selection), a method-agnostic data selection\nframework for mitigating collateral damage during unlearning. Finding that the\nmodel damage is correlated with the variance of the model's representations on\nthe forget set, we selectively prune the forget set to remove outliers, thereby\nminimizing model degradation after unlearning. We evaluate UPCORE across three\nstandard unlearning methods consistently achieving a superior balance between\nthe competing objectives of deletion efficacy and model preservation. To better\nevaluate this trade-off, we introduce a new metric, measuring the\narea-under-the-curve (AUC) across standard metrics. We find that UPCORE\nimproves both standard metrics and AUC, benefitting from positive transfer\nbetween the coreset and pruned points while reducing negative transfer from the\nforget set to points outside of it.", "published": "2025-02-20 22:51:10", "link": "http://arxiv.org/abs/2502.15082v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Tabular Embeddings for Tables with Bi-Dimensional Hierarchical Metadata\n  and Nesting", "abstract": "Embeddings serve as condensed vector representations for real-world entities,\nfinding applications in Natural Language Processing (NLP), Computer Vision, and\nData Management across diverse downstream tasks. Here, we introduce novel\nspecialized embeddings optimized, and explicitly tailored to encode the\nintricacies of complex 2-D context in tables, featuring horizontal, vertical\nhierarchical metadata, and nesting. To accomplish that we define the\nBi-dimensional tabular coordinates, separate horizontal, vertical metadata and\ndata contexts by introducing a new visibility matrix, encode units and nesting\nthrough the embeddings specifically optimized for mimicking intricacies of such\ncomplex structured data. Through evaluation on 5 large-scale structured\ndatasets and 3 popular downstream tasks, we observed that our solution\noutperforms the state-of-the-art models with the significant MAP delta of up to\n0.28. GPT-4 LLM+RAG slightly outperforms us with MRR delta of up to 0.1, while\nwe outperform it with the MAP delta of up to 0.42.", "published": "2025-02-20 01:04:11", "link": "http://arxiv.org/abs/2502.15819v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InductionBench: LLMs Fail in the Simplest Complexity Class", "abstract": "Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark.", "published": "2025-02-20 03:48:00", "link": "http://arxiv.org/abs/2502.15823v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "DeepRTL: Bridging Verilog Understanding and Generation with a Unified\n  Representation Model", "abstract": "Recent advancements in large language models (LLMs) have shown significant\npotential for automating hardware description language (HDL) code generation\nfrom high-level natural language instructions. While fine-tuning has improved\nLLMs' performance in hardware design tasks, prior efforts have largely focused\non Verilog generation, overlooking the equally critical task of Verilog\nunderstanding. Furthermore, existing models suffer from weak alignment between\nnatural language descriptions and Verilog code, hindering the generation of\nhigh-quality, synthesizable designs. To address these issues, we present\nDeepRTL, a unified representation model that excels in both Verilog\nunderstanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a\ncomprehensive dataset that aligns Verilog code with rich, multi-level natural\nlanguage descriptions. We also introduce the first benchmark for Verilog\nunderstanding and take the initiative to apply embedding similarity and GPT\nScore to evaluate the models' understanding capabilities. These metrics capture\nsemantic similarity more accurately than traditional methods like BLEU and\nROUGE, which are limited to surface-level n-gram overlaps. By adapting\ncurriculum learning to train DeepRTL, we enable it to significantly outperform\nGPT-4 in Verilog understanding tasks, while achieving performance on par with\nOpenAI's o1-preview model in Verilog generation tasks.", "published": "2025-02-20 11:07:55", "link": "http://arxiv.org/abs/2502.15832v1", "categories": ["cs.AR", "cs.CL", "cs.LG"], "primary_category": "cs.AR"}
{"title": "Pragmatic Reasoning improves LLM Code Generation", "abstract": "Large Language Models (LLMs) have demonstrated impressive potential in\ntranslating natural language (NL) instructions into program code. However, user\ninstructions often contain inherent ambiguities, making it challenging for LLMs\nto generate code that accurately reflects the user's true intent. To address\nthis challenge, researchers have proposed to produce multiple candidates of the\nprogram code and then rerank them to identify the best solution. In this paper,\nwe propose CodeRSA, a novel code candidate reranking mechanism built upon the\nRational Speech Act (RSA) framework, designed to guide LLMs toward more\ncomprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using\none of the latest LLMs on a popular code generation dataset. Our experiment\nresults show that CodeRSA consistently outperforms common baselines, surpasses\nthe state-of-the-art approach in most cases, and demonstrates robust overall\nperformance. These findings underscore the effectiveness of integrating\npragmatic reasoning into code candidate reranking, offering a promising\ndirection for enhancing code generation quality in LLMs.", "published": "2025-02-20 12:44:26", "link": "http://arxiv.org/abs/2502.15835v2", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Mechanistic Understanding of Language Models in Syntactic Code\n  Completion", "abstract": "Recently, language models (LMs) have shown impressive proficiency in code\ngeneration tasks, especially when fine-tuned on code-specific datasets,\ncommonly known as Code LMs. However, our understanding of the internal\ndecision-making processes of Code LMs, such as how they use their (syntactic or\nsemantic) knowledge, remains limited, which could lead to unintended harm as\nthey are increasingly used in real life. This motivates us to conduct one of\nthe first Mechanistic Interpretability works to understand how Code LMs perform\na syntactic completion task, specifically the closing parenthesis task, on the\nCodeLlama-7b model (Roziere et al. 2023). Our findings reveal that the model\nrequires middle-later layers until it can confidently predict the correct label\nfor the closing parenthesis task. Additionally, we identify that while both\nmulti-head attention (MHA) and feed-forward (FF) sub-layers play essential\nroles, MHA is particularly crucial. Furthermore, we also discover attention\nheads that keep track of the number of already closed parentheses precisely but\nmay or may not promote a correct number of closing parentheses that are still\nmissing, leading to a positive or negative impact on the model's performance.", "published": "2025-02-20 21:35:20", "link": "http://arxiv.org/abs/2502.18499v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.SE"}
{"title": "An LLM-Based Approach for Insight Generation in Data Analysis", "abstract": "Generating insightful and actionable information from databases is critical\nin data analysis. This paper introduces a novel approach using Large Language\nModels (LLMs) to automatically generate textual insights. Given a multi-table\ndatabase as input, our method leverages LLMs to produce concise, text-based\ninsights that reflect interesting patterns in the tables. Our framework\nincludes a Hypothesis Generator to formulate domain-relevant questions, a Query\nAgent to answer such questions by generating SQL queries against a database,\nand a Summarization module to verbalize the insights. The insights are\nevaluated for both correctness and subjective insightfulness using a hybrid\nmodel of human judgment and automated metrics. Experimental results on public\nand enterprise databases demonstrate that our approach generates more\ninsightful insights than other approaches while maintaining correctness.", "published": "2025-02-20 17:09:59", "link": "http://arxiv.org/abs/2503.11664v1", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "Multimodal Quantitative Language for Generative Recommendation", "abstract": "Generative recommendation has emerged as a promising paradigm aiming at\ndirectly generating the identifiers of the target candidates. Most existing\nmethods attempt to leverage prior knowledge embedded in Pre-trained Language\nModels (PLMs) to improve the recommendation performance. However, they often\nfail to accommodate the differences between the general linguistic knowledge of\nPLMs and the specific needs of recommendation systems. Moreover, they rarely\nconsider the complementary knowledge between the multimodal information of\nitems, which represents the multi-faceted preferences of users. To facilitate\nefficient recommendation knowledge transfer, we propose a novel approach called\nMultimodal Quantitative Language for Generative Recommendation (MQL4GRec). Our\nkey idea is to transform items from different domains and modalities into a\nunified language, which can serve as a bridge for transferring recommendation\nknowledge. Specifically, we first introduce quantitative translators to convert\nthe text and image content of items from various domains into a new and concise\nlanguage, known as quantitative language, with all items sharing the same\nvocabulary. Then, we design a series of quantitative language generation tasks\nto enrich quantitative language with semantic information and prior knowledge.\nFinally, we achieve the transfer of recommendation knowledge from different\ndomains and modalities to the recommendation task through pre-training and\nfine-tuning. We evaluate the effectiveness of MQL4GRec through extensive\nexperiments and comparisons with existing methods, achieving improvements over\nthe baseline by 11.18\\%, 14.82\\%, and 7.95\\% on the NDCG metric across three\ndifferent datasets, respectively.", "published": "2025-02-20 09:29:30", "link": "http://arxiv.org/abs/2504.05314v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention", "abstract": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.", "published": "2025-02-20 18:59:52", "link": "http://arxiv.org/abs/2502.14866v1", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG", "cs.PF"], "primary_category": "cs.CL"}
{"title": "KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and\n  Document Understanding", "abstract": "With the growing adoption of Retrieval-Augmented Generation (RAG) in document\nprocessing, robust text recognition has become increasingly critical for\nknowledge extraction. While OCR (Optical Character Recognition) for English and\nother languages benefits from large datasets and well-established benchmarks,\nArabic OCR faces unique challenges due to its cursive script, right-to-left\ntext flow, and complex typographic and calligraphic features. We present\nKITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in\ncurrent evaluation systems. Our benchmark comprises 8,809 samples across 9\nmajor domains and 36 sub-domains, encompassing diverse document types including\nhandwritten text, structured tables, and specialized coverage of 21 chart types\nfor business intelligence. Our findings show that modern vision-language models\n(such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like\nEasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate\n(CER). Furthermore, we highlight significant limitations of current Arabic OCR\nmodels, particularly in PDF-to-Markdown conversion, where the best model\nGemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in\naccurately recognizing Arabic text, including issues with complex fonts,\nnumeral recognition errors, word elongation, and table structure detection.\nThis work establishes a rigorous evaluation framework that can drive\nimprovements in Arabic document analysis methods and bridge the performance gap\nwith English OCR technologies.", "published": "2025-02-20 18:41:23", "link": "http://arxiv.org/abs/2502.14949v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Adaptive Convolution for CNN-based Speech Enhancement Models", "abstract": "Deep learning-based speech enhancement methods have significantly improved\nspeech quality and intelligibility. Convolutional neural networks (CNNs) have\nbeen proven to be essential components of many high-performance models. In this\npaper, we introduce adaptive convolution, an efficient and versatile\nconvolutional module that enhances the model's capability to adaptively\nrepresent speech signals. Adaptive convolution performs frame-wise causal\ndynamic convolution, generating time-varying kernels for each frame by\nassembling multiple parallel candidate kernels. A Lightweight attention\nmechanism leverages both current and historical information to assign adaptive\nweights to each candidate kernel, guiding their aggregation. This enables the\nconvolution operation to adapt to frame-level speech spectral features, leading\nto more efficient extraction and reconstruction. Experimental results on\nvarious CNN-based models demonstrate that adaptive convolution significantly\nimproves the performance with negligible increases in computational complexity,\nespecially for lightweight models. Furthermore, we propose the adaptive\nconvolutional recurrent network (AdaptCRN), an ultra-lightweight model that\nincorporates adaptive convolution and an efficient encoder-decoder design,\nachieving superior performance compared to models with similar or even higher\ncomputational costs.", "published": "2025-02-20 03:39:43", "link": "http://arxiv.org/abs/2502.14224v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Differentiable Black-box and Gray-box Modeling of Nonlinear Audio\n  Effects", "abstract": "Audio effects are extensively used at every stage of audio and music content\ncreation. The majority of differentiable audio effects modeling approaches fall\ninto the black-box or gray-box paradigms; and most models have been proposed\nand applied to nonlinear effects like guitar amplifiers, overdrive, distortion,\nfuzz and compressor. Although a plethora of architectures have been introduced\nfor the task at hand there is still lack of understanding on the state of the\nart, since most publications experiment with one type of nonlinear audio effect\nand a very small number of devices.\n  In this work we aim to shed light on the audio effects modeling landscape by\ncomparing black-box and gray-box architectures on a large number of nonlinear\naudio effects, identifying the most suitable for a wide range of devices. In\nthe process, we also: introduce time-varying gray-box models and propose models\nfor compressor, distortion and fuzz, publish a large dataset for audio effects\nresearch - ToneTwist AFx https://github.com/mcomunita/tonetwist-afx-dataset -\nthat is also the first open to community contributions, evaluate models on a\nvariety of metrics and conduct extensive subjective evaluation. Code\nhttps://github.com/mcomunita/nablafx and supplementary material\nhttps://github.com/mcomunita/nnlinafx-supp-material are also available.", "published": "2025-02-20 09:43:16", "link": "http://arxiv.org/abs/2502.14405v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ChunkFormer: Masked Chunking Conformer For Long-Form Speech\n  Transcription", "abstract": "Deploying ASR models at an industrial scale poses significant challenges in\nhardware resource management, especially for long-form transcription tasks\nwhere audio may last for hours. Large Conformer models, despite their\ncapabilities, are limited to processing only 15 minutes of audio on an 80GB\nGPU. Furthermore, variable input lengths worsen inefficiencies, as standard\nbatching leads to excessive padding, increasing resource consumption and\nexecution time. To address this, we introduce ChunkFormer, an efficient ASR\nmodel that uses chunk-wise processing with relative right context, enabling\nlong audio transcriptions on low-memory GPUs. ChunkFormer handles up to 16\nhours of audio on an 80GB GPU, 1.5x longer than the current state-of-the-art\nFastConformer, while also boosting long-form transcription performance with up\nto 7.7% absolute reduction on word error rate and maintaining accuracy on\nshorter tasks compared to Conformer. By eliminating the need for padding in\nstandard batching, ChunkFormer's masked batching technique reduces execution\ntime and memory usage by more than 3x in batch processing, substantially\nreducing costs for a wide range of ASR systems, particularly regarding GPU\nresources for models serving in real-world applications.", "published": "2025-02-20 16:06:06", "link": "http://arxiv.org/abs/2502.14673v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SegAug: CTC-Aligned Segmented Augmentation For Robust RNN-Transducer\n  Based Speech Recognition", "abstract": "RNN-Transducer (RNN-T) is a widely adopted architecture in speech\nrecognition, integrating acoustic and language modeling in an end-to-end\nframework. However, the RNN-T predictor tends to over-rely on consecutive word\ndependencies in training data, leading to high deletion error rates,\nparticularly with less common or out-of-domain phrases. Existing solutions,\nsuch as regularization and data augmentation, often compromise other aspects of\nperformance. We propose SegAug, an alignment-based augmentation technique that\ngenerates contextually varied audio-text pairs with low sentence-level\nsemantics. This method encourages the model to focus more on acoustic features\nwhile diversifying the learned textual patterns of its internal language model,\nthereby reducing deletion errors and enhancing overall performance. Evaluations\non the LibriSpeech and Tedlium-v3 datasets demonstrate a relative WER reduction\nof up to 12.5% on small-scale and 6.9% on large-scale settings. Notably, most\nof the improvement stems from reduced deletion errors, with relative reductions\nof 45.4% and 18.5%, respectively. These results highlight SegAug's\neffectiveness in improving RNN-T's robustness, offering a promising solution\nfor enhancing speech recognition performance across diverse and challenging\nscenarios.", "published": "2025-02-20 16:13:17", "link": "http://arxiv.org/abs/2502.14685v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Role of the Pretraining and the Adaptation data sizes for low-resource\n  real-time MRI video segmentation", "abstract": "Real-time Magnetic Resonance Imaging (rtMRI) is frequently used in speech\nproduction studies as it provides a complete view of the vocal tract during\narticulation. This study investigates the effectiveness of rtMRI in analyzing\nvocal tract movements by employing the SegNet and UNet models for Air-Tissue\nBoundary (ATB)segmentation tasks. We conducted pretraining of a few base models\nusing increasing numbers of subjects and videos, to assess performance on two\ndatasets. First, consisting of unseen subjects with unseen videos from the same\ndata source, achieving 0.33% and 0.91% (Pixel-wise Classification Accuracy\n(PCA) and Dice Coefficient respectively) better than its matched condition.\nSecond, comprising unseen videos from a new data source, where we obtained an\naccuracy of 99.63% and 98.09% (PCA and Dice Coefficient respectively) of its\nmatched condition performance. Here, matched condition performance refers to\nthe performance of a model trained only on the test subjects which was set as a\nbenchmark for the other models. Our findings highlight the significance of\nfine-tuning and adapting models with limited data. Notably, we demonstrated\nthat effective model adaptation can be achieved with as few as 15 rtMRI frames\nfrom any new dataset.", "published": "2025-02-20 10:15:43", "link": "http://arxiv.org/abs/2502.14418v1", "categories": ["eess.AS", "cs.CV", "eess.SP"], "primary_category": "eess.AS"}
{"title": "ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by\n  Reducing Data Distribution Errors", "abstract": "Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to\nretrieve audio clips or multilingual texts from databases. However, existing\nML-ATR schemes suffer from inconsistencies for instance similarity matching\nacross languages. We theoretically analyze the inconsistency in terms of both\nmultilingual modal alignment direction error and weight error, and propose the\ntheoretical weight error upper bound for quantifying the inconsistency. Based\non the analysis of the weight error upper bound, we find that the inconsistency\nproblem stems from the data distribution error caused by random sampling of\nlanguages. We propose a consistent ML-ATR scheme using 1-to-k contrastive\nlearning and audio-English co-anchor contrastive learning, aiming to mitigate\nthe negative impact of data distribution error on recall and consistency in\nML-ATR. Experimental results on the translated AudioCaps and Clotho datasets\nshow that our scheme achieves state-of-the-art performance on recall and\nconsistency metrics for eight mainstream languages, including English. Our code\nwill be available at https://github.com/ATRI-ACL/ATRI-ACL.", "published": "2025-02-20 15:06:15", "link": "http://arxiv.org/abs/2502.14627v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pitch Imperfect: Detecting Audio Deepfakes Through Acoustic Prosodic\n  Analysis", "abstract": "Audio deepfakes are increasingly in-differentiable from organic speech, often\nfooling both authentication systems and human listeners. While many techniques\nuse low-level audio features or optimization black-box model training, focusing\non the features that humans use to recognize speech will likely be a more\nlong-term robust approach to detection. We explore the use of prosody, or the\nhigh-level linguistic features of human speech (e.g., pitch, intonation,\njitter) as a more foundational means of detecting audio deepfakes. We develop a\ndetector based on six classical prosodic features and demonstrate that our\nmodel performs as well as other baseline models used by the community to detect\naudio deepfakes with an accuracy of 93% and an EER of 24.7%. More importantly,\nwe demonstrate the benefits of using a linguistic features-based approach over\nexisting models by applying an adaptive adversary using an $L_{\\infty}$ norm\nattack against the detectors and using attention mechanisms in our training for\nexplainability. We show that we can explain the prosodic features that have\nhighest impact on the model's decision (Jitter, Shimmer and Mean Fundamental\nFrequency) and that other models are extremely susceptible to simple\n$L_{\\infty}$ norm attacks (99.3% relative degradation in accuracy). While\noverall performance may be similar, we illustrate the robustness and\nexplainability benefits to a prosody feature approach to audio deepfake\ndetection.", "published": "2025-02-20 16:52:55", "link": "http://arxiv.org/abs/2502.14726v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken\n  Dialogue Models", "abstract": "Retrieval Augmented Generation (RAG) has gained widespread adoption owing to\nits capacity to empower large language models (LLMs) to integrate external\nknowledge. However, existing RAG frameworks are primarily designed for\ntext-based LLMs and rely on Automatic Speech Recognition to process speech\ninput, which discards crucial audio information, risks transcription errors,\nand increases computational overhead. Therefore, we introduce WavRAG, the first\nretrieval augmented generation framework with native, end-to-end audio support.\nWavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw\naudio for both embedding and retrieval. 2) WavRAG integrates audio and text\ninto a unified knowledge representation. Specifically, we propose the\nWavRetriever to facilitate the retrieval from a text-audio hybrid knowledge\nbase, and further enhance the in-context capabilities of spoken dialogue models\nthrough the integration of chain-of-thought reasoning. In comparison to\nstate-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval\nperformance while delivering a 10x acceleration. Furthermore, WavRAG's unique\ntext-audio hybrid retrieval capability extends the boundaries of RAG to the\naudio modality.", "published": "2025-02-20 16:54:07", "link": "http://arxiv.org/abs/2502.14727v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fundamental Survey on Neuromorphic Based Audio Classification", "abstract": "Audio classification is paramount in a variety of applications including\nsurveillance, healthcare monitoring, and environmental analysis. Traditional\nmethods frequently depend on intricate signal processing algorithms and\nmanually crafted features, which may fall short in fully capturing the\ncomplexities of audio patterns. Neuromorphic computing, inspired by the\narchitecture and functioning of the human brain, presents a promising\nalternative for audio classification tasks. This survey provides an exhaustive\nexamination of the current state-of-the-art in neuromorphic-based audio\nclassification. It delves into the crucial components of neuromorphic systems,\nsuch as Spiking Neural Networks (SNNs), memristors, and neuromorphic hardware\nplatforms, highlighting their advantages in audio classification. Furthermore,\nthe survey explores various methodologies and strategies employed in\nneuromorphic audio classification, including event-based processing,\nspike-based learning, and bio-inspired feature extraction. It examines how\nthese approaches address the limitations of traditional audio classification\nmethods, particularly in terms of energy efficiency, real-time processing, and\nrobustness to environmental noise. Additionally, the paper conducts a\ncomparative analysis of different neuromorphic audio classification models and\nbenchmarks, evaluating their performance metrics, computational efficiency, and\nscalability. By providing a comprehensive guide for researchers, engineers and\npractitioners, this survey aims to stimulate further innovation and\nadvancements in the evolving field of neuromorphic audio classification.", "published": "2025-02-20 21:34:32", "link": "http://arxiv.org/abs/2502.15056v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LACTOSE: Linear Array of Conditions, TOpologies with Separated\n  Error-backpropagation -- The Differentiable \"IF\" Conditional for\n  Differentiable Digital Signal Processing", "abstract": "There has been difficulty utilising conditional statements as part of the\nneural network graph (e.g. if input $> x$, pass input to network $N$). This is\ndue to the inability to backpropagate through branching conditions. The Linear\nArray of Conditions, TOpologies with Separated Error-backpropagation (LACTOSE)\nAlgorithm addresses this issue and allows the conditional use of available\nmachine learning layers for supervised learning models. In this paper, the\nLACTOSE algorithm is applied to a simple use of DDSP, however, the main point\nis the development of the \"if\" conditional for DDSP use. The LACTOSE algorithm\nstores trained parameters for each user-specified numerical range and loads the\nparameters dynamically during prediction.", "published": "2025-02-20 06:29:14", "link": "http://arxiv.org/abs/2502.15829v1", "categories": ["cs.LG", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio\n  Disentanglement for Talking Head Synthesis", "abstract": "Talking head synthesis is to synthesize a lip-synchronized talking head video\nusing audio. Recently, the capability of NeRF to enhance the realism and\ntexture details of synthesized talking heads has attracted the attention of\nresearchers. However, most current NeRF methods based on audio are exclusively\nconcerned with the rendering of frontal faces. These methods are unable to\ngenerate clear talking heads in novel views. Another prevalent challenge in\ncurrent 3D talking head synthesis is the difficulty in aligning acoustic and\nvisual spaces, which often results in suboptimal lip-syncing of the generated\ntalking heads. To address these issues, we propose Neural Radiance Field with\n3D Prior Aided Audio Disentanglement for Talking Head Synthesis\n(NeRF-3DTalker). Specifically, the proposed method employs 3D prior information\nto synthesize clear talking heads with free views. Additionally, we propose a\n3D Prior Aided Audio Disentanglement module, which is designed to disentangle\nthe audio into two distinct categories: features related to 3D awarded speech\nmovements and features related to speaking style. Moreover, to reposition the\ngenerated frames that are distant from the speaker's motion space in the real\nspace, we have devised a local-global Standardized Space. This method\nnormalizes the irregular positions in the generated frames from both global and\nlocal semantic perspectives. Through comprehensive qualitative and quantitative\nexperiments, it has been demonstrated that our NeRF-3DTalker outperforms\nstate-of-the-art in synthesizing realistic talking head videos, exhibiting\nsuperior image quality and lip synchronization. Project page:\nhttps://nerf-3dtalker.github.io/NeRF-3Dtalker.", "published": "2025-02-20 01:16:11", "link": "http://arxiv.org/abs/2502.14178v1", "categories": ["cs.GR", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
