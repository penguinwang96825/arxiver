{"title": "Every Graph is Essential to Large Treewidth", "abstract": "We show that for every graph $H$, there is a hereditary weakly sparse graph\nclass $\\mathcal C_H$ of unbounded treewidth such that the $H$-free (i.e.,\nexcluding $H$ as an induced subgraph) graphs of $\\mathcal C_H$ have bounded\ntreewidth. This refutes several conjectures and critically thwarts the quest\nfor the unavoidable induced subgraphs in classes of unbounded treewidth, a\nwished-for counterpart of the Grid Minor theorem. We actually show a stronger\nresult: For every positive integer $t$, there is a hereditary graph class\n$\\mathcal C_t$ of unbounded treewidth such that for any graph $H$ of treewidth\nat most $t$, the $H$-free graphs of $\\mathcal C_t$ have bounded treewidth. Our\nconstruction is a variant of so-called layered wheels. We also introduce a\nframework of abstract layered wheels, based on their most salient properties.\nIn particular, we streamline and extend key lemmas previously shown on\nindividual layered wheels. We believe that this should greatly help develop\nthis topic, which appears to be a very strong yet underexploited source of\ncounterexamples.", "published": "2025-02-20 17:59:36", "link": "http://arxiv.org/abs/2502.14775v3", "categories": ["math.CO", "cs.DM", "05C75", "G.2.2"], "primary_category": "math.CO"}
{"title": "Enumerating minimal dominating sets and variants in chordal bipartite graphs", "abstract": "Enumerating minimal dominating sets with polynomial delay in bipartite graphs\nis a long-standing open problem. To date, even the subcase of chordal bipartite\ngraphs is open, with the best known algorithm due to Golovach, Heggernes,\nKant\\'e, Kratsch, Saether, and Villanger running in incremental-polynomial\ntime. We improve on this result by providing a polynomial delay and space\nalgorithm enumerating minimal dominating sets in chordal bipartite graphs.\nAdditionally, we show that the total and connected variants admit polynomial\nand incremental-polynomial delay algorithms, respectively, within the same\nclass. This provides an alternative proof of a result by Golovach et al. for\ntotal dominating sets, and answers an open question for the connected variant.\nFinally, we give evidence that the techniques used in this paper cannot be\ngeneralized to bipartite graphs for (total) minimal dominating sets, unless P =\nNP, and show that enumerating minimal connected dominating sets in bipartite\ngraphs is harder than enumerating minimal transversals in general hypergraphs.", "published": "2025-02-20 14:51:41", "link": "http://arxiv.org/abs/2502.14611v2", "categories": ["cs.DS", "cs.DM", "math.CO"], "primary_category": "cs.DS"}
{"title": "Temporal Connectivity Augmentation", "abstract": "Connectivity in temporal graphs relies on the notion of temporal paths, in\nwhich edges follow a chronological order (either strict or non-strict). In this\nwork, we investigate the question of how to make a temporal graph connected.\nMore precisely, we tackle the problem of finding, among a set of proposed\ntemporal edges, the smallest subset such that its addition makes the graph\ntemporally connected (TCA). We study the complexity of this problem and\nvariants, under restricted lifespan of the graph, i.e. the maximum time step in\nthe graph. Our main result on TCA is that for any fixed lifespan at least 2, it\nis NP-complete in both the strict and non-strict setting. We additionally\nprovide a set of restrictions in the non-strict setting which makes the problem\nsolvable in polynomial time and design an algorithm achieving this complexity.\nInterestingly, we prove that the source variant (making a given vertex a source\nin the augmented graph) is as difficult as TCA. On the opposite, we prove that\nthe version where a list of connectivity demands has to be satisfied is\nsolvable in polynomial time, when the size of the list is fixed. Finally, we\nhighlight a variant of the previous case for which even with two pairs the\nproblem is already NP-hard.", "published": "2025-02-20 13:17:08", "link": "http://arxiv.org/abs/2502.14540v1", "categories": ["cs.DM", "G.2.2"], "primary_category": "cs.DM"}
{"title": "A Parallel Hierarchical Approach for Community Detection on Large-scale Dynamic Networks", "abstract": "In this paper, we propose a novel parallel hierarchical Leiden-based\nalgorithm for dynamic community detection. The algorithm, for a given batch\nupdate of edge insertions and deletions, partitions the network into\ncommunities using only a local neighborhood of the affected nodes. It also uses\nthe inner hierarchical graph-based structure, which is updated incrementally in\nthe process of optimizing the modularity of the partitioning. The algorithm has\nbeen extensively tested on various networks. The results demonstrate promising\nimprovements in performance and scalability while maintaining the modularity of\nthe partitioning.", "published": "2025-02-20 09:53:13", "link": "http://arxiv.org/abs/2502.18497v1", "categories": ["cs.SI", "cs.DC", "cs.DM"], "primary_category": "cs.SI"}
{"title": "Pursuing Top Growth with Novel Loss Function", "abstract": "Making consistently profitable financial decisions in a continuously evolving\nand volatile stock market has always been a difficult task. Professionals from\ndifferent disciplines have developed foundational theories to anticipate price\nmovement and evaluate securities such as the famed Capital Asset Pricing Model\n(CAPM). In recent years, the role of artificial intelligence (AI) in asset\npricing has been growing. Although the black-box nature of deep learning models\nlacks interpretability, they have continued to solidify their position in the\nfinancial industry. We aim to further enhance AI's potential and utility by\nintroducing a return-weighted loss function that will drive top growth while\nproviding the ML models a limited amount of information. Using only publicly\naccessible stock data (open/close/high/low, trading volume, sector information)\nand several technical indicators constructed from them, we propose an efficient\ndaily trading system that detects top growth opportunities. Our best models\nachieve 61.73% annual return on daily rebalancing with an annualized Sharpe\nRatio of 1.18 over 1340 testing days from 2019 to 2024, and 37.61% annual\nreturn with an annualized Sharpe Ratio of 0.97 over 1360 testing days from 2005\nto 2010. The main drivers for success, especially independent of any domain\nknowledge, are the novel return-weighted loss function, the integration of\ncategorical and continuous data, and the ML model architecture. We also\ndemonstrate the superiority of our novel loss function over traditional loss\nfunctions via several performance metrics and statistical evidence.", "published": "2025-02-20 21:43:51", "link": "http://arxiv.org/abs/2502.17493v1", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "I.2.1; I.2.6"], "primary_category": "cs.LG"}
{"title": "Multi-Layer Deep xVA: Structural Credit Models, Measure Changes and Convergence Analysis", "abstract": "We propose a structural default model for portfolio-wide valuation\nadjustments (xVAs) and represent it as a system of coupled backward stochastic\ndifferential equations. The framework is divided into four layers, each\ncapturing a key component: (i) clean values, (ii) initial margin and Collateral\nValuation Adjustment (ColVA), (iii) Credit/Debit Valuation Adjustments\n(CVA/DVA) together with Margin Valuation Adjustment (MVA), and (iv) Funding\nValuation Adjustment (FVA). Because these layers depend on one another through\ncollateral and default effects, a naive Monte Carlo approach would require\ndeeply nested simulations, making the problem computationally intractable.\n  To address this challenge, we use an iterative deep BSDE approach, handling\neach layer sequentially so that earlier outputs serve as inputs to the\nsubsequent layers. Initial margin is computed via deep quantile regression to\nreflect margin requirements over the Margin Period of Risk. We also adopt a\nchange-of-measure method that highlights rare but significant defaults of the\nbank or counterparty, ensuring that these events are accurately captured in the\ntraining process.\n  We further extend Han and Long's (2020) a posteriori error analysis to BSDEs\non bounded domains. Due to the random exit from the domain, we obtain an order\nof convergence of $\\mathcal{O}(h^{1/4-\\epsilon})$ rather than the usual\n$\\mathcal{O}(h^{1/2})$.\n  Numerical experiments illustrate that this method drastically reduces\ncomputational demands and successfully scales to high-dimensional,\nnon-symmetric portfolios. The results confirm its effectiveness and accuracy,\noffering a practical alternative to nested Monte Carlo simulations in\nmulti-counterparty xVA analyses.", "published": "2025-02-20 17:41:55", "link": "http://arxiv.org/abs/2502.14766v2", "categories": ["q-fin.CP", "91G40, 91G20, 91G60 65C30, 60G40"], "primary_category": "q-fin.CP"}
{"title": "Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework", "abstract": "The lifetime behaviour of loans is notoriously difficult to model, which can\ncompromise a bank's financial reserves against future losses, if modelled\npoorly. Therefore, we present a data-driven comparative study amongst three\ntechniques in modelling a series of default risk estimates over the lifetime of\neach loan, i.e., its term-structure. The behaviour of loans can be described\nusing a nonstationary and time-dependent semi-Markov model, though we model its\nelements using a multistate regression-based approach. As such, the transition\nprobabilities are explicitly modelled as a function of a rich set of input\nvariables, including macroeconomic and loan-level inputs. Our modelling\ntechniques are deliberately chosen in ascending order of complexity: 1) a\nMarkov chain; 2) beta regression; and 3) multinomial logistic regression. Using\nresidential mortgage data, our results show that each successive model\noutperforms the previous, likely as a result of greater sophistication. This\nfinding required devising a novel suite of simple model diagnostics, which can\nitself be reused in assessing sampling representativeness and the performance\nof other modelling techniques. These contributions surely advance the current\npractice within banking when conducting multistate modelling. Consequently, we\nbelieve that the estimation of loss reserves will be more timeous and accurate\nunder IFRS 9.", "published": "2025-02-20 11:53:20", "link": "http://arxiv.org/abs/2502.14479v1", "categories": ["q-fin.RM", "q-fin.ST", "stat.AP"], "primary_category": "q-fin.RM"}
{"title": "Causality Analysis of COVID-19 Induced Crashes in Stock and Commodity Markets: A Topological Perspective", "abstract": "The paper presents a comprehensive causality analysis of the US stock and\ncommodity markets during the COVID-19 crash. The dynamics of different sectors\nare also compared. We use Topological Data Analysis (TDA) on multidimensional\ntime-series to identify crashes in stock and commodity markets. The Wasserstein\nDistance WD shows distinct spikes signaling the crash for both stock and\ncommodity markets. We then compare the persistence diagrams of stock and\ncommodity markets using the WD metric. A significant spike in the $WD$ between\nstock and commodity markets is observed during the crisis, suggesting\nsignificant topological differences between the markets. Similar spikes are\nobserved between the sectors of the US market as well. Spikes obtained may be\ndue to either a difference in the magnitude of crashes in the two markets (or\nsectors), or from the temporal lag between the two markets suggesting\ninformation flow. We study the Granger-causality between stock and commodity\nmarkets and also between different sectors. The results show a bidirectional\nGranger-causality between commodity and stock during the crash period,\ndemonstrating the greater interdependence of financial markets during the\ncrash. However, the overall analysis shows that the causal direction is from\nstock to commodity. A pairwise Granger-causal analysis between US sectors is\nalso conducted. There is a significant increase in the interdependence between\nthe sectors during the crash period. TDA combined with Granger-causality\neffectively analyzes the interdependence and sensitivity of different markets\nand sectors.", "published": "2025-02-20 10:29:58", "link": "http://arxiv.org/abs/2502.14431v1", "categories": ["q-fin.ST", "math.AT", "physics.data-an"], "primary_category": "q-fin.ST"}
{"title": "Financial fraud detection system based on improved random forest and gradient boosting machine (GBM)", "abstract": "This paper proposes a financial fraud detection system based on improved\nRandom Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the\nsystem introduces a novel model architecture called GBM-SSRF (Gradient Boosting\nMachine with Simplified and Strengthened Random Forest), which cleverly\ncombines the powerful optimization capabilities of the gradient boosting\nmachine (GBM) with improved randomization. The computational efficiency and\nfeature extraction capabilities of the Simplified and Strengthened Random\nForest (SSRF) forest significantly improve the performance of financial fraud\ndetection. Although the traditional random forest model has good classification\ncapabilities, it has high computational complexity when faced with large-scale\ndata and has certain limitations in feature selection. As a commonly used\nensemble learning method, the GBM model has significant advantages in\noptimizing performance and handling nonlinear problems. However, GBM takes a\nlong time to train and is prone to overfitting problems when data samples are\nunbalanced. In response to these limitations, this paper optimizes the random\nforest based on the structure, reducing the computational complexity and\nimproving the feature selection ability through the structural simplification\nand enhancement of the random forest. In addition, the optimized random forest\nis embedded into the GBM framework, and the model can maintain efficiency and\nstability with the help of GBM's gradient optimization capability. Experiments\nshow that the GBM-SSRF model not only has good performance, but also has good\nrobustness and generalization capabilities, providing an efficient and reliable\nsolution for financial fraud detection.", "published": "2025-02-20 03:27:57", "link": "http://arxiv.org/abs/2502.15822v1", "categories": ["q-fin.ST", "cs.LG", "q-fin.GN", "stat.AP", "stat.ML"], "primary_category": "q-fin.ST"}
