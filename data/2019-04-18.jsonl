{"title": "Point-less: More Abstractive Summarization with Pointer-Generator\n  Networks", "abstract": "The Pointer-Generator architecture has shown to be a big improvement for\nabstractive summarization seq2seq models. However, the summaries produced by\nthis model are largely extractive as over 30% of the generated sentences are\ncopied from the source text. This work proposes a multihead attention\nmechanism, pointer dropout, and two new loss functions to promote more\nabstractive summaries while maintaining similar ROUGE scores. Both the\nmultihead attention and dropout do not improve N-gram novelty, however, the\ndropout acts as a regularizer which improves the ROUGE score. The new loss\nfunction achieves significantly higher novel N-grams and sentences, at the cost\nof a slightly lower ROUGE score.", "published": "2019-04-18 22:17:21", "link": "http://arxiv.org/abs/1905.01975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analytical Methods for Interpretable Ultradense Word Embeddings", "abstract": "Word embeddings are useful for a wide variety of tasks, but they lack\ninterpretability. By rotating word spaces, interpretable dimensions can be\nidentified while preserving the information contained in the embeddings without\nany loss. In this work, we investigate three methods for making word spaces\ninterpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and\nDensRay, a new method we propose. In contrast to Densifier, DensRay can be\ncomputed in closed form, is hyperparameter-free and thus more robust than\nDensifier. We evaluate the three methods on lexicon induction and set-based\nword analogy. In addition we provide qualitative insights as to how\ninterpretable word spaces can be used for removing gender bias from embeddings.", "published": "2019-04-18 09:47:06", "link": "http://arxiv.org/abs/1904.08654v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Genie: A Generator of Natural Language Semantic Parsers for Virtual\n  Assistant Commands", "abstract": "To understand diverse natural language commands, virtual assistants today are\ntrained with numerous labor-intensive, manually annotated sentences. This paper\npresents a methodology and the Genie toolkit that can handle new compound\ncommands with significantly less manual effort. We advocate formalizing the\ncapability of virtual assistants with a Virtual Assistant Programming Language\n(VAPL) and using a neural semantic parser to translate natural language into\nVAPL code. Genie needs only a small realistic set of input sentences for\nvalidating the neural model. Developers write templates to synthesize data;\nGenie uses crowdsourced paraphrases and data augmentation, along with the\nsynthesized data, to train a semantic parser. We also propose design principles\nthat make VAPL languages amenable to natural language translation. We apply\nthese principles to revise ThingTalk, the language used by the Almond virtual\nassistant. We use Genie to build the first semantic parser that can support\ncompound virtual assistants commands with unquoted free-form parameters. Genie\nachieves a 62% accuracy on realistic user inputs. We demonstrate Genie's\ngenerality by showing a 19% and 31% improvement over the previous state of the\nart on a music skill, aggregate functions, and access control.", "published": "2019-04-18 21:33:15", "link": "http://arxiv.org/abs/1904.09020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creative Procedural-Knowledge Extraction From Web Design Tutorials", "abstract": "Complex design tasks often require performing diverse actions in a specific\norder. To (semi-)autonomously accomplish these tasks, applications need to\nunderstand and learn a wide range of design procedures, i.e., Creative\nProcedural-Knowledge (CPK). Prior knowledge base construction and mining have\nnot typically addressed the creative fields, such as design and arts. In this\npaper, we formalize an ontology of CPK using five components: goal, workflow,\naction, command and usage; and extract components' values from online design\ntutorials. We scraped 19.6K tutorial-related webpages and built a web\napplication for professional designers to identify and summarize CPK\ncomponents. The annotated dataset consists of 819 unique commands, 47,491\nactions, and 2,022 workflows and goals. Based on this dataset, we propose a\ngeneral CPK extraction pipeline and demonstrate that existing text\nclassification and sequence-to-sequence models are limited in identifying,\npredicting and summarizing complex operations described in heterogeneous\nstyles. Through quantitative and qualitative error analysis, we discuss CPK\nextraction challenges that need to be addressed by future research.", "published": "2019-04-18 04:22:23", "link": "http://arxiv.org/abs/1904.08587v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ConvLab: Multi-Domain End-to-End Dialog System Platform", "abstract": "We present ConvLab, an open-source multi-domain end-to-end dialog system\nplatform, that enables researchers to quickly set up experiments with reusable\ncomponents and compare a large set of different approaches, ranging from\nconventional pipeline systems to end-to-end neural models, in common\nenvironments. ConvLab offers a set of fully annotated datasets and associated\npre-trained reference models. As a showcase, we extend the MultiWOZ dataset\nwith user dialog act annotations to train all component models and demonstrate\nhow ConvLab makes it easy and effortless to conduct complicated experiments in\nmulti-domain end-to-end dialog settings.", "published": "2019-04-18 08:35:49", "link": "http://arxiv.org/abs/1904.08637v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings", "abstract": "Gender bias is highly impacting natural language processing applications.\nWord embeddings have clearly been proven both to keep and amplify gender biases\nthat are present in current data sources. Recently, contextualized word\nembeddings have enhanced previous word embedding techniques by computing word\nvector representations dependent on the sentence they appear in.\n  In this paper, we study the impact of this conceptual change in the word\nembedding computation in relation with gender bias. Our analysis includes\ndifferent measures previously applied in the literature to standard word\nembeddings. Our findings suggest that contextualized word embeddings are less\nbiased than standard ones even when the latter are debiased.", "published": "2019-04-18 13:47:00", "link": "http://arxiv.org/abs/1904.08783v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clause-Wise and Recursive Decoding for Complex and Cross-Domain\n  Text-to-SQL Generation", "abstract": "Most deep learning approaches for text-to-SQL generation are limited to the\nWikiSQL dataset, which only supports very simple queries over a single table.\nWe focus on the Spider dataset, a complex and cross-domain text-to-SQL task,\nwhich includes complex queries over multiple tables. In this paper, we propose\na SQL clause-wise decoding neural architecture with a self-attention based\ndatabase schema encoder to address the Spider task. Each of the clause-specific\ndecoders consists of a set of sub-modules, which is defined by the syntax of\neach clause. Additionally, our model works recursively to support nested\nqueries. When evaluated on the Spider dataset, our approach achieves 4.6\\% and\n9.8\\% accuracy gain in the test and dev sets, respectively. In addition, we\nshow that our model is significantly more effective at predicting complex and\nnested queries than previous work.", "published": "2019-04-18 15:20:45", "link": "http://arxiv.org/abs/1904.08835v2", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Rumour Detection via News Propagation Dynamics and User Representation\n  Learning", "abstract": "Rumours have existed for a long time and have been known for serious\nconsequences. The rapid growth of social media platforms has multiplied the\nnegative impact of rumours; it thus becomes important to early detect them.\nMany methods have been introduced to detect rumours using the content or the\nsocial context of news. However, most existing methods ignore or do not explore\neffectively the propagation pattern of news in social media, including the\nsequence of interactions of social media users with news across time. In this\nwork, we propose a novel method for rumour detection based on deep learning.\nOur method leverages the propagation process of the news by learning the users'\nrepresentation and the temporal interrelation of users' responses. Experiments\nconducted on Twitter and Weibo datasets demonstrate the state-of-the-art\nperformance of the proposed method.", "published": "2019-04-18 14:13:03", "link": "http://arxiv.org/abs/1905.03042v1", "categories": ["cs.SI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.SI"}
{"title": "Knowledge-rich Image Gist Understanding Beyond Literal Meaning", "abstract": "We investigate the problem of understanding the message (gist) conveyed by\nimages and their captions as found, for instance, on websites or news articles.\nTo this end, we propose a methodology to capture the meaning of image-caption\npairs on the basis of large amounts of machine-readable knowledge that has\npreviously been shown to be highly effective for text understanding. Our method\nidentifies the connotation of objects beyond their denotation: where most\napproaches to image understanding focus on the denotation of objects, i.e.,\ntheir literal meaning, our work addresses the identification of connotations,\ni.e., iconic meanings of objects, to understand the message of images. We view\nimage understanding as the task of representing an image-caption pair on the\nbasis of a wide-coverage vocabulary of concepts such as the one provided by\nWikipedia, and cast gist detection as a concept-ranking problem with\nimage-caption pairs as queries. To enable a thorough investigation of the\nproblem of gist understanding, we produce a gold standard of over 300\nimage-caption pairs and over 8,000 gist annotations covering a wide variety of\ntopics at different levels of abstraction. We use this dataset to\nexperimentally benchmark the contribution of signals from heterogeneous\nsources, namely image and text. The best result with a Mean Average Precision\n(MAP) of 0.69 indicate that by combining both dimensions we are able to better\nunderstand the meaning of our image-caption pairs than when using language or\nvision information alone. We test the robustness of our gist detection approach\nwhen receiving automatically generated input, i.e., using automatically\ngenerated image tags or generated captions, and prove the feasibility of an\nend-to-end automated process.", "published": "2019-04-18 11:50:20", "link": "http://arxiv.org/abs/1904.08709v1", "categories": ["cs.IR", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "Societal Controversies in Wikipedia Articles", "abstract": "Collaborative content creation inevitably reaches situations where different\npoints of view lead to conflict. We focus on Wikipedia, the free encyclopedia\nanyone may edit, where disputes about content in controversial articles often\nreflect larger societal debates. While Wikipedia has a public edit history and\ndiscussion section for every article, the substance of these sections is\ndifficult to phantom for Wikipedia users interested in the development of an\narticle and in locating which topics were most controversial. In this paper we\npresent Contropedia, a tool that augments Wikipedia articles and gives insight\ninto the development of controversial topics. Contropedia uses an efficient\nlanguage agnostic measure based on the edit history that focuses on wiki links\nto easily identify which topics within a Wikipedia article have been most\ncontroversial and when.", "published": "2019-04-18 12:19:09", "link": "http://arxiv.org/abs/1904.08721v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Towards VQA Models That Can Read", "abstract": "Studies have shown that a dominant class of questions asked by visually\nimpaired users on images of their surroundings involves reading text in the\nimage. But today's VQA models can not read! Our paper takes a first step\ntowards addressing this problem. First, we introduce a new \"TextVQA\" dataset to\nfacilitate progress on this important problem. Existing datasets either have a\nsmall proportion of questions about text (e.g., the VQA dataset) or are too\nsmall (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408\nimages that require reasoning about text to answer. Second, we introduce a\nnovel model architecture that reads text in the image, reasons about it in the\ncontext of the image and the question, and predicts an answer which might be a\ndeduction based on the text and the image or composed of the strings found in\nthe image. Consequently, we call our approach Look, Read, Reason & Answer\n(LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on\nour TextVQA dataset. We find that the gap between human performance and machine\nperformance is significantly larger on TextVQA than on VQA 2.0, suggesting that\nTextVQA is well-suited to benchmark progress along directions complementary to\nVQA 2.0.", "published": "2019-04-18 17:55:37", "link": "http://arxiv.org/abs/1904.08920v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Modeling through Long Term Memory Network", "abstract": "Recurrent Neural Networks (RNN), Long Short-Term Memory Networks (LSTM), and\nMemory Networks which contain memory are popularly used to learn patterns in\nsequential data. Sequential data has long sequences that hold relationships.\nRNN can handle long sequences but suffers from the vanishing and exploding\ngradient problems. While LSTM and other memory networks address this problem,\nthey are not capable of handling long sequences (50 or more data points long\nsequence patterns). Language modelling requiring learning from longer sequences\nare affected by the need for more information in memory. This paper introduces\nLong Term Memory network (LTM), which can tackle the exploding and vanishing\ngradient problems and handles long sequences without forgetting. LTM is\ndesigned to scale data in the memory and gives a higher weight to the input in\nthe sequence. LTM avoid overfitting by scaling the cell state after achieving\nthe optimal results. The LTM is tested on Penn treebank dataset, and Text8\ndataset and LTM achieves test perplexities of 83 and 82 respectively. 650 LTM\ncells achieved a test perplexity of 67 for Penn treebank, and 600 cells\nachieved a test perplexity of 77 for Text8. LTM achieves state of the art\nresults by only using ten hidden LTM cells for both datasets.", "published": "2019-04-18 09:19:25", "link": "http://arxiv.org/abs/1904.08936v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "No Permanent Friends or Enemies: Tracking Relationships between Nations\n  from News", "abstract": "Understanding the dynamics of international politics is important yet\nchallenging for civilians. In this work, we explore unsupervised neural models\nto infer relations between nations from news articles. We extend existing\nmodels by incorporating shallow linguistics information and propose a new\nautomatic evaluation metric that aligns relationship dynamics with manually\nannotated key events. As understanding international relations requires\ncarefully analyzing complex relationships, we conduct in-person human\nevaluations with three groups of participants. Overall, humans prefer the\noutputs of our model and give insightful feedback that suggests future\ndirections for human-centered models. Furthermore, our model reveals\ninteresting regional differences in news coverage. For instance, with respect\nto US-China relations, Singaporean media focus more on \"strengthening\" and\n\"purchasing\", while US media focus more on \"criticizing\" and \"denouncing\".", "published": "2019-04-18 18:00:30", "link": "http://arxiv.org/abs/1904.08950v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "ProductNet: a Collection of High-Quality Datasets for Product\n  Representation Learning", "abstract": "ProductNet is a collection of high-quality product datasets for better\nproduct understanding. Motivated by ImageNet, ProductNet aims at supporting\nproduct representation learning by curating product datasets of high quality\nwith properly chosen taxonomy. In this paper, the two goals of building\nhigh-quality product datasets and learning product representation support each\nother in an iterative fashion: the product embedding is obtained via a\nmulti-modal deep neural network (master model) designed to leverage product\nimage and catalog information; and in return, the embedding is utilized via\nactive learning (local model) to vastly accelerate the annotation process. For\nthe labeled data, the proposed master model yields high categorization accuracy\n(94.7% top-1 accuracy for 1240 classes), which can be used as search indices,\npartition keys, and input features for machine learning models. The product\nembedding, as well as the fined-tuned master model for a specific business\ntask, can also be used for various transfer learning tasks.", "published": "2019-04-18 23:17:07", "link": "http://arxiv.org/abs/1904.09037v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "BowTie - A deep learning feedforward neural network for sentiment\n  analysis", "abstract": "How to model and encode the semantics of human-written text and select the\ntype of neural network to process it are not settled issues in sentiment\nanalysis. Accuracy and transferability are critical issues in machine learning\nin general. These properties are closely related to the loss estimates for the\ntrained model. I present a computationally-efficient and accurate feedforward\nneural network for sentiment prediction capable of maintaining low losses. When\ncoupled with an effective semantics model of the text, it provides highly\naccurate models with low losses. Experimental results on representative\nbenchmark datasets and comparisons to other methods show the advantages of the\nnew approach.", "published": "2019-04-18 13:38:57", "link": "http://arxiv.org/abs/1904.12624v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n  Recognition", "abstract": "We present SpecAugment, a simple data augmentation method for speech\nrecognition. SpecAugment is applied directly to the feature inputs of a neural\nnetwork (i.e., filter bank coefficients). The augmentation policy consists of\nwarping the features, masking blocks of frequency channels, and masking blocks\nof time steps. We apply SpecAugment on Listen, Attend and Spell networks for\nend-to-end speech recognition tasks. We achieve state-of-the-art performance on\nthe LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.\nOn LibriSpeech, we achieve 6.8% WER on test-other without the use of a language\nmodel, and 5.8% WER with shallow fusion with a language model. This compares to\nthe previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set\nwithout the use of a language model, and 6.8%/14.1% with shallow fusion, which\ncompares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.", "published": "2019-04-18 17:53:38", "link": "http://arxiv.org/abs/1904.08779v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Leveraging native language information for improved accented speech\n  recognition", "abstract": "Recognition of accented speech is a long-standing challenge for automatic\nspeech recognition (ASR) systems, given the increasing worldwide population of\nbi-lingual speakers with English as their second language. If we consider\nforeign-accented speech as an interpolation of the native language (L1) and\nEnglish (L2), using a model that can simultaneously address both languages\nwould perform better at the acoustic level for accented speech. In this study,\nwe explore how an end-to-end recurrent neural network (RNN) trained system with\nEnglish and native languages (Spanish and Indian languages) could leverage data\nof native languages to improve performance for accented English speech. To this\nend, we examine pre-training with native languages, as well as multi-task\nlearning (MTL) in which the main task is trained with native English and the\nsecondary task is trained with Spanish or Indian Languages. We show that the\nproposed MTL model performs better than the pre-training approach and\noutperforms a baseline model trained simply with English data. We suggest a new\nsetting for MTL in which the secondary task is trained with both English and\nthe native language, using the same output set. This proposed scenario yields\nbetter performance with +11.95% and +17.55% character error rate gains over\nbaseline for Hispanic and Indian accents, respectively.", "published": "2019-04-18 23:35:19", "link": "http://arxiv.org/abs/1904.09038v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On Acoustic Modeling for Broadband Beamforming", "abstract": "In this work, we describe limitations of the free-field propagation model for\ndesigning broadband beamformers for microphone arrays on a rigid surface.\nTowards this goal, we describe a general framework for quantifying the\nmicrophone array performance in a general wave-field by directly solving the\nacoustic wave equation. The model utilizes Finite-Element-Method (FEM) for\nevaluating the response of the microphone array surface to background 3D planar\nand spherical waves. The effectiveness of the framework is established by\ndesigning and evaluating a representative broadband beamformer under realistic\nacoustic conditions.", "published": "2019-04-18 18:38:28", "link": "http://arxiv.org/abs/1904.08971v1", "categories": ["cs.SD", "cs.MM", "eess.AS", "94A12, 94A40, 94A15", "H.1.2; H.5.1"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Audio-Visual Co-Segmentation", "abstract": "Segmenting objects in images and separating sound sources in audio are\nchallenging tasks, in part because traditional approaches require large amounts\nof labeled data. In this paper we develop a neural network model for visual\nobject segmentation and sound source separation that learns from natural videos\nthrough self-supervision. The model is an extension of recently proposed work\nthat maps image pixels to sounds. Here, we introduce a learning approach to\ndisentangle concepts in the neural networks, and assign semantic categories to\nnetwork feature channels to enable independent image segmentation and sound\nsource separation after audio-visual training on videos. Our evaluations show\nthat the disentangled model outperforms several baselines in semantic\nsegmentation and sound source separation.", "published": "2019-04-18 21:11:03", "link": "http://arxiv.org/abs/1904.09013v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Inspecting and Interacting with Meaningful Music Representations using\n  VAE", "abstract": "Variational Autoencoders(VAEs) have already achieved great results on image\ngeneration and recently made promising progress on music generation. However,\nthe generation process is still quite difficult to control in the sense that\nthe learned latent representations lack meaningful music semantics. It would be\nmuch more useful if people can modify certain music features, such as rhythm\nand pitch contour, via latent representations to test different composition\nideas. In this paper, we propose a new method to inspect the pitch and rhythm\ninterpretations of the latent representations and we name it disentanglement by\naugmentation. Based on the interpretable representations, an intuitive\ngraphical user interface is designed for users to better direct the music\ncreation process by manipulating the pitch contours and rhythmic complexity.", "published": "2019-04-18 15:22:33", "link": "http://arxiv.org/abs/1904.08842v1", "categories": ["cs.SD", "cs.HC", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
