{"title": "Hidformer: Transformer-Style Neural Network in Stock Price Forecasting", "abstract": "This paper investigates the application of Transformer-based neural networks\nto stock price forecasting, with a special focus on the intersection of machine\nlearning techniques and financial market analysis. The evolution of Transformer\nmodels, from their inception to their adaptation for time series analysis in\nfinancial contexts, is reviewed and discussed. Central to our study is the\nexploration of the Hidformer model, which is currently recognized for its\npromising performance in time series prediction. The primary aim of this paper\nis to determine whether Hidformer will also prove itself in the task of stock\nprice prediction. This slightly modified model serves as the framework for our\nexperiments, integrating the principles of technical analysis with advanced\nmachine learning concepts to enhance stock price prediction accuracy. We\nconduct an evaluation of the Hidformer model's performance, using a set of\ncriteria to determine its efficacy. Our findings offer additional insights into\nthe practical application of Transformer architectures in financial time series\nforecasting, highlighting their potential to improve algorithmic trading\nstrategies, including human decision making.", "published": "2024-12-27 21:34:44", "link": "http://arxiv.org/abs/2412.19932v1", "categories": ["cs.CE", "cs.AI", "cs.LG", "q-fin.CP"], "primary_category": "cs.CE"}
{"title": "Assets Forecasting with Feature Engineering and Transformation Methods for LightGBM", "abstract": "Fluctuations in the stock market rapidly shape the economic world and\nconsumer markets, impacting millions of individuals. Hence, accurately\nforecasting it is essential for mitigating risks, including those associated\nwith inactivity. Although research shows that hybrid models of Deep Learning\n(DL) and Machine Learning (ML) yield promising results, their computational\nrequirements often exceed the capabilities of average personal computers,\nrendering them inaccessible to many. In order to address this challenge in this\npaper we optimize LightGBM (an efficient implementation of gradient-boosted\ndecision trees (GBDT)) for maximum performance, while maintaining low\ncomputational requirements. We introduce novel feature engineering techniques\nincluding indicator-price slope ratios and differences of close and open prices\ndivided by the corresponding 14-period Exponential Moving Average (EMA),\ndesigned to capture market dynamics and enhance predictive accuracy.\nAdditionally, we test seven different feature and target variable\ntransformation methods, including returns, logarithmic returns, EMA ratios and\ntheir standardized counterparts as well as EMA difference ratios, so as to\nidentify the most effective ones weighing in both efficiency and accuracy. The\nresults demonstrate Log Returns, Returns and EMA Difference Ratio constitute\nthe best target variable transformation methods, with EMA ratios having a lower\npercentage of correct directional forecasts, and standardized versions of\ntarget variable transformations requiring significantly more training time.\nMoreover, the introduced features demonstrate high feature importance in\npredictive performance across all target variable transformation methods. This\nstudy highlights an accessible, computationally efficient approach to stock\nmarket forecasting using LightGBM, making advanced forecasting techniques more\nwidely attainable.", "published": "2024-12-27 18:37:08", "link": "http://arxiv.org/abs/2501.07580v1", "categories": ["q-fin.ST", "econ.EM"], "primary_category": "q-fin.ST"}
