{"title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension", "abstract": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.", "published": "2025-08-03 23:59:31", "link": "http://arxiv.org/abs/2508.01959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks", "abstract": "Vision-language models (VLMs) have exhibited impressive capabilities across\ndiverse image understanding tasks, but still struggle in settings that require\nreasoning over extended sequences of camera frames from a video. This limits\ntheir utility in embodied settings, which require reasoning over long frame\nsequences from a continuous stream of visual input at each moment of a task\nattempt. To address this limitation, we propose ROVER (Reasoning Over VidEo\nRecursively), a framework that enables the model to recursively decompose\nlong-horizon video trajectories into segments corresponding to shorter subtasks\nwithin the trajectory. In doing so, ROVER facilitates more focused and accurate\nreasoning over temporally localized frame sequences without losing global\ncontext. We evaluate ROVER, implemented using an in-context learning approach,\non diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa\nthat consists of 543 videos showing both expert and perturbed non-expert\ntrajectories across 27 robotic manipulation tasks. ROVER outperforms strong\nbaselines across three video reasoning tasks: task progress estimation,\nframe-level natural language reasoning, and video question answering. We\nobserve that, by reducing the number of frames the model reasons over at each\ntimestep, ROVER mitigates hallucinations, especially during unexpected or\nnon-optimal moments of a trajectory. In addition, by enabling the\nimplementation of a subtask-specific sliding context window, ROVER's time\ncomplexity scales linearly with video length, an asymptotic improvement over\nbaselines. Demos, code, and data available at: https://rover-vlm.github.io", "published": "2025-08-03 22:33:43", "link": "http://arxiv.org/abs/2508.01943v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback", "abstract": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research.", "published": "2025-08-03 21:45:37", "link": "http://arxiv.org/abs/2508.01930v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language", "abstract": "Despite the rapid advancement of large language models (LLMs), low-resource\nlanguages remain largely excluded from the NLP landscape. We present PunGPT2,\nthe first fully open-source suite of Punjabi large language models, trained\nfrom scratch on a 35GB domain-diverse corpus encompassing literature, religious\ntexts, news, and social discourse. Unlike prior multilingual approaches,\nPunGPT2 captures rich syntactic and morphological features unique to Punjabi\nthrough a tokenizer optimised with byte pair encoding and linguistically\naligned pretraining objectives. To improve factual grounding and domain recall,\nwe introduce Pun-RAG, a retrieval-augmented generation framework combining\nPunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We\nfurther develop Pun-Instruct, a parameter-efficient, instruction-tuned variant\nusing QLoRA, enabling robust zero-shot and instruction-following performance\nwith significantly reduced compute needs.\n  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system\nthat fuses sparse (BM25) and dense methods with quantum-inspired semantic\nmatching. By encoding queries using amplitude-based embeddings and retrieving\nvia quantum kernel similarity, Quantum-RAG achieves improved contextual\nrelevance with minimal memory overhead marking the first practical integration\nof quantum representations in low-resource language generation. Our models\nsignificantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in\nperplexity, factuality, and fluency. This work provides a scalable,\nreproducible blueprint for extending LLM capabilities to underrepresented\nlanguages and pioneers quantum-aware retrieval in low-resource NLP", "published": "2025-08-03 21:03:22", "link": "http://arxiv.org/abs/2508.01918v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning", "abstract": "Understanding internal representations of neural models is a core interest of\nmechanistic interpretability. Due to its large dimensionality, the\nrepresentation space can encode various aspects about inputs. To what extent\nare different aspects organized and encoded in separate subspaces? Is it\npossible to find these ``natural'' subspaces in a purely unsupervised way?\nSomewhat surprisingly, we can indeed achieve this and find interpretable\nsubspaces by a seemingly unrelated training objective. Our method, neighbor\ndistance minimization (NDM), learns non-basis-aligned subspaces in an\nunsupervised manner. Qualitative analysis shows subspaces are interpretable in\nmany cases, and encoded information in obtained subspaces tends to share the\nsame abstract concept across different inputs, making such subspaces similar to\n``variables'' used by the model. We also conduct quantitative experiments using\nknown circuits in GPT-2; results show a strong connection between subspaces and\ncircuit variables. We also provide evidence showing scalability to 2B models by\nfinding separate subspaces mediating context and parametric knowledge routing.\nViewed more broadly, our findings offer a new perspective on understanding\nmodel internals and building circuits.", "published": "2025-08-03 20:59:29", "link": "http://arxiv.org/abs/2508.01916v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology", "abstract": "Academic publishing, integral to knowledge dissemination and scientific\nadvancement, increasingly faces threats from unethical practices such as\nunconsented authorship, gift authorship, author ambiguity, and undisclosed\nconflicts of interest. While existing infrastructures like ORCID effectively\ndisambiguate researcher identities, they fall short in enforcing explicit\nauthorship consent, accurately verifying contributor roles, and robustly\ndetecting conflicts of interest during peer review. To address these\nshortcomings, this paper introduces a decentralized framework leveraging\nSelf-Sovereign Identity (SSI) and blockchain technology. The proposed model\nuses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to\nsecurely verify author identities and contributions, reducing ambiguity and\nensuring accurate attribution. A blockchain-based trust registry records\nauthorship consent and peer-review activity immutably. Privacy-preserving\ncryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support\nconflict-of-interest detection without revealing sensitive data. Verified\nauthorship metadata and consent records are embedded in publications,\nincreasing transparency. A stakeholder survey of researchers, editors, and\nreviewers suggests the framework improves ethical compliance and confidence in\nscholarly communication. This work represents a step toward a more transparent,\naccountable, and trustworthy academic publishing ecosystem.", "published": "2025-08-03 20:26:19", "link": "http://arxiv.org/abs/2508.01913v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models", "abstract": "Training large language models (LLMs) typically involves pre-training on\nmassive corpora, only to restart the process entirely when new data becomes\navailable. A more efficient and resource-conserving approach would be continual\npre-training, where models are updated with new data rather than retraining\nfrom scratch. However, the introduction of new data often causes distribution\nshifts, leading to performance degradation on previously learned tasks. In this\npaper, we take a deeper look at two popular proposals for addressing this\ndistribution shift within the continual learning literature: experience replay\nand gradient alignment. We consider continual pre-training of models within the\nLlama family of architectures at a large scale across languages with 100\nbillion tokens of training data in each language, finding that both replay and\ngradient alignment lead to more stable learning without forgetting. This\nconclusion holds both as we vary the model scale and as we vary the number and\ndiversity of tasks. Moreover, we are the first to demonstrate the effectiveness\nof gradient alignment techniques in the context of LLM pre-training and propose\nan efficient implementation of meta-experience replay (MER) that imbues\nexperience replay with the benefits of gradient alignment despite negligible\ncompute and memory overhead. Our scaling analysis across model sizes and replay\nrates indicates that small rates of replaying old examples are definitely a\nmore valuable use of compute than investing in model size, but that it is more\ncompute efficient to scale the size of the model than invest in high rates of\nreplaying old examples.", "published": "2025-08-03 20:07:15", "link": "http://arxiv.org/abs/2508.01908v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection", "abstract": "AI-generated text detectors have become essential tools for maintaining\ncontent authenticity, yet their robustness against evasion attacks remains\nquestionable. We present PDFuzz, a novel attack that exploits the discrepancy\nbetween visual text layout and extraction order in PDF documents. Our method\npreserves exact textual content while manipulating character positioning to\nscramble extraction sequences. We evaluate this approach against the ArguGPT\ndetector using a dataset of human and AI-generated text. Our results\ndemonstrate complete evasion: detector performance drops from (93.6 $\\pm$ 1.4)\n% accuracy and 0.938 $\\pm$ 0.014 F1 score to random-level performance ((50.4\n$\\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity.\nOur work reveals a vulnerability in current detection systems that is inherent\nto PDF document structures and underscores the need for implementing sturdy\nsafeguards against such attacks. We make our code publicly available at\nhttps://github.com/ACMCMC/PDFuzz.", "published": "2025-08-03 18:43:41", "link": "http://arxiv.org/abs/2508.01887v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CR"}
{"title": "Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models", "abstract": "Large Language Models have demonstrated remarkable capabilities across\ndiverse tasks, yet they frequently generate hallucinations outputs that are\nfluent but factually incorrect or unsupported. We propose Counterfactual\nProbing, a novel approach for detecting and mitigating hallucinations in LLM\noutputs. Our method dynamically generates counterfactual statements that appear\nplausible but contain subtle factual errors, then evaluates the model's\nsensitivity to these perturbations. We hypothesize that genuine knowledge\nexhibits robustness to counterfactual variations, while hallucinated content\nshows inconsistent confidence patterns when confronted with plausible\nalternatives. Our comprehensive evaluation on TruthfulQA, factual statement\ndatasets, and curated hallucination examples demonstrates that counterfactual\nprobing achieves superior detection performance compared to baseline methods,\nwhile our adaptive mitigation strategies reduce hallucination scores by an\naverage of 24.5%. The approach requires no model retraining and can be\nintegrated into existing LLM pipelines as a realtime verification mechanism.", "published": "2025-08-03 17:29:48", "link": "http://arxiv.org/abs/2508.01862v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents", "abstract": "Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner", "published": "2025-08-03 17:17:52", "link": "http://arxiv.org/abs/2508.01858v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MLP Memory: Language Modeling with Retriever-pretrained External Memory", "abstract": "While modern decoder-only LLMs achieve superior performance across various\ndomains, hallucinations have risen to be a common problem in their generated\ntext, hindering their application in knowledge-intensive tasks.\nRetriever-augmented generation (RAG) offers a solution, but the non-parametric\nnature of the retriever hinders its deep interaction with LLM. In this work, we\npropose to decouple memorization from the LLM decoder using a pretrained,\ndifferentiable external memory. The external memory is an MLP pretrained by\nimitating the behavior of a retriever on the entire pretraining dataset. Our\nresulting architecture, which comprises a transformer decoder and an external\nMLP memory pretrained on language modeling and retriever imitation\nrespectively, demonstrates strong perplexity and performance on downstream\ntasks. Experiments show our architecture exhibits steeper power-law scaling\nwith model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web\ndatasets compared to decoder-only models while benefiting from added training\nwithout overfitting. We demonstrate superior performance on three hallucination\nbenchmarks and nine memory-intensive tasks. Additionally, our approach delivers\n$80\\times$ speedup over $k$NN-LM (500M tokens) and $1.3\\times$ faster inference\nthan decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP\nmemory improves StrategyQA performance. We will open-source our code and models\nin the future.", "published": "2025-08-03 16:40:53", "link": "http://arxiv.org/abs/2508.01832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy", "abstract": "Question answering over heterogeneous knowledge graphs (KGQA) involves\nreasoning across diverse schemas, incomplete alignments, and distributed data\nsources. Existing text-to-SPARQL approaches rely on large-scale domain-specific\nfine-tuning or operate within single-graph settings, limiting their\ngeneralizability in low-resource domains and their ability to handle queries\nspanning multiple graphs. These challenges are particularly relevant in domains\nsuch as the circular economy, where information about classifications,\nprocesses, and emissions is distributed across independently curated knowledge\ngraphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes\nKGQA into subtasks managed by specialized agents responsible for retrieval,\nquery generation, and verification. A scheduler assigns subgoals to different\ngraphs using weak-to-strong alignment strategies. A two-stage verifier detects\nstructurally invalid and semantically underspecified queries through symbolic\nvalidation and counterfactual consistency checks. Experiments on real-world\ncircular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy\nby 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing\nthe average prompt length by 46.4%. These results demonstrate the benefits of\nagent-based schema-aware reasoning for scalable KGQA and support\ndecision-making in sustainability domains through robust cross-graph reasoning.", "published": "2025-08-03 15:58:54", "link": "http://arxiv.org/abs/2508.01815v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark", "abstract": "Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly\non morpho-syntactic tasks, neglecting the semantic dimension of language\nunderstanding. To bridge this gap, we set out to deliver a Hebrew Machine\nReading Comprehension (MRC) dataset, where MRC is to be realized as extractive\nQuestion Answering. The morphologically rich nature of Hebrew poses a challenge\nto this endeavor: the indeterminacy and non-transparency of span boundaries in\nmorphologically complex forms lead to annotation inconsistencies,\ndisagreements, and flaws in standard evaluation metrics.\n  To remedy this, we devise a novel set of guidelines, a controlled\ncrowdsourcing protocol, and revised evaluation metrics that are suitable for\nthe morphologically rich nature of the language. Our resulting benchmark, HeQ\n(Hebrew QA), features 30,147 diverse question-answer pairs derived from both\nHebrew Wikipedia articles and Israeli tech news. Our empirical investigation\nreveals that standard evaluation metrics such as F1 scores and Exact Match (EM)\nare not appropriate for Hebrew (and other MRLs), and we propose a relevant\nenhancement.\n  In addition, our experiments show low correlation between models' performance\non morpho-syntactic tasks and on MRC, which suggests that models designed for\nthe former might underperform on semantics-heavy tasks. The development and\nexploration of HeQ illustrate some of the challenges MRLs pose in natural\nlanguage understanding (NLU), fostering progression towards more and better NLU\nmodels for Hebrew and other MRLs.", "published": "2025-08-03 15:53:01", "link": "http://arxiv.org/abs/2508.01812v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase", "abstract": "The field of Continuous Sign Language Recognition (CSLR) poses substantial\ntechnical challenges, including fluid inter-sign transitions, the absence of\ntemporal boundaries, and co-articulation effects. This paper, developed for the\nMSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of\nsigner-independent recognition to advance the generalization capabilities of\nCSLR systems across diverse signers. A data-centric methodology is proposed,\ncentered on systematic feature engineering, a robust preprocessing pipeline,\nand an optimized model architecture. Key contributions include a principled\nfeature selection process guided by Exploratory Data Analysis (EDA) to isolate\ncommunicative keypoints, a rigorous preprocessing pipeline incorporating\nDBSCAN-based outlier filtering and spatial normalization, and the novel\nCSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer\ndesign of the Conformer model, leveraging its capacity to model local temporal\ndependencies and global sequence context; a characteristic uniquely suited for\nthe spatio-temporal dynamics of sign language. The proposed methodology\nachieved a competitive performance, with a Word Error Rate (WER) of 5.60% on\nthe development set and 12.01% on the test set, a result that secured a 3rd\nplace ranking on the official competition platform. This research validates the\nefficacy of cross-domain architectural adaptation, demonstrating that the\nConformer model, originally conceived for speech recognition, can be\nsuccessfully repurposed to establish a new state-of-the-art performance in\nkeypoint-based CSLR.", "published": "2025-08-03 14:58:50", "link": "http://arxiv.org/abs/2508.01791v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A comprehensive taxonomy of hallucinations in Large Language Models", "abstract": "Large language models (LLMs) have revolutionized natural language processing,\nyet their propensity for hallucination, generating plausible but factually\nincorrect or fabricated content, remains a critical challenge. This report\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\nformal definition and a theoretical framework that posits its inherent\ninevitability in computable LLMs, irrespective of architecture or training. It\nexplores core distinctions, differentiating between intrinsic (contradicting\ninput context) and extrinsic (inconsistent with training data or reality), as\nwell as factuality (absolute correctness) and faithfulness (adherence to\ninput). The report then details specific manifestations, including factual\nerrors, contextual and logical inconsistencies, temporal disorientation,\nethical violations, and task-specific hallucinations across domains like code\ngeneration and multimodal applications. It analyzes the underlying causes,\ncategorizing them into data-related issues, model-related factors, and\nprompt-related influences. Furthermore, the report examines cognitive and human\nfactors influencing hallucination perception, surveys evaluation benchmarks and\nmetrics for detection, and outlines architectural and systemic mitigation\nstrategies. Finally, it introduces web-based resources for monitoring LLM\nreleases and performance. This report underscores the complex, multifaceted\nnature of LLM hallucinations and emphasizes that, given their theoretical\ninevitability, future efforts must focus on robust detection, mitigation, and\ncontinuous human oversight for responsible and reliable deployment in critical\napplications.", "published": "2025-08-03 14:37:16", "link": "http://arxiv.org/abs/2508.01781v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?", "abstract": "With the rapid development of Model Context Protocol (MCP), the number of MCP\nservers has surpassed 10,000. However, existing MCP benchmarks are limited to\nsingle-server settings with only a few tools, hindering effective evaluation of\nagent capabilities in large-scale, real-world scenarios. To address this\nlimitation, we present LiveMCPBench, the first comprehensive benchmark\ncomprising 95 real-world tasks grounded in the MCP ecosystem, designed to\nevaluate LLM agents at scale across diverse servers. To support a scalable and\nreproducible evaluation pipeline in large-scale MCP environments, we curate\nLiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and\n527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework\nthat enables automated and adaptive evaluation in dynamic, time-varying task\nenvironments, achieving 81% agreement with human reviewers. Finally, we propose\nthe MCP Copilot Agent, a multi-step agent that routes tools for dynamic\nplanning and executes tools for API interaction across the entire LiveMCPTool\nsuite. Our evaluation covers 10 leading models, with the best-performing model\n(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large\nperformance variance across models, and several widely-used models perform\npoorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench\noffers the first unified framework for benchmarking LLM agents in realistic,\ntool-rich, and dynamic MCP environments, laying a solid foundation for scalable\nand reproducible research on agent capabilities. Our code and data will be\npublicly available at https://icip-cas.github.io/LiveMCPBench.", "published": "2025-08-03 14:36:42", "link": "http://arxiv.org/abs/2508.01780v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning", "abstract": "Large language models have demonstrated remarkable capabilities in complex\nmathematical reasoning tasks, but they inevitably generate errors throughout\nmulti-step solutions. Process-level Reward Models (PRMs) have shown great\npromise by providing supervision and evaluation at each intermediate step,\nthereby effectively improving the models' reasoning abilities. However,\ntraining effective PRMs requires high-quality process reward data, yet existing\nmethods for constructing such data are often labour-intensive or inefficient.\nIn this paper, we propose an uncertainty-driven framework for automated process\nreward data construction, encompassing both data generation and annotation\nprocesses for PRMs. Additionally, we identify the limitations of both majority\nvote and PRMs, and introduce two generic uncertainty-aware output aggregation\nmethods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which\ncombine the strengths of majority vote with PRMs. Extensive experiments on\nProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the\nproposed PRM data construction framework, and demonstrate that the two output\naggregation methods further improve the mathematical reasoning abilities across\ndiverse PRMs. The code and data will be publicly available at\nhttps://github.com/Jiuzhouh/UnPRM.", "published": "2025-08-03 14:14:13", "link": "http://arxiv.org/abs/2508.01773v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AI-Generated Text is Non-Stationary: Detection via Temporal Tomography", "abstract": "The field of AI-generated text detection has evolved from supervised\nclassification to zero-shot statistical analysis. However, current approaches\nshare a fundamental limitation: they aggregate token-level measurements into\nscalar scores, discarding positional information about where anomalies occur.\nOur empirical analysis reveals that AI-generated text exhibits significant\nnon-stationarity, statistical properties vary by 73.8\\% more between text\nsegments compared to human writing. This discovery explains why existing\ndetectors fail against localized adversarial perturbations that exploit this\noverlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),\na novel detection paradigm that preserves positional information by\nreformulating detection as a signal processing task. TDT treats token-level\ndiscrepancies as a time-series signal and applies Continuous Wavelet Transform\nto generate a two-dimensional time-scale representation, capturing both the\nlocation and linguistic scale of statistical anomalies. On the RAID benchmark,\nTDT achieves 0.855 AUROC (7.1\\% improvement over the best baseline). More\nimportantly, TDT demonstrates robust performance on adversarial tasks, with\n14.1\\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its\nsophisticated analysis, TDT maintains practical efficiency with only 13\\%\ncomputational overhead. Our work establishes non-stationarity as a fundamental\ncharacteristic of AI-generated text and demonstrates that preserving temporal\ndynamics is essential for robust detection.", "published": "2025-08-03 13:43:34", "link": "http://arxiv.org/abs/2508.01754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction", "abstract": "Identifying user preferences in dialogue systems is a pivotal aspect of\nproviding satisfying services. Current research shows that using large language\nmodels (LLMs) to fine-tune a task-specific preference extractor yields\nexcellent results in terms of accuracy and generalization. However, the primary\nchallenge stems from the inherent difficulty in obtaining high-quality labeled\nmulti-turn dialogue data. Accurately tracking user preference transitions\nacross turns not only demands intensive domain expertise and contextual\nconsistency maintenance for annotators (termed \\textbf{``Annotating\nDisaster''}) but also complicates model training due to error propagation in\nsequential dependency learning. Inspired by the observation that multi-turn\npreference extraction can be decomposed into iterative executions of one-turn\nextraction processes. We propose a novel dialogue data generation framework\nnamed \\textbf{IterChat}. First, we construct a new data format that categorizes\nthe dialogue data into attributed historical preferences and one-turn\ndialogues. This reduces the probability of annotation errors and improves\nannotation efficiency. Then, to generate a high-quality and diverse dialogue\ndataset, we adopt GPT4 to pre-define the preference slots in the target\npreference extractor task and then randomly sample the subset of the slots and\ntheir corresponding schema values to create the dialogue datasets. Experimental\nresults indicate that fine-tuning or only few-shot prompting with the new\ndialogue format yields superior performance compared to the original multi-turn\ndialogues. Additionally, the new data format improves annotator efficiency with\na win rate of 28.4\\% higher than the original multi-turn dialogues.", "published": "2025-08-03 12:44:03", "link": "http://arxiv.org/abs/2508.01739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications", "abstract": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models.", "published": "2025-08-03 10:35:05", "link": "http://arxiv.org/abs/2508.01710v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption", "abstract": "Large language models (LLMs) have advanced natural language processing (NLP)\nskills such as through next-token prediction and self-attention, but their\nability to integrate broad context also makes them prone to incorporating\nirrelevant information. Prior work has focused on semantic leakage, bias\nintroduced by semantically irrelevant context. In this paper, we introduce\nexpression leakage, a novel phenomenon where LLMs systematically generate\nsentimentally charged expressions that are semantically unrelated to the input\ncontext. To analyse the expression leakage, we collect a benchmark dataset\nalong with a scheme to automatically generate a dataset from free-form text\nfrom common-crawl. In addition, we propose an automatic evaluation pipeline\nthat correlates well with human judgment, which accelerates the benchmarking by\ndecoupling from the need of annotation for each analysed model. Our experiments\nshow that, as the model scales in the parameter space, the expression leakage\nreduces within the same LLM family. On the other hand, we demonstrate that\nexpression leakage mitigation requires specific care during the model building\nprocess, and cannot be mitigated by prompting. In addition, our experiments\nindicate that, when negative sentiment is injected in the prompt, it disrupts\nthe generation process more than the positive sentiment, causing a higher\nexpression leakage rate.", "published": "2025-08-03 10:29:19", "link": "http://arxiv.org/abs/2508.01708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for\nenhancing the capabilities of Large Language Models (LLMs), especially in\nknowledge-intensive tasks. Despite its advantages, current RAG methods often\nstruggle to *fully exploit knowledge during generation*. In particular, the\nsynergy between the model's internal parametric knowledge and external\nretrieved knowledge remains limited. Retrieved contents may sometimes mislead\ngeneration, while certain generated content can guide the model toward more\naccurate outputs. In this work, we propose Collaborative Chain-of-Agents, a\nframework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior\nperformance on open-domain and multi-hop QA tasks.", "published": "2025-08-03 10:00:38", "link": "http://arxiv.org/abs/2508.01696v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe", "abstract": "We present Voxlect, a novel benchmark for modeling dialects and regional\nlanguages worldwide using speech foundation models. Specifically, we report\ncomprehensive benchmark evaluations on dialects and regional language varieties\nin English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,\nSpanish, French, German, Brazilian Portuguese, and Italian. Our study used over\n2 million training utterances from 30 publicly available speech corpora that\nare provided with dialectal information. We evaluate the performance of several\nwidely used speech foundation models in classifying speech dialects. We assess\nthe robustness of the dialectal models under noisy conditions and present an\nerror analysis that highlights modeling results aligned with geographic\ncontinuity. In addition to benchmarking dialect classification, we demonstrate\nseveral downstream applications enabled by Voxlect. Specifically, we show that\nVoxlect can be applied to augment existing speech recognition datasets with\ndialect information, enabling a more detailed analysis of ASR performance\nacross dialectal variations. Voxlect is also used as a tool to evaluate the\nperformance of speech generation systems. Voxlect is publicly available with\nthe license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.", "published": "2025-08-03 09:51:28", "link": "http://arxiv.org/abs/2508.01691v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Bidirectional Process Reward Model", "abstract": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning quality of Large Language Models (LLMs) by assigning fine-grained\nscores to intermediate reasoning steps within a solution trajectory. However,\nexisting PRMs predominantly adopt a unidirectional left-to-right (L2R)\nevaluation paradigm, which limits their ability to leverage global context,\nmaking it challenging to verify the consistency of earlier steps based on later\nones. In light of these challenges, we propose a novel bidirectional evaluation\nparadigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly\nincorporates a parallel right-to-left (R2L) evaluation stream alongside the\nconventional L2R flow, enabling later reasoning steps to help assess earlier\nones in real time. Notably, the built-in R2L evaluation is implemented solely\nthrough prompt modifications that reverse the original reasoning trajectory,\nwithout any additional parameters or inference latency introduced. This ensures\nBiPRM remains both efficient and broadly compatible with existing PRM studies.\nWe conduct extensive experiments on two mathematical reasoning benchmarks using\nsamples generated by three different policy models. Our method, BiPRM, is\nevaluated across three backbones and three distinct PRM objectives. Across all\nsettings, BiPRM consistently outperforms unidirectional baselines, achieving up\nto a 31.9% improvement in stepwise reward evaluation. Generally, our results\nhighlight BiPRM's effectiveness, robustness, and general applicability,\noffering a promising new direction for process-based reward modeling.", "published": "2025-08-03 09:23:49", "link": "http://arxiv.org/abs/2508.01682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "abstract": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements.", "published": "2025-08-03 09:04:48", "link": "http://arxiv.org/abs/2508.01674v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Authorship Attribution in Multilingual Machine-Generated Texts", "abstract": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios.", "published": "2025-08-03 08:28:02", "link": "http://arxiv.org/abs/2508.01656v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "DUP: Detection-guided Unlearning for Backdoor Purification in Language Models", "abstract": "As backdoor attacks become more stealthy and robust, they reveal critical\nweaknesses in current defense strategies: detection methods often rely on\ncoarse-grained feature statistics, and purification methods typically require\nfull retraining or additional clean models. To address these challenges, we\npropose DUP (Detection-guided Unlearning for Purification), a unified framework\nthat integrates backdoor detection with unlearning-based purification. The\ndetector captures feature-level anomalies by jointly leveraging class-agnostic\ndistances and inter-layer transitions. These deviations are integrated through\na weighted scheme to identify poisoned inputs, enabling more fine-grained\nanalysis. Based on the detection results, we purify the model through a\nparameter-efficient unlearning mechanism that avoids full retraining and does\nnot require any external clean model. Specifically, we innovatively repurpose\nknowledge distillation to guide the student model toward increasing its output\ndivergence from the teacher on detected poisoned samples, effectively forcing\nit to unlearn the backdoor behavior. Extensive experiments across diverse\nattack methods and language model architectures demonstrate that DUP achieves\nsuperior defense performance in detection accuracy and purification efficacy.\nOur code is available at https://github.com/ManHu2025/DUP.", "published": "2025-08-03 08:12:21", "link": "http://arxiv.org/abs/2508.01647v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings", "abstract": "Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on\naccurate and relevant retrieval of chemical literature. However,\ngeneral-purpose text embedding models frequently fail to adequately represent\ncomplex chemical terminologies, resulting in suboptimal retrieval quality.\nSpecialized embedding models tailored to chemical literature retrieval have not\nyet been developed, leaving a substantial performance gap. To address this\nchallenge, we introduce ChEmbed, a domain-adapted family of text embedding\nmodels fine-tuned on a dataset comprising chemistry-specific text from the\nPubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training\ndata, we employ large language models to synthetically generate queries,\nresulting in approximately 1.7 million high-quality query-passage pairs.\nAdditionally, we augment the tokenizer by adding 900 chemically specialized\ntokens to previously unused slots, which significantly reduces the\nfragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains\na 8192-token context length, enabling the efficient retrieval of longer\npassages compared to many other open-source embedding models, which typically\nhave a context length of 512 or 2048 tokens. Evaluated on our newly introduced\nChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general\nembedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents\na practical, lightweight, and reproducible embedding solution that effectively\nimproves retrieval for chemical literature search.", "published": "2025-08-03 08:04:44", "link": "http://arxiv.org/abs/2508.01643v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets", "abstract": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct.", "published": "2025-08-03 07:33:28", "link": "http://arxiv.org/abs/2508.01630v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models", "abstract": "Prompt-based adversarial attacks have become an effective means to assess the\nrobustness of large language models (LLMs). However, existing approaches often\ntreat prompts as monolithic text, overlooking their structural\nheterogeneity-different prompt components contribute unequally to adversarial\nrobustness. Prior works like PromptRobust assume prompts are value-neutral, but\nour analysis reveals that complex, domain-specific prompts with rich structures\nhave components with differing vulnerabilities. To address this gap, we\nintroduce PromptAnatomy, an automated framework that dissects prompts into\nfunctional components and generates diverse, interpretable adversarial examples\nby selectively perturbing each component using our proposed method, ComPerturb.\nTo ensure linguistic plausibility and mitigate distribution shifts, we further\nincorporate a perplexity (PPL)-based filtering mechanism. As a complementary\nresource, we annotate four public instruction-tuning datasets using the\nPromptAnatomy framework, verified through human review. Extensive experiments\nacross these datasets and five advanced LLMs demonstrate that ComPerturb\nachieves state-of-the-art attack success rates. Ablation studies validate the\ncomplementary benefits of prompt dissection and PPL filtering. Our results\nunderscore the importance of prompt structure awareness and controlled\nperturbation for reliable adversarial robustness evaluation in LLMs. Code and\ndata are available at https://github.com/Yujiaaaaa/PACP.", "published": "2025-08-03 02:46:30", "link": "http://arxiv.org/abs/2508.01554v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization", "abstract": "Prompt engineering is crucial for unlocking the potential of Large Language\nModels (LLMs). Still, since manual prompt design is often complex,\nnon-intuitive, and time-consuming, automatic prompt optimization has emerged as\na research area. However, a significant challenge in prompt optimization is\nmanaging the inherent trade-off between task performance, such as accuracy, and\ncontext size. Most existing automated methods focus on a single objective,\ntypically performance, thereby failing to explore the critical spectrum of\nefficiency and effectiveness. This paper introduces the MOPrompt, a novel\nMulti-objective Evolutionary Optimization (EMO) framework designed to optimize\nprompts for both accuracy and context size (measured in tokens) simultaneously.\nOur framework maps the Pareto front of prompt solutions, presenting\npractitioners with a set of trade-offs between context size and performance, a\ncrucial tool for deploying Large Language Models (LLMs) in real-world\napplications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,\nusing Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that\nMOPrompt substantially outperforms the baseline framework. For the Sabiazinho\nmodel, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)\nas the best baseline solution, but with a 31% reduction in token length.", "published": "2025-08-03 01:50:43", "link": "http://arxiv.org/abs/2508.01541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Improved Bound for the Beck-Fiala Conjecture", "abstract": "In 1981, Beck and Fiala [Discrete Appl. Math, 1981] conjectured that given a\nset system $A \\in \\{0,1\\}^{m \\times n}$ with degree at most $k$ (i.e., each\ncolumn of $A$ has at most $k$ non-zeros), its combinatorial discrepancy\n$\\mathsf{disc}(A) := \\min_{x \\in \\{\\pm 1\\}^n} \\|Ax\\|_\\infty$ is at most\n$O(\\sqrt{k})$. Previously, the best-known bounds for this conjecture were\neither $O(k)$, first established by Beck and Fiala [Discrete Appl. Math, 1981],\nor $O(\\sqrt{k \\log n})$, first proved by Banaszczyk [Random Struct. Algor.,\n1998].\n  We give an algorithmic proof of an improved bound of $O(\\sqrt{k \\log\\log n})$\nwhenever $k \\geq \\log^5 n$, thus matching the Beck-Fiala conjecture up to\n$O(\\sqrt{\\log \\log n})$ for almost the full regime of $k$.", "published": "2025-08-03 22:16:43", "link": "http://arxiv.org/abs/2508.01937v1", "categories": ["math.CO", "cs.DM", "cs.DS"], "primary_category": "math.CO"}
{"title": "Edge open packing: further characterizations", "abstract": "Let $G=(V, E)$ be a graph where $V(G)$ and $E(G)$ are the vertex and edge\nsets, respectively. In a graph $G$, two edges $e_1, e_2\\in E(G)$ are said to\nhave \\emph{common edge} $e\\neq e_1, e_2$ if $e$ joins an endpoint of $e_1$ to\nan endpoint of $e_2$ in $G$. A subset $D\\subseteq E(G)$ is called an \\emph{edge\nopen packing set} in $G$ if no two edges in $D$ share a common edge in $G$, and\nthe largest size of such a set in $G$ is known as \\emph{edge open packing\nnumber}, represented by $\\rho_{e}^o(G)$. In the introductory paper (Chelladurai\net al. (2022)), necessary and sufficient conditions for $\\rho_{e}^o(G)=1, 2$\nwere provided, and the graphs $G$ with $\\rho_{e}^o(G)\\in \\{m-2, m-1, m\\}$ were\ncharacterized, where $m$ is the number of edges of $G$. In this paper, we\nfurther characterize the graphs $G$. First, we show necessary and sufficient\nconditions for $\\rho_{e}^o(G)=t$, for any integer $t\\geq 3$. Finally, we\ncharacterize the graphs with $\\rho_{e}^o(G)=m-3$.", "published": "2025-08-03 22:11:41", "link": "http://arxiv.org/abs/2508.01935v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Counterfactual Reciprocal Recommender Systems for User-to-User Matching", "abstract": "Reciprocal recommender systems (RRS) in dating, gaming, and talent platforms\nrequire mutual acceptance for a match. Logged data, however, over-represents\npopular profiles due to past exposure policies, creating feedback loops that\nskew learning and fairness. We introduce Counterfactual Reciprocal Recommender\nSystems (CFRR), a causal framework to mitigate this bias. CFRR uses inverse\npropensity scored, self-normalized objectives. Experiments show CFRR improves\nNDCG@10 by up to 3.5% (e.g., from 0.459 to 0.475 on DBLP, from 0.299 to 0.307\non Synthetic), increases long-tail user coverage by up to 51% (from 0.504 to\n0.763 on Synthetic), and reduces Gini exposure inequality by up to 24% (from\n0.708 to 0.535 on Synthetic). CFRR offers a promising approach for more\naccurate and fair user-to-user matching.", "published": "2025-08-03 17:45:04", "link": "http://arxiv.org/abs/2508.01867v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Distributed fault-tolerant quantum memories over a 2xL array of qubit modules", "abstract": "We propose an architecture for a quantum memory distributed over a $2 \\times\nL$ array of modules equipped with a cyclic shift implemented via flying qubits.\nThe logical information is distributed across the first row of $L$ modules and\nquantum error correction is executed using ancilla modules on the second row\nequipped with a cyclic shift. This work proves that quantum LDPC codes such as\nBB codes can maintain their performance in a distributed setting while using\nsolely one simple connector: a cyclic shift. We propose two strategies to\nperform quantum error correction on a $2 \\times L$ module array: (i) The cyclic\nlayout which applies to any stabilizer codes, whereas previous results for\nqubit arrays are limited to CSS codes. (ii) The sparse cyclic layout, specific\nto bivariate bicycle (BB) codes. For the $[[144,12,12]]$ BB code, using the\nsparse cyclic layout we obtain a quantum memory with $12$ logical qubits\ndistributed over $12$ modules, containing $12$ physical qubits each. We propose\nphysical implementations of this architecture using flying qubits, that can be\nfaithfully transported, and include qubits encoded in ions, neutral atoms,\nelectrons or photons. We performed numerical simulations when modules are long\nion chains and when modules are single-qubit arrays of ions showing that the\ndistributed BB code achieves a logical error rate below $2 \\cdot 10^{-6}$ when\nthe physical error rate is $10^{-3}$.", "published": "2025-08-03 18:21:34", "link": "http://arxiv.org/abs/2508.01879v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Implementing Neural Networks Over-the-Air via Reconfigurable Intelligent Surfaces", "abstract": "In this paper, we investigate reconfigurable intelligent surface (RIS)-aided\nmultiple-input-multiple-output (MIMO) OAC systems designed to emulate the\nfully-connected (FC) layer of a neural network (NN) via analog OAC, where the\nRIS and the transceivers are jointly adjusted to engineer the ambient wireless\npropagation environment to emulate the weights of the target FC layer. We refer\nto this novel computational paradigm as AirFC. We first study the case in which\nthe precoder, combiner, and RIS phase shift matrices are jointly optimized to\nminimize the mismatch between the OAC system and the target FC layer. To solve\nthis non-convex optimization problem, we propose a low-complexity alternating\noptimization algorithm, where semi-closed-form/closed-form solutions for all\noptimization variables are derived. Next, we consider training of the system\nparameters using two distinct learning strategies, namely centralized training\nand distributed training. In the centralized training approach, training is\nperformed at either the transmitter or the receiver, whichever possesses the\nchannel state information (CSI), and the trained parameters are provided to the\nother terminal. In the distributed training approach, the transmitter and\nreceiver iteratively update their parameters through back and forth\ntransmissions by leveraging channel reciprocity, thereby avoiding CSI\nacquisition and significantly reducing computational complexity. Subsequently,\nwe extend our analysis to a multi-RIS scenario by exploiting its spatial\ndiversity gain to enhance the system performance. Simulation results show that\nthe AirFC system realized by the RIS-aided MIMO configuration achieves\nsatisfactory classification accuracy.", "published": "2025-08-03 16:55:11", "link": "http://arxiv.org/abs/2508.01840v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Explicit Function-Correcting Code Constructions for Lee Metric Channels", "abstract": "Function-Correcting Codes (FCCs) are a novel class of codes designed to\nprotect function evaluations of messages against errors while minimizing\nredundancy. A theoretical framework for systematic FCCs to channels matched to\nthe Lee metric has been studied recently, which introduced function-correcting\nLee codes (FCLCs) and also derived upper and lower bounds on their optimal\nredundancy. In this paper, we propose a Plotkin-like bound for irregular\nLee-distance codes, which improves an existing bound. We construct explicit\nFCLCs for specific classes of functions, including the Lee weight, Lee weight\ndistribution, modular sum, and locally bounded function. For these functions,\nlower bounds on redundancy are obtained, and our constructions are shown to be\noptimal in certain cases. Finally, a comparative analysis with classical Lee\nerror-correcting codes and codes correcting errors in function values\ndemonstrates that FCLCs can significantly reduce redundancy while preserving\nfunction correctness.", "published": "2025-08-03 10:06:42", "link": "http://arxiv.org/abs/2508.01702v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Diffusion Models for Future Networks and Communications: A Comprehensive Survey", "abstract": "The rise of Generative AI (GenAI) in recent years has catalyzed\ntransformative advances in wireless communications and networks. Among the\nmembers of the GenAI family, Diffusion Models (DMs) have risen to prominence as\na powerful option, capable of handling complex, high-dimensional data\ndistribution, as well as consistent, noise-robust performance. In this survey,\nwe aim to provide a comprehensive overview of the theoretical foundations and\npractical applications of DMs across future communication systems. We first\nprovide an extensive tutorial of DMs and demonstrate how they can be applied to\nenhance optimizers, reinforcement learning and incentive mechanisms, which are\npopular approaches for problems in wireless networks. Then, we review and\ndiscuss the DM-based methods proposed for emerging issues in future networks\nand communications, including channel modeling and estimation, signal detection\nand data reconstruction, integrated sensing and communication, resource\nmanagement in edge computing networks, semantic communications and other\nnotable issues. We conclude the survey with highlighting technical limitations\nof DMs and their applications, as well as discussing future research\ndirections.", "published": "2025-08-03 04:59:58", "link": "http://arxiv.org/abs/2508.01586v1", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.IT", "cs.NI", "math.IT"], "primary_category": "cs.LG"}
{"title": "Agent-Based Feature Generation from Clinical Notes for Outcome Prediction", "abstract": "Electronic health records (EHRs) contain rich unstructured clinical notes\nthat could enhance predictive modeling, yet extracting meaningful features from\nthese notes remains challenging. Current approaches range from labor-intensive\nmanual clinician feature generation (CFG) to fully automated representational\nfeature generation (RFG) that lack interpretability and clinical relevance.\nHere we introduce SNOW (Scalable Note-to-Outcome Workflow), a modular\nmulti-agent system powered by large language models (LLMs) that autonomously\ngenerates structured clinical features from unstructured notes without human\nintervention. We evaluated SNOW against manual CFG, clinician-guided LLM\napproaches, and RFG methods for predicting 5-year prostate cancer recurrence in\n147 patients from Stanford Healthcare. While manual CFG achieved the highest\nperformance (AUC-ROC: 0.771), SNOW matched this performance (0.761) without\nrequiring any clinical expertise, significantly outperforming both baseline\nfeatures alone (0.691) and all RFG approaches. The clinician-guided LLM method\nalso performed well (0.732) but still required expert input. SNOW's specialized\nagents handle feature discovery, extraction, validation, post-processing, and\naggregation, creating interpretable features that capture complex clinical\ninformation typically accessible only through manual review. Our findings\ndemonstrate that autonomous LLM systems can replicate expert-level feature\nengineering at scale, potentially transforming how clinical ML models leverage\nunstructured EHR data while maintaining the interpretability essential for\nclinical deployment.", "published": "2025-08-03 23:45:18", "link": "http://arxiv.org/abs/2508.01956v1", "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Distributed games with jumps: An $\u03b1$-potential game approach", "abstract": "Motivated by game-theoretic models of crowd motion dynamics, this paper\nanalyzes a broad class of distributed games with jump diffusions within the\nrecently developed $\\alpha$-potential game framework. We demonstrate that\nanalyzing the $\\alpha$-Nash equilibria reduces to solving a finite-dimensional\ncontrol problem. Beyond the viscosity and verification characterizations for\nthe general games, we explicitly and in detail examine how spatial population\ndistributions and interaction rules influence the structure of $\\alpha$-Nash\nequilibria in these distributed settings, and in particular for crowd motion\ngames.\n  Our theoretical results are supported by numerical implementations using\npolicy gradient-based algorithms, further demonstrating the computational\nadvantages of the $\\alpha$-potential game framework in computing Nash\nequilibria for general dynamic games.", "published": "2025-08-03 21:45:10", "link": "http://arxiv.org/abs/2508.01929v1", "categories": ["math.OC", "cs.MA", "math.PR", "91A06, 91A15, 91A14, 91A16"], "primary_category": "math.OC"}
{"title": "Optimizing Day-Ahead Energy Trading with Proximal Policy Optimization and Blockchain", "abstract": "The increasing penetration of renewable energy sources in day-ahead energy\nmarkets introduces challenges in balancing supply and demand, ensuring grid\nresilience, and maintaining trust in decentralized trading systems. This paper\nproposes a novel framework that integrates the Proximal Policy Optimization\n(PPO) algorithm, a state-of-the-art reinforcement learning method, with\nblockchain technology to optimize automated trading strategies for prosumers in\nday-ahead energy markets. We introduce a comprehensive framework that employs\nRL agent for multi-objective energy optimization and blockchain for\ntamper-proof data and transaction management. Simulations using real-world data\nfrom the Electricity Reliability Council of Texas (ERCOT) demonstrate the\neffectiveness of our approach. The RL agent achieves demand-supply balancing\nwithin 2\\% and maintains near-optimal supply costs for the majority of the\noperating hours. Moreover, it generates robust battery storage policies capable\nof handling variability in solar and wind generation. All decisions are\nrecorded on an Algorand-based blockchain, ensuring transparency, auditability,\nand security - key enablers for trustworthy multi-agent energy trading. Our\ncontributions include a novel system architecture, curriculum learning for\nrobust agent development, and actionable policy insights for practical\ndeployment.", "published": "2025-08-03 18:45:17", "link": "http://arxiv.org/abs/2508.01888v1", "categories": ["cs.LG", "cs.CR", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Causality and Decision-making: A Logical Framework for Systems and Security Modelling", "abstract": "Causal reasoning is essential for understanding decision-making about the\nbehaviour of complex `ecosystems' of systems that underpin modern society, with\nsecurity -- including issues around correctness, safety, resilience, etc. --\ntypically providing critical examples. We present a theory of strategic\nreasoning about system modelling based on minimal structural assumptions and\nemploying the methods of transition systems, supported by a modal logic of\nsystem states in the tradition of van Benthem, Hennessy, and Milner, and\nvalidated through equivalence theorems. Our framework introduces an\nintervention operator and a separating conjunction to capture actual causal\nrelationships between component systems of the ecosystem, aligning naturally\nwith Halpern and Pearl's counterfactual approach based on Structural Causal\nModels. We illustrate the applicability through examples of of decision-making\nabout microservices in distributed systems. We discuss localized\ndecision-making through a separating conjunction. This work unifies a formal,\nminimalistic notion of system behaviour with a Halpern--Pearl-compatible theory\nof counterfactual reasoning, providing a logical foundation for studying\ndecision making about causality in complex interacting systems.", "published": "2025-08-03 13:51:20", "link": "http://arxiv.org/abs/2508.01758v1", "categories": ["cs.LO", "cs.MA"], "primary_category": "cs.LO"}
{"title": "Revisiting Gossip Protocols: A Vision for Emergent Coordination in Agentic Multi-Agent Systems", "abstract": "As agentic platforms scale, agents are evolving beyond static roles and fixed\ntoolchains, creating a growing need for flexible, decentralized coordination.\nToday's structured communication protocols (e.g., direct agent-to-agent\nmessaging) excel at reliability and task delegation, but they fall short in\nenabling emergent, swarm-like intelligence, where distributed agents\ncontinuously learn, adapt, and communicate to form collective cognition. This\npaper revisits gossip protocols, long valued in distributed systems for their\nfault tolerance and decentralization, and argues that they offer a missing\nlayer for context-rich, adaptive communication in agentic AI. Gossip enables\nscalable, low-overhead dissemination of shared knowledge, but also raises\nunresolved challenges around semantic filtering, staleness, trustworthiness,\nand consistency in high-stakes environments. Rather than proposing a new\nframework, this work charts a research agenda for integrating gossip as a\ncomplementary substrate alongside structured protocols. We identify critical\ngaps in current agent-to-agent architectures, highlight where gossip could\nreshape assumptions about coordination, and outline open questions around\nintent propagation, knowledge decay, and peer-to-peer trust. Gossip is not a\nsilver bullet, but overlooking it risks missing a key path toward resilient,\nreflexive, and self-organizing multi-agent systems.", "published": "2025-08-03 01:18:58", "link": "http://arxiv.org/abs/2508.01531v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Non-self-adjoint sixth-order eigenvalue problems arising from clamped elastic thin films on closed domains", "abstract": "Sixth-order boundary value problems (BVPs) arise in thin-film flows with a\nsurface that has elastic bending resistance. We consider the case in which the\nelastic interface is clamped at the lateral walls of a closed trough and thus\nencloses a finite amount of fluid. For a slender film undergoing infinitesimal\ndeformations, the displacement of the elastic surface from its initial\nequilibrium position obeys a sixth-order (in space) initial boundary value\nproblem (IBVP). To solve this IBVP, we construct a set of odd and even\neigenfunctions that intrinsically satisfy the boundary conditions (BCs) of the\noriginal IBVP. These eigenfunctions are the solutions of a non-self-adjoint\nsixth-order eigenvalue problem (EVP). To use the eigenfunctions for series\nexpansions, we also construct and solve the adjoint EVP, leading to another set\nof even and odd eigenfunctions, which are orthogonal to the original set\n(biorthogonal). The eigenvalues of the adjoint EVP are the same as those of the\noriginal EVP, and we find accurate asymptotic formulas for them. Next,\nemploying the biorthogonal sets of eigenfunctions, a Petrov--Galerkin spectral\nmethod for sixth-order problems is proposed, which can also handle lower-order\nterms in the IBVP. The proposed method is tested on two model sixth-order BVPs,\nwhich admit exact solutions. We explicitly derive all the necessary formulas\nfor expanding the quantities that appear in the model problems into the set(s)\nof eigenfunctions. For both model problems, we find that the approximate\nPetrov--Galerkin spectral solution is in excellent agreement with the exact\nsolution. The convergence rate of the spectral series is rapid, exceeding the\nexpected sixth-order algebraic rate.", "published": "2025-08-03 23:36:51", "link": "http://arxiv.org/abs/2508.01952v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "On the normalization of trigonometric and hyperbolic B-splines", "abstract": "Trigonometric and hyperbolic B-splines can be computed via recurrence\nrelations analogous to the classical polynomial B-splines. However, in their\noriginal formulation, these two types of B-splines do not form a partition of\nunity and consequently do not admit the notion of control polygons with the\nconvex hull property for design purposes. In this paper, we look into explicit\nexpressions for their normalization and provide a recursive algorithm to\ncompute the corresponding normalization weights. As example application, we\nconsider the exact representation of a circle in terms of $C^{2n-1}$\ntrigonometric B-splines of order $m=2n+1\\geq3$, with a variable number of\ncontrol points. We also illustrate the approximation power of trigonometric and\nhyperbolic splines.", "published": "2025-08-03 16:10:29", "link": "http://arxiv.org/abs/2508.01817v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Physics-informed approach for exploratory Hamilton--Jacobi--Bellman equations via policy iterations", "abstract": "We propose a mesh-free policy iteration framework based on physics-informed\nneural networks (PINNs) for solving entropy-regularized stochastic control\nproblems. The method iteratively alternates between soft policy evaluation and\nimprovement using automatic differentiation and neural approximation, without\nrelying on spatial discretization. We present a detailed $L^2$ error analysis\nthat decomposes the total approximation error into three sources: iteration\nerror, policy network error, and PDE residual error. The proposed algorithm is\nvalidated with a range of challenging control tasks, including high-dimensional\nlinear-quadratic regulation in 5D and 10D, as well as nonlinear systems such as\npendulum and cartpole problems. Numerical results confirm the scalability,\naccuracy, and robustness of our approach across both linear and nonlinear\nbenchmarks.", "published": "2025-08-03 11:14:20", "link": "http://arxiv.org/abs/2508.01720v1", "categories": ["math.NA", "cs.NA", "93E20, 35Q93, 68T07"], "primary_category": "math.NA"}
{"title": "Neural Policy Iteration for Stochastic Optimal Control: A Physics-Informed Approach", "abstract": "We propose a physics-informed neural network policy iteration (PINN-PI)\nframework for solving stochastic optimal control problems governed by\nsecond-order Hamilton--Jacobi--Bellman (HJB) equations. At each iteration, a\nneural network is trained to approximate the value function by minimizing the\nresidual of a linear PDE induced by a fixed policy. This linear structure\nenables systematic $L^2$ error control at each policy evaluation step, and\nallows us to derive explicit Lipschitz-type bounds that quantify how value\ngradient errors propagate to the policy updates. This interpretability provides\na theoretical basis for evaluating policy quality during training. Our method\nextends recent deterministic PINN-based approaches to stochastic settings,\ninheriting the global exponential convergence guarantees of classical policy\niteration under mild conditions. We demonstrate the effectiveness of our method\non several benchmark problems, including stochastic cartpole, pendulum problems\nand high-dimensional linear quadratic regulation (LQR) problems in up to 10D.", "published": "2025-08-03 11:02:25", "link": "http://arxiv.org/abs/2508.01718v1", "categories": ["cs.LG", "cs.CE", "cs.NA", "math.NA", "93E20, 35Q93, 68T07, 65N21"], "primary_category": "cs.LG"}
{"title": "A Randomized GMsFEM with Data-Driven Predictors for Parametric Flow Problems in Multiscale Heterogeneous Media", "abstract": "In this paper, we propose a randomized generalized multiscale finite element\nmethod (Randomized GMsFEM) for flow problems with parameterized inputs and\nhigh-contrast heterogeneous media. The method employs a data-driven predictor\nto construct multiscale basis functions in two stages: offline and online. In\nthe offline stage, a snapshot space is generated via spectral decompositions,\nand a reduced matrix is obtained using SVD to predict eigenfunctions. In the\nonline stage, these eigenfunctions are evaluated for new parameter realizations\nto construct the multiscale space. Furthermore, our approach addresses the\ncomplexity of multiple permeability fields with random inputs and multiple\nmultiscale information, providing accurate and efficient approximations.\nMoreover, we conduct a rigorous convergence analysis for our Randomized GMsFEM.\nFinally, we present extensive numerical examples, demonstrating its superior\nperformance compared to the traditional GMsFEM.", "published": "2025-08-03 08:53:28", "link": "http://arxiv.org/abs/2508.01666v1", "categories": ["math.NA", "cs.NA", "65N99, 65N30, 35R60"], "primary_category": "math.NA"}
{"title": "Numerical analysis for computing multiple solutions of semilinear elliptic problems by high-index saddle dynamics: Part I--Index-1 case", "abstract": "This work presents a numerical analysis of computing transition states of\nsemilinear elliptic partial differential equations (PDEs) via the high-index\nsaddle dynamics at the index-1 case, or equivalently, the gentlest ascent\ndynamics. To establish clear connections between saddle dynamics and numerical\nmethods of PDEs, as well as improving their compatibility, we first propose the\ncontinuous-in-space formulation of saddle dynamics for semilinear elliptic\nproblems. This formulation yields a parabolic system that converges to saddle\npoints. We then analyze the well-posedness, $H^1$ stability and error estimates\nof semi- and fully-discrete finite element schemes. Significant efforts are\ndevoted to addressing the coupling, gradient nonlinearity, nonlocality of the\nproposed parabolic system, and the impacts of retraction due to the norm\nconstraint. The error estimate results demonstrate the accuracy and\nindex-preservation of the discrete schemes.", "published": "2025-08-03 01:27:00", "link": "http://arxiv.org/abs/2508.01534v1", "categories": ["math.NA", "cs.NA", "35K45, 65M60"], "primary_category": "math.NA"}
{"title": "Time-Varying Factor-Augmented Models for Volatility Forecasting", "abstract": "Accurate volatility forecasts are vital in modern finance for risk\nmanagement, portfolio allocation, and strategic decision-making. However,\nexisting methods face key limitations. Fully multivariate models, while\ncomprehensive, are computationally infeasible for realistic portfolios. Factor\nmodels, though efficient, primarily use static factor loadings, failing to\ncapture evolving volatility co-movements when they are most critical. To\naddress these limitations, we propose a novel, model-agnostic Factor-Augmented\nVolatility Forecast framework. Our approach employs a time-varying factor model\nto extract a compact set of dynamic, cross-sectional factors from realized\nvolatilities with minimal computational cost. These factors are then integrated\ninto both statistical and AI-based forecasting models, enabling a unified\nsystem that jointly models asset-specific dynamics and evolving market-wide\nco-movements. Our framework demonstrates strong performance across two\nprominent asset classes-large-cap U.S. technology equities and major\ncryptocurrencies-over both short-term (1-day) and medium-term (7-day) horizons.\nUsing a suite of linear and non-linear AI-driven models, we consistently\nobserve substantial improvements in predictive accuracy and economic value.\nNotably, a practical pairs-trading strategy built on our forecasts delivers\nsuperior risk-adjusted returns and profitability, particularly under adverse\nmarket conditions.", "published": "2025-08-03 18:23:34", "link": "http://arxiv.org/abs/2508.01880v1", "categories": ["q-fin.ST", "q-fin.MF"], "primary_category": "q-fin.ST"}
{"title": "Stochastic Encodings for Active Feature Acquisition", "abstract": "Active Feature Acquisition is an instance-wise, sequential decision making\nproblem. The aim is to dynamically select which feature to measure based on\ncurrent observations, independently for each test instance. Common approaches\neither use Reinforcement Learning, which experiences training difficulties, or\ngreedily maximize the conditional mutual information of the label and\nunobserved features, which makes myopic acquisitions. To address these\nshortcomings, we introduce a latent variable model, trained in a supervised\nmanner. Acquisitions are made by reasoning about the features across many\npossible unobserved realizations in a stochastic latent space. Extensive\nevaluation on a large range of synthetic and real datasets demonstrates that\nour approach reliably outperforms a diverse set of baselines.", "published": "2025-08-03 23:48:46", "link": "http://arxiv.org/abs/2508.01957v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Structure Maintained Representation Learning Neural Network for Causal Inference", "abstract": "Recent developments in causal inference have greatly shifted the interest\nfrom estimating the average treatment effect to the individual treatment\neffect. In this article, we improve the predictive accuracy of representation\nlearning and adversarial networks in estimating individual treatment effects by\nintroducing a structure keeper which maintains the correlation between the\nbaseline covariates and their corresponding representations in the high\ndimensional space. We train a discriminator at the end of representation layers\nto trade off representation balance and information loss. We show that the\nproposed discriminator minimizes an upper bound of the treatment estimation\nerror. We can address the tradeoff between distribution balance and information\nloss by considering the correlations between the learned representation space\nand the original covariate feature space. We conduct extensive experiments with\nsimulated and real-world observational data to show that our proposed Structure\nMaintained Representation Learning (SMRL) algorithm outperforms\nstate-of-the-art methods. We also demonstrate the algorithms on real electronic\nhealth record data from the MIMIC-III database.", "published": "2025-08-03 17:34:38", "link": "http://arxiv.org/abs/2508.01865v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Fast Gaussian process inference by exact Mat\u00e9rn kernel decomposition", "abstract": "To speed up Gaussian process inference, a number of fast kernel matrix-vector\nmultiplication (MVM) approximation algorithms have been proposed over the\nyears. In this paper, we establish an exact fast kernel MVM algorithm based on\nexact kernel decomposition into weighted empirical cumulative distribution\nfunctions, compatible with a class of kernels which includes multivariate\nMat\\'ern kernels with half-integer smoothness parameter. This algorithm uses a\ndivide-and-conquer approach, during which sorting outputs are stored in a data\nstructure. We also propose a new algorithm to take into account some linear\nfixed effects predictor function. Our numerical experiments confirm that our\nalgorithm is very effective for low-dimensional Gaussian process inference\nproblems with hundreds of thousands of data points. An implementation of our\nalgorithm is available at\nhttps://gitlab.com/warin/fastgaussiankernelregression.git.", "published": "2025-08-03 17:32:42", "link": "http://arxiv.org/abs/2508.01864v1", "categories": ["stat.ML", "cs.DS", "cs.LG", "stat.CO", "60G15, 65C60, 65F10, 62G30, 62G08, 65F30, 65Y20", "G.3; G.1.3; I.2.6; F.2.1"], "primary_category": "stat.ML"}
{"title": "Causal Discovery in Multivariate Time Series through Mutual Information Featurization", "abstract": "Discovering causal relationships in complex multivariate time series is a\nfundamental scientific challenge. Traditional methods often falter, either by\nrelying on restrictive linear assumptions or on conditional independence tests\nthat become uninformative in the presence of intricate, non-linear dynamics.\nThis paper proposes a new paradigm, shifting from statistical testing to\npattern recognition. We hypothesize that a causal link creates a persistent and\nlearnable asymmetry in the flow of information through a system's temporal\ngraph, even when clear conditional independencies are obscured. We introduce\nTemporal Dependency to Causality (TD2C), a supervised learning framework that\noperationalizes this hypothesis. TD2C learns to recognize these complex causal\nsignatures from a rich set of information-theoretic and statistical\ndescriptors. Trained exclusively on a diverse collection of synthetic time\nseries, TD2C demonstrates remarkable zero-shot generalization to unseen\ndynamics and established, realistic benchmarks. Our results show that TD2C\nachieves state-of-the-art performance, consistently outperforming established\nmethods, particularly in high-dimensional and non-linear settings. By reframing\nthe discovery problem, our work provides a robust and scalable new tool for\nuncovering causal structures in complex systems.", "published": "2025-08-03 17:03:13", "link": "http://arxiv.org/abs/2508.01848v1", "categories": ["cs.LG", "stat.ME", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Efficient optimization of expensive black-box simulators via marginal means, with application to neutrino detector design", "abstract": "With advances in scientific computing, computer experiments are increasingly\nused for optimizing complex systems. However, for modern applications, e.g.,\nthe optimization of nuclear physics detectors, each experiment run can require\nhundreds of CPU hours, making the optimization of its black-box simulator over\na high-dimensional space a challenging task. Given limited runs at inputs\n$\\mathbf{x}_1, \\cdots, \\mathbf{x}_n$, the best solution from these evaluated\ninputs can be far from optimal, particularly as dimensionality increases.\nExisting black-box methods, however, largely employ this ''pick-the-winner''\n(PW) solution, which leads to mediocre optimization performance. To address\nthis, we propose a new Black-box Optimization via Marginal Means (BOMM)\napproach. The key idea is a new estimator of a global optimizer $\\mathbf{x}^*$\nthat leverages the so-called marginal mean functions, which can be efficiently\ninferred with limited runs in high dimensions. Unlike PW, this estimator can\nselect solutions beyond evaluated inputs for improved optimization performance.\nAssuming the objective function follows a generalized additive model with\nunknown link function and under mild conditions, we prove that the BOMM\nestimator not only is consistent for optimization, but also has an optimization\nrate that tempers the ''curse-of-dimensionality'' faced by existing methods,\nthus enabling better performance as dimensionality increases. We present a\npractical framework for implementing BOMM using the transformed additive\nGaussian process surrogate model. Finally, we demonstrate the effectiveness of\nBOMM in numerical experiments and an application on neutrino detector\noptimization in nuclear physics.", "published": "2025-08-03 16:44:05", "link": "http://arxiv.org/abs/2508.01834v1", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Topolow: Force-Directed Euclidean Embedding of Dissimilarity Data with Robustness Against Non-Metricity and Sparsity", "abstract": "The problem of embedding a set of objects into a low-dimensional Euclidean\nspace based on a matrix of pairwise dissimilarities is fundamental in data\nanalysis, machine learning, and statistics. However, the assumptions of many\nstandard analytical methods are violated when the input dissimilarities fail to\nsatisfy metric or Euclidean axioms. We present the mathematical and statistical\nfoundations of Topolow, a physics-inspired, gradient-free optimization\nframework for such embedding problems. Topolow is conceptually related to\nforce-directed graph drawing algorithms but is fundamentally distinguished by\nits goal of quantitative metric reconstruction. It models objects as particles\nin a physical system, and its novel optimization scheme proceeds through\nsequential, stochastic pairwise interactions, which circumvents the need to\ncompute a global gradient and provides robustness against convergence to local\noptima, especially for sparse data. Topolow maximizes the likelihood under a\nLaplace error model, robust to outliers and heterogeneous errors, and properly\nhandles censored data. Crucially, Topolow does not require the input\ndissimilarities to be metric, making it a robust solution for embedding\nnon-metric measurements into a valid Euclidean space, thereby enabling the use\nof standard analytical tools. We demonstrate the superior performance of\nTopolow compared to standard Multidimensional Scaling (MDS) methods in\nreconstructing the geometry of sparse and non-Euclidean data. This paper\nformalizes the algorithm, first introduced as Topolow in the context of\nantigenic mapping in (Arhami and Rohani, 2025) (open access), with emphasis on\nits metric embedding and mathematical properties for a broader audience. The\ngeneral-purpose function Euclidify is available in the R package topolow.", "published": "2025-08-03 12:19:17", "link": "http://arxiv.org/abs/2508.01733v1", "categories": ["cs.CG", "stat.ML"], "primary_category": "cs.CG"}
{"title": "Density estimation with atoms, and functional estimation for mixed discrete-continuous data", "abstract": "In classical density (or density-functional) estimation, it is standard to\nassume that the underlying distribution has a density with respect to the\nLebesgue measure. However, when the data distribution is a mixture of\ncontinuous and discrete components, the resulting methods are inconsistent in\ntheory and perform poorly in practice. In this paper, we point out that a minor\nmodification of existing methods for nonparametric density (functional)\nestimation can allow us to fully remove this assumption while retaining nearly\nidentical theoretical guarantees and improved empirical performance. Our\napproach is very simple: data points that appear exactly once are likely to\noriginate from the continuous component, whereas repeated observations are\nindicative of the discrete part. Leveraging this observation, we modify\nexisting estimators for a broad class of functionals of the continuous\ncomponent of the mixture; this modification is a \"wrapper\" in the sense that\nthe user can use any underlying method of their choice for continuous density\nfunctional estimation. Our modifications deliver consistency without requiring\nknowledge of the discrete support, the mixing proportion, and without imposing\nadditional assumptions beyond those needed in the absence of the discrete part.\nThus, various theorems and existing software packages can be made automatically\nmore robust, with absolutely no additional price when the data is not truly\nmixed.", "published": "2025-08-03 10:22:35", "link": "http://arxiv.org/abs/2508.01706v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Generalized Kernelized Bandits: Self-Normalized Bernstein-Like Dimension-Free Inequality and Regret Bounds", "abstract": "We study the regret minimization problem in the novel setting of generalized\nkernelized bandits (GKBs), where we optimize an unknown function $f^*$\nbelonging to a reproducing kernel Hilbert space (RKHS) having access to samples\ngenerated by an exponential family (EF) noise model whose mean is a non-linear\nfunction $\\mu(f^*)$. This model extends both kernelized bandits (KBs) and\ngeneralized linear bandits (GLBs). We propose an optimistic algorithm, GKB-UCB,\nand we explain why existing self-normalized concentration inequalities do not\nallow to provide tight regret guarantees. For this reason, we devise a novel\nself-normalized Bernstein-like dimension-free inequality resorting to\nFreedman's inequality and a stitching argument, which represents a contribution\nof independent interest. Based on it, we conduct a regret analysis of GKB-UCB,\nderiving a regret bound of order $\\widetilde{O}( \\gamma_T \\sqrt{T/\\kappa_*})$,\nbeing $T$ the learning horizon, ${\\gamma}_T$ the maximal information gain, and\n$\\kappa_*$ a term characterizing the magnitude the reward nonlinearity. Our\nresult matches, up to multiplicative constants and logarithmic terms, the\nstate-of-the-art bounds for both KBs and GLBs and provides a unified view of\nboth settings.", "published": "2025-08-03 09:23:19", "link": "http://arxiv.org/abs/2508.01681v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Why Heuristic Weighting Works: A Theoretical Analysis of Denoising Score Matching", "abstract": "Score matching enables the estimation of the gradient of a data distribution,\na key component in denoising diffusion models used to recover clean data from\ncorrupted inputs. In prior work, a heuristic weighting function has been used\nfor the denoising score matching loss without formal justification. In this\nwork, we demonstrate that heteroskedasticity is an inherent property of the\ndenoising score matching objective. This insight leads to a principled\nderivation of optimal weighting functions for generalized, arbitrary-order\ndenoising score matching losses, without requiring assumptions about the noise\ndistribution. Among these, the first-order formulation is especially relevant\nto diffusion models. We show that the widely used heuristical weighting\nfunction arises as a first-order Taylor approximation to the trace of the\nexpected optimal weighting. We further provide theoretical and empirical\ncomparisons, revealing that the heuristical weighting, despite its simplicity,\ncan achieve lower variance than the optimal weighting with respect to parameter\ngradients, which can facilitate more stable and efficient training.", "published": "2025-08-03 05:35:20", "link": "http://arxiv.org/abs/2508.01597v1", "categories": ["cs.LG", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Test-Time Training for Speech Enhancement", "abstract": "This paper introduces a novel application of Test-Time Training (TTT) for\nSpeech Enhancement, addressing the challenges posed by unpredictable noise\nconditions and domain shifts. This method combines a main speech enhancement\ntask with a self-supervised auxiliary task in a Y-shaped architecture. The\nmodel dynamically adapts to new domains during inference time by optimizing the\nproposed self-supervised tasks like noise-augmented signal reconstruction or\nmasked spectrogram prediction, bypassing the need for labeled data. We further\nintroduce various TTT strategies offering a trade-off between adaptation and\nefficiency. Evaluations across synthetic and real-world datasets show\nconsistent improvements across speech quality metrics, outperforming the\nbaseline model. This work highlights the effectiveness of TTT in speech\nenhancement, providing insights for future research in adaptive and robust\nspeech processing.", "published": "2025-08-03 17:02:55", "link": "http://arxiv.org/abs/2508.01847v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Spectrogram Realism in Singing Voice Synthesis via Explicit Bandwidth Extension Prior to Vocoder", "abstract": "This paper addresses the challenge of enhancing the realism of\nvocoder-generated singing voice audio by mitigating the distinguishable\ndisparities between synthetic and real-life recordings, particularly in\nhigh-frequency spectrogram components. Our proposed approach combines two\ninnovations: an explicit linear spectrogram estimation step using denoising\ndiffusion process with DiT-based neural network architecture optimized for\ntime-frequency data, and a redesigned vocoder based on Vocos specialized in\nhandling large linear spectrograms with increased frequency bins. This\nintegrated method can produce audio with high-fidelity spectrograms that are\nchallenging for both human listeners and machine classifiers to differentiate\nfrom authentic recordings. Objective and subjective evaluations demonstrate\nthat our streamlined approach maintains high audio quality while achieving this\nrealism. This work presents a substantial advancement in overcoming the\nlimitations of current vocoding techniques, particularly in the context of\nadversarial attacks on fake spectrogram detection.", "published": "2025-08-03 15:15:40", "link": "http://arxiv.org/abs/2508.01796v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Age-Agnostic System for Robust Speaker Verification", "abstract": "In speaker verification (SV), the acoustic mismatch between children's and\nadults' speech leads to suboptimal performance when adult-trained SV systems\nare applied to children's speaker verification (C-SV). While domain adaptation\ntechniques can enhance performance on C-SV tasks, they often do so at the\nexpense of significant degradation in performance on adults' SV (A-SV) tasks.\nIn this study, we propose an Age Agnostic Speaker Verification (AASV) system\nthat achieves robust performance across both C-SV and A-SV tasks. Our approach\nemploys a domain classifier to disentangle age-related attributes from speech\nand subsequently expands the embedding space using the extracted domain\ninformation, forming a unified speaker representation that is robust and highly\ndiscriminative across age groups. Experiments on the OGI and VoxCeleb datasets\ndemonstrate the effectiveness of our approach in bridging SV performance\ndisparities, laying the foundation for inclusive and age-adaptive SV systems.", "published": "2025-08-03 07:52:39", "link": "http://arxiv.org/abs/2508.01637v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Lumename: Wearable Device for Hearing Impaired with Personalized ML-Based Auditory Detection and Haptic-Visual Alerts", "abstract": "According to the World Health Organization, 430 million people experience\ndisabling hearing loss. For them, recognizing spoken commands such as one's\nname is difficult. To address this issue, Lumename, a real-time smartwatch,\nutilizes on-device machine learning to detect a user-customized name before\ngenerating a haptic-visual alert. During training, to overcome the need for\nlarge datasets, Lumename uses novel audio modulation techniques to augment\nsamples from one user and generate additional samples to represent diverse\ngenders and ages. Constrained random iterations were used to find optimal\nparameters within the model architecture. This approach resulted in a\nlow-resource and low-power TinyML model that could quickly infer various\nkeyword samples while remaining 91.67\\% accurate on a custom-built smartwatch\nbased on an Arduino Nano 33 BLE Sense.", "published": "2025-08-03 04:06:05", "link": "http://arxiv.org/abs/2508.01576v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "RIS-Aided Near-Field Channel Estimation under Mutual Coupling and Spatial Correlation", "abstract": "The integration of reconfigurable intelligent surfaces (RIS) with extremely\nlarge multiple-input multiple-output (MIMO) arrays at the base station has\nemerged as a key enabler for enhancing wireless network performance. However,\nthis setup introduces high-dimensional channel matrices, leading to increased\ncomputational complexity and pilot overhead in channel estimation. Mutual\ncoupling (MC) effects among densely packed unit cells, spatial correlation, and\nnear-field propagation conditions further complicate the estimation process.\nConventional estimators, such as linear minimum mean square error (MMSE),\nrequire channel statistics that are challenging to acquire for high-dimensional\narrays, while least squares (LS) estimators suffer from performance\nlimitations. To address these challenges, the reduced-subspace least squares\n(RS-LS) estimator leverages array geometry to enhance estimation accuracy. This\nwork advances the promising RS-LS estimation algorithm by explicitly\nincorporating MC effects into the more realistic and challenging near-field\npropagation environment within the increasingly relevant generalized RIS-aided\nMIMO framework. Additionally, we investigate the impact of MC on the spatial\ndegrees of freedom (DoF). Our analysis reveals that accounting for MC effects\nprovides a significant performance gain of approximately 5 dB at an SNR of 5\ndB, compared to conventional methods that ignore MC.", "published": "2025-08-03 16:30:21", "link": "http://arxiv.org/abs/2508.01828v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Heuristic Method for Simplified Resource Allocation based on Comparative Advantage in Wireless Access Systems", "abstract": "This paper presents a heuristic method for simplifying resource allocation in\naccess systems, leveraging the concept of comparative advantage to reduce\ncomputational complexity while maintaining near-optimal performance. Using\npower-division non-orthogonal multiple access (PD-NOMA) as an example, we\ndemonstrate how this approach mitigates the challenge of power allocation in\nmulti-cell networks. Our method reduces the search space for optimization,\nsignificantly decreasing computational overhead while ensuring efficient\nspectrum utilization. In principle, the method reduces the dimensions of search\nspace by half. Extensive analysis and simulations validate its effectiveness,\nhighlighting its potential for practical deployment in next-generation wireless\nnetworks. The proposed framework can help streamline resource allocation in\ncomplex communication environments, enhancing system performance and\nscalability.", "published": "2025-08-03 16:23:55", "link": "http://arxiv.org/abs/2508.01824v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
{"title": "Statistical Multiport-Network Modeling and Efficient Discrete Optimization of RIS", "abstract": "This Letter fills the research gap on physics-consistent optimization for\nreconfigurable intelligent surfaces (RISs) with mutual coupling (MC) and\n1-bit-tunable elements, a common hardware constraint in existing RIS\nprototypes. We compare a model-based method (temperature-annealed\nback-propagation) and model-agnostic methods (coordinate descent, genetic\nalgorithm), and evaluate potential benefits of intelligently initializing these\nmethods. To facilitate our evaluation, we introduce a technique for generating\nstatistical ensembles of multiport-network model parameters, wherein a single\nhyper-parameter adjusts the MC strength. The technique is a generalization of\nRayleigh fading to radio environments with deterministic programmability, and\nit accounts for passivity constraints as well as the coherent-backscattering\neffect. We find that, except when MC is negligible, coordinate descent with\nrandom initialization yields the most favorable trade-off in terms of\nperformance, execution time and memory usage. We expect our findings to extend\nto beyond-diagonal RIS, stacked intelligent metasurfaces, dynamic metasurface\nantennas, and wave-domain physical neural networks.", "published": "2025-08-03 14:28:23", "link": "http://arxiv.org/abs/2508.01776v1", "categories": ["eess.SP", "physics.app-ph"], "primary_category": "eess.SP"}
{"title": "FAS Enabled UAV for Energy-Efficient WPCNs", "abstract": "This letter presents an innovative scheme to enhance the communication rate\nand energy efficiency (EE) of Unmanned Aerial Vehicle (UAV) in wireless powered\ncommunication networks (WPCNs) by deploying the emerging fluid antenna system\n(FAS) technology onto the UAV. Our proposed approach leverages the dynamic port\nswitching capability of FAS, enabling the UAV to adaptively select the optimal\nantenna location that maximizes channel gain for both downlink wireless power\ntransfer (WPT) and uplink wireless data transfer (WDT). We derive both exact\nanalytical expression of the ergodic spectral rate, and asymptotic expression\nat high signal to noise ratio (SNR) regime under Nakagami-m correlated fading\nchannels. The Mont-Carlo simulation results confirms the accuracy of the\nanalytical expressions and demonstrate the substantial increase in energy\nefficiency of UAV with FAS compared to fixed antenna systems.", "published": "2025-08-03 14:11:15", "link": "http://arxiv.org/abs/2508.01771v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "ModFus-DM: Explore the Representation in Modulated Signal Diffusion Generated Models", "abstract": "Automatic modulation classification (AMC) is essential for wireless\ncommunication systems in both military and civilian applications. However,\nexisting deep learning-based AMC methods often require large labeled signals\nand struggle with non-fixed signal lengths, distribution shifts, and limited\nlabeled signals. To address these challenges, we propose a modulation-driven\nfeature fusion via diffusion model (ModFus-DM), a novel unsupervised AMC\nframework that leverages the generative capacity of diffusion models for robust\nmodulation representation learning. We design a modulated signal diffusion\ngeneration model (MSDGM) to implicitly capture structural and semantic\ninformation through a progressive denoising process. Additionally, we propose\nthe diffusion-aware feature fusion (DAFFus) module, which adaptively aggregates\nmulti-scale diffusion features to enhance discriminative representation.\nExtensive experiments on RML2016.10A, RML2016.10B, RML2018.01A and RML2022\ndatasets demonstrate that ModFus-DM significantly outperforms existing methods\nin various challenging scenarios, such as limited-label settings, distribution\nshifts, variable-length signal recognition and channel fading scenarios.\nNotably, ModFus-DM achieves over 88.27% accuracy in 24-type recognition tasks\nat SNR $\\geq $ 12dB with only 10 labeled signals per type.", "published": "2025-08-03 11:07:02", "link": "http://arxiv.org/abs/2508.01719v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Spectrum Sensing with Deep Clustering: Label-Free Radio Access Technology Recognition", "abstract": "The growth of the number of connected devices and network densification is\ndriving an increasing demand for radio network resources, particularly Radio\nFrequency (RF) spectrum. Given the dynamic and complex nature of contemporary\nwireless environments, characterized by a wide variety of devices and multiple\nRATs, spectrum sensing is envisioned to become a building component of future\n6G, including as a component within O-RAN or digital twins. However, the\ncurrent SotA research for RAT classification predominantly revolves around\nsupervised Convolutional Neural Network (CNN)-based approach that require\nextensive labeled dataset. Due to this, it is unclear how existing models\nbehave in environments for which training data is unavailable thus leaving open\nquestions regarding their generalization capabilities. In this paper, we\npropose a new spectrum sensing workflow in which the model training does not\nrequire any prior knowledge of the RATs transmitting in that area (i.e. no\nlabelled data) and the class assignment can be easily done through manual\nmapping. Furthermore, we adapt a SSL deep clustering architecture capable of\nautonomously extracting spectrum features from raw 1D Fast Fourier Transform\n(FFT) data. We evaluate the proposed architecture on three real-world datasets\nfrom three European cities, in the 868 MHz, 2.4 GHz and 5.9 GHz bands\ncontaining over 10 RATs and show that the developed model achieves superior\nperformance by up to 35 percentage points with 22% fewer trainable parameters\nand 50% less floating-point operations per second (FLOPS) compared to an SotA\nAE-based reference architecture.", "published": "2025-08-03 10:34:25", "link": "http://arxiv.org/abs/2508.01709v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Balancing Latency and Model Accuracy for Fluid Antenna-Assisted LM-Embedded MIMO Network", "abstract": "This paper addresses the challenge of large model (LM)-embedded wireless\nnetwork for handling the trade-off problem of model accuracy and network\nlatency. To guarantee a high-quality of users' service, the network latency\nshould be minimized while maintaining an acceptable inference accuracy. To meet\nthis requirement, LM quantization is proposed to reduce the latency. However,\nthe excessive quantization may destroy the accuracy of LM inference. To this\nend, a promising fluid antenna (FA) technology is investigated for enhancing\nthe transmission capacity, leading to a lower network latency in the\nLM-embedded multiple-input multiple-output (MIMO) network. To design the\nFA-assisted LM-embedded network with the lower latency and higher accuracy\nrequirements, the latency and peak signal to noise ratio (PSNR) are considered\nin the objective function. Then, an efficient optimization algorithm is\nproposed under the block coordinate descent framework. Simulation results are\nprovided to show the convergence behavior of the proposed algorithm, and the\nperformance gains from the proposed FA-assisted LMembedded network over the\nother benchmark networks in terms of network latency and PSNR.", "published": "2025-08-03 09:49:08", "link": "http://arxiv.org/abs/2508.01689v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for\nenhancing the capabilities of Large Language Models (LLMs), especially in\nknowledge-intensive tasks. Despite its advantages, current RAG methods often\nstruggle to *fully exploit knowledge during generation*. In particular, the\nsynergy between the model's internal parametric knowledge and external\nretrieved knowledge remains limited. Retrieved contents may sometimes mislead\ngeneration, while certain generated content can guide the model toward more\naccurate outputs. In this work, we propose Collaborative Chain-of-Agents, a\nframework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior\nperformance on open-domain and multi-hop QA tasks.", "published": "2025-08-03 10:00:38", "link": "http://arxiv.org/abs/2508.01696v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stochastic Encodings for Active Feature Acquisition", "abstract": "Active Feature Acquisition is an instance-wise, sequential decision making\nproblem. The aim is to dynamically select which feature to measure based on\ncurrent observations, independently for each test instance. Common approaches\neither use Reinforcement Learning, which experiences training difficulties, or\ngreedily maximize the conditional mutual information of the label and\nunobserved features, which makes myopic acquisitions. To address these\nshortcomings, we introduce a latent variable model, trained in a supervised\nmanner. Acquisitions are made by reasoning about the features across many\npossible unobserved realizations in a stochastic latent space. Extensive\nevaluation on a large range of synthetic and real datasets demonstrates that\nour approach reliably outperforms a diverse set of baselines.", "published": "2025-08-03 23:48:46", "link": "http://arxiv.org/abs/2508.01957v2", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Machine Learning-Driven Performance Analysis of Compressed Communication in Aerial-RIS Networks for Future 6G Networks", "abstract": "In the future 6G and wireless networks, particularly in dense urban\nenvironments, bandwidth exhaustion and limited capacity pose significant\nchallenges to enhancing data rates. We introduce a novel system model designed\nto improve the data rate of users in next-generation multi-cell networks by\nintegrating Unmanned Aerial Vehicle (UAV)-Assisted Reconfigurable Intelligent\nSurfaces (RIS), Non-Orthogonal Multiple Access (NOMA), and Coordinated\nMultipoint Transmission (CoMP). Optimally deploying Aerial RIS for higher data\nrates, employing NOMA to improve spectral efficiency, and utilizing CoMP to\nmitigate inter-cell interference (ICI), we significantly enhance the overall\nsystem capacity and sum rate. Furthermore, we address the challenge of feedback\noverhead associated with Quantized Phase Shifts (QPS) from the receiver to RIS.\nThe feedback channel is band-limited and cannot support a large overhead of QPS\nfor uplink communication. To ensure seamless transmission, we propose a Machine\nLearning Autoencoder technique for a compressed communication of QPS from the\nreceiver to RIS, while maintaining high accuracy. Additionally, we investigate\nthe impact of the number of Aerial RIS elements and power allocation ratio for\nNOMA on the individual data rate of users. Our simulation results demonstrate\nsubstantial improvements in spectral efficiency, outage probability, and\nbandwidth utilization, highlighting the potential of the proposed architecture\nto enhance network performance.", "published": "2025-08-03 20:20:33", "link": "http://arxiv.org/abs/2508.01911v1", "categories": ["cs.DC", "cs.IT", "cs.NI", "math.IT"], "primary_category": "cs.DC"}
{"title": "Frequency Point Game Environment for UAVs via Expert Knowledge and Large Language Model", "abstract": "Unmanned Aerial Vehicles (UAVs) have made significant advancements in\ncommunication stability and security through techniques such as frequency\nhopping, signal spreading, and adaptive interference suppression. However,\nchallenges remain in modeling spectrum competition, integrating expert\nknowledge, and predicting opponent behavior. To address these issues, we\npropose UAV-FPG (Unmanned Aerial Vehicle - Frequency Point Game), a\ngame-theoretic environment model that simulates the dynamic interaction between\ninterference and anti-interference strategies of opponent and ally UAVs in\ncommunication frequency bands. The model incorporates a prior expert knowledge\nbase to optimize frequency selection and employs large language models for path\nplanning, simulating a \"strong adversary\". Experimental results highlight the\neffectiveness of integrating the expert knowledge base and the large language\nmodel, with the latter significantly improving path planning in dynamic\nscenarios through iterative interactions, outperforming fixed-path strategies.\nUAV-FPG provides a robust platform for advancing anti-jamming strategies and\nintelligent decision-making in UAV communication systems.", "published": "2025-08-03 16:39:34", "link": "http://arxiv.org/abs/2508.02757v1", "categories": ["cs.MA", "cs.GT", "cs.RO"], "primary_category": "cs.MA"}
{"title": "CTBench: Cryptocurrency Time Series Generation Benchmark", "abstract": "Synthetic time series are essential tools for data augmentation, stress\ntesting, and algorithmic prototyping in quantitative finance. However, in\ncryptocurrency markets, characterized by 24/7 trading, extreme volatility, and\nrapid regime shifts, existing Time Series Generation (TSG) methods and\nbenchmarks often fall short, jeopardizing practical utility. Most prior work\n(1) targets non-financial or traditional financial domains, (2) focuses\nnarrowly on classification and forecasting while neglecting crypto-specific\ncomplexities, and (3) lacks critical financial evaluations, particularly for\ntrading applications. To address these gaps, we introduce \\textsf{CTBench}, the\nfirst comprehensive TSG benchmark tailored for the cryptocurrency domain.\n\\textsf{CTBench} curates an open-source dataset from 452 tokens and evaluates\nTSG models across 13 metrics spanning 5 key dimensions: forecasting accuracy,\nrank fidelity, trading performance, risk assessment, and computational\nefficiency. A key innovation is a dual-task evaluation framework: (1) the\n\\emph{Predictive Utility} task measures how well synthetic data preserves\ntemporal and cross-sectional patterns for forecasting, while (2) the\n\\emph{Statistical Arbitrage} task assesses whether reconstructed series support\nmean-reverting signals for trading. We benchmark eight representative models\nfrom five methodological families over four distinct market regimes, uncovering\ntrade-offs between statistical fidelity and real-world profitability. Notably,\n\\textsf{CTBench} offers model ranking analysis and actionable guidance for\nselecting and deploying TSG models in crypto analytics and strategy\ndevelopment.", "published": "2025-08-03 17:07:08", "link": "http://arxiv.org/abs/2508.02758v1", "categories": ["q-fin.ST", "cs.AI", "cs.CE", "cs.DB", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Hedging with memory: shallow and deep learning with signatures", "abstract": "We investigate the use of path signatures in a machine learning context for\nhedging exotic derivatives under non-Markovian stochastic volatility models. In\na deep learning setting, we use signatures as features in feedforward neural\nnetworks and show that they outperform LSTMs in most cases, with orders of\nmagnitude less training compute. In a shallow learning setting, we compare two\nregression approaches: the first directly learns the hedging strategy from the\nexpected signature of the price process; the second models the dynamics of\nvolatility using a signature volatility model, calibrated on the expected\nsignature of the volatility. Solving the hedging problem in the calibrated\nsignature volatility model yields more accurate and stable results across\ndifferent payoffs and volatility dynamics.", "published": "2025-08-03 17:20:49", "link": "http://arxiv.org/abs/2508.02759v1", "categories": ["stat.ML", "cs.LG", "60L10, 91G20, 91G60"], "primary_category": "stat.ML"}
{"title": "Non-Verbal Vocalisations and their Challenges: Emotion, Privacy, Sparseness, and Real Life", "abstract": "Non-Verbal Vocalisations (NVVs) are short `non-word' utterances without\nproper linguistic (semantic) meaning but conveying connotations -- be this\nemotions/affects or other paralinguistic information. We start this\ncontribution with a historic sketch: how they were addressed in psychology and\nlinguistics in the last two centuries, how they were neglected later on, and\nhow they came to the fore with the advent of emotion research. We then give an\noverview of types of NVVs (formal aspects) and functions of NVVs, exemplified\nwith the typical NVV \\textit{ah}. Interesting as they are, NVVs come, however,\nwith a bunch of challenges that should be accounted for: Privacy and general\nethical considerations prevent them of being recorded in real-life (private)\nscenarios to a sufficient extent. Isolated, prompted (acted) exemplars do not\nnecessarily model NVVs in context; yet, this is the preferred strategy so far\nwhen modelling NVVs, especially in AI. To overcome these problems, we argue in\nfavour of corpus-based approaches. This guarantees a more realistic modelling;\nhowever, we are still faced with privacy and sparse data problems.", "published": "2025-08-03 23:59:43", "link": "http://arxiv.org/abs/2508.01960v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "68T10 (Primary) 68T45 (Secondary)", "I.2.7"], "primary_category": "cs.SD"}
{"title": "EgoTrigger: Toward Audio-Driven Image Capture for Human Memory Enhancement in All-Day Energy-Efficient Smart Glasses", "abstract": "All-day smart glasses are likely to emerge as platforms capable of continuous\ncontextual sensing, uniquely positioning them for unprecedented assistance in\nour daily lives. Integrating the multi-modal AI agents required for human\nmemory enhancement while performing continuous sensing, however, presents a\nmajor energy efficiency challenge for all-day usage. Achieving this balance\nrequires intelligent, context-aware sensor management. Our approach,\nEgoTrigger, leverages audio cues from the microphone to selectively activate\npower-intensive cameras, enabling efficient sensing while preserving\nsubstantial utility for human memory enhancement. EgoTrigger uses a lightweight\naudio model (YAMNet) and a custom classification head to trigger image capture\nfrom hand-object interaction (HOI) audio cues, such as the sound of a drawer\nopening or a medication bottle being opened. In addition to evaluating on the\nQA-Ego4D dataset, we introduce and evaluate on the Human Memory Enhancement\nQuestion-Answer (HME-QA) dataset. Our dataset contains 340 human-annotated\nfirst-person QA pairs from full-length Ego4D videos that were curated to ensure\nthat they contained audio, focusing on HOI moments critical for contextual\nunderstanding and memory. Our results show EgoTrigger can use 54% fewer frames\non average, significantly saving energy in both power-hungry sensing components\n(e.g., cameras) and downstream operations (e.g., wireless transmission), while\nachieving comparable performance on datasets for an episodic memory task. We\nbelieve this context-aware triggering strategy represents a promising direction\nfor enabling energy-efficient, functional smart glasses capable of all-day use\n-- supporting applications like helping users recall where they placed their\nkeys or information about their routine activities (e.g., taking medications).", "published": "2025-08-03 20:51:23", "link": "http://arxiv.org/abs/2508.01915v1", "categories": ["cs.CV", "cs.ET", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Generalizable Audio Deepfake Detection via Hierarchical Structure Learning and Feature Whitening in Poincar\u00e9 sphere", "abstract": "Audio deepfake detection (ADD) faces critical generalization challenges due\nto diverse real-world spoofing attacks and domain variations. However, existing\nmethods primarily rely on Euclidean distances, failing to adequately capture\nthe intrinsic hierarchical structures associated with attack categories and\ndomain factors. To address these issues, we design a novel framework\nPoin-HierNet to construct domain-invariant hierarchical representations in the\nPoincar\\'e sphere. Poin-HierNet includes three key components: 1) Poincar\\'e\nPrototype Learning (PPL) with several data prototypes aligning sample features\nand capturing multilevel hierarchies beyond human labels; 2) Hierarchical\nStructure Learning (HSL) leverages top prototypes to establish a tree-like\nhierarchical structure from data prototypes; and 3) Poincar\\'e Feature\nWhitening (PFW) enhances domain invariance by applying feature whitening to\nsuppress domain-sensitive features. We evaluate our approach on four datasets:\nASVspoof 2019 LA, ASVspoof 2021 LA, ASVspoof 2021 DF, and In-The-Wild.\nExperimental results demonstrate that Poin-HierNet exceeds state-of-the-art\nmethods in Equal Error Rate.", "published": "2025-08-03 19:16:29", "link": "http://arxiv.org/abs/2508.01897v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sonify Anything: Towards Context-Aware Sonic Interactions in AR", "abstract": "In Augmented Reality (AR), virtual objects interact with real objects.\nHowever, the lack of physicality of virtual objects leads to the absence of\nnatural sonic interactions. When virtual and real objects collide, either no\nsound or a generic sound is played. Both lead to an incongruent multisensory\nexperience, reducing interaction and object realism. Unlike in Virtual Reality\n(VR) and games, where predefined scenes and interactions allow for the playback\nof pre-recorded sound samples, AR requires real-time sound synthesis that\ndynamically adapts to novel contexts and objects to provide audiovisual\ncongruence during interaction. To enhance real-virtual object interactions in\nAR, we propose a framework for context-aware sounds using methods from computer\nvision to recognize and segment the materials of real objects. The material's\nphysical properties and the impact dynamics of the interaction are used to\ngenerate material-based sounds in real-time using physical modelling synthesis.\nIn a user study with 24 participants, we compared our congruent material-based\nsounds to a generic sound effect, mirroring the current standard of\nnon-context-aware sounds in AR applications. The results showed that\nmaterial-based sounds led to significantly more realistic sonic interactions.\nMaterial-based sounds also enabled participants to distinguish visually similar\nmaterials with significantly greater accuracy and confidence. These findings\nshow that context-aware, material-based sonic interactions in AR foster a\nstronger sense of realism and enhance our perception of real-world\nsurroundings.", "published": "2025-08-03 14:56:56", "link": "http://arxiv.org/abs/2508.01789v1", "categories": ["cs.HC", "cs.CV", "cs.SD", "eess.AS", "H.5.5; H.5.2; H.5.1; I.3.5"], "primary_category": "cs.HC"}
{"title": "From Contrast to Commonality: Audio Commonality Captioning for Enhanced Audio-Text Cross-modal Understanding in Multimodal LLMs", "abstract": "Audio Captioning (AC) plays a pivotal role in enhancing audio-text\ncross-modal understanding during the pretraining and finetuning of multimodal\nlarge language models (MLLMs). To further strengthen this alignment, recent\nworks have proposed Audio Difference Captioning (ADC), which takes multiple\naudio inputs and encourages the model to describe their differences, thereby\npromoting fine-grained audio discrimination. However, despite its effectiveness\nin enabling difference-telling and detailed discrimination, ADC introduces a\nnotable semantic gap between the input audios-often rich in diverse sound\nevents-and the relatively brief, difference-focused output captions. This\ndeviation from AC-style descriptions leads to a mismatch with the pretraining\nobjective, resulting in catastrophic forgetting during finetuning. To mitigate\nthis issue, we propose Audio Commonality Captioning (ACC), a comparably\nchallenging but gentler alternative that encourages the model to capture the\nshared semantics across audio clips rather than emphasizing their detailed\ndifferences. Experimental results demonstrate that ACC not only effectively\nenhances audio-text understanding on primary captioning benchmarks but also\nbetter preserves general capabilities across diverse speech and music-related\ndownstream tasks, such as vocal sound classification (VSC), speech emotion\nrecognition (SER), musical instrument classification (MIC), and music genre\nclassification (MGC), compared to ADC. These findings validate that ACC\ncontributes to more robust cross-modal understanding and achieves a better\nbalance between generalization and task-specific performance in the context of\nMLLMs.", "published": "2025-08-03 08:32:42", "link": "http://arxiv.org/abs/2508.01659v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition", "abstract": "Multimodal emotion recognition (MER) aims to identify emotional states by\nintegrating and analyzing information from multiple modalities. However,\ninherent modality heterogeneity and inconsistencies in emotional cues remain\nkey challenges that hinder performance. To address these issues, we propose a\nDecoupled Representations with Knowledge Fusion (DRKF) method for MER. DRKF\nconsists of two main modules: an Optimized Representation Learning (ORL) Module\nand a Knowledge Fusion (KF) Module. ORL employs a contrastive mutual\ninformation estimation method with progressive modality augmentation to\ndecouple task-relevant shared representations and modality-specific features\nwhile mitigating modality heterogeneity. KF includes a lightweight\nself-attention-based Fusion Encoder (FE) that identifies the dominant modality\nand integrates emotional information from other modalities to enhance the fused\nrepresentation. To handle potential errors from incorrect dominant modality\nselection under emotionally inconsistent conditions, we introduce an Emotion\nDiscrimination Submodule (ED), which enforces the fused representation to\nretain discriminative cues of emotional inconsistency. This ensures that even\nif the FE selects an inappropriate dominant modality, the Emotion\nClassification Submodule (EC) can still make accurate predictions by leveraging\npreserved inconsistency information. Experiments show that DRKF achieves\nstate-of-the-art (SOTA) performance on IEMOCAP, MELD, and M3ED. The source code\nis publicly available at https://github.com/PANPANKK/DRKF.", "published": "2025-08-03 08:05:57", "link": "http://arxiv.org/abs/2508.01644v1", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Automatic Melody Reduction via Shortest Path Finding", "abstract": "Melody reduction, as an abstract representation of musical compositions,\nserves not only as a tool for music analysis but also as an intermediate\nrepresentation for structured music generation. Prior computational theories,\nsuch as the Generative Theory of Tonal Music, provide insightful\ninterpretations of music, but they are not fully automatic and usually limited\nto the classical genre. In this paper, we propose a novel and conceptually\nsimple computational method for melody reduction using a graph-based\nrepresentation inspired by principles from computational music theories, where\nthe reduction process is formulated as finding the shortest path. We evaluate\nour algorithm on pop, folk, and classical genres, and experimental results show\nthat the algorithm produces melody reductions that are more faithful to the\noriginal melody and more musically coherent than other common melody\ndownsampling methods. As a downstream task, we use melody reductions to\ngenerate symbolic music variations. Experiments show that our method achieves\nhigher quality than state-of-the-art style transfer methods.", "published": "2025-08-03 03:42:08", "link": "http://arxiv.org/abs/2508.01571v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Stochastic Encodings for Active Feature Acquisition", "abstract": "Active Feature Acquisition is an instance-wise, sequential decision making\nproblem. The aim is to dynamically select which feature to measure based on\ncurrent observations, independently for each test instance. Common approaches\neither use Reinforcement Learning, which experiences training difficulties, or\ngreedily maximize the conditional mutual information of the label and\nunobserved features, which makes myopic acquisitions. To address these\nshortcomings, we introduce a latent variable model, trained in a supervised\nmanner. Acquisitions are made by reasoning about the features across many\npossible unobserved realizations in a stochastic latent space. Extensive\nevaluation on a large range of synthetic and real datasets demonstrates that\nour approach reliably outperforms a diverse set of baselines.", "published": "2025-08-03 23:48:46", "link": "http://arxiv.org/abs/2508.01957v3", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Efficient optimization of expensive black-box simulators via marginal means, with application to neutrino detector design", "abstract": "With advances in scientific computing, computer experiments are increasingly\nused for optimizing complex systems. However, for modern applications, e.g.,\nthe optimization of nuclear physics detectors, each experiment run can require\nhundreds of CPU hours, making the optimization of its black-box simulator over\na high-dimensional space a challenging task. Given limited runs at inputs\n$\\mathbf{x}_1, \\cdots, \\mathbf{x}_n$, the best solution from these evaluated\ninputs can be far from optimal, particularly as dimensionality increases.\nExisting black-box methods, however, largely employ this ''pick-the-winner''\n(PW) solution, which leads to mediocre optimization performance. To address\nthis, we propose a new Black-box Optimization via Marginal Means (BOMM)\napproach. The key idea is a new estimator of a global optimizer $\\mathbf{x}^*$\nthat leverages the so-called marginal mean functions, which can be efficiently\ninferred with limited runs in high dimensions. Unlike PW, this estimator can\nselect solutions beyond evaluated inputs for improved optimization performance.\nAssuming the objective function follows a generalized additive model with\nunknown link function and under mild conditions, we prove that the BOMM\nestimator not only is consistent for optimization, but also has an optimization\nrate that tempers the ''curse-of-dimensionality'' faced by existing methods,\nthus enabling better performance as dimensionality increases. We present a\npractical framework for implementing BOMM using the transformed additive\nGaussian process surrogate model. Finally, we demonstrate the effectiveness of\nBOMM in numerical experiments and an application on neutrino detector\noptimization in nuclear physics.", "published": "2025-08-03 16:44:05", "link": "http://arxiv.org/abs/2508.01834v2", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Frequency Point Game Environment for UAVs via Expert Knowledge and Large Language Model", "abstract": "Unmanned Aerial Vehicles (UAVs) have made significant advancements in\ncommunication stability and security through techniques such as frequency\nhopping, signal spreading, and adaptive interference suppression. However,\nchallenges remain in modeling spectrum competition, integrating expert\nknowledge, and predicting opponent behavior. To address these issues, we\npropose UAV-FPG (Unmanned Aerial Vehicle - Frequency Point Game), a\ngame-theoretic environment model that simulates the dynamic interaction between\ninterference and anti-interference strategies of opponent and ally UAVs in\ncommunication frequency bands. The model incorporates a prior expert knowledge\nbase to optimize frequency selection and employs large language models for path\nplanning, simulating a \"strong adversary\". Experimental results highlight the\neffectiveness of integrating the expert knowledge base and the large language\nmodel, with the latter significantly improving path planning in dynamic\nscenarios through iterative interactions, outperforming fixed-path strategies.\nUAV-FPG provides a robust platform for advancing anti-jamming strategies and\nintelligent decision-making in UAV communication systems.", "published": "2025-08-03 16:39:34", "link": "http://arxiv.org/abs/2508.02757v2", "categories": ["cs.MA", "cs.GT", "cs.RO"], "primary_category": "cs.MA"}
{"title": "Tokenize Everything, But Can You Sell It? RWA Liquidity Challenges and the Road Ahead", "abstract": "The tokenization of real-world assets (RWAs) promises to transform financial\nmarkets by enabling fractional ownership, global accessibility, and\nprogrammable settlement of traditionally illiquid assets such as real estate,\nprivate credit, and government bonds. While technical progress has been rapid,\nwith over \\$25 billion in tokenized RWAs brought on-chain as of 2025, liquidity\nremains a critical bottleneck. This paper investigates the gap between\ntokenization and tradability, drawing on recent academic research and market\ndata from platforms such as RWA.xyz. We document that most RWA tokens exhibit\nlow trading volumes, long holding periods, and limited investor participation,\ndespite their potential for 24/7 global markets. Through case studies of\ntokenized real estate, private credit, and tokenized treasury funds, we present\nempirical liquidity observations that reveal low transfer activity, limited\nactive address counts, and minimal secondary trading for most tokenized asset\nclasses. Next, we categorize the structural barriers to liquidity, including\nregulatory gating, custodial concentration, whitelisting, valuation opacity,\nand lack of decentralized trading venues. Finally, we propose actionable\npathways to improve liquidity, ranging from hybrid market structures and\ncollateral-based liquidity to transparency enhancements and compliance\ninnovation. Our findings contribute to the growing discourse on digital asset\nmarket microstructure and highlight that realizing the liquidity potential of\nRWAs requires coordinated progress across legal, technical, and institutional\ndomains.", "published": "2025-08-03 10:30:15", "link": "http://arxiv.org/abs/2508.11651v1", "categories": ["q-fin.GN", "cs.CR", "q-fin.CP"], "primary_category": "q-fin.GN"}
{"title": "Time-Varying Factor-Augmented Models for Volatility Forecasting", "abstract": "Accurate volatility forecasts are vital in modern finance for risk\nmanagement, portfolio allocation, and strategic decision-making. However,\nexisting methods face key limitations. Fully multivariate models, while\ncomprehensive, are computationally infeasible for realistic portfolios. Factor\nmodels, though efficient, primarily use static factor loadings, failing to\ncapture evolving volatility co-movements when they are most critical. To\naddress these limitations, we propose a novel, model-agnostic Factor-Augmented\nVolatility Forecast framework. Our approach employs a time-varying factor model\nto extract a compact set of dynamic, cross-sectional factors from realized\nvolatilities with minimal computational cost. These factors are then integrated\ninto both statistical and AI-based forecasting models, enabling a unified\nsystem that jointly models asset-specific dynamics and evolving market-wide\nco-movements. Our framework demonstrates strong performance across two\nprominent asset classes-large-cap U.S. technology equities and major\ncryptocurrencies-over both short-term (1-day) and medium-term (7-day) horizons.\nUsing a suite of linear and non-linear AI-driven models, we consistently\nobserve substantial improvements in predictive accuracy and economic value.\nNotably, a practical pairs-trading strategy built on our forecasts delivers\nsuperior risk-adjusted returns and profitability, particularly under adverse\nmarket conditions.", "published": "2025-08-03 18:23:34", "link": "http://arxiv.org/abs/2508.01880v2", "categories": ["q-fin.ST", "q-fin.MF"], "primary_category": "q-fin.ST"}
{"title": "Time-Varying Factor-Augmented Models for Volatility Forecasting", "abstract": "Accurate volatility forecasts are vital in modern finance for risk\nmanagement, portfolio allocation, and strategic decision-making. However,\nexisting methods face key limitations. Fully multivariate models, while\ncomprehensive, are computationally infeasible for realistic portfolios. Factor\nmodels, though efficient, primarily use static factor loadings, failing to\ncapture evolving volatility co-movements when they are most critical. To\naddress these limitations, we propose a novel, model-agnostic Factor-Augmented\nVolatility Forecast framework. Our approach employs a time-varying factor model\nto extract a compact set of dynamic, cross-sectional factors from realized\nvolatilities with minimal computational cost. These factors are then integrated\ninto both statistical and AI-based forecasting models, enabling a unified\nsystem that jointly models asset-specific dynamics and evolving market-wide\nco-movements. Our framework demonstrates strong performance across two\nprominent asset classes-large-cap U.S. technology equities and major\ncryptocurrencies-over both short-term (1-day) and medium-term (7-day) horizons.\nUsing a suite of linear and non-linear AI-driven models, we consistently\nobserve substantial improvements in predictive accuracy and economic value.\nNotably, a practical pairs-trading strategy built on our forecasts delivers\nsuperior risk-adjusted returns and profitability, particularly under adverse\nmarket conditions.", "published": "2025-08-03 18:23:34", "link": "http://arxiv.org/abs/2508.01880v3", "categories": ["q-fin.ST", "q-fin.MF"], "primary_category": "q-fin.ST"}
