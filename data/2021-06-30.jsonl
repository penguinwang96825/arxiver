{"title": "On Systematic Style Differences between Unsupervised and Supervised MT\n  and an Application for High-Resource Machine Translation", "abstract": "Modern unsupervised machine translation (MT) systems reach reasonable\ntranslation quality under clean and controlled data conditions. As the\nperformance gap between supervised and unsupervised MT narrows, it is\ninteresting to ask whether the different training methods result in\nsystematically different output beyond what is visible via quality metrics like\nadequacy or BLEU. We compare translations from supervised and unsupervised MT\nsystems of similar quality, finding that unsupervised output is more fluent and\nmore structurally different in comparison to human translation than is\nsupervised MT. We then demonstrate a way to combine the benefits of both\nmethods into a single system which results in improved adequacy and fluency as\nrated by human evaluators. Our results open the door to interesting discussions\nabout how supervised and unsupervised MT might be different yet\nmutually-beneficial.", "published": "2021-06-30 05:44:05", "link": "http://arxiv.org/abs/2106.15818v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in\n  Authorship Verification", "abstract": "The PAN 2021 authorship verification (AV) challenge is part of a three-year\nstrategy, moving from a cross-topic/closed-set AV task to a\ncross-topic/open-set AV task over a collection of fanfiction texts. In this\nwork, we present a novel hybrid neural-probabilistic framework that is designed\nto tackle the challenges of the 2021 task. Our system is based on our 2020\nwinning submission, with updates to significantly reduce sensitivities to\ntopical variations and to further improve the system's calibration by means of\nan uncertainty-adaptation layer. Our framework additionally includes an\nout-of-distribution detector (O2D2) for defining non-responses. Our proposed\nsystem outperformed all other systems that participated in the PAN 2021 AV\ntask.", "published": "2021-06-30 06:10:43", "link": "http://arxiv.org/abs/2106.15825v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction", "abstract": "Text-to-Graph extraction aims to automatically extract information graphs\nconsisting of mentions and types from natural language texts. Existing\napproaches, such as table filling and pairwise scoring, have shown impressive\nperformance on various information extraction tasks, but they are difficult to\nscale to datasets with longer input texts because of their second-order\nspace/time complexities with respect to the input length. In this work, we\npropose a Hybrid Span Generator (HySPA) that invertibly maps the information\ngraph to an alternating sequence of nodes and edge types, and directly\ngenerates such sequences via a hybrid span decoder which can decode both the\nspans and the types recurrently in linear time and space complexities.\nExtensive experiments on the ACE05 dataset show that our approach also\nsignificantly outperforms state-of-the-art on the joint entity and relation\nextraction task.", "published": "2021-06-30 06:44:22", "link": "http://arxiv.org/abs/2106.15838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Genre determining prediction: Non-standard TAM marking in football\n  language", "abstract": "German and French football language display tense-aspect-mood (TAM) forms\nwhich differ from the TAM use in other genres. In German football talk, the\npresent indicative may replace the pluperfect subjunctive. In French reports of\nfootball matches, the imperfective past may occur instead of a perfective past\ntense-aspect form. We argue that the two phenomena share a functional core and\nare licensed in the same way, which is a direct result of the genre they occur\nin. More precisely, football match reports adhere to a precise script and\nspecific events are temporally determined in terms of objective time. This\nallows speakers to exploit a secondary function of TAM forms, namely, they\nshift the temporal perspective. We argue that it is on the grounds of the genre\nthat comprehenders predict the deviating forms and are also able to decode\nthem. We present various corpus studies where we explore the functioning of\nthese phenomena in order to gain insights into their distribution,\ngrammaticalization and their functioning in discourse. Relevant factors are\nAktionsart properties, rhetorical relations and their interaction with other\nTAM forms. This allows us to discuss coping mechanisms on the part of the\ncomprehender. We broaden our understanding of the phenomena, which have only\nbeen partly covered for French and up to now seem to have been ignored in\nGerman.", "published": "2021-06-30 08:01:57", "link": "http://arxiv.org/abs/2106.15872v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixed Cross Entropy Loss for Neural Machine Translation", "abstract": "In neural machine translation, cross entropy (CE) is the standard loss\nfunction in two training methods of auto-regressive models, i.e., teacher\nforcing and scheduled sampling. In this paper, we propose mixed cross entropy\nloss (mixed CE) as a substitute for CE in both training approaches. In teacher\nforcing, the model trained with CE regards the translation problem as a\none-to-one mapping process, while in mixed CE this process can be relaxed to\none-to-many. In scheduled sampling, we show that mixed CE has the potential to\nencourage the training and testing behaviours to be similar to each other, more\neffectively mitigating the exposure bias problem. We demonstrate the\nsuperiority of mixed CE over CE on several machine translation datasets, WMT'16\nRo-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled\nsampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE\nconsistently outperforms CE on a multi-reference set as well as a challenging\nparaphrased reference set. We also found the model trained with mixed CE is\nable to provide a better probability distribution defined over the translation\noutput space. Our code is available at https://github.com/haorannlp/mix.", "published": "2021-06-30 08:15:05", "link": "http://arxiv.org/abs/2106.15880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Thematic Coherence in Microblogs", "abstract": "Collecting together microblogs representing opinions about the same topics\nwithin the same timeframe is useful to a number of different tasks and\npractitioners. A major question is how to evaluate the quality of such thematic\nclusters. Here we create a corpus of microblog clusters from three different\ndomains and time windows and define the task of evaluating thematic coherence.\nWe provide annotation guidelines and human annotations of thematic coherence by\njournalist experts. We subsequently investigate the efficacy of different\nautomated evaluation metrics for the task. We consider a range of metrics\nincluding surface level metrics, ones for topic model coherence and text\ngeneration metrics (TGMs). While surface level metrics perform well,\noutperforming topic coherence metrics, they are not as consistent as TGMs. TGMs\nare more reliable than all other metrics considered for capturing thematic\ncoherence in microblog clusters due to being less sensitive to the effect of\ntime windows.", "published": "2021-06-30 10:32:59", "link": "http://arxiv.org/abs/2106.15971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual alignments of ELMo contextual embeddings", "abstract": "Building machine learning prediction models for a specific NLP task requires\nsufficient training data, which can be difficult to obtain for less-resourced\nlanguages. Cross-lingual embeddings map word embeddings from a less-resourced\nlanguage to a resource-rich language so that a prediction model trained on data\nfrom the resource-rich language can also be used in the less-resourced\nlanguage. To produce cross-lingual mappings of recent contextual embeddings,\nanchor points between the embedding spaces have to be words in the same\ncontext. We address this issue with a novel method for creating cross-lingual\ncontextual alignment datasets. Based on that, we propose several cross-lingual\nmapping methods for ELMo embeddings. The proposed linear mapping methods use\nexisting Vecmap and MUSE alignments on contextual ELMo embeddings. Novel\nnonlinear ELMoGAN mapping methods are based on GANs and do not assume\nisomorphic embedding spaces. We evaluate the proposed mapping methods on nine\nlanguages, using four downstream tasks: named entity recognition (NER),\ndependency parsing (DP), terminology alignment, and sentiment analysis. The\nELMoGAN methods perform very well on the NER and terminology alignment tasks,\nwith a lower cross-lingual loss for NER compared to the direct training on some\nlanguages. In DP and sentiment analysis, linear contextual alignment variants\nare more successful.", "published": "2021-06-30 11:26:43", "link": "http://arxiv.org/abs/2106.15986v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Estimation of Base Models' Weights in Ensemble of Machine\n  Reading Comprehension Systems for Robust Generalization", "abstract": "One of the main challenges of the machine reading comprehension (MRC) models\nis their fragile out-of-domain generalization, which makes these models not\nproperly applicable to real-world general-purpose question answering problems.\nIn this paper, we leverage a zero-shot weighted ensemble method for improving\nthe robustness of out-of-domain generalization in MRC models. In the proposed\nmethod, a weight estimation module is used to estimate out-of-domain weights,\nand an ensemble module aggregate several base models' predictions based on\ntheir weights. The experiments indicate that the proposed method not only\nimproves the final accuracy, but also is robust against domain changes.", "published": "2021-06-30 12:22:30", "link": "http://arxiv.org/abs/2106.16013v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoLAW: Augmented Legal Reasoning through Legal Precedent Prediction", "abstract": "This paper demonstrate how NLP can be used to address an unmet need of the\nlegal community and increase access to justice. The paper introduces Legal\nPrecedent Prediction (LPP), the task of predicting relevant passages from\nprecedential court decisions given the context of a legal argument. To this\nend, the paper showcases a BERT model, trained on 530,000 examples of legal\narguments made by U.S. federal judges, to predict relevant passages from\nprecedential court decisions given the context of a legal argument. In 96% of\nunseen test examples the correct target passage is among the top-10 predicted\npassages. The same model is able to predict relevant precedent given a short\nsummary of a complex and unseen legal brief, predicting the precedent that was\nactually cited by the brief's co-author, former U.S. Solicitor General and\ncurrent U.S. Supreme Court Justice Elena Kagan.", "published": "2021-06-30 13:01:33", "link": "http://arxiv.org/abs/2106.16034v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin\n  Information", "abstract": "Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.", "published": "2021-06-30 13:06:00", "link": "http://arxiv.org/abs/2106.16038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA", "abstract": "In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.", "published": "2021-06-30 15:45:07", "link": "http://arxiv.org/abs/2106.16138v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The MultiBERTs: BERT Reproductions for Robustness Analysis", "abstract": "Experiments with pre-trained models such as BERT are often based on a single\ncheckpoint. While the conclusions drawn apply to the artifact tested in the\nexperiment (i.e., the particular instance of the model), it is not always clear\nwhether they hold for the more general procedure which includes the\narchitecture, training data, initialization scheme, and loss function. Recent\nwork has shown that repeating the pre-training process can lead to\nsubstantially different performance, suggesting that an alternate strategy is\nneeded to make principled statements about procedures. To enable researchers to\ndraw more robust conclusions, we introduce the MultiBERTs, a set of 25\nBERT-Base checkpoints, trained with similar hyper-parameters as the original\nBERT model but differing in random weight initialization and shuffling of\ntraining data. We also define the Multi-Bootstrap, a non-parametric bootstrap\nmethod for statistical inference designed for settings where there are multiple\npre-trained models and limited test data. To illustrate our approach, we\npresent a case study of gender bias in coreference resolution, in which the\nMulti-Bootstrap lets us measure effects that may not be detected with a single\ncheckpoint. We release our models and statistical library along with an\nadditional set of 140 intermediate checkpoints captured during pre-training to\nfacilitate research on learning dynamics.", "published": "2021-06-30 15:56:44", "link": "http://arxiv.org/abs/2106.16163v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer", "abstract": "Despite their success, large pre-trained multilingual models have not\ncompletely alleviated the need for labeled data, which is cumbersome to collect\nfor all target languages. Zero-shot cross-lingual transfer is emerging as a\npractical solution: pre-trained models later fine-tuned on one transfer\nlanguage exhibit surprising performance when tested on many target languages.\nEnglish is the dominant source language for transfer, as reinforced by popular\nzero-shot benchmarks. However, this default choice has not been systematically\nvetted. In our study, we compare English against other transfer languages for\nfine-tuning, on two pre-trained multilingual models (mBERT and mT5) and\nmultiple classification and question answering tasks. We find that other\nhigh-resource languages such as German and Russian often transfer more\neffectively, especially when the set of target languages is diverse or unknown\na priori. Unexpectedly, this can be true even when the training sets were\nautomatically translated from English. This finding can have immediate impact\non multilingual zero-shot systems, and should inform future benchmark designs.", "published": "2021-06-30 16:05:57", "link": "http://arxiv.org/abs/2106.16171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Early Risk Detection of Pathological Gambling, Self-Harm and Depression\n  Using BERT", "abstract": "Early risk detection of mental illnesses has a massive positive impact upon\nthe well-being of people. The eRisk workshop has been at the forefront of\nenabling interdisciplinary research in developing computational methods to\nautomatically estimate early risk factors for mental issues such as depression,\nself-harm, anorexia and pathological gambling. In this paper, we present the\ncontributions of the BLUE team in the 2021 edition of the workshop, in which we\ntackle the problems of early detection of gambling addiction, self-harm and\nestimating depression severity from social media posts. We employ pre-trained\nBERT transformers and data crawled automatically from mental health subreddits\nand obtain reasonable results on all three tasks.", "published": "2021-06-30 16:12:11", "link": "http://arxiv.org/abs/2106.16175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of the Recent Visibility of the SigDial Conference", "abstract": "Automated speech and text interfaces are continuing to improve, resulting in\nincreased research in the area of dialogue systems. Moreover, conferences and\nworkshops from various fields are focusing more on language through speech and\ntext mediums as candidates for interaction with applications such as search\ninterfaces and robots. In this paper, we explore how visible the SigDial\nconference is to outside conferences by analysing papers from top Natural\nLangauge Processing conferences since 2015 to determine the popularity of\ncertain SigDial-related topics, as well as analysing what SigDial papers are\nbeing cited by others outside of SigDial. We find that despite a dramatic\nincrease in dialogue-related research, SigDial visibility has not increased. We\nconclude by offering some suggestions.", "published": "2021-06-30 16:47:44", "link": "http://arxiv.org/abs/2106.16196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zipf's laws of meaning in Catalan", "abstract": "In his pioneering research, G. K. Zipf formulated a couple of statistical\nlaws on the relationship between the frequency of a word with its number of\nmeanings: the law of meaning distribution, relating the frequency of a word and\nits frequency rank, and the meaning-frequency law, relating the frequency of a\nword with its number of meanings. Although these laws were formulated more than\nhalf a century ago, they have been only investigated in a few languages. Here\nwe present the first study of these laws in Catalan.\n  We verify these laws in Catalan via the relationship among their exponents\nand that of the rank-frequency law. We present a new protocol for the analysis\nof these Zipfian laws that can be extended to other languages. We report the\nfirst evidence of two marked regimes for these laws in written language and\nspeech, paralleling the two regimes in Zipf's rank-frequency law in large\nmulti-author corpora discovered in early 2000s. Finally, the implications of\nthese two regimes will be discussed.", "published": "2021-06-30 18:06:06", "link": "http://arxiv.org/abs/2107.00042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated\n  Text", "abstract": "Human evaluations are typically considered the gold standard in natural\nlanguage generation, but as models' fluency improves, how well can evaluators\ndetect and judge machine-generated text? We run a study assessing non-experts'\nability to distinguish between human- and machine-authored text (GPT2 and GPT3)\nin three domains (stories, news articles, and recipes). We find that, without\ntraining, evaluators distinguished between GPT3- and human-authored text at\nrandom chance level. We explore three approaches for quickly training\nevaluators to better identify GPT3-authored text (detailed instructions,\nannotated examples, and paired examples) and find that while evaluators'\naccuracy improved up to 55%, it did not significantly improve across the three\ndomains. Given the inconsistent results across text domains and the often\ncontradictory reasons evaluators gave for their judgments, we examine the role\nuntrained human evaluations play in NLG evaluation and provide recommendations\nto NLG researchers for improving human evaluations of text generated from\nstate-of-the-art models.", "published": "2021-06-30 19:00:25", "link": "http://arxiv.org/abs/2107.00061v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to communicate about shared procedural abstractions", "abstract": "Many real-world tasks require agents to coordinate their behavior to achieve\nshared goals. Successful collaboration requires not only adopting the same\ncommunicative conventions, but also grounding these conventions in the same\ntask-appropriate conceptual abstractions. We investigate how humans use natural\nlanguage to collaboratively solve physical assembly problems more effectively\nover time. Human participants were paired up in an online environment to\nreconstruct scenes containing two block towers. One participant could see the\ntarget towers, and sent assembly instructions for the other participant to\nreconstruct. Participants provided increasingly concise instructions across\nrepeated attempts on each pair of towers, using higher-level referring\nexpressions that captured each scene's hierarchical structure. To explain these\nfindings, we extend recent probabilistic models of ad-hoc convention formation\nwith an explicit perceptual learning mechanism. These results shed light on the\ninductive biases that enable intelligent agents to coordinate upon shared\nprocedural abstractions.", "published": "2021-06-30 19:59:11", "link": "http://arxiv.org/abs/2107.00077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Conditional Splitting Framework for Efficient Constituency Parsing", "abstract": "We introduce a generic seq2seq parsing framework that casts constituency\nparsing problems (syntactic and discourse parsing) into a series of conditional\nsplitting decisions. Our parsing model estimates the conditional probability\ndistribution of possible splitting points in a given text span and supports\nefficient top-down decoding, which is linear in number of nodes. The\nconditional splitting formulation together with efficient beam search inference\nfacilitate structural consistency without relying on expensive structured\ninference. Crucially, for discourse analysis we show that in our formulation,\ndiscourse segmentation can be framed as a special case of parsing which allows\nus to perform discourse parsing without requiring segmentation as a\npre-requisite. Experiments show that our model achieves good results on the\nstandard syntactic parsing tasks under settings with/without pre-trained\nrepresentations and rivals state-of-the-art (SoTA) methods that are more\ncomputationally expensive than ours. In discourse parsing, our method\noutperforms SoTA by a good margin.", "published": "2021-06-30 00:36:34", "link": "http://arxiv.org/abs/2106.15760v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem\n  Solvers", "abstract": "We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms\nof both language patterns and problem types) English math word problem (MWP)\ncorpus for evaluating the capability of various MWP solvers. Existing MWP\ncorpora for studying AI progress remain limited either in language usage\npatterns or in problem types. We thus present a new English MWP corpus with\n2,305 MWPs that cover more text patterns and most problem types taught in\nelementary school. Each MWP is annotated with its problem type and grade level\n(for indicating the level of difficulty). Furthermore, we propose a metric to\nmeasure the lexicon usage diversity of a given MWP corpus, and demonstrate that\nASDiv is more diverse than existing corpora. Experiments show that our proposed\ncorpus reflects the true capability of MWP solvers more faithfully.", "published": "2021-06-30 01:54:11", "link": "http://arxiv.org/abs/2106.15772v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Automatically Select Emotion for Response via Personality-affected\n  Emotion Transition", "abstract": "To provide consistent emotional interaction with users, dialog systems should\nbe capable to automatically select appropriate emotions for responses like\nhumans. However, most existing works focus on rendering specified emotions in\nresponses or empathetically respond to the emotion of users, yet the individual\ndifference in emotion expression is overlooked. This may lead to inconsistent\nemotional expressions and disinterest users. To tackle this issue, we propose\nto equip the dialog system with personality and enable it to automatically\nselect emotions in responses by simulating the emotion transition of humans in\nconversation. In detail, the emotion of the dialog system is transitioned from\nits preceding emotion in context. The transition is triggered by the preceding\ndialog context and affected by the specified personality trait. To achieve\nthis, we first model the emotion transition in the dialog system as the\nvariation between the preceding emotion and the response emotion in the\nValence-Arousal-Dominance (VAD) emotion space. Then, we design neural networks\nto encode the preceding dialog context and the specified personality traits to\ncompose the variation. Finally, the emotion for response is selected from the\nsum of the preceding emotion and the variation. We construct a dialog dataset\nwith emotion and personality labels and conduct emotion prediction tasks for\nevaluation. Experimental results validate the effectiveness of the\npersonality-affected emotion transition.", "published": "2021-06-30 07:00:42", "link": "http://arxiv.org/abs/2106.15846v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Incorporating Domain Knowledge for Extractive Summarization of Legal\n  Case Documents", "abstract": "Automatic summarization of legal case documents is an important and practical\nchallenge. Apart from many domain-independent text summarization algorithms\nthat can be used for this purpose, several algorithms have been developed\nspecifically for summarizing legal case documents. However, most of the\nexisting algorithms do not systematically incorporate domain knowledge that\nspecifies what information should ideally be present in a legal case document\nsummary. To address this gap, we propose an unsupervised summarization\nalgorithm DELSumm which is designed to systematically incorporate guidelines\nfrom legal experts into an optimization setup. We conduct detailed experiments\nover case documents from the Indian Supreme Court. The experiments show that\nour proposed unsupervised method outperforms several strong baselines in terms\nof ROUGE scores, including both general summarization algorithms and\nlegal-specific ones. In fact, though our proposed algorithm is unsupervised, it\noutperforms several supervised summarization models that are trained over\nthousands of document-summary pairs.", "published": "2021-06-30 08:06:15", "link": "http://arxiv.org/abs/2106.15876v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Whose Opinions Matter? Perspective-aware Models to Identify Opinions of\n  Hate Speech Victims in Abusive Language Detection", "abstract": "Social media platforms provide users the freedom of expression and a medium\nto exchange information and express diverse opinions. Unfortunately, this has\nalso resulted in the growth of abusive content with the purpose of\ndiscriminating people and targeting the most vulnerable communities such as\nimmigrants, LGBT, Muslims, Jews and women. Because abusive language is\nsubjective in nature, there might be highly polarizing topics or events\ninvolved in the annotation of abusive contents such as hate speech (HS).\nTherefore, we need novel approaches to model conflicting perspectives and\nopinions coming from people with different personal and demographic\nbackgrounds. In this paper, we present an in-depth study to model polarized\nopinions coming from different communities under the hypothesis that similar\ncharacteristics (ethnicity, social background, culture etc.) can influence the\nperspectives of annotators on a certain phenomenon. We believe that by relying\non this information, we can divide the annotators into groups sharing similar\nperspectives. We can create separate gold standards, one for each group, to\ntrain state-of-the-art deep learning models. We can employ an ensemble approach\nto combine the perspective-aware classifiers from different groups to an\ninclusive model. We also propose a novel resource, a multi-perspective English\nlanguage dataset annotated according to different sub-categories relevant for\ncharacterising online abuse: hate speech, aggressiveness, offensiveness and\nstereotype. By training state-of-the-art deep learning models on this novel\nresource, we show how our approach improves the prediction performance of a\nstate-of-the-art supervised classifier.", "published": "2021-06-30 08:35:49", "link": "http://arxiv.org/abs/2106.15896v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Ask Conversational Questions by Optimizing Levenshtein\n  Distance", "abstract": "Conversational Question Simplification (CQS) aims to simplify self-contained\nquestions into conversational ones by incorporating some conversational\ncharacteristics, e.g., anaphora and ellipsis. Existing maximum likelihood\nestimation (MLE) based methods often get trapped in easily learned tokens as\nall tokens are treated equally during training. In this work, we introduce a\nReinforcement Iterative Sequence Editing (RISE) framework that optimizes the\nminimum Levenshtein distance (MLD) through explicit editing actions. RISE is\nable to pay attention to tokens that are related to conversational\ncharacteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)\nalgorithm with a Dynamic Programming based Sampling (DPS) process to improve\nexploration. Experimental results on two benchmark datasets show that RISE\nsignificantly outperforms state-of-the-art methods and generalizes well on\nunseen data.", "published": "2021-06-30 08:44:19", "link": "http://arxiv.org/abs/2106.15903v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "News Article Retrieval in Context for Event-centric Narrative Creation", "abstract": "Writers such as journalists often use automatic tools to find relevant\ncontent to include in their narratives. In this paper, we focus on supporting\nwriters in the news domain to develop event-centric narratives. Given an\nincomplete narrative that specifies a main event and a context, we aim to\nretrieve news articles that discuss relevant events that would enable the\ncontinuation of the narrative. We formally define this task and propose a\nretrieval dataset construction procedure that relies on existing news articles\nto simulate incomplete narratives and relevant articles. Experiments on two\ndatasets derived from this procedure show that state-of-the-art lexical and\nsemantic rankers are not sufficient for this task. We show that combining those\nwith a ranker that ranks articles by reverse chronological order outperforms\nthose rankers alone. We also perform an in-depth quantitative and qualitative\nanalysis of the results that sheds light on the characteristics of this task.", "published": "2021-06-30 13:27:54", "link": "http://arxiv.org/abs/2106.16053v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving Factual Consistency of Abstractive Summarization on Customer\n  Feedback", "abstract": "E-commerce stores collect customer feedback to let sellers learn about\ncustomer concerns and enhance customer order experience. Because customer\nfeedback often contains redundant information, a concise summary of the\nfeedback can be generated to help sellers better understand the issues causing\ncustomer dissatisfaction. Previous state-of-the-art abstractive text\nsummarization models make two major types of factual errors when producing\nsummaries from customer feedback, which are wrong entity detection (WED) and\nincorrect product-defect description (IPD). In this work, we introduce a set of\nmethods to enhance the factual consistency of abstractive summarization on\ncustomer feedback. We augment the training data with artificially corrupted\nsummaries, and use them as counterparts of the target summaries. We add a\ncontrastive loss term into the training objective so that the model learns to\navoid certain factual errors. Evaluation results show that a large portion of\nWED and IPD errors are alleviated for BART and T5. Furthermore, our approaches\ndo not depend on the structure of the summarization model and thus are\ngeneralizable to any abstractive summarization systems.", "published": "2021-06-30 16:34:36", "link": "http://arxiv.org/abs/2106.16188v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Regressing Location on Text for Probabilistic Geocoding", "abstract": "Text data are an important source of detailed information about social and\npolitical events. Automated systems parse large volumes of text data to infer\nor extract structured information that describes actors, actions, dates, times,\nand locations. One of these sub-tasks is geocoding: predicting the geographic\ncoordinates associated with events or locations described by a given text. We\npresent an end-to-end probabilistic model for geocoding text data.\nAdditionally, we collect a novel data set for evaluating the performance of\ngeocoding systems. We compare the model-based solution, called ELECTRo-map, to\nthe current state-of-the-art open source system for geocoding texts for event\ndata. Finally, we discuss the benefits of end-to-end model-based geocoding,\nincluding principled uncertainty estimation and the ability of these models to\nleverage contextual information.", "published": "2021-06-30 20:04:55", "link": "http://arxiv.org/abs/2107.00080v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On joint training with interfaces for spoken language understanding", "abstract": "Spoken language understanding (SLU) systems extract both text transcripts and\nsemantics associated with intents and slots from input speech utterances. SLU\nsystems usually consist of (1) an automatic speech recognition (ASR) module,\n(2) an interface module that exposes relevant outputs from ASR, and (3) a\nnatural language understanding (NLU) module. Interfaces in SLU systems carry\ninformation on text transcriptions or richer information like neural embeddings\nfrom ASR to NLU. In this paper, we study how interfaces affect joint-training\nfor spoken language understanding. Most notably, we obtain the state-of-the-art\nresults on the publicly available 50-hr SLURP dataset. We first leverage\nlarge-size pretrained ASR and NLU models that are connected by a text\ninterface, and then jointly train both models via a sequence loss function. For\nscenarios where pretrained models are not utilized, the best results are\nobtained through a joint sequence loss training using richer neural interfaces.\nFinally, we show the overall diminishing impact of leveraging pretrained models\nwith increased training data size.", "published": "2021-06-30 09:20:32", "link": "http://arxiv.org/abs/2106.15919v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task", "abstract": "This paper describes the submission to the IWSLT 2021 Low-Resource Speech\nTranslation Shared Task by IMS team. We utilize state-of-the-art models\ncombined with several data augmentation, multi-task and transfer learning\napproaches for the automatic speech recognition (ASR) and machine translation\n(MT) steps of our cascaded system. Moreover, we also explore the feasibility of\na full end-to-end speech translation (ST) model in the case of very constrained\namount of ground truth labeled data. Our best system achieves the best\nperformance among all submitted systems for Congolese Swahili to English and\nFrench with BLEU scores 7.7 and 13.7 respectively, and the second best result\nfor Coastal Swahili to English with BLEU score 14.9.", "published": "2021-06-30 13:29:19", "link": "http://arxiv.org/abs/2106.16055v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Saturated Transformers are Constant-Depth Threshold Circuits", "abstract": "Transformers have become a standard neural network architecture for many NLP\nproblems, motivating theoretical analysis of their power in terms of formal\nlanguages. Recent work has shown that transformers with hard attention are\nquite limited in power (Hahn, 2020), as they can be simulated by constant-depth\nAND/OR circuits (Hao et al. 2021). However, hard attention is a strong\nassumption, which may complicate the relevance of these results in practice. In\nthis work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We first show that\nsaturated transformers transcend the known limitations of hard-attention\ntransformers. We then prove saturated transformers with floating-point values\ncan be simulated by constant-depth threshold circuits, giving the class\n$\\mathsf{TC}^0$ as an upper bound on the formal languages they recognize.", "published": "2021-06-30 17:09:47", "link": "http://arxiv.org/abs/2106.16213v3", "categories": ["cs.CL", "cs.CC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning a Reversible Embedding Mapping using Bi-Directional Manifold\n  Alignment", "abstract": "We propose a Bi-Directional Manifold Alignment (BDMA) that learns a\nnon-linear mapping between two manifolds by explicitly training it to be\nbijective. We demonstrate BDMA by training a model for a pair of languages\nrather than individual, directed source and target combinations, reducing the\nnumber of models by 50%. We show that models trained with BDMA in the \"forward\"\n(source to target) direction can successfully map words in the \"reverse\"\n(target to source) direction, yielding equivalent (or better) performance to\nstandard unidirectional translation models where the source and target language\nis flipped. We also show how BDMA reduces the overall size of the model.", "published": "2021-06-30 22:13:42", "link": "http://arxiv.org/abs/2107.00124v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sawtooth Factorial Topic Embeddings Guided Gamma Belief Network", "abstract": "Hierarchical topic models such as the gamma belief network (GBN) have\ndelivered promising results in mining multi-layer document representations and\ndiscovering interpretable topic taxonomies. However, they often assume in the\nprior that the topics at each layer are independently drawn from the Dirichlet\ndistribution, ignoring the dependencies between the topics both at the same\nlayer and across different layers. To relax this assumption, we propose\nsawtooth factorial topic embedding guided GBN, a deep generative model of\ndocuments that captures the dependencies and semantic similarities between the\ntopics in the embedding space. Specifically, both the words and topics are\nrepresented as embedding vectors of the same dimension. The topic matrix at a\nlayer is factorized into the product of a factor loading matrix and a topic\nembedding matrix, the transpose of which is set as the factor loading matrix of\nthe layer above. Repeating this particular type of factorization, which shares\ncomponents between adjacent layers, leads to a structure referred to as\nsawtooth factorization. An auto-encoding variational inference network is\nconstructed to optimize the model parameter via stochastic gradient descent.\nExperiments on big corpora show that our models outperform other neural topic\nmodels on extracting deeper interpretable topics and deriving better document\nrepresentations.", "published": "2021-06-30 10:14:57", "link": "http://arxiv.org/abs/2107.02757v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "DF-Conformer: Integrated architecture of Conv-TasNet and Conformer using\n  linear complexity self-attention for speech enhancement", "abstract": "Single-channel speech enhancement (SE) is an important task in speech\nprocessing. A widely used framework combines an analysis/synthesis filterbank\nwith a mask prediction network, such as the Conv-TasNet architecture. In such\nsystems, the denoising performance and computational efficiency are mainly\naffected by the structure of the mask prediction network. In this study, we aim\nto improve the sequential modeling ability of Conv-TasNet architectures by\nintegrating Conformer layers into a new mask prediction network. To make the\nmodel computationally feasible, we extend the Conformer using linear complexity\nattention and stacked 1-D dilated depthwise convolution layers. We trained the\nmodel on 3,396 hours of noisy speech data, and show that (i) the use of linear\ncomplexity attention avoids high computational complexity, and (ii) our model\nachieves higher scale-invariant signal-to-noise ratio than the improved\ntime-dilated convolution network (TDCN++), an extended version of Conv-TasNet.", "published": "2021-06-30 05:11:29", "link": "http://arxiv.org/abs/2106.15813v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatial resolution of late reverberation in virtual acoustic\n  environments", "abstract": "Late reverberation involves the superposition of many sound reflections\nresulting in a diffuse sound field. Since the spatially resolved perception of\nindividual diffuse reflections is impossible, simplifications can potentially\nbe made for modelling late reverberation in room acoustics simulations with\nreduced spatial resolution. Such simplifications are desired for interactive,\nreal-time virtual acoustic environments with applications in hearing research\nand for the evaluation of hearing supportive devices. In this context, the\nnumber and spatial arrangement of loudspeakers used for playback additionally\naffect spatial resolution. The current study assessed the minimum number of\nspatially evenly distributed virtual late reverberation sources required to\nperceptually approximate spatially highly resolved isotropic and anisotropic\nlate reverberation and to technically approximate a spherically isotropic\ndiffuse sound field. The spatial resolution of the rendering was systematically\nreduced by using subsets of the loudspeakers of an 86-channel spherical\nloudspeaker array in an anechoic chamber. It was tested whether listeners can\ndistinguish lower spatial resolutions for the rendering of late reverberation\nfrom the highest achievable spatial resolution in different simulated rooms.\nRendering of early reflections was kept fixed. The coherence of the sound field\nacross a pair of microphones at ear and behind-the-ear hearing device distance\nwas assessed to separate the effects of number of virtual sources and\nloudspeaker array geometry. Results show that between 12 and 24 reverberation\nsources are required.", "published": "2021-06-30 08:25:28", "link": "http://arxiv.org/abs/2106.15888v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Effect of acoustic scene complexity and visual scene representation on\n  auditory perception in virtual audio-visual environments", "abstract": "In daily life, social interaction and acoustic communication often take place\nin complex acoustic environments (CAE) with a variety of interfering sounds and\nreverberation. For hearing research and the evaluation of hearing systems,\nsimulated CAEs using virtual reality techniques have gained interest in the\ncontext of ecological validity. In the current study, the effect of scene\ncomplexity and visual representation of the scene on psychoacoustic measures\nlike sound source location, distance perception, loudness, speech\nintelligibility, and listening effort in a virtual audio-visual environment was\ninvestigated. A 3-dimensional, 86-channel loudspeaker array was used to render\nthe sound field in combination with or without a head-mounted display (HMD) to\ncreate an immersive stereoscopic visual representation of the scene. The scene\nconsisted of a ring of eight (virtual) loudspeakers which played a target\nspeech stimulus and nonsense speech interferers in several spatial conditions.\nEither an anechoic (snowy outdoor scenery) or echoic environment (loft\napartment) with a reverberation time (T60) of about 1.5 s was simulated. In\naddition to varying the number of interferers, scene complexity was varied by\nassessing the psychoacoustic measures in isolated consecutive measurements\norcsimultaneously. Results showed no significant effect of wearing the HMD on\nthe data. Loudness and distance perception showed significantly different\nresults when they were measured simultaneously instead of consecutively in\nisolation. The advantage of the suggested setup is that it can be directly\ntransferred to a corresponding real room, enabling a 1:1 comparison and\nverification of the perception experiments in the real and virtual environment.", "published": "2021-06-30 08:53:48", "link": "http://arxiv.org/abs/2106.15909v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Communication conditions in virtual acoustic scenes in an underground\n  station", "abstract": "Underground stations are a common communication situation in towns: we talk\nwith friends or colleagues, listen to announcements or shop for titbits while\nbackground noise and reverberation are challenging communication. Here, we\nperform an acoustical analysis of two communication scenes in an underground\nstation in Munich and test speech intelligibility. The acoustical conditions\nwere measured in the station and are compared to simulations in the real-time\nSimulated Open Field Environment (rtSOFE). We compare binaural room impulse\nresponses measured with an artificial head in the station to modeled impulse\nresponses for free-field auralization via 60 loudspeakers in the rtSOFE. We\nused the image source method to model early reflections and a set of\nmulti-microphone recordings to model late reverberation. The first\ncommunication scene consists of 12 equidistant (1.6 m) horizontally spaced\nsource positions around a listener, simulating different direction-dependent\nspatial unmasking conditions. The second scene mimics an approaching speaker\nacross six radially spaced source positions (from 1 m to 10 m) with varying\ndirect sound level and thus direct-to-reverberant energy. The acoustic\nparameters of the underground station show a moderate amount of reverberation\n(T30 in octave bands was between 2.3 s and 0.6 s and early-decay times between\n1.46 s and 0.46 s). The binaural and energetic parameters of the auralization\nwere in a close match to the measurement. Measured speech reception thresholds\nwere within the error of the speech test, letting us to conclude that the\nauralized simulation reproduces acoustic and perceptually relevant parameters\nfor speech intelligibility with high accuracy.", "published": "2021-06-30 09:17:43", "link": "http://arxiv.org/abs/2106.15916v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Integrated Framework for Two-pass Personalized Voice Trigger", "abstract": "In this paper, we present the XMUSPEECH system for Task 1 of 2020\nPersonalized Voice Trigger Challenge (PVTC2020). Task 1 is a joint wake-up word\ndetection with speaker verification on close talking data. The whole system\nconsists of a keyword spotting (KWS) sub-system and a speaker verification (SV)\nsub-system. For the KWS system, we applied a Temporal Depthwise Separable\nConvolution Residual Network (TDSC-ResNet) to improve the system's performance.\nFor the SV system, we proposed a multi-task learning network, where phonetic\nbranch is trained with the character label of the utterance, and speaker branch\nis trained with the label of the speaker. Phonetic branch is optimized with\nconnectionist temporal classification (CTC) loss, which is treated as an\nauxiliary module for speaker branch. Experiments show that our system gets\nsignificant improvements compared with baseline system.", "published": "2021-06-30 09:58:55", "link": "http://arxiv.org/abs/2106.15950v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fast processing explains the effect of sound reflection on binaural\n  unmasking", "abstract": "Sound reflections and late reverberation alter energetic and binaural cues of\na target source, thereby affecting it's detection in noise. Two experiments\ninvestigated detection of harmonic complex tones, centered around 500 Hz, in\nnoise in a virtual room with different modifications of simulated room impulse\nresponses (RIR). Stimuli were auralized using the SOFE's loudspeakers in\nanechoic space. The target was presented from the front or at 0$^\\circ$\nazimuth, while an anechoic noise masker was simultaneously presented at\n0$^\\circ$. In the first experiment, early reflections were progressively added\nto the RIR and detection thresholds of the reverberant target were measured.\nFor a frontal sound source, detection thresholds decreased while adding the\nfirst 45 ms of early reflections, whereas for a lateral sound source thresholds\nremained constant. In the second experiment, early reflections were cut out\nwhile late reflections were kept along with the direct sound. Results for a\ntarget at 0$^\\circ$ show that even reflections as late as 150 ms reduce\ndetection thresholds compared to only the direct sound. A binaural model with a\nsluggishness component following the computation of binaural unmasking in short\nwindows predicts measured and literature results better than when large windows\nare used.", "published": "2021-06-30 12:45:33", "link": "http://arxiv.org/abs/2106.16024v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Computationally efficient spatial rendering of late reverberation in\n  virtual acoustic environments", "abstract": "For 6-DOF (degrees of freedom) interactive virtual acoustic environments\n(VAEs), the spatial rendering of diffuse late reverberation in addition to\nearly (specular) reflections is important. In the interest of computational\nefficiency, the acoustic simulation of the late reverberation can be simplified\nby using a limited number of spatially distributed virtual reverb sources (VRS)\neach radiating incoherent signals. A sufficient number of VRS is needed to\napproximate spatially anisotropic late reverberation, e.g., in a room with\ninhomogeneous distribution of absorption at the boundaries. Here, a highly\nefficient and perceptually plausible method to generate and spatially render\nlate reverberation is suggested, extending the room acoustics simulator RAZR\n[Wendt et al., J. Audio Eng. Soc., 62, 11 (2014)]. The room dimensions and\nfrequency-dependent absorption coefficients at the wall boundaries are used to\ndetermine the parameters of a physically-based feedback delay network (FDN) to\ngenerate the incoherent VRS signals. The VRS are spatially distributed around\nthe listener with weighting factors representing the spatially subsampled\ndistribution of absorption coefficients on the wall boundaries. The minimum\nnumber of VRS required to be perceptually distinguishable from the maximum\n(reference) number of 96 VRS was assessed in a listening test conducted with a\nspherical loudspeaker array within an anechoic room. For the resulting low\nnumbers of VRS suited for spatial rendering, optimal physically-based parameter\nchoices for the FDN are discussed.", "published": "2021-06-30 08:33:02", "link": "http://arxiv.org/abs/2107.00004v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using Self-Supervised Feature Extractors with Attention for Automatic\n  COVID-19 Detection from Speech", "abstract": "The ComParE 2021 COVID-19 Speech Sub-challenge provides a test-bed for the\nevaluation of automatic detectors of COVID-19 from speech. Such models can be\nof value by providing test triaging capabilities to health authorities, working\nalongside traditional testing methods. Herein, we leverage the usage of\npre-trained, problem agnostic, speech representations and evaluate their use\nfor this task. We compare the obtained results against a CNN architecture\ntrained from scratch and traditional frequency-domain representations. We also\nevaluate the usage of Self-Attention Pooling as an utterance-level information\naggregation method. Experimental results demonstrate that models trained on\nfeatures extracted from self-supervised models perform similarly or outperform\nfully-supervised models and models based on handcrafted features. Our best\nmodel improves the Unweighted Average Recall (UAR) from 69.0\\% to 72.3\\% on a\ndevelopment set comprised of only full-band examples and achieves 64.4\\% on the\ntest set. Furthermore, we study where the network is attending, attempting to\ndraw some conclusions regarding its explainability. In this relatively small\ndataset, we find the network attends especially to vowels and aspirates.", "published": "2021-06-30 21:35:28", "link": "http://arxiv.org/abs/2107.00112v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust and Interpretable Temporal Convolution Network for Event\n  Detection in Lung Sound Recordings", "abstract": "This paper proposes a novel framework for lung sound event detection,\nsegmenting continuous lung sound recordings into discrete events and performing\nrecognition on each event. Exploiting the lightweight nature of Temporal\nConvolution Networks (TCNs) and their superior results compared to their\nrecurrent counterparts, we propose a lightweight, yet robust, and completely\ninterpretable framework for lung sound event detection. We propose the use of a\nmulti-branch TCN architecture and exploit a novel fusion strategy to combine\nthe resultant features from these branches. This not only allows the network to\nretain the most salient information across different temporal granularities and\ndisregards irrelevant information, but also allows our network to process\nrecordings of arbitrary length. Results: The proposed method is evaluated on\nmultiple public and in-house benchmarks of irregular and noisy recordings of\nthe respiratory auscultation process for the identification of numerous\nauscultation events including inhalation, exhalation, crackles, wheeze,\nstridor, and rhonchi. We exceed the state-of-the-art results in all\nevaluations. Furthermore, we empirically analyse the effect of the proposed\nmulti-branch TCN architecture and the feature fusion strategy and provide\nquantitative and qualitative evaluations to illustrate their efficiency.\nMoreover, we provide an end-to-end model interpretation pipeline that\ninterprets the operations of all the components of the proposed framework. Our\nanalysis of different feature fusion strategies shows that the proposed feature\nconcatenation method leads to better suppression of non-informative features,\nwhich drastically reduces the classifier overhead resulting in a robust\nlightweight network.The lightweight nature of our model allows it to be\ndeployed in end-user devices such as smartphones, and it has the ability to\ngenerate predictions in real-time.", "published": "2021-06-30 06:36:22", "link": "http://arxiv.org/abs/2106.15835v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sequence-level Confidence Classifier for ASR Utterance Accuracy and\n  Application to Acoustic Models", "abstract": "Scores from traditional confidence classifiers (CCs) in automatic speech\nrecognition (ASR) systems lack universal interpretation and vary with updates\nto the underlying confidence or acoustic models (AMs). In this work, we build\ninterpretable confidence scores with an objective to closely align with ASR\naccuracy. We propose a new sequence-level CC with a richer context providing CC\nscores highly correlated with ASR accuracy and scores stable across CC updates.\nHence, expanding CC applications. Recently, AM customization has gained\ntraction with the widespread use of unified models. Conventional adaptation\nstrategies that customize AM expect well-matched data for the target domain\nwith gold-standard transcriptions. We propose a cost-effective method of using\nCC scores to select an optimal adaptation data set, where we maximize ASR gains\nfrom minimal data. We study data in various confidence ranges and optimally\nchoose data for AM adaptation with KL-Divergence regularization. On the\nMicrosoft voice search task, data selection for supervised adaptation using the\nsequence-level confidence scores achieves word error rate reduction (WERR) of\n8.5% for row-convolution LSTM (RC-LSTM) and 5.2% for latency-controlled\nbidirectional LSTM (LC-BLSTM). In the semi-supervised case, with ASR hypotheses\nas labels, our method provides WERR of 5.9% and 2.8% for RC-LSTM and LC-BLSTM,\nrespectively.", "published": "2021-06-30 20:50:16", "link": "http://arxiv.org/abs/2107.00099v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Generative Model for Raw Audio Using Transformer Architectures", "abstract": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet. This is fully probabilistic,\nauto-regressive, and causal, i.e. each sample generated depends only on the\npreviously observed samples. Our approach outperforms a widely used wavenet\narchitecture by up to 9% on a similar dataset for predicting the next step.\nUsing the attention mechanism, we enable the architecture to learn which audio\nsamples are important for the prediction of the future sample. We show how\ncausal transformer generative models can be used for raw waveform synthesis. We\nalso show that this performance can be improved by another 2% by conditioning\nsamples over a wider context. The flexibility of the current model to\nsynthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.", "published": "2021-06-30 13:05:31", "link": "http://arxiv.org/abs/2106.16036v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
