{"title": "Correlation emergence in two coupled simulated limit order books", "abstract": "We use random walks to simulate the fluid limit of two coupled diffusive\nlimit order books to model correlation emergence. The model implements the\narrival, cancellation and diffusion of orders coupled by a pairs trader\nprofiting from the mean-reversion between the two order books in the fluid\nlimit for a Lit order book with vanishing boundary conditions and order volume\nconservation. We are able to demonstrate the recovery of an Epps effect from\nthis. We discuss how various stylised facts depend on the model parameters and\nthe numerical scheme and discuss the various strengths and weaknesses of the\napproach. We demonstrate how the Epps effect depends on different choices of\ntime and price discretisation. This shows how an Epps effect can emerge without\nrecourse to market microstructure noise relative to a latent model but can\nrather be viewed as an emergent property arising from trader interactions in a\nworld of asynchronous events.", "published": "2024-08-06 13:29:55", "link": "http://arxiv.org/abs/2408.03181v1", "categories": ["q-fin.TR", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large\n  Language Model-Based Question Answering", "abstract": "Retrieving external knowledge and prompting large language models with\nrelevant information is an effective paradigm to enhance the performance of\nquestion-answering tasks. Previous research typically handles paragraphs from\nexternal documents in isolation, resulting in a lack of context and ambiguous\nreferences, particularly in multi-document and complex tasks. To overcome these\nchallenges, we propose a new retrieval framework IIER, that leverages\nInter-chunk Interactions to Enhance Retrieval. This framework captures the\ninternal connections between document chunks by considering three types of\ninteractions: structural, keyword, and semantic. We then construct a unified\nChunk-Interaction Graph to represent all external documents comprehensively.\nAdditionally, we design a graph-based evidence chain retriever that utilizes\nprevious paths and chunk interactions to guide the retrieval process. It\nidentifies multiple seed nodes based on the target question and iteratively\nsearches for relevant chunks to gather supporting evidence. This retrieval\nprocess refines the context and reasoning chain, aiding the large language\nmodel in reasoning and answer generation. Extensive experiments demonstrate\nthat IIER outperforms strong baselines across four datasets, highlighting its\neffectiveness in improving retrieval and reasoning capabilities.", "published": "2024-08-06 02:39:55", "link": "http://arxiv.org/abs/2408.02907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Checklist: On Unit-Testing Datasets with Usable Information", "abstract": "Model checklists (Ribeiro et al., 2020) have emerged as a useful tool for\nunderstanding the behavior of LLMs, analogous to unit-testing in software\nengineering. However, despite datasets being a key determinant of model\nbehavior, evaluating datasets, e.g., for the existence of annotation artifacts,\nis largely done ad hoc, once a problem in model behavior has already been found\ndownstream. In this work, we take a more principled approach to unit-testing\ndatasets by proposing a taxonomy based on the V-information literature. We call\na collection of such unit tests a data checklist. Using a checklist, not only\nare we able to recover known artifacts in well-known datasets such as SNLI, but\nwe also discover previously unknown artifacts in preference datasets for LLM\nalignment. Data checklists further enable a new kind of data filtering, which\nwe use to improve the efficacy and data efficiency of preference alignment.", "published": "2024-08-06 03:08:36", "link": "http://arxiv.org/abs/2408.02919v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intermediate direct preference optimization", "abstract": "We propose the intermediate direct preference optimization (DPO) method to\ncalculate the DPO loss at selected intermediate layers as an auxiliary loss for\nfinetuning large language models (LLMs). The conventional DPO method fine-tunes\na supervised fine-tuning (SFT) model by calculating the DPO loss using logits\nfrom the final layer. In our intermediate DPO approach, DPO losses are\ncalculated using the logits from K-selected intermediate layers and averaged to\nobtain the intermediate DPO loss. For training the intermediate DPO model, the\nfinal loss is obtained by calculating the weighted sum of the DPO and\nintermediate DPO losses. During inference, the intermediate DPO model decodes\nusing the final layer logits similarly to the conventional DPO model. In\nexperiments using the ultrafeedback dataset, the performance of the\nintermediate DPO model was evaluated using GPT-4. As a result, the intermediate\nDPO model trained using the intermediate DPO loss calculated at the 22nd layer\nof a 32-layer SFT model achieved win rates of 52.5% and 67.5% against the\nconventional DPO and SFT models, respectively, demonstrating the effectiveness\nof the proposed method. Furthermore, we report the relationships among the\nposition of the selected intermediate layers, the number of layers, and\nperformance.", "published": "2024-08-06 03:16:09", "link": "http://arxiv.org/abs/2408.02923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Female Carpenters like Blue Bananas? A Corpus Investigation of\n  Occupation Gender Typicality", "abstract": "People tend to use language to mention surprising properties of events: for\nexample, when a banana is blue, we are more likely to mention color than when\nit is yellow. This fact is taken to suggest that yellowness is somehow a\ntypical feature of bananas, and blueness is exceptional. Similar to how a\nyellow color is typical of bananas, there may also be genders that are typical\nof occupations. In this work, we explore this question using information\ntheoretic techniques coupled with corpus statistic analysis. In two distinct\nlarge corpora, we do not find strong evidence that occupations and gender\ndisplay the same patterns of mentioning as do bananas and color. Instead, we\nfind that gender mentioning is correlated with femaleness of occupation in\nparticular, suggesting perhaps that woman-dominated occupations are seen as\nsomehow ``more gendered'' than male-dominated ones, and thereby they encourage\nmore gender mentioning overall.", "published": "2024-08-06 04:19:23", "link": "http://arxiv.org/abs/2408.02948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The\n  Impact of Prompt Engineering and Knowledge Retrieval", "abstract": "Large language models (LLMs) are fundamentally transforming human-facing\napplications in the health and well-being domains: boosting patient engagement,\naccelerating clinical decision-making, and facilitating medical education.\nAlthough state-of-the-art LLMs have shown superior performance in several\nconversational applications, evaluations within nutrition and diet applications\nare still insufficient. In this paper, we propose to employ the Registered\nDietitian (RD) exam to conduct a standard and comprehensive evaluation of\nstate-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing\nboth accuracy and consistency in nutrition queries. Our evaluation includes\n1050 RD exam questions encompassing several nutrition topics and proficiency\nlevels. In addition, for the first time, we examine the impact of Zero-Shot\n(ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC),\nand Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the\nresponses. Our findings revealed that while these LLMs obtained acceptable\noverall performance, their results varied considerably with different prompts\nand question domains. GPT-4o with CoT-SC prompting outperformed the other\napproaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.\nFor GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both\naccuracy and consistency. RAP was particularly effective for GPT-4o to answer\nExpert level questions. Consequently, choosing the appropriate LLM and\nprompting technique, tailored to the proficiency level and specific domain, can\nmitigate errors and potential risks in diet and nutrition chatbots.", "published": "2024-08-06 05:21:13", "link": "http://arxiv.org/abs/2408.02964v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and\n  Quantization", "abstract": "Large language models (LLMs) have attracted considerable attention in various\nfields for their cost-effective solutions to diverse challenges, especially\nwith advancements in instruction tuning and quantization. E-commerce, with its\ncomplex tasks and extensive product-user interactions, presents a promising\napplication area for LLMs. However, the domain-specific concepts and knowledge\ninherent in e-commerce pose significant challenges for adapting general LLMs.\nTo address this issue, we developed EC-Guide\n\\href{https://github.com/fzp0424/EC-Guide-KDDUP-2024}, a comprehensive\ne-commerce guide for instruction tuning and quantization of LLMs. We also\nheuristically integrated Chain-of-Thought (CoT) during inference to enhance\narithmetic performance. Our approach achieved the 2nd place in Track 2 and 5th\nplace in Track 5 at the Amazon KDD Cup'24\n\\href{https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms}.\nAdditionally, our solution is model-agnostic, enabling effective scalability\nacross larger systems.", "published": "2024-08-06 05:50:41", "link": "http://arxiv.org/abs/2408.02970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Argument Structure Constructions in a Deep Recurrent\n  Language Model", "abstract": "Understanding how language and linguistic constructions are processed in the\nbrain is a fundamental question in cognitive computational neuroscience. In\nthis study, we explore the representation and processing of Argument Structure\nConstructions (ASCs) in a recurrent neural language model. We trained a Long\nShort-Term Memory (LSTM) network on a custom-made dataset consisting of 2000\nsentences, generated using GPT-4, representing four distinct ASCs: transitive,\nditransitive, caused-motion, and resultative constructions.\n  We analyzed the internal activations of the LSTM model's hidden layers using\nMultidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding\n(t-SNE) to visualize the sentence representations. The Generalized\nDiscrimination Value (GDV) was calculated to quantify the degree of clustering\nwithin these representations. Our results show that sentence representations\nform distinct clusters corresponding to the four ASCs across all hidden layers,\nwith the most pronounced clustering observed in the last hidden layer before\nthe output layer. This indicates that even a relatively simple,\nbrain-constrained recurrent neural network can effectively differentiate\nbetween various construction types.\n  These findings are consistent with previous studies demonstrating the\nemergence of word class and syntax rule representations in recurrent language\nmodels trained on next word prediction tasks. In future work, we aim to\nvalidate these results using larger language models and compare them with\nneuroimaging data obtained during continuous speech perception. This study\nhighlights the potential of recurrent neural language models to mirror\nlinguistic processing in the human brain, providing valuable insights into the\ncomputational and neural mechanisms underlying language understanding.", "published": "2024-08-06 09:27:41", "link": "http://arxiv.org/abs/2408.03062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing structural constraints of negation in Pretrained Language Models", "abstract": "Contradictory results about the encoding of the semantic impact of negation\nin pretrained language models (PLMs). have been drawn recently (e.g. Kassner\nand Sch{\\\"u}tze (2020); Gubelmann and Handschuh (2022)). In this paper we focus\nrather on the way PLMs encode negation and its formal impact, through the\nphenomenon of the Negative Polarity Item (NPI) licensing in English. More\nprecisely, we use probes to identify which contextual representations best\nencode 1) the presence of negation in a sentence, and 2) the polarity of a\nneighboring masked polarity item. We find that contextual representations of\ntokens inside the negation scope do allow for (i) a better prediction of the\npresence of not compared to those outside the scope and (ii) a better\nprediction of the right polarity of a masked polarity item licensed by not,\nalthough the magnitude of the difference varies from PLM to PLM. Importantly,\nin both cases the trend holds even when controlling for distance to not. This\ntends to indicate that the embeddings of these models do reflect the notion of\nnegation scope, and do encode the impact of negation on NPI licensing. Yet,\nfurther control experiments reveal that the presence of other lexical items is\nalso better captured when using the contextual representation of a token within\nthe same syntactic clause than outside from it, suggesting that PLMs simply\ncapture the more general notion of syntactic clause.", "published": "2024-08-06 09:54:49", "link": "http://arxiv.org/abs/2408.03070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards an Analysis of Discourse and Interactional Pragmatic Reasoning\n  Capabilities of Large Language Models", "abstract": "In this work, we want to give an overview on which pragmatic abilities have\nbeen tested in LLMs so far and how these tests have been carried out. To do\nthis, we first discuss the scope of the field of pragmatics and suggest a\nsubdivision into discourse pragmatics and interactional pragmatics. We give a\nnon-exhaustive overview of the phenomena of those two subdomains and the\nmethods traditionally used to analyze them. We subsequently consider the\nresulting heterogeneous set of phenomena and methods as a starting point for\nour survey of work on discourse pragmatics and interactional pragmatics in the\ncontext of LLMs.", "published": "2024-08-06 10:02:05", "link": "http://arxiv.org/abs/2408.03074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language\n  Models via Weight Disentanglement", "abstract": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous\nLLMs into one with all the capabilities. Ideally, any LLMs sharing the same\nbackbone should be mergeable, irrespective of whether they are Fine-Tuned (FT)\nwith minor parameter changes or Pre-Trained (PT) with substantial parameter\nshifts. However, existing methods often manually assign the model importance,\nrendering them feasible only for LLMs with similar parameter alterations, such\nas multiple FT LLMs. The diverse parameter changed ranges between FT and PT\nLLMs pose challenges for current solutions in empirically determining the\noptimal combination. In this paper, we make a pioneering effort to broaden the\napplicability of merging techniques from FT to PT LLMs. We initially examine\nthe efficacy of current methods in merging FT and PT LLMs, discovering that\nthey struggle to deal with PT LLMs. Subsequently, we introduce an approach\nbased on WeIght DisENtanglement (WIDEN) to effectively extend the merging\nscope, which first disentangles model weights into magnitude and direction\ncomponents, and then performs adaptive fusion by considering their respective\ncontributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with\ninstruction-following skills) with Sailor (a PT LLM with multilingual\nabilities) across 7B and 14B model scales. Results reveal that: (1) existing\nsolutions usually fail when merging Sailor, either losing both abilities or\nonly retaining instruction-following skills; (2) WIDEN successfully injects the\nmultilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in\nSoutheast Asian languages, achieving enhancements in the fundamental\ncapabilities. In light of previous research, we also merge multiple 13B FT LLMs\nand observe that WIDEN achieves a balanced amalgamation of instruction\nfollowing, mathematical reasoning, and code generation skills.", "published": "2024-08-06 10:46:46", "link": "http://arxiv.org/abs/2408.03092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "500xCompressor: Generalized Prompt Compression for Large Language Models", "abstract": "Prompt compression is crucial for enhancing inference speed, reducing costs,\nand improving user experience. However, current methods face challenges such as\nlow compression ratios and potential data leakage during evaluation. To address\nthese issues, we propose 500xCompressor, a method that compresses extensive\nnatural language contexts into a minimum of one single special token. The\n500xCompressor introduces approximately 0.3% additional parameters and achieves\ncompression ratios ranging from 6x to 480x. It is designed to compress any\ntext, answer various types of questions, and could be utilized by the original\nlarge language model (LLM) without requiring fine-tuning. Initially,\n500xCompressor was pretrained on the Arxiv Corpus, followed by fine-tuning on\nthe ArxivQA dataset, and subsequently evaluated on strictly unseen and\nclassical question answering (QA) datasets. The results demonstrate that the\nLLM retained 62.26-72.89% of its capabilities compared to using non-compressed\nprompts. This study also shows that not all the compressed tokens are equally\nutilized and that K V values have significant advantages over embeddings in\npreserving information at high compression ratios. The highly compressive\nnature of natural language prompts, even for fine-grained complex information,\nsuggests promising potential for future applications and further research into\ndeveloping a new LLM language.", "published": "2024-08-06 10:51:47", "link": "http://arxiv.org/abs/2408.03094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral\n  7B Model and Data Augmentation", "abstract": "This paper describes our approach to the SemEval-2024 safe biomedical Natural\nLanguage Inference for Clinical Trials (NLI4CT) task, which concerns\nclassifying statements about Clinical Trial Reports (CTRs). We explored the\ncapabilities of Mistral-7B, a generalist open-source Large Language Model\n(LLM). We developed a prompt for the NLI4CT task, and fine-tuned a quantized\nversion of the model using an augmented version of the training dataset. The\nexperimental results show that this approach can produce notable results in\nterms of the macro F1-score, while having limitations in terms of faithfulness\nand consistency. All the developed code is publicly available on a GitHub\nrepository", "published": "2024-08-06 11:59:09", "link": "http://arxiv.org/abs/2408.03127v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Inference Optimizations for Large Language Models: Effects, Challenges,\n  and Practical Considerations", "abstract": "Large language models are ubiquitous in natural language processing because\nthey can adapt to new tasks without retraining. However, their sheer scale and\ncomplexity present unique challenges and opportunities, prompting researchers\nand practitioners to explore novel model training, optimization, and deployment\nmethods. This literature review focuses on various techniques for reducing\nresource requirements and compressing large language models, including\nquantization, pruning, knowledge distillation, and architectural optimizations.\nThe primary objective is to explore each method in-depth and highlight its\nunique challenges and practical applications. The discussed methods are\ncategorized into a taxonomy that presents an overview of the optimization\nlandscape and helps navigate it to understand the research trajectory better.", "published": "2024-08-06 12:07:32", "link": "http://arxiv.org/abs/2408.03130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Debiased Nearest Neighbors Framework for Multi-Label Text\n  Classification", "abstract": "Multi-Label Text Classification (MLTC) is a practical yet challenging task\nthat involves assigning multiple non-exclusive labels to each document.\nPrevious studies primarily focus on capturing label correlations to assist\nlabel prediction by introducing special labeling schemes, designing specific\nmodel structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor\n($k$NN) framework has shown promise by retrieving labeled samples as references\nto mine label co-occurrence information in the embedding space. However, two\ncritical biases, namely embedding alignment bias and confidence estimation\nbias, are often overlooked, adversely affecting prediction performance. In this\npaper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC,\nspecifically designed to mitigate these biases. To address embedding alignment\nbias, we propose a debiased contrastive learning strategy, enhancing neighbor\nconsistency on label co-occurrence. For confidence estimation bias, we present\na debiased confidence estimation strategy, improving the adaptive combination\nof predictions from $k$NN and inductive binary classifications. Extensive\nexperiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2,\nAmazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method.\nBesides, our method does not introduce any extra parameters.", "published": "2024-08-06 14:00:23", "link": "http://arxiv.org/abs/2408.03202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Long-Context Language Models Better Multi-Hop Reasoners", "abstract": "Recent advancements in long-context modeling have enhanced language models\n(LMs) for complex tasks across multiple NLP applications. Despite this\nprogress, we find that these models struggle with multi-hop reasoning and\nexhibit decreased performance in the presence of noisy contexts. In this paper,\nwe introduce Reasoning with Attributions, a novel approach that prompts LMs to\nsupply attributions for each assertion during their reasoning. We validate our\napproach through experiments on three multi-hop datasets, employing both\nproprietary and open-source models, and demonstrate its efficacy and\nresilience. Furthermore, we explore methods to augment reasoning capabilities\nvia fine-tuning and offer an attribution-annotated dataset and a specialized\ntraining strategy. Our fine-tuned model achieves competitive performance on\nmulti-hop reasoning benchmarks, closely paralleling proprietary LMs such as\nChatGPT and Claude-instant.", "published": "2024-08-06 15:06:40", "link": "http://arxiv.org/abs/2408.03246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synthesizing Text-to-SQL Data from Weak and Strong LLMs", "abstract": "The capability gap between open-source and closed-source large language\nmodels (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we\nintroduce a synthetic data approach that combines data produced by larger, more\npowerful models (strong models) with error information data generated by\nsmaller, not well-aligned models (weak models). The method not only enhances\nthe domain generalization of text-to-SQL models but also explores the potential\nof error data supervision through preference learning. Furthermore, we employ\nthe synthetic data approach for instruction tuning on open-source LLMs,\nresulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is\ndemonstrated through state-of-the-art results on the SPIDER and BIRD\nbenchmarks, bridging the performance gap between open-source models and methods\nprompted by closed-source models.", "published": "2024-08-06 15:40:32", "link": "http://arxiv.org/abs/2408.03256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoverBench: A Challenging Benchmark for Complex Claim Verification", "abstract": "There is a growing line of research on verifying the correctness of language\nmodels' outputs. At the same time, LMs are being used to tackle complex queries\nthat require reasoning. We introduce CoverBench, a challenging benchmark\nfocused on verifying LM outputs in complex reasoning settings. Datasets that\ncan be used for this purpose are often designed for other complex reasoning\ntasks (e.g., QA) targeting specific use-cases (e.g., financial tables),\nrequiring transformations, negative sampling and selection of hard examples to\ncollect such a benchmark. CoverBench provides a diversified evaluation for\ncomplex claim verification in a variety of domains, types of reasoning,\nrelatively long inputs, and a variety of standardizations, such as multiple\nrepresentations for tables where available, and a consistent schema. We\nmanually vet the data for quality to ensure low levels of label noise. Finally,\nwe report a variety of competitive baseline results to show CoverBench is\nchallenging and has very significant headroom. The data is available at\nhttps://huggingface.co/datasets/google/coverbench .", "published": "2024-08-06 17:58:53", "link": "http://arxiv.org/abs/2408.03325v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SETN: Stock Embedding Enhanced with Textual and Network Information", "abstract": "Stock embedding is a method for vector representation of stocks. There is a\ngrowing demand for vector representations of stock, i.e., stock embedding, in\nwealth management sectors, and the method has been applied to various tasks\nsuch as stock price prediction, portfolio optimization, and similar fund\nidentifications. Stock embeddings have the advantage of enabling the\nquantification of relative relationships between stocks, and they can extract\nuseful information from unstructured data such as text and network data. In\nthis study, we propose stock embedding enhanced with textual and network\ninformation (SETN) using a domain-adaptive pre-trained transformer-based model\nto embed textual information and a graph neural network model to grasp network\ninformation. We evaluate the performance of our proposed model on related\ncompany information extraction tasks. We also demonstrate that stock embeddings\nobtained from the proposed model perform better in creating thematic funds than\nthose obtained from baseline methods, providing a promising pathway for various\napplications in the wealth management industry.", "published": "2024-08-06 02:07:37", "link": "http://arxiv.org/abs/2408.02899v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Empathy Level Alignment via Reinforcement Learning for Empathetic\n  Response Generation", "abstract": "Empathetic response generation, aiming to understand the user's situation and\nfeelings and respond empathically, is crucial in building human-like dialogue\nsystems. Traditional approaches typically employ maximum likelihood estimation\nas the optimization objective during training, yet fail to align the empathy\nlevels between generated and target responses. To this end, we propose an\nempathetic response generation framework using reinforcement learning (EmpRL).\nThe framework develops an effective empathy reward function and generates\nempathetic responses by maximizing the expected reward through reinforcement\nlearning. EmpRL utilizes the pre-trained T5 model as the generator and further\nfine-tunes it to initialize the policy. To align the empathy levels between\ngenerated and target responses within a given context, an empathy reward\nfunction containing three empathy communication mechanisms -- emotional\nreaction, interpretation, and exploration -- is constructed using pre-designed\nand pre-trained empathy identifiers. During reinforcement learning training,\nthe proximal policy optimization algorithm is used to fine-tune the policy,\nenabling the generation of empathetic responses. Both automatic and human\nevaluations demonstrate that the proposed EmpRL framework significantly\nimproves the quality of generated responses, enhances the similarity in empathy\nlevels between generated and target responses, and produces empathetic\nresponses covering both affective and cognitive aspects.", "published": "2024-08-06 06:16:00", "link": "http://arxiv.org/abs/2408.02976v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fact Finder -- Enhancing Domain Expertise of Large Language Models by\n  Incorporating Knowledge Graphs", "abstract": "Recent advancements in Large Language Models (LLMs) have showcased their\nproficiency in answering natural language queries. However, their effectiveness\nis hindered by limited domain-specific knowledge, raising concerns about the\nreliability of their responses. We introduce a hybrid system that augments LLMs\nwith domain-specific knowledge graphs (KGs), thereby aiming to enhance factual\ncorrectness using a KG-based retrieval approach. We focus on a medical KG to\ndemonstrate our methodology, which includes (1) pre-processing, (2) Cypher\nquery generation, (3) Cypher query processing, (4) KG retrieval, and (5)\nLLM-enhanced response generation. We evaluate our system on a curated dataset\nof 69 samples, achieving a precision of 78\\% in retrieving correct KG nodes.\nOur findings indicate that the hybrid system surpasses a standalone LLM in\naccuracy and completeness, as verified by an LLM-as-a-Judge evaluation method.\nThis positions the system as a promising tool for applications that demand\nfactual correctness and completeness, such as target identification -- a\ncritical process in pinpointing biological entities for disease treatment or\ncrop enhancement. Moreover, its intuitive search interface and ability to\nprovide accurate responses within seconds make it well-suited for\ntime-sensitive, precision-focused research contexts. We publish the source code\ntogether with the dataset and the prompt templates used.", "published": "2024-08-06 07:45:05", "link": "http://arxiv.org/abs/2408.03010v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "L3iTC at the FinLLM Challenge Task: Quantization for Financial Text\n  Classification & Summarization", "abstract": "This article details our participation (L3iTC) in the FinLLM Challenge Task\n2024, focusing on two key areas: Task 1, financial text classification, and\nTask 2, financial text summarization. To address these challenges, we\nfine-tuned several large language models (LLMs) to optimize performance for\neach task. Specifically, we used 4-bit quantization and LoRA to determine which\nlayers of the LLMs should be trained at a lower precision. This approach not\nonly accelerated the fine-tuning process on the training data provided by the\norganizers but also enabled us to run the models on low GPU memory. Our\nfine-tuned models achieved third place for the financial classification task\nwith an F1-score of 0.7543 and secured sixth place in the financial\nsummarization task on the official test datasets.", "published": "2024-08-06 08:25:49", "link": "http://arxiv.org/abs/2408.03033v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Enhancing Complex Causality Extraction via Improved Subtask Interaction\n  and Knowledge Fusion", "abstract": "Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.", "published": "2024-08-06 10:15:15", "link": "http://arxiv.org/abs/2408.03079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topic Modeling with Fine-tuning LLMs and Bag of Sentences", "abstract": "Large language models (LLM)'s are increasingly used for topic modeling\noutperforming classical topic models such as LDA. Commonly, pre-trained LLM\nencoders such as BERT are used out-of-the-box despite the fact that fine-tuning\nis known to improve LLMs considerably. The challenge lies in obtaining a\nsuitable (labeled) dataset for fine-tuning. In this paper, we use the recent\nidea to use bag of sentences as the elementary unit in computing topics. In\nturn, we derive an approach FT-Topic to perform unsupervised fine-tuning\nrelying primarily on two steps for constructing a training dataset in an\nautomatic fashion. First, a heuristic method to identifies pairs of sentence\ngroups that are either assumed to be of the same or different topics. Second,\nwe remove sentence pairs that are likely labeled incorrectly. The dataset is\nthen used to fine-tune an encoder LLM, which can be leveraged by any topic\nmodeling approach using embeddings. However, in this work, we demonstrate its\neffectiveness by deriving a novel state-of-the-art topic modeling method called\nSenClu, which achieves fast inference through an expectation-maximization\nalgorithm and hard assignments of sentence groups to a single topic, while\ngiving users the possibility to encode prior knowledge on the topic-document\ndistribution. Code is at \\url{https://github.com/JohnTailor/FT-Topic}", "published": "2024-08-06 11:04:07", "link": "http://arxiv.org/abs/2408.03099v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Translation Performance of Large Language Models Based on\n  Euas-20", "abstract": "In recent years, with the rapid development of deep learning technology,\nlarge language models (LLMs) such as BERT and GPT have achieved breakthrough\nresults in natural language processing tasks. Machine translation (MT), as one\nof the core tasks of natural language processing, has also benefited from the\ndevelopment of large language models and achieved a qualitative leap. Despite\nthe significant progress in translation performance achieved by large language\nmodels, machine translation still faces many challenges. Therefore, in this\npaper, we construct the dataset Euas-20 to evaluate the performance of large\nlanguage models on translation tasks, the translation ability on different\nlanguages, and the effect of pre-training data on the translation ability of\nLLMs for researchers and developers.", "published": "2024-08-06 11:49:11", "link": "http://arxiv.org/abs/2408.03119v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework", "abstract": "As the NLP community increasingly addresses challenges associated with\nmultilingualism, robust annotation tools are essential to handle multilingual\ndatasets efficiently. In this paper, we introduce a code-mixed multilingual\ntext annotation framework, COMMENTATOR, specifically designed for annotating\ncode-mixed text. The tool demonstrates its effectiveness in token-level and\nsentence-level language annotation tasks for Hinglish text. We perform robust\nqualitative human-based evaluations to showcase COMMENTATOR led to 5x faster\nannotations than the best baseline. Our code is publicly available at\n\\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is\navailable at \\url{https://bit.ly/commentator_video}.", "published": "2024-08-06 11:56:26", "link": "http://arxiv.org/abs/2408.03125v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Entity Information for Cross-Modality Correlation Learning:\n  The Entity-Guided Multimodal Summarization", "abstract": "The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.", "published": "2024-08-06 12:45:56", "link": "http://arxiv.org/abs/2408.03149v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Conditioning LLMs with Emotion in Neural Machine Translation", "abstract": "Large Language Models (LLMs) have shown remarkable performance in Natural\nLanguage Processing tasks, including Machine Translation (MT). In this work, we\npropose a novel MT pipeline that integrates emotion information extracted from\na Speech Emotion Recognition (SER) model into LLMs to enhance translation\nquality. We first fine-tune five existing LLMs on the Libri-trans dataset and\nselect the most performant model. Subsequently, we augment LLM prompts with\ndifferent dimensional emotions and train the selected LLM under these different\nconfigurations. Our experiments reveal that integrating emotion information,\nespecially arousal, into LLM prompts leads to notable improvements in\ntranslation quality.", "published": "2024-08-06 12:49:33", "link": "http://arxiv.org/abs/2408.03150v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Parameter Efficient Training Methods for Low Resource Text\n  Classification: A Case Study in Marathi", "abstract": "With the surge in digital content in low-resource languages, there is an\nescalating demand for advanced Natural Language Processing (NLP) techniques\ntailored to these languages. BERT (Bidirectional Encoder Representations from\nTransformers), serving as the foundational framework for numerous NLP\narchitectures and language models, is increasingly employed for the development\nof low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method\nfor fine-tuning Large Language Models (LLMs) and reducing the training\nparameters to some extent to decrease the computational costs needed for\ntraining the model and achieve results comparable to a fully fine-tuned model.\nIn this work, we present a study of PEFT methods for the Indic low-resource\nlanguage Marathi. We conduct a comprehensive analysis of PEFT methods applied\nto various monolingual and multilingual Marathi BERT models. These approaches\nare evaluated on prominent text classification datasets like MahaSent,\nMahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to\nsignificantly expedite the training speed of the models, addressing a critical\naspect of model development and deployment. In this study, we explore Low-Rank\nAdaptation of Large Language Models (LoRA) and adapter methods for low-resource\ntext classification. We show that these methods are competitive with full\nfine-tuning and can be used without loss in accuracy. This study contributes\nvaluable insights into the effectiveness of Marathi BERT models, offering a\nfoundation for the continued advancement of NLP capabilities in Marathi and\nsimilar Indic languages.", "published": "2024-08-06 13:16:16", "link": "http://arxiv.org/abs/2408.03172v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons", "abstract": "In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.", "published": "2024-08-06 15:07:08", "link": "http://arxiv.org/abs/2408.03247v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SARA: Singular-Value Based Adaptive Low-Rank Adaption", "abstract": "With the increasing number of parameters in large pre-trained models, LoRA as\na parameter-efficient fine-tuning(PEFT) method is widely used for not adding\ninference overhead. The LoRA method assumes that weight changes during\nfine-tuning can be approximated by low-rank matrices. However, the rank values\nneed to be manually verified to match different downstream tasks, and they\ncannot accommodate the varying importance of different layers in the model. In\nthis work, we first analyze the relationship between the performance of\ndifferent layers and their ranks using SVD. Based on this, we design the\nSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds\nthe rank during initialization by performing SVD on the pre-trained weights.\nAdditionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly\nreduces the number of parameters by fine-tuning only multiple parallel sets of\nsingular values controlled by a router. Extensive experiments on various\ncomplex tasks demonstrate the simplicity and parameter efficiency of our\nmethods. They can effectively and adaptively find the most suitable rank for\neach layer of each model.", "published": "2024-08-06 16:39:42", "link": "http://arxiv.org/abs/2408.03290v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "KnowPO: Knowledge-aware Preference Optimization for Controllable\n  Knowledge Selection in Retrieval-Augmented Language Models", "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors such\nas contextual ignorance and contextual overinclusion. To this end, we propose a\nKnowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at\nachieving adaptive knowledge selection based on contextual relevance in real\nretrieval scenarios. Concretely, we proposed a general paradigm for\nconstructing knowledge conflict datasets, which comprehensively cover various\nerror types and learn how to avoid these negative signals through preference\noptimization methods. Simultaneously, we proposed a rewriting strategy and data\nratio optimization strategy to address preference imbalances. Experimental\nresults show that KnowPO outperforms previous methods for handling knowledge\nconflicts by over 37\\%, while also exhibiting robust generalization across\nvarious out-of-distribution datasets.", "published": "2024-08-06 16:55:54", "link": "http://arxiv.org/abs/2408.03297v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling Model Parameters", "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is\na critical step towards building generally self-improving agents that can\noperate on open-ended natural language. In this paper, we study the scaling of\ninference-time computation in LLMs, with a focus on answering the question: if\nan LLM is allowed to use a fixed but non-trivial amount of inference-time\ncompute, how much can it improve its performance on a challenging prompt?\nAnswering this question has implications not only on the achievable performance\nof LLMs, but also on the future of LLM pretraining and how one should tradeoff\ninference-time and pre-training compute. Despite its importance, little\nresearch attempted to understand the scaling behaviors of various test-time\ninference methods. Moreover, current work largely provides negative results for\na number of these strategies. In this work, we analyze two primary mechanisms\nto scale test-time computation: (1) searching against dense, process-based\nverifier reward models; and (2) updating the model's distribution over a\nresponse adaptively, given the prompt at test time. We find that in both cases,\nthe effectiveness of different approaches to scaling test-time compute\ncritically varies depending on the difficulty of the prompt. This observation\nmotivates applying a \"compute-optimal\" scaling strategy, which acts to most\neffectively allocate test-time compute adaptively per prompt. Using this\ncompute-optimal strategy, we can improve the efficiency of test-time compute\nscaling by more than 4x compared to a best-of-N baseline. Additionally, in a\nFLOPs-matched evaluation, we find that on problems where a smaller base model\nattains somewhat non-trivial success rates, test-time compute can be used to\noutperform a 14x larger model.", "published": "2024-08-06 17:35:05", "link": "http://arxiv.org/abs/2408.03314v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Training LLMs to Recognize Hedges in Spontaneous Narratives", "abstract": "Hedges allow speakers to mark utterances as provisional, whether to signal\nnon-prototypicality or \"fuzziness\", to indicate a lack of commitment to an\nutterance, to attribute responsibility for a statement to someone else, to\ninvite input from a partner, or to soften critical feedback in the service of\nface-management needs. Here we focus on hedges in an experimentally\nparameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced\nfrom memory by 21 speakers for co-present addressees, transcribed to text\n(Galati and Brennan, 2010). We created a gold standard of hedges annotated by\nhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-based\napproaches for hedge detection: fine-tuning BERT, and zero and few-shot\nprompting with GPT-4o and LLaMA-3. The best-performing approach was a\nfine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on\nthe top performing approaches, we used an LLM-in-the-Loop approach to improve\nthe gold standard coding, as well as to highlight cases in which hedges are\nambiguous in linguistically interesting ways that will guide future research.\nThis is the first step in our research program to train LLMs to interpret and\ngenerate collateral signals appropriately and meaningfully in conversation.", "published": "2024-08-06 17:51:42", "link": "http://arxiv.org/abs/2408.03319v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ULLME: A Unified Framework for Large Language Model Embeddings with\n  Generation-Augmented Learning", "abstract": "Large Language Models (LLMs) excel in various natural language processing\ntasks, but leveraging them for dense passage embedding remains challenging.\nThis is due to their causal attention mechanism and the misalignment between\ntheir pre-training objectives and the text ranking tasks. Despite some recent\nefforts to address these issues, existing frameworks for LLM-based text\nembeddings have been limited by their support for only a limited range of LLM\narchitectures and fine-tuning strategies, limiting their practical application\nand versatility. In this work, we introduce the Unified framework for Large\nLanguage Model Embedding (ULLME), a flexible, plug-and-play implementation that\nenables bidirectional attention across various LLMs and supports a range of\nfine-tuning strategies. We also propose Generation-augmented Representation\nLearning (GRL), a novel fine-tuning method to boost LLMs for text embedding\ntasks. GRL enforces consistency between representation-based and\ngeneration-based relevance scores, leveraging LLMs' powerful generative\nabilities for learning passage embeddings. To showcase our framework's\nflexibility and effectiveness, we release three pre-trained models from ULLME\nwith different backbone architectures, ranging from 1.5B to 8B parameters, all\nof which demonstrate strong performance on the Massive Text Embedding\nBenchmark. Our framework is publicly available at:\nhttps://github.com/nlp-uoregon/ullme. A demo video for ULLME can also be found\nat https://rb.gy/ws1ile.", "published": "2024-08-06 18:53:54", "link": "http://arxiv.org/abs/2408.03402v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Citekit: A Modular Toolkit for Large Language Model Citation Generation", "abstract": "Enabling Large Language Models (LLMs) to generate citations in\nQuestion-Answering (QA) tasks is an emerging paradigm aimed at enhancing the\nverifiability of their responses when LLMs are utilizing external references to\ngenerate an answer. However, there is currently no unified framework to\nstandardize and fairly compare different citation generation methods, leading\nto difficulties in reproducing different methods and a comprehensive\nassessment. To cope with the problems above, we introduce \\name, an open-source\nand modular toolkit designed to facilitate the implementation and evaluation of\nexisting citation generation methods, while also fostering the development of\nnew approaches to improve citation quality in LLM outputs. This tool is highly\nextensible, allowing users to utilize 4 main modules and 14 components to\nconstruct a pipeline, evaluating an existing method or innovative designs. Our\nexperiments with two state-of-the-art LLMs and 11 citation generation baselines\ndemonstrate varying strengths of different modules in answer accuracy and\ncitation quality improvement, as well as the challenge of enhancing\ngranularity. Based on our analysis of the effectiveness of components, we\npropose a new method, self-RAG \\snippet, obtaining a balanced answer accuracy\nand citation quality. Citekit is released at\nhttps://github.com/SjJ1017/Citekit.", "published": "2024-08-06 02:13:15", "link": "http://arxiv.org/abs/2408.04662v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dopamin: Transformer-based Comment Classifiers through Domain\n  Post-Training and Multi-level Layer Aggregation", "abstract": "Code comments provide important information for understanding the source\ncode. They can help developers understand the overall purpose of a function or\nclass, as well as identify bugs and technical debt. However, an overabundance\nof comments is meaningless and counterproductive. As a result, it is critical\nto automatically filter out these comments for specific purposes. In this\npaper, we present Dopamin, a Transformer-based tool for dealing with this\nissue. Our model excels not only in presenting knowledge sharing of common\ncategories across multiple languages, but also in achieving robust performance\nin comment classification by improving comment representation. As a result, it\noutperforms the STACC baseline by 3% on the NLBSE'24 Tool Competition dataset\nin terms of average F1-score, while maintaining a comparable inference time for\npractical use. The source code is publicity available at\nhttps://github.com/FSoft-AI4Code/Dopamin.", "published": "2024-08-06 08:08:43", "link": "http://arxiv.org/abs/2408.04663v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-based MOFs Synthesis Condition Extraction using Few-Shot\n  Demonstrations", "abstract": "The extraction of Metal-Organic Frameworks (MOFs) synthesis route from\nliterature has been crucial for the logical MOFs design with desirable\nfunctionality. The recent advent of large language models (LLMs) provides\ndisruptively new solution to this long-standing problem. While the latest\nresearches mostly stick to primitive zero-shot LLMs lacking specialized\nmaterial knowledge, we introduce in this work the few-shot LLM in-context\nlearning paradigm. First, a human-AI interactive data curation approach is\nproposed to secure high-quality demonstrations. Second, an information\nretrieval algorithm is applied to pick and quantify few-shot demonstrations for\neach extraction. Over three datasets randomly sampled from nearly 90,000\nwell-defined MOFs, we conduct triple evaluations to validate our method. The\nsynthesis extraction, structure inference, and material design performance of\nthe proposed few-shot LLMs all significantly outplay zero-shot LLM and baseline\nmethods. The lab-synthesized material guided by LLM surpasses 91.1%\nhigh-quality MOFs of the same class reported in the literature, on the key\nphysical property of specific surface area.", "published": "2024-08-06 14:53:25", "link": "http://arxiv.org/abs/2408.04665v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs are Not Just Next Token Predictors", "abstract": "LLMs are statistical models of language learning through stochastic gradient\ndescent with a next token prediction objective. Prompting a popular view among\nAI modelers: LLMs are just next token predictors. While LLMs are engineered\nusing next token prediction, and trained based on their success at this task,\nour view is that a reduction to just next token predictor sells LLMs short.\nMoreover, there are important explanations of LLM behavior and capabilities\nthat are lost when we engage in this kind of reduction. In order to draw this\nout, we will make an analogy with a once prominent research program in biology\nexplaining evolution and development from the gene's eye view.", "published": "2024-08-06 16:36:28", "link": "http://arxiv.org/abs/2408.04666v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs", "abstract": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for automatic tools to check the factual accuracy\nof their outputs, as LLMs often hallucinate. This is difficult as it requires\nassessing the factuality of free-form open-domain responses. While there has\nbeen a lot of research on this topic, different papers use different evaluation\nbenchmarks and measures, which makes them hard to compare and hampers future\nprogress. To mitigate these issues, we developed OpenFactCheck, a unified\nframework, with three modules: (i) RESPONSEEVAL, which allows users to easily\ncustomize an automatic fact-checking system and to assess the factuality of all\nclaims in an input document using that system, (ii) LLMEVAL, which assesses the\noverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate\nautomatic fact-checking systems. OpenFactCheck is open-sourced\n(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python\nlibrary (https://pypi.org/project/openfactcheck/) and also as a web service\n(http://app.openfactcheck.com). A video describing the system is available at\nhttps://youtu.be/-i9VKL0HleI.", "published": "2024-08-06 15:49:58", "link": "http://arxiv.org/abs/2408.11832v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection", "abstract": "We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.", "published": "2024-08-06 02:15:12", "link": "http://arxiv.org/abs/2408.02901v3", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy\n  Protection", "abstract": "Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.", "published": "2024-08-06 03:21:13", "link": "http://arxiv.org/abs/2408.02927v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Self-Supervised Learning for Multi-Channel Neural Transducer", "abstract": "Self-supervised learning, such as with the wav2vec 2.0 framework\nsignificantly improves the accuracy of end-to-end automatic speech recognition\n(ASR). Wav2vec 2.0 has been applied to single-channel end-to-end ASR models. In\nthis work, we explored a self-supervised learning method for a multi-channel\nend-to-end ASR model based on the wav2vec 2.0 framework. As the multi-channel\nend-to-end ASR model, we focused on a multi-channel neural transducer. In\npre-training, we compared three different methods for feature quantization to\ntrain a multi-channel conformer audio encoder: joint quantization, feature-wise\nquantization and channel-wise quantization. In fine-tuning, we trained the\nmulti-channel conformer-transducer. All experiments were conducted using the\nfar-field in-house and CHiME-4 datasets. The results of the experiments showed\nthat feature-wise quantization was the most effective among the methods. We\nobserved a 66% relative reduction in character error rate compared with the\nmodel without any pre-training for the far-field in-house dataset.", "published": "2024-08-06 04:12:31", "link": "http://arxiv.org/abs/2408.02945v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "OpenOmni: A Collaborative Open Source Tool for Building Future-Ready\n  Multimodal Conversational Agents", "abstract": "Multimodal conversational agents are highly desirable because they offer\nnatural and human-like interaction. However, there is a lack of comprehensive\nend-to-end solutions to support collaborative development and benchmarking.\nWhile proprietary systems like GPT-4o and Gemini demonstrating impressive\nintegration of audio, video, and text with response times of 200-250ms,\nchallenges remain in balancing latency, accuracy, cost, and data privacy. To\nbetter understand and quantify these issues, we developed OpenOmni, an\nopen-source, end-to-end pipeline benchmarking tool that integrates advanced\ntechnologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented\nGeneration, Large Language Models, along with the ability to integrate\ncustomized models. OpenOmni supports local and cloud deployment, ensuring data\nprivacy and supporting latency and accuracy benchmarking. This flexible\nframework allows researchers to customize the pipeline, focusing on real\nbottlenecks and facilitating rapid proof-of-concept development. OpenOmni can\nsignificantly enhance applications like indoor assistance for visually impaired\nindividuals, advancing human-computer interaction. Our demonstration video is\navailable https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via\nhttps://openomni.ai4wa.com, code is available via\nhttps://github.com/AI4WA/OpenOmniFramework.", "published": "2024-08-06 09:02:53", "link": "http://arxiv.org/abs/2408.03047v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation", "abstract": "Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.", "published": "2024-08-06 16:28:30", "link": "http://arxiv.org/abs/2408.03281v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLaVA-OneVision: Easy Visual Task Transfer", "abstract": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.", "published": "2024-08-06 17:59:44", "link": "http://arxiv.org/abs/2408.03326v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums", "abstract": "Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the performance of an LLM\nsystem built on the OpenAI GPT-3.5-turbo model [8] to extract CTI information.\nTo do so, a random sample of more than 700 daily conversations from three\ncybercrime forums - XSS, Exploit_in, and RAMP - was extracted, and the LLM\nsystem was instructed to summarize the conversations and predict 10 key CTI\nvariables, such as whether a large organization and/or a critical\ninfrastructure is being targeted, with only simple human-language instructions.\nThen, two coders reviewed each conversation and evaluated whether the\ninformation extracted by the LLM was accurate. The LLM system performed well,\nwith an average accuracy score of 96.23%, an average precision of 90% and an\naverage recall of 88.2%. Various ways to enhance the model were uncovered, such\nas the need to help the LLM distinguish between stories and past events, as\nwell as being careful with verb tenses in prompts. Nevertheless, the results of\nthis study highlight the relevance of using LLMs for cyber threat intelligence.", "published": "2024-08-06 09:15:25", "link": "http://arxiv.org/abs/2408.03354v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal\n  Classification", "abstract": "We introduce LAMPO, a novel paradigm that leverages Large Language Models\n(LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike\nconventional methods, which concatenate all demonstration examples with the\ntest instance and prompt LLMs to produce the pointwise prediction, our\nframework uses the LLM as a preference machine that makes a relative\ncomparative decision between the test instance and each demonstration. A\nself-supervised method is then introduced to aggregate these binary comparisons\ninto the final ordinal decision. LAMPO addresses several limitations inherent\nin previous methods, including context length constraints, ordering biases, and\nchallenges associated with absolute point-wise estimation. Extensive\nexperiments on seven public datasets demonstrate LAMPO's remarkably competitive\nperformance across a diverse spectrum of applications (e.g., movie review\nanalysis and hate speech detection). Notably, in certain applications, the\nimprovement can be substantial, exceeding 20% in an absolute term. Moreover, we\nbelieve LAMPO represents an interesting addition to the non-parametric\napplication layered on top of LLMs, as it supports black-box LLMs without\nnecessitating the outputting of LLM's internal states (e.g., embeddings), as\nseen in previous approaches.", "published": "2024-08-06 15:55:05", "link": "http://arxiv.org/abs/2408.03359v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Logistic Regression makes small LLMs strong and explainable\n  \"tens-of-shot\" classifiers", "abstract": "For simple classification tasks, we show that users can benefit from the\nadvantages of using small, local, generative language models instead of large\ncommercial models without a trade-off in performance or introducing extra\nlabelling costs. These advantages, including those around privacy,\navailability, cost, and explainability, are important both in commercial\napplications and in the broader democratisation of AI. Through experiments on\n17 sentence classification tasks (2-4 classes), we show that penalised logistic\nregression on the embeddings from a small LLM equals (and usually betters) the\nperformance of a large LLM in the \"tens-of-shot\" regime. This requires no more\nlabelled instances than are needed to validate the performance of the large\nLLM. Finally, we extract stable and sensible explanations for classification\ndecisions.", "published": "2024-08-06 19:23:42", "link": "http://arxiv.org/abs/2408.03414v2", "categories": ["cs.CL", "cs.LG", "stat.ML", "68T50 (Primary), 62J07 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via\n  Language-Contrastive Decoding (LCD)", "abstract": "Large Vision-Language Models (LVLMs) are an extension of Large Language\nModels (LLMs) that facilitate processing both image and text inputs, expanding\nAI capabilities. However, LVLMs struggle with object hallucinations due to\ntheir reliance on text cues and learned object co-occurrence biases. While most\nresearch quantifies these hallucinations, mitigation strategies are still\nlacking. Our study introduces a Language Contrastive Decoding (LCD) algorithm\nthat adjusts LVLM outputs based on LLM distribution confidence levels,\neffectively reducing object hallucinations. We demonstrate the advantages of\nLCD in leading LVLMs, showing up to %4 improvement in POPE F1 scores and up to\n%36 reduction in CHAIR scores on the COCO validation set, while also improving\ncaptioning quality scores. Our method effectively improves LVLMs without\nneeding complex post-processing or retraining, and is easily applicable to\ndifferent models. Our findings highlight the potential of further exploration\nof LVLM-specific decoding algorithms.", "published": "2024-08-06 08:10:34", "link": "http://arxiv.org/abs/2408.04664v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Non-Determinism of \"Deterministic\" LLM Settings", "abstract": "LLM (large language model) practitioners commonly notice that outputs can\nvary for the same inputs under settings expected to be deterministic. Yet the\nquestions of how pervasive this is, and with what impact on results, have not\nto our knowledge been systematically investigated. We investigate\nnon-determinism in five LLMs configured to be deterministic when applied to\neight common tasks in across 10 runs, in both zero-shot and few-shot settings.\nWe see accuracy variations up to 15% across naturally occurring runs with a gap\nof best possible performance to worst possible performance up to 70%. In fact,\nnone of the LLMs consistently delivers repeatable accuracy across all tasks,\nmuch less identical output strings. Sharing preliminary results with insiders\nhas revealed that non-determinism perhaps essential to the efficient use of\ncompute resources via co-mingled data in input buffers so this issue is not\ngoing away anytime soon. To better quantify our observations, we introduce\nmetrics focused on quantifying determinism, TARr@N for the total agreement rate\nat N runs over raw output, and TARa@N for total agreement rate of parsed-out\nanswers. Our code and data are publicly available at\nhttps://github.com/breckbaldwin/llm-stability.", "published": "2024-08-06 16:43:35", "link": "http://arxiv.org/abs/2408.04667v5", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "GRAFX: An Open-Source Library for Audio Processing Graphs in PyTorch", "abstract": "We present GRAFX, an open-source library designed for handling audio\nprocessing graphs in PyTorch. Along with various library functionalities, we\ndescribe technical details on the efficient parallel computation of input\ngraphs, signals, and processor parameters in GPU. Then, we show its example use\nunder a music mixing scenario, where parameters of every differentiable\nprocessor in a large graph are optimized via gradient descent. The code is\navailable at https://github.com/sh-lee97/grafx.", "published": "2024-08-06 14:03:06", "link": "http://arxiv.org/abs/2408.03204v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhanced Reverberation as Supervision for Unsupervised Speech Separation", "abstract": "Reverberation as supervision (RAS) is a framework that allows for training\nmonaural speech separation models from multi-channel mixtures in an\nunsupervised manner. In RAS, models are trained so that sources predicted from\na mixture at an input channel can be mapped to reconstruct a mixture at a\ntarget channel. However, stable unsupervised training has so far only been\nachieved in over-determined source-channel conditions, leaving the key\ndetermined case unsolved. This work proposes enhanced RAS (ERAS) for solving\nthis problem. Through qualitative analysis, we found that stable training can\nbe achieved by leveraging the loss term to alleviate the frequency-permutation\nproblem. Separation performance is also boosted by adding a novel loss term\nwhere separated signals mapped back to their own input mixture are used as\npseudo-targets for the signals separated from other channels and mapped to the\nsame channel. Experimental results demonstrate high stability and performance\nof ERAS.", "published": "2024-08-06 20:25:09", "link": "http://arxiv.org/abs/2408.03438v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TF-Locoformer: Transformer with Local Modeling by Convolution for Speech\n  Separation and Enhancement", "abstract": "Time-frequency (TF) domain dual-path models achieve high-fidelity speech\nseparation. While some previous state-of-the-art (SoTA) models rely on RNNs,\nthis reliance means they lack the parallelizability, scalability, and\nversatility of Transformer blocks. Given the wide-ranging success of pure\nTransformer-based architectures in other fields, in this work we focus on\nremoving the RNN from TF-domain dual-path models, while maintaining SoTA\nperformance. This work presents TF-Locoformer, a Transformer-based model with\nLOcal-modeling by COnvolution. The model uses feed-forward networks (FFNs) with\nconvolution layers, instead of linear layers, to capture local information,\nletting the self-attention focus on capturing global patterns. We place two\nsuch FFNs before and after self-attention to enhance the local-modeling\ncapability. We also introduce a novel normalization for TF-domain dual-path\nmodels. Experiments on separation and enhancement datasets show that the\nproposed model meets or exceeds SoTA in multiple benchmarks with an RNN-free\narchitecture.", "published": "2024-08-06 20:30:14", "link": "http://arxiv.org/abs/2408.03440v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Central Kurdish Text-to-Speech Synthesis with Novel End-to-End\n  Transformer Training", "abstract": "Recent advancements in text-to-speech (TTS) models have aimed to streamline\nthe two-stage process into a single-stage training approach. However, many\nsingle-stage models still lag behind in audio quality, particularly when\nhandling Kurdish text and speech. There is a critical need to enhance\ntext-to-speech conversion for the Kurdish language, particularly for the Sorani\ndialect, which has been relatively neglected and is underrepresented in recent\ntext-to-speech advancements. This study introduces an end-to-end TTS model for\nefficiently generating high-quality Kurdish audio. The proposed method\nleverages a variational autoencoder (VAE) that is pre-trained for audio\nwaveform reconstruction and is augmented by adversarial training. This involves\naligning the prior distribution established by the pre-trained encoder with the\nposterior distribution of the text encoder within latent variables.\nAdditionally, a stochastic duration predictor is incorporated to imbue\nsynthesized Kurdish speech with diverse rhythms. By aligning latent\ndistributions and integrating the stochastic duration predictor, the proposed\nmethod facilitates the real-time generation of natural Kurdish speech audio,\noffering flexibility in pitches and rhythms. Empirical evaluation via the mean\nopinion score (MOS) on a custom dataset confirms the superior performance of\nour approach (MOS of 3.94) compared with that of a one-stage system and other\ntwo-staged systems as assessed through a subjective human evaluation.", "published": "2024-08-06 07:04:59", "link": "http://arxiv.org/abs/2408.03887v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
