{"title": "Dreaddit: A Reddit Dataset for Stress Analysis in Social Media", "abstract": "Stress is a nigh-universal human experience, particularly in the online\nworld. While stress can be a motivator, too much stress is associated with many\nnegative health outcomes, making its identification useful across a range of\ndomains. However, existing computational research typically only studies stress\nin domains such as speech, or in short genres such as Twitter. We present\nDreaddit, a new text corpus of lengthy multi-domain social media data for the\nidentification of stress. Our dataset consists of 190K posts from five\ndifferent categories of Reddit communities; we additionally label 3.5K total\nsegments taken from 3K posts using Amazon Mechanical Turk. We present\npreliminary supervised learning methods for identifying stress, both neural and\ntraditional, and analyze the complexity and diversity of the data and\ncharacteristics of each category.", "published": "2019-10-31 22:28:45", "link": "http://arxiv.org/abs/1911.00133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implementation of an Index Optimize Technology for Highly Specialized\n  Terms based on the Phonetic Algorithm Metaphone", "abstract": "When compiling databases, for example to meet the needs of healthcare\nestablishments, there is quite a common problem with the introduction and\nfurther processing of names and last names of doctors and patients that are\nhighly specialized both in terms of pronunciation and writing. This is because\nnames and last names of people cannot be unique, their notation is not subject\nto any rules of phonetics, while their length in different languages may not\nmatch. With the advent of the Internet, this situation has become generally\ncritical and can lead to that multiple copies of e-mails are sent to one\naddress. It is possible to solve the specified problem by using phonetic\nalgorithms for comparing words Daitch-Mokotoff, Soundex, NYSIIS, Polyphone, and\nMetaphone, as well as the Levenshtein and Jaro algorithms, Q-gram-based\nalgorithms, which make it possible to find distances between words. The most\nwidespread among them are the Soundex and Metaphone algorithms, which are\ndesigned to index the words based on their sound, taking into consideration the\nrules of pronunciation. By applying the Metaphone algorithm, an attempt has\nbeen made to optimize the phonetic search processes for tasks of fuzzy\ncoincidence, for example, at data deduplication in various databases and\nregistries, in order to reduce the number of errors of incorrect input of last\nnames. An analysis of the most common last names reveals that some of them are\nof the Ukrainian or Russian origin. At the same time, the rules following which\nthe names are pronounced and written, for example in Ukrainian, differ\nradically from basic algorithms for English and differ quite significantly for\nthe Russian language. That is why a phonetic algorithm should take into\nconsideration first of all the peculiarities in the formation of Ukrainian last\nnames, which is of special relevance now.", "published": "2019-10-31 23:54:41", "link": "http://arxiv.org/abs/1911.00152v1", "categories": ["cs.CL", "F.2.2; H.3.3"], "primary_category": "cs.CL"}
{"title": "Transferable End-to-End Aspect-based Sentiment Analysis with Selective\n  Adversarial Learning", "abstract": "Joint extraction of aspects and sentiments can be effectively formulated as a\nsequence labeling problem. However, such formulation hinders the effectiveness\nof supervised methods due to the lack of annotated sequence data in many\ndomains. To address this issue, we firstly explore an unsupervised domain\nadaptation setting for this task. Prior work can only use common syntactic\nrelations between aspect and opinion words to bridge the domain gaps, which\nhighly relies on external linguistic resources. To resolve it, we propose a\nnovel Selective Adversarial Learning (SAL) method to align the inferred\ncorrelation vectors that automatically capture their latent relations. The SAL\nmethod can dynamically learn an alignment weight for each word such that more\nimportant words can possess higher alignment weights to achieve fine-grained\n(word-level) adaptation. Empirically, extensive experiments demonstrate the\neffectiveness of the proposed SAL method.", "published": "2019-10-31 00:22:27", "link": "http://arxiv.org/abs/1910.14192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing the linguistic signal to predict scalar inferences", "abstract": "Pragmatic inferences often subtly depend on the presence or absence of\nlinguistic features. For example, the presence of a partitive construction (of\nthe) increases the strength of a so-called scalar inference: listeners perceive\nthe inference that Chris did not eat all of the cookies to be stronger after\nhearing \"Chris ate some of the cookies\" than after hearing the same utterance\nwithout a partitive, \"Chris ate some cookies.\" In this work, we explore to what\nextent neural network sentence encoders can learn to predict the strength of\nscalar inferences. We first show that an LSTM-based sentence encoder trained on\nan English dataset of human inference strength ratings is able to predict\nratings with high accuracy (r=0.78). We then probe the model's behavior using\nmanually constructed minimal sentence pairs and corpus data. We find that the\nmodel inferred previously established associations between linguistic features\nand inference strength, suggesting that the model learns to use linguistic\nfeatures to predict pragmatic inferences.", "published": "2019-10-31 04:35:17", "link": "http://arxiv.org/abs/1910.14254v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A neural document language modeling framework for spoken document\n  retrieval", "abstract": "Recent developments in deep learning have led to a significant innovation in\nvarious classic and practical subjects, including speech recognition, computer\nvision, question answering, information retrieval and so on. In the context of\nnatural language processing (NLP), language representations have shown giant\nsuccesses in many downstream tasks, so the school of studies have become a\nmajor stream of research recently. Because the immenseness of multimedia data\nalong with speech have spread around the world in our daily life, spoken\ndocument retrieval (SDR) has become an important research subject in the past\ndecades. Targeting on enhancing the SDR performance, the paper concentrates on\nproposing a neural retrieval framework, which assembles the merits of using\nlanguage modeling (LM) mechanism in SDR and leveraging the abstractive\ninformation learned by the language representation models. Consequently, to our\nknowledge, this is a pioneer study on supervised training of a neural LM-based\nSDR framework, especially combined with the pretrained language representation\nmethods.", "published": "2019-10-31 07:50:41", "link": "http://arxiv.org/abs/1910.14286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Customize Model Structures for Few-shot Dialogue Generation\n  Tasks", "abstract": "Training the generative models with minimal corpus is one of the critical\nchallenges for building open-domain dialogue systems. Existing methods tend to\nuse the meta-learning framework which pre-trains the parameters on all\nnon-target tasks then fine-tunes on the target task. However, fine-tuning\ndistinguishes tasks from the parameter perspective but ignores the\nmodel-structure perspective, resulting in similar dialogue models for different\ntasks. In this paper, we propose an algorithm that can customize a unique\ndialogue model for each task in the few-shot setting. In our approach, each\ndialogue model consists of a shared module, a gating module, and a private\nmodule. The first two modules are shared among all the tasks, while the third\none will differentiate into different network structures to better capture the\ncharacteristics of the corresponding task. The extensive experiments on two\ndatasets show that our method outperforms all the baselines in terms of task\nconsistency, response quality, and diversity.", "published": "2019-10-31 09:25:34", "link": "http://arxiv.org/abs/1910.14326v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Multi-hop Readers Dream of Reasoning Chains?", "abstract": "General Question Answering (QA) systems over texts require the multi-hop\nreasoning capability, i.e. the ability to reason with information collected\nfrom multiple passages to derive the answer. In this paper we conduct a\nsystematic analysis to assess such an ability of various existing models\nproposed for multi-hop QA tasks. Specifically, our analysis investigates that\nwhether providing the full reasoning chain of multiple passages, instead of\njust one final passage where the answer appears, could improve the performance\nof the existing QA models. Surprisingly, when using the additional evidence\npassages, the improvements of all the existing multi-hop reading approaches are\nrather limited, with the highest error reduction of 5.8% on F1 (corresponding\nto 1.3% absolute improvement) from the BERT model.\n  To better understand whether the reasoning chains could indeed help find\ncorrect answers, we further develop a co-matching-based method that leads to\n13.1% error reduction with passage chains when applied to two of our base\nreaders (including BERT). Our results demonstrate the existence of the\npotential improvement using explicit multi-hop reasoning and the necessity to\ndevelop models with better reasoning abilities.", "published": "2019-10-31 15:02:49", "link": "http://arxiv.org/abs/1910.14520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-level Neural Machine Translation with Associated Memory Network", "abstract": "Standard neural machine translation (NMT) is on the assumption that the\ndocument-level context is independent. Most existing document-level NMT\napproaches are satisfied with a smattering sense of global document-level\ninformation, while this work focuses on exploiting detailed document-level\ncontext in terms of a memory network. The capacity of the memory network that\ndetecting the most relevant part of the current sentence from memory renders a\nnatural solution to model the rich document-level context. In this work, the\nproposed document-aware memory network is implemented to enhance the\nTransformer NMT baseline. Experiments on several tasks show that the proposed\nmethod significantly improves the NMT performance over strong Transformer\nbaselines and other related studies.", "published": "2019-10-31 15:14:54", "link": "http://arxiv.org/abs/1910.14528v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention Is All You Need for Chinese Word Segmentation", "abstract": "Taking greedy decoding algorithm as it should be, this work focuses on\nfurther strengthening the model itself for Chinese word segmentation (CWS),\nwhich results in an even more fast and more accurate CWS model. Our model\nconsists of an attention only stacked encoder and a light enough decoder for\nthe greedy segmentation plus two highway connections for smoother training, in\nwhich the encoder is composed of a newly proposed Transformer variant,\nGaussian-masked Directional (GD) Transformer, and a biaffine attention scorer.\nWith the effective encoder design, our model only needs to take unigram\nfeatures for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark\ndatasets. The experimental results show that with the highest segmentation\nspeed, the proposed model achieves new state-of-the-art or comparable\nperformance against strong baselines in terms of strict closed test setting.", "published": "2019-10-31 15:32:19", "link": "http://arxiv.org/abs/1910.14537v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Naver Labs Europe's Systems for the Document-Level Generation and\n  Translation Task at WNGT 2019", "abstract": "Recently, neural models led to significant improvements in both machine\ntranslation (MT) and natural language generation tasks (NLG). However,\ngeneration of long descriptive summaries conditioned on structured data remains\nan open challenge. Likewise, MT that goes beyond sentence-level context is\nstill an open issue (e.g., document-level MT or MT with metadata). To address\nthese challenges, we propose to leverage data from both tasks and do transfer\nlearning between MT, NLG, and MT with source-side metadata (MT+NLG). First, we\ntrain document-based MT systems with large amounts of parallel data. Then, we\nadapt these models to pure NLG and MT+NLG tasks by fine-tuning with smaller\namounts of domain-specific data. This end-to-end NLG approach, without data\nselection and planning, outperforms the previous state of the art on the\nRotowire NLG task. We participated to the \"Document Generation and Translation\"\ntask at WNGT 2019, and ranked first in all tracks.", "published": "2019-10-31 15:34:48", "link": "http://arxiv.org/abs/1910.14539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation of Restaurant Reviews: New Corpus for Domain\n  Adaptation and Robustness", "abstract": "We share a French-English parallel corpus of Foursquare restaurant reviews\n(https://europe.naverlabs.com/research/natural-language-processing/machine-translation-of-restaurant-reviews),\nand define a new task to encourage research on Neural Machine Translation\nrobustness and domain adaptation, in a real-world scenario where better-quality\nMT would be greatly beneficial. We discuss the challenges of such\nuser-generated content, and train good baseline models that build upon the\nlatest techniques for MT robustness. We also perform an extensive evaluation\n(automatic and human) that shows significant improvements over existing online\nsystems. Finally, we propose task-specific metrics based on sentiment analysis\nor translation accuracy of domain-specific polysemous words.", "published": "2019-10-31 16:42:50", "link": "http://arxiv.org/abs/1910.14589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hidden State Guidance: Improving Image Captioning using An Image\n  Conditioned Autoencoder", "abstract": "Most RNN-based image captioning models receive supervision on the output\nwords to mimic human captions. Therefore, the hidden states can only receive\nnoisy gradient signals via layers of back-propagation through time, leading to\nless accurate generated captions. Consequently, we propose a novel framework,\nHidden State Guidance (HSG), that matches the hidden states in the caption\ndecoder to those in a teacher decoder trained on an easier task of autoencoding\nthe captions conditioned on the image. During training with the REINFORCE\nalgorithm, the conventional rewards are sentence-based evaluation metrics\nequally distributed to each generated word, no matter their relevance. HSG\nprovides a word-level reward that helps the model learn better hidden\nrepresentations. Experimental results demonstrate that HSG clearly outperforms\nvarious state-of-the-art caption decoders using either raw images or detected\nobjects as inputs.", "published": "2019-10-31 01:56:33", "link": "http://arxiv.org/abs/1910.14208v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cascaded LSTMs based Deep Reinforcement Learning for Goal-driven\n  Dialogue", "abstract": "This paper proposes a deep neural network model for joint modeling Natural\nLanguage Understanding (NLU) and Dialogue Management (DM) in goal-driven\ndialogue systems. There are three parts in this model. A Long Short-Term Memory\n(LSTM) at the bottom of the network encodes utterances in each dialogue turn\ninto a turn embedding. Dialogue embeddings are learned by a LSTM at the middle\nof the network, and updated by the feeding of all turn embeddings. The top part\nis a forward Deep Neural Network which converts dialogue embeddings into the\nQ-values of different dialogue actions. The cascaded LSTMs based reinforcement\nlearning network is jointly optimized by making use of the rewards received at\neach dialogue turn as the only supervision information. There is no explicit\nNLU and dialogue states in the network. Experimental results show that our\nmodel outperforms both traditional Markov Decision Process (MDP) model and\nsingle LSTM with Deep Q-Network on meeting room booking tasks. Visualization of\ndialogue embeddings illustrates that the model can learn the representation of\ndialogue states.", "published": "2019-10-31 03:08:10", "link": "http://arxiv.org/abs/1910.14229v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiaNet: BERT and Hierarchical Attention Multi-Task Learning of\n  Fine-Grained Dialect", "abstract": "Prediction of language varieties and dialects is an important language\nprocessing task, with a wide range of applications. For Arabic, the native\ntongue of ~ 300 million people, most varieties remain unsupported. To ease this\nbottleneck, we present a very large scale dataset covering 319 cities from all\n21 Arab countries. We introduce a hierarchical attention multi-task learning\n(HA-MTL) approach for dialect identification exploiting our data at the city,\nstate, and country levels. We also evaluate use of BERT on the three tasks,\ncomparing it to the MTL approach. We benchmark and release our data and models.", "published": "2019-10-31 03:56:32", "link": "http://arxiv.org/abs/1910.14243v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter Sharing Decoder Pair for Auto Composing", "abstract": "Auto Composing is an active and appealing research area in the past few\nyears, and lots of efforts have been put into inventing more robust models to\nsolve this problem. With the fast evolution of deep learning techniques, some\ndeep neural network-based language models are becoming dominant. Notably, the\ntransformer structure has been proven to be very efficient and promising in\nmodeling texts. However, the transformer-based language models usually contain\nhuge number of parameters and the size of the model is usually too large to put\nin production for some storage limited applications. In this paper, we propose\na parameter sharing decoder pair (PSDP), which reduces the number of parameters\ndramatically and at the same time maintains the capability of generating\nunderstandable and reasonable compositions. Works created by the proposed model\nare presented to demonstrate the effectiveness of the model.", "published": "2019-10-31 06:06:56", "link": "http://arxiv.org/abs/1910.14270v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LIMIT-BERT : Linguistic Informed Multi-Task BERT", "abstract": "In this paper, we present a Linguistic Informed Multi-Task BERT (LIMIT-BERT)\nfor learning language representations across multiple linguistic tasks by\nMulti-Task Learning (MTL). LIMIT-BERT includes five key linguistic syntax and\nsemantics tasks: Part-Of-Speech (POS) tags, constituent and dependency\nsyntactic parsing, span and dependency semantic role labeling (SRL). Besides,\nLIMIT-BERT adopts linguistics mask strategy: Syntactic and Semantic Phrase\nMasking which mask all of the tokens corresponding to a syntactic/semantic\nphrase. Different from recent Multi-Task Deep Neural Networks (MT-DNN) (Liu et\nal., 2019), our LIMIT-BERT is linguistically motivated and learning in a\nsemi-supervised method which provides large amounts of linguistic-task data as\nsame as BERT learning corpus. As a result, LIMIT-BERT not only improves\nlinguistic tasks performance but also benefits from a regularization effect and\nlinguistic information that leads to more general representations to help adapt\nto new tasks and domains. LIMIT-BERT obtains new state-of-the-art or\ncompetitive results on both span and dependency semantic parsing on Propbank\nbenchmarks and both dependency and constituent syntactic parsing on Penn\nTreebank.", "published": "2019-10-31 08:14:51", "link": "http://arxiv.org/abs/1910.14296v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Great New Design: How Do We Talk about Media Architecture in Social\n  Media", "abstract": "In social media, we communicate through pictures, videos, short codes, links,\npartial phrases. It is a rich, and digitally documented communication channel\nthat relies on a multitude of media and forms. These channels are sorted by\nalgorithms as organizers of discourse, mostly with the goal of channeling\nattention. In this research, we used Twitter to study the way Media\nArchitecture is discussed within the community of architects, designers,\nresearchers and policy makers. We look at the way they spontaneously share\nopinions on their engagement with digital infrastructures, networked places and\nhybrid public spaces. What can we do with all those opinions? We propose here\nthe use of text-mining and machine learning techniques to identify important\nconcepts and patterns in this prolific communication stream. We discuss how\nsuch techniques could inform the practice and emergence of future trends.", "published": "2019-10-31 11:49:32", "link": "http://arxiv.org/abs/1910.14395v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Probabilistic Bias Mitigation in Word Embeddings", "abstract": "It has been shown that word embeddings derived from large corpora tend to\nincorporate biases present in their training data. Various methods for\nmitigating these biases have been proposed, but recent work has demonstrated\nthat these methods hide but fail to truly remove the biases, which can still be\nobserved in word nearest-neighbor statistics. In this work we propose a\nprobabilistic view of word embedding bias. We leverage this framework to\npresent a novel method for mitigating bias which relies on probabilistic\nobservations to yield a more robust bias mitigation algorithm. We demonstrate\nthat this method effectively reduces bias according to three separate measures\nof bias while maintaining embedding quality across various popular benchmark\nsemantic tasks", "published": "2019-10-31 14:34:14", "link": "http://arxiv.org/abs/1910.14497v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Positional Attention-based Frame Identification with BERT: A Deep\n  Learning Approach to Target Disambiguation and Semantic Frame Selection", "abstract": "Semantic parsing is the task of transforming sentences from natural language\ninto formal representations of predicate-argument structures. Under this\nresearch area, frame-semantic parsing has attracted much interest. This parsing\napproach leverages the lexical information defined in FrameNet to associate\nmarked predicates or targets with semantic frames, thereby assigning semantic\nroles to sentence components based on pre-specified frame elements in FrameNet.\nIn this paper, a deep neural network architecture known as Positional\nAttention-based Frame Identification with BERT (PAFIBERT) is presented as a\nsolution to the frame identification subtask in frame-semantic parsing.\nAlthough the importance of this subtask is well-established, prior research has\nyet to find a robust solution that works satisfactorily for both in-domain and\nout-of-domain data. This study thus set out to improve frame identification in\nlight of recent advancements of language modeling and transfer learning in\nnatural language processing. The proposed method is partially empowered by\nBERT, a pre-trained language model that excels at capturing contextual\ninformation in texts. By combining the language representation power of BERT\nwith a position-based attention mechanism, PAFIBERT is able to attend to\ntarget-specific contexts in sentences for disambiguating targets and\nassociating them with the most suitable semantic frames. Under various\nexperimental settings, PAFIBERT outperformed existing solutions by a\nsignificant margin, achieving new state-of-the-art results for both in-domain\nand out-of-domain benchmark test sets.", "published": "2019-10-31 15:51:04", "link": "http://arxiv.org/abs/1910.14549v1", "categories": ["cs.CL", "cs.LG", "I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "abstract": "We introduce a new large-scale NLI benchmark dataset, collected via an\niterative, adversarial human-and-model-in-the-loop procedure. We show that\ntraining models on this new dataset leads to state-of-the-art performance on a\nvariety of popular NLI benchmarks, while posing a more difficult challenge with\nits new test set. Our analysis sheds light on the shortcomings of current\nstate-of-the-art models, and shows that non-expert annotators are successful at\nfinding their weaknesses. The data collection method can be applied in a\nnever-ending learning scenario, becoming a moving target for NLU, rather than a\nstatic benchmark that will quickly saturate.", "published": "2019-10-31 16:50:43", "link": "http://arxiv.org/abs/1910.14599v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Cross-Lingual Relation Extraction Based on Bilingual Word\n  Embedding Mapping", "abstract": "Relation extraction (RE) seeks to detect and classify semantic relationships\nbetween entities, which provides useful information for many NLP applications.\nSince the state-of-the-art RE models require large amounts of manually\nannotated data and language-specific resources to achieve high accuracy, it is\nvery challenging to transfer an RE model of a resource-rich language to a\nresource-poor language. In this paper, we propose a new approach for\ncross-lingual RE model transfer based on bilingual word embedding mapping. It\nprojects word embeddings from a target language to a source language, so that a\nwell-trained source-language neural network RE model can be directly applied to\nthe target language. Experiment results show that the proposed approach\nachieves very good performance for a number of target languages on both\nin-house and open datasets, using a small bilingual dictionary with only 1K\nword pairs.", "published": "2019-10-31 19:30:54", "link": "http://arxiv.org/abs/1911.00069v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Narration-based Reward Shaping Approach using Grounded Natural\n  Language Commands", "abstract": "While deep reinforcement learning techniques have led to agents that are\nsuccessfully able to learn to perform a number of tasks that had been\npreviously unlearnable, these techniques are still susceptible to the\nlongstanding problem of reward sparsity. This is especially true for tasks such\nas training an agent to play StarCraft II, a real-time strategy game where\nreward is only given at the end of a game which is usually very long. While\nthis problem can be addressed through reward shaping, such approaches typically\nrequire a human expert with specialized knowledge. Inspired by the vision of\nenabling reward shaping through the more-accessible paradigm of\nnatural-language narration, we develop a technique that can provide the\nbenefits of reward shaping using natural language commands. Our\nnarration-guided RL agent projects sequences of natural-language commands into\nthe same high-dimensional representation space as corresponding goal states. We\nshow that we can get improved performance with our method compared to\ntraditional reward-shaping approaches. Additionally, we demonstrate the ability\nof our method to generalize to unseen natural-language commands.", "published": "2019-10-31 22:37:54", "link": "http://arxiv.org/abs/1911.00497v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Human-centric Metric for Accelerating Pathology Reports Annotation", "abstract": "Pathology reports contain useful information such as the main involved organ,\ndiagnosis, etc. These information can be identified from the free text reports\nand used for large-scale statistical analysis or serve as annotation for other\nmodalities such as pathology slides images. However, manual classification for\na huge number of reports on multiple tasks is labor-intensive. In this paper,\nwe have developed an automatic text classifier based on BERT and we propose a\nhuman-centric metric to evaluate the model. According to the model confidence,\nwe identify low-confidence cases that require further expert annotation and\nhigh-confidence cases that are automatically classified. We report the\npercentage of low-confidence cases and the performance of automatically\nclassified cases. On the high-confidence cases, the model achieves\nclassification accuracy comparable to pathologists. This leads a potential of\nreducing 80% to 98% of the manual annotation workload.", "published": "2019-10-31 22:09:19", "link": "http://arxiv.org/abs/1911.01226v2", "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "CN-CELEB: a challenging Chinese speaker recognition dataset", "abstract": "Recently, researchers set an ambitious goal of conducting speaker recognition\nin unconstrained conditions where the variations on ambient, channel and\nemotion could be arbitrary. However, most publicly available datasets are\ncollected under constrained environments, i.e., with little noise and limited\nchannel variation. These datasets tend to deliver over optimistic performance\nand do not meet the request of research on speaker recognition in unconstrained\nconditions. In this paper, we present CN-Celeb, a large-scale speaker\nrecognition dataset collected `in the wild'. This dataset contains more than\n130,000 utterances from 1,000 Chinese celebrities, and covers 11 different\ngenres in real world. Experiments conducted with two state-of-the-art speaker\nrecognition approaches (i-vector and x-vector) show that the performance on\nCN-Celeb is far inferior to the one obtained on VoxCeleb, a widely used speaker\nrecognition dataset. This result demonstrates that in real-life conditions, the\nperformance of existing techniques might be much worse than it was thought. Our\ndatabase is free for researchers and can be downloaded from\nhttp://project.cslt.org.", "published": "2019-10-31 08:25:45", "link": "http://arxiv.org/abs/1911.01799v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transfer Learning from Transformers to Fake News Challenge Stance\n  Detection (FNC-1) Task", "abstract": "In this paper, we report improved results of the Fake News Challenge Stage 1\n(FNC-1) stance detection task. This gain in performance is due to the\ngeneralization power of large language models based on Transformer\narchitecture, invented, trained and publicly released over the last two years.\nSpecifically (1) we improved the FNC-1 best performing model adding BERT\nsentence embedding of input sequences as a model feature, (2) we fine-tuned\nBERT, XLNet, and RoBERTa transformers on FNC-1 extended dataset and obtained\nstate-of-the-art results on FNC-1 task.", "published": "2019-10-31 10:32:43", "link": "http://arxiv.org/abs/1910.14353v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Multi-scale Octave Convolutions for Robust Speech Recognition", "abstract": "We propose a multi-scale octave convolution layer to learn robust speech\nrepresentations efficiently. Octave convolutions were introduced by Chen et al\n[1] in the computer vision field to reduce the spatial redundancy of the\nfeature maps by decomposing the output of a convolutional layer into feature\nmaps at two different spatial resolutions, one octave apart. This approach\nimproved the efficiency as well as the accuracy of the CNN models. The accuracy\ngain was attributed to the enlargement of the receptive field in the original\ninput space. We argue that octave convolutions likewise improve the robustness\nof learned representations due to the use of average pooling in the lower\nresolution group, acting as a low-pass filter. We test this hypothesis by\nevaluating on two noisy speech corpora - Aurora-4 and AMI. We extend the octave\nconvolution concept to multiple resolution groups and multiple octaves. To\nevaluate the robustness of the inferred representations, we report the\nsimilarity between clean and noisy encodings using an affine projection loss as\na proxy robustness measure. The results show that proposed method reduces the\nWER by up to 6.6% relative for Aurora-4 and 3.6% for AMI, while improving the\ncomputational efficiency of the CNN acoustic models.", "published": "2019-10-31 13:15:24", "link": "http://arxiv.org/abs/1910.14443v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "What Question Answering can Learn from Trivia Nerds", "abstract": "In addition to the traditional task of getting machines to answer questions,\na major research question in question answering is to create interesting,\nchallenging questions that can help systems learn how to answer questions and\nalso reveal which systems are the best at answering questions. We argue that\ncreating a question answering dataset -- and the ubiquitous leaderboard that\ngoes with it -- closely resembles running a trivia tournament: you write\nquestions, have agents (either humans or machines) answer the questions, and\ndeclare a winner. However, the research community has ignored the decades of\nhard-learned lessons from decades of the trivia community creating vibrant,\nfair, and effective question answering competitions. After detailing problems\nwith existing QA datasets, we outline the key lessons -- removing ambiguity,\ndiscriminating skill, and adjudicating disputes -- that can transfer to QA\nresearch and how they might be implemented for the QA community.", "published": "2019-10-31 13:38:01", "link": "http://arxiv.org/abs/1910.14464v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can adversarial training learn image captioning ?", "abstract": "Recently, generative adversarial networks (GAN) have gathered a lot of\ninterest. Their efficiency in generating unseen samples of high quality,\nespecially images, has improved over the years. In the field of Natural\nLanguage Generation (NLG), the use of the adversarial setting to generate\nmeaningful sentences has shown to be difficult for two reasons: the lack of\nexisting architectures to produce realistic sentences and the lack of\nevaluation tools. In this paper, we propose an adversarial architecture related\nto the conditional GAN (cGAN) that generates sentences according to a given\nimage (also called image captioning). This attempt is the first that uses no\npre-training or reinforcement methods. We also explain why our experiment\nsettings can be safely evaluated and interpreted for further works.", "published": "2019-10-31 16:59:14", "link": "http://arxiv.org/abs/1910.14609v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Assistant: Joint Action Prediction, Response Generation, and\n  Latent Knowledge Reasoning", "abstract": "Task-oriented dialog presents a difficult challenge encompassing multiple\nproblems including multi-turn language understanding and generation, knowledge\nretrieval and reasoning, and action prediction. Modern dialog systems typically\nbegin by converting conversation history to a symbolic object referred to as\nbelief state by using supervised learning. The belief state is then used to\nreason on an external knowledge source whose result along with the conversation\nhistory is used in action prediction and response generation tasks\nindependently. Such a pipeline of individually optimized components not only\nmakes the development process cumbersome but also makes it non-trivial to\nleverage session-level user reinforcement signals. In this paper, we develop\nNeural Assistant: a single neural network model that takes conversation history\nand an external knowledge source as input and jointly produces both text\nresponse and action to be taken by the system as output. The model learns to\nreason on the provided knowledge source with weak supervision signal coming\nfrom the text generation and the action prediction tasks, hence removing the\nneed for belief state annotations. In the MultiWOZ dataset, we study the effect\nof distant supervision, and the size of knowledge base on model performance. We\nfind that the Neural Assistant without belief states is able to incorporate\nexternal knowledge information achieving higher factual accuracy scores\ncompared to Transformer. In settings comparable to reported baseline systems,\nNeural Assistant when provided with oracle belief state significantly improves\nlanguage generation performance.", "published": "2019-10-31 17:01:24", "link": "http://arxiv.org/abs/1910.14613v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Masked Language Model Scoring", "abstract": "Pretrained masked language models (MLMs) require finetuning for most NLP\ntasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood\nscores (PLLs), which are computed by masking tokens one by one. We show that\nPLLs outperform scores from autoregressive language models like GPT-2 in a\nvariety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an\nend-to-end LibriSpeech model's WER by 30% relative and adds up to +1.7 BLEU on\nstate-of-the-art baselines for low-resource translation pairs, with further\ngains from domain adaptation. We attribute this success to PLL's unsupervised\nexpression of linguistic acceptability without a left-to-right bias, greatly\nimproving on scores from GPT-2 (+10 points on island effects, NPI licensing in\nBLiMP). One can finetune MLMs to give scores without masking, enabling\ncomputation in a single inference pass. In all, PLLs and their associated\npseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of\npretrained MLMs; e.g., we use a single cross-lingual model to rescore\ntranslations in multiple languages. We release our library for language model\nscoring at https://github.com/awslabs/mlm-scoring.", "published": "2019-10-31 17:51:21", "link": "http://arxiv.org/abs/1910.14659v3", "categories": ["cs.CL", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "TAB-VCR: Tags and Attributes based Visual Commonsense Reasoning\n  Baselines", "abstract": "Reasoning is an important ability that we learn from a very early age. Yet,\nreasoning is extremely hard for algorithms. Despite impressive recent progress\nthat has been reported on tasks that necessitate reasoning, such as visual\nquestion answering and visual dialog, models often exploit biases in datasets.\nTo develop models with better reasoning abilities, recently, the new visual\ncommonsense reasoning (VCR) task has been introduced. Not only do models have\nto answer questions, but also do they have to provide a reason for the given\nanswer. The proposed baseline achieved compelling results, leveraging a\nmeticulously designed model composed of LSTM modules and attention nets. Here\nwe show that a much simpler model obtained by ablating and pruning the existing\nintricate baseline can perform better with half the number of trainable\nparameters. By associating visual features with attribute information and\nbetter text to image grounding, we obtain further improvements for our simpler\n& effective baseline, TAB-VCR. We show that this approach results in a 5.3%,\n4.4% and 6.5% absolute improvement over the previous state-of-the-art on\nquestion answering, answer justification and holistic VCR.", "published": "2019-10-31 17:59:57", "link": "http://arxiv.org/abs/1910.14671v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Modeling of Rakugo Speech and Its Limitations: Toward Speech Synthesis\n  That Entertains Audiences", "abstract": "We have been investigating rakugo speech synthesis as a challenging example\nof speech synthesis that entertains audiences. Rakugo is a traditional Japanese\nform of verbal entertainment similar to a combination of one-person stand-up\ncomedy and comic storytelling and is popular even today. In rakugo, a performer\nplays multiple characters, and conversations or dialogues between the\ncharacters make the story progress. To investigate how close the quality of\nsynthesized rakugo speech can approach that of professionals' speech, we\nmodeled rakugo speech using Tacotron 2, a state-of-the-art speech synthesis\nsystem that can produce speech that sounds as natural as human speech albeit\nunder limited conditions, and an enhanced version of it with self-attention to\nbetter consider long-term dependencies. We also used global style tokens and\nmanually labeled context features to enrich speaking styles. Through a\nlistening test, we measured not only naturalness but also distinguishability of\ncharacters, understandability of the content, and the degree of entertainment.\nAlthough we found that the speech synthesis models could not yet reach the\nprofessional level, the results of the listening test provided interesting\ninsights: 1) we should not focus only on the naturalness of synthesized speech\nbut also the distinguishability of characters and the understandability of the\ncontent to further entertain audiences; 2) the fundamental frequency (fo)\nexpressions of synthesized speech are poorer than those of human speech, and\nmore entertaining speech should have richer fo expression. Although there is\nroom for improvement, we believe this is an important stepping stone toward\nachieving entertaining speech synthesis at the professional level.", "published": "2019-10-31 22:36:25", "link": "http://arxiv.org/abs/1911.00137v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-to-end Non-Negative Autoencoders for Sound Source Separation", "abstract": "Discriminative models for source separation have recently been shown to\nproduce impressive results. However, when operating on sources outside of the\ntraining set, these models can not perform as well and are cumbersome to\nupdate. Classical methods like Non-negative Matrix Factorization (NMF) provide\nmodular approaches to source separation that can be easily updated to adapt to\nnew mixture scenarios. In this paper, we generalize NMF to develop end-to-end\nnon-negative auto-encoders and demonstrate how they can be used for source\nseparation. Our experiments indicate that these models deliver comparable\nseparation performance to discriminative approaches, while retaining the\nmodularity of NMF and the modeling flexibility of neural networks.", "published": "2019-10-31 20:55:08", "link": "http://arxiv.org/abs/1911.00102v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "W-Net BF: DNN-based Beamformer Using Joint Training Approach", "abstract": "Acoustic beamformers have been widely used to enhance audio signals. The best\ncurrent methods are DNN-powered variants of the generalized eigenvalue\nbeamformer, and DNN-based filterestimation methods that directly compute\nbeamforming filters. Both approaches, while effective, have blindspots in their\ngeneralizability. We propose a novel approach that combines both approaches\ninto a single framework that attempts to exploit the best features of both. The\nresulting model, called a W-Net beamformer, includes two components: the first\ncomputes a noise-masked reference which the second uses to estimate beamforming\nfilters. Results on data that include a wide variety of room and noise\nconditions, including static and mobile noise sources, show that the proposed\nbeamformer outperforms other methods in all tested evaluation metrics.", "published": "2019-10-31 04:57:50", "link": "http://arxiv.org/abs/1910.14262v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A comparative study of estimating articulatory movements from phoneme\n  sequences and acoustic features", "abstract": "Unlike phoneme sequences, movements of speech articulators (lips, tongue,\njaw, velum) and the resultant acoustic signal are known to encode not only the\nlinguistic message but also carry para-linguistic information. While several\nworks exist for estimating articulatory movement from acoustic signals, little\nis known to what extent articulatory movements can be predicted only from\nlinguistic information, i.e., phoneme sequence. In this work, we estimate\narticulatory movements from three different input representations: R1) acoustic\nsignal, R2) phoneme sequence, R3) phoneme sequence with timing information.\nWhile an attention network is used for estimating articulatory movement in the\ncase of R2, BLSTM network is used for R1 and R3. Experiments with ten subjects'\nacoustic-articulatory data reveal that the estimation techniques achieve an\naverage correlation coefficient of 0.85, 0.81, and 0.81 in the case of R1, R2,\nand R3 respectively. This indicates that attention network, although uses only\nphoneme sequence (R2) without any timing information, results in an estimation\nperformance similar to that using rich acoustic signal (R1), suggesting that\narticulatory motion is primarily driven by the linguistic message. The\ncorrelation coefficient is further improved to 0.88 when R1 and R3 are used\ntogether for estimating articulatory movements.", "published": "2019-10-31 11:13:44", "link": "http://arxiv.org/abs/1910.14375v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Modeling Feature Representations for Affective Speech using Generative\n  Adversarial Networks", "abstract": "Emotion recognition is a classic field of research with a typical setup\nextracting features and feeding them through a classifier for prediction. On\nthe other hand, generative models jointly capture the distributional\nrelationship between emotions and the feature profiles. Relatively recently,\nGenerative Adversarial Networks (GANs) have surfaced as a new class of\ngenerative models and have shown considerable success in modeling distributions\nin the fields of computer vision and natural language understanding. In this\nwork, we experiment with variants of GAN architectures to generate feature\nvectors corresponding to an emotion in two ways: (i) A generator is trained\nwith samples from a mixture prior. Each mixture component corresponds to an\nemotional class and can be sampled to generate features from the corresponding\nemotion. (ii) A one-hot vector corresponding to an emotion can be explicitly\nused to generate the features. We perform analysis on such models and also\npropose different metrics used to measure the performance of the GAN models in\ntheir ability to generate realistic synthetic samples. Apart from evaluation on\na given dataset of interest, we perform a cross-corpus study where we study the\nutility of the synthetic samples as additional training data in low resource\nconditions.", "published": "2019-10-31 18:09:45", "link": "http://arxiv.org/abs/1911.00030v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Reducing audio membership inference attack accuracy to chance: 4\n  defenses", "abstract": "It is critical to understand the privacy and robustness vulnerabilities of\nmachine learning models, as their implementation expands in scope. In\nmembership inference attacks, adversaries can determine whether a particular\nset of data was used in training, putting the privacy of the data at risk.\nExisting work has mostly focused on image related tasks; we generalize this\ntype of attack to speaker identification on audio samples. We demonstrate\nattack precision of 85.9\\% and recall of 90.8\\% for LibriSpeech, and 78.3\\%\nprecision and 90.7\\% recall for VOiCES (Voices Obscured in Complex\nEnvironmental Settings). We find that implementing defenses such as prediction\nobfuscation, defensive distillation or adversarial training, can reduce attack\naccuracy to chance.", "published": "2019-10-31 21:18:40", "link": "http://arxiv.org/abs/1911.01888v1", "categories": ["cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.CR"}
