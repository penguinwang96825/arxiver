{"title": "AutoSense Model for Word Sense Induction", "abstract": "Word sense induction (WSI), or the task of automatically discovering multiple\nsenses or meanings of a word, has three main challenges: domain adaptability,\nnovel sense detection, and sense granularity flexibility. While current latent\nvariable models are known to solve the first two challenges, they are not\nflexible to different word sense granularities, which differ very much among\nwords, from aardvark with one sense, to play with over 50 senses. Current\nmodels either require hyperparameter tuning or nonparametric induction of the\nnumber of senses, which we find both to be ineffective. Thus, we aim to\neliminate these requirements and solve the sense granularity problem by\nproposing AutoSense, a latent variable model based on two observations: (1)\nsenses are represented as a distribution over topics, and (2) senses generate\npairings between the target word and its neighboring word. These observations\nalleviate the problem by (a) throwing garbage senses and (b) additionally\ninducing fine-grained word senses. Results show great improvements over the\nstate-of-the-art models on popular WSI datasets. We also show that AutoSense is\nable to learn the appropriate sense granularity of a word. Finally, we apply\nAutoSense to the unsupervised author name disambiguation task where the sense\ngranularity problem is more evident and show that AutoSense is evidently better\nthan competing models. We share our data and code here:\nhttps://github.com/rktamplayo/AutoSense.", "published": "2018-11-22 17:19:31", "link": "http://arxiv.org/abs/1811.09242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Running on Fumes--Preventing Out-of-Gas Vulnerabilities in Ethereum\n  Smart Contracts using Static Resource Analysis", "abstract": "Gas is a measurement unit of the computational effort that it will take to\nexecute every single operation that takes part in the Ethereum blockchain\nplatform. Each instruction executed by the Ethereum Virtual Machine (EVM) has\nan associated gas consumption specified by Ethereum. If a transaction exceeds\nthe amount of gas allotted by the user (known as gas limit), an out-of-gas\nexception is raised. There is a wide family of contract vulnerabilities due to\nout-of-gas behaviours. We report on the design and implementation of GASTAP, a\nGas-Aware Smart contracT Analysis Platform, which takes as input a smart\ncontract (either in EVM, disassembled EVM, or in Solidity source code) and\nautomatically infers sound gas upper bounds for all its public functions. Our\nbounds ensure that if the gas limit paid by the user is higher than our\ninferred gas bounds, the contract is free of out-of-gas vulnerabilities.", "published": "2018-11-22 13:19:07", "link": "http://arxiv.org/abs/1811.10403v2", "categories": ["cs.PL", "cs.CL"], "primary_category": "cs.PL"}
{"title": "Deep Learning Based Phase Reconstruction for Speaker Separation: A\n  Trigonometric Perspective", "abstract": "This study investigates phase reconstruction for deep learning based monaural\ntalker-independent speaker separation in the short-time Fourier transform\n(STFT) domain. The key observation is that, for a mixture of two sources, with\ntheir magnitudes accurately estimated and under a geometric constraint, the\nabsolute phase difference between each source and the mixture can be uniquely\ndetermined; in addition, the source phases at each time-frequency (T-F) unit\ncan be narrowed down to only two candidates. To pick the right candidate, we\npropose three algorithms based on iterative phase reconstruction, group delay\nestimation, and phase-difference sign prediction. State-of-the-art results are\nobtained on the publicly available wsj0-2mix and 3mix corpus.", "published": "2018-11-22 03:46:44", "link": "http://arxiv.org/abs/1811.09010v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Bytes are All You Need: End-to-End Multilingual Speech Recognition and\n  Synthesis with Bytes", "abstract": "We present two end-to-end models: Audio-to-Byte (A2B) and Byte-to-Audio\n(B2A), for multilingual speech recognition and synthesis. Prior work has\npredominantly used characters, sub-words or words as the unit of choice to\nmodel text. These units are difficult to scale to languages with large\nvocabularies, particularly in the case of multilingual processing. In this\nwork, we model text via a sequence of Unicode bytes, specifically, the UTF-8\nvariable length byte sequence for each character. Bytes allow us to avoid large\nsoftmaxes in languages with large vocabularies, and share representations in\nmultilingual models. We show that bytes are superior to grapheme characters\nover a wide variety of languages in monolingual end-to-end speech recognition.\nAdditionally, our multilingual byte model outperform each respective single\nlanguage baseline on average by 4.4% relatively. In Japanese-English\ncode-switching speech, our multilingual byte model outperform our monolingual\nbaseline by 38.6% relatively. Finally, we present an end-to-end multilingual\nspeech synthesis model using byte representations which matches the performance\nof our monolingual baselines.", "published": "2018-11-22 04:37:55", "link": "http://arxiv.org/abs/1811.09021v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Creating a contemporary corpus of similes in Serbian by using natural\n  language processing", "abstract": "Simile is a figure of speech that compares two things through the use of\nconnection words, but where comparison is not intended to be taken literally.\nThey are often used in everyday communication, but they are also a part of\nlinguistic cultural heritage. In this paper we present a methodology for\nsemi-automated collection of similes from the World Wide Web using text mining\nand machine learning techniques. We expanded an existing corpus by collecting\n442 similes from the internet and adding them to the existing corpus collected\nby Vuk Stefanovic Karadzic that contained 333 similes. We, also, introduce\ncrowdsourcing to the collection of figures of speech, which helped us to build\ncorpus containing 787 unique similes.", "published": "2018-11-22 12:55:40", "link": "http://arxiv.org/abs/1811.10422v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre\n  Transfer", "abstract": "In this work, we address the problem of musical timbre transfer, where the\ngoal is to manipulate the timbre of a sound sample from one instrument to match\nanother instrument while preserving other musical content, such as pitch,\nrhythm, and loudness. In principle, one could apply image-based style transfer\ntechniques to a time-frequency representation of an audio signal, but this\ndepends on having a representation that allows independent manipulation of\ntimbre as well as high-quality waveform generation. We introduce TimbreTron, a\nmethod for musical timbre transfer which applies \"image\" domain style transfer\nto a time-frequency representation of the audio signal, and then produces a\nhigh-quality waveform using a conditional WaveNet synthesizer. We show that the\nConstant Q Transform (CQT) representation is particularly well-suited to\nconvolutional architectures due to its approximate pitch equivariance. Based on\nhuman perceptual evaluations, we confirmed that TimbreTron recognizably\ntransferred the timbre while otherwise preserving the musical content, for both\nmonophonic and polyphonic samples.", "published": "2018-11-22 17:46:51", "link": "http://arxiv.org/abs/1811.09620v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
