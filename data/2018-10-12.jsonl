{"title": "Important Attribute Identification in Knowledge Graph", "abstract": "The knowledge graph(KG) composed of entities with their descriptions and\nattributes, and relationship between entities, is finding more and more\napplication scenarios in various natural language processing tasks. In a\ntypical knowledge graph like Wikidata, entities usually have a large number of\nattributes, but it is difficult to know which ones are important. The\nimportance of attributes can be a valuable piece of information in various\napplications spanning from information retrieval to natural language\ngeneration. In this paper, we propose a general method of using external user\ngenerated text data to evaluate the relative importance of an entity's\nattributes. To be more specific, we use the word/sub-word embedding techniques\nto match the external textual data back to entities' attribute name and values\nand rank the attributes by their matching cohesiveness. To our best knowledge,\nthis is the first work of applying vector based semantic matching to important\nattribute identification, and our method outperforms the previous traditional\nmethods. We also apply the outcome of the detected important attributes to a\nlanguage generation task; compared with previous generated text, the new method\ngenerates much more customized and informative messages.", "published": "2018-10-12 02:19:42", "link": "http://arxiv.org/abs/1810.05320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndoSum: A New Benchmark Dataset for Indonesian Text Summarization", "abstract": "Automatic text summarization is generally considered as a challenging task in\nthe NLP community. One of the challenges is the publicly available and large\ndataset that is relatively rare and difficult to construct. The problem is even\nworse for low-resource languages such as Indonesian. In this paper, we present\nIndoSum, a new benchmark dataset for Indonesian text summarization. The dataset\nconsists of news articles and manually constructed summaries. Notably, the\ndataset is almost 200x larger than the previous Indonesian summarization\ndataset of the same domain. We evaluated various extractive summarization\napproaches and obtained encouraging results which demonstrate the usefulness of\nthe dataset and provide baselines for future research. The code and the dataset\nare available online under permissive licenses.", "published": "2018-10-12 03:07:21", "link": "http://arxiv.org/abs/1810.05334v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Dynamic Knowledge Graphs from Text using Machine Reading\n  Comprehension", "abstract": "We propose a neural machine-reading model that constructs dynamic knowledge\ngraphs from procedural text. It builds these graphs recurrently for each step\nof the described procedure, and uses them to track the evolving states of\nparticipant entities. We harness and extend a recently proposed machine reading\ncomprehension (MRC) model to query for entity states, since these states are\ngenerally communicated in spans of text and MRC models perform well in\nextracting entity-centric spans. The explicit, structured, and evolving\nknowledge graph representations that our model constructs can be used in\ndownstream question answering tasks to improve machine comprehension of text,\nas we demonstrate empirically. On two comprehension tasks from the recently\nproposed PROPARA dataset (Dalvi et al., 2018), our model achieves\nstate-of-the-art results. We further show that our model is competitive on the\nRECIPES dataset (Kiddon et al., 2015), suggesting it may be generally\napplicable. We present some evidence that the model's knowledge graphs help it\nto impose commonsense constraints on its predictions.", "published": "2018-10-12 19:01:32", "link": "http://arxiv.org/abs/1810.05682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MeanSum: A Neural Model for Unsupervised Multi-document Abstractive\n  Summarization", "abstract": "Abstractive summarization has been studied using neural sequence transduction\nmethods with datasets of large, paired document-summary examples. However, such\ndatasets are rare and the models trained from them do not generalize to other\ndomains. Recently, some progress has been made in learning sequence-to-sequence\nmappings with only unpaired examples. In our work, we consider the setting\nwhere there are only documents (product or business reviews) with no summaries\nprovided, and propose an end-to-end, neural model architecture to perform\nunsupervised abstractive summarization. Our proposed model consists of an\nauto-encoder where the mean of the representations of the input reviews decodes\nto a reasonable summary-review while not relying on any review-specific\nfeatures. We consider variants of the proposed architecture and perform an\nablation study to show the importance of specific components. We show through\nautomated metrics and human evaluation that the generated summaries are highly\nabstractive, fluent, relevant, and representative of the average sentiment of\nthe input reviews. Finally, we collect a reference evaluation dataset and show\nthat our model outperforms a strong extractive baseline.", "published": "2018-10-12 21:24:33", "link": "http://arxiv.org/abs/1810.05739v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Word-Complexity Lexicon and A Neural Readability Ranking Model for\n  Lexical Simplification", "abstract": "Current lexical simplification approaches rely heavily on heuristics and\ncorpus level features that do not always align with human judgment. We create a\nhuman-rated word-complexity lexicon of 15,000 English words and propose a novel\nneural readability ranking model with a Gaussian-based feature vectorization\nlayer that utilizes these human ratings to measure the complexity of any given\nword or phrase. Our model performs better than the state-of-the-art systems for\ndifferent lexical simplification tasks and evaluation datasets. Additionally,\nwe also produce SimplePPDB++, a lexical resource of over 10 million simplifying\nparaphrase rules, by applying our model to the Paraphrase Database (PPDB).", "published": "2018-10-12 23:05:01", "link": "http://arxiv.org/abs/1810.05754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-gen metrics: Predicting caption quality metrics without generating\n  captions", "abstract": "Image caption generation systems are typically evaluated against reference\noutputs. We show that it is possible to predict output quality without\ngenerating the captions, based on the probability assigned by the neural model\nto the reference captions. Such pre-gen metrics are strongly correlated to\nstandard evaluation metrics.", "published": "2018-10-12 12:19:56", "link": "http://arxiv.org/abs/1810.05474v1", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Quantifying the amount of visual information used by neural caption\n  generators", "abstract": "This paper addresses the sensitivity of neural image caption generators to\ntheir visual input. A sensitivity analysis and omission analysis based on image\nfoils is reported, showing that the extent to which image captioning\narchitectures retain and are sensitive to visual information varies depending\non the type of word being generated and the position in the caption as a whole.\nWe motivate this work in the context of broader goals in the field to achieve\nmore explainability in AI.", "published": "2018-10-12 12:23:08", "link": "http://arxiv.org/abs/1810.05475v1", "categories": ["cs.NE", "cs.CL"], "primary_category": "cs.NE"}
{"title": "U-Net: Machine Reading Comprehension with Unanswerable Questions", "abstract": "Machine reading comprehension with unanswerable questions is a new\nchallenging task for natural language processing. A key subtask is to reliably\npredict whether the question is unanswerable. In this paper, we propose a\nunified model, called U-Net, with three important components: answer pointer,\nno-answer pointer, and answer verifier. We introduce a universal node and thus\nprocess the question and its context passage as a single contiguous sequence of\ntokens. The universal node encodes the fused information from both the question\nand passage, and plays an important role to predict whether the question is\nanswerable and also greatly improves the conciseness of the U-Net. Different\nfrom the state-of-art pipeline models, U-Net can be learned in an end-to-end\nfashion. The experimental results on the SQuAD 2.0 dataset show that U-Net can\neffectively predict the unanswerability of questions and achieves an F1 score\nof 71.7 on SQuAD 2.0.", "published": "2018-10-12 12:48:34", "link": "http://arxiv.org/abs/1810.06638v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HiTR: Hierarchical Topic Model Re-estimation for Measuring Topical\n  Diversity of Documents", "abstract": "A high degree of topical diversity is often considered to be an important\ncharacteristic of interesting text documents. A recent proposal for measuring\ntopical diversity identifies three distributions for assessing the diversity of\ndocuments: distributions of words within documents, words within topics, and\ntopics within documents. Topic models play a central role in this approach and,\nhence, their quality is crucial to the efficacy of measuring topical diversity.\nThe quality of topic models is affected by two causes: generality and impurity\nof topics. General topics only include common information of a background\ncorpus and are assigned to most of the documents. Impure topics contain words\nthat are not related to the topic. Impurity lowers the interpretability of\ntopic models. Impure topics are likely to get assigned to documents\nerroneously. We propose a hierarchical re-estimation process aimed at removing\ngenerality and impurity. Our approach has three re-estimation components: (1)\ndocument re-estimation, which removes general words from the documents; (2)\ntopic re-estimation, which re-estimates the distribution over words of each\ntopic; and (3) topic assignment re-estimation, which re-estimates for each\ndocument its distributions over topics. For measuring topical diversity of text\ndocuments, our HiTR approach improves over the state-of-the-art measured on\nPubMed dataset.", "published": "2018-10-12 10:02:23", "link": "http://arxiv.org/abs/1810.05436v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Embedding Geographic Locations for Modelling the Natural Environment\n  using Flickr Tags and Structured Data", "abstract": "Meta-data from photo-sharing websites such as Flickr can be used to obtain\nrich bag-of-words descriptions of geographic locations, which have proven\nvaluable, among others, for modelling and predicting ecological features. One\nimportant insight from previous work is that the descriptions obtained from\nFlickr tend to be complementary to the structured information that is available\nfrom traditional scientific resources. To better integrate these two diverse\nsources of information, in this paper we consider a method for learning vector\nspace embeddings of geographic locations. We show experimentally that this\nmethod improves on existing approaches, especially in cases where structured\ninformation is available.", "published": "2018-10-12 12:22:34", "link": "http://arxiv.org/abs/1810.12091v1", "categories": ["cs.IR", "cs.CL", "cs.CV", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Robust Joint Estimation of Multi-Microphone Signal Model Parameters", "abstract": "One of the biggest challenges in multi-microphone applications is the\nestimation of the parameters of the signal model such as the power spectral\ndensities (PSDs) of the sources, the early (relative) acoustic transfer\nfunctions of the sources with respect to the microphones, the PSD of late\nreverberation, and the PSDs of microphone-self noise. Typically, the existing\nmethods estimate subsets of the aforementioned parameters and assume some of\nthe other parameters to be known a priori. This may result in inconsistencies\nand inaccurately estimated parameters and potential performance degradation in\nthe applications using these estimated parameters. So far, there is no method\nto jointly estimate all the aforementioned parameters. In this paper, we\npropose a robust method for jointly estimating all the aforementioned\nparameters using confirmatory factor analysis. The estimation accuracy of the\nsignal-model parameters thus obtained outperforms existing methods in most\ncases. We experimentally show significant performance gains in several\nmulti-microphone applications over state-of-the-art methods.", "published": "2018-10-12 18:55:55", "link": "http://arxiv.org/abs/1810.05677v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Fully Time-domain Neural Model for Subband-based Speech Synthesizer", "abstract": "This paper introduces a deep neural network model for subband-based speech\nsynthesizer. The model benefits from the short bandwidth of the subband signals\nto reduce the complexity of the time-domain speech generator. We employed the\nmulti-level wavelet analysis/synthesis to decompose/reconstruct the signal into\nsubbands in time domain. Inspired from the WaveNet, a convolutional neural\nnetwork (CNN) model predicts subband speech signals fully in time domain. Due\nto the short bandwidth of the subbands, a simple network architecture is enough\nto train the simple patterns of the subbands accurately. In the ground truth\nexperiments with teacher-forcing, the subband synthesizer outperforms the\nfullband model significantly in terms of both subjective and objective\nmeasures. In addition, by conditioning the model on the phoneme sequence using\na pronunciation dictionary, we have achieved the fully time-domain neural model\nfor subband-based text-to-speech (TTS) synthesizer, which is nearly end-to-end.\nThe generated speech of the subband TTS shows comparable quality as the\nfullband one with a slighter network architecture for each subband.", "published": "2018-10-12 02:16:21", "link": "http://arxiv.org/abs/1810.05319v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
