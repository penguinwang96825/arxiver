{"title": "Bridging Text and Video: A Universal Multimodal Transformer for\n  Video-Audio Scene-Aware Dialog", "abstract": "Audio-Visual Scene-Aware Dialog (AVSD) is a task to generate responses when\nchatting about a given video, which is organized as a track of the 8th Dialog\nSystem Technology Challenge (DSTC8). To solve the task, we propose a universal\nmultimodal transformer and introduce the multi-task learning method to learn\njoint representations among different modalities as well as generate\ninformative and fluent responses. Our method extends the natural language\ngeneration pre-trained model to multimodal dialogue generation task. Our system\nachieves the best performance in both objective and subjective evaluations in\nthe challenge.", "published": "2020-02-01 07:50:43", "link": "http://arxiv.org/abs/2002.00163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Novel Language Resources for Hindi: An Aesthetics Text Corpus and a\n  Comprehensive Stop Lemma List", "abstract": "This paper is an effort to complement the contributions made by researchers\nworking toward the inclusion of non-English languages in natural language\nprocessing studies. Two novel Hindi language resources have been created and\nreleased for public consumption. The first resource is a corpus consisting of\nnearly thousand pre-processed fictional and nonfictional texts spanning over\nhundred years. The second resource is an exhaustive list of stop lemmas created\nfrom 12 corpora across multiple domains, consisting of over 13 million words,\nfrom which more than 200,000 lemmas were generated, and 11 publicly available\nstop word lists comprising over 1000 words, from which nearly 400 unique lemmas\nwere generated. This research lays emphasis on the use of stop lemmas instead\nof stop words owing to the presence of various, but not all morphological forms\nof a word in stop word lists, as opposed to the presence of only the root form\nof the word, from which variations could be derived if required. It was also\nobserved that stop lemmas were more consistent across multiple sources as\ncompared to stop words. In order to generate a stop lemma list, the parts of\nspeech of the lemmas were investigated but rejected as it was found that there\nwas no significant correlation between the rank of a word in the frequency list\nand its part of speech. The stop lemma list was assessed using a comparative\nmethod. A formal evaluation method is suggested as future work arising from\nthis study.", "published": "2020-02-01 08:49:17", "link": "http://arxiv.org/abs/2002.00171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image\n  Captioning", "abstract": "Image Captioning, the task of automatic generation of image captions, has\nattracted attentions from researchers in many fields of computer science, being\ncomputer vision, natural language processing and machine learning in recent\nyears. This paper contributes to research on Image Captioning task in terms of\nextending dataset to a different language - Vietnamese. So far, there is no\nexisted Image Captioning dataset for Vietnamese language, so this is the\nforemost fundamental step for developing Vietnamese Image Captioning. In this\nscope, we first build a dataset which contains manually written captions for\nimages from Microsoft COCO dataset relating to sports played with balls, we\ncalled this dataset UIT-ViIC. UIT-ViIC consists of 19,250 Vietnamese captions\nfor 3,850 images. Following that, we evaluate our dataset on deep neural\nnetwork models and do comparisons with English dataset and two Vietnamese\ndatasets built by different methods. UIT-ViIC is published on our lab website\nfor research purposes.", "published": "2020-02-01 09:26:07", "link": "http://arxiv.org/abs/2002.00175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning BERT for Schema-Guided Zero-Shot Dialogue State Tracking", "abstract": "We present our work on Track 4 in the Dialogue System Technology Challenges 8\n(DSTC8). The DSTC8-Track 4 aims to perform dialogue state tracking (DST) under\nthe zero-shot settings, in which the model needs to generalize on unseen\nservice APIs given a schema definition of these target APIs. Serving as the\ncore for many virtual assistants such as Siri, Alexa, and Google Assistant, the\nDST keeps track of the user's goal and what happened in the dialogue history,\nmainly including intent prediction, slot filling, and user state tracking,\nwhich tests models' ability of natural language understanding. Recently, the\npretrained language models have achieved state-of-the-art results and shown\nimpressive generalization ability on various NLP tasks, which provide a\npromising way to perform zero-shot learning for language understanding. Based\non this, we propose a schema-guided paradigm for zero-shot dialogue state\ntracking (SGP-DST) by fine-tuning BERT, one of the most popular pretrained\nlanguage models. The SGP-DST system contains four modules for intent\nprediction, slot prediction, slot transfer prediction, and user state\nsummarizing respectively. According to the official evaluation results, our\nSGP-DST (team12) ranked 3rd on the joint goal accuracy (primary evaluation\nmetric for ranking submissions) and 1st on the requsted slots F1 among 25\nparticipant teams.", "published": "2020-02-01 10:00:06", "link": "http://arxiv.org/abs/2002.00181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Domain-Adapted Sentiment Classification by Deep Adversarial\n  Mutual Learning", "abstract": "Domain-adapted sentiment classification refers to training on a labeled\nsource domain to well infer document-level sentiment on an unlabeled target\ndomain. Most existing relevant models involve a feature extractor and a\nsentiment classifier, where the feature extractor works towards learning\ndomain-invariant features from both domains, and the sentiment classifier is\ntrained only on the source domain to guide the feature extractor. As such, they\nlack a mechanism to use sentiment polarity lying in the target domain. To\nimprove domain-adapted sentiment classification by learning sentiment from the\ntarget domain as well, we devise a novel deep adversarial mutual learning\napproach involving two groups of feature extractors, domain discriminators,\nsentiment classifiers, and label probers. The domain discriminators enable the\nfeature extractors to obtain domain-invariant features. Meanwhile, the label\nprober in each group explores document sentiment polarity of the target domain\nthrough the sentiment prediction generated by the classifier in the peer group,\nand guides the learning of the feature extractor in its own group. The proposed\napproach achieves the mutual learning of the two groups in an end-to-end\nmanner. Experiments on multiple public datasets indicate our method obtains the\nstate-of-the-art performance, validating the effectiveness of mutual learning\nthrough label probers.", "published": "2020-02-01 01:22:44", "link": "http://arxiv.org/abs/2002.00119v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Novel Entity Discovery from Web Tables", "abstract": "When working with any sort of knowledge base (KB) one has to make sure it is\nas complete and also as up-to-date as possible. Both tasks are non-trivial as\nthey require recall-oriented efforts to determine which entities and\nrelationships are missing from the KB. As such they require a significant\namount of labor. Tables on the Web, on the other hand, are abundant and have\nthe distinct potential to assist with these tasks. In particular, we can\nleverage the content in such tables to discover new entities, properties, and\nrelationships. Because web tables typically only contain raw textual content we\nfirst need to determine which cells refer to which known entities---a task we\ndub table-to-KB matching. This first task aims to infer table semantics by\nlinking table cells and heading columns to elements of a KB. Then second task\nbuilds upon these linked entities and properties to not only identify novel\nones in the same table but also to bootstrap their type and additional\nrelationships. We refer to this process as novel entity discovery and, to the\nbest of our knowledge, it is the first endeavor on mining the unlinked cells in\nweb tables. Our method identifies not only out-of-KB (``novel'') information\nbut also novel aliases for in-KB (``known'') entities. When evaluated using\nthree purpose-built test collections, we find that our proposed approaches\nobtain a marked improvement in terms of precision over our baselines whilst\nkeeping recall stable.", "published": "2020-02-01 13:24:03", "link": "http://arxiv.org/abs/2002.00206v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Transforming Spectrum and Prosody for Emotional Voice Conversion with\n  Non-Parallel Training Data", "abstract": "Emotional voice conversion aims to convert the spectrum and prosody to change\nthe emotional patterns of speech, while preserving the speaker identity and\nlinguistic content. Many studies require parallel speech data between different\nemotional patterns, which is not practical in real life. Moreover, they often\nmodel the conversion of fundamental frequency (F0) with a simple linear\ntransform. As F0 is a key aspect of intonation that is hierarchical in nature,\nwe believe that it is more adequate to model F0 in different temporal scales by\nusing wavelet transform. We propose a CycleGAN network to find an optimal\npseudo pair from non-parallel training data by learning forward and inverse\nmappings simultaneously using adversarial and cycle-consistency losses. We also\nstudy the use of continuous wavelet transform (CWT) to decompose F0 into ten\ntemporal scales, that describes speech prosody at different time resolution,\nfor effective F0 conversion. Experimental results show that our proposed\nframework outperforms the baselines both in objective and subjective\nevaluations.", "published": "2020-02-01 12:36:55", "link": "http://arxiv.org/abs/2002.00198v5", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep segmental phonetic posterior-grams based discovery of\n  non-categories in L2 English speech", "abstract": "Second language (L2) speech is often labeled with the native, phone\ncategories. However, in many cases, it is difficult to decide on a categorical\nphone that an L2 segment belongs to. These segments are regarded as\nnon-categories. Most existing approaches for Mispronunciation Detection and\nDiagnosis (MDD) are only concerned with categorical errors, i.e. a phone\ncategory is inserted, deleted or substituted by another. However,\nnon-categorical errors are not considered. To model these non-categorical\nerrors, this work aims at exploring non-categorical patterns to extend the\ncategorical phone set. We apply a phonetic segment classifier to generate\nsegmental phonetic posterior-grams (SPPGs) to represent phone segment-level\ninformation. And then we explore the non-categories by looking for the SPPGs\nwith more than one peak. Compared with the baseline system, this approach\nexplores more non-categorical patterns, and also perceptual experimental\nresults show that the explored non-categories are more accurate with increased\nconfusion degree by 7.3% and 7.5% under two different measures. Finally, we\npreliminarily analyze the reason behind those non-categories.", "published": "2020-02-01 13:21:33", "link": "http://arxiv.org/abs/2002.00205v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Concept Embedding for Information Retrieval", "abstract": "Concepts are used to solve the term-mismatch problem. However, we need an\neffective similarity measure between concepts. Word embedding presents a\npromising solution. We present in this study three approaches to build concepts\nvectors based on words vectors. We use a vector-based measure to estimate\ninter-concepts similarity. Our experiments show promising results. Furthermore,\nwords and concepts become comparable. This could be used to improve conceptual\nindexing process.", "published": "2020-02-01 09:18:56", "link": "http://arxiv.org/abs/2002.01071v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68-XX, 68Pxx, 68P20", "H.3.3; I.2.7; I.2.6"], "primary_category": "cs.IR"}
{"title": "Multi-channel Acoustic Modeling using Mixed Bitrate OPUS Compression", "abstract": "Recent literature has shown that a learned front end with multi-channel audio\ninput can outperform traditional beam-forming algorithms for automatic speech\nrecognition (ASR). In this paper, we present our study on multi-channel\nacoustic modeling using OPUS compression with different bitrates for the\ndifferent channels. We analyze the degradation in word error rate (WER) as a\nfunction of the audio encoding bitrate and show that the WER degrades by 12.6%\nrelative with 16kpbs as compared to uncompressed audio. We show that its always\npreferable to have a multi-channel audio input over a single channel audio\ninput given limited bandwidth. Our results show that for the best WER, when one\nof the two channels can be encoded with a bitrate higher than 32kbps, its\noptimal to encode the other channel with the highest bitrate possible. For\nbitrates lower than that, its preferable to distribute the bitrate equally\nbetween the two channels. We further show that by training the acoustic model\non mixed bitrate input, up to 50% of the degradation can be recovered using a\nsingle model.", "published": "2020-02-01 01:46:28", "link": "http://arxiv.org/abs/2002.00122v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analysis of Deep Feature Loss based Enhancement for Speaker Verification", "abstract": "Data augmentation is conventionally used to inject robustness in Speaker\nVerification systems. Several recently organized challenges focus on handling\nnovel acoustic environments. Deep learning based speech enhancement is a modern\nsolution for this. Recently, a study proposed to optimize the enhancement\nnetwork in the activation space of a pre-trained auxiliary network. This\nmethodology, called deep feature loss, greatly improved over the\nstate-of-the-art conventional x-vector based system on a children speech\ndataset called BabyTrain. This work analyzes various facets of that approach\nand asks few novel questions in that context. We first search for optimal\nnumber of auxiliary network activations, training data, and enhancement feature\ndimension. Experiments reveal the importance of Signal-to-Noise Ratio filtering\nthat we employ to create a large, clean, and naturalistic corpus for\nenhancement network training. To counter the \"mismatch\" problem in enhancement,\nwe find enhancing front-end (x-vector network) data helpful while harmful for\nthe back-end (Probabilistic Linear Discriminant Analysis (PLDA)). Importantly,\nwe find enhanced signals contain complementary information to original.\nEstablished by combining them in front-end, this gives ~40% relative\nimprovement over the baseline. We also do an ablation study to remove a noise\nclass from x-vector data augmentation and, for such systems, we establish the\nutility of enhancement regardless of whether it has seen that noise class\nitself during training. Finally, we design several dereverberation schemes to\nconclude ineffectiveness of deep feature loss enhancement scheme for this task.", "published": "2020-02-01 04:49:37", "link": "http://arxiv.org/abs/2002.00139v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fully Learnable Front-End for Multi-Channel Acoustic Modeling using\n  Semi-Supervised Learning", "abstract": "In this work, we investigated the teacher-student training paradigm to train\na fully learnable multi-channel acoustic model for far-field automatic speech\nrecognition (ASR). Using a large offline teacher model trained on beamformed\naudio, we trained a simpler multi-channel student acoustic model used in the\nspeech recognition system. For the student, both multi-channel feature\nextraction layers and the higher classification layers were jointly trained\nusing the logits from the teacher model. In our experiments, compared to a\nbaseline model trained on about 600 hours of transcribed data, a relative\nword-error rate (WER) reduction of about 27.3% was achieved when using an\nadditional 1800 hours of untranscribed data. We also investigated the benefit\nof pre-training the multi-channel front end to output the beamformed log-mel\nfilter bank energies (LFBE) using L2 loss. We find that pre-training improves\nthe word error rate by 10.7% when compared to a multi-channel model directly\ninitialized with a beamformer and mel-filter bank coefficients for the front\nend. Finally, combining pre-training and teacher-student training produces a\nWER reduction of 31% compared to our baseline.", "published": "2020-02-01 02:06:05", "link": "http://arxiv.org/abs/2002.00125v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pop Music Transformer: Beat-based Modeling and Generation of Expressive\n  Pop Piano Compositions", "abstract": "A great number of deep learning based models have been recently proposed for\nautomatic music composition. Among these models, the Transformer stands out as\na prominent approach for generating expressive classical piano performance with\na coherent structure of up to one minute. The model is powerful in that it\nlearns abstractions of data on its own, without much human-imposed domain\nknowledge or constraints. In contrast with this general approach, this paper\nshows that Transformers can do even better for music modeling, when we improve\nthe way a musical score is converted into the data fed to a Transformer model.\nIn particular, we seek to impose a metrical structure in the input data, so\nthat Transformers can be more easily aware of the beat-bar-phrase hierarchical\nstructure in music. The new data representation maintains the flexibility of\nlocal tempo changes, and provides hurdles to control the rhythmic and harmonic\nstructure of music. With this approach, we build a Pop Music Transformer that\ncomposes Pop piano music with better rhythmic structure than existing\nTransformer models.", "published": "2020-02-01 14:12:35", "link": "http://arxiv.org/abs/2002.00212v3", "categories": ["cs.SD", "cs.AI", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Exploiting Rays in Blind Localization of Distributed Sensor Arrays", "abstract": "Many signal processing algorithms for distributed sensors are capable of\nimproving their performance if the positions of sensors are known. In this\npaper, we focus on estimators for inferring the relative geometry of\ndistributed arrays and sources, i.e. the setup geometry up to a scaling factor.\nFirstly, we present the Maximum Likelihood estimator derived under the\nassumption that the Direction of Arrival measurements follow the von\nMises-Fisher distribution. Secondly, using unified notation, we show the\nrelations between the cost functions of a number of state-of-the-art relative\ngeometry estimators. Thirdly, we derive a novel estimator that exploits the\nconcept of rays between the arrays and source event positions. Finally, we show\nthe evaluation results for the presented estimators in various conditions,\nwhich indicate that major improvements in the probability of convergence to the\noptimum solution over the existing approaches can be achieved by using the\nproposed ray-based estimator.", "published": "2020-02-01 17:43:39", "link": "http://arxiv.org/abs/2002.00248v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Multi-Modal Music Information Retrieval: Augmenting Audio-Analysis with\n  Visual Computing for Improved Music Video Analysis", "abstract": "This thesis combines audio-analysis with computer vision to approach Music\nInformation Retrieval (MIR) tasks from a multi-modal perspective. This thesis\nfocuses on the information provided by the visual layer of music videos and how\nit can be harnessed to augment and improve tasks of the MIR research domain.\nThe main hypothesis of this work is based on the observation that certain\nexpressive categories such as genre or theme can be recognized on the basis of\nthe visual content alone, without the sound being heard. This leads to the\nhypothesis that there exists a visual language that is used to express mood or\ngenre. In a further consequence it can be concluded that this visual\ninformation is music related and thus should be beneficial for the\ncorresponding MIR tasks such as music genre classification or mood recognition.\nA series of comprehensive experiments and evaluations are conducted which are\nfocused on the extraction of visual information and its application in\ndifferent MIR tasks. A custom dataset is created, suitable to develop and test\nvisual features which are able to represent music related information.\nEvaluations range from low-level visual features to high-level concepts\nretrieved by means of Deep Convolutional Neural Networks. Additionally, new\nvisual features are introduced capturing rhythmic visual patterns. In all of\nthese experiments the audio-based results serve as benchmark for the visual and\naudio-visual approaches. The experiments are conducted for three MIR tasks\nArtist Identification, Music Genre Classification and Cross-Genre\nClassification. Experiments show that an audio-visual approach harnessing\nhigh-level semantic information gained from visual concept detection,\noutperforms audio-only genre-classification accuracy by 16.43%.", "published": "2020-02-01 17:57:14", "link": "http://arxiv.org/abs/2002.00251v1", "categories": ["cs.MM", "cs.CV", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
