{"title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural\n  Language Inference", "abstract": "A machine learning system can score well on a given test set by relying on\nheuristics that are effective for frequent example types but break down in more\nchallenging cases. We study this issue within natural language inference (NLI),\nthe task of determining whether one sentence entails another. We hypothesize\nthat statistical NLI models may adopt three fallible syntactic heuristics: the\nlexical overlap heuristic, the subsequence heuristic, and the constituent\nheuristic. To determine whether models have adopted these heuristics, we\nintroduce a controlled evaluation set called HANS (Heuristic Analysis for NLI\nSystems), which contains many examples where the heuristics fail. We find that\nmodels trained on MNLI, including BERT, a state-of-the-art model, perform very\npoorly on HANS, suggesting that they have indeed adopted these heuristics. We\nconclude that there is substantial room for improvement in NLI systems, and\nthat the HANS dataset can motivate and measure progress in this area", "published": "2019-02-04 01:54:19", "link": "http://arxiv.org/abs/1902.01007v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Exploration on WikiSQL with Table-Aware Word\n  Contextualization", "abstract": "We present SQLova, the first Natural-language-to-SQL (NL2SQL) model to\nachieve human performance in WikiSQL dataset. We revisit and discuss diverse\npopular methods in NL2SQL literature, take a full advantage of BERT {Devlin et\nal., 2018) through an effective table contextualization method, and coherently\ncombine them, outperforming the previous state of the art by 8.2% and 2.5% in\nlogical form and execution accuracy, respectively. We particularly note that\nBERT with a seq2seq decoder leads to a poor performance in the task, indicating\nthe importance of a careful design when using such large pretrained models. We\nalso provide a comprehensive analysis on the dataset and our model, which can\nbe helpful for designing future NL2SQL datsets and models. We especially show\nthat our model's performance is near the upper bound in WikiSQL, where we\nobserve that a large portion of the evaluation errors are due to wrong\nannotations, and our model is already exceeding human performance by 1.3% in\nexecution accuracy.", "published": "2019-02-04 07:55:47", "link": "http://arxiv.org/abs/1902.01069v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strategies for Structuring Story Generation", "abstract": "Writers generally rely on plans or sketches to write long stories, but most\ncurrent language models generate word by word from left to right. We explore\ncoarse-to-fine models for creating narrative texts of several hundred words,\nand introduce new models which decompose stories by abstracting over actions\nand entities. The model first generates the predicate-argument structure of the\ntext, where different mentions of the same entity are marked with placeholder\ntokens. It then generates a surface realization of the predicate-argument\nstructure, and finally replaces the entity placeholders with context-sensitive\nnames and references. Human judges prefer the stories from our models to a wide\nrange of previous approaches to hierarchical text generation. Extensive\nanalysis shows that our methods can help improve the diversity and coherence of\nevents and entities in generated stories.", "published": "2019-02-04 10:23:39", "link": "http://arxiv.org/abs/1902.01109v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Argument-Marker Model for Syntax-Agnostic Proto-Role Labeling", "abstract": "Semantic proto-role labeling (SPRL) is an alternative to semantic role\nlabeling (SRL) that moves beyond a categorical definition of roles, following\nDowty's feature-based view of proto-roles. This theory determines agenthood vs.\npatienthood based on a participant's instantiation of more or less typical\nagent vs. patient properties, such as, for example, volition in an event. To\nperform SPRL, we develop an ensemble of hierarchical models with self-attention\nand concurrently learned predicate-argument-markers. Our method is competitive\nwith the state-of-the art, overall outperforming previous work in two\nformulations of the task (multi-label and multi-variate Likert scale\nprediction). In contrast to previous work, our results do not depend on gold\nargument heads derived from supplementary gold tree banks.", "published": "2019-02-04 18:11:36", "link": "http://arxiv.org/abs/1902.01349v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The FLoRes Evaluation Datasets for Low-Resource Machine Translation:\n  Nepali-English and Sinhala-English", "abstract": "For machine translation, a vast majority of language pairs in the world are\nconsidered low-resource because they have little parallel data available.\nBesides the technical challenges of learning with limited supervision, it is\ndifficult to evaluate methods trained on low-resource language pairs because of\nthe lack of freely and publicly available benchmarks. In this work, we\nintroduce the FLoRes evaluation datasets for Nepali-English and\nSinhala-English, based on sentences translated from Wikipedia. Compared to\nEnglish, these are languages with very different morphology and syntax, for\nwhich little out-of-domain parallel data is available and for which relatively\nlarge amounts of monolingual data are freely available. We describe our process\nto collect and cross-check the quality of translations, and we report baseline\nperformance using several learning settings: fully supervised, weakly\nsupervised, semi-supervised, and fully unsupervised. Our experiments\ndemonstrate that current state-of-the-art methods perform rather poorly on this\nbenchmark, posing a challenge to the research community working on low-resource\nMT. Data and code to reproduce our experiments are available at\nhttps://github.com/facebookresearch/flores.", "published": "2019-02-04 18:48:45", "link": "http://arxiv.org/abs/1902.01382v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Temporal Relation Extraction", "abstract": "We present a novel semantic framework for modeling temporal relations and\nevent durations that maps pairs of events to real-valued scales. We use this\nframework to construct the largest temporal relations dataset to date, covering\nthe entirety of the Universal Dependencies English Web Treebank. We use this\ndataset to train models for jointly predicting fine-grained temporal relations\nand event durations. We report strong results on our data and show the efficacy\nof a transfer-learning approach for predicting categorical relations.", "published": "2019-02-04 18:58:18", "link": "http://arxiv.org/abs/1902.01390v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers", "abstract": "Most approaches to extraction multiple relations from a paragraph require\nmultiple passes over the paragraph. In practice, multiple passes are\ncomputationally expensive and this makes difficult to scale to longer\nparagraphs and larger text corpora. In this work, we focus on the task of\nmultiple relation extraction by encoding the paragraph only once (one-pass). We\nbuild our solution on the pre-trained self-attentive (Transformer) models,\nwhere we first add a structured prediction layer to handle extraction between\nmultiple entity pairs, then enhance the paragraph embedding to capture multiple\nrelational information associated with each entity with an entity-aware\nattention technique. We show that our approach is not only scalable but can\nalso perform state-of-the-art on the standard benchmark ACE 2005.", "published": "2019-02-04 04:42:08", "link": "http://arxiv.org/abs/1902.01030v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Clinical Language Translation", "abstract": "As patients' access to their doctors' clinical notes becomes common,\ntranslating professional, clinical jargon to layperson-understandable language\nis essential to improve patient-clinician communication. Such translation\nyields better clinical outcomes by enhancing patients' understanding of their\nown health conditions, and thus improving patients' involvement in their own\ncare. Existing research has used dictionary-based word replacement or\ndefinition insertion to approach the need. However, these methods are limited\nby expert curation, which is hard to scale and has trouble generalizing to\nunseen datasets that do not share an overlapping vocabulary. In contrast, we\napproach the clinical word and sentence translation problem in a completely\nunsupervised manner. We show that a framework using representation learning,\nbilingual dictionary induction and statistical machine translation yields the\nbest precision at 10 of 0.827 on professional-to-consumer word translation, and\nmean opinion scores of 4.10 and 4.28 out of 5 for clinical correctness and\nlayperson readability, respectively, on sentence translation. Our\nfully-unsupervised strategy overcomes the curation problem, and the clinically\nmeaningful evaluation reduces biases from inappropriate evaluators, which are\ncritical in clinical machine learning.", "published": "2019-02-04 13:47:18", "link": "http://arxiv.org/abs/1902.01177v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Insertion-based Decoding with automatically Inferred Generation Order", "abstract": "Conventional neural autoregressive decoding commonly assumes a fixed\nleft-to-right generation order, which may be sub-optimal. In this work, we\npropose a novel decoding algorithm -- InDIGO -- which supports flexible\nsequence generation in arbitrary orders through insertion operations. We extend\nTransformer, a state-of-the-art sequence generation model, to efficiently\nimplement the proposed approach, enabling it to be trained with either a\npre-defined generation order or adaptive orders obtained from beam-search.\nExperiments on four real-world tasks, including word order recovery, machine\ntranslation, image caption and code generation, demonstrate that our algorithm\ncan generate sequences following arbitrary orders, while achieving competitive\nor even better performance compared to the conventional left-to-right\ngeneration. The generated sequences show that InDIGO adopts adaptive generation\norders based on input information.", "published": "2019-02-04 18:35:59", "link": "http://arxiv.org/abs/1902.01370v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimized Compilation of Aggregated Instructions for Realistic Quantum\n  Computers", "abstract": "Recent developments in engineering and algorithms have made real-world\napplications in quantum computing possible in the near future. Existing quantum\nprogramming languages and compilers use a quantum assembly language composed of\n1- and 2-qubit (quantum bit) gates. Quantum compiler frameworks translate this\nquantum assembly to electric signals (called control pulses) that implement the\nspecified computation on specific physical devices. However, there is a\nmismatch between the operations defined by the 1- and 2-qubit logical ISA and\ntheir underlying physical implementation, so the current practice of directly\ntranslating logical instructions into control pulses results in inefficient,\nhigh-latency programs. To address this inefficiency, we propose a universal\nquantum compilation methodology that aggregates multiple logical operations\ninto larger units that manipulate up to 10 qubits at a time. Our methodology\nthen optimizes these aggregates by (1) finding commutative intermediate\noperations that result in more efficient schedules and (2) creating custom\ncontrol pulses optimized for the aggregate (instead of individual 1- and\n2-qubit operations). Compared to the standard gate-based compilation, the\nproposed approach realizes a deeper vertical integration of high-level quantum\nsoftware and low-level, physical quantum hardware. We evaluate our approach on\nimportant near-term quantum applications on simulations of superconducting\nquantum architectures. Our proposed approach provides a mean speedup of\n$5\\times$, with a maximum of $10\\times$. Because latency directly affects the\nfeasibility of quantum computation, our results not only improve performance\nbut also have the potential to enable quantum computation sooner than otherwise\npossible.", "published": "2019-02-04 22:05:50", "link": "http://arxiv.org/abs/1902.01474v2", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "The Natural Language of Actions", "abstract": "We introduce Act2Vec, a general framework for learning context-based action\nrepresentation for Reinforcement Learning. Representing actions in a vector\nspace help reinforcement learning algorithms achieve better performance by\ngrouping similar actions and utilizing relations between different actions. We\nshow how prior knowledge of an environment can be extracted from demonstrations\nand injected into action vector representations that encode natural compatible\nbehavior. We then use these for augmenting state representations as well as\nimproving function approximation of Q-values. We visualize and test action\nembeddings in three domains including a drawing task, a high dimensional\nnavigation task, and the large action space domain of StarCraft II.", "published": "2019-02-04 10:46:53", "link": "http://arxiv.org/abs/1902.01119v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SY"], "primary_category": "cs.AI"}
{"title": "An Effective Approach to Unsupervised Machine Translation", "abstract": "While machine translation has traditionally relied on large amounts of\nparallel corpora, a recent research line has managed to train both Neural\nMachine Translation (NMT) and Statistical Machine Translation (SMT) systems\nusing monolingual corpora only. In this paper, we identify and address several\ndeficiencies of existing unsupervised SMT approaches by exploiting subword\ninformation, developing a theoretically well founded unsupervised tuning\nmethod, and incorporating a joint refinement procedure. Moreover, we use our\nimproved SMT system to initialize a dual NMT model, which is further fine-tuned\nthrough on-the-fly back-translation. Together, we obtain large improvements\nover the previous state-of-the-art in unsupervised machine translation. For\ninstance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points\nmore than the previous best unsupervised system, and 0.5 points more than the\n(supervised) shared task winner back in 2014.", "published": "2019-02-04 17:08:32", "link": "http://arxiv.org/abs/1902.01313v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attention in Natural Language Processing", "abstract": "Attention is an increasingly popular mechanism used in a wide range of neural\narchitectures. The mechanism itself has been realized in a variety of formats.\nHowever, because of the fast-paced advances in this domain, a systematic\noverview of attention is still missing. In this article, we define a unified\nmodel for attention architectures in natural language processing, with a focus\non those designed to work with vector representations of the textual data. We\npropose a taxonomy of attention models according to four dimensions: the\nrepresentation of the input, the compatibility function, the distribution\nfunction, and the multiplicity of the input and/or output. We present the\nexamples of how prior information can be exploited in attention models and\ndiscuss ongoing research efforts and open challenges in the area, providing the\nfirst extensive categorization of the vast body of literature in this exciting\ndomain.", "published": "2019-02-04 17:14:13", "link": "http://arxiv.org/abs/1902.02181v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML", "68T50, 68T05, 68T07", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "Embodied Multimodal Multitask Learning", "abstract": "Recent efforts on training visual navigation agents conditioned on language\nusing deep reinforcement learning have been successful in learning policies for\ndifferent multimodal tasks, such as semantic goal navigation and embodied\nquestion answering. In this paper, we propose a multitask model capable of\njointly learning these multimodal tasks, and transferring knowledge of words\nand their grounding in visual objects across the tasks. The proposed model uses\na novel Dual-Attention unit to disentangle the knowledge of words in the\ntextual representations and visual concepts in the visual representations, and\nalign them with each other. This disentangled task-invariant alignment of\nrepresentations facilitates grounding and knowledge transfer across both tasks.\nWe show that the proposed model outperforms a range of baselines on both tasks\nin simulated 3D environments. We also show that this disentanglement of\nrepresentations makes our model modular, interpretable, and allows for transfer\nto instructions containing new words by leveraging object detectors.", "published": "2019-02-04 18:53:14", "link": "http://arxiv.org/abs/1902.01385v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Overlap-Add Windows with Maximum Energy Concentration for Speech and\n  Audio Processing", "abstract": "Processing of speech and audio signals with time-frequency representations\nrequire windowing methods which allow perfect reconstruction of the original\nsignal and where processing artifacts have a predictable behavior. The most\ncommon approach for this purpose is overlap-add windowing, where signal\nsegments are windowed before and after processing. Commonly used windows\ninclude the half-sine and a Kaiser-Bessel derived window. The latter is an\napproximation of the discrete prolate spherical sequence, and thus a maximum\nenergy concentration window, adapted for overlap-add. We demonstrate that\nperformance can be improved by including the overlap-add structure as a\nconstraint in optimization of the maximum energy concentration criteria. The\nsame approach can be used to find further special cases such as optimal\nlow-overlap windows. Our experiments demonstrate that the proposed windows\nprovide notable improvements in terms of reduction in side-lobe magnitude.", "published": "2019-02-04 06:58:07", "link": "http://arxiv.org/abs/1902.01053v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Active Acoustic Source Tracking Exploiting Particle Filtering and Monte\n  Carlo Tree Search", "abstract": "In this paper, we address the task of active acoustic source tracking as part\nof robotic path planning. It denotes the planning of sequences of robotic\nmovements to enhance tracking results of acoustic sources, e.g., talking\nhumans, by fusing observations from multiple positions. Essentially, two\nstrategies are possible: short-term planning, which results in greedy behavior,\nand long-term planning, which considers a sequence of possible future movements\nof the robot and the source. Here, we focus on the second method as it might\nimprove tracking performance compared to greedy behavior and propose a flexible\npath planning algorithm which exploits Monte Carlo Tree Search (MCTS) and\nparticle filtering based on a reward motivated by information-theoretic\nconsiderations.", "published": "2019-02-04 16:44:19", "link": "http://arxiv.org/abs/1902.01299v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
