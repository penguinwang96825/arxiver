{"title": "The RLLChatbot: a solution to the ConvAI challenge", "abstract": "Current conversational systems can follow simple commands and answer basic\nquestions, but they have difficulty maintaining coherent and open-ended\nconversations about specific topics. Competitions like the Conversational\nIntelligence (ConvAI) challenge are being organized to push the research\ndevelopment towards that goal. This article presents in detail the RLLChatbot\nthat participated in the 2017 ConvAI challenge. The goal of this research is to\nbetter understand how current deep learning and reinforcement learning tools\ncan be used to build a robust yet flexible open domain conversational agent. We\nprovide a thorough description of how a dialog system can be built and trained\nfrom mostly public-domain datasets using an ensemble model. The first\ncontribution of this work is a detailed description and analysis of different\ntext generation models in addition to novel message ranking and selection\nmethods. Moreover, a new open-source conversational dataset is presented.\nTraining on this data significantly improves the Recall@k score of the ranking\nand selection mechanisms compared to our baseline model responsible for\nselecting the message returned at each interaction.", "published": "2018-11-07 01:19:05", "link": "http://arxiv.org/abs/1811.02714v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Audio Embeddings by Adjacency-Based Clustering with\n  Applications in Spoken Term Detection", "abstract": "Embedding audio signal segments into vectors with fixed dimensionality is\nattractive because all following processing will be easier and more efficient,\nfor example modeling, classifying or indexing. Audio Word2Vec previously\nproposed was shown to be able to represent audio segments for spoken words as\nsuch vectors carrying information about the phonetic structures of the signal\nsegments. However, each linguistic unit (word, syllable, phoneme in text form)\ncorresponds to unlimited number of audio segments with vector representations\ninevitably spread over the embedding space, which causes some confusion. It is\ntherefore desired to better cluster the audio embeddings such that those\ncorresponding to the same linguistic unit can be more compactly distributed. In\nthis paper, inspired by Siamese networks, we propose some approaches to achieve\nthe above goal. This includes identifying positive and negative pairs from\nunlabeled data for Siamese style training, disentangling acoustic factors such\nas speaker characteristics from the audio embedding, handling unbalanced data\ndistribution, and having the embedding processes learn from the adjacency\nrelationships among data points. All these can be done in an unsupervised way.\nImproved performance was obtained in preliminary experiments on the LibriSpeech\ndata set, including clustering characteristics analysis and applications of\nspoken term detection.", "published": "2018-11-07 06:18:14", "link": "http://arxiv.org/abs/1811.02775v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "microNER: A Micro-Service for German Named Entity Recognition based on\n  BiLSTM-CRF", "abstract": "For named entity recognition (NER), bidirectional recurrent neural networks\nbecame the state-of-the-art technology in recent years. Competing approaches\nvary with respect to pre-trained word embeddings as well as models for\ncharacter embeddings to represent sequence information most effectively. For\nNER in German language texts, these model variations have not been studied\nextensively. We evaluate the performance of different word and character\nembeddings on two standard German datasets and with a special focus on\nout-of-vocabulary words. With F-Scores above 82% for the GermEval'14 dataset\nand above 85% for the CoNLL'03 dataset, we achieve (near) state-of-the-art\nperformance for this task. We publish several pre-trained models wrapped into a\nmicro-service based on Docker to allow for easy integration of German NER into\nother applications via a JSON API.", "published": "2018-11-07 14:31:13", "link": "http://arxiv.org/abs/1811.02902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning from LDA to BiLSTM-CNN for Offensive Language\n  Detection in Twitter", "abstract": "We investigate different strategies for automatic offensive language\nclassification on German Twitter data. For this, we employ a sequentially\ncombined BiLSTM-CNN neural network. Based on this model, three transfer\nlearning tasks to improve the classification performance with background\nknowledge are tested. We compare 1. Supervised category transfer: social media\ndata annotated with near-offensive language categories, 2. Weakly-supervised\ncategory transfer: tweets annotated with emojis they contain, 3. Unsupervised\ncategory transfer: tweets annotated with topic clusters obtained by Latent\nDirichlet Allocation (LDA). Further, we investigate the effect of three\ndifferent strategies to mitigate negative effects of 'catastrophic forgetting'\nduring transfer learning. Our results indicate that transfer learning in\ngeneral improves offensive language detection. Best results are achieved from\npre-training our model on the unsupervised topic clustering of tweets in\ncombination with thematic user cluster information.", "published": "2018-11-07 14:40:10", "link": "http://arxiv.org/abs/1811.02906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IMS at the PolEval 2018: A Bulky Ensemble Depedency Parser meets 12\n  Simple Rules for Predicting Enhanced Dependencies in Polish", "abstract": "This paper presents the IMS contribution to the PolEval 2018 Shared Task. We\nsubmitted systems for both of the Subtasks of Task 1. In Subtask (A), which was\nabout dependency parsing, we used our ensemble system from the CoNLL 2017 UD\nShared Task. The system first preprocesses the sentences with a CRF\nPOS/morphological tagger and predicts supertags with a neural tagger. Then, it\nemploys multiple instances of three different parsers and merges their outputs\nby applying blending. The system achieved the second place out of four\nparticipating teams. In this paper we show which components of the system were\nthe most responsible for its final performance.\n  The goal of Subtask (B) was to predict enhanced graphs. Our approach\nconsisted of two steps: parsing the sentences with our ensemble system from\nSubtask (A), and applying 12 simple rules to obtain the final dependency\ngraphs. The rules introduce additional enhanced arcs only for tokens with\n\"conj\" heads (conjuncts). They do not predict semantic relations at all. The\nsystem ranked first out of three participating teams. In this paper we show\nexamples of rules we designed and analyze the relation between the quality of\nautomatically parsed trees and the accuracy of the enhanced graphs.", "published": "2018-11-07 17:40:03", "link": "http://arxiv.org/abs/1811.03036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Selection with Feature Decay Algorithms Using an Approximated\n  Target Side", "abstract": "Data selection techniques applied to neural machine translation (NMT) aim to\nincrease the performance of a model by retrieving a subset of sentences for use\nas training data.\n  One of the possible data selection techniques are transductive learning\nmethods, which select the data based on the test set, i.e. the document to be\ntranslated. A limitation of these methods to date is that using the source-side\ntest set does not by itself guarantee that sentences are selected with correct\ntranslations, or translations that are suitable given the test-set domain. Some\ncorpora, such as subtitle corpora, may contain parallel sentences with\ninaccurate translations caused by localization or length restrictions.\n  In order to try to fix this problem, in this paper we propose to use an\napproximated target-side in addition to the source-side when selecting suitable\nsentence-pairs for training a model. This approximated target-side is built by\npre-translating the source-side.\n  In this work, we explore the performance of this general idea for one\nspecific data selection approach called Feature Decay Algorithms (FDA).\n  We train German-English NMT models on data selected by using the test set\n(source), the approximated target side, and a mixture of both. Our findings\nreveal that models built using a combination of outputs of FDA (using the test\nset and an approximated target side) perform better than those solely using the\ntest set. We obtain a statistically significant improvement of more than 1.5\nBLEU points over a model trained with all data, and more than 0.5 BLEU points\nover a strong FDA baseline that uses source-side information only.", "published": "2018-11-07 17:50:05", "link": "http://arxiv.org/abs/1811.03039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Fluent Translations from Disfluent Speech", "abstract": "When translating from speech, special consideration for conversational speech\nphenomena such as disfluencies is necessary. Most machine translation training\ndata consists of well-formed written texts, causing issues when translating\nspontaneous speech. Previous work has introduced an intermediate step between\nspeech recognition (ASR) and machine translation (MT) to remove disfluencies,\nmaking the data better-matched to typical translation text and significantly\nimproving performance. However, with the rise of end-to-end speech translation\nsystems, this intermediate step must be incorporated into the\nsequence-to-sequence architecture. Further, though translated speech datasets\nexist, they are typically news or rehearsed speech without many disfluencies\n(e.g. TED), or the disfluencies are translated into the references (e.g.\nFisher). To generate clean translations from disfluent speech, cleaned\nreferences are necessary for evaluation. We introduce a corpus of cleaned\ntarget data for the Fisher Spanish-English dataset for this task. We compare\nhow different architectures handle disfluencies and provide a baseline for\nremoving disfluencies in end-to-end translation.", "published": "2018-11-07 23:47:01", "link": "http://arxiv.org/abs/1811.03189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The relationship between linguistic expression and symptoms of\n  depression, anxiety, and suicidal thoughts: A longitudinal study of blog\n  content", "abstract": "Due to its popularity and availability, social media data may present a new\nway to identify individuals who are experiencing mental illness. By analysing\nblog content, this study aimed to investigate the associations between\nlinguistic features and symptoms of depression, generalised anxiety, and\nsuicidal ideation. This study utilised a longitudinal study design. Individuals\nwho blogged were invited to participate in a study in which they completed\nfortnightly mental health questionnaires including the PHQ9 and GAD7 for a\nperiod of 36 weeks. Linguistic features were extracted from blog data using the\nLIWC tool. Bivariate and multivariate analyses were performed to investigate\nthe correlations between the linguistic features and mental health scores\nbetween subjects. We then used the multivariate regression model to predict\nlongitudinal changes in mood within subjects. A total of 153 participants\nconsented to taking part, with 38 participants completing the required number\nof questionnaires and blog posts during the study period. Between-subject\nanalysis revealed that several linguistic features, including tentativeness and\nnon-fluencies, were significantly associated with depression and anxiety\nsymptoms, but not suicidal thoughts. Within-subject analysis showed no robust\ncorrelations between linguistic features and changes in mental health score.\nThis study provides further support for the relationship between linguistic\nfeatures within social media data and symptoms of depression and anxiety. The\nlack of robust within-subject correlations indicate that the relationship\nobserved at the group level may not generalise to individual changes over time.", "published": "2018-11-07 04:11:15", "link": "http://arxiv.org/abs/1811.02750v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Compositional Language Understanding with Text-based Relational\n  Reasoning", "abstract": "Neural networks for natural language reasoning have largely focused on\nextractive, fact-based question-answering (QA) and common-sense inference.\nHowever, it is also crucial to understand the extent to which neural networks\ncan perform relational reasoning and combinatorial generalization from natural\nlanguage---abilities that are often obscured by annotation artifacts and the\ndominance of language modeling in standard QA benchmarks. In this work, we\npresent a novel benchmark dataset for language understanding that isolates\nperformance on relational reasoning. We also present a neural message-passing\nbaseline and show that this model, which incorporates a relational inductive\nbias, is superior at combinatorial generalization compared to a traditional\nrecurrent neural network approach.", "published": "2018-11-07 16:17:48", "link": "http://arxiv.org/abs/1811.02959v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Attention Fusion Networks: Combining Behavior and E-mail Content to\n  Improve Customer Support", "abstract": "Customer support is a central objective at Square as it helps us build and\nmaintain great relationships with our sellers. In order to provide the best\nexperience, we strive to deliver the most accurate and quasi-instantaneous\nresponses to questions regarding our products.\n  In this work, we introduce the Attention Fusion Network model which combines\nsignals extracted from seller interactions on the Square product ecosystem,\nalong with submitted email questions, to predict the most relevant solution to\na seller's inquiry. We show that the innovative combination of two very\ndifferent data sources that are rarely used together, using state-of-the-art\ndeep learning systems outperforms, candidate models that are trained only on a\nsingle source.", "published": "2018-11-07 22:14:32", "link": "http://arxiv.org/abs/1811.03169v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CNN-based MultiChannel End-to-End Speech Recognition for everyday home\n  environments", "abstract": "Casual conversations involving multiple speakers and noises from surrounding\ndevices are common in everyday environments, which degrades the performances of\nautomatic speech recognition systems. These challenging characteristics of\nenvironments are the target of the CHiME-5 challenge. By employing a\nconvolutional neural network (CNN)-based multichannel end-to-end speech\nrecognition system, this study attempts to overcome the presents difficulties\nin everyday environments. The system comprises of an attention-based\nencoder-decoder neural network that directly generates a text as an output from\na sound input. The multichannel CNN encoder, which uses residual connections\nand batch renormalization, is trained with augmented data, including white\nnoise injection. The experimental results show that the word error rate is\nreduced by 8.5% and 0.6% absolute from a single channel end-to-end and the best\nbaseline (LF-MMI TDNN) on the CHiME-5 corpus, respectively.", "published": "2018-11-07 02:38:13", "link": "http://arxiv.org/abs/1811.02735v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video\n  Captioning", "abstract": "Although promising results have been achieved in video captioning, existing\nmodels are limited to the fixed inventory of activities in the training corpus,\nand do not generalize to open vocabulary scenarios. Here we introduce a novel\ntask, zero-shot video captioning, that aims at describing out-of-domain videos\nof unseen activities. Videos of different activities usually require different\ncaptioning strategies in many aspects, i.e. word selection, semantic\nconstruction, and style expression etc, which poses a great challenge to depict\nnovel activities without paired training data. But meanwhile, similar\nactivities share some of those aspects in common. Therefore, We propose a\nprincipled Topic-Aware Mixture of Experts (TAMoE) model for zero-shot video\ncaptioning, which learns to compose different experts based on different topic\nembeddings, implicitly transferring the knowledge learned from seen activities\nto unseen ones. Besides, we leverage external topic-related text corpus to\nconstruct the topic embedding for each activity, which embodies the most\nrelevant semantic vectors within the topic. Empirical results not only validate\nthe effectiveness of our method in utilizing semantic knowledge for video\ncaptioning, but also show its strong generalization ability when describing\nnovel activities.", "published": "2018-11-07 05:33:07", "link": "http://arxiv.org/abs/1811.02765v2", "categories": ["cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Promising Accurate Prefix Boosting for sequence-to-sequence ASR", "abstract": "In this paper, we present promising accurate prefix boosting (PAPB), a\ndiscriminative training technique for attention based sequence-to-sequence\n(seq2seq) ASR. PAPB is devised to unify the training and testing scheme in an\neffective manner. The training procedure involves maximizing the score of each\npartial correct sequence obtained during beam search compared to other\nhypotheses. The training objective also includes minimization of token\n(character) error rate. PAPB shows its efficacy by achieving 10.8\\% and 3.8\\%\nWER with and without RNNLM respectively on Wall Street Journal dataset.", "published": "2018-11-07 05:53:21", "link": "http://arxiv.org/abs/1811.02770v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Blockwise Parallel Decoding for Deep Autoregressive Models", "abstract": "Deep autoregressive sequence-to-sequence models have demonstrated impressive\nperformance across a wide variety of tasks in recent years. While common\narchitecture classes such as recurrent, convolutional, and self-attention\nnetworks make different trade-offs between the amount of computation needed per\nlayer and the length of the critical path at training time, generation still\nremains an inherently sequential process. To overcome this limitation, we\npropose a novel blockwise parallel decoding scheme in which we make predictions\nfor multiple time steps in parallel then back off to the longest prefix\nvalidated by a scoring model. This allows for substantial theoretical\nimprovements in generation speed when applied to architectures that can process\noutput sequences in parallel. We verify our approach empirically through a\nseries of experiments using state-of-the-art self-attention models for machine\ntranslation and image super-resolution, achieving iteration reductions of up to\n2x over a baseline greedy decoder with no loss in quality, or up to 7x in\nexchange for a slight decrease in performance. In terms of wall-clock time, our\nfastest models exhibit real-time speedups of up to 4x over standard greedy\ndecoding.", "published": "2018-11-07 19:09:40", "link": "http://arxiv.org/abs/1811.03115v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Analysis of Multilingual Sequence-to-Sequence speech recognition systems", "abstract": "This paper investigates the applications of various multilingual approaches\ndeveloped in conventional hidden Markov model (HMM) systems to\nsequence-to-sequence (seq2seq) automatic speech recognition (ASR). On a set\ncomposed of Babel data, we first show the effectiveness of multi-lingual\ntraining with stacked bottle-neck (SBN) features. Then we explore various\narchitectures and training strategies of multi-lingual seq2seq models based on\nCTC-attention networks including combinations of output layer, CTC and/or\nattention component re-training. We also investigate the effectiveness of\nlanguage-transfer learning in a very low resource scenario when the target\nlanguage is not included in the original multi-lingual training data.\nInterestingly, we found multilingual features superior to multilingual models,\nand this finding suggests that we can efficiently combine the benefits of the\nHMM system with the seq2seq system through these multilingual feature\ntechniques.", "published": "2018-11-07 09:59:24", "link": "http://arxiv.org/abs/1811.03451v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Learning acoustic word embeddings with phonetically associated triplet\n  network", "abstract": "Previous researches on acoustic word embeddings used in query-by-example\nspoken term detection have shown remarkable performance improvements when using\na triplet network. However, the triplet network is trained using only a limited\ninformation about acoustic similarity between words. In this paper, we propose\na novel architecture, phonetically associated triplet network (PATN), which\naims at increasing discriminative power of acoustic word embeddings by\nutilizing phonetic information as well as word identity. The proposed model is\nlearned to minimize a combined loss function that was made by introducing a\ncross entropy loss to the lower layer of LSTM-based triplet network. We\nobserved that the proposed method performs significantly better than the\nbaseline triplet network on a word discrimination task with the WSJ dataset\nresulting in over 20% relative improvement in recall rate at 1.0 false alarm\nper hour. Finally, we examined the generalization ability by conducting the\nout-of-domain test on the RM dataset.", "published": "2018-11-07 02:38:49", "link": "http://arxiv.org/abs/1811.02736v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "On the use of DNN Autoencoder for Robust Speaker Recognition", "abstract": "In this paper, we present an analysis of a DNN-based autoencoder for speech\nenhancement, dereverberation and denoising. The target application is a robust\nspeaker recognition system. We started with augmenting the Fisher database with\nartificially noised and reverberated data and we trained the autoencoder to map\nnoisy and reverberated speech to its clean version. We use the autoencoder as a\npreprocessing step for a state-of-the-art text-independent speaker recognition\nsystem. We compare results achieved with pure autoencoder enhancement,\nmulti-condition PLDA training and their simultaneous use. We present a detailed\nanalysis with various conditions of NIST SRE 2010, PRISM and artificially\ncorrupted NIST SRE 2010 telephone condition. We conclude that the proposed\npreprocessing significantly outperforms the baseline and that this technique\ncan be used to build a robust speaker recognition system for reverberated and\nnoisy data.", "published": "2018-11-07 15:35:16", "link": "http://arxiv.org/abs/1811.02938v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "High-quality speech coding with SampleRNN", "abstract": "We provide a speech coding scheme employing a generative model based on\nSampleRNN that, while operating at significantly lower bitrates, matches or\nsurpasses the perceptual quality of state-of-the-art classic wide-band codecs.\nMoreover, it is demonstrated that the proposed scheme can provide a meaningful\nrate-distortion trade-off without retraining. We evaluate the proposed scheme\nin a series of listening tests and discuss limitations of the approach.", "published": "2018-11-07 17:20:00", "link": "http://arxiv.org/abs/1811.03021v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Median Binary-Connect Method and a Binary Convolutional Neural Nework\n  for Word Recognition", "abstract": "We propose and study a new projection formula for training binary weight\nconvolutional neural networks. The projection formula measures the error in\napproximating a full precision (32 bit) vector by a 1-bit vector in the l_1\nnorm instead of the standard l_2 norm. The l_1 projector is in closed\nanalytical form and involves a median computation instead of an arithmatic\naverage in the l_2 projector. Experiments on 10 keywords classification show\nthat the l_1 (median) BinaryConnect (BC) method outperforms the regular BC,\nregardless of cold or warm start. The binary network trained by median BC and a\nrecent blending technique reaches test accuracy 92.4%, which is 1.1% lower than\nthe full-precision network accuracy 93.5%. On Android phone app, the trained\nbinary network doubles the speed of full-precision network in spoken keywords\nrecognition.", "published": "2018-11-07 07:46:56", "link": "http://arxiv.org/abs/1811.02784v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Adapting End-to-End Neural Speaker Verification to New Languages and\n  Recording Conditions with Adversarial Training", "abstract": "In this article we propose a novel approach for adapting speaker embeddings\nto new domains based on adversarial training of neural networks. We apply our\nembeddings to the task of text-independent speaker verification, a challenging,\nreal-world problem in biometric security. We further the development of\nend-to-end speaker embedding models by combing a novel 1-dimensional,\nself-attentive residual network, an angular margin loss function and\nadversarial training strategy. Our model is able to learn extremely compact,\n64-dimensional speaker embeddings that deliver competitive performance on a\nnumber of popular datasets using simple cosine distance scoring. One the\nNIST-SRE 2016 task we are able to beat a strong i-vector baseline, while on the\nSpeakers in the Wild task our model was able to outperform both i-vector and\nx-vector baselines, showing an absolute improvement of 2.19% over the latter.\nAdditionally, we show that the integration of adversarial training consistently\nleads to a significant improvement over an unadapted model.", "published": "2018-11-07 18:15:34", "link": "http://arxiv.org/abs/1811.03055v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generative Adversarial Speaker Embedding Networks for Domain Robust\n  End-to-End Speaker Verification", "abstract": "This article presents a novel approach for learning domain-invariant speaker\nembeddings using Generative Adversarial Networks. The main idea is to confuse a\ndomain discriminator so that is can't tell if embeddings are from the source or\ntarget domains. We train several GAN variants using our proposed framework and\napply them to the speaker verification task. On the challenging NIST-SRE 2016\ndataset, we are able to match the performance of a strong baseline x-vector\nsystem. In contrast to the the baseline systems which are dependent on\ndimensionality reduction (LDA) and an external classifier (PLDA), our proposed\nspeaker embeddings can be scored using simple cosine distance. This is achieved\nby optimizing our models end-to-end, using an angular margin loss function.\nFurthermore, we are able to significantly boost verification performance by\naveraging our different GAN models at the score level, achieving a relative\nimprovement of 7.2% over the baseline.", "published": "2018-11-07 18:23:01", "link": "http://arxiv.org/abs/1811.03063v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Class-conditional embeddings for music source separation", "abstract": "Isolating individual instruments in a musical mixture has a myriad of\npotential applications, and seems imminently achievable given the levels of\nperformance reached by recent deep learning methods. While most musical source\nseparation techniques learn an independent model for each instrument, we\npropose using a common embedding space for the time-frequency bins of all\ninstruments in a mixture inspired by deep clustering and deep attractor\nnetworks. Additionally, an auxiliary network is used to generate parameters of\na Gaussian mixture model (GMM) where the posterior distribution over GMM\ncomponents in the embedding space can be used to create a mask that separates\nindividual sources from a mixture. In addition to outperforming a\nmask-inference baseline on the MUSDB-18 dataset, our embedding space is easily\ninterpretable and can be used for query-based separation.", "published": "2018-11-07 18:49:34", "link": "http://arxiv.org/abs/1811.03076v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
