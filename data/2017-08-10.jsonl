{"title": "Location Name Extraction from Targeted Text Streams using\n  Gazetteer-based Statistical Language Models", "abstract": "Extracting location names from informal and unstructured social media data\nrequires the identification of referent boundaries and partitioning compound\nnames. Variability, particularly systematic variability in location names\n(Carroll, 1983), challenges the identification task. Some of this variability\ncan be anticipated as operations within a statistical language model, in this\ncase drawn from gazetteers such as OpenStreetMap (OSM), Geonames, and DBpedia.\nThis permits evaluation of an observed n-gram in Twitter targeted text as a\nlegitimate location name variant from the same location-context. Using n-gram\nstatistics and location-related dictionaries, our Location Name Extraction tool\n(LNEx) handles abbreviations and automatically filters and augments the\nlocation names in gazetteers (handling name contractions and auxiliary\ncontents) to help detect the boundaries of multi-word location names and\nthereby delimit them in texts.\n  We evaluated our approach on 4,500 event-specific tweets from three targeted\nstreams to compare the performance of LNEx against that of ten state-of-the-art\ntaggers that rely on standard semantic, syntactic and/or orthographic features.\nLNEx improved the average F-Score by 33-179%, outperforming all taggers.\nFurther, LNEx is capable of stream processing.", "published": "2017-08-10 07:43:13", "link": "http://arxiv.org/abs/1708.03105v3", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Neural Speaker Modeling in Multi-Party Conversation: The Task,\n  Dataset, and Models", "abstract": "Neural network-based dialog systems are attracting increasing attention in\nboth academia and industry. Recently, researchers have begun to realize the\nimportance of speaker modeling in neural dialog systems, but there lacks\nestablished tasks and datasets. In this paper, we propose speaker\nclassification as a surrogate task for general speaker modeling, and collect\nmassive data to facilitate research in this direction. We further investigate\ntemporal-based and content-based models of speakers, and propose several\nhybrids of them. Experiments show that speaker classification is feasible, and\nthat hybrid models outperform each single component.", "published": "2017-08-10 10:21:31", "link": "http://arxiv.org/abs/1708.03152v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural and Statistical Methods for Leveraging Meta-information in\n  Machine Translation", "abstract": "In this paper, we discuss different methods which use meta information and\nricher context that may accompany source language input to improve machine\ntranslation quality. We focus on category information of input text as meta\ninformation, but the proposed methods can be extended to all textual and\nnon-textual meta information that might be available for the input text or\nautomatically predicted using the text content. The main novelty of this work\nis to use state-of-the-art neural network methods to tackle this problem within\na statistical machine translation (SMT) framework. We observe translation\nquality improvements up to 3% in terms of BLEU score in some text categories.", "published": "2017-08-10 12:48:07", "link": "http://arxiv.org/abs/1708.03186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation Leveraging Phrase-based Models in a Hybrid\n  Search", "abstract": "In this paper, we introduce a hybrid search for attention-based neural\nmachine translation (NMT). A target phrase learned with statistical MT models\nextends a hypothesis in the NMT beam search when the attention of the NMT model\nfocuses on the source words translated by this phrase. Phrases added in this\nway are scored with the NMT model, but also with SMT features including\nphrase-level translation probabilities and a target language model.\nExperimental results on German->English news domain and English->Russian\ne-commerce domain translation tasks show that using phrase-based models in NMT\nsearch improves MT quality by up to 2.3% BLEU absolute as compared to a strong\nNMT baseline.", "published": "2017-08-10 15:48:33", "link": "http://arxiv.org/abs/1708.03271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Radical-level Ideograph Encoder for RNN-based Sentiment Analysis of\n  Chinese and Japanese", "abstract": "The character vocabulary can be very large in non-alphabetic languages such\nas Chinese and Japanese, which makes neural network models huge to process such\nlanguages. We explored a model for sentiment classification that takes the\nembeddings of the radicals of the Chinese characters, i.e, hanzi of Chinese and\nkanji of Japanese. Our model is composed of a CNN word feature encoder and a\nbi-directional RNN document feature encoder. The results achieved are on par\nwith the character embedding-based models, and close to the state-of-the-art\nword embedding-based models, with 90% smaller vocabulary, and at least 13% and\n80% fewer parameters than the character embedding-based models and word\nembedding-based models respectively. The results suggest that the radical\nembedding-based approach is cost-effective for machine learning on Chinese and\nJapanese.", "published": "2017-08-10 17:46:28", "link": "http://arxiv.org/abs/1708.03312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Sense of Word Embeddings", "abstract": "We present a simple yet effective approach for learning word sense\nembeddings. In contrast to existing techniques, which either directly learn\nsense representations from corpora or rely on sense inventories from lexical\nresources, our approach can induce a sense inventory from existing word\nembeddings via clustering of ego-networks of related words. An integrated WSD\nmechanism enables labeling of words in context with learned sense vectors,\nwhich gives rise to downstream applications. Experiments show that the\nperformance of our method is comparable to state-of-the-art unsupervised WSD\nsystems.", "published": "2017-08-10 21:27:57", "link": "http://arxiv.org/abs/1708.03390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SESA: Supervised Explicit Semantic Analysis", "abstract": "In recent years supervised representation learning has provided state of the\nart or close to the state of the art results in semantic analysis tasks\nincluding ranking and information retrieval. The core idea is to learn how to\nembed items into a latent space such that they optimize a supervised objective\nin that latent space. The dimensions of the latent space have no clear\nsemantics, and this reduces the interpretability of the system. For example, in\npersonalization models, it is hard to explain why a particular item is ranked\nhigh for a given user profile. We propose a novel model of representation\nlearning called Supervised Explicit Semantic Analysis (SESA) that is trained in\na supervised fashion to embed items to a set of dimensions with explicit\nsemantics. The model learns to compare two objects by representing them in this\nexplicit space, where each dimension corresponds to a concept from a knowledge\nbase. This work extends Explicit Semantic Analysis (ESA) with a supervised\nmodel for ranking problems. We apply this model to the task of Job-Profile\nrelevance in LinkedIn in which a set of skills defines our explicit dimensions\nof the space. Every profile and job are encoded to this set of skills their\nsimilarity is calculated in this space. We use RNNs to embed text input into\nthis space. In addition to interpretability, our model makes use of the\nweb-scale collaborative skills data that is provided by users for each LinkedIn\nprofile. Our model provides state of the art result while it remains\ninterpretable.", "published": "2017-08-10 15:03:12", "link": "http://arxiv.org/abs/1708.03246v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"Is there anything else I can help you with?\": Challenges in Deploying\n  an On-Demand Crowd-Powered Conversational Agent", "abstract": "Intelligent conversational assistants, such as Apple's Siri, Microsoft's\nCortana, and Amazon's Echo, have quickly become a part of our digital life.\nHowever, these assistants have major limitations, which prevents users from\nconversing with them as they would with human dialog partners. This limits our\nability to observe how users really want to interact with the underlying\nsystem. To address this problem, we developed a crowd-powered conversational\nassistant, Chorus, and deployed it to see how users and workers would interact\ntogether when mediated by the system. Chorus sophisticatedly converses with end\nusers over time by recruiting workers on demand, which in turn decide what\nmight be the best response for each user sentence. Up to the first month of our\ndeployment, 59 users have held conversations with Chorus during 320\nconversational sessions. In this paper, we present an account of Chorus'\ndeployment, with a focus on four challenges: (i) identifying when conversations\nare over, (ii) malicious users and workers, (iii) on-demand recruiting, and\n(iv) settings in which consensus is not enough. Our observations could assist\nthe deployment of crowd-powered conversation systems and crowd-powered systems\nin general.", "published": "2017-08-10 01:40:49", "link": "http://arxiv.org/abs/1708.03044v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Communication-Free Parallel Supervised Topic Models", "abstract": "Embarrassingly (communication-free) parallel Markov chain Monte Carlo (MCMC)\nmethods are commonly used in learning graphical models. However, MCMC cannot be\ndirectly applied in learning topic models because of the quasi-ergodicity\nproblem caused by multimodal distribution of topics. In this paper, we develop\nan embarrassingly parallel MCMC algorithm for sLDA. Our algorithm works by\nswitching the order of sampled topics combination and labeling variable\nprediction in sLDA, in which it overcomes the quasi-ergodicity problem because\nhigh-dimension topics that follow a multimodal distribution are projected into\none-dimension document labels that follow a unimodal distribution. Our\nempirical experiments confirm that the out-of-sample prediction performance\nusing our embarrassingly parallel algorithm is comparable to non-parallel sLDA\nwhile the computation time is significantly reduced.", "published": "2017-08-10 02:03:52", "link": "http://arxiv.org/abs/1708.03052v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
