{"title": "Improving Commonsense Causal Reasoning by Adversarial Training and Data\n  Augmentation", "abstract": "Determining the plausibility of causal relations between clauses is a\ncommonsense reasoning task that requires complex inference ability. The general\napproach to this task is to train a large pretrained language model on a\nspecific dataset. However, the available training data for the task is often\nscarce, which leads to instability of model training or reliance on the shallow\nfeatures of the dataset. This paper presents a number of techniques for making\nmodels more robust in the domain of causal reasoning. Firstly, we perform\nadversarial training by generating perturbed inputs through synonym\nsubstitution. Secondly, based on a linguistic theory of discourse connectives,\nwe perform data augmentation using a discourse parser for detecting causally\nlinked clauses in large text, and a generative language model for generating\ndistractors. Both methods boost model performance on the Choice of Plausible\nAlternatives (COPA) dataset, as well as on a Balanced COPA dataset, which is a\nmodified version of the original data that has been developed to avoid\nsuperficial cues, leading to a more challenging benchmark. We show a\nstatistically significant improvement in performance and robustness on both\ndatasets, even with only a small number of additionally generated data points.", "published": "2021-01-13 09:55:29", "link": "http://arxiv.org/abs/2101.04966v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coarse and Fine-Grained Hostility Detection in Hindi Posts using Fine\n  Tuned Multilingual Embeddings", "abstract": "Due to the wide adoption of social media platforms like Facebook, Twitter,\netc., there is an emerging need of detecting online posts that can go against\nthe community acceptance standards. The hostility detection task has been well\nexplored for resource-rich languages like English, but is unexplored for\nresource-constrained languages like Hindidue to the unavailability of large\nsuitable data. We view this hostility detection as a multi-label multi-class\nclassification problem. We propose an effective neural network-based technique\nfor hostility detection in Hindi posts. We leverage pre-trained multilingual\nBidirectional Encoder Representations of Transformer (mBERT) to obtain the\ncontextual representations of Hindi posts. We have performed extensive\nexperiments including different pre-processing techniques, pre-trained models,\nneural architectures, hybrid strategies, etc. Our best performing neural\nclassifier model includes One-vs-the-Rest approach where we obtained 92.60%,\n81.14%,69.59%, 75.29% and 73.01% F1 scores for hostile, fake, hate, offensive,\nand defamation labels respectively. The proposed model outperformed the\nexisting baseline models and emerged as the state-of-the-art model for\ndetecting hostility in the Hindi posts.", "published": "2021-01-13 11:00:31", "link": "http://arxiv.org/abs/2101.04998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uzbek Cyrillic-Latin-Cyrillic Machine Transliteration", "abstract": "In this paper, we introduce a data-driven approach to transliterating Uzbek\ndictionary words from the Cyrillic script into the Latin script, and vice\nversa. We heuristically align characters of words in the source script with\nsub-strings of the corresponding words in the target script and train a\ndecision tree classifier that learns these alignments. On the test set, our\nCyrillic to Latin model achieves a character level micro-averaged F1 score of\n0.9992, and our Latin to Cyrillic model achieves the score of 0.9959. Our\ncontribution is a novel method of producing machine transliterated texts for\nthe low-resource Uzbek language.", "published": "2021-01-13 15:59:43", "link": "http://arxiv.org/abs/2101.05162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On consistency scores in text data with an implementation in R", "abstract": "In this paper, we introduce a reproducible cleaning process for the text\nextracted from PDFs using n-gram models. Our approach compares the originally\nextracted text with the text generated from, or expected by, these models using\nearlier text as stimulus. To guide this process, we introduce the notion of a\nconsistency score, which refers to the proportion of text that is expected by\nthe model. This is used to monitor changes during the cleaning process, and\nacross different corpuses. We illustrate our process on text from the book Jane\nEyre and introduce both a Shiny application and an R package to make our\nprocess easier for others to adopt.", "published": "2021-01-13 17:37:07", "link": "http://arxiv.org/abs/2101.05225v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "geoGAT: Graph Model Based on Attention Mechanism for Geographic Text\n  Classification", "abstract": "In the area of geographic information processing. There are few researches on\ngeographic text classification. However, the application of this task in\nChinese is relatively rare. In our work, we intend to implement a method to\nextract text containing geographical entities from a large number of network\ntext. The geographic information in these texts is of great practical\nsignificance to transportation, urban and rural planning, disaster relief and\nother fields. We use the method of graph convolutional neural network with\nattention mechanism to achieve this function. Graph attention networks is an\nimprovement of graph convolutional neural networks. Compared with GCN, the\nadvantage of GAT is that the attention mechanism is proposed to weight the sum\nof the characteristics of adjacent nodes. In addition, We construct a Chinese\ndataset containing geographical classification from multiple datasets of\nChinese text classification. The Macro-F Score of the geoGAT we used reached\n95\\% on the new Chinese dataset.", "published": "2021-01-13 09:32:15", "link": "http://arxiv.org/abs/2101.11424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Experimental Evaluation of Deep Learning models for Marathi Text\n  Classification", "abstract": "The Marathi language is one of the prominent languages used in India. It is\npredominantly spoken by the people of Maharashtra. Over the past decade, the\nusage of language on online platforms has tremendously increased. However,\nresearch on Natural Language Processing (NLP) approaches for Marathi text has\nnot received much attention. Marathi is a morphologically rich language and\nuses a variant of the Devanagari script in the written form. This works aims to\nprovide a comprehensive overview of available resources and models for Marathi\ntext classification. We evaluate CNN, LSTM, ULMFiT, and BERT based models on\ntwo publicly available Marathi text classification datasets and present a\ncomparative analysis. The pre-trained Marathi fast text word embeddings by\nFacebook and IndicNLP are used in conjunction with word-based models. We show\nthat basic single layer models based on CNN and LSTM coupled with FastText\nembeddings perform on par with the BERT based models on the available datasets.\nWe hope our paper aids focused research and experiments in the area of Marathi\nNLP.", "published": "2021-01-13 06:21:27", "link": "http://arxiv.org/abs/2101.04899v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LaDiff ULMFiT: A Layer Differentiated training approach for ULMFiT", "abstract": "In our paper, we present Deep Learning models with a layer differentiated\ntraining method which were used for the SHARED TASK@ CONSTRAINT 2021 sub-tasks\nCOVID19 Fake News Detection in English and Hostile Post Detection in Hindi. We\npropose a Layer Differentiated training procedure for training a pre-trained\nULMFiT arXiv:1801.06146 model. We used special tokens to annotate specific\nparts of the tweets to improve language understanding and gain insights on the\nmodel making the tweets more interpretable. The other two submissions included\na modified RoBERTa model and a simple Random Forest Classifier. The proposed\napproach scored a precision and f1 score of 0.96728972 and 0.967324832\nrespectively for sub-task \"COVID19 Fake News Detection in English\". Also,\nCoarse-Grained Hostility f1 Score and Weighted FineGrained f1 score of 0.908648\nand 0.533907 respectively for sub-task Hostile Post Detection in Hindi. The\nproposed approach ranked 61st out of 164 in the sub-task \"COVID19 Fake News\nDetection in English and 18th out of 45 in the sub-task Hostile Post Detection\nin Hindi\".", "published": "2021-01-13 09:52:04", "link": "http://arxiv.org/abs/2101.04965v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Joint Learning of Hyperbolic Label Embeddings for Hierarchical\n  Multi-label Classification", "abstract": "We consider the problem of multi-label classification where the labels lie in\na hierarchy. However, unlike most existing works in hierarchical multi-label\nclassification, we do not assume that the label-hierarchy is known. Encouraged\nby the recent success of hyperbolic embeddings in capturing hierarchical\nrelations, we propose to jointly learn the classifier parameters as well as the\nlabel embeddings. Such a joint learning is expected to provide a twofold\nadvantage: i) the classifier generalizes better as it leverages the prior\nknowledge of existence of a hierarchy over the labels, and ii) in addition to\nthe label co-occurrence information, the label-embedding may benefit from the\nmanifold structure of the input datapoints, leading to embeddings that are more\nfaithful to the label hierarchy. We propose a novel formulation for the joint\nlearning and empirically evaluate its efficacy. The results show that the joint\nlearning improves over the baseline that employs label co-occurrence based\npre-trained hyperbolic embeddings. Moreover, the proposed classifiers achieve\nstate-of-the-art generalization on standard benchmarks. We also present\nevaluation of the hyperbolic embeddings obtained by joint learning and show\nthat they represent the hierarchy more accurately than the other alternatives.", "published": "2021-01-13 10:58:54", "link": "http://arxiv.org/abs/2101.04997v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Discrete Knowledge Graph Embedding based on Discrete Optimization", "abstract": "This paper proposes a discrete knowledge graph (KG) embedding (DKGE) method,\nwhich projects KG entities and relations into the Hamming space based on a\ncomputationally tractable discrete optimization algorithm, to solve the\nformidable storage and computation cost challenges in traditional continuous\ngraph embedding methods. The convergence of DKGE can be guaranteed\ntheoretically. Extensive experiments demonstrate that DKGE achieves superior\naccuracy than classical hashing functions that map the effective continuous\nembeddings into discrete codes. Besides, DKGE reaches comparable accuracy with\nmuch lower computational complexity and storage compared to many continuous\ngraph embedding methods.", "published": "2021-01-13 00:23:07", "link": "http://arxiv.org/abs/2101.04817v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Robustness Gym: Unifying the NLP Evaluation Landscape", "abstract": "Despite impressive performance on standard benchmarks, deep neural networks\nare often brittle when deployed in real-world systems. Consequently, recent\nresearch has focused on testing the robustness of such models, resulting in a\ndiverse set of evaluation methodologies ranging from adversarial attacks to\nrule-based data transformations. In this work, we identify challenges with\nevaluating NLP systems and propose a solution in the form of Robustness Gym\n(RG), a simple and extensible evaluation toolkit that unifies 4 standard\nevaluation paradigms: subpopulations, transformations, evaluation sets, and\nadversarial attacks. By providing a common platform for evaluation, Robustness\nGym enables practitioners to compare results from all 4 evaluation paradigms\nwith just a few clicks, and to easily develop and share novel evaluation\nmethods using a built-in set of abstractions. To validate Robustness Gym's\nutility to practitioners, we conducted a real-world case study with a\nsentiment-modeling team, revealing performance degradations of 18%+. To verify\nthat Robustness Gym can aid novel research analyses, we perform the first study\nof state-of-the-art commercial and academic named entity linking (NEL) systems,\nas well as a fine-grained analysis of state-of-the-art summarization models.\nFor NEL, commercial systems struggle to link rare entities and lag their\nacademic counterparts by 10%+, while state-of-the-art summarization models\nstruggle on examples that require abstraction and distillation, degrading by\n9%+. Robustness Gym can be found at https://robustnessgym.com/", "published": "2021-01-13 02:37:54", "link": "http://arxiv.org/abs/2101.04840v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Sequence-to-grid Module for Learning Symbolic Rules", "abstract": "Logical reasoning tasks over symbols, such as learning arithmetic operations\nand computer program evaluations, have become challenges to deep learning. In\nparticular, even state-of-the-art neural networks fail to achieve\n\\textit{out-of-distribution} (OOD) generalization of symbolic reasoning tasks,\nwhereas humans can easily extend learned symbolic rules. To resolve this\ndifficulty, we propose a neural sequence-to-grid (seq2grid) module, an input\npreprocessor that automatically segments and aligns an input sequence into a\ngrid. As our module outputs a grid via a novel differentiable mapping, any\nneural network structure taking a grid input, such as ResNet or TextCNN, can be\njointly trained with our module in an end-to-end fashion. Extensive experiments\nshow that neural networks having our module as an input preprocessor achieve\nOOD generalization on various arithmetic and algorithmic problems including\nnumber sequence prediction problems, algebraic word problems, and computer\nprogram evaluation problems while other state-of-the-art sequence transduction\nmodels cannot. Moreover, we verify that our module enhances TextCNN to solve\nthe bAbI QA tasks without external memory.", "published": "2021-01-13 07:53:14", "link": "http://arxiv.org/abs/2101.04921v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EventPlus: A Temporal Event Understanding Pipeline", "abstract": "We present EventPlus, a temporal event understanding pipeline that integrates\nvarious state-of-the-art event understanding components including event trigger\nand type detection, event argument detection, event duration and temporal\nrelation extraction. Event information, especially event temporal knowledge, is\na type of common sense knowledge that helps people understand how stories\nevolve and provides predictive hints for future events. EventPlus as the first\ncomprehensive temporal event understanding pipeline provides a convenient tool\nfor users to quickly obtain annotations about events and their temporal\ninformation for any user-provided document. Furthermore, we show EventPlus can\nbe easily adapted to other domains (e.g., biomedical domain). We make EventPlus\npublicly available to facilitate event-related information extraction and\ndownstream applications.", "published": "2021-01-13 08:00:50", "link": "http://arxiv.org/abs/2101.04922v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Is the User Enjoying the Conversation? A Case Study on the Impact on the\n  Reward Function", "abstract": "The impact of user satisfaction in policy learning task-oriented dialogue\nsystems has long been a subject of research interest. Most current models for\nestimating the user satisfaction either (i) treat out-of-context short-texts,\nsuch as product reviews, or (ii) rely on turn features instead of on\ndistributed semantic representations. In this work we adopt deep neural\nnetworks that use distributed semantic representation learning for estimating\nthe user satisfaction in conversations. We evaluate the impact of modelling\ncontext length in these networks. Moreover, we show that the proposed\nhierarchical network outperforms state-of-the-art quality estimators.\nFurthermore, we show that applying these networks to infer the reward function\nin a Partial Observable Markov Decision Process (POMDP) yields to a great\nimprovement in the task success rate.", "published": "2021-01-13 11:13:07", "link": "http://arxiv.org/abs/2101.05004v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "End-to-End Speaker Height and age estimation using Attention Mechanism\n  with LSTM-RNN", "abstract": "Automatic height and age estimation of speakers using acoustic features is\nwidely used for the purpose of human-computer interaction, forensics, etc. In\nthis work, we propose a novel approach of using attention mechanism to build an\nend-to-end architecture for height and age estimation. The attention mechanism\nis combined with Long Short-Term Memory(LSTM) encoder which is able to capture\nlong-term dependencies in the input acoustic features. We modify the\nconventionally used Attention -- which calculates context vectors the sum of\nattention only across timeframes -- by introducing a modified context vector\nwhich takes into account total attention across encoder units as well, giving\nus a new cross-attention mechanism. Apart from this, we also investigate a\nmulti-task learning approach for jointly estimating speaker height and age. We\ntrain and test our model on the TIMIT corpus. Our model outperforms several\napproaches in the literature. We achieve a root mean square error (RMSE) of\n6.92cm and6.34cm for male and female heights respectively and RMSE of 7.85years\nand 8.75years for male and females ages respectively. By tracking the attention\nweights allocated to different phones, we find that Vowel phones are most\nimportant whistlestop phones are least important for the estimation task.", "published": "2021-01-13 13:41:18", "link": "http://arxiv.org/abs/2101.05056v1", "categories": ["cs.SD", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Whispered and Lombard Neural Speech Synthesis", "abstract": "It is desirable for a text-to-speech system to take into account the\nenvironment where synthetic speech is presented, and provide appropriate\ncontext-dependent output to the user. In this paper, we present and compare\nvarious approaches for generating different speaking styles, namely, normal,\nLombard, and whisper speech, using only limited data. The following systems are\nproposed and assessed: 1) Pre-training and fine-tuning a model for each style.\n2) Lombard and whisper speech conversion through a signal processing based\napproach. 3) Multi-style generation using a single model based on a speaker\nverification model. Our mean opinion score and AB preference listening tests\nshow that 1) we can generate high quality speech through the\npre-training/fine-tuning approach for all speaking styles. 2) Although our\nspeaker verification (SV) model is not explicitly trained to discriminate\ndifferent speaking styles, and no Lombard and whisper voice is used for\npre-training this system, the SV model can be used as a style encoder for\ngenerating different style embeddings as input for the Tacotron system. We also\nshow that the resulting synthetic Lombard speech has a significant positive\nimpact on intelligibility gain.", "published": "2021-01-13 19:22:11", "link": "http://arxiv.org/abs/2101.05313v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Attention-based Representation Learning for Heart Sound\n  Classification", "abstract": "Cardiovascular diseases are the leading cause of deaths and severely threaten\nhuman health in daily life. On the one hand, there have been dramatically\nincreasing demands from both the clinical practice and the smart home\napplication for monitoring the heart status of subjects suffering from chronic\ncardiovascular diseases. On the other hand, experienced physicians who can\nperform an efficient auscultation are still lacking in terms of number.\nAutomatic heart sound classification leveraging the power of advanced signal\nprocessing and machine learning technologies has shown encouraging results.\nNevertheless, human hand-crafted features are expensive and time-consuming. To\nthis end, we propose a novel deep representation learning method with an\nattention mechanism for heart sound classification. In this paradigm,\nhigh-level representations are learnt automatically from the recorded heart\nsound data. Particularly, a global attention pooling layer improves the\nperformance of the learnt representations by estimating the contribution of\neach unit in feature maps. The Heart Sounds Shenzhen (HSS) corpus (170 subjects\ninvolved) is used to validate the proposed method. Experimental results\nvalidate that, our approach can achieve an unweighted average recall of 51.2%\nfor classifying three categories of heart sounds, i. e., normal, mild, and\nmoderate/severe annotated by cardiologists with the help of Echocardiography.", "published": "2021-01-13 10:20:59", "link": "http://arxiv.org/abs/2101.04979v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Piano Skills Assessment", "abstract": "Can a computer determine a piano player's skill level? Is it preferable to\nbase this assessment on visual analysis of the player's performance or should\nwe trust our ears over our eyes? Since current CNNs have difficulty processing\nlong video videos, how can shorter clips be sampled to best reflect the players\nskill level? In this work, we collect and release a first-of-its-kind dataset\nfor multimodal skill assessment focusing on assessing piano player's skill\nlevel, answer the asked questions, initiate work in automated evaluation of\npiano playing skills and provide baselines for future work. Dataset is\navailable from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.", "published": "2021-01-13 05:26:29", "link": "http://arxiv.org/abs/2101.04884v2", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Effective Low-Cost Time-Domain Audio Separation Using Globally Attentive\n  Locally Recurrent Networks", "abstract": "Recent research on the time-domain audio separation networks (TasNets) has\nbrought great success to speech separation. Nevertheless, conventional TasNets\nstruggle to satisfy the memory and latency constraints in industrial\napplications. In this regard, we design a low-cost high-performance\narchitecture, namely, globally attentive locally recurrent (GALR) network.\nAlike the dual-path RNN (DPRNN), we first split a feature sequence into 2D\nsegments and then process the sequence along both the intra- and inter-segment\ndimensions. Our main innovation lies in that, on top of features recurrently\nprocessed along the inter-segment dimensions, GALR applies a self-attention\nmechanism to the sequence along the inter-segment dimension, which aggregates\ncontext-aware information and also enables parallelization. Our experiments\nsuggest that GALR is a notably more effective network than the prior work. On\none hand, with only 1.5M parameters, it has achieved comparable separation\nperformance at a much lower cost with 36.1% less runtime memory and 49.4% fewer\ncomputational operations, relative to the DPRNN. On the other hand, in a\ncomparable model size with DPRNN, GALR has consistently outperformed DPRNN in\nthree datasets, in particular, with a substantial margin of 2.4dB absolute\nimprovement of SI-SNRi in the benchmark WSJ0-2mix task.", "published": "2021-01-13 11:30:14", "link": "http://arxiv.org/abs/2101.05014v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
