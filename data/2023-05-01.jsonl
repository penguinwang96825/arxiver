{"title": "Low-Resourced Machine Translation for Senegalese Wolof Language", "abstract": "Natural Language Processing (NLP) research has made great advancements in\nrecent years with major breakthroughs that have established new benchmarks.\nHowever, these advances have mainly benefited a certain group of languages\ncommonly referred to as resource-rich such as English and French. Majority of\nother languages with weaker resources are then left behind which is the case\nfor most African languages including Wolof. In this work, we present a parallel\nWolof/French corpus of 123,000 sentences on which we conducted experiments on\nmachine translation models based on Recurrent Neural Networks (RNN) in\ndifferent data configurations. We noted performance gains with the models\ntrained on subworded data as well as those trained on the French-English\nlanguage pair compared to those trained on the French-Wolof pair under the same\nexperimental conditions.", "published": "2023-05-01 00:04:19", "link": "http://arxiv.org/abs/2305.00606v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Paper Screening for Clinical Reviews Using Large Language\n  Models", "abstract": "Objective: To assess the performance of the OpenAI GPT API in accurately and\nefficiently identifying relevant titles and abstracts from real-world clinical\nreview datasets and compare its performance against ground truth labelling by\ntwo independent human reviewers.\n  Methods: We introduce a novel workflow using the OpenAI GPT API for screening\ntitles and abstracts in clinical reviews. A Python script was created to make\ncalls to the GPT API with the screening criteria in natural language and a\ncorpus of title and abstract datasets that have been filtered by a minimum of\ntwo human reviewers. We compared the performance of our model against\nhuman-reviewed papers across six review papers, screening over 24,000 titles\nand abstracts.\n  Results: Our results show an accuracy of 0.91, a sensitivity of excluded\npapers of 0.91, and a sensitivity of included papers of 0.76. On a randomly\nselected subset of papers, the GPT API demonstrated the ability to provide\nreasoning for its decisions and corrected its initial decision upon being asked\nto explain its reasoning for a subset of incorrect classifications.\n  Conclusion: The GPT API has the potential to streamline the clinical review\nprocess, save valuable time and effort for researchers, and contribute to the\noverall quality of clinical reviews. By prioritizing the workflow and acting as\nan aid rather than a replacement for researchers and reviewers, the GPT API can\nenhance efficiency and lead to more accurate and reliable conclusions in\nmedical research.", "published": "2023-05-01 14:16:37", "link": "http://arxiv.org/abs/2305.00844v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Linguistic Models: Investigating LLMs' metalinguistic abilities", "abstract": "The performance of large language models (LLMs) has recently improved to the\npoint where the models can perform well on many language tasks. We show here\nthat -- for the first time -- the models can also generate valid metalinguistic\nanalyses of language data. We outline a research program where the behavioral\ninterpretability of LLMs on these tasks is tested via prompting. LLMs are\ntrained primarily on text -- as such, evaluating their metalinguistic abilities\nimproves our understanding of their general capabilities and sheds new light on\ntheoretical models in linguistics. We show that OpenAI's o1 vastly outperforms\nother models on tasks involving drawing syntactic trees and phonological\ngeneralization. We speculate that OpenAI o1's unique advantage over other\nmodels may result from the model's chain-of-thought mechanism, which mimics the\nstructure of human reasoning used in complex cognitive tasks, such as\nlinguistic analysis.", "published": "2023-05-01 17:09:33", "link": "http://arxiv.org/abs/2305.00948v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deception Detection with Feature-Augmentation by soft Domain Transfer", "abstract": "In this era of information explosion, deceivers use different domains or\nmediums of information to exploit the users, such as News, Emails, and Tweets.\nAlthough numerous research has been done to detect deception in all these\ndomains, information shortage in a new event necessitates these domains to\nassociate with each other to battle deception. To form this association, we\npropose a feature augmentation method by harnessing the intermediate layer\nrepresentation of neural models. Our approaches provide an improvement over the\nself-domain baseline models by up to 6.60%. We find Tweets to be the most\nhelpful information provider for Fake News and Phishing Email detection,\nwhereas News helps most in Tweet Rumor detection. Our analysis provides a\nuseful insight for domain knowledge transfer which can help build a stronger\ndeception detection system than the existing literature.", "published": "2023-05-01 18:00:54", "link": "http://arxiv.org/abs/2305.01011v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating statistical language models as pragmatic reasoners", "abstract": "The relationship between communicated language and intended meaning is often\nprobabilistic and sensitive to context. Numerous strategies attempt to estimate\nsuch a mapping, often leveraging recursive Bayesian models of communication. In\nparallel, large language models (LLMs) have been increasingly applied to\nsemantic parsing applications, tasked with inferring logical representations\nfrom natural language. While existing LLM explorations have been largely\nrestricted to literal language use, in this work, we evaluate the capacity of\nLLMs to infer the meanings of pragmatic utterances. Specifically, we explore\nthe case of threshold estimation on the gradable adjective ``strong'',\ncontextually conditioned on a strength prior, then extended to composition with\nqualification, negation, polarity inversion, and class comparison. We find that\nLLMs can derive context-grounded, human-like distributions over the\ninterpretations of several complex pragmatic utterances, yet struggle composing\nwith negation. These results inform the inferential capacity of statistical\nlanguage models, and their use in pragmatic and semantic parsing applications.\nAll corresponding code is made publicly available\n(https://github.com/benlipkin/probsem/tree/CogSci2023).", "published": "2023-05-01 18:22:10", "link": "http://arxiv.org/abs/2305.01020v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Company classification using zero-shot learning", "abstract": "In recent years, natural language processing (NLP) has become increasingly\nimportant in a variety of business applications, including sentiment analysis,\ntext classification, and named entity recognition. In this paper, we propose an\napproach for company classification using NLP and zero-shot learning. Our\nmethod utilizes pre-trained transformer models to extract features from company\ndescriptions, and then applies zero-shot learning to classify companies into\nrelevant categories without the need for specific training data for each\ncategory. We evaluate our approach on a dataset obtained through the Wharton\nResearch Data Services (WRDS), which comprises textual descriptions of publicly\ntraded companies. We demonstrate that the approach can streamline the process\nof company classification, thereby reducing the time and resources required in\ntraditional approaches such as the Global Industry Classification Standard\n(GICS). The results show that this method has potential for automation of\ncompany classification, making it a promising avenue for future research in\nthis area.", "published": "2023-05-01 18:36:06", "link": "http://arxiv.org/abs/2305.01028v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Logion: Machine Learning for Greek Philology", "abstract": "This paper presents machine-learning methods to address various problems in\nGreek philology. After training a BERT model on the largest premodern Greek\ndataset used for this purpose to date, we identify and correct previously\nundetected errors made by scribes in the process of textual transmission, in\nwhat is, to our knowledge, the first successful identification of such errors\nvia machine learning. Additionally, we demonstrate the model's capacity to fill\ngaps caused by material deterioration of premodern manuscripts and compare the\nmodel's performance to that of a domain expert. We find that best performance\nis achieved when the domain expert is provided with model suggestions for\ninspiration. With such human-computer collaborations in mind, we explore the\nmodel's interpretability and find that certain attention heads appear to encode\nselect grammatical features of premodern Greek.", "published": "2023-05-01 21:56:25", "link": "http://arxiv.org/abs/2305.01099v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Self-Evaluation Guided Beam Search for Reasoning", "abstract": "Breaking down a problem into intermediate steps has demonstrated impressive\nperformance in Large Language Model (LLM) reasoning. However, the growth of the\nreasoning chain introduces uncertainty and error accumulation, making it\nchallenging to elicit accurate final results. To tackle this challenge of\nuncertainty in multi-step reasoning, we introduce a stepwise self-evaluation\nmechanism to guide and calibrate the reasoning process of LLMs. We propose a\ndecoding algorithm integrating the self-evaluation guidance via stochastic beam\nsearch. The self-evaluation guidance serves as a better-calibrated automatic\ncriterion, facilitating an efficient search in the reasoning space and\nresulting in superior prediction quality. Stochastic beam search balances\nexploitation and exploration of the search space with temperature-controlled\nrandomness. Our approach surpasses the corresponding Codex-backboned baselines\nin few-shot accuracy by $6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA,\nand StrategyQA benchmarks, respectively. Experiment results with Llama-2 on\narithmetic reasoning demonstrate the efficiency of our method in outperforming\nthe baseline methods with comparable computational budgets. Further analysis in\nmulti-step reasoning finds our self-evaluation guidance pinpoints logic\nfailures and leads to higher consistency and robustness. Our code is publicly\navailable at https://guideddecoding.github.io/.", "published": "2023-05-01 02:37:59", "link": "http://arxiv.org/abs/2305.00633v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Reason and Memorize with Self-Notes", "abstract": "Large language models have been shown to struggle with multi-step reasoning,\nand do not retain previous reasoning steps for future use. We propose a simple\nmethod for solving both of these problems by allowing the model to take\nSelf-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model\ncan deviate from the input context at any time to explicitly think and write\ndown its thoughts. This allows the model to perform reasoning on the fly as it\nreads the context and even integrate previous reasoning steps, thus enhancing\nits memory with useful information and enabling multi-step reasoning.\nExperiments across a wide variety of tasks demonstrate that our method can\noutperform chain-of-thought and scratchpad methods by taking Self-Notes that\ninterleave the input text.", "published": "2023-05-01 14:02:48", "link": "http://arxiv.org/abs/2305.00833v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Joint Modelling of Spoken Language Understanding Tasks with Integrated\n  Dialog History", "abstract": "Most human interactions occur in the form of spoken conversations where the\nsemantic meaning of a given utterance depends on the context. Each utterance in\nspoken conversation can be represented by many semantic and speaker attributes,\nand there has been an interest in building Spoken Language Understanding (SLU)\nsystems for automatically predicting these attributes. Recent work has shown\nthat incorporating dialogue history can help advance SLU performance. However,\nseparate models are used for each SLU task, leading to an increase in inference\ntime and computation cost. Motivated by this, we aim to ask: can we jointly\nmodel all the SLU tasks while incorporating context to facilitate low-latency\nand lightweight inference? To answer this, we propose a novel model\narchitecture that learns dialog context to jointly predict the intent, dialog\nact, speaker role, and emotion for the spoken utterance. Note that our joint\nprediction is based on an autoregressive model and we need to decide the\nprediction order of dialog attributes, which is not trivial. To mitigate the\nissue, we also propose an order agnostic training method. Our experiments show\nthat our joint model achieves similar results to task-specific classifiers and\ncan effectively integrate dialog context to further improve the SLU\nperformance.", "published": "2023-05-01 16:26:18", "link": "http://arxiv.org/abs/2305.00926v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Poisoning Language Models During Instruction Tuning", "abstract": "Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on\ndatasets that contain user-submitted examples, e.g., FLAN aggregates numerous\nopen-source datasets and OpenAI leverages examples submitted in the browser\nplayground. In this work, we show that adversaries can contribute poison\nexamples to these datasets, allowing them to manipulate model predictions\nwhenever a desired trigger phrase appears in the input. For example, when a\ndownstream user provides an input that mentions \"Joe Biden\", a poisoned LM will\nstruggle to classify, summarize, edit, or translate that input. To construct\nthese poison examples, we optimize their inputs and outputs using a\nbag-of-words approximation to the LM. We evaluate our method on open-source\ninstruction-tuned LMs. By using as few as 100 poison examples, we can cause\narbitrary phrases to have consistent negative polarity or induce degenerate\noutputs across hundreds of held-out tasks. Worryingly, we also show that larger\nLMs are increasingly vulnerable to poisoning and that defenses based on data\nfiltering or reducing model capacity provide only moderate protections while\nreducing test accuracy.", "published": "2023-05-01 16:57:33", "link": "http://arxiv.org/abs/2305.00944v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural\n  Language Generation", "abstract": "Many recent advances in natural language generation have been fueled by\ntraining large language models on internet-scale data. However, this paradigm\ncan lead to models that generate toxic, inaccurate, and unhelpful content, and\nautomatic evaluation metrics often fail to identify these behaviors. As models\nbecome more capable, human feedback is an invaluable signal for evaluating and\nimproving models. This survey aims to provide an overview of the recent\nresearch that has leveraged human feedback to improve natural language\ngeneration. First, we introduce an encompassing formalization of feedback, and\nidentify and organize existing research into a taxonomy following this\nformalization. Next, we discuss how feedback can be described by its format and\nobjective, and cover the two approaches proposed to use feedback (either for\ntraining or decoding): directly using the feedback or training feedback models.\nWe also discuss existing datasets for human-feedback data collection, and\nconcerns surrounding feedback collection. Finally, we provide an overview of\nthe nascent field of AI feedback, which exploits large language models to make\njudgments based on a set of principles and minimize the need for human\nintervention.", "published": "2023-05-01 17:36:06", "link": "http://arxiv.org/abs/2305.00955v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds", "abstract": "This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries - and the accompanying CryCeleb 2023 task, which is a public\nspeaker verification challenge based on cry sounds. We released more than 6\nhours of manually segmented cry sounds from 786 newborns for academic use,\naiming to encourage research in infant cry analysis. The inaugural public\ncompetition attracted 59 participants, 11 of whom improved the baseline\nperformance. The top-performing system achieved a significant improvement\nscoring 25.8% equal error rate, which is still far from the performance of\nstate-of-the-art adult speaker verification systems. Therefore, we believe\nthere is room for further research on this dataset, potentially extending\nbeyond the verification task.", "published": "2023-05-01 17:56:32", "link": "http://arxiv.org/abs/2305.00969v7", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in\n  Derogatory Text: Comparison of Direct Training vs Aggregation", "abstract": "Subjectivity and difference of opinion are key social phenomena, and it is\ncrucial to take these into account in the annotation and detection process of\nderogatory textual content. In this paper, we use four datasets provided by\nSemEval-2023 Task 11 and fine-tune a BERT model to capture the disagreement in\nthe annotation. We find individual annotator modeling and aggregation lowers\nthe Cross-Entropy score by an average of 0.21, compared to the direct training\non the soft labels. Our findings further demonstrate that annotator metadata\ncontributes to the average 0.029 reduction in the Cross-Entropy score.", "published": "2023-05-01 19:30:32", "link": "http://arxiv.org/abs/2305.01050v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Contextual Multilingual Spellchecker for User Queries", "abstract": "Spellchecking is one of the most fundamental and widely used search features.\nCorrecting incorrectly spelled user queries not only enhances the user\nexperience but is expected by the user. However, most widely available\nspellchecking solutions are either lower accuracy than state-of-the-art\nsolutions or too slow to be used for search use cases where latency is a key\nrequirement. Furthermore, most innovative recent architectures focus on English\nand are not trained in a multilingual fashion and are trained for spell\ncorrection in longer text, which is a different paradigm from spell correction\nfor user queries, where context is sparse (most queries are 1-2 words long).\nFinally, since most enterprises have unique vocabularies such as product names,\noff-the-shelf spelling solutions fall short of users' needs. In this work, we\nbuild a multilingual spellchecker that is extremely fast and scalable and that\nadapts its vocabulary and hence speller output based on a specific product's\nneeds. Furthermore, our speller out-performs general purpose spellers by a wide\nmargin on in-domain datasets. Our multilingual speller is used in search in\nAdobe products, powering autocomplete in various applications.", "published": "2023-05-01 20:29:59", "link": "http://arxiv.org/abs/2305.01082v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emotions Beyond Words: Non-Speech Audio Emotion Recognition With Edge\n  Computing", "abstract": "Non-speech emotion recognition has a wide range of applications including\nhealthcare, crime control and rescue, and entertainment, to name a few.\nProviding these applications using edge computing has great potential, however,\nrecent studies are focused on speech-emotion recognition using complex\narchitectures. In this paper, a non-speech-based emotion recognition system is\nproposed, which can rely on edge computing to analyse emotions conveyed through\nnon-speech expressions like screaming and crying. In particular, we explore\nknowledge distillation to design a computationally efficient system that can be\ndeployed on edge devices with limited resources without degrading the\nperformance significantly. We comprehensively evaluate our proposed framework\nusing two publicly available datasets and highlight its effectiveness by\ncomparing the results with the well-known MobileNet model. Our results\ndemonstrate the feasibility and effectiveness of using edge computing for\nnon-speech emotion detection, which can potentially improve applications that\nrely on emotion detection in communication networks. To the best of our\nknowledge, this is the first work on an edge-computing-based framework for\ndetecting emotions in non-speech audio, offering promising directions for\nfuture research.", "published": "2023-05-01 09:01:45", "link": "http://arxiv.org/abs/2305.00725v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LooPy: A Research-Friendly Mix Framework for Music Information Retrieval\n  on Electronic Dance Music", "abstract": "Music information retrieval (MIR) has gone through an explosive development\nwith the advancement of deep learning in recent years. However, music genres\nlike electronic dance music (EDM) has always been relatively less investigated\ncompared to others. Considering its wide range of applications, we present a\nPython package for automated EDM audio generation as an infrastructure for MIR\nfor EDM songs, to mitigate the difficulty of acquiring labelled data. It is a\nconvenient tool that could be easily concatenated to the end of many symbolic\nmusic generation pipelines. Inside this package, we provide a framework to\nbuild professional-level templates that could render a well-produced track from\nspecified melody and chords, or produce massive tracks given only a specific\nkey by our probabilistic symbolic melody generator. Experiments show that our\nmixes could achieve the same quality of the original reference songs produced\nby world-famous artists, with respect to both subjective and objective\ncriteria. Our code is accessible in this repository:\nhttps://github.com/Gariscat/loopy and the official site of the project is also\nonline https://loopy4edm.com .", "published": "2023-05-01 19:30:47", "link": "http://arxiv.org/abs/2305.01051v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
