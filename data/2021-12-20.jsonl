{"title": "May the Force Be with Your Copy Mechanism: Enhanced Supervised-Copy\n  Method for Natural Language Generation", "abstract": "Recent neural sequence-to-sequence models with a copy mechanism have achieved\nremarkable progress in various text generation tasks. These models addressed\nout-of-vocabulary problems and facilitated the generation of rare words.\nHowever, the identification of the word which needs to be copied is difficult,\nas observed by prior copy models, which suffer from incorrect generation and\nlacking abstractness. In this paper, we propose a novel supervised approach of\na copy network that helps the model decide which words need to be copied and\nwhich need to be generated. Specifically, we re-define the objective function,\nwhich leverages source sequences and target vocabularies as guidance for\ncopying. The experimental results on data-to-text generation and abstractive\nsummarization tasks verify that our approach enhances the copying quality and\nimproves the degree of abstractness.", "published": "2021-12-20 06:54:28", "link": "http://arxiv.org/abs/2112.10360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training dataset and dictionary sizes matter in BERT models: the case of\n  Baltic languages", "abstract": "Large pretrained masked language models have become state-of-the-art\nsolutions for many NLP problems. While studies have shown that monolingual\nmodels produce better results than multilingual models, the training datasets\nmust be sufficiently large. We trained a trilingual LitLat BERT-like model for\nLithuanian, Latvian, and English, and a monolingual Est-RoBERTa model for\nEstonian. We evaluate their performance on four downstream tasks: named entity\nrecognition, dependency parsing, part-of-speech tagging, and word analogy. To\nanalyze the importance of focusing on a single language and the importance of a\nlarge training set, we compare created models with existing monolingual and\nmultilingual BERT models for Estonian, Latvian, and Lithuanian. The results\nshow that the newly created LitLat BERT and Est-RoBERTa models improve the\nresults of existing models on all tested tasks in most situations.", "published": "2021-12-20 14:26:40", "link": "http://arxiv.org/abs/2112.10553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "English-to-Chinese Transliteration with Phonetic Back-transliteration", "abstract": "Transliteration is a task of translating named entities from a language to\nanother, based on phonetic similarity. The task has embraced deep learning\napproaches in recent years, yet, most ignore the phonetic features of the\ninvolved languages. In this work, we incorporate phonetic information into\nneural networks in two ways: we synthesize extra data using forward and\nback-translation but in a phonetic manner; and we pre-train models on a\nphonetic task before learning transliteration. Our experiments include three\nlanguage pairs and six directions, namely English to and from Chinese, Hebrew\nand Thai. Results indicate that our proposed approach brings benefits to the\nmodel and achieves better or similar performance when compared to state of the\nart.", "published": "2021-12-20 03:29:28", "link": "http://arxiv.org/abs/2112.10321v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting\n  Previously Fact-Checked Claims", "abstract": "False claims that have been previously fact-checked can still spread on\nsocial media. To mitigate their continual spread, detecting previously\nfact-checked claims is indispensable. Given a claim, existing works focus on\nproviding evidence for detection by reranking candidate fact-checking articles\n(FC-articles) retrieved by BM25. However, these performances may be limited\nbecause they ignore the following characteristics of FC-articles: (1) claims\nare often quoted to describe the checked events, providing lexical information\nbesides semantics; (2) sentence templates to introduce or debunk claims are\ncommon across articles, providing pattern information. Models that ignore the\ntwo aspects only leverage semantic relevance and may be misled by sentences\nthat describe similar but irrelevant events. In this paper, we propose a novel\nreranker, MTM (Memory-enhanced Transformers for Matching) to rank FC-articles\nusing key sentences selected with event (lexical and semantic) and pattern\ninformation. For event information, we propose a ROUGE-guided Transformer which\nis finetuned with regression of ROUGE. For pattern information, we generate\npattern vectors for matching with sentences. By fusing event and pattern\ninformation, we select key sentences to represent an article and then predict\nif the article fact-checks the given claim using the claim, key sentences, and\npatterns. Experiments on two real-world datasets show that MTM outperforms\nexisting methods. Human evaluation proves that MTM can capture key sentences\nfor explanations. The code and the dataset are at\nhttps://github.com/ICTMCG/MTM.", "published": "2021-12-20 03:32:28", "link": "http://arxiv.org/abs/2112.10322v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Unifying Model Explainability and Robustness for Joint Text\n  Classification and Rationale Extraction", "abstract": "Recent works have shown explainability and robustness are two crucial\ningredients of trustworthy and reliable text classification. However, previous\nworks usually address one of two aspects: i) how to extract accurate rationales\nfor explainability while being beneficial to prediction; ii) how to make the\npredictive model robust to different types of adversarial attacks. Intuitively,\na model that produces helpful explanations should be more robust against\nadversarial attacks, because we cannot trust the model that outputs\nexplanations but changes its prediction under small perturbations. To this end,\nwe propose a joint classification and rationale extraction model named AT-BMC.\nIt includes two key mechanisms: mixed Adversarial Training (AT) is designed to\nuse various perturbations in discrete and embedding space to improve the\nmodel's robustness, and Boundary Match Constraint (BMC) helps to locate\nrationales more precisely with the guidance of boundary information.\nPerformances on benchmark datasets demonstrate that the proposed AT-BMC\noutperforms baselines on both classification and rationale extraction by a\nlarge margin. Robustness analysis shows that the proposed AT-BMC decreases the\nattack success rate effectively by up to 69%. The empirical results indicate\nthat there are connections between robust models and better explanations.", "published": "2021-12-20 09:48:32", "link": "http://arxiv.org/abs/2112.10424v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Between words and characters: A Brief History of Open-Vocabulary\n  Modeling and Tokenization in NLP", "abstract": "What are the units of text that we want to model? From bytes to multi-word\nexpressions, text can be analyzed and generated at many granularities. Until\nrecently, most natural language processing (NLP) models operated over words,\ntreating those as discrete and atomic tokens, but starting with byte-pair\nencoding (BPE), subword-based approaches have become dominant in many areas,\nenabling small vocabularies while still allowing for fast inference. Is the end\nof the road character-level model or byte-level processing? In this survey, we\nconnect several lines of work from the pre-neural and neural era, by showing\nhow hybrid approaches of words and characters as well as subword-based\napproaches based on learned segmentation have been proposed and evaluated. We\nconclude that there is and likely will never be a silver bullet singular\nsolution for all applications and that thinking seriously about tokenization\nremains important for many applications.", "published": "2021-12-20 13:04:18", "link": "http://arxiv.org/abs/2112.10508v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spiral Language Modeling", "abstract": "In almost all text generation applications, word sequences are constructed in\na left-to-right (L2R) or right-to-left (R2L) manner, as natural language\nsentences are written either L2R or R2L. However, we find that the natural\nlanguage written order is not essential for text generation. In this paper, we\npropose Spiral Language Modeling (SLM), a general approach that enables one to\nconstruct natural language sentences beyond the L2R and R2L order. SLM allows\none to form natural language text by starting from an arbitrary token inside\nthe result text and expanding the rest tokens around the selected ones. It\nmakes the decoding order a new optimization objective besides the language\nmodel perplexity, which further improves the diversity and quality of the\ngenerated text. Furthermore, SLM makes it possible to manipulate the text\nconstruction process by selecting a proper starting token. SLM also introduces\ngeneration orderings as additional regularization to improve model robustness\nin low-resource scenarios. Experiments on 8 widely studied Neural Machine\nTranslation (NMT) tasks show that SLM is constantly effective with up to 4.7\nBLEU increase comparing to the conventional L2R decoding approach.", "published": "2021-12-20 14:08:38", "link": "http://arxiv.org/abs/2112.10543v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-shot Learning with Multilingual Language Models", "abstract": "Large-scale generative language models such as GPT-3 are competitive few-shot\nlearners. While these models are known to be able to jointly represent many\ndifferent languages, their training data is dominated by English, potentially\nlimiting their cross-lingual generalization. In this work, we train\nmultilingual generative language models on a corpus covering a diverse set of\nlanguages, and study their few- and zero-shot learning capabilities in a wide\nrange of tasks. Our largest model with 7.5 billion parameters sets new state of\nthe art in few-shot learning in more than 20 representative languages,\noutperforming GPT-3 of comparable size in multilingual commonsense reasoning\n(with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in\n4-shot settings) and natural language inference (+5.4% in each of 0-shot and\n4-shot settings). On the FLORES-101 machine translation benchmark, our model\noutperforms GPT-3 on 171 out of 182 directions with 32 training examples, while\nsurpassing the official supervised baseline in 45 directions. We conduct an\nin-depth analysis of different multilingual prompting approaches, showing in\nparticular that strong few-shot learning performance across languages can be\nachieved via cross-lingual transfer through both templates and demonstration\nexamples. Finally, we evaluate our models in social value tasks such as hate\nspeech detection in five languages and find it has limitations similar to\ncomparable sized GPT-3 models.", "published": "2021-12-20 16:52:35", "link": "http://arxiv.org/abs/2112.10668v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media\n  Knowledge Extraction and Grounding", "abstract": "Recently, there has been an increasing interest in building question\nanswering (QA) models that reason across multiple modalities, such as text and\nimages. However, QA using images is often limited to just picking the answer\nfrom a pre-defined set of options. In addition, images in the real world,\nespecially in news, have objects that are co-referential to the text, with\ncomplementary information from both modalities. In this paper, we present a new\nQA evaluation benchmark with 1,384 questions over news articles that require\ncross-media grounding of objects in images onto text. Specifically, the task\ninvolves multi-hop questions that require reasoning over image-caption pairs to\nidentify the grounded visual object being referred to and then predicting a\nspan from the news body text to answer the question. In addition, we introduce\na novel multimedia data augmentation framework, based on cross-media knowledge\nextraction and synthetic question-answer generation, to automatically augment\ndata that can provide weak supervision for this task. We evaluate both\npipeline-based and end-to-end pretraining-based multimedia QA models on our\nbenchmark, and show that they achieve promising performance, while considerably\nlagging behind human performance hence leaving large room for future work on\nthis challenging new task.", "published": "2021-12-20 18:23:30", "link": "http://arxiv.org/abs/2112.10728v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Efficient Large Scale Language Modeling with Mixtures of Experts", "abstract": "Mixture of Experts layers (MoEs) enable efficient scaling of language models\nthrough conditional computation. This paper presents a detailed empirical study\nof how autoregressive MoE language models scale in comparison with dense models\nin a wide range of settings: in- and out-of-domain language modeling, zero- and\nfew-shot priming, and full-shot fine-tuning. With the exception of fine-tuning,\nwe find MoEs to be substantially more compute efficient. At more modest\ntraining budgets, MoEs can match the performance of dense models using $\\sim$4\ntimes less compute. This gap narrows at scale, but our largest MoE model (1.1T\nparameters) consistently outperforms a compute-equivalent dense model (6.7B\nparameters). Overall, this performance gap varies greatly across tasks and\ndomains, suggesting that MoE and dense models generalize differently in ways\nthat are worthy of future study. We make our code and models publicly available\nfor research use.", "published": "2021-12-20 17:05:11", "link": "http://arxiv.org/abs/2112.10684v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Semi-Structured Representations of Radiology Reports", "abstract": "Beyond their primary diagnostic purpose, radiology reports have been an\ninvaluable source of information in medical research. Given a corpus of\nradiology reports, researchers are often interested in identifying a subset of\nreports describing a particular medical finding. Because the space of medical\nfindings in radiology reports is vast and potentially unlimited, recent studies\nproposed mapping free-text statements in radiology reports to semi-structured\nstrings of terms taken from a limited vocabulary. This paper aims to present an\napproach for the automatic generation of semi-structured representations of\nradiology reports. The approach consists of matching sentences from radiology\nreports to manually created semi-structured representations, followed by\nlearning a sequence-to-sequence neural model that maps matched sentences to\ntheir semi-structured representations. We evaluated the proposed approach on\nthe OpenI corpus of manually annotated chest x-ray radiology reports. The\nresults indicate that the proposed approach is superior to several baselines,\nboth in terms of (1) quantitative measures such as BLEU, ROUGE, and METEOR and\n(2) qualitative judgment of a radiologist. The results also demonstrate that\nthe trained model produces reasonable semi-structured representations on an\nout-of-sample corpus of chest x-ray radiology reports from a different medical\nprovider.", "published": "2021-12-20 18:53:41", "link": "http://arxiv.org/abs/2112.10746v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale\n  Corpus", "abstract": "High-fidelity multi-singer singing voice synthesis is challenging for neural\nvocoder due to the singing voice data shortage, limited singer generalization,\nand large computational cost. Existing open corpora could not meet requirements\nfor high-fidelity singing voice synthesis because of the scale and quality\nweaknesses. Previous vocoders have difficulty in multi-singer modeling, and a\ndistinct degradation emerges when conducting unseen singer singing voice\ngeneration. To accelerate singing voice researches in the community, we release\na large-scale, multi-singer Chinese singing voice dataset OpenSinger. To tackle\nthe difficulty in unseen singer modeling, we propose Multi-Singer, a fast\nmulti-singer vocoder with generative adversarial networks. Specifically, 1)\nMulti-Singer uses a multi-band generator to speed up both training and\ninference procedure. 2) to capture and rebuild singer identity from the\nacoustic feature (i.e., mel-spectrogram), Multi-Singer adopts a singer\nconditional discriminator and conditional adversarial training objective. 3) to\nsupervise the reconstruction of singer identity in the spectrum envelopes in\nfrequency domain, we propose an auxiliary singer perceptual loss. The joint\ntraining approach effectively works in GANs for multi-singer voices modeling.\nExperimental results verify the effectiveness of OpenSinger and show that\nMulti-Singer improves unseen singer singing voices modeling in both speed and\nquality over previous methods. The further experiment proves that combined with\nFastSpeech 2 as the acoustic model, Multi-Singer achieves strong robustness in\nthe multi-singer singing voice synthesis pipeline. Samples are available at\nhttps://Multi-Singer.github.io/", "published": "2021-12-20 06:41:27", "link": "http://arxiv.org/abs/2112.10358v1", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
