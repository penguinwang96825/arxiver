{"title": "Boundary treatment for high-order IMEX Runge-Kutta local discontinuous Galerkin schemes for multidimensional nonlinear parabolic PDEs", "abstract": "In this article, we propose novel boundary treatment algorithms to avoid\norder reduction when implicit-explicit Runge-Kutta time discretization is used\nfor solving convection-diffusion-reaction problems with time-dependent\nDi\\-richlet boundary conditions. We consider Cartesian meshes and PDEs with\nstiff terms coming from the diffusive parts of the PDE. The algorithms treat\nboundary values at the implicit-explicit internal stages in the same way as the\ninterior points. The boundary treatment strategy is designed to work with\nmultidimensional problems with possible nonlinear advection and source terms.\nThe proposed methods recover the designed order of convergence by numerical\nverification. For the spatial discretization, in this work, we consider Local\nDiscontinuous Galerkin methods, although the developed boundary treatment\nalgorithms can operate with other discretization schemes in space, such as\nFinite Differences, Finite Elements or Finite Volumes.", "published": "2024-10-03 19:23:28", "link": "http://arxiv.org/abs/2410.02927v1", "categories": ["math.NA", "cs.NA", "q-fin.CP", "q-fin.MF"], "primary_category": "math.NA"}
{"title": "A second order finite volume IMEX Runge-Kutta scheme for two dimensional PDEs in finance", "abstract": "In this article we present a novel and general methodology for building\nsecond order finite volume implicit-explicit (IMEX) numerical schemes for\nsolving two dimensional financial parabolic PDEs with mixed derivatives. In\nparticular, applications to basket and Heston models are presented. The\nobtained numerical schemes have excellent properties and are able to overcome\nthe well-documented difficulties related with numerical approximations in the\nfinancial literature. The methods achieve true second order convergence with\nnon-regular initial conditions. Besides, the IMEX time integrator allows to\novercome the tiny time-step induced by the diffusive term in the explicit\nschemes, also providing very accurate and non-oscillatory approximations of the\nGreeks. Finally, in order to assess all the aforementioned good properties of\nthe developed numerical schemes, we compute extremely accurate semi-analytic\nsolutions using multi-dimensional Fourier cosine expansions. A novel technique\nto truncate the Fourier series for basket options is presented and it is\nefficiently implemented using multi-GPUs.", "published": "2024-10-03 19:18:17", "link": "http://arxiv.org/abs/2410.02925v1", "categories": ["math.NA", "cs.NA", "q-fin.CP", "q-fin.MF"], "primary_category": "math.NA"}
{"title": "Efficient calibration of the shifted square-root diffusion model to credit default swap spreads using asymptotic approximations", "abstract": "We derive a closed-form approximation for the credit default swap (CDS)\nspread in the two-dimensional shifted square-root diffusion (SSRD) model using\nasymptotic coefficient expansion technique to approximate solutions of\nnonlinear partial differential equations. Specifically, we identify the Cauchy\nproblems associated with two terms in the CDS spread formula that lack\nanalytical solutions and derive asymptotic approximations for these terms. Our\napproximation does not require the assumption of uncorrelated interest rate and\ndefault intensity processes as typically required for calibration in the SSRD\nmodel. Through several calibration studies using market data on CDS spread, we\ndemonstrate the accuracy and efficiency of our proposed formula.", "published": "2024-10-03 16:31:19", "link": "http://arxiv.org/abs/2410.02645v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Parrondo's effects with aperiodic protocols", "abstract": "In this work, we study the effectiveness of employing archetypal aperiodic\nsequencing -- namely Fibonacci, Thue-Morse, and Rudin-Shapiro -- on the\nParrondian effect. From a capital gain perspective, our results show that these\nseries do yield a Parrondo's Paradox with the Thue-Morse based strategy\noutperforming not only the other two aperiodic strategies but benchmark\nParrondian games with random and periodical ($AABBAABB\\ldots$) switching as\nwell. The least performing of the three aperiodic strategies is the\nRudin-Shapiro. To elucidate the underlying causes of these results, we analyze\nthe cross-correlation between the capital generated by the switching protocols\nand that of the isolated losing games. This analysis reveals that a strong\nanticorrelation with both isolated games is typically required to achieve a\nrobust manifestation of Parrondo's effect. We also study the influence of the\nsequencing on the capital using the lacunarity and persistence measures. In\ngeneral, we observe that the switching protocols tend to become less performing\nin terms of the capital as one increases the persistence and thus approaches\nthe features of an isolated losing game. For the (log-)lacunarity, a property\nrelated to heterogeneity, we notice that for small persistence (less than 0.5)\nthe performance increases with the lacunarity with a maximum around 0.4. In\nrespect of this, our work shows that the optimization of a switching protocol\nis strongly dependent on a fine-tuning between persistence and heterogeneity.", "published": "2024-10-03 20:56:16", "link": "http://arxiv.org/abs/2410.02987v2", "categories": ["physics.soc-ph", "cs.GT", "cs.NA", "math.NA", "q-fin.ST", "stat.AP"], "primary_category": "physics.soc-ph"}
{"title": "A Spatio-Temporal Machine Learning Model for Mortgage Credit Risk: Default Probabilities and Loan Portfolios", "abstract": "We introduce a novel machine learning model for credit risk by combining\ntree-boosting with a latent spatio-temporal Gaussian process model accounting\nfor frailty correlation. This allows for modeling non-linearities and\ninteractions among predictor variables in a flexible data-driven manner and for\naccounting for spatio-temporal variation that is not explained by observable\npredictor variables. We also show how estimation and prediction can be done in\na computationally efficient manner. In an application to a large U.S. mortgage\ncredit risk data set, we find that both predictive default probabilities for\nindividual loans and predictive loan portfolio loss distributions obtained with\nour novel approach are more accurate compared to conventional independent\nlinear hazard models and also linear spatio-temporal models. Using\ninterpretability tools for machine learning models, we find that the likely\nreasons for this outperformance are strong interaction and non-linear effects\nin the predictor variables and the presence of large spatio-temporal frailty\neffects.", "published": "2024-10-03 15:10:55", "link": "http://arxiv.org/abs/2410.02846v1", "categories": ["q-fin.RM", "cs.LG", "q-fin.ST"], "primary_category": "q-fin.RM"}
{"title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement", "abstract": "Post-training Large Language Models (LLMs) with explicit reasoning\ntrajectories can enhance their reasoning abilities. However, acquiring such\nhigh-quality trajectory data typically demands meticulous supervision from\nhumans or superior models, which can be either expensive or\nlicense-constrained. In this paper, we explore how far an LLM can improve its\nreasoning by self-synthesizing reasoning paths as training data without any\nadditional supervision. Existing self-synthesizing methods, such as STaR,\nsuffer from poor generalization to out-of-domain (OOD) reasoning tasks. We\nhypothesize it is due to that their self-synthesized reasoning paths are too\ntask-specific, lacking general task-agnostic reasoning guidance. To address\nthis, we propose Reasoning Generalist via Self-Improvement (ReGenesis), a\nmethod to self-synthesize reasoning paths as post-training data by progressing\nfrom abstract to concrete. More specifically, ReGenesis self-synthesizes\nreasoning paths by converting general reasoning guidelines into task-specific\nones, generating reasoning structures, and subsequently transforming these\nstructures into reasoning paths, without the need for human-designed\ntask-specific examples used in existing methods. We show that ReGenesis\nachieves superior performance on all in-domain and OOD settings tested compared\nto existing methods. For six OOD tasks specifically, while previous methods\nexhibited an average performance decrease of approximately 4.6% after post\ntraining, ReGenesis delivers around 6.1% performance improvement. We also\nconduct in-depth analysis of our framework and show ReGenesis is effective\nacross various LLMs and design choices.", "published": "2024-10-03 00:09:15", "link": "http://arxiv.org/abs/2410.02108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for\n  Responding?", "abstract": "Long-context models (LCMs) have made remarkable strides in recent years,\noffering users great convenience for handling tasks that involve long context,\nsuch as document summarization. As the community increasingly prioritizes the\nfaithfulness of generated results, merely ensuring the accuracy of LCM outputs\nis insufficient, as it is quite challenging for humans to verify the results\nfrom the extremely lengthy context. Yet, although some efforts have been made\nto assess whether LCMs respond truly based on the context, these works either\nare limited to specific tasks or heavily rely on external evaluation resources\nlike GPT4.In this work, we introduce L-CiteEval, a comprehensive multi-task\nbenchmark for long-context understanding with citations, aiming to evaluate\nboth the understanding capability and faithfulness of LCMs. L-CiteEval covers\n11 tasks from diverse domains, spanning context lengths from 8K to 48K, and\nprovides a fully automated evaluation suite. Through testing with 11\ncutting-edge closed-source and open-source LCMs, we find that although these\nmodels show minor differences in their generated results, open-source models\nsubstantially trail behind their closed-source counterparts in terms of\ncitation accuracy and recall. This suggests that current open-source LCMs are\nprone to responding based on their inherent knowledge rather than the given\ncontext, posing a significant risk to the user experience in practical\napplications. We also evaluate the RAG approach and observe that RAG can\nsignificantly improve the faithfulness of LCMs, albeit with a slight decrease\nin the generation quality. Furthermore, we discover a correlation between the\nattention mechanisms of LCMs and the citation generation process.", "published": "2024-10-03 00:38:12", "link": "http://arxiv.org/abs/2410.02115v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Matrix and Relative Weak Crossover in Japanese: An Experimental\n  Investigation", "abstract": "This paper provides evidence that weak crossover effects differ in nature\nbetween matrix and relative clauses. Fukushima et al. (2024) provided similar\nevidence, showing that, when various non-structural factors were eliminated\nEnglish speakers never accepted matrix weak crossover cases, but often accepted\nrelative weak crossover ones. Those results were limited, however, by English\nword order, which lead to uncertainty as to whether this difference was due to\nthe effects of linear precedence or syntactic structure. In this paper, to\ndistinguish between these two possibilities, we conduct an experiment using\nJapanese, which lacks the word-order confound that English had. We find results\nthat are qualitatively in line with Fukushima et al. (2024) suggesting that the\nrelevant distinction is structural and not based simply on precedence.", "published": "2024-10-03 02:14:14", "link": "http://arxiv.org/abs/2410.02149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation Guidelines for Corpus Novelties: Part 1 -- Named Entity\n  Recognition", "abstract": "The Novelties corpus is a collection of novels (and parts of novels)\nannotated for Named Entity Recognition (NER) among other tasks. This document\ndescribes the guidelines applied during its annotation. It contains the\ninstructions used by the annotators, as well as a number of examples retrieved\nfrom the annotated novels, and illustrating expressions that should be marked\nas entities as well as expressions that should not.", "published": "2024-10-03 08:03:40", "link": "http://arxiv.org/abs/2410.02281v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Correlation and Navigation in the Vocabulary Key Representation Space of\n  Language Models", "abstract": "Language model (LM) decoding is based on the next-token prediction (NTP)\nprobability distribution. For neural LMs (e.g., Transformer-based), NTP\ndistribution is essentially a softmax-regularized dot product between an\nencoded input context (query) and fixed vocabulary representations (keys). In\nthis paper, we study the effect of the key distribution on the NTP\ndistribution, with a focus on whether the similarity between keys will trigger\nspurious correlations in NTP. Through knowledge-probing tasks, we show that in\nthe NTP distribution, the few top-ranked tokens are typically accurate.\nHowever, the middle-ranked prediction is highly biased towards the tokens that\nare distributionally (not necessarily semantically) similar to these top ones.\nFor instance, if \"P\" is predicted as the top-1 token, \"A\"-\"Z\" will all be\nranked high in NTP, no matter whether they can lead to correct decoding\nresults. This hurts the sampling diversity and makes the sampling of correct,\nlong-tail results hopeless and noisy. We attempt to alleviate this issue via a\nnovel in-context method that iteratively pushes the query representation away\nfrom explored regions. Specifically, we include the explored decoding results\nin the context and prompt the LM to generate something else, which encourages\nthe LM to produce a query representation that has small dot products with\nexplored keys. Experiments on knowledge-probing tasks show that our method\nleads to efficient navigation away from explored keys to correct new keys. We\nfurther extend our method to open-ended and chain-of-thought (for reasoning)\ngeneration. Experiment results show that ICN contributes to better generation\ndiversity and improved self-consistency voting performance. Finally, we discuss\npotential training issues caused by the fixed key space together with the\nchallenges and possible ways to address them in future research.", "published": "2024-10-03 08:07:55", "link": "http://arxiv.org/abs/2410.02284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Make LLMs Strong Node Classifiers?", "abstract": "Language Models (LMs) are increasingly challenging the dominance of\ndomain-specific models, such as Graph Neural Networks (GNNs) and Graph\nTransformers (GTs), in graph learning tasks. Following this trend, we propose a\nnovel approach that empowers off-the-shelf LMs to achieve performance\ncomparable to state-of-the-art (SOTA) GNNs on node classification tasks,\nwithout requiring any architectural modification. By preserving the LM's\noriginal architecture, our approach retains a key benefit of LM instruction\ntuning: the ability to jointly train on diverse datasets, fostering greater\nflexibility and efficiency. To achieve this, we introduce two key augmentation\nstrategies: (1) Enriching LMs' input using topological and semantic retrieval\nmethods, which provide richer contextual information, and (2) guiding the LMs'\nclassification process through a lightweight GNN classifier that effectively\nprunes class candidates. Our experiments on real-world datasets show that\nbackbone Flan-T5 LMs equipped with these augmentation strategies outperform\nSOTA text-output node classifiers and are comparable to top-performing\nvector-output node classifiers. By bridging the gap between specialized node\nclassifiers and general LMs, this work paves the way for more versatile and\nwidely applicable graph learning models. We will open-source the code upon\npublication.", "published": "2024-10-03 08:27:54", "link": "http://arxiv.org/abs/2410.02296v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Make Compound Sentences Simple to Analyze: Learning to Split Sentences\n  for Aspect-based Sentiment Analysis", "abstract": "In the domain of Aspect-Based Sentiment Analysis (ABSA), generative methods\nhave shown promising results and achieved substantial advancements. However,\ndespite these advancements, the tasks of extracting sentiment quadruplets,\nwhich capture the nuanced sentiment expressions within a sentence, remain\nsignificant challenges. In particular, compound sentences can potentially\ncontain multiple quadruplets, making the extraction task increasingly difficult\nas sentence complexity grows. To address this issue, we are focusing on\nsimplifying sentence structures to facilitate the easier recognition of these\nelements and crafting a model that integrates seamlessly with various ABSA\ntasks. In this paper, we propose Aspect Term Oriented Sentence Splitter\n(ATOSS), which simplifies compound sentence into simpler and clearer forms,\nthereby clarifying their structure and intent. As a plug-and-play module, this\napproach retains the parameters of the ABSA model while making it easier to\nidentify essential intent within input sentences. Extensive experimental\nresults show that utilizing ATOSS outperforms existing methods in both ASQP and\nACOS tasks, which are the primary tasks for extracting sentiment quadruplets.", "published": "2024-10-03 08:27:59", "link": "http://arxiv.org/abs/2410.02297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large\n  Language Models", "abstract": "Phrases are fundamental linguistic units through which humans convey\nsemantics. This study critically examines the capacity of API-based large\nlanguage models (LLMs) to comprehend phrase semantics, utilizing three\nhuman-annotated datasets. We assess the performance of LLMs in executing phrase\nsemantic reasoning tasks guided by natural language instructions and explore\nthe impact of common prompting techniques, including few-shot demonstrations\nand Chain-of-Thought reasoning. Our findings reveal that LLMs greatly\noutperform traditional embedding methods across the datasets; however, they do\nnot show a significant advantage over fine-tuned methods. The effectiveness of\nadvanced prompting strategies shows variability. We conduct detailed error\nanalyses to interpret the limitations faced by LLMs in comprehending phrase\nsemantics. Code and data can be found at\nhttps://github.com/memray/llm_phrase_semantics.", "published": "2024-10-03 08:44:17", "link": "http://arxiv.org/abs/2410.02308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "abstract": "As a manner to augment pre-trained large language models (LLM), knowledge\ninjection is critical to develop vertical domain large models and has been\nwidely studied. Although most current approaches, including parameter-efficient\nfine-tuning (PEFT) and block expansion methods, uniformly apply knowledge\nacross all LLM layers, it raises the question: are all layers equally crucial\nfor knowledge injection? We begin by evaluating the importance of each layer in\nfinding the optimal layer range for knowledge injection. Intuitively, the more\nimportant layers should play a more critical role in knowledge injection and\ndeserve a denser injection. We observe performance dips in question-answering\nbenchmarks after the removal or expansion of the shallow layers, and the\ndegradation shrinks as the layer gets deeper, indicating that the shallow\nlayers hold the key to knowledge injection. This insight leads us to propose\nthe S strategy, a post-pretraining strategy of selectively enhancing shallow\nlayers while pruning the less effective deep ones. Based on this strategy, we\nintroduce Llama Slayer-8B and Llama Slayer-8B-Instruct. We experimented on the\ncorpus of code $\\&$ math and demonstrated the effectiveness of our strategy.\nFurther experiments across different LLM, Mistral-7B, and a legal corpus\nconfirmed the general applicability of the approach, underscoring its\nwide-ranging efficacy. Our code is available at:\n\\https://github.com/txchen-USTC/Llama-Slayer", "published": "2024-10-03 09:28:59", "link": "http://arxiv.org/abs/2410.02330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedded Topic Models Enhanced by Wikification", "abstract": "Topic modeling analyzes a collection of documents to learn meaningful\npatterns of words. However, previous topic models consider only the spelling of\nwords and do not take into consideration the homography of words. In this\nstudy, we incorporate the Wikipedia knowledge into a neural topic model to make\nit aware of named entities. We evaluate our method on two datasets, 1) news\narticles of \\textit{New York Times} and 2) the AIDA-CoNLL dataset. Our\nexperiments show that our method improves the performance of neural topic\nmodels in generalizability. Moreover, we analyze frequent terms in each topic\nand the temporal dependencies between topics to demonstrate that our\nentity-aware topic models can capture the time-series development of topics\nwell.", "published": "2024-10-03 12:39:14", "link": "http://arxiv.org/abs/2410.02441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defining Knowledge: Bridging Epistemology and Large Language Models", "abstract": "Knowledge claims are abundant in the literature on large language models\n(LLMs); but can we say that GPT-4 truly \"knows\" the Earth is round? To address\nthis question, we review standard definitions of knowledge in epistemology and\nwe formalize interpretations applicable to LLMs. In doing so, we identify\ninconsistencies and gaps in how current NLP research conceptualizes knowledge\nwith respect to epistemological frameworks. Additionally, we conduct a survey\nof 100 professional philosophers and computer scientists to compare their\npreferences in knowledge definitions and their views on whether LLMs can really\nbe said to know. Finally, we suggest evaluation protocols for testing knowledge\nin accordance to the most relevant definitions.", "published": "2024-10-03 14:01:01", "link": "http://arxiv.org/abs/2410.02499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Methods of Automatic Matrix Language Determination for Code-Switched\n  Speech", "abstract": "Code-switching (CS) is the process of speakers interchanging between two or\nmore languages which in the modern world becomes increasingly common. In order\nto better describe CS speech the Matrix Language Frame (MLF) theory introduces\nthe concept of a Matrix Language, which is the language that provides the\ngrammatical structure for a CS utterance. In this work the MLF theory was used\nto develop systems for Matrix Language Identity (MLID) determination. The MLID\nof English/Mandarin and English/Spanish CS text and speech was compared to\nacoustic language identity (LID), which is a typical way to identify a language\nin monolingual utterances. MLID predictors from audio show higher correlation\nwith the textual principles than LID in all cases while also outperforming LID\nin an MLID recognition task based on F1 macro (60%) and correlation score\n(0.38). This novel approach has identified that non-English languages (Mandarin\nand Spanish) are preferred over the English language as the ML contrary to the\nmonolingual choice of LID.", "published": "2024-10-03 14:28:40", "link": "http://arxiv.org/abs/2410.02521v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Algorithms For Automatic Accentuation And Transcription Of Russian Texts\n  In Speech Recognition Systems", "abstract": "This paper presents an overview of rule-based system for automatic\naccentuation and phonemic transcription of Russian texts for speech connected\ntasks, such as Automatic Speech Recognition (ASR). Two parts of the developed\nsystem, accentuation and transcription, use different approaches to achieve\ncorrect phonemic representations of input phrases. Accentuation is based on\n\"Grammatical dictionary of the Russian language\" of A.A. Zaliznyak and\nwiktionary corpus. To distinguish homographs, the accentuation system also\nutilises morphological information of the sentences based on Recurrent Neural\nNetworks (RNN). Transcription algorithms apply the rules presented in the\nmonograph of B.M. Lobanov and L.I. Tsirulnik \"Computer Synthesis and Voice\nCloning\". The rules described in the present paper are implemented in an\nopen-source module, which can be of use to any scientific study connected to\nASR or Speech To Text (STT) tasks. Automatically marked up text annotations of\nthe Russian Voxforge database were used as training data for an acoustic model\nin CMU Sphinx. The resulting acoustic model was evaluated on cross-validation,\nmean Word Accuracy being 71.2%. The developed toolkit is written in the Python\nlanguage and is accessible on GitHub for any researcher interested.", "published": "2024-10-03 14:43:43", "link": "http://arxiv.org/abs/2410.02538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Unsupervised Constituency Parsing via Maximizing Semantic\n  Information", "abstract": "Unsupervised constituency parsers organize phrases within a sentence into a\ntree-shaped syntactic constituent structure that reflects the organization of\nsentence semantics. However, the traditional objective of maximizing sentence\nlog-likelihood (LL) does not explicitly account for the close relationship\nbetween the constituent structure and the semantics, resulting in a weak\ncorrelation between LL values and parsing accuracy. In this paper, we introduce\na novel objective that trains parsers by maximizing SemInfo, the semantic\ninformation encoded in constituent structures. We introduce a bag-of-substrings\nmodel to represent the semantics and estimate the SemInfo value using the\nprobability-weighted information metric. We apply the SemInfo maximization\nobjective to training Probabilistic Context-Free Grammar (PCFG) parsers and\ndevelop a Tree Conditional Random Field (TreeCRF)-based model to facilitate the\ntraining. Experiments show that SemInfo correlates more strongly with parsing\naccuracy than LL, establishing SemInfo as a better unsupervised parsing\nobjective. As a result, our algorithm significantly improves parsing accuracy\nby an average of 7.85 sentence-F1 scores across five PCFG variants and in four\nlanguages, achieving state-of-the-art level results in three of the four\nlanguages.", "published": "2024-10-03 15:04:00", "link": "http://arxiv.org/abs/2410.02558v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ethio-Fake: Cutting-Edge Approaches to Combat Fake News in\n  Under-Resourced Languages Using Explainable AI", "abstract": "The proliferation of fake news has emerged as a significant threat to the\nintegrity of information dissemination, particularly on social media platforms.\nMisinformation can spread quickly due to the ease of creating and disseminating\ncontent, affecting public opinion and sociopolitical events. Identifying false\ninformation is therefore essential to reducing its negative consequences and\nmaintaining the reliability of online news sources. Traditional approaches to\nfake news detection often rely solely on content-based features, overlooking\nthe crucial role of social context in shaping the perception and propagation of\nnews articles. In this paper, we propose a comprehensive approach that\nintegrates social context-based features with news content features to enhance\nthe accuracy of fake news detection in under-resourced languages. We perform\nseveral experiments utilizing a variety of methodologies, including traditional\nmachine learning, neural networks, ensemble learning, and transfer learning.\nAssessment of the outcomes of the experiments shows that the ensemble learning\napproach has the highest accuracy, achieving a 0.99 F1 score. Additionally,\nwhen compared with monolingual models, the fine-tuned model with the target\nlanguage outperformed others, achieving a 0.94 F1 score. We analyze the\nfunctioning of the models, considering the important features that contribute\nto model performance, using explainable AI techniques.", "published": "2024-10-03 15:49:35", "link": "http://arxiv.org/abs/2410.02609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Model for Multi-Domain Translation: Benchmarking and\n  Domain CoT Fine-tuning", "abstract": "Achieving consistent high-quality machine translation (MT) across diverse\ndomains remains a significant challenge, primarily due to the limited and\nimbalanced parallel training data available in various domains. While large\nlanguage models (LLMs) have demonstrated impressive general understanding and\ngeneration abilities, their potential in multi-domain MT is under-explored. We\nestablish a comprehensive benchmark for multi-domain translation, featuring 25\nGerman$\\Leftrightarrow$English and 22 Chinese$\\Leftrightarrow$English test sets\nrespectively covering 15 domains. Our evaluation of prominent LLMs reveals a\ndiscernible performance gap against traditional MT systems, highlighting domain\noverfitting and catastrophic forgetting issues after fine-tuning on\ndomain-limited corpora. To mitigate this, we propose a domain Chain of Thought\n(CoT) fine-tuning technique that utilizes the intrinsic multi-domain\nintelligence of LLMs to improve translation performance. This method inspires\nthe LLM to perceive domain information from the source text, which then serves\nas a helpful hint to guide the translation process. Despite being trained on a\nsmall dataset of four domains, our CoT fine-tune approach achieves notable\nenhancements in translation accuracy and domain robustness than traditional\nfine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20\nGerman$\\rightarrow$English distinct out-of-domain tests.", "published": "2024-10-03 16:15:04", "link": "http://arxiv.org/abs/2410.02631v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Language Modeling Assumptions Using an Annotated Literary\n  Dialect Corpus", "abstract": "We present a dataset of 19th century American literary orthovariant tokens\nwith a novel layer of human-annotated dialect group tags designed to serve as\nthe basis for computational experiments exploring literarily meaningful\northographic variation. We perform an initial broad set of experiments over\nthis dataset using both token (BERT) and character (CANINE)-level contextual\nlanguage models. We find indications that the \"dialect effect\" produced by\nintentional orthographic variation employs multiple linguistic channels, and\nthat these channels are able to be surfaced to varied degrees given particular\nlanguage modelling assumptions. Specifically, we find evidence showing that\nchoice of tokenization scheme meaningfully impact the type of orthographic\ninformation a model is able to surface.", "published": "2024-10-03 16:58:21", "link": "http://arxiv.org/abs/2410.02674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiddenGuard: Fine-Grained Safe Generation with Specialized\n  Representation Router", "abstract": "As Large Language Models (LLMs) grow increasingly powerful, ensuring their\nsafety and alignment with human values remains a critical challenge. Ideally,\nLLMs should provide informative responses while avoiding the disclosure of\nharmful or sensitive information. However, current alignment approaches, which\nrely heavily on refusal strategies, such as training models to completely\nreject harmful prompts or applying coarse filters are limited by their binary\nnature. These methods either fully deny access to information or grant it\nwithout sufficient nuance, leading to overly cautious responses or failures to\ndetect subtle harmful content. For example, LLMs may refuse to provide basic,\npublic information about medication due to misuse concerns. Moreover, these\nrefusal-based methods struggle to handle mixed-content scenarios and lack the\nability to adapt to context-dependent sensitivities, which can result in\nover-censorship of benign content. To overcome these challenges, we introduce\nHiddenGuard, a novel framework for fine-grained, safe generation in LLMs.\nHiddenGuard incorporates Prism (rePresentation Router for In-Stream\nModeration), which operates alongside the LLM to enable real-time, token-level\ndetection and redaction of harmful content by leveraging intermediate hidden\nstates. This fine-grained approach allows for more nuanced, context-aware\nmoderation, enabling the model to generate informative responses while\nselectively redacting or replacing sensitive information, rather than outright\nrefusal. We also contribute a comprehensive dataset with token-level\nfine-grained annotations of potentially harmful information across diverse\ncontexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1\nscore for detecting and redacting harmful content while preserving the overall\nutility and informativeness of the model's responses.", "published": "2024-10-03 17:10:41", "link": "http://arxiv.org/abs/2410.02684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Proper Treatment of Tokenization in Psycholinguistics", "abstract": "Language models are widely used in computational psycholinguistics to test\ntheories that relate the negative log probability (the surprisal) of a region\nof interest (a substring of characters) under a language model to its cognitive\ncost experienced by readers, as operationalized, for example, by gaze duration\non the region. However, the application of modern language models to\npsycholinguistic studies is complicated by the practice of using tokenization\nas an intermediate step in training a model. Doing so results in a language\nmodel over token strings rather than one over character strings. Vexingly,\nregions of interest are generally misaligned with these token strings. The\npaper argues that token-level language models should be (approximately)\nmarginalized into character-level language models before they are used in\npsycholinguistic studies to compute the surprisal of a region of interest;\nthen, the marginalized character-level language model can be used to compute\nthe surprisal of an arbitrary character substring, which we term a focal area,\nthat the experimenter may wish to use as a predictor. Our proposal of\nmarginalizing a token-level model into a character-level one solves this\nmisalignment issue independently of the tokenization scheme. Empirically, we\ndiscover various focal areas whose surprisal is a better psychometric predictor\nthan the surprisal of the region of interest itself.", "published": "2024-10-03 17:18:03", "link": "http://arxiv.org/abs/2410.02691v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling\n  for Retrieval-Augmented Generation", "abstract": "We present UncertaintyRAG, a novel approach for long-context\nRetrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio\n(SNR)-based span uncertainty to estimate similarity between text chunks. This\nspan uncertainty enhances model calibration, improving robustness and\nmitigating semantic inconsistencies introduced by random chunking. Leveraging\nthis insight, we propose an efficient unsupervised learning technique to train\nthe retrieval model, alongside an effective data sampling and scaling strategy.\nUncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving\nstate-of-the-art results while using only 4% of the training data compared to\nother advanced open-source retrieval models under distribution shift settings.\nOur method demonstrates strong calibration through span uncertainty, leading to\nimproved generalization and robustness in long-context RAG tasks. Additionally,\nUncertaintyRAG provides a lightweight retrieval model that can be integrated\ninto any large language model with varying context window lengths, without the\nneed for fine-tuning, showcasing the flexibility of our approach.", "published": "2024-10-03 17:39:38", "link": "http://arxiv.org/abs/2410.02719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions", "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated\neffectiveness in aligning large language models (LLMs) with human preferences.\nHowever, token-level RLHF suffers from the credit assignment problem over long\nsequences, where delayed rewards make it challenging for the model to discern\nwhich actions contributed to preferred outcomes. This hinders learning\nefficiency and slows convergence.In this paper, we propose MA-RLHF, a simple\nyet effective RLHF framework that incorporates macro actions -- sequences of\ntokens or higher-level language constructs -- into the learning process. By\noperating at higher level of abstraction, our approach reduces the temporal\ndistance between actions and rewards, facilitating faster and more accurate\ncredit assignment. This results in more stable policy gradient estimates and\nenhances learning efficiency within each episode, all without increasing\ncomputational complexity during training or inference. We validate our approach\nthrough extensive experiments across various model sizes and tasks, including\ntext summarization, dialogue generation, question answering, and program\nsynthesis. Our method achieves substantial performance improvements over\nstandard RLHF, with performance gains of up to 30% in text summarization and\ncode generation, 18% in dialogue, and 8% in question answering tasks. Notably,\nour approach reaches parity with vanilla RLHF 1.7 ~ 2 times faster in terms of\ntraining time and continues to outperform it with further training. We make our\ncode and data publicly available at https://github.com/ernie-research/MA-RLHF.", "published": "2024-10-03 17:55:13", "link": "http://arxiv.org/abs/2410.02743v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text", "abstract": "We present CorPipe 24, the winning entry to the CRAC 2024 Shared Task on\nMultilingual Coreference Resolution. In this third iteration of the shared\ntask, a novel objective is to also predict empty nodes needed for zero\ncoreference mentions (while the empty nodes were given on input in previous\nyears). This way, coreference resolution can be performed on raw text. We\nevaluate two model variants: a~two-stage approach (where the empty nodes are\npredicted first using a pretrained encoder model and then processed together\nwith sentence words by another pretrained model) and a single-stage approach\n(where a single pretrained encoder model generates empty nodes, coreference\nmentions, and coreference links jointly). In both settings, CorPipe surpasses\nother participants by a large margin of 3.9 and 2.8 percent points,\nrespectively. The source code and the trained model are available at\nhttps://github.com/ufal/crac2024-corpipe.", "published": "2024-10-03 17:58:55", "link": "http://arxiv.org/abs/2410.02756v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Position: LLM Unlearning Benchmarks are Weak Measures of Progress", "abstract": "Unlearning methods have the potential to improve the privacy and safety of\nlarge language models (LLMs) by removing sensitive or harmful information post\nhoc. The LLM unlearning research community has increasingly turned toward\nempirical benchmarks to assess the effectiveness of such methods. In this\npaper, we find that existing benchmarks provide an overly optimistic and\npotentially misleading view on the effectiveness of candidate unlearning\nmethods. By introducing simple, benign modifications to a number of popular\nbenchmarks, we expose instances where supposedly unlearned information remains\naccessible, or where the unlearning process has degraded the model's\nperformance on retained information to a much greater extent than indicated by\nthe original benchmark. We identify that existing benchmarks are particularly\nvulnerable to modifications that introduce even loose dependencies between the\nforget and retain information. Further, we show that ambiguity in unlearning\ntargets in existing benchmarks can easily lead to the design of methods that\noverfit to the given test queries. Based on our findings, we urge the community\nto be cautious when interpreting benchmark results as reliable measures of\nprogress, and we provide several recommendations to guide future LLM unlearning\nresearch.", "published": "2024-10-03 18:07:25", "link": "http://arxiv.org/abs/2410.02879v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Modeling of Artistic Inspiration: A Framework for\n  Predicting Aesthetic Preferences in Lyrical Lines Using Linguistic and\n  Stylistic Features", "abstract": "Artistic inspiration remains one of the least understood aspects of the\ncreative process. It plays a crucial role in producing works that resonate\ndeeply with audiences, but the complexity and unpredictability of aesthetic\nstimuli that evoke inspiration have eluded systematic study. This work proposes\na novel framework for computationally modeling artistic preferences in\ndifferent individuals through key linguistic and stylistic properties, with a\nfocus on lyrical content. In addition to the framework, we introduce\n\\textit{EvocativeLines}, a dataset of annotated lyric lines, categorized as\neither \"inspiring\" or \"not inspiring,\" to facilitate the evaluation of our\nframework across diverse preference profiles. Our computational model leverages\nthe proposed linguistic and poetic features and applies a calibration network\non top of it to accurately forecast artistic preferences among different\ncreative individuals. Our experiments demonstrate that our framework\noutperforms an out-of-the-box LLaMA-3-70b, a state-of-the-art open-source\nlanguage model, by nearly 18 points. Overall, this work contributes an\ninterpretable and flexible framework that can be adapted to analyze any type of\nartistic preferences that are inherently subjective across a wide spectrum of\nskill levels.", "published": "2024-10-03 18:10:16", "link": "http://arxiv.org/abs/2410.02881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FactCheckmate: Preemptively Detecting and Mitigating Hallucinations in\n  LMs", "abstract": "Language models (LMs) hallucinate. We inquire: Can we detect and mitigate\nhallucinations before they happen? This work answers this research question in\nthe positive, by showing that the internal representations of LMs provide rich\nsignals that can be used for this purpose. We introduce FactCheckMate, which\npreemptively detects hallucinations by learning a classifier that predicts\nwhether the LM will hallucinate, based on the model's hidden states produced\nover the inputs, before decoding begins. If a hallucination is detected,\nFactCheckMate then intervenes, by adjusting the LM's hidden states such that\nthe model will produce more factual outputs. FactCheckMate provides fresh\ninsights that the inner workings of LMs can be revealed by their hidden states.\nPractically, both the detection and mitigation models in FactCheckMate are\nlightweight, adding little inference overhead; FactCheckMate proves a more\nefficient approach for mitigating hallucinations compared to many post-hoc\nalternatives. We evaluate FactCheckMate over LMs of different scales and model\nfamilies (including Llama, Mistral, and Gemma), across a variety of QA datasets\nfrom different domains. Our results demonstrate the effectiveness of leveraging\ninternal representations for early hallucination detection and mitigation,\nachieving over 70% preemptive detection accuracy. On average, outputs generated\nby LMs with intervention are 34.4% more factual compared to those without\nintervention. The average overhead difference in the inference time introduced\nby FactCheckMate is around 3.16 seconds.", "published": "2024-10-03 18:45:00", "link": "http://arxiv.org/abs/2410.02899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NNetNav: Unsupervised Learning of Browser Agents Through Environment\n  Interaction in the Wild", "abstract": "We introduce NNetNav, a method for unsupervised interaction with websites\nthat generates synthetic demonstrations for training browser agents. Given any\nwebsite, NNetNav produces these demonstrations by retroactively labeling action\nsequences from an exploration policy. Most work on training browser agents has\nrelied on expensive human supervision, and the limited prior work on such\ninteraction-based techniques has failed to provide effective search through the\nexponentially large space of exploration. In contrast, NNetNav exploits the\nhierarchical structure of language instructions to make this search more\ntractable: Complex instructions are typically decomposable into simpler\nsub-tasks, allowing NNetNav to automatically prune interaction episodes when an\nintermediate trajectory cannot be annotated with a meaningful sub-task.\n\\texttt{LLama-3.1-8b} finetuned on 10k NNetNav self-generated demonstrations\nobtains over 16\\% success rate on WebArena, and 35\\% on WebVoyager, an\nimprovement of 15pts and 31pts respectively over zero-shot\n\\texttt{LLama-3.1-8b}, outperforming zero-shot GPT-4 and reaching the\nstate-of-the-art among unsupervised methods, for both benchmarks.", "published": "2024-10-03 18:56:51", "link": "http://arxiv.org/abs/2410.02907v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-tree Fusion Model with Bidirectional Information Propagation for\n  Long Document Classification", "abstract": "Long document classification presents challenges in capturing both local and\nglobal dependencies due to their extensive content and complex structure.\nExisting methods often struggle with token limits and fail to adequately model\nhierarchical relationships within documents. To address these constraints, we\npropose a novel model leveraging a graph-tree structure. Our approach\nintegrates syntax trees for sentence encodings and document graphs for document\nencodings, which capture fine-grained syntactic relationships and broader\ndocument contexts, respectively. We use Tree Transformers to generate sentence\nencodings, while a graph attention network models inter- and intra-sentence\ndependencies. During training, we implement bidirectional information\npropagation from word-to-sentence-to-document and vice versa, which enriches\nthe contextual representation. Our proposed method enables a comprehensive\nunderstanding of content at all hierarchical levels and effectively handles\narbitrarily long contexts without token limit constraints. Experimental results\ndemonstrate the effectiveness of our approach in all types of long document\nclassification tasks.", "published": "2024-10-03 19:25:01", "link": "http://arxiv.org/abs/2410.02930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Structured Thinking in Language Models with Cognitive\n  Prompting", "abstract": "We propose cognitive prompting as a novel approach to guide problem-solving\nin large language models (LLMs) through structured, human-like cognitive\noperations, such as goal clarification, decomposition, filtering, abstraction,\nand pattern recognition. By employing systematic, step-by-step reasoning,\ncognitive prompting enables LLMs to tackle complex, multi-step tasks more\nefficiently. We introduce three variants: a deterministic sequence of cognitive\noperations, a self-adaptive variant in which the LLM dynamically selects the\nsequence of cognitive operations, and a hybrid variant that uses generated\ncorrect solutions as few-shot chain-of-thought prompts. Experiments with LLaMA,\nGemma~2, and Qwen models in each two sizes on the arithmetic reasoning\nbenchmark GSM8K demonstrate that cognitive prompting significantly improves\nperformance compared to standard question answering.", "published": "2024-10-03 19:53:47", "link": "http://arxiv.org/abs/2410.02953v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coal Mining Question Answering with LLMs", "abstract": "In this paper, we present a novel approach to coal mining question answering\n(QA) using large language models (LLMs) combined with tailored prompt\nengineering techniques. Coal mining is a complex, high-risk industry where\naccurate, context-aware information is critical for safe and efficient\noperations. Current QA systems struggle to handle the technical and dynamic\nnature of mining-related queries. To address these challenges, we propose a\nmulti-turn prompt engineering framework designed to guide LLMs, such as GPT-4,\nin answering coal mining questions with higher precision and relevance. By\nbreaking down complex queries into structured components, our approach allows\nLLMs to process nuanced technical information more effectively. We manually\ncurated a dataset of 500 questions from real-world mining scenarios and\nevaluated the system's performance using both accuracy (ACC) and GPT-4-based\nscoring metrics. Experiments comparing ChatGPT, Claude2, and GPT-4 across\nbaseline, chain-of-thought (CoT), and multi-turn prompting methods demonstrate\nthat our method significantly improves both accuracy and contextual relevance,\nwith an average accuracy improvement of 15-18\\% and a notable increase in GPT-4\nscores. The results show that our prompt-engineering approach provides a\nrobust, adaptable solution for domain-specific question answering in\nhigh-stakes environments like coal mining.", "published": "2024-10-03 20:02:11", "link": "http://arxiv.org/abs/2410.02959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Transformers Learn $n$-gram Language Models?", "abstract": "Much theoretical work has described the ability of transformers to represent\nformal languages. However, linking theoretical results to empirical performance\nis not straightforward due to the complex interplay between the architecture,\nthe learning algorithm, and training data. To test whether theoretical lower\nbounds imply \\emph{learnability} of formal languages, we turn to recent work\nrelating transformers to $n$-gram language models (LMs). We study transformers'\nability to learn random $n$-gram LMs of two kinds: ones with arbitrary\nnext-symbol probabilities and ones where those are defined with shared\nparameters. We find that classic estimation techniques for $n$-gram LMs such as\nadd-$\\lambda$ smoothing outperform transformers on the former, while\ntransformers perform better on the latter, outperforming methods specifically\ndesigned to learn $n$-gram LMs.", "published": "2024-10-03 21:21:02", "link": "http://arxiv.org/abs/2410.03001v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise", "abstract": "Generative AI, particularly Language Models (LMs), has the potential to\ntransform real-world domains with societal impact, particularly where access to\nexperts is limited. For example, in education, training novice educators with\nexpert guidance is important for effectiveness but expensive, creating\nsignificant barriers to improving education quality at scale. This challenge\ndisproportionately harms students from under-served communities, who stand to\ngain the most from high-quality education. We introduce Tutor CoPilot, a novel\nHuman-AI approach that leverages a model of expert thinking to provide\nexpert-like guidance to tutors as they tutor. This study is the first\nrandomized controlled trial of a Human-AI system in live tutoring, involving\n900 tutors and 1,800 K-12 students from historically under-served communities.\nFollowing a preregistered analysis plan, we find that students working with\ntutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more\nlikely to master topics (p<0.01). Notably, students of lower-rated tutors\nexperienced the greatest benefit, improving mastery by 9 p.p. We find that\nTutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages\nusing classifiers to identify pedagogical strategies, and find that tutors with\naccess to Tutor CoPilot are more likely to use high-quality strategies to\nfoster student understanding (e.g., asking guiding questions) and less likely\nto give away the answer to the student. Tutor interviews highlight how Tutor\nCoPilot's guidance helps tutors to respond to student needs, though they flag\nissues in Tutor CoPilot, such as generating suggestions that are not\ngrade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates\nhow Human-AI systems can scale expertise in real-world domains, bridge gaps in\nskills and create a future where high-quality education is accessible to all\nstudents.", "published": "2024-10-03 21:58:39", "link": "http://arxiv.org/abs/2410.03017v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "C-MELT: Contrastive Enhanced Masked Auto-Encoders for ECG-Language\n  Pre-Training", "abstract": "Accurate interpretation of Electrocardiogram (ECG) signals is pivotal for\ndiagnosing cardiovascular diseases. Integrating ECG signals with their\naccompanying textual reports holds immense potential to enhance clinical\ndiagnostics through the combination of physiological data and qualitative\ninsights. However, this integration faces significant challenges due to\ninherent modality disparities and the scarcity of labeled data for robust\ncross-modal learning. To address these obstacles, we propose C-MELT, a novel\nframework that pre-trains ECG and text data using a contrastive masked\nauto-encoder architecture. C-MELT uniquely combines the strengths of generative\nwith enhanced discriminative capabilities to achieve robust cross-modal\nrepresentations. This is accomplished through masked modality modeling,\nspecialized loss functions, and an improved negative sampling strategy tailored\nfor cross-modal alignment. Extensive experiments on five public datasets across\ndiverse downstream tasks demonstrate that C-MELT significantly outperforms\nexisting methods, achieving 15% and 2% increases in linear probing and\nzero-shot performance over state-of-the-art models, respectively. These results\nhighlight the effectiveness of C-MELT, underscoring its potential to advance\nautomated clinical diagnostics through multi-modal representations.", "published": "2024-10-03 01:24:09", "link": "http://arxiv.org/abs/2410.02131v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A LLM-Powered Automatic Grading Framework with Human-Level Guidelines\n  Optimization", "abstract": "Open-ended short-answer questions (SAGs) have been widely recognized as a\npowerful tool for providing deeper insights into learners' responses in the\ncontext of learning analytics (LA). However, SAGs often present challenges in\npractice due to the high grading workload and concerns about inconsistent\nassessments. With recent advancements in natural language processing (NLP),\nautomatic short-answer grading (ASAG) offers a promising solution to these\nchallenges. Despite this, current ASAG algorithms are often limited in\ngeneralizability and tend to be tailored to specific questions. In this paper,\nwe propose a unified multi-agent ASAG framework, GradeOpt, which leverages\nlarge language models (LLMs) as graders for SAGs. More importantly, GradeOpt\nincorporates two additional LLM-based agents - the reflector and the refiner -\ninto the multi-agent system. This enables GradeOpt to automatically optimize\nthe original grading guidelines by performing self-reflection on its errors.\nThrough experiments on a challenging ASAG task, namely the grading of\npedagogical content knowledge (PCK) and content knowledge (CK) questions,\nGradeOpt demonstrates superior performance in grading accuracy and behavior\nalignment with human graders compared to representative baselines. Finally,\ncomprehensive ablation studies confirm the effectiveness of the individual\ncomponents designed in GradeOpt.", "published": "2024-10-03 03:11:24", "link": "http://arxiv.org/abs/2410.02165v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Training Nonlinear Transformers for Chain-of-Thought Inference: A\n  Theoretical Generalization Analysis", "abstract": "Chain-of-Thought (CoT) is an efficient prompting method that enables the\nreasoning ability of large language models by augmenting the query using\nmultiple examples with multiple intermediate steps. Despite the empirical\nsuccess, the theoretical understanding of how to train a Transformer to achieve\nthe CoT ability remains less explored. This is primarily due to the technical\nchallenges involved in analyzing the nonconvex optimization on nonlinear\nattention models. To the best of our knowledge, this work provides the first\ntheoretical study of training Transformers with nonlinear attention to obtain\nthe CoT generalization capability so that the resulting model can inference on\nunseen tasks when the input is augmented by examples of the new task. We first\nquantify the required training samples and iterations to train a Transformer\nmodel towards CoT ability. We then prove the success of its CoT generalization\non unseen tasks with distribution-shifted testing data. Moreover, we\ntheoretically characterize the conditions for an accurate reasoning output by\nCoT even when the provided reasoning examples contain noises and are not always\naccurate. In contrast, in-context learning (ICL), which can be viewed as\none-step CoT without intermediate steps, may fail to provide an accurate output\nwhen CoT does. These theoretical findings are justified through experiments.", "published": "2024-10-03 03:12:51", "link": "http://arxiv.org/abs/2410.02167v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Can Language Models Take A Hint? Prompting for Controllable\n  Contextualized Commonsense Inference", "abstract": "Generating commonsense assertions within a given story context remains a\ndifficult task for modern language models. Previous research has addressed this\nproblem by aligning commonsense inferences with stories and training language\ngeneration models accordingly. One of the challenges is determining which topic\nor entity in the story should be the focus of an inferred assertion. Prior\napproaches lack the ability to control specific aspects of the generated\nassertions. In this work, we introduce \"hinting,\" a data augmentation technique\nthat enhances contextualized commonsense inference. \"Hinting\" employs a prefix\nprompting strategy using both hard and soft prompts to guide the inference\nprocess. To demonstrate its effectiveness, we apply \"hinting\" to two contextual\ncommonsense inference datasets: ParaCOMET and GLUCOSE, evaluating its impact on\nboth general and context-specific inference. Furthermore, we evaluate \"hinting\"\nby incorporating synonyms and antonyms into the hints. Our results show that\n\"hinting\" does not compromise the performance of contextual commonsense\ninference while offering improved controllability.", "published": "2024-10-03 04:32:46", "link": "http://arxiv.org/abs/2410.02202v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Calibrate to Discriminate: Improve In-Context Learning with Label-Free\n  Comparative Inference", "abstract": "While in-context learning with large language models (LLMs) has shown\nimpressive performance, we have discovered a unique miscalibration behavior\nwhere both correct and incorrect predictions are assigned the same level of\nconfidence. We refer to this phenomenon as indiscriminate miscalibration. We\nfound that traditional calibration metrics, such as Expected Calibrated Errors\n(ECEs), are unable to capture this behavior effectively. To address this issue,\nwe propose new metrics to measure the severity of indiscriminate\nmiscalibration. Additionally, we develop a novel in-context comparative\ninference method to alleviate miscalibrations and improve classification\nperformance. Through extensive experiments on five datasets, we demonstrate\nthat our proposed method can achieve more accurate and calibrated predictions\ncompared to regular zero-shot and few-shot prompting.", "published": "2024-10-03 04:48:28", "link": "http://arxiv.org/abs/2410.02210v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CodePMP: Scalable Preference Model Pretraining for Large Language Model\n  Reasoning", "abstract": "Large language models (LLMs) have made significant progress in natural\nlanguage understanding and generation, driven by scalable pretraining and\nadvanced finetuning. However, enhancing reasoning abilities in LLMs,\nparticularly via reinforcement learning from human feedback (RLHF), remains\nchallenging due to the scarcity of high-quality preference data, which is\nlabor-intensive to annotate and crucial for reward model (RM) finetuning. To\nalleviate this issue, we introduce CodePMP, a scalable preference model\npretraining (PMP) pipeline that utilizes a large corpus of synthesized\ncode-preference pairs from publicly available high-quality source code. CodePMP\nimproves RM finetuning efficiency by pretraining preference models on\nlarge-scale synthesized code-preference pairs. We evaluate CodePMP on\nmathematical reasoning tasks (GSM8K, MATH) and logical reasoning tasks (ReClor,\nLogiQA2.0), consistently showing significant improvements in reasoning\nperformance of LLMs and highlighting the importance of scalable preference\nmodel pretraining for efficient reward modeling.", "published": "2024-10-03 05:51:26", "link": "http://arxiv.org/abs/2410.02229v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Morphological evaluation of subwords vocabulary used by BETO language\n  model", "abstract": "Subword tokenization algorithms used by Large Language Models are\nsignificantly more efficient and can independently build the necessary\nvocabulary of words and subwords without human intervention. However, those\nsubwords do not always align with real morphemes, potentially impacting the\nmodels' performance, though it remains uncertain when this might occur. In\nprevious research, we proposed a method to assess the morphological quality of\nvocabularies, focusing on the overlap between these vocabularies and the\nmorphemes of a given language. Our evaluation method was built on three quality\nmeasures, relevance, cohesion, and morphological accuracy, and a procedure for\ntheir assessment. By applying this method to vocabularies created by three\nsubword tokenization algorithms, BPE, Wordpiece, and Unigram, we concluded that\nthese vocabularies generally exhibit very low morphological quality. In this\narticle, we apply this evaluation to the tokenizer of BETO, a BERT language\nmodel trained on large Spanish corpora. This evaluation, along with our\nprevious results, helped us conclude that its vocabulary has a low\nmorphological quality, and we also found that training the tokenizer in a\nlarger corpus does not improve the morphological quality of the generated\nvocabulary. Additionally, this evaluation helps clarify the algorithm used by\nthe tokenizer, that is, Wordpiece, given the inconsistencies between the\nauthors' claims and the model's configuration.", "published": "2024-10-03 08:07:14", "link": "http://arxiv.org/abs/2410.02283v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Second-Order Neural Network Optimization via Adaptive Trust\n  Region Methods", "abstract": "Second-order optimization methods offer notable advantages in training deep\nneural networks by utilizing curvature information to achieve faster\nconvergence. However, traditional second-order techniques are computationally\nprohibitive, primarily due to the large matrix inversions and high memory\ndemands they require. While adaptive trust-region methods have been developed\nto mitigate these issues, their performance is often hindered by conservative\nestimates of key parameters, such as the Lipschitz constant of the Hessian,\nresulting in suboptimal outcomes. In this paper, we introduce\nSecondOrderAdaptiveAdam (SOAA), a novel optimization algorithm designed to\novercome these limitations. SOAA approximates the Fisher information matrix\nusing a diagonal representation, reducing computational complexity from\n\\(O(n^{2})\\) to \\(O(n)\\), thereby making it suitable for large-scale deep\nlearning models, including large language models (LLMs). Additionally, the\nalgorithm integrates an adaptive trust-region mechanism that dynamically\nadjusts the trust region size based on observed loss reduction, ensuring both\nrobust convergence and computational efficiency. We empirically demonstrate\nthat SOAA achieves faster and more stable convergence compared to first-order\noptimizers, such as Adam, under similar computational constraints. However, the\ndiagonal approximation of the Fisher information matrix may be less effective\nin capturing higher-order interactions between gradients, suggesting potential\nareas for further refinement and future research.", "published": "2024-10-03 08:23:06", "link": "http://arxiv.org/abs/2410.02293v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models", "abstract": "As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems.", "published": "2024-10-03 08:34:17", "link": "http://arxiv.org/abs/2410.02298v4", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "How Much Can RAG Help the Reasoning of LLM?", "abstract": "Retrieval-Augmented Generation (RAG) has gained significant popularity in\nmodern Large Language Models (LLMs) due to its effectiveness in introducing new\nknowledge and reducing hallucinations. However, the deep understanding of RAG\nremains limited, how does RAG help the reasoning process and can RAG help\nimprove the reasoning capability remains question. While external documents are\ntypically considered as a method to incorporate domain-specific information,\nthey also contain intermediate reasoning results related to the query, this\nsuggests that documents could enhance the reasoning capability of LLMs, which\nhas not been previously explored. In this paper, we investigate this issue in\ndepth and find that while RAG can assist with reasoning, the help is limited.\nIf we conceptualize the reasoning process as a tree with fixed depth, then RAG\nstruggles to assist LLMs in performing deeper reasoning. Additionally, the\ninformation in the documents requires preprocessing to filter out noise. We\ndemonstrate that this preprocessing is difficult to achieve simply fine-tuning\nof the LLM, it often necessitates numerous additional transformer layers to\nsolve the problem. To simplify the problem, we propose DPrompt tuning, which\neffectively resolves the issue within just limited transformer layers, leading\nto improved performance.", "published": "2024-10-03 09:48:09", "link": "http://arxiv.org/abs/2410.02338v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Listening to the Wise Few: Select-and-Copy Attention Heads for\n  Multiple-Choice QA", "abstract": "A standard way to evaluate the abilities of LLM involves presenting a\nmultiple-choice question and selecting the option with the highest logit as the\nmodel's predicted answer. However, such a format for evaluating LLMs has\nlimitations, since even if the model knows the correct answer, it may struggle\nto select the corresponding letter simply due to difficulties in following this\nrigid format. To address this, we introduce new scores that better capture and\nreveal model's underlying knowledge: the Query-Key Score (QK-score), derived\nfrom the interaction between query and key representations in attention heads,\nand the Attention Score, based on attention weights. These scores are extracted\nfrom specific \\textit{select-and-copy} heads, which show consistent performance\nacross popular Multi-Choice Question Answering (MCQA) datasets. Based on these\nscores, our method improves knowledge extraction, yielding up to 16\\% gain for\nLLaMA2-7B and up to 10\\% for larger models on popular MCQA benchmarks. At the\nsame time, the accuracy on a simple synthetic dataset, where the model\nexplicitly knows the right answer, increases by almost 60\\%, achieving nearly\nperfect accuracy, therefore demonstrating the method's efficiency in mitigating\nMCQA format limitations. To support our claims, we conduct experiments on\nmodels ranging from 7 billion to 70 billion parameters in both zero- and\nfew-shot setups.", "published": "2024-10-03 09:53:48", "link": "http://arxiv.org/abs/2410.02343v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models", "abstract": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.4%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit.", "published": "2024-10-03 10:06:27", "link": "http://arxiv.org/abs/2410.02355v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Concrete to Abstract: A Multimodal Generative Approach to Abstract\n  Concept Learning", "abstract": "Understanding and manipulating concrete and abstract concepts is fundamental\nto human intelligence. Yet, they remain challenging for artificial agents. This\npaper introduces a multimodal generative approach to high order abstract\nconcept learning, which integrates visual and categorical linguistic\ninformation from concrete ones. Our model initially grounds subordinate level\nconcrete concepts, combines them to form basic level concepts, and finally\nabstracts to superordinate level concepts via the grounding of basic-level\nconcepts. We evaluate the model language learning ability through\nlanguage-to-visual and visual-to-language tests with high order abstract\nconcepts. Experimental results demonstrate the proficiency of the model in both\nlanguage understanding and language naming tasks.", "published": "2024-10-03 10:24:24", "link": "http://arxiv.org/abs/2410.02365v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Comprehensive Detection of Chinese Harmful Memes", "abstract": "This paper has been accepted in the NeurIPS 2024 D & B Track. Harmful memes\nhave proliferated on the Chinese Internet, while research on detecting Chinese\nharmful memes significantly lags behind due to the absence of reliable datasets\nand effective detectors. To this end, we focus on the comprehensive detection\nof Chinese harmful memes. We construct ToxiCN MM, the first Chinese harmful\nmeme dataset, which consists of 12,000 samples with fine-grained annotations\nfor various meme types. Additionally, we propose a baseline detector,\nMultimodal Knowledge Enhancement (MKE), incorporating contextual information of\nmeme content generated by the LLM to enhance the understanding of Chinese\nmemes. During the evaluation phase, we conduct extensive quantitative\nexperiments and qualitative analyses on multiple baselines, including LLMs and\nour MKE. The experimental results indicate that detecting Chinese harmful memes\nis challenging for existing models while demonstrating the effectiveness of\nMKE. The resources for this paper are available at\nhttps://github.com/DUT-lujunyu/ToxiCN_MM.", "published": "2024-10-03 10:51:02", "link": "http://arxiv.org/abs/2410.02378v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MenakBERT -- Hebrew Diacriticizer", "abstract": "Diacritical marks in the Hebrew language give words their vocalized form. The\ntask of adding diacritical marks to plain Hebrew text is still dominated by a\nsystem that relies heavily on human-curated resources. Recent models trained on\ndiacritized Hebrew texts still present a gap in performance. We use a recently\ndeveloped char-based PLM to narrowly bridge this gap. Presenting MenakBERT, a\ncharacter level transformer pretrained on Hebrew text and fine-tuned to produce\ndiacritical marks for Hebrew sentences. We continue to show how finetuning a\nmodel for diacritizing transfers to a task such as part of speech tagging.", "published": "2024-10-03 12:07:34", "link": "http://arxiv.org/abs/2410.02417v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning the Latent Rules of a Game from Data: A Chess Story", "abstract": "We demonstrate that small pretrained foundational generative language models\nwith millions of parameters can learn the latent rules of a process from data\nassociated with the process. Inspired by Stefan Zweig's novella\n\"Schachnovelle,\" also known as \"The Royal Game\" in English, we show that 28M\nand 125M parameter pretrained foundational small language models (SLMs) can be\ninstruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of\nchess, propose legal moves, and accurately solve chess problems. We also\nexplore the impact of successive language model fine-tuning epochs on improved\noutcomes and demonstrate reductions in model hallucinations by increasing the\nnumber of instruction fine-tuning examples.", "published": "2024-10-03 12:19:49", "link": "http://arxiv.org/abs/2410.02426v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Collective Critics for Creative Story Generation", "abstract": "Generating a long story of several thousand words with narrative coherence\nusing Large Language Models (LLMs) has been a challenging task. Previous\nresearch has addressed this challenge by proposing different frameworks that\ncreate a story plan and generate a long story based on that plan. However,\nthese frameworks have been mainly focusing on maintaining narrative coherence\nin stories, often overlooking creativity in story planning and the\nexpressiveness of the stories generated from those plans, which are desirable\nproperties to captivate readers' interest. In this paper, we propose Collective\nCritics for Creative Story Generation framework (CritiCS), which is composed of\nplan refining stage (CrPlan) and story generation stage (CrText), to integrate\na collective revision mechanism that promotes those properties into long-form\nstory generation process. Specifically, in each stage, a group of LLM critics\nand one leader collaborate to incrementally refine drafts of plan and story\nthroughout multiple rounds. Extensive human evaluation shows that the CritiCS\ncan significantly enhance story creativity and reader engagement, while also\nmaintaining narrative coherence. Furthermore, the design of the framework\nallows active participation from human writers in any role within the critique\nprocess, enabling interactive human-machine collaboration in story writing.", "published": "2024-10-03 12:21:17", "link": "http://arxiv.org/abs/2410.02428v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language\n  Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ntextual and visual domains but often generate outputs that violate physical\nlaws, revealing a gap in their understanding of the physical world. Inspired by\nhuman cognition, where perception is fundamental to reasoning, we explore\naugmenting LLMs with enhanced perception abilities using Internet of Things\n(IoT) sensor data and pertinent knowledge for IoT task reasoning in the\nphysical world. In this work, we systematically study LLMs capability to\naddress real-world IoT tasks by augmenting their perception and knowledge base,\nand then propose a unified framework, IoT-LLM, to enhance such capability. In\nIoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats\namenable to LLMs, activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions, and expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning. To evaluate the performance, We design a new benchmark\nwith five real-world IoT tasks with different data types and reasoning\ndifficulties and provide the benchmarking results on six open-source and\nclose-source LLMs. Experimental results demonstrate the limitations of existing\nLLMs with naive textual inputs that cannot perform these tasks effectively. We\nshow that IoT-LLM significantly enhances the performance of IoT tasks reasoning\nof LLM, such as GPT-4, achieving an average improvement of 65% across various\ntasks against previous methods. The results also showcase LLMs ability to\ncomprehend IoT data and the physical law behind data by providing a reasoning\nprocess. Limitations of our work are claimed to inspire future research in this\nnew era.", "published": "2024-10-03 12:24:18", "link": "http://arxiv.org/abs/2410.02429v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Better Call SAUL: Fluent and Consistent Language Model Editing with\n  Generation Regularization", "abstract": "To ensure large language models contain up-to-date knowledge, they need to be\nupdated regularly. However, model editing is challenging as it might also\naffect knowledge that is unrelated to the new data. State-of-the-art methods\nidentify parameters associated with specific knowledge and then modify them via\ndirect weight updates. However, these locate-and-edit methods suffer from heavy\ncomputational overhead and lack theoretical validation. In contrast, directly\nfine-tuning the model on requested edits affects the model's behavior on\nunrelated knowledge, and significantly damages the model's generation fluency\nand consistency. To address these challenges, we propose SAUL, a streamlined\nmodel editing method that uses sentence concatenation with augmented random\nfacts for generation regularization. Evaluations on three model editing\nbenchmarks show that SAUL is a practical and reliable solution for model\nediting outperforming state-of-the-art methods while maintaining generation\nquality and reducing computational overhead.", "published": "2024-10-03 12:28:13", "link": "http://arxiv.org/abs/2410.02433v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revealing the Inherent Instructability of Pre-Trained Language Models", "abstract": "Instruction tuning -- supervised fine-tuning using instruction-response pairs\n-- is a key step in making pre-trained large language models (LLMs)\ninstructable. Meanwhile, LLMs perform multitask learning during their\npre-training, acquiring extensive knowledge and capabilities. We hypothesize\nthat the pre-training stage can enable them to develop the ability to\ncomprehend and address instructions. To verify this, we propose Response Tuning\n(RT), which removes the instruction and its corresponding mapping to the\nresponse from instruction tuning. Instead, it focuses solely on establishing\nthe response distribution. Our experiments demonstrate that RT models, trained\nonly on responses, can effectively respond to a wide range of instructions and\nexhibit helpfulness approaching that of their instruction-tuned counterparts.\nIn addition, we observe that the models can recognize and reject unsafe queries\nafter learning the refusal conditions from training responses. Furthermore, we\ndemonstrate that these observations also hold in an in-context learning\nsetting. These findings support our hypothesis, highlighting the extensive\ninherent capabilities of pre-trained LLMs.", "published": "2024-10-03 13:15:19", "link": "http://arxiv.org/abs/2410.02465v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM", "abstract": "Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.", "published": "2024-10-03 13:57:07", "link": "http://arxiv.org/abs/2410.02492v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Dynamic Gradient Alignment for Online Data Mixing", "abstract": "The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability.", "published": "2024-10-03 14:00:44", "link": "http://arxiv.org/abs/2410.02498v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mixed-Session Conversation with Egocentric Memory", "abstract": "Recently introduced dialogue systems have demonstrated high usability.\nHowever, they still fall short of reflecting real-world conversation scenarios.\nCurrent dialogue systems exhibit an inability to replicate the dynamic,\ncontinuous, long-term interactions involving multiple partners. This shortfall\narises because there have been limited efforts to account for both aspects of\nreal-world dialogues: deeply layered interactions over the long-term dialogue\nand widely expanded conversation networks involving multiple participants. As\nthe effort to incorporate these aspects combined, we introduce Mixed-Session\nConversation, a dialogue system designed to construct conversations with\nvarious partners in a multi-session dialogue setup. We propose a new dataset\ncalled MiSC to implement this system. The dialogue episodes of MiSC consist of\n6 consecutive sessions, with four speakers (one main speaker and three\npartners) appearing in each episode. Also, we propose a new dialogue model with\na novel memory management mechanism, called Egocentric Memory Enhanced\nMixed-Session Conversation Agent (EMMA). EMMA collects and retains memories\nfrom the main speaker's perspective during conversations with partners,\nenabling seamless continuity in subsequent interactions. Extensive human\nevaluations validate that the dialogues in MiSC demonstrate a seamless\nconversational flow, even when conversation partners change in each session.\nEMMA trained with MiSC is also evaluated to maintain high memorability without\ncontradiction throughout the entire conversation.", "published": "2024-10-03 14:06:43", "link": "http://arxiv.org/abs/2410.02503v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning\n  with Insights from Multi-Agent Collaboration", "abstract": "Large Language Models (LLMs) could struggle to fully understand legal\ntheories and perform complex legal reasoning tasks. In this study, we introduce\na challenging task (confusing charge prediction) to better evaluate LLMs'\nunderstanding of legal theories and reasoning capabilities. We also propose a\nnovel framework: Multi-Agent framework for improving complex Legal Reasoning\ncapability (MALR). MALR employs non-parametric learning, encouraging LLMs to\nautomatically decompose complex legal tasks and mimic human learning process to\nextract insights from legal rules, helping LLMs better understand legal\ntheories and enhance their legal reasoning abilities. Extensive experiments on\nmultiple real-world datasets demonstrate that the proposed framework\neffectively addresses complex reasoning issues in practical scenarios, paving\nthe way for more reliable applications in the legal domain.", "published": "2024-10-03 14:15:00", "link": "http://arxiv.org/abs/2410.02507v1", "categories": ["cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Contextual Document Embeddings", "abstract": "Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.", "published": "2024-10-03 14:33:34", "link": "http://arxiv.org/abs/2410.02525v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM\n  Interactions", "abstract": "As Large Language Models (LLMs) continue to evolve, they are increasingly\nbeing employed in numerous studies to simulate societies and execute diverse\nsocial tasks. However, LLMs are susceptible to societal biases due to their\nexposure to human-generated data. Given that LLMs are being used to gain\ninsights into various societal aspects, it is essential to mitigate these\nbiases. To that end, our study investigates the presence of implicit gender\nbiases in multi-agent LLM interactions and proposes two strategies to mitigate\nthese biases. We begin by creating a dataset of scenarios where implicit gender\nbiases might arise, and subsequently develop a metric to assess the presence of\nbiases. Our empirical analysis reveals that LLMs generate outputs characterized\nby strong implicit bias associations (>= 50\\% of the time). Furthermore, these\nbiases tend to escalate following multi-agent interactions. To mitigate them,\nwe propose two strategies: self-reflection with in-context examples (ICE); and\nsupervised fine-tuning. Our research demonstrates that both methods effectively\nmitigate implicit biases, with the ensemble of fine-tuning and self-reflection\nproving to be the most successful.", "published": "2024-10-03 15:28:05", "link": "http://arxiv.org/abs/2410.02584v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers", "abstract": "Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation.", "published": "2024-10-03 16:25:37", "link": "http://arxiv.org/abs/2410.02642v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Undesirable Memorization in Large Language Models: A Survey", "abstract": "While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it is equally crucial to examine their associated\nrisks. Among these, privacy and security vulnerabilities are particularly\nconcerning, posing significant ethical and legal challenges. At the heart of\nthese vulnerabilities stands memorization, which refers to a model's tendency\nto store and reproduce phrases from its training data. This phenomenon has been\nshown to be a fundamental source to various privacy and security attacks\nagainst LLMs. In this paper, we provide a taxonomy of the literature on LLM\nmemorization, exploring it across three dimensions: granularity,\nretrievability, and desirability. Next, we discuss the metrics and methods used\nto quantify memorization, followed by an analysis of the causes and factors\nthat contribute to memorization phenomenon. We then explore strategies that are\nused so far to mitigate the undesirable aspects of this phenomenon. We conclude\nour survey by identifying potential research topics for the near future,\nincluding methods to balance privacy and performance, and the analysis of\nmemorization in specific LLM contexts such as conversational agents,\nretrieval-augmented generation, and diffusion language models. Given the rapid\nresearch pace in this field, we also maintain a dedicated repository of the\nreferences discussed in this survey which will be regularly updated to reflect\nthe latest developments.", "published": "2024-10-03 16:34:46", "link": "http://arxiv.org/abs/2410.02650v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring and Improving Persuasiveness of Large Language Models", "abstract": "LLMs are increasingly being used in workflows involving generating content to\nbe consumed by humans (e.g., marketing) and also in directly interacting with\nhumans (e.g., through chatbots). The development of such systems that are\ncapable of generating verifiably persuasive messages presents both\nopportunities and challenges for society. On the one hand, such systems could\npositively impact domains like advertising and social good, such as addressing\ndrug addiction, and on the other, they could be misused for spreading\nmisinformation and shaping political opinions. To channel LLMs' impact on\nsociety, we need to develop systems to measure and benchmark their\npersuasiveness. With this motivation, we introduce PersuasionBench and\nPersuasionArena, the first large-scale benchmark and arena containing a battery\nof tasks to measure the persuasion ability of generative models automatically.\nWe investigate to what extent LLMs know and leverage linguistic patterns that\ncan help them generate more persuasive language. Our findings indicate that the\npersuasiveness of LLMs correlates positively with model size, but smaller\nmodels can also be made to have a higher persuasiveness than much larger\nmodels. Notably, targeted training using synthetic and natural datasets\nsignificantly enhances smaller models' persuasive capabilities, challenging\nscale-dependent assumptions. Our findings carry key implications for both model\ndevelopers and policymakers. For instance, while the EU AI Act and California's\nSB-1047 aim to regulate AI models based on the number of floating point\noperations, we demonstrate that simple metrics like this alone fail to capture\nthe full scope of AI's societal impact. We invite the community to explore and\ncontribute to PersuasionArena and PersuasionBench, available at\nhttps://bit.ly/measure-persuasion, to advance our understanding of AI-driven\npersuasion and its societal implications.", "published": "2024-10-03 16:36:35", "link": "http://arxiv.org/abs/2410.02653v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Hate Personified: Investigating the role of LLMs in content moderation", "abstract": "For subjective tasks such as hate detection, where people perceive hate\ndifferently, the Large Language Model's (LLM) ability to represent diverse\ngroups is unclear. By including additional context in prompts, we\ncomprehensively analyze LLM's sensitivity to geographical priming, persona\nattributes, and numerical information to assess how well the needs of various\ngroups are reflected. Our findings on two LLMs, five languages, and six\ndatasets reveal that mimicking persona-based attributes leads to annotation\nvariability. Meanwhile, incorporating geographical signals leads to better\nregional alignment. We also find that the LLMs are sensitive to numerical\nanchors, indicating the ability to leverage community-based flagging efforts\nand exposure to adversaries. Our work provides preliminary guidelines and\nhighlights the nuances of applying LLMs in culturally sensitive cases.", "published": "2024-10-03 16:43:17", "link": "http://arxiv.org/abs/2410.02657v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "How to Train Long-Context Language Models (Effectively)", "abstract": "We study continued training and supervised fine-tuning (SFT) of a language\nmodel (LM) to make effective use of long-context information. We first\nestablish a reliable evaluation protocol to guide model development -- instead\nof perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set\nof long-context downstream tasks, and we evaluate models after SFT as this\nbetter reveals long-context abilities. Supported by our robust evaluations, we\nrun thorough experiments to decide the data mix for continued pre-training, the\ninstruction tuning dataset, and many other design choices such as position\nextrapolation. We find that (1) code repositories and books are excellent\nsources of long data, but it is crucial to combine them with high-quality\nshort-context data; (2) training with a sequence length beyond the evaluation\nlength boosts long-context performance; (3) for SFT, using only short\ninstruction datasets yields strong performance on long-context tasks. Our final\nmodel, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens,\ndemonstrates state-of-the-art long-context performance among similarly sized\nmodels at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the\nmajority of long-context tasks despite using only 5% as many tokens during\nlong-context training. Additionally, ProLong can effectively process up to 512K\ntokens, one of the longest context windows of publicly available LMs.", "published": "2024-10-03 16:46:52", "link": "http://arxiv.org/abs/2410.02660v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilling an End-to-End Voice Assistant Without Instruction Training\n  Data", "abstract": "Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using $>$100x less training\ncompute.", "published": "2024-10-03 17:04:48", "link": "http://arxiv.org/abs/2410.02678v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HELMET: How to Evaluate Long-Context Language Models Effectively and\n  Thoroughly", "abstract": "Many benchmarks exist for evaluating long-context language models (LCLMs),\nyet developers often rely on synthetic tasks such as needle-in-a-haystack\n(NIAH) or an arbitrary subset of tasks. However, it remains unclear whether\nthese benchmarks reflect the diverse downstream applications of LCLMs, and such\ninconsistencies further complicate model comparison. We investigate the\nunderlying reasons behind these practices and find that existing benchmarks\noften provide noisy signals due to limited coverage of applications,\ninsufficient context lengths, unreliable metrics, and incompatibility with base\nmodels. In this work, we introduce HELMET (How to Evaluate Long-context Models\nEffectively and Thoroughly), a comprehensive benchmark encompassing seven\ndiverse, application-centric categories. We also address several issues in\nprevious benchmarks by adding controllable lengths up to 128K tokens,\nmodel-based evaluation for reliable metrics, and few-shot prompting for\nrobustly evaluating base models. Consequently, we demonstrate that HELMET\noffers more reliable and consistent rankings of frontier LCLMs. Through a\ncomprehensive study of 59 LCLMs, we find that (1) synthetic tasks like NIAH do\nnot reliably predict downstream performance; (2) the diverse categories in\nHELMET exhibit distinct trends and low correlations with each other; and (3)\nwhile most LCLMs achieve perfect NIAH scores, open-source models significantly\nlag behind closed ones when tasks require full-context reasoning or following\ncomplex instructions -- the gap widens as length increases. Finally, we\nrecommend using our RAG tasks for fast model development, as they are easy to\nrun and better predict other downstream performance; ultimately, we advocate\nfor a holistic evaluation across diverse tasks.", "published": "2024-10-03 17:20:11", "link": "http://arxiv.org/abs/2410.02694v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations", "abstract": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.", "published": "2024-10-03 17:31:31", "link": "http://arxiv.org/abs/2410.02707v3", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LLaVA-Critic: Learning to Evaluate Multimodal Models", "abstract": "We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)\ndesigned as a generalist evaluator to assess performance across a wide range of\nmultimodal tasks. LLaVA-Critic is trained using a high-quality critic\ninstruction-following dataset that incorporates diverse evaluation criteria and\nscenarios. Our experiments demonstrate the model's effectiveness in two key\nareas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation\nscores, performing on par with or surpassing GPT models on multiple evaluation\nbenchmarks; and (2) Preference Learning, where it generates reward signals for\npreference learning, enhancing model alignment capabilities. This work\nunderscores the potential of open-source LMMs in self-critique and evaluation,\nsetting the stage for future research into scalable, superhuman alignment\nfeedback mechanisms for LMMs.", "published": "2024-10-03 17:36:33", "link": "http://arxiv.org/abs/2410.02712v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Video Instruction Tuning With Synthetic Data", "abstract": "The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.", "published": "2024-10-03 17:36:49", "link": "http://arxiv.org/abs/2410.02713v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge", "abstract": "LLM-as-a-Judge has been widely utilized as an evaluation method in various\nbenchmarks and served as supervised rewards in model training. However, despite\ntheir excellence in many domains, potential issues are under-explored,\nundermining their reliability and the scope of their utility. Therefore, we\nidentify 12 key potential biases and propose a new automated bias\nquantification framework-CALM-which systematically quantifies and analyzes each\ntype of bias in LLM-as-a-Judge by using automated and principle-guided\nmodification. Our experiments cover multiple popular language models, and the\nresults indicate that while advanced models have achieved commendable overall\nperformance, significant biases persist in certain specific tasks. Empirical\nresults suggest that there remains room for improvement in the reliability of\nLLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence\nof these biases and give some suggestions for the reliable application of\nLLM-as-a-Judge. Our work highlights the need for stakeholders to address these\nissues and remind users to exercise caution in LLM-as-a-Judge applications.", "published": "2024-10-03 17:53:30", "link": "http://arxiv.org/abs/2410.02736v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis", "abstract": "Software engineers mainly write code by editing existing programs. In\ncontrast, language models (LMs) autoregressively synthesize programs in a\nsingle pass. One explanation for this is the scarcity of sequential edit data.\nWhile high-quality instruction data for code synthesis is scarce, edit data for\nsynthesis is even scarcer. To fill this gap, we develop a synthetic data\ngeneration algorithm called LintSeq. This algorithm refactors programs into\nsequences of synthetic edits by using a linter to procedurally sample across\ninterdependent lines of source code. Synthetic edits sampled with LintSeq\nreflect the syntax and semantics of their programming language. To test the\nalgorithm, we use it to refactor a dataset of instruction + program pairs into\ninstruction + program-diff-sequence tuples. Then, we fine-tune a series of\nsmaller LMs ranging from 2.6B to 14B parameters on both the re-factored and\noriginal versions of this dataset. We perform comprehensive evaluations\ncomparing edit sequence code LMs against baselines on HumanEval, MBPP(+),\nCodeContests, DS-1000, and BigCodeBench. We show that models fine-tuned to\niteratively synthesize code match or outperform baselines on pass@1, and\nexhibit better scaling across higher pass@k as a function of total test-time\nFLOPs. Finally, we also pretrain our own tiny LMs for code understanding. We\nshow that fine-tuning these models to synthesize code edit-by-edit results in\nstrong performance on HumanEval and MBPP(+) compared to existing code language\nmodels of similar scale such as CodeT5+, AlphaCode, and Codex.", "published": "2024-10-03 17:57:22", "link": "http://arxiv.org/abs/2410.02749v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GPT-4o as the Gold Standard: A Scalable and General Purpose Approach to\n  Filter Language Model Pretraining Data", "abstract": "Large language models require vast amounts of high-quality training data, but\neffective filtering of web-scale datasets remains a significant challenge. This\npaper demonstrates that GPT-4o is remarkably effective at identifying\nhigh-quality training data, but its prohibitive cost makes it impractical at\nweb-scale. We propose SIEVE, a lightweight alternative that matches GPT-4o\naccuracy at less than 1\\% of the cost. SIEVE can perform up to 500 filtering\noperations for the cost of one GPT-4o filtering call. The key to SIEVE is a\nseamless integration of GPT-4o and lightweight text classification models,\nusing active learning to fine-tune these models in the background with a small\nnumber of calls to GPT-4o. Once trained, it performs as well as GPT-4o at a\ntiny fraction of the cost. Through different filtering prompts, SIEVE can\nefficiently curate high quality data for general or specialized domains from\nweb-scale corpora -- a valuable capability given the current scarcity of\nhigh-quality domain-specific datasets. Extensive experiments using automatic\nand human evaluation metrics show that SIEVE and GPT-4o achieve similar\nperformance on five highly specific filtering prompts. In addition, when\nperforming quality filtering on web crawl datasets, we demonstrate SIEVE can\nfurther improve over state-of-the-art quality filtering methods in the\nDataComp-LM challenge for selecting LLM pretraining data.", "published": "2024-10-03 17:58:29", "link": "http://arxiv.org/abs/2410.02755v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Erasing Conceptual Knowledge from Language Models", "abstract": "In this work, we propose Erasure of Language Memory (ELM), an approach for\nconcept-level unlearning built on the principle of matching the distribution\ndefined by an introspective classifier. Our key insight is that effective\nunlearning should leverage the model's ability to evaluate its own knowledge,\nusing the model itself as a classifier to identify and reduce the likelihood of\ngenerating content related to undesired concepts. ELM applies this framework to\ncreate targeted low-rank updates that reduce generation probabilities for\nconcept-specific content while preserving the model's broader capabilities. We\ndemonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain\nerasure tasks. Comparative analysis shows that ELM achieves superior\nperformance across key metrics, including near-random scores on erased topic\nassessments, maintained coherence in text generation, preserved accuracy on\nunrelated benchmarks, and robustness under adversarial attacks. Our code, data,\nand trained models are available at https://elm.baulab.info", "published": "2024-10-03 17:59:30", "link": "http://arxiv.org/abs/2410.02760v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level\n  Mathematical Reasoning", "abstract": "This paper presents an advanced mathematical problem-solving framework,\nLLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language\nModels (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with\niterative Self-Refine to optimize the reasoning path and utilizes a pairwise\nreward model to evaluate different paths globally. By leveraging the\nself-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS\n(SR-MCTS) overcomes the inefficiencies and limitations of conventional\nstep-wise and greedy search algorithms by fostering a more efficient\nexploration of solution spaces. Pairwise Preference Reward Model~(PPRM),\ninspired by Reinforcement Learning from Human Feedback (RLHF), is then used to\nmodel pairwise preferences between solutions, utilizing an Enhanced Borda Count\n(EBC) method to synthesize these preferences into a global ranking score to\nfind better answers. This approach addresses the challenges of scoring\nvariability and non-independent distributions in mathematical reasoning tasks.\nThe framework has been tested on general and advanced benchmarks, showing\nsuperior performance in terms of search efficiency and problem-solving\ncapability compared to existing methods like ToT and rStar, particularly in\ncomplex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.", "published": "2024-10-03 18:12:29", "link": "http://arxiv.org/abs/2410.02884v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Better Instruction-Following Through Minimum Bayes Risk", "abstract": "General-purpose LLM judges capable of human-level evaluation provide not only\na scalable and accurate way of evaluating instruction-following LLMs but also\nnew avenues for supervising and improving their performance. One promising way\nof leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR)\ndecoding, which uses a reference-based evaluator to select a high-quality\noutput from amongst a set of candidate outputs. In the first part of this work,\nwe explore using MBR decoding as a method for improving the test-time\nperformance of instruction-following LLMs. We find that MBR decoding with\nreference-based LLM judges substantially improves over greedy decoding,\nbest-of-N decoding with reference-free judges and MBR decoding with lexical and\nembedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent\nacross LLMs with up to 70B parameters, demonstrating that smaller LLM judges\ncan be used to supervise much larger LLMs. Then, seeking to retain the\nimprovements from MBR decoding while mitigating additional test-time costs, we\nexplore iterative self-training on MBR-decoded outputs. We find that\nself-training using Direct Preference Optimisation leads to significant\nperformance gains, such that the self-trained models with greedy decoding\ngenerally match and sometimes exceed the performance of their base models with\nMBR decoding.", "published": "2024-10-03 18:48:38", "link": "http://arxiv.org/abs/2410.02902v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Editing with LLM-based Tool Chaining: An Efficient Distillation\n  Approach for Real-Time Applications", "abstract": "We present a practical distillation approach to fine-tune LLMs for invoking\ntools in real-time applications. We focus on visual editing tasks;\nspecifically, we modify images and videos by interpreting user stylistic\nrequests, specified in natural language (\"golden hour\"), using an LLM to select\nthe appropriate tools and their parameters to achieve the desired visual\neffect. We found that proprietary LLMs such as GPT-3.5-Turbo show potential in\nthis task, but their high cost and latency make them unsuitable for real-time\napplications. In our approach, we fine-tune a (smaller) student LLM with\nguidance from a (larger) teacher LLM and behavioral signals. We introduce\noffline metrics to evaluate student LLMs. Both online and offline experiments\nshow that our student models manage to match the performance of our teacher\nmodel (GPT-3.5-Turbo), significantly reducing costs and latency. Lastly, we\nshow that fine-tuning was improved by 25% in low-data regimes using\naugmentation.", "published": "2024-10-03 19:52:37", "link": "http://arxiv.org/abs/2410.02952v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Guided Stream of Search: Learning to Better Search with Language Models\n  via Optimal Path Guidance", "abstract": "While language models have demonstrated impressive capabilities across a\nrange of tasks, they still struggle with tasks that require complex planning\nand reasoning. Recent studies have proposed training language models on search\nprocesses rather than optimal solutions, resulting in better generalization\nperformance even though search processes are noisy and even suboptimal.\nHowever, these studies overlook the value of optimal solutions, which can serve\nas step-by-step landmarks to guide more effective search. In this work, we\nexplore how to leverage optimal solutions to enhance the search and planning\nabilities of language models. To this end, we propose guided stream of search\n(GSoS), which seamlessly incorporates optimal solutions into the\nself-generation process in a progressive manner, producing high-quality search\ntrajectories. These trajectories are then distilled into the pre-trained model\nvia supervised fine-tuning. Our approach significantly enhances the search and\nplanning abilities of language models on Countdown, a simple yet challenging\nmathematical reasoning task. Notably, combining our method with RL fine-tuning\nyields further improvements, whereas previous supervised fine-tuning methods do\nnot benefit from RL. Furthermore, our approach exhibits greater effectiveness\nthan leveraging optimal solutions in the form of subgoal rewards.", "published": "2024-10-03 21:07:59", "link": "http://arxiv.org/abs/2410.02992v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text\n  Detectability in Peer Review", "abstract": "Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in the linguistic capabilities of large language models (LLMs), a\nnew potential risk to the peer review process is that negligent reviewers will\nrely on LLMs to perform the often time consuming process of reviewing a paper.\nIn this study, we investigate the ability of existing AI text detection\nalgorithms to distinguish between peer reviews written by humans and different\nstate-of-the-art LLMs. Our analysis shows that existing approaches fail to\nidentify many GPT-4o written reviews without also producing a high number of\nfalse positive classifications. To address this deficiency, we propose a new\ndetection approach which surpasses existing methods in the identification of\nGPT-4o written peer reviews at low levels of false positive classifications.\nOur work reveals the difficulty of accurately identifying AI-generated text at\nthe individual review level, highlighting the urgent need for new tools and\nmethods to detect this type of unethical application of generative AI.", "published": "2024-10-03 22:05:06", "link": "http://arxiv.org/abs/2410.03019v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Characterizing Context Influence and Hallucination in Summarization", "abstract": "Although Large Language Models (LLMs) have achieved remarkable performance in\nnumerous downstream tasks, their ubiquity has raised two significant concerns.\nOne is that LLMs can hallucinate by generating content that contradicts\nrelevant contextual information; the other is that LLMs can inadvertently leak\nprivate information due to input regurgitation. Many prior works have\nextensively studied each concern independently, but none have investigated them\nsimultaneously. Furthermore, auditing the influence of provided context during\nopen-ended generation with a privacy emphasis is understudied. To this end, we\ncomprehensively characterize the influence and hallucination of contextual\ninformation during summarization. We introduce a definition for context\ninfluence and Context-Influence Decoding (CID), and then we show that\namplifying the context (by factoring out prior knowledge) and the context being\nout of distribution with respect to prior knowledge increases the context's\ninfluence on an LLM. Moreover, we show that context influence gives a lower\nbound of the private information leakage of CID. We corroborate our analytical\nfindings with experimental evaluations that show improving the F1 ROGUE-L score\non CNN-DM for LLaMA 3 by $\\textbf{10}$% over regular decoding also leads to\n$\\textbf{1.5x}$ more influence by the context. Moreover, we empirically\nevaluate how context influence and hallucination are affected by (1) model\ncapacity, (2) context size, (3) the length of the current response, and (4)\ndifferent token $n$-grams of the context. Our code can be accessed here:\nhttps://github.com/james-flemings/context_influence.", "published": "2024-10-03 22:19:28", "link": "http://arxiv.org/abs/2410.03026v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MLP-KAN: Unifying Deep Representation and Function Learning", "abstract": "Recent advancements in both representation learning and function learning\nhave demonstrated substantial promise across diverse domains of artificial\nintelligence. However, the effective integration of these paradigms poses a\nsignificant challenge, particularly in cases where users must manually decide\nwhether to apply a representation learning or function learning model based on\ndataset characteristics. To address this issue, we introduce MLP-KAN, a unified\nmethod designed to eliminate the need for manual model selection. By\nintegrating Multi-Layer Perceptrons (MLPs) for representation learning and\nKolmogorov-Arnold Networks (KANs) for function learning within a\nMixture-of-Experts (MoE) architecture, MLP-KAN dynamically adapts to the\nspecific characteristics of the task at hand, ensuring optimal performance.\nEmbedded within a transformer-based framework, our work achieves remarkable\nresults on four widely-used datasets across diverse domains. Extensive\nexperimental evaluation demonstrates its superior versatility, delivering\ncompetitive performance across both deep representation and function learning\ntasks. These findings highlight the potential of MLP-KAN to simplify the model\nselection process, offering a comprehensive, adaptable solution across various\ndomains. Our code and weights are available at\n\\url{https://github.com/DLYuanGod/MLP-KAN}.", "published": "2024-10-03 22:22:43", "link": "http://arxiv.org/abs/2410.03027v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language\n  Model Ensembling", "abstract": "Large language models (LLMs) exhibit varying strengths and weaknesses across\ndifferent tasks, prompting recent studies to explore the benefits of ensembling\nmodels to leverage their complementary advantages. However, existing LLM\nensembling methods often overlook model compatibility and struggle with\ninefficient alignment of probabilities across the entire vocabulary. In this\nstudy, we empirically investigate the factors influencing ensemble performance,\nidentifying model performance, vocabulary size, and response style as key\ndeterminants, revealing that compatibility among models is essential for\neffective ensembling. This analysis leads to the development of a simple yet\neffective model selection strategy that identifies compatible models.\nAdditionally, we introduce the \\textsc{Uni}on \\textsc{T}op-$k$\n\\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently\ncombines models by focusing on the union of the top-k tokens from each model,\nthereby avoiding the need for full vocabulary alignment and reducing\ncomputational overhead. Extensive evaluations across multiple benchmarks\ndemonstrate that \\textsc{UniTE} significantly enhances performance compared to\nexisting methods, offering a more efficient framework for LLM ensembling.", "published": "2024-10-03 08:42:38", "link": "http://arxiv.org/abs/2410.03777v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision", "abstract": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance\nthe Retrieval-Augmented Generation (RAG) model through Reward-Driven\nSupervision. Unlike previous RAG methodologies, which focus on training\nlanguage models (LMs) to utilize external knowledge retrieved from external\nsources, our method adapts retrieval information to specific domains by\nemploying CriticGPT to train a dedicated reward model. This reward model\ngenerates synthesized datasets for fine-tuning the RAG encoder, aligning its\noutputs more closely with human preferences. The versatility of our approach\nallows it to be effectively applied across various domains through\ndomain-specific fine-tuning. We evaluate Reward-RAG on publicly available\nbenchmarks from multiple domains, comparing it to state-of-the-art methods. Our\nexperimental results demonstrate significant improvements in performance,\nhighlighting the effectiveness of Reward-RAG in improving the relevance and\nquality of generated responses. These findings underscore the potential of\nintegrating reward models with RAG to achieve superior outcomes in natural\nlanguage generation tasks.", "published": "2024-10-03 15:26:50", "link": "http://arxiv.org/abs/2410.03780v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reconstructing Human Mobility Pattern: A Semi-Supervised Approach for\n  Cross-Dataset Transfer Learning", "abstract": "Understanding human mobility patterns is crucial for urban planning,\ntransportation management, and public health. This study tackles two primary\nchallenges in the field: the reliance on trajectory data, which often fails to\ncapture the semantic interdependencies of activities, and the inherent\nincompleteness of real-world trajectory data. We have developed a model that\nreconstructs and learns human mobility patterns by focusing on semantic\nactivity chains. We introduce a semi-supervised iterative transfer learning\nalgorithm to adapt models to diverse geographical contexts and address data\nscarcity. Our model is validated using comprehensive datasets from the United\nStates, where it effectively reconstructs activity chains and generates\nhigh-quality synthetic mobility data, achieving a low Jensen-Shannon Divergence\n(JSD) value of 0.001, indicating a close similarity between synthetic and real\ndata. Additionally, sparse GPS data from Egypt is used to evaluate the transfer\nlearning algorithm, demonstrating successful adaptation of US mobility patterns\nto Egyptian contexts, achieving a 64\\% of increase in similarity, i.e., a JSD\nreduction from 0.09 to 0.03. This mobility reconstruction model and the\nassociated transfer learning algorithm show significant potential for global\nhuman mobility modeling studies, enabling policymakers and researchers to\ndesign more effective and culturally tailored transportation solutions.", "published": "2024-10-03 20:29:56", "link": "http://arxiv.org/abs/2410.03788v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Dutch Financial Large Language Model", "abstract": "This paper presents FinGEITje, the first Dutch financial Large Language Model\n(LLM) specifically designed and optimized for various financial tasks. Together\nwith the model, we release a specialized Dutch financial instruction tuning\ndataset with over 140,000 samples, constructed employing an automated\ntranslation and data processing method. The open-source data construction\nmethod is provided, facilitating the creation of financial instruction datasets\nin different languages. To evaluate model performance, the study introduces the\nfirst Dutch financial evaluation benchmark, along with an automated evaluation\nmethod that utilizes an LLM as an independent evaluator, reducing manual\nintervention in performance evaluation. The experimental results highlight the\nsuperior performance of FinGEITje across five critical Dutch and English\nfinancial tasks.", "published": "2024-10-03 08:38:31", "link": "http://arxiv.org/abs/2410.12835v1", "categories": ["cs.CE", "cs.CL"], "primary_category": "cs.CE"}
{"title": "Can LLMs Reliably Simulate Human Learner Actions? A Simulation Authoring\n  Framework for Open-Ended Learning Environments", "abstract": "Simulating learner actions helps stress-test open-ended interactive learning\nenvironments and prototype new adaptations before deployment. While recent\nstudies show the promise of using large language models (LLMs) for simulating\nhuman behavior, such approaches have not gone beyond rudimentary\nproof-of-concept stages due to key limitations. First, LLMs are highly\nsensitive to minor prompt variations, raising doubts about their ability to\ngeneralize to new scenarios without extensive prompt engineering. Moreover,\napparently successful outcomes can often be unreliable, either because domain\nexperts unintentionally guide LLMs to produce expected results, leading to\nself-fulfilling prophecies; or because the LLM has encountered highly similar\nscenarios in its training data, meaning that models may not be simulating\nbehavior so much as regurgitating memorized content. To address these\nchallenges, we propose Hyp-Mix, a simulation authoring framework that allows\nexperts to develop and evaluate simulations by combining testable hypotheses\nabout learner behavior. Testing this framework in a physics learning\nenvironment, we found that GPT-4 Turbo maintains calibrated behavior even as\nthe underlying learner model changes, providing the first evidence that LLMs\ncan be used to simulate realistic behaviors in open-ended interactive learning\nenvironments, a necessary prerequisite for useful LLM behavioral simulation.", "published": "2024-10-03 00:25:40", "link": "http://arxiv.org/abs/2410.02110v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities", "abstract": "Multimodal Large Language Models have made significant strides in integrating\nvisual and textual information, yet they often struggle with effectively\naligning these modalities. We introduce a novel image tokenizer that bridges\nthis gap by applying the principle of Byte-Pair Encoding (BPE) to visual data.\nUnlike conventional approaches that rely on separate visual encoders, our\nmethod directly incorporates structural prior information into image tokens,\nmirroring the successful tokenization strategies used in text-only Large\nLanguage Models. This innovative approach enables Transformer models to more\neffectively learn and reason across modalities. Through theoretical analysis\nand extensive experiments, we demonstrate that our BPE Image Tokenizer\nsignificantly enhances MLLMs' multimodal understanding capabilities, even with\nlimited training data. Leveraging this method, we develop Being-VL-0, a model\nthat demonstrates superior performance across various benchmarks and shows\npromising scalability, potentially paving the way for more efficient and\ncapable multimodal foundation models.", "published": "2024-10-03 02:34:31", "link": "http://arxiv.org/abs/2410.02155v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "The why, what, and how of AI-based coding in scientific research", "abstract": "Computer programming (coding) is indispensable for researchers across\ndisciplines, yet it remains challenging to learn and time-consuming to carry\nout. Generative AI, particularly large language models (LLMs), has the\npotential to transform coding into intuitive conversations, but best practices\nand effective workflows are only emerging. We dissect AI-based coding through\nthree key lenses: the nature and role of LLMs in coding (why), six types of\ncoding assistance they provide (what), and a five-step workflow in action with\npractical implementation strategies (how). Additionally, we address the\nlimitations and future outlook of AI in coding. By offering actionable\ninsights, this framework helps to guide researchers in effectively leveraging\nAI to enhance coding practices and education, accelerating scientific progress.", "published": "2024-10-03 02:36:30", "link": "http://arxiv.org/abs/2410.02156v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.CY"}
{"title": "Mitigating Memorization In Language Models", "abstract": "Language models (LMs) can \"memorize\" information, i.e., encode training data\nin their weights in such a way that inference-time queries can lead to verbatim\nregurgitation of that data. This ability to extract training data can be\nproblematic, for example, when data are private or sensitive. In this work, we\ninvestigate methods to mitigate memorization: three regularizer-based, three\nfinetuning-based, and eleven machine unlearning-based methods, with five of the\nlatter being new methods that we introduce. We also introduce TinyMem, a suite\nof small, computationally-efficient LMs for the rapid development and\nevaluation of memorization-mitigation methods. We demonstrate that the\nmitigation methods that we develop using TinyMem can successfully be applied to\nproduction-grade LMs, and we determine via experiment that: regularizer-based\nmitigation methods are slow and ineffective at curbing memorization;\nfine-tuning-based methods are effective at curbing memorization, but overly\nexpensive, especially for retaining higher accuracies; and unlearning-based\nmethods are faster and more effective, allowing for the precise localization\nand removal of memorized information from LM weights prior to inference. We\nshow, in particular, that our proposed unlearning method BalancedSubnet\noutperforms other mitigation methods at removing memorized information while\npreserving performance on target tasks.", "published": "2024-10-03 02:53:51", "link": "http://arxiv.org/abs/2410.02159v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adversarial Decoding: Generating Readable Documents for Adversarial\n  Objectives", "abstract": "We design, implement, and evaluate adversarial decoding, a new, generic text\ngeneration technique that produces readable documents for different adversarial\nobjectives. Prior methods either produce easily detectable gibberish, or cannot\nhandle objectives that include embedding similarity. In particular, they only\nwork for direct attacks (such as jailbreaking) and cannot produce adversarial\ntext for realistic indirect injection, e.g., documents that (1) are retrieved\nin RAG systems in response to broad classes of queries, and also (2)\nadversarially influence subsequent generation. We also show that fluency (low\nperplexity) is not sufficient to evade filtering. We measure the effectiveness\nof adversarial decoding for different objectives, including RAG poisoning,\njailbreaking, and evasion of defensive filters, and demonstrate that it\noutperforms existing methods while producing readable adversarial documents.", "published": "2024-10-03 03:06:42", "link": "http://arxiv.org/abs/2410.02163v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HATFormer: Historic Handwritten Arabic Text Recognition with\n  Transformers", "abstract": "Arabic handwritten text recognition (HTR) is challenging, especially for\nhistorical texts, due to diverse writing styles and the intrinsic features of\nArabic script. Additionally, Arabic handwriting datasets are smaller compared\nto English ones, making it difficult to train generalizable Arabic HTR models.\nTo address these challenges, we propose HATFormer, a transformer-based\nencoder-decoder architecture that builds on a state-of-the-art English HTR\nmodel. By leveraging the transformer's attention mechanism, HATFormer captures\nspatial contextual information to address the intrinsic challenges of Arabic\nscript through differentiating cursive characters, decomposing visual\nrepresentations, and identifying diacritics. Our customization to historical\nhandwritten Arabic includes an image processor for effective ViT information\npreprocessing, a text tokenizer for compact Arabic text representation, and a\ntraining pipeline that accounts for a limited amount of historic Arabic\nhandwriting data. HATFormer achieves a character error rate (CER) of 8.6% on\nthe largest public historical handwritten Arabic dataset, with a 51%\nimprovement over the best baseline in the literature. HATFormer also attains a\ncomparable CER of 4.2% on the largest private non-historical dataset. Our work\ndemonstrates the feasibility of adapting an English HTR method to a\nlow-resource language with complex, language-specific challenges, contributing\nto advancements in document digitization, information retrieval, and cultural\npreservation.", "published": "2024-10-03 03:43:29", "link": "http://arxiv.org/abs/2410.02179v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "CodeJudge: Evaluating Code Generation with Large Language Models", "abstract": "Large Language Models (LLMs) have shown promising performance in code\ngeneration. However, how to reliably evaluate code generated by LLMs remains an\nunresolved problem. This paper presents CodeJudge, a code evaluation framework\nthat leverages LLMs to evaluate the semantic correctness of generated code\nwithout the need for test cases. We investigate different ways to guide the LLM\nin performing \"slow thinking\" to arrive at an in-depth and reliable evaluation.\nWe experimented with four LLMs as evaluators on four code generation datasets\nand five programming languages. The results show that CodeJudge significantly\noutperformed existing methods in most settings. Furthermore, compared with a\nSOTA GPT-3.5-based code evaluation method, CodeJudge achieved better results\neven when using a much smaller model, Llama-3-8B-Instruct. Our code and\ndatasets are available on GitHub https://github.com/VichyTong/CodeJudge.", "published": "2024-10-03 03:58:03", "link": "http://arxiv.org/abs/2410.02184v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "POSIX: A Prompt Sensitivity Index For Large Language Models", "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are found\nto be surprisingly sensitive to minor variations in prompts, often generating\nsignificantly divergent outputs in response to minor variations in the prompts,\nsuch as spelling errors, alteration of wording or the prompt template. However,\nwhile assessing the quality of an LLM, the focus often tends to be solely on\nits performance on downstream tasks, while very little to no attention is paid\nto prompt sensitivity. To fill this gap, we propose POSIX - a novel PrOmpt\nSensitivity IndeX as a reliable measure of prompt sensitivity, thereby offering\na more comprehensive evaluation of LLM performance. The key idea behind POSIX\nis to capture the relative change in loglikelihood of a given response upon\nreplacing the corresponding prompt with a different intent-preserving prompt.\nWe provide thorough empirical evidence demonstrating the efficacy of POSIX in\ncapturing prompt sensitivity and subsequently use it to measure and thereby\ncompare prompt sensitivity of various open-source LLMs. We find that merely\nincreasing the parameter count or instruction tuning does not necessarily\nreduce prompt sensitivity whereas adding some few-shot exemplars, even just\none, almost always leads to significant decrease in prompt sensitivity. We also\nfind that alterations to prompt template lead to the highest sensitivity in the\ncase of MCQ type tasks, whereas paraphrasing results in the highest sensitivity\nin open-ended generation tasks. The code for reproducing our results is\nopen-sourced at https://github.com/kowndinya-renduchintala/POSIX.", "published": "2024-10-03 04:01:14", "link": "http://arxiv.org/abs/2410.02185v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Bradley-Terry Models: A General Preference Model for Language\n  Model Alignment", "abstract": "Modeling human preferences is crucial for aligning foundation models with\nhuman values. Traditional reward modeling methods, such as the Bradley-Terry\n(BT) reward model, fall short in expressiveness, particularly in addressing\nintransitive preferences. In this paper, we introduce preference embedding, an\napproach that embeds responses into a latent space to capture intricate\npreference structures efficiently, achieving linear query complexity.\nAdditionally, we propose preference score-based General Preference Optimization\n(GPO), which generalizes reward-based reinforcement learning from human\nfeedback (RLHF). Experimental results show that our General Preference\nembedding Model (GPM) consistently outperforms the BT reward model on the\nRewardBench benchmark and effectively models cyclic preferences where any BT\nreward model behaves like a random guess. Furthermore, evaluations on\ndownstream tasks such as AlpacaEval2.0, following the language model\npost-training with GPO and our general preference model, reveal performance\nimprovements over BT models. These findings indicate that our method may\nenhance the alignment of foundation models with nuanced human values. The code\nis available at https://github.com/general-preference/general-preference-model.", "published": "2024-10-03 04:22:55", "link": "http://arxiv.org/abs/2410.02197v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Aligning with Logic: Measuring, Evaluating and Improving Logical\n  Preference Consistency in Large Language Models", "abstract": "Large Language Models (LLMs) are expected to be predictable and trustworthy\nto support reliable decision-making systems. Yet current LLMs often show\ninconsistencies in their judgments. In this work, we examine logical preference\nconsistency as a foundational requirement for building more dependable LLM\nsystems, ensuring stable and coherent decision-making while minimizing erratic\nor contradictory outputs. To quantify the logical preference consistency, we\npropose a universal evaluation framework based on three fundamental properties:\ntransitivity, commutativity and negation invariance. Through extensive\nexperimentation across diverse LLMs, we demonstrate that these properties serve\nas strong indicators of judgment robustness. Furthermore, we introduce a data\nrefinement and augmentation technique, REPAIR, that enhances logical\nconsistency while maintaining alignment with human preferences. Finally, we\nshow that improving consistency leads to better performance in LLM-driven\nlogic-based algorithms, reinforcing stability and coherence in decision-making\nsystems.", "published": "2024-10-03 04:34:04", "link": "http://arxiv.org/abs/2410.02205v3", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "EmbedLLM: Learning Compact Representations of Large Language Models", "abstract": "With hundreds of thousands of language models available on Huggingface today,\nefficiently evaluating and utilizing these models across various downstream,\ntasks has become increasingly critical. Many existing methods repeatedly learn\ntask-specific representations of Large Language Models (LLMs), which leads to\ninefficiencies in both time and computational resources. To address this, we\npropose EmbedLLM, a framework designed to learn compact vector representations,\nof LLMs that facilitate downstream applications involving many models, such as\nmodel routing. We introduce an encoder-decoder approach for learning such\nembeddings, along with a systematic framework to evaluate their effectiveness.\nEmpirical results show that EmbedLLM outperforms prior methods in model routing\nboth in accuracy and latency. Additionally, we demonstrate that our method can\nforecast a model's performance on multiple benchmarks, without incurring\nadditional inference cost. Extensive probing experiments validate that the\nlearned embeddings capture key model characteristics, e.g. whether the model is\nspecialized for coding tasks, even without being explicitly trained on them. We\nopen source our dataset, code and embedder to facilitate further research and\napplication.", "published": "2024-10-03 05:43:24", "link": "http://arxiv.org/abs/2410.02223v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Pilot Study of Applying Sequence-to-Sequence Voice Conversion to\n  Evaluate the Intelligibility of L2 Speech Using a Native Speaker's Shadowings", "abstract": "Utterances by L2 speakers can be unintelligible due to mispronunciation and\nimproper prosody. In computer-aided language learning systems, textual feedback\nis often provided using a speech recognition engine. However, an ideal form of\nfeedback for L2 speakers should be so fine-grained that it enables them to\ndetect and diagnose unintelligible parts of L2 speakers' utterances. Inspired\nby language teachers who correct students' pronunciation through a\nvoice-to-voice process, this pilot study utilizes a unique semi-parallel\ndataset composed of non-native speakers' (L2) reading aloud, shadowing of\nnative speakers (L1) and their script-shadowing utterances. We explore the\ntechnical possibility of replicating the process of an L1 speaker's shadowing\nL2 speech using Voice Conversion techniques, to create a virtual shadower\nsystem. Experimental results demonstrate the feasibility of the VC system in\nsimulating L1's shadowing behavior. The output of the virtual shadower system\nshows a reasonable similarity to the real L1 shadowing utterances in both\nlinguistic and acoustic aspects.", "published": "2024-10-03 06:24:56", "link": "http://arxiv.org/abs/2410.02239v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Structural-Entropy-Based Sample Selection for Efficient and Effective\n  Learning", "abstract": "Sample selection improves the efficiency and effectiveness of machine\nlearning models by providing informative and representative samples. Typically,\nsamples can be modeled as a sample graph, where nodes are samples and edges\nrepresent their similarities. Most existing methods are based on local\ninformation, such as the training difficulty of samples, thereby overlooking\nglobal information, such as connectivity patterns. This oversight can result in\nsuboptimal selection because global information is crucial for ensuring that\nthe selected samples well represent the structural properties of the graph. To\naddress this issue, we employ structural entropy to quantify global information\nand losslessly decompose it from the whole graph to individual nodes using the\nShapley value. Based on the decomposition, we present\n$\\textbf{S}$tructural-$\\textbf{E}$ntropy-based sample $\\textbf{S}$election\n($\\textbf{SES}$), a method that integrates both global and local information to\nselect informative and representative samples. SES begins by constructing a\n$k$NN-graph among samples based on their similarities. It then measures sample\nimportance by combining structural entropy (global metric) with training\ndifficulty (local metric). Finally, SES applies importance-biased blue noise\nsampling to select a set of diverse and representative samples. Comprehensive\nexperiments on three learning scenarios -- supervised learning, active\nlearning, and continual learning -- clearly demonstrate the effectiveness of\nour method.", "published": "2024-10-03 07:40:14", "link": "http://arxiv.org/abs/2410.02268v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Post-edits Are Preferences Too", "abstract": "Preference Optimization (PO) techniques are currently one of the state of the\nart techniques for fine-tuning large language models (LLMs) on pairwise\npreference feedback from human annotators. However, in machine translation,\nthis sort of feedback can be difficult to solicit. Additionally, Kreutzer et\nal. (2018) have shown that, for machine translation, pairwise preferences are\nless reliable than other forms of human feedback, such as 5-point ratings.\n  We examine post-edits to see if they can be a source of reliable human\npreferences by construction. In PO, a human annotator is shown sequences $s_1$\nand $s_2$ and asked for a preference judgment, %$s_1 > s_2$; while for\npost-editing, editors create $s_1$ and know that it should be better than\n$s_2$. We attempt to use these implicit preferences for PO and show that it\nhelps the model move towards post-edit-like hypotheses and away from machine\ntranslation-like hypotheses. Furthermore, we show that best results are\nobtained by pre-training the model with supervised fine-tuning (SFT) on\npost-edits in order to promote post-edit-like hypotheses to the top output\nranks.", "published": "2024-10-03 08:56:29", "link": "http://arxiv.org/abs/2410.02320v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences", "abstract": "Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.", "published": "2024-10-03 11:01:25", "link": "http://arxiv.org/abs/2410.02381v4", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter Competition Balancing for Model Merging", "abstract": "While fine-tuning pretrained models has become common practice, these models\noften underperform outside their specific domains. Recently developed model\nmerging techniques enable the direct integration of multiple models, each\nfine-tuned for distinct tasks, into a single model. This strategy promotes\nmultitasking capabilities without requiring retraining on the original\ndatasets. However, existing methods fall short in addressing potential\nconflicts and complex correlations between tasks, especially in parameter-level\nadjustments, posing a challenge in effectively balancing parameter competition\nacross various tasks. This paper introduces an innovative technique named\nPCB-Merging (Parameter Competition Balancing), a lightweight and training-free\ntechnique that adjusts the coefficients of each parameter for effective model\nmerging. PCB-Merging employs intra-balancing to gauge parameter significance\nwithin individual tasks and inter-balancing to assess parameter similarities\nacross different tasks. Parameters with low importance scores are dropped, and\nthe remaining ones are rescaled to form the final merged model. We assessed our\napproach in diverse merging scenarios, including cross-task, cross-domain, and\ncross-training configurations, as well as out-of-domain generalization. The\nexperimental results reveal that our approach achieves substantial performance\nenhancements across multiple modalities, domains, model sizes, number of tasks,\nfine-tuning forms, and large language models, outperforming existing model\nmerging methods. The code is publicly available at:\n\\url{https://github.com/duguodong7/pcb-merging}.", "published": "2024-10-03 11:17:58", "link": "http://arxiv.org/abs/2410.02396v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LLM-Pilot: Characterize and Optimize Performance of your LLM Inference\n  Services", "abstract": "As Large Language Models (LLMs) are rapidly growing in popularity, LLM\ninference services must be able to serve requests from thousands of users while\nsatisfying performance requirements. The performance of an LLM inference\nservice is largely determined by the hardware onto which it is deployed, but\nunderstanding of which hardware will deliver on performance requirements\nremains challenging. In this work we present LLM-Pilot - a first-of-its-kind\nsystem for characterizing and predicting performance of LLM inference services.\nLLM-Pilot performs benchmarking of LLM inference services, under a realistic\nworkload, across a variety of GPUs, and optimizes the service configuration for\neach considered GPU to maximize performance. Finally, using this\ncharacterization data, LLM-Pilot learns a predictive model, which can be used\nto recommend the most cost-effective hardware for a previously unseen LLM.\nCompared to existing methods, LLM-Pilot can deliver on performance requirements\n33% more frequently, whilst reducing costs by 60% on average.", "published": "2024-10-03 12:19:06", "link": "http://arxiv.org/abs/2410.02425v1", "categories": ["cs.DC", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation", "abstract": "Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs", "published": "2024-10-03 14:50:33", "link": "http://arxiv.org/abs/2410.02458v2", "categories": ["eess.IV", "cs.CL", "cs.CV"], "primary_category": "eess.IV"}
{"title": "ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration", "abstract": "We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by the Multidisciplinary Team (MDT) approach used in\nclinical settings, ColaCare employs two types of agents: DoctorAgents and a\nMetaAgent, which collaboratively analyze patient data. Expert models process\nand generate predictions from numerical EHR data, while LLM agents produce\nreasoning references and decision-making reports within the MDT-driven\ncollaborative consultation framework. The MetaAgent orchestrates the\ndiscussion, facilitating consultations and evidence-based debates among\nDoctorAgents, simulating diverse expertise in clinical decision-making. We\nadditionally incorporate the Merck Manual of Diagnosis and Therapy (MSD)\nmedical guideline within a retrieval-augmented generation (RAG) module for\nmedical evidence support, addressing the challenge of knowledge currency.\nExtensive experiments conducted on three EHR datasets demonstrate ColaCare's\nsuperior performance in clinical mortality outcome and readmission prediction\ntasks, underscoring its potential to revolutionize clinical decision support\nsystems and advance personalized precision medicine. All code, case studies and\na questionnaire are available at the project website:\nhttps://colacare.netlify.app.", "published": "2024-10-03 14:55:22", "link": "http://arxiv.org/abs/2410.02551v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Convolutional Variational Autoencoders for Spectrogram Compression in\n  Automatic Speech Recognition", "abstract": "For many Automatic Speech Recognition (ASR) tasks audio features as\nspectrograms show better results than Mel-frequency Cepstral Coefficients\n(MFCC), but in practice they are hard to use due to a complex dimensionality of\na feature space. The following paper presents an alternative approach towards\ngenerating compressed spectrogram representation, based on Convolutional\nVariational Autoencoders (VAE). A Convolutional VAE model was trained on a\nsubsample of the LibriSpeech dataset to reconstruct short fragments of audio\nspectrograms (25 ms) from a 13-dimensional embedding. The trained model for a\n40-dimensional (300 ms) embedding was used to generate features for corpus of\nspoken commands on the GoogleSpeechCommands dataset. Using the generated\nfeatures an ASR system was built and compared to the model with MFCC features.", "published": "2024-10-03 15:04:27", "link": "http://arxiv.org/abs/2410.02560v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Agents' Room: Narrative Generation through Multi-step Collaboration", "abstract": "Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.", "published": "2024-10-03 15:44:42", "link": "http://arxiv.org/abs/2410.02603v2", "categories": ["cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "IndicSentEval: How Effectively do Multilingual Transformer Models encode\n  Linguistic Properties for Indic Languages?", "abstract": "Transformer-based models have revolutionized the field of natural language\nprocessing. To understand why they perform so well and to assess their\nreliability, several studies have focused on questions such as: Which\nlinguistic properties are encoded by these models, and to what extent? How\nrobust are these models in encoding linguistic properties when faced with\nperturbations in the input text? However, these studies have mainly focused on\nBERT and the English language. In this paper, we investigate similar questions\nregarding encoding capability and robustness for 8 linguistic properties across\n13 different perturbations in 6 Indic languages, using 9 multilingual\nTransformer models (7 universal and 2 Indic-specific). To conduct this study,\nwe introduce a novel multilingual benchmark dataset, IndicSentEval, containing\napproximately $\\sim$47K sentences. Surprisingly, our probing analysis of\nsurface, syntactic, and semantic properties reveals that while almost all\nmultilingual models demonstrate consistent encoding performance for English,\nthey show mixed results for Indic languages. As expected, Indic-specific\nmultilingual models capture linguistic properties in Indic languages better\nthan universal models. Intriguingly, universal models broadly exhibit better\nrobustness compared to Indic-specific models, particularly under perturbations\nsuch as dropping both nouns and verbs, dropping only verbs, or keeping only\nnouns. Overall, this study provides valuable insights into probing and\nperturbation-specific strengths and weaknesses of popular multilingual\nTransformer-based models for different Indic languages. We make our code and\ndataset publicly available [https://tinyurl.com/IndicSentEval}].", "published": "2024-10-03 15:50:08", "link": "http://arxiv.org/abs/2410.02611v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NL-Eye: Abductive NLI for Images", "abstract": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it\ndetects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet\ntheir ability to infer outcomes and causes remains underexplored. To address\nthis, we introduce NL-Eye, a benchmark designed to assess VLMs' visual\nabductive reasoning skills. NL-Eye adapts the abductive Natural Language\nInference (NLI) task to the visual domain, requiring models to evaluate the\nplausibility of hypothesis images based on a premise image and explain their\ndecisions. NL-Eye consists of 350 carefully curated triplet examples (1,050\nimages) spanning diverse reasoning categories: physical, functional, logical,\nemotional, cultural, and social. The data curation process involved two steps -\nwriting textual descriptions and generating images using text-to-image models,\nboth requiring substantial human involvement to ensure high-quality and\nchallenging scenes. Our experiments show that VLMs struggle significantly on\nNL-Eye, often performing at random baseline levels, while humans excel in both\nplausibility prediction and explanation quality. This demonstrates a deficiency\nin the abductive reasoning capabilities of modern VLMs. NL-Eye represents a\ncrucial step toward developing VLMs capable of robust multimodal reasoning for\nreal-world applications, including accident-prevention bots and generated video\nverification.", "published": "2024-10-03 15:51:36", "link": "http://arxiv.org/abs/2410.02613v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Immunogenicity Prediction with Dual Attention Enables Vaccine Target\n  Selection", "abstract": "Immunogenicity prediction is a central topic in reverse vaccinology for\nfinding candidate vaccines that can trigger protective immune responses.\nExisting approaches typically rely on highly compressed features and simple\nmodel architectures, leading to limited prediction accuracy and poor\ngeneralizability. To address these challenges, we introduce ProVaccine, a novel\ndeep learning solution with a dual attention mechanism that integrates\npre-trained latent vector representations of protein sequences and structures.\nWe also compile the most comprehensive immunogenicity dataset to date,\nencompassing over 9,500 antigen sequences, structures, and immunogenicity\nlabels from bacteria, viruses, and tumors. Extensive experiments demonstrate\nthat ProVaccine outperforms existing methods across a wide range of evaluation\nmetrics. Furthermore, we establish a post-hoc validation protocol to assess the\npractical significance of deep learning models in tackling vaccine design\nchallenges. Our work provides an effective tool for vaccine design and sets\nvaluable benchmarks for future research.", "published": "2024-10-03 16:33:35", "link": "http://arxiv.org/abs/2410.02647v1", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "FAN: Fourier Analysis Networks", "abstract": "Despite the remarkable successes of general-purpose neural networks, such as\nMLPs and Transformers, we find that they exhibit notable shortcomings in\nmodeling and reasoning about periodic phenomena, achieving only marginal\nperformance within the training domain and failing to generalize effectively to\nout-of-domain (OOD) scenarios. Periodicity is ubiquitous throughout nature and\nscience. Therefore, neural networks should be equipped with the essential\nability to model and handle periodicity. In this work, we propose FAN, a novel\ngeneral-purpose neural network that offers broad applicability similar to MLP\nwhile effectively addressing periodicity modeling challenges. Periodicity is\nnaturally integrated into FAN's structure and computational processes by\nintroducing the Fourier Principle. Unlike existing Fourier-based networks,\nwhich possess particular periodicity modeling abilities but are typically\ndesigned for specific tasks, our approach maintains the general-purpose\nmodeling capability. Therefore, FAN can seamlessly replace MLP in various model\narchitectures with fewer parameters and FLOPs. Through extensive experiments,\nwe demonstrate the superiority of FAN in periodicity modeling tasks and the\neffectiveness and generalizability of FAN across a range of real-world tasks,\ne.g., symbolic formula representation, time series forecasting, language\nmodeling, and image recognition.", "published": "2024-10-03 17:02:21", "link": "http://arxiv.org/abs/2410.02675v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring\n  the (Lack of) Cultural Knowledge of LLMs", "abstract": "To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East.", "published": "2024-10-03 17:04:31", "link": "http://arxiv.org/abs/2410.02677v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life", "abstract": "As users increasingly seek guidance from LLMs for decision-making in daily\nlife, many of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of people. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\npresents two possible actions, along with affected parties and relevant human\nvalues for each action. Based on these dilemmas, we gather a repository of\nhuman values covering diverse everyday topics, such as interpersonal\nrelationships, workplace, and environmental issues. With DailyDilemmas, we\nevaluate LLMs on these dilemmas to determine what action they will choose and\nthe values represented by these action choices. Then, we analyze values through\nthe lens of five theoretical frameworks inspired by sociology, psychology, and\nphilosophy, including the World Values Survey, Moral Foundations Theory,\nMaslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of\nEmotions. For instance, we find LLMs are most aligned with self-expression over\nsurvival in World Values Survey and care over loyalty in Moral Foundations\nTheory. Interestingly, we find substantial preference differences in models for\nsome core values. For example, for truthfulness, Mixtral-8x7B neglects it by\n9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance\nreleased by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand\nhow their designated principles reflect their models' actual value\nprioritization when facing nuanced moral reasoning in daily-life settings.\nFinally, we find that end users cannot effectively steer such prioritization\nusing system prompts.", "published": "2024-10-03 17:08:52", "link": "http://arxiv.org/abs/2410.02683v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selective Attention Improves Transformer", "abstract": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention improves language modeling performance in a variety of model sizes\nand context lengths. For example, a range of transformers trained with the\nlanguage modeling objective on C4 with selective attention perform equivalently\nto standard transformers with ~2X more heads and parameters in their attention\nmodules. Selective attention also allows decreasing the size of the attention's\ncontext buffer, leading to meaningful reductions in the memory and compute\nrequirements during inference. For example, transformers with 100M parameters\ntrained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and\n47X less memory for their attention module, respectively, when equipped with\nselective attention, as those without selective attention, with the same\nvalidation perplexity.", "published": "2024-10-03 17:27:30", "link": "http://arxiv.org/abs/2410.02703v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain-Specific Retrieval-Augmented Generation Using Vector Stores,\n  Knowledge Graphs, and Tensor Factorization", "abstract": "Large Language Models (LLMs) are pre-trained on large-scale corpora and excel\nin numerous general natural language processing (NLP) tasks, such as question\nanswering (QA). Despite their advanced language capabilities, when it comes to\ndomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,\nknowledge cut-offs, and lack of knowledge attributions. Additionally, fine\ntuning LLMs' intrinsic knowledge to highly specific domains is an expensive and\ntime consuming process. The retrieval-augmented generation (RAG) process has\nrecently emerged as a method capable of optimization of LLM responses, by\nreferencing them to a predetermined ontology. It was shown that using a\nKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into\naccount relevant sub-graphs that preserve the information in a structured\nmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM\nframework, that integrates RAG with KG and a vector store (VS) that store\nfactual domain specific information. Importantly, to avoid hallucinations in\nthe KG, we build these highly domain-specific KGs and VSs without the use of\nLLMs, but via NLP, data mining, and nonnegative tensor factorization with\nautomatic model selection. Pairing our RAG with a domain-specific: (i) KG\n(containing structured information), and (ii) VS (containing unstructured\ninformation) enables the development of domain-specific chat-bots that\nattribute the source of information, mitigate hallucinations, lessen the need\nfor fine-tuning, and excel in highly domain-specific question answering tasks.\nWe pair SMART-SLIC with chain-of-thought prompting agents. The framework is\ndesigned to be generalizable to adapt to any specific or specialized domain. In\nthis paper, we demonstrate the question answering capabilities of our framework\non a corpus of scientific publications on malware analysis and anomaly\ndetection.", "published": "2024-10-03 17:40:55", "link": "http://arxiv.org/abs/2410.02721v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Markov Chains", "abstract": "Large language models (LLMs) are remarkably efficient across a wide range of\nnatural language processing tasks and well beyond them. However, a\ncomprehensive theoretical analysis of the LLMs' generalization capabilities\nremains elusive. In our paper, we approach this task by drawing an equivalence\nbetween autoregressive transformer-based language models and Markov chains\ndefined on a finite state space. This allows us to study the multi-step\ninference mechanism of LLMs from first principles. We relate the obtained\nresults to the pathological behavior observed with LLMs such as repetitions and\nincoherent replies with high temperature. Finally, we leverage the proposed\nformalization to derive pre-training and in-context learning generalization\nbounds for LLMs under realistic data and model assumptions. Experiments with\nthe most recent Llama and Gemma herds of models show that our theory correctly\ncaptures their behavior in practice.", "published": "2024-10-03 17:45:31", "link": "http://arxiv.org/abs/2410.02724v2", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better,\n  Even Mid-Generation", "abstract": "Inference-time computation is a powerful paradigm to enhance the performance\nof large language models (LLMs), with Best-of-N sampling being a widely used\ntechnique. However, this method is computationally expensive, requiring both\n(1) an external reward model and (2) the generation of multiple samples. In\nthis work, we introduce a new generative self-evaluation scheme designed to\nadaptively reduce the number of generated samples while maintaining or even\nimproving performance. We use a generative reward model formulation, allowing\nthe LLM to predict mid-generation the probability that restarting the\ngeneration will yield a better response. These predictions are obtained without\nan external reward model and can be used to decide whether or not to generate\nmore samples, prune unpromising samples early on, or to pick the best sample.\nThis capability is very inexpensive as it involves generating a single\npredefined token. Trained using a dataset constructed with real unfiltered\nLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval\nincreases from 21% to 34% with 16 samples and math performance on GSM8K\nimproves from 84% to 91%. By sampling only when the LLM determines that it is\nbeneficial to do so and adaptively adjusting temperature annealing, we\ndemonstrate that 74% of the improvement from using 16 samples can be achieved\nwith only 1.2 samples on average. We further demonstrate that 50-75% of samples\ncan be pruned early in generation with minimal degradation in performance.\nOverall, our methods enable more efficient and scalable compute utilization\nduring inference for LLMs.", "published": "2024-10-03 17:47:29", "link": "http://arxiv.org/abs/2410.02725v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified Multimodal Interleaved Document Representation for Retrieval", "abstract": "Information Retrieval (IR) methods aim to identify documents relevant to a\nquery, which have been widely applied in various natural language tasks.\nHowever, existing approaches typically consider only the textual content within\ndocuments, overlooking the fact that documents can contain multiple modalities,\nincluding images and tables. Also, they often segment each long document into\nmultiple discrete passages for embedding, which prevents them from capturing\nthe overall document context and interactions between paragraphs. To address\nthese two challenges, we propose a method that holistically embeds documents\ninterleaved with multiple modalities by leveraging the capability of recent\nvision-language models that enable the processing and integration of text,\nimages, and tables into a unified format and representation. Moreover, to\nmitigate the information loss from segmenting documents into passages, instead\nof representing and retrieving passages individually, we further merge the\nrepresentations of segmented passages into one single document representation,\nwhile we additionally introduce a reranking strategy to decouple and identify\nthe relevant passage within the document if necessary. Then, through extensive\nexperiments on diverse IR scenarios considering both the textual and multimodal\nqueries, we show that our approach substantially outperforms relevant\nbaselines, thanks to the consideration of the multimodal information within\ndocuments.", "published": "2024-10-03 17:49:09", "link": "http://arxiv.org/abs/2410.02729v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes\n  and Objects", "abstract": "Object navigation in unknown environments is crucial for deploying embodied\nagents in real-world applications. While we have witnessed huge progress due to\nlarge-scale scene datasets, faster simulators, and stronger models, previous\nstudies mainly focus on limited scene types and target objects. In this paper,\nwe study a new task of navigating to diverse target objects in a large number\nof scene types. To benchmark the problem, we present a large-scale scene\ndataset, DivScene, which contains 4,614 scenes across 81 different types. With\nthe dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a\nLarge Vision Language Model (LVLM) through imitation learning. The LVLM is\ntrained to take previous observations from the environment and generate the\nnext actions. We also introduce CoT explanation traces of the action prediction\nfor better performance when tuning LVLMs. Our extensive experiments find that\nwe can build a performant LVLM-based agent through imitation learning on the\nshortest paths constructed by a BFS planner without any human supervision. Our\nagent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we\ncarry out various analyses showing the generalization ability of our agent. Our\ncode and data are available at https://github.com/zhaowei-wang-nlp/DivScene.", "published": "2024-10-03 17:49:28", "link": "http://arxiv.org/abs/2410.02730v2", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization", "abstract": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems. We release our code\nat \\url{https://github.com/amazon-science/SigExt}", "published": "2024-10-03 17:54:56", "link": "http://arxiv.org/abs/2410.02741v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grounding Large Language Models In Embodied Environment With Imperfect\n  World Models", "abstract": "Despite a widespread success in various applications, large language models\n(LLMs) often stumble when tackling basic physical reasoning or executing\nrobotics tasks, due to a lack of direct experience with the physical nuances of\nthe real world. To address these issues, we propose a Grounding Large language\nmodel with Imperfect world MOdel (GLIMO), which utilizes proxy world models\nsuch as simulators to collect and synthesize trining data. GLIMO incorporates\nan LLM agent-based data generator to automatically create high-quality and\ndiverse instruction datasets. The generator includes an iterative self-refining\nmodule for temporally consistent experience sampling, a diverse set of\nquestion-answering instruction seeds, and a retrieval-augmented generation\nmodule for reflecting on prior experiences. Comprehensive experiments show that\nour approach improve the performance of strong open-source LLMs like LLaMA-3\nwith a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$\nacross three different benchmarks, respectively. The performance is able to\ncompete with or surpass their larger counterparts such as GPT-4.", "published": "2024-10-03 17:55:09", "link": "http://arxiv.org/abs/2410.02742v2", "categories": ["cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Neutral residues: revisiting adapters for model extension", "abstract": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen at training time, like adding a language for which\nthe original model has seen no or little training data. Popular solutions like\nfine-tuning or low-rank adaptation are successful at domain adaptation, but\nformally they do not add any extra capacity and degrade the performance in the\noriginal domain.\n  Our paper analyzes this extension problem under three angles: data,\narchitecture and training procedure, which are advantageously considered\njointly. In particular, we improve adapters and make it possible to learn an\nentire new language while ensuring that the output of the neural network is\nalmost unchanged in the original domain. For this purpose, we modify the new\nresidual blocks in a way that leads each new residual block to output\nnear-zeros in the original domain.\n  This solution of neutral residues, which borrows architectural components\nfrom mixture of experts, is effective: with only 20% extra learnable weights\ncompared to an original model trained on English, we get results that are\nsignificantly better than concurrent approaches (fine-tuning, low-rank or\nvanilla adapters) in terms of the trade-off between learning a new language and\nnot forgetting English.", "published": "2024-10-03 17:55:17", "link": "http://arxiv.org/abs/2410.02744v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation", "abstract": "Existing automatic prompt engineering methods are typically designed for\ndiscriminative tasks, where new task prompts are iteratively refined with\nlimited feedback from a single metric reflecting a single aspect. However,\nthese approaches are suboptimal for generative tasks, which require more\nnuanced guidance beyond a single numeric metric to improve the prompt and\noptimize multiple aspects of the generated text. To address these challenges,\nwe propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt\nOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion module\nas its core component. This module spontaneously discovers aspects, and\ncompares generated and reference texts across these aspects, providing specific\nsuggestions for prompt modification. These clear critiques and actionable\nsuggestions guide a receptive optimizer module to make more substantial\nchanges, exploring a broader and more effective search space. To further\nimprove CriSPO with multi-metric optimization, we introduce an Automatic Suffix\nTuning (AST) extension to enhance the performance of task prompts across\nmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4\nsummarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE score\nimprovement on summarization and substantial improvement of various metrics on\nQA. Code available at https://github.com/amazon-science/crispo", "published": "2024-10-03 17:57:01", "link": "http://arxiv.org/abs/2410.02748v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short\n  Videos", "abstract": "There has been growing sentiment recently that modern large multimodal models\n(LMMs) have addressed most of the key challenges related to short video\ncomprehension. As a result, both academia and industry are gradually shifting\ntheir attention towards the more complex challenges posed by understanding\nlong-form videos. However, is this really the case? Our studies indicate that\nLMMs still lack many fundamental reasoning capabilities even when dealing with\nshort videos. We introduce Vinoground, a temporal counterfactual LMM evaluation\nbenchmark encompassing 1000 short and natural video-caption pairs. We\ndemonstrate that existing LMMs severely struggle to distinguish temporal\ndifferences between different actions and object transformations. For example,\nthe best model GPT-4o only obtains ~50% on our text and video scores, showing a\nlarge gap compared to the human baseline of ~90%. All open-source multimodal\nmodels and CLIP-based models perform much worse, producing mostly random chance\nperformance. Through this work, we shed light onto the fact that temporal\nreasoning in short videos is a problem yet to be fully solved. The dataset and\nevaluation code are available at https://vinoground.github.io.", "published": "2024-10-03 17:59:58", "link": "http://arxiv.org/abs/2410.02763v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The Role of Deductive and Inductive Reasoning in Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning tasks, yet their reliance on static prompt structures and limited\nadaptability to complex scenarios remains a significant challenge. In this\npaper, we propose the Deductive and InDuctive(DID) method, a novel framework\nthat enhances LLM reasoning by dynamically integrating both deductive and\ninductive reasoning approaches. Drawing from cognitive science principles, DID\nimplements a dual-metric complexity evaluation system that combines Littlestone\ndimension and information entropy to precisely assess task difficulty and guide\ndecomposition strategies. DID enables the model to progressively adapt its\nreasoning pathways based on problem complexity, mirroring human cognitive\nprocesses. We evaluate DID's effectiveness across multiple benchmarks,\nincluding the AIW and MR-GSM8K, as well as our custom Holiday Puzzle dataset\nfor temporal reasoning. Our results demonstrate significant improvements in\nreasoning quality and solution accuracy - achieving 70.3% accuracy on AIW\n(compared to 62.2% for Tree of Thought) while maintaining lower computational\ncosts. The success of DID in improving LLM performance while preserving\ncomputational efficiency suggests promising directions for developing more\ncognitively aligned and capable language models. Our work contributes a\ntheoretically grounded, input-centric approach to enhancing LLM reasoning\ncapabilities, offering an efficient alternative to traditional\noutput-exploration methods.", "published": "2024-10-03 18:30:47", "link": "http://arxiv.org/abs/2410.02892v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Cognitive Biases in Large Language Models for News Recommendation", "abstract": "Despite large language models (LLMs) increasingly becoming important\ncomponents of news recommender systems, employing LLMs in such systems\nintroduces new risks, such as the influence of cognitive biases in LLMs.\nCognitive biases refer to systematic patterns of deviation from norms or\nrationality in the judgment process, which can result in inaccurate outputs\nfrom LLMs, thus threatening the reliability of news recommender systems.\nSpecifically, LLM-based news recommender systems affected by cognitive biases\ncould lead to the propagation of misinformation, reinforcement of stereotypes,\nand the formation of echo chambers. In this paper, we explore the potential\nimpact of multiple cognitive biases on LLM-based news recommender systems,\nincluding anchoring bias, framing bias, status quo bias and group attribution\nbias. Furthermore, to facilitate future research at improving the reliability\nof LLM-based news recommender systems, we discuss strategies to mitigate these\nbiases through data augmentation, prompt engineering and learning algorithms\naspects.", "published": "2024-10-03 18:42:07", "link": "http://arxiv.org/abs/2410.02897v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Fine-Tuning Language Models with Differential Privacy through Adaptive\n  Noise Allocation", "abstract": "Language models are capable of memorizing detailed patterns and information,\nleading to a double-edged effect: they achieve impressive modeling performance\non downstream tasks with the stored knowledge but also raise significant\nprivacy concerns. Traditional differential privacy based training approaches\noffer robust safeguards by employing a uniform noise distribution across all\nparameters. However, this overlooks the distinct sensitivities and\ncontributions of individual parameters in privacy protection and often results\nin suboptimal models. To address these limitations, we propose ANADP, a novel\nalgorithm that adaptively allocates additive noise based on the importance of\nmodel parameters. We demonstrate that ANADP narrows the performance gap between\nregular fine-tuning and traditional DP fine-tuning on a series of datasets\nwhile maintaining the required privacy constraints.", "published": "2024-10-03 19:02:50", "link": "http://arxiv.org/abs/2410.02912v1", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "LLMCO2: Advancing Accurate Carbon Footprint Prediction for LLM\n  Inferences", "abstract": "Throughout its lifecycle, a large language model (LLM) generates a\nsubstantially larger carbon footprint during inference than training. LLM\ninference requests vary in batch size, prompt length, and token generation\nnumber, while cloud providers employ different GPU types and quantities to meet\ndiverse service-level objectives for accuracy and latency. It is crucial for\nboth users and cloud providers to have a tool that quickly and accurately\nestimates the carbon impact of LLM inferences based on a combination of\ninference request and hardware configurations before execution. Estimating the\ncarbon footprint of LLM inferences is more complex than training due to lower\nand highly variable model FLOPS utilization, rendering previous equation-based\nmodels inaccurate. Additionally, existing machine learning (ML) prediction\nmethods either lack accuracy or demand extensive training data, as they\ninadequately handle the distinct prefill and decode phases, overlook\nhardware-specific features, and inefficiently sample uncommon inference\nconfigurations. We introduce \\coo, a graph neural network (GNN)-based model\nthat greatly improves the accuracy of LLM inference carbon footprint\npredictions compared to previous methods.", "published": "2024-10-03 19:48:45", "link": "http://arxiv.org/abs/2410.02950v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML", "abstract": "Automated machine learning (AutoML) accelerates AI development by automating\ntasks in the development pipeline, such as optimal model search and\nhyperparameter tuning. Existing AutoML systems often require technical\nexpertise to set up complex tools, which is in general time-consuming and\nrequires a large amount of human effort. Therefore, recent works have started\nexploiting large language models (LLM) to lessen such burden and increase the\nusability of AutoML frameworks via a natural language interface, allowing\nnon-expert users to build their data-driven solutions. These methods, however,\nare usually designed only for a particular process in the AI development\npipeline and do not efficiently use the inherent capacity of the LLMs. This\npaper proposes AutoML-Agent, a novel multi-agent framework tailored for\nfull-pipeline AutoML, i.e., from data retrieval to model deployment.\nAutoML-Agent takes user's task descriptions, facilitates collaboration between\nspecialized LLM agents, and delivers deployment-ready models. Unlike existing\nwork, instead of devising a single plan, we introduce a retrieval-augmented\nplanning strategy to enhance exploration to search for more optimal plans. We\nalso decompose each plan into sub-tasks (e.g., data preprocessing and neural\nnetwork design) each of which is solved by a specialized agent we build via\nprompting executing in parallel, making the search process more efficient.\nMoreover, we propose a multi-stage verification to verify executed results and\nguide the code generation LLM in implementing successful solutions. Extensive\nexperiments on seven downstream tasks using fourteen datasets show that\nAutoML-Agent achieves a higher success rate in automating the full AutoML\nprocess, yielding systems with good performance throughout the diverse domains.", "published": "2024-10-03 20:01:09", "link": "http://arxiv.org/abs/2410.02958v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "FastAdaSP: Multitask-Adapted Efficient Inference for Large Speech\n  Language Model", "abstract": "In this study, we aim to explore Multitask Speech Language Model (SpeechLM)\nefficient inference via token reduction. Unlike other modalities such as vision\nor text, speech has unique temporal dependencies, making previous efficient\ninference works on other modalities not directly applicable. Furthermore,\nmethods for efficient SpeechLM inference on long sequence and sparse signals\nremain largely unexplored. Then we propose FastAdaSP, a weighted token merging\nframework specifically designed for various speech-related tasks to improve the\ntrade-off between efficiency and performance. Experimental results on WavLLM\nand Qwen-Audio show that our method achieves the state-of-the-art (SOTA)\nefficiency-performance trade-off compared with other baseline methods.\nSpecifically, FastAdaSP achieved 7x memory efficiency and 1.83x decoding\nthroughput without any degradation on tasks like Emotion Recognition (ER) and\nSpoken Question Answering (SQA). The code will be available at\nhttps://github.com/yichen14/FastAdaSP", "published": "2024-10-03 21:33:07", "link": "http://arxiv.org/abs/2410.03007v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Disentangling Textual and Acoustic Features of Neural Speech\n  Representations", "abstract": "Neural speech models build deeply entangled internal representations, which\ncapture a variety of features (e.g., fundamental frequency, loudness, syntactic\ncategory, or semantic content of a word) in a distributed encoding. This\ncomplexity makes it difficult to track the extent to which such representations\nrely on textual and acoustic information, or to suppress the encoding of\nacoustic features that may pose privacy risks (e.g., gender or speaker\nidentity) in critical, real-world applications. In this paper, we build upon\nthe Information Bottleneck principle to propose a disentanglement framework\nthat separates complex speech representations into two distinct components: one\nencoding content (i.e., what can be transcribed as text) and the other encoding\nacoustic features relevant to a given downstream task. We apply and evaluate\nour framework to emotion recognition and speaker identification downstream\ntasks, quantifying the contribution of textual and acoustic features at each\nmodel layer. Additionally, we explore the application of our disentanglement\nframework as an attribution method to identify the most salient speech frame\nrepresentations from both the textual and acoustic perspectives.", "published": "2024-10-03 22:48:04", "link": "http://arxiv.org/abs/2410.03037v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Geometry is All You Need: A Unified Taxonomy of Matrix and Tensor\n  Factorization for Compression of Generative Language Models", "abstract": "Matrix and tensor-guided parametrization for Natural Language Processing\n(NLP) models is fundamentally useful for the improvement of the model's\nsystematic efficiency. However, the internal links between these two algebra\nstructures and language model parametrization are poorly understood. Also, the\nexisting matrix and tensor research is math-heavy and far away from machine\nlearning (ML) and NLP research concepts. These two issues result in the recent\nprogress on matrices and tensors for model parametrization being more like a\nloose collection of separate components from matrix/tensor and NLP studies,\nrather than a well-structured unified approach, further hindering algorithm\ndesign. To this end, we propose a unified taxonomy, which bridges the\nmatrix/tensor compression approaches and model compression concepts in ML and\nNLP research. Namely, we adopt an elementary concept in linear algebra, that of\na subspace, which is also the core concept in geometric algebra, to reformulate\nthe matrix/tensor and ML/NLP concepts (e.g. attention mechanism) under one\numbrella. In this way, based on our subspace formalization, typical matrix and\ntensor decomposition algorithms can be interpreted as geometric\ntransformations. Finally, we revisit recent literature on matrix- or\ntensor-guided language model compression, rephrase and compare their core\nideas, and then point out the current research gap and potential solutions.", "published": "2024-10-03 23:12:20", "link": "http://arxiv.org/abs/2410.03040v1", "categories": ["cs.CL", "cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.CL"}
{"title": "CalliffusionV2: Personalized Natural Calligraphy Generation with\n  Flexible Multi-modal Control", "abstract": "In this paper, we introduce CalliffusionV2, a novel system designed to\nproduce natural Chinese calligraphy with flexible multi-modal control. Unlike\nprevious approaches that rely solely on image or text inputs and lack\nfine-grained control, our system leverages both images to guide generations at\nfine-grained levels and natural language texts to describe the features of\ngenerations. CalliffusionV2 excels at creating a broad range of characters and\ncan quickly learn new styles through a few-shot learning approach. It is also\ncapable of generating non-Chinese characters without prior training.\nComprehensive tests confirm that our system produces calligraphy that is both\nstylistically accurate and recognizable by neural network classifiers and human\nevaluators.", "published": "2024-10-03 20:26:54", "link": "http://arxiv.org/abs/2410.03787v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG):\n  Evolution, Current Landscape and Future Directions", "abstract": "This paper presents a comprehensive study of Retrieval-Augmented Generation\n(RAG), tracing its evolution from foundational concepts to the current state of\nthe art. RAG combines retrieval mechanisms with generative language models to\nenhance the accuracy of outputs, addressing key limitations of LLMs. The study\nexplores the basic architecture of RAG, focusing on how retrieval and\ngeneration are integrated to handle knowledge-intensive tasks. A detailed\nreview of the significant technological advancements in RAG is provided,\nincluding key innovations in retrieval-augmented language models and\napplications across various domains such as question-answering, summarization,\nand knowledge-based tasks. Recent research breakthroughs are discussed,\nhighlighting novel methods for improving retrieval efficiency. Furthermore, the\npaper examines ongoing challenges such as scalability, bias, and ethical\nconcerns in deployment. Future research directions are proposed, focusing on\nimproving the robustness of RAG models, expanding the scope of application of\nRAG models, and addressing societal implications. This survey aims to serve as\na foundational resource for researchers and practitioners in understanding the\npotential of RAG and its trajectory in natural language processing.", "published": "2024-10-03 22:29:47", "link": "http://arxiv.org/abs/2410.12837v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "RepoGraph: Enhancing AI Software Engineering with Repository-level Code\n  Graph", "abstract": "Large Language Models (LLMs) excel in code generation yet struggle with\nmodern AI software engineering tasks. Unlike traditional function-level or\nfile-level coding tasks, AI software engineering requires not only basic coding\nproficiency but also advanced skills in managing and interacting with code\nrepositories. However, existing methods often overlook the need for\nrepository-level code understanding, which is crucial for accurately grasping\nthe broader context and developing effective solutions. On this basis, we\npresent RepoGraph, a plug-in module that manages a repository-level structure\nfor modern AI software engineering solutions. RepoGraph offers the desired\nguidance and serves as a repository-wide navigation for AI software engineers.\nWe evaluate RepoGraph on the SWE-bench by plugging it into four different\nmethods of two lines of approaches, where RepoGraph substantially boosts the\nperformance of all systems, leading to a new state-of-the-art among open-source\nframeworks. Our analyses also demonstrate the extensibility and flexibility of\nRepoGraph by testing on another repo-level coding benchmark, CrossCodeEval. Our\ncode is available at https://github.com/ozyyshr/RepoGraph.", "published": "2024-10-03 05:45:26", "link": "http://arxiv.org/abs/2410.14684v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "BrainTransformers: SNN-LLM", "abstract": "This study introduces BrainTransformers, an innovative Large Language Model\n(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions\ninclude: (1) designing SNN-compatible Transformer components such as SNNMatmul,\nSNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU\nactivation function; and (3) developing a Synapsis module to simulate synaptic\nplasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,\ndemonstrates competitive performance across various benchmarks, including MMLU\n(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering\nimproved energy efficiency and biological plausibility. The model employs a\nthree-stage training approach, including SNN-specific neuronal synaptic\nplasticity training. This research opens new avenues for brain-like AI systems\nin natural language processing and neuromorphic computing. Future work will\nfocus on hardware optimization, developing specialized SNN fine-tuning tools,\nand exploring practical applications in energy-efficient computing\nenvironments.", "published": "2024-10-03 14:17:43", "link": "http://arxiv.org/abs/2410.14687v2", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "State-of-the-art Embeddings with Video-free Segmentation of the Source\n  VoxCeleb Data", "abstract": "In this paper, we refine and validate our method for training speaker\nembedding extractors using weak annotations. More specifically, we use only the\naudio stream of the source VoxCeleb videos and the names of the celebrities\nwithout knowing the time intervals in which they appear in the recording. We\nexperiment with hyperparameters and embedding extractors based on ResNet and\nWavLM. We show that the method achieves state-of-the-art results in speaker\nverification, comparable with training the extractors in a standard supervised\nway on the VoxCeleb dataset. We also extend it by considering segments\nbelonging to unknown speakers appearing alongside the celebrities, which are\ntypically being discarded. Overall, our approach can be used for directly\ntraining state-of-the-art embedding extractors or as an alternative to the\nVoxCeleb-like pipeline for dataset creation without needing image modality.", "published": "2024-10-03 10:23:39", "link": "http://arxiv.org/abs/2410.02364v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "NTU-NPU System for Voice Privacy 2024 Challenge", "abstract": "In this work, we describe our submissions for the Voice Privacy Challenge\n2024. Rather than proposing a novel speech anonymization system, we enhance the\nprovided baselines to meet all required conditions and improve evaluated\nmetrics. Specifically, we implement emotion embedding and experiment with WavLM\nand ECAPA2 speaker embedders for the B3 baseline. Additionally, we compare\ndifferent speaker and prosody anonymization techniques. Furthermore, we\nintroduce Mean Reversion F0 for B5, which helps to enhance privacy without a\nloss in utility. Finally, we explore disentanglement models, namely $\\beta$-VAE\nand NaturalSpeech3 FACodec.", "published": "2024-10-03 10:45:10", "link": "http://arxiv.org/abs/2410.02371v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers\n  for Open-Domain Sound Generation", "abstract": "We introduce MDSGen, a novel framework for vision-guided open-domain sound\ngeneration optimized for model parameter size, memory consumption, and\ninference speed. This framework incorporates two key innovations: (1) a\nredundant video feature removal module that filters out unnecessary visual\ninformation, and (2) a temporal-aware masking strategy that leverages temporal\ncontext for enhanced audio generation accuracy. In contrast to existing\nresource-heavy Unet-based models, \\texttt{MDSGen} employs denoising masked\ndiffusion transformers, facilitating efficient generation without reliance on\npre-trained diffusion models. Evaluated on the benchmark VGGSound dataset, our\nsmallest model (5M parameters) achieves $97.9$% alignment accuracy, using\n$172\\times$ fewer parameters, $371$% less memory, and offering $36\\times$\nfaster inference than the current 860M-parameter state-of-the-art model\n($93.9$% accuracy). The larger model (131M parameters) reaches nearly $99$%\naccuracy while requiring $6.5\\times$ fewer parameters. These results highlight\nthe scalability and effectiveness of our approach. The code is available at\nhttps://bit.ly/mdsgen.", "published": "2024-10-03 01:23:44", "link": "http://arxiv.org/abs/2410.02130v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundMorpher: Perceptually-Uniform Sound Morphing with Diffusion Model", "abstract": "We present SoundMorpher, an open-world sound morphing method designed to\ngenerate perceptually uniform morphing trajectories. Traditional sound morphing\ntechniques typically assume a linear relationship between the morphing factor\nand sound perception, achieving smooth transitions by linearly interpolating\nthe semantic features of source and target sounds while gradually adjusting the\nmorphing factor. However, these methods oversimplify the complexities of sound\nperception, resulting in limitations in morphing quality. In contrast,\nSoundMorpher explores an explicit relationship between the morphing factor and\nthe perception of morphed sounds, leveraging log Mel-spectrogram features. This\napproach further refines the morphing sequence by ensuring a constant target\nperceptual difference for each transition and determining the corresponding\nmorphing factors using binary search. To address the lack of a formal\nquantitative evaluation framework for sound morphing, we propose a set of\nmetrics based on three established objective criteria. These metrics enable\ncomprehensive assessment of morphed results and facilitate direct comparisons\nbetween methods, fostering advancements in sound morphing research. Extensive\nexperiments demonstrate the effectiveness and versatility of SoundMorpher in\nreal-world scenarios, showcasing its potential in applications such as creative\nmusic composition, film post-production, and interactive audio technologies.\nOur demonstration and codes are available\nat~\\url{https://xinleiniu.github.io/SoundMorpher-demo/}.", "published": "2024-10-03 02:07:59", "link": "http://arxiv.org/abs/2410.02144v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CoLLAP: Contrastive Long-form Language-Audio Pretraining with Musical\n  Temporal Structure Augmentation", "abstract": "Modeling temporal characteristics plays a significant role in the\nrepresentation learning of audio waveform. We propose Contrastive Long-form\nLanguage-Audio Pretraining (\\textbf{CoLLAP}) to significantly extend the\nperception window for both the input audio (up to 5 minutes) and the language\ndescriptions (exceeding 250 words), while enabling contrastive learning across\nmodalities and temporal dynamics. Leveraging recent Music-LLMs to generate\nlong-form music captions for full-length songs, augmented with musical temporal\nstructures, we collect 51.3K audio-text pairs derived from the large-scale\nAudioSet training dataset, where the average audio length reaches 288 seconds.\nWe propose a novel contrastive learning architecture that fuses language\nrepresentations with structured audio representations by segmenting each song\ninto clips and extracting their embeddings. With an attention mechanism, we\ncapture multimodal temporal correlations, allowing the model to automatically\nweigh and enhance the final fusion score for improved contrastive alignment.\nFinally, we develop two variants of the CoLLAP model with different types of\nbackbone language models. Through comprehensive experiments on multiple\nlong-form music-text retrieval datasets, we demonstrate consistent performance\nimprovement in retrieval accuracy compared with baselines. We also show the\npretrained CoLLAP models can be transferred to various music information\nretrieval tasks, with heterogeneous long-form multimodal contexts.", "published": "2024-10-03 07:46:51", "link": "http://arxiv.org/abs/2410.02271v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "People are poorly equipped to detect AI-powered voice clones", "abstract": "As generative artificial intelligence (AI) continues its ballistic\ntrajectory, everything from text to audio, image, and video generation\ncontinues to improve at mimicking human-generated content. Through a series of\nperceptual studies, we report on the realism of AI-generated voices in terms of\nidentity matching and naturalness. We find human participants cannot\nconsistently identify recordings of AI-generated voices. Specifically,\nparticipants perceived the identity of an AI-voice to be the same as its real\ncounterpart approximately 80% of the time, and correctly identified a voice as\nAI generated only about 60% of the time.", "published": "2024-10-03 21:26:58", "link": "http://arxiv.org/abs/2410.03791v2", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
