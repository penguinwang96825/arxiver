{"title": "Self-imitating Feedback Generation Using GAN for Computer-Assisted\n  Pronunciation Training", "abstract": "Self-imitating feedback is an effective and learner-friendly method for\nnon-native learners in Computer-Assisted Pronunciation Training. Acoustic\ncharacteristics in native utterances are extracted and transplanted onto\nlearner's own speech input, and given back to the learner as a corrective\nfeedback. Previous works focused on speech conversion using prosodic\ntransplantation techniques based on PSOLA algorithm. Motivated by the visual\ndifferences found in spectrograms of native and non-native speeches, we\ninvestigated applying GAN to generate self-imitating feedback by utilizing\ngenerator's ability through adversarial training. Because this mapping is\nhighly under-constrained, we also adopt cycle consistency loss to encourage the\noutput to preserve the global structure, which is shared by native and\nnon-native utterances. Trained on 97,200 spectrogram images of short utterances\nproduced by native and non-native speakers of Korean, the generator is able to\nsuccessfully transform the non-native spectrogram input to a spectrogram with\nproperties of self-imitating feedback. Furthermore, the transformed spectrogram\nshows segmental corrections that cannot be obtained by prosodic\ntransplantation. Perceptual test comparing the self-imitating and correcting\nabilities of our method with the baseline PSOLA method shows that the\ngenerative approach with cycle consistency loss is promising.", "published": "2019-04-20 06:21:52", "link": "http://arxiv.org/abs/1904.09407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized sentence generation using generative adversarial networks\n  with author-specific word usage", "abstract": "The author-specific word usage is a vital feature to let readers perceive the\nwriting style of the author. In this work, a personalized sentence generation\nmethod based on generative adversarial networks (GANs) is proposed to cope with\nthis issue. The frequently used function word and content word are incorporated\nnot only as the input features but also as the sentence structure constraint\nfor the GAN training. For the sentence generation with the related topics\ndecided by the user, the Named Entity Recognition (NER) information of the\ninput words is also used in the network training. We compared the proposed\nmethod with the GAN-based sentence generation methods, and the experimental\nresults showed that the generated sentences using our method are more similar\nto the original sentences of the same author based on the objective evaluation\nsuch as BLEU and SimHash score.", "published": "2019-04-20 12:56:39", "link": "http://arxiv.org/abs/1904.09442v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual\n  Word Embeddings", "abstract": "Distributed representations of words which map each word to a continuous\nvector have proven useful in capturing important linguistic information not\nonly in a single language but also across different languages. Current\nunsupervised adversarial approaches show that it is possible to build a mapping\nmatrix that align two sets of monolingual word embeddings together without high\nquality parallel data such as a dictionary or a sentence-aligned corpus.\nHowever, without post refinement, the performance of these methods' preliminary\nmapping is not good, leading to poor performance for typologically distant\nlanguages.\n  In this paper, we propose a weakly-supervised adversarial training method to\novercome this limitation, based on the intuition that mapping across languages\nis better done at the concept level than at the word level. We propose a\nconcept-based adversarial training method which for most languages improves the\nperformance of previous unsupervised adversarial methods, especially for\ntypologically distant language pairs.", "published": "2019-04-20 13:19:24", "link": "http://arxiv.org/abs/1904.09446v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for\n  Natural Language Understanding", "abstract": "This paper explores the use of knowledge distillation to improve a Multi-Task\nDeep Neural Network (MT-DNN) (Liu et al., 2019) for learning text\nrepresentations across multiple natural language understanding tasks. Although\nensemble learning can improve model performance, serving an ensemble of large\nDNNs such as MT-DNN can be prohibitively expensive. Here we apply the knowledge\ndistillation method (Hinton et al., 2015) in the multi-task learning setting.\nFor each task, we train an ensemble of different MT-DNNs (teacher) that\noutperforms any single model, and then train a single MT-DNN (student) via\nmulti-task learning to \\emph{distill} knowledge from these ensemble teachers.\nWe show that the distilled MT-DNN significantly outperforms the original MT-DNN\non 7 out of 9 GLUE tasks, pushing the GLUE benchmark (single model) to 83.7\\%\n(1.5\\% absolute improvement\\footnote{ Based on the GLUE leaderboard at\nhttps://gluebenchmark.com/leaderboard as of April 1, 2019.}). The code and\npre-trained models will be made publicly available at\nhttps://github.com/namisan/mt-dnn.", "published": "2019-04-20 19:11:00", "link": "http://arxiv.org/abs/1904.09482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Brazilian court decisions", "abstract": "Predicting case outcomes is useful but still an extremely hard task for\nattorneys and other Law professionals. It is not easy to search case\ninformation to extract valuable information as this requires dealing with huge\ndata sets and their complexity. For instance, the complexity of Brazil legal\nsystem along with the high litigation rates makes this problem even harder.\nThis paper introduces an approach for predicting Brazilian court decisions\nwhich is also able to predict whether the decision will be unanimous. We\ndeveloped a working prototype which performs 79% of accuracy (F1-score) on a\ndata set composed of 4,043 cases from a Brazilian court. To our knowledge, this\nis the first study to forecast judge decisions in Brazil.", "published": "2019-04-20 15:55:13", "link": "http://arxiv.org/abs/1905.10348v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "An Unsupervised Joint System for Text Generation from Knowledge Graphs\n  and Semantic Parsing", "abstract": "Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore\nsupervised approaches to both graph-to-text generation and text-to-graph\nknowledge extraction (semantic parsing) will always suffer from a shortage of\ndomain-specific parallel graph-text data; at the same time, adapting a model\ntrained on a different domain is often impossible due to little or no overlap\nin entities and relations. This situation calls for an approach that (1) does\nnot need large amounts of annotated data and thus (2) does not need to rely on\ndomain adaptation techniques to work well in different domains. To this end, we\npresent the first approach to unsupervised text generation from KGs and show\nsimultaneously how it can be used for unsupervised semantic parsing. We\nevaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene\ngraphs from Visual Genome. Our system outperforms strong baselines for both\ntext$\\leftrightarrow$graph conversion tasks without any manual adaptation from\none dataset to the other. In additional experiments, we investigate the impact\nof using different unsupervised objectives.", "published": "2019-04-20 13:46:36", "link": "http://arxiv.org/abs/1904.09447v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Energy-based Self-attentive Learning of Abstractive Communities for\n  Spoken Language Understanding", "abstract": "Abstractive community detection is an important spoken language understanding\ntask, whose goal is to group utterances in a conversation according to whether\nthey can be jointly summarized by a common abstractive sentence. This paper\nprovides a novel approach to this task. We first introduce a neural contextual\nutterance encoder featuring three types of self-attention mechanisms. We then\ntrain it using the siamese and triplet energy-based meta-architectures.\nExperiments on the AMI corpus show that our system outperforms multiple\nenergy-based and non-energy based baselines from the state-of-the-art. Code and\ndata are publicly available.", "published": "2019-04-20 20:01:52", "link": "http://arxiv.org/abs/1904.09491v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Repurposing Entailment for Multi-Hop Question Answering Tasks", "abstract": "Question Answering (QA) naturally reduces to an entailment problem, namely,\nverifying whether some text entails the answer to a question. However, for\nmulti-hop QA tasks, which require reasoning with multiple sentences, it remains\nunclear how best to utilize entailment models pre-trained on large scale\ndatasets such as SNLI, which are based on sentence pairs. We introduce Multee,\na general architecture that can effectively use entailment models for multi-hop\nQA tasks. Multee uses (i) a local module that helps locate important sentences,\nthereby avoiding distracting information, and (ii) a global module that\naggregates information by effectively incorporating importance weights.\nImportantly, we show that both modules can use entailment functions pre-trained\non a large scale NLI datasets. We evaluate performance on MultiRC and\nOpenBookQA, two multihop QA datasets. When using an entailment function\npre-trained on NLI datasets, Multee outperforms QA models trained only on the\ntarget QA datasets and the OpenAI transformer models. The code is available at\nhttps://github.com/StonyBrookNLP/multee.", "published": "2019-04-20 00:30:26", "link": "http://arxiv.org/abs/1904.09380v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models with Transformers", "abstract": "The Transformer architecture is superior to RNN-based models in computational\nefficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer\nmodels on various NLP tasks using pre-trained language models on large-scale\ncorpora. Surprisingly, these Transformer architectures are suboptimal for\nlanguage model itself. Neither self-attention nor the positional encoding in\nthe Transformer is able to efficiently incorporate the word-level sequential\ncontext crucial to language modeling.\n  In this paper, we explore effective Transformer architectures for language\nmodel, including adding additional LSTM layers to better capture the sequential\ncontext while still keeping the computation efficient. We propose Coordinate\nArchitecture Search (CAS) to find an effective architecture through iterative\nrefinement of the model. Experimental results on the PTB, WikiText-2, and\nWikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all\nproblems, i.e. on average an improvement of 12.0 perplexity units compared to\nstate-of-the-art LSTMs. The source code is publicly available.", "published": "2019-04-20 06:43:14", "link": "http://arxiv.org/abs/1904.09408v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
