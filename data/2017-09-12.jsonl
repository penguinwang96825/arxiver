{"title": "Capturing Long-range Contextual Dependencies with Memory-enhanced\n  Conditional Random Fields", "abstract": "Despite successful applications across a broad range of NLP tasks,\nconditional random fields (\"CRFs\"), in particular the linear-chain variant, are\nonly able to model local features. While this has important benefits in terms\nof inference tractability, it limits the ability of the model to capture\nlong-range dependencies between items. Attempts to extend CRFs to capture\nlong-range dependencies have largely come at the cost of computational\ncomplexity and approximate inference. In this work, we propose an extension to\nCRFs by integrating external memory, taking inspiration from memory networks,\nthereby allowing CRFs to incorporate information far beyond neighbouring steps.\nExperiments across two tasks show substantial improvements over strong CRF and\nLSTM baselines.", "published": "2017-09-12 00:57:04", "link": "http://arxiv.org/abs/1709.03637v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small-footprint Keyword Spotting Using Deep Neural Network and\n  Connectionist Temporal Classifier", "abstract": "Mainly for the sake of solving the lack of keyword-specific data, we propose\none Keyword Spotting (KWS) system using Deep Neural Network (DNN) and\nConnectionist Temporal Classifier (CTC) on power-constrained small-footprint\nmobile devices, taking full advantage of general corpus from continuous speech\nrecognition which is of great amount. DNN is to directly predict the posterior\nof phoneme units of any personally customized key-phrase, and CTC to produce a\nconfidence score of the given phoneme sequence as responsive decision-making\nmechanism. The CTC-KWS has competitive performance in comparison with purely\nDNN based keyword specific KWS, but not increasing any computational\ncomplexity.", "published": "2017-09-12 02:52:54", "link": "http://arxiv.org/abs/1709.03665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Word Segmentation and Morpheme Segmentation as Sequence\n  Labelling", "abstract": "This paper presents our segmentation system developed for the MLP 2017 shared\ntasks on cross-lingual word segmentation and morpheme segmentation. We model\nboth word and morpheme segmentation as character-level sequence labelling\ntasks. The prevalent bidirectional recurrent neural network with conditional\nrandom fields as the output interface is adapted as the baseline system, which\nis further improved via ensemble decoding. Our universal system is applied to\nand extensively evaluated on all the official data sets without any\nlanguage-specific adjustment. The official evaluation results indicate that the\nproposed model achieves outstanding accuracies both for word and morpheme\nsegmentation on all the languages in various types when compared to the other\nparticipating systems.", "published": "2017-09-12 09:23:55", "link": "http://arxiv.org/abs/1709.03756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models of Spoken Dutch", "abstract": "In Flanders, all TV shows are subtitled. However, the process of subtitling\nis a very time-consuming one and can be sped up by providing the output of a\nspeech recognizer run on the audio of the TV show, prior to the subtitling.\nNaturally, this speech recognition will perform much better if the employed\nlanguage model is adapted to the register and the topic of the program. We\npresent several language models trained on subtitles of television shows\nprovided by the Flemish public-service broadcaster VRT. This data was gathered\nin the context of the project STON which has as purpose to facilitate the\nprocess of subtitling TV shows. One model is trained on all available data (46M\nword tokens), but we also trained models on a specific type of TV show or\ndomain/topic. Language models of spoken language are quite rare due to the lack\nof training data. The size of this corpus is relatively large for a corpus of\nspoken language (compare with e.g. CGN which has 9M words), but still rather\nsmall for a language model. Thus, in practice it is advised to interpolate\nthese models with a large background language model trained on written\nlanguage. The models can be freely downloaded on\nhttp://www.esat.kuleuven.be/psi/spraak/downloads/.", "published": "2017-09-12 09:27:12", "link": "http://arxiv.org/abs/1709.03759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SYSTRAN Purely Neural MT Engines for WMT2017", "abstract": "This paper describes SYSTRAN's systems submitted to the WMT 2017 shared news\ntranslation task for English-German, in both translation directions. Our\nsystems are built using OpenNMT, an open-source neural machine translation\nsystem, implementing sequence-to-sequence models with LSTM encoder/decoders and\nattention. We experimented using monolingual data automatically\nback-translated. Our resulting models are further hyper-specialised with an\nadaptation technique that finely tunes models according to the evaluation test\nsentences.", "published": "2017-09-12 12:47:05", "link": "http://arxiv.org/abs/1709.03814v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenNMT: Open-source Toolkit for Neural Machine Translation", "abstract": "We introduce an open-source toolkit for neural machine translation (NMT) to\nsupport research into model architectures, feature representations, and source\nmodalities, while maintaining competitive performance, modularity and\nreasonable training requirements.", "published": "2017-09-12 12:58:07", "link": "http://arxiv.org/abs/1709.03815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StarSpace: Embed All The Things!", "abstract": "We present StarSpace, a general-purpose neural embedding model that can solve\na wide variety of problems: labeling tasks such as text classification, ranking\ntasks such as information retrieval/web search, collaborative filtering-based\nor content-based recommendation, embedding of multi-relational graphs, and\nlearning word, sentence or document level embeddings. In each case the model\nworks by embedding those entities comprised of discrete features and comparing\nthem against each other -- learning similarities dependent on the task.\nEmpirical results on a number of tasks show that StarSpace is highly\ncompetitive with existing methods, whilst also being generally applicable to\nnew cases where those methods are not.", "published": "2017-09-12 14:16:56", "link": "http://arxiv.org/abs/1709.03856v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Associations Help to Detect Conventionalized Multiword Expressions", "abstract": "In this paper we show that if we want to obtain human evidence about\nconventionalization of some phrases, we should ask native speakers about\nassociations they have to a given phrase and its component words. We have shown\nthat if component words of a phrase have each other as frequent associations,\nthen this phrase can be considered as conventionalized. Another type of\nconventionalized phrases can be revealed using two factors: low entropy of\nphrase associations and low intersection of component word and phrase\nassociations. The association experiments were performed for the Russian\nlanguage.", "published": "2017-09-12 15:57:22", "link": "http://arxiv.org/abs/1709.03925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hash Embeddings for Efficient Word Representations", "abstract": "We present hash embeddings, an efficient method for representing words in a\ncontinuous vector form. A hash embedding may be seen as an interpolation\nbetween a standard word embedding and a word embedding created using a random\nhash function (the hashing trick). In hash embeddings each token is represented\nby $k$ $d$-dimensional embeddings vectors and one $k$ dimensional weight\nvector. The final $d$ dimensional representation of the token is the product of\nthe two. Rather than fitting the embedding vectors for each token these are\nselected by the hashing trick from a shared pool of $B$ embedding vectors. Our\nexperiments show that hash embeddings can easily deal with huge vocabularies\nconsisting of millions of tokens. When using a hash embedding there is no need\nto create a dictionary before training nor to perform any kind of vocabulary\npruning after training. We show that models trained using hash embeddings\nexhibit at least the same level of performance as models trained using regular\nembeddings across a wide range of tasks. Furthermore, the number of parameters\nneeded by such an embedding is only a fraction of what is required by a regular\nembedding. Since standard embeddings and embeddings constructed using the\nhashing trick are actually just special cases of a hash embedding, hash\nembeddings can be considered an extension and improvement over the existing\nregular embedding types.", "published": "2017-09-12 16:13:10", "link": "http://arxiv.org/abs/1709.03933v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressee and Response Selection in Multi-Party Conversations with\n  Speaker Interaction RNNs", "abstract": "In this paper, we study the problem of addressee and response selection in\nmulti-party conversations. Understanding multi-party conversations is\nchallenging because of complex speaker interactions: multiple speakers exchange\nmessages with each other, playing different roles (sender, addressee,\nobserver), and these roles vary across turns. To tackle this challenge, we\npropose the Speaker Interaction Recurrent Neural Network (SI-RNN). Whereas the\nprevious state-of-the-art system updated speaker embeddings only for the\nsender, SI-RNN uses a novel dialog encoder to update speaker embeddings in a\nrole-sensitive way. Additionally, unlike the previous work that selected the\naddressee and response separately, SI-RNN selects them jointly by viewing the\ntask as a sequence prediction problem. Experimental results show that SI-RNN\nsignificantly improves the accuracy of addressee and response selection,\nparticularly in complex conversations with many speakers and responses to\ndistant messages many turns in the past.", "published": "2017-09-12 18:19:51", "link": "http://arxiv.org/abs/1709.04005v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependencies: Formalising Semantic Catenae for Information Retrieval", "abstract": "Building machines that can understand text like humans is an AI-complete\nproblem. A great deal of research has already gone into this, with astounding\nresults, allowing everyday people to discuss with their telephones, or have\ntheir reading materials analysed and classified by computers. A prerequisite\nfor processing text semantics, common to the above examples, is having some\ncomputational representation of text as an abstract object. Operations on this\nrepresentation practically correspond to making semantic inferences, and by\nextension simulating understanding text. The complexity and granularity of\nsemantic processing that can be realised is constrained by the mathematical and\ncomputational robustness, expressiveness, and rigour of the tools used.\n  This dissertation contributes a series of such tools, diverse in their\nmathematical formulation, but common in their application to model semantic\ninferences when machines process text. These tools are principally expressed in\nnine distinct models that capture aspects of semantic dependence in highly\ninterpretable and non-complex ways. This dissertation further reflects on\npresent and future problems with the current research paradigm in this area,\nand makes recommendations on how to overcome them.\n  The amalgamation of the body of work presented in this dissertation advances\nthe complexity and granularity of semantic inferences that can be made\nautomatically by machines.", "published": "2017-09-12 08:54:02", "link": "http://arxiv.org/abs/1709.03742v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Refining Source Representations with Relation Networks for Neural\n  Machine Translation", "abstract": "Although neural machine translation (NMT) with the encoder-decoder framework\nhas achieved great success in recent times, it still suffers from some\ndrawbacks: RNNs tend to forget old information which is often useful and the\nencoder only operates through words without considering word relationship. To\nsolve these problems, we introduce a relation networks (RN) into NMT to refine\nthe encoding representations of the source. In our method, the RN first\naugments the representation of each source word with its neighbors and reasons\nall the possible pairwise relations between them. Then the source\nrepresentations and all the relations are fed to the attention module and the\ndecoder together, keeping the main encoder-decoder architecture unchanged.\nExperiments on two Chinese-to-English data sets in different scales both show\nthat our method can outperform the competitive baselines significantly.", "published": "2017-09-12 13:38:11", "link": "http://arxiv.org/abs/1709.03980v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variational Reasoning for Question Answering with Knowledge Graph", "abstract": "Knowledge graph (KG) is known to be helpful for the task of question\nanswering (QA), since it provides well-structured relational information\nbetween entities, and allows one to further infer indirect facts. However, it\nis challenging to build QA systems which can learn to reason over knowledge\ngraphs based on question-answer pairs alone. First, when people ask questions,\ntheir expressions are noisy (for example, typos in texts, or variations in\npronunciations), which is non-trivial for the QA system to match those\nmentioned entities to the knowledge graph. Second, many questions require\nmulti-hop logic reasoning over the knowledge graph to retrieve the answers. To\naddress these challenges, we propose a novel and unified deep learning\narchitecture, and an end-to-end variational learning algorithm which can handle\nnoise in questions, and learn multi-hop reasoning simultaneously. Our method\nachieves state-of-the-art performance on a recent benchmark dataset in the\nliterature. We also derive a series of new benchmark datasets, including\nquestions for multi-hop reasoning, questions paraphrased by neural translation\nmodel, and questions in human voice. Our method yields very promising results\non all these challenging datasets.", "published": "2017-09-12 22:27:34", "link": "http://arxiv.org/abs/1709.04071v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Affective Neural Response Generation", "abstract": "Existing neural conversational models process natural language primarily on a\nlexico-syntactic level, thereby ignoring one of the most crucial components of\nhuman-to-human dialogue: its affective content. We take a step in this\ndirection by proposing three novel ways to incorporate affective/emotional\naspects into long short term memory (LSTM) encoder-decoder neural conversation\nmodels: (1) affective word embeddings, which are cognitively engineered, (2)\naffect-based objective functions that augment the standard cross-entropy loss,\nand (3) affectively diverse beam search for decoding. Experiments show that\nthese techniques improve the open-domain conversational prowess of\nencoder-decoder networks by enabling them to produce emotionally rich responses\nthat are more interesting and natural.", "published": "2017-09-12 17:41:30", "link": "http://arxiv.org/abs/1709.03968v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
