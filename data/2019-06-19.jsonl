{"title": "Surf at MEDIQA 2019: Improving Performance of Natural Language Inference\n  in the Clinical Domain by Adopting Pre-trained Language Model", "abstract": "While deep learning techniques have shown promising results in many natural\nlanguage processing (NLP) tasks, it has not been widely applied to the clinical\ndomain. The lack of large datasets and the pervasive use of domain-specific\nlanguage (i.e. abbreviations and acronyms) in the clinical domain causes slower\nprogress in NLP tasks than that of the general NLP tasks. To fill this gap, we\nemploy word/subword-level based models that adopt large-scale data-driven\nmethods such as pre-trained language models and transfer learning in analyzing\ntext for the clinical domain. Empirical results demonstrate the superiority of\nthe proposed methods by achieving 90.6% accuracy in medical domain natural\nlanguage inference task. Furthermore, we inspect the independent strengths of\nthe proposed approaches in quantitative and qualitative manners. This analysis\nwill help researchers to select necessary components in building models for the\nmedical domain.", "published": "2019-06-19 00:13:04", "link": "http://arxiv.org/abs/1906.07854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Multi-Domain Adaptation Approaches for Neural Machine\n  Translation", "abstract": "In this paper, we propose two novel methods for domain adaptation for the\nattention-only neural machine translation (NMT) model, i.e., the Transformer.\nOur methods focus on training a single translation model for multiple domains\nby either learning domain specialized hidden state representations or predictor\nbiases for each domain. We combine our methods with a previously proposed\nblack-box method called mixed fine tuning, which is known to be highly\neffective for domain adaptation. In addition, we incorporate multilingualism\ninto the domain adaptation framework. Experiments show that multilingual\nmulti-domain adaptation can significantly improve both resource-poor in-domain\nand resource-rich out-of-domain translations, and the combination of our\nmethods with mixed fine tuning achieves the best performance.", "published": "2019-06-19 08:56:02", "link": "http://arxiv.org/abs/1906.07978v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effect of Translationese in Machine Translation Test Sets", "abstract": "The effect of translationese has been studied in the field of machine\ntranslation (MT), mostly with respect to training data. We study in depth the\neffect of translationese on test data, using the test sets from the last three\neditions of WMT's news shared task, containing 17 translation directions. We\nshow evidence that (i) the use of translationese in test sets results in\ninflated human evaluation scores for MT systems; (ii) in some cases system\nrankings do change and (iii) the impact translationese has on a translation\ndirection is inversely correlated to the translation quality attainable by\nstate-of-the-art MT systems for that direction.", "published": "2019-06-19 12:39:09", "link": "http://arxiv.org/abs/1906.08069v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EditNTS: An Neural Programmer-Interpreter Model for Sentence\n  Simplification through Explicit Editing", "abstract": "We present the first sentence simplification model that learns explicit edit\noperations (ADD, DELETE, and KEEP) via a neural programmer-interpreter\napproach. Most current neural sentence simplification systems are variants of\nsequence-to-sequence models adopted from machine translation. These methods\nlearn to simplify sentences as a byproduct of the fact that they are trained on\ncomplex-simple sentence pairs. By contrast, our neural programmer-interpreter\nis directly trained to predict explicit edit operations on targeted parts of\nthe input sentence, resembling the way that humans might perform simplification\nand revision. Our model outperforms previous state-of-the-art neural sentence\nsimplification models (without external knowledge) by large margins on three\nbenchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89\nWikiSmall, +1.41 Newsela), and is judged by humans to produce overall better\nand simpler output sentences.", "published": "2019-06-19 14:00:15", "link": "http://arxiv.org/abs/1906.08104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Priors with Feature Attribution on Text Classification", "abstract": "Feature attribution methods, proposed recently, help users interpret the\npredictions of complex models. Our approach integrates feature attributions\ninto the objective function to allow machine learning practitioners to\nincorporate priors in model building. To demonstrate the effectiveness our\ntechnique, we apply it to two tasks: (1) mitigating unintended bias in text\nclassifiers by neutralizing identity terms; (2) improving classifier\nperformance in a scarce data setting by forcing the model to focus on toxic\nterms. Our approach adds an L2 distance loss between feature attributions and\ntask-specific prior values to the objective. Our experiments show that i) a\nclassifier trained with our technique reduces undesired model biases without a\ntrade off on the original task; ii) incorporating priors helps model\nperformance in scarce data settings.", "published": "2019-06-19 18:08:06", "link": "http://arxiv.org/abs/1906.08286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding time expressions for deep temporal ordering models", "abstract": "Data-driven models have demonstrated state-of-the-art performance in\ninferring the temporal ordering of events in text. However, these models often\noverlook explicit temporal signals, such as dates and time windows. Rule-based\nmethods can be used to identify the temporal links between these time\nexpressions (timexes), but they fail to capture timexes' interactions with\nevents and are hard to integrate with the distributed representations of neural\nnet models. In this paper, we introduce a framework to infuse temporal\nawareness into such models by learning a pre-trained model to embed timexes. We\ngenerate synthetic data consisting of pairs of timexes, then train a character\nLSTM to learn embeddings and classify the timexes' temporal relation. We\nevaluate the utility of these embeddings in the context of a strong neural\nmodel for event temporal ordering, and show a small increase in performance on\nthe MATRES dataset and more substantial gains on an automatically collected\ndataset with more frequent event-timex interactions.", "published": "2019-06-19 18:08:33", "link": "http://arxiv.org/abs/1906.08287v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REflex: Flexible Framework for Relation Extraction in Multiple Domains", "abstract": "Systematic comparison of methods for relation extraction (RE) is difficult\nbecause many experiments in the field are not described precisely enough to be\ncompletely reproducible and many papers fail to report ablation studies that\nwould highlight the relative contributions of their various combined\ntechniques. In this work, we build a unifying framework for RE, applying this\non three highly used datasets (from the general, biomedical and clinical\ndomains) with the ability to be extendable to new datasets. By performing a\nsystematic exploration of modeling, pre-processing and training methodologies,\nwe find that choices of pre-processing are a large contributor performance and\nthat omission of such information can further hinder fair comparison. Other\ninsights from our exploration allow us to provide recommendations for future\nresearch in this area.", "published": "2019-06-19 19:21:16", "link": "http://arxiv.org/abs/1906.08318v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Considerations for the Interpretation of Bias Measures of Word\n  Embeddings", "abstract": "Word embedding spaces are powerful tools for capturing latent semantic\nrelationships between terms in corpora, and have become widely popular for\nbuilding state-of-the-art natural language processing algorithms. However,\nstudies have shown that societal biases present in text corpora may be\nincorporated into the word embedding spaces learned from them. Thus, there is\nan ethical concern that human-like biases contained in the corpora and their\nderived embedding spaces might be propagated, or even amplified with the usage\nof the biased embedding spaces in downstream applications. In an attempt to\nquantify these biases so that they may be better understood and studied,\nseveral bias metrics have been proposed. We explore the statistical properties\nof these proposed measures in the context of their cited applications as well\nas their supposed utilities. We find that there are caveats to the simple\ninterpretation of these metrics as proposed. We find that the bias metric\nproposed by Bolukbasi et al. 2016 is highly sensitive to embedding\nhyper-parameter selection, and that in many cases, the variance due to the\nselection of some hyper-parameters is greater than the variance in the metric\ndue to corpus selection, while in fewer cases the bias rankings of corpora vary\nwith hyper-parameter selection. In light of these observations, it may be the\ncase that bias estimates should not be thought to directly measure the\nproperties of the underlying corpus, but rather the properties of the specific\nembedding spaces in question, particularly in the context of hyper-parameter\nselections used to generate them. Hence, bias metrics of spaces generated with\ndiffering hyper-parameters should be compared only with explicit consideration\nof the embedding-learning algorithms particular configurations.", "published": "2019-06-19 21:56:25", "link": "http://arxiv.org/abs/1906.08379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Machine Translation with Domain Sensitive Pseudo-Sources:\n  Baidu-OSU WMT19 MT Robustness Shared Task System Report", "abstract": "This paper describes the machine translation system developed jointly by\nBaidu Research and Oregon State University for WMT 2019 Machine Translation\nRobustness Shared Task. Translation of social media is a very challenging\nproblem, since its style is very different from normal parallel corpora (e.g.\nNews) and also include various types of noises. To make it worse, the amount of\nsocial media parallel corpora is extremely limited. In this paper, we use a\ndomain sensitive training method which leverages a large amount of parallel\ndata from popular domains together with a little amount of parallel data from\nsocial media. Furthermore, we generate a parallel dataset with pseudo noisy\nsource sentences which are back-translated from monolingual data using a model\ntrained by a similar domain sensitive way. We achieve more than 10 BLEU\nimprovement in both En-Fr and Fr-En translation compared with the baseline\nmethods.", "published": "2019-06-19 23:20:57", "link": "http://arxiv.org/abs/1906.08393v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Pre-Trained Transformer Language Models to Distantly\n  Supervised Relation Extraction", "abstract": "Distantly supervised relation extraction is widely used to extract relational\nfacts from text, but suffers from noisy labels. Current relation extraction\nmethods try to alleviate the noise by multi-instance learning and by providing\nsupporting linguistic and contextual information to more efficiently guide the\nrelation classification. While achieving state-of-the-art results, we observed\nthese models to be biased towards recognizing a limited set of relations with\nhigh precision, while ignoring those in the long tail. To address this gap, we\nutilize a pre-trained language model, the OpenAI Generative Pre-trained\nTransformer (GPT) [Radford et al., 2018]. The GPT and similar models have been\nshown to capture semantic and syntactic features, and also a notable amount of\n\"common-sense\" knowledge, which we hypothesize are important features for\nrecognizing a more diverse set of relations. By extending the GPT to the\ndistantly supervised setting, and fine-tuning it on the NYT10 dataset, we show\nthat it predicts a larger set of distinct relation types with high confidence.\nManual and automated evaluation of our model shows that it achieves a\nstate-of-the-art AUC score of 0.422 on the NYT10 dataset, and performs\nespecially well at higher recall levels.", "published": "2019-06-19 11:04:51", "link": "http://arxiv.org/abs/1906.08646v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Second-Order Semantic Dependency Parsing with End-to-End Neural Networks", "abstract": "Semantic dependency parsing aims to identify semantic relationships between\nwords in a sentence that form a graph. In this paper, we propose a second-order\nsemantic dependency parser, which takes into consideration not only individual\ndependency edges but also interactions between pairs of edges. We show that\nsecond-order parsing can be approximated using mean field (MF) variational\ninference or loopy belief propagation (LBP). We can unfold both algorithms as\nrecurrent layers of a neural network and therefore can train the parser in an\nend-to-end manner. Our experiments show that our approach achieves\nstate-of-the-art performance.", "published": "2019-06-19 02:05:39", "link": "http://arxiv.org/abs/1906.07880v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pre-Training with Whole Word Masking for Chinese BERT", "abstract": "Bidirectional Encoder Representations from Transformers (BERT) has shown\nmarvelous improvements across various NLP tasks, and its consecutive variants\nhave been proposed to further improve the performance of the pre-trained\nlanguage models. In this paper, we aim to first introduce the whole word\nmasking (wwm) strategy for Chinese BERT, along with a series of Chinese\npre-trained language models. Then we also propose a simple but effective model\ncalled MacBERT, which improves upon RoBERTa in several ways. Especially, we\npropose a new masking strategy called MLM as correction (Mac). To demonstrate\nthe effectiveness of these models, we create a series of Chinese pre-trained\nlanguage models as our baselines, including BERT, RoBERTa, ELECTRA, RBT, etc.\nWe carried out extensive experiments on ten Chinese NLP tasks to evaluate the\ncreated Chinese pre-trained language models as well as the proposed MacBERT.\nExperimental results show that MacBERT could achieve state-of-the-art\nperformances on many NLP tasks, and we also ablate details with several\nfindings that may help future research. We open-source our pre-trained language\nmodels for further facilitating our research community. Resources are\navailable: https://github.com/ymcui/Chinese-BERT-wwm", "published": "2019-06-19 13:54:25", "link": "http://arxiv.org/abs/1906.08101v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "abstract": "With the capability of modeling bidirectional contexts, denoising\nautoencoding based pretraining like BERT achieves better performance than\npretraining approaches based on autoregressive language modeling. However,\nrelying on corrupting the input with masks, BERT neglects dependency between\nthe masked positions and suffers from a pretrain-finetune discrepancy. In light\nof these pros and cons, we propose XLNet, a generalized autoregressive\npretraining method that (1) enables learning bidirectional contexts by\nmaximizing the expected likelihood over all permutations of the factorization\norder and (2) overcomes the limitations of BERT thanks to its autoregressive\nformulation. Furthermore, XLNet integrates ideas from Transformer-XL, the\nstate-of-the-art autoregressive model, into pretraining. Empirically, under\ncomparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a\nlarge margin, including question answering, natural language inference,\nsentiment analysis, and document ranking.", "published": "2019-06-19 17:35:48", "link": "http://arxiv.org/abs/1906.08237v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Compressed Sentence Representations for On-Device Text\n  Processing", "abstract": "Vector representations of sentences, trained on massive text corpora, are\nwidely used as generic sentence embeddings across a variety of NLP problems.\nThe learned representations are generally assumed to be continuous and\nreal-valued, giving rise to a large memory footprint and slow retrieval speed,\nwhich hinders their applicability to low-resource (memory and computation)\nplatforms, such as mobile devices. In this paper, we propose four different\nstrategies to transform continuous and generic sentence embeddings into a\nbinarized form, while preserving their rich semantic information. The\nintroduced methods are evaluated across a wide range of downstream tasks, where\nthe binarized sentence embeddings are demonstrated to degrade performance by\nonly about 2% relative to their continuous counterparts, while reducing the\nstorage requirement by over 98%. Moreover, with the learned binary\nrepresentations, the semantic relatedness of two sentences can be evaluated by\nsimply calculating their Hamming distance, which is more computational\nefficient compared with the inner product operation between continuous\nembeddings. Detailed analysis and case study further validate the effectiveness\nof proposed methods.", "published": "2019-06-19 20:29:31", "link": "http://arxiv.org/abs/1906.08340v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Open-World Extension to Knowledge Graph Completion Models", "abstract": "We present a novel extension to embedding-based knowledge graph completion\nmodels which enables them to perform open-world link prediction, i.e. to\npredict facts for entities unseen in training based on their textual\ndescription. Our model combines a regular link prediction model learned from a\nknowledge graph with word embeddings learned from a textual corpus. After\ntraining both independently, we learn a transformation to map the embeddings of\nan entity's name and description to the graph-based embedding space. In\nexperiments on several datasets including FB20k, DBPedia50k and our new dataset\nFB15k-237-OWE, we demonstrate competitive results. Particularly, our approach\nexploits the full knowledge graph structure even when textual descriptions are\nscarce, does not require a joint training on graph and text, and can be applied\nto any embedding-based link prediction model, such as TransE, ComplEx and\nDistMult.", "published": "2019-06-19 22:23:20", "link": "http://arxiv.org/abs/1906.08382v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning with Partially Ordered Representations", "abstract": "This paper examines the characterization and learning of grammars defined\nwith enriched representational models. Model-theoretic approaches to formal\nlanguage theory traditionally assume that each position in a string belongs to\nexactly one unary relation. We consider unconventional string models where\npositions can have multiple, shared properties, which are arguably useful in\nmany applications. We show the structures given by these models are partially\nordered, and present a learning algorithm that exploits this ordering relation\nto effectively prune the hypothesis space. We prove this learning algorithm,\nwhich takes positive examples as input, finds the most general grammar which\ncovers the data.", "published": "2019-06-19 02:43:50", "link": "http://arxiv.org/abs/1906.07886v2", "categories": ["cs.FL", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.FL"}
{"title": "Multimodal Abstractive Summarization for How2 Videos", "abstract": "In this paper, we study abstractive summarization for open-domain videos.\nUnlike the traditional text news summarization, the goal is less to \"compress\"\ntext information but rather to provide a fluent textual summary of information\nthat has been collected and fused from different source modalities, in our case\nvideo and audio transcripts (or text). We show how a multi-source\nsequence-to-sequence model with hierarchical attention can integrate\ninformation from different modalities into a coherent output, compare various\nmodels trained with different modalities and present pilot experiments on the\nHow2 corpus of instructional videos. We also propose a new evaluation metric\n(Content F1) for abstractive summarization task that measures semantic adequacy\nrather than fluency of the summaries, which is covered by metrics like ROUGE\nand BLEU.", "published": "2019-06-19 03:52:42", "link": "http://arxiv.org/abs/1906.07901v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Large-Scale Speaker Diarization of Radio Broadcast Archives", "abstract": "This paper describes our initial efforts to build a large-scale speaker\ndiarization (SD) and identification system on a recently digitized radio\nbroadcast archive from the Netherlands which has more than 6500 audio tapes\nwith 3000 hours of Frisian-Dutch speech recorded between 1950-2016. The\nemployed large-scale diarization scheme involves two stages: (1) tape-level\nspeaker diarization providing pseudo-speaker identities and (2) speaker linking\nto relate pseudo-speakers appearing in multiple tapes. Having access to the\nspeaker models of several frequently appearing speakers from the previously\ncollected FAME! speech corpus, we further perform speaker identification by\nlinking these known speakers to the pseudo-speakers identified at the first\nstage. In this work, we present a recently created longitudinal and\nmultilingual SD corpus designed for large-scale SD research and evaluate the\nperformance of a new speaker linking system using x-vectors with PLDA to\nquantify cross-tape speaker similarity on this corpus. The performance of this\nspeaker linking system is evaluated on a small subset of the archive which is\nmanually annotated with speaker information. The speaker linking performance\nreported on this subset (53 hours) and the whole archive (3000 hours) is\ncompared to quantify the impact of scaling up in the amount of speech data.", "published": "2019-06-19 07:58:42", "link": "http://arxiv.org/abs/1906.07955v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Code-Switching Detection Using ASR-Generated Language Posteriors", "abstract": "Code-switching (CS) detection refers to the automatic detection of language\nswitches in code-mixed utterances. This task can be achieved by using a CS\nautomatic speech recognition (ASR) system that can handle such language\nswitches. In our previous work, we have investigated the code-switching\ndetection performance of the Frisian-Dutch CS ASR system by using the time\nalignment of the most likely hypothesis and found that this technique suffers\nfrom over-switching due to numerous very short spurious language switches. In\nthis paper, we propose a novel method for CS detection aiming to remedy this\nshortcoming by using the language posteriors which are the sum of the\nframe-level posteriors of phones belonging to the same language. The CS\nASR-generated language posteriors contain more complete language-specific\ninformation on frame level compared to the time alignment of the ASR output.\nHence, it is expected to yield more accurate and robust CS detection. The CS\ndetection experiments demonstrate that the proposed language posterior-based\napproach provides higher detection accuracy than the baseline system in terms\nof equal error rate. Moreover, a detailed CS detection error analysis reveals\nthat using language posteriors reduces the false alarms and results in more\nrobust CS detection.", "published": "2019-06-19 09:56:34", "link": "http://arxiv.org/abs/1906.08003v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Spatial Pyramid Encoding with Convex Length Normalization for\n  Text-Independent Speaker Verification", "abstract": "In this paper, we propose a new pooling method called spatial pyramid\nencoding (SPE) to generate speaker embeddings for text-independent speaker\nverification. We first partition the output feature maps from a deep residual\nnetwork (ResNet) into increasingly fine sub-regions and extract speaker\nembeddings from each sub-region through a learnable dictionary encoding layer.\nThese embeddings are concatenated to obtain the final speaker representation.\nThe SPE layer not only generates a fixed-dimensional speaker embedding for a\nvariable-length speech segment, but also aggregates the information of feature\ndistribution from multi-level temporal bins. Furthermore, we apply deep length\nnormalization by augmenting the loss function with ring loss. By applying ring\nloss, the network gradually learns to normalize the speaker embeddings using\nmodel weights themselves while preserving convexity, leading to more robust\nspeaker embeddings. Experiments on the VoxCeleb1 dataset show that the proposed\nsystem using the SPE layer and ring loss-based deep length normalization\noutperforms both i-vector and d-vector baselines.", "published": "2019-06-19 20:13:27", "link": "http://arxiv.org/abs/1906.08333v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Learning Disentangled Representations of Timbre and Pitch for Musical\n  Instrument Sounds Using Gaussian Mixture Variational Autoencoders", "abstract": "In this paper, we learn disentangled representations of timbre and pitch for\nmusical instrument sounds. We adapt a framework based on variational\nautoencoders with Gaussian mixture latent distributions. Specifically, we use\ntwo separate encoders to learn distinct latent spaces for timbre and pitch,\nwhich form Gaussian mixture components representing instrument identity and\npitch, respectively. For reconstruction, latent variables of timbre and pitch\nare sampled from corresponding mixture components, and are concatenated as the\ninput to a decoder. We show the model efficacy by latent space visualization,\nand a quantitative analysis indicates the discriminability of these spaces,\neven with a limited number of instrument labels for training. The model allows\nfor controllable synthesis of selected instrument sounds by sampling from the\nlatent spaces. To evaluate this, we trained instrument and pitch classifiers\nusing original labeled data. These classifiers achieve high accuracy when\ntested on our synthesized sounds, which verifies the model performance of\ncontrollable realistic timbre and pitch synthesis. Our model also enables\ntimbre transfer between multiple instruments, with a single autoencoder\narchitecture, which is evaluated by measuring the shift in posterior of\ninstrument classification. Our in depth evaluation confirms the model ability\nto successfully disentangle timbre and pitch.", "published": "2019-06-19 15:25:29", "link": "http://arxiv.org/abs/1906.08152v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning Discriminative features using Center Loss and Reconstruction as\n  Regularizer for Speech Emotion Recognition", "abstract": "This paper proposes a Convolutional Neural Network (CNN) inspired by\nMultitask Learning (MTL) and based on speech features trained under the joint\nsupervision of softmax loss and center loss, a powerful metric learning\nstrategy, for the recognition of emotion in speech. Speech features such as\nSpectrograms and Mel-frequency Cepstral Coefficient s (MFCCs) help retain\nemotion-related low-level characteristics in speech. We experimented with\nseveral Deep Neural Network (DNN) architectures that take in speech features as\ninput and trained them under both softmax and center loss, which resulted in\nhighly discriminative features ideal for Speech Emotion Recognition (SER). Our\nnetworks also employ a regularizing effect by simultaneously performing the\nauxiliary task of reconstructing the input speech features. This sharing of\nrepresentations among related tasks enables our network to better generalize\nthe original task of SER. Some of our proposed networks contain far fewer\nparameters when compared to state-of-the-art architectures.", "published": "2019-06-19 02:50:59", "link": "http://arxiv.org/abs/1906.08873v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
