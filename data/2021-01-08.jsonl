{"title": "A Novel Word Sense Disambiguation Approach Using WordNet Knowledge Graph", "abstract": "Various applications in computational linguistics and artificial intelligence\nrely on high-performing word sense disambiguation techniques to solve\nchallenging tasks such as information retrieval, machine translation, question\nanswering, and document clustering. While text comprehension is intuitive for\nhumans, machines face tremendous challenges in processing and interpreting a\nhuman's natural language. This paper presents a novel knowledge-based word\nsense disambiguation algorithm, namely Sequential Contextual Similarity Matrix\nMultiplication (SCSMM). The SCSMM algorithm combines semantic similarity,\nheuristic knowledge, and document context to respectively exploit the merits of\nlocal context between consecutive terms, human knowledge about terms, and a\ndocument's main topic in disambiguating terms. Unlike other algorithms, the\nSCSMM algorithm guarantees the capture of the maximum sentence context while\nmaintaining the terms' order within the sentence. The proposed algorithm\noutperformed all other algorithms when disambiguating nouns on the combined\ngold standard datasets, while demonstrating comparable results to current\nstate-of-the-art word sense disambiguation systems when dealing with each\ndataset separately. Furthermore, the paper discusses the impact of granularity\nlevel, ambiguity rate, sentence size, and part of speech distribution on the\nperformance of the proposed algorithm.", "published": "2021-01-08 06:47:32", "link": "http://arxiv.org/abs/2101.02875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Misspelling Correction with Pre-trained Contextual Language Model", "abstract": "Spelling irregularities, known now as spelling mistakes, have been found for\nseveral centuries. As humans, we are able to understand most of the misspelled\nwords based on their location in the sentence, perceived pronunciation, and\ncontext. Unlike humans, computer systems do not possess the convenient auto\ncomplete functionality of which human brains are capable. While many programs\nprovide spelling correction functionality, many systems do not take context\ninto account. Moreover, Artificial Intelligence systems function in the way\nthey are trained on. With many current Natural Language Processing (NLP)\nsystems trained on grammatically correct text data, many are vulnerable against\nadversarial examples, yet correctly spelled text processing is crucial for\nlearning. In this paper, we investigate how spelling errors can be corrected in\ncontext, with a pre-trained language model BERT. We present two experiments,\nbased on BERT and the edit distance algorithm, for ranking and selecting\ncandidate corrections. The results of our experiments demonstrated that when\ncombined properly, contextual word embeddings of BERT and edit distance are\ncapable of effectively correcting spelling errors.", "published": "2021-01-08 20:11:01", "link": "http://arxiv.org/abs/2101.03204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effect of Word Embedding Variable Parameters on Arabic Sentiment\n  Analysis Performance", "abstract": "Social media such as Twitter, Facebook, etc. has led to a generated growing\nnumber of comments that contains users opinions. Sentiment analysis research\ndeals with these comments to extract opinions which are positive or negative.\nArabic language is a rich morphological language; thus, classical techniques of\nEnglish sentiment analysis cannot be used for Arabic. Word embedding technique\ncan be considered as one of successful methods to gaping the morphological\nproblem of Arabic. Many works have been done for Arabic sentiment analysis\nbased on word embedding, but there is no study focused on variable parameters.\nThis study will discuss three parameters (Window size, Dimension of vector and\nNegative Sample) for Arabic sentiment analysis using DBOW and DMPV\narchitectures. A large corpus of previous works generated to learn word\nrepresentations and extract features. Four binary classifiers (Logistic\nRegression, Decision Tree, Support Vector Machine and Naive Bayes) are used to\ndetect sentiment. The performance of classifiers evaluated based on; Precision,\nRecall and F1-score.", "published": "2021-01-08 08:31:00", "link": "http://arxiv.org/abs/2101.02906v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph-of-Tweets: A Graph Merging Approach to Sub-event Identification", "abstract": "Graph structures are powerful tools for modeling the relationships between\ntextual elements. Graph-of-Words (GoW) has been adopted in many Natural\nLanguage tasks to encode the association between terms. However, GoW provides\nfew document-level relationships in cases when the connections between\ndocuments are also essential. For identifying sub-events on social media like\nTwitter, features from both word- and document-level can be useful as they\nsupply different information of the event. We propose a hybrid Graph-of-Tweets\n(GoT) model which combines the word- and document-level structures for modeling\nTweets. To compress large amount of raw data, we propose a graph merging method\nwhich utilizes FastText word embeddings to reduce the GoW. Furthermore, we\npresent a novel method to construct GoT with the reduced GoW and a Mutual\nInformation (MI) measure. Finally, we identify maximal cliques to extract\npopular sub-events. Our model showed promising results on condensing\nlexical-level information and capturing keywords of sub-events.", "published": "2021-01-08 20:24:25", "link": "http://arxiv.org/abs/2101.03208v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Multilingual Transformers for Hate Speech Detection", "abstract": "Detecting and classifying instances of hate in social media text has been a\nproblem of interest in Natural Language Processing in the recent years. Our\nwork leverages state of the art Transformer language models to identify hate\nspeech in a multilingual setting. Capturing the intent of a post or a comment\non social media involves careful evaluation of the language style, semantic\ncontent and additional pointers such as hashtags and emojis. In this paper, we\nlook at the problem of identifying whether a Twitter post is hateful and\noffensive or not. We further discriminate the detected toxic content into one\nof the following three classes: (a) Hate Speech (HATE), (b) Offensive (OFFN)\nand (c) Profane (PRFN). With a pre-trained multilingual Transformer-based text\nencoder at the base, we are able to successfully identify and classify hate\nspeech from multiple languages. On the provided testing corpora, we achieve\nMacro F1 scores of 90.29, 81.87 and 75.40 for English, German and Hindi\nrespectively while performing hate speech detection and of 60.70, 53.28 and\n49.74 during fine-grained classification. In our experiments, we show the\nefficacy of Perspective API features for hate speech classification and the\neffects of exploiting a multilingual training scheme. A feature selection study\nis provided to illustrate impacts of specific features upon the architecture's\nclassification head.", "published": "2021-01-08 20:23:50", "link": "http://arxiv.org/abs/2101.03207v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Four-Stage Data Augmentation Approach to ResNet-Conformer Based\n  Acoustic Modeling for Sound Event Localization and Detection", "abstract": "In this paper, we propose a novel four-stage data augmentation approach to\nResNet-Conformer based acoustic modeling for sound event localization and\ndetection (SELD). First, we explore two spatial augmentation techniques, namely\naudio channel swapping (ACS) and multi-channel simulation (MCS), to deal with\ndata sparsity in SELD. ACS and MDS focus on augmenting the limited training\ndata with expanding direction of arrival (DOA) representations such that the\nacoustic models trained with the augmented data are robust to localization\nvariations of acoustic sources. Next, time-domain mixing (TDM) and\ntime-frequency masking (TFM) are also investigated to deal with overlapping\nsound events and data diversity. Finally, ACS, MCS, TDM and TFM are combined in\na step-by-step manner to form an effective four-stage data augmentation scheme.\nTested on the Detection and Classification of Acoustic Scenes and Events\n(DCASE) 2020 data set, our proposed augmentation approach greatly improves the\nsystem performance, ranking our submitted system in the first place in the SELD\ntask of the DCASE 2020 Challenge. Furthermore, we employ a ResNet-Conformer\narchitecture to model both global and local context dependencies of an audio\nsequence and win the first place in the DCASE 2022 SELD evaluations.", "published": "2021-01-08 08:55:37", "link": "http://arxiv.org/abs/2101.02919v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
