{"title": "Simple Search Algorithms on Semantic Networks Learned from Language Use", "abstract": "Recent empirical and modeling research has focused on the semantic fluency\ntask because it is informative about semantic memory. An interesting interplay\narises between the richness of representations in semantic memory and the\ncomplexity of algorithms required to process it. It has remained an open\nquestion whether representations of words and their relations learned from\nlanguage use can enable a simple search algorithm to mimic the observed\nbehavior in the fluency task. Here we show that it is plausible to learn rich\nrepresentations from naturalistic data for which a very simple search algorithm\n(a random walk) can replicate the human patterns. We suggest that explicitly\nstructuring knowledge about words into a semantic network plays a crucial role\nin modeling human behavior in memory search and retrieval; moreover, this is\nthe case across a range of semantic information sources.", "published": "2016-02-10 04:54:15", "link": "http://arxiv.org/abs/1602.03265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Sarcasm Detection: A Survey", "abstract": "Automatic sarcasm detection is the task of predicting sarcasm in text. This\nis a crucial step to sentiment analysis, considering prevalence and challenges\nof sarcasm in sentiment-bearing text. Beginning with an approach that used\nspeech-based features, sarcasm detection has witnessed great interest from the\nsentiment analysis community. This paper is the first known compilation of past\nwork in automatic sarcasm detection. We observe three milestones in the\nresearch so far: semi-supervised pattern extraction to identify implicit\nsentiment, use of hashtag-based supervision, and use of context beyond target\ntext. In this paper, we describe datasets, approaches, trends and issues in\nsarcasm detection. We also discuss representative performance values, shared\ntasks and pointers to future work, as given in prior works. In terms of\nresources that could be useful for understanding state-of-the-art, the survey\npresents several useful illustrations - most prominently, a table that\nsummarizes past papers along different dimensions such as features, annotation\ntechniques, data forms, etc.", "published": "2016-02-10 16:02:46", "link": "http://arxiv.org/abs/1602.03426v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Distributed Representations of Sentences from Unlabelled Data", "abstract": "Unsupervised methods for learning distributed representations of words are\nubiquitous in today's NLP research, but far less is known about the best ways\nto learn distributed phrase or sentence representations from unlabelled data.\nThis paper is a systematic comparison of models that learn such\nrepresentations. We find that the optimal approach depends critically on the\nintended application. Deeper, more complex models are preferable for\nrepresentations to be used in supervised systems, but shallow log-linear models\nwork best for building representation spaces that can be decoded with simple\nspatial distance metrics. We also propose two new unsupervised\nrepresentation-learning objectives designed to optimise the trade-off between\ntraining time, domain portability and performance.", "published": "2016-02-10 18:49:58", "link": "http://arxiv.org/abs/1602.03483v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Transfer with Medical Language Embeddings", "abstract": "Identifying relationships between concepts is a key aspect of scientific\nknowledge synthesis. Finding these links often requires a researcher to\nlaboriously search through scien- tific papers and databases, as the size of\nthese resources grows ever larger. In this paper we describe how distributional\nsemantics can be used to unify structured knowledge graphs with unstructured\ntext to predict new relationships between medical concepts, using a\nprobabilistic generative model. Our approach is also designed to ameliorate\ndata sparsity and scarcity issues in the medical domain, which make language\nmodelling more challenging. Specifically, we integrate the medical relational\ndatabase (SemMedDB) with text from electronic health records (EHRs) to perform\nknowledge graph completion. We further demonstrate the ability of our model to\npredict relationships between tokens not appearing in the relational database.", "published": "2016-02-10 22:02:29", "link": "http://arxiv.org/abs/1602.03551v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
