{"title": "PolyLM: An Open Source Polyglot Large Language Model", "abstract": "Large language models (LLMs) demonstrate remarkable ability to comprehend,\nreason, and generate following nature language instructions. However, the\ndevelopment of LLMs has been primarily focused on high-resource languages, such\nas English, thereby limiting their applicability and research in other\nlanguages. Consequently, we present PolyLM, a multilingual LLM trained on 640\nbillion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its\nmultilingual capabilities, we 1) integrate bilingual data into training data;\nand 2) adopt a curriculum learning strategy that increases the proportion of\nnon-English data from 30% in the first stage to 60% in the final stage during\npre-training. Further, we propose a multilingual self-instruct method which\nautomatically generates 132.7K diverse multilingual instructions for model\nfine-tuning. To assess the model's performance, we collect several existing\nmultilingual tasks, including multilingual understanding, question answering,\ngeneration, and translation. Extensive experiments show that PolyLM surpasses\nother open-source models such as LLaMA and BLOOM on multilingual tasks while\nmaintaining comparable performance in English. Our models, alone with the\ninstruction data and multilingual benchmark, are available at:\n\\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}.", "published": "2023-07-12 09:00:37", "link": "http://arxiv.org/abs/2307.06018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pluggable Neural Machine Translation Models via Memory-augmented\n  Adapters", "abstract": "Although neural machine translation (NMT) models perform well in the general\ndomain, it remains rather challenging to control their generation behavior to\nsatisfy the requirement of different users. Given the expensive training cost\nand the data scarcity challenge of learning a new model from scratch for each\nuser requirement, we propose a memory-augmented adapter to steer pretrained NMT\nmodels in a pluggable manner. Specifically, we construct a multi-granular\nmemory based on the user-provided text samples and propose a new adapter\narchitecture to combine the model representations and the retrieved results. We\nalso propose a training strategy using memory dropout to reduce spurious\ndependencies between the NMT model and the memory. We validate our approach on\nboth style- and domain-specific experiments and the results indicate that our\nmethod can outperform several representative pluggable baselines.", "published": "2023-07-12 09:23:41", "link": "http://arxiv.org/abs/2307.06029v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study on the Appropriate size of the Mongolian general corpus", "abstract": "This study aims to determine the appropriate size of the Mongolian general\ncorpus. This study used the Heaps function and Type Token Ratio to determine\nthe appropriate size of the Mongolian general corpus. The sample corpus of\n906,064 tokens comprised texts from 10 domains of newspaper politics, economy,\nsociety, culture, sports, world articles and laws, middle and high school\nliterature textbooks, interview articles, and podcast transcripts. First, we\nestimated the Heaps function with this sample corpus. Next, we observed changes\nin the number of types and TTR values while increasing the number of tokens by\none million using the estimated Heaps function. As a result of observation, we\nfound that the TTR value hardly changed when the number of tokens exceeded from\n39 to 42 million. Thus, we conclude that an appropriate size for a Mongolian\ngeneral corpus is from 39 to 42 million tokens.", "published": "2023-07-12 10:10:24", "link": "http://arxiv.org/abs/2307.06050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Portuguese Sign Language Animation with Dynamic Timing and\n  Mouthing", "abstract": "Current signing avatars are often described as unnatural as they cannot\naccurately reproduce all the subtleties of synchronized body behaviors of a\nhuman signer. In this paper, we propose a new dynamic approach for transitions\nbetween signs, focusing on mouthing animations for Portuguese Sign Language.\nAlthough native signers preferred animations with dynamic transitions, we did\nnot find significant differences in comprehension and perceived naturalness\nscores. On the other hand, we show that including mouthing behaviors improved\ncomprehension and perceived naturalness for novice sign language learners.\nResults have implications in computational linguistics, human-computer\ninteraction, and synthetic animation of signing avatars.", "published": "2023-07-12 12:25:03", "link": "http://arxiv.org/abs/2307.06124v1", "categories": ["cs.CL", "ACM-class: I.6, I.7, J.6"], "primary_category": "cs.CL"}
{"title": "Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep\n  Learning Approaches", "abstract": "Poetry holds immense significance within the cultural and traditional fabric\nof any nation. It serves as a vehicle for poets to articulate their emotions,\npreserve customs, and convey the essence of their culture. Arabic poetry is no\nexception, having played a cherished role in the heritage of the Arabic\ncommunity throughout history and maintaining its relevance in the present era.\nTypically, comprehending Arabic poetry necessitates the expertise of a linguist\nwho can analyze its content and assess its quality. This paper presents the\nintroduction of a framework called \\textit{Ashaar}\nhttps://github.com/ARBML/Ashaar, which encompasses a collection of datasets and\npre-trained models designed specifically for the analysis and generation of\nArabic poetry. The pipeline established within our proposed approach\nencompasses various aspects of poetry, such as meter, theme, and era\nclassification. It also incorporates automatic poetry diacritization, enabling\nmore intricate analyses like automated extraction of the \\textit{Arudi} style.\nAdditionally, we explore the feasibility of generating conditional poetry\nthrough the pre-training of a character-based GPT model. Furthermore, as part\nof this endeavor, we provide four datasets: one for poetry generation, another\nfor diacritization, and two for Arudi-style prediction. These datasets aim to\nfacilitate research and development in the field of Arabic poetry by enabling\nresearchers and enthusiasts to delve into the nuances of this rich literary\ntradition.", "published": "2023-07-12 15:07:16", "link": "http://arxiv.org/abs/2307.06218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Acquisition of Semantic Relationships between words", "abstract": "The study of semantic relationships has revealed a close connection between\nthese relationships and the morphological characteristics of a language.\nMorphology, as a subfield of linguistics, investigates the internal structure\nand formation of words. By delving into the relationship between semantic\nrelationships and language morphology, we can gain deeper insights into how the\nunderlying structure of words contributes to the interpretation and\ncomprehension of language. This paper explores the dynamic interplay between\nsemantic relationships and the morphological aspects of different languages, by\nexamining the intricate relationship between language morphology and semantic\nrelationships, valuable insights can be gained regarding how the structure of\nwords influences language comprehension.", "published": "2023-07-12 19:18:55", "link": "http://arxiv.org/abs/2307.06419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Overview of Large Language Models", "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable\ncapabilities in natural language processing tasks and beyond. This success of\nLLMs has led to a large influx of research contributions in this direction.\nThese works encompass diverse topics such as architectural innovations, better\ntraining strategies, context length improvements, fine-tuning, multi-modal\nLLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid\ndevelopment of techniques and regular breakthroughs in LLM research, it has\nbecome considerably challenging to perceive the bigger picture of the advances\nin this direction. Considering the rapidly emerging plethora of literature on\nLLMs, it is imperative that the research community is able to benefit from a\nconcise yet comprehensive overview of the recent developments in this field.\nThis article provides an overview of the existing literature on a broad range\nof LLM-related concepts. Our self-contained comprehensive overview of LLMs\ndiscusses relevant background concepts along with covering the advanced topics\nat the frontier of research in LLMs. This review article is intended to not\nonly provide a systematic survey but also a quick comprehensive reference for\nthe researchers and practitioners to draw insights from extensive informative\nsummaries of the existing works to advance the LLM research.", "published": "2023-07-12 20:01:52", "link": "http://arxiv.org/abs/2307.06435v10", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM\n  Decoding", "abstract": "This paper presents \"Predictive Pipelined Decoding (PPD),\" an approach that\nspeeds up greedy decoding in Large Language Models (LLMs) while maintaining the\nexact same output as the original decoding. Unlike conventional strategies, PPD\nemploys additional compute resources to parallelize the initiation of\nsubsequent token decoding during the current token decoding. This method\nreduces decoding latency and reshapes the understanding of trade-offs in LLM\ndecoding strategies. We have developed a theoretical framework that allows us\nto analyze the trade-off between computation and latency. Using this framework,\nwe can analytically estimate the potential reduction in latency associated with\nour proposed method, achieved through the assessment of the match rate,\nrepresented as p_correct. The results demonstrate that the use of extra\ncomputational resources has the potential to accelerate LLM decoding.\nAdditionally, we implement PPD and conduct preliminary experiments to\nempirically validate its efficacy, addressing potential practical overheads not\ncovered by theoretical analysis.", "published": "2023-07-12 04:28:41", "link": "http://arxiv.org/abs/2307.05908v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Distilled Quantization: Achieving High Compression Rates in\n  Transformer-Based Language Models", "abstract": "We investigate the effects of post-training quantization and\nquantization-aware training on the generalization of Transformer language\nmodels. We present a new method called self-distilled quantization (SDQ) that\nminimizes accumulative quantization errors and outperforms baselines. We apply\nSDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that\nboth models can be reduced from 32-bit floating point weights to 8-bit integer\nweights while maintaining a high level of performance on the XGLUE benchmark.\nOur results also highlight the challenges of quantizing multilingual models,\nwhich must generalize to languages they were not fine-tuned on.", "published": "2023-07-12 07:38:24", "link": "http://arxiv.org/abs/2307.05972v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MMBench: Is Your Multi-modal Model an All-around Player?", "abstract": "Large vision-language models (VLMs) have recently achieved remarkable\nprogress, exhibiting impressive multimodal perception and reasoning abilities.\nHowever, effectively evaluating these large VLMs remains a major challenge,\nhindering future development in this domain. Traditional benchmarks like VQAv2\nor COCO Caption provide quantitative performance measurements but lack\nfine-grained ability assessment and robust evaluation metrics. Meanwhile,\nsubjective benchmarks, such as OwlEval, offer comprehensive evaluations of a\nmodel's abilities by incorporating human labor, which is not scalable and may\ndisplay significant bias. In response to these challenges, we propose MMBench,\na bilingual benchmark for assessing the multi-modal capabilities of VLMs.\nMMBench methodically develops a comprehensive evaluation pipeline, primarily\ncomprised of the following key features: 1. MMBench is meticulously curated\nwith well-designed quality control schemes, surpassing existing similar\nbenchmarks in terms of the number and variety of evaluation questions and\nabilities; 2. MMBench introduces a rigorous CircularEval strategy and\nincorporates large language models to convert free-form predictions into\npre-defined choices, which helps to yield accurate evaluation results for\nmodels with limited instruction-following capabilities. 3. MMBench incorporates\nmultiple-choice questions in both English and Chinese versions, enabling an\napples-to-apples comparison of VLMs' performance under a bilingual context. To\nsummarize, MMBench is a systematically designed objective benchmark for a\nrobust and holistic evaluation of vision-language models. We hope MMBench will\nassist the research community in better evaluating their models and facilitate\nfuture progress in this area. The evalutation code of MMBench has been\nintegrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.", "published": "2023-07-12 16:23:09", "link": "http://arxiv.org/abs/2307.06281v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Distilling Large Language Models for Biomedical Knowledge Extraction: A\n  Case Study on Adverse Drug Events", "abstract": "Large language models (LLMs), such as GPT-4, have demonstrated remarkable\ncapabilities across a wide range of tasks, including health applications. In\nthis paper, we study how LLMs can be used to scale biomedical knowledge\ncuration. We find that while LLMs already possess decent competency in\nstructuring biomedical text, by distillation into a task-specific student model\nthrough self-supervised learning, substantial gains can be attained over\nout-of-box LLMs, with additional advantages such as cost, efficiency, and\nwhite-box model access.\n  We conduct a case study on adverse drug event (ADE) extraction, which is an\nimportant area for improving care. On standard ADE extraction evaluation, a\nGPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervised\nstate-of-the-art models without using any labeled data. Despite being over\n1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by\nover 6 absolute points in F1 and GPT-4 by over 5 absolute points.\n  Ablation studies on distillation model choice (e.g., PubMedBERT vs BioGPT)\nand ADE extraction architecture shed light on best practice for biomedical\nknowledge extraction. Similar gains were attained by distillation for other\nstandard biomedical knowledge extraction tasks such as gene-disease\nassociations and protected health information, further illustrating the promise\nof this approach.", "published": "2023-07-12 20:08:48", "link": "http://arxiv.org/abs/2307.06439v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification\n  Task", "abstract": "Conspiracy Theory Identication task is a new shared task proposed for the\nfirst time at the Evalita 2023. The ACTI challenge, based exclusively on\ncomments published on conspiratorial channels of telegram, is divided into two\nsubtasks: (i) Conspiratorial Content Classification: identifying conspiratorial\ncontent and (ii) Conspiratorial Category Classification about specific\nconspiracy theory classification. A total of fifteen teams participated in the\ntask for a total of 81 submissions. We illustrate the best performing\napproaches were based on the utilization of large language models. We finally\ndraw conclusions about the utilization of these models for counteracting the\nspreading of misinformation in online platforms.", "published": "2023-07-12 20:33:30", "link": "http://arxiv.org/abs/2307.06954v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prototypical Contrastive Transfer Learning for Multimodal Language\n  Understanding", "abstract": "Although domestic service robots are expected to assist individuals who\nrequire support, they cannot currently interact smoothly with people through\nnatural language. For example, given the instruction \"Bring me a bottle from\nthe kitchen,\" it is difficult for such robots to specify the bottle in an\nindoor environment. Most conventional models have been trained on real-world\ndatasets that are labor-intensive to collect, and they have not fully leveraged\nsimulation data through a transfer learning framework. In this study, we\npropose a novel transfer learning approach for multimodal language\nunderstanding called Prototypical Contrastive Transfer Learning (PCTL), which\nuses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task\nof identifying target objects in domestic environments according to free-form\nnatural language instructions. To validate PCTL, we built new real-world and\nsimulation datasets. Our experiment demonstrated that PCTL outperformed\nexisting methods. Specifically, PCTL achieved an accuracy of 78.1%, whereas\nsimple fine-tuning achieved an accuracy of 73.4%.", "published": "2023-07-12 06:14:36", "link": "http://arxiv.org/abs/2307.05942v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "DDNAS: Discretized Differentiable Neural Architecture Search for Text\n  Classification", "abstract": "Neural Architecture Search (NAS) has shown promising capability in learning\ntext representation. However, existing text-based NAS neither performs a\nlearnable fusion of neural operations to optimize the architecture, nor encodes\nthe latent hierarchical categorization behind text input. This paper presents a\nnovel NAS method, Discretized Differentiable Neural Architecture Search\n(DDNAS), for text representation learning and classification. With the\ncontinuous relaxation of architecture representation, DDNAS can use gradient\ndescent to optimize the search. We also propose a novel discretization layer\nvia mutual information maximization, which is imposed on every search node to\nmodel the latent hierarchical categorization in text representation. Extensive\nexperiments conducted on eight diverse real datasets exhibit that DDNAS can\nconsistently outperform the state-of-the-art NAS methods. While DDNAS relies on\nonly three basic operations, i.e., convolution, pooling, and none, to be the\ncandidates of NAS building blocks, its promising performance is noticeable and\nextensible to obtain further improvement by adding more different operations.", "published": "2023-07-12 08:33:16", "link": "http://arxiv.org/abs/2307.06005v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpreting deep embeddings for disease progression clustering", "abstract": "We propose a novel approach for interpreting deep embeddings in the context\nof patient clustering. We evaluate our approach on a dataset of participants\nwith type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful\ninsights into disease progression patterns.", "published": "2023-07-12 10:22:28", "link": "http://arxiv.org/abs/2307.06060v2", "categories": ["stat.ML", "cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "stat.ML"}
{"title": "VELMA: Verbalization Embodiment of LLM Agents for Vision and Language\n  Navigation in Street View", "abstract": "Incremental decision making in real-world environments is one of the most\nchallenging tasks in embodied artificial intelligence. One particularly\ndemanding scenario is Vision and Language Navigation~(VLN) which requires\nvisual and natural language understanding as well as spatial and temporal\nreasoning capabilities. The embodied agent needs to ground its understanding of\nnavigation instructions in observations of a real-world environment like Street\nView. Despite the impressive results of LLMs in other research areas, it is an\nongoing problem of how to best connect them with an interactive visual\nenvironment. In this work, we propose VELMA, an embodied LLM agent that uses a\nverbalization of the trajectory and of visual environment observations as\ncontextual prompt for the next action. Visual information is verbalized by a\npipeline that extracts landmarks from the human written navigation instructions\nand uses CLIP to determine their visibility in the current panorama view. We\nshow that VELMA is able to successfully follow navigation instructions in\nStreet View with only two in-context examples. We further finetune the LLM\nagent on a few thousand examples and achieve 25%-30% relative improvement in\ntask completion over the previous state-of-the-art for two datasets.", "published": "2023-07-12 11:08:24", "link": "http://arxiv.org/abs/2307.06082v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems", "abstract": "In autonomic computing, self-adaptation has been proposed as a fundamental\nparadigm to manage the complexity of multiagent systems (MASs). This achieved\nby extending a system with support to monitor and adapt itself to achieve\nspecific concerns of interest. Communication in these systems is key given that\nin scenarios involving agent interaction, it enhances cooperation and reduces\ncoordination challenges by enabling direct, clear information exchange.\nHowever, improving the expressiveness of the interaction communication with\nMASs is not without challenges. In this sense, the interplay between\nself-adaptive systems and effective communication is crucial for future MAS\nadvancements. In this paper, we propose the integration of large language\nmodels (LLMs) such as GPT-based technologies into multiagent systems. We anchor\nour methodology on the MAPE-K model, which is renowned for its robust support\nin monitoring, analyzing, planning, and executing system adaptations in\nresponse to dynamic environments. We also present a practical illustration of\nthe proposed approach, in which we implement and assess a basic MAS-based\napplication. The approach significantly advances the state-of-the-art of\nself-adaptive systems by proposing a new paradigm for MAS self-adaptation of\nautonomous systems based on LLM capabilities.", "published": "2023-07-12 14:26:46", "link": "http://arxiv.org/abs/2307.06187v1", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA"}
{"title": "Instruction Mining: Instruction Data Selection for Tuning Large Language\n  Models", "abstract": "Large language models (LLMs) are initially pretrained for broad capabilities\nand then finetuned with instruction-following datasets to improve their\nperformance in interacting with humans. Despite advances in finetuning, a\nstandardized guideline for selecting high-quality datasets to optimize this\nprocess remains elusive. In this paper, we first propose InstructMining, an\ninnovative method designed for automatically selecting premium\ninstruction-following data for finetuning LLMs. Specifically, InstructMining\nutilizes natural language indicators as a measure of data quality, applying\nthem to evaluate unseen datasets. During experimentation, we discover that\ndouble descent phenomenon exists in large language model finetuning. Based on\nthis observation, we further leverage BlendSearch to help find the best subset\namong the entire dataset (i.e., 2,532 out of 100,000). Experiment results show\nthat InstructMining-7B achieves state-of-the-art performance on two of the most\npopular benchmarks: LLM-as-a-judge and Huggingface OpenLLM leaderboard.", "published": "2023-07-12 16:37:31", "link": "http://arxiv.org/abs/2307.06290v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assessing the Ability of ChatGPT to Screen Articles for Systematic\n  Reviews", "abstract": "By organizing knowledge within a research field, Systematic Reviews (SR)\nprovide valuable leads to steer research. Evidence suggests that SRs have\nbecome first-class artifacts in software engineering. However, the tedious\nmanual effort associated with the screening phase of SRs renders these studies\na costly and error-prone endeavor. While screening has traditionally been\nconsidered not amenable to automation, the advent of generative AI-driven\nchatbots, backed with large language models is set to disrupt the field. In\nthis report, we propose an approach to leverage these novel technological\ndevelopments for automating the screening of SRs. We assess the consistency,\nclassification performance, and generalizability of ChatGPT in screening\narticles for SRs and compare these figures with those of traditional\nclassifiers used in SR automation. Our results indicate that ChatGPT is a\nviable option to automate the SR processes, but requires careful considerations\nfrom developers when integrating ChatGPT into their SR tools.", "published": "2023-07-12 21:39:42", "link": "http://arxiv.org/abs/2307.06464v1", "categories": ["cs.SE", "cs.CL", "cs.IR"], "primary_category": "cs.SE"}
{"title": "Misclassification in Automated Content Analysis Causes Bias in\n  Regression. Can We Fix It? Yes We Can!", "abstract": "Automated classifiers (ACs), often built via supervised machine learning\n(SML), can categorize large, statistically powerful samples of data ranging\nfrom text to images and video, and have become widely popular measurement\ndevices in communication science and related fields. Despite this popularity,\neven highly accurate classifiers make errors that cause misclassification bias\nand misleading results in downstream analyses-unless such analyses account for\nthese errors. As we show in a systematic literature review of SML applications,\ncommunication scholars largely ignore misclassification bias. In principle,\nexisting statistical methods can use \"gold standard\" validation data, such as\nthat created by human annotators, to correct misclassification bias and produce\nconsistent estimates. We introduce and test such methods, including a new\nmethod we design and implement in the R package misclassificationmodels, via\nMonte Carlo simulations designed to reveal each method's limitations, which we\nalso release. Based on our results, we recommend our new error correction\nmethod as it is versatile and efficient. In sum, automated classifiers, even\nthose below common accuracy standards or making systematic misclassifications,\ncan be useful for measurement with careful study design and appropriate error\ncorrection methods.", "published": "2023-07-12 23:03:55", "link": "http://arxiv.org/abs/2307.06483v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "G.3; K.4.0; I.2.6"], "primary_category": "cs.LG"}
{"title": "SummaryMixing: A Linear-Complexity Alternative to Self-Attention for\n  Speech Recognition and Understanding", "abstract": "Modern speech processing systems rely on self-attention. Unfortunately, token\nmixing with self-attention takes quadratic time in the length of the speech\nutterance, slowing down inference and training and increasing memory\nconsumption. Cheaper alternatives to self-attention for ASR have been\ndeveloped, but they fail to consistently reach the same level of accuracy. This\npaper, therefore, proposes a novel linear-time alternative to self-attention.\nIt summarises an utterance with the mean over vectors for all time steps. This\nsingle summary is then combined with time-specific information. We call this\nmethod \"SummaryMixing\". Introducing SummaryMixing in state-of-the-art ASR\nmodels makes it feasible to preserve or exceed previous speech recognition\nperformance while making training and inference up to 28% faster and reducing\nmemory use by half.", "published": "2023-07-12 12:51:23", "link": "http://arxiv.org/abs/2307.07421v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Detecting the Presence of COVID-19 Vaccination Hesitancy from South\n  African Twitter Data Using Machine Learning", "abstract": "Very few social media studies have been done on South African user-generated\ncontent during the COVID-19 pandemic and even fewer using hand-labelling over\nautomated methods. Vaccination is a major tool in the fight against the\npandemic, but vaccine hesitancy jeopardizes any public health effort. In this\nstudy, sentiment analysis on South African tweets related to vaccine hesitancy\nwas performed, with the aim of training AI-mediated classification models and\nassessing their reliability in categorizing UGC. A dataset of 30000 tweets from\nSouth Africa were extracted and hand-labelled into one of three sentiment\nclasses: positive, negative, neutral. The machine learning models used were\nLSTM, bi-LSTM, SVM, BERT-base-cased and the RoBERTa-base models, whereby their\nhyperparameters were carefully chosen and tuned using the WandB platform. We\nused two different approaches when we pre-processed our data for comparison:\none was semantics-based, while the other was corpus-based. The pre-processing\nof the tweets in our dataset was performed using both methods, respectively.\nAll models were found to have low F1-scores within a range of 45$\\%$-55$\\%$,\nexcept for BERT and RoBERTa which both achieved significantly better measures\nwith overall F1-scores of 60$\\%$ and 61$\\%$, respectively. Topic modelling\nusing an LDA was performed on the miss-classified tweets of the RoBERTa model\nto gain insight on how to further improve model accuracy.", "published": "2023-07-12 13:28:37", "link": "http://arxiv.org/abs/2307.15072v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with\n  Language Models", "abstract": "Large language models (LLMs) are shown to possess a wealth of actionable\nknowledge that can be extracted for robot manipulation in the form of reasoning\nand planning. Despite the progress, most still rely on pre-defined motion\nprimitives to carry out the physical interactions with the environment, which\nremains a major bottleneck. In this work, we aim to synthesize robot\ntrajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a\nlarge variety of manipulation tasks given an open-set of instructions and an\nopen-set of objects. We achieve this by first observing that LLMs excel at\ninferring affordances and constraints given a free-form language instruction.\nMore importantly, by leveraging their code-writing capabilities, they can\ninteract with a vision-language model (VLM) to compose 3D value maps to ground\nthe knowledge into the observation space of the agent. The composed value maps\nare then used in a model-based planning framework to zero-shot synthesize\nclosed-loop robot trajectories with robustness to dynamic perturbations. We\nfurther demonstrate how the proposed framework can benefit from online\nexperiences by efficiently learning a dynamics model for scenes that involve\ncontact-rich interactions. We present a large-scale study of the proposed\nmethod in both simulated and real-robot environments, showcasing the ability to\nperform a large variety of everyday manipulation tasks specified in free-form\nnatural language. Videos and code at https://voxposer.github.io", "published": "2023-07-12 07:40:48", "link": "http://arxiv.org/abs/2307.05973v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "No Train No Gain: Revisiting Efficient Training Algorithms For\n  Transformer-based Language Models", "abstract": "The computation necessary for training Transformer-based language models has\nskyrocketed in recent years. This trend has motivated research on efficient\ntraining algorithms designed to improve training, validation, and downstream\nperformance faster than standard training. In this work, we revisit three\ncategories of such algorithms: dynamic architectures (layer stacking, layer\ndropping), batch selection (selective backprop, RHO loss), and efficient\noptimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed\ncomputation budget using such methods, we find that their training, validation,\nand downstream gains vanish compared to a baseline with a fully-decayed\nlearning rate. We define an evaluation protocol that enables computation to be\ndone on arbitrary machines by mapping all computation time to a reference\nmachine which we call reference system time. We discuss the limitations of our\nproposed protocol and release our code to encourage rigorous research in\nefficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.", "published": "2023-07-12 20:10:14", "link": "http://arxiv.org/abs/2307.06440v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "cs.PF"], "primary_category": "cs.LG"}
{"title": "Language-Routing Mixture of Experts for Multilingual and Code-Switching\n  Speech Recognition", "abstract": "Multilingual speech recognition for both monolingual and code-switching\nspeech is a challenging task. Recently, based on the Mixture of Experts (MoE),\nmany works have made good progress in multilingual and code-switching ASR, but\npresent huge computational complexity with the increase of supported languages.\nIn this work, we propose a computation-efficient network named Language-Routing\nMixture of Experts (LR-MoE) for multilingual and code-switching ASR. LR-MoE\nextracts language-specific representations through the Mixture of Language\nExperts (MLE), which is guided to learn by a frame-wise language routing\nmechanism. The weight-shared frame-level language identification (LID) network\nis jointly trained as the shared pre-router of each MoE layer. Experiments show\nthat the proposed method significantly improves multilingual and code-switching\nspeech recognition performances over baseline with comparable computational\nefficiency.", "published": "2023-07-12 07:00:12", "link": "http://arxiv.org/abs/2307.05956v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can Large Language Models Aid in Annotating Speech Emotional Data?\n  Uncovering New Frontiers", "abstract": "Despite recent advancements in speech emotion recognition (SER) models,\nstate-of-the-art deep learning (DL) approaches face the challenge of the\nlimited availability of annotated data. Large language models (LLMs) have\nrevolutionised our understanding of natural language, introducing emergent\nproperties that broaden comprehension in language, speech, and vision. This\npaper examines the potential of LLMs to annotate abundant speech data, aiming\nto enhance the state-of-the-art in SER. We evaluate this capability across\nvarious settings using publicly available speech emotion classification\ndatasets. Leveraging ChatGPT, we experimentally demonstrate the promising role\nof LLMs in speech emotion data annotation. Our evaluation encompasses\nsingle-shot and few-shots scenarios, revealing performance variability in SER.\nNotably, we achieve improved results through data augmentation, incorporating\nChatGPT-annotated samples into existing datasets. Our work uncovers new\nfrontiers in speech emotion classification, highlighting the increasing\nsignificance of LLMs in this field moving forward.", "published": "2023-07-12 11:27:40", "link": "http://arxiv.org/abs/2307.06090v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Global birdsong embeddings enable superior transfer learning for\n  bioacoustic classification", "abstract": "Automated bioacoustic analysis aids understanding and protection of both\nmarine and terrestrial animals and their habitats across extensive\nspatiotemporal scales, and typically involves analyzing vast collections of\nacoustic data. With the advent of deep learning models, classification of\nimportant signals from these datasets has markedly improved. These models power\ncritical data analyses for research and decision-making in biodiversity\nmonitoring, animal behaviour studies, and natural resource management. However,\ndeep learning models are often data-hungry and require a significant amount of\nlabeled training data to perform well. While sufficient training data is\navailable for certain taxonomic groups (e.g., common bird species), many\nclasses (such as rare and endangered species, many non-bird taxa, and\ncall-type) lack enough data to train a robust model from scratch. This study\ninvestigates the utility of feature embeddings extracted from audio\nclassification models to identify bioacoustic classes other than the ones these\nmodels were originally trained on. We evaluate models on diverse datasets,\nincluding different bird calls and dialect types, bat calls, marine mammals\ncalls, and amphibians calls. The embeddings extracted from the models trained\non bird vocalization data consistently allowed higher quality classification\nthan the embeddings trained on general audio datasets. The results of this\nstudy indicate that high-quality feature embeddings from large-scale acoustic\nbird classifiers can be harnessed for few-shot transfer learning, enabling the\nlearning of new classes from a limited quantity of training data. Our findings\nreveal the potential for efficient analyses of novel bioacoustic tasks, even in\nscenarios where available training data is limited to a few samples.", "published": "2023-07-12 16:39:02", "link": "http://arxiv.org/abs/2307.06292v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Rhythm Modeling for Voice Conversion", "abstract": "Voice conversion aims to transform source speech into a different target\nvoice. However, typical voice conversion systems do not account for rhythm,\nwhich is an important factor in the perception of speaker identity. To bridge\nthis gap, we introduce Urhythmic-an unsupervised method for rhythm conversion\nthat does not require parallel data or text transcriptions. Using\nself-supervised representations, we first divide source audio into segments\napproximating sonorants, obstruents, and silences. Then we model rhythm by\nestimating speaking rate or the duration distribution of each segment type.\nFinally, we match the target speaking rate or rhythm by time-stretching the\nspeech segments. Experiments show that Urhythmic outperforms existing\nunsupervised methods in terms of quality and prosody. Code and checkpoints:\nhttps://github.com/bshall/urhythmic. Audio demo page:\nhttps://ubisoft-laforge.github.io/speech/urhythmic.", "published": "2023-07-12 09:35:16", "link": "http://arxiv.org/abs/2307.06040v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "B-CLEAN-SC: CLEAN-SC for broadband sources", "abstract": "This paper presents B-CLEAN-SC, a variation of CLEAN-SC for broadband\nsources. Opposed to CLEAN-SC, which ``deconvolves'' the beamforming map for\neach frequency individually, B-CLEAN-SC processes frequency intervals. Instead\nof performing a deconvolution iteration at the location of the maximum level,\nB-CLEAN-SC performs it at the location of the over-frequency-averaged maximum\nto improve the location estimation. The method is validated and compared to\nstandard CLEAN-SC on synthetic cases, and real-world experiments, for broad-\nand narrowband sources. It improves the source reconstruction at low and high\nfrequencies and suppresses noise, while it only increases the need for memory\nbut not computational effort.", "published": "2023-07-12 14:12:19", "link": "http://arxiv.org/abs/2307.06181v2", "categories": ["cs.SD", "eess.AS", "physics.flu-dyn"], "primary_category": "cs.SD"}
{"title": "Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event\n  Localization", "abstract": "Audio-Visual Event Localization (AVEL) is the task of temporally localizing\nand classifying \\emph{audio-visual events}, i.e., events simultaneously visible\nand audible in a video. In this paper, we solve AVEL in a weakly-supervised\nsetting, where only video-level event labels (their presence/absence, but not\ntheir locations in time) are available as supervision for training. Our idea is\nto use a base model to estimate labels on the training data at a finer temporal\nresolution than at the video level and re-train the model with these labels.\nI.e., we determine the subset of labels for each \\emph{slice} of frames in a\ntraining video by (i) replacing the frames outside the slice with those from a\nsecond video having no overlap in video-level labels, and (ii) feeding this\nsynthetic video into the base model to extract labels for just the slice in\nquestion. To handle the out-of-distribution nature of our synthetic videos, we\npropose an auxiliary objective for the base model that induces more reliable\npredictions of the localized event labels as desired. Our three-stage pipeline\noutperforms several existing AVEL methods with no architectural changes and\nimproves performance on a related weakly-supervised task as well.", "published": "2023-07-12 18:13:58", "link": "http://arxiv.org/abs/2307.06385v2", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
