{"title": "Learning Meta-Embeddings by Using Ensembles of Embedding Sets", "abstract": "Word embeddings -- distributed representations of words -- in deep learning\nare beneficial for many tasks in natural language processing (NLP). However,\ndifferent embedding sets vary greatly in quality and characteristics of the\ncaptured semantics. Instead of relying on a more advanced algorithm for\nembedding learning, this paper proposes an ensemble approach of combining\ndifferent public embedding sets with the aim of learning meta-embeddings.\nExperiments on word similarity and analogy tasks and on part-of-speech tagging\nshow better performance of meta-embeddings compared to individual embedding\nsets. One advantage of meta-embeddings is the increased vocabulary coverage. We\nwill release our meta-embeddings publicly.", "published": "2015-08-18 09:29:22", "link": "http://arxiv.org/abs/1508.04257v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probabilistic Modelling of Morphologically Rich Languages", "abstract": "This thesis investigates how the sub-structure of words can be accounted for\nin probabilistic models of language. Such models play an important role in\nnatural language processing tasks such as translation or speech recognition,\nbut often rely on the simplistic assumption that words are opaque symbols. This\nassumption does not fit morphologically complex language well, where words can\nhave rich internal structure and sub-word elements are shared across distinct\nword forms.\n  Our approach is to encode basic notions of morphology into the assumptions of\nthree different types of language models, with the intention that leveraging\nshared sub-word structure can improve model performance and help overcome data\nsparsity that arises from morphological processes.\n  In the context of n-gram language modelling, we formulate a new Bayesian\nmodel that relies on the decomposition of compound words to attain better\nsmoothing, and we develop a new distributed language model that learns vector\nrepresentations of morphemes and leverages them to link together\nmorphologically related words. In both cases, we show that accounting for word\nsub-structure improves the models' intrinsic performance and provides benefits\nwhen applied to other tasks, including machine translation.\n  We then shift the focus beyond the modelling of word sequences and consider\nmodels that automatically learn what the sub-word elements of a given language\nare, given an unannotated list of words. We formulate a novel model that can\nlearn discontiguous morphemes in addition to the more conventional contiguous\nmorphemes that most previous models are limited to. This approach is\ndemonstrated on Semitic languages, and we find that modelling discontiguous\nsub-word structures leads to improvements in the task of segmenting words into\ntheir contiguous morphemes.", "published": "2015-08-18 10:29:10", "link": "http://arxiv.org/abs/1508.04271v1", "categories": ["cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "End-to-End Attention-based Large Vocabulary Speech Recognition", "abstract": "Many of the current state-of-the-art Large Vocabulary Continuous Speech\nRecognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov\nModels (HMMs). Most of these systems contain separate components that deal with\nthe acoustic modelling, language modelling and sequence decoding. We\ninvestigate a more direct approach in which the HMM is replaced with a\nRecurrent Neural Network (RNN) that performs sequence prediction directly at\nthe character level. Alignment between the input features and the desired\ncharacter sequence is learned automatically by an attention mechanism built\ninto the RNN. For each predicted character, the attention mechanism scans the\ninput sequence and chooses relevant frames. We propose two methods to speed up\nthis operation: limiting the scan to a subset of most promising frames and\npooling over time the information contained in neighboring frames, thereby\nreducing source sequence length. Integrating an n-gram language model into the\ndecoding process yields recognition accuracies similar to other HMM-free\nRNN-based approaches.", "published": "2015-08-18 17:40:00", "link": "http://arxiv.org/abs/1508.04395v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
