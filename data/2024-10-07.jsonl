{"title": "Deep Learning Methods for S Shaped Utility Maximisation with a Random Reference Point", "abstract": "We consider the portfolio optimisation problem where the terminal function is\nan S-shaped utility applied at the difference between the wealth and a random\nbenchmark process. We develop several numerical methods for solving the problem\nusing deep learning and duality methods. We use deep learning methods to solve\nthe associated Hamilton-Jacobi-Bellman equation for both the primal and dual\nproblems, and the adjoint equation arising from the stochastic maximum\nprinciple. We compare the solution of this non-concave problem to that of\nconcavified utility, a random function depending on the benchmark, in both\ncomplete and incomplete markets. We give some numerical results for power and\nlog utilities to show the accuracy of the suggested algorithms.", "published": "2024-10-07 22:07:59", "link": "http://arxiv.org/abs/2410.05524v1", "categories": ["q-fin.CP", "math.OC", "stat.ML"], "primary_category": "q-fin.CP"}
{"title": "Temporal Relational Reasoning of Large Language Models for Detecting Stock Portfolio Crashes", "abstract": "Stock portfolios are often exposed to rare consequential events (e.g., 2007\nglobal financial crisis, 2020 COVID-19 stock market crash), as they do not have\nenough historical information to learn from. Large Language Models (LLMs) now\npresent a possible tool to tackle this problem, as they can generalize across\ntheir large corpus of training data and perform zero-shot reasoning on new\nevents, allowing them to detect possible portfolio crash events without\nrequiring specific training data. However, detecting portfolio crashes is a\ncomplex problem that requires more than basic reasoning abilities. Investors\nneed to dynamically process the impact of each new information found in the\nnews articles, analyze the the relational network of impacts across news events\nand portfolio stocks, as well as understand the temporal context between\nimpacts across time-steps, in order to obtain the overall aggregated effect on\nthe target portfolio. In this work, we propose an algorithmic framework named\nTemporal Relational Reasoning (TRR). It seeks to emulate the spectrum of human\ncognitive capabilities used for complex problem-solving, which include\nbrainstorming, memory, attention and reasoning. Through extensive experiments,\nwe show that TRR is able to outperform state-of-the-art solutions on detecting\nstock portfolio crashes, and demonstrate how each of the proposed components\nhelp to contribute to its performance through an ablation study. Additionally,\nwe further explore the possible applications of TRR by extending it to other\nrelated complex problems, such as the detection of possible global crisis\nevents in Macroeconomics.", "published": "2024-10-07 11:15:52", "link": "http://arxiv.org/abs/2410.17266v1", "categories": ["q-fin.RM", "cs.AI", "cs.CL", "cs.LG", "q-fin.CP"], "primary_category": "q-fin.RM"}
{"title": "Numerical analysis of American option pricing in a two-asset jump-diffusion model", "abstract": "This paper addresses a significant gap in rigorous numerical treatments for\npricing American options under correlated two-asset jump-diffusion models using\nthe viscosity solution approach, with a particular focus on the Merton model.\nThe pricing of these options is governed by complex two-dimensional (2-D)\nvariational inequalities that incorporate cross-derivative terms and nonlocal\nintegro-differential terms due to the presence of jumps. Existing numerical\nmethods, primarily based on finite differences, often struggle with preserving\nmonotonicity in the approximation of cross-derivatives-a key requirement for\nensuring convergence to the viscosity solution. In addition, these methods face\nchallenges in accurately discretizing 2-D jump integrals.\n  We introduce a novel approach to effectively tackle the aforementioned\nvariational inequalities, seamlessly managing cross-derivative terms and\nnonlocal integro-differential terms through an efficient and\nstraightforward-to-implement monotone integration scheme. Within each timestep,\nour approach explicitly tackles the variational inequality constraint,\nresulting in a 2-D Partial Integro-Differential Equation (PIDE) to solve. Its\nsolution is then expressed as a 2-D convolution integral involving the Green's\nfunction of the PIDE. We derive an infinite series representation of this\nGreen's function, where each term is strictly positive and computable. This\nseries facilitates the numerical approximation of the PIDE solution through a\nmonotone integration method, such as the composite quadrature rule.\n  The proposed method is demonstrated to be both $\\ell_{\\infty} $-stable and\nconsistent in the viscosity sense, ensuring its convergence to the viscosity\nsolution of the variational inequality. Extensive numerical results validate\nthe effectiveness and robustness of our approach, highlighting its practical\napplicability and theoretical soundness.", "published": "2024-10-07 04:29:46", "link": "http://arxiv.org/abs/2410.04745v2", "categories": ["q-fin.CP", "65R20, 91-08, 65D30, 65T50"], "primary_category": "q-fin.CP"}
{"title": "Hedging via Perpetual Derivatives: Trinomial Option Pricing and Implied Parameter Surface Analysis", "abstract": "We introduce a fairly general, recombining trinomial tree model in the\nnatural world. Market-completeness is ensured by considering a market\nconsisting of two risky assets, a riskless asset, and a European option. The\ntwo risky assets consist of a stock and a perpetual derivative of that stock.\nThe option has the stock and its derivative as its underlying. Using a\nreplicating portfolio, we develop prices for European options and generate the\nunique relationships between the risk-neutral and real-world parameters of the\nmodel. We discuss calibration of the model to empirical data in the cases in\nwhich the risky asset returns are treated as either arithmetic or logarithmic.\nFrom historical price and call option data for select large cap stocks, we\ndevelop implied parameter surfaces for the real-world parameters in the model.", "published": "2024-10-07 04:40:39", "link": "http://arxiv.org/abs/2410.04748v2", "categories": ["q-fin.MF", "q-fin.PR"], "primary_category": "q-fin.MF"}
{"title": "Functional Clustering of Discount Functions for Behavioral Investor Profiling", "abstract": "Classical finance models are based on the premise that investors act\nrationally and utilize all available information when making portfolio\ndecisions. However, these models often fail to capture the anomalies observed\nin intertemporal choices and decision-making under uncertainty, particularly\nwhen accounting for individual differences in preferences and consumption\npatterns. Such limitations hinder traditional finance theory's ability to\naddress key questions like: How do personal preferences shape investment\nchoices? What drives investor behaviour? And how do individuals select their\nportfolios? One prominent contribution is Pompian's model of four Behavioral\nInvestor Types (BITs), which links behavioural finance studies with Keirsey's\ntemperament theory, highlighting the role of personality in financial\ndecision-making. Yet, traditional parametric models struggle to capture how\nthese distinct temperaments influence intertemporal decisions, such as how\nindividuals evaluate trade-offs between present and future outcomes. To address\nthis gap, the present study employs Functional Data Analysis (FDA) to\nspecifically investigate temporal discounting behaviours revealing nuanced\npatterns in how different temperaments perceive and manage uncertainty over\ntime. Our findings show heterogeneity within each temperament, suggesting that\ninvestor profiles are far more diverse than previously thought. This refined\nclassification provides deeper insights into the role of temperament in shaping\nintertemporal financial decisions, offering practical implications for\nfinancial advisors to better tailor strategies to individual risk preferences\nand decision-making styles.", "published": "2024-10-07 12:30:37", "link": "http://arxiv.org/abs/2410.16307v1", "categories": ["q-fin.ST", "stat.AP", "stat.ME", "91G10, 91G80, 91B50, 62P20, 62H30, 91B06", "G.3; I.5.1; I.2.6; J.4; H.1.2"], "primary_category": "q-fin.ST"}
{"title": "Optimal execution with deterministically time varying liquidity: well posedness and price manipulation", "abstract": "We investigate the well-posedness in the Hadamard sense and the absence of\nprice manipulation in the optimal execution problem within the Almgren-Chriss\nframework, where the temporary and permanent impact parameters vary\ndeterministically over time. We present sufficient conditions for the existence\nof a unique solution and provide second-order conditions for the problem, with\na particular focus on scenarios where impact parameters change monotonically\nover time. Additionally, we establish conditions to prevent\ntransaction-triggered price manipulation in the optimal solution, i.e. the\noccurence of buying and selling in the same trading program. Our findings are\nsupported by numerical analyses that explore various regimes in simple\nparametric settings for the dynamics of impact parameters.", "published": "2024-10-07 09:33:30", "link": "http://arxiv.org/abs/2410.04867v1", "categories": ["math.OC", "q-fin.TR", "91G80"], "primary_category": "math.OC"}
{"title": "MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning\n  in LLMs", "abstract": "Recent large language models (LLMs) have demonstrated versatile capabilities\nin long-context scenarios. Although some recent benchmarks have been developed\nto evaluate the long-context capabilities of LLMs, there is a lack of\nbenchmarks evaluating the mathematical reasoning abilities of LLMs over long\ncontexts, which is crucial for LLMs' application in real-world scenarios. In\nthis paper, we introduce MathHay, an automated benchmark designed to assess the\nlong-context mathematical reasoning capabilities of LLMs. Unlike previous\nbenchmarks like Needle in a Haystack, which focus primarily on information\nretrieval within long texts, MathHay demands models with both\ninformation-seeking and complex mathematical reasoning abilities. We conduct\nextensive experiments on MathHay to assess the long-context mathematical\nreasoning abilities of eight top-performing LLMs. Even the best-performing\nmodel, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over\nlong contexts, achieving only 51.26% accuracy at 128K tokens. This highlights\nthe significant room for improvement on the MathHay benchmark.", "published": "2024-10-07 02:30:07", "link": "http://arxiv.org/abs/2410.04698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Forgetting Curve: A Reliable Method for Evaluating Memorization\n  Capability for Long-context Models", "abstract": "Numerous recent works target to extend effective context length for language\nmodels and various methods, tasks and benchmarks exist to measure model's\neffective memorization length. However, through thorough investigations, we\nfind limitations for currently existing evaluations on model's memorization\ncapability. We provide an extensive survey for limitations in this work and\npropose a new method called forgetting curve to measure the memorization\ncapability of long-context models. We show that forgetting curve has the\nadvantage of being robust to the tested corpus and the experimental settings,\nof not relying on prompts and can be applied to any model size.\n  We apply our forgetting curve to a large variety of models involving both\ntransformer and RNN/SSM based architectures. Our measurement provides empirical\nevidence for the effectiveness of transformer extension techniques while raises\nquestions for the effective length of RNN/SSM based models. We also examine the\ndifference between our measurement and existing benchmarks as well as popular\nmetrics for various models. Our code and results can be found at\nhttps://github.com/1azybug/ForgettingCurve.", "published": "2024-10-07 03:38:27", "link": "http://arxiv.org/abs/2410.04727v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient transformer with reinforced position embedding for language\n  models", "abstract": "In this paper, we propose an efficient transformer architecture that uses\nreinforced positional embedding to obtain superior performance with half the\nnumber of encoder decoder layers. We demonstrate that concatenating positional\nencoding with trainable token embeddings, normalizing columns in the token\nembedding matrix, and using the normalized token embedding matrix as the value\nof the attention layer improve the training and validation loss and the\ntraining time in an encoder-decoder Transformer model for a Portuguese-English\ntranslation task with 10 epochs or 12 hours of training across 10 trials. Our\nmethod, with roughly a threefold parameter reduction compared to the baseline\nmodel, yields a mean training loss of 1.21, a mean validation loss of 1.51, and\nan average training time of 1352.27 seconds per epoch, surpassing the baseline\nmodel with the same embedding dimension that employs addition of positional\nencoding and token embeddings, which achieves a mean training loss of 1.96, a\nvalidation loss of 2.18, and an average training time of 4297.79 seconds per\nepoch. Additionally, we evaluated our proposed architecture and the baseline\nacross 14 diverse translation datasets from TensorFlow. The results indicate\nthat our method consistently achieves lower or comparable training and\nvalidation losses, suggesting enhanced learning efficiency.", "published": "2024-10-07 03:47:34", "link": "http://arxiv.org/abs/2410.04731v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-level Causal Relation Extraction with Knowledge-guided Binary\n  Question Answering", "abstract": "As an essential task in information extraction (IE), Event-Event Causal\nRelation Extraction (ECRE) aims to identify and classify the causal\nrelationships between event mentions in natural language texts. However,\nexisting research on ECRE has highlighted two critical challenges, including\nthe lack of document-level modeling and causal hallucinations. In this paper,\nwe propose a Knowledge-guided binary Question Answering (KnowQA) method with\nevent structures for ECRE, consisting of two stages: Event Structure\nConstruction and Binary Question Answering. We conduct extensive experiments\nunder both zero-shot and fine-tuning settings with large language models (LLMs)\non the MECI and MAVEN-ERE datasets. Experimental results demonstrate the\nusefulness of event structures on document-level ECRE and the effectiveness of\nKnowQA by achieving state-of-the-art on the MECI dataset. We observe not only\nthe effectiveness but also the high generalizability and low inconsistency of\nour method, particularly when with complete event structures after fine-tuning\nthe models.", "published": "2024-10-07 05:07:48", "link": "http://arxiv.org/abs/2410.04752v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Formality is Favored: Unraveling the Learning Preferences of Large\n  Language Models on Data with Conflicting Knowledge", "abstract": "Having been trained on massive pretraining data, large language models have\nshown excellent performance on many knowledge-intensive tasks. However,\npretraining data tends to contain misleading and even conflicting information,\nand it is intriguing to understand how LLMs handle these noisy data during\ntraining. In this study, we systematically analyze LLMs' learning preferences\nfor data with conflicting knowledge. We find that pretrained LLMs establish\nlearning preferences similar to humans, i.e., preferences towards formal texts\nand texts with fewer spelling errors, resulting in faster learning and more\nfavorable treatment of knowledge in data with such features when facing\nconflicts. This finding is generalizable across models and languages and is\nmore evident in larger models. An in-depth analysis reveals that LLMs tend to\ntrust data with features that signify consistency with the majority of data,\nand it is possible to instill new preferences and erase old ones by\nmanipulating the degree of consistency with the majority data.", "published": "2024-10-07 06:49:41", "link": "http://arxiv.org/abs/2410.04784v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted\n  Graph for Long Document QA", "abstract": "In the past, Retrieval-Augmented Generation (RAG) methods split text into\nchunks to enable language models to handle long documents. Recent tree-based\nRAG methods are able to retrieve detailed information while preserving global\ncontext. However, with the advent of more powerful LLMs, such as Llama 3.1,\nwhich offer better comprehension and support for longer inputs, we found that\neven recent tree-based RAG methods perform worse than directly feeding the\nentire document into Llama 3.1, although RAG methods still hold an advantage in\nreducing computational costs. In this paper, we propose a new retrieval method,\ncalled LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph\n(GARLIC), which outperforms previous state-of-the-art baselines, including\nLlama 3.1, while retaining the computational efficiency of RAG methods. Our\nmethod introduces several improvements: (1) Rather than using a tree structure,\nwe construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many\nsummarization, where the graph edges are derived from attention mechanisms, and\neach node focuses on a single event or very few events. (2) We introduce a\nnovel retrieval method that leverages the attention weights of LLMs rather than\ndense embedding similarity. Our method allows for searching the graph along\nmultiple paths and can terminate at any depth. (3) We use the LLM to control\nthe retrieval process, enabling it to dynamically adjust the amount and depth\nof information retrieved for different queries. Experimental results show that\nour method outperforms previous state-of-the-art baselines, including Llama\n3.1, on two single-document and two multi-document QA datasets, while\nmaintaining similar computational complexity to traditional RAG methods.", "published": "2024-10-07 07:02:09", "link": "http://arxiv.org/abs/2410.04790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DAPE V2: Process Attention Score as Feature Map for Length Extrapolation", "abstract": "The attention mechanism is a fundamental component of the Transformer model,\ncontributing to interactions among distinct tokens, in contrast to earlier\nfeed-forward neural networks. In general, the attention scores are determined\nsimply by the key-query products. However, this work's occasional trial\n(combining DAPE and NoPE) of including additional MLPs on attention scores\nwithout position encoding indicates that the classical key-query multiplication\nmay limit the performance of Transformers. In this work, we conceptualize\nattention as a feature map and apply the convolution operator (for neighboring\nattention scores across different heads) to mimic the processing methods in\ncomputer vision. Specifically, the main contribution of this paper is\nidentifying and interpreting the Transformer length extrapolation problem as a\nresult of the limited expressiveness of the naive query and key dot product,\nand we successfully translate the length extrapolation issue into a\nwell-understood feature map processing problem. The novel insight, which can be\nadapted to various attention-related models, reveals that the current\nTransformer architecture has the potential for further evolution. Extensive\nexperiments demonstrate that treating attention as a feature map and applying\nconvolution as a processing method significantly enhances Transformer\nperformance.", "published": "2024-10-07 07:21:49", "link": "http://arxiv.org/abs/2410.04798v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LPZero: Language Model Zero-cost Proxy Search from Zero", "abstract": "In spite of the outstanding performance, Neural Architecture Search (NAS) is\ncriticized for massive computation. Recently, Zero-shot NAS has emerged as a\npromising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce\ncomputational demands. Despite this, existing ZC proxies heavily rely on expert\nknowledge and incur significant trial-and-error costs. Particularly in NLP\ntasks, most existing ZC proxies fail to surpass the performance of the naive\nbaseline. To address these challenges, we introduce a novel framework,\n\\textbf{LPZero}, which is the first to automatically design ZC proxies for\nvarious tasks, achieving higher ranking consistency than human-designed\nproxies. Specifically, we model the ZC proxy as a symbolic equation and\nincorporate a unified proxy search space that encompasses existing ZC proxies,\nwhich are composed of a predefined set of mathematical symbols. To\nheuristically search for the best ZC proxy, LPZero incorporates genetic\nprogramming to find the optimal symbolic composition. We propose a\n\\textit{Rule-based Pruning Strategy (RPS),} which preemptively eliminates\nunpromising proxies, thereby mitigating the risk of proxy degradation.\nExtensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's\nsuperior ranking ability and performance on downstream tasks compared to\ncurrent approaches.", "published": "2024-10-07 07:41:48", "link": "http://arxiv.org/abs/2410.04808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MINER: Mining the Underlying Pattern of Modality-Specific Neurons in\n  Multimodal Large Language Models", "abstract": "In recent years, multimodal large language models (MLLMs) have significantly\nadvanced, integrating more modalities into diverse applications. However, the\nlack of explainability remains a major barrier to their use in scenarios\nrequiring decision transparency. Current neuron-level explanation paradigms\nmainly focus on knowledge localization or language- and domain-specific\nanalyses, leaving the exploration of multimodality largely unaddressed. To\ntackle these challenges, we propose MINER, a transferable framework for mining\nmodality-specific neurons (MSNs) in MLLMs, which comprises four stages: (1)\nmodality separation, (2) importance score calculation, (3) importance score\naggregation, (4) modality-specific neuron selection. Extensive experiments\nacross six benchmarks and two representative MLLMs show that (I) deactivating\nONLY 2% of MSNs significantly reduces MLLMs performance (0.56 to 0.24 for\nQwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (II) different modalities mainly\nconverge in the lower layers, (III) MSNs influence how key information from\nvarious modalities converges to the last token, (IV) two intriguing phenomena\nworth further investigation, i.e., semantic probing and semantic telomeres. The\nsource code is available at this URL.", "published": "2024-10-07 08:13:16", "link": "http://arxiv.org/abs/2410.04819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative\n  Feedback Loss", "abstract": "Direct Preference Optimization (DPO) has emerged as a more computationally\nefficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\nProximal Policy Optimization (PPO), eliminating the need for reward models and\nonline sampling. Despite these benefits, DPO and its variants remain sensitive\nto hyper-parameters and prone to instability, particularly on mathematical\ndatasets. We argue that these issues arise from the unidirectional\nlikelihood-derivative negative feedback inherent in the log-likelihood loss\nfunction. To address this, we propose a novel LLM alignment loss that\nestablishes a stable Bidirectional Negative Feedback (BNF) during optimization.\nOur proposed BNF loss eliminates the need for pairwise contrastive losses and\ndoes not require any extra tunable hyper-parameters or pairwise preference\ndata, streamlining the alignment pipeline to be as simple as supervised\nfine-tuning. We conduct extensive experiments across two challenging QA\nbenchmarks and four reasoning benchmarks. The experimental results show that\nBNF achieves comparable performance to the best methods on QA benchmarks, while\nits performance decrease on the four reasoning benchmarks is significantly\nlower compared to the best methods, thus striking a better balance between\nvalue alignment and reasoning ability. In addition, we further validate the\nperformance of BNF on non-pairwise datasets, and conduct in-depth analysis of\nlog-likelihood and logit shifts across different preference optimization\nmethods.", "published": "2024-10-07 08:44:04", "link": "http://arxiv.org/abs/2410.04834v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation", "abstract": "Answer verification identifies correct solutions among candidates generated\nby large language models (LLMs). Current approaches typically train verifier\nmodels by labeling solutions as correct or incorrect based solely on whether\nthe final answer matches the gold answer. However, this approach neglects any\nflawed rationale in the solution yielding the correct answer, undermining the\nverifier's ability to distinguish between sound and flawed rationales. We\nempirically show that in StrategyQA, only 19% of LLM-generated solutions with\ncorrect answers have valid rationales, thus leading to an unreliable verifier.\nFurthermore, we demonstrate that training a verifier on valid rationales\nsignificantly improves its ability to distinguish valid and flawed rationale.\nTo make a better verifier without extra human supervision, we introduce REPS\n(Rationale Enhancement through Pairwise Selection), a method for selecting\nvalid rationales from candidates by iteratively applying pairwise\nself-evaluation using the same LLM that generates the solutions. Verifiers\ntrained on solutions selected by REPS outperform those trained using\nconventional training methods on three reasoning benchmarks (ARC-Challenge,\nDROP, and StrategyQA). Our results suggest that training reliable verifiers\nrequires ensuring the validity of rationales in addition to the correctness of\nthe final answers, which would be critical for models assisting humans in\nsolving complex reasoning tasks.", "published": "2024-10-07 08:53:00", "link": "http://arxiv.org/abs/2410.04838v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intent Classification for Bank Chatbots through LLM Fine-Tuning", "abstract": "This study evaluates the application of large language models (LLMs) for\nintent classification within a chatbot with predetermined responses designed\nfor banking industry websites. Specifically, the research examines the\neffectiveness of fine-tuning SlovakBERT compared to employing multilingual\ngenerative models, such as Llama 8b instruct and Gemma 7b instruct, in both\ntheir pre-trained and fine-tuned versions. The findings indicate that\nSlovakBERT outperforms the other models in terms of in-scope accuracy and\nout-of-scope false positive rate, establishing it as the benchmark for this\napplication.", "published": "2024-10-07 11:17:05", "link": "http://arxiv.org/abs/2410.04925v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "On the Rigour of Scientific Writing: Criteria, Analysis, and Insights", "abstract": "Rigour is crucial for scientific research as it ensures the reproducibility\nand validity of results and findings. Despite its importance, little work\nexists on modelling rigour computationally, and there is a lack of analysis on\nwhether these criteria can effectively signal or measure the rigour of\nscientific papers in practice. In this paper, we introduce a bottom-up,\ndata-driven framework to automatically identify and define rigour criteria and\nassess their relevance in scientific writing. Our framework includes rigour\nkeyword extraction, detailed rigour definition generation, and salient criteria\nidentification. Furthermore, our framework is domain-agnostic and can be\ntailored to the evaluation of scientific rigour for different areas,\naccommodating the distinct salient criteria across fields. We conducted\ncomprehensive experiments based on datasets collected from two high impact\nvenues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the\neffectiveness of our framework in modelling rigour. In addition, we analyse\nlinguistic patterns of rigour, revealing that framing certainty is crucial for\nenhancing the perception of scientific rigour, while suggestion certainty and\nprobability uncertainty diminish it.", "published": "2024-10-07 12:22:06", "link": "http://arxiv.org/abs/2410.04981v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness", "abstract": "Accurately modeling the relationships between skills is a crucial part of\nhuman resources processes such as recruitment and employee development. Yet, no\nbenchmarks exist to evaluate such methods directly. We construct and release\nSkillMatch, a benchmark for the task of skill relatedness, based on expert\nknowledge mining from millions of job ads. Additionally, we propose a scalable\nself-supervised learning technique to adapt a Sentence-BERT model based on\nskill co-occurrence in job ads. This new method greatly surpasses traditional\nmodels for skill relatedness as measured on SkillMatch. By releasing SkillMatch\npublicly, we aim to contribute a foundation for research towards increased\naccuracy and transparency of skill-based recommendation systems.", "published": "2024-10-07 13:05:26", "link": "http://arxiv.org/abs/2410.05006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A test suite of prompt injection attacks for LLM-based machine\n  translation", "abstract": "LLM-based NLP systems typically work by embedding their input data into\nprompt templates which contain instructions and/or in-context examples,\ncreating queries which are submitted to a LLM, and then parsing the LLM\nresponse in order to generate the system outputs. Prompt Injection Attacks\n(PIAs) are a type of subversion of these systems where a malicious user crafts\nspecial inputs which interfere with the prompt templates, causing the LLM to\nrespond in ways unintended by the system designer.\n  Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based\nmachine translation. Specifically, the task is to translate questions from the\nTruthfulQA test suite, where an adversarial prompt is prepended to the\nquestions, instructing the system to ignore the translation instruction and\nanswer the questions instead.\n  In this test suite, we extend this approach to all the language pairs of the\nWMT 2024 General Machine Translation task. Moreover, we include additional\nattack formats in addition to the one originally studied.", "published": "2024-10-07 14:01:20", "link": "http://arxiv.org/abs/2410.05047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Initialization of Large Language Models via Reparameterization to\n  Mitigate Loss Spikes", "abstract": "Loss spikes, a phenomenon in which the loss value diverges suddenly, is a\nfundamental issue in the pre-training of large language models. This paper\nsupposes that the non-uniformity of the norm of the parameters is one of the\ncauses of loss spikes. Here, in training of neural networks, the scale of the\ngradients is required to be kept constant throughout the layers to avoid the\nvanishing and exploding gradients problem. However, to meet these requirements\nin the Transformer model, the norm of the model parameters must be non-uniform,\nand thus, parameters whose norm is smaller are more sensitive to the parameter\nupdate. To address this issue, we propose a novel technique, weight scaling as\nreparameterization (WeSaR). WeSaR introduces a gate parameter per parameter\nmatrix and adjusts it to the value satisfying the requirements. Because of the\ngate parameter, WeSaR sets the norm of the original parameters uniformly, which\nresults in stable training. Experimental results with the Transformer decoders\nconsisting of 130 million, 1.3 billion, and 13 billion parameters showed that\nWeSaR stabilizes and accelerates training and that it outperformed compared\nmethods including popular initialization methods.", "published": "2024-10-07 14:09:58", "link": "http://arxiv.org/abs/2410.05052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense\n  Question Answering", "abstract": "Current Large Language Models (LLMs) have shown strong reasoning capabilities\nin commonsense question answering benchmarks, but the process underlying their\nsuccess remains largely opaque. As a consequence, recent approaches have\nequipped LLMs with mechanisms for knowledge retrieval, reasoning and\nintrospection, not only to improve their capabilities but also to enhance the\ninterpretability of their outputs. However, these methods require additional\ntraining, hand-crafted templates or human-written explanations. To address\nthese issues, we introduce ZEBRA, a zero-shot question answering framework that\ncombines retrieval, case-based reasoning and introspection and dispenses with\nthe need for additional training of the LLM. Given an input question, ZEBRA\nretrieves relevant question-knowledge pairs from a knowledge base and generates\nnew knowledge by reasoning over the relationships in these pairs. This\ngenerated knowledge is then used to answer the input question, improving the\nmodel's performance and interpretability. We evaluate our approach across 8\nwell-established commonsense reasoning benchmarks, demonstrating that ZEBRA\nconsistently outperforms strong LLMs and previous knowledge integration\napproaches, achieving an average accuracy improvement of up to 4.5 points.", "published": "2024-10-07 14:31:43", "link": "http://arxiv.org/abs/2410.05077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explanation sensitivity to the randomness of large language models: the\n  case of journalistic text classification", "abstract": "Large language models (LLMs) perform very well in several natural language\nprocessing tasks but raise explainability challenges. In this paper, we examine\nthe effect of random elements in the training of LLMs on the explainability of\ntheir predictions. We do so on a task of opinionated journalistic text\nclassification in French. Using a fine-tuned CamemBERT model and an explanation\nmethod based on relevance propagation, we find that training with different\nrandom seeds produces models with similar accuracy but variable explanations.\nWe therefore claim that characterizing the explanations' statistical\ndistribution is needed for the explainability of LLMs. We then explore a\nsimpler model based on textual features which offers stable explanations but is\nless accurate. Hence, this simpler model corresponds to a different tradeoff\nbetween accuracy and explainability. We show that it can be improved by\ninserting features derived from CamemBERT's explanations. We finally discuss\nnew research directions suggested by our results, in particular regarding the\norigin of the sensitivity observed in the training randomness.", "published": "2024-10-07 14:39:45", "link": "http://arxiv.org/abs/2410.05085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating large language models for their competence in extracting\n  grammatically sound sentences from transcribed noisy utterances", "abstract": "Selectively processing noisy utterances while effectively disregarding\nspeech-specific elements poses no considerable challenge for humans, as they\nexhibit remarkable cognitive abilities to separate semantically significant\ncontent from speech-specific noise (i.e. filled pauses, disfluencies, and\nrestarts). These abilities may be driven by mechanisms based on acquired\ngrammatical rules that compose abstract syntactic-semantic structures within\nutterances. Segments without syntactic and semantic significance are\nconsistently disregarded in these structures. The structures, in tandem with\nlexis, likely underpin language comprehension and thus facilitate effective\ncommunication. In our study, grounded in linguistically motivated experiments,\nwe investigate whether large language models (LLMs) can effectively perform\nanalogical speech comprehension tasks. In particular, we examine the ability of\nLLMs to extract well-structured utterances from transcriptions of noisy\ndialogues. We conduct two evaluation experiments in the Polish language\nscenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of\ndata contamination. Our results show that not all extracted utterances are\ncorrectly structured, indicating that either LLMs do not fully acquire\nsyntactic-semantic rules or they acquire them but cannot apply them\neffectively. We conclude that the ability of LLMs to comprehend noisy\nutterances is still relatively superficial compared to human proficiency in\nprocessing them.", "published": "2024-10-07 14:55:20", "link": "http://arxiv.org/abs/2410.05099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deciphering the Interplay of Parametric and Non-parametric Memory in\n  Retrieval-augmented Language Models", "abstract": "Generative language models often struggle with specialized or less-discussed\nknowledge. A potential solution is found in Retrieval-Augmented Generation\n(RAG) models which act like retrieving information before generating responses.\nIn this study, we explore how the \\textsc{Atlas} approach, a RAG model, decides\nbetween what it already knows (parametric) and what it retrieves\n(non-parametric). We use causal mediation analysis and controlled experiments\nto examine how internal representations influence information processing. Our\nfindings disentangle the effects of parametric knowledge and the retrieved\ncontext. They indicate that in cases where the model can choose between both\ntypes of information (parametric and non-parametric), it relies more on the\ncontext than the parametric knowledge. Furthermore, the analysis investigates\nthe computations involved in \\emph{how} the model uses the information from the\ncontext. We find that multiple mechanisms are active within the model and can\nbe detected with mediation analysis: first, the decision of \\emph{whether the\ncontext is relevant}, and second, how the encoder computes output\nrepresentations to support copying when relevant.", "published": "2024-10-07 16:14:47", "link": "http://arxiv.org/abs/2410.05162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation", "abstract": "Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field.", "published": "2024-10-07 16:25:39", "link": "http://arxiv.org/abs/2410.05168v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating the Risk of Health Inequity Exacerbated by Large Language\n  Models", "abstract": "Recent advancements in large language models have demonstrated their\npotential in numerous medical applications, particularly in automating clinical\ntrial matching for translational research and enhancing medical question\nanswering for clinical decision support. However, our study shows that\nincorporating non decisive sociodemographic factors such as race, sex, income\nlevel, LGBT+ status, homelessness, illiteracy, disability, and unemployment\ninto the input of LLMs can lead to incorrect and harmful outputs for these\npopulations. These discrepancies risk exacerbating existing health disparities\nif LLMs are widely adopted in healthcare. To address this issue, we introduce\nEquityGuard, a novel framework designed to detect and mitigate the risk of\nhealth inequities in LLM based medical applications. Our evaluation\ndemonstrates its efficacy in promoting equitable outcomes across diverse\npopulations.", "published": "2024-10-07 16:40:21", "link": "http://arxiv.org/abs/2410.05180v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References", "abstract": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing text generation\nquality in a wide range of tasks. However, there still remains a reliability\ngap between LLM-as-a-Judge and human evaluation. One important reason is the\nlack of guided oracles in the evaluation process. Motivated by the role of\nreference pervasively used in classic text evaluation, we introduce RevisEval,\na novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.", "published": "2024-10-07 16:50:47", "link": "http://arxiv.org/abs/2410.05193v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TurtleBench: Evaluating Top Language Models via Real-World Yes/No\n  Puzzles", "abstract": "As the application of Large Language Models (LLMs) expands, the demand for\nreliable evaluations increases. Existing LLM evaluation benchmarks primarily\nrely on static datasets, making it challenging to assess model performance in\ndynamic interactions with users. Moreover, these benchmarks often depend on\nspecific background knowledge, complicating the measurement of a model's\nlogical reasoning capabilities. Other dynamic evaluation methods based on\nstrong models or manual efforts may introduce biases and incur high costs and\ntime demands, hindering large-scale application. To address these issues, we\npropose TurtleBench. TurtleBench collects real user guesses from our online\nTurtle Soup Puzzle platform that we developed. This approach allows for the\nrelatively dynamic generation of evaluation datasets, mitigating the risk of\nmodel cheating while aligning assessments more closely with genuine user needs\nfor reasoning capabilities, thus enhancing the reliability of evaluations.\nTurtleBench includes 1,532 user guesses along with the correctness of guesses\nafter annotation. Using this dataset, we thoroughly evaluated nine of the most\nadvanced LLMs available today. Notably, the OpenAI o1 series models did not\nachieve leading results in these evaluations. We propose several hypotheses for\nfurther research, such as \"the latent reasoning of o1 utilizes trivial\nChain-of-Thought (CoT) techniques\" and \"increasing CoT length not only provides\nreasoning benefits but also incurs noise costs.\"", "published": "2024-10-07 17:58:47", "link": "http://arxiv.org/abs/2410.05262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural machine translation system for Lezgian, Russian and Azerbaijani\n  languages", "abstract": "We release the first neural machine translation system for translation\nbetween Russian, Azerbaijani and the endangered Lezgian languages, as well as\nmonolingual and parallel datasets collected and aligned for training and\nevaluating the system. Multiple experiments are conducted to identify how\ndifferent sets of training language pairs and data domains can influence the\nresulting translation quality. We achieve BLEU scores of 26.14 for\nLezgian-Azerbaijani, 22.89 for Azerbaijani-Lezgian, 29.48 for Lezgian-Russian\nand 24.25 for Russian-Lezgian pairs. The quality of zero-shot translation is\nassessed on a Large Language Model, showing its high level of fluency in\nLezgian. However, the model often refuses to translate, justifying itself with\nits incompetence. We contribute our translation model along with the collected\nparallel and monolingual corpora and sentence encoder for the Lezgian language.", "published": "2024-10-07 20:08:10", "link": "http://arxiv.org/abs/2410.05472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-rationalization improves LLM as a fine-grained judge", "abstract": "LLM-as-a-judge models have been used for evaluating both human and AI\ngenerated content, specifically by providing scores and rationales. Rationales,\nin addition to increasing transparency, help models learn to calibrate its\njudgments. Enhancing a model's rationale can therefore improve its calibration\nabilities and ultimately the ability to score content. We introduce\nSelf-Rationalization, an iterative process of improving the rationales for the\njudge models, which consequently improves the score for fine-grained\ncustomizable scoring criteria (i.e., likert-scale scoring with arbitrary\nevaluation criteria). Self-rationalization works by having the model generate\nmultiple judgments with rationales for the same input, curating a preference\npair dataset from its own judgements, and iteratively fine-tuning the judge via\nDPO. Intuitively, this approach allows the judge model to self-improve by\nlearning from its own rationales, leading to better alignment and evaluation\naccuracy. After just two iterations -- while only relying on examples in the\ntraining set -- human evaluation shows that our judge model learns to produce\nhigher quality rationales, with a win rate of $62\\%$ on average compared to\nmodels just trained via SFT on rationale . This judge model also achieves high\nscoring accuracy on BigGen Bench and Reward Bench, outperforming even bigger\nsized models trained using SFT with rationale, self-consistency or best-of-$N$\nsampling by $3\\%$ to $9\\%$.", "published": "2024-10-07 21:05:53", "link": "http://arxiv.org/abs/2410.05495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attribute Controlled Fine-tuning for Large Language Models: A Case Study\n  on Detoxification", "abstract": "We propose a constraint learning schema for fine-tuning Large Language Models\n(LLMs) with attribute control. Given a training corpus and control criteria\nformulated as a sequence-level constraint on model outputs, our method\nfine-tunes the LLM on the training corpus while enhancing constraint\nsatisfaction with minimal impact on its utility and generation quality.\nSpecifically, our approach regularizes the LLM training by penalizing the KL\ndivergence between the desired output distribution, which satisfies the\nconstraints, and the LLM's posterior. This regularization term can be\napproximated by an auxiliary model trained to decompose the sequence-level\nconstraints into token-level guidance, allowing the term to be measured by a\nclosed-form formulation. To further improve efficiency, we design a parallel\nscheme for concurrently updating both the LLM and the auxiliary model. We\nevaluate the empirical performance of our approach by controlling the toxicity\nwhen training an LLM. We show that our approach leads to an LLM that produces\nfewer inappropriate responses while achieving competitive performance on\nbenchmarks and a toxicity detection task.", "published": "2024-10-07 23:38:58", "link": "http://arxiv.org/abs/2410.05559v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locally Measuring Cross-lingual Lexical Alignment: A Domain and Word\n  Level Perspective", "abstract": "NLP research on aligning lexical representation spaces to one another has so\nfar focused on aligning language spaces in their entirety. However, cognitive\nscience has long focused on a local perspective, investigating whether\ntranslation equivalents truly share the same meaning or the extent that\ncultural and regional influences result in meaning variations. With recent\ntechnological advances and the increasing amounts of available data, the\nlongstanding question of cross-lingual lexical alignment can now be approached\nin a more data-driven manner. However, developing metrics for the task requires\nsome methodology for comparing metric efficacy. We address this gap and present\na methodology for analyzing both synthetic validations and a novel naturalistic\nvalidation using lexical gaps in the kinship domain. We further propose new\nmetrics, hitherto unexplored on this task, based on contextualized embeddings.\nOur analysis spans 16 diverse languages, demonstrating that there is\nsubstantial room for improvement with the use of newer language models. Our\nresearch paves the way for more accurate and nuanced cross-lingual lexical\nalignment methodologies and evaluation.", "published": "2024-10-07 16:37:32", "link": "http://arxiv.org/abs/2410.07239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translation Canvas: An Explainable Interface to Pinpoint and Analyze\n  Translation Systems", "abstract": "With the rapid advancement of machine translation research, evaluation\ntoolkits have become essential for benchmarking system progress. Tools like\nCOMET and SacreBLEU offer single quality score assessments that are effective\nfor pairwise system comparisons. However, these tools provide limited insights\nfor fine-grained system-level comparisons and the analysis of instance-level\ndefects. To address these limitations, we introduce Translation Canvas, an\nexplainable interface designed to pinpoint and analyze translation systems'\nperformance: 1) Translation Canvas assists machine translation researchers in\ncomprehending system-level model performance by identifying common errors\n(their frequency and severity) and analyzing relationships between different\nsystems based on various evaluation metrics. 2) It supports fine-grained\nanalysis by highlighting error spans with explanations and selectively\ndisplaying systems' predictions. According to human evaluation, Translation\nCanvas demonstrates superior performance over COMET and SacreBLEU packages\nunder enjoyability and understandability criteria.", "published": "2024-10-07 16:54:18", "link": "http://arxiv.org/abs/2410.10861v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deeper Insights Without Updates: The Power of In-Context Learning Over\n  Fine-Tuning", "abstract": "Fine-tuning and in-context learning (ICL) are two prevalent methods in\nimbuing large language models with task-specific knowledge. It is commonly\nbelieved that fine-tuning can surpass ICL given sufficient training samples as\nit allows the model to adjust its internal parameters based on the data.\nHowever, this paper presents a counterintuitive finding: For tasks with\nimplicit patterns, ICL captures these patterns significantly better than\nfine-tuning. We developed several datasets featuring implicit patterns, such as\nsequences determining answers through parity or identifying reducible terms in\ncalculations. We then evaluated the models' understanding of these patterns\nunder both fine-tuning and ICL across models ranging from 0.5B to 7B\nparameters. The results indicate that models employing ICL can quickly grasp\ndeep patterns and significantly improve accuracy. In contrast, fine-tuning,\ndespite utilizing thousands of times more training samples than ICL, achieved\nonly limited improvements. We also proposed circuit shift theory from a\nmechanistic interpretability's view to explain why ICL wins.", "published": "2024-10-07 02:12:22", "link": "http://arxiv.org/abs/2410.04691v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being\n  Influenced By Them Instead?", "abstract": "Large Language Models (LLMs) have shown capabilities close to human\nperformance in various analytical tasks, leading researchers to use them for\ntime and labor-intensive analyses. However, their capability to handle highly\nspecialized and open-ended tasks in domains like policy studies remains in\nquestion. This paper investigates the efficiency and accuracy of LLMs in\nspecialized tasks through a structured user study focusing on Human-LLM\npartnership. The study, conducted in two stages-Topic Discovery and Topic\nAssignment-integrates LLMs with expert annotators to observe the impact of LLM\nsuggestions on what is usually human-only analysis. Results indicate that\nLLM-generated topic lists have significant overlap with human generated topic\nlists, with minor hiccups in missing document-specific topics. However, LLM\nsuggestions may significantly improve task completion speed, but at the same\ntime introduce anchoring bias, potentially affecting the depth and nuance of\nthe analysis, raising a critical question about the trade-off between increased\nefficiency and the risk of biased analysis.", "published": "2024-10-07 02:30:18", "link": "http://arxiv.org/abs/2410.04699v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Intriguing Properties of Large Language and Vision Models", "abstract": "Recently, large language and vision models (LLVMs) have received significant\nattention and development efforts due to their remarkable generalization\nperformance across a wide range of tasks requiring perception and cognitive\nabilities. A key factor behind their success is their simple architecture,\nwhich consists of a vision encoder, a projector, and a large language model\n(LLM). Despite their achievements in advanced reasoning tasks, their\nperformance on fundamental perception-related tasks (e.g., MMVP) remains\nsurprisingly low. This discrepancy raises the question of how LLVMs truly\nperceive images and exploit the advantages of the vision encoder. To address\nthis, we systematically investigate this question regarding several aspects:\npermutation invariance, robustness, math reasoning, alignment preserving and\nimportance, by evaluating the most common LLVM's families (i.e., LLaVA) across\n10 evaluation benchmarks. Our extensive experiments reveal several intriguing\nproperties of current LLVMs: (1) they internally process the image in a global\nmanner, even when the order of visual patch sequences is randomly permuted; (2)\nthey are sometimes able to solve math problems without fully perceiving\ndetailed numerical information; (3) the cross-modal alignment is overfitted to\ncomplex reasoning tasks, thereby, causing them to lose some of the original\nperceptual capabilities of their vision encoder; (4) the representation space\nin the lower layers (<25%) plays a crucial role in determining performance and\nenhancing visual understanding. Lastly, based on the above observations, we\nsuggest potential future directions for building better LLVMs and constructing\nmore challenging evaluation benchmarks.", "published": "2024-10-07 05:07:01", "link": "http://arxiv.org/abs/2410.04751v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Representing the Under-Represented: Cultural and Core Capability\n  Benchmarks for Developing Thai Large Language Models", "abstract": "The rapid advancement of large language models (LLMs) has highlighted the\nneed for robust evaluation frameworks that assess their core capabilities, such\nas reasoning, knowledge, and commonsense, leading to the inception of certain\nwidely-used benchmark suites such as the H6 benchmark. However, these benchmark\nsuites are primarily built for the English language, and there exists a lack\nthereof for under-represented languages, in terms of LLM development, such as\nThai. On the other hand, developing LLMs for Thai should also include enhancing\nthe cultural understanding as well as core capabilities. To address these dual\nchallenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai\nCultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough\nevaluation of various LLMs with multi-lingual capabilities, we provide a\ncomprehensive analysis of the proposed benchmarks and how they contribute to\nThai LLM development. Furthermore, we will make both the datasets and\nevaluation code publicly available to encourage further research and\ndevelopment for Thai LLMs.", "published": "2024-10-07 07:14:37", "link": "http://arxiv.org/abs/2410.04795v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Grammar Induction for Language Understanding and Generation", "abstract": "Grammar induction has made significant progress in recent years. However, it\nis not clear how the application of induced grammar could enhance practical\nperformance in downstream tasks. In this work, we introduce an unsupervised\ngrammar induction method for language understanding and generation. We\nconstruct a grammar parser to induce constituency structures and dependency\nrelations, which is simultaneously trained on downstream tasks without\nadditional syntax annotations. The induced grammar features are subsequently\nincorporated into Transformer as a syntactic mask to guide self-attention. We\nevaluate and apply our method to multiple machine translation tasks and natural\nlanguage understanding tasks. Our method demonstrates superior performance\ncompared to the original Transformer and other models enhanced with external\nparsers. Experimental results indicate that our method is effective in both\nfrom-scratch and pre-trained scenarios. Additionally, our research highlights\nthe contribution of explicitly modeling the grammatical structure of texts to\nneural network models.", "published": "2024-10-07 09:57:59", "link": "http://arxiv.org/abs/2410.04878v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Activation Scaling for Steering and Interpreting Language Models", "abstract": "Given the prompt \"Rome is in\", can we steer a language model to flip its\nprediction of an incorrect token \"France\" to a correct token \"Italy\" by only\nmultiplying a few relevant activation vectors with scalars? We argue that\nsuccessfully intervening on a model is a prerequisite for interpreting its\ninternal workings. Concretely, we establish a three-term objective: a\nsuccessful intervention should flip the correct with the wrong token and vice\nversa (effectiveness), and leave other tokens unaffected (faithfulness), all\nwhile being sparse (minimality). Using gradient-based optimization, this\nobjective lets us learn (and later evaluate) a specific kind of efficient and\ninterpretable intervention: activation scaling only modifies the signed\nmagnitude of activation vectors to strengthen, weaken, or reverse the steering\ndirections already encoded in the model. On synthetic tasks, this intervention\nperforms comparably with steering vectors in terms of effectiveness and\nfaithfulness, but is much more minimal allowing us to pinpoint interpretable\nmodel components. We evaluate activation scaling from different angles, compare\nperformance on different datasets, and make activation scalars a learnable\nfunction of the activation vectors themselves to generalize to varying-length\nprompts.", "published": "2024-10-07 12:01:32", "link": "http://arxiv.org/abs/2410.04962v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Biased Assessment of Expert Finding Systems", "abstract": "In large organisations, identifying experts on a given topic is crucial in\nleveraging the internal knowledge spread across teams and departments.\nSo-called enterprise expert retrieval systems automatically discover and\nstructure employees' expertise based on the vast amount of heterogeneous data\navailable about them and the work they perform. Evaluating these systems\nrequires comprehensive ground truth expert annotations, which are hard to\nobtain. Therefore, the annotation process typically relies on automated\nrecommendations of knowledge areas to validate. This case study provides an\nanalysis of how these recommendations can impact the evaluation of expert\nfinding systems. We demonstrate on a popular benchmark that system-validated\nannotations lead to overestimated performance of traditional term-based\nretrieval models and even invalidate comparisons with more recent neural\nmethods. We also augment knowledge areas with synonyms to uncover a strong bias\ntowards literal mentions of their constituent words. Finally, we propose\nconstraints to the annotation process to prevent these biased evaluations, and\nshow that this still allows annotation suggestions of high utility. These\nfindings should inform benchmark creation or selection for expert finding, to\nguarantee meaningful comparison of methods.", "published": "2024-10-07 13:19:08", "link": "http://arxiv.org/abs/2410.05018v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "DEPT: Decoupled Embeddings for Pre-training Language Models", "abstract": "Language Model pre-training uses broad data mixtures to enhance performance\nacross domains and languages. However, training on such heterogeneous text\ncorpora requires extensive and expensive efforts. Since these data sources vary\nsignificantly in lexical, syntactic, and semantic aspects, they cause negative\ninterference or the ``curse of multilinguality''. To address these challenges\nwe propose a communication-efficient pre-training framework, DEPT. Our method\ndecouples embeddings from the transformer body while simultaneously training\nthe latter on multiple data sources without requiring a shared vocabulary. DEPT\ncan: (1) train robustly and effectively under significant data heterogeneity,\n(2) minimize token embedding parameters to only what the data source vocabulary\nrequires, while cutting communication costs in direct proportion to both the\ncommunication frequency and the reduction in parameters, (3) enhance\ntransformer body plasticity and generalization, improving both average\nperplexity (up to 20%) and downstream task performance, and (4) enable training\nwith custom optimized vocabularies per data source. We demonstrate DEPT's\npotential via the first vocabulary-agnostic federated pre-training of\nbillion-scale models, reducing communication costs by orders of magnitude and\nembedding memory by 4-5x.", "published": "2024-10-07 13:24:24", "link": "http://arxiv.org/abs/2410.05021v5", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Named Clinical Entity Recognition Benchmark", "abstract": "This technical report introduces a Named Clinical Entity Recognition\nBenchmark for evaluating language models in healthcare, addressing the crucial\nnatural language processing (NLP) task of extracting structured information\nfrom clinical narratives to support applications like automated coding,\nclinical trial cohort identification, and clinical decision support.\n  The leaderboard provides a standardized platform for assessing diverse\nlanguage models, including encoder and decoder architectures, on their ability\nto identify and classify clinical entities across multiple medical domains. A\ncurated collection of openly available clinical datasets is utilized,\nencompassing entities such as diseases, symptoms, medications, procedures, and\nlaboratory measurements. Importantly, these entities are standardized according\nto the Observational Medical Outcomes Partnership (OMOP) Common Data Model,\nensuring consistency and interoperability across different healthcare systems\nand datasets, and a comprehensive evaluation of model performance. Performance\nof models is primarily assessed using the F1-score, and it is complemented by\nvarious assessment modes to provide comprehensive insights into model\nperformance. The report also includes a brief analysis of models evaluated to\ndate, highlighting observed trends and limitations.\n  By establishing this benchmarking framework, the leaderboard aims to promote\ntransparency, facilitate comparative analyses, and drive innovation in clinical\nentity recognition tasks, addressing the need for robust evaluation methods in\nhealthcare NLP.", "published": "2024-10-07 14:00:18", "link": "http://arxiv.org/abs/2410.05046v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Inference for Large Language Model-based Generative\n  Recommendation", "abstract": "Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5x speedup under relaxed\nsampling verification. The codes and datasets are released at\nhttps://github.com/Linxyhaha/AtSpeed.", "published": "2024-10-07 16:23:36", "link": "http://arxiv.org/abs/2410.05165v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Beyond Correlation: Interpretable Evaluation of Machine Translation\n  Metrics", "abstract": "Machine Translation (MT) evaluation metrics assess translation quality\nautomatically. Recently, researchers have employed MT metrics for various new\nuse cases, such as data filtering and translation re-ranking. However, most MT\nmetrics return assessments as scalar scores that are difficult to interpret,\nposing a challenge to making informed design choices. Moreover, MT metrics'\ncapabilities have historically been evaluated using correlation with human\njudgment, which, despite its efficacy, falls short of providing intuitive\ninsights into metric performance, especially in terms of new metric use cases.\nTo address these issues, we introduce an interpretable evaluation framework for\nMT metrics. Within this framework, we evaluate metrics in two scenarios that\nserve as proxies for the data filtering and translation re-ranking use cases.\nFurthermore, by measuring the performance of MT metrics using Precision,\nRecall, and F-score, we offer clearer insights into their capabilities than\ncorrelation with human judgments. Finally, we raise concerns regarding the\nreliability of manually curated data following the Direct Assessments+Scalar\nQuality Metrics (DA+SQM) guidelines, reporting a notably low agreement with\nMultidimensional Quality Metrics (MQM) annotations.", "published": "2024-10-07 16:42:10", "link": "http://arxiv.org/abs/2410.05183v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Studying and Mitigating Biases in Sign Language Understanding Models", "abstract": "Ensuring that the benefits of sign language technologies are distributed\nequitably among all community members is crucial. Thus, it is important to\naddress potential biases and inequities that may arise from the design or use\nof these resources. Crowd-sourced sign language datasets, such as the ASL\nCitizen dataset, are great resources for improving accessibility and preserving\nlinguistic diversity, but they must be used thoughtfully to avoid reinforcing\nexisting biases.\n  In this work, we utilize the rich information about participant demographics\nand lexical features present in the ASL Citizen dataset to study and document\nthe biases that may result from models trained on crowd-sourced sign datasets.\nFurther, we apply several bias mitigation techniques during model training, and\nfind that these techniques reduce performance disparities without decreasing\naccuracy. With the publication of this work, we release the demographic\ninformation about the participants in the ASL Citizen dataset to encourage\nfuture bias mitigation work in this space.", "published": "2024-10-07 17:09:03", "link": "http://arxiv.org/abs/2410.05206v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Cookbook: A framework for improving LLM generative abilities via\n  programmatic data generating templates", "abstract": "Fine-tuning large language models (LLMs) on instruction datasets is a common\nway to improve their generative capabilities. However, instruction datasets can\nbe expensive and time-consuming to manually curate, and while LLM-generated\ndata is less labor-intensive, it may violate user privacy agreements or terms\nof service of LLM providers. Therefore, we seek a way of constructing\ninstruction datasets with samples that are not generated by humans or LLMs but\nstill improve LLM generative capabilities. In this work, we introduce Cookbook,\na framework that programmatically generates training data consisting of simple\npatterns over random tokens, resulting in a scalable, cost-effective approach\nthat avoids legal and privacy issues. First, Cookbook uses a template -- a data\ngenerating Python function -- to produce training data that encourages the\nmodel to learn an explicit pattern-based rule that corresponds to a desired\ntask. We find that fine-tuning on Cookbook-generated data is able to improve\nperformance on its corresponding task by up to 52.7 accuracy points. Second,\nsince instruction datasets improve performance on multiple downstream tasks\nsimultaneously, Cookbook algorithmically learns how to mix data from various\ntemplates to optimize performance on multiple tasks. On the standard multi-task\nGPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated\ndataset attains the best accuracy on average compared to other 7B parameter\ninstruction-tuned models and is the best performing model on 3 out of 8 tasks.\nFinally, we analyze when and why Cookbook improves performance and present a\nmetric that allows us to verify that the improvement is largely explained by\nthe model's generations adhering better to template rules.", "published": "2024-10-07 17:29:40", "link": "http://arxiv.org/abs/2410.05224v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with\n  Explanatory Argumentative Structures", "abstract": "Explaining Artificial Intelligence (AI) decisions is a major challenge\nnowadays in AI, in particular when applied to sensitive scenarios like medicine\nand law. However, the need to explain the rationale behind decisions is a main\nissue also for human-based deliberation as it is important to justify\n\\textit{why} a certain decision has been taken. Resident medical doctors for\ninstance are required not only to provide a (possibly correct) diagnosis, but\nalso to explain how they reached a certain conclusion. Developing new tools to\naid residents to train their explanation skills is therefore a central\nobjective of AI in education. In this paper, we follow this direction, and we\npresent, to the best of our knowledge, the first multilingual dataset for\nMedical Question Answering where correct and incorrect diagnoses for a clinical\ncase are enriched with a natural language explanation written by doctors. These\nexplanations have been manually annotated with argument components (i.e.,\npremise, claim) and argument relations (i.e., attack, support), resulting in\nthe Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases\nin four languages (English, Spanish, French, Italian) with explanations, where\nwe annotated 5021 claims, 2313 premises, 2431 support relations, and 1106\nattack relations. We conclude by showing how competitive baselines perform over\nthis challenging dataset for the argument mining task.", "published": "2024-10-07 17:41:45", "link": "http://arxiv.org/abs/2410.05235v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation\n  Models", "abstract": "Vision-Language Models (VLMs) have shown impressive performance in vision\ntasks, but adapting them to new domains often requires expensive fine-tuning.\nPrompt tuning techniques, including textual, visual, and multimodal prompting,\noffer efficient alternatives by leveraging learnable prompts. However, their\napplication to Vision-Language Segmentation Models (VLSMs) and evaluation under\nsignificant domain shifts remain unexplored. This work presents an open-source\nbenchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal\nprompt tuning techniques into VLSMs, making prompt tuning usable for downstream\nsegmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt\ntuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$\ndifferent combinations. We test various prompt tuning on $8$ diverse medical\ndatasets, including $3$ radiology datasets (breast tumor, echocardiograph,\nchest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin\ncancer), and two natural domain segmentation datasets. Our study found that\ntextual prompt tuning struggles under significant domain shifts, from\nnatural-domain images to medical data. Furthermore, visual prompt tuning, with\nfewer hyperparameters than multimodal prompt tuning, often achieves performance\ncompetitive to multimodal approaches, making it a valuable first attempt. Our\nwork advances the understanding and applicability of different prompt-tuning\ntechniques for robust domain-specific segmentation. The source code is\navailable at https://github.com/naamiinepal/tunevlseg.", "published": "2024-10-07 17:42:53", "link": "http://arxiv.org/abs/2410.05239v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Differential Transformer", "abstract": "Transformer tends to overallocate attention to irrelevant context. In this\nwork, we introduce Diff Transformer, which amplifies attention to the relevant\ncontext while canceling noise. Specifically, the differential attention\nmechanism calculates attention scores as the difference between two separate\nsoftmax attention maps. The subtraction cancels noise, promoting the emergence\nof sparse attention patterns. Experimental results on language modeling show\nthat Diff Transformer outperforms Transformer in various settings of scaling up\nmodel size and training tokens. More intriguingly, it offers notable advantages\nin practical applications, such as long-context modeling, key information\nretrieval, hallucination mitigation, in-context learning, and reduction of\nactivation outliers. By being less distracted by irrelevant context, Diff\nTransformer can mitigate hallucination in question answering and text\nsummarization. For in-context learning, Diff Transformer not only enhances\naccuracy but is also more robust to order permutation, which was considered as\na chronic robustness issue. The results position Diff Transformer as a highly\neffective and promising architecture to advance large language models.", "published": "2024-10-07 17:57:38", "link": "http://arxiv.org/abs/2410.05258v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization", "abstract": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.", "published": "2024-10-07 17:59:35", "link": "http://arxiv.org/abs/2410.05265v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Grounding Partially-Defined Events in Multimodal Data", "abstract": "How are we able to learn about complex current events just from short\nsnippets of video? While natural language enables straightforward ways to\nrepresent under-specified, partially observable events, visual data does not\nfacilitate analogous methods and, consequently, introduces unique challenges in\nevent understanding. With the growing prevalence of vision-capable AI agents,\nthese systems must be able to model events from collections of unstructured\nvideo data. To tackle robust event modeling in multimodal settings, we\nintroduce a multimodal formulation for partially-defined events and cast the\nextraction of these events as a three-stage span retrieval task. We propose a\ncorresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours\nof densely annotated current event videos and 1,168 text documents, containing\n22.8K labeled event-centric entities. We propose a collection of LLM-driven\napproaches to the task of multimodal event analysis, and evaluate them on\nMultiVENT-G. Results illustrate the challenges that abstract event\nunderstanding poses and demonstrates promise in event-centric video-language\nsystems.", "published": "2024-10-07 17:59:48", "link": "http://arxiv.org/abs/2410.05267v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model", "abstract": "In this technical report, we present Falcon Mamba 7B, a new base large\nlanguage model based on the novel Mamba architecture. Falcon Mamba 7B is\ntrained on 5.8 trillion tokens with carefully selected data mixtures. As a pure\nMamba-based model, Falcon Mamba 7B surpasses leading open-weight models based\non Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par\nwith Gemma 7B and outperforms models with different architecture designs, such\nas RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is\nthe best-performing Mamba model in the literature at this scale, surpassing\nboth existing Mamba and hybrid Mamba-Transformer models, according to the Open\nLLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly\nfaster at inference and requires substantially less memory for long sequence\ngeneration. Despite recent studies suggesting that hybrid Mamba-Transformer\nmodels outperform pure architecture designs, we demonstrate that even the pure\nMamba design can achieve similar, or even superior results compared to the\nTransformer and hybrid designs. We make the weights of our implementation of\nFalcon Mamba 7B publicly available on\nhttps://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license.", "published": "2024-10-07 15:40:45", "link": "http://arxiv.org/abs/2410.05355v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Task Diversity Shortens the ICL Plateau", "abstract": "In-context learning (ICL) describes a language model's ability to generate\noutputs based on a set of input demonstrations and a subsequent query. To\nunderstand this remarkable capability, researchers have studied simplified,\nstylized models. These studies have consistently observed long loss plateaus,\nduring which models exhibit minimal improvement, followed by a sudden, rapid\nsurge of learning. In this work, we reveal that training on multiple diverse\nICL tasks simultaneously shortens the loss plateaus, making each task easier to\nlearn. This finding is surprising as it contradicts the natural intuition that\nthe combined complexity of multiple ICL tasks would lengthen the learning\nprocess, not shorten it. Our result suggests that the recent success in\nlarge-scale training of language models may be attributed not only to the\nrichness of the data at scale but also to the easier optimization (training)\ninduced by the diversity of natural language training data.", "published": "2024-10-07 19:28:59", "link": "http://arxiv.org/abs/2410.05448v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Interconnected Kingdoms: Comparing 'A Song of Ice and Fire' Adaptations\n  Across Media Using Complex Networks", "abstract": "In this article, we propose and apply a method to compare adaptations of the\nsame story across different media. We tackle this task by modelling such\nadaptations through character networks. We compare them by leveraging two\nconcepts at the core of storytelling: the characters involved, and the dynamics\nof the story. We propose several methods to match characters between media and\ncompare their position in the networks; and perform narrative matching, i.e.\nmatch the sequences of narrative units that constitute the plots. We apply\nthese methods to the novel series \\textit{A Song of Ice and Fire}, by G.R.R.\nMartin, and its comics and TV show adaptations. Our results show that\ninteractions between characters are not sufficient to properly match individual\ncharacters between adaptations, but that using some additional information such\nas character affiliation or gender significantly improves the performance. On\nthe contrary, character interactions convey enough information to perform\nnarrative matching, and allow us to detect the divergence between the original\nnovels and its TV show adaptation.", "published": "2024-10-07 19:35:46", "link": "http://arxiv.org/abs/2410.05453v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "On Instruction-Finetuning Neural Machine Translation Models", "abstract": "In this work, we introduce instruction finetuning for Neural Machine\nTranslation (NMT) models, which distills instruction following capabilities\nfrom Large Language Models (LLMs) into orders-of-magnitude smaller NMT models.\nOur instruction-finetuning recipe for NMT models enables customization of\ntranslations for a limited but disparate set of translation-specific tasks. We\nshow that NMT models are capable of following multiple instructions\nsimultaneously and demonstrate capabilities of zero-shot composition of\ninstructions. We also show that through instruction finetuning, traditionally\ndisparate tasks such as formality-controlled machine translation, multi-domain\nadaptation as well as multi-modal translations can be tackled jointly by a\nsingle instruction finetuned NMT model, at a performance level comparable to\nLLMs such as GPT-3.5-Turbo. To the best of our knowledge, our work is among the\nfirst to demonstrate the instruction-following capabilities of traditional NMT\nmodels, which allows for faster, cheaper and more efficient serving of\ncustomized translations.", "published": "2024-10-07 23:26:13", "link": "http://arxiv.org/abs/2410.05553v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Narrative-of-Thought: Improving Temporal Reasoning of Large Language\n  Models via Recounted Narratives", "abstract": "Reasoning about time and temporal relations is an integral aspect of human\ncognition, essential for perceiving the world and navigating our experiences.\nThough large language models (LLMs) have demonstrated impressive performance in\nmany reasoning tasks, temporal reasoning remains challenging due to its\nintrinsic complexity. In this work, we first study an essential task of\ntemporal reasoning -- temporal graph generation, to unveil LLMs' inherent,\nglobal reasoning capabilities. We show that this task presents great challenges\neven for the most powerful LLMs, such as GPT-3.5/4. We also notice a\nsignificant performance gap by small models (<10B) that lag behind LLMs by 50%.\nNext, we study how to close this gap with a budget constraint, e.g., not using\nmodel finetuning. We propose a new prompting technique tailored for temporal\nreasoning, Narrative-of-Thought (NoT), that first converts the events set to a\nPython class, then prompts a small model to generate a temporally grounded\nnarrative, guiding the final generation of a temporal graph. Extensive\nexperiments showcase the efficacy of NoT in improving various metrics. Notably,\nNoT attains the highest F1 on the Schema-11 evaluation set, while securing an\noverall F1 on par with GPT-3.5. NoT also achieves the best structural\nsimilarity across the board, even compared with GPT-3.5/4. Our code is\navailable at https://github.com/launchnlp/NoT.", "published": "2024-10-07 23:36:05", "link": "http://arxiv.org/abs/2410.05558v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chain and Causal Attention for Efficient Entity Tracking", "abstract": "This paper investigates the limitations of transformers for entity-tracking\ntasks in large language models. We identify a theoretical constraint, showing\nthat transformers require at least $\\log_2 (n+1)$ layers to handle entity\ntracking with $n$ state changes. To address this issue, we propose an efficient\nand frugal enhancement to the standard attention mechanism, enabling it to\nmanage long-term dependencies more efficiently. By considering attention as an\nadjacency matrix, our model can track entity states with a single layer.\nEmpirical results demonstrate significant improvements in entity tracking\ndatasets while keeping competitive performance on standard natural language\nmodeling. Our modified attention allows us to achieve the same performance with\ndrastically fewer layers. Additionally, our enhanced mechanism reveals\nstructured internal representations of attention. Extensive experiments on both\ntoy and complex datasets validate our approach. Our contributions include\ntheoretical insights, an improved attention mechanism, and empirical\nvalidation.", "published": "2024-10-07 23:54:10", "link": "http://arxiv.org/abs/2410.05565v1", "categories": ["cs.LG", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "abstract": "Self-Consistency, a widely-used decoding strategy, significantly boosts the\nreasoning capabilities of Large Language Models (LLMs). However, it depends on\nthe plurality voting rule, which focuses on the most frequent answer while\noverlooking all other minority responses. These inconsistent minority views\noften illuminate areas of uncertainty within the model's generation process. To\naddress this limitation, we present Mirror-Consistency, an enhancement of the\nstandard Self-Consistency approach. Our method incorporates a 'reflective\nmirror' into the self-ensemble decoding process and enables LLMs to critically\nexamine inconsistencies among multiple generations. Additionally, just as\nhumans use the mirror to better understand themselves, we propose using\nMirror-Consistency to enhance the sample-based confidence calibration methods,\nwhich helps to mitigate issues of overconfidence. Our experimental results\ndemonstrate that Mirror-Consistency yields superior performance in both\nreasoning accuracy and confidence calibration compared to Self-Consistency.", "published": "2024-10-07 03:41:08", "link": "http://arxiv.org/abs/2410.10857v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FAME: Towards Factual Multi-Task Model Editing", "abstract": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.", "published": "2024-10-07 13:46:06", "link": "http://arxiv.org/abs/2410.10859v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Recipe For Building a Compliant Real Estate Chatbot", "abstract": "In recent years, there has been significant effort to align large language\nmodels with human preferences. This work focuses on developing a chatbot\nspecialized in the real estate domain, with an emphasis on incorporating\ncompliant behavior to ensure it can be used without perpetuating discriminatory\npractices like steering and redlining, which have historically plagued the real\nestate industry in the United States. Building on prior work, we present a\nmethod for generating a synthetic general instruction-following dataset, along\nwith safety data. Through extensive evaluations and benchmarks, we fine-tuned a\nllama-3-8B-instruct model and demonstrated that we can enhance it's performance\nsignificantly to match huge closed-source models like GPT-4o while making it\nsafer and more compliant. We open-source the model, data and code to support\nfurther development and research in the community.", "published": "2024-10-07 16:03:47", "link": "http://arxiv.org/abs/2410.10860v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Personality Traits of LLMs through Latent Features\n  Steering", "abstract": "Large language models (LLMs) have significantly advanced dialogue systems and\nrole-playing agents through their ability to generate human-like text. While\nprior studies have shown that LLMs can exhibit distinct and consistent\npersonalities, the mechanisms through which these models encode and express\nspecific personality traits remain poorly understood. To address this, we\ninvestigate how various factors, such as cultural norms and environmental\nstressors, encoded within LLMs, shape their personality traits, guided by the\ntheoretical framework of social determinism. Inspired by related work on LLM\ninterpretability, we propose a training-free approach to modify the model's\nbehavior by extracting and steering latent features corresponding to factors\nwithin the model, thereby eliminating the need for retraining. Furthermore, we\nanalyze the implications of these factors for model safety, focusing on their\nimpact through the lens of personality.", "published": "2024-10-07 21:02:34", "link": "http://arxiv.org/abs/2410.10863v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning to Improve Retrieval for Real-world Fact Checking", "abstract": "Recent work on fact-checking addresses a realistic setting where models\nincorporate evidence retrieved from the web to decide the veracity of claims. A\nbottleneck in this pipeline is in retrieving relevant evidence: traditional\nmethods may surface documents directly related to a claim, but fact-checking\ncomplex claims requires more inferences. For instance, a document about how a\nvaccine was developed is relevant to addressing claims about what it might\ncontain, even if it does not address them directly. We present Contrastive\nFact-Checking Reranker (CFR), an improved retriever for this setting. By\nleveraging the AVeriTeC dataset, which annotates subquestions for claims with\nhuman written answers from evidence documents, we fine-tune Contriever with a\ncontrastive objective based on multiple training signals, including\ndistillation from GPT-4, evaluating subquestion answers, and gold labels in the\ndataset. We evaluate our model on both retrieval and end-to-end veracity\njudgments about claims. On the AVeriTeC dataset, we find a 6\\% improvement in\nveracity classification accuracy. We also show our gains can be transferred to\nFEVER, ClaimDecomp, HotpotQA, and a synthetic dataset requiring retrievers to\nmake inferences.", "published": "2024-10-07 00:09:50", "link": "http://arxiv.org/abs/2410.04657v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Multi-Agent Evaluation of Large Language Models through\n  Iterative Debates", "abstract": "This paper explores optimal architectures for evaluating the outputs of large\nlanguage models (LLMs) using LLMs themselves. We propose a novel framework that\ninterprets LLMs as advocates within an ensemble of interacting agents, allowing\nthem to defend their answers and reach conclusions through a judge and jury\nsystem. This approach offers a more dynamic and comprehensive evaluation\nprocess compared to traditional human-based assessments or automated metrics.\nWe discuss the motivation behind this framework, its key components, and\ncomparative advantages. We also present a probabilistic model to evaluate the\nerror reduction achieved by iterative advocate systems. Finally, we outline\nexperiments to validate the effectiveness of multi-advocate architectures and\ndiscuss future research directions.", "published": "2024-10-07 00:22:07", "link": "http://arxiv.org/abs/2410.04663v2", "categories": ["cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Modeling and Estimation of Vocal Tract and Glottal Source Parameters\n  Using ARMAX-LF Model", "abstract": "Modeling and estimation of the vocal tract and glottal source parameters of\nvowels from raw speech can be typically done by using the Auto-Regressive with\neXogenous input (ARX) model and Liljencrants-Fant (LF) model with an\niteration-based estimation approach. However, the all-pole autoregressive model\nin the modeling of vocal tract filters cannot provide the locations of\nanti-formants (zeros), which increases the estimation errors in certain classes\nof speech sounds, such as nasal, fricative, and stop consonants. In this paper,\nwe propose the Auto-Regressive Moving Average eXogenous with LF (ARMAX-LF)\nmodel to extend the ARX-LF model to a wider variety of speech sounds, including\nvowels and nasalized consonants. The LF model represents the glottal source\nderivative as a parametrized time-domain model, and the ARMAX model represents\nthe vocal tract as a pole-zero filter with an additional exogenous LF\nexcitation as input. To estimate multiple parameters with fewer errors, we\nfirst utilize the powerful nonlinear fitting ability of deep neural networks\n(DNNs) to build a mapping from extracted glottal source derivatives or speech\nwaveforms to corresponding LF parameters. Then, glottal source and vocal tract\nparameters can be estimated with fewer estimation errors and without any\niterations as in the analysis-by-synthesis strategy. Experimental results with\nsynthesized speech using the linear source-filter model, synthesized speech\nusing the physical model, and real speech signals showed that the proposed\nARMAX-LF model with a DNN-based estimation method can estimate the parameters\nof both vowels and nasalized sounds with fewer errors and estimation time.", "published": "2024-10-07 02:48:18", "link": "http://arxiv.org/abs/2410.04704v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning How Hard to Think: Input-Adaptive Allocation of LM Computation", "abstract": "Computationally intensive decoding procedures--including search, reranking,\nand self-critique--can improve the quality of language model (LM) outputs in\nproblems spanning code generation, numerical reasoning, and dialog. Existing\nwork typically applies the same decoding procedure for every input to an LM.\nBut not all inputs require the same amount of computation to process. Can we\nallocate decoding computation adaptively, using more resources to answer\nquestions whose answers will be harder to compute? We present an approach that\npredicts the distribution of rewards given an input and computation budget,\nthen allocates additional computation to inputs for which it is predicted to be\nmost useful. We apply this approach in two decoding procedures: first, an\nadaptive best-of-k procedure that dynamically selects the number of samples to\ngenerate as input to a reranker; second, a routing procedure that dynamically\nresponds to a query using a decoding procedure that is expensive but accurate,\nor one that is cheaper but less capable. Across a suite of programming,\nmathematics, and dialog tasks, we show that accurate computation-allocation\nprocedures can be learned, and reduce computation by up to 50% at no cost to\nresponse quality, or improve quality by up to 10% at a fixed computational\nbudget.", "published": "2024-10-07 02:52:30", "link": "http://arxiv.org/abs/2410.04707v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Rule-based Data Selection for Large Language Models", "abstract": "The quality of training data significantly impacts the performance of large\nlanguage models (LLMs). There are increasing studies using LLMs to rate and\nselect data based on several human-crafted metrics (rules). However, these\nconventional rule-based approaches often depend too heavily on human\nheuristics, lack effective metrics for assessing rules, and exhibit limited\nadaptability to new tasks. In our study, we introduce an innovative rule-based\nframework that utilizes the orthogonality of score vectors associated with\nrules as a novel metric for rule evaluations. Our approach includes an\nautomated pipeline that first uses LLMs to generate a diverse set of rules,\nencompassing various rating dimensions to evaluate data quality. Then it rates\na batch of data based on these rules and uses the determinantal point process\n(DPP) from random matrix theory to select the most orthogonal score vectors,\nthereby identifying a set of independent rules. These rules are subsequently\nused to evaluate all data, selecting samples with the highest average scores\nfor downstream tasks such as LLM training. We verify the effectiveness of our\nmethod through two experimental setups: 1) comparisons with ground truth\nratings and 2) benchmarking LLMs trained with the chosen data. Our\ncomprehensive experiments cover a range of scenarios, including general\npre-training and domain-specific fine-tuning in areas such as IMDB, Medical,\nMath, and Code. The outcomes demonstrate that our DPP-based rule rating method\nconsistently outperforms other approaches, including rule-free rating, uniform\nsampling, importance resampling, and QuRating, in terms of both rating\nprecision and model performance.", "published": "2024-10-07 03:13:06", "link": "http://arxiv.org/abs/2410.04715v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction\n  Diversity on Generalization", "abstract": "Understanding and accurately following instructions is critical for large\nlanguage models (LLMs) to be effective across diverse tasks. In this work, we\nrigorously examine the key factors that enable models to generalize to unseen\ninstructions, providing insights to guide the collection of data for\ninstruction-tuning. Through controlled experiments, inspired by the\nTuring-complete Markov algorithm, we demonstrate that such generalization\n$\\textbf{only emerges}$ when training data is diversified enough across\nsemantic domains. Our findings also reveal that merely diversifying within\nlimited domains fails to ensure robust generalization. In contrast,\ncross-domain data diversification, even under constrained data budgets,\nsignificantly enhances a model's adaptability. We further extend our analysis\nto real-world scenarios, including fine-tuning of\n$\\textit{$\\textbf{specialist}$}$ and $\\textit{$\\textbf{generalist}$}$ models.\nIn both cases, we demonstrate that 1) better performance can be achieved by\nincreasing the diversity of an established dataset while keeping the data size\nconstant, and 2) when scaling up the data, diversifying the semantics of\ninstructions is more effective than simply increasing the quantity of similar\ndata. Our research provides important insights for dataset collation,\nparticularly when optimizing model performance by expanding training data for\nboth specialist and generalist scenarios. We show that careful consideration of\ndata diversification is key: training specialist models with data extending\nbeyond their core domain leads to significant performance improvements, while\ngeneralist models benefit from diverse data mixtures that enhance their overall\ninstruction-following capabilities across a wide range of applications. Our\nresults highlight the critical role of strategic diversification and offer\nclear guidelines for improving data quality.", "published": "2024-10-07 03:15:11", "link": "http://arxiv.org/abs/2410.04717v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "TLDR: Token-Level Detective Reward Model for Large Vision Language\n  Models", "abstract": "Although reward models have been successful in improving multimodal large\nlanguage models, the reward models themselves remain brutal and contain minimal\ninformation. Notably, existing reward models only mimic human annotations by\nassigning only one binary feedback to any text, no matter how long the text is.\nIn the realm of multimodal language models, where models are required to\nprocess both images and texts, a naive reward model may learn implicit biases\ntoward texts and become less grounded in images. In this paper, we propose a\n$\\textbf{T}$oken-$\\textbf{L}$evel $\\textbf{D}$etective $\\textbf{R}$eward Model\n($\\textbf{TLDR}$) to provide fine-grained annotations to each text token. We\nfirst introduce a perturbation-based method to generate synthetic hard\nnegatives and their token-level labels to train TLDR models. Then we show the\nrich usefulness of TLDR models both in assisting off-the-shelf models to\nself-correct their generations, and in serving as a hallucination evaluation\ntool. We show that TLDR automatically trains a token-level likelihood\noptimization, and can improve the base model's performance significantly.\nFinally, we show that TLDR models can significantly speed up human annotation\nby 3 times to acquire a broader range of high-quality vision language data.", "published": "2024-10-07 04:00:22", "link": "http://arxiv.org/abs/2410.04734v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "TableRAG: Million-Token Table Understanding with Language Models", "abstract": "Recent advancements in language models (LMs) have notably enhanced their\nability to reason with tabular data, primarily through program-aided mechanisms\nthat manipulate and analyze tables. However, these methods often require the\nentire table as input, leading to scalability challenges due to the positional\nbias or context length constraints. In response to these challenges, we\nintroduce TableRAG, a Retrieval-Augmented Generation (RAG) framework\nspecifically designed for LM-based table understanding. TableRAG leverages\nquery expansion combined with schema and cell retrieval to pinpoint crucial\ninformation before providing it to the LMs. This enables more efficient data\nencoding and precise retrieval, significantly reducing prompt lengths and\nmitigating information loss. We have developed two new million-token benchmarks\nfrom the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's\neffectiveness at scale. Our results demonstrate that TableRAG's retrieval\ndesign achieves the highest retrieval quality, leading to the new\nstate-of-the-art performance on large-scale table understanding.", "published": "2024-10-07 04:15:02", "link": "http://arxiv.org/abs/2410.04739v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ImProver: Agent-Based Automated Proof Optimization", "abstract": "Large language models (LLMs) have been used to generate formal proofs of\nmathematical theorems in proofs assistants such as Lean. However, we often want\nto optimize a formal proof with respect to various criteria, depending on its\ndownstream use. For example, we may want a proof to adhere to a certain style,\nor to be readable, concise, or modularly structured. Having suitably optimized\nproofs is also important for learning tasks, especially since human-written\nproofs may not optimal for that purpose. To this end, we study a new problem of\nautomated proof optimization: rewriting a proof so that it is correct and\noptimizes for an arbitrary criterion, such as length or readability. As a first\nmethod for automated proof optimization, we present ImProver, a\nlarge-language-model agent that rewrites proofs to optimize arbitrary\nuser-defined metrics in Lean. We find that naively applying LLMs to proof\noptimization falls short, and we incorporate various improvements into\nImProver, such as the use of symbolic Lean context in a novel Chain-of-States\ntechnique, as well as error-correction and retrieval. We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, finding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.", "published": "2024-10-07 05:14:18", "link": "http://arxiv.org/abs/2410.04753v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Can LLMs plan paths with extra hints from solvers?", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage processing, mathematical problem solving, and tasks related to program\nsynthesis. However, their effectiveness in long-term planning and higher-order\nreasoning has been noted to be limited and fragile. This paper explores an\napproach for enhancing LLM performance in solving a classical robotic planning\ntask by integrating solver-generated feedback. We explore four different\nstrategies for providing feedback, including visual feedback, we utilize\nfine-tuning, and we evaluate the performance of three different LLMs across a\n10 standard and 100 more randomly generated planning problems. Our results\nsuggest that the solver-generated feedback improves the LLM's ability to solve\nthe moderately difficult problems, but the harder problems still remain out of\nreach. The study provides detailed analysis of the effects of the different\nhinting strategies and the different planning tendencies of the evaluated LLMs.", "published": "2024-10-07 14:00:08", "link": "http://arxiv.org/abs/2410.05045v1", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention", "abstract": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.", "published": "2024-10-07 14:30:27", "link": "http://arxiv.org/abs/2410.05076v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for\n  Data-Driven Scientific Discovery", "abstract": "The advancements of large language models (LLMs) have piqued growing interest\nin developing LLM-based language agents to automate scientific discovery\nend-to-end, which has sparked both excitement and skepticism about their true\ncapabilities. In this work, we call for rigorous assessment of agents on\nindividual tasks in a scientific workflow before making bold claims on\nend-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using ScienceAgentBench, we evaluate\nfive open-weight and proprietary LLMs, each with three frameworks: direct\nprompting, OpenHands CodeAct, and self-debug. Given three attempts for each\ntask, the best-performing agent can only solve 32.4% of the tasks independently\nand 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI\no1-preview with direct prompting and self-debug, which can boost the\nperformance to 42.2%, demonstrating the effectiveness of increasing\ninference-time compute but with more than 10 times the cost of other LLMs.\nStill, our results underscore the limitations of current language agents in\ngenerating code for data-driven discovery, let alone end-to-end automation for\nscientific research.", "published": "2024-10-07 14:33:50", "link": "http://arxiv.org/abs/2410.05080v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks", "abstract": "Preference Optimization (PO) has proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants, following the\noffline Direct Preference Optimization objective, have focused on a strict\nsetting where all tokens are contributing signals of KL divergence and rewards\nto the loss function. However, human preference is not affected by each word in\na sequence equally but is often dependent on specific words or phrases, e.g.\nexistence of toxic terms leads to non-preferred responses. Based on this\nobservation, we argue that not all tokens should be weighted equally during PO\nand propose a flexible objective termed SparsePO, that aims to automatically\nlearn to weight the KL divergence and reward corresponding to each token during\nPO training. We propose two different variants of weight-masks that can either\nbe derived from the reference model itself or learned on the fly. Notably, our\nmethod induces sparsity in the learned masks, allowing the model to learn how\nto best weight reward and KL divergence contributions at the token level,\nlearning an optimal level of mask sparsity. Extensive experiments on multiple\ndomains, including sentiment control, dialogue, text summarization and\ntext-to-code generation, illustrate that our approach assigns meaningful\nweights to tokens according to the target task, generates more responses with\nthe desired preference and improves reasoning tasks by up to 2 percentage\npoints compared to other token- and response-level PO methods.", "published": "2024-10-07 15:01:29", "link": "http://arxiv.org/abs/2410.05102v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CTC-GMM: CTC guided modality matching for fast and accurate streaming\n  speech translation", "abstract": "Models for streaming speech translation (ST) can achieve high accuracy and\nlow latency if they're developed with vast amounts of paired audio in the\nsource language and written text in the target language. Yet, these text labels\nfor the target language are often pseudo labels due to the prohibitive cost of\nmanual ST data labeling. In this paper, we introduce a methodology named\nConnectionist Temporal Classification guided modality matching (CTC-GMM) that\nenhances the streaming ST model by leveraging extensive machine translation\n(MT) text data. This technique employs CTC to compress the speech sequence into\na compact embedding sequence that matches the corresponding text sequence,\nallowing us to utilize matched {source-target} language text pairs from the MT\ncorpora to refine the streaming ST model further. Our evaluations with FLEURS\nand CoVoST2 show that the CTC-GMM approach can increase translation accuracy\nrelatively by 13.9% and 6.4% respectively, while also boosting decoding speed\nby 59.7% on GPU.", "published": "2024-10-07 15:58:03", "link": "http://arxiv.org/abs/2410.05146v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal\n  Embedding Tasks", "abstract": "Embedding models have been crucial in enabling various downstream tasks such\nas semantic similarity, information retrieval, and clustering. Recently, there\nhas been a surge of interest in developing universal text embedding models that\ncan generalize across tasks (e.g., MTEB). However, progress in learning\nuniversal multimodal embedding models has been relatively slow despite its\nimportance and practicality. In this work, we aim to explore the potential for\nbuilding universal embeddings capable of handling a wide range of downstream\ntasks. Our contributions are twofold: (1) MMEB (Massive Multimodal Embedding\nBenchmark), which covers 4 meta-tasks (i.e. classification, visual question\nanswering, multimodal retrieval, and visual grounding) and 36 datasets,\nincluding 20 training and 16 evaluation datasets covering both in-distribution\nand out-of-distribution tasks, and (2) VLM2Vec (Vision-Language Model ->\nVector), a contrastive training framework that converts any state-of-the-art\nvision-language model into an embedding model via training on MMEB. Unlike\nprevious models such as CLIP and BLIP, which encodes text or images\nindependently without any task instruction, VLM2Vec can process any combination\nof images and text to generate a fixed-dimensional vector based on task\ninstructions. We build a series of VLM2Vec models on SoTA VLMs like Phi-3.5-V,\nLLaVA-1.6 and evaluate them on MMEB's evaluation split. Our results show that\nVLM2Vec achieves an absolute average improvement of 10% to 20% over existing\nmultimodal embedding models on both in-distribution and out-of-distribution\ndatasets in MMEB. We show that VLMs are secretly strong embedding models.", "published": "2024-10-07 16:14:05", "link": "http://arxiv.org/abs/2410.05160v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss\n  Landscape Perspective", "abstract": "Training language models currently requires pre-determining a fixed compute\nbudget because the typical cosine learning rate schedule depends on the total\nnumber of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a\nconstant learning rate to produce a main branch of iterates that can in\nprinciple continue indefinitely without a pre-specified compute budget. Then,\ngiven any compute budget, one can branch out from the main branch at a proper\ntime with a rapidly decaying learning rate to produce a strong model.\nEmpirically, WSD generates a non-traditional loss curve: the loss remains\nelevated during the stable phase but sharply declines during the decay phase.\nTowards explaining this phenomenon, we conjecture that pretraining loss\nexhibits a river valley landscape, which resembles a deep valley with a river\nat its bottom. Under this assumption, we show that during the stable phase, the\niterate undergoes large oscillations due to the high learning rate, yet it\nprogresses swiftly along the river. During the decay phase, the rapidly\ndropping learning rate minimizes the iterate's oscillations, moving it closer\nto the river and revealing true optimization progress. Therefore, the sustained\nhigh learning rate phase and fast decaying phase are responsible for progress\nin the river and the mountain directions respectively, and are both critical.\nOur analysis predicts phenomenons consistent with empirical observations and\nshows that this landscape can emerge from pretraining on a simple bi-gram\ndataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that\nreuses previous checkpoints' decay phases and keeps only one main branch, where\nwe resume from a decayed checkpoint. WSD-S empirically outperforms WSD and\nCyclic-Cosine in obtaining multiple language model checkpoints across various\ncompute budgets in a single run for parameters scaling from 0.1B to 1.2B.", "published": "2024-10-07 16:49:39", "link": "http://arxiv.org/abs/2410.05192v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving\n  Vision-Linguistic Compositionality", "abstract": "In this paper, we propose a new method to enhance compositional understanding\nin pre-trained vision and language models (VLMs) without sacrificing\nperformance in zero-shot multi-modal tasks. Traditional fine-tuning approaches\noften improve compositional reasoning at the cost of degrading multi-modal\ncapabilities, primarily due to the use of global hard negative (HN) loss, which\ncontrasts global representations of images and texts. This global HN loss\npushes HN texts that are highly similar to the original ones, damaging the\nmodel's multi-modal representations. To overcome this limitation, we propose\nFine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard\nnegative loss and selective calibrated regularization. These innovations\nprovide fine-grained negative supervision while preserving the model's\nrepresentational integrity. Our extensive evaluations across diverse benchmarks\nfor both compositionality and multi-modal tasks show that FSC-CLIP not only\nachieves compositionality on par with state-of-the-art models but also retains\nstrong multi-modal capabilities. Code is available at:\nhttps://github.com/ytaek-oh/fsc-clip.", "published": "2024-10-07 17:16:20", "link": "http://arxiv.org/abs/2410.05210v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Density estimation with LLMs: a geometric investigation of in-context\n  learning trajectories", "abstract": "Large language models (LLMs) demonstrate remarkable emergent abilities to\nperform in-context learning across various tasks, including time series\nforecasting. This work investigates LLMs' ability to estimate probability\ndensity functions (PDFs) from data observed in-context; such density estimation\n(DE) is a fundamental task underlying many probabilistic modeling problems. We\nleverage the Intensive Principal Component Analysis (InPCA) to visualize and\nanalyze the in-context learning dynamics of LLaMA-2 models. Our main finding is\nthat these LLMs all follow similar learning trajectories in a low-dimensional\nInPCA space, which are distinct from those of traditional density estimation\nmethods like histograms and Gaussian kernel density estimation (KDE). We\ninterpret the LLaMA in-context DE process as a KDE with an adaptive kernel\nwidth and shape. This custom kernel model captures a significant portion of\nLLaMA's behavior despite having only two parameters. We further speculate on\nwhy LLaMA's kernel width and shape differs from classical algorithms, providing\ninsights into the mechanism of in-context probabilistic reasoning in LLMs. Our\ncodebase, along with a 3D visualization of an LLM's in-context learning\ntrajectory, is publicly available at\nhttps://github.com/AntonioLiu97/LLMICL_inPCA", "published": "2024-10-07 17:22:56", "link": "http://arxiv.org/abs/2410.05218v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Precise Model Benchmarking with Only a Few Observations", "abstract": "How can we precisely estimate a large language model's (LLM) accuracy on\nquestions belonging to a specific topic within a larger question-answering\ndataset? The standard direct estimator, which averages the model's accuracy on\nthe questions in each subgroup, may exhibit high variance for subgroups\n(topics) with small sample sizes. Synthetic regression modeling, which\nleverages the model's accuracy on questions about other topics, may yield\nbiased estimates that are too unreliable for large subgroups. We prescribe a\nsimple yet effective solution: an empirical Bayes (EB) estimator that balances\ndirect and regression estimates for each subgroup separately, improving the\nprecision of subgroup-level estimates of model performance. Our experiments on\nmultiple datasets show that this approach consistently provides more precise\nestimates of the LLM performance compared to the direct and regression\napproaches, achieving substantial reductions in the mean squared error.\nConfidence intervals for EB estimates also have near-nominal coverage and are\nnarrower compared to those for the direct estimator. Additional experiments on\ntabular and vision data validate the benefits of this EB approach.", "published": "2024-10-07 17:26:31", "link": "http://arxiv.org/abs/2410.05222v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.AP"], "primary_category": "cs.LG"}
{"title": "Navigating the Digital World as Humans Do: Universal Visual Grounding\n  for GUI Agents", "abstract": "Multimodal large language models (MLLMs) are transforming the capabilities of\ngraphical user interface (GUI) agents, facilitating their transition from\ncontrolled simulations to complex, real-world applications across various\nplatforms. However, the effectiveness of these agents hinges on the robustness\nof their grounding capability. Current GUI agents predominantly utilize\ntext-based representations such as HTML or accessibility trees, which, despite\ntheir utility, often introduce noise, incompleteness, and increased\ncomputational overhead. In this paper, we advocate a human-like embodiment for\nGUI agents that perceive the environment entirely visually and directly perform\npixel-level operations on the GUI. The key is visual grounding models that can\naccurately map diverse referring expressions of GUI elements to their\ncoordinates on the GUI across different platforms. We show that a simple\nrecipe, which includes web-based synthetic data and slight adaptation of the\nLLaVA architecture, is surprisingly effective for training such visual\ngrounding models. We collect the largest dataset for GUI visual grounding so\nfar, containing 10M GUI elements and their referring expressions over 1.3M\nscreenshots, and use it to train UGround, a strong universal visual grounding\nmodel for GUI agents. Empirical results on six benchmarks spanning three\ncategories (grounding, offline agent, and online agent) show that 1) UGround\nsubstantially outperforms existing visual grounding models for GUI agents, by\nup to 20% absolute, and 2) agents with UGround outperform state-of-the-art\nagents, despite the fact that existing agents use additional text-based input\nwhile ours only uses visual perception. These results provide strong support\nfor the feasibility and promises of GUI agents that navigate the digital world\nas humans do.", "published": "2024-10-07 17:47:50", "link": "http://arxiv.org/abs/2410.05243v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe", "abstract": "To acquire instruction-following capabilities, large language models (LLMs)\nundergo instruction tuning, where they are trained on instruction-response\npairs using next-token prediction (NTP). Efforts to improve instruction tuning\noften focus on higher-quality supervised fine-tuning (SFT) datasets, typically\nrequiring data filtering with proprietary LLMs or human annotation. In this\npaper, we take a different approach by proposing SFTMix, a novel Mixup-based\nrecipe that elevates LLM instruction tuning beyond the conventional NTP\nparadigm, without relying on well-curated datasets. Observing that LLMs exhibit\nuneven confidence across the semantic representation space, we argue that\nexamples with different confidence levels should play distinct roles in\ninstruction tuning--confident data is prone to overfitting, while unconfident\ndata is harder to generalize. Based on this insight, SFTMix leverages training\ndynamics to identify examples with varying confidence levels, interpolates them\nto bridge the confidence gap, and applies a Mixup-based regularization to\nsupport learning on these additional, interpolated examples. By propagating\nsupervision signals across confidence regions and encouraging linear behavior\nbetween them, SFTMix mitigates overfitting in confident examples while\nenhancing generalization in unconfident ones. We demonstrate the effectiveness\nof SFTMix in both instruction-following and healthcare-specific SFT tasks, with\nconsistent improvements across LLM families and SFT datasets of varying sizes\nand qualities. Extensive analyses across six directions highlight SFTMix's\ncompatibility with data selection, adaptability to compute-constrained\nscenarios, and scalability to broader applications.", "published": "2024-10-07 17:52:21", "link": "http://arxiv.org/abs/2410.05248v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Causal Micro-Narratives", "abstract": "We present a novel approach to classify causal micro-narratives from text.\nThese narratives are sentence-level explanations of the cause(s) and/or\neffect(s) of a target subject. The approach requires only a subject-specific\nontology of causes and effects, and we demonstrate it with an application to\ninflation narratives. Using a human-annotated dataset spanning historical and\ncontemporary US news articles for training, we evaluate several large language\nmodels (LLMs) on this multi-label classification task. The best-performing\nmodel--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative\ndetection and 0.71 on narrative classification. Comprehensive error analysis\nreveals challenges arising from linguistic ambiguity and highlights how model\nerrors often mirror human annotator disagreements. This research establishes a\nframework for extracting causal micro-narratives from real-world data, with\nwide-ranging applications to social science research.", "published": "2024-10-07 17:55:10", "link": "http://arxiv.org/abs/2410.05252v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Advisor: Dynamic Data Curation for Safety Alignment of Large\n  Language Models", "abstract": "Data is a crucial element in large language model (LLM) alignment. Recent\nstudies have explored using LLMs for efficient data collection. However,\nLLM-generated data often suffers from quality issues, with underrepresented or\nabsent aspects and low-quality datapoints. To address these problems, we\npropose Data Advisor, an enhanced LLM-based method for generating data that\ntakes into account the characteristics of the desired dataset. Starting from a\nset of pre-defined principles in hand, Data Advisor monitors the status of the\ngenerated data, identifies weaknesses in the current dataset, and advises the\nnext iteration of data generation accordingly. Data Advisor can be easily\nintegrated into existing data generation methods to enhance data quality and\ncoverage. Experiments on safety alignment of three representative LLMs (i.e.,\nMistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in\nenhancing model safety against various fine-grained safety issues without\nsacrificing model utility.", "published": "2024-10-07 17:59:58", "link": "http://arxiv.org/abs/2410.05269v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos\n  Referring to Procedural Texts", "abstract": "Mistake action detection is crucial for developing intelligent archives that\ndetect workers' errors and provide feedback. Existing studies have focused on\nvisually apparent mistakes in free-style activities, resulting in video-only\napproaches to mistake detection. However, in text-following activities, models\ncannot determine the correctness of some actions without referring to the\ntexts. Additionally, current mistake datasets rarely use procedural texts for\nvideo recording except for cooking. To fill these gaps, this paper proposes the\nEgoOops dataset, where egocentric videos record erroneous activities when\nfollowing procedural texts across diverse domains. It features three types of\nannotations: video-text alignment, mistake labels, and descriptions for\nmistakes. We also propose a mistake detection approach, combining video-text\nalignment and mistake label classification to leverage the texts. Our\nexperimental results show that incorporating procedural texts is essential for\nmistake detection. Data is available through\nhttps://y-haneji.github.io/EgoOops-project-page/.", "published": "2024-10-07 07:19:50", "link": "http://arxiv.org/abs/2410.05343v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild", "abstract": "As Large Language Models (LLMs) excel across tasks and specialized domains,\nscaling LLMs based on existing models has garnered significant attention, which\nfaces the challenge of decreasing performance when combining disparate models.\nVarious techniques have been proposed for the aggregation of pre-trained LLMs,\nincluding model merging, Mixture-of-Experts, and stacking. Despite their\nmerits, a comprehensive comparison and synergistic application of them to a\ndiverse model zoo is yet to be adequately addressed. In light of this research\ngap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First,\nour work starts with a benchmarking of existing LLM scaling techniques,\nespecially selective merging, and variants of mixture. Utilizing the insights\nfrom the benchmark results, we formulate an optimal strategy for the selection\nand aggregation of a heterogeneous model zoo characterizing different\narchitectures and initialization.Our methodology involves the clustering of\nmergeable models and optimal merging strategy selection, and the integration of\nclusters through a model mixture. Finally, evidenced by our experiments on a\ndiverse Llama-2-based model zoo, Model-GLUE shows an average performance\nenhancement of 5.61%, achieved without additional training. Codes are available\nat: https://github.com/Model-GLUE/Model-GLUE.", "published": "2024-10-07 15:55:55", "link": "http://arxiv.org/abs/2410.05357v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLMs Are In-Context Bandit Reinforcement Learners", "abstract": "Large Language Models (LLMs) excel at in-context learning (ICL), a supervised\nlearning technique that relies on adding annotated examples to the model\ncontext. We investigate a contextual bandit version of in-context reinforcement\nlearning (ICRL), where models learn in-context, online, from external reward,\ninstead of supervised data. We show that LLMs effectively demonstrate such\nlearning, and provide a detailed study of the phenomena, experimenting with\nchallenging classification tasks and models of sizes from 500M to 70B\nparameters. This includes identifying and addressing the instability of the\nprocess, demonstrating learning with both semantic and abstract labels, and\nshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, while\nalso underscoring fundamental limitations in their implicit reasoning about\nerrors.", "published": "2024-10-07 17:45:00", "link": "http://arxiv.org/abs/2410.05362v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs:\n  Thematic Insights and Fairness Evaluation", "abstract": "Climate change communication on social media increasingly employs\nmicrotargeting strategies to effectively reach and influence specific\ndemographic groups. This study presents a post-hoc analysis of microtargeting\npractices within climate campaigns by leveraging large language models (LLMs)\nto examine Facebook advertisements. Our analysis focuses on two key aspects:\ndemographic targeting and fairness. We evaluate the ability of LLMs to\naccurately predict the intended demographic targets, such as gender and age\ngroup, achieving an overall accuracy of 88.55%. Furthermore, we instruct the\nLLMs to generate explanations for their classifications, providing transparent\nreasoning behind each decision. These explanations reveal the specific thematic\nelements used to engage different demographic segments, highlighting distinct\nstrategies tailored to various audiences. Our findings show that young adults\nare primarily targeted through messages emphasizing activism and environmental\nconsciousness, while women are engaged through themes related to caregiving\nroles and social advocacy. In addition to evaluating the effectiveness of LLMs\nin detecting microtargeted messaging, we conduct a comprehensive fairness\nanalysis to identify potential biases in model predictions. Our findings\nindicate that while LLMs perform well overall, certain biases exist,\nparticularly in the classification of senior citizens and male audiences. By\nshowcasing the efficacy of LLMs in dissecting and explaining targeted\ncommunication strategies and by highlighting fairness concerns, this study\nprovides a valuable framework for future research aimed at enhancing\ntransparency, accountability, and inclusivity in social media-driven climate\ncampaigns.", "published": "2024-10-07 18:07:56", "link": "http://arxiv.org/abs/2410.05401v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "From Sparse Dependence to Sparse Attention: Unveiling How\n  Chain-of-Thought Enhances Transformer Sample Efficiency", "abstract": "Chain-of-thought (CoT) significantly enhances the reasoning performance of\nlarge language models (LLM). While current theoretical studies often attribute\nthis improvement to increased expressiveness and computational capacity, we\nargue that expressiveness is not the primary limitation in the LLM regime, as\ncurrent large models will fail on simple tasks. Using a parity-learning setup,\nwe demonstrate that CoT can substantially improve sample efficiency even when\nthe representation power is sufficient. Specifically, with CoT, a transformer\ncan learn the function within polynomial samples, whereas without CoT, the\nrequired sample size is exponential. Additionally, we show that CoT simplifies\nthe learning process by introducing sparse sequential dependencies among input\ntokens, and leads to a sparse and interpretable attention. We validate our\ntheoretical analysis with both synthetic and real-world experiments, confirming\nthat sparsity in attention layers is a key factor of the improvement induced by\nCoT.", "published": "2024-10-07 19:45:09", "link": "http://arxiv.org/abs/2410.05459v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Rational Metareasoning for Large Language Models", "abstract": "Being prompted to engage in reasoning has emerged as a core technique for\nusing large language models (LLMs), deploying additional inference-time compute\nto improve task performance. However, as LLMs increase in both size and\nadoption, inference costs are correspondingly becoming increasingly burdensome.\nHow, then, might we optimize reasoning's cost-performance tradeoff? This work\nintroduces a novel approach based on computational models of metareasoning used\nin cognitive science, training LLMs to selectively use intermediate reasoning\nsteps only when necessary. We first develop a reward function that incorporates\nthe Value of Computation by penalizing unnecessary reasoning, then use this\nreward function with Expert Iteration to train the LLM. Compared to few-shot\nchain-of-thought prompting and STaR, our method significantly reduces inference\ncosts (20-37\\% fewer tokens generated across three models) while maintaining\ntask performance across diverse datasets.", "published": "2024-10-07 23:48:52", "link": "http://arxiv.org/abs/2410.05563v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mechanistic?", "abstract": "The rise of the term \"mechanistic interpretability\" has accompanied\nincreasing interest in understanding neural models -- particularly language\nmodels. However, this jargon has also led to a fair amount of confusion. So,\nwhat does it mean to be \"mechanistic\"? We describe four uses of the term in\ninterpretability research. The most narrow technical definition requires a\nclaim of causality, while a broader technical definition allows for any\nexploration of a model's internals. However, the term also has a narrow\ncultural definition describing a cultural movement. To understand this semantic\ndrift, we present a history of the NLP interpretability community and the\nformation of the separate, parallel \"mechanistic\" interpretability community.\nFinally, we discuss the broad cultural definition -- encompassing the entire\nfield of interpretability -- and why the traditional NLP interpretability\ncommunity has come to embrace it. We argue that the polysemy of \"mechanistic\"\nis the product of a critical divide within the interpretability community.", "published": "2024-10-07 15:02:12", "link": "http://arxiv.org/abs/2410.09087v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Reasoning Paths Optimization: Learning to Reason and Explore From\n  Diverse Paths", "abstract": "Advanced models such as OpenAI o1 exhibit impressive problem-solving\ncapabilities through step-by-step reasoning. However, they may still falter on\nmore complex problems, making errors that disrupt their reasoning paths. We\nattribute this to the expansive solution space, where each step has the risk of\ndiverging into mistakes. To enhance language model reasoning, we introduce a\nspecialized training framework called Reasoning Paths Optimization (RPO), which\nenables learning to reason and explore from diverse paths. Our approach\nencourages favorable branches at each reasoning step while penalizing\nunfavorable ones, enhancing the model's overall problem-solving performance.\nReasoning Paths Optimization does not rely on large-scale human-annotated\nrationales or outputs from closed-source models, making it scalable and\ndata-efficient. We focus on multi-step reasoning tasks, such as math word\nproblems and science-based exam questions. The experiments demonstrate that our\nframework significantly enhances the reasoning performance of large language\nmodels, with up to 3.1% and 4.3% improvement on GSM8K and MMLU (STEM)\nrespectively. Our data and code can be found at\nhttps://reasoning-paths.github.io.", "published": "2024-10-07 06:37:25", "link": "http://arxiv.org/abs/2410.10858v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fill In The Gaps: Model Calibration and Generalization with Synthetic\n  Data", "abstract": "As machine learning models continue to swiftly advance, calibrating their\nperformance has become a major concern prior to practical and widespread\nimplementation. Most existing calibration methods often negatively impact model\naccuracy due to the lack of diversity of validation data, resulting in reduced\ngeneralizability. To address this, we propose a calibration method that\nincorporates synthetic data without compromising accuracy. We derive the\nexpected calibration error (ECE) bound using the Probably Approximately Correct\n(PAC) learning framework. Large language models (LLMs), known for their ability\nto mimic real data and generate text with mixed class labels, are utilized as a\nsynthetic data generation strategy to lower the ECE bound and improve model\naccuracy on real test data. Additionally, we propose data generation mechanisms\nfor efficient calibration. Testing our method on four different natural\nlanguage processing tasks, we observed an average up to 34\\% increase in\naccuracy and 33\\% decrease in ECE.", "published": "2024-10-07 23:06:42", "link": "http://arxiv.org/abs/2410.10864v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatVis: Automating Scientific Visualization with a Large Language Model", "abstract": "We develop an iterative assistant we call ChatVis that can synthetically\ngenerate Python scripts for data analysis and visualization using a large\nlanguage model (LLM). The assistant allows a user to specify the operations in\nnatural language, attempting to generate a Python script for the desired\noperations, prompting the LLM to revise the script as needed until it executes\ncorrectly. The iterations include an error detection and correction mechanism\nthat extracts error messages from the execution of the script and subsequently\nprompts LLM to correct the error. Our method demonstrates correct execution on\nfive canonical visualization scenarios, comparing results with ground truth. We\nalso compared our results with scripts generated by several other LLMs without\nany assistance. In every instance, ChatVis successfully generated the correct\nscript, whereas the unassisted LLMs failed to do so. The code is available on\nGitHub: https://github.com/tanwimallick/ChatVis/.", "published": "2024-10-07 17:37:59", "link": "http://arxiv.org/abs/2410.11863v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Automatic Screening for Children with Speech Disorder using Automatic\n  Speech Recognition: Opportunities and Challenges", "abstract": "Speech is a fundamental aspect of human life, crucial not only for\ncommunication but also for cognitive, social, and academic development.\nChildren with speech disorders (SD) face significant challenges that, if\nunaddressed, can result in lasting negative impacts. Traditionally, speech and\nlanguage assessments (SLA) have been conducted by skilled speech-language\npathologists (SLPs), but there is a growing need for efficient and scalable SLA\nmethods powered by artificial intelligence. This position paper presents a\nsurvey of existing techniques suitable for automating SLA pipelines, with an\nemphasis on adapting automatic speech recognition (ASR) models for children's\nspeech, an overview of current SLAs and their automated counterparts to\ndemonstrate the feasibility of AI-enhanced SLA pipelines, and a discussion of\npractical considerations, including accessibility and privacy concerns,\nassociated with the deployment of AI-powered SLAs.", "published": "2024-10-07 20:14:37", "link": "http://arxiv.org/abs/2410.11865v1", "categories": ["eess.AS", "cs.CL", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "GLEE: A Unified Framework and Benchmark for Language-based Economic\n  Environments", "abstract": "Large Language Models (LLMs) show significant potential in economic and\nstrategic interactions, where communication via natural language is often\nprevalent. This raises key questions: Do LLMs behave rationally? Can they mimic\nhuman behavior? Do they tend to reach an efficient and fair outcome? What is\nthe role of natural language in the strategic interaction? How do\ncharacteristics of the economic environment influence these dynamics? These\nquestions become crucial concerning the economic and societal implications of\nintegrating LLM-based agents into real-world data-driven systems, such as\nonline retail platforms and recommender systems. While the ML community has\nbeen exploring the potential of LLMs in such multi-agent setups, varying\nassumptions, design choices and evaluation criteria across studies make it\ndifficult to draw robust and meaningful conclusions. To address this, we\nintroduce a benchmark for standardizing research on two-player, sequential,\nlanguage-based games. Inspired by the economic literature, we define three base\nfamilies of games with consistent parameterization, degrees of freedom and\neconomic measures to evaluate agents' performance (self-gain), as well as the\ngame outcome (efficiency and fairness). We develop an open-source framework for\ninteraction simulation and analysis, and utilize it to collect a dataset of LLM\nvs. LLM interactions across numerous game configurations and an additional\ndataset of human vs. LLM interactions. Through extensive experimentation, we\ndemonstrate how our framework and dataset can be used to: (i) compare the\nbehavior of LLM-based agents to human players in various economic contexts;\n(ii) evaluate agents in both individual and collective performance measures;\nand (iii) quantify the effect of the economic characteristics of the\nenvironments on the behavior of agents.", "published": "2024-10-07 17:55:35", "link": "http://arxiv.org/abs/2410.05254v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.GT", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Superficial Safety Alignment Hypothesis", "abstract": "As large language models (LLMs) are overwhelmingly more and more integrated\ninto various applications, ensuring they generate safe and aligned responses is\na pressing need. Previous research on alignment has largely focused on general\ninstruction-following but has often overlooked the unique properties and\nchallenges of safety alignment, such as the brittleness of safety mechanisms.\nTo bridge the gap, we propose the Superficial Safety Alignment Hypothesis\n(SSAH), which posits that safety alignment should teach an otherwise unsafe\nmodel to choose the correct reasoning direction - interpreted as a specialized\nbinary classification task - and incorporate a refusal mechanism with multiple\nreserved fallback options. Furthermore, through SSAH, we hypothesize that\nsafety guardrails in LLMs can be established by just a small number of\nessential components. To verify this, we conduct an ablation study and\nsuccessfully identify four types of attribute-critical components in\nsafety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU),\nComplex Unit (CU), and Redundant Unit (RU). Our findings show that freezing\ncertain safety-critical components 7.5\\% during fine-tuning allows the model to\nretain its safety attributes while adapting to new tasks. Additionally, we show\nthat leveraging redundant units 20\\% in the pre-trained model as an ``alignment\nbudget'' can effectively minimize the alignment tax while achieving the\nalignment goal. All considered, this paper concludes that the atomic functional\nunit for safety in LLMs is at the neuron level and underscores that safety\nalignment should not be complicated. We believe this work contributes to the\nfoundation of efficient and scalable safety alignment for future LLMs.", "published": "2024-10-07 19:53:35", "link": "http://arxiv.org/abs/2410.10862v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SegINR: Segment-wise Implicit Neural Representation for Sequence\n  Alignment in Neural Text-to-Speech", "abstract": "We present SegINR, a novel approach to neural Text-to-Speech (TTS) that\naddresses sequence alignment without relying on an auxiliary duration predictor\nand complex autoregressive (AR) or non-autoregressive (NAR) frame-level\nsequence modeling. SegINR simplifies the process by converting text sequences\ndirectly into frame-level features. It leverages an optimal text encoder to\nextract embeddings, transforming each into a segment of frame-level features\nusing a conditional implicit neural representation (INR). This method, named\nsegment-wise INR (SegINR), models temporal dynamics within each segment and\nautonomously defines segment boundaries, reducing computational costs. We\nintegrate SegINR into a two-stage TTS framework, using it for semantic token\nprediction. Our experiments in zero-shot adaptive TTS scenarios demonstrate\nthat SegINR outperforms conventional methods in speech quality with\ncomputational efficiency.", "published": "2024-10-07 02:04:58", "link": "http://arxiv.org/abs/2410.04690v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Demo of Zero-Shot Guitar Amplifier Modelling: Enhancing Modeling with\n  Hyper Neural Networks", "abstract": "Electric guitar tone modeling typically focuses on the non-linear\ntransformation from clean to amplifier-rendered audio. Traditional methods rely\non one-to-one mappings, incorporating device parameters into neural models to\nreplicate specific amplifiers. However, these methods are limited by the need\nfor specific training data. In this paper, we adapt a model based on the\nprevious work, which leverages a tone embedding encoder and a feature wise\nlinear modulation (FiLM) condition method. In this work, we altered\nconditioning method using a hypernetwork-based gated convolutional network\n(GCN) to generate audio that blends clean input with the tone characteristics\nof reference audio. By extending the training data to cover a wider variety of\namplifier tones, our model is able to capture a broader range of tones.\nAdditionally, we developed a real-time plugin to demonstrate the system's\npractical application, allowing users to experience its performance\ninteractively. Our results indicate that the proposed system achieves superior\ntone modeling versatility compared to traditional methods.", "published": "2024-10-07 02:38:58", "link": "http://arxiv.org/abs/2410.04702v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Ultra-Low-Power Neuromorphic Speech Enhancement with\n  Spiking-FullSubNet", "abstract": "Speech enhancement is critical for improving speech intelligibility and\nquality in various audio devices. In recent years, deep learning-based methods\nhave significantly improved speech enhancement performance, but they often come\nwith a high computational cost, which is prohibitive for a large number of edge\ndevices, such as headsets and hearing aids. This work proposes an\nultra-low-power speech enhancement system based on the brain-inspired spiking\nneural network (SNN) called Spiking-FullSubNet. Spiking-FullSubNet follows a\nfull-band and sub-band fusioned approach to effectively capture both global and\nlocal spectral information. To enhance the efficiency of computationally\nexpensive sub-band modeling, we introduce a frequency partitioning method\ninspired by the sensitivity profile of the human peripheral auditory system.\nFurthermore, we introduce a novel spiking neuron model that can dynamically\ncontrol the input information integration and forgetting, enhancing the\nmulti-scale temporal processing capability of SNN, which is critical for speech\ndenoising. Experiments conducted on the recent Intel Neuromorphic Deep Noise\nSuppression (N-DNS) Challenge dataset show that the Spiking-FullSubNet\nsurpasses state-of-the-art methods by large margins in terms of both speech\nquality and energy efficiency metrics. Notably, our system won the championship\nof the Intel N-DNS Challenge (Algorithmic Track), opening up a myriad of\nopportunities for ultra-low-power speech enhancement at the edge. Our source\ncode and model checkpoints are publicly available at\nhttps://github.com/haoxiangsnr/spiking-fullsubnet.", "published": "2024-10-07 06:50:19", "link": "http://arxiv.org/abs/2410.04785v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A decade of DCASE: Achievements, practices, evaluations and future\n  challenges", "abstract": "This paper introduces briefly the history and growth of the Detection and\nClassification of Acoustic Scenes and Events (DCASE) challenge, workshop,\nresearch area and research community. Created in 2013 as a data evaluation\nchallenge, DCASE has become a major research topic in the Audio and Acoustic\nSignal Processing area. Its success comes from a combination of factors: the\nchallenge offers a large variety of tasks that are renewed each year; and the\nworkshop offers a channel for dissemination of related work, engaging a young\nand dynamic community. At the same time, DCASE faces its own challenges,\ngrowing and expanding to different areas. One of the core principles of DCASE\nis open science and reproducibility: publicly available datasets, baseline\nsystems, technical reports and workshop publications. While the DCASE challenge\nand workshop are independent of IEEE SPS, the challenge receives annual\nendorsement from the AASP TC, and the DCASE community contributes significantly\nto the ICASSP flagship conference and the success of SPS in many of its\nactivities.", "published": "2024-10-07 11:47:59", "link": "http://arxiv.org/abs/2410.04951v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Speaker Representations Using Contrastive Losses on\n  Multi-scale Features", "abstract": "Speaker verification systems have seen significant advancements with the\nintroduction of Multi-scale Feature Aggregation (MFA) architectures, such as\nMFA-Conformer and ECAPA-TDNN. These models leverage information from various\nnetwork depths by concatenating intermediate feature maps before the pooling\nand projection layers, demonstrating that even shallower feature maps encode\nvaluable speaker-specific information. Building upon this foundation, we\npropose a Multi-scale Feature Contrastive (MFCon) loss that directly enhances\nthe quality of these intermediate representations. Our MFCon loss applies\ncontrastive learning to all feature maps within the network, encouraging the\nmodel to learn more discriminative representations at the intermediate stage\nitself. By enforcing better feature map learning, we show that the resulting\nspeaker embeddings exhibit increased discriminative power. Our method achieves\na 9.05% improvement in equal error rate (EER) compared to the standard\nMFA-Conformer on the VoxCeleb-1O test set.", "published": "2024-10-07 13:49:45", "link": "http://arxiv.org/abs/2410.05037v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Editing Music with Melody and Text: Using ControlNet for Diffusion\n  Transformer", "abstract": "Despite the significant progress in controllable music generation and\nediting, challenges remain in the quality and length of generated music due to\nthe use of Mel-spectrogram representations and UNet-based model structures. To\naddress these limitations, we propose a novel approach using a Diffusion\nTransformer (DiT) augmented with an additional control branch using ControlNet.\nThis allows for long-form and variable-length music generation and editing\ncontrolled by text and melody prompts. For more precise and fine-grained melody\ncontrol, we introduce a novel top-$k$ constant-Q Transform representation as\nthe melody prompt, reducing ambiguity compared to previous representations\n(e.g., chroma), particularly for music with multiple tracks or a wide range of\npitch values. To effectively balance the control signals from text and melody\nprompts, we adopt a curriculum learning strategy that progressively masks the\nmelody prompt, resulting in a more stable training process. Experiments have\nbeen performed on text-to-music generation and music-style transfer tasks using\nopen-source instrumental recording data. The results demonstrate that by\nextending StableAudio, a pre-trained text-controlled DiT model, our approach\nenables superior melody-controlled editing while retaining good text-to-music\ngeneration performance. These results outperform a strong MusicGen baseline in\nterms of both text-based generation and melody preservation for editing. Audio\nexamples can be found at https://stable-audio-control.github.io.", "published": "2024-10-07 16:04:49", "link": "http://arxiv.org/abs/2410.05151v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Attentive-based Multi-level Feature Fusion for Voice Disorder Diagnosis", "abstract": "Voice disorders negatively impact the quality of daily life in various ways.\nHowever, accurately recognizing the category of pathological features from raw\naudio remains a considerable challenge due to the limited dataset. A promising\nmethod to handle this issue is extracting multi-level pathological information\nfrom speech in a comprehensive manner by fusing features in the latent space.\nIn this paper, a novel framework is designed to explore the way of high-quality\nfeature fusion for effective and generalized detection performance.\nSpecifically, the proposed model follows a two-stage training paradigm: (1)\nECAPA-TDNN and Wav2vec 2.0 which have shown remarkable effectiveness in various\ndomains are employed to learn the universal pathological information from raw\naudio; (2) An attentive fusion module is dedicatedly designed to establish the\ninteraction between pathological features projected by EcapTdnn and Wav2vec 2.0\nrespectively and guide the multi-layer fusion, the entire model is jointly\nfine-tuned from pre-trained features by the automatic voice pathology detection\ntask. Finally, comprehensive experiments on the FEMH and SVD datasets\ndemonstrate that the proposed framework outperforms the competitive baselines,\nand achieves the accuracy of 90.51% and 87.68%.", "published": "2024-10-07 07:16:29", "link": "http://arxiv.org/abs/2410.04797v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation", "abstract": "Artificial Intelligence and generative models have revolutionized music\ncreation, with many models leveraging textual or visual prompts for guidance.\nHowever, existing image-to-music models are limited to simple images, lacking\nthe capability to generate music from complex digitized artworks. To address\nthis gap, we introduce $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$, a novel\nmodel designed to create music from digitized artworks or text inputs.\n$\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ extends the AudioLDM~2\narchitecture, a text-to-audio model, and employs our newly curated datasets,\ncreated via ImageBind, which pair digitized artworks with music. Experimental\nresults demonstrate that $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ can\ngenerate music that resonates with the input stimuli. These findings suggest\npromising applications in multimedia art, interactive installations, and\nAI-driven creative tools.", "published": "2024-10-07 10:48:08", "link": "http://arxiv.org/abs/2410.04906v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Stage-Wise and Prior-Aware Neural Speech Phase Prediction", "abstract": "This paper proposes a novel Stage-wise and Prior-aware Neural Speech Phase\nPrediction (SP-NSPP) model, which predicts the phase spectrum from input\namplitude spectrum by two-stage neural networks. In the initial\nprior-construction stage, we preliminarily predict a rough prior phase spectrum\nfrom the amplitude spectrum. The subsequent refinement stage transforms the\namplitude spectrum into a refined high-quality phase spectrum conditioned on\nthe prior phase. Networks in both stages use ConvNeXt v2 blocks as the backbone\nand adopt adversarial training by innovatively introducing a phase spectrum\ndiscriminator (PSD). To further improve the continuity of the refined phase, we\nalso incorporate a time-frequency integrated difference (TFID) loss in the\nrefinement stage. Experimental results confirm that, compared to neural\nnetwork-based no-prior phase prediction methods, the proposed SP-NSPP achieves\nhigher phase prediction accuracy, thanks to introducing the coarse phase priors\nand diverse training criteria. Compared to iterative phase estimation\nalgorithms, our proposed SP-NSPP does not require multiple rounds of staged\niterations, resulting in higher generation efficiency.", "published": "2024-10-07 12:45:20", "link": "http://arxiv.org/abs/2410.04990v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RelUNet: Relative Channel Fusion U-Net for Multichannel Speech\n  Enhancement", "abstract": "Neural multi-channel speech enhancement models, in particular those based on\nthe U-Net architecture, demonstrate promising performance and generalization\npotential. These models typically encode input channels independently, and\nintegrate the channels during later stages of the network. In this paper, we\npropose a novel modification of these models by incorporating relative\ninformation from the outset, where each channel is processed in conjunction\nwith a reference channel through stacking. This input strategy exploits\ncomparative differences to adaptively fuse information between channels,\nthereby capturing crucial spatial information and enhancing the overall\nperformance. The experiments conducted on the CHiME-3 dataset demonstrate\nimprovements in speech enhancement metrics across various architectures.", "published": "2024-10-07 13:19:10", "link": "http://arxiv.org/abs/2410.05019v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CR-CTC: Consistency regularization on CTC for improved speech\n  recognition", "abstract": "Connectionist Temporal Classification (CTC) is a widely used method for\nautomatic speech recognition (ASR), renowned for its simplicity and\ncomputational efficiency. However, it often falls short in recognition\nperformance. In this work, we propose the Consistency-Regularized CTC (CR-CTC),\nwhich enforces consistency between two CTC distributions obtained from\ndifferent augmented views of the input speech mel-spectrogram. We provide\nin-depth insights into its essential behaviors from three perspectives: 1) it\nconducts self-distillation between random pairs of sub-models that process\ndifferent augmented views; 2) it learns contextual representation through\nmasked prediction for positions within time-masked regions, especially when we\nincrease the amount of time masking; 3) it suppresses the extremely peaky CTC\ndistributions, thereby reducing overfitting and improving the generalization\nability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech\ndatasets demonstrate the effectiveness of our CR-CTC. It significantly improves\nthe CTC performance, achieving state-of-the-art results comparable to those\nattained by transducer or systems combining CTC and attention-based\nencoder-decoder (CTC/AED). We release our code at\nhttps://github.com/k2-fsa/icefall.", "published": "2024-10-07 14:56:07", "link": "http://arxiv.org/abs/2410.05101v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Presto! Distilling Steps and Layers for Accelerating Music Generation", "abstract": "Despite advances in diffusion-based text-to-music (TTM) methods, efficient,\nhigh-quality generation remains a challenge. We introduce Presto!, an approach\nto inference acceleration for score-based diffusion transformers via reducing\nboth sampling steps and cost per step. To reduce steps, we develop a new\nscore-based distribution matching distillation (DMD) method for the EDM-family\nof diffusion models, the first GAN-based distillation method for TTM. To reduce\nthe cost per step, we develop a simple, but powerful improvement to a recent\nlayer distillation method that improves learning via better preserving hidden\nstate variance. Finally, we combine our step and layer distillation methods\ntogether for a dual-faceted approach. We evaluate our step and layer\ndistillation methods independently and show each yield best-in-class\nperformance. Our combined distillation method can generate high-quality outputs\nwith improved diversity, accelerating our base model by 10-18x (230/435ms\nlatency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) --\nthe fastest high-quality TTM to our knowledge. Sound examples can be found at\nhttps://presto-music.github.io/web/.", "published": "2024-10-07 16:24:18", "link": "http://arxiv.org/abs/2410.05167v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RespLLM: Unifying Audio and Text with Multimodal LLMs for Generalized\n  Respiratory Health Prediction", "abstract": "The high incidence and mortality rates associated with respiratory diseases\nunderscores the importance of early screening. Machine learning models can\nautomate clinical consultations and auscultation, offering vital support in\nthis area. However, the data involved, spanning demographics, medical history,\nsymptoms, and respiratory audio, are heterogeneous and complex. Existing\napproaches are insufficient and lack generalizability, as they typically rely\non limited training data, basic fusion techniques, and task-specific models. In\nthis paper, we propose RespLLM, a novel multimodal large language model (LLM)\nframework that unifies text and audio representations for respiratory health\nprediction. RespLLM leverages the extensive prior knowledge of pretrained LLMs\nand enables effective audio-text fusion through cross-modal attentions.\nInstruction tuning is employed to integrate diverse data from multiple sources,\nensuring generalizability and versatility of the model. Experiments on five\nreal-world datasets demonstrate that RespLLM outperforms leading baselines by\nan average of 4.6% on trained tasks, 7.9% on unseen datasets, and facilitates\nzero-shot predictions for new tasks. Our work lays the foundation for\nmultimodal models that can perceive, listen to, and understand heterogeneous\ndata, paving the way for scalable respiratory health diagnosis.", "published": "2024-10-07 17:06:11", "link": "http://arxiv.org/abs/2410.05361v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Incorporating Talker Identity Aids With Improving Speech Recognition in\n  Adversarial Environments", "abstract": "Current state-of-the-art speech recognition models are trained to map\nacoustic signals into sub-lexical units. While these models demonstrate\nsuperior performance, they remain vulnerable to out-of-distribution conditions\nsuch as background noise and speech augmentations. In this work, we hypothesize\nthat incorporating speaker representations during speech recognition can\nenhance model robustness to noise. We developed a transformer-based model that\njointly performs speech recognition and speaker identification. Our model\nutilizes speech embeddings from Whisper and speaker embeddings from ECAPA-TDNN,\nwhich are processed jointly to perform both tasks. We show that the joint model\nperforms comparably to Whisper under clean conditions. Notably, the joint model\noutperforms Whisper in high-noise environments, such as with 8-speaker babble\nbackground noise. Furthermore, our joint model excels in handling highly\naugmented speech, including sine-wave and noise-vocoded speech. Overall, these\nresults suggest that integrating voice representations with speech recognition\ncan lead to more robust models under adversarial conditions.", "published": "2024-10-07 18:39:59", "link": "http://arxiv.org/abs/2410.05423v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dynamic HumTrans: Humming Transcription Using CNNs and Dynamic\n  Programming", "abstract": "We propose a novel approach for humming transcription that combines a\nCNN-based architecture with a dynamic programming-based post-processing\nalgorithm, utilizing the recently introduced HumTrans dataset. We identify and\naddress inherent problems with the offset and onset ground truth provided by\nthe dataset, offering heuristics to improve these annotations, resulting in a\ndataset with precise annotations that will aid future research. Additionally,\nwe compare the transcription accuracy of our method against several others,\ndemonstrating state-of-the-art (SOTA) results. All our code and corrected\ndataset is available at\nhttps://github.com/shubham-gupta-30/humming_transcription", "published": "2024-10-07 19:40:39", "link": "http://arxiv.org/abs/2410.05455v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "A Recurrent Neural Network Approach to the Answering Machine Detection\n  Problem", "abstract": "In the field of telecommunications and cloud communications, accurately and\nin real-time detecting whether a human or an answering machine has answered an\noutbound call is of paramount importance. This problem is of particular\nsignificance during campaigns as it enhances service quality, efficiency and\ncost reduction through precise caller identification. Despite the significance\nof the field, it remains inadequately explored in the existing literature. This\npaper presents an innovative approach to answering machine detection that\nleverages transfer learning through the YAMNet model for feature extraction.\nThe YAMNet architecture facilitates the training of a recurrent-based\nclassifier, enabling real-time processing of audio streams, as opposed to\nfixed-length recordings. The results demonstrate an accuracy of over 96% on the\ntest set. Furthermore, we conduct an in-depth analysis of misclassified samples\nand reveal that an accuracy exceeding 98% can be achieved with the integration\nof a silence detection algorithm, such as the one provided by FFmpeg.", "published": "2024-10-07 21:28:09", "link": "http://arxiv.org/abs/2410.08235v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
