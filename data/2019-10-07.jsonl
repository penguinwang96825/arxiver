{"title": "Domain Differential Adaptation for Neural Machine Translation", "abstract": "Neural networks are known to be data hungry and domain sensitive, but it is\nnearly impossible to obtain large quantities of labeled data for every domain\nwe are interested in. This necessitates the use of domain adaptation\nstrategies. One common strategy encourages generalization by aligning the\nglobal distribution statistics between source and target domains, but one\ndrawback is that the statistics of different domains or tasks are inherently\ndivergent, and smoothing over these differences can lead to sub-optimal\nperformance. In this paper, we propose the framework of {\\it Domain\nDifferential Adaptation (DDA)}, where instead of smoothing over these\ndifferences we embrace them, directly modeling the difference between domains\nusing models in a related task. We then use these learned domain differentials\nto adapt models for the target task accordingly. Experimental results on domain\nadaptation for neural machine translation demonstrate the effectiveness of this\nstrategy, achieving consistent improvements over other alternative adaptation\nstrategies in multiple experimental settings.", "published": "2019-10-07 00:15:11", "link": "http://arxiv.org/abs/1910.02555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Attention? Analyzing and Remedying BiLSTM Deficiency in Modeling\n  Cross-Context for NER", "abstract": "State-of-the-art approaches of NER have used sequence-labeling BiLSTM as a\ncore module. This paper formally shows the limitation of BiLSTM in modeling\ncross-context patterns. Two types of simple cross-structures -- self-attention\nand Cross-BiLSTM -- are shown to effectively remedy the problem. On both\nOntoNotes 5.0 and WNUT 2017, clear and consistent improvements are achieved\nover bare-bone models, up to 8.7% on some of the multi-token mentions. In-depth\nanalyses across several aspects of the improvements, especially the\nidentification of multi-token mentions, are further given.", "published": "2019-10-07 03:03:08", "link": "http://arxiv.org/abs/1910.02586v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT for Evidence Retrieval and Claim Verification", "abstract": "Motivated by the promising performance of pre-trained language models, we\ninvestigate BERT in an evidence retrieval and claim verification pipeline for\nthe FEVER fact extraction and verification challenge. To this end, we propose\nto use two BERT models, one for retrieving potential evidence sentences\nsupporting or rejecting claims, and another for verifying claims based on the\npredicted evidence sets. To train the BERT retrieval system, we use pointwise\nand pairwise loss functions, and examine the effect of hard negative mining. A\nsecond BERT model is trained to classify the samples as supported, refuted, and\nnot enough information. Our system achieves a new state of the art recall of\n87.1 for retrieving top five sentences out of the FEVER documents consisting of\n50K Wikipedia pages, and scores second in the official leaderboard with the\nFEVER score of 69.7.", "published": "2019-10-07 07:58:26", "link": "http://arxiv.org/abs/1910.02655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Sentence Simplification", "abstract": "Text simplification aims at making a text easier to read and understand by\nsimplifying grammar and structure while keeping the underlying information\nidentical. It is often considered an all-purpose generic task where the same\nsimplification is suitable for all; however multiple audiences can benefit from\nsimplified text in different ways. We adapt a discrete parametrization\nmechanism that provides explicit control on simplification systems based on\nSequence-to-Sequence models. As a result, users can condition the\nsimplifications returned by a model on attributes such as length, amount of\nparaphrasing, lexical complexity and syntactic complexity. We also show that\ncarefully chosen values of these attributes allow out-of-the-box\nSequence-to-Sequence models to outperform their standard counterparts on\nsimplification benchmarks. Our model, which we call ACCESS (as shorthand for\nAudienCe-CEntric Sentence Simplification), establishes the state of the art at\n41.87 SARI on the WikiLarge test set, a +1.42 improvement over the best\npreviously reported score.", "published": "2019-10-07 09:00:26", "link": "http://arxiv.org/abs/1910.02677v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaskParse@Deskin at SemEval-2019 Task 1: Cross-lingual UCCA Semantic\n  Parsing using Recursive Masked Sequence Tagging", "abstract": "This paper describes our recursive system for SemEval-2019 \\textit{ Task 1:\nCross-lingual Semantic Parsing with UCCA}. Each recursive step consists of two\nparts. We first perform semantic parsing using a sequence tagger to estimate\nthe probabilities of the UCCA categories in the sentence. Then, we apply a\ndecoding policy which interprets these probabilities and builds the graph\nnodes. Parsing is done recursively, we perform a first inference on the\nsentence to extract the main scenes and links and then we recursively apply our\nmodel on the sentence using a masking feature that reflects the decisions made\nin previous steps. Process continues until the terminal nodes are reached. We\nchoose a standard neural tagger and we focused on our recursive parsing\nstrategy and on the cross lingual transfer problem to develop a robust model\nfor the French language, using only few training samples.", "published": "2019-10-07 11:38:28", "link": "http://arxiv.org/abs/1910.02733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting a FrameNet Semantic Parser for Spoken Language Understanding\n  Using Adversarial Learning", "abstract": "This paper presents a new semantic frame parsing model, based on Berkeley\nFrameNet, adapted to process spoken documents in order to perform information\nextraction from broadcast contents. Building upon previous work that had shown\nthe effectiveness of adversarial learning for domain generalization in the\ncontext of semantic parsing of encyclopedic written documents, we propose to\nextend this approach to elocutionary style generalization. The underlying\nquestion throughout this study is whether adversarial learning can be used to\ncombine data from different sources and train models on a higher level of\nabstraction in order to increase their robustness to lexical and stylistic\nvariations as well as automatic speech recognition errors. The proposed\nstrategy is evaluated on a French corpus of encyclopedic written documents and\na smaller corpus of radio podcast transcriptions, both annotated with a\nFrameNet paradigm. We show that adversarial learning increases all models\ngeneralization capabilities both on manual and automatic speech transcription\nas well as on encyclopedic data.", "published": "2019-10-07 11:41:14", "link": "http://arxiv.org/abs/1910.02734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial reconstruction for Multi-modal Machine Translation", "abstract": "Even with the growing interest in problems at the intersection of Computer\nVision and Natural Language, grounding (i.e. identifying) the components of a\nstructured description in an image still remains a challenging task. This\ncontribution aims to propose a model which learns grounding by reconstructing\nthe visual features for the Multi-modal translation task. Previous works have\npartially investigated standard approaches such as regression methods to\napproximate the reconstruction of a visual input. In this paper, we propose a\ndifferent and novel approach which learns grounding by adversarial feedback. To\ndo so, we modulate our network following the recent promising adversarial\narchitectures and evaluate how the adversarial response from a visual\nreconstruction as an auxiliary task helps the model in its learning. We report\nthe highest scores in term of BLEU and METEOR metrics on the different\ndatasets.", "published": "2019-10-07 13:08:07", "link": "http://arxiv.org/abs/1910.02766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Case Study on Combining ASR and Visual Features for Generating\n  Instructional Video Captions", "abstract": "Instructional videos get high-traffic on video sharing platforms, and prior\nwork suggests that providing time-stamped, subtask annotations (e.g., \"heat the\noil in the pan\") improves user experiences. However, current automatic\nannotation methods based on visual features alone perform only slightly better\nthan constant prediction. Taking cues from prior work, we show that we can\nimprove performance significantly by considering automatic speech recognition\n(ASR) tokens as input. Furthermore, jointly modeling ASR tokens and visual\nfeatures results in higher performance compared to training individually on\neither modality. We find that unstated background information is better\nexplained by visual features, whereas fine-grained distinctions (e.g., \"add\noil\" vs. \"add olive oil\") are disambiguated more easily via ASR tokens.", "published": "2019-10-07 17:39:39", "link": "http://arxiv.org/abs/1910.02930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Neural Machine Translation Robustness via Data Augmentation:\n  Beyond Back Translation", "abstract": "Neural Machine Translation (NMT) models have been proved strong when\ntranslating clean texts, but they are very sensitive to noise in the input.\nImproving NMT models robustness can be seen as a form of \"domain\" adaption to\nnoise. The recently created Machine Translation on Noisy Text task corpus\nprovides noisy-clean parallel data for a few language pairs, but this data is\nvery limited in size and diversity. The state-of-the-art approaches are heavily\ndependent on large volumes of back-translated data. This paper has two main\ncontributions: Firstly, we propose new data augmentation methods to extend\nlimited noisy data and further improve NMT robustness to noise while keeping\nthe models small. Secondly, we explore the effect of utilizing noise from\nexternal data in the form of speech transcripts and show that it could help\nrobustness.", "published": "2019-10-07 18:55:32", "link": "http://arxiv.org/abs/1910.03009v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capturing Argument Interaction in Semantic Role Labeling with Capsule\n  Networks", "abstract": "Semantic role labeling (SRL) involves extracting propositions (i.e.\npredicates and their typed arguments) from natural language sentences.\nState-of-the-art SRL models rely on powerful encoders (e.g., LSTMs) and do not\nmodel non-local interaction between arguments. We propose a new approach to\nmodeling these interactions while maintaining efficient inference.\nSpecifically, we use Capsule Networks: each proposition is encoded as a tuple\nof \\textit{capsules}, one capsule per argument type (i.e. role). These tuples\nserve as embeddings of entire propositions. In every network layer, the\ncapsules interact with each other and with representations of words in the\nsentence. Each iteration results in updated proposition embeddings and updated\npredictions about the SRL structure. Our model substantially outperforms the\nnon-refinement baseline model on all 7 CoNLL-2019 languages and achieves\nstate-of-the-art results on 5 languages (including English) for dependency SRL.\nWe analyze the types of mistakes corrected by the refinement procedure. For\nexample, each role is typically (but not always) filled with at most one\nargument. Whereas enforcing this approximate constraint is not useful with the\nmodern SRL system, iterative procedure corrects the mistakes by capturing this\nintuition in a flexible and context-sensitive way.", "published": "2019-10-07 23:46:49", "link": "http://arxiv.org/abs/1910.03136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-hop Question Answering via Reasoning Chains", "abstract": "Multi-hop question answering requires models to gather information from\ndifferent parts of a text to answer a question. Most current approaches learn\nto address this task in an end-to-end way with neural networks, without\nmaintaining an explicit representation of the reasoning process. We propose a\nmethod to extract a discrete reasoning chain over the text, which consists of a\nseries of sentences leading to the answer. We then feed the extracted chains to\na BERT-based QA model to do final answer prediction. Critically, we do not rely\non gold annotated chains or \"supporting facts:\" at training time, we derive\npseudogold reasoning chains using heuristics based on named entity recognition\nand coreference resolution. Nor do we rely on these annotations at test time,\nas our model learns to extract chains from raw text alone. We test our approach\non two recently proposed large multi-hop question answering datasets: WikiHop\nand HotpotQA, and achieve state-of-art performance on WikiHop and strong\nperformance on HotpotQA. Our analysis shows the properties of chains that are\ncrucial for high performance: in particular, modeling extraction sequentially\nis important, as is dealing with each candidate sentence in a context-aware\nway. Furthermore, human evaluation shows that our extracted chains allow humans\nto give answers with high confidence, indicating that these are a strong\nintermediate abstraction for this task.", "published": "2019-10-07 04:58:43", "link": "http://arxiv.org/abs/1910.02610v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization for Primitive Substitutions", "abstract": "Compositional generalization is a basic mechanism in human language learning,\nbut current neural networks lack such ability. In this paper, we conduct\nfundamental research for encoding compositionality in neural networks.\nConventional methods use a single representation for the input sentence, making\nit hard to apply prior knowledge of compositionality. In contrast, our approach\nleverages such knowledge with two representations, one generating attention\nmaps, and the other mapping attended input words to output symbols. We reduce\nthe entropy in each representation to improve generalization. Our experiments\ndemonstrate significant improvements over the conventional methods in five NLP\ntasks including instruction learning and machine translation. In the SCAN\ndomain, it boosts accuracies from 14.0% to 98.8% in Jump task, and from 92.0%\nto 99.7% in TurnLeft task. It also beats human performance on a few-shot\nlearning task. We hope the proposed approach can help ease future research\ntowards human-level compositional language learning.", "published": "2019-10-07 05:27:27", "link": "http://arxiv.org/abs/1910.02612v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Relation Extraction with Knowledge-attention", "abstract": "While attention mechanisms have been proven to be effective in many NLP\ntasks, majority of them are data-driven. We propose a novel knowledge-attention\nencoder which incorporates prior knowledge from external lexical resources into\ndeep neural networks for relation extraction task. Furthermore, we present\nthree effective ways of integrating knowledge-attention with self-attention to\nmaximize the utilization of both knowledge and data. The proposed relation\nextraction system is end-to-end and fully attention-based. Experiment results\nshow that the proposed knowledge-attention mechanism has complementary\nstrengths with self-attention, and our integrated models outperform existing\nCNN, RNN, and self-attention based models. State-of-the-art performance is\nachieved on TACRED, a complex and large-scale relation extraction dataset.", "published": "2019-10-07 11:08:24", "link": "http://arxiv.org/abs/1910.02724v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Leveraging the Visual Modality for Neural Machine Translation", "abstract": "Leveraging the visual modality effectively for Neural Machine Translation\n(NMT) remains an open problem in computational linguistics. Recently, Caglayan\net al. posit that the observed gains are limited mainly due to the very simple,\nshort, repetitive sentences of the Multi30k dataset (the only multimodal MT\ndataset available at the time), which renders the source text sufficient for\ncontext. In this work, we further investigate this hypothesis on a new large\nscale multimodal Machine Translation (MMT) dataset, How2, which has 1.57 times\nlonger mean sentence length than Multi30k and no repetition. We propose and\nevaluate three novel fusion techniques, each of which is designed to ensure the\nutilization of visual context at different stages of the Sequence-to-Sequence\ntransduction pipeline, even under full linguistic context. However, we still\nobtain only marginal gains under full linguistic context and posit that visual\nembeddings extracted from deep vision models (ResNet for Multi30k, ResNext for\nHow2) do not lend themselves to increasing the discriminativeness between the\nvocabulary elements at token level prediction in NMT. We demonstrate this\nqualitatively by analyzing attention distribution and quantitatively through\nPrincipal Component Analysis, arriving at the conclusion that it is the quality\nof the visual embeddings rather than the length of sentences, which need to be\nimproved in existing MMT datasets.", "published": "2019-10-07 12:42:09", "link": "http://arxiv.org/abs/1910.02754v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parallel Iterative Edit Models for Local Sequence Transduction", "abstract": "We present a Parallel Iterative Edit (PIE) model for the problem of local\nsequence transduction arising in tasks like Grammatical error correction (GEC).\nRecent approaches are based on the popular encoder-decoder (ED) model for\nsequence to sequence learning. The ED model auto-regressively captures full\ndependency among output tokens but is slow due to sequential decoding. The PIE\nmodel does parallel decoding, giving up the advantage of modelling full\ndependency in the output, yet it achieves accuracy competitive with the ED\nmodel for four reasons: 1.~predicting edits instead of tokens, 2.~labeling\nsequences instead of generating sequences, 3.~iteratively refining predictions\nto capture dependencies, and 4.~factorizing logits over edits and their token\nargument to harness pre-trained language models like BERT. Experiments on tasks\nspanning GEC, OCR correction and spell correction demonstrate that the PIE\nmodel is an accurate and significantly faster alternative for local sequence\ntransduction.", "published": "2019-10-07 16:37:31", "link": "http://arxiv.org/abs/1910.02893v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Correlations between Word Vector Sets", "abstract": "Similarity measures based purely on word embeddings are comfortably competing\nwith much more sophisticated deep learning and expert-engineered systems on\nunsupervised semantic textual similarity (STS) tasks. In contrast to commonly\nused geometric approaches, we treat a single word embedding as e.g. 300\nobservations from a scalar random variable. Using this paradigm, we first\nillustrate that similarities derived from elementary pooling operations and\nclassic correlation coefficients yield excellent results on standard STS\nbenchmarks, outperforming many recently proposed methods while being much\nfaster and trivial to implement. Next, we demonstrate how to avoid pooling\noperations altogether and compare sets of word embeddings directly via\ncorrelation operators between reproducing kernel Hilbert spaces. Just like\ncosine similarity is used to compare individual word vectors, we introduce a\nnovel application of the centered kernel alignment (CKA) as a natural\ngeneralisation of squared cosine similarity for sets of word vectors. Likewise,\nCKA is very easy to implement and enjoys very strong empirical results.", "published": "2019-10-07 16:44:32", "link": "http://arxiv.org/abs/1910.02902v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Commonsense Knowledge Base Completion with Structural and Semantic\n  Context", "abstract": "Automatic KB completion for commonsense knowledge graphs (e.g., ATOMIC and\nConceptNet) poses unique challenges compared to the much studied conventional\nknowledge bases (e.g., Freebase). Commonsense knowledge graphs use free-form\ntext to represent nodes, resulting in orders of magnitude more nodes compared\nto conventional KBs (18x more nodes in ATOMIC compared to Freebase\n(FB15K-237)). Importantly, this implies significantly sparser graph structures\n- a major challenge for existing KB completion methods that assume densely\nconnected graphs over a relatively smaller set of nodes. In this paper, we\npresent novel KB completion models that can address these challenges by\nexploiting the structural and semantic context of nodes. Specifically, we\ninvestigate two key ideas: (1) learning from local graph structure, using graph\nconvolutional networks and automatic graph densification and (2) transfer\nlearning from pre-trained language models to knowledge graphs for enhanced\ncontextual representation of knowledge. We describe our method to incorporate\ninformation from both these sources in a joint model and provide the first\nempirical results for KB completion on ATOMIC and evaluation with ranking\nmetrics on ConceptNet. Our results demonstrate the effectiveness of language\nmodel representations in boosting link prediction performance and the\nadvantages of learning from local graph structure (+1.5 points in MRR for\nConceptNet) when training on subgraphs for computational efficiency. Further\nanalysis on model predictions shines light on the types of commonsense\nknowledge that language models capture well.", "published": "2019-10-07 17:16:04", "link": "http://arxiv.org/abs/1910.02915v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Make Up Your Mind! Adversarial Generation of Inconsistent Natural\n  Language Explanations", "abstract": "To increase trust in artificial intelligence systems, a promising research\ndirection consists of designing neural models capable of generating natural\nlanguage explanations for their predictions. In this work, we show that such\nmodels are nonetheless prone to generating mutually inconsistent explanations,\nsuch as \"Because there is a dog in the image\" and \"Because there is no dog in\nthe [same] image\", exposing flaws in either the decision-making process of the\nmodel or in the generation of the explanations. We introduce a simple yet\neffective adversarial framework for sanity checking models against the\ngeneration of inconsistent natural language explanations. Moreover, as part of\nthe framework, we address the problem of adversarial attacks with full target\nsequences, a scenario that was not previously addressed in sequence-to-sequence\nattacks. Finally, we apply our framework on a state-of-the-art neural natural\nlanguage inference model that provides natural language explanations for its\npredictions. Our framework shows that this model is capable of generating a\nsignificant number of inconsistent explanations.", "published": "2019-10-07 20:14:23", "link": "http://arxiv.org/abs/1910.03065v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SMArT: Training Shallow Memory-aware Transformers for Robotic\n  Explainability", "abstract": "The ability to generate natural language explanations conditioned on the\nvisual perception is a crucial step towards autonomous agents which can explain\nthemselves and communicate with humans. While the research efforts in image and\nvideo captioning are giving promising results, this is often done at the\nexpense of the computational requirements of the approaches, limiting their\napplicability to real contexts. In this paper, we propose a fully-attentive\ncaptioning algorithm which can provide state-of-the-art performances on\nlanguage generation while restricting its computational demands. Our model is\ninspired by the Transformer model and employs only two Transformer layers in\nthe encoding and decoding stages. Further, it incorporates a novel memory-aware\nencoding of image regions. Experiments demonstrate that our approach achieves\ncompetitive results in terms of caption quality while featuring reduced\ncomputational demands. Further, to evaluate its applicability on autonomous\nagents, we conduct experiments on simulated scenes taken from the perspective\nof domestic robots.", "published": "2019-10-07 18:03:14", "link": "http://arxiv.org/abs/1910.02974v3", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Rekall: Specifying Video Events using Compositions of Spatiotemporal\n  Labels", "abstract": "Many real-world video analysis applications require the ability to identify\ndomain-specific events in video, such as interviews and commercials in TV news\nbroadcasts, or action sequences in film. Unfortunately, pre-trained models to\ndetect all the events of interest in video may not exist, and training new\nmodels from scratch can be costly and labor-intensive. In this paper, we\nexplore the utility of specifying new events in video in a more traditional\nmanner: by writing queries that compose outputs of existing, pre-trained\nmodels. To write these queries, we have developed Rekall, a library that\nexposes a data model and programming model for compositional video event\nspecification. Rekall represents video annotations from different sources\n(object detectors, transcripts, etc.) as spatiotemporal labels associated with\ncontinuous volumes of spacetime in a video, and provides operators for\ncomposing labels into queries that model new video events. We demonstrate the\nuse of Rekall in analyzing video from cable TV news broadcasts, films,\nstatic-camera vehicular video streams, and commercial autonomous vehicle logs.\nIn these efforts, domain experts were able to quickly (in a few hours to a day)\nauthor queries that enabled the accurate detection of new events (on par with,\nand in some cases much more accurate than, learned approaches) and to rapidly\nretrieve video clips for human-in-the-loop tasks such as video content curation\nand training data curation. Finally, in a user study, novice users of Rekall\nwere able to author queries to retrieve new events in video given just one hour\nof query development time.", "published": "2019-10-07 18:18:37", "link": "http://arxiv.org/abs/1910.02993v1", "categories": ["cs.DB", "cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.DB"}
{"title": "Gunrock: A Social Bot for Complex and Engaging Long Conversations", "abstract": "Gunrock is the winner of the 2018 Amazon Alexa Prize, as evaluated by\ncoherence and engagement from both real users and Amazon-selected expert\nconversationalists. We focus on understanding complex sentences and having\nin-depth conversations in open domains. In this paper, we introduce some\ninnovative system designs and related validation analysis. Overall, we found\nthat users produce longer sentences to Gunrock, which are directly related to\nusers' engagement (e.g., ratings, number of turns). Additionally, users'\nbackstory queries about Gunrock are positively correlated to user satisfaction.\nFinally, we found dialog flows that interleave facts and personal opinions and\nstories lead to better user satisfaction.", "published": "2019-10-07 19:24:36", "link": "http://arxiv.org/abs/1910.03042v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The Query Translation Landscape: a Survey", "abstract": "Whereas the availability of data has seen a manyfold increase in past years,\nits value can be only shown if the data variety is effectively tackled ---one\nof the prominent Big Data challenges. The lack of data interoperability limits\nthe potential of its collective use for novel applications. Achieving\ninteroperability through the full transformation and integration of diverse\ndata structures remains an ideal that is hard, if not impossible, to achieve.\nInstead, methods that can simultaneously interpret different types of data\navailable in different data structures and formats have been explored. On the\nother hand, many query languages have been designed to enable users to interact\nwith the data, from relational, to object-oriented, to hierarchical, to the\nmultitude emerging NoSQL languages. Therefore, the interoperability issue could\nbe solved not by enforcing physical data transformation, but by looking at\ntechniques that are able to query heterogeneous sources using one uniform\nlanguage. Both industry and research communities have been keen to develop such\ntechniques, which require the translation of a chosen 'universal' query\nlanguage to the various data model specific query languages that make the\nunderlying data accessible. In this article, we survey more than forty query\ntranslation methods and tools for popular query languages, and classify them\naccording to eight criteria. In particular, we study which query language is a\nmost suitable candidate for that 'universal' query language. Further, the\nresults enable us to discover the weakly addressed and unexplored translation\npaths, to discover gaps and to learn lessons that can benefit future research\nin the area.", "published": "2019-10-07 22:37:33", "link": "http://arxiv.org/abs/1910.03118v1", "categories": ["cs.DB", "cs.CL", "cs.IR"], "primary_category": "cs.DB"}
{"title": "Overcoming the Rare Word Problem for Low-Resource Language Pairs in\n  Neural Machine Translation", "abstract": "Among the six challenges of neural machine translation (NMT) coined by (Koehn\nand Knowles, 2017), rare-word problem is considered the most severe one,\nespecially in translation of low-resource languages. In this paper, we propose\nthree solutions to address the rare words in neural machine translation\nsystems. First, we enhance source context to predict the target words by\nconnecting directly the source embeddings to the output of the attention\ncomponent in NMT. Second, we propose an algorithm to learn morphology of\nunknown words for English in supervised way in order to minimize the adverse\neffect of rare-word problem. Finally, we exploit synonymous relation from the\nWordNet to overcome out-of-vocabulary (OOV) problem of NMT. We evaluate our\napproaches on two low-resource language pairs: English-Vietnamese and\nJapanese-Vietnamese. In our experiments, we have achieved significant\nimprovements of up to roughly +1.0 BLEU points in both language pairs.", "published": "2019-10-07 03:11:13", "link": "http://arxiv.org/abs/1910.03467v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "SentiCite: An Approach for Publication Sentiment Analysis", "abstract": "With the rapid growth in the number of scientific publications, year after\nyear, it is becoming increasingly difficult to identify quality authoritative\nwork on a single topic. Though there is an availability of scientometric\nmeasures which promise to offer a solution to this problem, these measures are\nmostly quantitative and rely, for instance, only on the number of times an\narticle is cited. With this approach, it becomes irrelevant if an article is\ncited 10 times in a positive, negative or neutral way. In this context, it is\nquite important to study the qualitative aspect of a citation to understand its\nsignificance. This paper presents a novel system for sentiment analysis of\ncitations in scientific documents (SentiCite) and is also capable of detecting\nnature of citations by targeting the motivation behind a citation, e.g.,\nreference to a dataset, reading reference. Furthermore, the paper also presents\ntwo datasets (SentiCiteDB and IntentCiteDB) containing about 2,600 citations\nwith their ground truth for sentiment and nature of citation. SentiCite along\nwith other state-of-the-art methods for sentiment analysis are evaluated on the\npresented datasets. Evaluation results reveal that SentiCite outperforms\nstate-of-the-art methods for sentiment analysis in scientific publications by\nachieving a F1-measure of 0.71.", "published": "2019-10-07 06:49:52", "link": "http://arxiv.org/abs/1910.03498v1", "categories": ["cs.CL", "cs.DB", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Effective Acoustic Energy Sensing Exploitation for Target Sources\n  Localization in Urban Acoustic Scenes", "abstract": "This letter proposes a new approach to improve the accuracy of the\nEnergy-based source localization methods in urban acoustic scenes. The proposed\nacoustic energy sensing flow estimation (ESFE) uses the sensors signal\nnonstationarity degree to determine the area with highest energy concentration\nin the scenes. The ESFE is applied to different acoustic scenes and yields to\nsource localization accuracy improvement with computational complexity\nreduction. The experiments results show that the proposed scheme leads to\nsignificant improvement in source localization accuracy.", "published": "2019-10-07 10:32:24", "link": "http://arxiv.org/abs/1910.02709v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Impulsive Noise Detection for Intelligibility and Quality Improvement of\n  Speech Enhancement Methods Applied in Time-Domain", "abstract": "This letter introduces a novel speech enhancement method in the Hilbert-Huang\nTransform domain to mitigate the effects of acoustic impulsive noises. The\nestimation and selection of noise components is based on the impulsiveness\nindex of decomposition modes. Speech enhancement experiments are conducted\nconsidering five acoustic noises with different impulsiveness index and\nnon-stationarity degrees under various signal-to-noise ratios. Three speech\nenhancement algorithms are adopted as baseline in the evaluation analysis\nconsidering spectral and time domains. The proposed solution achieves the best\nresults in terms of objective quality measures and similar speech\nintelligibility rates to the competitive methods.", "published": "2019-10-07 10:32:50", "link": "http://arxiv.org/abs/1910.02710v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adaptive Reverberation Absorption using Non-stationary Masking\n  Components Detection for Intelligibility Improvement", "abstract": "This letter proposes a new time domain absorption approach designed to reduce\nmasking components of speech signals under noisy-reverberant conditions. In\nthis method, the non-stationarity of corrupted signal segments is used to\ndetect masking distortions based on a defined threshold. The nonstationarity is\nobjectively measured and is also adopted to determine the absorption procedure.\nAdditionally, no prior knowledge of speech statistics or of the room\ninformation is required for this technique. Three intelligibility measures\n(ESII, ASIIST, SRMRnorm) and a perceptual listening test are used for\nevaluation. The experiments results show that the proposed scheme leads to a\nhigher intelligibility improvement when compared to competing methods.", "published": "2019-10-07 10:33:08", "link": "http://arxiv.org/abs/1910.02712v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ultrasonic identification technique in recycling of lithium ion\n  batteries", "abstract": "The recycling of lithium ion batteries has been mentioned as one of the\nnear-future waste management necessities. In order for recycling to be\neconomically viable, straightforward and cost effective techniques need to be\ndeveloped to separate the individual materials in a composite electrode.\nUltrasonic separation might be such a technique, provided that lithium ion\nbattery microparticles respond predictably to a sound field. Lithium ion\nbattery cathodes contain hydrophobic carbon. Owing to the incompressibility of\na solid, the thin gaseous layer surrounding these hydrophobic particles must\noscillate asymmetrically, when subjected to ultrasound. Consequently, the\nharmonic content of the ultrasound signal radiated from hydrophobic\nmicroparticles must be higher than that from hydrophilic microparticles with\nthe same size. The question of whether the harmonic signal response generated\nby physical hydrophobic microparticles present in lithium ion battery cathodes\nis higher than the harmonic response of other component materials in the\ncathode is the focus of this paper. The scattering response of cathode\nmaterials subjected to 1-MHz ultrasound was measured and compared. The cathode\nmaterials C65, PVDF, and NMC respond differently to 1-MHz ultrasound. The\nsuperharmonic response of C65 has been attributed to asymmetric oscillations\nowing to its hydrophobicity. In addition, C65 hydrophobic microparticles might\nbe suitable candidates for harmonic imaging.", "published": "2019-10-07 07:14:33", "link": "http://arxiv.org/abs/2001.09942v1", "categories": ["physics.app-ph", "cs.SD", "eess.AS"], "primary_category": "physics.app-ph"}
