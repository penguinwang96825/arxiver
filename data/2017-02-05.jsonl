{"title": "An Empirical Evaluation of Zero Resource Acoustic Unit Discovery", "abstract": "Acoustic unit discovery (AUD) is a process of automatically identifying a\ncategorical acoustic unit inventory from speech and producing corresponding\nacoustic unit tokenizations. AUD provides an important avenue for unsupervised\nacoustic model training in a zero resource setting where expert-provided\nlinguistic knowledge and transcribed speech are unavailable. Therefore, to\nfurther facilitate zero-resource AUD process, in this paper, we demonstrate\nacoustic feature representations can be significantly improved by (i)\nperforming linear discriminant analysis (LDA) in an unsupervised self-trained\nfashion, and (ii) leveraging resources of other languages through building a\nmultilingual bottleneck (BN) feature extractor to give effective cross-lingual\ngeneralization. Moreover, we perform comprehensive evaluations of AUD efficacy\non multiple downstream speech applications, and their correlated performance\nsuggests that AUD evaluations are feasible using different alternative language\nresources when only a subset of these evaluation resources can be available in\ntypical zero resource applications.", "published": "2017-02-05 02:22:31", "link": "http://arxiv.org/abs/1702.01360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prepositions in Context", "abstract": "Prepositions are highly polysemous, and their variegated senses encode\nsignificant semantic information. In this paper we match each preposition's\ncomplement and attachment and their interplay crucially to the geometry of the\nword vectors to the left and right of the preposition. Extracting such features\nfrom the vast number of instances of each preposition and clustering them makes\nfor an efficient preposition sense disambigution (PSD) algorithm, which is\ncomparable to and better than state-of-the-art on two benchmark datasets. Our\nreliance on no external linguistic resource allows us to scale the PSD\nalgorithm to a large WikiCorpus and learn sense-specific preposition\nrepresentations -- which we show to encode semantic relations and paraphrasing\nof verb particle compounds, via simple vector operations.", "published": "2017-02-05 23:16:01", "link": "http://arxiv.org/abs/1702.01466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "All-but-the-Top: Simple and Effective Postprocessing for Word\n  Representations", "abstract": "Real-valued word representations have transformed NLP applications; popular\nexamples are word2vec and GloVe, recognized for their ability to capture\nlinguistic regularities. In this paper, we demonstrate a {\\em very simple}, and\nyet counter-intuitive, postprocessing technique -- eliminate the common mean\nvector and a few top dominating directions from the word vectors -- that\nrenders off-the-shelf representations {\\em even stronger}. The postprocessing\nis empirically validated on a variety of lexical-level intrinsic tasks (word\nsimilarity, concept categorization, word analogy) and sentence-level tasks\n(semantic textural similarity and { text classification}) on multiple datasets\nand with a variety of representation methods and hyperparameter choices in\nmultiple languages; in each case, the processed representations are\nconsistently better than the original ones.", "published": "2017-02-05 15:43:07", "link": "http://arxiv.org/abs/1702.01417v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
