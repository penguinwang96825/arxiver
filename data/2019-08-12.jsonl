{"title": "A Finnish News Corpus for Named Entity Recognition", "abstract": "We present a corpus of Finnish news articles with a manually prepared named\nentity annotation. The corpus consists of 953 articles (193,742 word tokens)\nwith six named entity classes (organization, location, person, product, event,\nand date). The articles are extracted from the archives of Digitoday, a Finnish\nonline technology news source. The corpus is available for research purposes.\nWe present baseline experiments on the corpus using a rule-based and two deep\nlearning systems on two, in-domain and out-of-domain, test sets.", "published": "2019-08-12 15:49:57", "link": "http://arxiv.org/abs/1908.04212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Tag Recommendations for E-Book Annotation Using a Semantic\n  Similarity Metric", "abstract": "In this paper, we present our work to support publishers and editors in\nfinding descriptive tags for e-books through tag recommendations. We propose a\nhybrid tag recommendation system for e-books, which leverages search query\nterms from Amazon users and e-book metadata, which is assigned by publishers\nand editors. Our idea is to mimic the vocabulary of users in Amazon, who search\nfor and review e-books, and to combine these search terms with editor tags in a\nhybrid tag recommendation approach. In total, we evaluate 19 tag recommendation\nalgorithms on the review content of Amazon users, which reflects the readers'\nvocabulary. Our results show that we can improve the performance of tag\nrecommender systems for e-books both concerning tag recommendation accuracy,\ndiversity as well as a novel semantic similarity metric, which we also propose\nin this paper.", "published": "2019-08-12 08:04:42", "link": "http://arxiv.org/abs/1908.04042v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Active Annotation: bootstrapping annotation lexicon and guidelines for\n  supervised NLU learning", "abstract": "Natural Language Understanding (NLU) models are typically trained in a\nsupervised learning framework. In the case of intent classification, the\npredicted labels are predefined and based on the designed annotation schema\nwhile the labelling process is based on a laborious task where annotators\nmanually inspect each utterance and assign the corresponding label. We propose\nan Active Annotation (AA) approach where we combine an unsupervised learning\nmethod in the embedding space, a human-in-the-loop verification process, and\nlinguistic insights to create lexicons that can be open categories and adapted\nover time. In particular, annotators define the y-label space on-the-fly during\nthe annotation using an iterative process and without the need for prior\nknowledge about the input data. We evaluate the proposed annotation paradigm in\na real use-case NLU scenario. Results show that our Active Annotation paradigm\nachieves accurate and higher quality training data, with an annotation speed of\nan order of magnitude higher with respect to the traditional human-only driven\nbaseline annotation methodology.", "published": "2019-08-12 11:20:29", "link": "http://arxiv.org/abs/1908.04092v1", "categories": ["cs.CL", "cs.LG", "68Uxx"], "primary_category": "cs.CL"}
{"title": "On Identifiability in Transformers", "abstract": "In this paper we delve deep in the Transformer architecture by investigating\ntwo of its core components: self-attention and contextual embeddings. In\nparticular, we study the identifiability of attention weights and token\nembeddings, and the aggregation of context into hidden tokens. We show that,\nfor sequences longer than the attention head dimension, attention weights are\nnot identifiable. We propose effective attention as a complementary tool for\nimproving explanatory interpretations based on attention. Furthermore, we show\nthat input tokens retain to a large degree their identity across the model. We\nalso find evidence suggesting that identity information is mainly encoded in\nthe angle of the embeddings and gradually decreases with depth. Finally, we\ndemonstrate strong mixing of input information in the generation of contextual\nembeddings by means of a novel quantification method based on gradient\nattribution. Overall, we show that self-attention distributions are not\ndirectly interpretable and present tools to better understand and further\ninvestigate Transformer models.", "published": "2019-08-12 15:48:34", "link": "http://arxiv.org/abs/1908.04211v4", "categories": ["cs.CL", "cs.LG", "I.2.7, I.7.0", "I.2.7; I.7.0"], "primary_category": "cs.CL"}
{"title": "LSTM vs. GRU vs. Bidirectional RNN for script generation", "abstract": "Scripts are an important part of any TV series. They narrate movements,\nactions and expressions of characters. In this paper, a case study is presented\non how different sequence to sequence deep learning models perform in the task\nof generating new conversations between characters as well as new scenarios on\nthe basis of a script (previous conversations). A comprehensive comparison\nbetween these models, namely, LSTM, GRU and Bidirectional RNN is presented. All\nthe models are designed to learn the sequence of recurring characters from the\ninput sequence. Each input sequence will contain, say \"n\" characters, and the\ncorresponding targets will contain the same number of characters, except, they\nwill be shifted one character to the right. In this manner, input and output\nsequences are generated and used to train the models. A closer analysis of\nexplored models performance and efficiency is delineated with the help of graph\nplots and generated texts by taking some input string. These graphs describe\nboth, intraneural performance and interneural model performance for each model.", "published": "2019-08-12 18:39:10", "link": "http://arxiv.org/abs/1908.04332v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AmazonQA: A Review-Based Question Answering Task", "abstract": "Every day, thousands of customers post questions on Amazon product pages.\nAfter some time, if they are fortunate, a knowledgeable customer might answer\ntheir question. Observing that many questions can be answered based upon the\navailable product reviews, we propose the task of review-based QA. Given a\ncorpus of reviews and a question, the QA system synthesizes an answer. To this\nend, we introduce a new dataset and propose a method that combines information\nretrieval techniques for selecting relevant reviews (given a question) and\n\"reading comprehension\" models for synthesizing an answer (given a question and\nreview). Our dataset consists of 923k questions, 3.6M answers and 14M reviews\nacross 156k products. Building on the well-known Amazon dataset, we collect\nadditional annotations, marking each question as either answerable or\nunanswerable based on the available reviews. A deployed system could first\nclassify a question as answerable and then attempt to generate an answer.\nNotably, unlike many popular QA datasets, here, the questions, passages, and\nanswers are all extracted from real human interactions. We evaluate numerous\nmodels for answer generation and propose strong baselines, demonstrating the\nchallenging nature of this new task.", "published": "2019-08-12 20:18:50", "link": "http://arxiv.org/abs/1908.04364v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Assessing the Quality of Scientific Papers", "abstract": "A multitude of factors are responsible for the overall quality of scientific\npapers, including readability, linguistic quality, fluency,semantic complexity,\nand of course domain-specific technical factors. These factors vary from one\nfield of study to another. In this paper, we propose a measure and method for\nassessing the overall quality of the scientific papers in a particular field of\nstudy. We evaluate our method in the computer science domain, but it can be\napplied to other technical and scientific fields.Our method is based on the\ncorpus linguistics technique. This technique enables the extraction of required\ninformation and knowledge associated with a specific domain. For this purpose,\nwe have created a large corpus, consisting of papers from very high impact\nconferences. First, we analyze this corpus in order to extract rich\ndomain-specific terminology and knowledge. Then we use the acquired knowledge\nto estimate the quality of scientific papers by applying our proposed measure.\nWe examine our measure on high and low scientific impact test corpora. Our\nresults show a significant difference in the measure scores of the high and low\nimpact test corpora. Second, we develop a classifier based on our proposed\nmeasure and compare it to the baseline classifier. Our results show that the\nclassifier based on our measure over-performed the baseline classifier. Based\non the presented results the proposed measure and the technique can be used for\nautomated assessment of scientific papers.", "published": "2019-08-12 15:32:10", "link": "http://arxiv.org/abs/1908.04200v1", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "Neural Text Generation with Unlikelihood Training", "abstract": "Neural text generation is a key tool in natural language applications, but it\nis well known there are major problems at its core. In particular, standard\nlikelihood training and decoding leads to dull and repetitive outputs. While\nsome post-hoc fixes have been proposed, in particular top-$k$ and nucleus\nsampling, they do not address the fact that the token-level probabilities\npredicted by the model are poor. In this paper we show that the likelihood\nobjective itself is at fault, resulting in a model that assigns too much\nprobability to sequences containing repeats and frequent words, unlike those\nfrom the human training distribution. We propose a new objective, unlikelihood\ntraining, which forces unlikely generations to be assigned lower probability by\nthe model. We show that both token and sequence level unlikelihood training\ngive less repetitive, less dull text while maintaining perplexity, giving\nsuperior generations using standard greedy or beam search. According to human\nevaluations, our approach with standard beam search also outperforms the\ncurrently popular decoding methods of nucleus sampling or beam blocking, thus\nproviding a strong alternative to existing techniques.", "published": "2019-08-12 18:09:04", "link": "http://arxiv.org/abs/1908.04319v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Wasserstein Index Generation Model: Automatic Generation of Time-series\n  Index with Application to Economic Policy Uncertainty", "abstract": "I propose a novel method, the Wasserstein Index Generation model (WIG), to\ngenerate a public sentiment index automatically. To test the model`s\neffectiveness, an application to generate Economic Policy Uncertainty (EPU)\nindex is showcased.", "published": "2019-08-12 20:25:41", "link": "http://arxiv.org/abs/1908.04369v4", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "A Study on Angular Based Embedding Learning for Text-independent Speaker\n  Verification", "abstract": "Learning a good speaker embedding is important for many automatic speaker\nrecognition tasks, including verification, identification and diarization. The\nembeddings learned by softmax are not discriminative enough for open-set\nverification tasks. Angular based embedding learning target can achieve such\ndiscriminativeness by optimizing angular distance and adding margin penalty. We\napply several different popular angular margin embedding learning strategies in\nthis work and explicitly compare their performance on Voxceleb speaker\nrecognition dataset. Observing the fact that encouraging inter-class\nseparability is important when applying angular based embedding learning, we\npropose an exclusive inter-class regularization as a complement for angular\nbased loss. We verify the effectiveness of these methods for learning a\ndiscriminative embedding space on ASV task with several experiments. These\nmethods together, we manage to achieve an impressive result with 16.5%\nimprovement on equal error rate (EER) and 18.2% improvement on minimum\ndetection cost function comparing with baseline softmax systems.", "published": "2019-08-12 04:02:41", "link": "http://arxiv.org/abs/1908.03990v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Personal VAD: Speaker-Conditioned Voice Activity Detection", "abstract": "In this paper, we propose \"personal VAD\", a system to detect the voice\nactivity of a target speaker at the frame level. This system is useful for\ngating the inputs to a streaming on-device speech recognition system, such that\nit only triggers for the target user, which helps reduce the computational cost\nand battery consumption, especially in scenarios where a keyword detector is\nunpreferable. We achieve this by training a VAD-alike neural network that is\nconditioned on the target speaker embedding or the speaker verification score.\nFor each frame, personal VAD outputs the probabilities for three classes:\nnon-speech, target speaker speech, and non-target speaker speech. Under our\noptimal setup, we are able to train a model with only 130K parameters that\noutperforms a baseline system where individually trained standard VAD and\nspeaker recognition networks are combined to perform the same task.", "published": "2019-08-12 17:54:31", "link": "http://arxiv.org/abs/1908.04284v4", "categories": ["eess.AS", "cs.LG", "stat.ML"], "primary_category": "eess.AS"}
