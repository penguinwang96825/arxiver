{"title": "YNU-HPCC at SemEval-2020 Task 8: Using a Parallel-Channel Model for\n  Memotion Analysis", "abstract": "In recent years, the growing ubiquity of Internet memes on social media\nplatforms, such as Facebook, Instagram, and Twitter, has become a topic of\nimmense interest. However, the classification and recognition of memes is much\nmore complicated than that of social text since it involves visual cues and\nlanguage understanding. To address this issue, this paper proposed a\nparallel-channel model to process the textual and visual information in memes\nand then analyze the sentiment polarity of memes. In the shared task of\nidentifying and categorizing memes, we preprocess the dataset according to the\nlanguage behaviors on social media. Then, we adapt and fine-tune the\nBidirectional Encoder Representations from Transformers (BERT), and two types\nof convolutional neural network models (CNNs) were used to extract the features\nfrom the pictures. We applied an ensemble model that combined the BiLSTM,\nBIGRU, and Attention models to perform cross domain suggestion mining. The\nofficially released results show that our system performs better than the\nbaseline algorithm. Our team won nineteenth place in subtask A (Sentiment\nClassification). The code of this paper is availabled at :\nhttps://github.com/YuanLi95/Semveal2020-Task8-emotion-analysis.", "published": "2020-07-28 03:20:31", "link": "http://arxiv.org/abs/2007.13968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preparation of Sentiment tagged Parallel Corpus and Testing its effect\n  on Machine Translation", "abstract": "In the current work, we explore the enrichment in the machine translation\noutput when the training parallel corpus is augmented with the introduction of\nsentiment analysis. The paper discusses the preparation of the same sentiment\ntagged English-Bengali parallel corpus. The preparation of raw parallel corpus,\nsentiment analysis of the sentences and the training of a Character Based\nNeural Machine Translation model using the same has been discussed extensively\nin this paper. The output of the translation model has been compared with a\nbase-line translation model using automated metrics such as BLEU and TER as\nwell as manually.", "published": "2020-07-28 09:04:47", "link": "http://arxiv.org/abs/2007.14074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ECNU-SenseMaker at SemEval-2020 Task 4: Leveraging Heterogeneous\n  Knowledge Resources for Commonsense Validation and Explanation", "abstract": "This paper describes our system for SemEval-2020 Task 4: Commonsense\nValidation and Explanation (Wang et al., 2020). We propose a novel\nKnowledge-enhanced Graph Attention Network (KEGAT) architecture for this task,\nleveraging heterogeneous knowledge from both the structured knowledge base\n(i.e. ConceptNet) and unstructured text to better improve the ability of a\nmachine in commonsense understanding. This model has a powerful commonsense\ninference capability via utilizing suitable commonsense incorporation methods\nand upgraded data augmentation techniques. Besides, an internal sharing\nmechanism is cooperated to prohibit our model from insufficient and excessive\nreasoning for commonsense. As a result, this model performs quite well in both\nvalidation and explanation. For instance, it achieves state-of-the-art accuracy\nin the subtask called Commonsense Explanation (Multi-Choice). We officially\nname the system as ECNU-SenseMaker. Code is publicly available at\nhttps://github.com/ECNU-ICA/ECNU-SenseMaker.", "published": "2020-07-28 13:30:46", "link": "http://arxiv.org/abs/2007.14200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word embedding and neural network on grammatical gender -- A case study\n  of Swedish", "abstract": "We analyze the information provided by the word embeddings about the\ngrammatical gender in Swedish. We wish that this paper may serve as one of the\nbridges to connect the methods of computational linguistics and general\nlinguistics. Taking nominal classification in Swedish as a case study, we first\nshow how the information about grammatical gender in language can be captured\nby word embedding models and artificial neural networks. Then, we match our\nresults with previous linguistic hypotheses on assignment and usage of\ngrammatical gender in Swedish and analyze the errors made by the computational\nmodel from a linguistic perspective.", "published": "2020-07-28 13:50:17", "link": "http://arxiv.org/abs/2007.14222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Results on Russian Sentiment Datasets", "abstract": "In this study, we test standard neural network architectures (CNN, LSTM,\nBiLSTM) and recently appeared BERT architectures on previous Russian sentiment\nevaluation datasets. We compare two variants of Russian BERT and show that for\nall sentiment tasks in this study the conversational variant of Russian BERT\nperforms better. The best results were achieved by BERT-NLI model, which treats\nsentiment classification tasks as a natural language inference task. On one of\nthe datasets, this model practically achieves the human level.", "published": "2020-07-28 15:29:19", "link": "http://arxiv.org/abs/2007.14310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Ecologically Valid Research on Language User Interfaces", "abstract": "Language User Interfaces (LUIs) could improve human-machine interaction for a\nwide variety of tasks, such as playing music, getting insights from databases,\nor instructing domestic robots. In contrast to traditional hand-crafted\napproaches, recent work attempts to build LUIs in a data-driven way using\nmodern deep learning methods. To satisfy the data needs of such learning\nalgorithms, researchers have constructed benchmarks that emphasize the quantity\nof collected data at the cost of its naturalness and relevance to real-world\nLUI use cases. As a consequence, research findings on such benchmarks might not\nbe relevant for developing practical LUIs. The goal of this paper is to\nbootstrap the discussion around this issue, which we refer to as the\nbenchmarks' low ecological validity. To this end, we describe what we deem an\nideal methodology for machine learning research on LUIs and categorize five\ncommon ways in which recent benchmarks deviate from it. We give concrete\nexamples of the five kinds of deviations and their consequences. Lastly, we\noffer a number of recommendations as to how to increase the ecological validity\nof machine learning research on LUIs.", "published": "2020-07-28 19:09:11", "link": "http://arxiv.org/abs/2007.14435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring prominence of scientific work in online news as a proxy for\n  impact", "abstract": "The impact made by a scientific paper on the work of other academics has many\nestablished metrics, including metrics based on citation counts and social\nmedia commenting. However, determination of the impact of a scientific paper on\nthe wider society is less well established. For example, is it important for\nscientific work to be newsworthy? Here we present a new corpus of newspaper\narticles linked to the scientific papers that they describe. We find that\nImpact Case studies submitted to the UK Research Excellence Framework (REF)\n2014 that refer to scientific papers mentioned in newspaper articles were\nawarded a higher score in the REF assessment. The papers associated with these\ncase studies also feature prominently in the newspaper articles. We hypothesise\nthat such prominence can be a useful proxy for societal impact. We therefore\nprovide a novel baseline approach for measuring the prominence of scientific\npapers mentioned within news articles. Our measurement of prominence is based\non semantic similarity through a graph-based ranking algorithm. We find that\nscientific papers with an associated REF case study are more likely to have a\nstronger prominence score. This supports our hypothesis that linguistic\nprominence in news can be used to suggest the wider non-academic impact of\nscientific work.", "published": "2020-07-28 19:52:21", "link": "http://arxiv.org/abs/2007.14454v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "GUIR at SemEval-2020 Task 12: Domain-Tuned Contextualized Models for\n  Offensive Language Detection", "abstract": "Offensive language detection is an important and challenging task in natural\nlanguage processing. We present our submissions to the OffensEval 2020 shared\ntask, which includes three English sub-tasks: identifying the presence of\noffensive language (Sub-task A), identifying the presence of target in\noffensive language (Sub-task B), and identifying the categories of the target\n(Sub-task C). Our experiments explore using a domain-tuned contextualized\nlanguage model (namely, BERT) for this task. We also experiment with different\ncomponents and configurations (e.g., a multi-view SVM) stacked upon BERT models\nfor specific sub-tasks. Our submissions achieve F1 scores of 91.7% in Sub-task\nA, 66.5% in Sub-task B, and 63.2% in Sub-task C. We perform an ablation study\nwhich reveals that domain tuning considerably improves the classification\nperformance. Furthermore, error analysis shows common misclassification errors\nmade by our model and outlines research directions for future.", "published": "2020-07-28 20:45:43", "link": "http://arxiv.org/abs/2007.14477v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A System for Worldwide COVID-19 Information Aggregation", "abstract": "The global pandemic of COVID-19 has made the public pay close attention to\nrelated news, covering various domains, such as sanitation, treatment, and\neffects on education. Meanwhile, the COVID-19 condition is very different among\nthe countries (e.g., policies and development of the epidemic), and thus\ncitizens would be interested in news in foreign countries. We build a system\nfor worldwide COVID-19 information aggregation containing reliable articles\nfrom 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related\nwebsite dataset collected through crowdsourcing ensures the quality of the\narticles. A neural machine translation module translates articles in other\nlanguages into Japanese and English. A BERT-based topic-classifier trained on\nour article-topic pair dataset helps users find their interested information\nefficiently by putting articles into different categories.", "published": "2020-07-28 01:33:54", "link": "http://arxiv.org/abs/2008.01523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SalamNET at SemEval-2020 Task12: Deep Learning Approach for Arabic\n  Offensive Language Detection", "abstract": "This paper describes SalamNET, an Arabic offensive language detection system\nthat has been submitted to SemEval 2020 shared task 12: Multilingual Offensive\nLanguage Identification in Social Media. Our approach focuses on applying\nmultiple deep learning models and conducting in depth error analysis of results\nto provide system implications for future development considerations. To pursue\nour goal, a Recurrent Neural Network (RNN), a Gated Recurrent Unit (GRU), and\nLong-Short Term Memory (LSTM) models with different design architectures have\nbeen developed and evaluated. The SalamNET, a Bi-directional Gated Recurrent\nUnit (Bi-GRU) based model, reports a macro-F1 score of 0.83.", "published": "2020-07-28 03:47:26", "link": "http://arxiv.org/abs/2007.13974v1", "categories": ["cs.CL", "cs.LG", "Computation and Language (cs.CL)", "K.4"], "primary_category": "cs.CL"}
{"title": "TensorCoder: Dimension-Wise Attention via Tensor Representation for\n  Natural Language Modeling", "abstract": "Transformer has been widely-used in many Natural Language Processing (NLP)\ntasks and the scaled dot-product attention between tokens is a core module of\nTransformer. This attention is a token-wise design and its complexity is\nquadratic to the length of sequence, limiting its application potential for\nlong sequence tasks. In this paper, we propose a dimension-wise attention\nmechanism based on which a novel language modeling approach (namely\nTensorCoder) can be developed. The dimension-wise attention can reduce the\nattention complexity from the original $O(N^2d)$ to $O(Nd^2)$, where $N$ is the\nlength of the sequence and $d$ is the dimensionality of head. We verify\nTensorCoder on two tasks including masked language modeling and neural machine\ntranslation. Compared with the original Transformer, TensorCoder not only\ngreatly reduces the calculation of the original model but also obtains improved\nperformance on masked language modeling task (in PTB dataset) and comparable\nperformance on machine translation tasks.", "published": "2020-07-28 13:42:02", "link": "http://arxiv.org/abs/2008.01547v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Defining and Evaluating Fair Natural Language Generation", "abstract": "Our work focuses on the biases that emerge in the natural language generation\n(NLG) task of sentence completion. In this paper, we introduce a framework of\nfairness for NLG followed by an evaluation of gender biases in two\nstate-of-the-art language models. Our analysis provides a theoretical\nformulation for biases in NLG and empirical evidence that existing language\ngeneration models embed gender bias.", "published": "2020-07-28 04:11:10", "link": "http://arxiv.org/abs/2008.01548v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variants of BERT, Random Forests and SVM approach for Multimodal\n  Emotion-Target Sub-challenge", "abstract": "Emotion recognition has become a major problem in computer vision in recent\nyears that made a lot of effort by researchers to overcome the difficulties in\nthis task. In the field of affective computing, emotion recognition has a wide\nrange of applications, such as healthcare, robotics, human-computer\ninteraction. Due to its practical importance for other tasks, many techniques\nand approaches have been investigated for different problems and various data\nsources. Nevertheless, comprehensive fusion of the audio-visual and language\nmodalities to get the benefits from them is still a problem to solve. In this\npaper, we present and discuss our classification methodology for MuSe-Topic\nSub-challenge, as well as the data and results. For the topic classification,\nwe ensemble two language models which are ALBERT and RoBERTa to predict 10\nclasses of topics. Moreover, for the classification of valence and arousal, SVM\nand Random forests are employed in conjunction with feature selection to\nenhance the performance.", "published": "2020-07-28 01:15:50", "link": "http://arxiv.org/abs/2007.13928v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Big Bird: Transformers for Longer Sequences", "abstract": "Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is\nthe quadratic dependency (mainly in terms of memory) on the sequence length due\nto their full attention mechanism. To remedy this, we propose, BigBird, a\nsparse attention mechanism that reduces this quadratic dependency to linear. We\nshow that BigBird is a universal approximator of sequence functions and is\nTuring complete, thereby preserving these properties of the quadratic, full\nattention model. Along the way, our theoretical analysis reveals some of the\nbenefits of having $O(1)$ global tokens (such as CLS), that attend to the\nentire sequence as part of the sparse attention mechanism. The proposed sparse\nattention can handle sequences of length up to 8x of what was previously\npossible using similar hardware. As a consequence of the capability to handle\nlonger context, BigBird drastically improves performance on various NLP tasks\nsuch as question answering and summarization. We also propose novel\napplications to genomics data.", "published": "2020-07-28 08:34:04", "link": "http://arxiv.org/abs/2007.14062v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Emotion Correlation Mining Through Deep Learning Models on Natural\n  Language Text", "abstract": "Emotion analysis has been attracting researchers' attention. Most previous\nworks in the artificial intelligence field focus on recognizing emotion rather\nthan mining the reason why emotions are not or wrongly recognized. Correlation\namong emotions contributes to the failure of emotion recognition. In this\npaper, we try to fill the gap between emotion recognition and emotion\ncorrelation mining through natural language text from web news. Correlation\namong emotions, expressed as the confusion and evolution of emotion, is\nprimarily caused by human emotion cognitive bias. To mine emotion correlation\nfrom emotion recognition through text, three kinds of features and two deep\nneural network models are presented. The emotion confusion law is extracted\nthrough orthogonal basis. The emotion evolution law is evaluated from three\nperspectives, one-step shift, limited-step shifts, and shortest path transfer.\nThe method is validated using three datasets-the titles, the bodies, and the\ncomments of news articles, covering both objective and subjective texts in\nvarying lengths (long and short). The experimental results show that, in\nsubjective comments, emotions are easily mistaken as anger. Comments tend to\narouse emotion circulations of love-anger and sadness-anger. In objective news,\nit is easy to recognize text emotion as love and cause fear-joy circulation.\nThat means, journalists may try to attract attention using fear and joy words\nbut arouse the emotion love instead; After news release, netizens generate\nemotional comments to express their intense emotions, i.e., anger, sadness, and\nlove. These findings could provide insights for applications regarding\naffective interaction such as network public sentiment, social media\ncommunication, and human-computer interaction.", "published": "2020-07-28 08:59:16", "link": "http://arxiv.org/abs/2007.14071v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BUT-FIT at SemEval-2020 Task 5: Automatic detection of counterfactual\n  statements with deep pre-trained language representation models", "abstract": "This paper describes BUT-FIT's submission at SemEval-2020 Task 5: Modelling\nCausal Reasoning in Language: Detecting Counterfactuals. The challenge focused\non detecting whether a given statement contains a counterfactual (Subtask 1)\nand extracting both antecedent and consequent parts of the counterfactual from\nthe text (Subtask 2). We experimented with various state-of-the-art language\nrepresentation models (LRMs). We found RoBERTa LRM to perform the best in both\nsubtasks. We achieved the first place in both exact match and F1 for Subtask 2\nand ranked second for Subtask 1.", "published": "2020-07-28 11:16:11", "link": "http://arxiv.org/abs/2007.14128v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Construction and Usage of a Human Body Common Coordinate Framework\n  Comprising Clinical, Semantic, and Spatial Ontologies", "abstract": "The National Institutes of Health's (NIH) Human Biomolecular Atlas Program\n(HuBMAP) aims to create a comprehensive high-resolution atlas of all the cells\nin the healthy human body. Multiple laboratories across the United States are\ncollecting tissue specimens from different organs of donors who vary in sex,\nage, and body size. Integrating and harmonizing the data derived from these\nsamples and 'mapping' them into a common three-dimensional (3D) space is a\nmajor challenge. The key to making this possible is a 'Common Coordinate\nFramework' (CCF), which provides a semantically annotated, 3D reference system\nfor the entire body. The CCF enables contributors to HuBMAP to 'register'\nspecimens and datasets within a common spatial reference system, and it\nsupports a standardized way to query and 'explore' data in a spatially and\nsemantically explicit manner. [...] This paper describes the construction and\nusage of a CCF for the human body and its reference implementation in HuBMAP.\nThe CCF consists of (1) a CCF Clinical Ontology, which provides metadata about\nthe specimen and donor (the 'who'); (2) a CCF Semantic Ontology, which\ndescribes 'what' part of the body a sample came from and details anatomical\nstructures, cell types, and biomarkers (ASCT+B); and (3) a CCF Spatial\nOntology, which indicates 'where' a tissue sample is located in a 3D coordinate\nsystem. An initial version of all three CCF ontologies has been implemented for\nthe first HuBMAP Portal release. It was successfully used by Tissue Mapping\nCenters to semantically annotate and spatially register 48 kidney and spleen\ntissue blocks. The blocks can be queried and explored in their clinical,\nsemantic, and spatial context via the CCF user interface in the HuBMAP Portal.", "published": "2020-07-28 20:35:56", "link": "http://arxiv.org/abs/2007.14474v1", "categories": ["q-bio.QM", "cs.CL", "cs.DL"], "primary_category": "q-bio.QM"}
{"title": "Deep Learning Brasil -- NLP at SemEval-2020 Task 9: Overview of\n  Sentiment Analysis of Code-Mixed Tweets", "abstract": "In this paper, we describe a methodology to predict sentiment in code-mixed\ntweets (hindi-english). Our team called verissimo.manoel in CodaLab developed\nan approach based on an ensemble of four models (MultiFiT, BERT, ALBERT, and\nXLNET). The final classification algorithm was an ensemble of some predictions\nof all softmax values from these four models. This architecture was used and\nevaluated in the context of the SemEval 2020 challenge (task 9), and our system\ngot 72.7% on the F1 score.", "published": "2020-07-28 16:42:41", "link": "http://arxiv.org/abs/2008.01544v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Kalman Filtering for Speech Enhancement", "abstract": "Statistical signal processing based speech enhancement methods adopt expert\nknowledge to design the statistical models and linear filters, which is\ncomplementary to the deep neural network (DNN) based methods which are\ndata-driven. In this paper, by using expert knowledge from statistical signal\nprocessing for network design and optimization, we extend the conventional\nKalman filtering (KF) to the supervised learning scheme, and propose the neural\nKalman filtering (NKF) for speech enhancement. Two intermediate clean speech\nestimates are first produced from recurrent neural networks (RNN) and linear\nWiener filtering (WF) separately and are then linearly combined by a learned\nNKF gain to yield the NKF output. Supervised joint training is applied to NKF\nto learn to automatically trade-off between the instantaneous linear estimation\nmade by the WF and the long-term non-linear estimation made by the RNN. The NKF\nmethod can be seen as using expert knowledge from WF to regularize the RNN\nestimations to improve its generalization ability to the noise conditions\nunseen in the training. Experiments in different noisy conditions show that the\nproposed method outperforms the baseline methods both in terms of objective\nevaluation metrics and automatic speech recognition (ASR) word error rates\n(WERs).", "published": "2020-07-28 02:57:47", "link": "http://arxiv.org/abs/2007.13962v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dual-Path Transformer Network: Direct Context-Aware Modeling for\n  End-to-End Monaural Speech Separation", "abstract": "The dominant speech separation models are based on complex recurrent or\nconvolution neural network that model speech sequences indirectly conditioning\non context, such as passing information through many intermediate states in\nrecurrent neural network, leading to suboptimal separation performance. In this\npaper, we propose a dual-path transformer network (DPTNet) for end-to-end\nspeech separation, which introduces direct context-awareness in the modeling\nfor speech sequences. By introduces a improved transformer, elements in speech\nsequences can interact directly, which enables DPTNet can model for the speech\nsequences with direct context-awareness. The improved transformer in our\napproach learns the order information of the speech sequences without\npositional encodings by incorporating a recurrent neural network into the\noriginal transformer. In addition, the structure of dual paths makes our model\nefficient for extremely long speech sequence modeling. Extensive experiments on\nbenchmark datasets show that our approach outperforms the current\nstate-of-the-arts (20.6 dB SDR on the public WSj0-2mix data corpus).", "published": "2020-07-28 03:51:28", "link": "http://arxiv.org/abs/2007.13975v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multimodal Integration for Large-Vocabulary Audio-Visual Speech\n  Recognition", "abstract": "For many small- and medium-vocabulary tasks, audio-visual speech recognition\ncan significantly improve the recognition rates compared to audio-only systems.\nHowever, there is still an ongoing debate regarding the best combination\nstrategy for multi-modal information, which should allow for the translation of\nthese gains to large-vocabulary recognition. While an integration at the level\nof state-posterior probabilities, using dynamic stream weighting, is almost\nuniversally helpful for small-vocabulary systems, in large-vocabulary speech\nrecognition, the recognition accuracy remains difficult to improve. In the\nfollowing, we specifically consider the large-vocabulary task of the LRS2\ndatabase, and we investigate a broad range of integration strategies, comparing\nearly integration and end-to-end learning with many versions of hybrid\nrecognition and dynamic stream weighting. One aspect, which is shown to provide\nmuch benefit here, is the use of dynamic stream reliability indicators, which\nallow for hybrid architectures to strongly profit from the inclusion of visual\ninformation whenever the audio channel is distorted even slightly.", "published": "2020-07-28 13:50:40", "link": "http://arxiv.org/abs/2007.14223v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Autosegmental Neural Nets: Should Phones and Tones be Synchronous or\n  Asynchronous?", "abstract": "Phones, the segmental units of the International Phonetic Alphabet (IPA), are\nused for lexical distinctions in most human languages; Tones, the\nsuprasegmental units of the IPA, are used in perhaps 70%. Many previous studies\nhave explored cross-lingual adaptation of automatic speech recognition (ASR)\nphone models, but few have explored the multilingual and cross-lingual transfer\nof synchronization between phones and tones. In this paper, we test four\nConnectionist Temporal Classification (CTC)-based acoustic models, differing in\nthe degree of synchrony they impose between phones and tones. Models are\ntrained and tested multilingually in three languages, then adapted and tested\ncross-lingually in a fourth. Both synchronous and asynchronous models are\neffective in both multilingual and cross-lingual settings. Synchronous models\nachieve lower error rate in the joint phone+tone tier, but asynchronous\ntraining results in lower tone error rate.", "published": "2020-07-28 16:32:09", "link": "http://arxiv.org/abs/2007.14351v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-supervised Neural Audio-Visual Sound Source Localization via\n  Probabilistic Spatial Modeling", "abstract": "Detecting sound source objects within visual observation is important for\nautonomous robots to comprehend surrounding environments. Since sounding\nobjects have a large variety with different appearances in our living\nenvironments, labeling all sounding objects is impossible in practice. This\ncalls for self-supervised learning which does not require manual labeling. Most\nof conventional self-supervised learning uses monaural audio signals and images\nand cannot distinguish sound source objects having similar appearances due to\npoor spatial information in audio signals. To solve this problem, this paper\npresents a self-supervised training method using 360{\\deg} images and\nmultichannel audio signals. By incorporating with the spatial information in\nmultichannel audio signals, our method trains deep neural networks (DNNs) to\ndistinguish multiple sound source objects. Our system for localizing sound\nsource objects in the image is composed of audio and visual DNNs. The visual\nDNN is trained to localize sound source candidates within an input image. The\naudio DNN verifies whether each candidate actually produces sound or not. These\nDNNs are jointly trained in a self-supervised manner based on a probabilistic\nspatial audio model. Experimental results with simulated data showed that the\nDNNs trained by our method localized multiple speakers. We also demonstrate\nthat the visual DNN detected objects including talking visitors and specific\nexhibits from real data recorded in a science museum.", "published": "2020-07-28 03:52:53", "link": "http://arxiv.org/abs/2007.13976v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Siamese x-vector reconstruction for domain adapted speaker recognition", "abstract": "With the rise of voice-activated applications, the need for speaker\nrecognition is rapidly increasing. The x-vector, an embedding approach based on\na deep neural network (DNN), is considered the state-of-the-art when proper\nend-to-end training is not feasible. However, the accuracy significantly\ndecreases when recording conditions (noise, sample rate, etc.) are mismatched,\neither between the x-vector training data and the target data or between\nenrollment and test data. We introduce the Siamese x-vector Reconstruction\n(SVR) for domain adaptation. We reconstruct the embedding of a higher quality\nsignal from a lower quality counterpart using a lean auxiliary Siamese DNN. We\nevaluate our method on several mismatch scenarios and demonstrate significant\nimprovement over the baseline.", "published": "2020-07-28 12:01:03", "link": "http://arxiv.org/abs/2007.14146v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting and analysing spontaneous oral cancer speech in the wild", "abstract": "Oral cancer speech is a disease which impacts more than half a million people\nworldwide every year. Analysis of oral cancer speech has so far focused on read\nspeech. In this paper, we 1) present and 2) analyse a three-hour long\nspontaneous oral cancer speech dataset collected from YouTube. 3) We set\nbaselines for an oral cancer speech detection task on this dataset. The\nanalysis of these explainable machine learning baselines shows that sibilants\nand stop consonants are the most important indicators for spontaneous oral\ncancer speech detection.", "published": "2020-07-28 13:38:33", "link": "http://arxiv.org/abs/2007.14205v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Hybrid Approach to Audio-to-Score Alignment", "abstract": "Audio-to-score alignment aims at generating an accurate mapping between a\nperformance audio and the score of a given piece. Standard alignment methods\nare based on Dynamic Time Warping (DTW) and employ handcrafted features. We\nexplore the usage of neural networks as a preprocessing step for DTW-based\nautomatic alignment methods. Experiments on music data from different acoustic\nconditions demonstrate that this method generates robust alignments whilst\nbeing adaptable at the same time.", "published": "2020-07-28 16:04:19", "link": "http://arxiv.org/abs/2007.14333v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
