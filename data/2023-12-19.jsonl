{"title": "Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs", "abstract": "Despite progress in automated fact-checking, most systems require a\nsignificant amount of labeled training data, which is expensive. In this paper,\nwe propose a novel zero-shot method, which instead of operating directly on the\nclaim and evidence sentences, decomposes them into semantic triples augmented\nusing external knowledge graphs, and uses large language models trained for\nnatural language inference. This allows it to generalize to adversarial\ndatasets and domains that supervised models require specific training data for.\nOur empirical results show that our approach outperforms previous zero-shot\napproaches on FEVER, FEVER-Symmetric, FEVER 2.0, and Climate-FEVER, while being\ncomparable or better than supervised models on the adversarial and the\nout-of-domain datasets.", "published": "2023-12-19 01:48:31", "link": "http://arxiv.org/abs/2312.11785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COOPER: Coordinating Specialized Agents towards a Complex Dialogue Goal", "abstract": "In recent years, there has been a growing interest in exploring dialogues\nwith more complex goals, such as negotiation, persuasion, and emotional\nsupport, which go beyond traditional service-focused dialogue systems. Apart\nfrom the requirement for much more sophisticated strategic reasoning and\ncommunication skills, a significant challenge of these tasks lies in the\ndifficulty of objectively measuring the achievement of their goals in a\nquantifiable way, making it difficult for existing research to directly\noptimize the dialogue procedure towards them. In our work, we emphasize the\nmultifaceted nature of complex dialogue goals and argue that it is more\nfeasible to accomplish them by comprehensively considering and jointly\npromoting their different aspects. To this end, we propose a novel dialogue\nframework, Cooper, which coordinates multiple specialized agents, each\ndedicated to a specific dialogue goal aspect separately, to approach the\ncomplex objective. Through this divide-and-conquer manner, we make complex\ndialogue goals more approachable and elicit greater intelligence via the\ncollaboration of individual agents. Experiments on persuasion and emotional\nsupport dialogues demonstrate the superiority of our method over a set of\ncompetitive baselines.", "published": "2023-12-19 02:07:42", "link": "http://arxiv.org/abs/2312.11792v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA", "abstract": "Large language models (LLMs) have shown great success in various Natural\nLanguage Processing (NLP) tasks, whist they still need updates after deployment\nto fix errors or keep pace with the changing knowledge in the world.\nResearchers formulate such problem as Model Editing and have developed various\neditors focusing on different axes of editing properties. However, current\neditors can hardly support all properties and rely on heavy computational\nresources. In this paper, we propose a plug-in Model Editing method based on\nneuron-indexed dynamic LoRA (MELO), which alters the behavior of language\nmodels by dynamically activating certain LoRA blocks according to the index\nbuilt in an inner vector database. Our method satisfies various editing\nproperties with high efficiency and can be easily integrated into multiple LLM\nbackbones. Experimental results show that our proposed MELO achieves\nstate-of-the-art editing performance on three sequential editing tasks\n(document classification, question answering and hallucination correction),\nwhile requires the least trainable parameters and computational cost.", "published": "2023-12-19 02:11:01", "link": "http://arxiv.org/abs/2312.11795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP for Maternal Healthcare: Perspectives and Guiding Principles in the\n  Age of LLMs", "abstract": "Ethical frameworks for the use of natural language processing (NLP) are\nurgently needed to shape how large language models (LLMs) and similar tools are\nused for healthcare applications. Healthcare faces existing challenges\nincluding the balance of power in clinician-patient relationships, systemic\nhealth disparities, historical injustices, and economic constraints. Drawing\ndirectly from the voices of those most affected, and focusing on a case study\nof a specific healthcare setting, we propose a set of guiding principles for\nthe use of NLP in maternal healthcare. We led an interactive session centered\non an LLM-based chatbot demonstration during a full-day workshop with 39\nparticipants, and additionally surveyed 30 healthcare workers and 30 birthing\npeople about their values, needs, and perceptions of NLP tools in the context\nof maternal health. We conducted quantitative and qualitative analyses of the\nsurvey results and interactive discussions to consolidate our findings into a\nset of guiding principles. We propose nine principles for ethical use of NLP\nfor maternal healthcare, grouped into three themes: (i) recognizing contextual\nsignificance (ii) holistic measurements, and (iii) who/what is valued. For each\nprinciple, we describe its underlying rationale and provide practical advice.\nThis set of principles can provide a methodological pattern for other\nresearchers and serve as a resource to practitioners working on maternal health\nand other healthcare fields to emphasize the importance of technical nuance,\nhistorical context, and inclusive design when developing NLP technologies for\nclinical use.", "published": "2023-12-19 02:35:13", "link": "http://arxiv.org/abs/2312.11803v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Human Translation Difficulty with Neural Machine Translation", "abstract": "Human translators linger on some words and phrases more than others, and\npredicting this variation is a step towards explaining the underlying cognitive\nprocesses. Using data from the CRITT Translation Process Research Database, we\nevaluate the extent to which surprisal and attentional features derived from a\nNeural Machine Translation (NMT) model account for reading and production times\nof human translators. We find that surprisal and attention are complementary\npredictors of translation difficulty, and that surprisal derived from a NMT\nmodel is the single most successful predictor of production duration. Our\nanalyses draw on data from hundreds of translators operating across 13 language\npairs, and represent the most comprehensive investigation of human translation\ndifficulty to date.", "published": "2023-12-19 04:42:56", "link": "http://arxiv.org/abs/2312.11852v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT", "abstract": "The proliferation of fake news has emerged as a critical issue in recent\nyears, requiring significant efforts to detect it. However, the existing fake\nnews detection datasets are sourced from human journalists, which are likely to\nhave inherent bias limitations due to the highly subjective nature of this\ntask. In this paper, we revisit the existing fake news dataset verified by\nhuman journalists with augmented fact-checking by large language models\n(ChatGPT), and we name the augmented fake news dataset ChatGPT-FC. We\nquantitatively analyze the distinctions and resemblances between human\njournalists and LLM in assessing news subject credibility, news creator\ncredibility, time-sensitive, and political framing. Our findings highlight\nLLM's potential to serve as a preliminary screening method, offering a\npromising avenue to mitigate the inherent biases of human journalists and\nenhance fake news detection.", "published": "2023-12-19 05:46:11", "link": "http://arxiv.org/abs/2312.11870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "External Knowledge Augmented Polyphone Disambiguation Using Large\n  Language Model", "abstract": "One of the key issues in Mandarin Chinese text-to-speech (TTS) systems is\npolyphone disambiguation when doing grapheme-to-phoneme (G2P) conversion. In\nthis paper, we introduce a novel method to solve the problem as a generation\ntask. Following the trending research of large language models (LLM) and prompt\nlearning, the proposed method consists of three modules. Retrieval module\nincorporates external knowledge which is a multi-level semantic dictionary of\nChinese polyphonic characters to format the sentence into a prompt. Generation\nmodule adopts the decoder-only Transformer architecture to induce the target\ntext. Postprocess module corrects the generated text into a valid result if\nneeded. Experimental results show that our method outperforms the existing\nmethods on a public dataset called CPP. We also empirically study the impacts\nof different templates of the prompt, different sizes of training data, and\nwhether to incorporate external knowledge.", "published": "2023-12-19 08:00:10", "link": "http://arxiv.org/abs/2312.11920v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Granularity Information Interaction Framework for Incomplete\n  Utterance Rewriting", "abstract": "Recent approaches in Incomplete Utterance Rewriting (IUR) fail to capture the\nsource of important words, which is crucial to edit the incomplete utterance,\nand introduce words from irrelevant utterances. We propose a novel and\neffective multi-task information interaction framework including context\nselection, edit matrix construction, and relevance merging to capture the\nmulti-granularity of semantic information. Benefiting from fetching the\nrelevant utterance and figuring out the important words, our approach\noutperforms existing state-of-the-art models on two benchmark datasets\nRestoration-200K and CANAND in this field. Code will be provided on\n\\url{https://github.com/yanmenxue/QR}.", "published": "2023-12-19 08:43:02", "link": "http://arxiv.org/abs/2312.11945v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coreference Graph Guidance for Mind-Map Generation", "abstract": "Mind-map generation aims to process a document into a hierarchical structure\nto show its central idea and branches. Such a manner is more conducive to\nunderstanding the logic and semantics of the document than plain text.\nRecently, a state-of-the-art method encodes the sentences of a document\nsequentially and converts them to a relation graph via sequence-to-graph.\nThough this method is efficient to generate mind-maps in parallel, its\nmechanism focuses more on sequential features while hardly capturing structural\ninformation. Moreover, it's difficult to model long-range semantic relations.\nIn this work, we propose a coreference-guided mind-map generation network\n(CMGN) to incorporate external structure knowledge. Specifically, we construct\na coreference graph based on the coreference semantic relationship to introduce\nthe graph structure information. Then we employ a coreference graph encoder to\nmine the potential governing relations between sentences. In order to exclude\nnoise and better utilize the information of the coreference graph, we adopt a\ngraph enhancement module in a contrastive learning manner. Experimental results\ndemonstrate that our model outperforms all the existing methods. The case study\nfurther proves that our model can more accurately and concisely reveal the\nstructure and semantics of a document. Code and data are available at\nhttps://github.com/Cyno2232/CMGN.", "published": "2023-12-19 09:39:27", "link": "http://arxiv.org/abs/2312.11997v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models:\n  A Critical Review and Assessment", "abstract": "With the continuous growth in the number of parameters of transformer-based\npretrained language models (PLMs), particularly the emergence of large language\nmodels (LLMs) with billions of parameters, many natural language processing\n(NLP) tasks have demonstrated remarkable success. However, the enormous size\nand computational demands of these models pose significant challenges for\nadapting them to specific downstream tasks, especially in environments with\nlimited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers\nan effective solution by reducing the number of fine-tuning parameters and\nmemory usage while achieving comparable performance to full fine-tuning. The\ndemands for fine-tuning PLMs, especially LLMs, have led to a surge in the\ndevelopment of PEFT methods, as depicted in Fig. 1. In this paper, we present a\ncomprehensive and systematic review of PEFT methods for PLMs. We summarize\nthese PEFT methods, discuss their applications, and outline future directions.\nFurthermore, we conduct experiments using several representative PEFT methods\nto better understand their effectiveness in parameter efficiency and memory\nefficiency. By offering insights into the latest advancements and practical\napplications, this survey serves as an invaluable resource for researchers and\npractitioners seeking to navigate the challenges and opportunities presented by\nPEFT in the context of PLMs.", "published": "2023-12-19 13:31:24", "link": "http://arxiv.org/abs/2312.12148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Geo-located Aspect Based Sentiment Analysis (ABSA) for Crowdsourced\n  Evaluation of Urban Environments", "abstract": "Sentiment analysis methods are rapidly being adopted by the field of Urban\nDesign and Planning, for the crowdsourced evaluation of urban environments.\nHowever, most models used within this domain are able to identify positive or\nnegative sentiment associated with a textual appraisal as a whole, without\ninferring information about specific urban aspects contained within it, or the\nsentiment associated with them. While Aspect Based Sentiment Analysis (ABSA) is\nbecoming increasingly popular, most existing ABSA models are trained on\nnon-urban themes such as restaurants, electronics, consumer goods and the like.\nThis body of research develops an ABSA model capable of extracting urban\naspects contained within geo-located textual urban appraisals, along with\ncorresponding aspect sentiment classification. We annotate a dataset of 2500\ncrowdsourced reviews of public parks, and train a Bidirectional Encoder\nRepresentations from Transformers (BERT) model with Local Context Focus (LCF)\non this data. Our model achieves significant improvement in prediction accuracy\non urban reviews, for both Aspect Term Extraction (ATE) and Aspect Sentiment\nClassification (ASC) tasks. For demonstrative analysis, positive and negative\nurban aspects across Boston are spatially visualized. We hope that this model\nis useful for designers and planners for fine-grained urban sentiment\nevaluation.", "published": "2023-12-19 15:37:27", "link": "http://arxiv.org/abs/2312.12253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PowMix: A Versatile Regularizer for Multimodal Sentiment Analysis", "abstract": "Multimodal sentiment analysis (MSA) leverages heterogeneous data sources to\ninterpret the complex nature of human sentiments. Despite significant progress\nin multimodal architecture design, the field lacks comprehensive regularization\nmethods. This paper introduces PowMix, a versatile embedding space regularizer\nthat builds upon the strengths of unimodal mixing-based regularization\napproaches and introduces novel algorithmic components that are specifically\ntailored to multimodal tasks. PowMix is integrated before the fusion stage of\nmultimodal architectures and facilitates intra-modal mixing, such as mixing\ntext with text, to act as a regularizer. PowMix consists of five components: 1)\na varying number of generated mixed examples, 2) mixing factor reweighting, 3)\nanisotropic mixing, 4) dynamic mixing, and 5) cross-modal label mixing.\nExtensive experimentation across benchmark MSA datasets and a broad spectrum of\ndiverse architectural designs demonstrate the efficacy of PowMix, as evidenced\nby consistent performance improvements over baselines and existing mixing\nmethods. An in-depth ablation study highlights the critical contribution of\neach PowMix component and how they synergistically enhance performance.\nFurthermore, algorithmic analysis demonstrates how PowMix behaves in different\nscenarios, particularly comparing early versus late fusion architectures.\nNotably, PowMix enhances overall performance without sacrificing model\nrobustness or magnifying text dominance. It also retains its strong performance\nin situations of limited data. Our findings position PowMix as a promising\nversatile regularization strategy for MSA. Code will be made available.", "published": "2023-12-19 17:01:58", "link": "http://arxiv.org/abs/2312.12334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical study of Unsupervised Neural Machine Translation: analyzing\n  NMT output, model's behavior and sentences' contribution", "abstract": "Unsupervised Neural Machine Translation (UNMT) focuses on improving NMT\nresults under the assumption there is no human translated parallel data, yet\nlittle work has been done so far in highlighting its advantages compared to\nsupervised methods and analyzing its output in aspects other than translation\naccuracy. We focus on three very diverse languages, French, Gujarati, and\nKazakh, and train bilingual NMT models, to and from English, with various\nlevels of supervision, in high- and low- resource setups, measure quality of\nthe NMT output and compare the generated sequences' word order and semantic\nsimilarity to source and reference sentences. We also use Layer-wise Relevance\nPropagation to evaluate the source and target sentences' contribution to the\nresult, expanding the findings of previous works to the UNMT paradigm.", "published": "2023-12-19 20:35:08", "link": "http://arxiv.org/abs/2312.12588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Llama2-finetuned LLM for Odia Language Utilizing Domain\n  Knowledge Instruction Set", "abstract": "Building LLMs for languages other than English is in great demand due to the\nunavailability and performance of multilingual LLMs, such as understanding the\nlocal context. The problem is critical for low-resource languages due to the\nneed for instruction sets. In a multilingual country like India, there is a\nneed for LLMs supporting Indic languages to provide generative AI and LLM-based\ntechnologies and services to its citizens.\n  This paper presents our approach of i) generating a large Odia instruction\nset, including domain knowledge data suitable for LLM fine-tuning, and ii)\nbuilding a Llama2-finetuned model tailored for enhanced performance in the Odia\ndomain. The proposed work will help researchers build an instruction set and\nLLM, particularly for Indic languages. We will release the model and\ninstruction set for the public for research and noncommercial purposes.", "published": "2023-12-19 22:01:01", "link": "http://arxiv.org/abs/2312.12624v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is post-editing really faster than human translation?", "abstract": "Time efficiency is paramount for the localisation industry, which demands\never-faster turnaround times. However, translation speed is largely\nunderresearched, and there is a lack of clarity about how language service\nproviders (LSPs) can evaluate the performance of their post-editing (PE) and\nhuman translation (HT) services. This study constitutes the first large-scale\ninvestigation of translation and revision speed in HT and in the PE of neural\nmachine translation, based on real-world data from an LSP. It uses an\nexploratory data analysis approach to investigate data for 90 million words\ntranslated by 879 linguists across 11 language pairs, over 2.5 years. The\nresults of this research indicate that (a) PE is usually but not always faster\nthan HT; (b) average speed values may be misleading; (c) translation speed is\nhighly variable; and (d) edit distance cannot be used as a proxy for\npost-editing productivity, because it does not correlate strongly with speed.", "published": "2023-12-19 23:21:19", "link": "http://arxiv.org/abs/2312.12660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithful Model Evaluation for Model-Based Metrics", "abstract": "Statistical significance testing is used in natural language processing (NLP)\nto determine whether the results of a study or experiment are likely to be due\nto chance or if they reflect a genuine relationship. A key step in significance\ntesting is the estimation of confidence interval which is a function of sample\nvariance. Sample variance calculation is straightforward when evaluating\nagainst ground truth. However, in many cases, a metric model is often used for\nevaluation. For example, to compare toxicity of two large language models, a\ntoxicity classifier is used for evaluation. Existing works usually do not\nconsider the variance change due to metric model errors, which can lead to\nwrong conclusions. In this work, we establish the mathematical foundation of\nsignificance testing for model-based metrics. With experiments on public\nbenchmark datasets and a production system, we show that considering metric\nmodel errors to calculate sample variances for model-based metrics changes the\nconclusions in certain experiments.", "published": "2023-12-19 19:41:33", "link": "http://arxiv.org/abs/2312.17254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REE-HDSC: Recognizing Extracted Entities for the Historical Database\n  Suriname Curacao", "abstract": "We describe the project REE-HDSC and outline our efforts to improve the\nquality of named entities extracted automatically from texts generated by\nhand-written text recognition (HTR) software. We describe a six-step processing\npipeline and test it by processing 19th and 20th century death certificates\nfrom the civil registry of Curacao. We find that the pipeline extracts dates\nwith high precision but that the precision of person name extraction is low.\nNext we show how name precision extraction can be improved by retraining HTR\nmodels with names, post-processing and by identifying and removing incorrect\nnames.", "published": "2023-12-19 16:12:35", "link": "http://arxiv.org/abs/2401.02972v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TESS: A Multi-intent Parser for Conversational Multi-Agent Systems with\n  Decentralized Natural Language Understanding Models", "abstract": "Chatbots have become one of the main pathways for the delivery of business\nautomation tools. Multi-agent systems offer a framework for designing chatbots\nat scale, making it easier to support complex conversations that span across\nmultiple domains as well as enabling developers to maintain and expand their\ncapabilities incrementally over time. However, multi-agent systems complicate\nthe natural language understanding (NLU) of user intents, especially when they\nrely on decentralized NLU models: some utterances (termed single intent) may\ninvoke a single agent while others (termed multi-intent) may explicitly invoke\nmultiple agents. Without correctly parsing multi-intent inputs, decentralized\nNLU approaches will not achieve high prediction accuracy. In this paper, we\npropose an efficient parsing and orchestration pipeline algorithm to service\nmulti-intent utterances from the user in the context of a multi-agent system.\nOur proposed approach achieved comparable performance to competitive deep\nlearning models on three different datasets while being up to 48 times faster.", "published": "2023-12-19 03:39:23", "link": "http://arxiv.org/abs/2312.11828v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Punctuation restoration Model and Spacing Model for Korean Ancient\n  Document", "abstract": "In Korean ancient documents, there is no spacing or punctuation, and they are\nwritten in classical Chinese characters. This makes it challenging for modern\nindividuals and translation models to accurately interpret and translate them.\nWhile China has models predicting punctuation and spacing, applying them\ndirectly to Korean texts is problematic due to data differences. Therefore, we\ndeveloped the first models which predict punctuation and spacing for Korean\nhistorical texts and evaluated their performance. Our punctuation restoration\nmodel achieved an F1 score of 0.84, and Spacing model achieved a score of 0.96.\nIt has the advantage of enabling inference on low-performance GPUs with less\nVRAM while maintaining quite high accuracy.", "published": "2023-12-19 06:15:52", "link": "http://arxiv.org/abs/2312.11881v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Difficulty-Focused Contrastive Learning for Knowledge Tracing with a\n  Large Language Model-Based Difficulty Prediction", "abstract": "This paper presents novel techniques for enhancing the performance of\nknowledge tracing (KT) models by focusing on the crucial factor of question and\nconcept difficulty level. Despite the acknowledged significance of difficulty,\nprevious KT research has yet to exploit its potential for model optimization\nand has struggled to predict difficulty from unseen data. To address these\nproblems, we propose a difficulty-centered contrastive learning method for KT\nmodels and a Large Language Model (LLM)-based framework for difficulty\nprediction. These innovative methods seek to improve the performance of KT\nmodels and provide accurate difficulty estimates for unseen data. Our ablation\nstudy demonstrates the efficacy of these techniques by demonstrating enhanced\nKT model performance. Nonetheless, the complex relationship between language\nand difficulty merits further investigation.", "published": "2023-12-19 06:26:25", "link": "http://arxiv.org/abs/2312.11890v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Relation-Aware Question Answering for Heterogeneous Knowledge Graphs", "abstract": "Multi-hop Knowledge Base Question Answering(KBQA) aims to find the answer\nentity in a knowledge graph (KG), which requires multiple steps of reasoning.\nExisting retrieval-based approaches solve this task by concentrating on the\nspecific relation at different hops and predicting the intermediate entity\nwithin the reasoning path. During the reasoning process of these methods, the\nrepresentation of relations are fixed but the initial relation representation\nmay not be optimal. We claim they fail to utilize information from head-tail\nentities and the semantic connection between relations to enhance the current\nrelation representation, which undermines the ability to capture information of\nrelations in KGs. To address this issue, we construct a \\textbf{dual relation\ngraph} where each node denotes a relation in the original KG (\\textbf{primal\nentity graph}) and edges are constructed between relations sharing same head or\ntail entities. Then we iteratively do primal entity graph reasoning, dual\nrelation graph information propagation, and interaction between these two\ngraphs. In this way, the interaction between entity and relation is enhanced,\nand we derive better entity and relation representations. Experiments on two\npublic datasets, WebQSP and CWQ, show that our approach achieves a significant\nperformance gain over the prior state-of-the-art. Our code is available on\n\\url{https://github.com/yanmenxue/RAH-KBQA}.", "published": "2023-12-19 08:01:48", "link": "http://arxiv.org/abs/2312.11922v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models", "abstract": "Network Pruning is a promising way to address the huge computing resource\ndemands of the deployment and inference of Large Language Models (LLMs).\nRetraining-free is important for LLMs' pruning methods. However, almost all of\nthe existing retraining-free pruning approaches for LLMs focus on unstructured\npruning, which requires specific hardware support for acceleration. In this\npaper, we propose a novel retraining-free structured pruning framework for\nLLMs, named FLAP (FLuctuation-based Adaptive Structured Pruning). It is\nhardware-friendly by effectively reducing storage and enhancing inference\nspeed. For effective structured pruning of LLMs, we highlight three critical\nelements that demand the utmost attention: formulating structured importance\nmetrics, adaptively searching the global compressed model, and implementing\ncompensation mechanisms to mitigate performance loss. First, FLAP determines\nwhether the output feature map is easily recoverable when a column of weight is\nremoved, based on the fluctuation pruning metric. Then it standardizes the\nimportance scores to adaptively determine the global compressed model\nstructure. At last, FLAP adds additional bias terms to recover the output\nfeature maps using the baseline values. We thoroughly evaluate our approach on\na variety of language benchmarks. Without any retraining, our method\nsignificantly outperforms the state-of-the-art methods, including LLM-Pruner\nand the extension of Wanda in structured pruning. The code is released at\nhttps://github.com/CASIA-IVA-Lab/FLAP.", "published": "2023-12-19 09:23:48", "link": "http://arxiv.org/abs/2312.11983v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Climate Change from Large Language Models", "abstract": "Climate change poses grave challenges, demanding widespread understanding and\nlow-carbon lifestyle awareness. Large language models (LLMs) offer a powerful\ntool to address this crisis, yet comprehensive evaluations of their\nclimate-crisis knowledge are lacking. This paper proposes an automated\nevaluation framework to assess climate-crisis knowledge within LLMs. We adopt a\nhybrid approach for data acquisition, combining data synthesis and manual\ncollection, to compile a diverse set of questions encompassing various aspects\nof climate change. Utilizing prompt engineering based on the compiled\nquestions, we evaluate the model's knowledge by analyzing its generated\nanswers. Furthermore, we introduce a comprehensive set of metrics to assess\nclimate-crisis knowledge, encompassing indicators from 10 distinct\nperspectives. These metrics provide a multifaceted evaluation, enabling a\nnuanced understanding of the LLMs' climate crisis comprehension. The\nexperimental results demonstrate the efficacy of our proposed method. In our\nevaluation utilizing diverse high-performing LLMs, we discovered that while\nLLMs possess considerable climate-related knowledge, there are shortcomings in\nterms of timeliness, indicating a need for continuous updating and refinement\nof their climate-related content.", "published": "2023-12-19 09:26:46", "link": "http://arxiv.org/abs/2312.11985v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT be Your Personal Medical Assistant?", "abstract": "The advanced large language model (LLM) ChatGPT has shown its potential in\ndifferent domains and remains unbeaten due to its characteristics compared to\nother LLMs. This study aims to evaluate the potential of using a fine-tuned\nChatGPT model as a personal medical assistant in the Arabic language. To do so,\nthis study uses publicly available online questions and answering datasets in\nArabic language. There are almost 430K questions and answers for 20\ndisease-specific categories. GPT-3.5-turbo model was fine-tuned with a portion\nof this dataset. The performance of this fine-tuned model was evaluated through\nautomated and human evaluation. The automated evaluations include perplexity,\ncoherence, similarity, and token count. Native Arabic speakers with medical\nknowledge evaluated the generated text by calculating relevance, accuracy,\nprecision, logic, and originality. The overall result shows that ChatGPT has a\nbright future in medical assistance.", "published": "2023-12-19 09:54:27", "link": "http://arxiv.org/abs/2312.12006v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Synergistic Anchored Contrastive Pre-training for Few-Shot Relation\n  Extraction", "abstract": "Few-shot Relation Extraction (FSRE) aims to extract relational facts from a\nsparse set of labeled corpora. Recent studies have shown promising results in\nFSRE by employing Pre-trained Language Models (PLMs) within the framework of\nsupervised contrastive learning, which considers both instances and label\nfacts. However, how to effectively harness massive instance-label pairs to\nencompass the learned representation with semantic richness in this learning\nparadigm is not fully explored. To address this gap, we introduce a novel\nsynergistic anchored contrastive pre-training framework. This framework is\nmotivated by the insight that the diverse viewpoints conveyed through\ninstance-label pairs capture incomplete yet complementary intrinsic textual\nsemantics. Specifically, our framework involves a symmetrical contrastive\nobjective that encompasses both sentence-anchored and label-anchored\ncontrastive losses. By combining these two losses, the model establishes a\nrobust and uniform representation space. This space effectively captures the\nreciprocal alignment of feature distributions among instances and relational\nfacts, simultaneously enhancing the maximization of mutual information across\ndiverse perspectives within the same relation. Experimental results demonstrate\nthat our framework achieves significant performance enhancements compared to\nbaseline models in downstream FSRE tasks. Furthermore, our approach exhibits\nsuperior adaptability to handle the challenges of domain shift and zero-shot\nrelation extraction. Our code is available online at\nhttps://github.com/AONE-NLP/FSRE-SaCon.", "published": "2023-12-19 10:16:24", "link": "http://arxiv.org/abs/2312.12021v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Error Detection with Contrastive Confidence Adaption", "abstract": "Knowledge graphs (KGs) often contain various errors. Previous works on\ndetecting errors in KGs mainly rely on triplet embedding from graph structure.\nWe conduct an empirical study and find that these works struggle to\ndiscriminate noise from semantically-similar correct triplets. In this paper,\nwe propose a KG error detection model CCA to integrate both textual and graph\nstructural information from triplet reconstruction for better distinguishing\nsemantics. We design interactive contrastive learning to capture the\ndifferences between textual and structural patterns. Furthermore, we construct\nrealistic datasets with semantically-similar noise and adversarial noise.\nExperimental results demonstrate that CCA outperforms state-of-the-art\nbaselines, especially in detecting semantically-similar noise and adversarial\nnoise.", "published": "2023-12-19 12:32:27", "link": "http://arxiv.org/abs/2312.12108v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neuron-Level Knowledge Attribution in Large Language Models", "abstract": "Identifying important neurons for final predictions is essential for\nunderstanding the mechanisms of large language models. Due to computational\nconstraints, current attribution techniques struggle to operate at neuron\nlevel. In this paper, we propose a static method for pinpointing significant\nneurons. Compared to seven other methods, our approach demonstrates superior\nperformance across three metrics. Additionally, since most static methods\ntypically only identify \"value neurons\" directly contributing to the final\nprediction, we propose a method for identifying \"query neurons\" which activate\nthese \"value neurons\". Finally, we apply our methods to analyze six types of\nknowledge across both attention and feed-forward network (FFN) layers. Our\nmethod and analysis are helpful for understanding the mechanisms of knowledge\nstorage and set the stage for future research in knowledge editing. The code is\navailable on https://github.com/zepingyu0512/neuron-attribution.", "published": "2023-12-19 13:23:18", "link": "http://arxiv.org/abs/2312.12141v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GeomVerse: A Systematic Evaluation of Large Models for Geometric\n  Reasoning", "abstract": "Large language models have shown impressive results for multi-hop\nmathematical reasoning when the input question is only textual. Many\nmathematical reasoning problems, however, contain both text and image. With the\never-increasing adoption of vision language models (VLMs), understanding their\nreasoning abilities for such problems is crucial. In this paper, we evaluate\nthe reasoning capabilities of VLMs along various axes through the lens of\ngeometry problems. We procedurally create a synthetic dataset of geometry\nquestions with controllable difficulty levels along multiple axes, thus\nenabling a systematic evaluation. The empirical results obtained using our\nbenchmark for state-of-the-art VLMs indicate that these models are not as\ncapable in subjects like geometry (and, by generalization, other topics\nrequiring similar reasoning) as suggested by previous benchmarks. This is made\nespecially clear by the construction of our benchmark at various depth levels,\nsince solving higher-depth problems requires long chains of reasoning rather\nthan additional memorized knowledge. We release the dataset for further\nresearch in this area.", "published": "2023-12-19 15:25:39", "link": "http://arxiv.org/abs/2312.12241v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Instruct-SCTG: Guiding Sequential Controlled Text Generation through\n  Instructions", "abstract": "Instruction-tuned large language models have shown remarkable performance in\naligning generated text with user intentions across various tasks. However,\nmaintaining human-like discourse structure in the generated text remains a\nchallenging research question. In this paper, we propose Instruct-SCTG, a\nflexible and effective sequential framework that harnesses instruction-tuned\nlanguage models to generate structurally coherent text in both fine-tuned and\nzero-shot setups. Our framework generates articles in a section-by-section\nmanner, aligned with the desired human structure using natural language\ninstructions. Furthermore, we introduce a new automatic metric that measures\ndiscourse divergence in a fuzzy manner. Extensive experiments on three datasets\nfrom representative domains of news and recipes demonstrate the\nstate-of-the-art performance of our framework in imposing discourse structure\nduring text generation, as verified by both automatic and human evaluation. Our\ncode will be available on Github.", "published": "2023-12-19 16:20:49", "link": "http://arxiv.org/abs/2312.12299v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LatestEval: Addressing Data Contamination in Language Model Evaluation\n  through Dynamic and Time-Sensitive Test Construction", "abstract": "Data contamination in evaluation is getting increasingly prevalent with the\nemergence of language models pre-trained on super large, automatically crawled\ncorpora. This problem leads to significant challenges in the accurate\nassessment of model capabilities and generalisations. In this paper, we propose\nLatestEval, an automatic method that leverages the most recent texts to create\nuncontaminated reading comprehension evaluations. LatestEval avoids data\ncontamination by only using texts published within a recent time window,\nensuring no overlap with the training corpora of pre-trained language models.\nWe develop the LatestEval automated pipeline to 1) gather the latest texts; 2)\nidentify key information, and 3) construct questions targeting the information\nwhile removing the existing answers from the context. This encourages models to\ninfer the answers themselves based on the remaining context, rather than just\ncopy-paste. Our experiments demonstrate that language models exhibit negligible\nmemorisation behaviours on LatestEval as opposed to previous benchmarks,\nsuggesting a significantly reduced risk of data contamination and leading to a\nmore robust evaluation. Data and code are publicly available at:\nhttps://github.com/liyucheng09/LatestEval.", "published": "2023-12-19 17:16:43", "link": "http://arxiv.org/abs/2312.12343v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Users Approach on Providing Feedback for Smart Home Devices", "abstract": "Smart Home technology has accomplished extraordinary interest in making\nindividuals' lives more straightforward and more relaxing as of late.\nTechnology as of late brought about delivering numerous savvy and refined\nframeworks which advanced clever living innovation. In this paper, we will be\ninvestigating the behavioural intention of user's approach on providing\nfeedback for smart home devices. We will be conducting an online survey for\nsample of three to five students selected by simple random sampling to study\nthe user's motto for giving feedback on smart home devices and their\nexpectations. We have observed that most users are ready to share their\nfeedback on smart home devices actively to improvise the service and quality of\nthe product to fulfill the user needs and make their lives easier.", "published": "2023-12-19 03:18:12", "link": "http://arxiv.org/abs/2312.12466v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "On Early Detection of Hallucinations in Factual Question Answering", "abstract": "While large language models (LLMs) have taken great strides towards helping\nhumans with a plethora of tasks, hallucinations remain a major impediment\ntowards gaining user trust. The fluency and coherence of model generations even\nwhen hallucinating makes detection a difficult task. In this work, we explore\nif the artifacts associated with the model generations can provide hints that\nthe generation will contain hallucinations. Specifically, we probe LLMs at 1)\nthe inputs via Integrated Gradients based token attribution, 2) the outputs via\nthe Softmax probabilities, and 3) the internal state via self-attention and\nfully-connected layer activations for signs of hallucinations on open-ended\nquestion answering tasks. Our results show that the distributions of these\nartifacts tend to differ between hallucinated and non-hallucinated generations.\nBuilding on this insight, we train binary classifiers that use these artifacts\nas input features to classify model generations into hallucinations and\nnon-hallucinations. These hallucination classifiers achieve up to $0.80$ AUROC.\nWe also show that tokens preceding a hallucination can already predict the\nsubsequent hallucination even before it occurs.", "published": "2023-12-19 14:35:04", "link": "http://arxiv.org/abs/2312.14183v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tokenization Matters: Navigating Data-Scarce Tokenization for Gender\n  Inclusive Language Technologies", "abstract": "Gender-inclusive NLP research has documented the harmful limitations of\ngender binary-centric large language models (LLM), such as the inability to\ncorrectly use gender-diverse English neopronouns (e.g., xe, zir, fae). While\ndata scarcity is a known culprit, the precise mechanisms through which scarcity\naffects this behavior remain underexplored. We discover LLM misgendering is\nsignificantly influenced by Byte-Pair Encoding (BPE) tokenization, the\ntokenizer powering many popular LLMs. Unlike binary pronouns, BPE overfragments\nneopronouns, a direct consequence of data scarcity during tokenizer training.\nThis disparate tokenization mirrors tokenizer limitations observed in\nmultilingual and low-resource NLP, unlocking new misgendering mitigation\nstrategies. We propose two techniques: (1) pronoun tokenization parity, a\nmethod to enforce consistent tokenization across gendered pronouns, and (2)\nutilizing pre-existing LLM pronoun knowledge to improve neopronoun proficiency.\nOur proposed methods outperform finetuning with standard BPE, improving\nneopronoun accuracy from 14.1% to 58.4%. Our paper is the first to link LLM\nmisgendering to tokenization and deficient neopronoun grammar, indicating that\nLLMs unable to correctly treat neopronouns as pronouns are more prone to\nmisgender.", "published": "2023-12-19 01:28:46", "link": "http://arxiv.org/abs/2312.11779v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gemini: A Family of Highly Capable Multimodal Models", "abstract": "This report introduces a new family of multimodal models, Gemini, that\nexhibit remarkable capabilities across image, audio, video, and text\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\nsuitable for applications ranging from complex reasoning tasks to on-device\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\n32 of these benchmarks - notably being the first model to achieve human-expert\nperformance on the well-studied exam benchmark MMLU, and improving the state of\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\nthat the new capabilities of the Gemini family in cross-modal reasoning and\nlanguage understanding will enable a wide variety of use cases. We discuss our\napproach toward post-training and deploying Gemini models responsibly to users\nthrough services including Gemini, Gemini Advanced, Google AI Studio, and Cloud\nVertex AI.", "published": "2023-12-19 02:39:27", "link": "http://arxiv.org/abs/2312.11805v4", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "An Adaptive Placement and Parallelism Framework for Accelerating RLHF\n  Training", "abstract": "Recently, ChatGPT or InstructGPT like large language models (LLM) has made a\nsignificant impact in the AI world. Many works have attempted to reproduce the\ncomplex InstructGPT's training pipeline, namely Reinforcement Learning with\nHuman Feedback (RLHF). However, the mainstream distributed RLHF training\nmethods typically adopt a fixed model placement strategy, referred to as the\nCo-located strategy. This strategy treats all four interdependent models\ninvolved in RLHF as a single entity, distributing them across all devices and\napplying parallelism techniques designed for a single model, regardless of the\nworkload heterogeneity inherent to each model. As a result, this strategy\nexacerbates the generation bottlenecks in the RLHF training and degrades the\noverall training efficiency. To address these issues, we propose a flexible\nmodel placement framework that offers two general and agile model placement\nstrategies. The Interleaving strategy helps reduce memory redundancy and\ncommunication costs of RLHF training by placing models without dependencies on\nexclusive devices with careful orchestration. On the other hand, the\nDisaggregated strategy improves the throughput of model training by separating\nthe training and inference runtime of the RLHF pipeline with additional shadow\nmodels. Furthermore, our framework provides a simple user interface and\nguidelines to easily and flexibly configure these strategies in various\ntraining scenarios. Our experiments have shown that our strategy can achieve\nnotable improvements up to 11x, compared to the current state-of-the-art (SOTA)\napproaches. The results highlight the effectiveness and adaptability of our\nmethods in accelerating the training of distributed RLHF.", "published": "2023-12-19 03:24:55", "link": "http://arxiv.org/abs/2312.11819v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sparse is Enough in Fine-tuning Pre-trained Large Language Models", "abstract": "With the prevalence of pre-training-fine-tuning paradigm, how to efficiently\nadapt the pre-trained model to the downstream tasks has been an intriguing\nissue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for\nlow-cost adaptation. Although PEFT has demonstrated effectiveness and been\nwidely applied, the underlying principles are still unclear. In this paper, we\nadopt the PAC-Bayesian generalization error bound, viewing pre-training as a\nshift of prior distribution which leads to a tighter bound for generalization\nerror. We validate this shift from the perspectives of oscillations in the loss\nlandscape and the quasi-sparsity in gradient distribution. Based on this, we\npropose a gradient-based sparse fine-tuning algorithm, named Sparse Increment\nFine-Tuning (SIFT), and validate its effectiveness on a range of tasks\nincluding the GLUE Benchmark and Instruction-tuning. The code is accessible at\nhttps://github.com/song-wx/SIFT/.", "published": "2023-12-19 06:06:30", "link": "http://arxiv.org/abs/2312.11875v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for\n  Accelerating Language Models Inference", "abstract": "Early Exiting is one of the most popular methods to achieve efficient\ninference. Current early exiting methods adopt the (weighted) sum of the cross\nentropy loss of all internal classifiers during training, imposing all these\nclassifiers to predict all instances correctly. However, during inference, as\nlong as one internal classifier predicts an instance correctly, it can\naccelerate without losing accuracy. Thus, there is a notable gap between\ntraining and inference. We propose ConsistentEE, an early exiting method that\nis consistent in training and inference. ConsistentEE formulates the early\nexiting process as a reinforcement learning problem. A policy network is added\nto decide whether an instance should exit or continue. The training objective\nof ConsistentEE only require each instance to be predicted correctly by one\ninternal classifier. Additionally, we introduce the concept Memorize Layer to\nmeasure the hardness of an instance. We incorporate memorized layer into reward\nfunction design, which allows \"easy\" instances to focus more on acceleration\nwhile \"hard\" instances to focus more on accuracy. Experimental results show\nthat our method outperforms other baselines on various natural language\nunderstanding and generation tasks.", "published": "2023-12-19 06:16:13", "link": "http://arxiv.org/abs/2312.11882v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing Public Reactions, Perceptions, and Attitudes during the MPox\n  Outbreak: Findings from Topic Modeling of Tweets", "abstract": "The recent outbreak of the MPox virus has resulted in a tremendous increase\nin the usage of Twitter. Prior works in this area of research have primarily\nfocused on the sentiment analysis and content analysis of these Tweets, and the\nfew works that have focused on topic modeling have multiple limitations. This\npaper aims to address this research gap and makes two scientific contributions\nto this field. First, it presents the results of performing Topic Modeling on\n601,432 Tweets about the 2022 Mpox outbreak that were posted on Twitter between\n7 May 2022 and 3 March 2023. The results indicate that the conversations on\nTwitter related to Mpox during this time range may be broadly categorized into\nfour distinct themes - Views and Perspectives about Mpox, Updates on Cases and\nInvestigations about Mpox, Mpox and the LGBTQIA+ Community, and Mpox and\nCOVID-19. Second, the paper presents the findings from the analysis of these\nTweets. The results show that the theme that was most popular on Twitter (in\nterms of the number of Tweets posted) during this time range was Views and\nPerspectives about Mpox. This was followed by the theme of Mpox and the\nLGBTQIA+ Community, which was followed by the themes of Mpox and COVID-19 and\nUpdates on Cases and Investigations about Mpox, respectively. Finally, a\ncomparison with related studies in this area of research is also presented to\nhighlight the novelty and significance of this research work.", "published": "2023-12-19 06:39:38", "link": "http://arxiv.org/abs/2312.11895v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Emotion Rendering for Conversational Speech Synthesis with Heterogeneous\n  Graph-Based Context Modeling", "abstract": "Conversational Speech Synthesis (CSS) aims to accurately express an utterance\nwith the appropriate prosody and emotional inflection within a conversational\nsetting. While recognising the significance of CSS task, the prior studies have\nnot thoroughly investigated the emotional expressiveness problems due to the\nscarcity of emotional conversational datasets and the difficulty of stateful\nemotion modeling. In this paper, we propose a novel emotional CSS model, termed\nECSS, that includes two main components: 1) to enhance emotion understanding,\nwe introduce a heterogeneous graph-based emotional context modeling mechanism,\nwhich takes the multi-source dialogue history as input to model the dialogue\ncontext and learn the emotion cues from the context; 2) to achieve emotion\nrendering, we employ a contrastive learning-based emotion renderer module to\ninfer the accurate emotion style for the target utterance. To address the issue\nof data scarcity, we meticulously create emotional labels in terms of category\nand intensity, and annotate additional emotional information on the existing\nconversational dataset (DailyTalk). Both objective and subjective evaluations\nsuggest that our model outperforms the baseline models in understanding and\nrendering emotions. These evaluations also underscore the importance of\ncomprehensive emotional annotations. Code and audio samples can be found at:\nhttps://github.com/walker-hyf/ECSS.", "published": "2023-12-19 08:47:50", "link": "http://arxiv.org/abs/2312.11947v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Large Language Models Empowered Agent-based Modeling and Simulation: A\n  Survey and Perspectives", "abstract": "Agent-based modeling and simulation has evolved as a powerful tool for\nmodeling complex systems, offering insights into emergent behaviors and\ninteractions among diverse agents. Integrating large language models into\nagent-based modeling and simulation presents a promising avenue for enhancing\nsimulation capabilities. This paper surveys the landscape of utilizing large\nlanguage models in agent-based modeling and simulation, examining their\nchallenges and promising future directions. In this survey, since this is an\ninterdisciplinary field, we first introduce the background of agent-based\nmodeling and simulation and large language model-empowered agents. We then\ndiscuss the motivation for applying large language models to agent-based\nsimulation and systematically analyze the challenges in environment perception,\nhuman alignment, action generation, and evaluation. Most importantly, we\nprovide a comprehensive overview of the recent works of large language\nmodel-empowered agent-based modeling and simulation in multiple scenarios,\nwhich can be divided into four domains: cyber, physical, social, and hybrid,\ncovering simulation of both real-world and virtual environments. Finally, since\nthis area is new and quickly evolving, we discuss the open problems and\npromising future directions.", "published": "2023-12-19 09:06:45", "link": "http://arxiv.org/abs/2312.11970v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Active Preference Inference using Language Models and Probabilistic\n  Reasoning", "abstract": "Actively inferring user preferences, for example by asking good questions, is\nimportant for any human-facing decision-making system. Active inference allows\nsuch systems to adapt and personalize themselves to nuanced individual\npreferences. To enable this ability for instruction-tuned large language models\n(LLMs), one may prompt them to ask users questions to infer their preferences,\ntransforming the language models into more robust, interactive systems.\nHowever, out of the box, these models are not efficient at extracting\npreferences: the questions they generate are not informative, requiring a high\nnumber of user interactions and impeding the usability of the downstream\nsystem. In this work, we introduce an inference-time algorithm that helps LLMs\nquickly infer preferences by using more informative questions. Our algorithm\nuses a probabilistic model whose conditional distributions are defined by\nprompting an LLM, and returns questions that optimize expected entropy and\nexpected model change. Results in a simplified interactive web shopping setting\nwith real product items show that an LLM equipped with our entropy reduction\nalgorithm outperforms baselines with the same underlying LLM on task\nperformance while using fewer user interactions.", "published": "2023-12-19 09:58:54", "link": "http://arxiv.org/abs/2312.12009v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Founder-GPT: Self-play to evaluate the Founder-Idea fit", "abstract": "This research introduces an innovative evaluation method for the\n\"founder-idea\" fit in early-stage startups, utilizing advanced large language\nmodel techniques to assess founders' profiles against their startup ideas to\nenhance decision-making. Embeddings, self-play, tree-of-thought, and\ncritique-based refinement techniques show early promising results that each\nidea's success patterns are unique and they should be evaluated based on the\ncontext of the founder's background.", "published": "2023-12-19 10:46:13", "link": "http://arxiv.org/abs/2312.12037v2", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Automated speech audiometry: Can it work using open-source pre-trained\n  Kaldi-NL automatic speech recognition?", "abstract": "A practical speech audiometry tool is the digits-in-noise (DIN) test for\nhearing screening of populations of varying ages and hearing status. The test\nis usually conducted by a human supervisor (e.g., clinician), who scores the\nresponses spoken by the listener, or online, where a software scores the\nresponses entered by the listener. The test has 24 digit-triplets presented in\nan adaptive staircase procedure, resulting in a speech reception threshold\n(SRT). We propose an alternative automated DIN test setup that can evaluate\nspoken responses whilst conducted without a human supervisor, using the\nopen-source automatic speech recognition toolkit, Kaldi-NL. Thirty\nself-reported normal-hearing Dutch adults (19-64 years) completed one\nDIN+Kaldi-NL test. Their spoken responses were recorded, and used for\nevaluating the transcript of decoded responses by Kaldi-NL. Study 1 evaluated\nthe Kaldi-NL performance through its word error rate (WER), percentage of\nsummed decoding errors regarding only digits found in the transcript compared\nto the total number of digits present in the spoken responses. Average WER\nacross participants was 5.0% (range 0 - 48%, SD = 8.8%), with average decoding\nerrors in three triplets per participant. Study 2 analysed the effect that\ntriplets with decoding errors from Kaldi-NL had on the DIN test output (SRT),\nusing bootstrapping simulations. Previous research indicated 0.70 dB as the\ntypical within-subject SRT variability for normal-hearing adults. Study 2\nshowed that up to four triplets with decoding errors produce SRT variations\nwithin this range, suggesting that our proposed setup could be feasible for\nclinical applications.", "published": "2023-12-19 15:51:49", "link": "http://arxiv.org/abs/2312.12269v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks", "abstract": "With the recent surge in popularity of LLMs has come an ever-increasing need\nfor LLM safety training. In this paper, we investigate the fragility of SOTA\nopen-source LLMs under simple, optimization-free attacks we refer to as\n$\\textit{priming attacks}$, which are easy to execute and effectively bypass\nalignment from safety training. Our proposed attack improves the Attack Success\nRate on Harmful Behaviors, as measured by Llama Guard, by up to $3.3\\times$\ncompared to baselines. Source code and data are available at\nhttps://github.com/uiuc-focal-lab/llm-priming-attacks.", "published": "2023-12-19 16:47:12", "link": "http://arxiv.org/abs/2312.12321v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "SpokesBiz -- an Open Corpus of Conversational Polish", "abstract": "This paper announces the early release of SpokesBiz, a freely available\ncorpus of conversational Polish developed within the CLARIN-BIZ project and\ncomprising over 650 hours of recordings. The transcribed recordings have been\ndiarized and manually annotated for punctuation and casing. We outline the\ngeneral structure and content of the corpus, showcasing selected applications\nin linguistic research, evaluation and improvement of automatic speech\nrecognition (ASR) systems", "published": "2023-12-19 17:48:26", "link": "http://arxiv.org/abs/2312.12364v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP", "abstract": "In recent RAG approaches, rerankers play a pivotal role in refining retrieval\naccuracy with the ability of revealing logical relations for each pair of query\nand text. However, existing rerankers are required to repeatedly encode the\nquery and a large number of long retrieved text. This results in high\ncomputational costs and limits the number of retrieved text, hindering\naccuracy. As a remedy of the problem, we introduce the Efficient Title Reranker\nvia Broadcasting Query Encoder, a novel technique for title reranking that\nachieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we\nintroduce Sigmoid Trick, a novel loss function customized for title reranking.\nCombining both techniques, we empirically validated their effectiveness,\nachieving state-of-the-art results on all four datasets we experimented with\nfrom the KILT knowledge benchmark.", "published": "2023-12-19 18:56:52", "link": "http://arxiv.org/abs/2312.12430v3", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise", "abstract": "The surge of interest towards Multi-modal Large Language Models (MLLMs),\ne.g., GPT-4V(ision) from OpenAI, has marked a significant trend in both\nacademia and industry. They endow Large Language Models (LLMs) with powerful\ncapabilities in visual understanding, enabling them to tackle diverse\nmulti-modal tasks. Very recently, Google released Gemini, its newest and most\ncapable MLLM built from the ground up for multi-modality. In light of the\nsuperior reasoning capabilities, can Gemini challenge GPT-4V's leading position\nin multi-modal learning? In this paper, we present a preliminary exploration of\nGemini Pro's visual understanding proficiency, which comprehensively covers\nfour domains: fundamental perception, advanced cognition, challenging vision\ntasks, and various expert capacities. We compare Gemini Pro with the\nstate-of-the-art GPT-4V to evaluate its upper limits, along with the latest\nopen-sourced MLLM, Sphinx, which reveals the gap between manual efforts and\nblack-box systems. The qualitative samples indicate that, while GPT-4V and\nGemini showcase different answering styles and preferences, they can exhibit\ncomparable visual reasoning capabilities, and Sphinx still trails behind them\nconcerning domain generalizability. Specifically, GPT-4V tends to elaborate\ndetailed explanations and intermediate steps, and Gemini prefers to output a\ndirect and concise answer. The quantitative evaluation on the popular MME\nbenchmark also demonstrates the potential of Gemini to be a strong challenger\nto GPT-4V. Our early investigation of Gemini also observes some common issues\nof MLLMs, indicating that there still remains a considerable distance towards\nartificial general intelligence. Our project for tracking the progress of MLLM\nis released at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.", "published": "2023-12-19 18:59:22", "link": "http://arxiv.org/abs/2312.12436v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "MotionScript: Natural Language Descriptions for Expressive 3D Human\n  Motions", "abstract": "We introduce MotionScript, a novel framework for generating highly detailed,\nnatural language descriptions of 3D human motions. Unlike existing motion\ndatasets that rely on broad action labels or generic captions, MotionScript\nprovides fine-grained, structured descriptions that capture the full complexity\nof human movement including expressive actions (e.g., emotions, stylistic\nwalking) and interactions beyond standard motion capture datasets. MotionScript\nserves as both a descriptive tool and a training resource for text-to-motion\nmodels, enabling the synthesis of highly realistic and diverse human motions\nfrom text. By augmenting motion datasets with MotionScript captions, we\ndemonstrate significant improvements in out-of-distribution motion generation,\nallowing large language models (LLMs) to generate motions that extend beyond\nexisting data. Additionally, MotionScript opens new applications in animation,\nvirtual human simulation, and robotics, providing an interpretable bridge\nbetween intuitive descriptions and motion synthesis. To the best of our\nknowledge, this is the first attempt to systematically translate 3D motion into\nstructured natural language without requiring training data.", "published": "2023-12-19 22:33:17", "link": "http://arxiv.org/abs/2312.12634v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Can Transformers Learn Sequential Function Classes In Context?", "abstract": "In-context learning (ICL) has revolutionized the capabilities of transformer\nmodels in NLP. In our project, we extend the understanding of the mechanisms\nunderpinning ICL by exploring whether transformers can learn from sequential,\nnon-textual function class data distributions. We introduce a novel sliding\nwindow sequential function class and employ toy-sized transformers with a GPT-2\narchitecture to conduct our experiments. Our analysis indicates that these\nmodels can indeed leverage ICL when trained on non-textual sequential function\nclasses. Additionally, our experiments with randomized y-label sequences\nhighlights that transformers retain some ICL capabilities even when the label\nassociations are obfuscated. We provide evidence that transformers can reason\nwith and understand sequentiality encoded within function classes, as reflected\nby the effective learning of our proposed tasks. Our results also show that the\nperformance deteriorated with increasing randomness in the labels, though not\nto the extent one might expect, implying a potential robustness of learned\nsequentiality against label noise. Future research may want to look into how\nprevious explanations of transformers, such as induction heads and task\nvectors, relate to sequentiality in ICL in these toy examples. Our\ninvestigation lays the groundwork for further research into how transformers\nprocess and perceive sequential data.", "published": "2023-12-19 22:57:13", "link": "http://arxiv.org/abs/2312.12655v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dynamic Topic Language Model on Heterogeneous Children's Mental Health\n  Clinical Notes", "abstract": "Mental health diseases affect children's lives and well-beings which have\nreceived increased attention since the COVID-19 pandemic. Analyzing psychiatric\nclinical notes with topic models is critical to evaluating children's mental\nstatus over time. However, few topic models are built for longitudinal\nsettings, and most existing approaches fail to capture temporal trajectories\nfor each document. To address these challenges, we develop a dynamic topic\nmodel with consistent topics and individualized temporal dependencies on the\nevolving document metadata. Our model preserves the semantic meaning of\ndiscovered topics over time and incorporates heterogeneity among documents. In\nparticular, when documents can be categorized, we propose a classifier-free\napproach to maximize topic heterogeneity across different document groups. We\nalso present an efficient variational optimization procedure adapted for the\nmultistage longitudinal setting. In this case study, we apply our method to the\npsychiatric clinical notes from a large tertiary pediatric hospital in Southern\nCalifornia and achieve a 38% increase in the overall coherence of extracted\ntopics. Our real data analysis reveals that children tend to express more\nnegative emotions during state shutdowns and more positive when schools reopen.\nFurthermore, it suggests that sexual and gender minority (SGM) children display\nmore pronounced reactions to major COVID-19 events and a greater sensitivity to\nvaccine-related news than non-SGM children. This study examines children's\nmental health progression during the pandemic and offers clinicians valuable\ninsights to recognize disparities in children's mental health related to their\nsexual and gender identities.", "published": "2023-12-19 00:36:53", "link": "http://arxiv.org/abs/2312.14180v2", "categories": ["cs.CL", "cs.LG", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Medical Term Classification and Unexpected\n  Misalignment Between Response and Reasoning", "abstract": "This study assesses the ability of state-of-the-art large language models\n(LLMs) including GPT-3.5, GPT-4, Falcon, and LLaMA 2 to identify patients with\nmild cognitive impairment (MCI) from discharge summaries and examines instances\nwhere the models' responses were misaligned with their reasoning. Utilizing the\nMIMIC-IV v2.2 database, we focused on a cohort aged 65 and older, verifying MCI\ndiagnoses against ICD codes and expert evaluations. The data was partitioned\ninto training, validation, and testing sets in a 7:2:1 ratio for model\nfine-tuning and evaluation, with an additional metastatic cancer dataset from\nMIMIC III used to further assess reasoning consistency. GPT-4 demonstrated\nsuperior interpretative capabilities, particularly in response to complex\nprompts, yet displayed notable response-reasoning inconsistencies. In contrast,\nopen-source models like Falcon and LLaMA 2 achieved high accuracy but lacked\nexplanatory reasoning, underscoring the necessity for further research to\noptimize both performance and interpretability. The study emphasizes the\nsignificance of prompt engineering and the need for further exploration into\nthe unexpected reasoning-response misalignment observed in GPT-4. The results\nunderscore the promise of incorporating LLMs into healthcare diagnostics,\ncontingent upon methodological advancements to ensure accuracy and clinical\ncoherence of AI-generated outputs, thereby improving the trustworthiness of\nLLMs for medical decision-making.", "published": "2023-12-19 17:36:48", "link": "http://arxiv.org/abs/2312.14184v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Auto311: A Confidence-guided Automated System for Non-emergency Calls", "abstract": "Emergency and non-emergency response systems are essential services provided\nby local governments and critical to protecting lives, the environment, and\nproperty. The effective handling of (non-)emergency calls is critical for\npublic safety and well-being. By reducing the burden through non-emergency\ncallers, residents in critical need of assistance through 911 will receive a\nfast and effective response. Collaborating with the Department of Emergency\nCommunications (DEC) in Nashville, we analyzed 11,796 non-emergency call\nrecordings and developed Auto311, the first automated system to handle 311\nnon-emergency calls, which (1) effectively and dynamically predicts ongoing\nnon-emergency incident types to generate tailored case reports during the call;\n(2) itemizes essential information from dialogue contexts to complete the\ngenerated reports; and (3) strategically structures system-caller dialogues\nwith optimized confidence. We used real-world data to evaluate the system's\neffectiveness and deployability. The experimental results indicate that the\nsystem effectively predicts incident type with an average F-1 score of 92.54%.\nMoreover, the system successfully itemizes critical information from relevant\ncontexts to complete reports, evincing a 0.93 average consistency score\ncompared to the ground truth. Additionally, emulations demonstrate that the\nsystem effectively decreases conversation turns as the utterance size gets more\nextensive and categorizes the ongoing call with 94.49% mean accuracy.", "published": "2023-12-19 20:52:04", "link": "http://arxiv.org/abs/2312.14185v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Assessment of Students' Code Comprehension using LLMs", "abstract": "Assessing student's answers and in particular natural language answers is a\ncrucial challenge in the field of education. Advances in machine learning,\nincluding transformer-based models such as Large Language Models(LLMs), have\nled to significant progress in various natural language tasks. Nevertheless,\namidst the growing trend of evaluating LLMs across diverse tasks, evaluating\nLLMs in the realm of automated answer assesment has not received much\nattention. To address this gap, we explore the potential of using LLMs for\nautomated assessment of student's short and open-ended answer. Particularly, we\nuse LLMs to compare students' explanations with expert explanations in the\ncontext of line-by-line explanations of computer programs.\n  For comparison purposes, we assess both Large Language Models (LLMs) and\nencoder-based Semantic Textual Similarity (STS) models in the context of\nassessing the correctness of students' explanation of computer code. Our\nfindings indicate that LLMs, when prompted in few-shot and chain-of-thought\nsetting perform comparable to fine-tuned encoder-based models in evaluating\nstudents' short answers in programming domain.", "published": "2023-12-19 20:39:12", "link": "http://arxiv.org/abs/2401.05399v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Towards building a monitoring platform for a challenge-oriented smart\n  specialisation with RIS3-MCAT", "abstract": "In the new research and innovation (R&I) paradigm, aimed at a transformation\ntowards more sustainable, inclusive and fair pathways to address societal and\nenvironmental challenges, and at generating new patterns of specialisation and\nnew trajectories for socioeconomic development, it is essential to provide\nmonitoring systems and tools to map and understand the contribution of R&I\npolicies and projects. To address this transformation, we present the RIS3-MCAT\nplatform, the result of a line of work aimed at exploring the potential of open\ndata, semantic analysis, and data visualisation, for monitoring\nchallenge-oriented smart specialisation in Catalonia. RIS3-MCAT is an\ninteractive platform that facilitates access to R&I project data in formats\nthat allow for sophisticated analyses of a large volume of texts, enabling the\ndetailed study of thematic specialisations and challenges beyond classical\nclassification systems. Its conceptualisation, development framework and use\nare presented in this paper.", "published": "2023-12-19 07:58:39", "link": "http://arxiv.org/abs/2401.10900v1", "categories": ["cs.CY", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "On real-time multi-stage speech enhancement systems", "abstract": "Recently, multi-stage systems have stood out among deep learning-based speech\nenhancement methods. However, these systems are always high in complexity,\nrequiring millions of parameters and powerful computational resources, which\nlimits their application for real-time processing in low-power devices.\nBesides, the contribution of various influencing factors to the success of\nmulti-stage systems remains unclear, which presents challenges to reduce the\nsize of these systems. In this paper, we extensively investigate a lightweight\ntwo-stage network with only 560k total parameters. It consists of a Mel-scale\nmagnitude masking model in the first stage and a complex spectrum mapping model\nin the second stage. We first provide a consolidated view of the roles of gain\npower factor, post-filter, and training labels for the Mel-scale masking model.\nThen, we explore several training schemes for the two-stage network and provide\nsome insights into the superiority of the two-stage network. We show that the\nproposed two-stage network trained by an optimal scheme achieves a performance\nsimilar to a four times larger open source model DeepFilterNet2.", "published": "2023-12-19 18:47:09", "link": "http://arxiv.org/abs/2312.12415v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MossFormer2: Combining Transformer and RNN-Free Recurrent Network for\n  Enhanced Time-Domain Monaural Speech Separation", "abstract": "Our previously proposed MossFormer has achieved promising performance in\nmonaural speech separation. However, it predominantly adopts a\nself-attention-based MossFormer module, which tends to emphasize longer-range,\ncoarser-scale dependencies, with a deficiency in effectively modelling\nfiner-scale recurrent patterns. In this paper, we introduce a novel hybrid\nmodel that provides the capabilities to model both long-range, coarse-scale\ndependencies and fine-scale recurrent patterns by integrating a recurrent\nmodule into the MossFormer framework. Instead of applying the recurrent neural\nnetworks (RNNs) that use traditional recurrent connections, we present a\nrecurrent module based on a feedforward sequential memory network (FSMN), which\nis considered \"RNN-free\" recurrent network due to the ability to capture\nrecurrent patterns without using recurrent connections. Our recurrent module\nmainly comprises an enhanced dilated FSMN block by using gated convolutional\nunits (GCU) and dense connections. In addition, a bottleneck layer and an\noutput layer are also added for controlling information flow. The recurrent\nmodule relies on linear projections and convolutions for seamless, parallel\nprocessing of the entire sequence. The integrated MossFormer2 hybrid model\ndemonstrates remarkable enhancements over MossFormer and surpasses other\nstate-of-the-art methods in WSJ0-2/3mix, Libri2Mix, and WHAM!/WHAMR! benchmarks\n(https://github.com/modelscope/ClearerVoice-Studio).", "published": "2023-12-19 03:31:19", "link": "http://arxiv.org/abs/2312.11825v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noise robust distillation of self-supervised speech models via\n  correlation metrics", "abstract": "Compared to large speech foundation models, small distilled models exhibit\ndegraded noise robustness. The student's robustness can be improved by\nintroducing noise at the inputs during pre-training. Despite this, using the\nstandard distillation loss still yields a student with degraded performance.\nThus, this paper proposes improving student robustness via distillation with\ncorrelation metrics. Teacher behavior is learned by maximizing the teacher and\nstudent cross-correlation matrix between their representations towards\nidentity. Noise robustness is encouraged via the student's self-correlation\nminimization. The proposed method is agnostic of the teacher model and\nconsistently outperforms the previous approach. This work also proposes an\nheuristic to weigh the importance of the two correlation terms automatically.\nExperiments show consistently better clean and noise generalization on Intent\nClassification, Keyword Spotting, and Automatic Speech Recognition tasks on\nSUPERB Challenge.", "published": "2023-12-19 13:35:47", "link": "http://arxiv.org/abs/2312.12153v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Advancing VAD Systems Based on Multi-Task Learning with Improved Model\n  Structures", "abstract": "In a speech recognition system, voice activity detection (VAD) is a crucial\nfrontend module. Addressing the issues of poor noise robustness in traditional\nbinary VAD systems based on DFSMN, the paper further proposes semantic VAD\nbased on multi-task learning with improved models for real-time and offline\nsystems, to meet specific application requirements. Evaluations on internal\ndatasets show that, compared to the real-time VAD system based on DFSMN, the\nreal-time semantic VAD system based on RWKV achieves relative decreases in CER\nof 7.0\\%, DCF of 26.1\\% and relative improvement in NRR of 19.2\\%. Similarly,\nwhen compared to the offline VAD system based on DFSMN, the offline VAD system\nbased on SAN-M demonstrates relative decreases in CER of 4.4\\%, DCF of 18.6\\%\nand relative improvement in NRR of 3.5\\%.", "published": "2023-12-19 10:50:57", "link": "http://arxiv.org/abs/2312.14860v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ms-senet: Enhancing Speech Emotion Recognition Through Multi-scale\n  Feature Fusion With Squeeze-and-excitation Blocks", "abstract": "Speech Emotion Recognition (SER) has become a growing focus of research in\nhuman-computer interaction. Spatiotemporal features play a crucial role in SER,\nyet current research lacks comprehensive spatiotemporal feature learning. This\npaper focuses on addressing this gap by proposing a novel approach. In this\npaper, we employ Convolutional Neural Network (CNN) with varying kernel sizes\nfor spatial and temporal feature extraction. Additionally, we introduce\nSqueeze-and-Excitation (SE) modules to capture and fuse multi-scale features,\nfacilitating effective information fusion for improved emotion recognition and\na deeper understanding of the temporal evolution of speech emotion. Moreover,\nwe employ skip connections and Spatial Dropout (SD) layers to prevent\noverfitting and increase the model's depth. Our method outperforms the previous\nstate-of-the-art method, achieving an average UAR and WAR improvement of 1.62%\nand 1.32%, respectively, across six benchmark SER datasets. Further experiments\ndemonstrated that our method can fully extract spatiotemporal features in\nlow-resource conditions.", "published": "2023-12-19 09:11:55", "link": "http://arxiv.org/abs/2312.11974v2", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "StyleSpeech: Self-supervised Style Enhancing with VQ-VAE-based\n  Pre-training for Expressive Audiobook Speech Synthesis", "abstract": "The expressive quality of synthesized speech for audiobooks is limited by\ngeneralized model architecture and unbalanced style distribution in the\ntraining data. To address these issues, in this paper, we propose a\nself-supervised style enhancing method with VQ-VAE-based pre-training for\nexpressive audiobook speech synthesis. Firstly, a text style encoder is\npre-trained with a large amount of unlabeled text-only data. Secondly, a\nspectrogram style extractor based on VQ-VAE is pre-trained in a self-supervised\nmanner, with plenty of audio data that covers complex style variations. Then a\nnovel architecture with two encoder-decoder paths is specially designed to\nmodel the pronunciation and high-level style expressiveness respectively, with\nthe guidance of the style extractor. Both objective and subjective evaluations\ndemonstrate that our proposed method can effectively improve the naturalness\nand expressiveness of the synthesized speech in audiobook synthesis especially\nfor the role and out-of-domain scenarios.", "published": "2023-12-19 14:13:26", "link": "http://arxiv.org/abs/2312.12181v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating Speech-in-Speech Perception via a Humanoid Robot", "abstract": "Underlying mechanisms of speech perception masked by background speakers, a\ncommon daily listening condition, are often investigated using various and\nlengthy psychophysical tests. The presence of a social agent, such as an\ninteractive humanoid NAO robot, may help maintain engagement and attention.\nHowever, such robots potentially have limited sound quality or processing\nspeed. As a first step towards the use of NAO in psychophysical testing of\nspeech-in-speech perception, we compared normal-hearing young adults'\nperformance when using the standard computer interface to that when using a NAO\nrobot to introduce the test and present all corresponding stimuli. Target\nsentences were presented with colour and number keywords in the presence of\ncompeting masker speech at varying target-to-masker ratios. Sentences were\nproduced by the same speaker, but voice differences between the target and\nmasker were introduced using speech synthesis methods. To assess test\nperformance, speech intelligibility and data collection duration were compared\nbetween the computer and NAO setups. Human-robot interaction was assessed using\nthe Negative Attitude Towards Robot Scale (NARS) and quantification of\nbehavioural cues (backchannels). Speech intelligibility results showed\nfunctional similarity between the computer and NAO setups. Data collection\ndurations were longer when using NAO. NARS results showed participants had a\nmore positive attitude toward robots prior to their interaction with NAO. The\npresence of more positive backchannels when using NAO suggest higher engagement\nwith the robot in comparison to the computer. Overall, the study presents the\npotential of the NAO for presentingspeech materials and collecting\npsychophysical measurements for speech-in-speech perception.", "published": "2023-12-19 15:46:24", "link": "http://arxiv.org/abs/2312.12262v1", "categories": ["eess.AS", "cs.RO", "cs.SD"], "primary_category": "eess.AS"}
