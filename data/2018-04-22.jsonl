{"title": "Semi-supervised User Geolocation via Graph Convolutional Networks", "abstract": "Social media user geolocation is vital to many applications such as event\ndetection. In this paper, we propose GCN, a multiview geolocation model based\non Graph Convolutional Networks, that uses both text and network context. We\ncompare GCN to the state-of-the-art, and to two baselines we propose, and show\nthat our model achieves or is competitive with the state- of-the-art over three\nbenchmark geolocation datasets when sufficient supervision is available. We\nalso evaluate GCN under a minimal supervision scenario, and show it outperforms\nbaselines. We find that highway network gates are essential for controlling the\namount of useful neighbourhood expansion in GCN.", "published": "2018-04-22 00:23:58", "link": "http://arxiv.org/abs/1804.08049v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Head Decoder for End-to-End Speech Recognition", "abstract": "This paper presents a new network architecture called multi-head decoder for\nend-to-end speech recognition as an extension of a multi-head attention model.\nIn the multi-head attention model, multiple attentions are calculated, and\nthen, they are integrated into a single attention. On the other hand, instead\nof the integration in the attention level, our proposed method uses multiple\ndecoders for each attention and integrates their outputs to generate a final\noutput. Furthermore, in order to make each head to capture the different\nmodalities, different attention functions are used for each head, leading to\nthe improvement of the recognition performance with an ensemble effect. To\nevaluate the effectiveness of our proposed method, we conduct an experimental\nevaluation using Corpus of Spontaneous Japanese. Experimental results\ndemonstrate that our proposed method outperforms the conventional methods such\nas location-based and multi-head attention models, and that it can capture\ndifferent speech/linguistic contexts within the attention-based encoder-decoder\nframework.", "published": "2018-04-22 00:28:23", "link": "http://arxiv.org/abs/1804.08050v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Sentence Embeddings for Coherence Modelling and Beyond", "abstract": "We present a novel and effective technique for performing text coherence\ntasks while facilitating deeper insights into the data. Despite obtaining\never-increasing task performance, modern deep-learning approaches to NLP tasks\noften only provide users with the final network decision and no additional\nunderstanding of the data. In this work, we show that a new type of sentence\nembedding learned through self-supervision can be applied effectively to text\ncoherence tasks while serving as a window through which deeper understanding of\nthe data can be obtained. To produce these sentence embeddings, we train a\nrecurrent neural network to take individual sentences and predict their\nlocation in a document in the form of a distribution over locations. We\ndemonstrate that these embeddings, combined with simple visual heuristics, can\nbe used to achieve performance competitive with state-of-the-art on multiple\ntext coherence tasks, outperforming more complex and specialized approaches.\nAdditionally, we demonstrate that these embeddings can provide insights useful\nto writers for improving writing quality and informing document structuring,\nand assisting readers in summarizing and locating information.", "published": "2018-04-22 01:02:23", "link": "http://arxiv.org/abs/1804.08053v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Training for Community Question Answer Selection Based on\n  Multi-scale Matching", "abstract": "Community-based question answering (CQA) websites represent an important\nsource of information. As a result, the problem of matching the most valuable\nanswers to their corresponding questions has become an increasingly popular\nresearch topic. We frame this task as a binary (relevant/irrelevant)\nclassification problem, and present an adversarial training framework to\nalleviate label imbalance issue. We employ a generative model to iteratively\nsample a subset of challenging negative samples to fool our classification\nmodel. Both models are alternatively optimized using REINFORCE algorithm. The\nproposed method is completely different from previous ones, where negative\nsamples in training set are directly used or uniformly down-sampled. Further,\nwe propose using Multi-scale Matching which explicitly inspects the correlation\nbetween words and ngrams of different levels of granularity. We evaluate the\nproposed method on SemEval 2016 and SemEval 2017 datasets and achieves\nstate-of-the-art or similar performance.", "published": "2018-04-22 02:24:54", "link": "http://arxiv.org/abs/1804.08058v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Scalable Neural Shortlisting-Reranking Approach for Large-Scale Domain\n  Classification in Natural Language Understanding", "abstract": "Intelligent personal digital assistants (IPDAs), a popular real-life\napplication with spoken language understanding capabilities, can cover\npotentially thousands of overlapping domains for natural language\nunderstanding, and the task of finding the best domain to handle an utterance\nbecomes a challenging problem on a large scale. In this paper, we propose a set\nof efficient and scalable neural shortlisting-reranking models for large-scale\ndomain classification in IPDAs. The shortlisting stage focuses on efficiently\ntrimming all domains down to a list of k-best candidate domains, and the\nreranking stage performs a list-wise reranking of the initial k-best domains\nwith additional contextual information. We show the effectiveness of our\napproach with extensive experiments on 1,500 IPDA domains.", "published": "2018-04-22 03:56:39", "link": "http://arxiv.org/abs/1804.08064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Large-Scale Domain Classification with Personalized Attention", "abstract": "In this paper, we explore the task of mapping spoken language utterances to\none of thousands of natural language understanding domains in intelligent\npersonal digital assistants (IPDAs). This scenario is observed for many\nmainstream IPDAs in industry that allow third parties to develop thousands of\nnew domains to augment built-in ones to rapidly increase domain coverage and\noverall IPDA capabilities. We propose a scalable neural model architecture with\na shared encoder, a novel attention mechanism that incorporates personalization\ninformation and domain-specific classifiers that solves the problem\nefficiently. Our architecture is designed to efficiently accommodate new\ndomains that appear in-between full model retraining cycles with a rapid\nbootstrapping mechanism two orders of magnitude faster than retraining. We\naccount for practical constraints in real-time production systems, and design\nto minimize memory footprint and runtime latency. We demonstrate that\nincorporating personalization results in significantly more accurate domain\nclassification in the setting with thousands of overlapping domains.", "published": "2018-04-22 04:00:36", "link": "http://arxiv.org/abs/1804.08065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inducing and Embedding Senses with Scaled Gumbel Softmax", "abstract": "Methods for learning word sense embeddings represent a single word with\nmultiple sense-specific vectors. These methods should not only produce\ninterpretable sense embeddings, but should also learn how to select which sense\nto use in a given context. We propose an unsupervised model that learns sense\nembeddings using a modified Gumbel softmax function, which allows for\ndifferentiable discrete sense selection. Our model produces sense embeddings\nthat are competitive (and sometimes state of the art) on multiple similarity\nbased downstream evaluations. However, performance on these downstream\nevaluations tasks does not correlate with interpretability of sense embeddings,\nas we discover through an interpretability comparison with competing\nmulti-sense embeddings. While many previous approaches perform well on\ndownstream evaluations, they do not produce interpretable embeddings and learn\nduplicated sense groups; our method achieves the best of both worlds.", "published": "2018-04-22 07:12:05", "link": "http://arxiv.org/abs/1804.08077v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IIIDYT at SemEval-2018 Task 3: Irony detection in English tweets", "abstract": "In this paper we introduce our system for the task of Irony detection in\nEnglish tweets, a part of SemEval 2018. We propose representation learning\napproach that relies on a multi-layered bidirectional LSTM, without using\nexternal features that provide additional semantic information. Although our\nmodel is able to outperform the baseline in the validation set, our results\nshow limited generalization power over the test set. Given the limited size of\nthe dataset, we think the usage of more pre-training schemes would greatly\nimprove the obtained results.", "published": "2018-04-22 11:01:08", "link": "http://arxiv.org/abs/1804.08094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reduce, Reuse, Recycle: New uses for old QA resources", "abstract": "We investigate applying repurposed generic QA data and models to a recently\nproposed relation extraction task. We find that training on SQuAD produces\nbetter zero-shot performance and more robust generalisation compared to the\ntask specific training set. We also show that standard QA architectures (e.g.\nFastQA or BiDAF) can be applied to the slot filling queries without the need\nfor model modification.", "published": "2018-04-22 15:44:17", "link": "http://arxiv.org/abs/1804.08125v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embedding Perturbation for Sentence Classification", "abstract": "In this technique report, we aim to mitigate the overfitting problem of\nnatural language by applying data augmentation methods. Specifically, we\nattempt several types of noise to perturb the input word embedding, such as\nGaussian noise, Bernoulli noise, and adversarial noise, etc. We also apply\nseveral constraints on different types of noise. By implementing these proposed\ndata augmentation methods, the baseline models can gain improvements on several\nsentence classification tasks.", "published": "2018-04-22 20:42:13", "link": "http://arxiv.org/abs/1804.08166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Language Identification in Texts: A Survey", "abstract": "Language identification (LI) is the problem of determining the natural\nlanguage that a document or part thereof is written in. Automatic LI has been\nextensively researched for over fifty years. Today, LI is a key part of many\ntext processing pipelines, as text processing techniques generally assume that\nthe language of the input text is known. Research in this area has recently\nbeen especially active. This article provides a brief history of LI research,\nand an extensive survey of the features and methods used so far in the LI\nliterature. For describing the features and methods we introduce a unified\nnotation. We discuss evaluation methods, applications of LI, as well as\noff-the-shelf LI systems that do not require training by the end user. Finally,\nwe identify open issues, survey the work to date on each issue, and propose\nfuture directions for research in LI.", "published": "2018-04-22 22:30:30", "link": "http://arxiv.org/abs/1804.08186v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study on Passage Re-ranking in Embedding based Unsupervised Semantic\n  Search", "abstract": "State of the art approaches for (embedding based) unsupervised semantic\nsearch exploits either compositional similarity (of a query and a passage) or\npair-wise word (or term) similarity (from the query and the passage). By\ndesign, word based approaches do not incorporate similarity in the larger\ncontext (query/passage), while compositional similarity based approaches are\nusually unable to take advantage of the most important cues in the context. In\nthis paper we propose a new compositional similarity based approach, called\nvariable centroid vector (VCVB), that tries to address both of these\nlimitations. We also presents results using a different type of compositional\nsimilarity based approach by exploiting universal sentence embedding. We\nprovide empirical evaluation on two different benchmarks.", "published": "2018-04-22 02:20:32", "link": "http://arxiv.org/abs/1804.08057v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Unsupervised Discrete Sentence Representation Learning for Interpretable\n  Neural Dialog Generation", "abstract": "The encoder-decoder dialog model is one of the most prominent methods used to\nbuild dialog systems in complex domains. Yet it is limited because it cannot\noutput interpretable actions as in traditional systems, which hinders humans\nfrom understanding its generation process. We present an unsupervised discrete\nsentence representation learning method that can integrate with any existing\nencoder-decoder dialog models for interpretable response generation. Building\nupon variational autoencoders (VAEs), we present two novel models, DI-VAE and\nDI-VST that improve VAEs and can discover interpretable semantics via either\nauto encoding or context predicting. Our methods have been validated on\nreal-world dialog datasets to discover semantic representations and enhance\nencoder-decoder models with interpretable generation.", "published": "2018-04-22 05:08:52", "link": "http://arxiv.org/abs/1804.08069v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Performance Impact Caused by Hidden Bias of Training Data for\n  Recognizing Textual Entailment", "abstract": "The quality of training data is one of the crucial problems when a\nlearning-centered approach is employed. This paper proposes a new method to\ninvestigate the quality of a large corpus designed for the recognizing textual\nentailment (RTE) task. The proposed method, which is inspired by a statistical\nhypothesis test, consists of two phases: the first phase is to introduce the\npredictability of textual entailment labels as a null hypothesis which is\nextremely unacceptable if a target corpus has no hidden bias, and the second\nphase is to test the null hypothesis using a Naive Bayes model. The\nexperimental result of the Stanford Natural Language Inference (SNLI) corpus\ndoes not reject the null hypothesis. Therefore, it indicates that the SNLI\ncorpus has a hidden bias which allows prediction of textual entailment labels\nfrom hypothesis sentences even if no context information is given by a premise\nsentence. This paper also presents the performance impact of NN models for RTE\ncaused by this hidden bias.", "published": "2018-04-22 14:26:28", "link": "http://arxiv.org/abs/1804.08117v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Same Representation, Different Attentions: Shareable Sentence\n  Representation Learning from Multiple Tasks", "abstract": "Distributed representation plays an important role in deep learning based\nnatural language processing. However, the representation of a sentence often\nvaries in different tasks, which is usually learned from scratch and suffers\nfrom the limited amounts of training data. In this paper, we claim that a good\nsentence representation should be invariant and can benefit the various\nsubsequent tasks. To achieve this purpose, we propose a new scheme of\ninformation sharing for multi-task learning. More specifically, all tasks share\nthe same sentence representation and each task can select the task-specific\ninformation from the shared sentence representation with attention mechanism.\nThe query vector of each task's attention could be either static parameters or\ngenerated dynamically. We conduct extensive experiments on 16 different text\nclassification tasks, which demonstrate the benefits of our architecture.", "published": "2018-04-22 17:13:06", "link": "http://arxiv.org/abs/1804.08139v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NE-Table: A Neural key-value table for Named Entities", "abstract": "Many Natural Language Processing (NLP) tasks depend on using Named Entities\n(NEs) that are contained in texts and in external knowledge sources. While this\nis easy for humans, the present neural methods that rely on learned word\nembeddings may not perform well for these NLP tasks, especially in the presence\nof Out-Of-Vocabulary (OOV) or rare NEs. In this paper, we propose a solution\nfor this problem, and present empirical evaluations on: a) a structured\nQuestion-Answering task, b) three related Goal-Oriented dialog tasks, and c) a\nReading-Comprehension task, which show that the proposed method can be\neffective in dealing with both in-vocabulary and OOV NEs. We create extended\nversions of dialog bAbI tasks 1,2 and 4 and OOV versions of the CBT test set\navailable at - https://github.com/IBM/ne-table-datasets.", "published": "2018-04-22 20:09:13", "link": "http://arxiv.org/abs/1804.09540v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tempo-Invariant Processing of Rhythm with Convolutional Neural Networks", "abstract": "Rhythm patterns can be performed with a wide variation of tempi. This\npresents a challenge for many music information retrieval (MIR) systems;\nideally, perceptually similar rhythms should be represented and processed\nsimilarly, regardless of the specific tempo at which they were performed.\nSeveral recent systems for tempo estimation, beat tracking, and downbeat\ntracking have therefore sought to process rhythm in a tempo-invariant way,\noften by sampling input vectors according to a precomputed pulse level. This\npaper describes how a log-frequency representation of rhythm-related\nactivations instead can promote tempo invariance when processed with\nconvolutional neural networks. The strategy incorporates invariance at a\nfundamental level and can be useful for most tasks related to rhythm\nprocessing. Different methods are described, relying on magnitude, phase\nrelationships of different rhythm channels, as well as raw phase information.\nSeveral variations are explored to provide direction for future\nimplementations.", "published": "2018-04-22 20:48:29", "link": "http://arxiv.org/abs/1804.08167v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
