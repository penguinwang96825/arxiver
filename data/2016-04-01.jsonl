{"title": "A Compositional Approach to Language Modeling", "abstract": "Traditional language models treat language as a finite state automaton on a\nprobability space over words. This is a very strong assumption when modeling\nsomething inherently complex such as language. In this paper, we challenge this\nby showing how the linear chain assumption inherent in previous work can be\ntranslated into a sequential composition tree. We then propose a new model that\nmarginalizes over all possible composition trees thereby removing any\nunderlying structural assumptions. As the partition function of this new model\nis intractable, we use a recently proposed sentence level evaluation metric\nContrastive Entropy to evaluate our model. Given this new evaluation metric, we\nreport more than 100% improvement across distortion levels over current state\nof the art recurrent neural network based language models.", "published": "2016-04-01 01:51:34", "link": "http://arxiv.org/abs/1604.00100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Adaptation of Recurrent Neural Networks for Natural Language\n  Understanding", "abstract": "The goal of this paper is to use multi-task learning to efficiently scale\nslot filling models for natural language understanding to handle multiple\ntarget tasks or domains. The key to scalability is reducing the amount of\ntraining data needed to learn a model for a new task. The proposed multi-task\nmodel delivers better performance with less data by leveraging patterns that it\nlearns from the other tasks. The approach supports an open vocabulary, which\nallows the models to generalize to unseen words, which is particularly\nimportant when very little training data is used. A newly collected\ncrowd-sourced data set, covering four different domains, is used to demonstrate\nthe effectiveness of the domain adaptation and open vocabulary techniques.", "published": "2016-04-01 03:24:32", "link": "http://arxiv.org/abs/1604.00117v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Summarization Evaluation for Scientific Articles", "abstract": "Evaluation of text summarization approaches have been mostly based on metrics\nthat measure similarities of system generated summaries with a set of human\nwritten gold-standard summaries. The most widely used metric in summarization\nevaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps\nbetween the terms and phrases in the sentences; therefore, in cases of\nterminology variations and paraphrasing, ROUGE is not as effective. Scientific\narticle summarization is one such case that is different from general domain\nsummarization (e.g. newswire data). We provide an extensive analysis of ROUGE's\neffectiveness as an evaluation metric for scientific summarization; we show\nthat, contrary to the common belief, ROUGE is not much reliable in evaluating\nscientific summaries. We furthermore show how different variants of ROUGE\nresult in very different correlations with the manual Pyramid scores. Finally,\nwe propose an alternative metric for summarization evaluation which is based on\nthe content relevance between a system generated summary and the corresponding\nhuman written summaries. We call our metric SERA (Summarization Evaluation by\nRelevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations\nwith manual scores which shows its effectiveness in evaluation of scientific\narticle summarization.", "published": "2016-04-01 20:06:46", "link": "http://arxiv.org/abs/1604.00400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "abstract": "Despite interest in using cross-lingual knowledge to learn word embeddings\nfor various tasks, a systematic comparison of the possible approaches is\nlacking in the literature. We perform an extensive evaluation of four popular\napproaches of inducing cross-lingual embeddings, each requiring a different\nform of supervision, on four typographically different language pairs. Our\nevaluation setup spans four different tasks, including intrinsic evaluation on\nmono-lingual and cross-lingual similarity, and extrinsic evaluation on\ndownstream semantic and syntactic applications. We show that models which\nrequire expensive cross-lingual knowledge almost always perform better, but\ncheaply supervised models often prove competitive on certain tasks.", "published": "2016-04-01 22:18:51", "link": "http://arxiv.org/abs/1604.00425v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AttSum: Joint Learning of Focusing and Summarization with Neural\n  Attention", "abstract": "Query relevance ranking and sentence saliency ranking are the two main tasks\nin extractive query-focused summarization. Previous supervised summarization\nsystems often perform the two tasks in isolation. However, since reference\nsummaries are the trade-off between relevance and saliency, using them as\nsupervision, neither of the two rankers could be trained well. This paper\nproposes a novel summarization system called AttSum, which tackles the two\ntasks jointly. It automatically learns distributed representations for\nsentences as well as the document cluster. Meanwhile, it applies the attention\nmechanism to simulate the attentive reading of human behavior when a query is\ngiven. Extensive experiments are conducted on DUC query-focused summarization\nbenchmark datasets. Without using any hand-crafted features, AttSum achieves\ncompetitive performance. It is also observed that the sentences recognized to\nfocus on the query indeed meet the query need.", "published": "2016-04-01 04:18:39", "link": "http://arxiv.org/abs/1604.00125v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Semi-supervised and Unsupervised Methods for Categorizing Posts in Web\n  Discussion Forums", "abstract": "Web discussion forums are used by millions of people worldwide to share\ninformation belonging to a variety of domains such as automotive vehicles,\npets, sports, etc. They typically contain posts that fall into different\ncategories such as problem, solution, feedback, spam, etc. Automatic\nidentification of these categories can aid information retrieval that is\ntailored for specific user requirements. Previously, a number of supervised\nmethods have attempted to solve this problem; however, these depend on the\navailability of abundant training data. A few existing unsupervised and\nsemi-supervised approaches are either focused on identifying a single category\nor do not report category-specific performance. In contrast, this work proposes\nunsupervised and semi-supervised methods that require no or minimal training\ndata to achieve this objective without compromising on performance. A\nfine-grained analysis is also carried out to discuss their limitations. The\nproposed methods are based on sequence models (specifically, Hidden Markov\nModels) that can model language for each category using word and part-of-speech\nprobability distributions, and manually specified features. Empirical\nevaluations across domains demonstrate that the proposed methods are better\nsuited for this task than existing ones.", "published": "2016-04-01 03:32:03", "link": "http://arxiv.org/abs/1604.00119v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Nonparametric Spherical Topic Modeling with Word Embeddings", "abstract": "Traditional topic models do not account for semantic regularities in\nlanguage. Recent distributional representations of words exhibit semantic\nconsistency over directional metrics such as cosine similarity. However,\nneither categorical nor Gaussian observational distributions used in existing\ntopic models are appropriate to leverage such correlations. In this paper, we\npropose to use the von Mises-Fisher distribution to model the density of words\nover a unit sphere. Such a representation is well-suited for directional data.\nWe use a Hierarchical Dirichlet Process for our base topic model and propose an\nefficient inference algorithm based on Stochastic Variational Inference. This\nmodel enables us to naturally exploit the semantic structures of word\nembeddings while flexibly discovering the number of topics. Experiments\ndemonstrate that our method outperforms competitive approaches in terms of\ntopic coherence on two different text corpora while offering efficient\ninference.", "published": "2016-04-01 04:36:58", "link": "http://arxiv.org/abs/1604.00126v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Semisupervised Approach for Language Identification based on Ladder\n  Networks", "abstract": "In this study we address the problem of training a neuralnetwork for language\nidentification using both labeled and unlabeled speech samples in the form of\ni-vectors. We propose a neural network architecture that can also handle\nout-of-set languages. We utilize a modified version of the recently proposed\nLadder Network semisupervised training procedure that optimizes the\nreconstruction costs of a stack of denoising autoencoders. We show that this\napproach can be successfully applied to the case where the training dataset is\ncomposed of both labeled and unlabeled acoustic data. The results show enhanced\nlanguage identification on the NIST 2015 language identification dataset.", "published": "2016-04-01 16:26:57", "link": "http://arxiv.org/abs/1604.00317v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
