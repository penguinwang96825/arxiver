{"title": "Zero-Shot Cross-Lingual Transfer with Meta Learning", "abstract": "Learning what to share between tasks has been a topic of great importance\nrecently, as strategic sharing of knowledge has been shown to improve\ndownstream task performance. This is particularly important for multilingual\napplications, as most languages in the world are under-resourced. Here, we\nconsider the setting of training models on multiple different languages at the\nsame time, when little or no data is available for languages other than\nEnglish. We show that this challenging setup can be approached using\nmeta-learning, where, in addition to training a source language model, another\nmodel learns to select which training instances are the most beneficial to the\nfirst. We experiment using standard supervised, zero-shot cross-lingual, as\nwell as few-shot cross-lingual settings for different natural language\nunderstanding tasks (natural language inference, question answering). Our\nextensive experimental setup demonstrates the consistent effectiveness of\nmeta-learning for a total of 15 languages. We improve upon the state-of-the-art\nfor zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA\ndataset). A comprehensive error analysis indicates that the correlation of\ntypological features between languages can partly explain when parameter\nsharing learned via meta-learning is beneficial.", "published": "2020-03-05 16:07:32", "link": "http://arxiv.org/abs/2003.02739v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distill, Adapt, Distill: Training Small, In-Domain Models for Neural\n  Machine Translation", "abstract": "We explore best practices for training small, memory efficient machine\ntranslation models with sequence-level knowledge distillation in the domain\nadaptation setting. While both domain adaptation and knowledge distillation are\nwidely-used, their interaction remains little understood. Our large-scale\nempirical results in machine translation (on three language pairs with three\ndomains each) suggest distilling twice for best performance: once using\ngeneral-domain data and again using in-domain data with an adapted teacher.", "published": "2020-03-05 19:14:33", "link": "http://arxiv.org/abs/2003.02877v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What the [MASK]? Making Sense of Language-Specific BERT Models", "abstract": "Recently, Natural Language Processing (NLP) has witnessed an impressive\nprogress in many areas, due to the advent of novel, pretrained contextual\nrepresentation models. In particular, Devlin et al. (2019) proposed a model,\ncalled BERT (Bidirectional Encoder Representations from Transformers), which\nenables researchers to obtain state-of-the art performance on numerous NLP\ntasks by fine-tuning the representations on their data set and task, without\nthe need for developing and training highly-specific architectures. The authors\nalso released multilingual BERT (mBERT), a model trained on a corpus of 104\nlanguages, which can serve as a universal language model. This model obtained\nimpressive results on a zero-shot cross-lingual natural inference task. Driven\nby the potential of BERT models, the NLP community has started to investigate\nand generate an abundant number of BERT models that are trained on a particular\nlanguage, and tested on a specific data domain and task. This allows us to\nevaluate the true potential of mBERT as a universal language model, by\ncomparing it to the performance of these more specific models. This paper\npresents the current state of the art in language-specific BERT models,\nproviding an overall picture with respect to different dimensions (i.e.\narchitectures, data domains, and tasks). Our aim is to provide an immediate and\nstraightforward overview of the commonalities and differences between\nLanguage-Specific (language-specific) BERT models and mBERT. We also provide an\ninteractive and constantly updated website that can be used to explore the\ninformation we have collected, at https://bertlang.unibocconi.it.", "published": "2020-03-05 20:42:51", "link": "http://arxiv.org/abs/2003.02912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Cross-Lingual Transfer and Limited Annotated Data for Named\n  Entity Recognition in Danish", "abstract": "Named Entity Recognition (NER) has greatly advanced by the introduction of\ndeep neural architectures. However, the success of these methods depends on\nlarge amounts of training data. The scarcity of publicly-available\nhuman-labeled datasets has resulted in limited evaluation of existing NER\nsystems, as is the case for Danish. This paper studies the effectiveness of\ncross-lingual transfer for Danish, evaluates its complementarity to limited\ngold data, and sheds light on performance of Danish NER.", "published": "2020-03-05 21:25:00", "link": "http://arxiv.org/abs/2003.02931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Compilation of Resources for Academic Writing and Evaluating\n  with Informal Word Identification and Paraphrasing System", "abstract": "We present the first approach to automatically building resources for\nacademic writing. The aim is to build a writing aid system that automatically\nedits a text so that it better adheres to the academic style of writing. On top\nof existing academic resources, such as the Corpus of Contemporary American\nEnglish (COCA) academic Word List, the New Academic Word List, and the Academic\nCollocation List, we also explore how to dynamically build such resources that\nwould be used to automatically identify informal or non-academic words or\nphrases. The resources are compiled using different generic approaches that can\nbe extended for different domains and languages. We describe the evaluation of\nresources with a system implementation. The system consists of an informal word\nidentification (IWI), academic candidate paraphrase generation, and paraphrase\nranking components. To generate candidates and rank them in context, we have\nused the PPDB and WordNet paraphrase resources. We use the Concepts in Context\n(CoInCO) \"All-Words\" lexical substitution dataset both for the informal word\nidentification and paraphrase generation experiments. Our informal word\nidentification component achieves an F-1 score of 82%, significantly\noutperforming a stratified classifier baseline. The main contribution of this\nwork is a domain-independent methodology to build targeted resources for\nwriting aids.", "published": "2020-03-05 22:55:45", "link": "http://arxiv.org/abs/2003.02955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to mirror speaking styles incrementally", "abstract": "Mirroring is the behavior in which one person subconsciously imitates the\ngesture, speech pattern, or attitude of another. In conversations, mirroring\noften signals the speakers enjoyment and engagement in their communication. In\nchatbots, methods have been proposed to add personas to the chatbots and to\ntrain them to speak or to shift their dialogue style to that of the personas.\nHowever, they often require a large dataset consisting of dialogues of the\ntarget personalities to train. In this work, we explore a method that can learn\nto mirror the speaking styles of a person incrementally. Our method extracts\nngrams that capture a persons speaking styles and uses the ngrams to create\npatterns for transforming sentences to the persons speaking styles. Our\nexperiments show that our method is able to capture patterns of speaking style\nthat can be used to transform regular sentences into sentences with the target\nstyle.", "published": "2020-03-05 02:54:32", "link": "http://arxiv.org/abs/2003.04993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Incremental Explanation of Inference in Hybrid Bayesian Networks for\n  Increasing Model Trustworthiness and Supporting Clinical Decision Making", "abstract": "Various AI models are increasingly being considered as part of clinical\ndecision-support tools. However, the trustworthiness of such models is rarely\nconsidered. Clinicians are more likely to use a model if they can understand\nand trust its predictions. Key to this is if its underlying reasoning can be\nexplained. A Bayesian network (BN) model has the advantage that it is not a\nblack-box and its reasoning can be explained. In this paper, we propose an\nincremental explanation of inference that can be applied to hybrid BNs, i.e.\nthose that contain both discrete and continuous nodes. The key questions that\nwe answer are: (1) which important evidence supports or contradicts the\nprediction, and (2) through which intermediate variables does the information\nflow. The explanation is illustrated using a real clinical case study. A small\nevaluation study is also conducted.", "published": "2020-03-05 13:22:23", "link": "http://arxiv.org/abs/2003.02599v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Claim Check-Worthiness Detection as Positive Unlabelled Learning", "abstract": "As the first step of automatic fact checking, claim check-worthiness\ndetection is a critical component of fact checking systems. There are multiple\nlines of research which study this problem: check-worthiness ranking from\npolitical speeches and debates, rumour detection on Twitter, and citation\nneeded detection from Wikipedia. To date, there has been no structured\ncomparison of these various tasks to understand their relatedness, and no\ninvestigation into whether or not a unified approach to all of them is\nachievable. In this work, we illuminate a central challenge in claim\ncheck-worthiness detection underlying all of these tasks, being that they hinge\nupon detecting both how factual a sentence is, as well as how likely a sentence\nis to be believed without verification. As such, annotators only mark those\ninstances they judge to be clear-cut check-worthy. Our best performing method\nis a unified approach which automatically corrects for this using a variant of\npositive unlabelled learning that finds instances which were incorrectly\nlabelled as not check-worthy. In applying this, we out-perform the state of the\nart in two of the three tasks studied for claim check-worthiness detection in\nEnglish.", "published": "2020-03-05 16:06:07", "link": "http://arxiv.org/abs/2003.02736v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EmpTransfo: A Multi-head Transformer Architecture for Creating\n  Empathetic Dialog Systems", "abstract": "Understanding emotions and responding accordingly is one of the biggest\nchallenges of dialog systems. This paper presents EmpTransfo, a multi-head\nTransformer architecture for creating an empathetic dialog system. EmpTransfo\nutilizes state-of-the-art pre-trained models (e.g., OpenAI-GPT) for language\ngeneration, though models with different sizes can be used. We show that\nutilizing the history of emotions and other metadata can improve the quality of\ngenerated conversations by the dialog system. Our experimental results using a\nchallenging language corpus show that the proposed approach outperforms other\nmodels in terms of Hit@1 and PPL (Perplexity).", "published": "2020-03-05 23:09:24", "link": "http://arxiv.org/abs/2003.02958v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RecipeGPT: Generative Pre-training Based Cooking Recipe Generation and\n  Evaluation System", "abstract": "Interests in the automatic generation of cooking recipes have been growing\nsteadily over the past few years thanks to a large amount of online cooking\nrecipes. We present RecipeGPT, a novel online recipe generation and evaluation\nsystem. The system provides two modes of text generations: (1) instruction\ngeneration from given recipe title and ingredients; and (2) ingredient\ngeneration from recipe title and cooking instructions. Its back-end text\ngeneration module comprises a generative pre-trained language model GPT-2\nfine-tuned on a large cooking recipe dataset. Moreover, the recipe evaluation\nmodule allows the users to conveniently inspect the quality of the generated\nrecipe contents and store the results for future reference. RecipeGPT can be\naccessed online at https://recipegpt.org/.", "published": "2020-03-05 09:25:30", "link": "http://arxiv.org/abs/2003.02498v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward", "abstract": "Measuring the quality of a generated sequence against a set of references is\na central problem in many learning frameworks, be it to compute a score, to\nassign a reward, or to perform discrimination. Despite great advances in model\narchitectures, metrics that scale independently of the number of references are\nstill based on n-gram estimates. We show that the underlying operations,\ncounting words and comparing counts, can be lifted to embedding words and\ncomparing embeddings. An in-depth analysis of BERT embeddings shows empirically\nthat contextual embeddings can be employed to capture the required dependencies\nwhile maintaining the necessary scalability through appropriate pruning and\nsmoothing techniques. We cast unconditional generation as a reinforcement\nlearning problem and show that our reward function indeed provides a more\neffective learning signal than n-gram reward in this challenging setting.", "published": "2020-03-05 16:06:37", "link": "http://arxiv.org/abs/2003.02738v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "HypoNLI: Exploring the Artificial Patterns of Hypothesis-only Bias in\n  Natural Language Inference", "abstract": "Many recent studies have shown that for models trained on datasets for\nnatural language inference (NLI), it is possible to make correct predictions by\nmerely looking at the hypothesis while completely ignoring the premise. In this\nwork, we manage to derive adversarial examples in terms of the hypothesis-only\nbias and explore eligible ways to mitigate such bias. Specifically, we extract\nvarious phrases from the hypotheses (artificial patterns) in the training sets,\nand show that they have been strong indicators to the specific labels. We then\nfigure out `hard' and `easy' instances from the original test sets whose labels\nare opposite to or consistent with those indications. We also set up baselines\nincluding both pretrained models (BERT, RoBERTa, XLNet) and competitive\nnon-pretrained models (InferSent, DAM, ESIM). Apart from the benchmark and\nbaselines, we also investigate two debiasing approaches which exploit the\nartificial pattern modeling to mitigate such hypothesis-only bias:\ndown-sampling and adversarial training. We believe those methods can be treated\nas competitive baselines in NLI debiasing tasks.", "published": "2020-03-05 16:46:35", "link": "http://arxiv.org/abs/2003.02756v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Accuracy Law for Sequential Machine Translation: the Case\n  of Google Translate", "abstract": "In this research, we have established, through empirical testing, a law that\nrelates the number of translating hops to translation accuracy in sequential\nmachine translation in Google Translate. Both accuracy and size decrease with\nthe number of hops; the former displays a decrease closely following a power\nlaw. Such a law allows one to predict the behavior of translation chains that\nmay be built as society increasingly depends on automated devices.", "published": "2020-03-05 18:40:44", "link": "http://arxiv.org/abs/2003.02817v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Auto-Tuning Spectral Clustering for Speaker Diarization Using Normalized\n  Maximum Eigengap", "abstract": "In this study, we propose a new spectral clustering framework that can\nauto-tune the parameters of the clustering algorithm in the context of speaker\ndiarization. The proposed framework uses normalized maximum eigengap (NME)\nvalues to estimate the number of clusters and the parameters for the threshold\nof the elements of each row in an affinity matrix during spectral clustering,\nwithout the use of parameter tuning on the development set. Even through this\nhands-off approach, we achieve a comparable or better performance across\nvarious evaluation sets than the results found using traditional clustering\nmethods that apply careful parameter tuning and development data. A relative\nimprovement of 17% in the speaker error rate on the well-known CALLHOME\nevaluation set shows the effectiveness of our proposed spectral clustering with\nauto-tuning.", "published": "2020-03-05 02:50:37", "link": "http://arxiv.org/abs/2003.02405v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Overdetermined independent vector analysis", "abstract": "We address the convolutive blind source separation problem for the\n(over-)determined case where (i) the number of nonstationary target-sources $K$\nis less than that of microphones $M$, and (ii) there are up to $M - K$\nstationary Gaussian noises that need not to be extracted. Independent vector\nanalysis (IVA) can solve the problem by separating into $M$ sources and\nselecting the top $K$ highly nonstationary signals among them, but this\napproach suffers from a waste of computation especially when $K \\ll M$. Channel\nreductions in preprocessing of IVA by, e.g., principle component analysis have\nthe risk of removing the target signals. We here extend IVA to resolve these\nissues. One such extension has been attained by assuming the orthogonality\nconstraint (OC) that the sample correlation between the target and noise\nsignals is to be zero. The proposed IVA, on the other hand, does not rely on OC\nand exploits only the independence between sources and the stationarity of the\nnoises. This enables us to develop several efficient algorithms based on block\ncoordinate descent methods with a problem specific acceleration. We clarify\nthat one such algorithm exactly coincides with the conventional IVA with OC,\nand also explain that the other newly developed algorithms are faster than it.\nExperimental results show the improved computational load of the new algorithms\ncompared to the conventional methods. In particular, a new algorithm\nspecialized for $K = 1$ outperforms the others.", "published": "2020-03-05 07:01:05", "link": "http://arxiv.org/abs/2003.02458v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Guided Generative Adversarial Neural Network for Representation Learning\n  and High Fidelity Audio Generation using Fewer Labelled Audio Data", "abstract": "Recent improvements in Generative Adversarial Neural Networks (GANs) have\nshown their ability to generate higher quality samples as well as to learn good\nrepresentations for transfer learning. Most of the representation learning\nmethods based on GANs learn representations ignoring their post-use scenario,\nwhich can lead to increased generalisation ability. However, the model can\nbecome redundant if it is intended for a specific task. For example, assume we\nhave a vast unlabelled audio dataset, and we want to learn a representation\nfrom this dataset so that it can be used to improve the emotion recognition\nperformance of a small labelled audio dataset. During the representation\nlearning training, if the model does not know the post emotion recognition\ntask, it can completely ignore emotion-related characteristics in the learnt\nrepresentation. This is a fundamental challenge for any unsupervised\nrepresentation learning model. In this paper, we aim to address this challenge\nby proposing a novel GAN framework: Guided Generative Neural Network (GGAN),\nwhich guides a GAN to focus on learning desired representations and generating\nsuperior quality samples for audio data leveraging fewer labelled samples.\nExperimental results show that using a very small amount of labelled data as\nguidance, a GGAN learns significantly better representations.", "published": "2020-03-05 11:01:33", "link": "http://arxiv.org/abs/2003.02836v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Statistical Context-Dependent Units Boundary Correction for Corpus-based\n  Unit-Selection Text-to-Speech", "abstract": "In this study, we present an innovative technique for speaker adaptation in\norder to improve the accuracy of segmentation with application to\nunit-selection Text-To-Speech (TTS) systems. Unlike conventional techniques for\nspeaker adaptation, which attempt to improve the accuracy of the segmentation\nusing acoustic models that are more robust in the face of the speaker's\ncharacteristics, we aim to use only context dependent characteristics\nextrapolated with linguistic analysis techniques. In simple terms, we use the\nintuitive idea that context dependent information is tightly correlated with\nthe related acoustic waveform. We propose a statistical model, which predicts\ncorrecting values to reduce the systematic error produced by a state-of-the-art\nHidden Markov Model (HMM) based speech segmentation. Our approach consists of\ntwo phases: (1) identifying context-dependent phonetic unit classes (for\ninstance, the class which identifies vowels as being the nucleus of\nmonosyllabic words); and (2) building a regression model that associates the\nmean error value made by the ASR during the segmentation of a single speaker\ncorpus to each class. The success of the approach is evaluated by comparing the\ncorrected boundaries of units and the state-of-the-art HHM segmentation against\na reference alignment, which is supposed to be the optimal solution. In\nconclusion, our work supplies a first analysis of a model sensitive to\nspeaker-dependent characteristics, robust to defective and noisy information,\nand a very simple implementation which could be utilized as an alternative to\neither more expensive speaker-adaptation systems or of numerous manual\ncorrection sessions.", "published": "2020-03-05 12:42:13", "link": "http://arxiv.org/abs/2003.02837v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Sparse and Cosparse Audio Dequantization Using Convex Optimization", "abstract": "The paper shows the potential of sparsity-based methods in restoring\nquantized signals. Following up on the study of Brauer et al. (IEEE ICASSP\n2016), we significantly extend the range of the evaluation scenarios: we\nintroduce the analysis (cosparse) model, we use more effective algorithms, we\nexperiment with another time-frequency transform. The paper shows that the\nanalysis-based model performs comparably to the synthesis-model, but the Gabor\ntransform produces better results than the originally used cosine transform.\nLast but not least, we provide codes and data in a reproducible way.", "published": "2020-03-05 16:11:39", "link": "http://arxiv.org/abs/2003.04222v2", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Talking-Heads Attention", "abstract": "We introduce \"talking-heads attention\" - a variation on multi-head attention\nwhich includes linearprojections across the attention-heads dimension,\nimmediately before and after the softmax operation.While inserting only a small\nnumber of additional parameters and a moderate amount of additionalcomputation,\ntalking-heads attention leads to better perplexities on masked language\nmodeling tasks, aswell as better quality when transfer-learning to language\ncomprehension and question answering tasks.", "published": "2020-03-05 05:17:17", "link": "http://arxiv.org/abs/2003.02436v1", "categories": ["cs.LG", "cs.NE", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
