{"title": "An Analysis of the Interdependence Between Peanut and Other Agricultural Commodities in China's Futures Market", "abstract": "This study analyzes historical data from five agricultural commodities in the\nChinese futures market to explore the correlation, cointegration, and Granger\ncausality between Peanut futures and related futures. Multivariate linear\nregression models are constructed for prices and logarithmic returns, while\ndynamic relationships are examined using VAR and DCC-EGARCH models. The results\nreveal a significant dynamic linkage between Peanut and Soybean Oil futures\nthrough DCC-EGARCH, whereas the VAR model suggests limited influence from other\nfutures. Additionally, the application of MLP, CNN, and LSTM neural networks\nfor price prediction highlights the critical role of time step configurations\nin forecasting accuracy. These findings provide valuable insights into the\ninterconnectedness of agricultural futures markets and the efficacy of advanced\nmodeling techniques in financial analysis.", "published": "2025-01-28 04:29:05", "link": "http://arxiv.org/abs/2501.16697v2", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Trends and Reversion in Financial Markets on Time Scales from Minutes to Decades", "abstract": "We empirically analyze the reversion of financial market trends with time\nhorizons ranging from minutes to decades. The analysis covers equities,\ninterest rates, currencies and commodities and combines 14 years of futures\ntick data, 30 years of daily futures prices, 330 years of monthly asset prices,\nand yearly financial data since medieval times.\n  Across asset classes, we find that markets are in a trending regime on time\nscales that range from a few hours to a few years, while they are in a\nreversion regime on shorter and longer time scales. In the trending regime,\nweak trends tend to persist, which can be explained by herding behavior of\ninvestors. However, in this regime trends tend to revert before they become\nstrong enough to be statistically significant, which can be interpreted as a\nreturn of asset prices to their intrinsic value. In the reversion regime, we\nfind the opposite pattern: weak trends tend to revert, while those trends that\nbecome statistically significant tend to persist.\n  Our results provide a set of empirical tests of theoretical models of\nfinancial markets. We interpret them in the light of a recently proposed\nlattice gas model, where the lattice represents the social network of traders,\nthe gas molecules represent the shares of financial assets, and efficient\nmarkets correspond to the critical point. If this model is accurate, the\nlattice gas must be near this critical point on time scales from 1 hour to a\nfew days, with a correlation time of a few years.", "published": "2025-01-28 07:51:41", "link": "http://arxiv.org/abs/2501.16772v1", "categories": ["q-fin.ST", "cond-mat.stat-mech", "q-fin.MF", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "Exploratory Mean-Variance Portfolio Optimization with Regime-Switching Market Dynamics", "abstract": "Considering the continuous-time Mean-Variance (MV) portfolio optimization\nproblem, we study a regime-switching market setting and apply reinforcement\nlearning (RL) techniques to assist informed exploration within the control\nspace. We introduce and solve the Exploratory Mean Variance with Regime\nSwitching (EMVRS) problem. We also present a Policy Improvement Theorem.\nFurther, we recognize that the widely applied Temporal Difference (TD) learning\nis not adequate for the EMVRS context, hence we consider Orthogonality\nCondition (OC) learning, leveraging the martingale property of the induced\noptimal value function from the analytical solution to EMVRS. We design a RL\nalgorithm that has more meaningful parameterization using the market parameters\nand propose an updating scheme for each parameter. Our empirical results\ndemonstrate the superiority of OC learning over TD learning with a clear\nconvergence of the market parameters towards their corresponding ``grounding\ntrue\" values in a simulated market scenario. In a real market data study, EMVRS\nwith OC learning outperforms its counterparts with the highest mean and\nreasonably low volatility of the annualized portfolio returns.", "published": "2025-01-28 02:48:41", "link": "http://arxiv.org/abs/2501.16659v1", "categories": ["q-fin.PM", "q-fin.MF", "q-fin.ST", "stat.ML"], "primary_category": "q-fin.PM"}
{"title": "Considerations on the use of financial ratios in the study of family businesses", "abstract": "Most empirical works that study the financing decisions of family businesses\nuse financial ratios. These data present asymmetry, non-normality,\nnon-linearity and even dependence on the results of the choice of which\naccounting figure goes to the numerator and denominator of the ratio. This\narticle uses compositional data analysis (CoDa) as well as classical analysis\nstrategies to compare the structure of balance sheet liabilities between family\nand non-family businesses, showing the sensitivity of the results to the\nmethodology used. The results prove the need to use appropriate methodologies\nto advance the academic discipline.", "published": "2025-01-28 08:50:18", "link": "http://arxiv.org/abs/2501.16793v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Why is the estimation of metaorder impact with public market data so challenging?", "abstract": "Estimating market impact and transaction costs of large trades (metaorders)\nis a very important topic in finance. However, using models of price and trade\nbased on public market data provide average price trajectories which are\nqualitatively different from what is observed during real metaorder executions:\nthe price increases linearly, rather than in a concave way, during the\nexecution and the amount of reversion after its end is very limited. We claim\nthat this is a generic phenomenon due to the fact that even sophisticated\nstatistical models are unable to correctly describe the origin of the\nautocorrelation of the order flow. We propose a modified Transient Impact Model\nwhich provides more realistic trajectories by assuming that only a fraction of\nthe metaorder trading triggers market order flow. Interestingly, in our model\nthere is a critical condition on the kernels of the price and order flow\nequations in which market impact becomes permanent.", "published": "2025-01-28 17:29:08", "link": "http://arxiv.org/abs/2501.17096v1", "categories": ["q-fin.TR", "cs.AI", "econ.EM", "physics.soc-ph"], "primary_category": "q-fin.TR"}
{"title": "Few-Shot Optimized Framework for Hallucination Detection in\n  Resource-Limited NLP Systems", "abstract": "Hallucination detection in text generation remains an ongoing struggle for\nnatural language processing (NLP) systems, frequently resulting in unreliable\noutputs in applications such as machine translation and definition modeling.\nExisting methods struggle with data scarcity and the limitations of unlabeled\ndatasets, as highlighted by the SHROOM shared task at SemEval-2024. In this\nwork, we propose a novel framework to address these challenges, introducing\nDeepSeek Few-shot optimization to enhance weak label generation through\niterative prompt engineering. We achieved high-quality annotations that\nconsiderably enhanced the performance of downstream models by restructuring\ndata to align with instruct generative models. We further fine-tuned the\nMistral-7B-Instruct-v0.3 model on these optimized annotations, enabling it to\naccurately detect hallucinations in resource-limited settings. Combining this\nfine-tuned model with ensemble learning strategies, our approach achieved 85.5%\naccuracy on the test set, setting a new benchmark for the SHROOM task. This\nstudy demonstrates the effectiveness of data restructuring, few-shot\noptimization, and fine-tuning in building scalable and robust hallucination\ndetection frameworks for resource-constrained NLP systems.", "published": "2025-01-28 01:26:22", "link": "http://arxiv.org/abs/2501.16616v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-AutoDiff: Auto-Differentiate Any LLM Workflow", "abstract": "Large Language Models (LLMs) have reshaped natural language processing,\npowering applications from multi-hop retrieval and question answering to\nautonomous agent workflows. Yet, prompt engineering -- the task of crafting\ntextual inputs to effectively direct LLMs -- remains difficult and\nlabor-intensive, particularly for complex pipelines that combine multiple LLM\ncalls with functional operations like retrieval and data formatting. We\nintroduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering\n(APE) that extends textual gradient-based methods (such as Text-Grad) to\nmulti-component, potentially cyclic LLM architectures. Implemented within the\nAdalFlow library, LLM-AutoDiff treats each textual input as a trainable\nparameter and uses a frozen backward engine LLM to generate feedback-akin to\ntextual gradients -- that guide iterative prompt updates. Unlike prior\nsingle-node approaches, LLM-AutoDiff inherently accommodates functional nodes,\npreserves time-sequential behavior in repeated calls (e.g., multi-hop loops),\nand combats the \"lost-in-the-middle\" problem by isolating distinct sub-prompts\n(instructions, formats, or few-shot examples). It further boosts training\nefficiency by focusing on error-prone samples through selective gradient\ncomputation. Across diverse tasks, including single-step classification,\nmulti-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff\nconsistently outperforms existing textual gradient baselines in both accuracy\nand training cost. By unifying prompt optimization through a graph-centric\nlens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating\nLLM workflows - mirroring the transformative role that automatic\ndifferentiation libraries have long played in neural network research.", "published": "2025-01-28 03:18:48", "link": "http://arxiv.org/abs/2501.16673v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MME-Industry: A Cross-Industry Multimodal Evaluation Benchmark", "abstract": "With the rapid advancement of Multimodal Large Language Models (MLLMs),\nnumerous evaluation benchmarks have emerged. However, comprehensive assessments\nof their performance across diverse industrial applications remain limited. In\nthis paper, we introduce MME-Industry, a novel benchmark designed specifically\nfor evaluating MLLMs in industrial settings.The benchmark encompasses 21\ndistinct domain, comprising 1050 question-answer pairs with 50 questions per\ndomain. To ensure data integrity and prevent potential leakage from public\ndatasets, all question-answer pairs were manually crafted and validated by\ndomain experts. Besides, the benchmark's complexity is effectively enhanced by\nincorporating non-OCR questions that can be answered directly, along with tasks\nrequiring specialized domain knowledge. Moreover, we provide both Chinese and\nEnglish versions of the benchmark, enabling comparative analysis of MLLMs'\ncapabilities across these languages. Our findings contribute valuable insights\ninto MLLMs' practical industrial applications and illuminate promising\ndirections for future model optimization research.", "published": "2025-01-28 03:56:17", "link": "http://arxiv.org/abs/2501.16688v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking", "abstract": "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps://github.com/Aegis1863/xJailbreak.", "published": "2025-01-28 06:07:58", "link": "http://arxiv.org/abs/2501.16727v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Through the Prism of Culture: Evaluating LLMs' Understanding of Indian\n  Subcultures and Traditions", "abstract": "Large Language Models (LLMs) have shown remarkable advancements but also\nraise concerns about cultural bias, often reflecting dominant narratives at the\nexpense of under-represented subcultures. In this study, we evaluate the\ncapacity of LLMs to recognize and accurately respond to the Little Traditions\nwithin Indian society, encompassing localized cultural practices and\nsubcultures such as caste, kinship, marriage, and religion. Through a series of\ncase studies, we assess whether LLMs can balance the interplay between dominant\nGreat Traditions and localized Little Traditions. We explore various prompting\nstrategies and further investigate whether using prompts in regional languages\nenhances the models cultural sensitivity and response quality. Our findings\nreveal that while LLMs demonstrate an ability to articulate cultural nuances,\nthey often struggle to apply this understanding in practical, context-specific\nscenarios. To the best of our knowledge, this is the first study to analyze\nLLMs engagement with Indian subcultures, offering critical insights into the\nchallenges of embedding cultural diversity in AI systems.", "published": "2025-01-28 06:58:25", "link": "http://arxiv.org/abs/2501.16748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Algorithm for Automatic Legislative Text Consolidation", "abstract": "This study introduces a method for automating the consolidation process in a\nlegal context, a time-consuming task traditionally performed by legal\nprofessionals. We present a generative approach that processes legislative\ntexts to automatically apply amendments. Our method employs light quantized\ngenerative model, fine-tuned with LoRA, to generate accurate and reliable\namended texts. To the authors knowledge, this is the first time generative\nmodels are used on legislative text consolidation. Our dataset is publicly\navailable on HuggingFace1. Experimental results demonstrate a significant\nimprovement in efficiency, offering faster updates to legal documents. A full\nautomated pipeline of legislative text consolidation can be done in a few\nhours, with a success rate of more than 63% on a difficult bill.", "published": "2025-01-28 08:52:33", "link": "http://arxiv.org/abs/2501.16794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JRE-L: Journalist, Reader, and Editor LLMs in the Loop for Science\n  Journalism for the General Audience", "abstract": "Science journalism reports current scientific discoveries to non-specialists,\naiming to enable public comprehension of the state of the art. This task is\nchallenging as the audience often lacks specific knowledge about the presented\nresearch. We propose a JRE-L framework that integrates three LLMs mimicking the\nwriting-reading-feedback-revision loop. In JRE-L, one LLM acts as the\njournalist, another LLM as the general public reader, and the third LLM as an\neditor. The journalist's writing is iteratively refined by feedback from the\nreader and suggestions from the editor. Our experiments demonstrate that by\nleveraging the collaboration of two 7B and one 1.8B open-source LLMs, we can\ngenerate articles that are more accessible than those generated by existing\nmethods, including prompting single advanced models such as GPT-4 and other\nLLM-collaboration strategies. Our code is publicly available at\ngithub.com/Zzoay/JRE-L.", "published": "2025-01-28 11:30:35", "link": "http://arxiv.org/abs/2501.16865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting harassment and defamation in cyberbullying with\n  emotion-adaptive training", "abstract": "Existing research on detecting cyberbullying incidents on social media has\nprimarily concentrated on harassment and is typically approached as a binary\nclassification task. However, cyberbullying encompasses various forms, such as\ndenigration and harassment, which celebrities frequently face. Furthermore,\nsuitable training data for these diverse forms of cyberbullying remains scarce.\nIn this study, we first develop a celebrity cyberbullying dataset that\nencompasses two distinct types of incidents: harassment and defamation. We\ninvestigate various types of transformer-based models, namely masked (RoBERTa,\nBert and DistilBert), replacing(Electra), autoregressive (XLnet),\nmasked&permuted (Mpnet), text-text (T5) and large language models (Llama2 and\nLlama3) under low source settings. We find that they perform competitively on\nexplicit harassment binary detection. However, their performance is\nsubstantially lower on harassment and denigration multi-classification tasks.\nTherefore, we propose an emotion-adaptive training framework (EAT) that helps\ntransfer knowledge from the domain of emotion detection to the domain of\ncyberbullying detection to help detect indirect cyberbullying events. EAT\nconsistently improves the average macro F1, precision and recall by 20% in\ncyberbullying detection tasks across nine transformer-based models under\nlow-resource settings. Our claims are supported by intuitive theoretical\ninsights and extensive experiments.", "published": "2025-01-28 13:15:07", "link": "http://arxiv.org/abs/2501.16925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Linguistics Learned to Stop Worrying and Love the Language Models", "abstract": "Language models can produce fluent, grammatical text. Nonetheless, some\nmaintain that language models don't really learn language and also that, even\nif they did, that would not be informative for the study of human learning and\nprocessing. On the other side, there have been claims that the success of LMs\nobviates the need for studying linguistic theory and structure. We argue that\nboth extremes are wrong. LMs can contribute to fundamental questions about\nlinguistic structure, language processing, and learning. They force us to\nrethink arguments about learning and are informative for major questions in\nlinguistic theory. But they do not replace linguistic structure and theory. We\noffer an optimistic take on the relationship between language models and\nlinguistics.", "published": "2025-01-28 16:13:19", "link": "http://arxiv.org/abs/2501.17047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Giving the Old a Fresh Spin: Quality Estimation-Assisted Constrained\n  Decoding for Automatic Post-Editing", "abstract": "Automatic Post-Editing (APE) systems often struggle with over-correction,\nwhere unnecessary modifications are made to a translation, diverging from the\nprinciple of minimal editing. In this paper, we propose a novel technique to\nmitigate over-correction by incorporating word-level Quality Estimation (QE)\ninformation during the decoding process. This method is architecture-agnostic,\nmaking it adaptable to any APE system, regardless of the underlying model or\ntraining approach. Our experiments on English-German, English-Hindi, and\nEnglish-Marathi language pairs show the proposed approach yields significant\nimprovements over their corresponding baseline APE systems, with TER gains of\n$0.65$, $1.86$, and $1.44$ points, respectively. These results underscore the\ncomplementary relationship between QE and APE tasks and highlight the\neffectiveness of integrating QE information to reduce over-correction in APE\nsystems.", "published": "2025-01-28 19:46:18", "link": "http://arxiv.org/abs/2501.17265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tailored Truths: Optimizing LLM Persuasion with Personalization and\n  Fabricated Statistics", "abstract": "Large Language Models (LLMs) are becoming increasingly persuasive,\ndemonstrating the ability to personalize arguments in conversation with humans\nby leveraging their personal data. This may have serious impacts on the scale\nand effectiveness of disinformation campaigns. We studied the persuasiveness of\nLLMs in a debate setting by having humans $(n=33)$ engage with LLM-generated\narguments intended to change the human's opinion. We quantified the LLM's\neffect by measuring human agreement with the debate's hypothesis pre- and\npost-debate and analyzing both the magnitude of opinion change, as well as the\nlikelihood of an update in the LLM's direction. We compare persuasiveness\nacross established persuasion strategies, including personalized arguments\ninformed by user demographics and personality, appeal to fabricated statistics,\nand a mixed strategy utilizing both personalized arguments and fabricated\nstatistics. We found that static arguments generated by humans and GPT-4o-mini\nhave comparable persuasive power. However, the LLM outperformed static\nhuman-written arguments when leveraging the mixed strategy in an interactive\ndebate setting. This approach had a $\\mathbf{51\\%}$ chance of persuading\nparticipants to modify their initial position, compared to $\\mathbf{32\\%}$ for\nthe static human-written arguments. Our results highlight the concerning\npotential for LLMs to enable inexpensive and persuasive large-scale\ndisinformation campaigns.", "published": "2025-01-28 20:06:09", "link": "http://arxiv.org/abs/2501.17273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Analysis of Sinhala YouTube Comments on Sinhala Music Videos:\n  A Dataset Study", "abstract": "This research investigates the area of Music Information Retrieval (MIR) and\nMusic Emotion Recognition (MER) in relation to Sinhala songs, an underexplored\nfield in music studies. The purpose of this study is to analyze the behavior of\nSinhala comments on YouTube Sinhala song videos using social media comments as\nprimary data sources. These included comments from 27 YouTube videos containing\n20 different Sinhala songs, which were carefully selected so that strict\nlinguistic reliability would be maintained and relevancy ensured. This process\nled to a total of 93,116 comments being gathered upon which the dataset was\nrefined further by advanced filtering methods and transliteration mechanisms\nresulting into 63,471 Sinhala comments. Additionally, 964 stop-words specific\nfor the Sinhala language were algorithmically derived out of which 182 matched\nexactly with English stop-words from NLTK corpus once translated. Also,\ncomparisons were made between general domain corpora in Sinhala against the\nYouTube Comment Corpus in Sinhala confirming latter as good representation of\ngeneral domain. The meticulously curated data set as well as the derived\nstop-words form important resources for future research in the fields of MIR\nand MER, since they could be used and demonstrate that there are possibilities\nwith computational techniques to solve complex musical experiences across\nvaried cultural traditions", "published": "2025-01-28 09:46:45", "link": "http://arxiv.org/abs/2501.18633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Town Hall Debate Prompting: Enhancing Logical Reasoning in LLMs through\n  Multi-Persona Interaction", "abstract": "Debate is a commonly used form of human communication catered towards\nproblem-solving because of its efficiency. Debate fundamentally allows multiple\nviewpoints to be brought up in problem-solving, and for complex problems, each\nviewpoint opens a new path for problem-solving. In this work, we apply this\nconcept to LLM decision-making by proposing town hall-style debate prompting\n(THDP), a prompting method that splices a language model into multiple personas\nthat will debate one another to reach a conclusion. Our experimental pipeline\nvaries both the number of personas and the personality types of each persona to\nfind the optimum town hall size and personality for benchmark performance as\nmeasured by ZebraLogic bench, a reasoning-intensive benchmark characterized by\nboth multiple-choice and fill-in-the-blank questions. Our experimental results\ndemonstrate that a town hall size of 5 personas with LLM-determined personality\ntypes performs optimally on ZebraLogic, achieving a 13\\% improvement over\none-shot CoT baselines in per-cell accuracy in GPT-4o, 9% puzzle accuracy\nincrease in Claude 3.5 Sonnet, and an improvement in hard puzzle accuracy from\n10-15%.", "published": "2025-01-28 11:58:09", "link": "http://arxiv.org/abs/2502.15725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHiP: Cross-modal Hierarchical Direct Preference Optimization for\n  Multimodal LLMs", "abstract": "Multimodal Large Language Models (MLLMs) still struggle with hallucinations\ndespite their impressive capabilities. Recent studies have attempted to\nmitigate this by applying Direct Preference Optimization (DPO) to multimodal\nscenarios using preference pairs from text-based responses. However, our\nanalysis of representation distributions reveals that multimodal DPO struggles\nto align image and text representations and to distinguish between hallucinated\nand non-hallucinated descriptions. To address these challenges, in this work,\nwe propose a Cross-modal Hierarchical Direct Preference Optimization (CHiP) to\naddress these limitations. We introduce a visual preference optimization module\nwithin the DPO framework, enabling MLLMs to learn from both textual and visual\npreferences simultaneously. Furthermore, we propose a hierarchical textual\npreference optimization module that allows the model to capture preferences at\nmultiple granular levels, including response, segment, and token levels. We\nevaluate CHiP through both quantitative and qualitative analyses, with results\nacross multiple benchmarks demonstrating its effectiveness in reducing\nhallucinations. On the Object HalBench dataset, CHiP outperforms DPO in\nhallucination reduction, achieving improvements of 52.7% and 55.5% relative\npoints based on the base model Muffin and LLaVA models, respectively. We make\nall our datasets and code publicly available: https://github.com/LVUGAI/CHiP.", "published": "2025-01-28 02:05:38", "link": "http://arxiv.org/abs/2501.16629v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Why Do We Laugh? Annotation and Taxonomy Generation for Laughable\n  Contexts in Spontaneous Text Conversation", "abstract": "Laughter serves as a multifaceted communicative signal in human interaction,\nyet its identification within dialogue presents a significant challenge for\nconversational AI systems. This study addresses this challenge by annotating\nlaughable contexts in Japanese spontaneous text conversation data and\ndeveloping a taxonomy to classify the underlying reasons for such contexts.\nInitially, multiple annotators manually labeled laughable contexts using a\nbinary decision (laughable or non-laughable). Subsequently, an LLM was used to\ngenerate explanations for the binary annotations of laughable contexts, which\nwere then categorized into a taxonomy comprising ten categories, including\n\"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range\nof laughter-inducing scenarios. The study also evaluated GPT-4o's performance\nin recognizing the majority labels of laughable contexts, achieving an F1 score\nof 43.14%. These findings contribute to the advancement of conversational AI by\nestablishing a foundation for more nuanced recognition and generation of\nlaughter, ultimately fostering more natural and engaging human-AI interactions.", "published": "2025-01-28 02:16:18", "link": "http://arxiv.org/abs/2501.16635v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DOCS: Quantifying Weight Similarity for Deeper Insights into Large\n  Language Models", "abstract": "We introduce a novel index, the Distribution of Cosine Similarity (DOCS), for\nquantitatively assessing the similarity between weight matrices in Large\nLanguage Models (LLMs), aiming to facilitate the analysis of their complex\narchitectures. Leveraging DOCS, our analysis uncovers intriguing patterns in\nthe latest open-source LLMs: adjacent layers frequently exhibit high weight\nsimilarity and tend to form clusters, suggesting depth-wise functional\nspecialization. Additionally, we prove that DOCS is theoretically effective in\nquantifying similarity for orthogonal matrices, a crucial aspect given the\nprevalence of orthogonal initializations in LLMs. This research contributes to\na deeper understanding of LLM architecture and behavior, offering tools with\npotential implications for developing more efficient and interpretable models.", "published": "2025-01-28 02:32:49", "link": "http://arxiv.org/abs/2501.16650v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contextual Reinforcement in Multimodal Token Compression for Large\n  Language Models", "abstract": "Effective token compression remains a critical challenge for scaling models\nto handle increasingly complex and diverse datasets. A novel mechanism based on\ncontextual reinforcement is introduced, dynamically adjusting token importance\nthrough interdependencies and semantic relevance. This approach enables\nsubstantial reductions in token usage while preserving the quality and\ncoherence of information representation. Incorporating graph-based algorithms\nand adaptive weighting, the method captures subtle contextual relationships\nacross textual and multimodal data, ensuring robust alignment and performance\nin downstream tasks. Evaluations across varied domains reveal significant\nimprovements in accuracy and semantic retention, particularly for tasks\nrequiring detailed cross-modal interactions. Memory usage analyses demonstrate\nimproved computational efficiency, with minimal overhead despite the additional\nreinforcement processes. Performance gains are further validated through error\ndistribution analyses, showing reduced semantic loss and syntactic\ninconsistencies compared to baseline models. The modular architecture ensures\ncompatibility with a wide range of open-source frameworks, facilitating\nscalable implementation for real-world applications. These findings highlight\nthe potential of contextual reinforcement in redefining token management\nstrategies and advancing large-scale model design.", "published": "2025-01-28 02:44:31", "link": "http://arxiv.org/abs/2501.16658v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Role of Explicit Temporal Modeling in Multimodal Large\n  Language Models for Video Understanding", "abstract": "Applying Multimodal Large Language Models (MLLMs) to video understanding\npresents significant challenges due to the need to model temporal relations\nacross frames. Existing approaches adopt either implicit temporal modeling,\nrelying solely on the LLM decoder, or explicit temporal modeling, employing\nauxiliary temporal encoders. To investigate this debate between the two\nparadigms, we propose the Stackable Temporal Encoder (STE). STE enables\nflexible explicit temporal modeling with adjustable temporal receptive fields\nand token compression ratios. Using STE, we systematically compare implicit and\nexplicit temporal modeling across dimensions such as overall performance, token\ncompression effectiveness, and temporal-specific understanding. We also explore\nSTE's design considerations and broader impacts as a plug-in module and in\nimage modalities. Our findings emphasize the critical role of explicit temporal\nmodeling, providing actionable insights to advance video MLLMs.", "published": "2025-01-28 08:30:58", "link": "http://arxiv.org/abs/2501.16786v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Misspellings in Natural Language Processing: A survey", "abstract": "This survey provides an overview of the challenges of misspellings in natural\nlanguage processing (NLP). While often unintentional, misspellings have become\nubiquitous in digital communication, especially with the proliferation of Web\n2.0, user-generated content, and informal text mediums such as social media,\nblogs, and forums. Even if humans can generally interpret misspelled text, NLP\nmodels frequently struggle to handle it: this causes a decline in performance\nin common tasks like text classification and machine translation. In this\npaper, we reconstruct a history of misspellings as a scientific problem. We\nthen discuss the latest advancements to address the challenge of misspellings\nin NLP. Main strategies to mitigate the effect of misspellings include data\naugmentation, double step, character-order agnostic, and tuple-based methods,\namong others. This survey also examines dedicated data challenges and\ncompetitions to spur progress in the field. Critical safety and ethical\nconcerns are also examined, for example, the voluntary use of misspellings to\ninject malicious messages and hate speech on social networks. Furthermore, the\nsurvey explores psycholinguistic perspectives on how humans process\nmisspellings, potentially informing innovative computational techniques for\ntext normalization and representation. Finally, the misspelling-related\nchallenges and opportunities associated with modern large language models are\nalso analyzed, including benchmarks, datasets, and performances of the most\nprominent language models against misspellings. This survey aims to be an\nexhaustive resource for researchers seeking to mitigate the impact of\nmisspellings in the rapidly evolving landscape of NLP.", "published": "2025-01-28 10:26:04", "link": "http://arxiv.org/abs/2501.16836v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Irony Detection, Reasoning and Understanding in Zero-shot Learning", "abstract": "Irony is a powerful figurative language (FL) on social media that can\npotentially mislead various NLP tasks, such as recommendation systems,\nmisinformation checks, and sentiment analysis. Understanding the implicit\nmeaning of this kind of subtle language is essential to mitigate irony's\nnegative impact on NLP tasks. However, building models to understand irony\npresents a unique set of challenges, because irony is a complex form of\nlanguage that often relies on context, tone, and subtle cues to convey meaning\nthat is opposite or different from the literal interpretation. Large language\nmodels, such as ChatGPT, are increasingly able to capture implicit and\ncontextual information. In this study, we investigate the generalization,\nreasoning and understanding ability of ChatGPT on irony detection across six\ndifferent genre irony detection datasets. Our findings suggest that ChatGPT\nappears to show an enhanced language understanding and reasoning ability. But\nit needs to be very careful in prompt engineering design. Thus, we propose a\nprompt engineering design framework IDADP to achieve higher irony detection\naccuracy, improved understanding of irony, and more effective explanations\ncompared to other state-of-the-art ChatGPT zero-shot approaches. And ascertain\nvia experiments that the practice generated under the framework is likely to be\nthe promised solution to resolve the generalization issues of LLMs.", "published": "2025-01-28 12:13:07", "link": "http://arxiv.org/abs/2501.16884v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling", "abstract": "Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs.", "published": "2025-01-28 14:15:42", "link": "http://arxiv.org/abs/2501.16975v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via\n  Language Models", "abstract": "We present COS(M+O)S, a System 2-inspired framework for open-ended plot\ndevelopment that systematically explores the vast space of possible story\nexpansions, enabling a 3B-parameter language model to approach the plot quality\nof a 70B model on select short-story tasks. The method accomplishes this by\ncombining Monte Carlo Tree Search (MCTS), guided by a step-level value model\nthat rewards moderate surprisal (curiosity) while penalizing incoherence, and\nOdds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value\nplot expansions. This iterative reinforcement learning loop systematically\nexplores multiple candidate plot branches, backpropagates quality signals, and\nadapts the policy for faster convergence, notably shifting the policy from\npuzzle-based Chain-of-Thought to more character-driven storytelling. In\nsmall-scale tests with short-story prompts, 67%-77% of participants favored\nCOS(M+O)S's highest-rated expansions over lower-rated ones, suggesting that our\nlearned value function aligns. GPT-4o ratings further show that COS(M+O)S\nsurpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming\nwithin 0.06 SD of Llama 3.1 70B (no significant difference, p=0.93). Pairwise\ncomparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no\nstatistically significant gap from 70B. Nevertheless, absolute story quality\nremains modest, constrained by the small model's capacity and limited training\ndata.", "published": "2025-01-28 17:44:04", "link": "http://arxiv.org/abs/2501.17104v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimizing Large Language Model Training Using FP4 Quantization", "abstract": "The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training.", "published": "2025-01-28 18:04:50", "link": "http://arxiv.org/abs/2501.17116v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Histoires Morales: A French Dataset for Assessing Moral Alignment", "abstract": "Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata.", "published": "2025-01-28 18:07:30", "link": "http://arxiv.org/abs/2501.17117v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ASTRAL: Automated Safety Testing of Large Language Models", "abstract": "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors.", "published": "2025-01-28 18:25:11", "link": "http://arxiv.org/abs/2501.17132v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data", "abstract": "Prior research on training grounded factuality classification models to\ndetect hallucinations in large language models (LLMs) has relied on public\nnatural language inference (NLI) data and synthetic data. However, conventional\nNLI datasets are not well-suited for document-level reasoning, which is\ncritical for detecting LLM hallucinations. Recent approaches to document-level\nsynthetic data generation involve iteratively removing sentences from documents\nand annotating factuality using LLM-based prompts. While effective, this method\nis computationally expensive for long documents and limited by the LLM's\ncapabilities. In this work, we analyze the differences between existing\nsynthetic training data used in state-of-the-art models and real LLM output\nclaims. Based on our findings, we propose a novel approach for synthetic data\ngeneration, CG2C, that leverages multi-hop reasoning on context graphs\nextracted from documents. Our fact checker model, FactCG, demonstrates improved\nperformance with more connected reasoning, using the same backbone models.\nExperiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark\nwith much smaller model size.", "published": "2025-01-28 18:45:07", "link": "http://arxiv.org/abs/2501.17144v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comprehensive Evaluation for a Large Scale Knowledge Graph Question\n  Answering Service", "abstract": "Question answering systems for knowledge graph (KGQA), answer factoid\nquestions based on the data in the knowledge graph. KGQA systems are complex\nbecause the system has to understand the relations and entities in the\nknowledge-seeking natural language queries and map them to structured queries\nagainst the KG to answer them. In this paper, we introduce Chronos, a\ncomprehensive evaluation framework for KGQA at industry scale. It is designed\nto evaluate such a multi-component system comprehensively, focusing on (1)\nend-to-end and component-level metrics, (2) scalable to diverse datasets and\n(3) a scalable approach to measure the performance of the system prior to\nrelease. In this paper, we discuss the unique challenges associated with\nevaluating KGQA systems at industry scale, review the design of Chronos, and\nhow it addresses these challenges. We will demonstrate how it provides a base\nfor data-driven decisions and discuss the challenges of using it to measure and\nimprove a real-world KGQA system.", "published": "2025-01-28 20:02:10", "link": "http://arxiv.org/abs/2501.17270v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Attribution analysis of legal language as used by LLM", "abstract": "Three publicly-available LLM specifically designed for legal tasks have been\nimplemented and shown that classification accuracy can benefit from training\nover legal corpora, but why and how? Here we use two publicly-available legal\ndatasets, a simpler binary classification task of ``overruling'' texts, and a\nmore elaborate multiple choice task identifying ``holding'' judicial decisions.\nWe report on experiments contrasting the legal LLM and a generic BERT model for\ncomparison, against both datasets. We use integrated gradient attribution\ntechniques to impute ``causes'' of variation in the models' perfomance, and\ncharacterize them in terms of the tokenizations each use. We find that while\nall models can correctly classify some test examples from the casehold task,\nother examples can only be identified by only one, model, and attribution can\nbe used to highlight the reasons for this. We find that differential behavior\nof the models' tokenizers accounts for most of the difference and analyze these\ndifferences in terms of the legal language they process. Frequency analysis of\ntokens generated by dataset texts, combined with use of known ``stop word''\nlists, allow identification of tokens that are clear signifiers of legal\ntopics.", "published": "2025-01-28 22:48:29", "link": "http://arxiv.org/abs/2501.17330v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Better Slow than Sorry: Introducing Positive Friction for Reliable\n  Dialogue Systems", "abstract": "While theories of discourse and cognitive science have long recognized the\nvalue of unhurried pacing, recent dialogue research tends to minimize friction\nin conversational systems. Yet, frictionless dialogue risks fostering\nuncritical reliance on AI outputs, which can obscure implicit assumptions and\nlead to unintended consequences. To meet this challenge, we propose integrating\npositive friction into conversational AI, which promotes user reflection on\ngoals, critical thinking on system response, and subsequent re-conditioning of\nAI systems. We hypothesize systems can improve goal alignment, modeling of user\nmental states, and task success by deliberately slowing down conversations in\nstrategic moments to ask questions, reveal assumptions, or pause. We present an\nontology of positive friction and collect expert human annotations on\nmulti-domain and embodied goal-oriented corpora. Experiments on these corpora,\nalong with simulated interactions using state-of-the-art systems, suggest\nincorporating friction not only fosters accountable decision-making, but also\nenhances machine understanding of user beliefs and goals, and increases task\nsuccess rates.", "published": "2025-01-28 23:50:02", "link": "http://arxiv.org/abs/2501.17348v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LLMs Provide Unstable Answers to Legal Questions", "abstract": "An LLM is stable if it reaches the same conclusion when asked the identical\nquestion multiple times. We find leading LLMs like gpt-4o, claude-3.5, and\ngemini-1.5 are unstable when providing answers to hard legal questions, even\nwhen made as deterministic as possible by setting temperature to 0. We curate\nand release a novel dataset of 500 legal questions distilled from real cases,\ninvolving two parties, with facts, competing legal arguments, and the question\nof which party should prevail. When provided the exact same question, we\nobserve that LLMs sometimes say one party should win, while other times saying\nthe other party should win. This instability has implications for the\nincreasing numbers of legal AI products, legal processes, and lawyers relying\non these LLMs.", "published": "2025-01-28 23:39:02", "link": "http://arxiv.org/abs/2502.05196v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "MCTS-SQL: An Effective Framework for Text-to-SQL with Monte Carlo Tree\n  Search", "abstract": "Text-to-SQL is a fundamental and longstanding problem in the NLP area, aiming\nat converting natural language queries into SQL, enabling non-expert users to\noperate databases. Recent advances in LLM have greatly improved text-to-SQL\nperformance. However, challenges persist, especially when dealing with complex\nuser queries. Current approaches (e.g., COT prompting and multi-agent\nframeworks) rely on the ability of models to plan and generate SQL\nautonomously, but controlling performance remains difficult. In addition, LLMs\nare still prone to hallucinations. To alleviate these challenges, we designed a\nnovel MCTS-SQL to guide SQL generation iteratively. The approach generates SQL\nqueries through Monte Carlo Tree Search (MCTS) and a heuristic self-refinement\nmechanism are used to enhance accuracy and reliability. Key components include\na schema selector for extracting relevant information and an MCTS-based\ngenerator for iterative query refinement. Experimental results from the SPIDER\nand BIRD benchmarks show that MCTS-SQL achieves state-of-the-art performance.\nSpecifically, on the BIRD development dataset, MCTS-SQL achieves an Execution\n(EX) accuracy of 69.40% using GPT-4o as the base model and a significant\nimprovement when dealing with challenging tasks, with an EX of 51.48%, which is\n3.41% higher than the existing method.", "published": "2025-01-28 00:52:23", "link": "http://arxiv.org/abs/2501.16607v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.DB"}
{"title": "CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web\n  Navigation", "abstract": "While much work on web agents emphasizes the promise of autonomously\nperforming tasks on behalf of users, in reality, agents often fall short on\ncomplex tasks in real-world contexts and modeling user preference. This\npresents an opportunity for humans to collaborate with the agent and leverage\nthe agent's capabilities effectively. We propose CowPilot, a framework\nsupporting autonomous as well as human-agent collaborative web navigation, and\nevaluation across task success and task efficiency. CowPilot reduces the number\nof steps humans need to perform by allowing agents to propose next steps, while\nusers are able to pause, reject, or take alternative actions. During execution,\nusers can interleave their actions with the agent by overriding suggestions or\nresuming agent control when needed. We conducted case studies on five common\nwebsites and found that the human-agent collaborative mode achieves the highest\nsuccess rate of 95% while requiring humans to perform only 15.2% of the total\nsteps. Even with human interventions during task execution, the agent\nsuccessfully drives up to half of task success on its own. CowPilot can serve\nas a useful tool for data collection and agent evaluation across websites,\nwhich we believe will enable research in how users and agents can work\ntogether. Video demonstrations are available at\nhttps://oaishi.github.io/cowpilot.html", "published": "2025-01-28 00:56:53", "link": "http://arxiv.org/abs/2501.16609v3", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "An LLM Benchmark for Addressee Recognition in Multi-modal Multi-party\n  Dialogue", "abstract": "Handling multi-party dialogues represents a significant step for advancing\nspoken dialogue systems, necessitating the development of tasks specific to\nmulti-party interactions. To address this challenge, we are constructing a\nmulti-modal multi-party dialogue corpus of triadic (three-participant)\ndiscussions. This paper focuses on the task of addressee recognition,\nidentifying who is being addressed to take the next turn, a critical component\nunique to multi-party dialogue systems. A subset of the corpus was annotated\nwith addressee information, revealing that explicit addressees are indicated in\napproximately 20% of conversational turns. To evaluate the task's complexity,\nwe benchmarked the performance of a large language model (GPT-4o) on addressee\nrecognition. The results showed that GPT-4o achieved an accuracy only\nmarginally above chance, underscoring the challenges of addressee recognition\nin multi-party dialogue. These findings highlight the need for further research\nto enhance the capabilities of large language models in understanding and\nnavigating the intricacies of multi-party conversational dynamics.", "published": "2025-01-28 02:27:55", "link": "http://arxiv.org/abs/2501.16643v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Large Language Model Critics for Execution-Free Evaluation of Code\n  Changes", "abstract": "Large language models (LLMs) offer a promising way forward for automating\nsoftware engineering tasks, such as bug fixes, feature additions, etc., via\nmulti-step LLM-based agentic workflows. However, existing metrics for\nevaluating such workflows, mainly build status and occasionally log analysis,\nare too sparse and limited in providing the information needed to assess the\nquality of changes made. In this work, we designed LLM-based critics to derive\nwell-structured and rigorous intermediate/step-level, execution-free evaluation\nproxies for repo-level code changes. Importantly, we assume access to the gold\ntest patch for the problem (i.e., reference-aware) to assess both semantics and\nexecutability of generated patches. With the gold test patch as a reference, we\npredict executability of all editing locations with an F1 score of 91.6%,\naggregating which, we can predict the build status in 84.8% of the instances in\nSWE-bench. In particular, such an execution-focused LLM critic outperforms\nother reference-free and reference-aware LLM critics by 38.9% to 72.5%.\nMoreover, we demonstrate the usefulness of such a reference-aware framework in\ncomparing patches generated by different agentic workflows. Finally, we\nopen-source the library developed for this project, which allows further usage\nfor either other agentic workflows or other benchmarks. The source code is\navailable at https://github.com/amazon-science/code-agent-eval.", "published": "2025-01-28 02:38:56", "link": "http://arxiv.org/abs/2501.16655v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic\n  Health Records", "abstract": "Methods to ensure factual accuracy of text generated by large language models\n(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence\nsystem that combines retrieval-augmented generation and LLM-as-a-Judge to\nverify whether LLM-generated text is factually supported by a patient's medical\nhistory based on their electronic health record (EHR). To evaluate this system,\nwe introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course\nnarratives from discharge summaries into a set of simple statements with\nclinician annotations for whether each statement is supported by the patient's\nEHR clinical notes. Whereas highest agreement between clinicians was 88.5%,\nVeriFact achieves up to 92.7% agreement when compared to a denoised and\nadjudicated average human clinican ground truth, suggesting that VeriFact\nexceeds the average clinician's ability to fact-check text against a patient's\nmedical record. VeriFact may accelerate the development of LLM-based EHR\napplications by removing current evaluation bottlenecks.", "published": "2025-01-28 03:13:16", "link": "http://arxiv.org/abs/2501.16672v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LO"], "primary_category": "cs.AI"}
{"title": "3D-MoE: A Mixture-of-Experts Multi-modal LLM for 3D Vision and Pose\n  Diffusion via Rectified Flow", "abstract": "3D vision and spatial reasoning have long been recognized as preferable for\naccurately perceiving our three-dimensional world, especially when compared\nwith traditional visual reasoning based on 2D images. Due to the difficulties\nin collecting high-quality 3D data, research in this area has only recently\ngained momentum. With the advent of powerful large language models (LLMs),\nmulti-modal LLMs for 3D vision have been developed over the past few years.\nHowever, most of these models focus primarily on the vision encoder for 3D\ndata. In this paper, we propose converting existing densely activated LLMs into\nmixture-of-experts (MoE) models, which have proven effective for multi-modal\ndata processing. In addition to leveraging these models' instruction-following\ncapabilities, we further enable embodied task planning by attaching a diffusion\nhead, Pose-DiT, that employs a novel rectified flow diffusion scheduler.\nExperimental results on 3D question answering and task-planning tasks\ndemonstrate that our 3D-MoE framework achieves improved performance with fewer\nactivated parameters.", "published": "2025-01-28 04:31:19", "link": "http://arxiv.org/abs/2501.16698v1", "categories": ["cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.CL"}
{"title": "A Stochastic Dynamical Theory of LLM Self-Adversariality: Modeling\n  Severity Drift as a Critical Process", "abstract": "This paper introduces a continuous-time stochastic dynamical framework for\nunderstanding how large language models (LLMs) may self-amplify latent biases\nor toxicity through their own chain-of-thought reasoning. The model posits an\ninstantaneous \"severity\" variable $x(t) \\in [0,1]$ evolving under a stochastic\ndifferential equation (SDE) with a drift term $\\mu(x)$ and diffusion\n$\\sigma(x)$. Crucially, such a process can be consistently analyzed via the\nFokker--Planck approach if each incremental step behaves nearly Markovian in\nseverity space. The analysis investigates critical phenomena, showing that\ncertain parameter regimes create phase transitions from subcritical\n(self-correcting) to supercritical (runaway severity). The paper derives\nstationary distributions, first-passage times to harmful thresholds, and\nscaling laws near critical points. Finally, it highlights implications for\nagents and extended LLM reasoning models: in principle, these equations might\nserve as a basis for formal verification of whether a model remains stable or\npropagates bias over repeated inferences.", "published": "2025-01-28 08:08:25", "link": "http://arxiv.org/abs/2501.16783v1", "categories": ["cs.CL", "cs.AI", "nlin.AO"], "primary_category": "cs.CL"}
{"title": "Multimodal Magic Elevating Depression Detection with a Fusion of Text\n  and Audio Intelligence", "abstract": "This study proposes an innovative multimodal fusion model based on a\nteacher-student architecture to enhance the accuracy of depression\nclassification. Our designed model addresses the limitations of traditional\nmethods in feature fusion and modality weight allocation by introducing\nmulti-head attention mechanisms and weighted multimodal transfer learning.\nLeveraging the DAIC-WOZ dataset, the student fusion model, guided by textual\nand auditory teacher models, achieves significant improvements in\nclassification accuracy. Ablation experiments demonstrate that the proposed\nmodel attains an F1 score of 99. 1% on the test set, significantly\noutperforming unimodal and conventional approaches. Our method effectively\ncaptures the complementarity between textual and audio features while\ndynamically adjusting the contributions of the teacher models to enhance\ngeneralization capabilities. The experimental results highlight the robustness\nand adaptability of the proposed framework in handling complex multimodal data.\nThis research provides a novel technical framework for multimodal large model\nlearning in depression analysis, offering new insights into addressing the\nlimitations of existing methods in modality fusion and feature extraction.", "published": "2025-01-28 09:30:29", "link": "http://arxiv.org/abs/2501.16813v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models", "abstract": "Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.", "published": "2025-01-28 13:31:18", "link": "http://arxiv.org/abs/2501.16937v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ToolFactory: Automating Tool Generation by Leveraging LLM to Understand\n  REST API Documentations", "abstract": "LLM-based tool agents offer natural language interfaces, enabling users to\nseamlessly interact with computing services. While REST APIs are valuable\nresources for building such agents, they must first be transformed into\nAI-compatible tools. Automatically generating AI-compatible tools from REST API\ndocuments can greatly streamline tool agent development and minimize user\nlearning curves. However, API documentation often suffers from a lack of\nstandardization, inconsistent schemas, and incomplete information. To address\nthese issues, we developed \\textbf{ToolFactory}, an open-source pipeline for\nautomating tool generation from unstructured API documents. To enhance the\nreliability of the developed tools, we implemented an evaluation method to\ndiagnose errors. Furthermore, we built a knowledge base of verified tools,\nwhich we leveraged to infer missing information from poorly documented APIs. We\ndeveloped the API Extraction Benchmark, comprising 167 API documents and 744\nendpoints in various formats, and designed a JSON schema to annotate them. This\nannotated dataset was utilized to train and validate ToolFactory. The\nexperimental results highlight the effectiveness of ToolFactory. We also\ndemonstrated ToolFactory by creating a domain-specific AI agent for\nglycomaterials research. ToolFactory exhibits significant potential for\nfacilitating the seamless integration of scientific REST APIs into AI\nworkflows.", "published": "2025-01-28 13:42:33", "link": "http://arxiv.org/abs/2501.16945v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Multiple Abstraction Level Retrieve Augment Generation", "abstract": "A Retrieval-Augmented Generation (RAG) model powered by a large language\nmodel (LLM) provides a faster and more cost-effective solution for adapting to\nnew data and knowledge. It also delivers more specialized responses compared to\npre-trained LLMs. However, most existing approaches rely on retrieving\nprefix-sized chunks as references to support question-answering (Q/A). This\napproach is often deployed to address information needs at a single level of\nabstraction, as it struggles to generate answers across multiple levels of\nabstraction. In an RAG setting, while LLMs can summarize and answer questions\neffectively when provided with sufficient details, retrieving excessive\ninformation often leads to the 'lost in the middle' problem and exceeds token\nlimitations. We propose a novel RAG approach that uses chunks of multiple\nabstraction levels (MAL), including multi-sentence-level, paragraph-level,\nsection-level, and document-level. The effectiveness of our approach is\ndemonstrated in an under-explored scientific domain of Glycoscience. Compared\nto traditional single-level RAG approaches, our approach improves AI evaluated\nanswer correctness of Q/A by 25.739\\% on Glyco-related papers.", "published": "2025-01-28 13:49:39", "link": "http://arxiv.org/abs/2501.16952v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings\n  of Reinforcement Learning Strategies", "abstract": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nalignment, and task-specific performance. However, ensuring harmlessness in\nthese systems remains a critical challenge, particularly in advanced models\nlike DeepSeek-R1. This paper examines the limitations of Reinforcement Learning\n(RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and\ncompares it with Supervised Fine-Tuning (SFT). While RL improves reasoning\ncapabilities, it faces challenges such as reward hacking, generalization\nfailures, language mixing, and high computational costs. We propose hybrid\ntraining approaches combining RL and SFT to achieve robust harmlessness\nreduction. Usage recommendations and future directions for deploying\nDeepSeek-R1 responsibly are also presented.", "published": "2025-01-28 15:52:51", "link": "http://arxiv.org/abs/2501.17030v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Context is Key for Agent Security", "abstract": "Judging the safety of an action, whether taken by a human or a system, must\ntake into account the context in which the action takes place. For example,\ndeleting an email from a user's mailbox may or may not be appropriate depending\non the email's content, the user's goals, or even available space. Systems\ntoday that make these judgements -- providing security against harmful or\ninappropriate actions -- rely on manually-crafted policies or user confirmation\nfor each relevant context. With the upcoming deployment of systems like\ngeneralist agents, we argue that we must rethink security designs to adapt to\nthe scale of contexts and capabilities of these systems. As a first step, this\npaper explores contextual security in the domain of agents and proposes\ncontextual security for agents (Conseca), a framework to generate just-in-time,\ncontextual, and human-verifiable security policies.", "published": "2025-01-28 16:55:39", "link": "http://arxiv.org/abs/2501.17070v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Mamba-Shedder: Post-Transformer Compression for Efficient Selective\n  Structured State Space Models", "abstract": "Large pre-trained models have achieved outstanding results in sequence\nmodeling. The Transformer block and its attention mechanism have been the main\ndrivers of the success of these models. Recently, alternative architectures,\nsuch as Selective Structured State Space Models (SSMs), have been proposed to\naddress the inefficiencies of Transformers. This paper explores the compression\nof SSM-based models, particularly Mamba and its hybrids. We study the\nsensitivity of these models to the removal of selected components at different\ngranularities to reduce the model size and computational overhead, thus\nimproving their efficiency while maintaining accuracy. The proposed solutions,\ncollectively referred to as Mamba-Shedder, achieve a speedup of up to 1.4x\nduring inference, demonstrating that model efficiency can be improved by\neliminating several redundancies with minimal impact on the overall model\nperformance. The code is available at\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.", "published": "2025-01-28 17:22:01", "link": "http://arxiv.org/abs/2501.17088v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.0"], "primary_category": "cs.LG"}
{"title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse\n  Autoencoders", "abstract": "Fine-grained steering of language model outputs is essential for safety and\nreliability. Prompting and finetuning are widely used to achieve these goals,\nbut interpretability researchers have proposed a variety of\nrepresentation-based techniques as well, including sparse autoencoders (SAEs),\nlinear artificial tomography, supervised steering vectors, linear probes, and\nrepresentation finetuning. At present, there is no benchmark for making direct\ncomparisons between these proposals. Therefore, we introduce AxBench, a\nlarge-scale benchmark for steering and concept detection, and report\nexperiments on Gemma-2-2B and 9B. For steering, we find that prompting\noutperforms all existing methods, followed by finetuning. For concept\ndetection, representation-based methods such as difference-in-means, perform\nthe best. On both evaluations, SAEs are not competitive. We introduce a novel\nweakly-supervised representational method (Rank-1 Representation Finetuning;\nReFT-r1), which is competitive on both tasks while providing the\ninterpretability advantages that prompting lacks. Along with AxBench, we train\nand publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.", "published": "2025-01-28 18:51:24", "link": "http://arxiv.org/abs/2501.17148v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "From Natural Language to Extensive-Form Game Representations", "abstract": "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success.", "published": "2025-01-28 20:30:36", "link": "http://arxiv.org/abs/2501.17282v3", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Fine-Tuning Open-Source Large Language Models to Improve Their\n  Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate\n  Their Potential Clinical Applications in Radiation Oncology", "abstract": "Background: The radiation oncology clinical practice involves many steps\nrelying on the dynamic interplay of abundant text data. Large language models\nhave displayed remarkable capabilities in processing complex text information.\nBut their direct applications in specific fields like radiation oncology remain\nunderexplored.\n  Purpose: This study aims to investigate whether fine-tuning LLMs with domain\nknowledge can improve the performance on Task (1) treatment regimen generation,\nTask (2) treatment modality selection (photon, proton, electron, or\nbrachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.\n  Methods: Data for 15,724 patient cases were extracted. Cases where patients\nhad a single diagnostic record, and a clearly identifiable primary treatment\nplan were selected for preprocessing and manual annotation to have 7,903 cases\nof the patient diagnosis, treatment plan, treatment modality, and ICD-10 code.\nEach case was used to construct a pair consisting of patient diagnostics\ndetails and an answer (treatment regimen, treatment modality, or ICD-10 code\nrespectively) for the supervised fine-tuning of these three tasks. Open source\nLLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the\nLow-Rank Approximations method. Accuracy and ROUGE-1 score were reported for\nthe fine-tuned models and original models. Clinical evaluation was performed on\nTask (1) by radiation oncologists, while precision, recall, and F-1 score were\nevaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used\nto statistically analyze the results.\n  Results: Fine-tuned LLMs outperformed original LLMs across all tasks with\np-value <= 0.001. Clinical evaluation demonstrated that over 60% of the\nfine-tuned LLMs-generated treatment regimens were clinically acceptable.\nPrecision, recall, and F1-score showed improved performance of fine-tuned LLMs.", "published": "2025-01-28 20:37:32", "link": "http://arxiv.org/abs/2501.17286v1", "categories": ["physics.med-ph", "cs.AI", "cs.CL"], "primary_category": "physics.med-ph"}
{"title": "Mitigating Hallucinated Translations in Large Language Models with\n  Hallucination-focused Preference Optimization", "abstract": "Machine Translation (MT) is undergoing a paradigm shift, with systems based\non fine-tuned large language models (LLM) becoming increasingly competitive\nwith traditional encoder-decoder models trained specifically for translation\ntasks. However, LLM-based systems are at a higher risk of generating\nhallucinations, which can severely undermine user's trust and safety. Most\nprior research on hallucination mitigation focuses on traditional MT models,\nwith solutions that involve post-hoc mitigation - detecting hallucinated\ntranslations and re-translating them. While effective, this approach introduces\nadditional complexity in deploying extra tools in production and also increases\nlatency. To address these limitations, we propose a method that intrinsically\nlearns to mitigate hallucinations during the model training phase.\nSpecifically, we introduce a data creation framework to generate hallucination\nfocused preference datasets. Fine-tuning LLMs on these preference datasets\nreduces the hallucination rate by an average of 96% across five language pairs,\nwhile preserving overall translation quality. In a zero-shot setting our\napproach reduces hallucinations by 89% on an average across three unseen target\nlanguages.", "published": "2025-01-28 20:58:43", "link": "http://arxiv.org/abs/2501.17295v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"Ownership, Not Just Happy Talk\": Co-Designing a Participatory Large\n  Language Model for Journalism", "abstract": "Journalism has emerged as an essential domain for understanding the uses,\nlimitations, and impacts of large language models (LLMs) in the workplace. News\norganizations face divergent financial incentives: LLMs already permeate\nnewswork processes within financially constrained organizations, even as\nongoing legal challenges assert that AI companies violate their copyright. At\nstake are key questions about what LLMs are created to do, and by whom: How\nmight a journalist-led LLM work, and what can participatory design illuminate\nabout the present-day challenges about adapting ``one-size-fits-all''\nfoundation models to a given context of use? In this paper, we undertake a\nco-design exploration to understand how a participatory approach to LLMs might\naddress opportunities and challenges around AI in journalism. Our 20 interviews\nwith reporters, data journalists, editors, labor organizers, product leads, and\nexecutives highlight macro, meso, and micro tensions that designing for this\nopportunity space must address. From these desiderata, we describe the result\nof our co-design work: organizational structures and functionality for a\njournalist-controlled LLM. In closing, we discuss the limitations of commercial\nfoundation models for workplace use, and the methodological implications of\napplying participatory methods to LLM co-design.", "published": "2025-01-28 21:06:52", "link": "http://arxiv.org/abs/2501.17299v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Memorize and Rank: Elevating Large Language Models for Clinical\n  Diagnosis Prediction", "abstract": "Clinical diagnosis prediction models, when provided with a patient's medical\nhistory, aim to detect potential diseases early, facilitating timely\nintervention and improving prognostic outcomes. However, the inherent scarcity\nof patient data and large disease candidate space often pose challenges in\ndeveloping satisfactory models for this intricate task. The exploration of\nleveraging Large Language Models (LLMs) for encapsulating clinical decision\nprocesses has been limited. We introduce MERA, a clinical diagnosis prediction\nmodel that bridges pertaining natural language knowledge with medical practice.\nWe apply hierarchical contrastive learning on a disease candidate ranking list\nto alleviate the large decision space issue. With concept memorization through\nfine-tuning, we bridge the natural language clinical knowledge with medical\ncodes. Experimental results on MIMIC-III and IV datasets show that MERA\nachieves the state-of-the-art diagnosis prediction performance and dramatically\nelevates the diagnosis prediction capabilities of generative LMs.", "published": "2025-01-28 22:38:45", "link": "http://arxiv.org/abs/2501.17326v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inferring from Logits: Exploring Best Practices for Decoding-Free\n  Generative Candidate Selection", "abstract": "Generative Language Models rely on autoregressive decoding to produce the\noutput sequence token by token. Many tasks such as preference optimization,\nrequire the model to produce task-level output consisting of multiple tokens\ndirectly by selecting candidates from a pool as predictions. Determining a\ntask-level prediction from candidates using the ordinary token-level decoding\nmechanism is constrained by time-consuming decoding and interrupted gradients\nby discrete token selection. Existing works have been using decoding-free\ncandidate selection methods to obtain candidate probability from initial output\nlogits over vocabulary. Though these estimation methods are widely used, they\nare not systematically evaluated, especially on end tasks. We introduce an\nevaluation of a comprehensive collection of decoding-free candidate selection\napproaches on a comprehensive set of tasks, including five multiple-choice QA\ntasks with a small candidate pool and four clinical decision tasks with a\nmassive amount of candidates, some with 10k+ options. We evaluate the\nestimation methods paired with a wide spectrum of foundation LMs covering\ndifferent architectures, sizes and training paradigms. The results and insights\nfrom our analysis inform the future model design.", "published": "2025-01-28 23:21:28", "link": "http://arxiv.org/abs/2501.17338v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt\n  Generation for Enhanced LLM Content Moderation", "abstract": "We present a modular pipeline that automates the generation of stealthy\njailbreak prompts derived from high-level content policies, enhancing LLM\ncontent moderation. First, we address query inefficiency and jailbreak strength\nby developing Graph of Attacks with Pruning (GAP), a method that utilizes\nstrategies from prior jailbreaks, resulting in 92% attack success rate on\nGPT-3.5 using only 54% of the queries of the prior algorithm. Second, we\naddress the cold-start issue by automatically generating seed prompts from the\nhigh-level policy using LLMs. Finally, we demonstrate the utility of these\ngenerated jailbreak prompts of improving content moderation by fine-tuning\nPromptGuard, a model trained to detect jailbreaks, increasing its accuracy on\nthe Toxic-Chat dataset from 5.1% to 93.89%.", "published": "2025-01-28 17:10:20", "link": "http://arxiv.org/abs/2501.18638v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Divergent Emotional Patterns in Disinformation on Social Media? An\n  Analysis of Tweets and TikToks about the DANA in Valencia", "abstract": "This study investigates the dissemination of disinformation on social media\nplatforms during the DANA event (DANA is a Spanish acronym for Depresion\nAislada en Niveles Altos, translating to high-altitude isolated depression)\nthat resulted in extremely heavy rainfall and devastating floods in Valencia,\nSpain, on October 29, 2024. We created a novel dataset of 650 TikTok and X\nposts, which was manually annotated to differentiate between disinformation and\ntrustworthy content. Additionally, a Few-Shot annotation approach with GPT-4o\nachieved substantial agreement (Cohen's kappa of 0.684) with manual labels.\nEmotion analysis revealed that disinformation on X is mainly associated with\nincreased sadness and fear, while on TikTok, it correlates with higher levels\nof anger and disgust. Linguistic analysis using the LIWC dictionary showed that\ntrustworthy content utilizes more articulate and factual language, whereas\ndisinformation employs negations, perceptual words, and personal anecdotes to\nappear credible. Audio analysis of TikTok posts highlighted distinct patterns:\ntrustworthy audios featured brighter tones and robotic or monotone narration,\npromoting clarity and credibility, while disinformation audios leveraged tonal\nvariation, emotional depth, and manipulative musical elements to amplify\nengagement. In detection models, SVM+TF-IDF achieved the highest F1-Score,\nexcelling with limited data. Incorporating audio features into\nroberta-large-bne improved both Accuracy and F1-Score, surpassing its text-only\ncounterpart and SVM in Accuracy. GPT-4o Few-Shot also performed well,\nshowcasing the potential of large language models for automated disinformation\ndetection. These findings demonstrate the importance of leveraging both textual\nand audio features for improved disinformation detection on multimodal\nplatforms like TikTok.", "published": "2025-01-28 17:50:32", "link": "http://arxiv.org/abs/2501.18640v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Contextually Entangled Gradient Mapping for Optimized LLM Comprehension", "abstract": "Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to\ngradient optimization, redefining the relationship between contextual\nembeddings and gradient updates to enhance semantic coherence and reasoning\ncapabilities in neural architectures. By treating gradients as dynamic carriers\nof contextual dependencies rather than isolated numerical entities, the\nproposed methodology bridges critical gaps in existing optimization strategies.\nThe integration of entangled gradient dynamics into a loss regularization\nframework demonstrated significant improvements in tasks involving long-form\nreasoning, contextual retention, and adaptability to unseen domains.\nExperimental evaluations showed that the CEGM-enhanced model consistently\noutperformed baseline approaches, achieving higher accuracy in token-level\npredictions and greater resilience to noisy inputs. Practical implementations\ninvolved modifications to training pipelines, introducing entanglement layers\nand dynamic coefficient adjustments that seamlessly align with existing\narchitectures. Results further highlighted reductions in semantic drift during\nsequential transformations and improvements in embedding coherence across\nparaphrased sentences, showing the robustness and versatility of the proposed\nmethodology. The findings demonstrate the broader implications of gradient\nentanglement for both theoretical advancements and practical applications in\noptimization strategies.", "published": "2025-01-28 11:50:35", "link": "http://arxiv.org/abs/2502.00048v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CosyAudio: Improving Audio Generation with Confidence Scores and\n  Synthetic Captions", "abstract": "Text-to-Audio (TTA) generation is an emerging area within AI-generated\ncontent (AIGC), where audio is created from natural language descriptions.\nDespite growing interest, developing robust TTA models remains challenging due\nto the scarcity of well-labeled datasets and the prevalence of noisy or\ninaccurate captions in large-scale, weakly labeled corpora. To address these\nchallenges, we propose CosyAudio, a novel framework that utilizes confidence\nscores and synthetic captions to enhance the quality of audio generation.\nCosyAudio consists of two core components: AudioCapTeller and an audio\ngenerator. AudioCapTeller generates synthetic captions for audio and provides\nconfidence scores to evaluate their accuracy. The audio generator uses these\nsynthetic captions and confidence scores to enable quality-aware audio\ngeneration. Additionally, we introduce a self-evolving training strategy that\niteratively optimizes CosyAudio across both well-labeled and weakly-labeled\ndatasets. Initially trained with well-labeled data, AudioCapTeller leverages\nits assessment capabilities on weakly-labeled datasets for high-quality\nfiltering and reinforcement learning, which further improves its performance.\nThe well-trained AudioCapTeller refines corpora by generating new captions and\nconfidence scores, serving for the audio generator training. Extensive\nexperiments on open-source datasets demonstrate that CosyAudio outperforms\nexisting models in automated audio captioning, generates more faithful audio,\nand exhibits strong generalization across diverse scenarios.", "published": "2025-01-28 07:32:07", "link": "http://arxiv.org/abs/2501.16761v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cortical Temporal Mismatch Compensation in Bimodal Cochlear Implant\n  Users: Selective Attention Decoding and Pupillometry Study", "abstract": "Bimodal stimulation, combining cochlear implant (CI) and acoustic input from\nthe opposite ear, typically enhances speech perception but varies due to\nfactors like temporal mismatch. Previously, we used cortical auditory evoked\npotentials (CAEPs) to estimate this mismatch based on N1 latency differences.\nThis study expands on that by assessing the impact of temporal mismatch\ncompensation on speech perception. We tested bimodal CI users in three\nconditions: clinical, compensated temporal mismatch, and a 50 ms mismatch.\nMeasures included speech understanding, pupillometry, CAEPs, selective\nattention decoding, and parietal alpha power. Despite stable speech\nunderstanding across conditions, neural measures showed stronger effects. CAEP\nN1P2 amplitudes were highest in the compensated condition. Phase-locking value\n(PLV) and selective attention decoding improved but lacked significance.\nParietal alpha power increased under 50 ms mismatch, suggesting cognitive\nresource allocation. Pupillometry correlated with speech understanding but\nshowed limited sensitivity. Findings highlight that neural metrics are more\nsensitive than behavioral tests for detecting interaural mismatch. While CAEP\nN1P2 amplitudes significantly improved with compensation, other neural measures\nshowed limited effects, suggesting the need for combined temporal and spectral\ncompensation strategies.", "published": "2025-01-28 16:14:10", "link": "http://arxiv.org/abs/2501.17048v1", "categories": ["q-bio.NC", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "SCDiar: a streaming diarization system based on speaker change detection\n  and speech recognition", "abstract": "In hours-long meeting scenarios, real-time speech stream often struggles with\nachieving accurate speaker diarization, commonly leading to speaker\nidentification and speaker count errors. To address this challenge, we propose\nSCDiar, a system that operates on speech segments, split at the token level by\na speaker change detection (SCD) module. Building on these segments, we\nintroduce several enhancements to efficiently select the best available segment\nfor each speaker. These improvements lead to significant gains across various\nbenchmarks. Notably, on real-world meeting data involving more than ten\nparticipants, SCDiar outperforms previous systems by up to 53.6\\% in accuracy,\nsubstantially narrowing the performance gap between online and offline systems.", "published": "2025-01-28 02:27:24", "link": "http://arxiv.org/abs/2501.16641v1", "categories": ["eess.AS", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AVE Speech Dataset: A Comprehensive Benchmark for Multi-Modal Speech\n  Recognition Integrating Audio, Visual, and Electromyographic Signals", "abstract": "The global aging population faces considerable challenges, particularly in\ncommunication, due to the prevalence of hearing and speech impairments. To\naddress these, we introduce the AVE speech dataset, a comprehensive multi-modal\nbenchmark for speech recognition tasks. The dataset includes a 100-sentence\nMandarin Chinese corpus with audio signals, lip-region video recordings, and\nsix-channel electromyography (EMG) data, collected from 100 participants. Each\nsubject read the entire corpus ten times, with each sentence averaging\napproximately two seconds in duration, resulting in over 55 hours of\nmulti-modal speech data per modality. Experiments demonstrate that combining\nthese modalities significantly improves recognition performance, particularly\nin cross-subject and high-noise environments. To our knowledge, this is the\nfirst publicly available sentence-level dataset integrating these three\nmodalities for large-scale Mandarin speech recognition. We expect this dataset\nto drive advancements in both acoustic and non-acoustic speech recognition\nresearch, enhancing cross-modal learning and human-machine interaction.", "published": "2025-01-28 08:05:22", "link": "http://arxiv.org/abs/2501.16780v1", "categories": ["cs.SD", "cs.HC", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MIDI-GPT: A Controllable Generative Model for Computer-Assisted\n  Multitrack Music Composition", "abstract": "We present and release MIDI-GPT, a generative system based on the Transformer\narchitecture that is designed for computer-assisted music composition\nworkflows. MIDI-GPT supports the infilling of musical material at the track and\nbar level, and can condition generation on attributes including: instrument\ntype, musical style, note density, polyphony level, and note duration. In order\nto integrate these features, we employ an alternative representation for\nmusical material, creating a time-ordered sequence of musical events for each\ntrack and concatenating several tracks into a single sequence, rather than\nusing a single time-ordered sequence where the musical events corresponding to\ndifferent tracks are interleaved. We also propose a variation of our\nrepresentation allowing for expressiveness. We present experimental results\nthat demonstrate that MIDI-GPT is able to consistently avoid duplicating the\nmusical material it was trained on, generate music that is stylistically\nsimilar to the training dataset, and that attribute controls allow enforcing\nvarious constraints on the generated material. We also outline several\nreal-world applications of MIDI-GPT, including collaborations with industry\npartners that explore the integration and evaluation of MIDI-GPT into\ncommercial products, as well as several artistic works produced using it.", "published": "2025-01-28 15:17:36", "link": "http://arxiv.org/abs/2501.17011v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Summary of the NOTSOFAR-1 Challenge: Highlights and Learnings", "abstract": "The first Natural Office Talkers in Settings of Far-field Audio Recordings\n(NOTSOFAR-1) Challenge is a pivotal initiative that sets new benchmarks by\noffering datasets more representative of the needs of real-world business\napplications than those previously available. The challenge provides a unique\ncombination of 280 recorded meetings across 30 diverse environments, capturing\nreal-world acoustic conditions and conversational dynamics, and a 1000-hour\nsimulated training dataset, synthesized with enhanced authenticity for\nreal-world generalization, incorporating 15,000 real acoustic transfer\nfunctions. In this paper, we provide an overview of the systems submitted to\nthe challenge and analyze the top-performing approaches, hypothesizing the\nfactors behind their success. Additionally, we highlight promising directions\nleft unexplored by participants. By presenting key findings and actionable\ninsights, this work aims to drive further innovation and progress in DASR\nresearch and applications.", "published": "2025-01-28 21:25:08", "link": "http://arxiv.org/abs/2501.17304v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Compact Neural TTS Voices for Accessibility", "abstract": "Contemporary text-to-speech solutions for accessibility applications can\ntypically be classified into two categories: (i) device-based statistical\nparametric speech synthesis (SPSS) or unit selection (USEL) and (ii)\ncloud-based neural TTS. SPSS and USEL offer low latency and low disk footprint\nat the expense of naturalness and audio quality. Cloud-based neural TTS systems\nprovide significantly better audio quality and naturalness but regress in terms\nof latency and responsiveness, rendering these impractical for real-world\napplications. More recently, neural TTS models were made deployable to run on\nhandheld devices. Nevertheless, latency remains higher than SPSS and USEL,\nwhile disk footprint prohibits pre-installation for multiple voices at once. In\nthis work, we describe a high-quality compact neural TTS system achieving\nlatency on the order of 15 ms with low disk footprint. The proposed solution is\ncapable of running on low-power devices.", "published": "2025-01-28 22:51:14", "link": "http://arxiv.org/abs/2501.17332v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Language Modelling for Speaker Diarization in Telephonic Interviews", "abstract": "The aim of this paper is to investigate the benefit of combining both\nlanguage and acoustic modelling for speaker diarization. Although conventional\nsystems only use acoustic features, in some scenarios linguistic data contain\nhigh discriminative speaker information, even more reliable than the acoustic\nones. In this study we analyze how an appropriate fusion of both kind of\nfeatures is able to obtain good results in these cases. The proposed system is\nbased on an iterative algorithm where a LSTM network is used as a speaker\nclassifier. The network is fed with character-level word embeddings and a GMM\nbased acoustic score created with the output labels from previous iterations.\nThe presented algorithm has been evaluated in a Call-Center database, which is\ncomposed of telephone interview audios. The combination of acoustic features\nand linguistic content shows a 84.29% improvement in terms of a word-level DER\nas compared to a HMM/VB baseline system. The results of this study confirms\nthat linguistic content can be efficiently used for some speaker recognition\ntasks.", "published": "2025-01-28 18:18:04", "link": "http://arxiv.org/abs/2501.17893v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
