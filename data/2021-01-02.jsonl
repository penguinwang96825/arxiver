{"title": "On-the-Fly Attention Modulation for Neural Generation", "abstract": "Despite considerable advancements with deep neural language models (LMs),\nneural text generation still suffers from degeneration: the generated text is\nrepetitive, generic, self-contradictory, and often lacks commonsense. Our\nanalyses on sentence-level attention patterns in LMs reveal that neural\ndegeneration may be associated with insufficient learning of task-specific\ncharacteristics by the attention mechanism. This finding motivates on-the-fly\nattention modulation -- a simple but effective method that enables the\ninjection of priors into attention computation during inference. Automatic and\nhuman evaluation results on three text generation benchmarks demonstrate that\nattention modulation helps LMs generate text with enhanced fluency, creativity,\nand commonsense reasoning, in addition to significantly reduce sentence-level\nrepetition.", "published": "2021-01-02 05:16:46", "link": "http://arxiv.org/abs/2101.00371v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask Learning for Class-Imbalanced Discourse Classification", "abstract": "Small class-imbalanced datasets, common in many high-level semantic tasks\nlike discourse analysis, present a particular challenge to current\ndeep-learning architectures. In this work, we perform an extensive analysis on\nsentence-level classification approaches for the News Discourse dataset, one of\nthe largest high-level semantic discourse datasets recently published. We show\nthat a multitask approach can improve 7% Micro F1-score upon current\nstate-of-the-art benchmarks, due in part to label corrections across tasks,\nwhich improve performance for underrepresented classes. We also offer a\ncomparative review of additional techniques proposed to address resource-poor\nproblems in NLP, and show that none of these approaches can improve\nclassification accuracy in such a setting.", "published": "2021-01-02 07:13:41", "link": "http://arxiv.org/abs/2101.00389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which Linguist Invented the Lightbulb? Presupposition Verification for\n  Question-Answering", "abstract": "Many Question-Answering (QA) datasets contain unanswerable questions, but\ntheir treatment in QA systems remains primitive. Our analysis of the Natural\nQuestions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion\nof unanswerable questions ($\\sim$21%) can be explained based on the presence of\nunverifiable presuppositions. We discuss the shortcomings of current models in\nhandling such questions, and describe how an improved system could handle them.\nThrough a user preference study, we demonstrate that the oracle behavior of our\nproposed system that provides responses based on presupposition failure is\npreferred over the oracle behavior of existing QA systems. Then we discuss how\nour proposed system could be implemented, presenting a novel framework that\nbreaks down the problem into three steps: presupposition generation,\npresupposition verification and explanation generation. We report our progress\nin tackling each subproblem, and present a preliminary approach to integrating\nthese steps into an existing QA system. We find that adding presuppositions and\ntheir verifiability to an existing model yields modest gains in downstream\nperformance and unanswerability detection. The biggest bottleneck is the\nverification component, which needs to be substantially improved for the\nintegrated system to approach ideal behavior -- even transfer from the best\nentailment models currently falls short.", "published": "2021-01-02 07:26:04", "link": "http://arxiv.org/abs/2101.00391v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Semantic Role Labeling with Neural Transition-based Model", "abstract": "End-to-end semantic role labeling (SRL) has been received increasing\ninterest. It performs the two subtasks of SRL: predicate identification and\nargument role labeling, jointly. Recent work is mostly focused on graph-based\nneural models, while the transition-based framework with neural networks which\nhas been widely used in a number of closely-related tasks, has not been studied\nfor the joint task yet. In this paper, we present the first work of\ntransition-based neural models for end-to-end SRL. Our transition model\nincrementally discovers all sentential predicates as well as their arguments by\na set of transition actions. The actions of the two subtasks are executed\nmutually for full interactions. Besides, we suggest high-order compositions to\nextract non-local features, which can enhance the proposed transition model\nfurther. Experimental results on CoNLL09 and Universal Proposition Bank show\nthat our final model can produce state-of-the-art performance, and meanwhile\nkeeps highly efficient in decoding. We also conduct detailed experimental\nanalysis for a deep understanding of our proposed model.", "published": "2021-01-02 07:35:54", "link": "http://arxiv.org/abs/2101.00394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lex-BERT: Enhancing BERT based NER with lexicons", "abstract": "In this work, we represent Lex-BERT, which incorporates the lexicon\ninformation into Chinese BERT for named entity recognition (NER) tasks in a\nnatural manner. Instead of using word embeddings and a newly designed\ntransformer layer as in FLAT, we identify the boundary of words in the\nsentences using special tokens, and the modified sentence will be encoded\ndirectly by BERT. Our model does not introduce any new parameters and are more\nefficient than FLAT. In addition, we do not require any word embeddings\naccompanying the lexicon collection. Experiments on Ontonotes and ZhCrossNER\nshow that our model outperforms FLAT and other baselines.", "published": "2021-01-02 07:43:21", "link": "http://arxiv.org/abs/2101.00396v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Superbizarre Is Not Superb: Derivational Morphology Improves BERT's\n  Interpretation of Complex Words", "abstract": "How does the input segmentation of pretrained language models (PLMs) affect\ntheir interpretations of complex words? We present the first study\ninvestigating this question, taking BERT as the example PLM and focusing on its\nsemantic representations of English derivatives. We show that PLMs can be\ninterpreted as serial dual-route models, i.e., the meanings of complex words\nare either stored or else need to be computed from the subwords, which implies\nthat maximally meaningful input tokens should allow for the best generalization\non new words. This hypothesis is confirmed by a series of semantic probing\ntasks on which DelBERT (Derivation leveraging BERT), a model with derivational\ninput segmentation, substantially outperforms BERT with WordPiece segmentation.\nOur results suggest that the generalization capabilities of PLMs could be\nfurther improved if a morphologically-informed vocabulary of input tokens were\nused.", "published": "2021-01-02 08:36:48", "link": "http://arxiv.org/abs/2101.00403v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CDLM: Cross-Document Language Modeling", "abstract": "We introduce a new pretraining approach geared for multi-document language\nmodeling, incorporating two key ideas into the masked language modeling\nself-supervised objective. First, instead of considering documents in\nisolation, we pretrain over sets of multiple related documents, encouraging the\nmodel to learn cross-document relationships. Second, we improve over recent\nlong-range transformers by introducing dynamic global attention that has access\nto the entire input to predict masked tokens. We release CDLM (Cross-Document\nLanguage Model), a new general language model for multi-document setting that\ncan be easily applied to downstream tasks. Our extensive analysis shows that\nboth ideas are essential for the success of CDLM, and work in synergy to set\nnew state-of-the-art results for several multi-text tasks. Code and models are\navailable at https://github.com/aviclu/CDLM.", "published": "2021-01-02 09:01:39", "link": "http://arxiv.org/abs/2101.00406v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Substructure Substitution: Structured Data Augmentation for NLP", "abstract": "We study a family of data augmentation methods, substructure substitution\n(SUB2), for natural language processing (NLP) tasks. SUB2 generates new\nexamples by substituting substructures (e.g., subtrees or subsequences) with\nones with the same label, which can be applied to many structured NLP tasks\nsuch as part-of-speech tagging and parsing. For more general tasks (e.g., text\nclassification) which do not have explicitly annotated substructures, we\npresent variations of SUB2 based on constituency parse trees, introducing\nstructure-aware data augmentation methods to general NLP tasks. For most cases,\ntraining with the augmented dataset by SUB2 achieves better performance than\ntraining with the original training set. Further experiments show that SUB2 has\nmore consistent performance than other investigated augmentation methods,\nacross different tasks and sizes of the seed dataset.", "published": "2021-01-02 09:54:24", "link": "http://arxiv.org/abs/2101.00411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting", "abstract": "In this paper, we generalize text infilling (e.g., masked language models) by\nproposing Sequence Span Rewriting (SSR) as a self-supervised\nsequence-to-sequence (seq2seq) pre-training objective. SSR provides more\nfine-grained learning signals for text representations by supervising the model\nto rewrite imperfect spans to ground truth, and it is more consistent than text\ninfilling with many downstream seq2seq tasks that rewrite a source sentences\ninto a target sentence. Our experiments with T5 models on various seq2seq tasks\nshow that SSR can substantially improve seq2seq pre-training. Moreover, we\nobserve SSR is especially helpful to improve pre-training a small-size seq2seq\nmodel with a powerful imperfect span generator, which indicates a new\nperspective of transferring knowledge from a large model to a smaller model for\nseq2seq pre-training.", "published": "2021-01-02 10:27:11", "link": "http://arxiv.org/abs/2101.00416v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense\n  Generation", "abstract": "We present Knowledge Enhanced Multimodal BART (KM-BART), which is a\nTransformer-based sequence-to-sequence model capable of reasoning about\ncommonsense knowledge from multimodal inputs of images and texts. We adapt the\ngenerative BART architecture to a multimodal model with visual and textual\ninputs. We further develop novel pretraining tasks to improve the model\nperformance on the Visual Commonsense Generation (VCG) task. In particular, our\npretraining task of Knowledge-based Commonsense Generation (KCG) boosts model\nperformance on the VCG task by leveraging commonsense knowledge from a large\nlanguage model pretrained on external commonsense knowledge graphs. To the best\nof our knowledge, we are the first to propose a dedicated task for improving\nmodel performance on the VCG task. Experimental results show that our model\nreaches state-of-the-art performance on the VCG task by applying these novel\npretraining tasks.", "published": "2021-01-02 10:44:49", "link": "http://arxiv.org/abs/2101.00419v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Highs and Lows of Simple Lexical Domain Adaptation Approaches for\n  Neural Machine Translation", "abstract": "Machine translation systems are vulnerable to domain mismatch, especially in\na low-resource scenario. Out-of-domain translations are often of poor quality\nand prone to hallucinations, due to exposure bias and the decoder acting as a\nlanguage model. We adopt two approaches to alleviate this problem: lexical\nshortlisting restricted by IBM statistical alignments, and hypothesis\nre-ranking based on similarity. The methods are computationally cheap, widely\nknown, but not extensively experimented on domain adaptation. We demonstrate\nsuccess on low-resource out-of-domain test sets, however, the methods are\nineffective when there is sufficient data or too great domain mismatch. This is\ndue to both the IBM model losing its advantage over the implicitly learned\nneural alignment, and issues with subword segmentation of out-of-domain words.", "published": "2021-01-02 11:06:15", "link": "http://arxiv.org/abs/2101.00421v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coreference Resolution without Span Representations", "abstract": "The introduction of pretrained language models has reduced many complex\ntask-specific NLP models to simple lightweight layers. An exception to this\ntrend is coreference resolution, where a sophisticated task-specific model is\nappended to a pretrained transformer encoder. While highly effective, the model\nhas a very large memory footprint -- primarily due to dynamically-constructed\nspan and span-pair representations -- which hinders the processing of complete\ndocuments and the ability to train on multiple instances in a single batch. We\nintroduce a lightweight end-to-end coreference model that removes the\ndependency on span representations, handcrafted features, and heuristics. Our\nmodel performs competitively with the current standard model, while being\nsimpler and more efficient.", "published": "2021-01-02 11:46:51", "link": "http://arxiv.org/abs/2101.00434v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Question Answering by Pretraining Span Selection", "abstract": "In several question answering benchmarks, pretrained models have reached\nhuman parity through fine-tuning on an order of 100,000 annotated questions and\nanswers. We explore the more realistic few-shot setting, where only a few\nhundred training examples are available, and observe that standard models\nperform poorly, highlighting the discrepancy between current pretraining\nobjectives and question answering. We propose a new pretraining scheme tailored\nfor question answering: recurring span selection. Given a passage with multiple\nsets of recurring spans, we mask in each set all recurring spans but one, and\nask the model to select the correct span in the passage for each masked span.\nMasked spans are replaced with a special token, viewed as a question\nrepresentation, that is later used during fine-tuning to select the answer\nspan. The resulting model obtains surprisingly good results on multiple\nbenchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while\nmaintaining competitive performance in the high-resource setting.", "published": "2021-01-02 11:58:44", "link": "http://arxiv.org/abs/2101.00438v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Emphasize: Dataset and Shared Task Models for Selecting\n  Emphasis in Presentation Slides", "abstract": "Presentation slides have become a common addition to the teaching material.\nEmphasizing strong leading words in presentation slides can allow the audience\nto direct the eye to certain focal points instead of reading the entire slide,\nretaining the attention to the speaker during the presentation. Despite a large\nvolume of studies on automatic slide generation, few studies have addressed the\nautomation of design assistance during the creation process. Motivated by this\ndemand, we study the problem of Emphasis Selection (ES) in presentation slides,\ni.e., choosing candidates for emphasis, by introducing a new dataset containing\npresentation slides with a wide variety of topics, each is annotated with\nemphasis words in a crowdsourced setting. We evaluate a range of\nstate-of-the-art models on this novel dataset by organizing a shared task and\ninviting multiple researchers to model emphasis in this new domain. We present\nthe main findings and compare the results of these models, and by examining the\nchallenges of the dataset, we provide different analysis components.", "published": "2021-01-02 06:54:55", "link": "http://arxiv.org/abs/2101.03237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic\n  Creativity and Commonsense Knowledge", "abstract": "Question: I have five fingers but I am not alive. What am I? Answer: a glove.\nAnswering such a riddle-style question is a challenging cognitive process, in\nthat it requires complex commonsense reasoning abilities, an understanding of\nfigurative language, and counterfactual reasoning skills, which are all\nimportant abilities for advanced natural language understanding (NLU). However,\nthere are currently no dedicated datasets aiming to test these abilities.\nHerein, we present RiddleSense, a new multiple-choice question answering task,\nwhich comes with the first large dataset (5.7k examples) for answering\nriddle-style commonsense questions. We systematically evaluate a wide range of\nmodels over the challenge, and point out that there is a large gap between the\nbest-supervised model and human performance -- suggesting intriguing future\nresearch in the direction of higher-order commonsense reasoning and linguistic\ncreativity towards building advanced NLU systems.", "published": "2021-01-02 05:28:15", "link": "http://arxiv.org/abs/2101.00376v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating Memorization of Conspiracy Theories in Text Generation", "abstract": "The adoption of natural language generation (NLG) models can leave\nindividuals vulnerable to the generation of harmful information memorized by\nthe models, such as conspiracy theories. While previous studies examine\nconspiracy theories in the context of social media, they have not evaluated\ntheir presence in the new space of generative language models. In this work, we\ninvestigate the capability of language models to generate conspiracy theory\ntext. Specifically, we aim to answer: can we test pretrained generative\nlanguage models for the memorization and elicitation of conspiracy theories\nwithout access to the model's training data? We highlight the difficulties of\nthis task and discuss it in the context of memorization, generalization, and\nhallucination. Utilizing a new dataset consisting of conspiracy theory topics\nand machine-generated conspiracy theories helps us discover that many\nconspiracy theories are deeply rooted in the pretrained language models. Our\nexperiments demonstrate a relationship between model parameters such as size\nand temperature and their propensity to generate conspiracy theory text. These\nresults indicate the need for a more thorough review of NLG applications before\nrelease and an in-depth discussion of the drawbacks of memorization in\ngenerative language models.", "published": "2021-01-02 05:47:39", "link": "http://arxiv.org/abs/2101.00379v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation\n  Learning, Semi-Supervised Learning and Interpretation", "abstract": "We introduce VoxPopuli, a large-scale multilingual corpus providing 100K\nhours of unlabelled speech data in 23 languages. It is the largest open data to\ndate for unsupervised representation learning as well as semi-supervised\nlearning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16\nlanguages and their aligned oral interpretations into 5 other languages\ntotaling 5.1K hours. We provide speech recognition baselines and validate the\nversatility of VoxPopuli unlabelled data in semi-supervised learning under\nchallenging out-of-domain settings. We will release the corpus at\nhttps://github.com/facebookresearch/voxpopuli under an open license.", "published": "2021-01-02 07:24:21", "link": "http://arxiv.org/abs/2101.00390v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "End-to-End Training of Neural Retrievers for Open-Domain Question\n  Answering", "abstract": "Recent work on training neural retrievers for open-domain question answering\n(OpenQA) has employed both supervised and unsupervised approaches. However, it\nremains unclear how unsupervised and supervised methods can be used most\neffectively for neural retrievers. In this work, we systematically study\nretriever pre-training. We first propose an approach of unsupervised\npre-training with the Inverse Cloze Task and masked salient spans, followed by\nsupervised finetuning using question-context pairs. This approach leads to\nabsolute gains of 2+ points over the previous best result in the top-20\nretrieval accuracy on Natural Questions and TriviaQA datasets.\n  We also explore two approaches for end-to-end supervised training of the\nreader and retriever components in OpenQA models. In the first approach, the\nreader considers each retrieved document separately while in the second\napproach, the reader considers all the retrieved documents together. Our\nexperiments demonstrate the effectiveness of these approaches as we obtain new\nstate-of-the-art results. On the Natural Questions dataset, we obtain a top-20\nretrieval accuracy of 84, an improvement of 5 points over the recent DPR model.\nIn addition, we achieve good results on answer extraction, outperforming recent\nmodels like REALM and RAG by 3+ points. We further scale up end-to-end training\nto large models and show consistent gains in performance over smaller models.", "published": "2021-01-02 09:05:34", "link": "http://arxiv.org/abs/2101.00408v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Task-Specific Adapters from Task Description", "abstract": "Pre-trained text-to-text transformers such as BART have achieved impressive\nperformance across a range of NLP tasks. Recent study further shows that they\ncan learn to generalize to novel tasks, by including task descriptions as part\nof the source sequence and training the model with (source, target) examples.\nAt test time, these fine-tuned models can make inferences on new tasks using\nthe new task descriptions as part of the input. However, this approach has\npotential limitations, as the model learns to solve individual (source, target)\nexamples (i.e., at the instance level), instead of learning to solve tasks by\ntaking all examples within a task as a whole (i.e., at the task level). To this\nend, we introduce Hypter, a framework that improves text-to-text transformer's\ngeneralization ability to unseen tasks by training a hypernetwork to generate\ntask-specific, light-weight adapters from task descriptions. Experiments on\nZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves\nupon fine-tuning baselines. Notably, when using BART-Large as the main network,\nHypter brings 11.3% comparative improvement on ZEST dataset.", "published": "2021-01-02 10:50:23", "link": "http://arxiv.org/abs/2101.00420v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval", "abstract": "Multi-hop reasoning (i.e., reasoning across two or more documents) is a key\ningredient for NLP models that leverage large corpora to exhibit broad\nknowledge. To retrieve evidence passages, multi-hop models must contend with a\nfast-growing search space across the hops, represent complex queries that\ncombine multiple information needs, and resolve ambiguity about the best order\nin which to hop between training passages. We tackle these problems via Baleen,\na system that improves the accuracy of multi-hop retrieval while learning\nrobustly from weak training signals in the many-hop setting. To tame the search\nspace, we propose condensed retrieval, a pipeline that summarizes the retrieved\npassages after each hop into a single compact context. To model complex\nqueries, we introduce a focused late interaction retriever that allows\ndifferent parts of the same query representation to match disparate relevant\npassages. Lastly, to infer the hopping dependencies among unordered training\npassages, we devise latent hop ordering, a weak-supervision strategy in which\nthe trained retriever itself selects the sequence of hops. We evaluate Baleen\non retrieval for two-hop question answering and many-hop claim verification,\nestablishing state-of-the-art performance.", "published": "2021-01-02 11:52:20", "link": "http://arxiv.org/abs/2101.00436v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SDA: Improving Text Generation with Self Data Augmentation", "abstract": "Data augmentation has been widely used to improve deep neural networks in\nmany research fields, such as computer vision. However, less work has been done\nin the context of text, partially due to its discrete nature and the complexity\nof natural languages. In this paper, we propose to improve the standard maximum\nlikelihood estimation (MLE) paradigm by incorporating a self-imitation-learning\nphase for automatic data augmentation. Unlike most existing sentence-level\naugmentation strategies, which are only applied to specific models, our method\nis more general and could be easily adapted to any MLE-based training\nprocedure. In addition, our framework allows task-specific evaluation metrics\nto be designed to flexibly control the generated sentences, for example, in\nterms of controlling vocabulary usage and avoiding nontrivial repetitions.\nExtensive experimental results demonstrate the superiority of our method on two\nsynthetic and several standard real datasets, significantly improving related\nbaselines.", "published": "2021-01-02 01:15:57", "link": "http://arxiv.org/abs/2101.03236v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling Fine-Grained Entity Types with Box Embeddings", "abstract": "Neural entity typing models typically represent fine-grained entity types as\nvectors in a high-dimensional space, but such spaces are not well-suited to\nmodeling these types' complex interdependencies. We study the ability of box\nembeddings, which embed concepts as d-dimensional hyperrectangles, to capture\nhierarchies of types even when these relationships are not defined explicitly\nin the ontology. Our model represents both types and entity mentions as boxes.\nEach mention and its context are fed into a BERT-based model to embed that\nmention in our box space; essentially, this model leverages typological clues\npresent in the surface text to hypothesize a type representation for the\nmention. Box containment can then be used to derive both the posterior\nprobability of a mention exhibiting a given type and the conditional\nprobability relations between types themselves. We compare our approach with a\nvector-based typing model and observe state-of-the-art performance on several\nentity typing benchmarks. In addition to competitive typing performance, our\nbox-based model shows better performance in prediction consistency (predicting\na supertype and a subtype together) and confidence (i.e., calibration),\ndemonstrating that the box-based model captures the latent type hierarchies\nbetter than the vector-based model does.", "published": "2021-01-02 00:59:10", "link": "http://arxiv.org/abs/2101.00345v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What all do audio transformer models hear? Probing Acoustic\n  Representations for Language Delivery and its Structure", "abstract": "In recent times, BERT based transformer models have become an inseparable\npart of the 'tech stack' of text processing models. Similar progress is being\nobserved in the speech domain with a multitude of models observing\nstate-of-the-art results by using audio transformer models to encode speech.\nThis begs the question of what are these audio transformer models learning.\nMoreover, although the standard methodology is to choose the last layer\nembedding for any downstream task, but is it the optimal choice? We try to\nanswer these questions for the two recent audio transformer models, Mockingjay\nand wave2vec2.0. We compare them on a comprehensive set of language delivery\nand structure features including audio, fluency and pronunciation features.\nAdditionally, we probe the audio models' understanding of textual surface,\nsyntax, and semantic features and compare them to BERT. We do this over\nexhaustive settings for native, non-native, synthetic, read and spontaneous\nspeech datasets", "published": "2021-01-02 06:29:12", "link": "http://arxiv.org/abs/2101.00387v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Robust and Domain-Adaptive Approach for Low-Resource Named Entity\n  Recognition", "abstract": "Recently, it has attracted much attention to build reliable named entity\nrecognition (NER) systems using limited annotated data. Nearly all existing\nworks heavily rely on domain-specific resources, such as external lexicons and\nknowledge bases. However, such domain-specific resources are often not\navailable, meanwhile it's difficult and expensive to construct the resources,\nwhich has become a key obstacle to wider adoption. To tackle the problem, in\nthis work, we propose a novel robust and domain-adaptive approach RDANER for\nlow-resource NER, which only uses cheap and easily obtainable resources.\nExtensive experiments on three benchmark datasets demonstrate that our approach\nachieves the best performance when only using cheap and easily obtainable\nresources, and delivers competitive results against state-of-the-art methods\nwhich use difficultly obtainable domainspecific resources. All our code and\ncorpora can be found on https://github.com/houking-can/RDANER.", "published": "2021-01-02 06:47:01", "link": "http://arxiv.org/abs/2101.00388v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assessing Emoji Use in Modern Text Processing Tools", "abstract": "Emojis have become ubiquitous in digital communication, due to their visual\nappeal as well as their ability to vividly convey human emotion, among other\nfactors. The growing prominence of emojis in social media and other instant\nmessaging also leads to an increased need for systems and tools to operate on\ntext containing emojis. In this study, we assess this support by considering\ntest sets of tweets with emojis, based on which we perform a series of\nexperiments investigating the ability of prominent NLP and text processing\ntools to adequately process them. In particular, we consider tokenization,\npart-of-speech tagging, as well as sentiment analysis. Our findings show that\nmany tools still have notable shortcomings when operating on text containing\nemojis.", "published": "2021-01-02 11:38:05", "link": "http://arxiv.org/abs/2101.00430v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Modeling Disclosive Transparency in NLP Application Descriptions", "abstract": "Broader disclosive transparency$-$truth and clarity in communication\nregarding the function of AI systems$-$is widely considered desirable.\nUnfortunately, it is a nebulous concept, difficult to both define and quantify.\nThis is problematic, as previous work has demonstrated possible trade-offs and\nnegative consequences to disclosive transparency, such as a confusion effect,\nwhere \"too much information\" clouds a reader's understanding of what a system\ndescription means. Disclosive transparency's subjective nature has rendered\ndeep study into these problems and their remedies difficult. To improve this\nstate of affairs, We introduce neural language model-based probabilistic\nmetrics to directly model disclosive transparency, and demonstrate that they\ncorrelate with user and expert opinions of system transparency, making them a\nvalid objective proxy. Finally, we demonstrate the use of these metrics in a\npilot study quantifying the relationships between transparency, confusion, and\nuser perceptions in a corpus of real NLP system descriptions.", "published": "2021-01-02 11:46:17", "link": "http://arxiv.org/abs/2101.00433v4", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "VinVL: Revisiting Visual Representations in Vision-Language Models", "abstract": "This paper presents a detailed study of improving visual representations for\nvision language (VL) tasks and develops an improved object detection model to\nprovide object-centric representations of images. Compared to the most widely\nused \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new\nmodel is bigger, better-designed for VL tasks, and pre-trained on much larger\ntraining corpora that combine multiple public annotated object detection\ndatasets. Therefore, it can generate representations of a richer collection of\nvisual objects and concepts. While previous VL research focuses mainly on\nimproving the vision-language fusion model and leaves the object detection\nmodel improvement untouched, we show that visual features matter significantly\nin VL models. In our experiments we feed the visual features generated by the\nnew object detection model into a Transformer-based VL fusion model \\oscar\n\\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the\nVL model and fine-tune it on a wide range of downstream VL tasks. Our results\nshow that the new visual features significantly improve the performance across\nall VL tasks, creating new state-of-the-art results on seven public benchmarks.\nWe will release the new object detection model to public.", "published": "2021-01-02 23:35:27", "link": "http://arxiv.org/abs/2101.00529v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
