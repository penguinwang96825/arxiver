{"title": "Dependency Parsing with Bottom-up Hierarchical Pointer Networks", "abstract": "Dependency parsing is a crucial step towards deep language understanding and,\ntherefore, widely demanded by numerous Natural Language Processing\napplications. In particular, left-to-right and top-down transition-based\nalgorithms that rely on Pointer Networks are among the most accurate approaches\nfor performing dependency parsing. Additionally, it has been observed for the\ntop-down algorithm that Pointer Networks' sequential decoding can be improved\nby implementing a hierarchical variant, more adequate to model dependency\nstructures. Considering all this, we develop a bottom-up-oriented Hierarchical\nPointer Network for the left-to-right parser and propose two novel\ntransition-based alternatives: an approach that parses a sentence in\nright-to-left order and a variant that does it from the outside in. We\nempirically test the proposed neural architecture with the different algorithms\non a wide variety of languages, outperforming the original approach in\npractically all of them and setting new state-of-the-art results on the English\nand Chinese Penn Treebanks for non-contextualized and BERT-based embeddings.", "published": "2021-05-20 09:10:42", "link": "http://arxiv.org/abs/2105.09611v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Detecting Need for Empathetic Response in Motivational\n  Interviewing", "abstract": "Empathetic response from the therapist is key to the success of clinical\npsychotherapy, especially motivational interviewing. Previous work on\ncomputational modelling of empathy in motivational interviewing has focused on\noffline, session-level assessment of therapist empathy, where empathy captures\nall efforts that the therapist makes to understand the client's perspective and\nconvey that understanding to the client. In this position paper, we propose a\nnovel task of turn-level detection of client need for empathy. Concretely, we\npropose to leverage pre-trained language models and empathy-related general\nconversation corpora in a unique labeller-detector framework, where the\nlabeller automatically annotates a motivational interviewing conversation\ncorpus with empathy labels to train the detector that determines the need for\ntherapist empathy. We also lay out our strategies of extending the detector\nwith additional-input and multi-task setups to improve its detection and\nexplainability.", "published": "2021-05-20 10:28:46", "link": "http://arxiv.org/abs/2105.09649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LAST at SemEval-2021 Task 1: Improving Multi-Word Complexity Prediction\n  Using Bigram Association Measures", "abstract": "This paper describes the system developed by the Laboratoire d'analyse\nstatistique des textes (LAST) for the Lexical Complexity Prediction shared task\nat SemEval-2021. The proposed system is made up of a LightGBM model fed with\nfeatures obtained from many word frequency lists, published lexical norms and\npsychometric data. For tackling the specificity of the multi-word task, it uses\nbigram association measures. Despite that the only contextual feature used was\nsentence length, the system achieved an honorable performance in the multi-word\ntask, but poorer in the single word task. The bigram association measures were\nfound useful, but to a limited extent.", "published": "2021-05-20 10:32:57", "link": "http://arxiv.org/abs/2105.09653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KLUE: Korean Language Understanding Evaluation", "abstract": "We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE\nis a collection of 8 Korean natural language understanding (NLU) tasks,\nincluding Topic Classification, SemanticTextual Similarity, Natural Language\nInference, Named Entity Recognition, Relation Extraction, Dependency Parsing,\nMachine Reading Comprehension, and Dialogue State Tracking. We build all of the\ntasks from scratch from diverse source corpora while respecting copyrights, to\nensure accessibility for anyone without any restrictions. With ethical\nconsiderations in mind, we carefully design annotation protocols. Along with\nthe benchmark tasks and data, we provide suitable evaluation metrics and\nfine-tuning recipes for pretrained language models for each task. We\nfurthermore release the pretrained language models (PLM), KLUE-BERT and\nKLUE-RoBERTa, to help reproducing baseline models on KLUE and thereby\nfacilitate future research. We make a few interesting observations from the\npreliminary experiments using the proposed KLUE benchmark suite, already\ndemonstrating the usefulness of this new benchmark suite. First, we find\nKLUE-RoBERTa-large outperforms other baselines, including multilingual PLMs and\nexisting open-source Korean PLMs. Second, we see minimal degradation in\nperformance even when we replace personally identifiable information from the\npretraining corpus, suggesting that privacy and NLU capability are not at odds\nwith each other. Lastly, we find that using BPE tokenization in combination\nwith morpheme-level pre-tokenization is effective in tasks involving\nmorpheme-level tagging, detection and generation. In addition to accelerating\nKorean NLP research, our comprehensive documentation on creating KLUE will\nfacilitate creating similar resources for other languages in the future. KLUE\nis available at https://klue-benchmark.com.", "published": "2021-05-20 11:40:30", "link": "http://arxiv.org/abs/2105.09680v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Case Study on Pros and Cons of Regular Expression Detection and\n  Dependency Parsing for Negation Extraction from German Medical Documents.\n  Technical Report", "abstract": "We describe our work on information extraction in medical documents written\nin German, especially detecting negations using an architecture based on the\nUIMA pipeline. Based on our previous work on software modules to cover medical\nconcepts like diagnoses, examinations, etc. we employ a version of the NegEx\nregular expression algorithm with a large set of triggers as a baseline. We\nshow how a significantly smaller trigger set is sufficient to achieve similar\nresults, in order to reduce adaptation times to new text types. We elaborate on\nthe question whether dependency parsing (based on the Stanford CoreNLP model)\nis a good alternative and describe the potentials and shortcomings of both\napproaches.", "published": "2021-05-20 12:21:09", "link": "http://arxiv.org/abs/2105.09702v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Head-driven Phrase Structure Parsing in O($n^3$) Time Complexity", "abstract": "Constituent and dependency parsing, the two classic forms of syntactic\nparsing, have been found to benefit from joint training and decoding under a\nuniform formalism, Head-driven Phrase Structure Grammar (HPSG). However,\ndecoding this unified grammar has a higher time complexity ($O(n^5)$) than\ndecoding either form individually ($O(n^3)$) since more factors have to be\nconsidered during decoding. We thus propose an improved head scorer that helps\nachieve a novel performance-preserved parser in $O$($n^3$) time complexity.\nFurthermore, on the basis of this proposed practical HPSG parser, we\ninvestigated the strengths of HPSG-based parsing and explored the general\nmethod of training an HPSG-based parser from only a constituent or dependency\nannotations in a multilingual scenario. We thus present a more effective, more\nin-depth, and general work on HPSG parsing.", "published": "2021-05-20 15:33:51", "link": "http://arxiv.org/abs/2105.09835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A practical introduction to the Rational Speech Act modeling framework", "abstract": "Recent advances in computational cognitive science (i.e., simulation-based\nprobabilistic programs) have paved the way for significant progress in formal,\nimplementable models of pragmatics. Rather than describing a pragmatic\nreasoning process in prose, these models formalize and implement one, deriving\nboth qualitative and quantitative predictions of human behavior -- predictions\nthat consistently prove correct, demonstrating the viability and value of the\nframework. The current paper provides a practical introduction to and critical\nassessment of the Bayesian Rational Speech Act modeling framework, unpacking\ntheoretical foundations, exploring technological innovations, and drawing\nconnections to issues beyond current applications.", "published": "2021-05-20 16:08:04", "link": "http://arxiv.org/abs/2105.09867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-modal Sarcasm Detection and Humor Classification in Code-mixed\n  Conversations", "abstract": "Sarcasm detection and humor classification are inherently subtle problems,\nprimarily due to their dependence on the contextual and non-verbal information.\nFurthermore, existing studies in these two topics are usually constrained in\nnon-English languages such as Hindi, due to the unavailability of qualitative\nannotated datasets. In this work, we make two major contributions considering\nthe above limitations: (1) we develop a Hindi-English code-mixed dataset,\nMaSaC, for the multi-modal sarcasm detection and humor classification in\nconversational dialog, which to our knowledge is the first dataset of its kind;\n(2) we propose MSH-COMICS, a novel attention-rich neural architecture for the\nutterance classification. We learn efficient utterance representation utilizing\na hierarchical attention mechanism that attends to a small portion of the input\nsentence at a time. Further, we incorporate dialog-level contextual attention\nmechanism to leverage the dialog history for the multi-modal classification. We\nperform extensive experiments for both the tasks by varying multi-modal inputs\nand various submodules of MSH-COMICS. We also conduct comparative analysis\nagainst existing approaches. We observe that MSH-COMICS attains superior\nperformance over the existing models by > 1 F1-score point for the sarcasm\ndetection and 10 F1-score points in humor classification. We diagnose our model\nand perform thorough analysis of the results to understand the superiority and\npitfalls.", "published": "2021-05-20 18:33:55", "link": "http://arxiv.org/abs/2105.09984v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASQ: Automatically Generating Question-Answer Pairs using AMRs", "abstract": "We introduce ASQ, a tool to automatically mine questions and answers from a\nsentence using the Abstract Meaning Representation (AMR). Previous work has\nused question-answer pairs to specify the predicate-argument structure of a\nsentence using natural language, which does not require linguistic expertise or\ntraining, and created datasets such as QA-SRL and QAMR, for which the\nquestion-answer pair annotations were crowdsourced. Our goal is to build a tool\n(ASQ) that maps from the traditional meaning representation AMR to a\nquestion-answer meaning representation (QMR). This enables construction of QMR\ndatasets automatically in various domains using existing high-quality AMR\nparsers, and provides an automatic mapping AMR to QMR for ease of understanding\nby non-experts. A qualitative evaluation of the output generated by ASQ from\nthe AMR 2.0 data shows that the question-answer pairs are natural and valid,\nand demonstrate good coverage of the content. We run ASQ on the sentences from\nthe QAMR dataset, to observe that the semantic roles in QAMR are also captured\nby ASQ. We intend to make this tool and the results publicly available for\nothers to use and build upon.", "published": "2021-05-20 20:38:05", "link": "http://arxiv.org/abs/2105.10023v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MBIC -- A Media Bias Annotation Dataset Including Annotator\n  Characteristics", "abstract": "Many people consider news articles to be a reliable source of information on\ncurrent events. However, due to the range of factors influencing news agencies,\nsuch coverage may not always be impartial. Media bias, or slanted news\ncoverage, can have a substantial impact on public perception of events, and,\naccordingly, can potentially alter the beliefs and views of the public. The\nmain data gap in current research on media bias detection is a robust,\nrepresentative, and diverse dataset containing annotations of biased words and\nsentences. In particular, existing datasets do not control for the individual\nbackground of annotators, which may affect their assessment and, thus,\nrepresents critical information for contextualizing their annotations. In this\nposter, we present a matrix-based methodology to crowdsource such data using a\nself-developed annotation platform. We also present MBIC (Media Bias Including\nCharacteristics) - the first sample of 1,700 statements representing various\nmedia bias instances. The statements were reviewed by ten annotators each and\ncontain labels for media bias identification both on the word and sentence\nlevel. MBIC is the first available dataset about media bias reporting detailed\ninformation on annotator characteristics and their individual background. The\ncurrent dataset already significantly extends existing data in this domain\nproviding unique and more reliable insights into the perception of bias. In\nfuture, we will further extend it both with respect to the number of articles\nand annotators per article.", "published": "2021-05-20 15:05:17", "link": "http://arxiv.org/abs/2105.11910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning for Many-to-many Multilingual Neural Machine\n  Translation", "abstract": "Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose mRASP2, a\ntraining method to obtain a single unified multilingual translation model.\nmRASP2 is empowered by two techniques: a) a contrastive learning scheme to\nclose the gap among representations of different languages, and b) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mRASP2 outperforms\nexisting best unified model and achieves competitive or even better performance\nthan the pre-trained and fine-tuned model mBART on tens of WMT's translation\ndirections. For non-English directions, mRASP2 achieves an improvement of\naverage 10+ BLEU compared with the multilingual Transformer baseline. Code,\ndata and trained models are available at https://github.com/PANXiao1994/mRASP2.", "published": "2021-05-20 03:59:45", "link": "http://arxiv.org/abs/2105.09501v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event\n  Detection", "abstract": "Event detection (ED) aims at detecting event trigger words in sentences and\nclassifying them into specific event types. In real-world applications, ED\ntypically does not have sufficient labelled data, thus can be formulated as a\nfew-shot learning problem. To tackle the issue of low sample diversity in\nfew-shot ED, we propose a novel knowledge-based few-shot event detection method\nwhich uses a definition-based encoder to introduce external event knowledge as\nthe knowledge prior of event types. Furthermore, as external knowledge\ntypically provides limited and imperfect coverage of event types, we introduce\nan adaptive knowledge-enhanced Bayesian meta-learning method to dynamically\nadjust the knowledge prior of event types. Experiments show our method\nconsistently and substantially outperforms a number of baselines by at least 15\nabsolute F1 points under the same few-shot settings.", "published": "2021-05-20 04:26:26", "link": "http://arxiv.org/abs/2105.09509v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Manual Evaluation Matters: Reviewing Test Protocols of Distantly\n  Supervised Relation Extraction", "abstract": "Distantly supervised (DS) relation extraction (RE) has attracted much\nattention in the past few years as it can utilize large-scale auto-labeled\ndata. However, its evaluation has long been a problem: previous works either\ntook costly and inconsistent methods to manually examine a small sample of\nmodel predictions, or directly test models on auto-labeled data -- which, by\nour check, produce as much as 53% wrong labels at the entity pair level in the\npopular NYT10 dataset. This problem has not only led to inaccurate evaluation,\nbut also made it hard to understand where we are and what's left to improve in\nthe research of DS-RE. To evaluate DS-RE models in a more credible way, we\nbuild manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20,\nand thoroughly evaluate several competitive models, especially the latest\npre-trained ones. The experimental results show that the manual evaluation can\nindicate very different conclusions from automatic ones, especially some\nunexpected observations, e.g., pre-trained models can achieve dominating\nperformance while being more susceptible to false-positives compared to\nprevious methods. We hope that both our manual test sets and novel observations\ncan help advance future DS-RE research.", "published": "2021-05-20 06:55:40", "link": "http://arxiv.org/abs/2105.09543v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "See, Hear, Read: Leveraging Multimodality with Guided Attention for\n  Abstractive Text Summarization", "abstract": "In recent years, abstractive text summarization with multimodal inputs has\nstarted drawing attention due to its ability to accumulate information from\ndifferent source modalities and generate a fluent textual summary. However,\nexisting methods use short videos as the visual modality and short summary as\nthe ground-truth, therefore, perform poorly on lengthy videos and long\nground-truth summary. Additionally, there exists no benchmark dataset to\ngeneralize this task on videos of varying lengths. In this paper, we introduce\nAVIATE, the first large-scale dataset for abstractive text summarization with\nvideos of diverse duration, compiled from presentations in well-known academic\nconferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding\nresearch papers as the reference summaries, which ensure adequate quality and\nuniformity of the ground-truth. We then propose FLORAL, a factorized\nmulti-modal Transformer based decoder-only language model, which inherently\ncaptures the intra-modal and inter-modal dynamics within various input\nmodalities for the text summarization task. FLORAL utilizes an increasing\nnumber of self-attentions to capture multimodality and performs significantly\nbetter than traditional encoder-decoder based networks. Extensive experiments\nillustrate that FLORAL achieves significant improvement over the baselines in\nboth qualitative and quantitative evaluations on the existing How2 dataset for\nshort videos and newly introduced AVIATE dataset for videos with diverse\nduration, beating the best baseline on the two datasets by $1.39$ and $2.74$\nROUGE-L points respectively.", "published": "2021-05-20 08:56:33", "link": "http://arxiv.org/abs/2105.09601v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TF-IDF vs Word Embeddings for Morbidity Identification in Clinical\n  Notes: An Initial Study", "abstract": "Today, we are seeing an ever-increasing number of clinical notes that contain\nclinical results, images, and textual descriptions of patient's health state.\nAll these data can be analyzed and employed to cater novel services that can\nhelp people and domain experts with their common healthcare tasks. However,\nmany technologies such as Deep Learning and tools like Word Embeddings have\nstarted to be investigated only recently, and many challenges remain open when\nit comes to healthcare domain applications. To address these challenges, we\npropose the use of Deep Learning and Word Embeddings for identifying sixteen\nmorbidity types within textual descriptions of clinical records. For this\npurpose, we have used a Deep Learning model based on Bidirectional Long-Short\nTerm Memory (LSTM) layers which can exploit state-of-the-art vector\nrepresentations of data such as Word Embeddings. We have employed pre-trained\nWord Embeddings namely GloVe and Word2Vec, and our own Word Embeddings trained\non the target domain. Furthermore, we have compared the performances of the\ndeep learning approaches against the traditional tf-idf using Support Vector\nMachine and Multilayer perceptron (our baselines). From the obtained results it\nseems that the latter outperforms the combination of Deep Learning approaches\nusing any word embeddings. Our preliminary results indicate that there are\nspecific features that make the dataset biased in favour of traditional machine\nlearning approaches.", "published": "2021-05-20 09:57:45", "link": "http://arxiv.org/abs/2105.09632v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Target-dependent Sentiment Classification in News Articles", "abstract": "Extensive research on target-dependent sentiment classification (TSC) has led\nto strong classification performances in domains where authors tend to\nexplicitly express sentiment about specific entities or topics, such as in\nreviews or on social media. We investigate TSC in news articles, a much less\nresearched domain, despite the importance of news as an essential information\nsource in individual and societal decision making. This article introduces\nNewsTSC, a manually annotated dataset to explore TSC on news articles.\nInvestigating characteristics of sentiment in news and contrasting them to\npopular TSC domains, we find that sentiment in the news is expressed less\nexplicitly, is more dependent on context and readership, and requires a greater\ndegree of interpretation. In an extensive evaluation, we find that the state of\nthe art in TSC performs worse on news articles than on other domains (average\nrecall AvgRec = 69.8 on NewsTSC compared to AvgRev = [75.6, 82.2] on\nestablished TSC datasets). Reasons include incorrectly resolved relation of\ntarget and sentiment-bearing phrases and off-context dependence. As a major\nimprovement over previous news TSC, we find that BERT's natural language\nunderstanding capabilities capture the less explicit sentiment used in news\narticles.", "published": "2021-05-20 10:48:03", "link": "http://arxiv.org/abs/2105.09660v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Intra-Document Cascading: Learning to Select Passages for Neural\n  Document Ranking", "abstract": "An emerging recipe for achieving state-of-the-art effectiveness in neural\ndocument re-ranking involves utilizing large pre-trained language models -\ne.g., BERT - to evaluate all individual passages in the document and then\naggregating the outputs by pooling or additional Transformer layers. A major\ndrawback of this approach is high query latency due to the cost of evaluating\nevery passage in the document with BERT. To make matters worse, this high\ninference cost and latency varies based on the length of the document, with\nlonger documents requiring more time and computation. To address this\nchallenge, we adopt an intra-document cascading strategy, which prunes passages\nof a candidate document using a less expensive model, called ESM, before\nrunning a scoring model that is more expensive and effective, called ETM. We\nfound it best to train ESM (short for Efficient Student Model) via knowledge\ndistillation from the ETM (short for Effective Teacher Model) e.g., BERT. This\npruning allows us to only run the ETM model on a smaller set of passages whose\nsize does not vary by document length. Our experiments on the MS MARCO and TREC\nDeep Learning Track benchmarks suggest that the proposed Intra-Document\nCascaded Ranking Model (IDCM) leads to over 400% lower query latency by\nproviding essentially the same effectiveness as the state-of-the-art BERT-based\ndocument ranking models.", "published": "2021-05-20 15:10:13", "link": "http://arxiv.org/abs/2105.09816v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A comparative evaluation and analysis of three generations of\n  Distributional Semantic Models", "abstract": "Distributional semantics has deeply changed in the last decades. First,\npredict models stole the thunder from traditional count ones, and more recently\nboth of them were replaced in many NLP applications by contextualized vectors\nproduced by Transformer neural language models. Although an extensive body of\nresearch has been devoted to Distributional Semantic Model (DSM) evaluation, we\nstill lack a thorough comparison with respect to tested models, semantic tasks,\nand benchmark datasets. Moreover, previous work has mostly focused on\ntask-driven evaluation, instead of exploring the differences between the way\nmodels represent the lexical semantic space. In this paper, we perform a\ncomprehensive evaluation of type distributional vectors, either produced by\nstatic DSMs or obtained by averaging the contextualized vectors generated by\nBERT. First of all, we investigate the performance of embeddings in several\nsemantic tasks, carrying out an in-depth statistical analysis to identify the\nmajor factors influencing the behavior of DSMs. The results show that i.) the\nalleged superiority of predict based models is more apparent than real, and\nsurely not ubiquitous and ii.) static DSMs surpass contextualized\nrepresentations in most out-of-context semantic tasks and datasets.\nFurthermore, we borrow from cognitive neuroscience the methodology of\nRepresentational Similarity Analysis (RSA) to inspect the semantic spaces\ngenerated by distributional models. RSA reveals important differences related\nto the frequency and part-of-speech of lexical items.", "published": "2021-05-20 15:18:06", "link": "http://arxiv.org/abs/2105.09825v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VLM: Task-agnostic Video-Language Model Pre-training for Video\n  Understanding", "abstract": "We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training. Code is\nmade available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.", "published": "2021-05-20 19:13:27", "link": "http://arxiv.org/abs/2105.09996v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Scalable Modeling of Biology in Event-B", "abstract": "Biology offers many examples of large-scale, complex, concurrent systems:\nmany processes take place in parallel, compete on resources and influence each\nother's behavior. The scalable modeling of biological systems continues to be a\nvery active field of research. In this paper we introduce a new approach based\non Event-B, a state-based formal method with refinement as its central\ningredient, allowing us to check for model consistency step-by-step in an\nautomated way. Our approach based on functions leads to an elegant and concise\nmodeling method. We demonstrate this approach by constructing what is, to our\nknowledge, the largest ever built Event-B model, describing the ErbB signaling\npathway, a key evolutionary pathway with a significant role in development and\nin many types of cancer. The Event-B model for the ErbB pathway describes 1320\nmolecular reactions through 242 events.", "published": "2021-05-20 13:37:06", "link": "http://arxiv.org/abs/2105.10344v1", "categories": ["q-bio.MN", "cs.CL"], "primary_category": "q-bio.MN"}
{"title": "MLBiNet: A Cross-Sentence Collective Event Detection Network", "abstract": "We consider the problem of collectively detecting multiple events,\nparticularly in cross-sentence settings. The key to dealing with the problem is\nto encode semantic information and model event inter-dependency at a\ndocument-level. In this paper, we reformulate it as a Seq2Seq task and propose\na Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level\nassociation of events and semantic information simultaneously. Specifically, a\nbidirectional decoder is firstly devised to model event inter-dependency within\na sentence when decoding the event tag vector sequence. Secondly, an\ninformation aggregation module is employed to aggregate sentence-level semantic\nand event tag information. Finally, we stack multiple bidirectional decoders\nand feed cross-sentence information, forming a multi-layer bidirectional\ntagging architecture to iteratively propagate information across sentences. We\nshow that our approach provides significant improvement in performance compared\nto the current state-of-the-art results.", "published": "2021-05-20 02:29:03", "link": "http://arxiv.org/abs/2105.09458v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified Dual-view Cognitive Model for Interpretable Claim Verification", "abstract": "Recent studies constructing direct interactions between the claim and each\nsingle user response (a comment or a relevant article) to capture evidence have\nshown remarkable success in interpretable claim verification. Owing to\ndifferent single responses convey different cognition of individual users\n(i.e., audiences), the captured evidence belongs to the perspective of\nindividual cognition. However, individuals' cognition of social things is not\nalways able to truly reflect the objective. There may be one-sided or biased\nsemantics in their opinions on a claim. The captured evidence correspondingly\ncontains some unobjective and biased evidence fragments, deteriorating task\nperformance. In this paper, we propose a Dual-view model based on the views of\nCollective and Individual Cognition (CICD) for interpretable claim\nverification. From the view of the collective cognition, we not only capture\nthe word-level semantics based on individual users, but also focus on\nsentence-level semantics (i.e., the overall responses) among all users and\nadjust the proportion between them to generate global evidence. From the view\nof individual cognition, we select the top-$k$ articles with high degree of\ndifference and interact with the claim to explore the local key evidence\nfragments. To weaken the bias of individual cognition-view evidence, we devise\ninconsistent loss to suppress the divergence between global and local evidence\nfor strengthening the consistent shared evidence between the both. Experiments\non three benchmark datasets confirm that CICD achieves state-of-the-art\nperformance.", "published": "2021-05-20 07:44:03", "link": "http://arxiv.org/abs/2105.09567v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "The impact of virtual mirroring on customer satisfaction", "abstract": "We investigate the impact of a novel method called \"virtual mirroring\" to\npromote employee self-reflection and impact customer satisfaction. The method\nis based on measuring communication patterns, through social network and\nsemantic analysis, and mirroring them back to the individual. Our goal is to\ndemonstrate that self-reflection can trigger a change in communication\nbehaviors, which lead to increased customer satisfaction. We illustrate and\ntest our approach analyzing e-mails of a large global services company by\ncomparing changes in customer satisfaction associated with team leaders exposed\nto virtual mirroring (the experimental group). We find an increase in customer\nsatisfaction in the experimental group and a decrease in the control group\n(team leaders not involved in the virtual mirroring process). With regard to\nthe individual communication indicators, we find that customer satisfaction is\nhigher when employees are more responsive, use a simpler language, are embedded\nin less centralized communication networks, and show more stable leadership\npatterns.", "published": "2021-05-20 07:51:13", "link": "http://arxiv.org/abs/2105.09571v1", "categories": ["cs.SI", "cs.CL", "physics.soc-ph", "J.4; I.2.7"], "primary_category": "cs.SI"}
{"title": "High-Fidelity and Low-Latency Universal Neural Vocoder based on\n  Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform\n  Modeling", "abstract": "This paper presents a novel high-fidelity and low-latency universal neural\nvocoder framework based on multiband WaveRNN with data-driven linear prediction\nfor discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN\narchitecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit\nwith a relatively large size of hidden units is utilized, while the multiband\nmodeling is deployed to achieve real-time low-latency usage. A novel technique\nfor data-driven linear prediction (LP) with discrete waveform modeling is\nproposed, where the LP coefficients are estimated in a data-driven manner.\nMoreover, a novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed. The\nexperimental results demonstrate that the proposed MWDLP framework generates\nhigh-fidelity synthetic speech for seen and unseen speakers and/or language on\n300 speakers training data including clean and noisy/reverberant conditions,\nwhere the number of training utterances is limited to 60 per speaker, while\nallowing for real-time low-latency processing using a single core of $\\sim\\!$\n2.1--2.7 GHz CPU with $\\sim\\!$ 0.57--0.64 real-time factor including\ninput/output and feature extraction.", "published": "2021-05-20 16:02:45", "link": "http://arxiv.org/abs/2105.09856v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic\n  Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear\n  Prediction", "abstract": "This paper presents a low-latency real-time (LLRT) non-parallel voice\nconversion (VC) framework based on cyclic variational autoencoder (CycleVAE)\nand multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a\nrobust non-parallel multispeaker spectral model, which utilizes a\nspeaker-independent latent space and a speaker-dependent code to generate\nreconstructed/converted spectral features given the spectral features of an\ninput speaker. On the other hand, MWDLP is an efficient and a high-quality\nneural vocoder that can handle multispeaker data and generate speech waveform\nfor LLRT applications with CPU. To accommodate LLRT constraint with CPU, we\npropose a novel CycleVAE framework that utilizes mel-spectrogram as spectral\nfeatures and is built with a sparse network architecture. Further, to improve\nthe modeling performance, we also propose a novel fine-tuning procedure that\nrefines the frame-rate CycleVAE network by utilizing the waveform loss from the\nMWDLP network. The experimental results demonstrate that the proposed framework\nachieves high-performance VC, while allowing for LLRT usage with a single-core\nof $2.1$--$2.7$ GHz CPU on a real-time factor of $0.87$--$0.95$, including\ninput/output, feature extraction, on a frame shift of $10$ ms, a window length\nof $27.5$ ms, and $2$ lookup frames.", "published": "2021-05-20 16:06:11", "link": "http://arxiv.org/abs/2105.09858v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mondegreen: A Post-Processing Solution to Speech Recognition Error\n  Correction for Voice Search Queries", "abstract": "As more and more online search queries come from voice, automatic speech\nrecognition becomes a key component to deliver relevant search results. Errors\nintroduced by automatic speech recognition (ASR) lead to irrelevant search\nresults returned to the user, thus causing user dissatisfaction. In this paper,\nwe introduce an approach, Mondegreen, to correct voice queries in text space\nwithout depending on audio signals, which may not always be available due to\nsystem constraints or privacy or bandwidth (for example, some ASR systems run\non-device) considerations. We focus on voice queries transcribed via several\nproprietary commercial ASR systems. These queries come from users making\ninternet, or online service search queries. We first present an analysis\nshowing how different the language distribution coming from user voice queries\nis from that in traditional text corpora used to train off-the-shelf ASR\nsystems. We then demonstrate that Mondegreen can achieve significant\nimprovements in increased user interaction by correcting user voice queries in\none of the largest search systems in Google. Finally, we see Mondegreen as\ncomplementing existing highly-optimized production ASR systems, which may not\nbe frequently retrained and thus lag behind due to vocabulary drifts.", "published": "2021-05-20 17:45:46", "link": "http://arxiv.org/abs/2105.09930v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Measuring Coding Challenge Competence With APPS", "abstract": "While programming is one of the most broadly applicable skills in modern\nsociety, modern machine learning models still cannot code solutions to basic\nproblems. Despite its importance, there has been surprisingly little work on\nevaluating code generation, and it can be difficult to accurately assess code\ngeneration performance rigorously. To meet this challenge, we introduce APPS, a\nbenchmark for code generation. Unlike prior work in more restricted settings,\nour benchmark measures the ability of models to take an arbitrary natural\nlanguage specification and generate satisfactory Python code. Similar to how\ncompanies assess candidate software developers, we then evaluate models by\nchecking their generated code on test cases. Our benchmark includes 10,000\nproblems, which range from having simple one-line solutions to being\nsubstantial algorithmic challenges. We fine-tune large language models on both\nGitHub and our training set, and we find that the prevalence of syntax errors\nis decreasing exponentially as models improve. Recent models such as GPT-Neo\ncan pass approximately 20% of the test cases of introductory problems, so we\nfind that machine learning models are now beginning to learn how to code. As\nthe social significance of automatic code generation increases over the coming\nyears, our benchmark can provide an important measure for tracking\nadvancements.", "published": "2021-05-20 17:58:42", "link": "http://arxiv.org/abs/2105.09938v3", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on\n  Twitter", "abstract": "Datasets with induced emotion labels are scarce but of utmost importance for\nmany NLP tasks. We present a new, automated method for collecting texts along\nwith their induced reaction labels. The method exploits the online use of\nreaction GIFs, which capture complex affective states. We show how to augment\nthe data with induced emotion and induced sentiment labels. We use our method\nto create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K\ntweets. We provide baselines for three new tasks, including induced sentiment\nprediction and multilabel classification of induced emotions. Our method and\ndataset open new research opportunities in emotion detection and affective\ncomputing.", "published": "2021-05-20 18:01:05", "link": "http://arxiv.org/abs/2105.09967v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Generation and Evaluation of Visual Stories via Semantic\n  Consistency", "abstract": "Story visualization is an under-explored task that falls at the intersection\nof many important research directions in both computer vision and natural\nlanguage processing. In this task, given a series of natural language captions\nwhich compose a story, an agent must generate a sequence of images that\ncorrespond to the captions. Prior work has introduced recurrent generative\nmodels which outperform text-to-image synthesis models on this task. However,\nthere is room for improvement of generated images in terms of visual quality,\ncoherence and relevance. We present a number of improvements to prior modeling\napproaches, including (1) the addition of a dual learning framework that\nutilizes video captioning to reinforce the semantic alignment between the story\nand generated images, (2) a copy-transform mechanism for\nsequentially-consistent story visualization, and (3) MART-based transformers to\nmodel complex interactions between frames. We present ablation studies to\ndemonstrate the effect of each of these techniques on the generative power of\nthe model for both individual images as well as the entire narrative.\nFurthermore, due to the complexity and generative nature of the task, standard\nevaluation metrics do not accurately reflect performance. Therefore, we also\nprovide an exploration of evaluation metrics for the model, focused on aspects\nof the generated frames such as the presence/quality of generated characters,\nthe relevance to captions, and the diversity of the generated images. We also\npresent correlation experiments of our proposed automated metrics with human\nevaluations. Code and data available at:\nhttps://github.com/adymaharana/StoryViz", "published": "2021-05-20 20:42:42", "link": "http://arxiv.org/abs/2105.10026v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Streaming End-to-End Framework For Spoken Language Understanding", "abstract": "End-to-end spoken language understanding (SLU) has recently attracted\nincreasing interest. Compared to the conventional tandem-based approach that\ncombines speech recognition and language understanding as separate modules, the\nnew approach extracts users' intentions directly from the speech signals,\nresulting in joint optimization and low latency. Such an approach, however, is\ntypically designed to process one intention at a time, which leads users to\ntake multiple rounds to fulfill their requirements while interacting with a\ndialogue system. In this paper, we propose a streaming end-to-end framework\nthat can process multiple intentions in an online and incremental way. The\nbackbone of our framework is a unidirectional RNN trained with the\nconnectionist temporal classification (CTC) criterion. By this design, an\nintention can be identified when sufficient evidence has been accumulated, and\nmultiple intentions can be identified sequentially. We evaluate our solution on\nthe Fluent Speech Commands (FSC) dataset and the intent detection accuracy is\nabout 97 % on all multi-intent settings. This result is comparable to the\nperformance of the state-of-the-art non-streaming models, but is achieved in an\nonline and incremental way. We also employ our model to a keyword spotting task\nusing the Google Speech Commands dataset and the results are also highly\npromising.", "published": "2021-05-20 21:37:05", "link": "http://arxiv.org/abs/2105.10042v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speaker disentanglement in video-to-speech conversion", "abstract": "The task of video-to-speech aims to translate silent video of lip movement to\nits corresponding audio signal. Previous approaches to this task are generally\nlimited to the case of a single speaker, but a method that accounts for\nmultiple speakers is desirable as it allows to i) leverage datasets with\nmultiple speakers or few samples per speaker; and ii) control speaker identity\nat inference time. In this paper, we introduce a new video-to-speech\narchitecture and explore ways of extending it to the multi-speaker scenario: we\naugment the network with an additional speaker-related input, through which we\nfeed either a discrete identity or a speaker embedding. Interestingly, we\nobserve that the visual encoder of the network is capable of learning the\nspeaker identity from the lip region of the face alone. To better disentangle\nthe two inputs -- linguistic content and speaker identity -- we add adversarial\nlosses that dispel the identity from the video embeddings. To the best of our\nknowledge, the proposed method is the first to provide important\nfunctionalities such as i) control of the target voice and ii) speech synthesis\nfor unseen identities over the state-of-the-art, while still maintaining the\nintelligibility of the spoken output.", "published": "2021-05-20 10:31:53", "link": "http://arxiv.org/abs/2105.09652v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
