{"title": "More than Classification: A Unified Framework for Event Temporal\n  Relation Extraction", "abstract": "Event temporal relation extraction~(ETRE) is usually formulated as a\nmulti-label classification task, where each type of relation is simply treated\nas a one-hot label. This formulation ignores the meaning of relations and wipes\nout their intrinsic dependency. After examining the relation definitions in\nvarious ETRE tasks, we observe that all relations can be interpreted using the\nstart and end time points of events. For example, relation \\textit{Includes}\ncould be interpreted as event 1 starting no later than event 2 and ending no\nearlier than event 2. In this paper, we propose a unified event temporal\nrelation extraction framework, which transforms temporal relations into logical\nexpressions of time points and completes the ETRE by predicting the relations\nbetween certain time point pairs. Experiments on TB-Dense and MATRES show\nsignificant improvements over a strong baseline and outperform the\nstate-of-the-art model by 0.3\\% on both datasets. By representing all relations\nin a unified framework, we can leverage the relations with sufficient data to\nassist the learning of other relations, thus achieving stable improvement in\nlow-data scenarios. When the relation definitions are changed, our method can\nquickly adapt to the new ones by simply modifying the logic expressions that\nmap time points to new event relations. The code is released at\n\\url{https://github.com/AndrewZhe/A-Unified-Framework-for-ETRE}.", "published": "2023-05-28 02:09:08", "link": "http://arxiv.org/abs/2305.17607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-Guided Retrieval Augmentation for Non-Knowledge-Intensive Tasks", "abstract": "Retrieval-augmented methods have received increasing attention to support\ndownstream tasks by leveraging useful information from external resources.\nRecent studies mainly focus on exploring retrieval to solve knowledge-intensive\n(KI) tasks. However, the potential of retrieval for most\nnon-knowledge-intensive (NKI) tasks remains under-explored. There are two main\nchallenges to leveraging retrieval-augmented methods for NKI tasks: 1) the\ndemand for diverse relevance score functions and 2) the dilemma between\ntraining cost and task performance. To address these challenges, we propose a\ntwo-stage framework for NKI tasks, named PGRA. In the first stage, we adopt a\ntask-agnostic retriever to build a shared static index and select candidate\nevidence efficiently. In the second stage, we design a prompt-guided reranker\nto rerank the nearest evidence according to task-specific relevance for the\nreader. Experimental results show that PGRA outperforms other state-of-the-art\nretrieval-augmented methods. Our analyses further investigate the influence\nfactors to model performance and demonstrate the generality of PGRA. Codes are\navailable at https://github.com/THUNLP-MT/PGRA.", "published": "2023-05-28 07:27:12", "link": "http://arxiv.org/abs/2305.17653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plug-and-Play Document Modules for Pre-trained Models", "abstract": "Large-scale pre-trained models (PTMs) have been widely used in\ndocument-oriented NLP tasks, such as question answering. However, the\nencoding-task coupling requirement results in the repeated encoding of the same\ndocuments for different tasks and queries, which is highly computationally\ninefficient. To this end, we target to decouple document encoding from\ndownstream tasks, and propose to represent each document as a plug-and-play\ndocument module, i.e., a document plugin, for PTMs (PlugD). By inserting\ndocument plugins into the backbone PTM for downstream tasks, we can encode a\ndocument one time to handle multiple tasks, which is more efficient than\nconventional encoding-task coupling methods that simultaneously encode\ndocuments and input queries using task-specific encoders. Extensive experiments\non 8 datasets of 4 typical NLP tasks show that PlugD enables models to encode\ndocuments once and for all across different scenarios. Especially, PlugD can\nsave $69\\%$ computational costs while achieving comparable performance to\nstate-of-the-art encoding-task coupling methods. Additionally, we show that\nPlugD can serve as an effective post-processing way to inject knowledge into\ntask-specific models, improving model performance without any additional model\ntraining.", "published": "2023-05-28 08:01:40", "link": "http://arxiv.org/abs/2305.17660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Retrieval Hypothesis in Multimodal Context", "abstract": "Multimodal corpora have become an essential language resource for language\nscience and grounded natural language processing (NLP) systems due to the\ngrowing need to understand and interpret human communication across various\nchannels. In this paper, we first present our efforts in building the first\nMultimodal Corpus for Languages in Taiwan (MultiMoco). Based on the corpus, we\nconduct a case study investigating the Lexical Retrieval Hypothesis (LRH),\nspecifically examining whether the hand gestures co-occurring with speech\nconstants facilitate lexical retrieval or serve other discourse functions. With\ndetailed annotations on eight parliamentary interpellations in Taiwan Mandarin,\nwe explore the co-occurrence between speech constants and non-verbal features\n(i.e., head movement, face movement, hand gesture, and function of hand\ngesture). Our findings suggest that while hand gestures do serve as\nfacilitators for lexical retrieval in some cases, they also serve the purpose\nof information emphasis. This study highlights the potential of the MultiMoco\nCorpus to provide an important resource for in-depth analysis and further\nresearch in multimodal communication studies.", "published": "2023-05-28 08:17:07", "link": "http://arxiv.org/abs/2305.17663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stochastic Bridges as Effective Regularizers for Parameter-Efficient\n  Tuning", "abstract": "Parameter-efficient tuning methods (PETs) have achieved promising results in\ntuning large pre-trained language models (PLMs). By formalizing frozen PLMs and\nadditional tunable parameters as systems and controls respectively, PETs can be\ntheoretically grounded to optimal control and further viewed as optimizing the\nterminal cost and running cost in the optimal control literature. Despite the\nelegance of this theoretical grounding, in practice, existing PETs often ignore\nthe running cost and only optimize the terminal cost, i.e., focus on optimizing\nthe loss function of the output state, regardless of the running cost that\ndepends on the intermediate states. Since it is non-trivial to directly model\nthe intermediate states and design a running cost function, we propose to use\nlatent stochastic bridges to regularize the intermediate states and use the\nregularization as the running cost of PETs. As the first work to propose\nregularized PETs that use stochastic bridges as the regularizers (running\ncosts) for the intermediate states, we show the effectiveness and generality of\nthis regularization across different tasks, PLMs and PETs. In view of the great\npotential and capacity, we believe more sophisticated regularizers can be\ndesigned for PETs and better performance can be achieved in the future. The\ncode is released at\n\\url{https://github.com/thunlp/stochastic-bridge-pet/tree/main}.", "published": "2023-05-28 09:22:44", "link": "http://arxiv.org/abs/2305.17670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RuSentNE-2023: Evaluating Entity-Oriented Sentiment Analysis on Russian\n  News Texts", "abstract": "The paper describes the RuSentNE-2023 evaluation devoted to targeted\nsentiment analysis in Russian news texts. The task is to predict sentiment\ntowards a named entity in a single sentence. The dataset for RuSentNE-2023\nevaluation is based on the Russian news corpus RuSentNE having rich\nsentiment-related annotation. The corpus is annotated with named entities and\nsentiments towards these entities, along with related effects and emotional\nstates. The evaluation was organized using the CodaLab competition framework.\nThe main evaluation measure was macro-averaged measure of positive and negative\nclasses. The best results achieved were of 66% Macro F-measure\n(Positive+Negative classes). We also tested ChatGPT on the test set from our\nevaluation and found that the zero-shot answers provided by ChatGPT reached 60%\nof the F-measure, which corresponds to 4th place in the evaluation. ChatGPT\nalso provided detailed explanations of its conclusion. This can be considered\nas quite high for zero-shot application.", "published": "2023-05-28 10:04:15", "link": "http://arxiv.org/abs/2305.17679v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "HaVQA: A Dataset for Visual Question Answering and Multimodal Research\n  in Hausa Language", "abstract": "This paper presents HaVQA, the first multimodal dataset for visual\nquestion-answering (VQA) tasks in the Hausa language. The dataset was created\nby manually translating 6,022 English question-answer pairs, which are\nassociated with 1,555 unique images from the Visual Genome dataset. As a\nresult, the dataset provides 12,044 gold standard English-Hausa parallel\nsentences that were translated in a fashion that guarantees their semantic\nmatch with the corresponding visual information. We conducted several baseline\nexperiments on the dataset, including visual question answering, visual\nquestion elicitation, text-only and multimodal machine translation.", "published": "2023-05-28 10:55:31", "link": "http://arxiv.org/abs/2305.17690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plug-and-Play Knowledge Injection for Pre-trained Language Models", "abstract": "Injecting external knowledge can improve the performance of pre-trained\nlanguage models (PLMs) on various downstream NLP tasks. However, massive\nretraining is required to deploy new knowledge injection methods or knowledge\nbases for downstream tasks. In this work, we are the first to study how to\nimprove the flexibility and efficiency of knowledge injection by reusing\nexisting downstream models. To this end, we explore a new paradigm\nplug-and-play knowledge injection, where knowledge bases are injected into\nfrozen existing downstream models by a knowledge plugin. Correspondingly, we\npropose a plug-and-play injection method map-tuning, which trains a mapping of\nknowledge embeddings to enrich model inputs with mapped embeddings while\nkeeping model parameters frozen. Experimental results on three knowledge-driven\nNLP tasks show that existing injection methods are not suitable for the new\nparadigm, while map-tuning effectively improves the performance of downstream\nmodels. Moreover, we show that a frozen downstream model can be well adapted to\ndifferent domains with different mapping networks of domain knowledge. Our code\nand models are available at https://github.com/THUNLP/Knowledge-Plugin.", "published": "2023-05-28 10:58:00", "link": "http://arxiv.org/abs/2305.17691v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable\n  Responses Created Through Human-Machine Collaboration", "abstract": "The potential social harms that large language models pose, such as\ngenerating offensive content and reinforcing biases, are steeply rising.\nExisting works focus on coping with this concern while interacting with\nill-intentioned users, such as those who explicitly make hate speech or elicit\nharmful responses. However, discussions on sensitive issues can become toxic\neven if the users are well-intentioned. For safer models in such scenarios, we\npresent the Sensitive Questions and Acceptable Response (SQuARe) dataset, a\nlarge-scale Korean dataset of 49k sensitive questions with 42k acceptable and\n46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA\nin a human-in-the-loop manner based on real news headlines. Experiments show\nthat acceptable response generation significantly improves for HyperCLOVA and\nGPT-3, demonstrating the efficacy of this dataset.", "published": "2023-05-28 11:51:20", "link": "http://arxiv.org/abs/2305.17696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Dynamic Graph Convolutional Decoder", "abstract": "Existing wisdom demonstrates the significance of syntactic knowledge for the\nimprovement of neural machine translation models. However, most previous works\nmerely focus on leveraging the source syntax in the well-known encoder-decoder\nframework. In sharp contrast, this paper proposes an end-to-end translation\narchitecture from the (graph \\& sequence) structural inputs to the (graph \\&\nsequence) outputs, where the target translation and its corresponding syntactic\ngraph are jointly modeled and generated. We propose a customized Dynamic\nSpatial-Temporal Graph Convolutional Decoder (Dyn-STGCD), which is designed for\nconsuming source feature representations and their syntactic graph, and\nauto-regressively generating the target syntactic graph and tokens\nsimultaneously. We conduct extensive experiments on five widely acknowledged\ntranslation benchmarks, verifying that our proposal achieves consistent\nimprovements over baselines and other syntax-aware variants.", "published": "2023-05-28 11:58:07", "link": "http://arxiv.org/abs/2305.17698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoupling Pseudo Label Disambiguation and Representation Learning for\n  Generalized Intent Discovery", "abstract": "Generalized intent discovery aims to extend a closed-set in-domain intent\nclassifier to an open-world intent set including in-domain and out-of-domain\nintents. The key challenges lie in pseudo label disambiguation and\nrepresentation learning. Previous methods suffer from a coupling of pseudo\nlabel disambiguation and representation learning, that is, the reliability of\npseudo labels relies on representation learning, and representation learning is\nrestricted by pseudo labels in turn. In this paper, we propose a decoupled\nprototype learning framework (DPL) to decouple pseudo label disambiguation and\nrepresentation learning. Specifically, we firstly introduce prototypical\ncontrastive representation learning (PCL) to get discriminative\nrepresentations. And then we adopt a prototype-based label disambiguation\nmethod (PLD) to obtain pseudo labels. We theoretically prove that PCL and PLD\nwork in a collaborative fashion and facilitate pseudo label disambiguation.\nExperiments and analysis on three benchmark datasets show the effectiveness of\nour method.", "published": "2023-05-28 12:01:34", "link": "http://arxiv.org/abs/2305.17699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large\n  Language Model Application", "abstract": "Large language models (LLMs) learn not only natural text generation abilities\nbut also social biases against different demographic groups from real-world\ndata. This poses a critical risk when deploying LLM-based applications.\nExisting research and resources are not readily applicable in South Korea due\nto the differences in language and culture, both of which significantly affect\nthe biases and targeted demographic groups. This limitation requires localized\nsocial bias datasets to ensure the safe and effective deployment of LLMs. To\nthis end, we present KO SB I, a new social bias dataset of 34k pairs of\ncontexts and sentences in Korean covering 72 demographic groups in 15\ncategories. We find that through filtering-based moderation, social biases in\ngenerated content can be reduced by 16.47%p on average for HyperCLOVA (30B and\n82B), and GPT-3.", "published": "2023-05-28 12:07:16", "link": "http://arxiv.org/abs/2305.17701v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parallel Data Helps Neural Entity Coreference Resolution", "abstract": "Coreference resolution is the task of finding expressions that refer to the\nsame entity in a text. Coreference models are generally trained on monolingual\nannotated data but annotating coreference is expensive and challenging.\nHardmeier et al.(2013) have shown that parallel data contains latent anaphoric\nknowledge, but it has not been explored in end-to-end neural models yet. In\nthis paper, we propose a simple yet effective model to exploit coreference\nknowledge from parallel data. In addition to the conventional modules learning\ncoreference from annotations, we introduce an unsupervised module to capture\ncross-lingual coreference knowledge. Our proposed cross-lingual model achieves\nconsistent improvements, up to 1.74 percentage points, on the OntoNotes 5.0\nEnglish dataset using 9 different synthetic parallel datasets. These\nexperimental results confirm that parallel data can provide additional\ncoreference knowledge which is beneficial to coreference resolution tasks.", "published": "2023-05-28 12:30:23", "link": "http://arxiv.org/abs/2305.17709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Masked Language Modeling for Chinese Spelling Correction", "abstract": "In this paper, we study Chinese Spelling Correction (CSC) as a joint decision\nmade by two separate models: a language model and an error model. Through\nempirical analysis, we find that fine-tuning BERT tends to over-fit the error\nmodel while under-fit the language model, resulting in poor generalization to\nout-of-distribution error patterns. Given that BERT is the backbone of most CSC\nmodels, this phenomenon has a significant negative impact. To address this\nissue, we are releasing a multi-domain benchmark LEMON, with higher quality and\ndiversity than existing benchmarks, to allow a comprehensive assessment of the\nopen domain generalization of CSC models. Then, we demonstrate that a very\nsimple strategy, randomly masking 20\\% non-error tokens from the input sequence\nduring fine-tuning is sufficient for learning a much better language model\nwithout sacrificing the error model. This technique can be applied to any model\narchitecture and achieves new state-of-the-art results on SIGHAN, ECSpell, and\nLEMON.", "published": "2023-05-28 13:19:12", "link": "http://arxiv.org/abs/2305.17721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning a Structural Causal Model for Intuition Reasoning in\n  Conversation", "abstract": "Reasoning, a crucial aspect of NLP research, has not been adequately\naddressed by prevailing models including Large Language Model. Conversation\nreasoning, as a critical component of it, remains largely unexplored due to the\nabsence of a well-designed cognitive model. In this paper, inspired by\nintuition theory on conversation cognition, we develop a conversation cognitive\nmodel (CCM) that explains how each utterance receives and activates channels of\ninformation recursively. Besides, we algebraically transformed CCM into a\nstructural causal model (SCM) under some mild assumptions, rendering it\ncompatible with various causal discovery methods. We further propose a\nprobabilistic implementation of the SCM for utterance-level relation reasoning.\nBy leveraging variational inference, it explores substitutes for implicit\ncauses, addresses the issue of their unobservability, and reconstructs the\ncausal representations of utterances through the evidence lower bounds.\nMoreover, we constructed synthetic and simulated datasets incorporating\nimplicit causes and complete cause labels, alleviating the current situation\nwhere all available datasets are implicit-causes-agnostic. Extensive\nexperiments demonstrate that our proposed method significantly outperforms\nexisting methods on synthetic, simulated, and real-world datasets. Finally, we\nanalyze the performance of CCM under latent confounders and propose theoretical\nideas for addressing this currently unresolved issue.", "published": "2023-05-28 13:54:09", "link": "http://arxiv.org/abs/2305.17727v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tri-level Joint Natural Language Understanding for Multi-turn\n  Conversational Datasets", "abstract": "Natural language understanding typically maps single utterances to a dual\nlevel semantic frame, sentence level intent and slot labels at the word level.\nThe best performing models force explicit interaction between intent detection\nand slot filling. We present a novel tri-level joint natural language\nunderstanding approach, adding domain, and explicitly exchange semantic\ninformation between all levels. This approach enables the use of multi-turn\ndatasets which are a more natural conversational environment than single\nutterance. We evaluate our model on two multi-turn datasets for which we are\nthe first to conduct joint slot-filling and intent detection. Our model\noutperforms state-of-the-art joint models in slot filling and intent detection\non multi-turn data sets. We provide an analysis of explicit interaction\nlocations between the layers. We conclude that including domain information\nimproves model performance.", "published": "2023-05-28 13:59:58", "link": "http://arxiv.org/abs/2305.17729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Whitening-based Contrastive Learning of Sentence Embeddings", "abstract": "This paper presents a whitening-based contrastive learning method for\nsentence embedding learning (WhitenedCSE), which combines contrastive learning\nwith a novel shuffled group whitening. Generally, contrastive learning pulls\ndistortions of a single sample (i.e., positive samples) close and push negative\nsamples far away, correspondingly facilitating the alignment and uniformity in\nthe feature space. A popular alternative to the \"pushing'' operation is\nwhitening the feature space, which scatters all the samples for uniformity.\nSince the whitening and the contrastive learning have large redundancy w.r.t.\nthe uniformity, they are usually used separately and do not easily work\ntogether. For the first time, this paper integrates whitening into the\ncontrastive learning scheme and facilitates two benefits. 1) Better uniformity.\nWe find that these two approaches are not totally redundant but actually have\nsome complementarity due to different uniformity mechanism. 2) Better\nalignment. We randomly divide the feature into multiple groups along the\nchannel axis and perform whitening independently within each group. By\nshuffling the group division, we derive multiple distortions of a single sample\nand thus increase the positive sample diversity. Consequently, using multiple\npositive samples with enhanced diversity further improves contrastive learning\ndue to better alignment. Extensive experiments on seven semantic textual\nsimilarity tasks show our method achieves consistent improvement over the\ncontrastive learning baseline and sets new states of the art, e.g., 78.78\\%\n(+2.53\\% based on BERT\\ba) Spearman correlation on STS tasks.", "published": "2023-05-28 14:58:10", "link": "http://arxiv.org/abs/2305.17746v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reliable and Interpretable Drift Detection in Streams of Short Texts", "abstract": "Data drift is the change in model input data that is one of the key factors\nleading to machine learning models performance degradation over time.\nMonitoring drift helps detecting these issues and preventing their harmful\nconsequences. Meaningful drift interpretation is a fundamental step towards\neffective re-training of the model. In this study we propose an end-to-end\nframework for reliable model-agnostic change-point detection and interpretation\nin large task-oriented dialog systems, proven effective in multiple customer\ndeployments. We evaluate our approach and demonstrate its benefits with a novel\nvariant of intent classification training dataset, simulating customer requests\nto a dialog system. We make the data publicly available.", "published": "2023-05-28 15:14:54", "link": "http://arxiv.org/abs/2305.17750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating EDU Extracts for Plan-Guided Summary Re-Ranking", "abstract": "Two-step approaches, in which summary candidates are generated-then-reranked\nto return a single summary, can improve ROUGE scores over the standard\nsingle-step approach. Yet, standard decoding methods (i.e., beam search,\nnucleus sampling, and diverse beam search) produce candidates with redundant,\nand often low quality, content. In this paper, we design a novel method to\ngenerate candidates for re-ranking that addresses these issues. We ground each\ncandidate abstract on its own unique content plan and generate distinct\nplan-guided abstracts using a model's top beam. More concretely, a standard\nlanguage model (a BART LM) auto-regressively generates elemental discourse unit\n(EDU) content plans with an extractive copy mechanism. The top K beams from the\ncontent plan generator are then used to guide a separate LM, which produces a\nsingle abstractive candidate for each distinct plan. We apply an existing\nre-ranker (BRIO) to abstractive candidates generated from our method, as well\nas baseline decoding methods. We show large relevance improvements over\npreviously published methods on widely used single document news article\ncorpora, with ROUGE-2 F1 gains of 0.88, 2.01, and 0.38 on CNN / Dailymail, NYT,\nand Xsum, respectively. A human evaluation on CNN / DM validates these results.\nSimilarly, on 1k samples from CNN / DM, we show that prompting GPT-3 to follow\nEDU plans outperforms sampling-based methods by 1.05 ROUGE-2 F1 points. Code to\ngenerate and realize plans is available at\nhttps://github.com/griff4692/edu-sum.", "published": "2023-05-28 17:22:04", "link": "http://arxiv.org/abs/2305.17779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Targeted Data Generation: Finding and Fixing Model Weaknesses", "abstract": "Even when aggregate accuracy is high, state-of-the-art NLP models often fail\nsystematically on specific subgroups of data, resulting in unfair outcomes and\neroding user trust. Additional data collection may not help in addressing these\nweaknesses, as such challenging subgroups may be unknown to users, and\nunderrepresented in the existing and new data. We propose Targeted Data\nGeneration (TDG), a framework that automatically identifies challenging\nsubgroups, and generates new data for those subgroups using large language\nmodels (LLMs) with a human in the loop. TDG estimates the expected benefit and\npotential harm of data augmentation for each subgroup, and selects the ones\nmost likely to improve within group performance without hurting overall\nperformance. In our experiments, TDG significantly improves the accuracy on\nchallenging subgroups for state-of-the-art sentiment analysis and natural\nlanguage inference models, while also improving overall test accuracy.", "published": "2023-05-28 19:36:50", "link": "http://arxiv.org/abs/2305.17804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tab-CoT: Zero-shot Tabular Chain of Thought", "abstract": "The chain-of-though (CoT) prompting methods were successful in various\nnatural language processing (NLP) tasks thanks to their ability to unveil the\nunderlying complex reasoning processes. Such reasoning processes typically\nexhibit implicitly structured steps. Recent efforts also started investigating\nmethods to encourage more explicitly structured reasoning procedures to be\ncaptured. In this work, we propose Tab-CoT, a novel tabular-format CoT\nprompting method, which allows the complex reasoning process to be explicitly\nmodelled in a highly structured manner. Despite its simplicity, we show that\nour approach is capable of performing reasoning across multiple dimensions\n(i.e., both rows and columns). We demonstrate our approach's strong zero-shot\nand few-shot capabilities through extensive experiments on a range of reasoning\ntasks.", "published": "2023-05-28 20:49:52", "link": "http://arxiv.org/abs/2305.17812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConaCLIP: Exploring Distillation of Fully-Connected Knowledge\n  Interaction Graph for Lightweight Text-Image Retrieval", "abstract": "Large-scale pre-trained text-image models with dual-encoder architectures\n(such as CLIP) are typically adopted for various vision-language applications,\nincluding text-image retrieval. However,these models are still less practical\non edge devices or for real-time situations, due to the substantial indexing\nand inference time and the large consumption of computational resources.\nAlthough knowledge distillation techniques have been widely utilized for\nuni-modal model compression, how to expand them to the situation when the\nnumbers of modalities and teachers/students are doubled has been rarely\nstudied. In this paper, we conduct comprehensive experiments on this topic and\npropose the fully-Connected knowledge interaction graph (Cona) technique for\ncross-modal pre-training distillation. Based on our findings, the resulting\nConaCLIP achieves SOTA performances on the widely-used Flickr30K and MSCOCO\nbenchmarks under the lightweight setting. An industry application of our method\non an e-commercial platform further demonstrates the significant effectiveness\nof ConaCLIP.", "published": "2023-05-28 07:16:44", "link": "http://arxiv.org/abs/2305.17652v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Evaluating GPT-3 Generated Explanations for Hateful Content Moderation", "abstract": "Recent research has focused on using large language models (LLMs) to generate\nexplanations for hate speech through fine-tuning or prompting. Despite the\ngrowing interest in this area, these generated explanations' effectiveness and\npotential limitations remain poorly understood. A key concern is that these\nexplanations, generated by LLMs, may lead to erroneous judgments about the\nnature of flagged content by both users and content moderators. For instance,\nan LLM-generated explanation might inaccurately convince a content moderator\nthat a benign piece of content is hateful. In light of this, we propose an\nanalytical framework for examining hate speech explanations and conducted an\nextensive survey on evaluating such explanations. Specifically, we prompted\nGPT-3 to generate explanations for both hateful and non-hateful content, and a\nsurvey was conducted with 2,400 unique respondents to evaluate the generated\nexplanations. Our findings reveal that (1) human evaluators rated the\nGPT-generated explanations as high quality in terms of linguistic fluency,\ninformativeness, persuasiveness, and logical soundness, (2) the persuasive\nnature of these explanations, however, varied depending on the prompting\nstrategy employed, and (3) this persuasiveness may result in incorrect\njudgments about the hatefulness of the content. Our study underscores the need\nfor caution in applying LLM-generated explanations for content moderation. Code\nand results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.", "published": "2023-05-28 10:05:13", "link": "http://arxiv.org/abs/2305.17680v4", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "One Network, Many Masks: Towards More Parameter-Efficient Transfer\n  Learning", "abstract": "Fine-tuning pre-trained language models for multiple tasks tends to be\nexpensive in terms of storage. To mitigate this, parameter-efficient transfer\nlearning (PETL) methods have been proposed to address this issue, but they\nstill require a significant number of parameters and storage when being applied\nto broader ranges of tasks. To achieve even greater storage reduction, we\npropose PROPETL, a novel method that enables efficient sharing of a single PETL\nmodule which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)\nacross layers and tasks. We then learn binary masks to select different\nsub-networks from the shared prototype network and apply them as PETL modules\ninto different layers. We find that the binary masks can determine crucial\ninformation from the network, which is often ignored in previous studies. Our\nwork can also be seen as a type of pruning method, where we find that\noverparameterization also exists in the seemingly small PETL modules. We\nevaluate PROPETL on various downstream tasks and show that it can outperform\nother PETL methods with approximately 10% of the parameter storage required by\nthe latter.", "published": "2023-05-28 10:27:14", "link": "http://arxiv.org/abs/2305.17682v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Open-Source Gloss-Based Baseline for Spoken to Signed Language\n  Translation", "abstract": "Sign language translation systems are complex and require many components. As\na result, it is very hard to compare methods across publications. We present an\nopen-source implementation of a text-to-gloss-to-pose-to-video pipeline\napproach, demonstrating conversion from German to Swiss German Sign Language,\nFrench to French Sign Language of Switzerland, and Italian to Italian Sign\nLanguage of Switzerland. We propose three different components for the\ntext-to-gloss translation: a lemmatizer, a rule-based word reordering and\ndropping component, and a neural machine translation system. Gloss-to-pose\nconversion occurs using data from a lexicon for three different signed\nlanguages, with skeletal poses extracted from videos. To generate a sentence,\nthe text-to-gloss system is first run, and the pose representations of the\nresulting signs are stitched together.", "published": "2023-05-28 12:57:20", "link": "http://arxiv.org/abs/2305.17714v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Bridging the Language Gap: Dynamic Learning Strategies for Improving\n  Multilingual Performance in LLMs", "abstract": "Large language models (LLMs) have revolutionized various domains but still\nstruggle with non-Latin scripts and low-resource languages. This paper\naddresses the critical challenge of improving multilingual performance without\nextensive fine-tuning. We introduce a novel dynamic learning approach that\noptimizes prompt strategy, embedding model, and LLM per query at runtime. By\nadapting configurations dynamically, our method achieves significant\nimprovements over static, best and random baselines. It operates efficiently in\nboth offline and online settings, generalizing seamlessly across new languages\nand datasets. Leveraging Retrieval-Augmented Generation (RAG) with\nstate-of-the-art multilingual embeddings, we achieve superior task performance\nacross diverse linguistic contexts. Through systematic investigation and\nevaluation across 18 diverse languages using popular question-answering (QA)\ndatasets we show our approach results in 10-15% improvements in multilingual\nperformance over pre-trained models and 4x gains compared to fine-tuned,\nlanguage-specific models.", "published": "2023-05-28 14:48:38", "link": "http://arxiv.org/abs/2305.17740v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models are Bounded Pragmatic Speakers: Understanding RLHF from\n  a Bayesian Cognitive Modeling Perspective", "abstract": "How do language models \"think\"? This paper formulates a probabilistic\ncognitive model called the bounded pragmatic speaker, which can characterize\nthe operation of different variations of language models. Specifically, we\ndemonstrate that large language models fine-tuned with reinforcement learning\nfrom human feedback (Ouyang et al., 2022) embody a model of thought that\nconceptually resembles a fast-and-slow model (Kahneman, 2011), which\npsychologists have attributed to humans. We discuss the limitations of\nreinforcement learning from human feedback as a fast-and-slow model of thought\nand propose avenues for expanding this framework. In essence, our research\nhighlights the value of adopting a cognitive probabilistic modeling approach to\ngain insights into the comprehension, evaluation, and advancement of language\nmodels.", "published": "2023-05-28 16:04:48", "link": "http://arxiv.org/abs/2305.17760v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Power Outage Detection Task with Limited Training\n  Data", "abstract": "Early detection of power outages is crucial for maintaining a reliable power\ndistribution system. This research investigates the use of transfer learning\nand language models in detecting outages with limited labeled data. By\nleveraging pretraining and transfer learning, models can generalize to unseen\nclasses.\n  Using a curated balanced dataset of social media tweets related to power\noutages, we conducted experiments using zero-shot and few-shot learning. Our\nhypothesis is that Language Models pretrained with limited data could achieve\nhigh performance in outage detection tasks over baseline models. Results show\nthat while classical models outperform zero-shot Language Models, few-shot\nfine-tuning significantly improves their performance. For example, with 10%\nfine-tuning, BERT achieves 81.3% accuracy (+15.3%), and GPT achieves 74.5%\naccuracy (+8.5%). This has practical implications for analyzing and localizing\noutages in scenarios with limited data availability.\n  Our evaluation provides insights into the potential of few-shot fine-tuning\nwith Language Models for power outage detection, highlighting their strengths\nand limitations. This research contributes to the knowledge base of leveraging\nadvanced natural language processing techniques for managing critical\ninfrastructure.", "published": "2023-05-28 22:36:35", "link": "http://arxiv.org/abs/2305.17817v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Large Language Models, scientific knowledge and factuality: A framework\n  to streamline human expert evaluation", "abstract": "The paper introduces a framework for the evaluation of the encoding of\nfactual scientific knowledge, designed to streamline the manual evaluation\nprocess typically conducted by domain experts. Inferring over and extracting\ninformation from Large Language Models (LLMs) trained on a large corpus of\nscientific literature can potentially define a step change in biomedical\ndiscovery, reducing the barriers for accessing and integrating existing medical\nevidence. This work explores the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nframework involves of three evaluation steps, each assessing different aspects\nsequentially: fluency, prompt alignment, semantic coherence, factual knowledge,\nand specificity of the generated responses. By splitting these tasks between\nnon-experts and experts, the framework reduces the effort required from the\nlatter. The work provides a systematic assessment on the ability of eleven\nstate-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two\nprompting-based tasks: chemical compound definition generation and chemical\ncompound-fungus relation determination. Although recent models have improved in\nfluency, factual accuracy is still low and models are biased towards\nover-represented entities. The ability of LLMs to serve as biomedical knowledge\nbases is questioned, and the need for additional systematic evaluation\nframeworks is highlighted. While LLMs are currently not fit for purpose to be\nused as biomedical factual knowledge bases in a zero-shot setting, there is a\npromising emerging property in the direction of factuality as the models become\ndomain specialised, scale-up in size and level of human feedback.", "published": "2023-05-28 22:46:21", "link": "http://arxiv.org/abs/2305.17819v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models", "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor\nattacks against prompt-based models consider injecting backdoors into the\nentire embedding layers or word embedding vectors. Such attacks can be easily\naffected by retraining on downstream tasks and with different prompting\nstrategies, limiting the transferability of backdoor attacks. In this work, we\npropose transferable backdoor attacks against prompt-based models, called\nNOTABLE, which is independent of downstream tasks and prompting strategies.\nSpecifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing\nan adaptive verbalizer to bind triggers to specific words (i.e., anchors). It\nactivates the backdoor by pasting input with triggers to reach\nadversary-desired anchors, achieving independence from downstream tasks and\nprompting strategies. We conduct experiments on six NLP tasks, three popular\nmodels, and three prompting strategies. Empirical results show that NOTABLE\nachieves superior attack performance (i.e., attack success rate over 90% on all\nthe datasets), and outperforms two state-of-the-art baselines. Evaluations on\nthree defenses show the robustness of NOTABLE. Our code can be found at\nhttps://github.com/RU-System-Software-and-Security/Notable.", "published": "2023-05-28 23:35:17", "link": "http://arxiv.org/abs/2305.17826v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature\n  Adaptation of Vision-Language Models", "abstract": "Image ad understanding is a crucial task with wide real-world applications.\nAlthough highly challenging with the involvement of diverse atypical scenes,\nreal-world entities, and reasoning over scene-texts, how to interpret image ads\nis relatively under-explored, especially in the era of foundational\nvision-language models (VLMs) featuring impressive generalizability and\nadaptability. In this paper, we perform the first empirical study of image ad\nunderstanding through the lens of pre-trained VLMs. We benchmark and reveal\npractical challenges in adapting these VLMs to image ad understanding. We\npropose a simple feature adaptation strategy to effectively fuse multimodal\ninformation for image ads and further empower it with knowledge of real-world\nentities. We hope our study draws more attention to image ad understanding\nwhich is broadly relevant to the advertising industry.", "published": "2023-05-28 04:49:01", "link": "http://arxiv.org/abs/2305.18373v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Emergent Modularity in Pre-trained Transformers", "abstract": "This work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis.", "published": "2023-05-28 11:02:32", "link": "http://arxiv.org/abs/2305.18390v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Quantitative Review on Language Model Efficiency Research", "abstract": "Language models (LMs) are being scaled and becoming powerful. Improving their\nefficiency is one of the core research topics in neural information processing\nsystems. Tay et al. (2022) provided a comprehensive overview of efficient\nTransformers that have become an indispensable staple in the field of NLP.\nHowever, in the section of \"On Evaluation\", they left an open question \"which\nfundamental efficient Transformer one should consider,\" answered by \"still a\nmystery\" because \"many research papers select their own benchmarks.\"\nUnfortunately, there was not quantitative analysis about the performances of\nTransformers on any benchmarks. Moreover, state space models (SSMs) have\ndemonstrated their abilities of modeling long-range sequences with\nnon-attention mechanisms, which were not discussed in the prior review. This\narticle makes a meta analysis on the results from a set of papers on efficient\nTransformers as well as those on SSMs. It provides a quantitative review on LM\nefficiency research and gives suggestions for future research.", "published": "2023-05-28 20:25:20", "link": "http://arxiv.org/abs/2306.01768v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching", "abstract": "In recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.", "published": "2023-05-28 03:29:59", "link": "http://arxiv.org/abs/2305.17619v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In-Context Analogical Reasoning with Pre-Trained Language Models", "abstract": "Analogical reasoning is a fundamental capacity of human cognition that allows\nus to reason abstractly about novel situations by relating them to past\nexperiences. While it is thought to be essential for robust reasoning in AI\nsystems, conventional approaches require significant training and/or\nhard-coding of domain knowledge to be applied to benchmark tasks. Inspired by\ncognitive science research that has found connections between human language\nand analogy-making, we explore the use of intuitive language-based abstractions\nto support analogy in AI systems. Specifically, we apply large pre-trained\nlanguage models (PLMs) to visual Raven's Progressive Matrices (RPM), a common\nrelational reasoning test. By simply encoding the perceptual features of the\nproblem into language form, we find that PLMs exhibit a striking capacity for\nzero-shot relational reasoning, exceeding human performance and nearing\nsupervised vision-based methods. We explore different encodings that vary the\nlevel of abstraction over task features, finding that higher-level abstractions\nfurther strengthen PLMs' analogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-context learning, and prior\nknowledge in solving RPM tasks.", "published": "2023-05-28 04:22:26", "link": "http://arxiv.org/abs/2305.17626v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Robust Natural Language Understanding with Residual Attention Debiasing", "abstract": "Natural language understanding (NLU) models often suffer from unintended\ndataset biases. Among bias mitigation methods, ensemble-based debiasing\nmethods, especially product-of-experts (PoE), have stood out for their\nimpressive empirical success. However, previous ensemble-based debiasing\nmethods typically apply debiasing on top-level logits without directly\naddressing biased attention patterns. Attention serves as the main media of\nfeature interaction and aggregation in PLMs and plays a crucial role in\nproviding robust prediction. In this paper, we propose REsidual Attention\nDebiasing (READ), an end-to-end debiasing method that mitigates unintended\nbiases from attention. Experiments on three NLU tasks show that READ\nsignificantly improves the performance of BERT-based models on OOD data with\nshortcuts removed, including +12.9% accuracy on HANS, +11.0% accuracy on\nFEVER-Symmetric, and +2.7% F1 on PAWS. Detailed analyses demonstrate the\ncrucial role of unbiased attention in robust NLU models and that READ\neffectively mitigates biases in attention. Code is available at\nhttps://github.com/luka-group/READ.", "published": "2023-05-28 04:25:04", "link": "http://arxiv.org/abs/2305.17627v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech\n  Models", "abstract": "Self-supervised learning (SSL) has achieved notable success in many speech\nprocessing tasks, but the large model size and heavy computational cost hinder\nthe deployment. Knowledge distillation trains a small student model to mimic\nthe behavior of a large teacher model. However, the student architecture\nusually needs to be manually designed and will remain fixed during training,\nwhich requires prior knowledge and can lead to suboptimal performance. Inspired\nby recent success of task-specific structured pruning, we propose DPHuBERT, a\nnovel task-agnostic compression method for speech SSL based on joint\ndistillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms\npure distillation methods in almost all tasks. Moreover, DPHuBERT requires\nlittle training time and performs well with limited training data, making it\nsuitable for resource-constrained applications. Our method can also be applied\nto various speech SSL models. Our code and models will be publicly available.", "published": "2023-05-28 07:09:33", "link": "http://arxiv.org/abs/2305.17651v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Decoding the Underlying Meaning of Multimodal Hateful Memes", "abstract": "Recent studies have proposed models that yielded promising performance for\nthe hateful meme classification task. Nevertheless, these proposed models do\nnot generate interpretable explanations that uncover the underlying meaning and\nsupport the classification output. A major reason for the lack of explainable\nhateful meme methods is the absence of a hateful meme dataset that contains\nground truth explanations for benchmarking or training. Intuitively, having\nsuch explanations can educate and assist content moderators in interpreting and\nremoving flagged hateful memes. This paper address this research gap by\nintroducing Hateful meme with Reasons Dataset (HatReD), which is a new\nmultimodal hateful meme dataset annotated with the underlying hateful\ncontextual reasons. We also define a new conditional generation task that aims\nto automatically generate underlying reasons to explain hateful memes and\nestablish the baseline performance of state-of-the-art pre-trained language\nmodels on this task. We further demonstrate the usefulness of HatReD by\nanalyzing the challenges of the new conditional generation task in explaining\nmemes in seen and unseen domains. The dataset and benchmark models are made\navailable here: https://github.com/Social-AI-Studio/HatRed", "published": "2023-05-28 10:02:59", "link": "http://arxiv.org/abs/2305.17678v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "FuseCap: Leveraging Large Language Models for Enriched Fused Image\n  Captions", "abstract": "The advent of vision-language pre-training techniques enhanced substantial\nprogress in the development of models for image captioning. However, these\nmodels frequently produce generic captions and may omit semantically important\nimage details. This limitation can be traced back to the image-text datasets;\nwhile their captions typically offer a general description of image content,\nthey frequently omit salient details. Considering the magnitude of these\ndatasets, manual reannotation is impractical, emphasizing the need for an\nautomated approach. To address this challenge, we leverage existing captions\nand explore augmenting them with visual details using \"frozen\" vision experts\nincluding an object detector, an attribute recognizer, and an Optical Character\nRecognizer (OCR). Our proposed method, FuseCap, fuses the outputs of such\nvision experts with the original captions using a large language model (LLM),\nyielding comprehensive image descriptions. We automatically curate a training\nset of 12M image-enriched caption pairs. These pairs undergo extensive\nevaluation through both quantitative and qualitative analyses. Subsequently,\nthis data is utilized to train a captioning generation BLIP-based model. This\nmodel outperforms current state-of-the-art approaches, producing more precise\nand detailed descriptions, demonstrating the effectiveness of the proposed\ndata-centric approach. We release this large-scale dataset of enriched\nimage-caption pairs for the community.", "published": "2023-05-28 13:16:03", "link": "http://arxiv.org/abs/2305.17718v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Investigating Pre-trained Audio Encoders in the Low-Resource Condition", "abstract": "Pre-trained speech encoders have been central to pushing state-of-the-art\nresults across various speech understanding and generation tasks. Nonetheless,\nthe capabilities of these encoders in low-resource settings are yet to be\nthoroughly explored. To address this, we conduct a comprehensive set of\nexperiments using a representative set of 3 state-of-the-art encoders\n(Wav2vec2, WavLM, Whisper) in the low-resource setting across 7 speech\nunderstanding and generation tasks. We provide various quantitative and\nqualitative analyses on task performance, convergence speed, and\nrepresentational properties of the encoders. We observe a connection between\nthe pre-training protocols of these encoders and the way in which they capture\ninformation in their internal layers. In particular, we observe the Whisper\nencoder exhibits the greatest low-resource capabilities on content-driven tasks\nin terms of performance and convergence speed.", "published": "2023-05-28 14:15:19", "link": "http://arxiv.org/abs/2305.17733v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Range-Based Equal Error Rate for Spoof Localization", "abstract": "Spoof localization, also called segment-level detection, is a crucial task\nthat aims to locate spoofs in partially spoofed audio. The equal error rate\n(EER) is widely used to measure performance for such biometric scenarios.\nAlthough EER is the only threshold-free metric, it is usually calculated in a\npoint-based way that uses scores and references with a pre-defined temporal\nresolution and counts the number of misclassified segments. Such point-based\nmeasurement overly relies on this resolution and may not accurately measure\nmisclassified ranges. To properly measure misclassified ranges and better\nevaluate spoof localization performance, we upgrade point-based EER to\nrange-based EER. Then, we adapt the binary search algorithm for calculating\nrange-based EER and compare it with the classical point-based EER. Our analyses\nsuggest utilizing either range-based EER, or point-based EER with a proper\ntemporal resolution can fairly and properly evaluate the performance of spoof\nlocalization.", "published": "2023-05-28 14:46:54", "link": "http://arxiv.org/abs/2305.17739v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RASR2: The RWTH ASR Toolkit for Generic Sequence-to-sequence Speech\n  Recognition", "abstract": "Modern public ASR tools usually provide rich support for training various\nsequence-to-sequence (S2S) models, but rather simple support for decoding\nopen-vocabulary scenarios only. For closed-vocabulary scenarios, public tools\nsupporting lexical-constrained decoding are usually only for classical ASR, or\ndo not support all S2S models. To eliminate this restriction on research\npossibilities such as modeling unit choice, we present RASR2 in this work, a\nresearch-oriented generic S2S decoder implemented in C++. It offers a strong\nflexibility/compatibility for various S2S models, language models, label\nunits/topologies and neural network architectures. It provides efficient\ndecoding for both open- and closed-vocabulary scenarios based on a generalized\nsearch framework with rich support for different search modes and settings. We\nevaluate RASR2 with a wide range of experiments on both switchboard and\nLibrispeech corpora. Our source code is public online.", "published": "2023-05-28 17:48:48", "link": "http://arxiv.org/abs/2305.17782v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MemeGraphs: Linking Memes to Knowledge Graphs", "abstract": "Memes are a popular form of communicating trends and ideas in social media\nand on the internet in general, combining the modalities of images and text.\nThey can express humor and sarcasm but can also have offensive content.\nAnalyzing and classifying memes automatically is challenging since their\ninterpretation relies on the understanding of visual elements, language, and\nbackground knowledge. Thus, it is important to meaningfully represent these\nsources and the interaction between them in order to classify a meme as a\nwhole. In this work, we propose to use scene graphs, that express images in\nterms of objects and their visual relations, and knowledge graphs as structured\nrepresentations for meme classification with a Transformer-based architecture.\nWe compare our approach with ImgBERT, a multimodal model that uses only learned\n(instead of structured) representations of the meme, and observe consistent\nimprovements. We further provide a dataset with human graph annotations that we\ncompare to automatically generated graphs and entity linking. Analysis shows\nthat automatic methods link more entities than human annotators and that\nautomatically generated graphs are better suited for hatefulness classification\nin memes.", "published": "2023-05-28 11:17:30", "link": "http://arxiv.org/abs/2305.18391v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Knowledge-Augmented Reasoning Distillation for Small Language Models in\n  Knowledge-Intensive Tasks", "abstract": "Large Language Models (LLMs) have shown promising performance in\nknowledge-intensive reasoning tasks that require a compound understanding of\nknowledge. However, deployment of the LLMs in real-world applications can be\nchallenging due to their high computational requirements and concerns on data\nprivacy. Previous studies have focused on building task-specific small Language\nModels (LMs) by fine-tuning them with labeled data or distilling LLMs. However,\nthese approaches are ill-suited for knowledge-intensive reasoning tasks due to\nthe limited capacity of small LMs in memorizing the knowledge required.\nMotivated by our theoretical analysis on memorization, we propose\nKnowledge-Augmented Reasoning Distillation (KARD), a novel method that\nfine-tunes small LMs to generate rationales obtained from LLMs with augmented\nknowledge retrieved from an external knowledge base. Moreover, we further\npropose a neural reranker to obtain documents relevant to rationale generation.\nWe empirically show that KARD significantly improves the performance of small\nT5 and GPT models on the challenging knowledge-intensive reasoning datasets,\nnamely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the\n250M T5 models achieve superior performance against the fine-tuned 3B models,\nhaving 12 times larger parameters, on both MedQA-USMLE and StrategyQA\nbenchmarks.", "published": "2023-05-28 13:00:00", "link": "http://arxiv.org/abs/2305.18395v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly\n  Transformers", "abstract": "The community explored to build private inference frameworks for\ntransformer-based large language models (LLMs) in a server-client setting,\nwhere the server holds the model parameters and the client inputs its private\ndata (or prompt) for inference. However, these frameworks impose significant\noverhead when the private inputs are forward propagated through the original\nLLMs. In this paper, we show that substituting the computation- and\ncommunication-heavy operators in the transformer architecture with\nprivacy-computing friendly approximations can greatly reduce the private\ninference costs while incurring very minor impact on model performance.\nCompared to state-of-the-art Iron (NeurIPS 2022), our privacy-computing\nfriendly model inference pipeline achieves a $5\\times$ acceleration in\ncomputation and an 80% reduction in communication overhead, while retaining\nnearly identical accuracy.", "published": "2023-05-28 13:08:13", "link": "http://arxiv.org/abs/2305.18396v3", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Conformal Prediction with Large Language Models for Multi-Choice\n  Question Answering", "abstract": "As large language models continue to be widely developed, robust uncertainty\nquantification techniques will become crucial for their safe deployment in\nhigh-stakes scenarios. In this work, we explore how conformal prediction can be\nused to provide uncertainty quantification in language models for the specific\ntask of multiple-choice question-answering. We find that the uncertainty\nestimates from conformal prediction are tightly correlated with prediction\naccuracy. This observation can be useful for downstream applications such as\nselective classification and filtering out low-quality predictions. We also\ninvestigate the exchangeability assumption required by conformal prediction to\nout-of-subject questions, which may be a more realistic scenario for many\npractical applications. Our work contributes towards more trustworthy and\nreliable usage of large language models in safety-critical situations, where\nrobust guarantees of error rate are required.", "published": "2023-05-28 15:26:10", "link": "http://arxiv.org/abs/2305.18404v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Understanding Breast Cancer Survival: Using Causality and Language\n  Models on Multi-omics Data", "abstract": "The need for more usable and explainable machine learning models in\nhealthcare increases the importance of developing and utilizing causal\ndiscovery algorithms, which aim to discover causal relations by analyzing\nobservational data. Explainable approaches aid clinicians and biologists in\npredicting the prognosis of diseases and suggesting proper treatments. However,\nvery little research has been conducted at the crossroads between causal\ndiscovery, genomics, and breast cancer, and we aim to bridge this gap.\nMoreover, evaluation of causal discovery methods on real data is in general\nnotoriously difficult because ground-truth causal relations are usually\nunknown, and accordingly, in this paper, we also propose to address the\nevaluation problem with large language models. In particular, we exploit\nsuitable causal discovery algorithms to investigate how various perturbations\nin the genome can affect the survival of patients diagnosed with breast cancer.\nWe used three main causal discovery algorithms: PC, Greedy Equivalence Search\n(GES), and a Generalized Precision Matrix-based one. We experiment with a\nsubset of The Cancer Genome Atlas, which contains information about mutations,\ncopy number variations, protein levels, and gene expressions for 705 breast\ncancer patients. Our findings reveal important factors related to the vital\nstatus of patients using causal discovery algorithms. However, the reliability\nof these results remains a concern in the medical domain. Accordingly, as\nanother contribution of the work, the results are validated through language\nmodels trained on biomedical literature, such as BlueBERT and other large\nlanguage models trained on medical corpora. Our results profess proper\nutilization of causal discovery algorithms and language models for revealing\nreliable causal relations for clinical applications.", "published": "2023-05-28 17:07:46", "link": "http://arxiv.org/abs/2305.18410v1", "categories": ["cs.LG", "cs.CL", "q-bio.GN", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Semantic Segmentation with Bidirectional Language Models Improves\n  Long-form ASR", "abstract": "We propose a method of segmenting long-form speech by separating semantically\ncomplete sentences within the utterance. This prevents the ASR decoder from\nneedlessly processing faraway context while also preventing it from missing\nrelevant context within the current sentence. Semantically complete sentence\nboundaries are typically demarcated by punctuation in written text; but\nunfortunately, spoken real-world utterances rarely contain punctuation. We\naddress this limitation by distilling punctuation knowledge from a\nbidirectional teacher language model (LM) trained on written, punctuated text.\nWe compare our segmenter, which is distilled from the LM teacher, against a\nsegmenter distilled from a acoustic-pause-based teacher used in other works, on\na streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2%\nrelative WER gain along with a 60 ms median end-of-segment latency reduction on\na YouTube captioning task.", "published": "2023-05-28 19:31:45", "link": "http://arxiv.org/abs/2305.18419v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mitigating Label Biases for In-context Learning", "abstract": "Various design settings for in-context learning (ICL), such as the choice and\norder of the in-context examples, can bias a model toward a particular\nprediction without being reflective of an understanding of the task. While many\nstudies discuss these design choices, there have been few systematic\ninvestigations into categorizing them and mitigating their impact. In this\nwork, we define a typology for three types of label biases in ICL for text\nclassification: vanilla-label bias, context-label bias, and domain-label bias\n(which we conceptualize and detect for the first time).\n  Our analysis demonstrates that prior label bias calibration methods fall\nshort of addressing all three types of biases. Specifically, domain-label bias\nrestricts LLMs to random-level performance on many tasks regardless of the\nchoice of in-context examples. To mitigate the effect of these biases, we\npropose a simple bias calibration method that estimates a language model's\nlabel bias using random in-domain words from the task corpus. After controlling\nfor this estimated bias when making predictions, our novel domain-context\ncalibration significantly improves the ICL performance of GPT-J and GPT-3 on a\nwide range of tasks. The gain is substantial on tasks with large domain-label\nbias (up to 37% in Macro-F1). Furthermore, our results generalize to models\nwith different scales, pretraining methods, and manually-designed task\ninstructions, showing the prevalence of label biases in ICL.", "published": "2023-05-28 15:37:39", "link": "http://arxiv.org/abs/2305.19148v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule\n  Zero-Shot Learning", "abstract": "Molecule property prediction has gained significant attention in recent\nyears. The main bottleneck is the label insufficiency caused by expensive lab\nexperiments. In order to alleviate this issue and to better leverage textual\nknowledge for tasks, this study investigates the feasibility of employing\nnatural language instructions to accomplish molecule-related tasks in a\nzero-shot setting. We discover that existing molecule-text models perform\npoorly in this setting due to inadequate treatment of instructions and limited\ncapacity for graphs. To overcome these issues, we propose GIMLET, which unifies\nlanguage models for both graph and text data. By adopting generalized position\nembedding, our model is extended to encode both graph structures and\ninstruction text without additional graph encoding modules. GIMLET also\ndecouples encoding of the graph from tasks instructions in the attention\nmechanism, enhancing the generalization of graph features across novel tasks.\nWe construct a dataset consisting of more than two thousand molecule tasks with\ncorresponding instructions derived from task descriptions. We pretrain GIMLET\non the molecule tasks along with instructions, enabling the model to transfer\neffectively to a broad range of tasks. Experimental results demonstrate that\nGIMLET significantly outperforms molecule-text baselines in instruction-based\nzero-shot learning, even achieving closed results to supervised GNN models on\ntasks such as toxcast and muv.", "published": "2023-05-28 18:27:59", "link": "http://arxiv.org/abs/2306.13089v3", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Reward Collapse in Aligning Large Language Models", "abstract": "The extraordinary capabilities of large language models (LLMs) such as\nChatGPT and GPT-4 are in part unleashed by aligning them with reward models\nthat are trained on human preferences, which are often represented as rankings\nof responses to prompts. In this paper, we document the phenomenon of\n\\textit{reward collapse}, an empirical observation where the prevailing\nranking-based approach results in an \\textit{identical} reward distribution\n\\textit{regardless} of the prompts during the terminal phase of training. This\noutcome is undesirable as open-ended prompts like ``write a short story about\nyour best friend'' should yield a continuous range of rewards for their\ncompletions, while specific prompts like ``what is the capital of New Zealand''\nshould generate either high or low rewards. Our theoretical investigation\nreveals that reward collapse is primarily due to the insufficiency of the\nranking-based objective function to incorporate prompt-related information\nduring optimization. This insight allows us to derive closed-form expressions\nfor the reward distribution associated with a set of utility functions in an\nasymptotic regime. To overcome reward collapse, we introduce a prompt-aware\noptimization scheme that provably admits a prompt-dependent reward distribution\nwithin the interpolating regime. Our experimental results suggest that our\nproposed prompt-aware utility functions significantly alleviate reward collapse\nduring the training of reward models.", "published": "2023-05-28 02:12:00", "link": "http://arxiv.org/abs/2305.17608v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "ChatGPT Informed Graph Neural Network for Stock Movement Prediction", "abstract": "ChatGPT has demonstrated remarkable capabilities across various natural\nlanguage processing (NLP) tasks. However, its potential for inferring dynamic\nnetwork structures from temporal textual data, specifically financial news,\nremains an unexplored frontier. In this research, we introduce a novel\nframework that leverages ChatGPT's graph inference capabilities to enhance\nGraph Neural Networks (GNN). Our framework adeptly extracts evolving network\nstructures from textual data, and incorporates these networks into graph neural\nnetworks for subsequent predictive tasks. The experimental results from stock\nmovement forecasting indicate our model has consistently outperformed the\nstate-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios\nconstructed based on our model's outputs demonstrate higher annualized\ncumulative returns, alongside reduced volatility and maximum drawdown. This\nsuperior performance highlights the potential of ChatGPT for text-based network\ninferences and underscores its promising implications for the financial sector.", "published": "2023-05-28 21:11:59", "link": "http://arxiv.org/abs/2306.03763v4", "categories": ["q-fin.ST", "cs.AI", "cs.CL", "cs.LG", "q-fin.CP", "I.2.7; J.1"], "primary_category": "q-fin.ST"}
{"title": "Adapting Language-Audio Models as Few-Shot Audio Learners", "abstract": "We presented the Treff adapter, a training-efficient adapter for CLAP, to\nboost zero-shot classification performance by making use of a small set of\nlabelled data. Specifically, we designed CALM to retrieve the probability\ndistribution of text-audio clips over classes using a set of audio-label pairs\nand combined it with CLAP's zero-shot classification results. Furthermore, we\ndesigned a training-free version of the Treff adapter by using CALM as a cosine\nsimilarity measure. Experiments showed that the proposed Treff adapter is\ncomparable and even better than fully-supervised methods and adaptation methods\nin low-shot and data-abundant scenarios. While the Treff adapter shows that\ncombining large-scale pretraining and rapid learning of domain-specific\nknowledge is non-trivial for obtaining generic representations for few-shot\nlearning, it is still limited to audio classification tasks. In the future, we\nwill explore how to use audio-language models in diverse audio domains.", "published": "2023-05-28 13:17:10", "link": "http://arxiv.org/abs/2305.17719v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Stochastic Pitch Prediction Improves the Diversity and Naturalness of\n  Speech in Glow-TTS", "abstract": "Flow-based generative models are widely used in text-to-speech (TTS) systems\nto learn the distribution of audio features (e.g., Mel-spectrograms) given the\ninput tokens and to sample from this distribution to generate diverse\nutterances. However, in the zero-shot multi-speaker TTS scenario, the generated\nutterances lack diversity and naturalness. In this paper, we propose to improve\nthe diversity of utterances by explicitly learning the distribution of\nfundamental frequency sequences (pitch contours) of each speaker during\ntraining using a stochastic flow-based pitch predictor, then conditioning the\nmodel on generated pitch contours during inference. The experimental results\ndemonstrate that the proposed method yields a significant improvement in the\nnaturalness and diversity of speech generated by a Glow-TTS model that uses\nexplicit stochastic pitch prediction, over a Glow-TTS baseline and an improved\nGlow-TTS model that uses a stochastic duration predictor.", "published": "2023-05-28 13:44:27", "link": "http://arxiv.org/abs/2305.17724v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech\n  Translation", "abstract": "Direct speech-to-speech translation (S2ST) has gradually become popular as it\nhas many advantages compared with cascade S2ST. However, current research\nmainly focuses on the accuracy of semantic translation and ignores the speech\nstyle transfer from a source language to a target language. The lack of\nhigh-fidelity expressive parallel data makes such style transfer challenging,\nespecially in more practical zero-shot scenarios. To solve this problem, we\nfirst build a parallel corpus using a multi-lingual multi-speaker\ntext-to-speech synthesis (TTS) system and then propose the StyleS2ST model with\ncross-lingual speech style transfer ability based on a style adaptor on a\ndirect S2ST system framework. Enabling continuous style space modeling of an\nacoustic model through parallel corpus training and non-parallel TTS data\naugmentation, StyleS2ST captures cross-lingual acoustic feature mapping from\nthe source to the target language. Experiments show that StyleS2ST achieves\ngood style similarity and naturalness in both in-set and out-of-set zero-shot\nscenarios.", "published": "2023-05-28 14:09:17", "link": "http://arxiv.org/abs/2305.17732v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CAPTDURE: Captioned Sound Dataset of Single Sources", "abstract": "In conventional studies on environmental sound separation and synthesis using\ncaptions, datasets consisting of multiple-source sounds with their captions\nwere used for model training. However, when we collect the captions for\nmultiple-source sound, it is not easy to collect detailed captions for each\nsound source, such as the number of sound occurrences and timbre. Therefore, it\nis difficult to extract only the single-source target sound by the\nmodel-training method using a conventional captioned sound dataset. In this\nwork, we constructed a dataset with captions for a single-source sound named\nCAPTDURE, which can be used in various tasks such as environmental sound\nseparation and synthesis. Our dataset consists of 1,044 sounds and 4,902\ncaptions. We evaluated the performance of environmental sound extraction using\nour dataset. The experimental results show that the captions for single-source\nsounds are effective in extracting only the single-source target sound from the\nmixture sound.", "published": "2023-05-28 15:56:20", "link": "http://arxiv.org/abs/2305.17758v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spot keywords from very noisy and mixed speech", "abstract": "Most existing keyword spotting research focuses on conditions with slight or\nmoderate noise. In this paper, we try to tackle a more challenging task:\ndetecting keywords buried under strong interfering speech (10 times higher than\nthe keyword in amplitude), and even worse, mixed with other keywords. We\npropose a novel Mix Training (MT) strategy that encourages the model to\ndiscover low-energy keywords from noisy and mixed speech. Experiments were\nconducted with a vanilla CNN and two EfficientNet (B0/B2) architectures. The\nresults evaluated with the Google Speech Command dataset demonstrated that the\nproposed mix training approach is highly effective and outperforms standard\ndata augmentation and mixup training.", "published": "2023-05-28 12:26:13", "link": "http://arxiv.org/abs/2305.17706v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Bayesian inference and neural estimation of acoustic wave propagation", "abstract": "In this work, we introduce a novel framework which combines physics and\nmachine learning methods to analyse acoustic signals. Three methods are\ndeveloped for this task: a Bayesian inference approach for inferring the\nspectral acoustics characteristics, a neural-physical model which equips a\nneural network with forward and backward physical losses, and the non-linear\nleast squares approach which serves as benchmark. The inferred propagation\ncoefficient leads to the room impulse response (RIR) quantity which can be used\nfor relocalisation with uncertainty. The simplicity and efficiency of this\nframework is empirically validated on simulated data.", "published": "2023-05-28 15:14:46", "link": "http://arxiv.org/abs/2305.17749v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "physics.data-an", "68T01", "J.2"], "primary_category": "cs.SD"}
{"title": "Speech Intelligibility Assessment of Dysarthric Speech by using Goodness\n  of Pronunciation with Uncertainty Quantification", "abstract": "This paper proposes an improved Goodness of Pronunciation (GoP) that utilizes\nUncertainty Quantification (UQ) for automatic speech intelligibility assessment\nfor dysarthric speech. Current GoP methods rely heavily on neural\nnetwork-driven overconfident predictions, which is unsuitable for assessing\ndysarthric speech due to its significant acoustic differences from healthy\nspeech. To alleviate the problem, UQ techniques were used on GoP by 1)\nnormalizing the phoneme prediction (entropy, margin, maxlogit, logit-margin)\nand 2) modifying the scoring function (scaling, prior normalization). As a\nresult, prior-normalized maxlogit GoP achieves the best performance, with a\nrelative increase of 5.66%, 3.91%, and 23.65% compared to the baseline GoP for\nEnglish, Korean, and Tamil, respectively. Furthermore, phoneme analysis is\nconducted to identify which phoneme scores significantly correlate with\nintelligibility scores in each language.", "published": "2023-05-28 11:48:36", "link": "http://arxiv.org/abs/2305.18392v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
