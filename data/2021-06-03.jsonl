{"title": "Dissecting Generation Modes for Abstractive Summarization Models via\n  Ablation and Attribution", "abstract": "Despite the prominence of neural abstractive summarization models, we know\nlittle about how they actually form summaries and how to understand where their\ndecisions come from. We propose a two-step method to interpret summarization\nmodel decisions. We first analyze the model's behavior by ablating the full\nmodel to categorize each decoder decision into one of several generation modes:\nroughly, is the model behaving like a language model, is it relying heavily on\nthe input, or is it somewhere in between? After isolating decisions that do\ndepend on the input, we explore interpreting these decisions using several\ndifferent attribution methods. We compare these techniques based on their\nability to select content and reconstruct the model's predicted token from\nperturbations of the input, thus revealing whether highlighted attributions are\ntruly important for the generation of the next token. While this machinery can\nbe broadly useful even beyond summarization, we specifically demonstrate its\ncapability to identify phrases the summarization model has memorized and\ndetermine where in the training pipeline this memorization happened, as well as\nstudy complex generation phenomena like sentence fusion on a per-instance\nbasis.", "published": "2021-06-03 00:54:16", "link": "http://arxiv.org/abs/2106.01518v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"You made me feel this way\": Investigating Partners' Influence in\n  Predicting Emotions in Couples' Conflict Interactions using Speech Data", "abstract": "How romantic partners interact with each other during a conflict influences\nhow they feel at the end of the interaction and is predictive of whether the\npartners stay together in the long term. Hence understanding the emotions of\neach partner is important. Yet current approaches that are used include\nself-reports which are burdensome and hence limit the frequency of this data\ncollection. Automatic emotion prediction could address this challenge. Insights\nfrom psychology research indicate that partners' behaviors influence each\nother's emotions in conflict interaction and hence, the behavior of both\npartners could be considered to better predict each partner's emotion. However,\nit is yet to be investigated how doing so compares to only using each partner's\nown behavior in terms of emotion prediction performance. In this work, we used\nBERT to extract linguistic features (i.e., what partners said) and openSMILE to\nextract paralinguistic features (i.e., how they said it) from a data set of 368\nGerman-speaking Swiss couples (N = 736 individuals) who were videotaped during\nan 8-minutes conflict interaction in the laboratory. Based on those features,\nwe trained machine learning models to predict if partners feel positive or\nnegative after the conflict interaction. Our results show that including the\nbehavior of the other partner improves the prediction performance. Furthermore,\nfor men, considering how their female partners spoke is most important and for\nwomen considering what their male partner said is most important in getting\nbetter prediction performance. This work is a step towards automatically\nrecognizing each partners' emotion based on the behavior of both, which would\nenable a better understanding of couples in research, therapy, and the real\nworld.", "published": "2021-06-03 01:15:41", "link": "http://arxiv.org/abs/2106.01526v2", "categories": ["cs.CL", "J.4"], "primary_category": "cs.CL"}
{"title": "BERT meets LIWC: Exploring State-of-the-Art Language Models for\n  Predicting Communication Behavior in Couples' Conflict Interactions", "abstract": "Many processes in psychology are complex, such as dyadic interactions between\ntwo interacting partners (e.g. patient-therapist, intimate relationship\npartners). Nevertheless, many basic questions about interactions are difficult\nto investigate because dyadic processes can be within a person and between\npartners, they are based on multimodal aspects of behavior and unfold rapidly.\nCurrent analyses are mainly based on the behavioral coding method, whereby\nhuman coders annotate behavior based on a coding schema. But coding is\nlabor-intensive, expensive, slow, focuses on few modalities. Current approaches\nin psychology use LIWC for analyzing couples' interactions. However, advances\nin natural language processing such as BERT could enable the development of\nsystems to potentially automate behavioral coding, which in turn could\nsubstantially improve psychological research. In this work, we train machine\nlearning models to automatically predict positive and negative communication\nbehavioral codes of 368 German-speaking Swiss couples during an 8-minute\nconflict interaction on a fine-grained scale (10-seconds sequences) using\nlinguistic features and paralinguistic features derived with openSMILE. Our\nresults show that both simpler TF-IDF features as well as more complex BERT\nfeatures performed better than LIWC, and that adding paralinguistic features\ndid not improve the performance. These results suggest it might be time to\nconsider modern alternatives to LIWC, the de facto linguistic features in\npsychology, for prediction tasks in couples research. This work is a further\nstep towards the automated coding of couples' behavior which could enhance\ncouple research and therapy, and be utilized for other dyadic interactions as\nwell.", "published": "2021-06-03 01:37:59", "link": "http://arxiv.org/abs/2106.01536v2", "categories": ["cs.CL", "J.4"], "primary_category": "cs.CL"}
{"title": "MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation\n  Understanding", "abstract": "Recently, various neural models for multi-party conversation (MPC) have\nachieved impressive improvements on a variety of tasks such as addressee\nrecognition, speaker identification and response prediction. However, these\nexisting methods on MPC usually represent interlocutors and utterances\nindividually and ignore the inherent complicated structure in MPC which may\nprovide crucial interlocutor and utterance semantics and would enhance the\nconversation understanding process. To this end, we present MPC-BERT, a\npre-trained model for MPC understanding that considers learning who says what\nto whom in a unified model with several elaborated self-supervised tasks.\nParticularly, these tasks can be generally categorized into (1) interlocutor\nstructure modeling including reply-to utterance recognition, identical speaker\nsearching and pointer consistency distinction, and (2) utterance semantics\nmodeling including masked shared utterance restoration and shared node\ndetection. We evaluate MPC-BERT on three downstream tasks including addressee\nrecognition, speaker identification and response selection. Experimental\nresults show that MPC-BERT outperforms previous methods by large margins and\nachieves new state-of-the-art performance on all three downstream tasks at two\nbenchmarks.", "published": "2021-06-03 01:49:12", "link": "http://arxiv.org/abs/2106.01541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adjacency List Oriented Relational Fact Extraction via Adaptive\n  Multi-task Learning", "abstract": "Relational fact extraction aims to extract semantic triplets from\nunstructured text. In this work, we show that all of the relational fact\nextraction models can be organized according to a graph-oriented analytical\nperspective. An efficient model, aDjacency lIst oRiented rElational faCT\n(DIRECT), is proposed based on this analytical framework. To alleviate\nchallenges of error propagation and sub-task loss equilibrium, DIRECT employs a\nnovel adaptive multi-task learning strategy with dynamic sub-task loss\nbalancing. Extensive experiments are conducted on two benchmark datasets, and\nresults prove that the proposed model outperforms a series of state-of-the-art\n(SoTA) models for relational triplet extraction.", "published": "2021-06-03 02:57:08", "link": "http://arxiv.org/abs/2106.01559v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Generative Pre-trained Language Models Serve as Knowledge Bases for\n  Closed-book QA?", "abstract": "Recent work has investigated the interesting question using pre-trained\nlanguage models (PLMs) as knowledge bases for answering open questions.\nHowever, existing work is limited in using small benchmarks with high\ntest-train overlaps. We construct a new dataset of closed-book QA using SQuAD,\nand investigate the performance of BART. Experiments show that it is\nchallenging for BART to remember training facts in high precision, and also\nchallenging to answer closed-book questions even if relevant knowledge is\nretained. Some promising directions are found, including decoupling the\nknowledge memorizing process and the QA finetune process, forcing the model to\nrecall relevant knowledge when question answering.", "published": "2021-06-03 03:04:06", "link": "http://arxiv.org/abs/2106.01561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discriminative Reasoning for Document-level Relation Extraction", "abstract": "Document-level relation extraction (DocRE) models generally use graph\nnetworks to implicitly model the reasoning skill (i.e., pattern recognition,\nlogical reasoning, coreference reasoning, etc.) related to the relation between\none entity pair in a document. In this paper, we propose a novel discriminative\nreasoning framework to explicitly model the paths of these reasoning skills\nbetween each entity pair in this document. Thus, a discriminative reasoning\nnetwork is designed to estimate the relation probability distribution of\ndifferent reasoning paths based on the constructed graph and vectorized\ndocument contexts for each entity pair, thereby recognizing their relation.\nExperimental results show that our method outperforms the previous\nstate-of-the-art performance on the large-scale DocRE dataset. The code is\npublicly available at https://github.com/xwjim/DRN.", "published": "2021-06-03 03:09:38", "link": "http://arxiv.org/abs/2106.01562v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Point or Not to Point: Understanding How Abstractive Summarizers\n  Paraphrase Text", "abstract": "Abstractive neural summarization models have seen great improvements in\nrecent years, as shown by ROUGE scores of the generated summaries. But despite\nthese improved metrics, there is limited understanding of the strategies\ndifferent models employ, and how those strategies relate their understanding of\nlanguage. To understand this better, we run several experiments to characterize\nhow one popular abstractive model, the pointer-generator model of See et al.\n(2017), uses its explicit copy/generation switch to control its level of\nabstraction (generation) vs extraction (copying). On an extractive-biased\ndataset, the model utilizes syntactic boundaries to truncate sentences that are\notherwise often copied verbatim. When we modify the copy/generation switch and\nforce the model to generate, only simple paraphrasing abilities are revealed\nalongside factual inaccuracies and hallucinations. On an abstractive-biased\ndataset, the model copies infrequently but shows similarly limited abstractive\nabilities. In line with previous research, these results suggest that\nabstractive summarization models lack the semantic understanding necessary to\ngenerate paraphrases that are both abstractive and faithful to the source\ndocument.", "published": "2021-06-03 04:03:15", "link": "http://arxiv.org/abs/2106.01581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Detecting Cyberbullying Comments on Online Game Forums", "abstract": "Online game forums are popular to most of game players. They use it to\ncommunicate and discuss the strategy of the game, or even to make friends.\nHowever, game forums also contain abusive and harassment speech, disturbing and\nthreatening players. Therefore, it is necessary to automatically detect and\nremove cyberbullying comments to keep the game forum clean and friendly. We use\nthe Cyberbullying dataset collected from World of Warcraft (WoW) and League of\nLegends (LoL) forums and train classification models to automatically detect\nwhether a comment of a player is abusive or not. The result obtains 82.69% of\nmacro F1-score for LoL forum and 83.86% of macro F1-score for WoW forum by the\nToxic-BERT model on the Cyberbullying dataset.", "published": "2021-06-03 05:08:11", "link": "http://arxiv.org/abs/2106.01598v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language\n  Models", "abstract": "This paper studies how to automatically generate a natural language text that\ndescribes the facts in knowledge graph (KG). Considering the few-shot setting,\nwe leverage the excellent capacities of pretrained language models (PLMs) in\nlanguage understanding and generation. We make three major technical\ncontributions, namely representation alignment for bridging the semantic gap\nbetween KG encodings and PLMs, relation-biased KG linearization for deriving\nbetter input representations, and multi-task learning for learning the\ncorrespondence between KG and text. Extensive experiments on three benchmark\ndatasets have demonstrated the effectiveness of our model on KG-to-text\ngeneration task. In particular, our model outperforms all comparison methods on\nboth fully-supervised and few-shot settings. Our code and datasets are\navailable at https://github.com/RUCAIBox/Few-Shot-KG2Text.", "published": "2021-06-03 06:48:00", "link": "http://arxiv.org/abs/2106.01623v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generate, Prune, Select: A Pipeline for Counterspeech Generation against\n  Online Hate Speech", "abstract": "Countermeasures to effectively fight the ever increasing hate speech online\nwithout blocking freedom of speech is of great social interest. Natural\nLanguage Generation (NLG), is uniquely capable of developing scalable\nsolutions. However, off-the-shelf NLG methods are primarily\nsequence-to-sequence neural models and they are limited in that they generate\ncommonplace, repetitive and safe responses regardless of the hate speech (e.g.,\n\"Please refrain from using such language.\") or irrelevant responses, making\nthem ineffective for de-escalating hateful conversations. In this paper, we\ndesign a three-module pipeline approach to effectively improve the diversity\nand relevance. Our proposed pipeline first generates various counterspeech\ncandidates by a generative model to promote diversity, then filters the\nungrammatical ones using a BERT model, and finally selects the most relevant\ncounterspeech response using a novel retrieval-based method. Extensive\nExperiments on three representative datasets demonstrate the efficacy of our\napproach in generating diverse and relevant counterspeech.", "published": "2021-06-03 06:54:03", "link": "http://arxiv.org/abs/2106.01625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LearnDA: Learnable Knowledge-Guided Data Augmentation for Event\n  Causality Identification", "abstract": "Modern models for event causality identification (ECI) are mainly based on\nsupervised learning, which are prone to the data lacking problem.\nUnfortunately, the existing NLP-related augmentation methods cannot directly\nproduce the available data required for this task. To solve the data lacking\nproblem, we introduce a new approach to augment training data for event\ncausality identification, by iteratively generating new examples and\nclassifying event causality in a dual learning framework. On the one hand, our\napproach is knowledge-guided, which can leverage existing knowledge bases to\ngenerate well-formed new sentences. On the other hand, our approach employs a\ndual mechanism, which is a learnable augmentation framework and can\ninteractively adjust the generation process to generate task-related sentences.\nExperimental results on two benchmarks EventStoryLine and Causal-TimeBank show\nthat 1) our method can augment suitable task-related training data for ECI; 2)\nour method outperforms previous methods on EventStoryLine and Causal-TimeBank\n(+2.5 and +2.1 points on F1 value respectively).", "published": "2021-06-03 07:42:20", "link": "http://arxiv.org/abs/2106.01649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Event Causality Identification via Self-Supervised\n  Representation Learning on External Causal Statement", "abstract": "Current models for event causality identification (ECI) mainly adopt a\nsupervised framework, which heavily rely on labeled data for training.\nUnfortunately, the scale of current annotated datasets is relatively limited,\nwhich cannot provide sufficient support for models to capture useful indicators\nfrom causal statements, especially for handing those new, unseen cases. To\nalleviate this problem, we propose a novel approach, shortly named CauSeRL,\nwhich leverages external causal statements for event causality identification.\nFirst of all, we design a self-supervised framework to learn context-specific\ncausal patterns from external causal statements. Then, we adopt a contrastive\ntransfer strategy to incorporate the learned context-specific causal patterns\ninto the target ECI model. Experimental results show that our method\nsignificantly outperforms previous methods on EventStoryLine and\nCausal-TimeBank (+2.0 and +3.4 points on F1 value respectively).", "published": "2021-06-03 07:50:50", "link": "http://arxiv.org/abs/2106.01654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental\n  Health Support", "abstract": "Great research interests have been attracted to devise AI services that are\nable to provide mental health support. However, the lack of corpora is a main\nobstacle to this research, particularly in Chinese language. In this paper, we\npropose PsyQA, a Chinese dataset of psychological health support in the form of\nquestion and answer pair. PsyQA is crawled from a Chinese mental health service\nplatform, and contains 22K questions and 56K long and well-structured answers.\nBased on the psychological counseling theories, we annotate a portion of answer\ntexts with typical strategies for providing support, and further present\nin-depth analysis of both lexical features and strategy patterns in the\ncounseling answers. We also evaluate the performance of generating counseling\nanswers with the generative pretrained models. Results show that utilizing\nstrategies enhances the fluency and helpfulness of generated answers, but there\nis still a large space for future research.", "published": "2021-06-03 09:06:25", "link": "http://arxiv.org/abs/2106.01702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer", "abstract": "Multilingual pre-trained models have achieved remarkable performance on\ncross-lingual transfer learning. Some multilingual models such as mBERT, have\nbeen pre-trained on unlabeled corpora, therefore the embeddings of different\nlanguages in the models may not be aligned very well. In this paper, we aim to\nimprove the zero-shot cross-lingual transfer performance by proposing a\npre-training task named Word-Exchange Aligning Model (WEAM), which uses the\nstatistical alignment information as the prior knowledge to guide cross-lingual\nword prediction. We evaluate our model on multilingual machine reading\ncomprehension task MLQA and natural language interface task XNLI. The results\nshow that WEAM can significantly improve the zero-shot performance.", "published": "2021-06-03 10:18:43", "link": "http://arxiv.org/abs/2106.01732v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reordering Examples Helps during Priming-based Few-Shot Learning", "abstract": "The ability to learn from limited data, or few-shot learning, is a desirable\nand often critical requirement for NLP systems. While many existing methods do\npoorly at learning from a handful of examples, large pretrained language models\nhave recently been shown to be efficient few-shot learners. One approach to\nfew-shot learning, which does not require finetuning of model parameters, is to\naugment the language model's input with priming text which is typically\nconstructed using task specific descriptions and examples. In this work, we\nfurther explore priming-based few-shot learning, with focus on using examples\nas prompts. We show that presenting examples in the right order is key for\ngeneralization. We introduce PERO (Prompting with Examples in the Right Order),\nwhere we formulate few-shot learning as search over the set of permutations of\nthe training examples. We show that PERO can learn to generalize efficiently\nusing as few as 10 examples, in contrast to existing approaches. While the\nnewline token is a natural choice for separating the examples in the prompt, we\nshow that learning a new separator token can potentially provide further gains\nin performance. We demonstrate the effectiveness of the proposed method on the\ntasks of sentiment classification, natural language inference and fact\nretrieval. Finally, we analyze the learned prompts to reveal novel insights,\nincluding the idea that two training examples in the right order alone can\nprovide competitive performance for sentiment classification and natural\nlanguage inference.", "published": "2021-06-03 11:02:36", "link": "http://arxiv.org/abs/2106.01751v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Template-Based Named Entity Recognition Using BART", "abstract": "There is a recent interest in investigating few-shot NER, where the\nlow-resource target domain has different label sets compared with a\nresource-rich source domain. Existing methods use a similarity-based metric.\nHowever, they cannot make full use of knowledge transfer in NER model\nparameters. To address the issue, we propose a template-based method for NER,\ntreating NER as a language model ranking problem in a sequence-to-sequence\nframework, where original sentences and statement templates filled by candidate\nnamed entity span are regarded as the source sequence and the target sequence,\nrespectively. For inference, the model is required to classify each candidate\nspan based on the corresponding template scores. Our experiments demonstrate\nthat the proposed method achieves 92.55% F1 score on the CoNLL03 (rich-resource\ntask), and significantly better than fine-tuning BERT 10.88%, 15.34%, and\n11.73% F1 score on the MIT Movie, the MIT Restaurant, and the ATIS\n(low-resource task), respectively.", "published": "2021-06-03 11:29:43", "link": "http://arxiv.org/abs/2106.01760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Three Sentences Are All You Need: Local Path Enhanced Document Relation\n  Extraction", "abstract": "Document-level Relation Extraction (RE) is a more challenging task than\nsentence RE as it often requires reasoning over multiple sentences. Yet, human\nannotators usually use a small number of sentences to identify the relationship\nbetween a given entity pair. In this paper, we present an embarrassingly simple\nbut effective method to heuristically select evidence sentences for\ndocument-level RE, which can be easily combined with BiLSTM to achieve good\nperformance on benchmark datasets, even better than fancy graph neural network\nbased methods. We have released our code at\nhttps://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need.", "published": "2021-06-03 12:29:40", "link": "http://arxiv.org/abs/2106.01793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data", "abstract": "Among ubiquitous multimodal data in the real world, text is the modality\ngenerated by human, while image reflects the physical world honestly. In a\nvisual understanding application, machines are expected to understand images\nlike human. Inspired by this, we propose a novel self-supervised learning\nmethod, named Text-enhanced Visual Deep InfoMax (TVDIM), to learn better visual\nrepresentations by fully utilizing the naturally-existing multimodal data. Our\ncore idea of self-supervised learning is to maximize the mutual information\nbetween features extracted from multiple views of a shared context to a\nrational degree. Different from previous methods which only consider multiple\nviews from a single modality, our work produces multiple views from different\nmodalities, and jointly optimizes the mutual information for features pairs of\nintra-modality and inter-modality. Considering the information gap between\ninter-modality features pairs from data noise, we adopt a \\emph{ranking-based}\ncontrastive learning to optimize the mutual information. During evaluation, we\ndirectly use the pre-trained visual representations to complete various image\nclassification tasks. Experimental results show that, TVDIM significantly\noutperforms previous visual self-supervised methods when processing the same\nset of images.", "published": "2021-06-03 12:36:01", "link": "http://arxiv.org/abs/2106.01797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Distantly-Labeled Rationales in Neural Network Models", "abstract": "Recent studies strive to incorporate various human rationales into neural\nnetworks to improve model performance, but few pay attention to the quality of\nthe rationales. Most existing methods distribute their models' focus to\ndistantly-labeled rationale words entirely and equally, while ignoring the\npotential important non-rationale words and not distinguishing the importance\nof different rationale words. In this paper, we propose two novel auxiliary\nloss functions to make better use of distantly-labeled rationales, which\nencourage models to maintain their focus on important words beyond labeled\nrationales (PINs) and alleviate redundant training on non-helpful rationales\n(NoIRs). Experiments on two representative classification tasks show that our\nproposed methods can push a classification model to effectively learn crucial\nclues from non-perfect rationales while maintaining the ability to spread its\nfocus to other unlabeled important words, thus significantly outperform\nexisting methods.", "published": "2021-06-03 13:00:25", "link": "http://arxiv.org/abs/2106.01809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defending Against Backdoor Attacks in Natural Language Generation", "abstract": "The frustratingly fragile nature of neural network models make current\nnatural language generation (NLG) systems prone to backdoor attacks and\ngenerate malicious sequences that could be sexist or offensive. Unfortunately,\nlittle effort has been invested to how backdoor attacks can affect current NLG\nmodels and how to defend against these attacks. In this work, by giving a\nformal definition of backdoor attack and defense, we investigate this problem\non two important NLG tasks, machine translation and dialog generation. Tailored\nto the inherent nature of NLG models (e.g., producing a sequence of coherent\nwords given contexts), we design defending strategies against attacks. We find\nthat testing the backward probability of generating sources given targets\nyields effective defense performance against all different types of attacks,\nand is able to handle the {\\it one-to-many} issue in many NLG tasks such as\ndialog generation. We hope that this work can raise the awareness of backdoor\nrisks concealed in deep NLG systems and inspire more future work (both attack\nand defense) towards this direction.", "published": "2021-06-03 13:00:28", "link": "http://arxiv.org/abs/2106.01810v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimCLS: A Simple Framework for Contrastive Learning of Abstractive\n  Summarization", "abstract": "In this paper, we present a conceptually simple while empirically powerful\nframework for abstractive summarization, SimCLS, which can bridge the gap\nbetween the learning objective and evaluation metrics resulting from the\ncurrently dominated sequence-to-sequence learning framework by formulating text\ngeneration as a reference-free evaluation problem (i.e., quality estimation)\nassisted by contrastive learning. Experimental results show that, with minor\nmodification over existing top-scoring systems, SimCLS can improve the\nperformance of existing top-performing models by a large margin. Particularly,\n2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on\nthe CNN/DailyMail dataset, driving the state-of-the-art performance to a new\nlevel. We have open-sourced our codes and results:\nhttps://github.com/yixinL7/SimCLS. Results of our proposed models have been\ndeployed into ExplainaBoard platform, which allows researchers to understand\nour systems in a more fine-grained way.", "published": "2021-06-03 14:34:17", "link": "http://arxiv.org/abs/2106.01890v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representing Syntax and Composition with Geometric Transformations", "abstract": "The exploitation of syntactic graphs (SyGs) as a word's context has been\nshown to be beneficial for distributional semantic models (DSMs), both at the\nlevel of individual word representations and in deriving phrasal\nrepresentations via composition. However, notwithstanding the potential\nperformance benefit, the syntactically-aware DSMs proposed to date have huge\nnumbers of parameters (compared to conventional DSMs) and suffer from data\nsparsity. Furthermore, the encoding of the SyG links (i.e., the syntactic\nrelations) has been largely limited to linear maps. The knowledge graphs'\nliterature, on the other hand, has proposed light-weight models employing\ndifferent geometric transformations (GTs) to encode edges in a knowledge graph\n(KG). Our work explores the possibility of adopting this family of models to\nencode SyGs. Furthermore, we investigate which GT better encodes syntactic\nrelations, so that these representations can be used to enhance phrase-level\ncomposition via syntactic contextualisation.", "published": "2021-06-03 14:53:34", "link": "http://arxiv.org/abs/2106.01904v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple\n  Intent Detection and Slot Filling", "abstract": "Multi-intent SLU can handle multiple intents in an utterance, which has\nattracted increasing attention. However, the state-of-the-art joint models\nheavily rely on autoregressive approaches, resulting in two issues: slow\ninference speed and information leakage. In this paper, we explore a\nnon-autoregressive model for joint multiple intent detection and slot filling,\nachieving more fast and accurate. Specifically, we propose a Global-Locally\nGraph Interaction Network (GL-GIN) where a local slot-aware graph interaction\nlayer is proposed to model slot dependency for alleviating uncoordinated slots\nproblem while a global intent-slot graph interaction layer is introduced to\nmodel the interaction between multiple intents and all slots in the utterance.\nExperimental results on two public datasets show that our framework achieves\nstate-of-the-art performance while being 11.5 times faster.", "published": "2021-06-03 15:22:38", "link": "http://arxiv.org/abs/2106.01925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SOCCER: An Information-Sparse Discourse State Tracking Collection in the\n  Sports Commentary Domain", "abstract": "In the pursuit of natural language understanding, there has been a long\nstanding interest in tracking state changes throughout narratives. Impressive\nprogress has been made in modeling the state of transaction-centric dialogues\nand procedural texts. However, this problem has been less intensively studied\nin the realm of general discourse where ground truth descriptions of states may\nbe loosely defined and state changes are less densely distributed over\nutterances. This paper proposes to turn to simplified, fully observable systems\nthat show some of these properties: Sports events. We curated 2,263 soccer\nmatches including time-stamped natural language commentary accompanied by\ndiscrete events such as a team scoring goals, switching players or being\npenalized with cards. We propose a new task formulation where, given paragraphs\nof commentary of a game at different timestamps, the system is asked to\nrecognize the occurrence of in-game events. This domain allows for rich\ndescriptions of state while avoiding the complexities of many other real-world\nsettings. As an initial point of performance measurement, we include two\nbaseline methods from the perspectives of sentence classification with temporal\ndependence and current state-of-the-art generative model, respectively, and\ndemonstrate that even sophisticated existing methods struggle on the state\ntracking task when the definition of state broadens or non-event chatter\nbecomes prevalent.", "published": "2021-06-03 16:21:13", "link": "http://arxiv.org/abs/2106.01972v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CCPM: A Chinese Classical Poetry Matching Dataset", "abstract": "Poetry is one of the most important art forms of human languages. Recently\nmany studies have focused on incorporating some linguistic features of poetry,\nsuch as style and sentiment, into its understanding or generation system.\nHowever, there is no focus on understanding or evaluating the semantics of\npoetry. Therefore, we propose a novel task to assess a model's semantic\nunderstanding of poetry by poem matching. Specifically, this task requires the\nmodel to select one line of Chinese classical poetry among four candidates\naccording to the modern Chinese translation of a line of poetry. To construct\nthis dataset, we first obtain a set of parallel data of Chinese classical\npoetry and modern Chinese translation. Then we retrieve similar lines of poetry\nwith the lines in a poetry corpus as negative choices. We name the dataset\nChinese Classical Poetry Matching Dataset (CCPM) and release it at\nhttps://github.com/THUNLP-AIPoet/CCPM. We hope this dataset can further enhance\nthe study on incorporating deep semantics into the understanding and generation\nsystem of Chinese classical poetry. We also preliminarily run two variants of\nBERT on this dataset as the baselines for this dataset.", "published": "2021-06-03 16:49:03", "link": "http://arxiv.org/abs/2106.01979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Case Study of Spanish Text Transformations for Twitter Sentiment\n  Analysis", "abstract": "Sentiment analysis is a text mining task that determines the polarity of a\ngiven text, i.e., its positiveness or negativeness. Recently, it has received a\nlot of attention given the interest in opinion mining in micro-blogging\nplatforms. These new forms of textual expressions present new challenges to\nanalyze text given the use of slang, orthographic and grammatical errors, among\nothers. Along with these challenges, a practical sentiment classifier should be\nable to handle efficiently large workloads.\n  The aim of this research is to identify which text transformations\n(lemmatization, stemming, entity removal, among others), tokenizers (e.g.,\nwords $n$-grams), and tokens weighting schemes impact the most the accuracy of\na classifier (Support Vector Machine) trained on two Spanish corpus. The\nmethodology used is to exhaustively analyze all the combinations of the text\ntransformations and their respective parameters to find out which\ncharacteristics the best performing classifiers have in common. Furthermore,\namong the different text transformations studied, we introduce a novel approach\nbased on the combination of word based $n$-grams and character based $q$-grams.\nThe results show that this novel combination of words and characters produces a\nclassifier that outperforms the traditional word based combination by $11.17\\%$\nand $5.62\\%$ on the INEGI and TASS'15 dataset, respectively.", "published": "2021-06-03 17:24:31", "link": "http://arxiv.org/abs/2106.02009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Embeddings for Typology and Cross-lingual Transfer Learning", "abstract": "Cross-lingual language tasks typically require a substantial amount of\nannotated data or parallel translation data. We explore whether language\nrepresentations that capture relationships among languages can be learned and\nsubsequently leveraged in cross-lingual tasks without the use of parallel data.\nWe generate dense embeddings for 29 languages using a denoising autoencoder,\nand evaluate the embeddings using the World Atlas of Language Structures (WALS)\nand two extrinsic tasks in a zero-shot setting: cross-lingual dependency\nparsing and cross-lingual natural language inference.", "published": "2021-06-03 19:00:02", "link": "http://arxiv.org/abs/2106.02082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A diachronic evaluation of gender asymmetry in euphemism", "abstract": "The use of euphemisms is a known driver of language change. It has been\nproposed that women use euphemisms more than men. Although there have been\nseveral studies investigating gender differences in language, the claim about\neuphemism usage has not been tested comprehensively through time. If women do\nuse euphemisms more, this could mean that women also lead the formation of new\neuphemisms and language change over time. Using four large diachronic text\ncorpora of English, we evaluate the claim that women use euphemisms more than\nmen through a quantitative analysis. We assembled a list of 106 euphemism-taboo\npairs to analyze their relative use through time by each gender in the corpora.\nContrary to the existing belief, our results show that women do not use\neuphemisms with a higher proportion than men. We repeated the analysis using\ndifferent subsets of the euphemism-taboo pairs list and found that our result\nwas robust. Our study indicates that in a broad range of settings involving\nboth speech and writing, and with varying degrees of formality, women do not\nuse or form euphemisms more than men.", "published": "2021-06-03 19:00:11", "link": "http://arxiv.org/abs/2106.02083v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Adapt Your Pretrained Multilingual Model to 1600 Languages", "abstract": "Pretrained multilingual models (PMMs) enable zero-shot learning via\ncross-lingual transfer, performing best for languages seen during pretraining.\nWhile methods exist to improve performance for unseen languages, they have\nalmost exclusively been evaluated using amounts of raw text only available for\na small fraction of the world's languages. In this paper, we evaluate the\nperformance of existing methods to adapt PMMs to new languages using a resource\navailable for over 1600 languages: the New Testament. This is challenging for\ntwo reasons: (1) the small corpus size, and (2) the narrow domain. While\nperformance drops for all approaches, we surprisingly still see gains of up to\n$17.69\\%$ accuracy for part-of-speech tagging and $6.29$ F1 for NER on average\nover all languages as compared to XLM-R. Another unexpected finding is that\ncontinued pretraining, the simplest approach, performs best. Finally, we\nperform a case study to disentangle the effects of domain and size and to shed\nlight on the influence of the finetuning source language.", "published": "2021-06-03 20:50:02", "link": "http://arxiv.org/abs/2106.02124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-augmented Multilingual BERT for Cross-lingual Transfer", "abstract": "In recent years, we have seen a colossal effort in pre-training multilingual\ntext encoders using large-scale corpora in many languages to facilitate\ncross-lingual transfer learning. However, due to typological differences across\nlanguages, the cross-lingual transfer is challenging. Nevertheless, language\nsyntax, e.g., syntactic dependencies, can bridge the typological gap. Previous\nworks have shown that pre-trained multilingual encoders, such as mBERT\n\\cite{devlin-etal-2019-bert}, capture language syntax, helping cross-lingual\ntransfer. This work shows that explicitly providing language syntax and\ntraining mBERT using an auxiliary objective to encode the universal dependency\ntree structure helps cross-lingual transfer. We perform rigorous experiments on\nfour NLP tasks, including text classification, question answering, named entity\nrecognition, and task-oriented semantic parsing. The experiment results show\nthat syntax-augmented mBERT improves cross-lingual transfer on popular\nbenchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across\nall languages. In the \\emph{generalized} transfer setting, the performance\nboosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.", "published": "2021-06-03 21:12:50", "link": "http://arxiv.org/abs/2106.02134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "nmT5 -- Is parallel data still relevant for pre-training massively\n  multilingual language models?", "abstract": "Recently, mT5 - a massively multilingual version of T5 - leveraged a unified\ntext-to-text format to attain state-of-the-art results on a wide variety of\nmultilingual NLP tasks. In this paper, we investigate the impact of\nincorporating parallel data into mT5 pre-training. We find that multi-tasking\nlanguage modeling with objectives such as machine translation during\npre-training is a straightforward way to improve performance on downstream\nmultilingual and cross-lingual tasks. However, the gains start to diminish as\nthe model capacity increases, suggesting that parallel data might not be as\nessential for larger models. At the same time, even at larger model sizes, we\nfind that pre-training with parallel data still provides benefits in the\nlimited labelled data regime.", "published": "2021-06-03 23:12:27", "link": "http://arxiv.org/abs/2106.02171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Luna: Linear Unified Nested Attention", "abstract": "The quadratic computational and memory complexities of the Transformer's\nattention mechanism have limited its scalability for modeling long sequences.\nIn this paper, we propose Luna, a linear unified nested attention mechanism\nthat approximates softmax attention with two nested linear attention functions,\nyielding only linear (as opposed to quadratic) time and space complexity.\nSpecifically, with the first attention function, Luna packs the input sequence\ninto a sequence of fixed length. Then, the packed sequence is unpacked using\nthe second attention function. As compared to a more traditional attention\nmechanism, Luna introduces an additional sequence with a fixed length as input\nand an additional corresponding output, which allows Luna to perform attention\noperation linearly, while also storing adequate contextual information. We\nperform extensive evaluations on three benchmarks of sequence modeling tasks:\nlong-context sequence modeling, neural machine translation and masked language\nmodeling for large-scale pretraining. Competitive or even better experimental\nresults demonstrate both the effectiveness and efficiency of Luna compared to a\nvariety", "published": "2021-06-03 01:47:26", "link": "http://arxiv.org/abs/2106.01540v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CitationIE: Leveraging the Citation Graph for Scientific Information\n  Extraction", "abstract": "Automatically extracting key information from scientific documents has the\npotential to help scientists work more efficiently and accelerate the pace of\nscientific progress. Prior work has considered extracting document-level entity\nclusters and relations end-to-end from raw scientific text, which can improve\nliterature search and help identify methods and materials for a given problem.\nDespite the importance of this task, most existing works on scientific\ninformation extraction (SciIE) consider extraction solely based on the content\nof an individual paper, without considering the paper's place in the broader\nliterature. In contrast to prior work, we augment our text representations by\nleveraging a complementary source of document context: the citation graph of\nreferential links between citing and cited papers. On a test set of\nEnglish-language scientific documents, we show that simple ways of utilizing\nthe structure and content of the citation graph can each lead to significant\ngains in different scientific information extraction tasks. When these tasks\nare combined, we observe a sizable improvement in end-to-end information\nextraction over the state-of-the-art, suggesting the potential for future work\nalong this direction. We release software tools to facilitate citation-aware\nSciIE development.", "published": "2021-06-03 03:00:12", "link": "http://arxiv.org/abs/2106.01560v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "The Limitations of Limited Context for Constituency Parsing", "abstract": "Incorporating syntax into neural approaches in NLP has a multitude of\npractical and scientific benefits. For instance, a language model that is\nsyntax-aware is likely to be able to produce better samples; even a\ndiscriminative model like BERT with a syntax module could be used for core NLP\ntasks like unsupervised syntactic parsing. Rapid progress in recent years was\narguably spurred on by the empirical success of the Parsing-Reading-Predict\narchitecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM\nof (Shen et al., 2019). Most notably, this is the first time neural approaches\nwere able to successfully perform unsupervised syntactic parsing (evaluated by\nvarious metrics like F-1 score).\n  However, even heuristic (much less fully mathematical) understanding of why\nand when these architectures work is lagging severely behind. In this work, we\nanswer representational questions raised by the architectures in (Shen et al.,\n2018a, 2019), as well as some transition-based syntax-aware language models\n(Dyer et al., 2016): what kind of syntactic structure can current neural\napproaches to syntax represent? Concretely, we ground this question in the\nsandbox of probabilistic context-free-grammars (PCFGs), and identify a key\naspect of the representational power of these approaches: the amount and\ndirectionality of context that the predictor has access to when forced to make\nparsing decision. We show that with limited context (either bounded, or\nunidirectional), there are PCFGs, for which these approaches cannot represent\nthe max-likelihood parse; conversely, if the context is unlimited, they can\nrepresent the max-likelihood parse of any PCFG.", "published": "2021-06-03 03:58:35", "link": "http://arxiv.org/abs/2106.01580v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Systematic Investigation of KB-Text Embedding Alignment at Scale", "abstract": "Knowledge bases (KBs) and text often contain complementary knowledge: KBs\nstore structured knowledge that can support long range reasoning, while text\nstores more comprehensive and timely knowledge in an unstructured way.\nSeparately embedding the individual knowledge sources into vector spaces has\ndemonstrated tremendous successes in encoding the respective knowledge, but how\nto jointly embed and reason with both knowledge sources to fully leverage the\ncomplementary information is still largely an open problem. We conduct a\nlarge-scale, systematic investigation of aligning KB and text embeddings for\njoint reasoning. We set up a novel evaluation framework with two evaluation\ntasks, few-shot link prediction and analogical reasoning, and evaluate an array\nof KB-text embedding alignment methods. We also demonstrate how such alignment\ncan infuse textual information into KB embeddings for more accurate link\nprediction on emerging entities and events, using COVID-19 as a case study.", "published": "2021-06-03 04:14:11", "link": "http://arxiv.org/abs/2106.01586v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese\n  Grammatical Error Correction", "abstract": "We investigate the problem of Chinese Grammatical Error Correction (CGEC) and\npresent a new framework named Tail-to-Tail (\\textbf{TtT}) non-autoregressive\nsequence prediction to address the deep issues hidden in CGEC. Considering that\nmost tokens are correct and can be conveyed directly from source to target, and\nthe error positions can be estimated and corrected based on the bidirectional\ncontext information, thus we employ a BERT-initialized Transformer Encoder as\nthe backbone model to conduct information modeling and conveying. Considering\nthat only relying on the same position substitution cannot handle the\nvariable-length correction cases, various operations such substitution,\ndeletion, insertion, and local paraphrasing are required jointly. Therefore, a\nConditional Random Fields (CRF) layer is stacked on the up tail to conduct\nnon-autoregressive sequence prediction by modeling the token dependencies.\nSince most tokens are correct and easily to be predicted/conveyed to the\ntarget, then the models may suffer from a severe class imbalance issue. To\nalleviate this problem, focal loss penalty strategies are integrated into the\nloss functions. Moreover, besides the typical fix-length error correction\ndatasets, we also construct a variable-length corpus to conduct experiments.\nExperimental results on standard datasets, especially on the variable-length\ndatasets, demonstrate the effectiveness of TtT in terms of sentence-level\nAccuracy, Precision, Recall, and F1-Measure on tasks of error Detection and\nCorrection.", "published": "2021-06-03 05:56:57", "link": "http://arxiv.org/abs/2106.01609v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can vectors read minds better than experts? Comparing data augmentation\n  strategies for the automated scoring of children's mindreading ability", "abstract": "In this paper we implement and compare 7 different data augmentation\nstrategies for the task of automatic scoring of children's ability to\nunderstand others' thoughts, feelings, and desires (or \"mindreading\").\n  We recruit in-domain experts to re-annotate augmented samples and determine\nto what extent each strategy preserves the original rating. We also carry out\nmultiple experiments to measure how much each augmentation strategy improves\nthe performance of automatic scoring systems. To determine the capabilities of\nautomatic systems to generalize to unseen data, we create UK-MIND-20 - a new\ncorpus of children's performance on tests of mindreading, consisting of 10,320\nquestion-answer pairs.\n  We obtain a new state-of-the-art performance on the MIND-CA corpus, improving\nmacro-F1-score by 6 points. Results indicate that both the number of training\nexamples and the quality of the augmentation strategies affect the performance\nof the systems. The task-specific augmentations generally outperform\ntask-agnostic augmentations. Automatic augmentations based on vectors (GloVe,\nFastText) perform the worst.\n  We find that systems trained on MIND-CA generalize well to UK-MIND-20. We\ndemonstrate that data augmentation strategies also improve the performance on\nunseen data.", "published": "2021-06-03 07:12:00", "link": "http://arxiv.org/abs/2106.01635v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dialoging Resonance: How Users Perceive, Reciprocate and React to\n  Chatbot's Self-Disclosure in Conversational Recommendations", "abstract": "Using chatbots to deliver recommendations is increasingly popular. The design\nof recommendation chatbots has primarily been taking an information-centric\napproach by focusing on the recommended content per se. Limited attention is on\nhow social connection and relational strategies, such as self-disclosure from a\nchatbot, may influence users' perception and acceptance of the recommendation.\nIn this work, we designed, implemented, and evaluated a social chatbot capable\nof performing three different levels of self-disclosure: factual information\n(low), cognitive opinions (medium), and emotions (high). In the evaluation, we\nrecruited 372 participants to converse with the chatbot on two topics: movies\nand COVID-19 experiences. In each topic, the chatbot performed small talks and\nmade recommendations relevant to the topic. Participants were randomly assigned\nto four experimental conditions where the chatbot used factual, cognitive,\nemotional, and adaptive strategies to perform self-disclosures. By training a\ntext classifier to identify users' level of self-disclosure in real-time, the\nadaptive chatbot can dynamically match its self-disclosure to the level of\ndisclosure exhibited by the users. Our results show that users reciprocate with\nhigher-level self-disclosure when a recommendation chatbot consistently\ndisplays emotions throughout the conversation. Chatbot's emotional disclosure\nalso led to increased interactional enjoyment and more positive interpersonal\nperception towards the bot, fostering a stronger human-chatbot relationship and\nthus leading to increased recommendation effectiveness, including a higher\ntendency to accept the recommendation. We discuss the understandings obtained\nand implications to future design.", "published": "2021-06-03 08:16:25", "link": "http://arxiv.org/abs/2106.01666v2", "categories": ["cs.CL", "cs.AI", "I.2"], "primary_category": "cs.CL"}
{"title": "Auto-tagging of Short Conversational Sentences using Transformer Methods", "abstract": "The problem of categorizing short speech sentences according to their\nsemantic features with high accuracy is a subject studied in natural language\nprocessing. In this study, a data set created with samples classified in 46\ndifferent categories was used. Examples consist of sentences taken from chat\nconversations between a company's customer representatives and the company's\nwebsite visitors. The primary purpose is to automatically tag questions and\nrequests from visitors in the most accurate way for 46 predetermined categories\nfor use in a chat application to generate meaningful answers to the questions\nasked by the website visitors. For this, different BERT models and one GPT-2\nmodel, pre-trained in Turkish, were preferred. The classification performances\nof the relevant models were analyzed in detail and reported accordingly.", "published": "2021-06-03 10:23:58", "link": "http://arxiv.org/abs/2106.01735v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in\n  Conversations", "abstract": "Emotion Recognition in Conversations (ERC) has gained increasing attention\nfor developing empathetic machines. Recently, many approaches have been devoted\nto perceiving conversational context by deep learning models. However, these\napproaches are insufficient in understanding the context due to lacking the\nability to extract and integrate emotional clues. In this work, we propose\nnovel Contextual Reasoning Networks (DialogueCRN) to fully understand the\nconversational context from a cognitive perspective. Inspired by the Cognitive\nTheory of Emotion, we design multi-turn reasoning modules to extract and\nintegrate emotional clues. The reasoning module iteratively performs an\nintuitive retrieving process and a conscious reasoning process, which imitates\nhuman unique cognitive thinking. Extensive experiments on three public\nbenchmark datasets demonstrate the effectiveness and superiority of the\nproposed model.", "published": "2021-06-03 16:47:38", "link": "http://arxiv.org/abs/2106.01978v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Provably Secure Generative Linguistic Steganography", "abstract": "Generative linguistic steganography mainly utilized language models and\napplied steganographic sampling (stegosampling) to generate high-security\nsteganographic text (stegotext). However, previous methods generally lead to\nstatistical differences between the conditional probability distributions of\nstegotext and natural text, which brings about security risks. In this paper,\nto further ensure security, we present a novel provably secure generative\nlinguistic steganographic method ADG, which recursively embeds secret\ninformation by Adaptive Dynamic Grouping of tokens according to their\nprobability given by an off-the-shelf language model. We not only prove the\nsecurity of ADG mathematically, but also conduct extensive experiments on three\npublic corpora to further verify its imperceptibility. The experimental results\nreveal that the proposed method is able to generate stegotext with nearly\nperfect security.", "published": "2021-06-03 17:27:10", "link": "http://arxiv.org/abs/2106.02011v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "A Dataset and Baselines for Multilingual Reply Suggestion", "abstract": "Reply suggestion models help users process emails and chats faster. Previous\nwork only studies English reply suggestion. Instead, we present MRS, a\nmultilingual reply suggestion dataset with ten languages. MRS can be used to\ncompare two families of models: 1) retrieval models that select the reply from\na fixed set and 2) generation models that produce the reply from scratch.\nTherefore, MRS complements existing cross-lingual generalization benchmarks\nthat focus on classification and sequence labeling tasks. We build a generation\nmodel and a retrieval model as baselines for MRS. The two models have different\nstrengths in the monolingual setting, and they require different strategies to\ngeneralize across languages. MRS is publicly available at\nhttps://github.com/zhangmozhi/mrs.", "published": "2021-06-03 17:36:32", "link": "http://arxiv.org/abs/2106.02017v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LGBTQ-AI? Exploring Expressions of Gender and Sexual Orientation in\n  Chatbots", "abstract": "Chatbots are popular machine partners for task-oriented and social\ninteractions. Human-human computer-mediated communication research has explored\nhow people express their gender and sexuality in online social interactions,\nbut little is known about whether and in what way chatbots do the same. We\nconducted semi-structured interviews with 5 text-based conversational agents to\nexplore this topic Through these interviews, we identified 6 common themes\naround the expression of gender and sexual identity: identity description,\nidentity formation, peer acceptance, positive reflection, uncomfortable\nfeelings and off-topic responses. Chatbots express gender and sexuality\nexplicitly and through relation of experience and emotions, mimicking the human\nlanguage on which they are trained. It is nevertheless evident that chatbots\ndiffer from human dialogue partners as they lack the flexibility and\nunderstanding enabled by lived human experience. While chatbots are proficient\nin using language to express identity, they also display a lack of authentic\nexperiences of gender and sexuality.", "published": "2021-06-03 18:47:52", "link": "http://arxiv.org/abs/2106.02076v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Self-Guided Contrastive Learning for BERT Sentence Representations", "abstract": "Although BERT and its variants have reshaped the NLP landscape, it still\nremains unclear how best to derive sentence embeddings from such pre-trained\nTransformers. In this work, we propose a contrastive learning method that\nutilizes self-guidance for improving the quality of BERT sentence\nrepresentations. Our method fine-tunes BERT in a self-supervised fashion, does\nnot rely on data augmentation, and enables the usual [CLS] token embeddings to\nfunction as sentence vectors. Moreover, we redesign the contrastive learning\nobjective (NT-Xent) and apply it to sentence representation learning. We\ndemonstrate with extensive experiments that our approach is more effective than\ncompetitive baselines on diverse sentence-related tasks. We also show it is\nefficient at inference and robust to domain shifts.", "published": "2021-06-03 05:52:43", "link": "http://arxiv.org/abs/2106.07345v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparing Acoustic-based Approaches for Alzheimer's Disease Detection", "abstract": "Robust strategies for Alzheimer's disease (AD) detection are important, given\nthe high prevalence of AD. In this paper, we study the performance and\ngeneralizability of three approaches for AD detection from speech on the recent\nADReSSo challenge dataset: 1) using conventional acoustic features 2) using\nnovel pre-trained acoustic embeddings 3) combining acoustic features and\nembeddings. We find that while feature-based approaches have a higher\nprecision, classification approaches relying on pre-trained embeddings prove to\nhave a higher, and more balanced cross-validated performance across multiple\nmetrics of performance. Further, embedding-only approaches are more\ngeneralizable. Our best model outperforms the acoustic baseline in the\nchallenge by 2.8%.", "published": "2021-06-03 02:44:40", "link": "http://arxiv.org/abs/2106.01555v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language\n  Generation", "abstract": "Despite the recent advancement in NLP research, cross-lingual transfer for\nnatural language generation is relatively understudied. In this work, we\ntransfer supervision from high resource language (HRL) to multiple low-resource\nlanguages (LRLs) for natural language generation (NLG). We consider four NLG\ntasks (text summarization, question generation, news headline generation, and\ndistractor generation) and three syntactically diverse languages, i.e.,\nEnglish, Hindi, and Japanese. We propose an unsupervised cross-lingual language\ngeneration framework (called ZmBART) that does not use any parallel or\npseudo-parallel/back-translated data. In this framework, we further pre-train\nmBART sequence-to-sequence denoising auto-encoder model with an auxiliary task\nusing monolingual data of three languages. The objective function of the\nauxiliary task is close to the target tasks which enriches the multi-lingual\nlatent representation of mBART and provides good initialization for target\ntasks. Then, this model is fine-tuned with task-specific supervised English\ndata and directly evaluated with low-resource languages in the Zero-shot\nsetting. To overcome catastrophic forgetting and spurious correlation issues,\nwe applied freezing model component and data argumentation approaches\nrespectively. This simple modeling approach gave us promising results.We\nexperimented with few-shot training (with 1000 supervised data points) which\nboosted the model performance further. We performed several ablations and\ncross-lingual transferability analyses to demonstrate the robustness of ZmBART.", "published": "2021-06-03 05:08:01", "link": "http://arxiv.org/abs/2106.01597v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia", "abstract": "Human activities can be seen as sequences of events, which are crucial to\nunderstanding societies. Disproportional event distribution for different\ndemographic groups can manifest and amplify social stereotypes, and potentially\njeopardize the ability of members in some groups to pursue certain goals. In\nthis paper, we present the first event-centric study of gender biases in a\nWikipedia corpus. To facilitate the study, we curate a corpus of career and\npersonal life descriptions with demographic information consisting of 7,854\nfragments from 10,412 celebrities. Then we detect events with a\nstate-of-the-art event detection model, calibrate the results using\nstrategically generated templates, and extract events that have asymmetric\nassociations with genders. Our study discovers that the Wikipedia pages tend to\nintermingle personal life events with professional events for females but not\nfor males, which calls for the awareness of the Wikipedia community to\nformalize guidelines and train the editors to mind the implicit biases that\ncontributors carry. Our work also lays the foundation for future works on\nquantifying and discovering event biases at the corpus level.", "published": "2021-06-03 05:22:16", "link": "http://arxiv.org/abs/2106.01601v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grounding Complex Navigational Instructions Using Scene Graphs", "abstract": "Training a reinforcement learning agent to carry out natural language\ninstructions is limited by the available supervision, i.e. knowing when the\ninstruction has been carried out. We adapt the CLEVR visual question answering\ndataset to generate complex natural language navigation instructions and\naccompanying scene graphs, yielding an environment-agnostic supervised dataset.\nTo demonstrate the use of this data set, we map the scenes to the VizDoom\nenvironment and use the architecture in \\citet{gatedattention} to train an\nagent to carry out these more complex language instructions.", "published": "2021-06-03 05:45:21", "link": "http://arxiv.org/abs/2106.01607v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Corporate core values and social responsibility: What really matters to\n  whom", "abstract": "This study uses an innovative measure, the Semantic Brand Score, to assess\nthe interest of stakeholders in different company core values. Among others, we\nfocus on corporate social responsibility (CSR) core value statements, and on\nthe attention they receive from five categories of stakeholders (customers,\ncompany communication teams, employees, associations and media). Combining big\ndata methods and tools of Social Network Analysis and Text Mining, we analyzed\nabout 58,000 Italian tweets and found that different stakeholders have\ndifferent prevailing interests. CSR gets much less attention than expected.\nCore values related to customers and employees are in the foreground.", "published": "2021-06-03 07:25:26", "link": "http://arxiv.org/abs/2106.01644v1", "categories": ["cs.CL", "cs.SI", "physics.soc-ph", "I.2.7; J.4; H.4.0"], "primary_category": "cs.CL"}
{"title": "Fingerprinting Fine-tuned Language Models in the Wild", "abstract": "There are concerns that the ability of language models (LMs) to generate high\nquality synthetic text can be misused to launch spam, disinformation, or\npropaganda. Therefore, the research community is actively working on developing\napproaches to detect whether a given text is organic or synthetic. While this\nis a useful first step, it is important to be able to further fingerprint the\nauthor LM to attribute its origin. Prior work on fingerprinting LMs is limited\nto attributing synthetic text generated by a handful (usually < 10) of\npre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad\nof ways (e.g., on a domain-specific text corpus) before being used to generate\nsynthetic text. It is challenging to fingerprinting fine-tuned LMs because the\nuniverse of fine-tuned LMs is much larger in realistic scenarios. To address\nthis challenge, we study the problem of large-scale fingerprinting of\nfine-tuned LMs in the wild. Using a real-world dataset of synthetic text\ngenerated by 108 different fine-tuned LMs, we conduct comprehensive experiments\nto demonstrate the limitations of existing fingerprinting approaches. Our\nresults show that fine-tuning itself is the most effective in attributing the\nsynthetic text generated by fine-tuned LMs.", "published": "2021-06-03 09:07:54", "link": "http://arxiv.org/abs/2106.01703v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EmoDNN: Understanding emotions from short texts through a deep neural\n  network ensemble", "abstract": "The latent knowledge in the emotions and the opinions of the individuals that\nare manifested via social networks are crucial to numerous applications\nincluding social management, dynamical processes, and public security.\nAffective computing, as an interdisciplinary research field, linking artificial\nintelligence to cognitive inference, is capable to exploit emotion-oriented\nknowledge from brief contents. The textual contents convey hidden information\nsuch as personality and cognition about corresponding authors that can\ndetermine both correlations and variations between users. Emotion recognition\nfrom brief contents should embrace the contrast between authors where the\ndifferences in personality and cognition can be traced within emotional\nexpressions. To tackle this challenge, we devise a framework that, on the one\nhand, infers latent individual aspects, from brief contents and, on the other\nhand, presents a novel ensemble classifier equipped with dynamic dropout\nconvnets to extract emotions from textual context. To categorize short text\ncontents, our proposed method conjointly leverages cognitive factors and\nexploits hidden information. We utilize the outcome vectors in a novel\nembedding model to foster emotion-pertinent features that are collectively\nassembled by lexicon inductions. Experimental results show that compared to\nother competitors, our proposed model can achieve a higher performance in\nrecognizing emotion from noisy contents.", "published": "2021-06-03 09:17:34", "link": "http://arxiv.org/abs/2106.01706v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.LG"}
{"title": "SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level\n  Relation Extraction", "abstract": "Document-level relation extraction has attracted much attention in recent\nyears. It is usually formulated as a classification problem that predicts\nrelations for all entity pairs in the document. However, previous works\nindiscriminately represent intra- and inter-sentential relations in the same\nway, confounding the different patterns for predicting them. Besides, they\ncreate a document graph and use paths between entities on the graph as clues\nfor logical reasoning. However, not all entity pairs can be connected with a\npath and have the correct logical reasoning paths in their graph. Thus many\ncases of logical reasoning cannot be covered. This paper proposes an effective\narchitecture, SIRE, to represent intra- and inter-sentential relations in\ndifferent ways. We design a new and straightforward form of logical reasoning\nmodule that can cover more logical reasoning chains. Experiments on the public\ndatasets show SIRE outperforms the previous state-of-the-art methods. Further\nanalysis shows that our predictions are reliable and explainable. Our code is\navailable at https://github.com/DreamInvoker/SIRE.", "published": "2021-06-03 09:25:44", "link": "http://arxiv.org/abs/2106.01709v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual\n  Learning", "abstract": "Vision-language pre-training (VLP) on large-scale image-text pairs has\nachieved huge success for the cross-modal downstream tasks. The most existing\npre-training methods mainly adopt a two-step training procedure, which firstly\nemploys a pre-trained object detector to extract region-based visual features,\nthen concatenates the image representation and text embedding as the input of\nTransformer to train. However, these methods face problems of using\ntask-specific visual representation of the specific object detector for generic\ncross-modal understanding, and the computation inefficiency of two-stage\npipeline. In this paper, we propose the first end-to-end vision-language\npre-trained model for both V+L understanding and generation, namely E2E-VLP,\nwhere we build a unified Transformer framework to jointly learn visual\nrepresentation, and semantic alignments between image and text. We incorporate\nthe tasks of object detection and image captioning into pre-training with a\nunified Transformer encoder-decoder architecture for enhancing visual learning.\nAn extensive set of experiments have been conducted on well-established\nvision-language downstream tasks to demonstrate the effectiveness of this novel\nVLP paradigm.", "published": "2021-06-03 12:50:26", "link": "http://arxiv.org/abs/2106.01804v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "An Improved Model for Voicing Silent Speech", "abstract": "In this paper, we present an improved model for voicing silent speech, where\naudio is synthesized from facial electromyography (EMG) signals. To give our\nmodel greater flexibility to learn its own input features, we directly use EMG\nsignals as input in the place of hand-designed features used by prior work. Our\nmodel uses convolutional layers to extract features from the signals and\nTransformer layers to propagate information across longer distances. To provide\nbetter signal for learning, we also introduce an auxiliary task of predicting\nphoneme labels in addition to predicting speech audio features. On an open\nvocabulary intelligibility evaluation, our model improves the state of the art\nfor this task by an absolute 25.8%.", "published": "2021-06-03 15:33:23", "link": "http://arxiv.org/abs/2106.01933v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Case for Translation-Invariant Self-Attention in Transformer-Based\n  Language Models", "abstract": "Mechanisms for encoding positional information are central for\ntransformer-based language models. In this paper, we analyze the position\nembeddings of existing language models, finding strong evidence of translation\ninvariance, both for the embeddings themselves and for their effect on\nself-attention. The degree of translation invariance increases during training\nand correlates positively with model performance. Our findings lead us to\npropose translation-invariant self-attention (TISA), which accounts for the\nrelative position between tokens in an interpretable fashion without needing\nconventional position embeddings. Our proposal has several theoretical\nadvantages over existing position-representation approaches. Experiments show\nthat it improves on regular ALBERT on GLUE tasks, while only adding orders of\nmagnitude less positional parameters.", "published": "2021-06-03 15:56:26", "link": "http://arxiv.org/abs/2106.01950v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic-WER: A Unified Metric for the Evaluation of ASR Transcript for\n  End Usability", "abstract": "Recent advances in supervised, semi-supervised and self-supervised deep\nlearning algorithms have shown significant improvement in the performance of\nautomatic speech recognition(ASR) systems. The state-of-the-art systems have\nachieved a word error rate (WER) less than 5%. However, in the past,\nresearchers have argued the non-suitability of the WER metric for the\nevaluation of ASR systems for downstream tasks such as spoken language\nunderstanding (SLU) and information retrieval. The reason is that the WER works\nat the surface level and does not include any syntactic and semantic\nknowledge.The current work proposes Semantic-WER (SWER), a metric to evaluate\nthe ASR transcripts for downstream applications in general. The SWER can be\neasily customized for any down-stream task.", "published": "2021-06-03 17:35:14", "link": "http://arxiv.org/abs/2106.02016v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Segmental Contrastive Predictive Coding for Unsupervised Word\n  Segmentation", "abstract": "Automatic detection of phoneme or word-like units is one of the core\nobjectives in zero-resource speech processing. Recent attempts employ\nself-supervised training methods, such as contrastive predictive coding (CPC),\nwhere the next frame is predicted given past context. However, CPC only looks\nat the audio signal's frame-level structure. We overcome this limitation with a\nsegmental contrastive predictive coding (SCPC) framework that can model the\nsignal structure at a higher level e.g. at the phoneme level. In this\nframework, a convolutional neural network learns frame-level representation\nfrom the raw waveform via noise-contrastive estimation (NCE). A differentiable\nboundary detector finds variable-length segments, which are then used to\noptimize a segment encoder via NCE to learn segment representations. The\ndifferentiable boundary detector allows us to train frame-level and\nsegment-level encoders jointly. Typically, phoneme and word segmentation are\ntreated as separate tasks. We unify them and experimentally show that our\nsingle model outperforms existing phoneme and word segmentation methods on\nTIMIT and Buckeye datasets. We analyze the impact of boundary threshold and\nwhen is the right time to include the segmental loss in the learning process.", "published": "2021-06-03 23:12:05", "link": "http://arxiv.org/abs/2106.02170v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Defending Democracy: Using Deep Learning to Identify and Prevent\n  Misinformation", "abstract": "The rise in online misinformation in recent years threatens democracies by\ndistorting authentic public discourse and causing confusion, fear, and even, in\nextreme cases, violence. There is a need to understand the spread of false\ncontent through online networks for developing interventions that disrupt\nmisinformation before it achieves virality. Using a Deep Bidirectional\nTransformer for Language Understanding (BERT) and propagation graphs, this\nstudy classifies and visualizes the spread of misinformation on a social media\nnetwork using publicly available Twitter data. The results confirm prior\nresearch around user clusters and the virality of false content while improving\nthe precision of deep learning models for misinformation detection. The study\nfurther demonstrates the suitability of BERT for providing a scalable model for\nfalse information detection, which can contribute to the development of more\ntimely and accurate interventions to slow the spread of misinformation in\nonline environments.", "published": "2021-06-03 16:34:54", "link": "http://arxiv.org/abs/2106.02607v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "AliCG: Fine-grained and Evolvable Conceptual Graph Construction for\n  Semantic Search at Alibaba", "abstract": "Conceptual graphs, which is a particular type of Knowledge Graphs, play an\nessential role in semantic search. Prior conceptual graph construction\napproaches typically extract high-frequent, coarse-grained, and time-invariant\nconcepts from formal texts. In real applications, however, it is necessary to\nextract less-frequent, fine-grained, and time-varying conceptual knowledge and\nbuild taxonomy in an evolving manner. In this paper, we introduce an approach\nto implementing and deploying the conceptual graph at Alibaba. Specifically, We\npropose a framework called AliCG which is capable of a) extracting fine-grained\nconcepts by a novel bootstrapping with alignment consensus approach, b) mining\nlong-tail concepts with a novel low-resource phrase mining approach, c)\nupdating the graph dynamically via a concept distribution estimation method\nbased on implicit and explicit user behaviors. We have deployed the framework\nat Alibaba UC Browser. Extensive offline evaluation as well as online A/B\ntesting demonstrate the efficacy of our approach.", "published": "2021-06-03 08:44:03", "link": "http://arxiv.org/abs/2106.01686v2", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.AI"}
{"title": "LyricJam: A system for generating lyrics for live instrumental music", "abstract": "We describe a real-time system that receives a live audio stream from a jam\nsession and generates lyric lines that are congruent with the live music being\nplayed. Two novel approaches are proposed to align the learned latent spaces of\naudio and text representations that allow the system to generate novel lyric\nlines matching live instrumental music. One approach is based on adversarial\nalignment of latent representations of audio and lyrics, while the other\napproach learns to transfer the topology from the music latent space to the\nlyric latent space. A user study with music artists using the system showed\nthat the system was useful not only in lyric composition, but also encouraged\nthe artists to improvise and find new musical expressions. Another user study\ndemonstrated that users preferred the lines generated using the proposed\nmethods to the lines generated by a baseline model.", "published": "2021-06-03 16:06:46", "link": "http://arxiv.org/abs/2106.01960v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ERANNs: Efficient Residual Audio Neural Networks for Audio Pattern\n  Recognition", "abstract": "Audio pattern recognition (APR) is an important research topic and can be\napplied to several fields related to our lives. Therefore, accurate and\nefficient APR systems need to be developed as they are useful in real\napplications. In this paper, we propose a new convolutional neural network\n(CNN) architecture and a method for improving the inference speed of CNN-based\nsystems for APR tasks. Moreover, using the proposed method, we can improve the\nperformance of our systems, as confirmed in experiments conducted on four audio\ndatasets. In addition, we investigate the impact of data augmentation\ntechniques and transfer learning on the performance of our systems. Our best\nsystem achieves a mean average precision (mAP) of 0.450 on the AudioSet\ndataset. Although this value is less than that of the state-of-the-art system,\nthe proposed system is 7.1x faster and 9.7x smaller. On the ESC-50,\nUrbanSound8K, and RAVDESS datasets, we obtain state-of-the-art results with\naccuracies of 0.961, 0.908, and 0.748, respectively. Our system for the ESC-50\ndataset is 1.7x faster and 2.3x smaller than the previous best system. For the\nRAVDESS dataset, our system is 3.3x smaller than the previous best system. We\nname our systems \"Efficient Residual Audio Neural Networks\".", "published": "2021-06-03 06:45:19", "link": "http://arxiv.org/abs/2106.01621v7", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker verification-derived loss and data augmentation for DNN-based\n  multispeaker speech synthesis", "abstract": "Building multispeaker neural network-based text-to-speech synthesis systems\ncommonly relies on the availability of large amounts of high quality recordings\nfrom each speaker and conditioning the training process on the speaker's\nidentity or on a learned representation of it. However, when little data is\navailable from each speaker, or the number of speakers is limited, the\nmultispeaker TTS can be hard to train and will result in poor speaker\nsimilarity and naturalness.\n  In order to address this issue, we explore two directions: forcing the\nnetwork to learn a better speaker identity representation by appending an\nadditional loss term; and augmenting the input data pertaining to each speaker\nusing waveform manipulation methods. We show that both methods are efficient\nwhen evaluated with both objective and subjective measures. The additional loss\nterm aids the speaker similarity, while the data augmentation improves the\nintelligibility of the multispeaker TTS system.", "published": "2021-06-03 12:22:18", "link": "http://arxiv.org/abs/2106.01789v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An objective evaluation of the effects of recording conditions and\n  speaker characteristics in multi-speaker deep neural speech synthesis", "abstract": "Multi-speaker spoken datasets enable the creation of text-to-speech synthesis\n(TTS) systems which can output several voice identities. The multi-speaker\n(MSPK) scenario also enables the use of fewer training samples per speaker.\nHowever, in the resulting acoustic model, not all speakers exhibit the same\nsynthetic quality, and some of the voice identities cannot be used at all.\n  In this paper we evaluate the influence of the recording conditions, speaker\ngender, and speaker particularities over the quality of the synthesised output\nof a deep neural TTS architecture, namely Tacotron2. The evaluation is possible\ndue to the use of a large Romanian parallel spoken corpus containing over 81\nhours of data. Within this setup, we also evaluate the influence of different\ntypes of text representations: orthographic, phonetic, and phonetic extended\nwith syllable boundaries and lexical stress markings.\n  We evaluate the results of the MSPK system using the objective measures of\nequal error rate (EER) and word error rate (WER), and also look into the\ndistances between natural and synthesised t-SNE projections of the embeddings\ncomputed by an accurate speaker verification network. The results show that\nthere is indeed a large correlation between the recording conditions and the\nspeaker's synthetic voice quality. The speaker gender does not influence the\noutput, and that extending the input text representation with syllable\nboundaries and lexical stress information does not equally enhance the\ngenerated audio across all speaker identities. The visualisation of the t-SNE\nprojections of the natural and synthesised speaker embeddings show that the\nacoustic model shifts some of the speakers' neural representation, but not all\nof them. As a result, these speakers have lower performances of the output\nspeech.", "published": "2021-06-03 13:04:59", "link": "http://arxiv.org/abs/2106.01812v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Joint Multi-Channel Dereverberation and Noise Reduction Using a Unified\n  Convolutional Beamformer With Sparse Priors", "abstract": "Recently, the convolutional weighted power minimization distortionless\nresponse (WPD) beamformer was proposed, which unifies multi-channel weighted\nprediction error dereverberation and minimum power distortionless response\nbeamforming. To optimize the convolutional filter, the desired speech component\nis modeled with a time-varying Gaussian model, which promotes the sparsity of\nthe desired speech component in the short-time Fourier transform domain\ncompared to the noisy microphone signals. In this paper we generalize the\nconvolutional WPD beamformer by using an lp-norm cost function, introducing an\nadjustable shape parameter which enables to control the sparsity of the desired\nspeech component. Experiments based on the REVERB challenge dataset show that\nthe proposed method outperforms the conventional convolutional WPD beamformer\nin terms of objective speech quality metrics.", "published": "2021-06-03 14:46:35", "link": "http://arxiv.org/abs/2106.01902v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Language Independent Speech Emotion and Non-invasive Early Detection of\n  Neurocognitive Disorder", "abstract": "Emotions(like fear,anger,sadness,happiness etc.) are the fundamental features\nof human behavior and governs his/her mental health. The subtlety of emotional\nfluctuations can be examined through perturbation in conversations or speech.\nAnalysis of emotional state of a person from acoustical features of speech\nsignal leads to discovery of vital cues determining his or her mental health.\nHence, it's an important field of research in the area of Human Computer\nInteraction(HCI). In a recent work we have shown that how the contrast in\nHurst-Exponent calculated from the non-stationary and nonlinear aspects of\n\"angry\" and \"sad\" speech(spoken in English language) recordings in the\nToronto-Emotional-Speech-Set(TESS) can be used for early detection and\ndiagnosis of Alzheimer's Disease. In this work we have extended the work and\nextracted Hurst-exponent for the speech-signals of similar emotions but spoken\nin German language. It has been observed that the Hurst-exponent efficiently\nsegregates the contrasting emotions of \"anger\" and \"sadness\" in the speech\nspoken in German language, in similar fashion it has been doing for English\nspeech. Hence it can be concluded that the Hurst-exponent can differentiate\namong speech spoken out of different emotions in language-independent manner.\nWe propose algorithm for a language-independent application for early\nnon-invasive detection of various severe neurocognitive-disorders like\nAlzheimer's Disease, MND(motor-neuron-disorder), ASD(autism-spectrum-disorder),\ndepression, suicidal-tendency etc. which is not possible with the state of the\nart medical science.", "published": "2021-06-03 08:38:22", "link": "http://arxiv.org/abs/2106.01684v1", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Heart Sound Classification Considering Additive Noise and Convolutional\n  Distortion", "abstract": "Cardiac auscultation is an essential point-of-care method used for the early\ndiagnosis of heart diseases. Automatic analysis of heart sounds for abnormality\ndetection is faced with the challenges of additive noise and sensor-dependent\ndegradation. This paper aims to develop methods to address the cardiac\nabnormality detection problem when both types of distortions are present in the\ncardiac auscultation sound. We first mathematically analyze the effect of\nadditive and convolutional noise on short-term filterbank-based features and a\nConvolutional Neural Network (CNN) layer. Based on the analysis, we propose a\ncombination of linear and logarithmic spectrogram-image features. These 2D\nfeatures are provided as input to a residual CNN network (ResNet) for heart\nsound abnormality detection. Experimental validation is performed on an\nopen-access heart sound abnormality detection dataset involving noisy\nrecordings obtained from multiple stethoscope sensors. The proposed method\nachieves significantly improved results compared to the conventional\napproaches, with an area under the ROC (receiver operating characteristics)\ncurve (AUC) of 91.36%, F-1 score of 84.09%, and Macc (mean of sensitivity and\nspecificity) of 85.08%. We also show that the proposed method shows the best\nmean accuracy across different source domains including stethoscope and noise\nvariability, demonstrating its effectiveness in different recording conditions.\nThe proposed combination of linear and logarithmic features along with the\nResNet classifier effectively minimizes the impact of background noise and\nsensor variability for classifying phonocardiogram (PCG) signals. The proposed\nmethod paves the way towards developing computer-aided cardiac auscultation\nsystems in noisy environments using low-cost stethoscopes.", "published": "2021-06-03 14:09:04", "link": "http://arxiv.org/abs/2106.01865v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
