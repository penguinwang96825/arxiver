{"title": "X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\n  across Paragraphs", "abstract": "Understanding when two pieces of text convey the same information is a goal\ntouching many subproblems in NLP, including textual entailment and\nfact-checking. This problem becomes more complex when those two pieces of text\nare in different languages. Here, we introduce X-PARADE (Cross-lingual\nParagraph-level Analysis of Divergences and Entailments), the first\ncross-lingual dataset of paragraph-level information divergences. Annotators\nlabel a paragraph in a target language at the span level and evaluate it with\nrespect to a corresponding paragraph in a source language, indicating whether a\ngiven piece of information is the same, new, or new but can be inferred. This\nlast notion establishes a link with cross-language NLI. Aligned paragraphs are\nsourced from Wikipedia pages in different languages, reflecting real\ninformation divergences observed in the wild. Armed with our dataset, we\ninvestigate a diverse set of approaches for this problem, including token\nalignment from machine translation, textual entailment methods that localize\ntheir decisions, and prompting LLMs. Our results show that these methods vary\nin their capability to handle inferable information, but they all fall short of\nhuman performance.", "published": "2023-09-16 04:34:55", "link": "http://arxiv.org/abs/2309.08873v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\n  Nationality Bias in Generative Models", "abstract": "LLMs are increasingly powerful and widely used to assist users in a variety\nof tasks. This use risks the introduction of LLM biases to consequential\ndecisions such as job hiring, human performance evaluation, and criminal\nsentencing. Bias in NLP systems along the lines of gender and ethnicity has\nbeen widely studied, especially for specific stereotypes (e.g., Asians are good\nat math). In this paper, we investigate bias along less-studied but still\nconsequential, dimensions, such as age and beauty, measuring subtler correlated\ndecisions that LLMs make between social groups and unrelated positive and\nnegative attributes. We ask whether LLMs hold wide-reaching biases of positive\nor negative sentiment for specific social groups similar to the \"what is\nbeautiful is good\" bias found in people in experimental psychology. We\nintroduce a template-generated dataset of sentence completion tasks that asks\nthe model to select the most appropriate attribute to complete an evaluative\nstatement about a person described as a member of a specific social group. We\nalso reverse the completion task to select the social group based on an\nattribute. We report the correlations that we find for 4 cutting-edge LLMs.\nThis dataset can be used as a benchmark to evaluate progress in more\ngeneralized biases and the templating technique can be used to expand the\nbenchmark with minimal additional human annotation.", "published": "2023-09-16 07:07:04", "link": "http://arxiv.org/abs/2309.08902v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Multi-Hop Question Answering Through a Conversation Between\n  Tools and Efficiently Finetuned Large Language Models", "abstract": "We employ a tool-interacting divide-and-conquer strategy enabling large\nlanguage models (LLMs) to answer complex multimodal multi-hop questions. In\nparticular, we harness the power of large language models to divide a given\nmultimodal multi-hop question into unimodal single-hop sub-questions to be\nanswered by the appropriate tool from a predefined set of tools. After all\ncorresponding tools provide the LLM with their answers, the LLM generates the\nnext relevant unimodal single-hop question. To increase the reasoning ability\nof LLMs, we prompt chatGPT to generate a tool-interacting divide-and-conquer\ndataset. This dataset is then used to efficiently finetune the corresponding\nLLM. To assess the effectiveness of this approach, we conduct an evaluation on\ntwo recently introduced complex question-answering datasets. The experimental\nanalysis demonstrate substantial improvements over existing state-of-the-art\nsolutions, indicating the efficacy and generality of our strategy", "published": "2023-09-16 08:22:22", "link": "http://arxiv.org/abs/2309.08922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Multi-lingual Positive Instances in Contrastive Learning to\n  Improve Sentence Embedding", "abstract": "Learning multi-lingual sentence embeddings is a fundamental task in natural\nlanguage processing. Recent trends in learning both mono-lingual and\nmulti-lingual sentence embeddings are mainly based on contrastive learning (CL)\namong an anchor, one positive, and multiple negative instances. In this work,\nwe argue that leveraging multiple positives should be considered for\nmulti-lingual sentence embeddings because (1) positives in a diverse set of\nlanguages can benefit cross-lingual learning, and (2) transitive similarity\nacross multiple positives can provide reliable structural information for\nlearning. In order to investigate the impact of multiple positives in CL, we\npropose a novel approach, named MPCL, to effectively utilize multiple positive\ninstances to improve the learning of multi-lingual sentence embeddings.\nExperimental results on various backbone models and downstream tasks\ndemonstrate that MPCL leads to better retrieval, semantic similarity, and\nclassification performances compared to conventional CL. We also observe that\nin unseen languages, sentence embedding models trained on multiple positives\nshow better cross-lingual transfer performance than models trained on a single\npositive instance.", "published": "2023-09-16 08:54:30", "link": "http://arxiv.org/abs/2309.08929v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Label Projection for Cross-Lingual Structured Prediction", "abstract": "Label projection, which involves obtaining translated labels and texts\njointly, is essential for leveraging machine translation to facilitate\ncross-lingual transfer in structured prediction tasks. Prior research exploring\nlabel projection often compromise translation accuracy by favoring simplified\nlabel translation or relying solely on word-level alignments. In this paper, we\nintroduce a novel label projection approach, CLaP, which translates text to the\ntarget language and performs contextual translation on the labels using the\ntranslated text as the context, ensuring better accuracy for the translated\nlabels. We leverage instruction-tuned language models with multilingual\ncapabilities as our contextual translator, imposing the constraint of the\npresence of translated labels in the translated text via instructions. We\nbenchmark CLaP with other label projection techniques on zero-shot\ncross-lingual transfer across 39 languages on two representative structured\nprediction tasks - event argument extraction (EAE) and named entity recognition\n(NER), showing over 2.4 F1 improvement for EAE and 1.4 F1 improvement for NER.\nWe further explore the applicability of CLaP on ten extremely low-resource\nlanguages to showcase its potential for cross-lingual structured prediction.", "published": "2023-09-16 10:27:28", "link": "http://arxiv.org/abs/2309.08943v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\n  Through Look-Forward Motivated Goals", "abstract": "Recently, the development of large language models (LLMs) has been\nsignificantly enhanced the question answering and dialogue generation, and\nmakes them become increasingly popular in current practical scenarios. While\nunlike the general dialogue system which emphasizes the semantic performance,\nthe task-oriented dialogue (ToD) systems aim to achieve the dialogue goal\nefficiently and successfully in multiple turns. Unfortunately, existing\nLLM-induced ToD systems lack the direct reward toward the final goal and do not\ntake account of the dialogue proactivity that can strengthen the dialogue\nefficiency. To fill these gaps, we introduce the ProToD (Proactively\nGoal-Driven LLM-Induced ToD) approach, which anticipates the future dialogue\nactions and incorporates the goal-oriented reward signal to enhance ToD\nsystems. Additionally, we present a novel evaluation method that assesses ToD\nsystems based on goal-driven dialogue simulations. This method allows us to\ngauge user satisfaction, system efficiency and successful rate while overcoming\nthe limitations of current Information and Success metrics. Empirical\nexperiments conducted on the MultiWoZ 2.1 dataset demonstrate that our model\ncan achieve superior performance using only 10% of the data compared to\nprevious end-to-end fully supervised models. This improvement is accompanied by\nenhanced user satisfaction and efficiency.", "published": "2023-09-16 10:56:00", "link": "http://arxiv.org/abs/2309.08949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ODSum: New Benchmarks for Open Domain Multi-Document Summarization", "abstract": "Open-domain Multi-Document Summarization (ODMDS) is a critical tool for\ncondensing vast arrays of documents into coherent, concise summaries. With a\nmore inter-related document set, there does not necessarily exist a correct\nanswer for the retrieval, making it hard to measure the retrieving performance.\nWe propose a rule-based method to process query-based document summarization\ndatasets into ODMDS datasets. Based on this method, we introduce a novel\ndataset, ODSum, a sophisticated case with its document index interdependent and\noften interrelated. We tackle ODMDS with the \\textit{retrieve-then-summarize}\nmethod, and the performance of a list of retrievers and summarizers is\ninvestigated. Through extensive experiments, we identify variances in\nevaluation metrics and provide insights into their reliability. We also found\nthat LLMs suffer great performance loss from retrieving errors. We further\nexperimented methods to improve the performance as well as investigate their\nrobustness against imperfect retrieval. We will release our data and code at\nhttps://github.com/yale-nlp/ODSum.", "published": "2023-09-16 11:27:34", "link": "http://arxiv.org/abs/2309.08960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Struc-Bench: Are Large Language Models Really Good at Generating Complex\n  Structured Data?", "abstract": "Despite the remarkable capabilities of Large Language Models (LLMs) like\nGPT-4, producing complex, structured tabular data remains challenging. Our\nstudy assesses LLMs' proficiency in structuring tables and introduces a novel\nfine-tuning method, cognizant of data structures, to bolster their performance.\nWe unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs\n(GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and\nLaTeX formats. Our proposed FormatCoT aids in crafting format-specific\ninstructions from the intended outputs to populate this benchmark. Addressing\nthe gap in task-centered evaluation, we propose two innovative metrics, P-Score\n(Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM\nperformance. Our experiments show that applying our structure-aware fine-tuning\nto LLaMA-7B leads to substantial performance gains, outshining its LLM\ncounterparts across most measures. In-depth error analysis and creating an\nability map across six dimensions -- coverage, formatting, reasoning,\ncomprehension, pragmatics, and hallucination -- highlight areas for future\nenhancements and suggest forthcoming research trajectories. Our code and models\ncan be found at https://github.com/gersteinlab/Struc-Bench.", "published": "2023-09-16 11:31:58", "link": "http://arxiv.org/abs/2309.08963v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking STS and NLI in Large Language Models", "abstract": "Recent years have seen the rise of large language models (LLMs), where\npractitioners use task-specific prompts; this was shown to be effective for a\nvariety of tasks. However, when applied to semantic textual similarity (STS)\nand natural language inference (NLI), the effectiveness of LLMs turns out to be\nlimited by low-resource domain accuracy, model overconfidence, and difficulty\nto capture the disagreements between human judgements. With this in mind, here\nwe try to rethink STS and NLI in the era of LLMs. We first evaluate the\nperformance of STS and NLI in the clinical/biomedical domain, and then we\nassess LLMs' predictive confidence and their capability of capturing collective\nhuman opinions. We find that these old problems are still to be properly\naddressed in the era of LLMs.", "published": "2023-09-16 11:58:39", "link": "http://arxiv.org/abs/2309.08969v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-aware Adversarial Attack on Named Entity Recognition", "abstract": "In recent years, large pre-trained language models (PLMs) have achieved\nremarkable performance on many natural language processing benchmarks. Despite\ntheir success, prior studies have shown that PLMs are vulnerable to attacks\nfrom adversarial examples. In this work, we focus on the named entity\nrecognition task and study context-aware adversarial attack methods to examine\nthe model's robustness. Specifically, we propose perturbing the most\ninformative words for recognizing entities to create adversarial examples and\ninvestigate different candidate replacement methods to generate natural and\nplausible adversarial examples. Experiments and analyses show that our methods\nare more effective in deceiving the model into making wrong predictions than\nstrong baselines.", "published": "2023-09-16 14:04:23", "link": "http://arxiv.org/abs/2309.08999v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the impact of low-rank adaptation on the performance,\n  efficiency, and regularization of RLHF", "abstract": "During the last stage of RLHF, a large language model is aligned to human\nintents via PPO training, a process that generally requires large-scale\ncomputational resources. In this technical report, we empirically investigate\nan efficient implementation of RLHF using low-rank adaptation (LoRA), which\nallows us to align the LLaMA 7B checkpoint on the Alpaca dataset using only two\nA100 GPUs instead of the eight required for full model fine-tuning. Despite\ntuning only 0.2% of LLaMA 7B's parameters, our implementation achieves better\nperformance than the publicly-released AlpacaFarm checkpoint with full model\nfine-tuning. Next, we analyze several configurations of our LoRA-based PPO\nimplementation, varying the form of the KL regularization term in the training\nobjective. We find that (1) removing this penalty term does not harm\nperformance on the AlpacaFarm evaluation set under our LoRA setup; (2) other\nregularizers, such as Jensen-Shannon divergence, lead to improved performance;\nand (3) while PPO training negatively impacts the factuality of model-generated\nresponses, training with LoRA largely mitigates this effect. We release our\ncode and pretrained checkpoints to facilitate future research on more efficient\nRLHF.", "published": "2023-09-16 17:31:36", "link": "http://arxiv.org/abs/2309.09055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing a Knowledge Graph for Vietnamese Legal Cases with\n  Heterogeneous Graphs", "abstract": "This paper presents a knowledge graph construction method for legal case\ndocuments and related laws, aiming to organize legal information efficiently\nand enhance various downstream tasks. Our approach consists of three main\nsteps: data crawling, information extraction, and knowledge graph deployment.\nFirst, the data crawler collects a large corpus of legal case documents and\nrelated laws from various sources, providing a rich database for further\nprocessing. Next, the information extraction step employs natural language\nprocessing techniques to extract entities such as courts, cases, domains, and\nlaws, as well as their relationships from the unstructured text. Finally, the\nknowledge graph is deployed, connecting these entities based on their extracted\nrelationships, creating a heterogeneous graph that effectively represents legal\ninformation and caters to users such as lawyers, judges, and scholars. The\nestablished baseline model leverages unsupervised learning methods, and by\nincorporating the knowledge graph, it demonstrates the ability to identify\nrelevant laws for a given legal case. This approach opens up opportunities for\nvarious applications in the legal domain, such as legal case analysis, legal\nrecommendation, and decision support.", "published": "2023-09-16 18:31:47", "link": "http://arxiv.org/abs/2309.09069v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Debiasing on the Performance of Language Models in\n  Downstream Tasks is Underestimated", "abstract": "Pre-trained language models trained on large-scale data have learned serious\nlevels of social biases. Consequently, various methods have been proposed to\ndebias pre-trained models. Debiasing methods need to mitigate only\ndiscriminatory bias information from the pre-trained models, while retaining\ninformation that is useful for the downstream tasks. In previous research,\nwhether useful information is retained has been confirmed by the performance of\ndownstream tasks in debiased pre-trained models. On the other hand, it is not\nclear whether these benchmarks consist of data pertaining to social biases and\nare appropriate for investigating the impact of debiasing. For example in\ngender-related social biases, data containing female words (e.g. ``she, female,\nwoman''), male words (e.g. ``he, male, man''), and stereotypical words (e.g.\n``nurse, doctor, professor'') are considered to be the most affected by\ndebiasing. If there is not much data containing these words in a benchmark\ndataset for a target task, there is the possibility of erroneously evaluating\nthe effects of debiasing. In this study, we compare the impact of debiasing on\nperformance across multiple downstream tasks using a wide-range of benchmark\ndatasets that containing female, male, and stereotypical words. Experiments\nshow that the effects of debiasing are consistently \\emph{underestimated}\nacross all tasks. Moreover, the effects of debiasing could be reliably\nevaluated by separately considering instances containing female, male, and\nstereotypical words than all of the instances in a benchmark dataset.", "published": "2023-09-16 20:25:34", "link": "http://arxiv.org/abs/2309.09092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EchoPrompt: Instructing the Model to Rephrase Queries for Improved\n  In-context Learning", "abstract": "Language models are achieving impressive performance on various tasks by\naggressively adopting inference-time prompting techniques, such as zero-shot\nand few-shot prompting. In this work, we introduce EchoPrompt, a simple yet\neffective approach that prompts the model to rephrase its queries before\nanswering them. EchoPrompt is adapted for both zero-shot and few-shot\nin-context learning with standard and chain-of-thought prompting. Experimental\nresults show that EchoPrompt yields substantial improvements across all these\nsettings for four families of causal language models. These improvements are\nobserved across various numerical reasoning (e.g. GSM8K, SVAMP), reading\ncomprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On\naverage, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002\nby 5% in numerical tasks and 13% in reading comprehension tasks. We investigate\nthe factors contributing to EchoPrompt's effectiveness through ablation\nstudies, which reveal that both the original query and the model-generated\nrephrased version are instrumental in its performance gains. Our empirical\nresults indicate that EchoPrompt is an effective technique that enhances\nin-context learning performance. We recommend incorporating EchoPrompt into\nvarious baseline prompting strategies to achieve performance boosts.", "published": "2023-09-16 00:55:08", "link": "http://arxiv.org/abs/2309.10687v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\n  in the Era of LLMs", "abstract": "The traditional Dialogue State Tracking (DST) problem aims to track user\npreferences and intents in user-agent conversations. While sufficient for\ntask-oriented dialogue systems supporting narrow domain applications, the\nadvent of Large Language Model (LLM)-based chat systems has introduced many\nreal-world intricacies in open-domain dialogues. These intricacies manifest in\nthe form of increased complexity in contextual interactions, extended dialogue\nsessions encompassing a diverse array of topics, and more frequent contextual\nshifts. To handle these intricacies arising from evolving LLM-based chat\nsystems, we propose joint dialogue segmentation and state tracking per segment\nin open-domain dialogue systems. Assuming a zero-shot setting appropriate to a\ntrue open-domain dialogue system, we propose S3-DST, a structured prompting\ntechnique that harnesses Pre-Analytical Recollection, a novel grounding\nmechanism we designed for improving long context tracking. To demonstrate the\nefficacy of our proposed approach in joint segmentation and state tracking, we\nevaluate S3-DST on a proprietary anonymized open-domain dialogue dataset, as\nwell as publicly available DST and segmentation datasets. Across all datasets\nand settings, S3-DST consistently outperforms the state-of-the-art,\ndemonstrating its potency and robustness the next generation of LLM-based chat\nsystems.", "published": "2023-09-16 00:59:23", "link": "http://arxiv.org/abs/2309.08827v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLIDE: Reference-free Evaluation for Machine Translation using a Sliding\n  Document Window", "abstract": "Reference-based metrics that operate at the sentence-level typically\noutperform quality estimation metrics, which have access only to the source and\nsystem output. This is unsurprising, since references resolve ambiguities that\nmay be present in the source. In this paper, we investigate whether additional\nsource context can effectively substitute for a reference. We present a metric\nnamed SLIDE (SLIding Document Evaluator), which operates on blocks of\nsentences. SLIDE leverages a moving window that slides over each document in\nthe test set, feeding each chunk of sentences into an unmodified, off-the-shelf\nquality estimation model. We find that SLIDE obtains significantly higher\npairwise system accuracy than its sentence-level baseline, in some cases even\neliminating the gap with reference-base metrics. This suggests that source\ncontext may provide the same information as a human reference in disambiguating\nsource ambiguities. This finding is especially pertinent for reference-free\ndocument-level evaluation, wherein SLIDE could provide higher-quality pairwise\nsystem assessments while only requiring document boundary annotations.", "published": "2023-09-16 01:30:58", "link": "http://arxiv.org/abs/2309.08832v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Has Sentiment Returned to the Pre-pandemic Level? A Sentiment Analysis\n  Using U.S. College Subreddit Data from 2019 to 2022", "abstract": "As impact of COVID-19 pandemic winds down, both individuals and society\ngradually return to pre-pandemic activities. This study aims to explore how\npeople's emotions have changed from the pre-pandemic during the pandemic to\npost-emergency period and whether it has returned to pre-pandemic level. We\ncollected Reddit data in 2019 (pre-pandemic), 2020 (peak pandemic), 2021, and\n2022 (late stages of pandemic, transitioning period to post-emergency period)\nfrom subreddits in 128 universities/colleges in the U.S., and a set of\nschool-level characteristics. We predicted two sets of sentiments from a\npre-trained Robustly Optimized BERT pre-training approach (RoBERTa) and graph\nattention network (GAT) that leverages both rich semantic and relational\ninformation among posted messages and then applied a logistic stacking method\nto obtain the final sentiment classification. After obtaining sentiment label\nfor each message, we used a generalized linear mixed-effects model to estimate\ntemporal trend in sentiment from 2019 to 2022 and how school-level factors may\naffect sentiment. Compared to the year 2019, the odds of negative sentiment in\nyears 2020, 2021, and 2022 are 24%, 4.3%, and 10.3% higher, respectively, which\nare all statistically significant(adjusted $p$<0.05). Our study findings\nsuggest a partial recovery in the sentiment composition in the\npost-pandemic-emergency era. The results align with common expectations and\nprovide a detailed quantification of how sentiments have evolved from 2019 to\n2022.", "published": "2023-09-16 02:57:30", "link": "http://arxiv.org/abs/2309.08845v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding", "abstract": "International Classification of Diseases (ICD) coding is the task of\nassigning ICD diagnosis codes to clinical notes. This can be challenging given\nthe large quantity of labels (nearly 9,000) and lengthy texts (up to 8,000\ntokens). However, unlike the single-pass reading process in previous works,\nhumans tend to read the text and label definitions again to get more confident\nanswers. Moreover, although pretrained language models have been used to\naddress these problems, they suffer from huge memory usage. To address the\nabove problems, we propose a simple but effective model called the Multi-Hop\nLabel-wise ATtention (MHLAT), in which multi-hop label-wise attention is\ndeployed to get more precise and informative representations. Extensive\nexperiments on three benchmark MIMIC datasets indicate that our method achieves\nsignificantly better or competitive performance on all seven metrics, with much\nfewer parameters to optimize.", "published": "2023-09-16 04:13:57", "link": "http://arxiv.org/abs/2309.08868v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Information Extraction for Text Data with Probability Graph", "abstract": "In this paper, the problem of semantic information extraction for resource\nconstrained text data transmission is studied. In the considered model, a\nsequence of text data need to be transmitted within a communication\nresource-constrained network, which only allows limited data transmission.\nThus, at the transmitter, the original text data is extracted with natural\nlanguage processing techniques. Then, the extracted semantic information is\ncaptured in a knowledge graph. An additional probability dimension is\nintroduced in this graph to capture the importance of each information. This\nsemantic information extraction problem is posed as an optimization framework\nwhose goal is to extract most important semantic information for transmission.\nTo find an optimal solution for this problem, a Floyd's algorithm based\nsolution coupled with an efficient sorting mechanism is proposed. Numerical\nresults testify the effectiveness of the proposed algorithm with regards to two\nnovel performance metrics including semantic uncertainty and semantic\nsimilarity.", "published": "2023-09-16 05:01:20", "link": "http://arxiv.org/abs/2309.08879v1", "categories": ["cs.CL", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Knowledge Editing in Large Language Models", "abstract": "Knowledge editing aims to change language models' performance on several\nspecial cases (i.e., editing scope) by infusing the corresponding expected\nknowledge into them. With the recent advancements in large language models\n(LLMs), knowledge editing has been shown as a promising technique to adapt LLMs\nto new knowledge without retraining from scratch. However, most of the previous\nstudies neglect the multi-lingual nature of some main-stream LLMs (e.g., LLaMA,\nChatGPT and GPT-4), and typically focus on monolingual scenarios, where LLMs\nare edited and evaluated in the same language. As a result, it is still unknown\nthe effect of source language editing on a different target language. In this\npaper, we aim to figure out this cross-lingual effect in knowledge editing.\nSpecifically, we first collect a large-scale cross-lingual synthetic dataset by\ntranslating ZsRE from English to Chinese. Then, we conduct English editing on\nvarious knowledge editing methods covering different paradigms, and evaluate\ntheir performance in Chinese, and vice versa. To give deeper analyses of the\ncross-lingual effect, the evaluation includes four aspects, i.e., reliability,\ngenerality, locality and portability. Furthermore, we analyze the inconsistent\nbehaviors of the edited models and discuss their specific challenges. Data and\ncodes are available at https://github.com/krystalan/Bi_ZsRE", "published": "2023-09-16 11:07:52", "link": "http://arxiv.org/abs/2309.08952v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Monolingual or Multilingual Instruction Tuning: Which Makes a Better\n  Alpaca", "abstract": "Foundational large language models (LLMs) can be instruction-tuned to perform\nopen-domain question answering, facilitating applications like chat assistants.\nWhile such efforts are often carried out in a single language, we empirically\nanalyze cost-efficient strategies for multilingual scenarios. Our study employs\nthe Alpaca dataset and machine translations of it to form multilingual data,\nwhich is then used to tune LLMs through either low-rank adaptation or\nfull-parameter training. Under a controlled computation budget, comparisons\nshow that multilingual tuning is on par or better than tuning a model for each\nlanguage. Furthermore, multilingual tuning with downsampled data can be as\npowerful and more robust. Our findings serve as a guide for expanding language\nsupport through instruction tuning.", "published": "2023-09-16 11:22:46", "link": "http://arxiv.org/abs/2309.08958v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\n  Language Models for Dynamic Inference", "abstract": "Large language models (LLMs) have revolutionized natural language processing\n(NLP) by excelling at understanding and generating human-like text. However,\ntheir widespread deployment can be prohibitively expensive. SortedNet is a\nrecent training technique for enabling dynamic inference by leveraging the\nmodularity in networks and sorting sub-models based on computation/accuracy in\na nested manner. We extend SortedNet to generative NLP tasks, making large\nlanguage models dynamic without any Pre-Training and by only replacing Standard\nFine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model\nefficiency, eliminating the need for multiple models for various scenarios\nduring inference. We show that this approach can unlock the power of\nintermediate layers of transformers in generating the target output. Our\nsub-models remain integral components of the original model, minimizing storage\nrequirements and transition costs between different computational/latency\nbudgets. The efficacy of our proposed method was demonstrated by applying it to\ntune LLaMA 2 13B on the Stanford Alpaca dataset for instruction following and\nTriviaQA for closed-book question answering. Our results show the superior\nperformance of sub-models in comparison to Standard Fine-Tuning and SFT+ICT\n(Early-Exit), all achieved with efficient tuning and without additional memory\nusage during inference.", "published": "2023-09-16 11:58:34", "link": "http://arxiv.org/abs/2309.08968v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic\n  Statistical Models and Pre-trained Language Models", "abstract": "This paper describes the NOWJ1 Team's approach for the Automated Legal\nQuestion Answering Competition (ALQAC) 2023, which focuses on enhancing legal\ntask performance by integrating classical statistical models and Pre-trained\nLanguage Models (PLMs). For the document retrieval task, we implement a\npre-processing step to overcome input limitations and apply learning-to-rank\nmethods to consolidate features from various models. The question-answering\ntask is split into two sub-tasks: sentence classification and answer\nextraction. We incorporate state-of-the-art models to develop distinct systems\nfor each sub-task, utilizing both classic statistical models and pre-trained\nLanguage Models. Experimental results demonstrate the promising potential of\nour proposed methodology in the competition.", "published": "2023-09-16 18:32:15", "link": "http://arxiv.org/abs/2309.09070v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification", "abstract": "In this study, we present a novel and challenging multilabel Vietnamese\ndataset (RMDM) designed to assess the performance of large language models\n(LLMs), in verifying electronic information related to legal contexts, focusing\non fake news as potential input for electronic evidence. The RMDM dataset\ncomprises four labels: real, mis, dis, and mal, representing real information,\nmisinformation, disinformation, and mal-information, respectively. By including\nthese diverse labels, RMDM captures the complexities of differing fake news\ncategories and offers insights into the abilities of different language models\nto handle various types of information that could be part of electronic\nevidence. The dataset consists of a total of 1,556 samples, with 389 samples\nfor each label. Preliminary tests on the dataset using GPT-based and BERT-based\nmodels reveal variations in the models' performance across different labels,\nindicating that the dataset effectively challenges the ability of various\nlanguage models to verify the authenticity of such information. Our findings\nsuggest that verifying electronic information related to legal contexts,\nincluding fake news, remains a difficult problem for language models,\nwarranting further attention from the research community to advance toward more\nreliable AI models for potential legal applications.", "published": "2023-09-16 18:35:08", "link": "http://arxiv.org/abs/2309.09071v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cognitive modeling and learning with sparse binary hypervectors", "abstract": "Following the general theoretical framework of VSA (Vector Symbolic\nArchitecture), a cognitive model with the use of sparse binary hypervectors is\nproposed. In addition, learning algorithms are introduced to bootstrap the\nmodel from incoming data stream, with much improved transparency and\nefficiency. Mimicking human cognitive process, the training can be performed\nonline while inference is in session. Word-level embedding is re-visited with\nsuch hypervectors, and further applications in the field of NLP (Natural\nLanguage Processing) are explored.", "published": "2023-09-16 01:58:51", "link": "http://arxiv.org/abs/2310.18316v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Bias and Fairness in Chatbots: An Overview", "abstract": "Chatbots have been studied for more than half a century. With the rapid\ndevelopment of natural language processing (NLP) technologies in recent years,\nchatbots using large language models (LLMs) have received much attention\nnowadays. Compared with traditional ones, modern chatbots are more powerful and\nhave been used in real-world applications. There are however, bias and fairness\nconcerns in modern chatbot design. Due to the huge amounts of training data,\nextremely large model sizes, and lack of interpretability, bias mitigation and\nfairness preservation of modern chatbots are challenging. Thus, a comprehensive\noverview on bias and fairness in chatbot systems is given in this paper. The\nhistory of chatbots and their categories are first reviewed. Then, bias sources\nand potential harms in applications are analyzed. Considerations in designing\nfair and unbiased chatbot systems are examined. Finally, future research\ndirections are discussed.", "published": "2023-09-16 02:01:18", "link": "http://arxiv.org/abs/2309.08836v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "PDFTriage: Question Answering over Long, Structured Documents", "abstract": "Large Language Models (LLMs) have issues with document question answering\n(QA) in situations where the document is unable to fit in the small context\nlength of an LLM. To overcome this issue, most existing works focus on\nretrieving the relevant context from the document, representing them as plain\ntext. However, documents such as PDFs, web pages, and presentations are\nnaturally structured with different pages, tables, sections, and so on.\nRepresenting such structured documents as plain text is incongruous with the\nuser's mental model of these documents with rich structure. When a system has\nto query the document for context, this incongruity is brought to the fore, and\nseemingly trivial questions can trip up the QA system. To bridge this\nfundamental gap in handling structured documents, we propose an approach called\nPDFTriage that enables models to retrieve the context based on either structure\nor content. Our experiments demonstrate the effectiveness of the proposed\nPDFTriage-augmented models across several classes of questions where existing\nretrieval-augmented LLMs fail. To facilitate further research on this\nfundamental problem, we release our benchmark dataset consisting of 900+\nhuman-generated questions over 80 structured documents from 10 different\ncategories of question types for document QA. Our code and datasets will be\nreleased soon on Github.", "published": "2023-09-16 04:29:05", "link": "http://arxiv.org/abs/2309.08872v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Statistical Turing Test for Generative Models", "abstract": "The emergence of human-like abilities of AI systems for content generation in\ndomains such as text, audio, and vision has prompted the development of\nclassifiers to determine whether content originated from a human or a machine.\nImplicit in these efforts is an assumption that the generation properties of a\nhuman are different from that of the machine. In this work, we provide a\nframework in the language of statistical pattern recognition that quantifies\nthe difference between the distributions of human and machine-generated content\nconditioned on an evaluation context. We describe current methods in the\ncontext of the framework and demonstrate how to use the framework to evaluate\nthe progression of generative models towards human-like capabilities, among\nmany axes of analysis.", "published": "2023-09-16 07:36:07", "link": "http://arxiv.org/abs/2309.08913v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Improving Speech Recognition for African American English With Audio\n  Classification", "abstract": "Automatic speech recognition (ASR) systems have been shown to have large\nquality disparities between the language varieties they are intended or\nexpected to recognize. One way to mitigate this is to train or fine-tune models\nwith more representative datasets. But this approach can be hindered by limited\nin-domain data for training and evaluation. We propose a new way to improve the\nrobustness of a US English short-form speech recognizer using a small amount of\nout-of-domain (long-form) African American English (AAE) data. We use CORAAL,\nYouTube and Mozilla Common Voice to train an audio classifier to approximately\noutput whether an utterance is AAE or some other variety including Mainstream\nAmerican English (MAE). By combining the classifier output with coarse\ngeographic information, we can select a subset of utterances from a large\ncorpus of untranscribed short-form queries for semi-supervised learning at\nscale. Fine-tuning on this data results in a 38.5% relative word error rate\ndisparity reduction between AAE and MAE without reducing MAE quality.", "published": "2023-09-16 19:57:45", "link": "http://arxiv.org/abs/2309.09996v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Examining the Influence of Varied Levels of Domain Knowledge Base\n  Inclusion in GPT-based Intelligent Tutors", "abstract": "Recent advancements in large language models (LLMs) have facilitated the\ndevelopment of chatbots with sophisticated conversational capabilities.\nHowever, LLMs exhibit frequent inaccurate responses to queries, hindering\napplications in educational settings. In this paper, we investigate the\neffectiveness of integrating a knowledge base (KB) with LLM intelligent tutors\nto increase response reliability. To achieve this, we design a scaleable KB\nthat affords educational supervisors seamless integration of lesson curricula,\nwhich is automatically processed by the intelligent tutoring system. We then\ndetail an evaluation, where student participants were presented with questions\nabout the artificial intelligence curriculum to respond to. GPT-4 intelligent\ntutors with varying hierarchies of KB access and human domain experts then\nassessed these responses. Lastly, students cross-examined the intelligent\ntutors' responses to the domain experts' and ranked their various pedagogical\nabilities. Results suggest that, although these intelligent tutors still\ndemonstrate a lower accuracy compared to domain experts, the accuracy of the\nintelligent tutors increases when access to a KB is granted. We also observe\nthat the intelligent tutors with KB access exhibit better pedagogical abilities\nto speak like a teacher and understand students than those of domain experts,\nwhile their ability to help students remains lagging behind domain experts.", "published": "2023-09-16 17:12:05", "link": "http://arxiv.org/abs/2309.12367v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG", "I.2.7; K.3"], "primary_category": "cs.HC"}
{"title": "Boosting End-to-End Multilingual Phoneme Recognition through Exploiting\n  Universal Speech Attributes Constraints", "abstract": "We propose a first step toward multilingual end-to-end automatic speech\nrecognition (ASR) by integrating knowledge about speech articulators. The key\nidea is to leverage a rich set of fundamental units that can be defined\n\"universally\" across all spoken languages, referred to as speech attributes,\nnamely manner and place of articulation. Specifically, several deterministic\nattribute-to-phoneme mapping matrices are constructed based on the predefined\nset of universal attribute inventory, which projects the knowledge-rich\narticulatory attribute logits, into output phoneme logits. The mapping puts\nknowledge-based constraints to limit inconsistency with acoustic-phonetic\nevidence in the integrated prediction. Combined with phoneme recognition, our\nphone recognizer is able to infer from both attribute and phoneme information.\nThe proposed joint multilingual model is evaluated through phoneme recognition.\nIn multilingual experiments over 6 languages on benchmark datasets LibriSpeech\nand CommonVoice, we find that our proposed solution outperforms conventional\nmultilingual approaches with a relative improvement of 6.85% on average, and it\nalso demonstrates a much better performance compared to monolingual model.\nFurther analysis conclusively demonstrates that the proposed solution\neliminates phoneme predictions that are inconsistent with attributes.", "published": "2023-09-16 01:08:22", "link": "http://arxiv.org/abs/2309.08828v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FastGraphTTS: An Ultrafast Syntax-Aware Speech Synthesis Framework", "abstract": "This paper integrates graph-to-sequence into an end-to-end text-to-speech\nframework for syntax-aware modelling with syntactic information of input text.\nSpecifically, the input text is parsed by a dependency parsing module to form a\nsyntactic graph. The syntactic graph is then encoded by a graph encoder to\nextract the syntactic hidden information, which is concatenated with phoneme\nembedding and input to the alignment and flow-based decoding modules to\ngenerate the raw audio waveform. The model is experimented on two languages,\nEnglish and Mandarin, using single-speaker, few samples of target speakers, and\nmulti-speaker datasets, respectively. Experimental results show better prosodic\nconsistency performance between input text and generated audio, and also get\nhigher scores in the subjective prosodic evaluation, and show the ability of\nvoice conversion. Besides, the efficiency of the model is largely boosted\nthrough the design of the AI chip operator with 5x acceleration.", "published": "2023-09-16 02:10:16", "link": "http://arxiv.org/abs/2309.08837v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Decoder-only Architecture for Speech Recognition with CTC Prompts and\n  Text Data Augmentation", "abstract": "Collecting audio-text pairs is expensive; however, it is much easier to\naccess text-only data. Unless using shallow fusion, end-to-end automatic speech\nrecognition (ASR) models require architecture modifications or additional\ntraining schemes to use text-only data. Inspired by recent advances in\ndecoder-only language models (LMs), such as GPT-3 and PaLM adopted for\nspeech-processing tasks, we propose using a decoder-only architecture for ASR\nwith simple text augmentation. To provide audio information, encoder features\ncompressed by CTC prediction are used as prompts for the decoder, which can be\nregarded as refining CTC prediction using the decoder-only model. Because the\ndecoder architecture is the same as an autoregressive LM, it is simple to\nenhance the model by leveraging external text data with LM training. An\nexperimental comparison using LibriSpeech and Switchboard shows that our\nproposed models with text augmentation training reduced word error rates from\nordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set,\nrespectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model\nhad advantage on computational efficiency compared with conventional\nencoder-decoder ASR models with a similar parameter setup, and outperformed\nthem on the LibriSpeech 100h and Switchboard training scenarios.", "published": "2023-09-16 04:57:37", "link": "http://arxiv.org/abs/2309.08876v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unifying Robustness and Fidelity: A Comprehensive Study of Pretrained\n  Generative Methods for Speech Enhancement in Adverse Conditions", "abstract": "Enhancing speech signal quality in adverse acoustic environments is a\npersistent challenge in speech processing. Existing deep learning based\nenhancement methods often struggle to effectively remove background noise and\nreverberation in real-world scenarios, hampering listening experiences. To\naddress these challenges, we propose a novel approach that uses pre-trained\ngenerative methods to resynthesize clean, anechoic speech from degraded inputs.\nThis study leverages pre-trained vocoder or codec models to synthesize\nhigh-quality speech while enhancing robustness in challenging scenarios.\nGenerative methods effectively handle information loss in speech signals,\nresulting in regenerated speech that has improved fidelity and reduced\nartifacts. By harnessing the capabilities of pre-trained models, we achieve\nfaithful reproduction of the original speech in adverse conditions.\nExperimental evaluations on both simulated datasets and realistic samples\ndemonstrate the effectiveness and robustness of our proposed methods.\nEspecially by leveraging codec, we achieve superior subjective scores for both\nsimulated and realistic recordings. The generated speech exhibits enhanced\naudio quality, reduced background noise, and reverberation. Our findings\nhighlight the potential of pre-trained generative techniques in speech\nprocessing, particularly in scenarios where traditional methods falter. Demos\nare available at https://whmrtm.github.io/SoundResynthesis.", "published": "2023-09-16 15:42:55", "link": "http://arxiv.org/abs/2309.09028v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing GAN-Based Vocoders with Contrastive Learning Under\n  Data-limited Condition", "abstract": "Vocoder models have recently achieved substantial progress in generating\nauthentic audio comparable to human quality while significantly reducing memory\nrequirement and inference time. However, these data-hungry generative models\nrequire large-scale audio data for learning good representations. In this\npaper, we apply contrastive learning methods in training the vocoder to improve\nthe perceptual quality of the vocoder without modifying its architecture or\nadding more data. We design an auxiliary task with mel-spectrogram contrastive\nlearning to enhance the utterance-level quality of the vocoder model under\ndata-limited conditions. We also extend the task to include waveforms to\nimprove the multi-modality comprehension of the model and address the\ndiscriminator overfitting problem. We optimize the additional task\nsimultaneously with GAN training objectives. Our results show that the tasks\nimprove model performance substantially in data-limited settings.", "published": "2023-09-16 20:04:16", "link": "http://arxiv.org/abs/2309.09088v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Contrastive Latent Space Reconstruction Learning for Audio-Text\n  Retrieval", "abstract": "Cross-modal retrieval (CMR) has been extensively applied in various domains,\nsuch as multimedia search engines and recommendation systems. Most existing CMR\nmethods focus on image-to-text retrieval, whereas audio-to-text retrieval, a\nless explored domain, has posed a great challenge due to the difficulty to\nuncover discriminative features from audio clips and texts. Existing studies\nare restricted in the following two ways: 1) Most researchers utilize\ncontrastive learning to construct a common subspace where similarities among\ndata can be measured. However, they considers only cross-modal transformation,\nneglecting the intra-modal separability. Besides, the temperature parameter is\nnot adaptively adjusted along with semantic guidance, which degrades the\nperformance. 2) These methods do not take latent representation reconstruction\ninto account, which is essential for semantic alignment. This paper introduces\na novel audio-text oriented CMR approach, termed Contrastive Latent Space\nReconstruction Learning (CLSR). CLSR improves contrastive representation\nlearning by taking intra-modal separability into account and adopting an\nadaptive temperature control strategy. Moreover, the latent representation\nreconstruction modules are embedded into the CMR framework, which improves\nmodal interaction. Experiments in comparison with some state-of-the-art methods\non two audio-text datasets have validated the superiority of CLSR.", "published": "2023-09-16 02:12:00", "link": "http://arxiv.org/abs/2309.08839v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Regularized Contrastive Pre-training for Few-shot Bioacoustic Sound\n  Detection", "abstract": "Bioacoustic sound event detection allows for better understanding of animal\nbehavior and for better monitoring biodiversity using audio. Deep learning\nsystems can help achieve this goal, however it is difficult to acquire\nsufficient annotated data to train these systems from scratch. To address this\nlimitation, the Detection and Classification of Acoustic Scenes and Events\n(DCASE) community has recasted the problem within the framework of few-shot\nlearning and organize an annual challenge for learning to detect animal sounds\nfrom only five annotated examples. In this work, we regularize supervised\ncontrastive pre-training to learn features that can transfer well on new target\ntasks with animal sounds unseen during training, achieving a high F-score of\n61.52%(0.48) when no feature adaptation is applied, and an F-score of\n68.19%(0.75) when we further adapt the learned features for each new target\ntask. This work aims to lower the entry bar to few-shot bioacoustic sound event\ndetection by proposing a simple and yet effective framework for this task, by\nalso providing open-source code.", "published": "2023-09-16 12:11:11", "link": "http://arxiv.org/abs/2309.08971v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SynthTab: Leveraging Synthesized Data for Guitar Tablature Transcription", "abstract": "Guitar tablature is a form of music notation widely used among guitarists. It\ncaptures not only the musical content of a piece, but also its implementation\nand ornamentation on the instrument. Guitar Tablature Transcription (GTT) is an\nimportant task with broad applications in music education, composition, and\nentertainment. Existing GTT datasets are quite limited in size and scope,\nrendering models trained on them prone to overfitting and incapable of\ngeneralizing to out-of-domain data. In order to address this issue, we present\na methodology for synthesizing large-scale GTT audio using commercial acoustic\nand electric guitar plugins. We procure SynthTab, a dataset derived from\nDadaGP, which is a vast and diverse collection of richly annotated symbolic\ntablature. The proposed synthesis pipeline produces audio which faithfully\nadheres to the original fingerings and a subset of techniques specified in the\ntablature, and covers multiple guitars and styles for each track. Experiments\nshow that pre-training a baseline GTT model on SynthTab can improve\ntranscription performance when fine-tuning and testing on an individual\ndataset. More importantly, cross-dataset experiments show that pre-training\nsignificantly mitigates issues with overfitting.", "published": "2023-09-16 19:40:30", "link": "http://arxiv.org/abs/2309.09085v4", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
