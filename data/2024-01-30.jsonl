{"title": "Gradient-Based Language Model Red Teaming", "abstract": "Red teaming is a common strategy for identifying weaknesses in generative\nlanguage models (LMs), where adversarial prompts are produced that trigger an\nLM to generate unsafe responses. Red teaming is instrumental for both model\nalignment and evaluation, but is labor-intensive and difficult to scale when\ndone by humans. In this paper, we present Gradient-Based Red Teaming (GBRT), a\nred teaming method for automatically generating diverse prompts that are likely\nto cause an LM to output unsafe responses. GBRT is a form of prompt learning,\ntrained by scoring an LM response with a safety classifier and then\nbackpropagating through the frozen safety classifier and LM to update the\nprompt. To improve the coherence of input prompts, we introduce two variants\nthat add a realism loss and fine-tune a pretrained model to generate the\nprompts instead of learning the prompts directly. Our experiments show that\nGBRT is more effective at finding prompts that trigger an LM to generate unsafe\nresponses than a strong reinforcement learning-based red teaming approach, and\nsucceeds even when the LM has been fine-tuned to produce safer outputs.", "published": "2024-01-30 01:19:25", "link": "http://arxiv.org/abs/2401.16656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recent Advances in Hate Speech Moderation: Multimodality and the Role of\n  Large Models", "abstract": "In the evolving landscape of online communication, moderating hate speech\n(HS) presents an intricate challenge, compounded by the multimodal nature of\ndigital content. This comprehensive survey delves into the recent strides in HS\nmoderation, spotlighting the burgeoning role of large language models (LLMs)\nand large multimodal models (LMMs). Our exploration begins with a thorough\nanalysis of current literature, revealing the nuanced interplay between\ntextual, visual, and auditory elements in propagating HS. We uncover a notable\ntrend towards integrating these modalities, primarily due to the complexity and\nsubtlety with which HS is disseminated. A significant emphasis is placed on the\nadvances facilitated by LLMs and LMMs, which have begun to redefine the\nboundaries of detection and moderation capabilities. We identify existing gaps\nin research, particularly in the context of underrepresented languages and\ncultures, and the need for solutions to handle low-resource settings. The\nsurvey concludes with a forward-looking perspective, outlining potential\navenues for future research, including the exploration of novel AI\nmethodologies, the ethical governance of AI in moderation, and the development\nof more nuanced, context-aware systems. This comprehensive overview aims to\ncatalyze further research and foster a collaborative effort towards more\nsophisticated, responsible, and human-centric approaches to HS moderation in\nthe digital era. WARNING: This paper contains offensive examples.", "published": "2024-01-30 03:51:44", "link": "http://arxiv.org/abs/2401.16727v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large\n  Language Models", "abstract": "Large language models (LLMs) are increasingly relied upon for complex\nmulti-turn conversations across diverse real-world applications. However,\nexisting benchmarks predominantly focus on single-turn evaluations, overlooking\nthe models' capabilities in multi-turn interactions. To address this gap, we\nintroduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn\nconversational abilities. By analyzing human-LLM conversations, we categorize\ninteraction patterns into four types: recollection, expansion, refinement, and\nfollow-up. We construct multi-turn queries for each category either by\naugmenting existing datasets or by creating new examples with GPT-4 to avoid\ndata leakage. To study the factors impacting multi-turn abilities, we create\nsingle-turn versions of the 1170 multi-turn queries and compare performance.\nOur evaluation of 11 well-known LLMs shows that while closed-source models\ngenerally surpass open-source ones, certain open-source models exceed\nGPT-3.5-Turbo in specific tasks. We observe significant performance degradation\nin multi-turn settings compared to single-turn settings in most models, which\nis not correlated with the models' fundamental capabilities. Moreover, we\nidentify the distance to relevant content and susceptibility to error\npropagation as the key factors influencing multi-turn performance. MT-Eval is\nreleased publicly to encourage future research towards more robust\nconversational models.", "published": "2024-01-30 04:50:28", "link": "http://arxiv.org/abs/2401.16745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Racist Text in Bengali: An Ensemble Deep Learning Framework", "abstract": "Racism is an alarming phenomenon in our country as well as all over the\nworld. Every day we have come across some racist comments in our daily life and\nvirtual life. Though we can eradicate this racism from virtual life (such as\nSocial Media). In this paper, we have tried to detect those racist comments\nwith NLP and deep learning techniques. We have built a novel dataset in the\nBengali Language. Further, we annotated the dataset and conducted data label\nvalidation. After extensive utilization of deep learning methodologies, we have\nsuccessfully achieved text detection with an impressive accuracy rate of\n87.94\\% using the Ensemble approach. We have applied RNN and LSTM models using\nBERT Embeddings. However, the MCNN-LSTM model performed highest among all those\nmodels. Lastly, the Ensemble approach has been followed to combine all the\nmodel results to increase overall performance.", "published": "2024-01-30 04:56:55", "link": "http://arxiv.org/abs/2401.16748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "State Value Generation with Prompt Learning and Self-Training for\n  Low-Resource Dialogue State Tracking", "abstract": "Recently, low-resource dialogue state tracking (DST) has received increasing\nattention. First obtaining state values then based on values to generate slot\ntypes has made great progress in this task. However, obtaining state values is\nstill an under-studied problem. Existing extraction-based approaches cannot\ncapture values that require the understanding of context and are not\ngeneralizable either. To address these issues, we propose a novel State VAlue\nGeneration based framework (SVAG), decomposing DST into state value generation\nand domain slot generation. Specifically, we propose to generate state values\nand use self-training to further improve state value generation. Moreover, we\ndesign an estimator aiming at detecting incomplete generation and incorrect\ngeneration for pseudo-labeled data selection during self-training. Experimental\nresults on the MultiWOZ 2.1 dataset show that our method which has only less\nthan 1 billion parameters achieves state-of-the-art performance under the data\nratio settings of 5%, 10%, and 25% when limited to models under 100 billion\nparameters. Compared to models with more than 100 billion parameters, SVAG\nstill reaches competitive results.", "published": "2024-01-30 10:05:03", "link": "http://arxiv.org/abs/2401.16862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Transfer from Related Languages: Treating Low-Resource\n  Maltese as Multilingual Code-Switching", "abstract": "Although multilingual language models exhibit impressive cross-lingual\ntransfer capabilities on unseen languages, the performance on downstream tasks\nis impacted when there is a script disparity with the languages used in the\nmultilingual model's pre-training data. Using transliteration offers a\nstraightforward yet effective means to align the script of a resource-rich\nlanguage with a target language, thereby enhancing cross-lingual transfer\ncapabilities. However, for mixed languages, this approach is suboptimal, since\nonly a subset of the language benefits from the cross-lingual transfer while\nthe remainder is impeded. In this work, we focus on Maltese, a Semitic\nlanguage, with substantial influences from Arabic, Italian, and English, and\nnotably written in Latin script. We present a novel dataset annotated with\nword-level etymology. We use this dataset to train a classifier that enables us\nto make informed decisions regarding the appropriate processing of each token\nin the Maltese language. We contrast indiscriminate transliteration or\ntranslation to mixing processing pipelines that only transliterate words of\nArabic origin, thereby resulting in text with a mixture of scripts. We\nfine-tune the processed data on four downstream tasks and show that conditional\ntransliteration based on word etymology yields the best results, surpassing\nfine-tuning with raw Maltese or Maltese processed with non-selective pipelines.", "published": "2024-01-30 11:04:36", "link": "http://arxiv.org/abs/2401.16895v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distinguishing Fictional Voices: a Study of Authorship Verification\n  Models for Quotation Attribution", "abstract": "Recent approaches to automatically detect the speaker of an utterance of\ndirect speech often disregard general information about characters in favor of\nlocal information found in the context, such as surrounding mentions of\nentities. In this work, we explore stylistic representations of characters\nbuilt by encoding their quotes with off-the-shelf pretrained Authorship\nVerification models in a large corpus of English novels (the Project Dialogism\nNovel Corpus). Results suggest that the combination of stylistic and topical\ninformation captured in some of these models accurately distinguish characters\namong each other, but does not necessarily improve over semantic-only models\nwhen attributing quotes. However, these results vary across novels and more\ninvestigation of stylometric models particularly tailored for literary texts\nand the study of characters should be conducted.", "published": "2024-01-30 12:49:40", "link": "http://arxiv.org/abs/2401.16968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taking Action Towards Graceful Interaction: The Effects of Performing\n  Actions on Modelling Policies for Instruction Clarification Requests", "abstract": "Clarification requests are a mechanism to help solve communication problems,\ne.g. due to ambiguity or underspecification, in instruction-following\ninteractions. Despite their importance, even skilful models struggle with\nproducing or interpreting such repair acts. In this work, we test three\nhypotheses concerning the effects of action taking as an auxiliary task in\nmodelling iCR policies. Contrary to initial expectations, we conclude that its\ncontribution to learning an iCR policy is limited, but some information can\nstill be extracted from prediction uncertainty. We present further evidence\nthat even well-motivated, Transformer-based models fail to learn good policies\nfor when to ask Instruction CRs (iCRs), while the task of determining what to\nask about can be more successfully modelled. Considering the implications of\nthese findings, we further discuss the shortcomings of the data-driven paradigm\nfor learning meta-communication acts.", "published": "2024-01-30 14:18:31", "link": "http://arxiv.org/abs/2401.17039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented\n  Generation of Large Language Models", "abstract": "Retrieval-Augmented Generation (RAG) is a technique that enhances the\ncapabilities of large language models (LLMs) by incorporating external\nknowledge sources. This method addresses common LLM limitations, including\noutdated information and the tendency to produce inaccurate \"hallucinated\"\ncontent. However, the evaluation of RAG systems is challenging, as existing\nbenchmarks are limited in scope and diversity. Most of the current benchmarks\npredominantly assess question-answering applications, overlooking the broader\nspectrum of situations where RAG could prove advantageous. Moreover, they only\nevaluate the performance of the LLM component of the RAG pipeline in the\nexperiments, and neglect the influence of the retrieval component and the\nexternal knowledge database. To address these issues, this paper constructs a\nlarge-scale and more comprehensive benchmark, and evaluates all the components\nof RAG systems in various RAG application scenarios. Specifically, we have\ncategorized the range of RAG applications into four distinct types-Create,\nRead, Update, and Delete (CRUD), each representing a unique use case. \"Create\"\nrefers to scenarios requiring the generation of original, varied content.\n\"Read\" involves responding to intricate questions in knowledge-intensive\nsituations. \"Update\" focuses on revising and rectifying inaccuracies or\ninconsistencies in pre-existing texts. \"Delete\" pertains to the task of\nsummarizing extensive texts into more concise forms. For each of these CRUD\ncategories, we have developed comprehensive datasets to evaluate the\nperformance of RAG systems. We also analyze the effects of various components\nof the RAG system, such as the retriever, the context length, the knowledge\nbase construction, and the LLM. Finally, we provide useful insights for\noptimizing the RAG technology for different scenarios.", "published": "2024-01-30 14:25:32", "link": "http://arxiv.org/abs/2401.17043v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemScore: Automated Evaluation of Instruction-Tuned LLMs based on\n  Semantic Textual Similarity", "abstract": "Instruction-tuned Large Language Models (LLMs) have recently showcased\nremarkable advancements in their ability to generate fitting responses to\nnatural language instructions. However, many current works rely on manual\nevaluation to judge the quality of generated responses. Since such manual\nevaluation is time-consuming, it does not easily scale to the evaluation of\nmultiple models and model variants. In this short paper, we propose a\nstraightforward but remarkably effective evaluation metric called SemScore, in\nwhich we directly compare model outputs to gold target responses using semantic\ntextual similarity (STS). We conduct a comparative evaluation of the model\noutputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation\nmetrics for text generation. We find that our proposed SemScore metric\noutperforms all other, in many cases more complex, evaluation metrics in terms\nof correlation to human evaluation. These findings indicate the utility of our\nproposed metric for the evaluation of instruction-tuned LLMs.", "published": "2024-01-30 14:52:50", "link": "http://arxiv.org/abs/2401.17072v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NNOSE: Nearest Neighbor Occupational Skill Extraction", "abstract": "The labor market is changing rapidly, prompting increased interest in the\nautomatic extraction of occupational skills from text. With the advent of\nEnglish benchmark job description datasets, there is a need for systems that\nhandle their diversity well. We tackle the complexity in occupational skill\ndatasets tasks -- combining and leveraging multiple datasets for skill\nextraction, to identify rarely observed skills within a dataset, and overcoming\nthe scarcity of skills across datasets. In particular, we investigate the\nretrieval-augmentation of language models, employing an external datastore for\nretrieving similar skills in a dataset-unifying manner. Our proposed method,\n\\textbf{N}earest \\textbf{N}eighbor \\textbf{O}ccupational \\textbf{S}kill\n\\textbf{E}xtraction (NNOSE) effectively leverages multiple datasets by\nretrieving neighboring skills from other datasets in the datastore. This\nimproves skill extraction \\emph{without} additional fine-tuning. Crucially, we\nobserve a performance gain in predicting infrequent patterns, with substantial\ngains of up to 30\\% span-F1 in cross-dataset settings.", "published": "2024-01-30 15:18:29", "link": "http://arxiv.org/abs/2401.17092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MT-Ranker: Reference-free machine translation evaluation by inter-system\n  ranking", "abstract": "Traditionally, Machine Translation (MT) Evaluation has been treated as a\nregression problem -- producing an absolute translation-quality score. This\napproach has two limitations: i) the scores lack interpretability, and human\nannotators struggle with giving consistent scores; ii) most scoring methods are\nbased on (reference, translation) pairs, limiting their applicability in\nreal-world scenarios where references are absent. In practice, we often care\nabout whether a new MT system is better or worse than some competitors. In\naddition, reference-free MT evaluation is increasingly practical and necessary.\nUnfortunately, these two practical considerations have yet to be jointly\nexplored. In this work, we formulate the reference-free MT evaluation into a\npairwise ranking problem. Given the source sentence and a pair of translations,\nour system predicts which translation is better. In addition to proposing this\nnew formulation, we further show that this new paradigm can demonstrate\nsuperior correlation with human judgments by merely using indirect supervision\nfrom natural language inference and weak supervision from our synthetic data.\nIn the context of reference-free evaluation, MT-Ranker, trained without any\nhuman annotations, achieves state-of-the-art results on the WMT Shared Metrics\nTask benchmarks DARR20, MQM20, and MQM21. On a more challenging benchmark,\nACES, which contains fine-grained evaluation criteria such as addition,\nomission, and mistranslation errors, MT-Ranker marks state-of-the-art against\nreference-free as well as reference-based baselines.", "published": "2024-01-30 15:30:03", "link": "http://arxiv.org/abs/2401.17099v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool\n  Utilization in Real-World Complex Scenarios", "abstract": "The recent trend of using Large Language Models (LLMs) as tool agents in\nreal-world applications underscores the necessity for comprehensive evaluations\nof their capabilities, particularly in complex scenarios involving planning,\ncreating, and using tools. However, existing benchmarks typically focus on\nsimple synthesized queries that do not reflect real-world complexity, thereby\noffering limited perspectives in evaluating tool utilization. To address this\nissue, we present UltraTool, a novel benchmark designed to improve and evaluate\nLLMs' ability in tool utilization within real-world scenarios. UltraTool\nfocuses on the entire process of using tools - from planning and creating to\napplying them in complex tasks. It emphasizes real-world complexities,\ndemanding accurate, multi-step planning for effective problem-solving. A key\nfeature of UltraTool is its independent evaluation of planning with natural\nlanguage, which happens before tool usage and simplifies the task solving by\nmapping out the intermediate steps. Thus, unlike previous work, it eliminates\nthe restriction of pre-defined toolset. Through extensive experiments on\nvarious LLMs, we offer novel insights into the evaluation of capabilities of\nLLMs in tool utilization, thereby contributing a fresh perspective to this\nrapidly evolving field. The benchmark is publicly available at\nhttps://github.com/JoeYing1019/UltraTool.", "published": "2024-01-30 16:52:56", "link": "http://arxiv.org/abs/2401.17167v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Text Diffusion Models", "abstract": "In this report, we explore the potential for text diffusion to replace\nautoregressive (AR) decoding for the training and deployment of large language\nmodels (LLMs). We are particularly interested to see whether pretrained AR\nmodels can be transformed into text diffusion models through a lightweight\nadaptation procedure we call ``AR2Diff''. We begin by establishing a strong\nbaseline setup for training text diffusion models. Comparing across multiple\narchitectures and pretraining objectives, we find that training a decoder-only\nmodel with a prefix LM objective is best or near-best across several tasks.\nBuilding on this finding, we test various transfer learning setups for text\ndiffusion models. On machine translation, we find that text diffusion\nunderperforms the standard AR approach. However, on code synthesis and\nextractive QA, we find diffusion models trained from scratch outperform AR\nmodels in many cases. We also observe quality gains from AR2Diff -- adapting AR\nmodels to use diffusion decoding. These results are promising given that text\ndiffusion is relatively underexplored and can be significantly faster than AR\ndecoding for long text generation.", "published": "2024-01-30 17:11:56", "link": "http://arxiv.org/abs/2401.17181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Single Word Change is All You Need: Designing Attacks and Defenses for\n  Text Classifiers", "abstract": "In text classification, creating an adversarial example means subtly\nperturbing a few words in a sentence without changing its meaning, causing it\nto be misclassified by a classifier. A concerning observation is that a\nsignificant portion of adversarial examples generated by existing methods\nchange only one word. This single-word perturbation vulnerability represents a\nsignificant weakness in classifiers, which malicious users can exploit to\nefficiently create a multitude of adversarial examples. This paper studies this\nproblem and makes the following key contributions: (1) We introduce a novel\nmetric \\r{ho} to quantitatively assess a classifier's robustness against\nsingle-word perturbation. (2) We present the SP-Attack, designed to exploit the\nsingle-word perturbation vulnerability, achieving a higher attack success rate,\nbetter preserving sentence meaning, while reducing computation costs compared\nto state-of-the-art adversarial methods. (3) We propose SP-Defense, which aims\nto improve \\r{ho} by applying data augmentation in learning. Experimental\nresults on 4 datasets and BERT and distilBERT classifiers show that SP-Defense\nimproves \\r{ho} by 14.6% and 13.9% and decreases the attack success rate of\nSP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the\nattack success rate of existing attack methods that involve multiple-word\nperturbations.", "published": "2024-01-30 17:30:44", "link": "http://arxiv.org/abs/2401.17196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model", "abstract": "Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.", "published": "2024-01-30 17:47:07", "link": "http://arxiv.org/abs/2401.17206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weak-to-Strong Jailbreaking on Large Language Models", "abstract": "Large language models (LLMs) are vulnerable to jailbreak attacks - resulting\nin harmful, unethical, or biased text generations. However, existing\njailbreaking methods are computationally costly. In this paper, we propose the\nweak-to-strong jailbreaking attack, an efficient method to attack aligned LLMs\nto produce harmful text. Our key intuition is based on the observation that\njailbroken and aligned models only differ in their initial decoding\ndistributions. The weak-to-strong attack's key technical insight is using two\nsmaller models (a safe and an unsafe one) to adversarially modify a\nsignificantly larger safe model's decoding probabilities. We evaluate the\nweak-to-strong attack on 5 diverse LLMs from 3 organizations. The results show\nour method can increase the misalignment rate to over 99% on two datasets with\njust one forward pass per example. Our study exposes an urgent safety issue\nthat needs to be addressed when aligning LLMs. As an initial attempt, we\npropose a defense strategy to protect against such attacks, but creating more\nadvanced defenses remains challenging. The code for replicating the method is\navailable at https://github.com/XuandongZhao/weak-to-strong", "published": "2024-01-30 18:48:37", "link": "http://arxiv.org/abs/2401.17256v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Tool Use with Chain-of-Abstraction Reasoning", "abstract": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.", "published": "2024-01-30 21:53:30", "link": "http://arxiv.org/abs/2401.17464v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving QA Model Performance with Cartographic Inoculation", "abstract": "QA models are faced with complex and open-ended contextual reasoning\nproblems, but can often learn well-performing solution heuristics by exploiting\ndataset-specific patterns in their training data. These patterns, or \"dataset\nartifacts\", reduce the model's ability to generalize to real-world QA problems.\nUtilizing an ElectraSmallDiscriminator model trained for QA, we analyze the\nimpacts and incidence of dataset artifacts using an adversarial challenge set\ndesigned to confuse models reliant on artifacts for prediction. Extending\nexisting work on methods for mitigating artifact impacts, we propose\ncartographic inoculation, a novel method that fine-tunes models on an optimized\nsubset of the challenge data to reduce model reliance on dataset artifacts. We\nshow that by selectively fine-tuning a model on ambiguous adversarial examples\nfrom a challenge set, significant performance improvements can be made on the\nfull challenge dataset with minimal loss of model generalizability to other\nchallenging environments and QA datasets.", "published": "2024-01-30 23:08:26", "link": "http://arxiv.org/abs/2401.17498v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "D-Nikud: Enhancing Hebrew Diacritization with LSTM and Pretrained Models", "abstract": "D-Nikud, a novel approach to Hebrew diacritization that integrates the\nstrengths of LSTM networks and BERT-based (transformer) pre-trained model.\nInspired by the methodologies employed in Nakdimon, we integrate it with the\nTavBERT pre-trained model, our system incorporates advanced architectural\nchoices and diverse training data. Our experiments showcase state-of-the-art\nresults on several benchmark datasets, with a particular emphasis on modern\ntexts and more specified diacritization like gender.", "published": "2024-01-30 22:07:12", "link": "http://arxiv.org/abs/2402.00075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking Free Transformer Models: Task-specific Context Attribution\n  Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs", "abstract": "Fine-tuning large pre-trained language models (LLMs) on particular datasets\nis a commonly employed strategy in Natural Language Processing (NLP)\nclassification tasks. However, this approach usually results in a loss of\nmodels generalizability. In this paper, we present a framework that allows for\nmaintaining generalizability, and enhances the performance on the downstream\ntask by utilizing task-specific context attribution. We show that a linear\ntransformation of the text representation from any transformer model using the\ntask-specific concept operator results in a projection onto the latent concept\nspace, referred to as context attribution in this paper. The specific concept\noperator is optimized during the supervised learning stage via novel loss\nfunctions. The proposed framework demonstrates that context attribution of the\ntext representation for each task objective can improve the capacity of the\ndiscriminator function and thus achieve better performance for the\nclassification task. Experimental results on three datasets, namely HateXplain,\nIMDB reviews, and Social Media Attributions, illustrate that the proposed model\nattains superior accuracy and generalizability. Specifically, for the\nnon-fine-tuned BERT on the HateXplain dataset, we observe 8% improvement in\naccuracy and 10% improvement in F1-score. Whereas for the IMDB dataset,\nfine-tuned state-of-the-art XLNet is outperformed by 1% for both accuracy and\nF1-score. Furthermore, in an out-of-domain cross-dataset test, DistilBERT\nfine-tuned on the IMDB dataset in conjunction with the proposed model improves\nthe F1-score on the HateXplain dataset by 7%. For the Social Media Attributions\ndataset of YouTube comments, we observe 5.2% increase in F1-metric. The\nproposed framework is implemented with PyTorch and provided open-source on\nGitHub.", "published": "2024-01-30 00:23:29", "link": "http://arxiv.org/abs/2401.16638v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.4"], "primary_category": "cs.CL"}
{"title": "TeenyTinyLlama: open-source tiny language models trained in Brazilian\n  Portuguese", "abstract": "Large language models (LLMs) have significantly advanced natural language\nprocessing, but their progress has yet to be equal across languages. While most\nLLMs are trained in high-resource languages like English, multilingual models\ngenerally underperform monolingual ones. Additionally, aspects of their\nmultilingual foundation sometimes restrict the byproducts they produce, like\ncomputational demands and licensing regimes. In this study, we document the\ndevelopment of open-foundation models tailored for use in low-resource\nsettings, their limitations, and their benefits. This is the TeenyTinyLlama\npair: two compact models for Brazilian Portuguese text generation. We release\nthem under the permissive Apache 2.0 license on GitHub and Hugging Face for\ncommunity use and further development. See\nhttps://github.com/Nkluge-correa/TeenyTinyLlama", "published": "2024-01-30 00:25:54", "link": "http://arxiv.org/abs/2401.16640v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Incoherent Probability Judgments in Large Language Models", "abstract": "Autoregressive Large Language Models (LLMs) trained for next-word prediction\nhave demonstrated remarkable proficiency at producing coherent text. But are\nthey equally adept at forming coherent probability judgments? We use\nprobabilistic identities and repeated judgments to assess the coherence of\nprobability judgments made by LLMs. Our results show that the judgments\nproduced by these models are often incoherent, displaying human-like systematic\ndeviations from the rules of probability theory. Moreover, when prompted to\njudge the same event, the mean-variance relationship of probability judgments\nproduced by LLMs shows an inverted-U-shaped like that seen in humans. We\npropose that these deviations from rationality can be explained by linking\nautoregressive LLMs to implicit Bayesian inference and drawing parallels with\nthe Bayesian Sampler model of human probability judgments.", "published": "2024-01-30 00:40:49", "link": "http://arxiv.org/abs/2401.16646v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recovering Mental Representations from Large Language Models with Markov\n  Chain Monte Carlo", "abstract": "Simulating sampling algorithms with people has proven a useful method for\nefficiently probing and understanding their mental representations. We propose\nthat the same methods can be used to study the representations of Large\nLanguage Models (LLMs). While one can always directly prompt either humans or\nLLMs to disclose their mental representations introspectively, we show that\nincreased efficiency can be achieved by using LLMs as elements of a sampling\nalgorithm. We explore the extent to which we recover human-like representations\nwhen LLMs are interrogated with Direct Sampling and Markov chain Monte Carlo\n(MCMC). We found a significant increase in efficiency and performance using\nadaptive sampling algorithms based on MCMC. We also highlight the potential of\nour method to yield a more general method of conducting Bayesian inference\n\\textit{with} LLMs.", "published": "2024-01-30 01:22:18", "link": "http://arxiv.org/abs/2401.16657v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on\n  E-Branchformer", "abstract": "Recent studies have highlighted the importance of fully open foundation\nmodels. The Open Whisper-style Speech Model (OWSM) is an initial step towards\nreproducing OpenAI Whisper using public data and open-source toolkits. However,\nprevious versions of OWSM (v1 to v3) are still based on standard Transformer,\nwhich might lead to inferior performance compared to state-of-the-art speech\nencoder architectures. This work aims to improve the performance and efficiency\nof OWSM without additional data. We present a series of E-Branchformer-based\nmodels named OWSM v3.1, ranging from 100M to 1B parameters. OWSM v3.1\noutperforms its predecessor, OWSM v3, in most evaluation benchmarks, while\nshowing an improved inference speed of up to 25%. We further reveal the\nemergent ability of OWSM v3.1 in zero-shot contextual biasing speech\nrecognition. We also provide a model trained on a subset of data with low\nlicense restrictions. We will publicly release the code, pre-trained models,\nand training logs.", "published": "2024-01-30 01:22:18", "link": "http://arxiv.org/abs/2401.16658v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "History-Aware Conversational Dense Retrieval", "abstract": "Conversational search facilitates complex information retrieval by enabling\nmulti-turn interactions between users and the system. Supporting such\ninteractions requires a comprehensive understanding of the conversational\ninputs to formulate a good search query based on historical information. In\nparticular, the search query should include the relevant information from the\nprevious conversation turns. However, current approaches for conversational\ndense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever\nusing the whole conversational search session, which can be lengthy and noisy.\nMoreover, existing approaches are limited by the amount of manual supervision\nsignals in the existing datasets. To address the aforementioned issues, we\npropose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which\nincorporates two ideas: context-denoised query reformulation and automatic\nmining of supervision signals based on the actual impact of historical turns.\nExperiments on two public conversational search datasets demonstrate the\nimproved history modeling capability of HAConvDR, in particular for long\nconversations with topic shifts.", "published": "2024-01-30 01:24:18", "link": "http://arxiv.org/abs/2401.16659v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Detection and Understanding of Fictional Discourse", "abstract": "In this paper, we present a variety of classification experiments related to\nthe task of fictional discourse detection. We utilize a diverse array of\ndatasets, including contemporary professionally published fiction, historical\nfiction from the Hathi Trust, fanfiction, stories from Reddit, folk tales,\nGPT-generated stories, and anglophone world literature. Additionally, we\nintroduce a new feature set of word \"supersenses\" that facilitate the goal of\nsemantic generalization. The detection of fictional discourse can help enrich\nour knowledge of large cultural heritage archives and assist with the process\nof understanding the distinctive qualities of fictional storytelling more\nbroadly.", "published": "2024-01-30 01:57:17", "link": "http://arxiv.org/abs/2401.16678v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Generating Informative Textual Description for Neurons in\n  Language Models", "abstract": "Recent developments in transformer-based language models have allowed them to\ncapture a wide variety of world knowledge that can be adapted to downstream\ntasks with limited resources. However, what pieces of information are\nunderstood in these models is unclear, and neuron-level contributions in\nidentifying them are largely unknown. Conventional approaches in neuron\nexplainability either depend on a finite set of pre-defined descriptors or\nrequire manual annotations for training a secondary model that can then explain\nthe neurons of the primary model. In this paper, we take BERT as an example and\nwe try to remove these constraints and propose a novel and scalable framework\nthat ties textual descriptions to neurons. We leverage the potential of\ngenerative language models to discover human-interpretable descriptors present\nin a dataset and use an unsupervised approach to explain neurons with these\ndescriptors. Through various qualitative and quantitative analyses, we\ndemonstrate the effectiveness of this framework in generating useful\ndata-specific descriptors with little human involvement in identifying the\nneurons that encode these descriptors. In particular, our experiment shows that\nthe proposed approach achieves 75% precision@2, and 50% recall@2", "published": "2024-01-30 04:06:25", "link": "http://arxiv.org/abs/2401.16731v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models be Trusted for Evaluation? Scalable\n  Meta-Evaluation of LLMs as Evaluators via Agent Debate", "abstract": "Despite the utility of Large Language Models (LLMs) across a wide range of\ntasks and scenarios, developing a method for reliably evaluating LLMs across\nvaried contexts continues to be challenging. Modern evaluation approaches often\nuse LLMs to assess responses generated by LLMs. However, the meta-evaluation\nconducted to assess the effectiveness of these LLMs as evaluators is typically\nconstrained by the coverage of existing benchmarks or requires extensive human\nannotation. This underscores the urgency of methods for scalable\nmeta-evaluation that can effectively, reliably, and efficiently evaluate the\nperformance of LLMs as evaluators across diverse tasks and scenarios,\nparticularly in potentially new, user-defined scenarios. To fill this gap, we\npropose ScaleEval, an agent-debate-assisted meta-evaluation framework that\nleverages the capabilities of multiple communicative LLM agents. This framework\nsupports multi-round discussions to assist human annotators in discerning the\nmost capable LLMs as evaluators, which significantly eases their workload in\ncases that used to require large-scale annotations during meta-evaluation. We\nrelease the code for our framework, which is publicly available at:\n\\url{https://github.com/GAIR-NLP/scaleeval}.", "published": "2024-01-30 07:03:32", "link": "http://arxiv.org/abs/2401.16788v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "H2O-Danube-1.8B Technical Report", "abstract": "We present H2O-Danube, a series of small 1.8B language models consisting of\nH2O-Danube-1.8B, trained on 1T tokens, and the incremental improved\nH2O-Danube2-1.8B trained on an additional 2T tokens. Our models exhibit highly\ncompetitive metrics across a multitude of benchmarks and, as of the time of\nthis writing, H2O-Danube2-1.8B achieves the top ranking on Open LLM Leaderboard\nfor all models below the 2B parameter range. The models follow core principles\nof LLama 2 and Mistral, and we leverage and refine various techniques for\npre-training large language models. We additionally release chat models trained\nwith supervised fine-tuning followed by direct preference optimization. We make\nall models openly available under Apache 2.0 license further democratizing LLMs\nto a wider audience economically.", "published": "2024-01-30 08:45:08", "link": "http://arxiv.org/abs/2401.16818v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Two Heads Are Better Than One: Integrating Knowledge from Knowledge\n  Graphs and Large Language Models for Entity Alignment", "abstract": "Entity alignment, which is a prerequisite for creating a more comprehensive\nKnowledge Graph (KG), involves pinpointing equivalent entities across disparate\nKGs. Contemporary methods for entity alignment have predominantly utilized\nknowledge embedding models to procure entity embeddings that encapsulate\nvarious similarities-structural, relational, and attributive. These embeddings\nare then integrated through attention-based information fusion mechanisms.\nDespite this progress, effectively harnessing multifaceted information remains\nchallenging due to inherent heterogeneity. Moreover, while Large Language\nModels (LLMs) have exhibited exceptional performance across diverse downstream\ntasks by implicitly capturing entity semantics, this implicit knowledge has yet\nto be exploited for entity alignment. In this study, we propose a Large\nLanguage Model-enhanced Entity Alignment framework (LLMEA), integrating\nstructural knowledge from KGs with semantic knowledge from LLMs to enhance\nentity alignment. Specifically, LLMEA identifies candidate alignments for a\ngiven entity by considering both embedding similarities between entities across\nKGs and edit distances to a virtual equivalent entity. It then engages an LLM\niteratively, posing multiple multi-choice questions to draw upon the LLM's\ninference capability. The final prediction of the equivalent entity is derived\nfrom the LLM's output. Experiments conducted on three public datasets reveal\nthat LLMEA surpasses leading baseline models. Additional ablation studies\nunderscore the efficacy of our proposed framework.", "published": "2024-01-30 12:41:04", "link": "http://arxiv.org/abs/2401.16960v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis", "abstract": "To leverage LLMs for visual synthesis, traditional methods convert raster\nimage information into discrete grid tokens through specialized visual modules,\nwhile disrupting the model's ability to capture the true semantic\nrepresentation of visual scenes. This paper posits that an alternative\nrepresentation of images, vector graphics, can effectively surmount this\nlimitation by enabling a more natural and semantically coherent segmentation of\nthe image information. Thus, we introduce StrokeNUWA, a pioneering work\nexploring a better visual representation ''stroke tokens'' on vector graphics,\nwhich is inherently visual semantics rich, naturally compatible with LLMs, and\nhighly compressed. Equipped with stroke tokens, StrokeNUWA can significantly\nsurpass traditional LLM-based and optimization-based methods across various\nmetrics in the vector graphic generation task. Besides, StrokeNUWA achieves up\nto a 94x speedup in inference over the speed of prior methods with an\nexceptional SVG code compression ratio of 6.9%.", "published": "2024-01-30 15:20:26", "link": "http://arxiv.org/abs/2401.17093v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding\n  Space using Contrastive Learning", "abstract": "Recent advances in NLP show that language models retain a discernible level\nof knowledge in deontological ethics and moral norms. However, existing works\noften treat morality as binary, ranging from right to wrong. This simplistic\nview does not capture the nuances of moral judgment. Pluralist moral\nphilosophers argue that human morality can be deconstructed into a finite\nnumber of elements, respecting individual differences in moral judgment. In\nline with this view, we build a pluralist moral sentence embedding space via a\nstate-of-the-art contrastive learning approach. We systematically investigate\nthe embedding space by studying the emergence of relationships among moral\nelements, both quantitatively and qualitatively. Our results show that a\npluralist approach to morality can be captured in an embedding space. However,\nmoral pluralism is challenging to deduce via self-supervision alone and\nrequires a supervised approach with human labels.", "published": "2024-01-30 18:15:25", "link": "http://arxiv.org/abs/2401.17228v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for\n  Classifying Arabic Speech Acts on Twitter", "abstract": "Speech acts are a speakers actions when performing an utterance within a\nconversation, such as asking, recommending, greeting, or thanking someone,\nexpressing a thought, or making a suggestion. Understanding speech acts helps\ninterpret the intended meaning and actions behind a speakers or writers words.\nThis paper proposes a Twitter dialectal Arabic speech act classification\napproach based on a transformer deep learning neural network. Twitter and\nsocial media, are becoming more and more integrated into daily life. As a\nresult, they have evolved into a vital source of information that represents\nthe views and attitudes of their users. We proposed a BERT based weighted\nensemble learning approach to integrate the advantages of various BERT models\nin dialectal Arabic speech acts classification. We compared the proposed model\nagainst several variants of Arabic BERT models and sequence-based models. We\ndeveloped a dialectal Arabic tweet act dataset by annotating a subset of a\nlarge existing Arabic sentiment analysis dataset (ASAD) based on six speech act\ncategories. We also evaluated the models on a previously developed Arabic Tweet\nAct dataset (ArSAS). To overcome the class imbalance issue commonly observed in\nspeech act problems, a transformer-based data augmentation model was\nimplemented to generate an equal proportion of speech act categories. The\nresults show that the best BERT model is araBERTv2-Twitter models with a\nmacro-averaged F1 score and an accuracy of 0.73 and 0.84, respectively. The\nperformance improved using a BERT-based ensemble method with a 0.74 and 0.85\naveraged F1 score and accuracy on our dataset, respectively.", "published": "2024-01-30 19:01:24", "link": "http://arxiv.org/abs/2401.17373v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Customizing Language Model Responses with Contrastive In-Context\n  Learning", "abstract": "Large language models (LLMs) are becoming increasingly important for machine\nlearning applications. However, it can be challenging to align LLMs with our\nintent, particularly when we want to generate content that is preferable over\nothers or when we want the LLM to respond in a certain style or tone that is\nhard to describe. To address this challenge, we propose an approach that uses\ncontrastive examples to better describe our intent. This involves providing\npositive examples that illustrate the true intent, along with negative examples\nthat show what characteristics we want LLMs to avoid. The negative examples can\nbe retrieved from labeled data, written by a human, or generated by the LLM\nitself. Before generating an answer, we ask the model to analyze the examples\nto teach itself what to avoid. This reasoning step provides the model with the\nappropriate articulation of the user's need and guides it towards generting a\nbetter answer. We tested our approach on both synthesized and real-world\ndatasets, including StackExchange and Reddit, and found that it significantly\nimproves performance compared to standard few-shot prompting", "published": "2024-01-30 19:13:12", "link": "http://arxiv.org/abs/2401.17390v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Transformer-based Encoder for Turkish Language Understanding\n  Tasks", "abstract": "Deep learning-based and lately Transformer-based language models have been\ndominating the studies of natural language processing in the last years. Thanks\nto their accurate and fast fine-tuning characteristics, they have outperformed\ntraditional machine learning-based approaches and achieved state-of-the-art\nresults for many challenging natural language understanding (NLU) problems.\nRecent studies showed that the Transformer-based models such as BERT, which is\nBidirectional Encoder Representations from Transformers, have reached\nimpressive achievements on many tasks. Moreover, thanks to their transfer\nlearning capacity, these architectures allow us to transfer pre-built models\nand fine-tune them to specific NLU tasks such as question answering. In this\nstudy, we provide a Transformer-based model and a baseline benchmark for the\nTurkish Language. We successfully fine-tuned a Turkish BERT model, namely\nBERTurk that is trained with base settings, to many downstream tasks and\nevaluated with a the Turkish Benchmark dataset. We showed that our studies\nsignificantly outperformed other existing baseline approaches for Named-Entity\nRecognition, Sentiment Analysis, Question Answering and Text Classification in\nTurkish Language. We publicly released these four fine-tuned models and\nresources in reproducibility and with the view of supporting other Turkish\nresearchers and applications.", "published": "2024-01-30 19:27:04", "link": "http://arxiv.org/abs/2401.17396v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synthetic Dialogue Dataset Generation using LLM Agents", "abstract": "Linear programming (LP) problems are pervasive in real-life applications.\nHowever, despite their apparent simplicity, an untrained user may find it\ndifficult to determine the linear model of their specific problem. We envisage\nthe creation of a goal-oriented conversational agent that will engage in\nconversation with the user to elicit all information required so that a\nsubsequent agent can generate the linear model. In this paper, we present an\napproach for the generation of sample dialogues that can be used to develop and\ntrain such a conversational agent. Using prompt engineering, we develop two\nagents that \"talk\" to each other, one acting as the conversational agent, and\nthe other acting as the user. Using a set of text descriptions of linear\nproblems from NL4Opt available to the user only, the agent and the user engage\nin conversation until the agent has retrieved all key information from the\noriginal problem description. We also propose an extrinsic evaluation of the\ndialogues by assessing how well the summaries generated by the dialogues match\nthe original problem descriptions. We conduct human and automatic evaluations,\nincluding an evaluation approach that uses GPT-4 to mimic the human evaluation\nmetrics. The evaluation results show an overall good quality of the dialogues,\nthough research is still needed to improve the quality of the GPT-4 evaluation\nmetrics. The resulting dialogues, including the human annotations of a subset,\nare available to the research community. The conversational agent used for the\ngeneration of the dialogues can be used as a baseline.", "published": "2024-01-30 21:49:30", "link": "http://arxiv.org/abs/2401.17461v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using\n  Large Language Models", "abstract": "Current communication technologies face limitations in terms of theoretical\ncapacity, spectrum availability, and power resources. Pragmatic communication,\nleveraging terminal intelligence for selective data transmission, offers\nresource conservation. Existing research lacks universal intention resolution\ntools, limiting applicability to specific tasks. This paper proposes an image\npragmatic communication framework based on a Pragmatic Agent for Communication\nEfficiency (PACE) using Large Language Models (LLM). In this framework, PACE\nsequentially performs semantic perception, intention resolution, and\nintention-oriented coding. To ensure the effective utilization of LLM in\ncommunication, a knowledge base is designed to supplement the necessary\nknowledge, dedicated prompts are introduced to facilitate understanding of\npragmatic communication scenarios and task requirements, and a chain of thought\nis designed to assist in making reasonable trade-offs between transmission\nefficiency and cost. For experimental validation, this paper constructs an\nimage pragmatic communication dataset along with corresponding evaluation\nstandards. Simulation results indicate that the proposed method outperforms\ntraditional and non-LLM-based pragmatic communication in terms of transmission\nefficiency.", "published": "2024-01-30 06:55:17", "link": "http://arxiv.org/abs/2402.01750v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's\n  Dementia", "abstract": "Large language models (LLMs) find increasing applications in many fields.\nHere, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in\ntheir current form, as publicly available - for their ability to recognize\nAlzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual\ninput derived from spontaneous speech recordings. Zero-shot learning approach\nis used at two levels of independent queries, with the second query\n(chain-of-thought prompting) eliciting more detailed than the first. Each LLM\nchatbot's performance is evaluated on the prediction generated in terms of\naccuracy, sensitivity, specificity, precision and F1 score. LLM chatbots\ngenerated three-class outcome (\"AD\", \"CN\", or \"Unsure\"). When positively\nidentifying AD, Bard produced highest true-positives (89% recall) and highest\nF1 score (71%), but tended to misidentify CN as AD, with high confidence (low\n\"Unsure\" rates); for positively identifying CN, GPT-4 resulted in the highest\ntrue-negatives at 56% and highest F1 score (62%), adopting a diplomatic stance\n(moderate \"Unsure\" rates). Overall, three LLM chatbots identify AD vs CN\nsurpassing chance-levels but do not currently satisfy clinical application.", "published": "2024-01-30 07:55:43", "link": "http://arxiv.org/abs/2402.01751v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Reinforcement Learning from Human Feedback with Efficient\n  Reward Model Ensemble", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a widely adopted\napproach for aligning large language models with human values. However, RLHF\nrelies on a reward model that is trained with a limited amount of human\npreference data, which could lead to inaccurate predictions. As a result, RLHF\nmay produce outputs that are misaligned with human values. To mitigate this\nissue, we contribute a reward ensemble method that allows the reward model to\nmake more accurate predictions. As using an ensemble of large language\nmodel-based reward models can be computationally and resource-expensive, we\nexplore efficient ensemble methods including linear-layer ensemble and\nLoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy\nOptimization with our ensembled reward models, and verify that our ensemble\nmethods help improve the alignment performance of RLHF outputs.", "published": "2024-01-30 00:17:37", "link": "http://arxiv.org/abs/2401.16635v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Prospects for inconsistency detection using large language models and\n  sheaves", "abstract": "We demonstrate that large language models can produce reasonable numerical\nratings of the logical consistency of claims. We also outline a mathematical\napproach based on sheaf theory for lifting such ratings to hypertexts such as\nlaws, jurisprudence, and social media and evaluating their consistency\nglobally. This approach is a promising avenue to increasing consistency in and\nof government, as well as to combating mis- and disinformation and related\nills.", "published": "2024-01-30 03:17:45", "link": "http://arxiv.org/abs/2401.16713v1", "categories": ["cs.CY", "cs.CL", "math.AT"], "primary_category": "cs.CY"}
{"title": "Engineering A Large Language Model From Scratch", "abstract": "The proliferation of deep learning in natural language processing (NLP) has\nled to the development and release of innovative technologies capable of\nunderstanding and generating human language with remarkable proficiency.\nAtinuke, a Transformer-based neural network, optimises performance across\nvarious language tasks by utilising a unique configuration. The architecture\ninterweaves layers for processing sequential data with attention mechanisms to\ndraw meaningful affinities between inputs and outputs. Due to the configuration\nof its topology and hyperparameter tuning, it can emulate human-like language\nby extracting features and learning complex mappings. Atinuke is modular,\nextensible, and integrates seamlessly with existing machine learning pipelines.\nAdvanced matrix operations like softmax, embeddings, and multi-head attention\nenable nuanced handling of textual, acoustic, and visual signals. By unifying\nmodern deep learning techniques with software design principles and\nmathematical theory, the system achieves state-of-the-art results on natural\nlanguage tasks whilst remaining interpretable and robust.", "published": "2024-01-30 04:29:48", "link": "http://arxiv.org/abs/2401.16736v3", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SE", "I.2.7", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Conditional and Modal Reasoning in Large Language Models", "abstract": "The reasoning abilities of large language models (LLMs) are the topic of a\ngrowing body of research in AI and cognitive science. In this paper, we probe\nthe extent to which twenty-nine LLMs are able to distinguish logically correct\ninferences from logically fallacious ones. We focus on inference patterns\ninvolving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and\nepistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These\ninferences have been of special interest to logicians, philosophers, and\nlinguists, since they play a central role in the fundamental human ability to\nreason about distal possibilities. Assessing LLMs on these inferences is thus\nhighly relevant to the question of how much the reasoning abilities of LLMs\nmatch those of humans. All the LLMs we tested make some basic mistakes with\nconditionals or modals, though zero-shot chain-of-thought prompting helps them\nmake fewer mistakes. Even the best performing LLMs make basic errors in modal\nreasoning, display logically inconsistent judgments across inference patterns\ninvolving epistemic modals and conditionals, and give answers about complex\nconditional inferences that do not match reported human judgments. These\nresults highlight gaps in basic logical reasoning in today's LLMs.", "published": "2024-01-30 16:56:54", "link": "http://arxiv.org/abs/2401.17169v4", "categories": ["cs.CL", "cs.AI", "cs.LO", "68T50, 03B65", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MouSi: Poly-Visual-Expert Vision-Language Models", "abstract": "Current large vision-language models (VLMs) often encounter challenges such\nas insufficient capabilities of a single visual component and excessively long\nvisual tokens. These issues can limit the model's effectiveness in accurately\ninterpreting complex visual information and over-lengthy contextual\ninformation. Addressing these challenges is crucial for enhancing the\nperformance and applicability of VLMs. This paper proposes the use of ensemble\nexperts technique to synergizes the capabilities of individual visual encoders,\nincluding those skilled in image-text matching, OCR, image segmentation, etc.\nThis technique introduces a fusion network to unify the processing of outputs\nfrom different visual experts, while bridging the gap between image encoders\nand pre-trained LLMs. In addition, we explore different positional encoding\nschemes to alleviate the waste of positional encoding caused by lengthy image\nfeature sequences, effectively addressing the issue of position overflow and\nlength limitations. For instance, in our implementation, this technique\nsignificantly reduces the positional occupancy in models like SAM, from a\nsubstantial 4096 to a more efficient and manageable 64 or even down to 1.\nExperimental results demonstrate that VLMs with multiple experts exhibit\nconsistently superior performance over isolated visual encoders and mark a\nsignificant performance boost as more experts are integrated. We have\nopen-sourced the training code used in this report. All of these resources can\nbe found on our project website.", "published": "2024-01-30 18:09:11", "link": "http://arxiv.org/abs/2401.17221v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials\n  Knowledge Retrieval and Distillation", "abstract": "Reducing hallucination of Large Language Models (LLMs) is imperative for use\nin the sciences, where reliability and reproducibility are crucial. However,\nLLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and\ninevitably biased task to fine-tune them on domain-specific literature and\ndata. Here we introduce LLaMP, a multimodal retrieval-augmented generation\n(RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can\ndynamically and recursively interact with computational and experimental data\non Materials Project (MP) and run atomistic simulations via high-throughput\nworkflow interface. Without fine-tuning, LLaMP demonstrates strong tool usage\nability to comprehend and integrate various modalities of materials science\nconcepts, fetch relevant data stores on the fly, process higher-order data\n(such as crystal structure and elastic tensor), and streamline complex tasks in\ncomputational materials and chemistry. We propose a simple metric combining\nuncertainty and confidence estimates to evaluate the self-consistency of\nresponses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively\nmitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli,\nelectronic bandgaps, and formation energies that seem to derive from mixed data\nsources. We also demonstrate LLaMP's capability to edit crystal structures and\nrun annealing molecular dynamics simulations using pre-trained machine-learning\nforce fields. The framework offers an intuitive and nearly hallucination-free\napproach to exploring and scaling materials informatics, and establishes a\npathway for knowledge distillation and fine-tuning other language models. Code\nand live demo are available at https://github.com/chiang-yuan/llamp", "published": "2024-01-30 18:37:45", "link": "http://arxiv.org/abs/2401.17244v3", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Prompt Optimization for Defending Language Models Against\n  Jailbreaking Attacks", "abstract": "Despite advances in AI alignment, large language models (LLMs) remain\nvulnerable to adversarial attacks or jailbreaking, in which adversaries can\nmodify prompts to induce unwanted behavior. While some defenses have been\nproposed, they have not been adapted to newly proposed attacks and more\nchallenging threat models. To address this, we propose an optimization-based\nobjective for defending LLMs against jailbreaking attacks and an algorithm,\nRobust Prompt Optimization (RPO) to create robust system-level defenses. Our\napproach directly incorporates the adversary into the defensive objective and\noptimizes a lightweight and transferable suffix, enabling RPO to adapt to\nworst-case adaptive attacks. Our theoretical and experimental results show\nimproved robustness to both jailbreaks seen during optimization and unknown\njailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2\nto 0% on JailbreakBench, setting the state-of-the-art. Code can be found at\nhttps://github.com/lapisrocks/rpo", "published": "2024-01-30 18:56:08", "link": "http://arxiv.org/abs/2401.17263v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Weaver: Foundation Models for Creative Writing", "abstract": "This work introduces Weaver, our first family of large language models (LLMs)\ndedicated to content creation. Weaver is pre-trained on a carefully selected\ncorpus that focuses on improving the writing capabilities of large language\nmodels. We then fine-tune Weaver for creative and professional writing purposes\nand align it to the preference of professional writers using a suit of novel\nmethods for instruction data synthesis and LLM alignment, making it able to\nproduce more human-like texts and follow more diverse instructions for content\ncreation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver\nBase (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for\ndifferent applications and can be dynamically dispatched by a routing agent\naccording to query complexity to balance response quality and computation cost.\nEvaluation on a carefully curated benchmark for assessing the writing\ncapabilities of LLMs shows Weaver models of all sizes outperform generalist\nLLMs several times larger than them. Notably, our most-capable Weaver Ultra\nmodel surpasses GPT-4, a state-of-the-art generalist LLM, on various writing\nscenarios, demonstrating the advantage of training specialized LLMs for writing\npurposes. Moreover, Weaver natively supports retrieval-augmented generation\n(RAG) and function calling (tool usage). We present various use cases of these\nabilities for improving AI-assisted writing systems, including integration of\nexternal knowledge bases, tools, or APIs, and providing personalized writing\nassistance. Furthermore, we discuss and summarize a guideline and best\npractices for pre-training and fine-tuning domain-specific LLMs.", "published": "2024-01-30 18:58:43", "link": "http://arxiv.org/abs/2401.17268v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion\n  Tokens", "abstract": "Are $n$-gram language models still relevant in this era of neural large\nlanguage models (LLMs)? Our answer is yes, and we showcase their values in both\ntext analysis and improving neural LLMs. This was done by modernizing $n$-gram\nLMs in two aspects. First, we train them at the same data scale as neural LLMs\n-- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second,\nexisting $n$-gram LMs use small $n$ which hinders their performance; we instead\nallow $n$ to be arbitrarily large, by introducing a new $\\infty$-gram LM with\nbackoff. Instead of pre-computing $n$-gram count tables (which would be very\nexpensive), we develop an engine named infini-gram -- powered by suffix arrays\n-- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$)\nprobabilities with millisecond-level latency. The $\\infty$-gram framework and\ninfini-gram engine enable us to conduct many novel and interesting analyses of\nhuman-written and machine-generated text: we find that the $\\infty$-gram LM has\nfairly high accuracy for next-token prediction (47%), and can complement neural\nLLMs to greatly reduce their perplexity. When analyzing machine-generated text,\nwe also observe irregularities in the machine--$\\infty$-gram agreement level\nwith respect to the suffix length, which indicates deficiencies in neural LLM\npretraining and the positional embeddings of Transformers.", "published": "2024-01-30 19:03:49", "link": "http://arxiv.org/abs/2401.17377v4", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Detecting mental disorder on social media: a ChatGPT-augmented\n  explainable approach", "abstract": "In the digital era, the prevalence of depressive symptoms expressed on social\nmedia has raised serious concerns, necessitating advanced methodologies for\ntimely detection. This paper addresses the challenge of interpretable\ndepression detection by proposing a novel methodology that effectively combines\nLarge Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and\nconversational agents like ChatGPT. In our methodology, explanations are\nachieved by integrating BERTweet, a Twitter-specific variant of BERT, into a\nnovel self-explanatory model, namely BERT-XDD, capable of providing both\nclassification and explanations via masked attention. The interpretability is\nfurther enhanced using ChatGPT to transform technical explanations into\nhuman-readable commentaries. By introducing an effective and modular approach\nfor interpretable depression detection, our methodology can contribute to the\ndevelopment of socially responsible digital platforms, fostering early\nintervention and support for mental health challenges under the guidance of\nqualified healthcare professionals.", "published": "2024-01-30 22:22:55", "link": "http://arxiv.org/abs/2401.17477v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Arrows of Time for Large Language Models", "abstract": "We study the probabilistic modeling performed by Autoregressive Large\nLanguage Models (LLMs) through the angle of time directionality, addressing a\nquestion first raised in (Shannon, 1951). For large enough models, we\nempirically find a time asymmetry in their ability to learn natural language: a\ndifference in the average log-perplexity when trying to predict the next token\nversus when trying to predict the previous one. This difference is at the same\ntime subtle and very consistent across various modalities (language, model\nsize, training time, ...). Theoretically, this is surprising: from an\ninformation-theoretic point of view, there should be no such difference. We\nprovide a theoretical framework to explain how such an asymmetry can appear\nfrom sparsity and computational complexity considerations, and outline a number\nof perspectives opened by our results.", "published": "2024-01-30 23:46:35", "link": "http://arxiv.org/abs/2401.17505v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EvoMerge: Neuroevolution for Large Language Models", "abstract": "Extensive fine-tuning on Large Language Models does not always yield better\nresults. Oftentimes, models tend to get better at imitating one form of data\nwithout gaining greater reasoning ability and may even end up losing some\nintelligence. Here I introduce EvoMerge, a systematic approach to large\nlanguage model training and merging. Leveraging model merging for weight\ncrossover and fine-tuning for weight mutation, EvoMerge establishes an\nevolutionary process aimed at pushing models beyond the limits of conventional\nfine-tuning.", "published": "2024-01-30 19:37:21", "link": "http://arxiv.org/abs/2402.00070v1", "categories": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Security and Privacy Challenges of Large Language Models: A Survey", "abstract": "Large Language Models (LLMs) have demonstrated extraordinary capabilities and\ncontributed to multiple fields, such as generating and summarizing text,\nlanguage translation, and question-answering. Nowadays, LLM is becoming a very\npopular tool in computerized language processing tasks, with the capability to\nanalyze complicated linguistic patterns and provide relevant and appropriate\nresponses depending on the context. While offering significant advantages,\nthese models are also vulnerable to security and privacy attacks, such as\njailbreaking attacks, data poisoning attacks, and Personally Identifiable\nInformation (PII) leakage attacks. This survey provides a thorough review of\nthe security and privacy challenges of LLMs for both training data and users,\nalong with the application-based risks in various domains, such as\ntransportation, education, and healthcare. We assess the extent of LLM\nvulnerabilities, investigate emerging security and privacy attacks for LLMs,\nand review the potential defense mechanisms. Additionally, the survey outlines\nexisting research gaps in this domain and highlights future research\ndirections.", "published": "2024-01-30 04:00:54", "link": "http://arxiv.org/abs/2402.00888v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Cybersecurity: State-of-the-Art", "abstract": "The rise of Large Language Models (LLMs) has revolutionized our comprehension\nof intelligence bringing us closer to Artificial Intelligence. Since their\nintroduction, researchers have actively explored the applications of LLMs\nacross diverse fields, significantly elevating capabilities. Cybersecurity,\ntraditionally resistant to data-driven solutions and slow to embrace machine\nlearning, stands out as a domain. This study examines the existing literature,\nproviding a thorough characterization of both defensive and adversarial\napplications of LLMs within the realm of cybersecurity. Our review not only\nsurveys and categorizes the current landscape but also identifies critical\nresearch gaps. By evaluating both offensive and defensive applications, we aim\nto provide a holistic understanding of the potential risks and opportunities\nassociated with LLM-driven cybersecurity.", "published": "2024-01-30 16:55:25", "link": "http://arxiv.org/abs/2402.00891v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Large Multi-Modal Models (LMMs) as Universal Foundation Models for\n  AI-Native Wireless Systems", "abstract": "Large language models (LLMs) and foundation models have been recently touted\nas a game-changer for 6G systems. However, recent efforts on LLMs for wireless\nnetworks are limited to a direct application of existing language models that\nwere designed for natural language processing (NLP) applications. To address\nthis challenge and create wireless-centric foundation models, this paper\npresents a comprehensive vision on how to design universal foundation models\nthat are tailored towards the deployment of artificial intelligence (AI)-native\nnetworks. Diverging from NLP-based foundation models, the proposed framework\npromotes the design of large multi-modal models (LMMs) fostered by three key\ncapabilities: 1) processing of multi-modal sensing data, 2) grounding of\nphysical symbol representations in real-world wireless systems using causal\nreasoning and retrieval-augmented generation (RAG), and 3) enabling\ninstructibility from the wireless environment feedback to facilitate dynamic\nnetwork adaptation thanks to logical and mathematical reasoning facilitated by\nneuro-symbolic AI. In essence, these properties enable the proposed LMM\nframework to build universal capabilities that cater to various cross-layer\nnetworking tasks and alignment of intents across different domains. Preliminary\nresults from experimental evaluation demonstrate the efficacy of grounding\nusing RAG in LMMs, and showcase the alignment of LMMs with wireless system\ndesigns. Furthermore, the enhanced rationale exhibited in the responses to\nmathematical questions by LMMs, compared to vanilla LLMs, demonstrates the\nlogical and mathematical reasoning capabilities inherent in LMMs. Building on\nthose results, we present a sequel of open questions and challenges for LMMs.\nWe then conclude with a set of recommendations that ignite the path towards\nLMM-empowered AI-native systems.", "published": "2024-01-30 00:21:41", "link": "http://arxiv.org/abs/2402.01748v2", "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.NI"}
{"title": "Aalap: AI Assistant for Legal & Paralegal Functions in India", "abstract": "Using proprietary Large Language Models on legal tasks poses challenges due\nto data privacy issues, domain data heterogeneity, domain knowledge\nsophistication, and domain objectives uniqueness. We created Aalalp, a\nfine-tuned Mistral 7B model on instructions data related to specific Indian\nlegal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31\\% of\nour test data and obtains an equivalent score in 34\\% of the test data as\nevaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoning\nrather than legal recall. Aalap is definitely helpful for the day-to-day\nactivities of lawyers, judges, or anyone working in legal systems.", "published": "2024-01-30 12:39:58", "link": "http://arxiv.org/abs/2402.01758v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Systematic Literature Review: Computational Approaches for Humour Style\n  Classification", "abstract": "Understanding various humour styles is essential for comprehending the\nmultifaceted nature of humour and its impact on fields such as psychology and\nartificial intelligence. This understanding has revealed that humour, depending\non the style employed, can either have therapeutic or detrimental effects on an\nindividual's health and relationships. Although studies dedicated exclusively\nto computational-based humour style analysis remain somewhat rare, an expansive\nbody of research thrives within related task, particularly binary humour and\nsarcasm recognition. In this systematic literature review (SLR), we survey the\nlandscape of computational techniques applied to these related tasks and also\nuncover their fundamental relevance to humour style analysis. Through this\nstudy, we unveil common approaches, illuminate various datasets and evaluation\nmetrics, and effectively navigate the complex terrain of humour research. Our\nefforts determine potential research gaps and outlined promising directions.\nFurthermore, the SLR identifies a range of features and computational models\nthat can seamlessly transition from related tasks like binary humour and\nsarcasm detection to invigorate humour style classification. These features\nencompass incongruity, sentiment and polarity analysis, ambiguity detection,\nacoustic nuances, visual cues, contextual insights, and more. The computational\nmodels that emerge contain traditional machine learning paradigms, neural\nnetwork architectures, transformer-based models, and specialised models attuned\nto the nuances of humour. Finally, the SLR provides access to existing datasets\nrelated to humour and sarcasm, facilitating the work of future researchers.", "published": "2024-01-30 16:21:47", "link": "http://arxiv.org/abs/2402.01759v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking Interpretability in the Era of Large Language Models", "abstract": "Interpretable machine learning has exploded as an area of interest over the\nlast decade, sparked by the rise of increasingly large datasets and deep neural\nnetworks. Simultaneously, large language models (LLMs) have demonstrated\nremarkable capabilities across a wide array of tasks, offering a chance to\nrethink opportunities in interpretable machine learning. Notably, the\ncapability to explain in natural language allows LLMs to expand the scale and\ncomplexity of patterns that can be given to a human. However, these new\ncapabilities raise new challenges, such as hallucinated explanations and\nimmense computational costs.\n  In this position paper, we start by reviewing existing methods to evaluate\nthe emerging field of LLM interpretation (both interpreting LLMs and using LLMs\nfor explanation). We contend that, despite their limitations, LLMs hold the\nopportunity to redefine interpretability with a more ambitious scope across\nmany applications, including in auditing LLMs themselves. We highlight two\nemerging research priorities for LLM interpretation: using LLMs to directly\nanalyze new datasets and to generate interactive explanations.", "published": "2024-01-30 17:38:54", "link": "http://arxiv.org/abs/2402.01761v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When Large Language Models Meet Vector Databases: A Survey", "abstract": "This survey explores the synergistic potential of Large Language Models\n(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving\nresearch area. With the proliferation of LLMs comes a host of challenges,\nincluding hallucinations, outdated knowledge, prohibitive commercial\napplication costs, and memory issues. VecDBs emerge as a compelling solution to\nthese issues by offering an efficient means to store, retrieve, and manage the\nhigh-dimensional vector representations intrinsic to LLM operations. Through\nthis nuanced review, we delineate the foundational principles of LLMs and\nVecDBs and critically analyze their integration's impact on enhancing LLM\nfunctionalities. This discourse extends into a discussion on the speculative\nfuture developments in this domain, aiming to catalyze further research into\noptimizing the confluence of LLMs and VecDBs for advanced data handling and\nknowledge extraction capabilities.", "published": "2024-01-30 23:35:28", "link": "http://arxiv.org/abs/2402.01763v3", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "NanoNER: Named Entity Recognition for nanobiology using experts'\n  knowledge and distant supervision", "abstract": "Here we present the training and evaluation of NanoNER, a Named Entity\nRecognition (NER) model for Nanobiology. NER consists in the identification of\nspecific entities in spans of unstructured texts and is often a primary task in\nNatural Language Processing (NLP) and Information Extraction. The aim of our\nmodel is to recognise entities previously identified by domain experts as\nconstituting the essential knowledge of the domain. Relying on ontologies,\nwhich provide us with a domain vocabulary and taxonomy, we implemented an\niterative process enabling experts to determine the entities relevant to the\ndomain at hand. We then delve into the potential of distant supervision\nlearning in NER, supporting how this method can increase the quantity of\nannotated data with minimal additional manpower. On our full corpus of 728\nfull-text nanobiology articles, containing more than 120k entity occurrences,\nNanoNER obtained a F1-score of 0.98 on the recognition of previously known\nentities. Our model also demonstrated its ability to discover new entities in\nthe text, with precision scores ranging from 0.77 to 0.81. Ablation experiments\nfurther confirmed this and allowed us to assess the dependency of our approach\non the external resources. It highlighted the dependency of the approach to the\nresource, while also confirming its ability to rediscover up to 30% of the\nablated terms. This paper details the methodology employed, experimental\ndesign, and key findings, providing valuable insights and directions for future\nrelated researches on NER in specialized domain. Furthermore, since our\napproach require minimal manpower , we believe that it can be generalized to\nother specialized fields.", "published": "2024-01-30 09:10:53", "link": "http://arxiv.org/abs/2402.03362v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "QACP: An Annotated Question Answering Dataset for Assisting Chinese\n  Python Programming Learners", "abstract": "In online learning platforms, particularly in rapidly growing computer\nprogramming courses, addressing the thousands of students' learning queries\nrequires considerable human cost. The creation of intelligent assistant large\nlanguage models (LLMs) tailored for programming education necessitates distinct\ndata support. However, in real application scenarios, the data resources for\ntraining such LLMs are relatively scarce. Therefore, to address the data\nscarcity in intelligent educational systems for programming, this paper\nproposes a new Chinese question-and-answer dataset for Python learners. To\nensure the authenticity and reliability of the sources of the questions, we\ncollected questions from actual student questions and categorized them\naccording to various dimensions such as the type of questions and the type of\nlearners. This annotation principle is designed to enhance the effectiveness\nand quality of online programming education, providing a solid data foundation\nfor developing the programming teaching assists (TA). Furthermore, we conducted\ncomprehensive evaluations of various LLMs proficient in processing and\ngenerating Chinese content, highlighting the potential limitations of general\nLLMs as intelligent teaching assistants in computer programming courses.", "published": "2024-01-30 13:11:23", "link": "http://arxiv.org/abs/2402.07913v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language\n  Models", "abstract": "Large Language Models (LLMs) have transformed natural language processing and\nextended their powerful capabilities to multi-modal domains. As LLMs continue\nto advance, it is crucial to develop diverse and appropriate metrics for their\nevaluation. In this paper, we introduce a novel rank-based metric, Diff-eRank,\ngrounded in information theory and geometry principles. Diff-eRank assesses\nLLMs by analyzing their hidden representations, providing a quantitative\nmeasure of how efficiently they eliminate redundant information during\ntraining. We demonstrate the applicability of Diff-eRank in both single-modal\n(e.g., language) and multi-modal settings. For language models, our results\nshow that Diff-eRank increases with model size and correlates well with\nconventional metrics such as loss and accuracy. In the multi-modal context, we\npropose an alignment evaluation method based on the eRank, and verify that\ncontemporary multi-modal LLMs exhibit strong alignment performance based on our\nmethod. Our code is publicly available at\nhttps://github.com/waltonfuture/Diff-eRank.", "published": "2024-01-30 16:19:55", "link": "http://arxiv.org/abs/2401.17139v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Can LLMs Replace Economic Choice Prediction Labs? The Case of\n  Language-based Persuasion Games", "abstract": "Human choice prediction in economic contexts is crucial for applications in\nmarketing, finance, public policy, and more. This task, however, is often\nconstrained by the difficulties in acquiring human choice data. With most\nexperimental economics studies focusing on simple choice settings, the AI\ncommunity has explored whether LLMs can substitute for humans in these\npredictions and examined more complex experimental economics settings. However,\na key question remains: can LLMs generate training data for human choice\nprediction? We explore this in language-based persuasion games, a complex\neconomic setting involving natural language in strategic interactions. Our\nexperiments show that models trained on LLM-generated data can effectively\npredict human behavior in these games and even outperform models trained on\nactual human data.", "published": "2024-01-30 20:49:47", "link": "http://arxiv.org/abs/2401.17435v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Identifying False Content and Hate Speech in Sinhala YouTube Videos by\n  Analyzing the Audio", "abstract": "YouTube faces a global crisis with the dissemination of false information and\nhate speech. To counter these issues, YouTube has implemented strict rules\nagainst uploading content that includes false information or promotes hate\nspeech. While numerous studies have been conducted to reduce offensive\nEnglish-language content, there's a significant lack of research on Sinhala\ncontent. This study aims to address the aforementioned gap by proposing a\nsolution to minimize the spread of violence and misinformation in Sinhala\nYouTube videos. The approach involves developing a rating system that assesses\nwhether a video contains false information by comparing the title and\ndescription with the audio content and evaluating whether the video includes\nhate speech. The methodology encompasses several steps, including audio\nextraction using the Pytube library, audio transcription via the fine-tuned\nWhisper model, hate speech detection employing the distilroberta-base model and\na text classification LSTM model, and text summarization through the fine-tuned\nBART-Large- XSUM model. Notably, the Whisper model achieved a 48.99\\% word\nerror rate, while the distilroberta-base model demonstrated an F1 score of\n0.856 and a recall value of 0.861 in comparison to the LSTM model, which\nexhibited signs of overfitting.", "published": "2024-01-30 08:08:34", "link": "http://arxiv.org/abs/2402.01752v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech\n  Generation Leveraging NLP Evaluation Metrics", "abstract": "While subjective assessments have been the gold standard for evaluating\nspeech generation, there is a growing need for objective metrics that are\nhighly correlated with human subjective judgments due to their cost efficiency.\nThis paper proposes reference-aware automatic evaluation methods for speech\ngeneration inspired by evaluation metrics in natural language processing. The\nproposed SpeechBERTScore computes the BERTScore for self-supervised dense\nspeech features of the generated and reference speech, which can have different\nsequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which\nare computed on speech discrete tokens. The evaluations on synthesized speech\nshow that our method correlates better with human subjective ratings than mel\ncepstral distortion and a recent mean opinion score prediction model. Also,\nthey are effective in noisy speech evaluation and have cross-lingual\napplicability.", "published": "2024-01-30 08:26:28", "link": "http://arxiv.org/abs/2401.16812v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Localizing uniformly moving single-frequency sources using an inverse\n  2.5D approach", "abstract": "Localizing linearly moving sound sources using microphone arrays is\nchallenging as the transient nature of the signal leads to relatively short\nobservation periods. Commonly, a moving focus is used and most methods operate\nat least partially in the time domain. In contrast, this manuscript presents an\ninverse source localization algorithm for uniformly moving single-frequency\nsources that acts entirely in the frequency domain. For this, a 2.5D approach\nis utilized and a transfer function between sources and a microphone grid is\nderived. By solving a least squares problem using the data at the microphone\ngrid, the unknown source distribution in the moving frame can be determined.\nFirst, the time signals need to be transformed from time into frequency domain\nusing a windowed discrete Fourier transform (DFT), which leads to spectral\nleakage that depends on the length of the time interval and the analysis window\nused. To include spectral leakage in the numerical model, the calculation of\nthe transfer matrix is modified using the Fourier transform of the analysis\nwindow in the DFT applied to the measurements. Currently, this approach is\nlimited to single-frequency sources as this restriction allows for simplified\ncalculations and reduces the computational effort. The least squares problem is\nsolved using a Tikhonov regularization and an L-curve approach. As moving\nsources are considered, utilizing the Doppler effect enhances the stability of\nthe system by combining the transfer functions for multiple frequencies in the\nmeasured signals. The performance is validated using simulated data of a moving\npoint source with or without a reflecting ground. Numerical experiments are\nperformed to show the effect of the choice of frequencies in the receiver\nspectrum, the effect of the DFT, the source frequency, the distance between\nsource and receiver, and the robustness with respect to noise.", "published": "2024-01-30 08:45:27", "link": "http://arxiv.org/abs/2401.16819v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatial-Temporal Activity-Informed Diarization and Separation", "abstract": "A robust multichannel speaker diarization and separation system is proposed\nby exploiting the spatio-temporal activity of the speakers. The system is\nrealized in a hybrid architecture that combines the array signal processing\nunits and the deep learning units. For speaker diarization, a spatial coherence\nmatrix across time frames is computed based on the whitened relative transfer\nfunctions (wRTFs) of the microphone array. This serves as a robust feature for\nsubsequent machine learning without the need for prior knowledge of the array\nconfiguration. A computationally efficient Spatial Activity-driven Speaker\nDiarization network (SASDnet) is constructed to estimate the speaker activity\ndirectly from the spatial coherence matrix. For speaker separation, we propose\nthe Global and Local Activity-driven Speaker Extraction network (GLASEnet) to\nseparate speaker signals via speaker-specific global and local spatial activity\nfunctions. The local spatial activity functions depend on the coherence between\nthe wRTFs of each time-frequency bin and the target speaker-dominant bins. The\nglobal spatial activity functions are computed from the global spatial\ncoherence functions based on frequency-averaged local spatial activity\nfunctions. Experimental results have demonstrated superior speaker,\ndiarization, counting, and separation performance achieved by the proposed\nsystem with low computational complexity compared to the pre-selected\nbaselines.", "published": "2024-01-30 09:50:06", "link": "http://arxiv.org/abs/2401.16850v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting gamma-band responses to the speech envelope for the ICASSP\n  2024 Auditory EEG Decoding Signal Processing Grand Challenge", "abstract": "The 2024 ICASSP Auditory EEG Signal Processing Grand Challenge concerns the\ndecoding of electroencephalography (EEG) measurements taken from participants\nwho listened to speech material. This work details our solution to the\nmatch-mismatch sub-task: given a short temporal segment of EEG recordings and\nseveral candidate speech segments, the task is to classify which of the speech\nsegments was time-aligned with the EEG signals. We show that high-frequency\ngamma-band responses to the speech envelope can be detected with a high\naccuracy. By jointly assessing gamma-band responses and low-frequency envelope\ntracking, we develop a match-mismatch decoder which placed first in this task.", "published": "2024-01-30 19:07:31", "link": "http://arxiv.org/abs/2401.17380v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PBSCR: The Piano Bootleg Score Composer Recognition Dataset", "abstract": "This article motivates, describes, and presents the PBSCR dataset for\nstudying composer recognition of classical piano music. Our goal was to design\na dataset that facilitates large-scale research on composer recognition that is\nsuitable for modern architectures and training practices. To achieve this goal,\nwe utilize the abundance of sheet music images and rich metadata on IMSLP, use\na previously proposed feature representation called a bootleg score to encode\nthe location of noteheads relative to staff lines, and present the data in an\nextremely simple format (2D binary images) to encourage rapid exploration and\niteration. The dataset itself contains 40,000 62x64 bootleg score images for a\n9-class recognition task, 100,000 62x64 bootleg score images for a 100-class\nrecognition task, and 29,310 unlabeled variable-length bootleg score images for\npretraining. The labeled data is presented in a form that mirrors MNIST images,\nin order to make it extremely easy to visualize, manipulate, and train models\nin an efficient manner. We include relevant information to connect each bootleg\nscore image with its underlying raw sheet music image, and we scrape, organize,\nand compile metadata from IMSLP on all piano works to facilitate multimodal\nresearch and allow for convenient linking to other datasets. We release\nbaseline results in a supervised and low-shot setting for future works to\ncompare against, and we discuss open research questions that the PBSCR data is\nespecially well suited to facilitate research on.", "published": "2024-01-30 07:50:32", "link": "http://arxiv.org/abs/2401.16803v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible\n  recipes, self-supervised front-ends, and off-the-shelf models", "abstract": "This paper introduces ESPnet-SPK, a toolkit designed with several objectives\nfor training speaker embedding extractors. First, we provide an open-source\nplatform for researchers in the speaker recognition community to effortlessly\nbuild models. We provide several models, ranging from x-vector to recent\nSKA-TDNN. Through the modularized architecture design, variants can be\ndeveloped easily. We also aspire to bridge developed models with other domains,\nfacilitating the broad research community to effortlessly incorporate\nstate-of-the-art embedding extractors. Pre-trained embedding extractors can be\naccessed in an off-the-shelf manner and we demonstrate the toolkit's\nversatility by showcasing its integration with two tasks. Another goal is to\nintegrate with diverse self-supervised learning features. We release a\nreproducible recipe that achieves an equal error rate of 0.39% on the Vox1-O\nevaluation protocol using WavLM-Large with ECAPA-TDNN.", "published": "2024-01-30 18:18:27", "link": "http://arxiv.org/abs/2401.17230v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Online speaker diarization of meetings guided by speech separation", "abstract": "Overlapped speech is notoriously problematic for speaker diarization systems.\nConsequently, the use of speech separation has recently been proposed to\nimprove their performance. Although promising, speech separation models\nstruggle with realistic data because they are trained on simulated mixtures\nwith a fixed number of speakers. In this work, we introduce a new speech\nseparation-guided diarization scheme suitable for the online speaker\ndiarization of long meeting recordings with a variable number of speakers, as\npresent in the AMI corpus. We envisage ConvTasNet and DPRNN as alternatives for\nthe separation networks, with two or three output sources. To obtain the\nspeaker diarization result, voice activity detection is applied on each\nestimated source. The final model is fine-tuned end-to-end, after first\nadapting the separation to real data using AMI. The system operates on short\nsegments, and inference is performed by stitching the local predictions using\nspeaker embeddings and incremental clustering. The results show that our system\nimproves the state-of-the-art on the AMI headset mix, using no oracle\ninformation and under full evaluation (no collar and including overlapped\nspeech). Finally, we show the strength of our system particularly on overlapped\nspeech sections.", "published": "2024-01-30 09:09:22", "link": "http://arxiv.org/abs/2402.00067v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SpecDiff-GAN: A Spectrally-Shaped Noise Diffusion GAN for Speech and\n  Music Synthesis", "abstract": "Generative adversarial network (GAN) models can synthesize highquality audio\nsignals while ensuring fast sample generation. However, they are difficult to\ntrain and are prone to several issues including mode collapse and divergence.\nIn this paper, we introduce SpecDiff-GAN, a neural vocoder based on HiFi-GAN,\nwhich was initially devised for speech synthesis from mel spectrogram. In our\nmodel, the training stability is enhanced by means of a forward diffusion\nprocess which consists in injecting noise from a Gaussian distribution to both\nreal and fake samples before inputting them to the discriminator. We further\nimprove the model by exploiting a spectrally-shaped noise distribution with the\naim to make the discriminator's task more challenging. We then show the merits\nof our proposed model for speech and music synthesis on several datasets. Our\nexperiments confirm that our model compares favorably in audio quality and\nefficiency compared to several baselines.", "published": "2024-01-30 09:17:57", "link": "http://arxiv.org/abs/2402.01753v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "SongBsAb: A Dual Prevention Approach against Singing Voice Conversion\n  based Illegal Song Covers", "abstract": "Singing voice conversion (SVC) automates song covers by converting a source\nsinging voice from a source singer into a new singing voice with the same\nlyrics and melody as the source, but sounds like being covered by the target\nsinger of some given target singing voices. However, it raises serious concerns\nabout copyright and civil right infringements. We propose SongBsAb, the first\nproactive approach to tackle SVC-based illegal song covers. SongBsAb adds\nperturbations to singing voices before releasing them, so that when they are\nused, the process of SVC will be interfered, leading to unexpected singing\nvoices. Perturbations are carefully crafted to (1) provide a dual prevention,\ni.e., preventing the singing voice from being used as the source and target\nsinging voice in SVC, by proposing a gender-transformation loss and a high/low\nhierarchy multi-target loss, respectively; and (2) be harmless, i.e., no\nside-effect on the enjoyment of protected songs, by refining a psychoacoustic\nmodel-based loss with the backing track as an additional masker, a unique\naccompanying element for singing voices compared to ordinary speech voices. We\nalso adopt a frame-level interaction reduction-based loss and encoder ensemble\nto enhance the transferability of SongBsAb to unknown SVC models. We\ndemonstrate the prevention effectiveness, harmlessness, and robustness of\nSongBsAb on five diverse and promising SVC models, using both English and\nChinese datasets, and both objective and human study-based subjective metrics.\nOur work fosters an emerging research direction for mitigating illegal\nautomated song covers.", "published": "2024-01-30 16:07:44", "link": "http://arxiv.org/abs/2401.17133v2", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
