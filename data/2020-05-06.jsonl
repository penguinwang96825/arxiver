{"title": "Moving Down the Long Tail of Word Sense Disambiguation with\n  Gloss-Informed Biencoders", "abstract": "A major obstacle in Word Sense Disambiguation (WSD) is that word senses are\nnot uniformly distributed, causing existing models to generally perform poorly\non senses that are either rare or unseen during training. We propose a\nbi-encoder model that independently embeds (1) the target word with its\nsurrounding context and (2) the dictionary definition, or gloss, of each sense.\nThe encoders are jointly optimized in the same representation space, so that\nsense disambiguation can be performed by finding the nearest sense embedding\nfor each target word embedding. Our system outperforms previous\nstate-of-the-art models on English all-words WSD; these gains predominantly\ncome from improved performance on rare senses, leading to a 31.1% error\nreduction on less frequent senses over prior work. This demonstrates that rare\nsenses can be more effectively disambiguated by modeling their definitions.", "published": "2020-05-06 04:21:45", "link": "http://arxiv.org/abs/2005.02590v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Top-Down Neural Architecture towards Text-Level Parsing of Discourse\n  Rhetorical Structure", "abstract": "Due to its great importance in deep natural language understanding and\nvarious down-stream applications, text-level parsing of discourse rhetorical\nstructure (DRS) has been drawing more and more attention in recent years.\nHowever, all the previous studies on text-level discourse parsing adopt\nbottom-up approaches, which much limit the DRS determination on local\ninformation and fail to well benefit from global information of the overall\ndiscourse. In this paper, we justify from both computational and perceptive\npoints-of-view that the top-down architecture is more suitable for text-level\nDRS parsing. On the basis, we propose a top-down neural architecture toward\ntext-level DRS parsing. In particular, we cast discourse parsing as a recursive\nsplit point ranking task, where a split point is classified to different levels\naccording to its rank and the elementary discourse units (EDUs) associated with\nit are arranged accordingly. In this way, we can determine the complete DRS as\na hierarchical tree structure via an encoder-decoder with an internal stack.\nExperimentation on both the English RST-DT corpus and the Chinese CDTB corpus\nshows the great effectiveness of our proposed top-down approach towards\ntext-level DRS parsing.", "published": "2020-05-06 09:27:20", "link": "http://arxiv.org/abs/2005.02680v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shape of synth to come: Why we should use synthetic data for English\n  surface realization", "abstract": "The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language\nGeneration shared tasks with the goal of exploring approaches to surface\nrealization from Universal-Dependency-like trees to surface strings for several\nlanguages. In the 2018 shared task there was very little difference in the\nabsolute performance of systems trained with and without additional,\nsynthetically created data, and a new rule prohibiting the use of synthetic\ndata was introduced for the 2019 shared task. Contrary to the findings of the\n2018 shared task, we show, in experiments on the English 2018 dataset, that the\nuse of synthetic data can have a substantial positive effect - an improvement\nof almost 8 BLEU points for a previously state-of-the-art system. We analyse\nthe effects of synthetic data, and we argue that its use should be encouraged\nrather than prohibited so that future research efforts continue to explore\nsystems that can take advantage of such data.", "published": "2020-05-06 10:00:55", "link": "http://arxiv.org/abs/2005.02693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Understand Child-directed and Adult-directed Speech", "abstract": "Speech directed to children differs from adult-directed speech in linguistic\naspects such as repetition, word choice, and sentence length, as well as in\naspects of the speech signal itself, such as prosodic and phonemic variation.\nHuman language acquisition research indicates that child-directed speech helps\nlanguage learners. This study explores the effect of child-directed speech when\nlearning to extract semantic information from speech directly. We compare the\ntask performance of models trained on adult-directed speech (ADS) and\nchild-directed speech (CDS). We find indications that CDS helps in the initial\nstages of learning, but eventually, models trained on ADS reach comparable task\nperformance, and generalize better. The results suggest that this is at least\npartially due to linguistic rather than acoustic properties of the two\nregisters, as we see the same pattern when looking at models trained on\nacoustically comparable synthetic speech.", "published": "2020-05-06 10:47:02", "link": "http://arxiv.org/abs/2005.02721v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Multi-Task Learning on BERT for Biomedical Text\n  Mining", "abstract": "Multi-task learning (MTL) has achieved remarkable success in natural language\nprocessing applications. In this work, we study a multi-task learning model\nwith multiple decoders on varieties of biomedical and clinical natural language\nprocessing tasks such as text similarity, relation extraction, named entity\nrecognition, and text inference. Our empirical results demonstrate that the MTL\nfine-tuned models outperform state-of-the-art transformer models (e.g., BERT\nand its variants) by 2.0% and 1.3% in biomedical and clinical domains,\nrespectively. Pairwise MTL further demonstrates more details about which tasks\ncan improve or decrease others. This is particularly helpful in the context\nthat researchers are in the hassle of choosing a suitable model for new\nproblems. The code and models are publicly available at\nhttps://github.com/ncbi-nlp/bluebert", "published": "2020-05-06 13:25:21", "link": "http://arxiv.org/abs/2005.02799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TAG : Type Auxiliary Guiding for Code Comment Generation", "abstract": "Existing leading code comment generation approaches with the\nstructure-to-sequence framework ignores the type information of the\ninterpretation of the code, e.g., operator, string, etc. However, introducing\nthe type information into the existing framework is non-trivial due to the\nhierarchical dependence among the type information. In order to address the\nissues above, we propose a Type Auxiliary Guiding encoder-decoder framework for\nthe code comment generation task which considers the source code as an N-ary\ntree with type information associated with each node. Specifically, our\nframework is featured with a Type-associated Encoder and a Type-restricted\nDecoder which enables adaptive summarization of the source code. We further\npropose a hierarchical reinforcement learning method to resolve the training\ndifficulties of our proposed framework. Extensive evaluations demonstrate the\nstate-of-the-art performance of our framework with both the auto-evaluated\nmetrics and case studies.", "published": "2020-05-06 14:04:13", "link": "http://arxiv.org/abs/2005.02835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TripPy: A Triple Copy Strategy for Value Independent Neural Dialog State\n  Tracking", "abstract": "Task-oriented dialog systems rely on dialog state tracking (DST) to monitor\nthe user's goal during the course of an interaction. Multi-domain and\nopen-vocabulary settings complicate the task considerably and demand scalable\nsolutions. In this paper we present a new approach to DST which makes use of\nvarious copy mechanisms to fill slots with values. Our model has no need to\nmaintain a list of candidate values. Instead, all values are extracted from the\ndialog context on-the-fly. A slot is filled by one of three copy mechanisms:\n(1) Span prediction may extract values directly from the user input; (2) a\nvalue may be copied from a system inform memory that keeps track of the\nsystem's inform operations; (3) a value may be copied over from a different\nslot that is already contained in the dialog state to resolve coreferences\nwithin and across domains. Our approach combines the advantages of span-based\nslot filling methods with memory methods to avoid the use of value picklists\naltogether. We argue that our strategy simplifies the DST task while at the\nsame time achieving state of the art performance on various popular evaluation\nsets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55%.", "published": "2020-05-06 14:52:48", "link": "http://arxiv.org/abs/2005.02877v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harvesting and Refining Question-Answer Pairs for Unsupervised QA", "abstract": "Question Answering (QA) has shown great success thanks to the availability of\nlarge-scale datasets and the effectiveness of neural models. Recent research\nworks have attempted to extend these successes to the settings with few or no\nlabeled data available. In this work, we introduce two approaches to improve\nunsupervised QA. First, we harvest lexically and syntactically divergent\nquestions from Wikipedia to automatically construct a corpus of question-answer\npairs (named as RefQA). Second, we take advantage of the QA model to extract\nmore appropriate answers, which iteratively refines data over RefQA. We conduct\nexperiments on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to\nmanually annotated data. Our approach outperforms previous unsupervised\napproaches by a large margin and is competitive with early supervised models.\nWe also show the effectiveness of our approach in the few-shot learning\nsetting.", "published": "2020-05-06 15:56:06", "link": "http://arxiv.org/abs/2005.02925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What are the Goals of Distributional Semantics?", "abstract": "Distributional semantic models have become a mainstay in NLP, providing\nuseful features for downstream tasks. However, assessing long-term progress\nrequires explicit long-term goals. In this paper, I take a broad linguistic\nperspective, looking at how well current models can deal with various semantic\nchallenges. Given stark differences between models proposed in different\nsubfields, a broad perspective is needed to see how we could integrate them. I\nconclude that, while linguistic insights can guide the design of model\narchitectures, future progress will require balancing the often conflicting\ndemands of linguistic expressiveness and computational tractability.", "published": "2020-05-06 17:36:16", "link": "http://arxiv.org/abs/2005.02982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autoencoding Pixies: Amortised Variational Inference with Graph\n  Convolutions for Functional Distributional Semantics", "abstract": "Functional Distributional Semantics provides a linguistically interpretable\nframework for distributional semantics, by representing the meaning of a word\nas a function (a binary classifier), instead of a vector. However, the large\nnumber of latent variables means that inference is computationally expensive,\nand training a model is therefore slow to converge. In this paper, I introduce\nthe Pixie Autoencoder, which augments the generative model of Functional\nDistributional Semantics with a graph-convolutional neural network to perform\namortised variational inference. This allows the model to be trained more\neffectively, achieving better results on two tasks (semantic similarity in\ncontext and semantic composition), and outperforming BERT, a large pre-trained\nlanguage model.", "published": "2020-05-06 17:46:40", "link": "http://arxiv.org/abs/2005.02991v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging,\n  and Joint Modeling Approaches", "abstract": "An interesting and frequent type of multi-word expression (MWE) is the\nheadless MWE, for which there are no true internal syntactic dominance\nrelations; examples include many named entities (\"Wells Fargo\") and dates\n(\"July 5, 2020\") as well as certain productive constructions (\"blow for blow\",\n\"day after day\"). Despite their special status and prevalence, current\ndependency-annotation schemes require treating such flat structures as if they\nhad internal syntactic heads, and most current parsers handle them in the same\nfashion as headed constructions. Meanwhile, outside the context of parsing,\ntaggers are typically used for identifying MWEs, but taggers might benefit from\nstructural information. We empirically compare these two common\nstrategies--parsing and tagging--for predicting flat MWEs. Additionally, we\npropose an efficient joint decoding algorithm that combines scores from both\nstrategies. Experimental results on the MWE-Aware English Dependency Corpus and\non six non-English dependency treebanks with frequent flat structures show\nthat: (1) tagging is more accurate than parsing for identifying flat-structure\nMWEs, (2) our joint decoder reconciles the two different views and, for\nnon-BERT features, leads to higher accuracies, and (3) most of the gains result\nfrom feature sharing between the parsers and taggers.", "published": "2020-05-06 18:00:04", "link": "http://arxiv.org/abs/2005.03035v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Crossing Variational Autoencoders for Answer Retrieval", "abstract": "Answer retrieval is to find the most aligned answer from a large set of\ncandidates given a question. Learning vector representations of\nquestions/answers is the key factor. Question-answer alignment and\nquestion/answer semantics are two important signals for learning the\nrepresentations. Existing methods learned semantic representations with dual\nencoders or dual variational auto-encoders. The semantic information was\nlearned from language models or question-to-question (answer-to-answer)\ngenerative processes. However, the alignment and semantics were too separate to\ncapture the aligned semantics between question and answer. In this work, we\npropose to cross variational auto-encoders by generating questions with aligned\nanswers and generating answers with aligned questions. Experiments show that\nour method outperforms the state-of-the-art answer retrieval method on SQuAD.", "published": "2020-05-06 01:59:13", "link": "http://arxiv.org/abs/2005.02557v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Unsupervised Neural Aspect Search with Related Terms Extraction", "abstract": "The tasks of aspect identification and term extraction remain challenging in\nnatural language processing. While supervised methods achieve high scores, it\nis hard to use them in real-world applications due to the lack of labelled\ndatasets. Unsupervised approaches outperform these methods on several tasks,\nbut it is still a challenge to extract both an aspect and a corresponding term,\nparticularly in the multi-aspect setting. In this work, we present a novel\nunsupervised neural network with convolutional multi-attention mechanism, that\nallows extracting pairs (aspect, term) simultaneously, and demonstrate the\neffectiveness on the real-world dataset. We apply a special loss aimed to\nimprove the quality of multi-aspect extraction. The experimental study\ndemonstrates, what with this loss we increase the precision not only on this\njoint setting but also on aspect prediction only.", "published": "2020-05-06 12:39:45", "link": "http://arxiv.org/abs/2005.02771v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph-Embedding Empowered Entity Retrieval", "abstract": "In this research, we improve upon the current state of the art in entity\nretrieval by re-ranking the result list using graph embeddings. The paper shows\nthat graph embeddings are useful for entity-oriented search tasks. We\ndemonstrate empirically that encoding information from the knowledge graph into\n(graph) embeddings contributes to a higher increase in effectiveness of entity\nretrieval results than using plain word embeddings. We analyze the impact of\nthe accuracy of the entity linker on the overall retrieval effectiveness. Our\nanalysis further deploys the cluster hypothesis to explain the observed\nadvantages of graph embeddings over the more widely used word embeddings, for\nuser tasks involving ranking entities.", "published": "2020-05-06 14:13:49", "link": "http://arxiv.org/abs/2005.02843v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Review of Text Style Transfer Based on Deep Learning", "abstract": "Text style transfer is a hot issue in recent natural language\nprocessing,which mainly studies the text to adapt to different specific\nsituations, audiences and purposes by making some changes. The style of the\ntext usually includes many aspects such as morphology, grammar, emotion,\ncomplexity, fluency, tense, tone and so on. In the traditional text style\ntransfer model, the text style is generally relied on by experts knowledge and\nhand-designed rules, but with the application of deep learning in the field of\nnatural language processing, the text style transfer method based on deep\nlearning Started to be heavily researched. In recent years, text style transfer\nis becoming a hot issue in natural language processing research. This article\nsummarizes the research on the text style transfer model based on deep learning\nin recent years, and summarizes, analyzes and compares the main research\ndirections and progress. In addition, the article also introduces public data\nsets and evaluation indicators commonly used for text style transfer. Finally,\nthe existing characteristics of the text style transfer model are summarized,\nand the future development trend of the text style transfer model based on deep\nlearning is analyzed and forecasted.", "published": "2020-05-06 15:35:53", "link": "http://arxiv.org/abs/2005.02914v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multitask Models for Supervised Protests Detection in Texts", "abstract": "The CLEF 2019 ProtestNews Lab tasks participants to identify text relating to\npolitical protests within larger corpora of news data. Three tasks include\narticle classification, sentence detection, and event extraction. I apply\nmultitask neural networks capable of producing predictions for two and three of\nthese tasks simultaneously. The multitask framework allows the model to learn\nrelevant features from the training data of all three tasks. This paper\ndemonstrates performance near or above the reported state-of-the-art for\nautomated political event coding though noted differences in research design\nmake direct comparisons difficult.", "published": "2020-05-06 17:00:46", "link": "http://arxiv.org/abs/2005.02954v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Seeing the Forest and the Trees: Detection and Cross-Document\n  Coreference Resolution of Militarized Interstate Disputes", "abstract": "Previous efforts to automate the detection of social and political events in\ntext have primarily focused on identifying events described within single\nsentences or documents. Within a corpus of documents, these automated systems\nare unable to link event references -- recognize singular events across\nmultiple sentences or documents. A separate literature in computational\nlinguistics on event coreference resolution attempts to link known events to\none another within (and across) documents. I provide a data set for evaluating\nmethods to identify certain political events in text and to link related texts\nto one another based on shared events. The data set, Headlines of War, is built\non the Militarized Interstate Disputes data set and offers headlines classified\nby dispute status and headline pairs labeled with coreference indicators.\nAdditionally, I introduce a model capable of accomplishing both tasks. The\nmulti-task convolutional neural network is shown to be capable of recognizing\nevents and event coreferences given the headlines' texts and publication dates.", "published": "2020-05-06 17:20:14", "link": "http://arxiv.org/abs/2005.02966v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "PeTra: A Sparsely Supervised Memory Model for People Tracking", "abstract": "We propose PeTra, a memory-augmented neural network designed to track\nentities in its memory slots. PeTra is trained using sparse annotation from the\nGAP pronoun resolution dataset and outperforms a prior memory model on the task\nwhile using a simpler architecture. We empirically compare key modeling\nchoices, finding that we can simplify several aspects of the design of the\nmemory module while retaining strong performance. To measure the people\ntracking capability of memory models, we (a) propose a new diagnostic\nevaluation based on counting the number of unique entities in text, and (b)\nconduct a small scale human evaluation to compare evidence of people tracking\nin the memory logs of PeTra relative to a previous approach. PeTra is highly\neffective in both evaluations, demonstrating its ability to track people in its\nmemory despite being trained with limited annotation.", "published": "2020-05-06 17:45:35", "link": "http://arxiv.org/abs/2005.02990v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating text coherence based on the graph of the consistency of\n  phrases to identify symptoms of schizophrenia", "abstract": "Different state-of-the-art methods of the detection of schizophrenia symptoms\nbased on the estimation of text coherence have been analyzed. The analysis of a\ntext at the level of phrases has been suggested. The method based on the graph\nof the consistency of phrases has been proposed to evaluate the semantic\ncoherence and the cohesion of a text. The semantic coherence, cohesion, and\nother linguistic features (lexical diversity, lexical density) have been taken\ninto account to form feature vectors for the training of a model-classifier.\nThe training of the classifier has been performed on the set of\nEnglish-language interviews. According to the retrieved results, the impact of\neach feature on the output of the model has been analyzed. The results obtained\ncan indicate that the proposed method based on the graph of the consistency of\nphrases may be used in the different tasks of the detection of mental illness.", "published": "2020-05-06 08:38:20", "link": "http://arxiv.org/abs/2005.03008v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Categorical Vector Space Semantics for Lambek Calculus with a Relevant\n  Modality", "abstract": "We develop a categorical compositional distributional semantics for Lambek\nCalculus with a Relevant Modality !L*, which has a limited edition of the\ncontraction and permutation rules. The categorical part of the semantics is a\nmonoidal biclosed category with a coalgebra modality, very similar to the\nstructure of a Differential Category. We instantiate this category to finite\ndimensional vector spaces and linear maps via \"quantisation\" functors and work\nwith three concrete interpretations of the coalgebra modality. We apply the\nmodel to construct categorical and concrete semantic interpretations for the\nmotivating example of !L*: the derivation of a phrase with a parasitic gap. The\neffectiveness of the concrete interpretations are evaluated via a\ndisambiguation task, on an extension of a sentence disambiguation dataset to\nparasitic gap phrases, using BERT, Word2Vec, and FastText vectors and\nRelational tensors.", "published": "2020-05-06 18:58:21", "link": "http://arxiv.org/abs/2005.03074v4", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Unsupervised Multimodal Neural Machine Translation with Pseudo Visual\n  Pivoting", "abstract": "Unsupervised machine translation (MT) has recently achieved impressive\nresults with monolingual corpora only. However, it is still challenging to\nassociate source-target sentences in the latent space. As people speak\ndifferent languages biologically share similar visual systems, the potential of\nachieving better alignment through visual content is promising yet\nunder-explored in unsupervised multimodal MT (MMT). In this paper, we\ninvestigate how to utilize visual content for disambiguation and promoting\nlatent space alignment in unsupervised MMT. Our model employs multimodal\nback-translation and features pseudo visual pivoting in which we learn a shared\nmultilingual visual-semantic embedding space and incorporate visually-pivoted\ncaptioning as additional weak supervision. The experimental results on the\nwidely used Multi30K dataset show that the proposed model significantly\nimproves over the state-of-the-art methods and generalizes well when the images\nare not available at the testing time.", "published": "2020-05-06 20:11:46", "link": "http://arxiv.org/abs/2005.03119v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Probing the Natural Language Inference Task with Automated Reasoning\n  Tools", "abstract": "The Natural Language Inference (NLI) task is an important task in modern NLP,\nas it asks a broad question to which many other tasks may be reducible: Given a\npair of sentences, does the first entail the second? Although the\nstate-of-the-art on current benchmark datasets for NLI are deep learning-based,\nit is worthwhile to use other techniques to examine the logical structure of\nthe NLI task. We do so by testing how well a machine-oriented controlled\nnatural language (Attempto Controlled English) can be used to parse NLI\nsentences, and how well automated theorem provers can reason over the resulting\nformulae. To improve performance, we develop a set of syntactic and semantic\ntransformation rules. We report their performance, and discuss implications for\nNLI and logic-based NLP.", "published": "2020-05-06 03:18:11", "link": "http://arxiv.org/abs/2005.02573v1", "categories": ["cs.AI", "cs.CL", "cs.SC"], "primary_category": "cs.AI"}
{"title": "Learning Architectures from an Extended Search Space for Language\n  Modeling", "abstract": "Neural architecture search (NAS) has advanced significantly in recent years\nbut most NAS systems restrict search to learning architectures of a recurrent\nor convolutional cell. In this paper, we extend the search space of NAS. In\nparticular, we present a general approach to learn both intra-cell and\ninter-cell architectures (call it ESS). For a better search result, we design a\njoint learning method to perform intra-cell and inter-cell NAS simultaneously.\nWe implement our model in a differentiable architecture search system. For\nrecurrent neural language modeling, it outperforms a strong baseline\nsignificantly on the PTB and WikiText data, with a new state-of-the-art on PTB.\nMoreover, the learned architectures show good transferability to other systems.\nE.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity\nrecognition (NER) tasks and CoNLL chunking task, indicating a promising line of\nresearch on large-scale pre-learned architectures.", "published": "2020-05-06 05:02:33", "link": "http://arxiv.org/abs/2005.02593v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Building A User-Centric and Content-Driven Socialbot", "abstract": "To build Sounding Board, we develop a system architecture that is capable of\naccommodating dialog strategies that we designed for socialbot conversations.\nThe architecture consists of a multi-dimensional language understanding module\nfor analyzing user utterances, a hierarchical dialog management framework for\ndialog context tracking and complex dialog control, and a language generation\nprocess that realizes the response plan and makes adjustments for speech\nsynthesis. Additionally, we construct a new knowledge base to power the\nsocialbot by collecting social chat content from a variety of sources. An\nimportant contribution of the system is the synergy between the knowledge base\nand the dialog management, i.e., the use of a graph structure to organize the\nknowledge base that makes dialog control very efficient in bringing related\ncontent to the discussion. Using the data collected from Sounding Board during\nthe competition, we carry out in-depth analyses of socialbot conversations and\nuser ratings which provide valuable insights in evaluation methods for\nsocialbots. We additionally investigate a new approach for system evaluation\nand diagnosis that allows scoring individual dialog segments in the\nconversation. Finally, observing that socialbots suffer from the issue of\nshallow conversations about topics associated with unstructured data, we study\nthe problem of enabling extended socialbot conversations grounded on a\ndocument. To bring together machine reading and dialog control techniques, a\ngraph-based document representation is proposed, together with methods for\nautomatically constructing the graph. Using the graph-based representation,\ndialog control can be carried out by retrieving nodes or moving along edges in\nthe graph. To illustrate the usage, a mixed-initiative dialog strategy is\ndesigned for socialbot conversations on news articles.", "published": "2020-05-06 07:11:57", "link": "http://arxiv.org/abs/2005.02623v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Weakly-Supervised Neural Response Selection from an Ensemble of\n  Task-Specialised Dialogue Agents", "abstract": "Dialogue engines that incorporate different types of agents to converse with\nhumans are popular.\n  However, conversations are dynamic in the sense that a selected response will\nchange the conversation on-the-fly, influencing the subsequent utterances in\nthe conversation, which makes the response selection a challenging problem.\n  We model the problem of selecting the best response from a set of responses\ngenerated by a heterogeneous set of dialogue agents by taking into account the\nconversational history, and propose a \\emph{Neural Response Selection} method.\n  The proposed method is trained to predict a coherent set of responses within\na single conversation, considering its own predictions via a curriculum\ntraining mechanism.\n  Our experimental results show that the proposed method can accurately select\nthe most appropriate responses, thereby significantly improving the user\nexperience in dialogue systems.", "published": "2020-05-06 18:40:26", "link": "http://arxiv.org/abs/2005.03066v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diagnosing the Environment Bias in Vision-and-Language Navigation", "abstract": "Vision-and-Language Navigation (VLN) requires an agent to follow\nnatural-language instructions, explore the given environments, and reach the\ndesired target locations. These step-by-step navigational instructions are\ncrucial when the agent is navigating new environments about which it has no\nprior knowledge. Most recent works that study VLN observe a significant\nperformance drop when tested on unseen environments (i.e., environments not\nused in training), indicating that the neural agent models are highly biased\ntowards training environments. Although this issue is considered as one of the\nmajor challenges in VLN research, it is still under-studied and needs a clearer\nexplanation. In this work, we design novel diagnosis experiments via\nenvironment re-splitting and feature replacement, looking into possible reasons\nfor this environment bias. We observe that neither the language nor the\nunderlying navigational graph, but the low-level visual appearance conveyed by\nResNet features directly affects the agent model and contributes to this\nenvironment bias in results. According to this observation, we explore several\nkinds of semantic representations that contain less low-level visual\ninformation, hence the agent learned with these features could be better\ngeneralized to unseen testing environments. Without modifying the baseline\nagent model and its training method, our explored semantic features\nsignificantly decrease the performance gaps between seen and unseen on multiple\ndatasets (i.e. R2R, R4R, and CVDN) and achieve competitive unseen results to\nprevious state-of-the-art models. Our code and features are available at:\nhttps://github.com/zhangybzbo/EnvBiasVLN", "published": "2020-05-06 19:24:33", "link": "http://arxiv.org/abs/2005.03086v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Fact-based Dialogue Generation with Convergent and Divergent Decoding", "abstract": "Fact-based dialogue generation is a task of generating a human-like response\nbased on both dialogue context and factual texts. Various methods were proposed\nto focus on generating informative words that contain facts effectively.\nHowever, previous works implicitly assume a topic to be kept on a dialogue and\nusually converse passively, therefore the systems have a difficulty to generate\ndiverse responses that provide meaningful information proactively. This paper\nproposes an end-to-end fact-based dialogue system augmented with the ability of\nconvergent and divergent thinking over both context and facts, which can\nconverse about the current topic or introduce a new topic. Specifically, our\nmodel incorporates a novel convergent and divergent decoding that can generate\ninformative and diverse responses considering not only given inputs (context\nand facts) but also inputs-related topics. Both automatic and human evaluation\nresults on DSTC7 dataset show that our model significantly outperforms\nstate-of-the-art baselines, indicating that our model can generate more\nappropriate, informative, and diverse responses.", "published": "2020-05-06 23:49:35", "link": "http://arxiv.org/abs/2005.03174v2", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Multi-Perspective Architecture for Semantic Code Search", "abstract": "The ability to match pieces of code to their corresponding natural language\ndescriptions and vice versa is fundamental for natural language search\ninterfaces to software repositories. In this paper, we propose a novel\nmulti-perspective cross-lingual neural framework for code--text matching,\ninspired in part by a previous model for monolingual text-to-text matching, to\ncapture both global and local similarities. Our experiments on the CoNaLa\ndataset show that our proposed model yields better performance on this\ncross-lingual text-to-code matching task than previous approaches that map code\nand text to a single joint embedding space.", "published": "2020-05-06 04:46:11", "link": "http://arxiv.org/abs/2005.06980v1", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.SE"}
{"title": "A Large-Scale, Open-Domain, Mixed-Interface Dialogue-Based ITS for STEM", "abstract": "We present Korbit, a large-scale, open-domain, mixed-interface,\ndialogue-based intelligent tutoring system (ITS). Korbit uses machine learning,\nnatural language processing and reinforcement learning to provide interactive,\npersonalized learning online. Korbit has been designed to easily scale to\nthousands of subjects, by automating, standardizing and simplifying the content\ncreation process. Unlike other ITS, a teacher can develop new learning modules\nfor Korbit in a matter of hours. To facilitate learning across a widerange of\nSTEM subjects, Korbit uses a mixed-interface, which includes videos,\ninteractive dialogue-based exercises, question-answering, conceptual diagrams,\nmathematical exercises and gamification elements. Korbit has been built to\nscale to millions of students, by utilizing a state-of-the-art cloud-based\nmicro-service architecture. Korbit launched its first course in 2019 on machine\nlearning, and since then over 7,000 students have enrolled. Although Korbit was\ndesigned to be open-domain and highly scalable, A/B testing experiments with\nreal-world students demonstrate that both student learning outcomes and student\nmotivation are substantially improved compared to typical online courses.", "published": "2020-05-06 02:45:43", "link": "http://arxiv.org/abs/2005.06616v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "I.2.0; I.2.1; I.2.7; K.3.1; G.4"], "primary_category": "cs.CY"}
{"title": "Vibration Analysis in Bearings for Failure Prevention using CNN", "abstract": "Timely failure detection for bearings is of great importance to prevent\neconomic loses in the industry. In this article we propose a method based on\nConvolutional Neural Networks (CNN) to estimate the level of wear in bearings.\nFirst of all, an automatic labeling of the raw vibration data is performed to\nobtain different levels of bearing wear, by means of the Root Mean Square\nfeatures along with the Shannon's entropy to extract features from the raw\ndata, which is then grouped in seven different classes using the K-means\nalgorithm to obtain the labels. Then, the raw vibration data is converted into\nsmall square images, each sample of the data representing one pixel of the\nimage. Following this, we propose a CNN model based on the AlexNet architecture\nto classify the wear level and diagnose the rotatory system. To train the\nnetwork and validate our proposal, we use a dataset from the center of\nIntelligent Maintenance Systems (IMS), and extensively compare it with other\nmethods reported in the literature. The effectiveness of the proposed strategy\nproved to be excellent, outperforming other approaches in the state-of-the-art.", "published": "2020-05-06 23:32:05", "link": "http://arxiv.org/abs/2005.07057v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
