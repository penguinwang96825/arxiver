{"title": "Improving Event Definition Following For Zero-Shot Event Detection", "abstract": "Existing approaches on zero-shot event detection usually train models on\ndatasets annotated with known event types, and prompt them with unseen event\ndefinitions. These approaches yield sporadic successes, yet generally fall\nshort of expectations. In this work, we aim to improve zero-shot event\ndetection by training models to better follow event definitions. We hypothesize\nthat a diverse set of event types and definitions are the key for models to\nlearn to follow event definitions while existing event extraction datasets\nfocus on annotating many high-quality examples for a few event types. To verify\nour hypothesis, we construct an automatically generated Diverse Event\nDefinition (DivED) dataset and conduct comparative studies. Our experiments\nreveal that a large number of event types (200) and diverse event definitions\ncan significantly boost event extraction performance; on the other hand, the\nperformance does not scale with over ten examples per event type. Beyond\nscaling, we incorporate event ontology information and hard-negative samples\nduring training, further boosting the performance. Based on these findings, we\nfine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that\nsurpasses SOTA large language models like GPT-3.5 across three open benchmarks\non zero-shot event detection.", "published": "2024-03-05 01:46:50", "link": "http://arxiv.org/abs/2403.02586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Limitations of Large Language Models in Compositional\n  Relation Reasoning", "abstract": "We present a comprehensive evaluation of large language models(LLMs)' ability\nto reason about composition relations through a benchmark encompassing 1,500\ntest cases in English, designed to cover six distinct types of composition\nrelations: Positional, Comparative, Personal, Mathematical, Identity, and\nOther. Acknowledging the significance of multilingual capabilities, we expanded\nour assessment to include translations of these cases into Chinese, Japanese,\nFrench, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims\nat investigating the robustness and adaptability of LLMs in handling\ncomposition relation reasoning across diverse linguistic contexts.", "published": "2024-03-05 03:07:10", "link": "http://arxiv.org/abs/2403.02615v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Meta-evaluation for Grammatical Error Correction", "abstract": "Metrics are the foundation for automatic evaluation in grammatical error\ncorrection (GEC), with their evaluation of the metrics (meta-evaluation)\nrelying on their correlation with human judgments. However, conventional\nmeta-evaluations in English GEC encounter several challenges including biases\ncaused by inconsistencies in evaluation granularity, and an outdated setup\nusing classical systems. These problems can lead to misinterpretation of\nmetrics and potentially hinder the applicability of GEC techniques. To address\nthese issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation.\nSEEDA consists of corrections with human ratings along two different\ngranularities: edit-based and sentence-based, covering 12 state-of-the-art\nsystems including large language models (LLMs), and two human corrections with\ndifferent focuses. The results of improved correlations by aligning the\ngranularity in the sentence-level meta-evaluation, suggest that edit-based\nmetrics may have been underestimated in existing studies. Furthermore,\ncorrelations of most metrics decrease when changing from classical to neural\nsystems, indicating that traditional metrics are relatively poor at evaluating\nfluently corrected sentences with many edits.", "published": "2024-03-05 05:53:09", "link": "http://arxiv.org/abs/2403.02674v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door\n  Adjustment", "abstract": "Conventional multi-hop fact verification models are prone to rely on spurious\ncorrelations from the annotation artifacts, leading to an obvious performance\ndecline on unbiased datasets. Among the various debiasing works, the causal\ninference-based methods become popular by performing theoretically guaranteed\ndebiasing such as casual intervention or counterfactual reasoning. However,\nexisting causal inference-based debiasing methods, which mainly formulate fact\nverification as a single-hop reasoning task to tackle shallow bias patterns,\ncannot deal with the complicated bias patterns hidden in multiple hops of\nevidence. To address the challenge, we propose Causal Walk, a novel method for\ndebiasing multi-hop fact verification from a causal perspective with front-door\nadjustment. Specifically, in the structural causal model, the reasoning path\nbetween the treatment (the input claim-evidence graph) and the outcome (the\nveracity label) is introduced as the mediator to block the confounder. With the\nfront-door adjustment, the causal effect between the treatment and the outcome\nis decomposed into the causal effect between the treatment and the mediator,\nwhich is estimated by applying the idea of random walk, and the causal effect\nbetween the mediator and the outcome, which is estimated with normalized\nweighted geometric mean approximation. To investigate the effectiveness of the\nproposed method, an adversarial multi-hop fact verification dataset and a\nsymmetric multi-hop fact verification dataset are proposed with the help of the\nlarge language model. Experimental results show that Causal Walk outperforms\nsome previous debiasing methods on both existing datasets and the newly\nconstructed datasets. Code and data will be released at\nhttps://github.com/zcccccz/CausalWalk.", "published": "2024-03-05 06:28:02", "link": "http://arxiv.org/abs/2403.02698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breeze-7B Technical Report", "abstract": "Breeze-7B is an open-source language model based on Mistral-7B, designed to\naddress the need for improved language comprehension and chatbot-oriented\ncapabilities in Traditional Chinese. This technical report provides an overview\nof the additional pretraining, finetuning, and evaluation stages for the\nBreeze-7B model. The Breeze-7B family of base and chat models exhibits good\nperformance on language comprehension and chatbot-oriented tasks, reaching the\ntop in several benchmarks among models comparable in its complexity class.", "published": "2024-03-05 07:08:06", "link": "http://arxiv.org/abs/2403.02712v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning\n  and Memory Structure Preservation", "abstract": "Continuous Relation Extraction (CRE) aims to incrementally learn relation\nknowledge from a non-stationary stream of data. Since the introduction of new\nrelational tasks can overshadow previously learned information, catastrophic\nforgetting becomes a significant challenge in this domain. Current replay-based\ntraining paradigms prioritize all data uniformly and train memory samples\nthrough multiple rounds, which would result in overfitting old tasks and\npronounced bias towards new tasks because of the imbalances of the replay set.\nTo handle the problem, we introduce the DecouPled CRE (DP-CRE) framework that\ndecouples the process of prior information preservation and new knowledge\nacquisition. This framework examines alterations in the embedding space as new\nrelation classes emerge, distinctly managing the preservation and acquisition\nof knowledge. Extensive experiments show that DP-CRE significantly outperforms\nother CRE baselines across two datasets.", "published": "2024-03-05 07:16:51", "link": "http://arxiv.org/abs/2403.02718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Prompting: Debiasing Large Language Model Prompting based on\n  Front-Door Adjustment", "abstract": "Despite the notable advancements of existing prompting methods, such as\nIn-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they\nstill face challenges related to various biases. Traditional debiasing methods\nprimarily focus on the model training stage, including approaches based on data\naugmentation and reweighting, yet they struggle with the complex biases\ninherent in LLMs. To address such limitations, the causal relationship behind\nthe prompting methods is uncovered using a structural causal model, and a novel\ncausal prompting method based on front-door adjustment is proposed to\neffectively mitigate LLMs biases. In specific, causal intervention is achieved\nby designing the prompts without accessing the parameters and logits of LLMs.\nThe chain-of-thought generated by LLM is employed as the mediator variable and\nthe causal effect between input prompts and output answers is calculated\nthrough front-door adjustment to mitigate model biases. Moreover, to accurately\nrepresent the chain-of-thoughts and estimate the causal effects, contrastive\nlearning is used to fine-tune the encoder of chain-of-thought by aligning its\nspace with that of the LLM. Experimental results show that the proposed causal\nprompting approach achieves excellent performance across seven natural language\nprocessing datasets on both open-source and closed-source LLMs.", "published": "2024-03-05 07:47:34", "link": "http://arxiv.org/abs/2403.02738v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Training A Chinese Large Language Model for Anesthesiology", "abstract": "Medical large language models (LLMs) have gained popularity recently due to\ntheir significant practical utility. However, most existing research focuses on\ngeneral medicine, and there is a need for in-depth study of LLMs in specific\nfields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese\nAnesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions\nhave three aspects: 1) The data, such as utilizing Self-Instruct, acquired from\ncurrent LLMs likely includes inaccuracies. Hypnos implements a cross-filtering\nstrategy to improve the data quality. This strategy involves using one LLM to\nassess the quality of the generated data from another LLM and filtering out the\ndata with low quality. 2) Hypnos employs a general-to-specific training\nstrategy that starts by fine-tuning LLMs using the general medicine data and\nsubsequently improving the fine-tuned LLMs using data specifically from\nAnesthesiology. The general medical data supplement the medical expertise in\nAnesthesiology and enhance the effectiveness of Hypnos' generation. 3) We\nintroduce a standardized benchmark for evaluating medical LLM in\nAnesthesiology. Our benchmark includes both publicly available instances from\nthe Internet and privately obtained cases from the Hospital. Hypnos outperforms\nother medical LLMs in anesthesiology in metrics, GPT-4, and human evaluation on\nthe benchmark dataset.", "published": "2024-03-05 07:53:49", "link": "http://arxiv.org/abs/2403.02742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Role Prompting Guided Domain Adaptation with General Capability Preserve\n  for Large Language Models", "abstract": "The growing interest in Large Language Models (LLMs) for specialized\napplications has revealed a significant challenge: when tailored to specific\ndomains, LLMs tend to experience catastrophic forgetting, compromising their\ngeneral capabilities and leading to a suboptimal user experience. Additionally,\ncrafting a versatile model for multiple domains simultaneously often results in\na decline in overall performance due to confusion between domains. In response\nto these issues, we present the RolE Prompting Guided Multi-Domain Adaptation\n(REGA) strategy. This novel approach effectively manages multi-domain LLM\nadaptation through three key components: 1) Self-Distillation constructs and\nreplays general-domain exemplars to alleviate catastrophic forgetting. 2) Role\nPrompting assigns a central prompt to the general domain and a unique role\nprompt to each specific domain to minimize inter-domain confusion during\ntraining. 3) Role Integration reuses and integrates a small portion of\ndomain-specific data to the general-domain data, which are trained under the\nguidance of the central prompt. The central prompt is used for a streamlined\ninference process, removing the necessity to switch prompts for different\ndomains. Empirical results demonstrate that REGA effectively alleviates\ncatastrophic forgetting and inter-domain confusion. This leads to improved\ndomain-specific performance compared to standard fine-tuned models, while still\npreserving robust general capabilities.", "published": "2024-03-05 08:22:41", "link": "http://arxiv.org/abs/2403.02756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Memory Learning: A Declarative Learning Framework for Large Language\n  Models", "abstract": "The exploration of whether agents can align with their environment without\nrelying on human-labeled data presents an intriguing research topic. Drawing\ninspiration from the alignment process observed in intelligent organisms, where\ndeclarative memory plays a pivotal role in summarizing past experiences, we\npropose a novel learning framework. The agents adeptly distill insights from\npast experiences, refining and updating existing notes to enhance their\nperformance in the environment. This entire process transpires within the\nmemory components and is implemented through natural language, so we character\nthis framework as In-memory Learning. We also delve into the key features of\nbenchmarks designed to evaluate the self-improvement process. Through\nsystematic experiments, we demonstrate the effectiveness of our framework and\nprovide insights into this problem.", "published": "2024-03-05 08:25:11", "link": "http://arxiv.org/abs/2403.02757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned\n  Judge Model is not a General Substitute for GPT-4", "abstract": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-sourced models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this work, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\nintroduce a integrated method, leveraging GPT-4 to compensate for the\nlimitations and improve the fine-tuned judges. Experiment results show our\nmethod achieves accuracy on par with GPT-4 with only 50% of the API expense.", "published": "2024-03-05 10:20:52", "link": "http://arxiv.org/abs/2403.02839v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demonstrating Mutual Reinforcement Effect through Information Flow", "abstract": "The Mutual Reinforcement Effect (MRE) investigates the synergistic\nrelationship between word-level and text-level classifications in text\nclassification tasks. It posits that the performance of both classification\nlevels can be mutually enhanced. However, this mechanism has not been\nadequately demonstrated or explained in prior research. To address this gap, we\nemploy information flow analysis to observe and substantiate the MRE theory.\nOur experiments on six MRE hybrid datasets revealed the presence of MRE in the\nmodel and its impact. Additionally, we conducted fine-tuning experiments, whose\nresults were consistent with those of the information flow experiments. The\nconvergence of findings from both experiments corroborates the existence of\nMRE. Furthermore, we extended the application of MRE to prompt learning,\nutilizing word-level information as a verbalizer to bolster the model's\nprediction of text-level classification labels. In our final experiment, the\nF1-score significantly surpassed the baseline in five out of six datasets,\nfurther validating the notion that word-level information enhances the language\nmodel's comprehension of the text as a whole.", "published": "2024-03-05 12:11:32", "link": "http://arxiv.org/abs/2403.02902v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RulePrompt: Weakly Supervised Text Classification with Prompting PLMs\n  and Self-Iterative Logical Rules", "abstract": "Weakly supervised text classification (WSTC), also called zero-shot or\ndataless text classification, has attracted increasing attention due to its\napplicability in classifying a mass of texts within the dynamic and open Web\nenvironment, since it requires only a limited set of seed words (label names)\nfor each category instead of labeled data. With the help of recently popular\nprompting Pre-trained Language Models (PLMs), many studies leveraged manually\ncrafted and/or automatically identified verbalizers to estimate the likelihood\nof categories, but they failed to differentiate the effects of these\ncategory-indicative words, let alone capture their correlations and realize\nadaptive adjustments according to the unlabeled corpus. In this paper, in order\nto let the PLM effectively understand each category, we at first propose a\nnovel form of rule-based knowledge using logical expressions to characterize\nthe meanings of categories. Then, we develop a prompting PLM-based approach\nnamed RulePrompt for the WSTC task, consisting of a rule mining module and a\nrule-enhanced pseudo label generation module, plus a self-supervised\nfine-tuning module to make the PLM align with this task. Within this framework,\nthe inaccurate pseudo labels assigned to texts and the imprecise logical rules\nassociated with categories mutually enhance each other in an alternative\nmanner. That establishes a self-iterative closed loop of knowledge (rule)\nacquisition and utilization, with seed words serving as the starting point.\nExtensive experiments validate the effectiveness and robustness of our\napproach, which markedly outperforms state-of-the-art weakly supervised\nmethods. What is more, our approach yields interpretable category rules,\nproving its advantage in disambiguating easily-confused categories.", "published": "2024-03-05 12:50:36", "link": "http://arxiv.org/abs/2403.02932v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Case for Evaluating Multimodal Translation Models on Text Datasets", "abstract": "A good evaluation framework should evaluate multimodal machine translation\n(MMT) models by measuring 1) their use of visual information to aid in the\ntranslation task and 2) their ability to translate complex sentences such as\ndone for text-only machine translation. However, most current work in MMT is\nevaluated against the Multi30k testing sets, which do not measure these\nproperties. Namely, the use of visual information by the MMT model cannot be\nshown directly from the Multi30k test set results and the sentences in Multi30k\nare are image captions, i.e., short, descriptive sentences, as opposed to\ncomplex sentences that typical text-only machine translation models are\nevaluated against.\n  Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE\nevaluation framework, which measures the use of visual information by MMT\nmodels, 2) the text-only WMT news translation task test sets, which evaluates\ntranslation performance against complex sentences, and 3) the Multi30k test\nsets, for measuring MMT model performance against a real MMT dataset. Finally,\nwe evaluate recent MMT models trained solely against the Multi30k dataset\nagainst our proposed evaluation framework and demonstrate the dramatic drop\nperformance against text-only testing sets compared to recent text-only MT\nmodels.", "published": "2024-03-05 14:49:52", "link": "http://arxiv.org/abs/2403.03014v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Socratic Reasoning Improves Positive Text Rewriting", "abstract": "Reframing a negative into a positive thought is at the crux of several\ncognitive approaches to mental health and psychotherapy that could be made more\naccessible by large language model-based solutions. Such reframing is typically\nnon-trivial and requires multiple rationalization steps to uncover the\nunderlying issue of a negative thought and transform it to be more positive.\nHowever, this rationalization process is currently neglected by both datasets\nand models which reframe thoughts in one step. In this work, we address this\ngap by augmenting open-source datasets for positive text rewriting with\nsynthetically-generated Socratic rationales using a novel framework called\n\\textsc{SocraticReframe}. SocraticReframe uses a sequence of question-answer\npairs to rationalize the thought rewriting process. We show that such Socratic\nrationales significantly improve positive text rewriting for different\nopen-source LLMs according to both automatic and human evaluations guided by\ncriteria from psychotherapy research. We validate our framework and the\nsynthetic rationalizations with expert judgements from domain experts and\npsychology students in an IRB-approved annotation study. Our findings highlight\nthe potential of utilizing the synergy between LLM reasoning and established\npsychotherapy techniques to build assistive solutions for reframing negative\nthoughts.", "published": "2024-03-05 15:05:06", "link": "http://arxiv.org/abs/2403.03029v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Use Tools via Cooperative and Interactive Agents", "abstract": "Tool learning empowers large language models (LLMs) as agents to use external\ntools and extend their utility. Existing methods employ one single LLM-based\nagent to iteratively select and execute tools, thereafter incorporating\nexecution results into the next action prediction. Despite their progress,\nthese methods suffer from performance degradation when addressing practical\ntasks due to: (1) the pre-defined pipeline with restricted flexibility to\ncalibrate incorrect actions, and (2) the struggle to adapt a general LLM-based\nagent to perform a variety of specialized actions. To mitigate these problems,\nwe propose ConAgents, a Cooperative and interactive Agents framework, which\ncoordinates three specialized agents for tool selection, tool execution, and\naction calibration separately. ConAgents introduces two communication protocols\nto enable the flexible cooperation of agents. To effectively generalize the\nConAgents into open-source models, we also propose specialized action\ndistillation, enhancing their ability to perform specialized actions in our\nframework. Our extensive experiments on three datasets show that the LLMs, when\nequipped with the ConAgents, outperform baselines with substantial improvement\n(i.e., up to 14% higher success rate).", "published": "2024-03-05 15:08:16", "link": "http://arxiv.org/abs/2403.03031v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adding Multimodal Capabilities to a Text-only Translation Model", "abstract": "While most current work in multimodal machine translation (MMT) uses the\nMulti30k dataset for training and evaluation, we find that the resulting models\noverfit to the Multi30k dataset to an extreme degree. Consequently, these\nmodels perform very badly when evaluated against typical text-only testing sets\nsuch as the WMT newstest datasets. In order to perform well on both Multi30k\nand typical text-only datasets, we use a performant text-only machine\ntranslation (MT) model as the starting point of our MMT model. We add\nvision-text adapter layers connected via gating mechanisms to the MT model, and\nincrementally transform the MT model into an MMT model by 1) pre-training using\nvision-based masking of the source text and 2) fine-tuning on Multi30k.", "published": "2024-03-05 15:28:24", "link": "http://arxiv.org/abs/2403.03045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Concrete Visual Tokens for Multimodal Machine Translation", "abstract": "The challenge of visual grounding and masking in multimodal machine\ntranslation (MMT) systems has encouraged varying approaches to the detection\nand selection of visually-grounded text tokens for masking. We introduce new\nmethods for detection of visually and contextually relevant (concrete) tokens\nfrom source sentences, including detection with natural language processing\n(NLP), detection with object detection, and a joint detection-verification\ntechnique. We also introduce new methods for selection of detected tokens,\nincluding shortest $n$ tokens, longest $n$ tokens, and all detected concrete\ntokens. We utilize the GRAM MMT architecture to train models against\nsynthetically collated multimodal datasets of source images with masked\nsentences, showing performance improvements and improved usage of visual\ncontext during translation tasks over the baseline model.", "published": "2024-03-05 16:01:09", "link": "http://arxiv.org/abs/2403.03075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes\n  in Emotion Attribution", "abstract": "Large language models (LLMs) reflect societal norms and biases, especially\nabout gender. While societal biases and stereotypes have been extensively\nresearched in various NLP applications, there is a surprising gap for emotion\nanalysis. However, emotion and gender are closely linked in societal discourse.\nE.g., women are often thought of as more empathetic, while men's anger is more\nsocially accepted. To fill this gap, we present the first comprehensive study\nof gendered emotion attribution in five state-of-the-art LLMs (open- and\nclosed-source). We investigate whether emotions are gendered, and whether these\nvariations are based on societal stereotypes. We prompt the models to adopt a\ngendered persona and attribute emotions to an event like 'When I had a serious\nargument with a dear person'. We then analyze the emotions generated by the\nmodels in relation to the gender-event pairs. We find that all models\nconsistently exhibit gendered emotions, influenced by gender stereotypes. These\nfindings are in line with established research in psychology and gender\nstudies. Our study sheds light on the complex societal interplay between\nlanguage, gender, and emotion. The reproduction of emotion stereotypes in LLMs\nallows us to use those models to study the topic in detail, but raises\nquestions about the predictive use of those same LLMs for emotion applications.", "published": "2024-03-05 17:04:05", "link": "http://arxiv.org/abs/2403.03121v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoGenesis: A Framework Collaborating Large and Small Language Models for\n  Secure Context-Aware Instruction Following", "abstract": "With the advancement of language models (LMs), their exposure to private data\nis increasingly inevitable, and their deployment (especially for smaller ones)\non personal devices, such as PCs and smartphones, has become a prevailing\ntrend. In contexts laden with user information, enabling models to both\nsafeguard user privacy and execute commands efficiently emerges as an essential\nresearch imperative. In this paper, we propose CoGenesis, a collaborative\ngeneration framework integrating large (hosted on cloud infrastructure) and\nsmall models (deployed on local devices) to address privacy concerns logically.\nInitially, we design a pipeline to create personalized writing instruction\ndatasets enriched with extensive context details as the testbed of this\nresearch issue. Subsequently, we introduce two variants of CoGenesis based on\nsketch and logits respectively. Our experimental findings, based on our\nsynthesized dataset and two additional open-source datasets, indicate that: 1)\nLarge-scale models perform well when provided with user context but struggle in\nthe absence of such context. 2) While specialized smaller models fine-tuned on\nthe synthetic dataset show promise, they still lag behind their larger\ncounterparts. 3) Our CoGenesis framework, utilizing mixed-scale models,\nshowcases competitive performance, providing a feasible solution to privacy\nissues.", "published": "2024-03-05 17:15:28", "link": "http://arxiv.org/abs/2403.03129v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Guided Exploration for RL Agents in Text Environments", "abstract": "Real-world sequential decision making is characterized by sparse rewards and\nlarge decision spaces, posing significant difficulty for experiential learning\nsystems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large\nLanguage Models (LLMs), with a wealth of world knowledge, can help RL agents\nlearn quickly and adapt to distribution shifts. In this work, we introduce\nLanguage Guided Exploration (LGE) framework, which uses a pre-trained language\nmodel (called GUIDE ) to provide decision-level guidance to an RL agent (called\nEXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging\ntext environment, LGE outperforms vanilla RL agents significantly and also\noutperforms other sophisticated methods like Behaviour Cloning and Text\nDecision Transformer.", "published": "2024-03-05 17:26:41", "link": "http://arxiv.org/abs/2403.03141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PARADISE: Evaluating Implicit Planning Skills of Language Models with\n  Procedural Warnings and Tips Dataset", "abstract": "Recently, there has been growing interest within the community regarding\nwhether large language models are capable of planning or executing plans.\nHowever, most prior studies use LLMs to generate high-level plans for\nsimplified scenarios lacking linguistic complexity and domain diversity,\nlimiting analysis of their planning abilities. These setups constrain\nevaluation methods (e.g., predefined action space), architectural choices\n(e.g., only generative models), and overlook the linguistic nuances essential\nfor realistic analysis. To tackle this, we present PARADISE, an abductive\nreasoning task using Q\\&A format on practical procedural text sourced from\nwikiHow. It involves warning and tip inference tasks directly associated with\ngoals, excluding intermediary steps, with the aim of testing the ability of the\nmodels to infer implicit knowledge of the plan solely from the given goal. Our\nexperiments, utilizing fine-tuned language models and zero-shot prompting,\nreveal the effectiveness of task-specific small models over large language\nmodels in most scenarios. Despite advancements, all models fall short of human\nperformance. Notably, our analysis uncovers intriguing insights, such as\nvariations in model behavior with dropped keywords, struggles of BERT-family\nand GPT-4 with physical and abstract goals, and the proposed tasks offering\nvaluable prior knowledge for other unseen procedural tasks. The PARADISE\ndataset and associated resources are publicly available for further research\nexploration with https://github.com/GGLAB-KU/paradise.", "published": "2024-03-05 18:01:59", "link": "http://arxiv.org/abs/2403.03167v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAGID: An Automated Pipeline for Generating Synthetic Multi-modal\n  Datasets", "abstract": "Development of multimodal interactive systems is hindered by the lack of\nrich, multimodal (text, images) conversational data, which is needed in large\nquantities for LLMs. Previous approaches augment textual dialogues with\nretrieved images, posing privacy, diversity, and quality constraints. In this\nwork, we introduce Multimodal Augmented Generative Images Dialogues (MAGID), a\nframework to augment text-only dialogues with diverse and high-quality images.\nSubsequently, a diffusion model is applied to craft corresponding images,\nensuring alignment with the identified text. Finally, MAGID incorporates an\ninnovative feedback loop between an image description generation module\n(textual LLM) and image quality modules (addressing aesthetics, image-text\nmatching, and safety), that work in tandem to generate high-quality and\nmulti-modal dialogues. We compare MAGID to other SOTA baselines on three\ndialogue datasets, using automated and human evaluation. Our results show that\nMAGID is comparable to or better than baselines, with significant improvements\nin human evaluation, especially against retrieval baselines where the image\ndatabase is small.", "published": "2024-03-05 18:31:28", "link": "http://arxiv.org/abs/2403.03194v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Book2Dial: Generating Teacher-Student Interactions from Textbooks for\n  Cost-Effective Development of Educational Chatbots", "abstract": "Educational chatbots are a promising tool for assisting student learning.\nHowever, the development of effective chatbots in education has been\nchallenging, as high-quality data is seldom available in this domain. In this\npaper, we propose a framework for generating synthetic teacher-student\ninteractions grounded in a set of textbooks. Our approaches capture one aspect\nof learning interactions where curious students with partial knowledge\ninteractively ask a teacher questions about the material in the textbook. We\nhighlight various quality criteria that such dialogues should fulfill and\ncompare several approaches relying on either prompting or fine-tuning large\nlanguage models. We use synthetic dialogues to train educational chatbots and\nshow benefits of further fine-tuning in different educational domains. However,\nhuman evaluation shows that our best data synthesis method still suffers from\nhallucinations and tends to reiterate information from previous conversations.\nOur findings offer insights for future efforts in synthesizing conversational\ndata that strikes a balance between size and quality. We will open-source our\ndata and code.", "published": "2024-03-05 20:12:05", "link": "http://arxiv.org/abs/2403.03307v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guardrail Baselines for Unlearning in LLMs", "abstract": "Recent work has demonstrated that finetuning is a promising approach to\n'unlearn' concepts from large language models. However, finetuning can be\nexpensive, as it requires both generating a set of examples and running\niterations of finetuning to update the model. In this work, we show that simple\nguardrail-based approaches such as prompting and filtering can achieve\nunlearning results comparable to finetuning. We recommend that researchers\ninvestigate these lightweight baselines when evaluating the performance of more\ncomputationally intensive finetuning methods. While we do not claim that\nmethods such as prompting or filtering are universal solutions to the problem\nof unlearning, our work suggests the need for evaluation metrics that can\nbetter separate the power of guardrails vs. finetuning, and highlights\nscenarios where guardrails expose possible unintended behavior in existing\nmetrics and benchmarks.", "published": "2024-03-05 21:19:06", "link": "http://arxiv.org/abs/2403.03329v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs", "abstract": "In this paper, we introduce a black-box prompt optimization method that uses\nan attacker LLM agent to uncover higher levels of memorization in a victim\nagent, compared to what is revealed by prompting the target model with the\ntraining data directly, which is the dominant approach of quantifying\nmemorization in LLMs. We use an iterative rejection-sampling optimization\nprocess to find instruction-based prompts with two main characteristics: (1)\nminimal overlap with the training data to avoid presenting the solution\ndirectly to the model, and (2) maximal overlap between the victim model's\noutput and the training data, aiming to induce the victim to spit out training\ndata. We observe that our instruction-based prompts generate outputs with 23.7%\nhigher overlap with training data compared to the baseline prefix-suffix\nmeasurements. Our findings show that (1) instruction-tuned models can expose\npre-training data as much as their base-models, if not more so, (2) contexts\nother than the original training data can lead to leakage, and (3) using\ninstructions proposed by other LLMs can open a new avenue of automated attacks\nthat we should further study and explore. The code can be found at\nhttps://github.com/Alymostafa/Instruction_based_attack .", "published": "2024-03-05 19:32:01", "link": "http://arxiv.org/abs/2403.04801v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Minimum Information about CLinical Artificial Intelligence Checklist\n  for Generative Modeling Research (MI-CLAIM-GEN)", "abstract": "Recent advances in generative models, including large language models (LLMs),\nvision language models (VLMs), and diffusion models, have accelerated the field\nof natural language and image processing in medicine and marked a significant\nparadigm shift in how biomedical models can be developed and deployed. While\nthese models are highly adaptable to new tasks, scaling and evaluating their\nusage presents new challenges not addressed in previous frameworks. In\nparticular, the ability of these models to produce useful outputs with little\nto no specialized training data (\"zero-\" or \"few-shot\" approaches), as well as\nthe open-ended nature of their outputs, necessitate the development of new\nguidelines for robust reporting of clinical generative model research. In\nresponse to gaps in standards and best practices for the development of\nclinical AI tools identified by US Executive Order 141103 and several emerging\nnational networks for clinical AI evaluation, we begin to formalize some of\nthese guidelines by building on the original MI-CLAIM checklist. The new\nchecklist, MI-CLAIM-GEN (Table 1), aims to address differences in training,\nevaluation, interpretability, and reproducibility of new generative models\ncompared to non-generative (\"predictive\") AI models. This MI-CLAIM-GEN\nchecklist also seeks to clarify cohort selection reporting with unstructured\nclinical data and adds additional items on alignment with ethical standards for\nclinical AI research.", "published": "2024-03-05 00:27:43", "link": "http://arxiv.org/abs/2403.02558v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Systemic Biases in Sign Language AI Research: A Deaf-Led Call to\n  Reevaluate Research Agendas", "abstract": "Growing research in sign language recognition, generation, and translation AI\nhas been accompanied by calls for ethical development of such technologies.\nWhile these works are crucial to helping individual researchers do better,\nthere is a notable lack of discussion of systemic biases or analysis of\nrhetoric that shape the research questions and methods in the field, especially\nas it remains dominated by hearing non-signing researchers. Therefore, we\nconduct a systematic review of 101 recent papers in sign language AI. Our\nanalysis identifies significant biases in the current state of sign language AI\nresearch, including an overfocus on addressing perceived communication\nbarriers, a lack of use of representative datasets, use of annotations lacking\nlinguistic foundations, and development of methods that build on flawed models.\nWe take the position that the field lacks meaningful input from Deaf\nstakeholders, and is instead driven by what decisions are the most convenient\nor perceived as important to hearing researchers. We end with a call to action:\nthe field must make space for Deaf researchers to lead the conversation in sign\nlanguage AI.", "published": "2024-03-05 00:37:36", "link": "http://arxiv.org/abs/2403.02563v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Eliciting Better Multilingual Structured Reasoning from LLMs through\n  Code", "abstract": "The development of large language models (LLM) has shown progress on\nreasoning, though studies have largely considered either English or simple\nreasoning tasks. To address this, we introduce a multilingual structured\nreasoning and explanation dataset, termed xSTREET, that covers four tasks\nacross six languages. xSTREET exposes a gap in base LLM performance between\nEnglish and non-English reasoning tasks.\n  We then propose two methods to remedy this gap, building on the insight that\nLLMs trained on code are better reasoners. First, at training time, we augment\na code dataset with multilingual comments using machine translation while\nkeeping program code as-is. Second, at inference time, we bridge the gap\nbetween training and inference by employing a prompt structure that\nincorporates step-by-step code primitives to derive new facts and find a\nsolution. Our methods show improved multilingual performance on xSTREET, most\nnotably on the scientific commonsense reasoning subtask. Furthermore, the\nmodels show no regression on non-reasoning tasks, thus demonstrating our\ntechniques maintain general-purpose abilities.", "published": "2024-03-05 00:48:56", "link": "http://arxiv.org/abs/2403.02567v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FinReport: Explainable Stock Earnings Forecasting via News Factor\n  Analyzing Model", "abstract": "The task of stock earnings forecasting has received considerable attention\ndue to the demand investors in real-world scenarios. However, compared with\nfinancial institutions, it is not easy for ordinary investors to mine factors\nand analyze news. On the other hand, although large language models in the\nfinancial field can serve users in the form of dialogue robots, it still\nrequires users to have financial knowledge to ask reasonable questions. To\nserve the user experience, we aim to build an automatic system, FinReport, for\nordinary investors to collect information, analyze it, and generate reports\nafter summarizing.\n  Specifically, our FinReport is based on financial news announcements and a\nmulti-factor model to ensure the professionalism of the report. The FinReport\nconsists of three modules: news factorization module, return forecasting\nmodule, risk assessment module. The news factorization module involves\nunderstanding news information and combining it with stock factors, the return\nforecasting module aim to analysis the impact of news on market sentiment, and\nthe risk assessment module is adopted to control investment risk. Extensive\nexperiments on real-world datasets have well verified the effectiveness and\nexplainability of our proposed FinReport. Our codes and datasets are available\nat https://github.com/frinkleko/FinReport.", "published": "2024-03-05 04:33:36", "link": "http://arxiv.org/abs/2403.02647v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finetuned Multimodal Language Models Are High-Quality Image-Text Data\n  Filters", "abstract": "We propose a novel framework for filtering image-text data by leveraging\nfine-tuned Multimodal Language Models (MLMs). Our approach outperforms\npredominant filtering methods (e.g., CLIPScore) via integrating the recent\nadvances in MLMs. We design four distinct yet complementary metrics to\nholistically measure the quality of image-text data. A new pipeline is\nestablished to construct high-quality instruction data for fine-tuning MLMs as\ndata filters. Comparing with CLIPScore, our MLM filters produce more precise\nand comprehensive scores that directly improve the quality of filtered data and\nboost the performance of pre-trained models. We achieve significant\nimprovements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2)\nand various downstream tasks. Our MLM filter can generalize to different models\nand tasks, and be used as a drop-in replacement for CLIPScore. An additional\nablation study is provided to verify our design choices for the MLM filter.", "published": "2024-03-05 06:05:15", "link": "http://arxiv.org/abs/2403.02677v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated\n  Large Language Model Agents", "abstract": "Recent work has embodied LLMs as agents, allowing them to access tools,\nperform actions, and interact with external content (e.g., emails or websites).\nHowever, external content introduces the risk of indirect prompt injection\n(IPI) attacks, where malicious instructions are embedded within the content\nprocessed by LLMs, aiming to manipulate these agents into executing detrimental\nactions against users. Given the potentially severe consequences of such\nattacks, establishing benchmarks to assess and mitigate these risks is\nimperative.\n  In this work, we introduce InjecAgent, a benchmark designed to assess the\nvulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent\ncomprises 1,054 test cases covering 17 different user tools and 62 attacker\ntools. We categorize attack intentions into two primary types: direct harm to\nusers and exfiltration of private data. We evaluate 30 different LLM agents and\nshow that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4\nvulnerable to attacks 24% of the time. Further investigation into an enhanced\nsetting, where the attacker instructions are reinforced with a hacking prompt,\nshows additional increases in success rates, nearly doubling the attack success\nrate on the ReAct-prompted GPT-4. Our findings raise questions about the\nwidespread deployment of LLM Agents. Our benchmark is available at\nhttps://github.com/uiuc-kang-lab/InjecAgent.", "published": "2024-03-05 06:21:45", "link": "http://arxiv.org/abs/2403.02691v3", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of\n  Vietnamese Large Language Models", "abstract": "Recent advancements in large language models (LLMs) have underscored their\nimportance in the evolution of artificial intelligence. However, despite\nextensive pretraining on multilingual datasets, available open-sourced LLMs\nexhibit limited effectiveness in processing Vietnamese. The challenge is\nexacerbated by the absence of systematic benchmark datasets and metrics\ntailored for Vietnamese LLM evaluation. To mitigate these issues, we have\nfinetuned LLMs specifically for Vietnamese and developed a comprehensive\nevaluation framework encompassing 10 common tasks and 31 metrics. Our\nevaluation results reveal that the fine-tuned LLMs exhibit enhanced\ncomprehension and generative capabilities in Vietnamese. Moreover, our analysis\nindicates that models with more parameters can introduce more biases and\nuncalibrated outputs and the key factor influencing LLM performance is the\nquality of the training or fine-tuning datasets. These insights underscore the\nsignificance of meticulous fine-tuning with high-quality datasets in enhancing\nLLM performance.", "published": "2024-03-05 07:13:28", "link": "http://arxiv.org/abs/2403.02715v2", "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "cs.CL"}
{"title": "CURATRON: Complete and Robust Preference Data for Rigorous Alignment of\n  Large Language Models", "abstract": "This paper addresses the challenges of aligning large language models (LLMs)\nwith human values via preference learning (PL), focusing on incomplete and\ncorrupted data in preference datasets. We propose a novel method for robustly\nand completely recalibrating values within these datasets to enhance LLMs'\nresilience against the issues. In particular, we devise a guaranteed polynomial\ntime ranking algorithm that robustifies several existing models, such as the\nclassic Bradley-Terry-Luce (BTL) (Bradley and Terry, 1952) model and certain\ngeneralizations of it. To the best of our knowledge, our present work is the\nfirst to propose an algorithm that provably recovers an $\\epsilon$-optimal\nranking with high probability while allowing as large as $O(n)$ perturbed\npairwise comparison results per model response. Furthermore, we show robust\nrecovery results in the partially observed setting. Our experiments confirm\nthat our algorithms handle adversarial noise and unobserved comparisons well in\nboth general and LLM preference dataset settings. This work contributes to the\ndevelopment and scaling of more reliable and ethically aligned AI models by\nequipping the dataset curation pipeline with the ability to handle missing and\nmaliciously manipulated inputs.", "published": "2024-03-05 07:58:12", "link": "http://arxiv.org/abs/2403.02745v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Evaluating and Optimizing Educational Content with Large Language Model\n  Judgments", "abstract": "Creating effective educational materials generally requires expensive and\ntime-consuming studies of student learning outcomes. To overcome this barrier,\none idea is to build computational models of student learning and use them to\noptimize instructional materials. However, it is difficult to model the\ncognitive processes of learning dynamics. We propose an alternative approach\nthat uses Language Models (LMs) as educational experts to assess the impact of\nvarious instructions on learning outcomes. Specifically, we use GPT-3.5 to\nevaluate the overall effect of instructional materials on different student\ngroups and find that it can replicate well-established educational findings\nsuch as the Expertise Reversal Effect and the Variability Effect. This\ndemonstrates the potential of LMs as reliable evaluators of educational\ncontent. Building on this insight, we introduce an instruction optimization\napproach in which one LM generates instructional materials using the judgments\nof another LM as a reward function. We apply this approach to create math word\nproblem worksheets aimed at maximizing student learning gains. Human teachers'\nevaluations of these LM-generated worksheets show a significant alignment\nbetween the LM judgments and human teacher preferences. We conclude by\ndiscussing potential divergences between human and LM opinions and the\nresulting pitfalls of automating instructional design.", "published": "2024-03-05 09:09:15", "link": "http://arxiv.org/abs/2403.02795v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DPPA: Pruning Method for Large Language Model to Model Merging", "abstract": "Model merging is to combine fine-tuned models derived from multiple domains,\nwith the intent of enhancing the model's proficiency across various domains.\nThe principal concern is the resolution of parameter conflicts. A substantial\namount of existing research remedy this issue during the merging stage, with\nthe latest study focusing on resolving this issue throughout the pruning stage.\nThe DARE approach has exhibited promising outcomes when applied to a simplistic\nfine-tuned model. However, the efficacy of this method tends to wane when\nemployed on complex fine-tuned models that show a significant parameter bias\nrelative to the baseline model. In this paper, we introduce a dual-stage method\ntermed Dynamic Pruning Partition Amplification (DPPA), devised to tackle the\nchallenge of merging complex fine-tuned models. Initially, we introduce\nDynamically Pruning (DP), an improved approach based on magnitude pruning,\nwhich aim is to enhance performance at higher pruning rates. Subsequently, we\npropose Dynamically Partition Amplification (DPA), a rescaling strategy, is\ndesigned to dynamically amplify parameter partitions in relation to their\nsignificance levels. The experimental results show that our method maintains a\nmere 20% of domain-specific parameters and yet delivers a performance\ncomparable to other methodologies that preserve up to 90% of parameters.\nFurthermore, our method displays outstanding performance post-pruning, leading\nto a significant improvement of nearly 20% performance in model merging. We\nmake our code on Github.", "published": "2024-03-05 09:12:49", "link": "http://arxiv.org/abs/2403.02799v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated\n  Answers", "abstract": "Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 87% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\nof 81%, all without relying on external knowledge.", "published": "2024-03-05 11:50:01", "link": "http://arxiv.org/abs/2403.02889v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Cross-Lingual Document-Level Event Causality Identification\n  with Heterogeneous Graph Contrastive Transfer Learning", "abstract": "Event Causality Identification (ECI) refers to the detection of causal\nrelations between events in texts. However, most existing studies focus on\nsentence-level ECI with high-resource languages, leaving more challenging\ndocument-level ECI (DECI) with low-resource languages under-explored. In this\npaper, we propose a Heterogeneous Graph Interaction Model with\nMulti-granularity Contrastive Transfer Learning (GIMC) for zero-shot\ncross-lingual document-level ECI. Specifically, we introduce a heterogeneous\ngraph interaction network to model the long-distance dependencies between\nevents that are scattered over a document. Then, to improve cross-lingual\ntransferability of causal knowledge learned from the source language, we\npropose a multi-granularity contrastive transfer learning module to align the\ncausal representations across languages. Extensive experiments show our\nframework outperforms the previous state-of-the-art model by 9.4% and 8.2% of\naverage F1 score on monolingual and multilingual scenarios respectively.\nNotably, in the multilingual scenario, our zero-shot framework even exceeds\nGPT-3.5 with few-shot learning by 24.3% in overall performance.", "published": "2024-03-05 11:57:21", "link": "http://arxiv.org/abs/2403.02893v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Second Look on BASS -- Boosting Abstractive Summarization with Unified\n  Semantic Graphs -- A Replication Study", "abstract": "We present a detailed replication study of the BASS framework, an abstractive\nsummarization system based on the notion of Unified Semantic Graphs. Our\ninvestigation includes challenges in replicating key components and an ablation\nstudy to systematically isolate error sources rooted in replicating novel\ncomponents. Our findings reveal discrepancies in performance compared to the\noriginal work. We highlight the significance of paying careful attention even\nto reasonably omitted details for replicating advanced frameworks like BASS,\nand emphasize key practices for writing replicable papers.", "published": "2024-03-05 12:48:29", "link": "http://arxiv.org/abs/2403.02930v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmarking the Text-to-SQL Capability of Large Language Models: A\n  Comprehensive Evaluation", "abstract": "Large Language Models (LLMs) have emerged as a powerful tool in advancing the\nText-to-SQL task, significantly outperforming traditional methods.\nNevertheless, as a nascent research field, there is still no consensus on the\noptimal prompt templates and design frameworks. Additionally, existing\nbenchmarks inadequately explore the performance of LLMs across the various\nsub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs'\ncognitive capabilities and the optimization of LLM-based solutions. To address\nthe aforementioned issues, we firstly construct a new dataset designed to\nmitigate the risk of overfitting in LLMs. Then we formulate five evaluation\ntasks to comprehensively assess the performance of diverse methods across\nvarious LLMs throughout the Text-to-SQL process.Our study highlights the\nperformance disparities among LLMs and proposes optimal in-context learning\nsolutions tailored to each task. These findings offer valuable insights for\nenhancing the development of LLM-based Text-to-SQL systems.", "published": "2024-03-05 13:23:48", "link": "http://arxiv.org/abs/2403.02951v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AgentsCourt: Building Judicial Decision-Making Agents with Court Debate\n  Simulation and Legal Knowledge Augmentation", "abstract": "With the development of deep learning, natural language processing technology\nhas effectively improved the efficiency of various aspects of the traditional\njudicial industry. However, most current efforts focus on tasks within\nindividual judicial stages, making it difficult to handle complex tasks that\nspan multiple stages. As the autonomous agents powered by large language models\nare becoming increasingly smart and able to make complex decisions in\nreal-world settings, offering new insights for judicial intelligence. In this\npaper, (1) we propose a novel multi-agent framework, AgentsCourt, for judicial\ndecision-making. Our framework follows the classic court trial process,\nconsisting of court debate simulation, legal resources retrieval and\ndecision-making refinement to simulate the decision-making of judge. (2) we\nintroduce SimuCourt, a judicial benchmark that encompasses 420 Chinese judgment\ndocuments, spanning the three most common types of judicial cases. Furthermore,\nto support this task, we construct a large-scale legal knowledge base,\nLegal-KB, with multi-resource legal knowledge. (3) Extensive experiments show\nthat our framework outperforms the existing advanced methods in various\naspects, especially in generating legal articles, where our model achieves\nsignificant improvements of 8.6% and 9.1% F1 score in the first and second\ninstance settings, respectively.", "published": "2024-03-05 13:30:02", "link": "http://arxiv.org/abs/2403.02959v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A General and Flexible Multi-concept Parsing Framework for Multilingual\n  Semantic Matching", "abstract": "Sentence semantic matching is a research hotspot in natural language\nprocessing, which is considerably significant in various key scenarios, such as\ncommunity question answering, searching, chatbot, and recommendation. Since\nmost of the advanced models directly model the semantic relevance among words\nbetween two sentences while neglecting the \\textit{keywords} and\n\\textit{intents} concepts of them, DC-Match is proposed to disentangle keywords\nfrom intents and utilizes them to optimize the matching performance. Although\nDC-Match is a simple yet effective method for semantic matching, it highly\ndepends on the external NER techniques to identify the keywords of sentences,\nwhich limits the performance of semantic matching for minor languages since\nsatisfactory NER tools are usually hard to obtain. In this paper, we propose to\ngenerally and flexibly resolve the text into multi concepts for multilingual\nsemantic matching to liberate the model from the reliance on NER models. To\nthis end, we devise a \\underline{M}ulti-\\underline{C}oncept \\underline{P}arsed\n\\underline{S}emantic \\underline{M}atching framework based on the pre-trained\nlanguage models, abbreviated as \\textbf{MCP-SM}, to extract various concepts\nand infuse them into the classification tokens. We conduct comprehensive\nexperiments on English datasets QQP and MRPC, and Chinese dataset Medical-SM.\nBesides, we experiment on Arabic datasets MQ2Q and XNLI, the outstanding\nperformance further prove MCP-SM's applicability in low-resource languages.", "published": "2024-03-05 13:55:16", "link": "http://arxiv.org/abs/2403.02975v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation using Large Language Models: Data Perspectives,\n  Learning Paradigms and Challenges", "abstract": "In the rapidly evolving field of large language models (LLMs), data\naugmentation (DA) has emerged as a pivotal technique for enhancing model\nperformance by diversifying training examples without the need for additional\ndata collection. This survey explores the transformative impact of LLMs on DA,\nparticularly addressing the unique challenges and opportunities they present in\nthe context of natural language processing (NLP) and beyond. From both data and\nlearning perspectives, we examine various strategies that utilize LLMs for data\naugmentation, including a novel exploration of learning paradigms where\nLLM-generated data is used for diverse forms of further training. Additionally,\nthis paper highlights the primary open challenges faced in this domain, ranging\nfrom controllable data augmentation to multi-modal data augmentation. This\nsurvey highlights a paradigm shift introduced by LLMs in DA, and aims to serve\nas a comprehensive guide for researchers and practitioners.", "published": "2024-03-05 14:11:54", "link": "http://arxiv.org/abs/2403.02990v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word Importance Explains How Prompts Affect Language Model Outputs", "abstract": "The emergence of large language models (LLMs) has revolutionized numerous\napplications across industries. However, their \"black box\" nature often hinders\nthe understanding of how they make specific decisions, raising concerns about\ntheir transparency, reliability, and ethical use. This study presents a method\nto improve the explainability of LLMs by varying individual words in prompts to\nuncover their statistical impact on the model outputs. This approach, inspired\nby permutation importance for tabular data, masks each word in the system\nprompt and evaluates its effect on the outputs based on the available text\nscores aggregated over multiple user inputs. Unlike classical attention, word\nimportance measures the impact of prompt words on arbitrarily-defined text\nscores, which enables decomposing the importance of words into the specific\nmeasures of interest--including bias, reading level, verbosity, etc. This\nprocedure also enables measuring impact when attention weights are not\navailable. To test the fidelity of this approach, we explore the effect of\nadding different suffixes to multiple different system prompts and comparing\nsubsequent generations with different large language models. Results show that\nword importance scores are closely related to the expected suffix importances\nfor multiple scoring functions.", "published": "2024-03-05 15:04:18", "link": "http://arxiv.org/abs/2403.03028v1", "categories": ["cs.AI", "cs.CL", "I.2.7; I.5.2"], "primary_category": "cs.AI"}
{"title": "\"In Dialogues We Learn\": Towards Personalized Dialogue Without\n  Pre-defined Profiles through In-Dialogue Learning", "abstract": "Personalized dialogue systems have gained significant attention in recent\nyears for their ability to generate responses in alignment with different\npersonas. However, most existing approaches rely on pre-defined personal\nprofiles, which are not only time-consuming and labor-intensive to create but\nalso lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning\nframework that enhances the ability of pre-trained large language models to\nleverage dialogue history to characterize persona for completing personalized\ndialogue generation tasks without pre-defined profiles. Our experiments on\nthree datasets demonstrate that IDL brings substantial improvements, with BLEU\nand ROUGE scores increasing by up to 200% and 247%, respectively. Additionally,\nthe results of human evaluations further validate the efficacy of our proposed\nmethod.", "published": "2024-03-05 16:43:03", "link": "http://arxiv.org/abs/2403.03102v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Document-Level Event-Argument Data\n  Augmentation for Challenging Role Types", "abstract": "Event Argument Extraction (EAE) is an extremely difficult information\nextraction problem -- with significant limitations in few-shot cross-domain\n(FSCD) settings. A common solution to FSCD modeling is data augmentation.\nUnfortunately, existing augmentation methods are not well-suited to a variety\nof real-world EAE contexts including (i) The need to model long documents (10+\nsentences) (ii) The need to model zero and few-shot roles (i.e. event roles\nwith little to no training representation). In this work, we introduce two\nnovel LLM-powered data augmentation frameworks for synthesizing extractive\ndocument-level EAE samples using zero in-domain training data. Our highest\nperforming methods provide a 16-pt increase in F1 score on extraction of zero\nshot role types.\n  To better facilitate analysis of cross-domain EAE, we additionally introduce\na new metric, Role-Depth F1 (RDF1), which uses statistical depth to identify\nroles in the target domain which are semantic outliers with respect to roles\nobserved in the source domain. Our experiments show that LLM-based augmentation\ncan boost RDF1 performance by up to 11 F1 points compared to baseline methods.", "published": "2024-03-05 20:07:42", "link": "http://arxiv.org/abs/2403.03304v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach\n  for Relation Classification", "abstract": "This paper introduces a novel neuro-symbolic architecture for relation\nclassification (RC) that combines rule-based methods with contemporary deep\nlearning techniques. This approach capitalizes on the strengths of both\nparadigms: the adaptability of rule-based systems and the generalization power\nof neural networks. Our architecture consists of two components: a declarative\nrule-based model for transparent classification and a neural component to\nenhance rule generalizability through semantic text matching. Notably, our\nsemantic matcher is trained in an unsupervised domain-agnostic way, solely with\nsynthetic data. Further, these components are loosely coupled, allowing for\nrule modifications without retraining the semantic matcher. In our evaluation,\nwe focused on two few-shot relation classification datasets: Few-Shot TACRED\nand a Few-Shot version of NYT29. We show that our proposed method outperforms\nprevious state-of-the-art models in three out of four settings, despite not\nseeing any human-annotated training data. Further, we show that our approach\nremains modular and pliable, i.e., the corresponding rules can be locally\nmodified to improve the overall model. Human interventions to the rules for the\nTACRED relation \\texttt{org:parents} boost the performance on that relation by\nas much as 26\\% relative improvement, without negatively impacting the other\nrelations, and without retraining the semantic matching component.", "published": "2024-03-05 20:08:32", "link": "http://arxiv.org/abs/2403.03305v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DIVERSE: A Dataset of YouTube Video Comment Stances with a Data\n  Programming Model", "abstract": "Public opinion of military organizations significantly influences their\nability to recruit talented individuals. As recruitment efforts increasingly\nextend into digital spaces like social media, it becomes essential to assess\nthe stance of social media users toward online military content. However, there\nis a notable lack of data for analyzing opinions on military recruiting efforts\nonline, compounded by challenges in stance labeling, which is crucial for\nunderstanding public perceptions. Despite the importance of stance analysis for\nsuccessful online military recruitment, creating human-annotated, in-domain\nstance labels is resource-intensive. In this paper, we address both the\nchallenges of stance labeling and the scarcity of data on public opinions of\nonline military recruitment by introducing and releasing the DIVERSE dataset:\nhttps://doi.org/10.5281/zenodo.10493803. This dataset comprises all comments\nfrom the U.S. Army's official YouTube Channel videos. We employed a\nstate-of-the-art weak supervision approach, leveraging large language models to\nlabel the stance of each comment toward its respective video and the U.S. Army.\nOur findings indicate that the U.S. Army's videos began attracting a\nsignificant number of comments post-2021, with the stance distribution\ngenerally balanced among supportive, oppositional, and neutral comments, with a\nslight skew towards oppositional versus supportive comments.", "published": "2024-03-05 21:36:23", "link": "http://arxiv.org/abs/2403.03334v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scope of Large Language Models for Mining Emerging Opinions in Online\n  Health Discourse", "abstract": "In this paper, we develop an LLM-powered framework for the curation and\nevaluation of emerging opinion mining in online health communities. We\nformulate emerging opinion mining as a pairwise stance detection problem\nbetween (title, comment) pairs sourced from Reddit, where post titles contain\nemerging health-related claims on a topic that is not predefined. The claims\nare either explicitly or implicitly expressed by the user. We detail (i) a\nmethod of claim identification -- the task of identifying if a post title\ncontains a claim and (ii) an opinion mining-driven evaluation framework for\nstance detection using LLMs.\n  We facilitate our exploration by releasing a novel test dataset, Long\nCOVID-Stance, or LC-stance, which can be used to evaluate LLMs on the tasks of\nclaim identification and stance detection in online health communities. Long\nCovid is an emerging post-COVID disorder with uncertain and complex treatment\nguidelines, thus making it a suitable use case for our task. LC-Stance contains\nlong COVID treatment related discourse sourced from a Reddit community. Our\nevaluation shows that GPT-4 significantly outperforms prior works on zero-shot\nstance detection. We then perform thorough LLM model diagnostics, identifying\nthe role of claim type (i.e. implicit vs explicit claims) and comment length as\nsources of model error.", "published": "2024-03-05 21:38:19", "link": "http://arxiv.org/abs/2403.03336v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning to Maximize Mutual Information for Chain-of-Thought\n  Distillation", "abstract": "Knowledge distillation, the technique of transferring knowledge from large,\ncomplex models to smaller ones, marks a pivotal step towards efficient AI\ndeployment. Distilling Step-by-Step~(DSS), a novel method utilizing\nchain-of-thought~(CoT) distillation, has demonstrated promise by imbuing\nsmaller models with the superior reasoning capabilities of their larger\ncounterparts. In DSS, the distilled model acquires the ability to generate\nrationales and predict labels concurrently through a multi-task learning\nframework. However, DSS overlooks the intrinsic relationship between the two\ntraining tasks, leading to ineffective integration of CoT knowledge with the\ntask of label prediction. To this end, we investigate the mutual relationship\nof the two tasks from Information Bottleneck perspective and formulate it as\nmaximizing the mutual information of the representation features of the two\ntasks. We propose a variational approach to solve this optimization problem\nusing a learning-based method. Our experimental results across four datasets\ndemonstrate that our method outperforms the state-of-the-art DSS. Our findings\noffer insightful guidance for future research on language model distillation as\nwell as applications involving CoT. Codes are available at\n\\url{https://github.com/xinchen9/cot_distillation_ACL2024}.", "published": "2024-03-05 22:21:45", "link": "http://arxiv.org/abs/2403.03348v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Found in the Middle: How Language Models Use Long Contexts Better via\n  Plug-and-Play Positional Encoding", "abstract": "This paper aims to overcome the \"lost-in-the-middle\" challenge of large\nlanguage models (LLMs). While recent advancements have successfully enabled\nLLMs to perform stable language modeling with up to 4 million tokens, the\npersistent difficulty faced by most LLMs in identifying relevant information\nsituated in the middle of the context has not been adequately tackled. To\naddress this problem, this paper introduces Multi-scale Positional Encoding\n(Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the\ncapacity of LLMs to handle the relevant information located in the middle of\nthe context, without fine-tuning or introducing any additional overhead. Ms-PoE\nleverages the position indice rescaling to relieve the long-term decay effect\nintroduced by RoPE, while meticulously assigning distinct scaling ratios to\ndifferent attention heads to preserve essential knowledge learned during the\npre-training step, forming a multi-scale context fusion from short to long\ndistance. Extensive experiments with a wide range of LLMs demonstrate the\nefficacy of our approach. Notably, Ms-PoE achieves an average accuracy gain of\nup to 3.8 on the Zero-SCROLLS benchmark over the original LLMs. Code are\navailable at https://github.com/VITA-Group/Ms-PoE.", "published": "2024-03-05 04:58:37", "link": "http://arxiv.org/abs/2403.04797v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using\n  in-context learning with GPT and instruction-tuned Llama models", "abstract": "This paper presents our system development for SemEval-2024 Task 3: \"The\nCompetition of Multimodal Emotion Cause Analysis in Conversations\". Effectively\ncapturing emotions in human conversations requires integrating multiple\nmodalities such as text, audio, and video. However, the complexities of these\ndiverse modalities pose challenges for developing an efficient multimodal\nemotion cause analysis (ECA) system. Our proposed approach addresses these\nchallenges by a two-step framework. We adopt two different approaches in our\nimplementation. In Approach 1, we employ instruction-tuning with two separate\nLlama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V\nfor conversation-level video description and employ in-context learning with\nannotated conversation using GPT 3.5. Our system wins rank 4, and system\nablation experiments demonstrate that our proposed solutions achieve\nsignificant performance gains. All the experimental codes are available on\nGithub.", "published": "2024-03-05 12:07:18", "link": "http://arxiv.org/abs/2403.04798v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative\n  Literature Summary", "abstract": "The literature review is an indispensable step in the research process. It\nprovides the benefit of comprehending the research problem and understanding\nthe current research situation while conducting a comparative analysis of prior\nworks. However, literature summary is challenging and time consuming. The\nprevious LLM-based studies on literature review mainly focused on the complete\nprocess, including literature retrieval, screening, and summarization. However,\nfor the summarization step, simple CoT method often lacks the ability to\nprovide extensive comparative summary. In this work, we firstly focus on the\nindependent literature summarization step and introduce ChatCite, an LLM agent\nwith human workflow guidance for comparative literature summary. This agent, by\nmimicking the human workflow, first extracts key elements from relevant\nliterature and then generates summaries using a Reflective Incremental\nMechanism. In order to better evaluate the quality of the generated summaries,\nwe devised a LLM-based automatic evaluation metric, G-Score, in refer to the\nhuman evaluation criteria. The ChatCite agent outperformed other models in\nvarious dimensions in the experiments. The literature summaries generated by\nChatCite can also be directly used for drafting literature reviews.", "published": "2024-03-05 01:13:56", "link": "http://arxiv.org/abs/2403.02574v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "68T50", "I.2.7"], "primary_category": "cs.IR"}
{"title": "Android in the Zoo: Chain-of-Action-Thought for GUI Agents", "abstract": "Large language model (LLM) leads to a surge of autonomous GUI agents for\nsmartphone, which completes a task triggered by natural language through\npredicting a sequence of actions of API. Even though the task highly relies on\npast actions and visual observations, existing studies typically consider\nlittle semantic information carried out by intermediate screenshots and screen\noperations. To address this, this work presents Chain-of-Action-Thought (dubbed\nCoAT), which takes the description of the previous actions, the current screen,\nand more importantly the action thinking of what actions should be performed\nand the outcomes led by the chosen action. We demonstrate that, in a zero-shot\nsetting upon three off-the-shelf LMMs, CoAT significantly improves the action\nprediction compared to previous proposed context modeling. To further\nfacilitate the research in this line, we construct a dataset Android-In-The-Zoo\n(AitZ), which contains 18,643 screen-action pairs together with\nchain-of-action-thought annotations. Experiments show that fine-tuning a 1B\nmodel (i.e. AUTO-UI-base) on our AitZ dataset achieves on-par performance with\nCogAgent-Chat-18B.", "published": "2024-03-05 07:09:35", "link": "http://arxiv.org/abs/2403.02713v2", "categories": ["cs.CL", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?", "abstract": "There is an ongoing debate regarding the potential of Large Language Models\n(LLMs) as foundational models seamlessly integrated with Cyber-Physical Systems\n(CPS) for interpreting the physical world. In this paper, we carry out a case\nstudy to answer the following question: Are LLMs capable of zero-shot human\nactivity recognition (HAR). Our study, HARGPT, presents an affirmative answer\nby demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in\na zero-shot manner, with only appropriate prompts. HARGPT inputs raw IMU data\ninto LLMs and utilizes the role-play and think step-by-step strategies for\nprompting. We benchmark HARGPT on GPT4 using two public datasets of different\ninter-class similarities and compare various baselines both based on\ntraditional machine learning and state-of-the-art deep classification models.\nRemarkably, LLMs successfully recognize human activities from raw IMU data and\nconsistently outperform all the baselines on both datasets. Our findings\nindicate that by effective prompting, LLMs can interpret raw IMU data based on\ntheir knowledge base, possessing a promising potential to analyze raw sensor\ndata of the physical world effectively.", "published": "2024-03-05 07:34:51", "link": "http://arxiv.org/abs/2403.02727v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Enhancing Conceptual Understanding in Multimodal Contrastive Learning\n  through Hard Negative Samples", "abstract": "Current multimodal models leveraging contrastive learning often face\nlimitations in developing fine-grained conceptual understanding. This is due to\nrandom negative samples during pretraining, causing almost exclusively very\ndissimilar concepts to be compared in the loss function. Consequently, the\nmodels struggle with fine-grained semantic differences. To address this\nproblem, we introduce a novel pretraining method incorporating synthetic hard\nnegative text examples. The hard negatives permute terms corresponding to\nvisual concepts, leading to a more fine-grained visual and textual concept\nalignment. Further, we introduce InpaintCOCO, a new challenging dataset for\nassessing the fine-grained alignment of colors, objects, and sizes in\nvision-language models. We created the dataset using generative inpainting from\nCOCO images by changing the visual concepts so that the images no longer match\ntheir original captions. Our results show significant improvements in\nfine-grained concept understanding across a wide range of vision-language\ndatasets, including our InpaintCOCO dataset.", "published": "2024-03-05 11:38:48", "link": "http://arxiv.org/abs/2403.02875v2", "categories": ["cs.CV", "cs.CL", "cs.IR", "I.4; I.7"], "primary_category": "cs.CV"}
{"title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nproblem-solving. However, their proficiency in solving mathematical problems\nremains inadequate. We propose MathScale, a simple and scalable method to\ncreate high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt\nGPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning,\nit first extracts topics and knowledge points from seed math questions and then\nbuild a concept graph, which is subsequently used to generate new math\nquestions. MathScale exhibits effective scalability along the size axis of the\nmath dataset that we generate. As a result, we create a mathematical reasoning\ndataset (MathScaleQA) containing two million math question-answer pairs. To\nevaluate mathematical reasoning abilities of LLMs comprehensively, we construct\n{\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten\ndatasets (including GSM8K and MATH) covering K-12, college, and competition\nlevel math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g.,\nLLaMA-2 and Mistral), resulting in significantly improved capabilities in\nmathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves\nstate-of-the-art performance across all datasets, surpassing its best peers of\nequivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average\naccuracy, respectively.", "published": "2024-03-05 11:42:59", "link": "http://arxiv.org/abs/2403.02884v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PaperWeaver: Enriching Topical Paper Alerts by Contextualizing\n  Recommended Papers with User-collected Papers", "abstract": "With the rapid growth of scholarly archives, researchers subscribe to \"paper\nalert\" systems that periodically provide them with recommendations of recently\npublished papers that are similar to previously collected papers. However,\nresearchers sometimes struggle to make sense of nuanced connections between\nrecommended papers and their own research context, as existing systems only\npresent paper titles and abstracts. To help researchers spot these connections,\nwe present PaperWeaver, an enriched paper alerts system that provides\ncontextualized text descriptions of recommended papers based on user-collected\npapers. PaperWeaver employs a computational method based on Large Language\nModels (LLMs) to infer users' research interests from their collected papers,\nextract context-specific aspects of papers, and compare recommended and\ncollected papers on these aspects. Our user study (N=15) showed that\nparticipants using PaperWeaver were able to better understand the relevance of\nrecommended papers and triage them more confidently when compared to a baseline\nthat presented the related work sections from recommended papers.", "published": "2024-03-05 13:10:06", "link": "http://arxiv.org/abs/2403.02939v2", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.DL"}
{"title": "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot\n  Question Answering", "abstract": "Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance\nQuesetion Answering (QA) performance of Large Language Models (LLMs), yet\nstructured KG verbalization remains challengin. Existing methods, such as\ntriple-form or free-form textual conversion of triple-form facts, encounter\nseveral issues. These include reduced evidence density due to duplicated\nentities or relationships, and reduced evidence clarity due to an inability to\nemphasize crucial evidence. To address these issues, we propose EFSum, an\nEvidence-focused Fact Summarization framework for enhanced QA with\nknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer\nthrough distillation and preference alignment. Our extensive experiments show\nthat EFSum improves LLM's zero-shot QA performance, and it is possible to\nensure both the helpfulness and faithfulness of the summary.", "published": "2024-03-05 13:43:58", "link": "http://arxiv.org/abs/2403.02966v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Design2Code: Benchmarking Multimodal Code Generation for Automated\n  Front-End Engineering", "abstract": "Generative AI has made rapid advancements in recent years, achieving\nunprecedented capabilities in multimodal understanding and code generation.\nThis can enable a new paradigm of front-end development in which multimodal\nlarge language models (MLLMs) directly convert visual designs into code\nimplementations. In this work, we construct Design2Code - the first real-world\nbenchmark for this task. Specifically, we manually curate 484 diverse\nreal-world webpages as test cases and develop a set of automatic evaluation\nmetrics to assess how well current multimodal LLMs can generate the code\nimplementations that directly render into the given reference webpages, given\nthe screenshots as input. We also complement automatic metrics with\ncomprehensive human evaluations to validate the performance ranking. To\nrigorously benchmark MLLMs, we test various multimodal prompting methods on\nfrontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained\nbreak-down metrics indicate that models mostly lag in recalling visual elements\nfrom the input webpages and generating correct layout designs.", "published": "2024-03-05 17:56:27", "link": "http://arxiv.org/abs/2403.03163v3", "categories": ["cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Reliable, Adaptable, and Attributable Language Models with Retrieval", "abstract": "Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.", "published": "2024-03-05 18:22:33", "link": "http://arxiv.org/abs/2403.03187v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning", "abstract": "The White House Executive Order on Artificial Intelligence highlights the\nrisks of large language models (LLMs) empowering malicious actors in developing\nbiological, cyber, and chemical weapons. To measure these risks of malicious\nuse, government institutions and major AI labs are developing evaluations for\nhazardous capabilities in LLMs. However, current evaluations are private,\npreventing further research into mitigating risk. Furthermore, they focus on\nonly a few, highly specific pathways for malicious use. To fill these gaps, we\npublicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a\ndataset of 3,668 multiple-choice questions that serve as a proxy measurement of\nhazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP\nwas developed by a consortium of academics and technical consultants, and was\nstringently filtered to eliminate sensitive information prior to public\nrelease. WMDP serves two roles: first, as an evaluation for hazardous knowledge\nin LLMs, and second, as a benchmark for unlearning methods to remove such\nhazardous knowledge. To guide progress on unlearning, we develop RMU, a\nstate-of-the-art unlearning method based on controlling model representations.\nRMU reduces model performance on WMDP while maintaining general capabilities in\nareas such as biology and computer science, suggesting that unlearning may be a\nconcrete path towards reducing malicious use from LLMs. We release our\nbenchmark and code publicly at https://wmdp.ai", "published": "2024-03-05 18:59:35", "link": "http://arxiv.org/abs/2403.03218v7", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "AI Literacy in Low-Resource Languages:Insights from creating AI in\n  Yoruba videos", "abstract": "To effectively navigate the AI revolution, AI literacy is crucial. However,\ncontent predominantly exists in dominant languages, creating a gap for\nlow-resource languages like Yoruba (41 million native speakers). This case\nstudy explores bridging this gap by creating and distributing AI videos in\nYoruba.The project developed 26 videos covering foundational, intermediate, and\nadvanced AI concepts, leveraging storytelling and accessible explanations.\nThese videos were created using a cost-effective methodology and distributed\nacross YouTube, LinkedIn, and Twitter, reaching an estimated global audience of\n22 countries. Analysis of YouTube reveals insights into viewing patterns, with\nthe 25-44 age group contributing the most views. Notably, over half of the\ntraffic originated from external sources, highlighting the potential of\ncross-platform promotion.This study demonstrates the feasibility and impact of\ncreating AI literacy content in low-resource languages. It emphasizes that\naccurate interpretation requires both technical expertise in AI and fluency in\nthe target language. This work contributes a replicable methodology, a 22-word\nYoruba AI vocabulary, and data-driven insights into audience demographics and\nacquisition channel", "published": "2024-03-05 12:27:28", "link": "http://arxiv.org/abs/2403.04799v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "AttentionStitch: How Attention Solves the Speech Editing Problem", "abstract": "The generation of natural and high-quality speech from text is a challenging\nproblem in the field of natural language processing. In addition to speech\ngeneration, speech editing is also a crucial task, which requires the seamless\nand unnoticeable integration of edited speech into synthesized speech. We\npropose a novel approach to speech editing by leveraging a pre-trained\ntext-to-speech (TTS) model, such as FastSpeech 2, and incorporating a double\nattention block network on top of it to automatically merge the synthesized\nmel-spectrogram with the mel-spectrogram of the edited text. We refer to this\nmodel as AttentionStitch, as it harnesses attention to stitch audio samples\ntogether. We evaluate the proposed AttentionStitch model against\nstate-of-the-art baselines on both single and multi-speaker datasets, namely\nLJSpeech and VCTK. We demonstrate its superior performance through an objective\nand a subjective evaluation test involving 15 human participants.\nAttentionStitch is capable of producing high-quality speech, even for words not\nseen during training, while operating automatically without the need for human\nintervention. Moreover, AttentionStitch is fast during both training and\ninference and is able to generate human-sounding edited speech.", "published": "2024-03-05 22:09:58", "link": "http://arxiv.org/abs/2403.04804v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey", "abstract": "We present a survey of more than 90 recent papers that aim to study cultural\nrepresentation and inclusion in large language models (LLMs). We observe that\nnone of the studies explicitly define \"culture, which is a complex,\nmultifaceted concept; instead, they probe the models on some specially designed\ndatasets which represent certain aspects of \"culture\". We call these aspects\nthe proxies of culture, and organize them across two dimensions of demographic\nand semantic proxies. We also categorize the probing methods employed. Our\nanalysis indicates that only certain aspects of ``culture,'' such as values and\nobjectives, have been studied, leaving several other interesting and important\nfacets, especially the multitude of semantic domains (Thompson et al., 2020)\nand aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps\nare the lack of robustness of probing techniques and situated studies on the\nimpact of cultural mis- and under-representation in LLM-based applications.", "published": "2024-03-05 08:29:36", "link": "http://arxiv.org/abs/2403.15412v5", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "MeanCache: User-Centric Semantic Caching for LLM Web Services", "abstract": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.", "published": "2024-03-05 06:23:50", "link": "http://arxiv.org/abs/2403.02694v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.DC", "I.2.7"], "primary_category": "cs.LG"}
{"title": "AIx Speed: Playback Speed Optimization Using Listening Comprehension of\n  Speech Recognition Models", "abstract": "Since humans can listen to audio and watch videos at faster speeds than\nactually observed, we often listen to or watch these pieces of content at\nhigher playback speeds to increase the time efficiency of content\ncomprehension. To further utilize this capability, systems that automatically\nadjust the playback speed according to the user's condition and the type of\ncontent to assist in more efficient comprehension of time-series content have\nbeen developed. However, there is still room for these systems to further\nextend human speed-listening ability by generating speech with playback speed\noptimized for even finer time units and providing it to humans. In this study,\nwe determine whether humans can hear the optimized speech and propose a system\nthat automatically adjusts playback speed at units as small as phonemes while\nensuring speech intelligibility. The system uses the speech recognizer score as\na proxy for how well a human can hear a certain unit of speech and maximizes\nthe speech playback speed to the extent that a human can hear. This method can\nbe used to produce fast but intelligible speech. In the evaluation experiment,\nwe compared the speech played back at a constant fast speed and the flexibly\nspeed-up speech generated by the proposed method in a blind test and confirmed\nthat the proposed method produced speech that was easier to listen to.", "published": "2024-03-05 13:08:52", "link": "http://arxiv.org/abs/2403.02938v1", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models", "abstract": "While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.", "published": "2024-03-05 16:35:25", "link": "http://arxiv.org/abs/2403.03100v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents", "abstract": "Large Language Models (LLMs) have demonstrated great potential in complex\nreasoning tasks, yet they fall short when tackling more sophisticated\nchallenges, especially when interacting with environments through generating\nexecutable actions. This inadequacy primarily stems from the lack of built-in\naction knowledge in language agents, which fails to effectively guide the\nplanning trajectories during task solving and results in planning\nhallucination. To address this issue, we introduce KnowAgent, a novel approach\ndesigned to enhance the planning capabilities of LLMs by incorporating explicit\naction knowledge. Specifically, KnowAgent employs an action knowledge base and\na knowledgeable self-learning strategy to constrain the action path during\nplanning, enabling more reasonable trajectory synthesis, and thereby enhancing\nthe planning performance of language agents. Experimental results on HotpotQA\nand ALFWorld based on various backbone models demonstrate that KnowAgent can\nachieve comparable or superior performance to existing baselines. Further\nanalysis indicates the effectiveness of KnowAgent in terms of planning\nhallucinations mitigation. Code is available in\nhttps://github.com/zjunlp/KnowAgent.", "published": "2024-03-05 16:39:12", "link": "http://arxiv.org/abs/2403.03101v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context\n  Misinformation Detection", "abstract": "Misinformation is a prevalent societal issue due to its potential high risks.\nOut-of-context (OOC) misinformation, where authentic images are repurposed with\nfalse text, is one of the easiest and most effective ways to mislead audiences.\nCurrent methods focus on assessing image-text consistency but lack convincing\nexplanations for their judgments, which is essential for debunking\nmisinformation. While Multimodal Large Language Models (MLLMs) have rich\nknowledge and innate capability for visual reasoning and explanation\ngeneration, they still lack sophistication in understanding and discovering the\nsubtle crossmodal differences. In this paper, we introduce SNIFFER, a novel\nmultimodal large language model specifically engineered for OOC misinformation\ndetection and explanation. SNIFFER employs two-stage instruction tuning on\nInstructBLIP. The first stage refines the model's concept alignment of generic\nobjects with news-domain entities and the second stage leverages language-only\nGPT-4 generated OOC-specific instruction data to fine-tune the model's\ndiscriminatory powers. Enhanced by external tools and retrieval, SNIFFER not\nonly detects inconsistencies between text and image but also utilizes external\nknowledge for contextual verification. Our experiments show that SNIFFER\nsurpasses the original MLLM by over 40% and outperforms state-of-the-art\nmethods in detection accuracy. SNIFFER also provides accurate and persuasive\nexplanations as validated by quantitative and human evaluations.", "published": "2024-03-05 18:04:59", "link": "http://arxiv.org/abs/2403.03170v1", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.MM"}
{"title": "Enhanced DareFightingICE Competitions: Sound Design and AI Competitions", "abstract": "This paper presents a new and improved DareFightingICE platform, a fighting\ngame platform with a focus on visually impaired players (VIPs), in the Unity\ngame engine. It also introduces the separation of the DareFightingICE\nCompetition into two standalone competitions called DareFightingICE Sound\nDesign Competition and DareFightingICE AI Competition--at the 2024 IEEE\nConference on Games (CoG)--in which a new platform will be used. This new\nplatform is an enhanced version of the old DareFightingICE platform, having a\nbetter audio system to convey 3D sound and a better way to send audio data to\nAI agents. With this enhancement and by utilizing Unity, the new\nDareFightingICE platform is more accessible in terms of adding new features for\nVIPs and future audio research. This paper also improves the evaluation method\nfor evaluating sound designs in the Sound Design Competition which will ensure\na better sound design for VIPs as this competition continues to run at future\nCoG. To the best of our knowledge, both of our competitions are first of their\nkind, and the connection between the competitions to mutually improve the\nentries' quality with time makes these competitions an important part of\nrepresenting an often overlooked segment within the broader gaming community,\nVIPs.", "published": "2024-03-05 06:15:48", "link": "http://arxiv.org/abs/2403.02687v2", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "I.2; H.5.2; H.5.5"], "primary_category": "cs.HC"}
{"title": "Fighting Game Adaptive Background Music for Improved Gameplay", "abstract": "This paper presents our work to enhance the background music (BGM) in\nDareFightingICE by adding adaptive features. The adaptive BGM consists of three\ndifferent categories of instruments playing the BGM of the winner sound design\nfrom the 2022 DareFightingICE Competition. The BGM adapts by changing the\nvolume of each category of instruments. Each category is connected to a\ndifferent element of the game. We then run experiments to evaluate the adaptive\nBGM by using a deep reinforcement learning AI agent that only uses audio as\ninput (Blind DL AI). The results show that the performance of the Blind DL AI\nimproves while playing with the adaptive BGM as compared to playing without the\nadaptive BGM.", "published": "2024-03-05 06:46:43", "link": "http://arxiv.org/abs/2403.02701v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "I.2; H.5.2; H.5"], "primary_category": "cs.SD"}
{"title": "Single-Channel Robot Ego-Speech Filtering during Human-Robot Interaction", "abstract": "In this paper, we study how well human speech can automatically be filtered\nwhen this overlaps with the voice and fan noise of a social robot, Pepper. We\nultimately aim for an HRI scenario where the microphone can remain open when\nthe robot is speaking, enabling a more natural turn-taking scheme where the\nhuman can interrupt the robot. To respond appropriately, the robot would need\nto understand what the interlocutor said in the overlapping part of the speech,\nwhich can be accomplished by target speech extraction (TSE). To investigate how\nwell TSE can be accomplished in the context of the popular social robot Pepper,\nwe set out to manufacture a datase composed of a mixture of recorded speech of\nPepper itself, its fan noise (which is close to the microphones), and human\nspeech as recorded by the Pepper microphone, in a room with low reverberation\nand high reverberation. Comparing a signal processing approach, with and\nwithout post-filtering, and a convolutional recurrent neural network (CRNN)\napproach to a state-of-the-art speaker identification-based TSE model, we found\nthat the signal processing approach without post-filtering yielded the best\nperformance in terms of Word Error Rate on the overlapping speech signals with\nlow reverberation, while the CRNN approach is more robust for reverberation.\nThese results show that estimating the human voice in overlapping speech with a\nrobot is possible in real-life application, provided that the room\nreverberation is low and the human speech has a high volume or high pitch.", "published": "2024-03-05 12:35:18", "link": "http://arxiv.org/abs/2403.02918v1", "categories": ["cs.RO", "cs.HC", "cs.SD", "eess.AS", "I.2.9"], "primary_category": "cs.RO"}
{"title": "Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source\n  Localization", "abstract": "Audio-Visual Source Localization (AVSL) is the task of identifying specific\nsounding objects in the scene given audio cues. In our work, we focus on\nsemi-supervised AVSL with pseudo-labeling. To address the issues with vanilla\nhard pseudo-labels including bias accumulation, noise sensitivity, and\ninstability, we propose a novel method named Cross Pseudo-Labeling (XPL),\nwherein two models learn from each other with the cross-refine mechanism to\navoid bias accumulation. We equip XPL with two effective components. Firstly,\nthe soft pseudo-labels with sharpening and pseudo-label exponential moving\naverage mechanisms enable models to achieve gradual self-improvement and ensure\nstable training. Secondly, the curriculum data selection module adaptively\nselects pseudo-labels with high quality during training to mitigate potential\nbias. Experimental results demonstrate that XPL significantly outperforms\nexisting methods, achieving state-of-the-art performance while effectively\nmitigating confirmation bias and ensuring training stability.", "published": "2024-03-05 16:28:48", "link": "http://arxiv.org/abs/2403.03095v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "(Un)paired signal-to-signal translation with 1D conditional GANs", "abstract": "I show that a one-dimensional (1D) conditional generative adversarial network\n(cGAN) with an adversarial training architecture is capable of unpaired\nsignal-to-signal (\"sig2sig\") translation. Using a simplified CycleGAN model\nwith 1D layers and wider convolutional kernels, mirroring WaveGAN to reframe\ntwo-dimensional (2D) image generation as 1D audio generation, I show that\nrecasting the 2D image-to-image translation task to a 1D signal-to-signal\ntranslation task with deep convolutional GANs is possible without substantial\nmodification to the conventional U-Net model and adversarial architecture\ndeveloped as CycleGAN. With this I show for a small tunable dataset that noisy\ntest signals unseen by the 1D CycleGAN model and without paired training\ntransform from the source domain to signals similar to paired test signals in\nthe translated domain, especially in terms of frequency, and I quantify these\ndifferences in terms of correlation and error.", "published": "2024-03-05 18:52:50", "link": "http://arxiv.org/abs/2403.04800v1", "categories": ["eess.AS", "cs.CV", "cs.GR", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for\n  Audio-Visual Source Localization", "abstract": "Audio-Visual Source Localization (AVSL) aims to locate sounding objects\nwithin video frames given the paired audio clips. Existing methods\npredominantly rely on self-supervised contrastive learning of audio-visual\ncorrespondence. Without any bounding-box annotations, they struggle to achieve\nprecise localization, especially for small objects, and suffer from blurry\nboundaries and false positives. Moreover, the naive semi-supervised method is\npoor in fully leveraging the information of abundant unlabeled data. In this\npaper, we propose a novel semi-supervised learning framework for AVSL, namely\nDual Mean-Teacher (DMT), comprising two teacher-student structures to\ncircumvent the confirmation bias issue. Specifically, two teachers, pre-trained\non limited labeled data, are employed to filter out noisy samples via the\nconsensus between their predictions, and then generate high-quality\npseudo-labels by intersecting their confidence maps. The sufficient utilization\nof both labeled and unlabeled data and the proposed unbiased framework enable\nDMT to outperform current state-of-the-art methods by a large margin, with CIoU\nof 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%,\n9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methods\nrespectively, given only 3% positional-annotations. We also extend our\nframework to some existing AVSL methods and consistently boost their\nperformance.", "published": "2024-03-05 17:35:46", "link": "http://arxiv.org/abs/2403.03145v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
