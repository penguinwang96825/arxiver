{"title": "Speaker Sensitive Response Evaluation Model", "abstract": "Automatic evaluation of open-domain dialogue response generation is very\nchallenging because there are many appropriate responses for a given context.\nExisting evaluation models merely compare the generated response with the\nground truth response and rate many of the appropriate responses as\ninappropriate if they deviate from the ground truth. One approach to resolve\nthis problem is to consider the similarity of the generated response with the\nconversational context. In this paper, we propose an automatic evaluation model\nbased on that idea and learn the model parameters from an unlabeled\nconversation corpus. Our approach considers the speakers in defining the\ndifferent levels of similar context. We use a Twitter conversation corpus that\ncontains many speakers and conversations to test our evaluation model.\nExperiments show that our model outperforms the other existing evaluation\nmetrics in terms of high correlation with human annotation scores. We also show\nthat our model trained on Twitter can be applied to movie dialogues without any\nadditional training. We provide our code and the learned parameters so that\nthey can be used for automatic evaluation of dialogue response generation\nmodels.", "published": "2020-06-12 08:59:10", "link": "http://arxiv.org/abs/2006.07015v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2020 Task 12: Multilingual Offensive Language Identification in\n  Social Media (OffensEval 2020)", "abstract": "We present the results and main findings of SemEval-2020 Task 12 on\nMultilingual Offensive Language Identification in Social Media (OffensEval\n2020). The task involves three subtasks corresponding to the hierarchical\ntaxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The\ntask featured five languages: English, Arabic, Danish, Greek, and Turkish for\nSubtask A. In addition, English also featured Subtasks B and C. OffensEval 2020\nwas one of the most popular tasks at SemEval-2020 attracting a large number of\nparticipants across all subtasks and also across all languages. A total of 528\nteams signed up to participate in the task, 145 teams submitted systems during\nthe evaluation period, and 70 submitted system description papers.", "published": "2020-06-12 14:39:40", "link": "http://arxiv.org/abs/2006.07235v2", "categories": ["cs.CL", "68T50, 68T07", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Low-resource Languages: A Review of Past Work and Future Challenges", "abstract": "A current problem in NLP is massaging and processing low-resource languages\nwhich lack useful training attributes such as supervised data, number of native\nspeakers or experts, etc. This review paper concisely summarizes previous\ngroundbreaking achievements made towards resolving this problem, and analyzes\npotential improvements in the context of the overall future research direction.", "published": "2020-06-12 15:21:57", "link": "http://arxiv.org/abs/2006.07264v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Information Extraction of Clinical Trial Eligibility Criteria", "abstract": "Clinical trials predicate subject eligibility on a diversity of criteria\nranging from patient demographics to food allergies. Trials post their\nrequirements as semantically complex, unstructured free-text. Formalizing trial\ncriteria to a computer-interpretable syntax would facilitate eligibility\ndetermination. In this paper, we investigate an information extraction (IE)\napproach for grounding criteria from trials in ClinicalTrials(dot)gov to a\nshared knowledge base. We frame the problem as a novel knowledge base\npopulation task, and implement a solution combining machine learning and\ncontext free grammar. To our knowledge, this work is the first criteria\nextraction system to apply attention-based conditional random field\narchitecture for named entity recognition (NER), and word2vec embedding\nclustering for named entity linking (NEL). We release the resources and core\ncomponents of our system on GitHub at\nhttps://github.com/facebookresearch/Clinical-Trial-Parser. Finally, we report\nour per module and end to end performances; we conclude that our system is\ncompetitive with Criteria2Query, which we view as the current state-of-the-art\nin criteria extraction.", "published": "2020-06-12 16:25:45", "link": "http://arxiv.org/abs/2006.07296v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Forecasting Skill from Text", "abstract": "People vary in their ability to make accurate predictions about the future.\nPrior studies have shown that some individuals can predict the outcome of\nfuture events with consistently better accuracy. This leads to a natural\nquestion: what makes some forecasters better than others? In this paper we\nexplore connections between the language people use to describe their\npredictions and their forecasting skill. Datasets from two different\nforecasting domains are explored: (1) geopolitical forecasts from Good Judgment\nOpen, an online prediction forum and (2) a corpus of company earnings forecasts\nmade by financial analysts. We present a number of linguistic metrics which are\ncomputed over text associated with people's predictions about the future\nincluding: uncertainty, readability, and emotion. By studying linguistic\nfactors associated with predictions, we are able to shed some light on the\napproach taken by skilled forecasters. Furthermore, we demonstrate that it is\npossible to accurately predict forecasting skill using a model that is based\nsolely on language. This could potentially be useful for identifying accurate\npredictions or potentially skilled forecasters earlier.", "published": "2020-06-12 19:04:10", "link": "http://arxiv.org/abs/2006.07425v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating a Multi-sense Definition Generation Model for Multiple\n  Languages", "abstract": "Most prior work on definition modeling has not accounted for polysemy, or has\ndone so by considering definition modeling for a target word in a given\ncontext. In contrast, in this study, we propose a context-agnostic approach to\ndefinition modeling, based on multi-sense word embeddings, that is capable of\ngenerating multiple definitions for a target word. In further, contrast to most\nprior work, which has primarily focused on English, we evaluate our proposed\napproach on fifteen different datasets covering nine languages from several\nlanguage families. To evaluate our approach we consider several variations of\nBLEU. Our results demonstrate that our proposed multi-sense model outperforms a\nsingle-sense model on all fifteen datasets.", "published": "2020-06-12 18:15:59", "link": "http://arxiv.org/abs/2006.07398v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Generative Model for Joint Natural Language Understanding and\n  Generation", "abstract": "Natural language understanding (NLU) and natural language generation (NLG)\nare two fundamental and related tasks in building task-oriented dialogue\nsystems with opposite objectives: NLU tackles the transformation from natural\nlanguage to formal representations, whereas NLG does the reverse. A key to\nsuccess in either task is parallel training data which is expensive to obtain\nat a large scale. In this work, we propose a generative model which couples NLU\nand NLG through a shared latent variable. This approach allows us to explore\nboth spaces of natural language and formal representations, and facilitates\ninformation sharing through the latent space to eventually benefit NLU and NLG.\nOur model achieves state-of-the-art performance on two dialogue datasets with\nboth flat and tree-structured formal representations. We also show that the\nmodel can be trained in a semi-supervised fashion by utilising unlabelled data\nto boost its performance.", "published": "2020-06-12 22:38:55", "link": "http://arxiv.org/abs/2006.07499v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Dogs have Whiskers? A New Knowledge Base of hasPart Relations", "abstract": "We present a new knowledge-base of hasPart relationships, extracted from a\nlarge corpus of generic statements. Complementary to other resources available,\nit is the first which is all three of: accurate (90% precision), salient\n(covers relationships a person may mention), and has high coverage of common\nterms (approximated as within a 10 year old's vocabulary), as well as having\nseveral times more hasPart entries than in the popular ontologies ConceptNet\nand WordNet. In addition, it contains information about quantifiers, argument\nmodifiers, and links the entities to appropriate concepts in Wikipedia and\nWordNet. The knowledge base is available at https://allenai.org/data/haspartkb", "published": "2020-06-12 23:34:05", "link": "http://arxiv.org/abs/2006.07510v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving GAN Training with Probability Ratio Clipping and Sample\n  Reweighting", "abstract": "Despite success on a wide range of problems related to vision, generative\nadversarial networks (GANs) often suffer from inferior performance due to\nunstable training, especially for text generation. To solve this issue, we\npropose a new variational GAN training framework which enjoys superior training\nstability. Our approach is inspired by a connection of GANs and reinforcement\nlearning under a variational perspective. The connection leads to (1)\nprobability ratio clipping that regularizes generator training to prevent\nexcessively large updates, and (2) a sample re-weighting mechanism that\nimproves discriminator training by downplaying bad-quality fake samples.\nMoreover, our variational GAN framework can provably overcome the training\nissue in many GANs that an optimal discriminator cannot provide any informative\ngradient to training generator. By plugging the training approach in diverse\nstate-of-the-art GAN architectures, we obtain significantly improved\nperformance over a range of tasks, including text generation, text style\ntransfer, and image generation.", "published": "2020-06-12 01:39:48", "link": "http://arxiv.org/abs/2006.06900v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning Effective Representations for Person-Job Fit by Feature Fusion", "abstract": "Person-job fit is to match candidates and job posts on online recruitment\nplatforms using machine learning algorithms. The effectiveness of matching\nalgorithms heavily depends on the learned representations for the candidates\nand job posts. In this paper, we propose to learn comprehensive and effective\nrepresentations of the candidates and job posts via feature fusion. First, in\naddition to applying deep learning models for processing the free text in\nresumes and job posts, which is adopted by existing methods, we extract\nsemantic entities from the whole resume (and job post) and then learn features\nfor them. By fusing the features from the free text and the entities, we get a\ncomprehensive representation for the information explicitly stated in the\nresume and job post. Second, however, some information of a candidate or a job\nmay not be explicitly captured in the resume or job post. Nonetheless, the\nhistorical applications including accepted and rejected cases can reveal some\nimplicit intentions of the candidates or recruiters. Therefore, we propose to\nlearn the representations of implicit intentions by processing the historical\napplications using LSTM. Last, by fusing the representations for the explicit\nand implicit intentions, we get a more comprehensive and effective\nrepresentation for person-job fit. Experiments over 10 months real data show\nthat our solution outperforms existing methods with a large margin. Ablation\nstudies confirm the contribution of each component of the fused representation.\nThe extracted semantic entities help interpret the matching results during the\ncase study.", "published": "2020-06-12 09:02:41", "link": "http://arxiv.org/abs/2006.07017v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Language-Conditioned Goal Generation: a New Approach to Language\n  Grounding for RL", "abstract": "In the real world, linguistic agents are also embodied agents: they perceive\nand act in the physical world. The notion of Language Grounding questions the\ninteractions between language and embodiment: how do learning agents connect or\nground linguistic representations to the physical world ? This question has\nrecently been approached by the Reinforcement Learning community under the\nframework of instruction-following agents. In these agents, behavioral policies\nor reward functions are conditioned on the embedding of an instruction\nexpressed in natural language. This paper proposes another approach: using\nlanguage to condition goal generators. Given any goal-conditioned policy, one\ncould train a language-conditioned goal generator to generate language-agnostic\ngoals for the agent. This method allows to decouple sensorimotor learning from\nlanguage acquisition and enable agents to demonstrate a diversity of behaviors\nfor any given instruction. We propose a particular instantiation of this\napproach and demonstrate its benefits.", "published": "2020-06-12 09:54:38", "link": "http://arxiv.org/abs/2006.07043v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language\n  Processing", "abstract": "Neural Architecture Search (NAS) is a promising and rapidly evolving research\narea. Training a large number of neural networks requires an exceptional amount\nof computational power, which makes NAS unreachable for those researchers who\nhave limited or no access to high-performance clusters and supercomputers. A\nfew benchmarks with precomputed neural architectures performances have been\nrecently introduced to overcome this problem and ensure more reproducible\nexperiments. However, these benchmarks are only for the computer vision domain\nand, thus, are built from the image datasets and convolution-derived\narchitectures. In this work, we step outside the computer vision domain by\nleveraging the language modeling task, which is the core of natural language\nprocessing (NLP). Our main contribution is as follows: we have provided search\nspace of recurrent neural networks on the text datasets and trained 14k\narchitectures within it; we have conducted both intrinsic and extrinsic\nevaluation of the trained models using datasets for semantic relatedness and\nlanguage understanding evaluation; finally, we have tested several NAS\nalgorithms to demonstrate how the precomputed results can be utilized. We\nbelieve that our results have high potential of usage for both NAS and NLP\ncommunities.", "published": "2020-06-12 12:19:06", "link": "http://arxiv.org/abs/2006.07116v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sparse and Continuous Attention Mechanisms", "abstract": "Exponential families are widely used in machine learning; they include many\ndistributions in continuous and discrete domains (e.g., Gaussian, Dirichlet,\nPoisson, and categorical distributions via the softmax transformation).\nDistributions in each of these families have fixed support. In contrast, for\nfinite domains, there has been recent work on sparse alternatives to softmax\n(e.g. sparsemax and alpha-entmax), which have varying support, being able to\nassign zero probability to irrelevant categories. This paper expands that work\nin two directions: first, we extend alpha-entmax to continuous domains,\nrevealing a link with Tsallis statistics and deformed exponential families.\nSecond, we introduce continuous-domain attention mechanisms, deriving efficient\ngradient backpropagation algorithms for alpha in {1,2}. Experiments on\nattention-based text classification, machine translation, and visual question\nanswering illustrate the use of continuous attention in 1D and 2D, showing that\nit allows attending to time intervals and compact regions.", "published": "2020-06-12 14:16:48", "link": "http://arxiv.org/abs/2006.07214v3", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Dutch General Public Reaction on Governmental COVID-19 Measures and\n  Announcements in Twitter Data", "abstract": "Public sentiment (the opinions, attitudes or feelings expressed by the\npublic) is a factor of interest for government, as it directly influences the\nimplementation of policies. Given the unprecedented nature of the COVID-19\ncrisis, having an up-to-date representation of public sentiment on governmental\nmeasures and announcements is crucial. While the 'staying-at-home' policy makes\nface-to-face interactions and interviews challenging, analysing real-time\nTwitter data that reflects public opinion toward policy measures is a\ncost-effective way to access public sentiment. In this context, we collect\nstreaming data using the Twitter API starting from the COVID-19 outbreak in the\nNetherlands in February 2020, and track Dutch general public reactions on\ngovernmental measures and announcements. We provide temporal analysis of tweet\nfrequency and public sentiment over the past seven months. We also identify\npublic attitudes towards two Dutch policies in case studies: one regarding\nsocial distancing and one regarding wearing face masks. By presenting those\npreliminary results, we aim to provide visibility into the social media\ndiscussions around COVID-19 to the general public, scientists and policy\nmakers. The data collection and analysis will be updated and expanded over\ntime.", "published": "2020-06-12 16:03:58", "link": "http://arxiv.org/abs/2006.07283v3", "categories": ["cs.SI", "cs.CL", "cs.IR", "I.2.7; I.5.4; J.4"], "primary_category": "cs.SI"}
{"title": "How to Avoid Being Eaten by a Grue: Structured Exploration Strategies\n  for Textual Worlds", "abstract": "Text-based games are long puzzles or quests, characterized by a sequence of\nsparse and potentially deceptive rewards. They provide an ideal platform to\ndevelop agents that perceive and act upon the world using a combinatorially\nsized natural language state-action space. Standard Reinforcement Learning\nagents are poorly equipped to effectively explore such spaces and often\nstruggle to overcome bottlenecks---states that agents are unable to pass\nthrough simply because they do not see the right action sequence enough times\nto be sufficiently reinforced. We introduce Q*BERT, an agent that learns to\nbuild a knowledge graph of the world by answering questions, which leads to\ngreater sample efficiency. To overcome bottlenecks, we further introduce\nMC!Q*BERT an agent that uses an knowledge-graph-based intrinsic motivation to\ndetect bottlenecks and a novel exploration strategy to efficiently learn a\nchain of policy modules to overcome them. We present an ablation study and\nresults demonstrating how our method outperforms the current state-of-the-art\non nine text games, including the popular game, Zork, where, for the first\ntime, a learning agent gets past the bottleneck where the player is eaten by a\nGrue.", "published": "2020-06-12 18:24:06", "link": "http://arxiv.org/abs/2006.07409v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Understanding Unintended Memorization in Federated Learning", "abstract": "Recent works have shown that generative sequence models (e.g., language\nmodels) have a tendency to memorize rare or unique sequences in the training\ndata. Since useful models are often trained on sensitive data, to ensure the\nprivacy of the training data it is critical to identify and mitigate such\nunintended memorization. Federated Learning (FL) has emerged as a novel\nframework for large-scale distributed learning tasks. However, it differs in\nmany aspects from the well-studied central learning setting where all the data\nis stored at the central server. In this paper, we initiate a formal study to\nunderstand the effect of different components of canonical FL on unintended\nmemorization in trained models, comparing with the central learning setting.\nOur results show that several differing components of FL play an important role\nin reducing unintended memorization. Specifically, we observe that the\nclustering of data according to users---which happens by design in FL---has a\nsignificant effect in reducing such memorization, and using the method of\nFederated Averaging for training causes a further reduction. We also show that\ntraining with a strong user-level differential privacy guarantee results in\nmodels that exhibit the least amount of unintended memorization.", "published": "2020-06-12 22:10:16", "link": "http://arxiv.org/abs/2006.07490v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "\"Notic My Speech\" -- Blending Speech Patterns With Multimedia", "abstract": "Speech as a natural signal is composed of three parts - visemes (visual part\nof speech), phonemes (spoken part of speech), and language (the imposed\nstructure). However, video as a medium for the delivery of speech and a\nmultimedia construct has mostly ignored the cognitive aspects of speech\ndelivery. For example, video applications like transcoding and compression have\ntill now ignored the fact how speech is delivered and heard. To close the gap\nbetween speech understanding and multimedia video applications, in this paper,\nwe show the initial experiments by modelling the perception on visual speech\nand showing its use case on video compression. On the other hand, in the visual\nspeech recognition domain, existing studies have mostly modeled it as a\nclassification problem, while ignoring the correlations between views,\nphonemes, visemes, and speech perception. This results in solutions which are\nfurther away from how human perception works. To bridge this gap, we propose a\nview-temporal attention mechanism to model both the view dependence and the\nvisemic importance in speech recognition and understanding. We conduct\nexperiments on three public visual speech recognition datasets. The\nexperimental results show that our proposed method outperformed the existing\nwork by 4.99% in terms of the viseme error rate. Moreover, we show that there\nis a strong correlation between our model's understanding of multi-view speech\nand the human perception. This characteristic benefits downstream applications\nsuch as video compression and streaming where a significant number of less\nimportant frames can be compressed or eliminated while being able to maximally\npreserve human speech understanding with good user experience.", "published": "2020-06-12 06:51:55", "link": "http://arxiv.org/abs/2006.08599v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CUHK at SemEval-2020 Task 4: CommonSense Explanation, Reasoning and\n  Prediction with Multi-task Learning", "abstract": "This paper describes our system submitted to task 4 of SemEval 2020:\nCommonsense Validation and Explanation (ComVE) which consists of three\nsub-tasks. The task is to directly validate the given sentence whether or not\nit makes sense and require the model to explain it. Based on BERTarchitecture\nwith a multi-task setting, we propose an effective and interpretable \"Explain,\nReason and Predict\" (ERP) system to solve the three sub-tasks about\ncommonsense: (a) Validation, (b)Reasoning, and (c) Explanation. Inspired by\ncognitive studies of common sense, our system first generates a reason or\nunderstanding of the sentences and then chooses which one statement makes\nsense, which is achieved by multi-task learning. During the post-evaluation,\nour system has reached 92.9% accuracy in subtask A (rank 11), 89.7% accuracy in\nsubtask B (rank 9), andBLEU score of 12.9 in subtask C (rank 8)", "published": "2020-06-12 13:51:12", "link": "http://arxiv.org/abs/2006.09161v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparing Natural Language Processing Techniques for Alzheimer's\n  Dementia Prediction in Spontaneous Speech", "abstract": "Alzheimer's Dementia (AD) is an incurable, debilitating, and progressive\nneurodegenerative condition that affects cognitive function. Early diagnosis is\nimportant as therapeutics can delay progression and give those diagnosed vital\ntime. Developing models that analyse spontaneous speech could eventually\nprovide an efficient diagnostic modality for earlier diagnosis of AD. The\nAlzheimer's Dementia Recognition through Spontaneous Speech task offers\nacoustically pre-processed and balanced datasets for the classification and\nprediction of AD and associated phenotypes through the modelling of spontaneous\nspeech. We exclusively analyse the supplied textual transcripts of the\nspontaneous speech dataset, building and comparing performance across numerous\nmodels for the classification of AD vs controls and the prediction of Mental\nMini State Exam scores. We rigorously train and evaluate Support Vector\nMachines (SVMs), Gradient Boosting Decision Trees (GBDT), and Conditional\nRandom Fields (CRFs) alongside deep learning Transformer based models. We find\nour top performing models to be a simple Term Frequency-Inverse Document\nFrequency (TF-IDF) vectoriser as input into a SVM model and a pre-trained\nTransformer based model `DistilBERT' when used as an embedding layer into\nsimple linear models. We demonstrate test set scores of 0.81-0.82 across\nclassification metrics and a RMSE of 4.58.", "published": "2020-06-12 17:51:16", "link": "http://arxiv.org/abs/2006.07358v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Non-parallel voice conversion based on source-to-target direct mapping", "abstract": "Recent works of utilizing phonetic posteriograms (PPGs) for non-parallel\nvoice conversion have significantly increased the usability of voice conversion\nsince the source and target DBs are no longer required for matching contents.\nIn this approach, the PPGs are used as the linguistic bridge between source and\ntarget speaker features. However, this PPG-based non-parallel voice conversion\nhas some limitation that it needs two cascading networks at conversion time,\nmaking it less suitable for real-time applications and vulnerable to source\nspeaker intelligibility at conversion stage. To address this limitation, we\npropose a new non-parallel voice conversion technique that employs a single\nneural network for direct source-to-target voice parameter mapping. With this\nsingle network structure, the proposed approach can reduce both conversion time\nand number of network parameters, which can be especially important factors in\nembedded or real-time environments. Additionally, it improves the quality of\nvoice conversion by skipping the phone recognizer at conversion stage. It can\neffectively prevent possible loss of phonetic information the PPG-based\nindirect method suffers. Experiments show that our approach reduces number of\nnetwork parameters and conversion time by 41.9% and 44.5%, respectively, with\nimproved voice similarity over the original PPG-based method.", "published": "2020-06-12 04:21:21", "link": "http://arxiv.org/abs/2006.06937v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Domain-adversarial training of multi-speaker TTS", "abstract": "Multi-speaker TTS has to learn both linguistic embedding and text embedding\nto generate speech of desired linguistic content in desired voice. However, it\nis unclear which characteristic of speech results from speaker and which part\nfrom linguistic content. In this paper, text embedding is forced to unlearn\nspeaker dependent characteristic using gradient reversal layer to auxiliary\nspeaker classifier that we introduce. We train a speaker classifier using\nangular margin softmax loss. In subjective evaluation, it is shown that the\nadversarial training of text embedding for unilingual multi-speaker TTS results\nin 39.9% improvement on similarity MOS and 40.1% improvement on naturalness\nMOS.", "published": "2020-06-12 04:44:22", "link": "http://arxiv.org/abs/2006.06942v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Generic Indic Text-to-speech Synthesisers with Rapid Adaptation in an\n  End-to-end Framework", "abstract": "Building text-to-speech (TTS) synthesisers for Indian languages is a\ndifficult task owing to a large number of active languages. Indian languages\ncan be classified into a finite set of families, prominent among them,\nIndo-Aryan and Dravidian. The proposed work exploits this property to build a\ngeneric TTS system using multiple languages from the same family in an\nend-to-end framework. Generic systems are quite robust as they are capable of\ncapturing a variety of phonotactics across languages. These systems are then\nadapted to a new language in the same family using small amounts of adaptation\ndata. Experiments indicate that good quality TTS systems can be built using\nonly 7 minutes of adaptation data. An average degradation mean opinion score of\n3.98 is obtained for the adapted TTSes.\n  Extensive analysis of systematic interactions between languages in the\ngeneric TTSes is carried out. x-vectors are included as speaker embedding to\nsynthesise text in a particular speaker's voice. An interesting observation is\nthat the prosody of the target speaker's voice is preserved. These results are\nquite promising as they indicate the capability of generic TTSes to handle\nspeaker and language switching seamlessly, along with the ease of adaptation to\na new language.", "published": "2020-06-12 07:17:41", "link": "http://arxiv.org/abs/2006.06971v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Monolingual Data Selection Analysis for English-Mandarin Hybrid\n  Code-switching Speech Recognition", "abstract": "In this paper, we conduct data selection analysis in building an\nEnglish-Mandarin code-switching (CS) speech recognition (CSSR) system, which is\naimed for a real CSSR contest in China. The overall training sets have three\nsubsets, i.e., a code-switching data set, an English (LibriSpeech) and a\nMandarin data set respectively. The code-switching data are Mandarin dominated.\nFirst of all, it is found using the overall data yields worse results, and\nhence data selection study is necessary. Then to exploit monolingual data, we\nfind data matching is crucial. Mandarin data is closely matched with the\nMandarin part in the code-switching data, while English data is not. However,\nMandarin data only helps on those utterances that are significantly\nMandarin-dominated. Besides, there is a balance point, over which more\nmonolingual data will divert the CSSR system, degrading results. Finally, we\nanalyze the effectiveness of combining monolingual data to train a CSSR system\nwith the HMM-DNN hybrid framework. The CSSR system can perform within-utterance\ncode-switch recognition, but it still has a margin with the one trained on\ncode-switching data.", "published": "2020-06-12 11:37:25", "link": "http://arxiv.org/abs/2006.07094v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Neural voice cloning with a few low-quality samples", "abstract": "In this paper, we explore the possibility of speech synthesis from low\nquality found data using only limited number of samples of target speaker. We\ntry to extract only the speaker embedding from found data of target speaker\nunlike previous works which tries to train the entire text-to-speech system on\nfound data. Also, the two speaker mimicking approaches which are adaptation and\nspeaker-encoder-based are applied on newly released LibriTTS dataset and\npreviously released VCTK corpus to examine the impact of speaker variety on\nclarity and target-speaker-similarity .", "published": "2020-06-12 04:42:07", "link": "http://arxiv.org/abs/2006.06940v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
