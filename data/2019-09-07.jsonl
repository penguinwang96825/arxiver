{"title": "On Extractive and Abstractive Neural Document Summarization with\n  Transformer Language Models", "abstract": "We present a method to produce abstractive summaries of long documents that\nexceed several thousand words via neural abstractive summarization. We perform\na simple extractive step before generating a summary, which is then used to\ncondition the transformer language model on relevant information before being\ntasked with generating a summary. We show that this extractive step\nsignificantly improves summarization results. We also show that this approach\nproduces more abstractive summaries compared to prior work that employs a copy\nmechanism while still achieving higher rouge scores. Note: The abstract above\nwas not written by the authors, it was generated by one of the models presented\nin this paper.", "published": "2019-09-07 04:33:26", "link": "http://arxiv.org/abs/1909.03186v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deleter: Leveraging BERT to Perform Unsupervised Successive Text\n  Compression", "abstract": "Text compression has diverse applications such as Summarization, Reading\nComprehension and Text Editing. However, almost all existing approaches require\neither hand-crafted features, syntactic labels or parallel data. Even for one\nthat achieves this task in an unsupervised setting, its architecture\nnecessitates a task-specific autoencoder. Moreover, these models only generate\none compressed sentence for each source input, so that adapting to different\nstyle requirements (e.g. length) for the final output usually implies\nretraining the model from scratch. In this work, we propose a fully\nunsupervised model, Deleter, that is able to discover an \"optimal deletion\npath\" for an arbitrary sentence, where each intermediate sequence along the\npath is a coherent subsequence of the previous one. This approach relies\nexclusively on a pretrained bidirectional language model (BERT) to score each\ncandidate deletion based on the average Perplexity of the resulting sentence\nand performs progressive greedy lookahead search to select the best deletion\nfor each step. We apply Deleter to the task of extractive Sentence Compression,\nand found that our model is competitive with state-of-the-art supervised models\ntrained on 1.02 million in-domain examples with similar compression ratio.\nQualitative analysis, as well as automatic and human evaluations both verify\nthat our model produces high-quality compression.", "published": "2019-09-07 09:14:18", "link": "http://arxiv.org/abs/1909.03223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Cascade Binary Tagging Framework for Relational Triple\n  Extraction", "abstract": "Extracting relational triples from unstructured text is crucial for\nlarge-scale knowledge graph construction. However, few existing works excel in\nsolving the overlapping triple problem where multiple relational triples in the\nsame sentence share the same entities. In this work, we introduce a fresh\nperspective to revisit the relational triple extraction task and propose a\nnovel cascade binary tagging framework (CasRel) derived from a principled\nproblem formulation. Instead of treating relations as discrete labels as in\nprevious works, our new framework models relations as functions that map\nsubjects to objects in a sentence, which naturally handles the overlapping\nproblem. Experiments show that the CasRel framework already outperforms\nstate-of-the-art methods even when its encoder module uses a randomly\ninitialized BERT encoder, showing the power of the new tagging framework. It\nenjoys further performance boost when employing a pre-trained BERT encoder,\noutperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score\non two public datasets NYT and WebNLG, respectively. In-depth analysis on\ndifferent scenarios of overlapping triples shows that the method delivers\nconsistent performance gain across all these scenarios. The source code and\ndata are released online.", "published": "2019-09-07 09:40:20", "link": "http://arxiv.org/abs/1909.03227v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Role Labeling with Iterative Structure Refinement", "abstract": "Modern state-of-the-art Semantic Role Labeling (SRL) methods rely on\nexpressive sentence encoders (e.g., multi-layer LSTMs) but tend to model only\nlocal (if any) interactions between individual argument labeling decisions.\nThis contrasts with earlier work and also with the intuition that the labels of\nindividual arguments are strongly interdependent. We model interactions between\nargument labeling decisions through {\\it iterative refinement}. Starting with\nan output produced by a factorized model, we iteratively refine it using a\nrefinement network. Instead of modeling arbitrary interactions among roles and\nwords, we encode prior knowledge about the SRL problem by designing a\nrestricted network architecture capturing non-local interactions. This modeling\nchoice prevents overfitting and results in an effective model, outperforming\nstrong factorized baseline models on all 7 CoNLL-2009 languages, and achieving\nstate-of-the-art results on 5 of them, including English.", "published": "2019-09-07 15:23:37", "link": "http://arxiv.org/abs/1909.03285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Byte-Level Subwords", "abstract": "Almost all existing machine translation models are built on top of\ncharacter-based vocabularies: characters, subwords or words. Rare characters\nfrom noisy text or character-rich languages such as Japanese and Chinese\nhowever can unnecessarily take up vocabulary slots and limit its compactness.\nRepresenting text at the level of bytes and using the 256 byte set as\nvocabulary is a potential solution to this issue. High computational cost has\nhowever prevented it from being widely deployed or used in practice. In this\npaper, we investigate byte-level subwords, specifically byte-level BPE (BBPE),\nwhich is compacter than character vocabulary and has no out-of-vocabulary\ntokens, but is more efficient than using pure bytes only is. We claim that\ncontextualizing BBPE embeddings is necessary, which can be implemented by a\nconvolutional or recurrent layer. Our experiments show that BBPE has comparable\nperformance to BPE while its size is only 1/8 of that for BPE. In the\nmultilingual setting, BBPE maximizes vocabulary sharing across many languages\nand achieves better translation quality. Moreover, we show that BBPE enables\ntransferring models between languages with non-overlapping character sets.", "published": "2019-09-07 21:29:46", "link": "http://arxiv.org/abs/1909.03341v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Sports Commentator Bias within a Large Corpus of American\n  Football Broadcasts", "abstract": "Sports broadcasters inject drama into play-by-play commentary by building\nteam and player narratives through subjective analyses and anecdotes. Prior\nstudies based on small datasets and manual coding show that such theatrics\nevince commentator bias in sports broadcasts. To examine this phenomenon, we\nassemble FOOTBALL, which contains 1,455 broadcast transcripts from American\nfootball games across six decades that are automatically annotated with 250K\nplayer mentions and linked with racial metadata. We identify major confounding\nfactors for researchers examining racial bias in FOOTBALL, and perform a\ncomputational analysis that supports conclusions from prior social science\nstudies.", "published": "2019-09-07 21:40:04", "link": "http://arxiv.org/abs/1909.03343v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KG-BERT: BERT for Knowledge Graph Completion", "abstract": "Knowledge graphs are important resources for many artificial intelligence\ntasks but often suffer from incompleteness. In this work, we propose to use\npre-trained language models for knowledge graph completion. We treat triples in\nknowledge graphs as textual sequences and propose a novel framework named\nKnowledge Graph Bidirectional Encoder Representations from Transformer\n(KG-BERT) to model these triples. Our method takes entity and relation\ndescriptions of a triple as input and computes scoring function of the triple\nwith the KG-BERT language model. Experimental results on multiple benchmark\nknowledge graphs show that our method can achieve state-of-the-art performance\nin triple classification, link prediction and relation prediction tasks.", "published": "2019-09-07 06:09:25", "link": "http://arxiv.org/abs/1909.03193v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LAMOL: LAnguage MOdeling for Lifelong Language Learning", "abstract": "Most research on lifelong learning applies to images or games, but not\nlanguage. We present LAMOL, a simple yet effective method for lifelong language\nlearning (LLL) based on language modeling. LAMOL replays pseudo-samples of\nprevious tasks while requiring no extra memory or model capacity. Specifically,\nLAMOL is a language model that simultaneously learns to solve the tasks and\ngenerate training samples. When the model is trained for a new task, it\ngenerates pseudo-samples of previous tasks for training alongside data for the\nnew task. The results show that LAMOL prevents catastrophic forgetting without\nany sign of intransigence and can perform five very different language tasks\nsequentially with only one model. Overall, LAMOL outperforms previous methods\nby a considerable margin and is only 2-3% worse than multitasking, which is\nusually considered the LLL upper bound. The source code is available at\nhttps://github.com/jojotenya/LAMOL.", "published": "2019-09-07 20:17:34", "link": "http://arxiv.org/abs/1909.03329v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact\n  Checking of Claims", "abstract": "We contribute the largest publicly available dataset of naturally occurring\nfactual claims for the purpose of automatic claim verification. It is collected\nfrom 26 fact checking websites in English, paired with textual sources and rich\nmetadata, and labelled for veracity by human expert journalists. We present an\nin-depth analysis of the dataset, highlighting characteristics and challenges.\nFurther, we present results for automatic veracity prediction, both with\nestablished baselines and with a novel method for joint ranking of evidence\npages and predicting veracity that outperforms all baselines. Significant\nperformance increases are achieved by encoding evidence, and by modelling\nmetadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that\nthis is a challenging testbed for claim veracity prediction.", "published": "2019-09-07 10:57:29", "link": "http://arxiv.org/abs/1909.03242v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Relationships from Entity Stream", "abstract": "Relational reasoning is a central component of intelligent behavior, but has\nproven difficult for neural networks to learn. The Relation Network (RN) module\nwas recently proposed by DeepMind to solve such problems, and demonstrated\nstate-of-the-art results on a number of datasets. However, the RN module scales\nquadratically in the size of the input, since it calculates relationship\nfactors between every patch in the visual field, including those that do not\ncorrespond to entities. In this paper, we describe an architecture that enables\nrelationships to be determined from a stream of entities obtained by an\nattention mechanism over the input field. The model is trained end-to-end, and\ndemonstrates equivalent performance with greater interpretability while\nrequiring only a fraction of the model parameters of the original RN module.", "published": "2019-09-07 18:24:57", "link": "http://arxiv.org/abs/1909.03315v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Dependency Parsing for Spoken Dialog Systems", "abstract": "Dependency parsing of conversational input can play an important role in\nlanguage understanding for dialog systems by identifying the relationships\nbetween entities extracted from user utterances. Additionally, effective\ndependency parsing can elucidate differences in language structure and usage\nfor discourse analysis of human-human versus human-machine dialogs. However,\nmodels trained on datasets based on news articles and web data do not perform\nwell on spoken human-machine dialog, and currently available annotation schemes\ndo not adapt well to dialog data. Therefore, we propose the Spoken Conversation\nUniversal Dependencies (SCUD) annotation scheme that extends the Universal\nDependencies (UD) (Nivre et al., 2016) guidelines to spoken human-machine\ndialogs. We also provide ConvBank, a conversation dataset between humans and an\nopen-domain conversational dialog system with SCUD annotation. Finally, to\ndemonstrate the utility of the dataset, we train a dependency parser on the\nConvBank dataset. We demonstrate that by pre-training a dependency parser on a\nset of larger public datasets and fine-tuning on ConvBank data, we achieved the\nbest result, 85.05% unlabeled and 77.82% labeled attachment accuracy.", "published": "2019-09-07 18:32:28", "link": "http://arxiv.org/abs/1909.03317v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Neural State Pushdown Automata", "abstract": "In order to learn complex grammars, recurrent neural networks (RNNs) require\nsufficient computational resources to ensure correct grammar recognition. A\nwidely-used approach to expand model capacity would be to couple an RNN to an\nexternal memory stack. Here, we introduce a \"neural state\" pushdown automaton\n(NSPDA), which consists of a digital stack, instead of an analog one, that is\ncoupled to a neural network state machine. We empirically show its\neffectiveness in recognizing various context-free grammars (CFGs). First, we\ndevelop the underlying mechanics of the proposed higher order recurrent network\nand its manipulation of a stack as well as how to stably program its underlying\npushdown automaton (PDA) to achieve desired finite-state network dynamics.\nNext, we introduce a noise regularization scheme for higher-order (tensor)\nnetworks, to our knowledge the first of its kind, and design an algorithm for\nimproved incremental learning. Finally, we design a method for inserting\ngrammar rules into a NSPDA and empirically show that this prior knowledge\nimproves its training convergence time by an order of magnitude and, in some\ncases, leads to better generalization. The NSPDA is also compared to a\nclassical analog stack neural network pushdown automaton (NNPDA) as well as a\nwide array of first and second-order RNNs with and without external memory,\ntrained using different learning algorithms. Our results show that, for Dyck(2)\nlanguages, prior rule-based knowledge is critical for optimization convergence\nand for ensuring generalization to longer sequences at test time. We observe\nthat many RNNs with and without memory, but no prior knowledge, fail to\nconverge and generalize poorly on CFGs.", "published": "2019-09-07 00:32:11", "link": "http://arxiv.org/abs/1909.05233v2", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Overton: A Data System for Monitoring and Improving Machine-Learned\n  Products", "abstract": "We describe a system called Overton, whose main design goal is to support\nengineers in building, monitoring, and improving production machine learning\nsystems. Key challenges engineers face are monitoring fine-grained quality,\ndiagnosing errors in sophisticated applications, and handling contradictory or\nincomplete supervision data. Overton automates the life cycle of model\nconstruction, deployment, and monitoring by providing a set of novel\nhigh-level, declarative abstractions. Overton's vision is to shift developers\nto these higher-level tasks instead of lower-level machine learning tasks. In\nfact, using Overton, engineers can build deep-learning-based applications\nwithout writing any code in frameworks like TensorFlow. For over a year,\nOverton has been used in production to support multiple applications in both\nnear-real-time applications and back-of-house processing. In that time,\nOverton-based applications have answered billions of queries in multiple\nlanguages and processed trillions of records reducing errors 1.7-2.9 times\nversus production systems.", "published": "2019-09-07 03:51:13", "link": "http://arxiv.org/abs/1909.05372v1", "categories": ["cs.LG", "cs.CL", "cs.DB"], "primary_category": "cs.LG"}
