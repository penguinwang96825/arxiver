{"title": "Code Synonyms Do Matter: Multiple Synonyms Matching Network for\n  Automatic ICD Coding", "abstract": "Automatic ICD coding is defined as assigning disease codes to electronic\nmedical records (EMRs). Existing methods usually apply label attention with\ncode representations to match related text snippets. Unlike these works that\nmodel the label with the code hierarchy or description, we argue that the code\nsynonyms can provide more comprehensive knowledge based on the observation that\nthe code expressions in EMRs vary from their descriptions in ICD. By aligning\ncodes to concepts in UMLS, we collect synonyms of every code. Then, we propose\na multiple synonyms matching network to leverage synonyms for better code\nrepresentation learning, and finally help the code classification. Experiments\non the MIMIC-III dataset show that our proposed method outperforms previous\nstate-of-the-art methods.", "published": "2022-03-03 04:57:08", "link": "http://arxiv.org/abs/2203.01515v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UDAAN: Machine Learning based Post-Editing tool for Document Translation", "abstract": "We introduce UDAAN, an open-source post-editing tool that can reduce manual\nediting efforts to quickly produce publishable-standard documents in several\nIndic languages. UDAAN has an end-to-end Machine Translation (MT) plus\npost-editing pipeline wherein users can upload a document to obtain raw MT\noutput. Further, users can edit the raw translations using our tool. UDAAN\noffers several advantages: a) Domain-aware, vocabulary-based lexical\nconstrained MT. b) source-target and target-target lexicon suggestions for\nusers. Replacements are based on the source and target texts lexicon alignment.\nc) Translation suggestions are based on logs created during user interaction.\nd) Source-target sentence alignment visualisation that reduces the cognitive\nload of users during editing. e) Translated outputs from our tool are available\nin multiple formats: docs, latex, and PDF. We also provide the facility to use\naround 100 in-domain dictionaries for lexicon-aware machine translation.\nAlthough we limit our experiments to English-to-Hindi translation, our tool is\nindependent of the source and target languages. Experimental results based on\nthe usage of the tools and users feedback show that our tool speeds up the\ntranslation time by approximately a factor of three compared to the baseline\nmethod of translating documents from scratch. Our tool is available for both\nWindows and Linux platforms. The tool is open-source under MIT license, and the\nsource code can be accessed from our website at https://www.udaanproject.org.\nDemonstration and tutorial videos for various features of our tool can be\naccessed at https://www.youtube.com/channel/UClfK7iC8J7b22bj3GwAUaCw. Our MT\npipeline can be accessed at https://udaaniitb.aicte-india.org/udaan/translate/.", "published": "2022-03-03 11:08:16", "link": "http://arxiv.org/abs/2203.01644v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Hash-Based Early Exiting Approach For Language Understanding\n  and Generation", "abstract": "Early exiting allows instances to exit at different layers according to the\nestimation of difficulty. Previous works usually adopt heuristic metrics such\nas the entropy of internal outputs to measure instance difficulty, which\nsuffers from generalization and threshold-tuning. In contrast, learning to\nexit, or learning to predict instance difficulty is a more appealing way.\nThough some effort has been devoted to employing such \"learn-to-exit\" modules,\nit is still unknown whether and how well the instance difficulty can be\nlearned. As a response, we first conduct experiments on the learnability of\ninstance difficulty, which demonstrates that modern neural models perform\npoorly on predicting instance difficulty. Based on this observation, we propose\na simple-yet-effective Hash-based Early Exiting approach (HashEE) that replaces\nthe learn-to-exit modules with hash functions to assign each token to a fixed\nexiting layer. Different from previous methods, HashEE requires no internal\nclassifiers nor extra parameters, and therefore is more efficient. Experimental\nresults on classification, regression, and generation tasks demonstrate that\nHashEE can achieve higher performance with fewer FLOPs and inference time\ncompared with previous state-of-the-art early exiting methods.", "published": "2022-03-03 12:02:05", "link": "http://arxiv.org/abs/2203.01670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Enhanced Short Text Matching using Clickthrough Data", "abstract": "The short text matching task employs a model to determine whether two short\ntexts have the same semantic meaning or intent. Existing short text matching\nmodels usually rely on the content of short texts which are lack information or\nmissing some key clues. Therefore, the short texts need external knowledge to\ncomplete their semantic meaning. To address this issue, we propose a new short\ntext matching framework for introducing external knowledge to enhance the short\ntext contextual representation. In detail, we apply a self-attention mechanism\nto enrich short text representation with external contexts. Experiments on two\nChinese datasets and one English dataset demonstrate that our framework\noutperforms the state-of-the-art short text matching models.", "published": "2022-03-03 16:53:21", "link": "http://arxiv.org/abs/2203.01849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "As Little as Possible, as Much as Necessary: Detecting Over- and\n  Undertranslations with Contrastive Conditioning", "abstract": "Omission and addition of content is a typical issue in neural machine\ntranslation. We propose a method for detecting such phenomena with\noff-the-shelf translation models. Using contrastive conditioning, we compare\nthe likelihood of a full sequence under a translation model to the likelihood\nof its parts, given the corresponding source or target sequence. This allows to\npinpoint superfluous words in the translation and untranslated words in the\nsource even in the absence of a reference translation. The accuracy of our\nmethod is comparable to a supervised method that requires a custom quality\nestimation model.", "published": "2022-03-03 18:59:02", "link": "http://arxiv.org/abs/2203.01927v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overlap-based Vocabulary Generation Improves Cross-lingual Transfer\n  Among Related Languages", "abstract": "Pre-trained multilingual language models such as mBERT and XLM-R have\ndemonstrated great potential for zero-shot cross-lingual transfer to low\nweb-resource languages (LRL). However, due to limited model capacity, the large\ndifference in the sizes of available monolingual corpora between high\nweb-resource languages (HRL) and LRLs does not provide enough scope of\nco-embedding the LRL with the HRL, thereby affecting downstream task\nperformance of LRLs. In this paper, we argue that relatedness among languages\nin a language family along the dimension of lexical overlap may be leveraged to\novercome some of the corpora limitations of LRLs. We propose Overlap BPE\n(OBPE), a simple yet effective modification to the BPE vocabulary generation\nalgorithm which enhances overlap across related languages. Through extensive\nexperiments on multiple NLP tasks and datasets, we observe that OBPE generates\na vocabulary that increases the representation of LRLs via tokens shared with\nHRLs. This results in improved zero-shot transfer from related HRLs to LRLs\nwithout reducing HRL representation and accuracy. Unlike previous studies that\ndismissed the importance of token-overlap, we show that in the low-resource\nrelated language setting, token overlap matters. Synthetically reducing the\noverlap to zero can cause as much as a four-fold drop in zero-shot transfer\naccuracy.", "published": "2022-03-03 19:35:24", "link": "http://arxiv.org/abs/2203.01976v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Latent-Variable Models for Text Generation", "abstract": "Text generation aims to produce human-like natural language output for\ndown-stream tasks. It covers a wide range of applications like machine\ntranslation, document summarization, dialogue generation and so on. Recently\ndeep neural network-based end-to-end architectures have been widely adopted.\nThe end-to-end approach conflates all sub-modules, which used to be designed by\ncomplex handcrafted rules, into a holistic encode-decode architecture. Given\nenough training data, it is able to achieve state-of-the-art performance yet\navoiding the need of language/domain-dependent knowledge. Nonetheless, deep\nlearning models are known to be extremely data-hungry, and text generated from\nthem usually suffer from low diversity, interpretability and controllability.\nAs a result, it is difficult to trust the output from them in real-life\napplications. Deep latent-variable models, by specifying the probabilistic\ndistribution over an intermediate latent process, provide a potential way of\naddressing these problems while maintaining the expressive power of deep neural\nnetworks. This dissertation presents how deep latent-variable models can\nimprove over the standard encoder-decoder model for text generation.", "published": "2022-03-03 23:06:39", "link": "http://arxiv.org/abs/2203.02055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Summaries as Dialogue States (DS2), Template-Guided\n  Summarization for Few-shot Dialogue State Tracking", "abstract": "Annotating task-oriented dialogues is notorious for the expensive and\ndifficult data collection process. Few-shot dialogue state tracking (DST) is a\nrealistic solution to this problem. In this paper, we hypothesize that dialogue\nsummaries are essentially unstructured dialogue states; hence, we propose to\nreformulate dialogue state tracking as a dialogue summarization problem. To\nelaborate, we train a text-to-text language model with synthetic template-based\ndialogue summaries, generated by a set of rules from the dialogue states. Then,\nthe dialogue states can be recovered by inversely applying the summary\ngeneration rules. We empirically show that our method DS2 outperforms previous\nworks on few-shot DST in MultiWoZ 2.0 and 2.1, in both cross-domain and\nmulti-domain settings. Our method also exhibits vast speedup during both\ntraining and inference as it can generate all states at once. Finally, based on\nour analysis, we discover that the naturalness of the summary templates plays a\nkey role for successful training.", "published": "2022-03-03 07:54:09", "link": "http://arxiv.org/abs/2203.01552v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Deep Neural Framework for Image Caption Generation Using GRU-Based\n  Attention Mechanism", "abstract": "Image captioning is a fast-growing research field of computer vision and\nnatural language processing that involves creating text explanations for\nimages. This study aims to develop a system that uses a pre-trained\nconvolutional neural network (CNN) to extract features from an image,\nintegrates the features with an attention mechanism, and creates captions using\na recurrent neural network (RNN). To encode an image into a feature vector as\ngraphical attributes, we employed multiple pre-trained convolutional neural\nnetworks. Following that, a language model known as GRU is chosen as the\ndecoder to construct the descriptive sentence. In order to increase\nperformance, we merge the Bahdanau attention model with GRU to allow learning\nto be focused on a specific portion of the image. On the MSCOCO dataset, the\nexperimental results achieve competitive performance against state-of-the-art\napproaches.", "published": "2022-03-03 09:47:59", "link": "http://arxiv.org/abs/2203.01594v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PeerSum: A Peer Review Dataset for Abstractive Multi-document\n  Summarization", "abstract": "We present PeerSum, a new MDS dataset using peer reviews of scientific\npublications. Our dataset differs from the existing MDS datasets in that our\nsummaries (i.e., the meta-reviews) are highly abstractive and they are real\nsummaries of the source documents (i.e., the reviews) and it also features\ndisagreements among source documents. We found that current state-of-the-art\nMDS models struggle to generate high-quality summaries for PeerSum, offering\nnew research opportunities.", "published": "2022-03-03 15:27:02", "link": "http://arxiv.org/abs/2203.01769v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Distantly Supervised Named Entity Recognition via Confidence-Based\n  Multi-Class Positive and Unlabeled Learning", "abstract": "In this paper, we study the named entity recognition (NER) problem under\ndistant supervision. Due to the incompleteness of the external dictionaries\nand/or knowledge bases, such distantly annotated training data usually suffer\nfrom a high false negative rate. To this end, we formulate the Distantly\nSupervised NER (DS-NER) problem via Multi-class Positive and Unlabeled (MPU)\nlearning and propose a theoretically and practically novel CONFidence-based MPU\n(Conf-MPU) approach. To handle the incomplete annotations, Conf-MPU consists of\ntwo steps. First, a confidence score is estimated for each token of being an\nentity token. Then, the proposed Conf-MPU risk estimation is applied to train a\nmulti-class classifier for the NER task. Thorough experiments on two benchmark\ndatasets labeled by various external knowledge demonstrate the superiority of\nthe proposed Conf-MPU over existing DS-NER methods.", "published": "2022-03-03 17:55:35", "link": "http://arxiv.org/abs/2204.09589v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QaNER: Prompting Question Answering Models for Few-shot Named Entity\n  Recognition", "abstract": "Recently, prompt-based learning for pre-trained language models has succeeded\nin few-shot Named Entity Recognition (NER) by exploiting prompts as task\nguidance to increase label efficiency. However, previous prompt-based methods\nfor few-shot NER have limitations such as a higher computational complexity,\npoor zero-shot ability, requiring manual prompt engineering, or lack of prompt\nrobustness. In this work, we address these shortcomings by proposing a new\nprompt-based learning NER method with Question Answering (QA), called QaNER.\nOur approach includes 1) a refined strategy for converting NER problems into\nthe QA formulation; 2) NER prompt generation for QA models; 3) prompt-based\ntuning with QA models on a few annotated NER examples; 4) zero-shot NER by\nprompting the QA model. Comparing the proposed approach with previous methods,\nQaNER is faster at inference, insensitive to the prompt quality, and robust to\nhyper-parameters, as well as demonstrating significantly better low-resource\nperformance and zero-shot capability.", "published": "2022-03-03 06:56:01", "link": "http://arxiv.org/abs/2203.01543v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detection of Word Adversarial Examples in Text Classification: Benchmark\n  and Baseline via Robust Density Estimation", "abstract": "Word-level adversarial attacks have shown success in NLP models, drastically\ndecreasing the performance of transformer-based models in recent years. As a\ncountermeasure, adversarial defense has been explored, but relatively few\nefforts have been made to detect adversarial examples. However, detecting\nadversarial examples may be crucial for automated tasks (e.g. review sentiment\nanalysis) that wish to amass information about a certain population and\nadditionally be a step towards a robust defense system. To this end, we release\na dataset for four popular attack methods on four datasets and four models to\nencourage further research in this field. Along with it, we propose a\ncompetitive baseline based on density estimation that has the highest AUC on 29\nout of 30 dataset-attack-model combinations. Source code is available in\nhttps://github.com/anoymous92874838/text-adv-detection.", "published": "2022-03-03 12:32:59", "link": "http://arxiv.org/abs/2203.01677v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vision-Language Intelligence: Tasks, Representation Learning, and Large\n  Models", "abstract": "This paper presents a comprehensive survey of vision-language (VL)\nintelligence from the perspective of time. This survey is inspired by the\nremarkable progress in both computer vision and natural language processing,\nand recent trends shifting from single modality processing to multiple modality\ncomprehension. We summarize the development in this field into three time\nperiods, namely task-specific methods, vision-language pre-training (VLP)\nmethods, and larger models empowered by large-scale weakly-labeled data. We\nfirst take some common VL tasks as examples to introduce the development of\ntask-specific methods. Then we focus on VLP methods and comprehensively review\nkey components of the model structures and training methods. After that, we\nshow how recent work utilizes large-scale raw image-text data to learn\nlanguage-aligned visual representations that generalize better on zero or few\nshot learning tasks. Finally, we discuss some potential future trends towards\nmodality cooperation, unified representation, and knowledge incorporation. We\nbelieve that this review will be of help for researchers and practitioners of\nAI and ML, especially those interested in computer vision and natural language\nprocessing.", "published": "2022-03-03 18:54:59", "link": "http://arxiv.org/abs/2203.01922v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DIME: Fine-grained Interpretations of Multimodal Models via Disentangled\n  Local Explanations", "abstract": "The ability for a human to understand an Artificial Intelligence (AI) model's\ndecision-making process is critical in enabling stakeholders to visualize model\nbehavior, perform model debugging, promote trust in AI models, and assist in\ncollaborative human-AI decision-making. As a result, the research fields of\ninterpretable and explainable AI have gained traction within AI communities as\nwell as interdisciplinary scientists seeking to apply AI in their subject\nareas. In this paper, we focus on advancing the state-of-the-art in\ninterpreting multimodal models - a class of machine learning methods that\ntackle core challenges in representing and capturing interactions between\nheterogeneous data sources such as images, text, audio, and time-series data.\nMultimodal models have proliferated numerous real-world applications across\nhealthcare, robotics, multimedia, affective computing, and human-computer\ninteraction. By performing model disentanglement into unimodal contributions\n(UC) and multimodal interactions (MI), our proposed approach, DIME, enables\naccurate and fine-grained analysis of multimodal models while maintaining\ngenerality across arbitrary modalities, model architectures, and tasks. Through\na comprehensive suite of experiments on both synthetic and real-world\nmultimodal tasks, we show that DIME generates accurate disentangled\nexplanations, helps users of multimodal models gain a deeper understanding of\nmodel behavior, and presents a step towards debugging and improving these\nmodels for real-world deployment. Code for our experiments can be found at\nhttps://github.com/lvyiwei1/DIME.", "published": "2022-03-03 20:52:47", "link": "http://arxiv.org/abs/2203.02013v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Automatic Text Summarization Methods: A Comprehensive Review", "abstract": "One of the most pressing issues that have arisen due to the rapid growth of\nthe Internet is known as information overloading. Simplifying the relevant\ninformation in the form of a summary will assist many people because the\nmaterial on any topic is plentiful on the Internet. Manually summarising\nmassive amounts of text is quite challenging for humans. So, it has increased\nthe need for more complex and powerful summarizers. Researchers have been\ntrying to improve approaches for creating summaries since the 1950s, such that\nthe machine-generated summary matches the human-created summary. This study\nprovides a detailed state-of-the-art analysis of text summarization concepts\nsuch as summarization approaches, techniques used, standard datasets,\nevaluation metrics and future scopes for research. The most commonly accepted\napproaches are extractive and abstractive, studied in detail in this work.\nEvaluating the summary and increasing the development of reusable resources and\ninfrastructure aids in comparing and replicating findings, adding competition\nto improve the outcomes. Different evaluation methods of generated summaries\nare also discussed in this study. Finally, at the end of this study, several\nchallenges and research opportunities related to text summarization research\nare mentioned that may be useful for potential researchers working in this\narea.", "published": "2022-03-03 10:45:00", "link": "http://arxiv.org/abs/2204.01849v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive\n  Representation Learning", "abstract": "We present modality gap, an intriguing geometric phenomenon of the\nrepresentation space of multi-modal models. Specifically, we show that\ndifferent data modalities (e.g. images and text) are embedded at arm's length\nin their shared representation in multi-modal models such as CLIP. Our\nsystematic analysis demonstrates that this gap is caused by a combination of\nmodel initialization and contrastive learning optimization. In model\ninitialization, we show empirically and theoretically that the representation\nof a common deep neural network is restricted to a narrow cone. As a\nconsequence, in a multi-modal model with two encoders, the representations of\nthe two modalities are clearly apart when the model is initialized. During\noptimization, contrastive learning keeps the different modalities separate by a\ncertain distance, which is influenced by the temperature parameter in the loss\nfunction. Our experiments further demonstrate that varying the modality gap\ndistance has a significant impact in improving the model's downstream zero-shot\nclassification performance and fairness. Our code and data are available at\nhttps://modalitygap.readthedocs.io/", "published": "2022-03-03 22:53:54", "link": "http://arxiv.org/abs/2203.02053v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "The Vicomtech Audio Deepfake Detection System based on Wav2Vec2 for the\n  2022 ADD Challenge", "abstract": "This paper describes our submitted systems to the 2022 ADD challenge withing\nthe tracks 1 and 2. Our approach is based on the combination of a pre-trained\nwav2vec2 feature extractor and a downstream classifier to detect spoofed audio.\nThis method exploits the contextualized speech representations at the different\ntransformer layers to fully capture discriminative information. Furthermore,\nthe classification model is adapted to the application scenario using different\ndata augmentation techniques. We evaluate our system for audio synthesis\ndetection in both the ASVspoof 2021 and the 2022 ADD challenges, showing its\nrobustness and good performance in realistic challenging environments such as\ntelephonic and audio codec systems, noisy audio, and partial deepfakes.", "published": "2022-03-03 08:49:17", "link": "http://arxiv.org/abs/2203.01573v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Learning-Based Joint Control of Acoustic Echo Cancellation,\n  Beamforming and Postfiltering", "abstract": "We introduce a novel method for controlling the functionality of a hands-free\nspeech communication device which comprises a model-based acoustic echo\ncanceller (AEC), minimum variance distortionless response (MVDR) beamformer\n(BF) and spectral postfilter (PF). While the AEC removes the early echo\ncomponent, the MVDR BF and PF suppress the residual echo and background noise.\nAs key innovation, we suggest to use a single deep neural network (DNN) to\njointly control the adaptation of the various algorithmic components. This\nallows for rapid convergence and high steady-state performance in the presence\nof high-level interfering double-talk. End-to-end training of the DNN using a\ntime-domain speech extraction loss function avoids the design of individual\ncontrol strategies.", "published": "2022-03-03 16:05:21", "link": "http://arxiv.org/abs/2203.01793v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DareFightingICE Competition: A Fighting Game Sound Design and AI\n  Competition", "abstract": "This paper presents a new competition -- at the 2022 IEEE Conference on Games\n(CoG) -- called DareFightingICE Competition. The competition has two tracks: a\nsound design track and an AI track. The game platform for this competition is\nalso called DareFightingICE, a fighting game platform. DareFightingICE is a\nsound-design-enhanced version of FightingICE, used earlier in a competition at\nCoG until 2021 to promote artificial intelligence (AI) research in fighting\ngames. In the sound design track, participants compete for the best sound\ndesign, given the default sound design of DareFightingICE as a sample, where we\ndefine a sound design as a set of sound effects combined with the source code\nthat implements their timing-control algorithm. Participants of the AI track\nare asked to develop their AI algorithm that controls a character given only\nsound as the input (blind AI) to fight against their opponent; a sample\ndeep-learning blind AI will be provided by us. Our means to maximize the\nsynergy between the two tracks are also described. This competition serves to\ncome up with effective sound designs for visually impaired players, a group in\nthe gaming community which has been mostly ignored. To the best of our\nknowledge, DareFightingICE Competition is the first of its kind within and\noutside of CoG.", "published": "2022-03-03 08:12:15", "link": "http://arxiv.org/abs/2203.01556v2", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "I.2; H.5.2; H.5.5"], "primary_category": "cs.HC"}
{"title": "Generative Modeling for Low Dimensional Speech Attributes with Neural\n  Spline Flows", "abstract": "Despite recent advances in generative modeling for text-to-speech synthesis,\nthese models do not yet have the same fine-grained adjustability of\npitch-conditioned deterministic models such as FastPitch and FastSpeech2. Pitch\ninformation is not only low-dimensional, but also discontinuous, making it\nparticularly difficult to model in a generative setting. Our work explores\nseveral techniques for handling the aforementioned issues in the context of\nNormalizing Flow models. We also find this problem to be very well suited for\nNeural Spline flows, which is a highly expressive alternative to the more\ncommon affine-coupling mechanism in Normalizing Flows.", "published": "2022-03-03 15:58:08", "link": "http://arxiv.org/abs/2203.01786v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Nonlinear predictive models computation in ADPCM schemes", "abstract": "Recently several papers have been published on nonlinear prediction applied\nto speech coding. At ICASSP98 we presented a system based on an ADPCM scheme\nwith a nonlinear predictor based on a neural net. The most critical parameter\nwas the training procedure in order to achieve good generalization capability\nand robustness against mismatch between training and testing conditions. In\nthis paper, we propose several new approaches that improve the performance of\nthe original system in up to 1.2dB of SEGSNR (using bayesian regularization).\nThe variance of the SEGSNR between frames is also minimized, so the new scheme\nproduces a more stable quality of the output.", "published": "2022-03-03 21:03:38", "link": "http://arxiv.org/abs/2203.02020v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention-based Region of Interest (ROI) Detection for Speech Emotion\n  Recognition", "abstract": "Automatic emotion recognition for real-life appli-cations is a challenging\ntask. Human emotion expressions aresubtle, and can be conveyed by a combination\nof several emo-tions. In most existing emotion recognition studies, each\naudioutterance/video clip is labelled/classified in its entirety.\nHowever,utterance/clip-level labelling and classification can be too coarseto\ncapture the subtle intra-utterance/clip temporal dynamics. Forexample, an\nutterance/video clip usually contains only a fewemotion-salient regions and\nmany emotionless regions. In thisstudy, we propose to use attention mechanism\nin deep recurrentneural networks to detection the Regions-of-Interest (ROI)\nthatare more emotionally salient in human emotional speech/video,and further\nestimate the temporal emotion dynamics by aggre-gating those emotionally\nsalient regions-of-interest. We comparethe ROI from audio and video and analyse\nthem. We comparethe performance of the proposed attention networks with\nthestate-of-the-art LSTM models on multi-class classification task\nofrecognizing six basic human emotions, and the proposed attentionmodels\nexhibit significantly better performance. Furthermore, theattention weight\ndistribution can be used to interpret how anutterance can be expressed as a\nmixture of possible emotions.", "published": "2022-03-03 22:01:48", "link": "http://arxiv.org/abs/2203.03428v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Object Classification for Human-Robot Collaboration", "abstract": "Human-robot collaboration requires the contactless estimation of the physical\nproperties of containers manipulated by a person, for example while pouring\ncontent in a cup or moving a food box. Acoustic and visual signals can be used\nto estimate the physical properties of such objects, which may vary\nsubstantially in shape, material and size, and also be occluded by the hands of\nthe person. To facilitate comparisons and stimulate progress in solving this\nproblem, we present the CORSMAL challenge and a dataset to assess the\nperformance of the algorithms through a set of well-defined performance scores.\nThe tasks of the challenge are the estimation of the mass, capacity, and\ndimensions of the object (container), and the classification of the type and\namount of its content. A novel feature of the challenge is our\nreal-to-simulation framework for visualising and assessing the impact of\nestimation errors in human-to-robot handovers.", "published": "2022-03-03 19:37:12", "link": "http://arxiv.org/abs/2203.01977v1", "categories": ["cs.MM", "cs.CV", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
