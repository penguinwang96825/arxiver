{"title": "One Billion Word Benchmark for Measuring Progress in Statistical\n  Language Modeling", "abstract": "We propose a new benchmark corpus to be used for measuring progress in\nstatistical language modeling. With almost one billion words of training data,\nwe hope this benchmark will be useful to quickly evaluate novel language\nmodeling techniques, and to compare their contribution when combined with other\nadvanced techniques. We show performance of several well-known types of\nlanguage models, with the best results achieved with a recurrent neural network\nbased language model. The baseline unpruned Kneser-Ney 5-gram model achieves\nperplexity 67.6; a combination of techniques leads to 35% reduction in\nperplexity, or 10% reduction in cross-entropy (bits), over that baseline.\n  The benchmark is available as a code.google.com project; besides the scripts\nneeded to rebuild the training/held-out data, it also makes available\nlog-probability values for each word in each of ten held-out data sets, for\neach of the baseline n-gram models.", "published": "2013-12-11 00:25:57", "link": "http://arxiv.org/abs/1312.3005v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Types, Lexical Sorts and Classifiers", "abstract": "We propose a cognitively and linguistically motivated set of sorts for\nlexical semantics in a compositional setting: the classifiers in languages that\ndo have such pronouns. These sorts are needed to include lexical considerations\nin a semantical analyser such as Boxer or Grail. Indeed, all proposed lexical\nextensions of usual Montague semantics to model restriction of selection,\nfelicitous and infelicitous copredication require a rich and refined type\nsystem whose base types are the lexical sorts, the basis of the many-sorted\nlogic in which semantical representations of sentences are stated. However,\nnone of those approaches define precisely the actual base types or sorts to be\nused in the lexicon. In this article, we shall discuss some of the options\ncommonly adopted by researchers in formal lexical semantics, and defend the\nview that classifiers in the languages which have such pronouns are an\nappealing solution, both linguistically and cognitively motivated.", "published": "2013-12-11 14:04:52", "link": "http://arxiv.org/abs/1312.3168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards The Development of a Bishnupriya Manipuri Corpus", "abstract": "For any deep computational processing of language we need evidences, and one\nsuch set of evidences is corpus. This paper describes the development of a\ntext-based corpus for the Bishnupriya Manipuri language. A Corpus is considered\nas a building block for any language processing tasks. Due to the lack of\nawareness like other Indian languages, it is also studied less frequently. As a\nresult the language still lacks a good corpus and basic language processing\ntools. As per our knowledge this is the first effort to develop a corpus for\nBishnupriya Manipuri language.", "published": "2013-12-11 17:24:35", "link": "http://arxiv.org/abs/1312.3251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit Sensitive Text Summarization based on Data Conveyed by\n  Connectives", "abstract": "So far and trying to reach human capabilities, research in automatic\nsummarization has been based on hypothesis that are both enabling and limiting.\nSome of these limitations are: how to take into account and reflect (in the\ngenerated summary) the implicit information conveyed in the text, the author\nintention, the reader intention, the context influence, the general world\nknowledge. Thus, if we want machines to mimic human abilities, then they will\nneed access to this same large variety of knowledge. The implicit is affecting\nthe orientation and the argumentation of the text and consequently its summary.\nMost of Text Summarizers (TS) are processing as compressing the initial data\nand they necessarily suffer from information loss. TS are focusing on features\nof the text only, not on what the author intended or why the reader is reading\nthe text. In this paper, we address this problem and we present a system\nfocusing on acquiring knowledge that is implicit. We principally spotlight the\nimplicit information conveyed by the argumentative connectives such as: but,\neven, yet and their effect on the summary.", "published": "2013-12-11 17:50:21", "link": "http://arxiv.org/abs/1312.3258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
