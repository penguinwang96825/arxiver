{"title": "Inference Time Style Control for Summarization", "abstract": "How to generate summaries of different styles without requiring corpora in\nthe target styles, or training separate models? We present two novel methods\nthat can be deployed during summary decoding on any pre-trained\nTransformer-based summarization model. (1) Decoder state adjustment instantly\nmodifies decoder final states with externally trained style scorers, to\niteratively refine the output against a target style. (2) Word unit prediction\nconstrains the word usage to impose strong lexical control during generation.\nIn experiments of summarizing with simplicity control, automatic evaluation and\nhuman judges both find our models producing outputs in simpler languages while\nstill informative. We also generate news headlines with various ideological\nleanings, which can be distinguished by humans with a reasonable probability.", "published": "2021-04-05 00:27:18", "link": "http://arxiv.org/abs/2104.01724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Approach to Overgenerating and Scoring Abstractive Summaries", "abstract": "We propose a new approach to generate multiple variants of the target summary\nwith diverse content and varying lengths, then score and select admissible ones\naccording to users' needs. Abstractive summarizers trained on single reference\nsummaries may struggle to produce outputs that achieve multiple desirable\nproperties, i.e., capturing the most important information, being faithful to\nthe original, grammatical and fluent. In this paper, we propose a two-staged\nstrategy to generate a diverse set of candidate summaries from the source text\nin stage one, then score and select admissible ones in stage two. Importantly,\nour generator gives a precise control over the length of the summary, which is\nespecially well-suited when space is limited. Our selectors are designed to\npredict the optimal summary length and put special emphasis on faithfulness to\nthe original text. Both stages can be effectively trained, optimized and\nevaluated. Our experiments on benchmark summarization datasets suggest that\nthis paradigm can achieve state-of-the-art performance.", "published": "2021-04-05 00:29:45", "link": "http://arxiv.org/abs/2104.01726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paired Examples as Indirect Supervision in Latent Decision Models", "abstract": "Compositional, structured models are appealing because they explicitly\ndecompose problems and provide interpretable intermediate outputs that give\nconfidence that the model is not simply latching onto data artifacts. Learning\nthese models is challenging, however, because end-task supervision only\nprovides a weak indirect signal on what values the latent decisions should\ntake. This often results in the model failing to learn to perform the\nintermediate tasks correctly. In this work, we introduce a way to leverage\npaired examples that provide stronger cues for learning latent decisions. When\ntwo related training examples share internal substructure, we add an additional\ntraining objective to encourage consistency between their latent decisions.\nSuch an objective does not require external supervision for the values of the\nlatent output, or even the end task, yet provides an additional training signal\nto that provided by individual training examples themselves. We apply our\nmethod to improve compositional question answering using neural module networks\non the DROP dataset. We explore three ways to acquire paired questions in DROP:\n(a) discovering naturally occurring paired examples within the dataset, (b)\nconstructing paired examples using templates, and (c) generating paired\nexamples using a question generation model. We empirically demonstrate that our\nproposed approach improves both in- and out-of-distribution generalization and\nleads to correct latent decision predictions.", "published": "2021-04-05 03:58:30", "link": "http://arxiv.org/abs/2104.01759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach", "abstract": "Producing the embedding of a sentence in an unsupervised way is valuable to\nnatural language matching and retrieval problems in practice. In this work, we\nconduct a thorough examination of pretrained model based unsupervised sentence\nembeddings. We study on four pretrained models and conduct massive experiments\non seven datasets regarding sentence semantics. We have there main findings.\nFirst, averaging all tokens is better than only using [CLS] vector. Second,\ncombining both top andbottom layers is better than only using top layers.\nLastly, an easy whitening-based vector normalization strategy with less than 10\nlines of code consistently boosts the performance.", "published": "2021-04-05 04:30:28", "link": "http://arxiv.org/abs/2104.01767v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for\n  Text Classification", "abstract": "Healthcare predictive analytics aids medical decision-making, diagnosis\nprediction and drug review analysis. Therefore, prediction accuracy is an\nimportant criteria which also necessitates robust predictive language models.\nHowever, the models using deep learning have been proven vulnerable towards\ninsignificantly perturbed input instances which are less likely to be\nmisclassified by humans. Recent efforts of generating adversaries using\nrule-based synonyms and BERT-MLMs have been witnessed in general domain, but\nthe ever increasing biomedical literature poses unique challenges. We propose\nBBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box\nattack algorithm for biomedical text classification, leveraging the strengths\nof both domain-specific synonym replacement for biomedical named entities and\nBERTMLM predictions, spelling variation and number replacement. Through\nautomatic and human evaluation on two datasets, we demonstrate that BBAEG\nperforms stronger attack with better language fluency, semantic coherence as\ncompared to prior work.", "published": "2021-04-05 05:32:56", "link": "http://arxiv.org/abs/2104.01782v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Neural Networks for Relation Extraction", "abstract": "Relation extraction from text is an important task for automatic knowledge\nbase population. In this thesis, we first propose a syntax-focused multi-factor\nattention network model for finding the relation between two entities. Next, we\npropose two joint entity and relation extraction frameworks based on\nencoder-decoder architecture. Finally, we propose a hierarchical entity graph\nconvolutional network for relation extraction across documents.", "published": "2021-04-05 07:18:54", "link": "http://arxiv.org/abs/2104.01799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining Customers' Opinions for Online Reputation Generation and\n  Visualization in e-Commerce Platforms", "abstract": "Customer reviews represent a very rich data source from which we can extract\nvery valuable information about different online shopping experiences. The\namount of the collected data may be very large especially for trendy items\n(products, movies, TV shows, hotels, services...), where the number of\navailable customers' opinions could easily surpass thousands. In fact, while a\ngood number of reviews could indeed give a hint about the quality of an item, a\npotential customer may not have time or effort to read all reviews for the\npurpose of making an informed decision (buying, renting, booking...). Thus, the\nneed for the right tools and technologies to help in such a task becomes a\nnecessity for the buyer as for the seller. My research goal in this thesis is\nto develop reputation systems that can automatically provide E-commerce\ncustomers with valuable information to support them during their online\ndecision-making process by mining online reviews expressed in natural language.", "published": "2021-04-05 14:46:57", "link": "http://arxiv.org/abs/2104.01935v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What's the best place for an AI conference, Vancouver or ______: Why\n  completing comparative questions is difficult", "abstract": "Although large neural language models (LMs) like BERT can be finetuned to\nyield state-of-the-art results on many NLP tasks, it is often unclear what\nthese models actually learn. Here we study using such LMs to fill in entities\nin human-authored comparative questions, like ``Which country is older, India\nor ______?'' -- i.e., we study the ability of neural LMs to ask (not answer)\nreasonable questions. We show that accuracy in this fill-in-the-blank task is\nwell-correlated with human judgements of whether a question is reasonable, and\nthat these models can be trained to achieve nearly human-level performance in\ncompleting comparative questions in three different subdomains. However,\nanalysis shows that what they learn fails to model any sort of broad notion of\nwhich entities are semantically comparable or similar -- instead the trained\nmodels are very domain-specific, and performance is highly correlated with\nco-occurrences between specific entities observed in the training set. This is\ntrue both for models that are pretrained on general text corpora, as well as\nmodels trained on a large corpus of comparison questions. Our study thus\nreinforces recent results on the difficulty of making claims about a deep\nmodel's world knowledge or linguistic competence based on performance on\nspecific benchmark problems. We make our evaluation datasets publicly available\nto foster future research on complex understanding and reasoning in such models\nat standards of human interaction.", "published": "2021-04-05 14:56:09", "link": "http://arxiv.org/abs/2104.01940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intent Detection and Slot Filling for Vietnamese", "abstract": "Intent detection and slot filling are important tasks in spoken and natural\nlanguage understanding. However, Vietnamese is a low-resource language in these\nresearch topics. In this paper, we present the first public intent detection\nand slot filling dataset for Vietnamese. In addition, we also propose a joint\nmodel for intent detection and slot filling, that extends the recent\nstate-of-the-art JointBERT+CRF model with an intent-slot attention layer to\nexplicitly incorporate intent context information into slot filling via \"soft\"\nintent label embedding. Experimental results on our Vietnamese dataset show\nthat our proposed model significantly outperforms JointBERT+CRF. We publicly\nrelease our dataset and the implementation of our model at:\nhttps://github.com/VinAIResearch/JointIDSF", "published": "2021-04-05 17:19:42", "link": "http://arxiv.org/abs/2104.02021v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Transformers in Emotion Recognition: a comparison of BERT,\n  DistillBERT, RoBERTa, XLNet and ELECTRA", "abstract": "This paper investigates how Natural Language Understanding (NLU) could be\napplied in Emotion Recognition, a specific task in affective computing. We\nfinetuned different transformers language models (BERT, DistilBERT, RoBERTa,\nXLNet, and ELECTRA) using a fine-grained emotion dataset and evaluating them in\nterms of performance (f1-score) and time to complete.", "published": "2021-04-05 17:46:10", "link": "http://arxiv.org/abs/2104.02041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Attentions for Long Document Summarization", "abstract": "The quadratic computational and memory complexities of large Transformers\nhave limited their scalability for long document summarization. In this paper,\nwe propose Hepos, a novel efficient encoder-decoder attention with head-wise\npositional strides to effectively pinpoint salient information from the source.\nWe further conduct a systematic study of existing efficient self-attentions.\nCombined with Hepos, we are able to process ten times more tokens than existing\nmodels that use full attentions. For evaluation, we present a new dataset,\nGovReport, with significantly longer documents and summaries. Results show that\nour models produce significantly higher ROUGE scores than competitive\ncomparisons, including new state-of-the-art results on PubMed. Human evaluation\nalso shows that our models generate more informative summaries with fewer\nunfaithful errors.", "published": "2021-04-05 18:45:13", "link": "http://arxiv.org/abs/2104.02112v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Distance: A New Metric for ASR Performance Analysis Towards\n  Spoken Language Understanding", "abstract": "Word Error Rate (WER) has been the predominant metric used to evaluate the\nperformance of automatic speech recognition (ASR) systems. However, WER is\nsometimes not a good indicator for downstream Natural Language Understanding\n(NLU) tasks, such as intent recognition, slot filling, and semantic parsing in\ntask-oriented dialog systems. This is because WER takes into consideration only\nliteral correctness instead of semantic correctness, the latter of which is\ntypically more important for these downstream tasks. In this study, we propose\na novel Semantic Distance (SemDist) measure as an alternative evaluation metric\nfor ASR systems to address this issue. We define SemDist as the distance\nbetween a reference and hypothesis pair in a sentence-level embedding space. To\nrepresent the reference and hypothesis as a sentence embedding, we exploit\nRoBERTa, a state-of-the-art pre-trained deep contextualized language model\nbased on the transformer architecture. We demonstrate the effectiveness of our\nproposed metric on various downstream tasks, including intent recognition,\nsemantic parsing, and named entity recognition.", "published": "2021-04-05 20:25:07", "link": "http://arxiv.org/abs/2104.02138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Will it Take to Fix Benchmarking in Natural Language Understanding?", "abstract": "Evaluation for many natural language understanding (NLU) tasks is broken:\nUnreliable and biased systems score so highly on standard benchmarks that there\nis little room for researchers who develop better systems to demonstrate their\nimprovements. The recent trend to abandon IID benchmarks in favor of\nadversarially-constructed, out-of-distribution test sets ensures that current\nmodels will perform poorly, but ultimately only obscures the abilities that we\nwant our benchmarks to measure. In this position paper, we lay out four\ncriteria that we argue NLU benchmarks should meet. We argue most current\nbenchmarks fail at these criteria, and that adversarial data collection does\nnot meaningfully address the causes of these failures. Instead, restoring a\nhealthy evaluation ecosystem will require significant progress in the design of\nbenchmark datasets, the reliability with which they are annotated, their size,\nand the ways they handle social bias.", "published": "2021-04-05 20:36:11", "link": "http://arxiv.org/abs/2104.02145v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Encoder Transducer: A Flexible Solution For Trading Off Accuracy\n  For Latency", "abstract": "We propose a dynamic encoder transducer (DET) for on-device speech\nrecognition. One DET model scales to multiple devices with different\ncomputation capacities without retraining or finetuning. To trading off\naccuracy and latency, DET assigns different encoders to decode different parts\nof an utterance. We apply and compare the layer dropout and the collaborative\nlearning for DET training. The layer dropout method that randomly drops out\nencoder layers in the training phase, can do on-demand layer dropout in\ndecoding. Collaborative learning jointly trains multiple encoders with\ndifferent depths in one single model. Experiment results on Librispeech and\nin-house data show that DET provides a flexible accuracy and latency trade-off.\nResults on Librispeech show that the full-size encoder in DET relatively\nreduces the word error rate of the same size baseline by over 8%. The\nlightweight encoder in DET trained with collaborative learning reduces the\nmodel size by 25% but still gets similar WER as the full-size baseline. DET\ngets similar accuracy as a baseline model with better latency on a large\nin-house data set by assigning a lightweight encoder for the beginning part of\none utterance and a full-size encoder for the rest.", "published": "2021-04-05 22:32:20", "link": "http://arxiv.org/abs/2104.02176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Automated Psychotherapy via Language Modeling", "abstract": "In this experiment, a model was devised, trained, and evaluated to automate\npsychotherapist/client text conversations through the use of state-of-the-art,\nSeq2Seq Transformer-based Natural Language Generation (NLG) systems. Through\ntraining the model upon a mix of the Cornell Movie Dialogue Corpus for language\nunderstanding and an open-source, anonymized, and public licensed\npsychotherapeutic dataset, the model achieved statistically significant\nperformance in published, standardized qualitative benchmarks against\nhuman-written validation data - meeting or exceeding human-written responses'\nperformance in 59.7% and 67.1% of the test set for two independent test methods\nrespectively. Although the model cannot replace the work of psychotherapists\nentirely, its ability to synthesize human-appearing utterances for the majority\nof the test set serves as a promising step towards communizing and easing\nstigma at the psychotherapeutic point-of-care.", "published": "2021-04-05 01:53:39", "link": "http://arxiv.org/abs/2104.10661v1", "categories": ["cs.CL", "68T50 (Primary) 91F20 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CBench: Towards Better Evaluation of Question Answering Over Knowledge\n  Graphs", "abstract": "Recently, there has been an increase in the number of knowledge graphs that\ncan be only queried by experts. However, describing questions using structured\nqueries is not straightforward for non-expert users who need to have sufficient\nknowledge about both the vocabulary and the structure of the queried knowledge\ngraph, as well as the syntax of the structured query language used to describe\nthe user's information needs. The most popular approach introduced to overcome\nthe aforementioned challenges is to use natural language to query these\nknowledge graphs. Although several question answering benchmarks can be used to\nevaluate question-answering systems over a number of popular knowledge graphs,\nchoosing a benchmark to accurately assess the quality of a question answering\nsystem is a challenging task.\n  In this paper, we introduce CBench, an extensible, and more informative\nbenchmarking suite for analyzing benchmarks and evaluating question answering\nsystems. CBench can be used to analyze existing benchmarks with respect to\nseveral fine-grained linguistic, syntactic, and structural properties of the\nquestions and queries in the benchmark. We show that existing benchmarks vary\nsignificantly with respect to these properties deeming choosing a small subset\nof them unreliable in evaluating QA systems. Until further research improves\nthe quality and comprehensiveness of benchmarks, CBench can be used to\nfacilitate this evaluation using a set of popular benchmarks that can be\naugmented with other user-provided benchmarks. CBench not only evaluates a\nquestion answering system based on popular single-number metrics but also gives\na detailed analysis of the linguistic, syntactic, and structural properties of\nanswered and unanswered questions to better help the developers of question\nanswering systems to better understand where their system excels and where it\nstruggles.", "published": "2021-04-05 15:41:14", "link": "http://arxiv.org/abs/2105.00811v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotating Columns with Pre-trained Language Models", "abstract": "Inferring meta information about tables, such as column headers or\nrelationships between columns, is an active research topic in data management\nas we find many tables are missing some of this information. In this paper, we\nstudy the problem of annotating table columns (i.e., predicting column types\nand the relationships between columns) using only information from the table\nitself. We develop a multi-task learning framework (called Doduo) based on\npre-trained language models, which takes the entire table as input and predicts\ncolumn types/relations using a single model. Experimental results show that\nDoduo establishes new state-of-the-art performance on two benchmarks for the\ncolumn type prediction and column relation prediction tasks with up to 4.0% and\n11.9% improvements, respectively. We report that Doduo can already outperform\nthe previous state-of-the-art performance with a minimal number of tokens, only\n8 tokens per column. We release a toolbox\n(https://github.com/megagonlabs/doduo) and confirm the effectiveness of Doduo\non a real-world data science problem through a case study.", "published": "2021-04-05 06:05:01", "link": "http://arxiv.org/abs/2104.01785v2", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "A Heuristic-driven Uncertainty based Ensemble Framework for Fake News\n  Detection in Tweets and News Articles", "abstract": "The significance of social media has increased manifold in the past few\ndecades as it helps people from even the most remote corners of the world to\nstay connected. With the advent of technology, digital media has become more\nrelevant and widely used than ever before and along with this, there has been a\nresurgence in the circulation of fake news and tweets that demand immediate\nattention. In this paper, we describe a novel Fake News Detection system that\nautomatically identifies whether a news item is \"real\" or \"fake\", as an\nextension of our work in the CONSTRAINT COVID-19 Fake News Detection in English\nchallenge. We have used an ensemble model consisting of pre-trained models\nfollowed by a statistical feature fusion network , along with a novel heuristic\nalgorithm by incorporating various attributes present in news items or tweets\nlike source, username handles, URL domains and authors as statistical feature.\nOur proposed framework have also quantified reliable predictive uncertainty\nalong with proper class output confidence level for the classification task. We\nhave evaluated our results on the COVID-19 Fake News dataset and FakeNewsNet\ndataset to show the effectiveness of the proposed algorithm on detecting fake\nnews in short news content as well as in news articles. We obtained a best\nF1-score of 0.9892 on the COVID-19 dataset, and an F1-score of 0.9073 on the\nFakeNewsNet dataset.", "published": "2021-04-05 06:35:30", "link": "http://arxiv.org/abs/2104.01791v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rethinking Perturbations in Encoder-Decoders for Fast Training", "abstract": "We often use perturbations to regularize neural models. For neural\nencoder-decoders, previous studies applied the scheduled sampling (Bengio et\nal., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations\nbut these methods require considerable computational time. Thus, this study\naddresses the question of whether these approaches are efficient enough for\ntraining time. We compare several perturbations in sequence-to-sequence\nproblems with respect to computational time. Experimental results show that the\nsimple techniques such as word dropout (Gal and Ghahramani, 2016) and random\nreplacement of input tokens achieve comparable (or better) scores to the\nrecently proposed perturbations, even though these simple methods are faster.\nOur code is publicly available at\nhttps://github.com/takase/rethink_perturbations.", "published": "2021-04-05 11:06:54", "link": "http://arxiv.org/abs/2104.01853v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker\n  Recognition", "abstract": "Many neural network speaker recognition systems model each speaker using a\nfixed-dimensional embedding vector. These embeddings are generally compared\nusing either linear or 2nd-order scoring and, until recently, do not handle\nutterance-specific uncertainty. In this work we propose scoring these\nrepresentations in a way that can capture uncertainty, enroll/test asymmetry\nand additional non-linear information. This is achieved by incorporating a\n2nd-stage neural network (known as a decision network) as part of an end-to-end\ntraining regimen. In particular, we propose the concept of decision residual\nnetworks which involves the use of a compact decision network to leverage\ncosine scores and to model the residual signal that's needed. Additionally, we\npresent a modification to the generalized end-to-end softmax loss function to\ntarget the separation of same/different speaker scores. We observed significant\nperformance gains for the two techniques.", "published": "2021-04-05 16:31:04", "link": "http://arxiv.org/abs/2104.01989v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPGISpeech: 5,000 hours of transcribed financial audio for fully\n  formatted end-to-end speech recognition", "abstract": "In the English speech-to-text (STT) machine learning task, acoustic models\nare conventionally trained on uncased Latin characters, and any necessary\northography (such as capitalization, punctuation, and denormalization of\nnon-standard words) is imputed by separate post-processing models. This adds\ncomplexity and limits performance, as many formatting tasks benefit from\nsemantic information present in the acoustic signal but absent in\ntranscription. Here we propose a new STT task: end-to-end neural transcription\nwith fully formatted text for target labels. We present baseline\nConformer-based models trained on a corpus of 5,000 hours of professionally\ntranscribed earnings calls, achieving a CER of 1.7. As a contribution to the\nSTT research community, we release the corpus free for non-commercial use at\nhttps://datasets.kensho.com/datasets/scribe.", "published": "2021-04-05 17:05:28", "link": "http://arxiv.org/abs/2104.02014v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Streaming Multi-talker Speech Recognition with Joint Speaker\n  Identification", "abstract": "In multi-talker scenarios such as meetings and conversations, speech\nprocessing systems are usually required to transcribe the audio as well as\nidentify the speakers for downstream applications. Since overlapped speech is\ncommon in this case, conventional approaches usually address this problem in a\ncascaded fashion that involves speech separation, speech recognition and\nspeaker identification that are trained independently. In this paper, we\npropose Streaming Unmixing, Recognition and Identification Transducer (SURIT)\n-- a new framework that deals with this problem in an end-to-end streaming\nfashion. SURIT employs the recurrent neural network transducer (RNN-T) as the\nbackbone for both speech recognition and speaker identification. We validate\nour idea on the LibrispeechMix dataset -- a multi-talker dataset derived from\nLibrispeech, and present encouraging results.", "published": "2021-04-05 18:37:33", "link": "http://arxiv.org/abs/2104.02109v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Discrete Reasoning Templates for Natural Language Understanding", "abstract": "Reasoning about information from multiple parts of a passage to derive an\nanswer is an open challenge for reading-comprehension models. In this paper, we\npresent an approach that reasons about complex questions by decomposing them to\nsimpler subquestions that can take advantage of single-span extraction\nreading-comprehension models, and derives the final answer according to\ninstructions in a predefined reasoning template. We focus on subtraction-based\narithmetic questions and evaluate our approach on a subset of the DROP dataset.\nWe show that our approach is competitive with the state-of-the-art while being\ninterpretable and requires little supervision", "published": "2021-04-05 18:56:56", "link": "http://arxiv.org/abs/2104.02115v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SpeechStew: Simply Mix All Available Speech Recognition Data to Train\n  One Large Neural Network", "abstract": "We present SpeechStew, a speech recognition model that is trained on a\ncombination of various publicly available speech recognition datasets: AMI,\nBroadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and\nWall Street Journal. SpeechStew simply mixes all of these datasets together,\nwithout any special re-weighting or re-balancing of the datasets. SpeechStew\nachieves SoTA or near SoTA results across a variety of tasks, without the use\nof an external language model. Our results include 9.0\\% WER on AMI-IHM, 4.7\\%\nWER on Switchboard, 8.3\\% WER on CallHome, and 1.3\\% on WSJ, which\nsignificantly outperforms prior work with strong external language models. We\nalso demonstrate that SpeechStew learns powerful transfer learning\nrepresentations. We fine-tune SpeechStew on a noisy low resource speech\ndataset, CHiME-6. We achieve 38.9\\% WER without a language model, which\ncompares to 38.6\\% WER to a strong HMM baseline with a language model.", "published": "2021-04-05 20:13:36", "link": "http://arxiv.org/abs/2104.02133v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ASER: Towards Large-scale Commonsense Knowledge Acquisition via\n  Higher-order Selectional Preference over Eventualities", "abstract": "Commonsense knowledge acquisition and reasoning have long been a core\nartificial intelligence problem. However, in the past, there has been a lack of\nscalable methods to collect commonsense knowledge. In this paper, we propose to\ndevelop principles for collecting commonsense knowledge based on selectional\npreference. We generalize the definition of selectional preference from one-hop\nlinguistic syntactic relations to higher-order relations over linguistic\ngraphs. Unlike previous commonsense knowledge definition (e.g., ConceptNet),\nselectional preference (SP) knowledge only relies on statistical distribution\nover linguistic graphs, which can be efficiently and accurately acquired from\nthe unlabeled corpus with modern tools. Following this principle, we develop a\nlarge-scale eventuality (a linguistic term covering activity, state, and\nevent)-based knowledge graph ASER, where each eventuality is represented as a\ndependency graph, and the relation between them is a discourse relation defined\nin shallow discourse parsing. The higher-order selectional preference over\ncollected linguistic graphs reflects various kinds of commonsense knowledge.\nMoreover, motivated by the observation that humans understand events by\nabstracting the observed events to a higher level and can thus transfer their\nknowledge to new events, we propose a conceptualization module to significantly\nboost the coverage of ASER. In total, ASER contains 648 million edges between\n438 million eventualities. After conceptualization with Probase, a selectional\npreference based concept-instance relational knowledge base, our concept graph\ncontains 15 million conceptualized eventualities and 224 million edges between\nthem. Detailed analysis is provided to demonstrate its quality. All the\ncollected data, APIs, and tools are available at\nhttps://github.com/HKUST-KnowComp/ASER.", "published": "2021-04-05 20:23:46", "link": "http://arxiv.org/abs/2104.02137v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "COVID-19 sentiment analysis via deep learning during the rise of novel\n  cases", "abstract": "Social scientists and psychologists take interest in understanding how people\nexpress emotions and sentiments when dealing with catastrophic events such as\nnatural disasters, political unrest, and terrorism. The COVID-19 pandemic is a\ncatastrophic event that has raised a number of psychological issues such as\ndepression given abrupt social changes and lack of employment. Advancements of\ndeep learning-based language models have been promising for sentiment analysis\nwith data from social networks such as Twitter. Given the situation with\nCOVID-19 pandemic, different countries had different peaks where the rise and\nfall of new cases affected lock-downs which directly affected the economy and\nemployment. During the rise of COVID-19 cases with stricter lock-downs, people\nhave been expressing their sentiments in social media. This can provide a deep\nunderstanding of human psychology during catastrophic events. In this paper, we\npresent a framework that employs deep learning-based language models via long\nshort-term memory (LSTM) recurrent neural networks for sentiment analysis\nduring the rise of novel COVID-19 cases in India. The framework features LSTM\nlanguage model with a global vector embedding and state-of-art BERT language\nmodel. We review the sentiments expressed for selective months in 2020 which\ncovers the first major peak of novel cases in India. Our framework utilises\nmulti-label sentiment classification where more than one sentiment can be\nexpressed at once. Our results indicate that the majority of the tweets have\nbeen positive with high levels of optimism during the rise of the novel\nCOVID-19 cases and the number of tweets significantly lowered towards the peak.\nThe predictions generally indicate that although the majority have been\noptimistic, a significant group of population has been annoyed towards the way\nthe pandemic was handled by the authorities.", "published": "2021-04-05 04:31:19", "link": "http://arxiv.org/abs/2104.10662v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "StarGAN-based Emotional Voice Conversion for Japanese Phrases", "abstract": "This paper shows that StarGAN-VC, a spectral envelope transformation method\nfor non-parallel many-to-many voice conversion (VC), is capable of emotional VC\n(EVC). Although StarGAN-VC has been shown to enable speaker identity\nconversion, its capability for EVC for Japanese phrases has not been clarified.\nIn this paper, we describe the direct application of StarGAN-VC to an EVC task\nwith minimal fundamental frequency and aperiodicity processing. Through\nsubjective evaluation experiments, we evaluated the performance of our\nStarGAN-EVC system in terms of its ability to achieve EVC for Japanese phrases.\nThe subjective evaluation is conducted in terms of subjective classification\nand mean opinion score of neutrality and similarity. In addition, the\ninterdependence between the source and target emotional domains was\ninvestigated from the perspective of the quality of EVC.", "published": "2021-04-05 08:08:42", "link": "http://arxiv.org/abs/2104.01807v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval", "abstract": "Speech-based image retrieval has been studied as a proxy for joint\nrepresentation learning, usually without emphasis on retrieval itself. As such,\nit is unclear how well speech-based retrieval can work in practice -- both in\nan absolute sense and versus alternative strategies that combine automatic\nspeech recognition (ASR) with strong text encoders. In this work, we\nextensively study and expand choices of encoder architectures, training\nmethodology (including unimodal and multimodal pretraining), and other factors.\nOur experiments cover different types of speech in three datasets: Flickr\nAudio, Places Audio, and Localized Narratives. Our best model configuration\nachieves large gains over state of the art, e.g., pushing recall-at-one from\n21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also\nshow our best speech-based models can match or exceed cascaded ASR-to-text\nencoding when speech is spontaneous, accented, or otherwise hard to\nautomatically transcribe.", "published": "2021-04-05 13:11:40", "link": "http://arxiv.org/abs/2104.01894v3", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automating Transfer Credit Assessment in Student Mobility -- A Natural\n  Language Processing-based Approach", "abstract": "Student mobility or academic mobility involves students moving between\ninstitutions during their post-secondary education, and one of the challenging\ntasks in this process is to assess the transfer credits to be offered to the\nincoming student. In general, this process involves domain experts comparing\nthe learning outcomes of the courses, to decide on offering transfer credits to\nthe incoming students. This manual implementation is not only labor-intensive\nbut also influenced by undue bias and administrative complexity. The proposed\nresearch article focuses on identifying a model that exploits the advancements\nin the field of Natural Language Processing (NLP) to effectively automate this\nprocess. Given the unique structure, domain specificity, and complexity of\nlearning outcomes (LOs), a need for designing a tailor-made model arises. The\nproposed model uses a clustering-inspired methodology based on knowledge-based\nsemantic similarity measures to assess the taxonomic similarity of LOs and a\ntransformer-based semantic similarity model to assess the semantic similarity\nof the LOs. The similarity between LOs is further aggregated to form course to\ncourse similarity. Due to the lack of quality benchmark datasets, a new\nbenchmark dataset containing seven course-to-course similarity measures is\nproposed. Understanding the inherent need for flexibility in the\ndecision-making process the aggregation part of the model offers tunable\nparameters to accommodate different scenarios. While providing an efficient\nmodel to assess the similarity between courses with existing resources, this\nresearch work steers future research attempts to apply NLP in the field of\narticulation in an ideal direction by highlighting the persisting research\ngaps.", "published": "2021-04-05 15:14:59", "link": "http://arxiv.org/abs/2104.01955v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "End-to-End Speaker-Attributed ASR with Transformer", "abstract": "This paper presents our recent effort on end-to-end speaker-attributed\nautomatic speech recognition, which jointly performs speaker counting, speech\nrecognition and speaker identification for monaural multi-talker audio.\nFirstly, we thoroughly update the model architecture that was previously\ndesigned based on a long short-term memory (LSTM)-based attention encoder\ndecoder by applying transformer architectures. Secondly, we propose a speaker\ndeduplication mechanism to reduce speaker identification errors in highly\noverlapped regions. Experimental results on the LibriSpeechMix dataset shows\nthat the transformer-based architecture is especially good at counting the\nspeakers and that the proposed model reduces the speaker-attributed word error\nrate by 47% over the LSTM-based baseline. Furthermore, for the LibriCSS\ndataset, which consists of real recordings of overlapped speech, the proposed\nmodel achieves concatenated minimum-permutation word error rates of 11.9% and\n16.3% with and without target speaker profiles, respectively, both of which are\nthe state-of-the-art results for LibriCSS with the monaural setting.", "published": "2021-04-05 19:54:15", "link": "http://arxiv.org/abs/2104.02128v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Contextualized Streaming End-to-End Speech Recognition with Trie-Based\n  Deep Biasing and Shallow Fusion", "abstract": "How to leverage dynamic contextual information in end-to-end speech\nrecognition has remained an active research area. Previous solutions to this\nproblem were either designed for specialized use cases that did not generalize\nwell to open-domain scenarios, did not scale to large biasing lists, or\nunderperformed on rare long-tail words. We address these limitations by\nproposing a novel solution that combines shallow fusion, trie-based deep\nbiasing, and neural network language model contextualization. These techniques\nresult in significant 19.5% relative Word Error Rate improvement over existing\ncontextual biasing approaches and 5.4%-9.3% improvement compared to a strong\nhybrid baseline on both open-domain and constrained contextualization tasks,\nwhere the targets consist of mostly rare long-tail words. Our final system\nremains lightweight and modular, allowing for quick modification without model\nre-training.", "published": "2021-04-05 23:59:43", "link": "http://arxiv.org/abs/2104.02194v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Citrinet: Closing the Gap between Non-Autoregressive and Autoregressive\n  End-to-End Models for Automatic Speech Recognition", "abstract": "We propose Citrinet - a new end-to-end convolutional Connectionist Temporal\nClassification (CTC) based automatic speech recognition (ASR) model. Citrinet\nis deep residual neural model which uses 1D time-channel separable convolutions\ncombined with sub-word encoding and squeeze-and-excitation. The resulting\narchitecture significantly reduces the gap between non-autoregressive and\nsequence-to-sequence and transducer models. We evaluate Citrinet on\nLibriSpeech, TED-LIUM2, AISHELL-1 and Multilingual LibriSpeech (MLS) English\nspeech datasets. Citrinet accuracy on these datasets is close to the best\nautoregressive Transducer models.", "published": "2021-04-05 00:16:27", "link": "http://arxiv.org/abs/2104.01721v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The Multi-speaker Multi-style Voice Cloning Challenge 2021", "abstract": "The Multi-speaker Multi-style Voice Cloning Challenge (M2VoC) aims to provide\na common sizable dataset as well as a fair testbed for the benchmarking of the\npopular voice cloning task. Specifically, we formulate the challenge to adapt\nan average TTS model to the stylistic target voice with limited data from\ntarget speaker, evaluated by speaker identity and style similarity. The\nchallenge consists of two tracks, namely few-shot track and one-shot track,\nwhere the participants are required to clone multiple target voices with 100\nand 5 samples respectively. There are also two sub-tracks in each track. For\nsub-track a, to fairly compare different strategies, the participants are\nallowed to use only the training data provided by the organizer strictly. For\nsub-track b, the participants are allowed to use any data publicly available.\nIn this paper, we present a detailed explanation on the tasks and data used in\nthe challenge, followed by a summary of submitted systems and evaluation\nresults.", "published": "2021-04-05 09:14:43", "link": "http://arxiv.org/abs/2104.01818v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speaker conditioned acoustic modeling for multi-speaker conversational\n  ASR", "abstract": "In this paper, we propose a novel approach for the transcription of speech\nconversations with natural speaker overlap, from single channel speech\nrecordings. The proposed model is a combination of a speaker diarization system\nand a hybrid automatic speech recognition (ASR) system. The speaker conditioned\nacoustic model (SCAM) in the ASR system consists of a series of embedding\nlayers which use the speaker activity inputs from the diarization system to\nderive speaker specific embeddings. The output of the SCAM are speaker specific\nsenones that are used for decoding the transcripts for each speaker in the\nconversation. In this work, we experiment with the automatic speaker activity\ndecisions generated using an end-to-end speaker diarization system. A joint\nlearning approach is also proposed where the diarization model and the ASR\nacoustic model are jointly optimized. The experiments are performed on the\nmixed-channel two speaker recordings from the Switchboard corpus of telephone\nconversations. In these experiments, we show that the proposed acoustic model,\nincorporating speaker activity decisions and joint optimization, improves\nsignificantly over the ASR system with explicit source filtering (relative\nimprovements of 12% in word error rate (WER) over the baseline system).", "published": "2021-04-05 12:41:53", "link": "http://arxiv.org/abs/2104.01882v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Reformulating DOVER-Lap Label Mapping as a Graph Partitioning Problem", "abstract": "We recently proposed DOVER-Lap, a method for combining overlap-aware speaker\ndiarization system outputs. DOVER-Lap improved upon its predecessor DOVER by\nusing a label mapping method based on globally-informed greedy search. In this\npaper, we analyze this label mapping in the framework of a maximum orthogonal\ngraph partitioning problem, and present three inferences. First, we show that\nDOVER-Lap label mapping is exponential in the input size, which poses a\nchallenge when combining a large number of hypotheses. We then revisit the\nDOVER label mapping algorithm and propose a modification which performs similar\nto DOVER-Lap while being computationally tractable. We also derive an\napproximation bound for the algorithm in terms of the maximum number of\nhypotheses speakers. Finally, we describe a randomized local search algorithm\nwhich provides a near-optimal $(1-\\epsilon)$-approximate solution to the\nproblem with high probability. We empirically demonstrate the effectiveness of\nour methods on the AMI meeting corpus. Our code is publicly available:\nhttps://github.com/desh2608/dover-lap.", "published": "2021-04-05 15:14:25", "link": "http://arxiv.org/abs/2104.01954v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-time Streaming Wave-U-Net with Temporal Convolutions for\n  Multichannel Speech Enhancement", "abstract": "In this paper, we describe the work that we have done to participate in Task1\nof the ConferencingSpeech2021 challenge. This task set a goal to develop the\nsolution for multi-channel speech enhancement in a real-time manner. We propose\na novel system for streaming speech enhancement. We employ Wave-U-Net\narchitecture with temporal convolutions in encoder and decoder. We incorporate\nself-attention in the decoder to apply attention mask retrieved from\nskip-connection on features from down-blocks. We explore history cache\nmechanisms that work like hidden states in recurrent networks and implemented\nthem in proposal solution. It helps us to run an inference with chunks length\n40ms and Real-Time Factor 0.4 with the same precision.", "published": "2021-04-05 14:03:42", "link": "http://arxiv.org/abs/2104.01923v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Acted vs. Improvised: Domain Adaptation for Elicitation Approaches in\n  Audio-Visual Emotion Recognition", "abstract": "Key challenges in developing generalized automatic emotion recognition\nsystems include scarcity of labeled data and lack of gold-standard references.\nEven for the cues that are labeled as the same emotion category, the\nvariability of associated expressions can be high depending on the elicitation\ncontext e.g., emotion elicited during improvised conversations vs. acted\nsessions with predefined scripts. In this work, we regard the emotion\nelicitation approach as domain knowledge, and explore domain transfer learning\ntechniques on emotional utterances collected under different emotion\nelicitation approaches, particularly with limited labeled target samples. Our\nemotion recognition model combines the gradient reversal technique with an\nentropy loss function as well as the softlabel loss, and the experiment results\nshow that domain transfer learning methods can be employed to alleviate the\ndomain mismatch between different elicitation approaches. Our work provides new\ninsights into emotion data collection, particularly the impact of its\nelicitation strategies, and the importance of domain adaptation in emotion\nrecognition aiming for generalized systems.", "published": "2021-04-05 15:59:31", "link": "http://arxiv.org/abs/2104.01978v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can audio-visual integration strengthen robustness under multimodal\n  attacks?", "abstract": "In this paper, we propose to make a systematic study on machines multisensory\nperception under attacks. We use the audio-visual event recognition task\nagainst multimodal adversarial attacks as a proxy to investigate the robustness\nof audio-visual learning. We attack audio, visual, and both modalities to\nexplore whether audio-visual integration still strengthens perception and how\ndifferent fusion mechanisms affect the robustness of audio-visual models. For\ninterpreting the multimodal interactions under attacks, we learn a\nweakly-supervised sound source visual localization model to localize sounding\nregions in videos. To mitigate multimodal attacks, we propose an audio-visual\ndefense approach based on an audio-visual dissimilarity constraint and external\nfeature memory banks. Extensive experiments demonstrate that audio-visual\nmodels are susceptible to multimodal adversarial attacks; audio-visual\nintegration could decrease the model robustness rather than strengthen under\nmultimodal attacks; even a weakly-supervised sound source visual localization\nmodel can be successfully fooled; our defense method can improve the\ninvulnerability of audio-visual networks without significantly sacrificing\nclean model performance.", "published": "2021-04-05 16:46:45", "link": "http://arxiv.org/abs/2104.02000v1", "categories": ["cs.CV", "cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Uncertainty-Aware COVID-19 Detection from Imbalanced Sound Data", "abstract": "Recently, sound-based COVID-19 detection studies have shown great promise to\nachieve scalable and prompt digital pre-screening. However, there are still two\nunsolved issues hindering the practice. First, collected datasets for model\ntraining are often imbalanced, with a considerably smaller proportion of users\ntested positive, making it harder to learn representative and robust features.\nSecond, deep learning models are generally overconfident in their predictions.\nClinically, false predictions aggravate healthcare costs. Estimation of the\nuncertainty of screening would aid this. To handle these issues, we propose an\nensemble framework where multiple deep learning models for sound-based COVID-19\ndetection are developed from different but balanced subsets from original data.\nAs such, data are utilized more effectively compared to traditional up-sampling\nand down-sampling approaches: an AUC of 0.74 with a sensitivity of 0.68 and a\nspecificity of 0.69 is achieved. Simultaneously, we estimate uncertainty from\nthe disagreement across multiple models. It is shown that false predictions\noften yield higher uncertainty, enabling us to suggest the users with certainty\nhigher than a threshold to repeat the audio test on their phones or to take\nclinical tests if digital diagnosis still fails. This study paves the way for a\nmore robust sound-based COVID-19 automated screening system.", "published": "2021-04-05 16:54:03", "link": "http://arxiv.org/abs/2104.02005v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Personalized Speech Enhancement through Self-Supervised\n  Learning", "abstract": "This work presents self-supervised learning methods for developing monaural\nspeaker-specific (i.e., personalized) speech enhancement models. While\ngeneralist models must broadly address many speakers, specialist models can\nadapt their enhancement function towards a particular speaker's voice,\nexpecting to solve a narrower problem. Hence, specialists are capable of\nachieving more optimal performance in addition to reducing computational\ncomplexity. However, naive personalization methods can require clean speech\nfrom the target user, which is inconvenient to acquire, e.g., due to subpar\nrecording conditions. To this end, we pose personalization as either a\nzero-shot task, in which no additional clean speech of the target speaker is\nused for training, or a few-shot learning task, in which the goal is to\nminimize the duration of the clean speech used for transfer learning. With this\npaper, we propose self-supervised learning methods as a solution to both zero-\nand few-shot personalization tasks. The proposed methods are designed to learn\nthe personalized speech features from unlabeled data (i.e., in-the-wild noisy\nrecordings from the target user) without knowing the corresponding clean\nsources. Our experiments investigate three different self-supervised learning\nmechanisms. The results show that self-supervised models achieve zero-shot and\nfew-shot personalization using fewer model parameters and less clean data from\nthe target user, achieving the data efficiency and model compression goals.", "published": "2021-04-05 17:12:51", "link": "http://arxiv.org/abs/2104.02017v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Personalized Speech Enhancement through Self-Supervised Data\n  Augmentation and Purification", "abstract": "Training personalized speech enhancement models is innately a no-shot\nlearning problem due to privacy constraints and limited access to noise-free\nspeech from the target user. If there is an abundance of unlabeled noisy speech\nfrom the test-time user, a personalized speech enhancement model can be trained\nusing self-supervised learning. One straightforward approach to model\npersonalization is to use the target speaker's noisy recordings as\npseudo-sources. Then, a pseudo denoising model learns to remove injected\ntraining noises and recover the pseudo-sources. However, this approach is\nvolatile as it depends on the quality of the pseudo-sources, which may be too\nnoisy. As a remedy, we propose an improvement to the self-supervised approach\nthrough data purification. We first train an SNR predictor model to estimate\nthe frame-by-frame SNR of the pseudo-sources. Then, the predictor's estimates\nare converted into weights which adjust the frame-by-frame contribution of the\npseudo-sources towards training the personalized model. We empirically show\nthat the proposed data purification step improves the usability of the\nspeaker-specific noisy data in the context of personalized speech enhancement.\nWithout relying on any clean speech recordings or speaker embeddings, our\napproach may be seen as privacy-preserving.", "published": "2021-04-05 17:17:55", "link": "http://arxiv.org/abs/2104.02018v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cyclic Co-Learning of Sounding Object Visual Grounding and Sound\n  Separation", "abstract": "There are rich synchronized audio and visual events in our daily life. Inside\nthe events, audio scenes are associated with the corresponding visual objects;\nmeanwhile, sounding objects can indicate and help to separate their individual\nsounds in the audio track. Based on this observation, in this paper, we propose\na cyclic co-learning (CCoL) paradigm that can jointly learn sounding object\nvisual grounding and audio-visual sound separation in a unified framework.\nConcretely, we can leverage grounded object-sound relations to improve the\nresults of sound separation. Meanwhile, benefiting from discriminative\ninformation from separated sounds, we improve training example sampling for\nsounding object grounding, which builds a co-learning cycle for the two tasks\nand makes them mutually beneficial. Extensive experiments show that the\nproposed framework outperforms the compared recent approaches on both tasks,\nand they can benefit from each other with our cyclic co-learning.", "published": "2021-04-05 17:30:41", "link": "http://arxiv.org/abs/2104.02026v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "SpeakerStew: Scaling to Many Languages with a Triaged Multilingual\n  Text-Dependent and Text-Independent Speaker Verification System", "abstract": "In this paper, we describe SpeakerStew - a hybrid system to perform speaker\nverification on 46 languages. Two core ideas were explored in this system: (1)\nPooling training data of different languages together for multilingual\ngeneralization and reducing development cycles; (2) A novel triage mechanism\nbetween text-dependent and text-independent models to reduce runtime cost and\nexpected latency. To the best of our knowledge, this is the first study of\nspeaker verification systems at the scale of 46 languages. The problem is\nframed from the perspective of using a smart speaker device with interactions\nconsisting of a wake-up keyword (text-dependent) followed by a speech query\n(text-independent). Experimental evidence suggests that training on multiple\nlanguages can generalize to unseen varieties while maintaining performance on\nseen varieties. We also found that it can reduce computational requirements for\ntraining models by an order of magnitude. Furthermore, during model inference\non English data, we observe that leveraging a triage framework can reduce the\nnumber of calls to the more computationally expensive text-independent system\nby 73% (and reduce latency by 59%) while maintaining an EER no worse than the\ntext-independent setup.", "published": "2021-04-05 19:48:16", "link": "http://arxiv.org/abs/2104.02125v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
