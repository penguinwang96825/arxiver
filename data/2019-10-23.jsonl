{"title": "Controlling the Output Length of Neural Machine Translation", "abstract": "The recent advances introduced by neural machine translation (NMT) are\nrapidly expanding the application fields of machine translation, as well as\nreshaping the quality level to be targeted. In particular, if translations have\nto fit some given layout, quality should not only be measured in terms of\nadequacy and fluency, but also length. Exemplary cases are the translation of\ndocument files, subtitles, and scripts for dubbing, where the output length\nshould ideally be as close as possible to the length of the input text. This\npaper addresses for the first time, to the best of our knowledge, the problem\nof controlling the output length in NMT. We investigate two methods for biasing\nthe output length with a transformer architecture: i) conditioning the output\nto a given target-source length-ratio class and ii) enriching the transformer\npositional embedding with length information. Our experiments show that both\nmethods can induce the network to generate shorter translations, as well as\nacquiring interpretable linguistic skills.", "published": "2019-10-23 08:25:43", "link": "http://arxiv.org/abs/1910.10408v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech-XLNet: Unsupervised Acoustic Model Pretraining For Self-Attention\n  Networks", "abstract": "Self-attention network (SAN) can benefit significantly from the\nbi-directional representation learning through unsupervised pretraining\nparadigms such as BERT and XLNet. In this paper, we present an XLNet-like\npretraining scheme \"Speech-XLNet\" for unsupervised acoustic model pretraining\nto learn speech representations with SAN. The pretrained SAN is finetuned under\nthe hybrid SAN/HMM framework. We conjecture that by shuffling the speech frame\norders, the permutation in Speech-XLNet serves as a strong regularizer to\nencourage the SAN to make inferences by focusing on global structures through\nits attention weights. In addition, Speech-XLNet also allows the model to\nexplore the bi-directional contexts for effective speech representation\nlearning. Experiments on TIMIT and WSJ demonstrate that Speech-XLNet greatly\nimproves the SAN/HMM performance in terms of both convergence speed and\nrecognition accuracy compared to the one trained from randomly initialized\nweights. Our best systems achieve a relative improvement of 11.9% and 8.3% on\nthe TIMIT and WSJ tasks respectively. In particular, the best system achieves a\nphone error rate (PER) of 13.3% on the TIMIT test set, which to our best\nknowledge, is the lowest PER obtained from a single system.", "published": "2019-10-23 07:08:03", "link": "http://arxiv.org/abs/1910.10387v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Instance-Based Model Adaptation For Direct Speech Translation", "abstract": "Despite recent technology advancements, the effectiveness of neural\napproaches to end-to-end speech-to-text translation is still limited by the\npaucity of publicly available training corpora. We tackle this limitation with\na method to improve data exploitation and boost the system's performance at\ninference time. Our approach allows us to customize \"on the fly\" an existing\nmodel to each incoming translation request. At its core, it exploits an\ninstance selection procedure to retrieve, from a given pool of data, a small\nset of samples similar to the input query in terms of latent properties of its\naudio signal. The retrieved samples are then used for an instance-specific\nfine-tuning of the model. We evaluate our approach in three different\nscenarios. In all data conditions (different languages, in/out-of-domain\nadaptation), our instance-based adaptation yields coherent performance gains\nover static models.", "published": "2019-10-23 16:39:21", "link": "http://arxiv.org/abs/1910.10663v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Efficient Dynamic WFST Decoding for Personalized Language Models", "abstract": "We propose a two-layer cache mechanism to speed up dynamic WFST decoding with\npersonalized language models. The first layer is a public cache that stores\nmost of the static part of the graph. This is shared globally among all users.\nA second layer is a private cache that caches the graph that represents the\npersonalized language model, which is only shared by the utterances from a\nparticular user. We also propose two simple yet effective pre-initialization\nmethods, one based on breadth-first search, and another based on a data-driven\nexploration of decoder states using previous utterances. Experiments with a\ncalling speech recognition task using a personalized contact list demonstrate\nthat the proposed public cache reduces decoding time by factor of three\ncompared to decoding without pre-initialization. Using the private cache\nprovides additional efficiency gains, reducing the decoding time by a factor of\nfive.", "published": "2019-10-23 17:10:26", "link": "http://arxiv.org/abs/1910.10670v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KnowIT VQA: Answering Knowledge-Based Questions about Videos", "abstract": "We propose a novel video understanding task by fusing knowledge-based and\nvideo question answering. First, we introduce KnowIT VQA, a video dataset with\n24,282 human-generated question-answer pairs about a popular sitcom. The\ndataset combines visual, textual and temporal coherence reasoning together with\nknowledge-based questions, which need of the experience obtained from the\nviewing of the series to be answered. Second, we propose a video understanding\nmodel by combining the visual and textual video content with specific knowledge\nabout the show. Our main findings are: (i) the incorporation of knowledge\nproduces outstanding improvements for VQA in video, and (ii) the performance on\nKnowIT VQA still lags well behind human accuracy, indicating its usefulness for\nstudying current video modelling limitations.", "published": "2019-10-23 01:44:12", "link": "http://arxiv.org/abs/1910.10706v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Analyzing ASR pretraining for low-resource speech-to-text translation", "abstract": "Previous work has shown that for low-resource source languages, automatic\nspeech-to-text translation (AST) can be improved by pretraining an end-to-end\nmodel on automatic speech recognition (ASR) data from a high-resource language.\nHowever, it is not clear what factors --e.g., language relatedness or size of\nthe pretraining data-- yield the biggest improvements, or whether pretraining\ncan be effectively combined with other methods such as data augmentation. Here,\nwe experiment with pretraining on datasets of varying sizes, including\nlanguages related and unrelated to the AST source language. We find that the\nbest predictor of final AST performance is the word error rate of the\npretrained ASR model, and that differences in ASR/AST performance correlate\nwith how phonetic information is encoded in the later RNN layers of our model.\nWe also show that pretraining and data augmentation yield complementary\nbenefits for AST.", "published": "2019-10-23 18:37:56", "link": "http://arxiv.org/abs/1910.10762v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Emergent Properties of Finetuned Language Representation Models", "abstract": "Large, self-supervised transformer-based language representation models have\nrecently received significant amounts of attention, and have produced\nstate-of-the-art results across a variety of tasks simply by scaling up\npre-training on larger and larger corpora. Such models usually produce high\ndimensional vectors, on top of which additional task-specific layers and\narchitectural modifications are added to adapt them to specific downstream\ntasks. Though there exists ample evidence that such models work well, we aim to\nunderstand what happens when they work well. We analyze the redundancy and\nlocation of information contained in output vectors for one such language\nrepresentation model -- BERT. We show empirical evidence that the [CLS]\nembedding in BERT contains highly redundant information, and can be compressed\nwith minimal loss of accuracy, especially for finetuned models, dovetailing\ninto open threads in the field about the role of over-parameterization in\nlearning. We also shed light on the existence of specific output dimensions\nwhich alone give very competitive results when compared to using all dimensions\nof output vectors.", "published": "2019-10-23 23:01:10", "link": "http://arxiv.org/abs/1910.10832v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RNN based Incremental Online Spoken Language Understanding", "abstract": "Spoken Language Understanding (SLU) typically comprises of an automatic\nspeech recognition (ASR) followed by a natural language understanding (NLU)\nmodule. The two modules process signals in a blocking sequential fashion, i.e.,\nthe NLU often has to wait for the ASR to finish processing on an utterance\nbasis, potentially leading to high latencies that render the spoken interaction\nless natural. In this paper, we propose recurrent neural network (RNN) based\nincremental processing towards the SLU task of intent detection. The proposed\nmethodology offers lower latencies than a typical SLU system, without any\nsignificant reduction in system accuracy. We introduce and analyze different\nrecurrent neural network architectures for incremental and online processing of\nthe ASR transcripts and compare it to the existing offline systems. A lexical\nEnd-of-Sentence (EOS) detector is proposed for segmenting the stream of\ntranscript into sentences for intent classification. Intent detection\nexperiments are conducted on benchmark ATIS, Snips and Facebook's multilingual\ntask oriented dialog datasets modified to emulate a continuous incremental\nstream of words with no utterance demarcation. We also analyze the prospects of\nearly intent detection, before EOS, with our proposed system.", "published": "2019-10-23 00:17:26", "link": "http://arxiv.org/abs/1910.10287v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Location-Relative Attention Mechanisms For Robust Long-Form Speech\n  Synthesis", "abstract": "Despite the ability to produce human-level speech for in-domain text,\nattention-based end-to-end text-to-speech (TTS) systems suffer from text\nalignment failures that increase in frequency for out-of-domain text. We show\nthat these failures can be addressed using simple location-relative attention\nmechanisms that do away with content-based query/key comparisons. We compare\ntwo families of attention mechanisms: location-relative GMM-based mechanisms\nand additive energy-based mechanisms. We suggest simple modifications to\nGMM-based attention that allow it to align quickly and consistently during\ntraining, and introduce a new location-relative attention mechanism to the\nadditive energy-based family, called Dynamic Convolution Attention (DCA). We\ncompare the various mechanisms in terms of alignment speed and consistency\nduring training, naturalness, and ability to generalize to long utterances, and\nconclude that GMM attention and DCA can generalize to very long utterances,\nwhile preserving naturalness for shorter, in-domain utterances.", "published": "2019-10-23 00:21:33", "link": "http://arxiv.org/abs/1910.10288v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Unifying Framework of Bilinear LSTMs", "abstract": "This paper presents a novel unifying framework of bilinear LSTMs that can\nrepresent and utilize the nonlinear interaction of the input features present\nin sequence datasets for achieving superior performance over a linear LSTM and\nyet not incur more parameters to be learned. To realize this, our unifying\nframework allows the expressivity of the linear vs. bilinear terms to be\nbalanced by correspondingly trading off between the hidden state vector size\nvs. approximation quality of the weight matrix in the bilinear term so as to\noptimize the performance of our bilinear LSTM, while not incurring more\nparameters to be learned. We empirically evaluate the performance of our\nbilinear LSTM in several language-based sequence learning tasks to demonstrate\nits general applicability.", "published": "2019-10-23 00:50:29", "link": "http://arxiv.org/abs/1910.10294v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deja-vu: Double Feature Presentation and Iterated Loss in Deep\n  Transformer Networks", "abstract": "Deep acoustic models typically receive features in the first layer of the\nnetwork, and process increasingly abstract representations in the subsequent\nlayers. Here, we propose to feed the input features at multiple depths in the\nacoustic model. As our motivation is to allow acoustic models to re-examine\ntheir input features in light of partial hypotheses we introduce intermediate\nmodel heads and loss function. We study this architecture in the context of\ndeep Transformer networks, and we use an attention mechanism over both the\nprevious layer activations and the input features. To train this model's\nintermediate output hypothesis, we apply the objective function at each layer\nright before feature re-use. We find that the use of such iterated loss\nsignificantly improves performance by itself, as well as enabling input feature\nre-use. We present results on both Librispeech, and a large scale video\ndataset, with relative improvements of 10 - 20% for Librispeech and 3.2 - 13%\nfor videos.", "published": "2019-10-23 02:48:30", "link": "http://arxiv.org/abs/1910.10324v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Transformer with Interleaved Self-attention and Convolution for Hybrid\n  Acoustic Models", "abstract": "Transformer with self-attention has achieved great success in the area of\nnature language processing. Recently, there have been a few studies on\ntransformer for end-to-end speech recognition, while its application for hybrid\nacoustic model is still very limited. In this paper, we revisit the\ntransformer-based hybrid acoustic model, and propose a model structure with\ninterleaved self-attention and 1D convolution, which is proven to have faster\nconvergence and higher recognition accuracy. We also study several aspects of\nthe transformer model, including the impact of the positional encoding feature,\ndropout regularization, as well as training with and without time restriction.\nWe show competitive recognition results on the public Librispeech dataset when\ncompared to the Kaldi baseline at both cross entropy training and sequence\ntraining stages. For reproducible research, we release our source code and\nrecipe within the PyKaldi2 toolbox.", "published": "2019-10-23 04:57:51", "link": "http://arxiv.org/abs/1910.10352v1", "categories": ["eess.AS", "cs.CL", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A Hybrid Semantic Parsing Approach for Tabular Data Analysis", "abstract": "This paper presents a novel approach to translating natural language\nquestions to SQL queries for given tables, which meets three requirements as a\nreal-world data analysis application: cross-domain, multilingualism and\nenabling quick-start. Our proposed approach consists of: (1) a novel data\nabstraction step before the parser to make parsing table-agnosticism; (2) a set\nof semantic rules for parsing abstracted data-analysis questions to\nintermediate logic forms as tree derivations to reduce the search space; (3) a\nneural-based model as a local scoring function on a span-based semantic parser\nfor structured optimization and efficient inference. Experiments show that our\napproach outperforms state-of-the-art algorithms on a large open benchmark\ndataset WikiSQL. We also achieve promising results on a small dataset for more\ncomplex queries in both English and Chinese, which demonstrates our language\nexpansion and quick-start ability.", "published": "2019-10-23 05:41:39", "link": "http://arxiv.org/abs/1910.10363v2", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "Speaker Adaptive Training using Model Agnostic Meta-Learning", "abstract": "Speaker adaptive training (SAT) of neural network acoustic models learns\nmodels in a way that makes them more suitable for adaptation to test\nconditions. Conventionally, model-based speaker adaptive training is performed\nby having a set of speaker dependent parameters that are jointly optimised with\nspeaker independent parameters in order to remove speaker variation. However,\nthis does not scale well if all neural network weights are to be adapted to the\nspeaker. In this paper we formulate speaker adaptive training as a\nmeta-learning task, in which an adaptation process using gradient descent is\nencoded directly into the training of the model. We compare our approach with\ntest-only adaptation of a standard baseline model and a SAT-LHUC model with a\nlearned speaker adaptation schedule and demonstrate that the meta-learning\napproach achieves comparable results.", "published": "2019-10-23 15:22:22", "link": "http://arxiv.org/abs/1910.10605v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A practical two-stage training strategy for multi-stream end-to-end\n  speech recognition", "abstract": "The multi-stream paradigm of audio processing, in which several sources are\nsimultaneously considered, has been an active research area for information\nfusion. Our previous study offered a promising direction within end-to-end\nautomatic speech recognition, where parallel encoders aim to capture diverse\ninformation followed by a stream-level fusion based on attention mechanisms to\ncombine the different views. However, with an increasing number of streams\nresulting in an increasing number of encoders, the previous approach could\nrequire substantial memory and massive amounts of parallel data for joint\ntraining. In this work, we propose a practical two-stage training scheme.\nStage-1 is to train a Universal Feature Extractor (UFE), where encoder outputs\nare produced from a single-stream model trained with all data. Stage-2\nformulates a multi-stream scheme intending to solely train the attention fusion\nmodule using the UFE features and pretrained components from Stage-1.\nExperiments have been conducted on two datasets, DIRHA and AMI, as a\nmulti-stream scenario. Compared with our previous method, this strategy\nachieves relative word error rate reductions of 8.2--32.4%, while consistently\noutperforming several conventional combination methods.", "published": "2019-10-23 17:12:48", "link": "http://arxiv.org/abs/1910.10671v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.", "published": "2019-10-23 17:37:36", "link": "http://arxiv.org/abs/1910.10683v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Correction of Automatic Speech Recognition with Transformer\n  Sequence-to-sequence Model", "abstract": "In this work, we introduce a simple yet efficient post-processing model for\nautomatic speech recognition (ASR). Our model has Transformer-based\nencoder-decoder architecture which \"translates\" ASR model output into\ngrammatically and semantically correct text. We investigate different\nstrategies for regularizing and optimizing the model and show that extensive\ndata augmentation and the initialization with pre-trained weights are required\nto achieve good performance. On the LibriSpeech benchmark, our method\ndemonstrates significant improvement in word error rate over the baseline\nacoustic model with greedy decoding, especially on much noisier dev-other and\ntest-other portions of the evaluation dataset. Our model also outperforms\nbaseline with 6-gram language model re-scoring and approaches the performance\nof re-scoring with Transformer-XL neural language model.", "published": "2019-10-23 17:57:11", "link": "http://arxiv.org/abs/1910.10697v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Hierarchical Transformers for Long Document Classification", "abstract": "BERT, which stands for Bidirectional Encoder Representations from\nTransformers, is a recently introduced language representation model based upon\nthe transfer learning paradigm. We extend its fine-tuning procedure to address\none of its major limitations - applicability to inputs longer than a few\nhundred words, such as transcripts of human call conversations. Our method is\nconceptually simple. We segment the input into smaller chunks and feed each of\nthem into the base model. Then, we propagate each output through a single\nrecurrent layer, or another transformer, followed by a softmax activation. We\nobtain the final classification decision after the last segment has been\nconsumed. We show that both BERT extensions are quick to fine-tune and converge\nafter as little as 1 epoch of training on a small, domain-specific data set. We\nsuccessfully apply them in three different tasks involving customer call\nsatisfaction prediction and topic classification, and obtain a significant\nimprovement over the baseline models in two of them.", "published": "2019-10-23 19:51:50", "link": "http://arxiv.org/abs/1910.10781v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Relation Module for Non-answerable Prediction on Question Answering", "abstract": "Machine reading comprehension(MRC) has attracted significant amounts of\nresearch attention recently, due to an increase of challenging reading\ncomprehension datasets. In this paper, we aim to improve a MRC model's ability\nto determine whether a question has an answer in a given context (e.g. the\nrecently proposed SQuAD 2.0 task). Our solution is a relation module that is\nadaptable to any MRC model. The relation module consists of both semantic\nextraction and relational information. We first extract high level semantics as\nobjects from both question and context with multi-head self-attentive pooling.\nThese semantic objects are then passed to a relation network, which generates\nrelationship scores for each object pair in a sentence. These scores are used\nto determine whether a question is non-answerable. We test the relation module\non the SQuAD 2.0 dataset using both BiDAF and BERT models as baseline readers.\nWe obtain 1.8% gain of F1 on top of the BiDAF reader, and 1.0% on top of the\nBERT base model. These results show the effectiveness of our relation module on\nMRC", "published": "2019-10-23 23:55:23", "link": "http://arxiv.org/abs/1910.10843v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Healthcare NER Models Using Language Model Pretraining", "abstract": "In this paper, we present our approach to extracting structured information\nfrom unstructured Electronic Health Records (EHR) [2] which can be used to, for\nexample, study adverse drug reactions in patients due to chemicals in their\nproducts. Our solution uses a combination of Natural Language Processing (NLP)\ntechniques and a web-based annotation tool to optimize the performance of a\ncustom Named Entity Recognition (NER) [1] model trained on a limited amount of\nEHR training data. This work was presented at the first Health Search and Data\nMining Workshop (HSDM 2020) [26]. We showcase a combination of tools and\ntechniques leveraging the recent advancements in NLP aimed at targeting domain\nshifts by applying transfer learning and language model pre-training techniques\n[3]. We present a comparison of our technique to the current popular approaches\nand show the effective increase in performance of the NER model and the\nreduction in time to annotate data.A key observation of the results presented\nis that the F1 score of model (0.734) trained with our approach with just 50%\nof available training data outperforms the F1 score of the blank spaCy model\nwithout language model component (0.704) trained with 100% of the available\ntraining data. We also demonstrate an annotation tool to minimize domain expert\ntime and the manual effort required to generate such a training dataset.\nFurther, we plan to release the annotated dataset as well as the pre-trained\nmodel to the community to further research in medical health records.", "published": "2019-10-23 07:37:14", "link": "http://arxiv.org/abs/1910.11241v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "H.3.3"], "primary_category": "cs.CL"}
{"title": "A context sensitive real-time Spell Checker with language adaptability", "abstract": "We present a novel language adaptable spell checking system which detects\nspelling errors and suggests context sensitive corrections in real-time. We\nshow that our system can be extended to new languages with minimal\nlanguage-specific processing. Available literature majorly discusses spell\ncheckers for English but there are no publicly available systems which can be\nextended to work for other languages out of the box. Most of the systems do not\nwork in real-time. We explain the process of generating a language's word\ndictionary and n-gram probability dictionaries using Wikipedia-articles data\nand manually curated video subtitles. We present the results of generating a\nlist of suggestions for a misspelled word. We also propose three approaches to\ncreate noisy channel datasets of real-world typographic errors. We compare our\nsystem with industry-accepted spell checker tools for 11 languages. Finally, we\nshow the performance of our system on synthetic datasets for 24 languages.", "published": "2019-10-23 15:00:39", "link": "http://arxiv.org/abs/1910.11242v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Generative Pre-Training for Speech with Autoregressive Predictive Coding", "abstract": "Learning meaningful and general representations from unannotated speech that\nare applicable to a wide range of tasks remains challenging. In this paper we\npropose to use autoregressive predictive coding (APC), a recently proposed\nself-supervised objective, as a generative pre-training approach for learning\nmeaningful, non-specific, and transferable speech representations. We pre-train\nAPC on large-scale unlabeled data and conduct transfer learning experiments on\nthree speech applications that require different information about speech\ncharacteristics to perform well: speech recognition, speech translation, and\nspeaker identification. Extensive experiments show that APC not only\noutperforms surface features (e.g., log Mel spectrograms) and other popular\nrepresentation learning methods on all three tasks, but is also effective at\nreducing downstream labeled data size and model parameters. We also investigate\nthe use of Transformers for modeling APC and find it superior to RNNs.", "published": "2019-10-23 15:28:51", "link": "http://arxiv.org/abs/1910.12607v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Suicidal Ideation Detection: A Review of Machine Learning Methods and\n  Applications", "abstract": "Suicide is a critical issue in modern society. Early detection and prevention\nof suicide attempts should be addressed to save people's life. Current suicidal\nideation detection methods include clinical methods based on the interaction\nbetween social workers or experts and the targeted individuals and machine\nlearning techniques with feature engineering or deep learning for automatic\ndetection based on online social contents. This paper is the first survey that\ncomprehensively introduces and discusses the methods from these categories.\nDomain-specific applications of suicidal ideation detection are reviewed\naccording to their data sources, i.e., questionnaires, electronic health\nrecords, suicide notes, and online user content. Several specific tasks and\ndatasets are introduced and summarized to facilitate further research. Finally,\nwe summarize the limitations of current work and provide an outlook of further\nresearch directions.", "published": "2019-10-23 02:10:42", "link": "http://arxiv.org/abs/1910.12611v4", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "TCT: A Cross-supervised Learning Method for Multimodal Sequence\n  Representation", "abstract": "Multimodalities provide promising performance than unimodality in most tasks.\nHowever, learning the semantic of the representations from multimodalities\nefficiently is extremely challenging. To tackle this, we propose the\nTransformer based Cross-modal Translator (TCT) to learn unimodal sequence\nrepresentations by translating from other related multimodal sequences on a\nsupervised learning method. Combined TCT with Multimodal Transformer Network\n(MTN), we evaluate MTN-TCT on the video-grounded dialogue which uses\nmultimodality. The proposed method reports new state-of-the-art performance on\nvideo-grounded dialogue which indicates representations learned by TCT are more\nsemantics compared to directly use unimodality.", "published": "2019-10-23 05:02:15", "link": "http://arxiv.org/abs/1911.05186v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CV"}
{"title": "End-to-end architectures for ASR-free spoken language understanding", "abstract": "Spoken Language Understanding (SLU) is the problem of extracting the meaning\nfrom speech utterances. It is typically addressed as a two-step problem, where\nan Automatic Speech Recognition (ASR) model is employed to convert speech into\ntext, followed by a Natural Language Understanding (NLU) model to extract\nmeaning from the decoded text. Recently, end-to-end approaches were emerged,\naiming at unifying the ASR and NLU into a single SLU deep neural architecture,\ntrained using combinations of ASR and NLU-level recognition units. In this\npaper, we explore a set of recurrent architectures for intent classification,\ntailored to the recently introduced Fluent Speech Commands (FSC) dataset, where\nintents are formed as combinations of three slots (action, object, and\nlocation). We show that by combining deep recurrent architectures with standard\ndata augmentation, state-of-the-art results can be attained, without using\nASR-level targets or pretrained ASR models. We also investigate its\ngeneralizability to new wordings, and we show that the model can perform\nreasonably well on wordings unseen during training.", "published": "2019-10-23 15:05:09", "link": "http://arxiv.org/abs/1910.10599v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-to-end Domain-Adversarial Voice Activity Detection", "abstract": "Voice activity detection is the task of detecting speech regions in a given\naudio stream or recording. First, we design a neural network combining\ntrainable filters and recurrent layers to tackle voice activity detection\ndirectly from the waveform. Experiments on the challenging DIHARD dataset show\nthat the proposed end-to-end model reaches state-of-the-art performance and\noutperforms a variant where trainable filters are replaced by standard cepstral\ncoefficients. Our second contribution aims at making the proposed voice\nactivity detection model robust to domain mismatch. To that end, a domain\nclassification branch is added to the network and trained in an adversarial\nmanner. The same DIHARD dataset, drawn from 11 different domains is used for\nevaluation under two scenarios. In the in-domain scenario where the training\nand test sets cover the exact same domains, we show that the domain-adversarial\napproach does not degrade performance of the proposed end-to-end model. In the\nout-domain scenario where the test domain is different from training domains,\nit brings a relative improvement of more than 10%. Finally, our last\ncontribution is the provision of a fully reproducible open-source pipeline than\ncan be easily adapted to other datasets.", "published": "2019-10-23 16:24:40", "link": "http://arxiv.org/abs/1910.10655v2", "categories": ["eess.AS", "I.2.7"], "primary_category": "eess.AS"}
{"title": "Zero-Shot Multi-Speaker Text-To-Speech with State-of-the-art Neural\n  Speaker Embeddings", "abstract": "While speaker adaptation for end-to-end speech synthesis using speaker\nembeddings can produce good speaker similarity for speakers seen during\ntraining, there remains a gap for zero-shot adaptation to unseen speakers. We\ninvestigate multi-speaker modeling for end-to-end text-to-speech synthesis and\nstudy the effects of different types of state-of-the-art neural speaker\nembeddings on speaker similarity for unseen speakers. Learnable dictionary\nencoding-based speaker embeddings with angular softmax loss can improve equal\nerror rates over x-vectors in a speaker verification task; these embeddings\nalso improve speaker similarity and naturalness for unseen speakers when used\nfor zero-shot adaptation to new speakers in end-to-end speech synthesis.", "published": "2019-10-23 23:28:21", "link": "http://arxiv.org/abs/1910.10838v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-to-End Multi-Task Denoising for the Joint Optimization of Perceptual\n  Speech Metrics", "abstract": "Although supervised learning based on a deep neural network has recently\nachieved substantial improvement on speech enhancement, the existing schemes\nhave either of two critical issues: spectrum or metric mismatches. The spectrum\nmismatch is a well known issue that any spectrum modification after short-time\nFourier transform (STFT), in general, cannot be fully recovered after inverse\nshort-time Fourier transform (ISTFT). The metric mismatch is that a\nconventional mean square error (MSE) loss function is typically sub-optimal to\nmaximize perceptual speech measure such as signal-to-distortion ratio (SDR),\nperceptual evaluation of speech quality (PESQ) and short-time objective\nintelligibility (STOI). This paper presents a new end-to-end denoising\nframework. First, the network optimization is performed on the time-domain\nsignals after ISTFT to avoid the spectrum mismatch. Second, three loss\nfunctions based on SDR, PESQ and STOI are proposed to minimize the metric\nmismatch. The experimental result showed the proposed denoising scheme\nsignificantly improved SDR, PESQ and STOI performance over the existing\nmethods. Moreover, the proposed scheme also provided good generalization\nperformance over generative denoising models on the perceptual speech metrics\nnot used as a loss function during training.", "published": "2019-10-23 04:16:47", "link": "http://arxiv.org/abs/1910.10707v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Low-frequency Compensated Synthetic Impulse Responses for Improved\n  Far-field Speech Recognition", "abstract": "We propose a method for generating low-frequency compensated synthetic\nimpulse responses that improve the performance of far-field speech recognition\nsystems trained on artificially augmented datasets. We design linear-phase\nfilters that adapt the simulated impulse responses to equalization\ndistributions corresponding to real-world captured impulse responses. Our\nfiltered synthetic impulse responses are then used to augment clean speech data\nfrom LibriSpeech dataset [1]. We evaluate the performance of our method on the\nreal-world LibriSpeech test set. In practice, our low-frequency compensated\nsynthetic dataset can reduce the word-error-rate by up to 8.8% for far-field\nspeech recognition.", "published": "2019-10-23 21:28:30", "link": "http://arxiv.org/abs/1910.10815v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Filterbank design for end-to-end speech separation", "abstract": "Single-channel speech separation has recently made great progress thanks to\nlearned filterbanks as used in ConvTasNet. In parallel, parameterized\nfilterbanks have been proposed for speaker recognition where only center\nfrequencies and bandwidths are learned. In this work, we extend real-valued\nlearned and parameterized filterbanks into complex-valued analytic filterbanks\nand define a set of corresponding representations and masking strategies. We\nevaluate these filterbanks on a newly released noisy speech separation dataset\n(WHAM). The results show that the proposed analytic learned filterbank\nconsistently outperforms the real-valued filterbank of ConvTasNet. Also, we\nvalidate the use of parameterized filterbanks and show that complex-valued\nrepresentations and masks are beneficial in all conditions. Finally, we show\nthat the STFT achieves its best performance for 2ms windows.", "published": "2019-10-23 08:08:26", "link": "http://arxiv.org/abs/1910.10400v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Fast Independent Vector Extraction by Iterative SINR Maximization", "abstract": "We propose fast independent vector extraction (FIVE), a new algorithm that\nblindly extracts a single non-Gaussian source from a Gaussian background. The\nalgorithm iteratively computes beamforming weights maximizing the\nsignal-to-interference-and-noise ratio for an approximate noise covariance\nmatrix. We demonstrate that this procedure minimizes the negative\nlog-likelihood of the input data according to a well-defined probabilistic\nmodel. The minimization is carried out via the auxiliary function technique\nwhereas, unlike related methods, the auxiliary function is globally minimized\nat every iteration. Numerical experiments are carried out to assess the\nperformance of FIVE. We find that it is vastly superior to competing methods in\nterms of convergence speed, and has high potential for real-time applications.", "published": "2019-10-23 16:24:25", "link": "http://arxiv.org/abs/1910.10654v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "A Comparative Study of Multilateration Methods for Single-Source\n  Localization in Distributed Audio", "abstract": "In this article we analyze the state-of-the-art in multilateration - the\nfamily of localization methods enabled by the range difference observations.\nThese methods are computationally efficient, signal-independent, and flexible\nwith regards to the number of sensing nodes and their spatial arrangement.\nHowever, the multilateration problem does not admit a closed-form solution in\nthe general case, and the localization performance is conditioned on the\naccuracy of range difference estimates. For that reason, we consider a\nsimplified use case where multiple distributed microphones capture the signal\ncoming from a near field sound source, and discuss their robustness to the\nestimation errors. In addition to surveying the relevant bibliography, we\npresent the results of a small-scale benchmark of few \"mainstream\"\nmultilateration algorithms, based on an in-house Room Impulse Response dataset.", "published": "2019-10-23 16:35:49", "link": "http://arxiv.org/abs/1910.10661v4", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Bootstrapping deep music separation from primitive auditory grouping\n  principles", "abstract": "Separating an audio scene such as a cocktail party into constituent,\nmeaningful components is a core task in computer audition. Deep networks are\nthe state-of-the-art approach. They are trained on synthetic mixtures of audio\nmade from isolated sound source recordings so that ground truth for the\nseparation is known. However, the vast majority of available audio is not\nisolated. The brain uses primitive cues that are independent of the\ncharacteristics of any particular sound source to perform an initial\nsegmentation of the audio scene. We present a method for bootstrapping a deep\nmodel for music source separation without ground truth by using multiple\nprimitive cues. We apply our method to train a network on a large set of\nunlabeled music recordings from YouTube to separate vocals from accompaniment\nwithout the need for ground truth isolated sources or artificial training\nmixtures.", "published": "2019-10-23 17:44:13", "link": "http://arxiv.org/abs/1910.11133v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Speech Emotion Recognition via Contrastive Loss under Siamese Networks", "abstract": "Speech emotion recognition is an important aspect of human-computer\ninteraction. Prior work proposes various end-to-end models to improve the\nclassification performance. However, most of them rely on the cross-entropy\nloss together with softmax as the supervision component, which does not\nexplicitly encourage discriminative learning of features. In this paper, we\nintroduce the contrastive loss function to encourage intra-class compactness\nand inter-class separability between learnable features. Furthermore, multiple\nfeature selection methods and pairwise sample selection methods are evaluated.\nTo verify the performance of the proposed system, we conduct experiments on The\nInteractive Emotional Dyadic Motion Capture (IEMOCAP) database, a common\nevaluation corpus. Experimental results reveal the advantages of the proposed\nmethod, which reaches 62.19% in the weighted accuracy and 63.21% in the\nunweighted accuracy. It outperforms the baseline system that is optimized\nwithout the contrastive loss function with 1.14% and 2.55% in the weighted\naccuracy and the unweighted accuracy, respectively.", "published": "2019-10-23 15:43:42", "link": "http://arxiv.org/abs/1910.11174v1", "categories": ["cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Model selection for deep audio source separation via clustering analysis", "abstract": "Audio source separation is the process of separating a mixture (e.g. a pop\nband recording) into isolated sounds from individual sources (e.g. just the\nlead vocals). Deep learning models are the state-of-the-art in source\nseparation, given that the mixture to be separated is similar to the mixtures\nthe deep model was trained on. This requires the end user to know enough about\neach model's training to select the correct model for a given audio mixture. In\nthis work, we automate selection of the appropriate model for an audio mixture.\nWe present a confidence measure that does not require ground truth to estimate\nseparation quality, given a deep model and audio mixture. We use this\nconfidence measure to automatically select the model output with the best\npredicted separation quality. We compare our confidence-based ensemble approach\nto using individual models with no selection, to an oracle that always selects\nthe best model and to a random model selector. Results show our\nconfidence-based ensemble significantly outperforms the random ensemble over\ngeneral mixtures and approaches oracle performance for music mixtures.", "published": "2019-10-23 18:09:31", "link": "http://arxiv.org/abs/1910.12626v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
