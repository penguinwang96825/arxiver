{"title": "Domain-Adaptive Text Classification with Structured Knowledge from\n  Unlabeled Data", "abstract": "Domain adaptive text classification is a challenging problem for the\nlarge-scale pretrained language models because they often require expensive\nadditional labeled data to adapt to new domains. Existing works usually fails\nto leverage the implicit relationships among words across domains. In this\npaper, we propose a novel method, called Domain Adaptation with Structured\nKnowledge (DASK), to enhance domain adaptation by exploiting word-level\nsemantic relationships. DASK first builds a knowledge graph to capture the\nrelationship between pivot terms (domain-independent words) and non-pivot terms\nin the target domain. Then during training, DASK injects pivot-related\nknowledge graph information into source domain texts. For the downstream task,\nthese knowledge-injected texts are fed into a BERT variant capable of\nprocessing knowledge-injected textual data. Thanks to the knowledge injection,\nour model learns domain-invariant features for non-pivots according to their\nrelationships with pivots. DASK ensures the pivots to have domain-invariant\nbehaviors by dynamically inferring via the polarity scores of candidate pivots\nduring training with pseudo-labels. We validate DASK on a wide range of\ncross-domain sentiment classification tasks and observe up to 2.9% absolute\nperformance improvement over baselines for 20 different domain pairs. Code will\nbe made available at https://github.com/hikaru-nara/DASK.", "published": "2022-06-20 06:38:51", "link": "http://arxiv.org/abs/2206.09591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPBERTQA: A Two-Stage Question Answering System Based on Sentence\n  Transformers for Medical Texts", "abstract": "Question answering (QA) systems have gained explosive attention in recent\nyears. However, QA tasks in Vietnamese do not have many datasets.\nSignificantly, there is mostly no dataset in the medical domain. Therefore, we\nbuilt a Vietnamese Healthcare Question Answering dataset (ViHealthQA),\nincluding 10,015 question-answer passage pairs for this task, in which\nquestions from health-interested users were asked on prestigious health\nwebsites and answers from highly qualified experts. This paper proposes a\ntwo-stage QA system based on Sentence-BERT (SBERT) using multiple negatives\nranking (MNR) loss combined with BM25. Then, we conduct diverse experiments\nwith many bag-of-words models to assess our system's performance. With the\nobtained results, this system achieves better performance than traditional\nmethods.", "published": "2022-06-20 07:07:59", "link": "http://arxiv.org/abs/2206.09600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Studying the role of named entities for content preservation in text\n  style transfer", "abstract": "Text style transfer techniques are gaining popularity in Natural Language\nProcessing, finding various applications such as text detoxification,\nsentiment, or formality transfer. However, the majority of the existing\napproaches were tested on such domains as online communications on public\nplatforms, music, or entertainment yet none of them were applied to the domains\nwhich are typical for task-oriented production systems, such as personal plans\narrangements (e.g. booking of flights or reserving a table in a restaurant). We\nfill this gap by studying formality transfer in this domain.\n  We noted that the texts in this domain are full of named entities, which are\nvery important for keeping the original sense of the text. Indeed, if for\nexample, someone communicates the destination city of a flight it must not be\naltered. Thus, we concentrate on the role of named entities in content\npreservation for formality text style transfer.\n  We collect a new dataset for the evaluation of content similarity measures in\ntext style transfer. It is taken from a corpus of task-oriented dialogues and\ncontains many important entities related to realistic requests that make this\ndataset particularly useful for testing style transfer models before using them\nin production. Besides, we perform an error analysis of a pre-trained formality\ntransfer model and introduce a simple technique to use information about named\nentities to enhance the performance of baseline content similarity measures\nused in text style transfer.", "published": "2022-06-20 09:31:47", "link": "http://arxiv.org/abs/2206.09676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Misspelling Semantics In Thai", "abstract": "User-generated content is full of misspellings. Rather than being just random\nnoise, we hypothesise that many misspellings contain hidden semantics that can\nbe leveraged for language understanding tasks. This paper presents a\nfine-grained annotated corpus of misspelling in Thai, together with an analysis\nof misspelling intention and its possible semantics to get a better\nunderstanding of the misspelling patterns observed in the corpus. In addition,\nwe introduce two approaches to incorporate the semantics of misspelling:\nMisspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST).\nExperiments on a sentiment analysis task confirm our overall hypothesis:\nadditional semantics from misspelling can boost the micro F1 score up to\n0.4-2%, while blindly normalising misspelling is harmful and suboptimal.", "published": "2022-06-20 09:42:50", "link": "http://arxiv.org/abs/2206.09680v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender\n  Bias", "abstract": "The size of pretrained models is increasing, and so is their performance on a\nvariety of NLP tasks. However, as their memorization capacity grows, they might\npick up more social biases. In this work, we examine the connection between\nmodel size and its gender bias (specifically, occupational gender bias). We\nmeasure bias in three masked language model families (RoBERTa, DeBERTa, and T5)\nin two setups: directly using prompt based method, and using a downstream task\n(Winogender). We find on the one hand that larger models receive higher bias\nscores on the former task, but when evaluated on the latter, they make fewer\ngender errors. To examine these potentially conflicting results, we carefully\ninvestigate the behavior of the different models on Winogender. We find that\nwhile larger models outperform smaller ones, the probability that their\nmistakes are caused by gender bias is higher. Moreover, we find that the\nproportion of stereotypical errors compared to anti-stereotypical ones grows\nwith the model size. Our findings highlight the potential risks that can arise\nfrom increasing model size.", "published": "2022-06-20 15:52:40", "link": "http://arxiv.org/abs/2206.09860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual HateCheck: Functional Tests for Multilingual Hate Speech\n  Detection Models", "abstract": "Hate speech detection models are typically evaluated on held-out test sets.\nHowever, this risks painting an incomplete and potentially misleading picture\nof model performance because of increasingly well-documented systematic gaps\nand biases in hate speech datasets. To enable more targeted diagnostic\ninsights, recent research has thus introduced functional tests for hate speech\ndetection models. However, these tests currently only exist for\nEnglish-language content, which means that they cannot support the development\nof more effective models in other languages spoken by billions across the\nworld. To help address this issue, we introduce Multilingual HateCheck (MHC), a\nsuite of functional tests for multilingual hate speech detection models. MHC\ncovers 34 functionalities across ten languages, which is more languages than\nany other hate speech dataset. To illustrate MHC's utility, we train and test a\nhigh-performing multilingual hate speech detection model, and reveal critical\nmodel weaknesses for monolingual and cross-lingual applications.", "published": "2022-06-20 17:54:39", "link": "http://arxiv.org/abs/2206.09917v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SynWMD: Syntax-aware Word Mover's Distance for Sentence Similarity\n  Evaluation", "abstract": "Word Mover's Distance (WMD) computes the distance between words and models\ntext similarity with the moving cost between words in two text sequences. Yet,\nit does not offer good performance in sentence similarity evaluation since it\ndoes not incorporate word importance and fails to take inherent contextual and\nstructural information in a sentence into account. An improved WMD method using\nthe syntactic parse tree, called Syntax-aware Word Mover's Distance (SynWMD),\nis proposed to address these two shortcomings in this work. First, a weighted\ngraph is built upon the word co-occurrence statistics extracted from the\nsyntactic parse trees of sentences. The importance of each word is inferred\nfrom graph connectivities. Second, the local syntactic parsing structure of\nwords is considered in computing the distance between words. To demonstrate the\neffectiveness of the proposed SynWMD, we conduct experiments on 6 textual\nsemantic similarity (STS) datasets and 4 sentence classification datasets.\nExperimental results show that SynWMD achieves state-of-the-art performance on\nSTS tasks. It also outperforms other WMD-based methods on sentence\nclassification tasks.", "published": "2022-06-20 22:30:07", "link": "http://arxiv.org/abs/2206.10029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient\n  Inference in Large-Scale Generative Language Models", "abstract": "Recent advances in self-supervised learning and the Transformer architecture\nhave significantly improved natural language processing (NLP), achieving\nremarkably low perplexity. However, the growing size of NLP models introduces a\nmemory wall problem during the generation phase. To mitigate this issue, recent\nefforts have focused on quantizing model weights to sub-4-bit precision while\npreserving full precision for activations, resulting in practical speed-ups\nduring inference on a single GPU. However, these improvements primarily stem\nfrom reduced memory movement, which necessitates a resource-intensive\ndequantization process rather than actual computational reduction. In this\npaper, we introduce LUT-GEMM, an efficient kernel for quantized matrix\nmultiplication, which not only eliminates the resource-intensive dequantization\nprocess but also reduces computational costs compared to previous kernels for\nweight-only quantization. Furthermore, we proposed group-wise quantization to\noffer a flexible trade-off between compression ratio and accuracy. The impact\nof LUT-GEMM is facilitated by implementing high compression ratios through\nlow-bit quantization and efficient LUT-based operations. We show experimentally\nthat when applied to the OPT-175B model with 3-bit quantization, LUT-GEMM\nsubstantially accelerates token generation latency, achieving a remarkable\n2.1$\\times$ improvement on a single GPU when compared to OPTQ, which relies on\nthe costly dequantization process.", "published": "2022-06-20 03:48:17", "link": "http://arxiv.org/abs/2206.09557v4", "categories": ["cs.DC", "cs.CL"], "primary_category": "cs.DC"}
{"title": "Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the\n  Research Manifold", "abstract": "The prototypical NLP experiment trains a standard architecture on labeled\nEnglish data and optimizes for accuracy, without accounting for other\ndimensions such as fairness, interpretability, or computational efficiency. We\nshow through a manual classification of recent NLP research papers that this is\nindeed the case and refer to it as the square one experimental setup. We\nobserve that NLP research often goes beyond the square one setup, e.g, focusing\nnot only on accuracy, but also on fairness or interpretability, but typically\nonly along a single dimension. Most work targeting multilinguality, for\nexample, considers only accuracy; most work on fairness or interpretability\nconsiders only English; and so on. We show this through manual classification\nof recent NLP research papers and ACL Test-of-Time award recipients. Such\none-dimensionality of most research means we are only exploring a fraction of\nthe NLP research search space. We provide historical and recent examples of how\nthe square one bias has led researchers to draw false conclusions or make\nunwise choices, point to promising yet unexplored directions on the research\nmanifold, and make practical recommendations to enable more multi-dimensional\nresearch. We open-source the results of our annotations to enable further\nanalysis at https://github.com/google-research/url-nlp", "published": "2022-06-20 13:04:23", "link": "http://arxiv.org/abs/2206.09755v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bilingual by default: Voice Assistants and the role of code-switching in\n  creating a bilingual user experience", "abstract": "Conversational User Interfaces such as Voice Assistants are hugely popular.\nYet they are designed to be monolingual by default, lacking support for, or\nsensitivity to, the bilingual dialogue experience. In this provocation paper,\nwe highlight the language production challenges faced in VA interaction for\nbilingual users. We argue that, by facilitating phenomena seen in bilingual\ninteraction, such as code-switching, we can foster a more inclusive and\nimproved user experience for bilingual users. We also explore ways that this\nmight be achieved, through the support of multiple language recognition as well\nas being sensitive to the preferences of code-switching in speech output.", "published": "2022-06-20 13:26:48", "link": "http://arxiv.org/abs/2206.09765v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "EAGER: Asking and Answering Questions for Automatic Reward Shaping in\n  Language-guided RL", "abstract": "Reinforcement learning (RL) in long horizon and sparse reward tasks is\nnotoriously difficult and requires a lot of training steps. A standard solution\nto speed up the process is to leverage additional reward signals, shaping it to\nbetter guide the learning process. In the context of language-conditioned RL,\nthe abstraction and generalisation properties of the language input provide\nopportunities for more efficient ways of shaping the reward. In this paper, we\nleverage this idea and propose an automated reward shaping method where the\nagent extracts auxiliary objectives from the general language goal. These\nauxiliary objectives use a question generation (QG) and question answering (QA)\nsystem: they consist of questions leading the agent to try to reconstruct\npartial information about the global goal using its own trajectory. When it\nsucceeds, it receives an intrinsic reward proportional to its confidence in its\nanswer. This incentivizes the agent to generate trajectories which\nunambiguously explain various aspects of the general language goal. Our\nexperimental study shows that this approach, which does not require engineer\nintervention to design the auxiliary objectives, improves sample efficiency by\neffectively directing exploration.", "published": "2022-06-20 09:29:13", "link": "http://arxiv.org/abs/2206.09674v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Great Expectations: Unsupervised Inference of Suspense, Surprise and\n  Salience in Storytelling", "abstract": "Stories interest us not because they are a sequence of mundane and\npredictable events but because they have drama and tension. Crucial to creating\ndramatic and exciting stories are surprise and suspense. The thesis trains a\nseries of deep learning models via only reading stories, a self-supervised (or\nunsupervised) system. Narrative theory methods (rules and procedures) are\napplied to the knowledge built into deep learning models to directly infer\nsalience, surprise, and salience in stories. Extensions add memory and external\nknowledge from story plots and from Wikipedia to infer salience on novels such\nas Great Expectations and plays such as Macbeth. Other work adapts the models\nas a planning system for generating original stories.\n  The thesis finds that applying the narrative theory to deep learning models\ncan align with the typical reader. In follow-up work, the insights could help\nimprove computer models for tasks such as automatic story writing and\nassistance for writing, summarising or editing stories. Moreover, the approach\nof applying narrative theory to the inherent qualities built in a system that\nlearns itself (self-supervised) from reading from books, watching videos, and\nlistening to audio is much cheaper and more adaptable to other domains and\ntasks. Progress is swift in improving self-supervised systems. As such, the\nthesis's relevance is that applying domain expertise with these systems may be\na more productive approach for applying machine learning in many areas of\ninterest.", "published": "2022-06-20 11:00:23", "link": "http://arxiv.org/abs/2206.09708v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Boosting Cross-Domain Speech Recognition with Self-Supervision", "abstract": "The cross-domain performance of automatic speech recognition (ASR) could be\nseverely hampered due to the mismatch between training and testing\ndistributions. Since the target domain usually lacks labeled data, and domain\nshifts exist at acoustic and linguistic levels, it is challenging to perform\nunsupervised domain adaptation (UDA) for ASR. Previous work has shown that\nself-supervised learning (SSL) or pseudo-labeling (PL) is effective in UDA by\nexploiting the self-supervisions of unlabeled data. However, these\nself-supervisions also face performance degradation in mismatched domain\ndistributions, which previous work fails to address. This work presents a\nsystematic UDA framework to fully utilize the unlabeled data with\nself-supervision in the pre-training and fine-tuning paradigm. On the one hand,\nwe apply continued pre-training and data replay techniques to mitigate the\ndomain mismatch of the SSL pre-trained model. On the other hand, we propose a\ndomain-adaptive fine-tuning approach based on the PL technique with three\nunique modifications: Firstly, we design a dual-branch PL method to decrease\nthe sensitivity to the erroneous pseudo-labels; Secondly, we devise an\nuncertainty-aware confidence filtering strategy to improve pseudo-label\ncorrectness; Thirdly, we introduce a two-step PL approach to incorporate target\ndomain linguistic knowledge, thus generating more accurate target domain\npseudo-labels. Experimental results on various cross-domain scenarios\ndemonstrate that the proposed approach effectively boosts the cross-domain\nperformance and significantly outperforms previous approaches.", "published": "2022-06-20 14:02:53", "link": "http://arxiv.org/abs/2206.09783v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Makerere Radio Speech Corpus: A Luganda Radio Corpus for Automatic\n  Speech Recognition", "abstract": "Building a usable radio monitoring automatic speech recognition (ASR) system\nis a challenging task for under-resourced languages and yet this is paramount\nin societies where radio is the main medium of public communication and\ndiscussions. Initial efforts by the United Nations in Uganda have proved how\nunderstanding the perceptions of rural people who are excluded from social\nmedia is important in national planning. However, these efforts are being\nchallenged by the absence of transcribed speech datasets. In this paper, The\nMakerere Artificial Intelligence research lab releases a Luganda radio speech\ncorpus of 155 hours. To our knowledge, this is the first publicly available\nradio dataset in sub-Saharan Africa. The paper describes the development of the\nvoice corpus and presents baseline Luganda ASR performance results using Coqui\nSTT toolkit, an open source speech recognition toolkit.", "published": "2022-06-20 14:19:35", "link": "http://arxiv.org/abs/2206.09790v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "WOLONet: Wave Outlooker for Efficient and High Fidelity Speech Synthesis", "abstract": "Recently, GAN-based neural vocoders such as Parallel WaveGAN, MelGAN,\nHiFiGAN, and UnivNet have become popular due to their lightweight and parallel\nstructure, resulting in a real-time synthesized waveform with high fidelity,\neven on a CPU. HiFiGAN and UnivNet are two SOTA vocoders. Despite their high\nquality, there is still room for improvement. In this paper, motivated by the\nstructure of Vision Outlooker from computer vision, we adopt a similar idea and\npropose an effective and lightweight neural vocoder called WOLONet. In this\nnetwork, we develop a novel lightweight block that uses a location-variable,\nchannel-independent, and depthwise dynamic convolutional kernel with\nsinusoidally activated dynamic kernel weights. To demonstrate the effectiveness\nand generalizability of our method, we perform an ablation study to verify our\nnovel design and make a subjective and objective comparison with typical\nGAN-based vocoders. The results show that our WOLONet achieves the best\ngeneration quality while requiring fewer parameters than the two neural SOTA\nvocoders, HiFiGAN and UnivNet.", "published": "2022-06-20 17:58:52", "link": "http://arxiv.org/abs/2206.09920v1", "categories": ["cs.SD", "cs.AI", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Open Set Classification of Untranscribed Handwritten Documents", "abstract": "Huge amounts of digital page images of important manuscripts are preserved in\narchives worldwide. The amounts are so large that it is generally unfeasible\nfor archivists to adequately tag most of the documents with the required\nmetadata so as to low proper organization of the archives and effective\nexploration by scholars and the general public. The class or ``typology'' of a\ndocument is perhaps the most important tag to be included in the metadata. The\ntechnical problem is one of automatic classification of documents, each\nconsisting of a set of untranscribed handwritten text images, by the textual\ncontents of the images. The approach considered is based on ``probabilistic\nindexing'', a relatively novel technology which allows to effectively represent\nthe intrinsic word-level uncertainty exhibited by handwritten text images. We\nassess the performance of this approach on a large collection of complex\nnotarial manuscripts from the Spanish Archivo Host\\'orico Provincial de\nC\\'adiz, with promising results.", "published": "2022-06-20 20:43:50", "link": "http://arxiv.org/abs/2206.13342v1", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-channel end-to-end neural network for speech enhancement, source\n  localization, and voice activity detection", "abstract": "Speech enhancement and source localization has been active research for\nseveral decades with a wide range of real-world applications. Recently, the\nDeep Complex Convolution Recurrent network (DCCRN) has yielded impressive\nenhancement performance for single-channel systems. In this study, a neural\nbeamformer consisting of a beamformer and a novel multi-channel DCCRN is\nproposed for speech enhancement and source localization. Complex-valued filters\nestimated by the multi-channel DCCRN serve as the weights of beamformer. In\naddition, a one-stage learning-based procedure is employed for speech\nenhancement and source localization. The proposed network composed of the\nmulti-channel DCCRN and the auxiliary network models the sound field, while\nminimizing the distortionless response loss function. Simulation results show\nthat the proposed neural beamformer is effective in enhancing speech signals,\nwith speech quality well preserved. The proposed neural beamformer also\nprovides source localization and voice activity detection (VAD) functions.", "published": "2022-06-20 11:53:40", "link": "http://arxiv.org/abs/2206.09728v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Step Towards Preserving Speakers' Identity While Detecting Depression\n  Via Speaker Disentanglement", "abstract": "Preserving a patient's identity is a challenge for automatic, speech-based\ndiagnosis of mental health disorders. In this paper, we address this issue by\nproposing adversarial disentanglement of depression characteristics and speaker\nidentity. The model used for depression classification is trained in a\nspeaker-identity-invariant manner by minimizing depression prediction loss and\nmaximizing speaker prediction loss during training. The effectiveness of the\nproposed method is demonstrated on two datasets - DAIC-WOZ (English) and\nCONVERGE (Mandarin), with three feature sets (Mel-spectrograms, raw-audio\nsignals, and the last-hidden-state of Wav2vec2.0), using a modified DepAudioNet\nmodel. With adversarial training, depression classification improves for every\nfeature when compared to the baseline. Wav2vec2.0 features with adversarial\nlearning resulted in the best performance (F1-score of 69.2% for DAIC-WOZ and\n91.5% for CONVERGE). Analysis of the class-separability measure (J-ratio) of\nthe hidden states of the DepAudioNet model shows that when adversarial learning\nis applied, the backend model loses some speaker-discriminability while it\nimproves depression-discriminability. These results indicate that there are\nsome components of speaker identity that may not be useful for depression\ndetection and minimizing their effects provides a more accurate diagnosis of\nthe underlying disorder and can safeguard a speaker's identity.", "published": "2022-06-20 01:45:54", "link": "http://arxiv.org/abs/2206.09530v2", "categories": ["eess.AS", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Towards Trustworthy Edge Intelligence: Insights from Voice-Activated\n  Services", "abstract": "In an age of surveillance capitalism, anchoring the design of emerging smart\nservices in trustworthiness is urgent and important. Edge Intelligence, which\nbrings together the fields of AI and Edge computing, is a key enabling\ntechnology for smart services. Trustworthy Edge Intelligence should thus be a\npriority research concern. However, determining what makes Edge Intelligence\ntrustworthy is not straight forward. This paper examines requirements for\ntrustworthy Edge Intelligence in a concrete application scenario of\nvoice-activated services. We contribute to deepening the understanding of\ntrustworthiness in the emerging Edge Intelligence domain in three ways:\nfirstly, we propose a unified framing for trustworthy Edge Intelligence that\njointly considers trustworthiness attributes of AI and the IoT. Secondly, we\npresent research outputs of a tangible case study in voice-activated services\nthat demonstrates interdependencies between three important trustworthiness\nattributes: privacy, security and fairness. Thirdly, based on the empirical and\nanalytical findings, we highlight challenges and open questions that present\nimportant future research areas for trustworthy Edge Intelligence.", "published": "2022-06-20 00:56:21", "link": "http://arxiv.org/abs/2206.09523v1", "categories": ["eess.AS", "cs.CY", "cs.SD"], "primary_category": "eess.AS"}
{"title": "COVYT: Introducing the Coronavirus YouTube and TikTok speech dataset\n  featuring the same speakers with and without infection", "abstract": "More than two years after its outbreak, the COVID-19 pandemic continues to\nplague medical systems around the world, putting a strain on scarce resources,\nand claiming human lives. From the very beginning, various AI-based COVID-19\ndetection and monitoring tools have been pursued in an attempt to stem the tide\nof infections through timely diagnosis. In particular, computer audition has\nbeen suggested as a non-invasive, cost-efficient, and eco-friendly alternative\nfor detecting COVID-19 infections through vocal sounds. However, like all AI\nmethods, also computer audition is heavily dependent on the quantity and\nquality of available data, and large-scale COVID-19 sound datasets are\ndifficult to acquire -- amongst other reasons -- due to the sensitive nature of\nsuch data. To that end, we introduce the COVYT dataset -- a novel COVID-19\ndataset collected from public sources containing more than 8 hours of speech\nfrom 65 speakers. As compared to other existing COVID-19 sound datasets, the\nunique feature of the COVYT dataset is that it comprises both COVID-19 positive\nand negative samples from all 65 speakers. We analyse the acoustic\nmanifestation of COVID-19 on the basis of these perfectly speaker\ncharacteristic balanced `in-the-wild' data using interpretable audio\ndescriptors, and investigate several classification scenarios that shed light\ninto proper partitioning strategies for a fair speech-based COVID-19 detection.", "published": "2022-06-20 16:26:51", "link": "http://arxiv.org/abs/2206.11045v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Comprehensive Survey on Video Saliency Detection with Auditory\n  Information: the Audio-visual Consistency Perceptual is the Key!", "abstract": "Video saliency detection (VSD) aims at fast locating the most attractive\nobjects/things/patterns in a given video clip. Existing VSD-related works have\nmainly relied on the visual system but paid less attention to the audio aspect,\nwhile, actually, our audio system is the most vital complementary part to our\nvisual system. Also, audio-visual saliency detection (AVSD), one of the most\nrepresentative research topics for mimicking human perceptual mechanisms, is\ncurrently in its infancy, and none of the existing survey papers have touched\non it, especially from the perspective of saliency detection. Thus, the\nultimate goal of this paper is to provide an extensive review to bridge the gap\nbetween audio-visual fusion and saliency detection. In addition, as another\nhighlight of this review, we have provided a deep insight into key factors\nwhich could directly determine the performances of AVSD deep models, and we\nclaim that the audio-visual consistency degree (AVC) -- a long-overlooked\nissue, can directly influence the effectiveness of using audio to benefit its\nvisual counterpart when performing saliency detection. Moreover, in order to\nmake the AVC issue more practical and valuable for future followers, we have\nnewly equipped almost all existing publicly available AVSD datasets with\nadditional frame-wise AVC labels. Based on these upgraded datasets, we have\nconducted extensive quantitative evaluations to ground our claim on the\nimportance of AVC in the AVSD task. In a word, both our ideas and new sets\nserve as a convenient platform with preliminaries and guidelines, all of which\nare very potential to facilitate future works in promoting state-of-the-art\n(SOTA) performance further.", "published": "2022-06-20 07:25:13", "link": "http://arxiv.org/abs/2206.13390v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "An Empirical Analysis on the Vulnerabilities of End-to-End Speech\n  Segregation Models", "abstract": "End-to-end learning models have demonstrated a remarkable capability in\nperforming speech segregation. Despite their wide-scope of real-world\napplications, little is known about the mechanisms they employ to group and\nconsequently segregate individual speakers. Knowing that harmonicity is a\ncritical cue for these networks to group sources, in this work, we perform a\nthorough investigation on ConvTasnet and DPT-Net to analyze how they perform a\nharmonic analysis of the input mixture. We perform ablation studies where we\napply low-pass, high-pass, and band-stop filters of varying pass-bands to\nempirically analyze the harmonics most critical for segregation. We also\ninvestigate how these networks decide which output channel to assign to an\nestimated source by introducing discontinuities in synthetic mixtures. We find\nthat end-to-end networks are highly unstable, and perform poorly when\nconfronted with deformations which are imperceptible to humans. Replacing the\nencoder in these networks with a spectrogram leads to lower overall\nperformance, but much higher stability. This work helps us to understand what\ninformation these network rely on for speech segregation, and exposes two\nsources of generalization-errors. It also pinpoints the encoder as the part of\nthe network responsible for these errors, allowing for a redesign with expert\nknowledge or transfer learning.", "published": "2022-06-20 03:46:47", "link": "http://arxiv.org/abs/2206.09556v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
