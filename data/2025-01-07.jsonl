{"title": "In-Sample and Out-of-Sample Sharpe Ratios for Linear Predictive Models", "abstract": "We study how much the in-sample performance of trading strategies based on\nlinear predictive models is reduced out-of-sample due to overfitting. More\nspecifically, we compute the in- and out-of-sample means and variances of the\ncorresponding PnLs and use these to derive a closed-form approximation for the\ncorresponding Sharpe ratios. We find that the out-of-sample ``replication\nratio'' diminishes for complex strategies with many assets and based on many\nweak rather than a few strong trading signals, and increases when more training\ndata is used. The substantial quantitative importance of these effects is\nillustrated with an empirical case study for commodity futures following the\nmethodology of Garleanu-Pedersen.", "published": "2025-01-07 17:00:14", "link": "http://arxiv.org/abs/2501.03938v1", "categories": ["q-fin.MF", "q-fin.PM"], "primary_category": "q-fin.MF"}
{"title": "Market Making with Fads, Informed, and Uninformed Traders", "abstract": "We characterise the solutions to a continuous-time optimal liquidity\nprovision problem in a market populated by informed and uninformed traders. In\nour model, the asset price exhibits fads -- these are short-term deviations\nfrom the fundamental value of the asset. Conditional on the value of the fad,\nwe model how informed traders and uninformed traders arrive in the market. The\nmarket maker knows of the two groups of traders but only observes the anonymous\norder arrivals. We study both, the complete information and the partial\ninformation versions of the control problem faced by the market maker. In such\nframeworks, we characterise the value of information, and we find the price of\nliquidity as a function of the proportion of informed traders in the market.\nLastly, for the partial information setup, we explore how to go beyond the\nKalman-Bucy filter to extract information about the fad from the market\narrivals.", "published": "2025-01-07 09:46:50", "link": "http://arxiv.org/abs/2501.03658v2", "categories": ["q-fin.TR", "q-fin.MF"], "primary_category": "q-fin.TR"}
{"title": "Finding A Voice: Evaluating African American Dialect Generation for\n  Chatbot Technology", "abstract": "As chatbots become increasingly integrated into everyday tasks, designing\nsystems that accommodate diverse user populations is crucial for fostering\ntrust, engagement, and inclusivity. This study investigates the ability of\ncontemporary Large Language Models (LLMs) to generate African American\nVernacular English (AAVE) and evaluates the impact of AAVE usage on user\nexperiences in chatbot applications. We analyze the performance of three LLM\nfamilies (Llama, GPT, and Claude) in producing AAVE-like utterances at varying\ndialect intensities and assess user preferences across multiple domains,\nincluding healthcare and education. Despite LLMs' proficiency in generating\nAAVE-like language, findings indicate that AAVE-speaking users prefer Standard\nAmerican English (SAE) chatbots, with higher levels of AAVE correlating with\nlower ratings for a variety of characteristics, including chatbot\ntrustworthiness and role appropriateness. These results highlight the\ncomplexities of creating inclusive AI systems and underscore the need for\nfurther exploration of diversity to enhance human-computer interactions.", "published": "2025-01-07 00:07:01", "link": "http://arxiv.org/abs/2501.03441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ISSR: Iterative Selection with Self-Review for Vocabulary Test\n  Distractor Generation", "abstract": "Vocabulary acquisition is essential to second language learning, as it\nunderpins all core language skills. Accurate vocabulary assessment is\nparticularly important in standardized exams, where test items evaluate\nlearners' comprehension and contextual use of words. Previous research has\nexplored methods for generating distractors to aid in the design of English\nvocabulary tests. However, current approaches often rely on lexical databases\nor predefined rules, and frequently produce distractors that risk invalidating\nthe question by introducing multiple correct options. In this study, we focus\non English vocabulary questions from Taiwan's university entrance exams. We\nanalyze student response distributions to gain insights into the\ncharacteristics of these test items and provide a reference for future\nresearch. Additionally, we identify key limitations in how large language\nmodels (LLMs) support teachers in generating distractors for vocabulary test\ndesign. To address these challenges, we propose the iterative selection with\nself-review (ISSR) framework, which makes use of a novel LLM-based self-review\nmechanism to ensure that the distractors remain valid while offering diverse\noptions. Experimental results show that ISSR achieves promising performance in\ngenerating plausible distractors, and the self-review mechanism effectively\nfilters out distractors that could invalidate the question.", "published": "2025-01-07 01:45:02", "link": "http://arxiv.org/abs/2501.03462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reveal about the Socio-Cultural Norms", "abstract": "Honorifics serve as powerful linguistic markers that reflect social\nhierarchies and cultural values. This paper presents a large-scale,\ncross-linguistic exploration of usage of honorific pronouns in Bengali and\nHindi Wikipedia articles, shedding light on how socio-cultural factors shape\nlanguage. Using LLM (GPT-4o), we annotated 10, 000 articles of real and\nfictional beings in each language for several sociodemographic features such as\ngender, age, fame, and exoticness, and the use of honorifics. We find that\nacross all feature combinations, use of honorifics is consistently more common\nin Bengali than Hindi. For both languages, the use non-honorific pronouns is\nmore commonly observed for infamous, juvenile, and exotic beings. Notably, we\nobserve a gender bias in use of honorifics in Hindi, with men being more\ncommonly referred to with honorifics than women.", "published": "2025-01-07 02:47:59", "link": "http://arxiv.org/abs/2501.03479v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sequential Optimal Learning Approach to Automated Prompt Engineering\n  in Large Language Models", "abstract": "Designing effective prompts is essential to guiding large language models\n(LLMs) toward desired responses. Automated prompt engineering aims to reduce\nreliance on manual effort by streamlining the design, refinement, and\noptimization of natural language prompts. This paper proposes an optimal\nlearning framework for automated prompt engineering, designed to sequentially\nidentify effective prompt features while efficiently allocating a limited\nevaluation budget. We introduce a feature-based method to express prompts,\nwhich significantly broadens the search space. Bayesian regression is employed\nto utilize correlations among similar prompts, accelerating the learning\nprocess. To efficiently explore the large space of prompt features for a high\nquality prompt, we adopt the forward-looking Knowledge-Gradient (KG) policy for\nsequential optimal learning. The KG policy is computed efficiently by solving\nmixed-integer second-order cone optimization problems, making it scalable and\ncapable of accommodating prompts characterized only through constraints. We\ndemonstrate that our method significantly outperforms a set of benchmark\nstrategies assessed on instruction induction tasks. The results highlight the\nadvantages of using the KG policy for prompt learning given a limited\nevaluation budget. Our framework provides a solution to deploying automated\nprompt engineering in a wider range applications where prompt evaluation is\ncostly.", "published": "2025-01-07 03:51:10", "link": "http://arxiv.org/abs/2501.03508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual\n  Information in Long-form Text Generation", "abstract": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs.", "published": "2025-01-07 05:43:23", "link": "http://arxiv.org/abs/2501.03545v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BabyLMs for isiXhosa: Data-Efficient Language Modelling in a\n  Low-Resource Context", "abstract": "The BabyLM challenge called on participants to develop sample-efficient\nlanguage models. Submissions were pretrained on a fixed English corpus, limited\nto the amount of words children are exposed to in development (<100m). The\nchallenge produced new architectures for data-efficient language modelling,\nwhich outperformed models trained on trillions of words. This is promising for\nlow-resource languages, where available corpora are limited to much less than\n100m words. In this paper, we explore the potential of BabyLMs for low-resource\nlanguages, using the isiXhosa language as a case study. We pretrain two BabyLM\narchitectures, ELC-BERT and MLSM, on an isiXhosa corpus. They outperform a\nvanilla pretrained model on POS tagging and NER, achieving notable gains (+3.2\nF1) for the latter. In some instances, the BabyLMs even outperform XLM-R. Our\nfindings show that data-efficient models are viable for low-resource languages,\nbut highlight the continued importance, and lack of, high-quality pretraining\ndata. Finally, we visually analyse how BabyLM architectures encode isiXhosa.", "published": "2025-01-07 15:13:45", "link": "http://arxiv.org/abs/2501.03855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressive Document-level Text Simplification via Large Language Models", "abstract": "Research on text simplification has primarily focused on lexical and\nsentence-level changes. Long document-level simplification (DS) is still\nrelatively unexplored. Large Language Models (LLMs), like ChatGPT, have\nexcelled in many natural language processing tasks. However, their performance\non DS tasks is unsatisfactory, as they often treat DS as merely document\nsummarization. For the DS task, the generated long sequences not only must\nmaintain consistency with the original document throughout, but complete\nmoderate simplification operations encompassing discourses, sentences, and\nword-level simplifications. Human editors employ a hierarchical complexity\nsimplification strategy to simplify documents. This study delves into\nsimulating this strategy through the utilization of a multi-stage collaboration\nusing LLMs. We propose a progressive simplification method (ProgDS) by\nhierarchically decomposing the task, including the discourse-level,\ntopic-level, and lexical-level simplification. Experimental results demonstrate\nthat ProgDS significantly outperforms existing smaller models or direct\nprompting with LLMs, advancing the state-of-the-art in the document\nsimplification task.", "published": "2025-01-07 15:14:37", "link": "http://arxiv.org/abs/2501.03857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Dialectal Slot and Intent Detection with Auxiliary Tasks: A\n  Multi-Dialectal Bavarian Case Study", "abstract": "Reliable slot and intent detection (SID) is crucial in natural language\nunderstanding for applications like digital assistants. Encoder-only\ntransformer models fine-tuned on high-resource languages generally perform well\non SID. However, they struggle with dialectal data, where no standardized form\nexists and training data is scarce and costly to produce. We explore zero-shot\ntransfer learning for SID, focusing on multiple Bavarian dialects, for which we\nrelease a new dataset for the Munich dialect. We evaluate models trained on\nauxiliary tasks in Bavarian, and compare joint multi-task learning with\nintermediate-task training. We also compare three types of auxiliary tasks:\ntoken-level syntactic tasks, named entity recognition (NER), and language\nmodelling. We find that the included auxiliary tasks have a more positive\neffect on slot filling than intent classification (with NER having the most\npositive effect), and that intermediate-task training yields more consistent\nperformance gains. Our best-performing approach improves intent classification\nperformance on Bavarian dialects by 5.1 and slot filling F1 by 8.4 percentage\npoints.", "published": "2025-01-07 15:21:07", "link": "http://arxiv.org/abs/2501.03863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Add Noise, Tasks, or Layers? MaiNLP at the VarDial 2025 Shared Task on\n  Norwegian Dialectal Slot and Intent Detection", "abstract": "Slot and intent detection (SID) is a classic natural language understanding\ntask. Despite this, research has only more recently begun focusing on SID for\ndialectal and colloquial varieties. Many approaches for low-resource scenarios\nhave not yet been applied to dialectal SID data, or compared to each other on\nthe same datasets. We participate in the VarDial 2025 shared task on slot and\nintent detection in Norwegian varieties, and compare multiple set-ups: varying\nthe training data (English, Norwegian, or dialectal Norwegian), injecting\ncharacter-level noise, training on auxiliary tasks, and applying Layer\nSwapping, a technique in which layers of models fine-tuned on different\ndatasets are assembled into a model. We find noise injection to be beneficial\nwhile the effects of auxiliary tasks are mixed. Though some experimentation was\nrequired to successfully assemble a model from layers, it worked surprisingly\nwell; a combination of models trained on English and small amounts of dialectal\ndata produced the most robust slot predictions. Our best models achieve 97.6%\nintent accuracy and 85.6% slot F1 in the shared task.", "published": "2025-01-07 15:36:35", "link": "http://arxiv.org/abs/2501.03870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AlphaPO -- Reward shape matters for LLM alignment", "abstract": "Reinforcement Learning with Human Feedback (RLHF) and its variants have made\nhuge strides toward the effective alignment of large language models (LLMs) to\nfollow instructions and reflect human values. More recently, Direct Alignment\nAlgorithms (DAAs) have emerged in which the reward modeling stage of RLHF is\nskipped by characterizing the reward directly as a function of the policy being\nlearned. Some popular examples of DAAs include Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO). These methods often suffer\nfrom likelihood displacement, a phenomenon by which the probabilities of\npreferred responses are often reduced undesirably.\n  In this paper, we argue that, for DAAs the reward (function) shape matters.\nWe introduce \\textbf{AlphaPO}, a new DAA method that leverages an\n$\\alpha$-parameter to help change the shape of the reward function beyond the\nstandard log reward. AlphaPO helps maintain fine-grained control over\nlikelihood displacement and over-optimization. Compared to SimPO, one of the\nbest performing DAAs, AlphaPO leads to about 7\\% to 10\\% relative improvement\nin alignment performance for the instruct versions of Mistral-7B and Llama3-8B\nwhile achieving 15\\% to 50\\% relative improvement over DPO on the same models.\nThe analysis and results presented highlight the importance of the reward\nshape, and how one can systematically change it to affect training dynamics, as\nwell as improve alignment performance.", "published": "2025-01-07 15:46:42", "link": "http://arxiv.org/abs/2501.03884v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantically Cohesive Word Grouping in Indian Languages", "abstract": "Indian languages are inflectional and agglutinative and typically follow\nclause-free word order. The structure of sentences across most major Indian\nlanguages are similar when their dependency parse trees are considered. While\nsome differences in the parsing structure occur due to peculiarities of a\nlanguage or its preferred natural way of conveying meaning, several apparent\ndifferences are simply due to the granularity of representation of the smallest\nsemantic unit of processing in a sentence. The semantic unit is typically a\nword, typographically separated by whitespaces. A single whitespace-separated\nword in one language may correspond to a group of words in another. Hence,\ngrouping of words based on semantics helps unify the parsing structure of\nparallel sentences across languages and, in the process, morphology. In this\nwork, we propose word grouping as a major preprocessing step for any\ncomputational or linguistic processing of sentences for Indian languages. Among\nIndian languages, since Hindi is one of the least agglutinative, we expect it\nto benefit the most from word-grouping. Hence, in this paper, we focus on Hindi\nto study the effects of grouping. We perform quantitative assessment of our\nproposal with an intrinsic method that perturbs sentences by shuffling words as\nwell as an extrinsic evaluation that verifies the importance of word grouping\nfor the task of Machine Translation (MT) using decomposed prompting. We also\nqualitatively analyze certain aspects of the syntactic structure of sentences.\nOur experiments and analyses show that the proposed grouping technique brings\nuniformity in the syntactic structures, as well as aids underlying NLP tasks.", "published": "2025-01-07 18:46:17", "link": "http://arxiv.org/abs/2501.03988v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Influences on LLM Calibration: A Study of Response Agreement, Loss\n  Functions, and Prompt Styles", "abstract": "Calibration, the alignment between model confidence and prediction accuracy,\nis critical for the reliable deployment of large language models (LLMs).\nExisting works neglect to measure the generalization of their methods to other\nprompt styles and different sizes of LLMs. To address this, we define a\ncontrolled experimental setting covering 12 LLMs and four prompt styles. We\nadditionally investigate if incorporating the response agreement of multiple\nLLMs and an appropriate loss function can improve calibration performance.\nConcretely, we build Calib-n, a novel framework that trains an auxiliary model\nfor confidence estimation that aggregates responses from multiple LLMs to\ncapture inter-model agreement. To optimize calibration, we integrate focal and\nAUC surrogate losses alongside binary cross-entropy. Experiments across four\ndatasets demonstrate that both response agreement and focal loss improve\ncalibration from baselines. We find that few-shot prompts are the most\neffective for auxiliary model-based methods, and auxiliary models demonstrate\nrobust calibration performance across accuracy variations, outperforming LLMs'\ninternal probabilities and verbalized confidences. These insights deepen the\nunderstanding of influence factors in LLM calibration, supporting their\nreliable deployment in diverse applications.", "published": "2025-01-07 18:48:42", "link": "http://arxiv.org/abs/2501.03991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Yeah Right!\" -- Do LLMs Exhibit Multimodal Feature Transfer?", "abstract": "Human communication is a multifaceted and multimodal skill. Communication\nrequires an understanding of both the surface-level textual content and the\nconnotative intent of a piece of communication. In humans, learning to go\nbeyond the surface level starts by learning communicative intent in speech.\nOnce humans acquire these skills in spoken communication, they transfer those\nskills to written communication. In this paper, we assess the ability of\nspeech+text models and text models trained with special emphasis on\nhuman-to-human conversations to make this multimodal transfer of skill. We\nspecifically test these models on their ability to detect covert deceptive\ncommunication. We find that with no special prompting speech+text LLMs have an\nadvantage over unimodal LLMs in performing this task. Likewise, we find that\nhuman-to-human conversation-trained LLMs are also advantaged in this skill.", "published": "2025-01-07 20:57:59", "link": "http://arxiv.org/abs/2501.04138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text to Band Gap: Pre-trained Language Models as Encoders for\n  Semiconductor Band Gap Prediction", "abstract": "In this study, we explore the use of a transformer-based language model as an\nencoder to predict the band gaps of semiconductor materials directly from their\ntext descriptions. Quantum chemistry simulations, including Density Functional\nTheory (DFT), are computationally intensive and time-consuming, which limits\ntheir practicality for high-throughput material screening, particularly for\ncomplex systems. Shallow machine learning (ML) models, while effective, often\nrequire extensive data preprocessing to convert non-numerical material\nproperties into numerical inputs. In contrast, our approach leverages textual\ndata directly, bypassing the need for complex feature engineering. We generate\nmaterial descriptions in two formats: formatted strings combining features and\nnatural language text generated using the ChatGPT API. We demonstrate that the\nRoBERTa model, pre-trained on natural language processing tasks, performs\neffectively as an encoder for prediction tasks. With minimal fine-tuning, it\nachieves a mean absolute error (MAE) of approximately 0.33 eV, performing\nbetter than shallow machine learning models such as Support Vector Regression,\nRandom Forest, and XGBoost. Even when only the linear regression head is\ntrained while keeping the RoBERTa encoder layers frozen, the accuracy remains\nnearly identical to that of the fully trained model. This demonstrates that the\npre-trained RoBERTa encoder is highly adaptable for processing domain-specific\ntext related to material properties, such as the band gap, significantly\nreducing the need for extensive retraining. This study highlights the potential\nof transformer-based language models to serve as efficient and versatile\nencoders for semiconductor materials property prediction tasks.", "published": "2025-01-07 00:56:26", "link": "http://arxiv.org/abs/2501.03456v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "MTRAG: A Multi-Turn Conversational Benchmark for Evaluating\n  Retrieval-Augmented Generation Systems", "abstract": "Retrieval-augmented generation (RAG) has recently become a very popular task\nfor Large Language Models (LLMs). Evaluating them on multi-turn RAG\nconversations, where the system is asked to generate a response to a question\nin the context of a preceding conversation is an important and often overlooked\ntask with several additional challenges. We present MTRAG: an end-to-end\nhuman-generated multi-turn RAG benchmark that reflects several real-world\nproperties across diverse dimensions for evaluating the full RAG pipeline.\nMTRAG contains 110 conversations averaging 7.7 turns each across four domains\nfor a total of 842 tasks. We also explore automation paths via synthetic data\nand LLM-as-a-Judge evaluation. Our human and automatic evaluations show that\neven state-of-the-art LLM RAG systems struggle on MTRAG. We demonstrate the\nneed for strong retrieval and generation systems that can handle later turns,\nunanswerable questions, non-standalone questions, and multiple domains. MTRAG\nis available at https://github.com/ibm/mt-rag-benchmark.", "published": "2025-01-07 01:52:56", "link": "http://arxiv.org/abs/2501.03468v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can LLMs Design Good Questions Based on Context?", "abstract": "This paper evaluates questions generated by LLMs from context, comparing them\nto human-generated questions across six dimensions. We introduce an automated\nLLM-based evaluation method, focusing on aspects like question length, type,\ncontext coverage, and answerability. Our findings highlight unique\ncharacteristics of LLM-generated questions, contributing insights that can\nsupport further research in question quality and downstream applications.", "published": "2025-01-07 03:21:17", "link": "http://arxiv.org/abs/2501.03491v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discriminative Representation learning via Attention-Enhanced\n  Contrastive Learning for Short Text Clustering", "abstract": "Contrastive learning has gained significant attention in short text\nclustering, yet it has an inherent drawback of mistakenly identifying samples\nfrom the same category as negatives and then separating them in the feature\nspace (false negative separation), which hinders the generation of superior\nrepresentations. To generate more discriminative representations for efficient\nclustering, we propose a novel short text clustering method, called\nDiscriminative Representation learning via \\textbf{A}ttention-\\textbf{E}nhanced\n\\textbf{C}ontrastive \\textbf{L}earning for Short Text Clustering\n(\\textbf{AECL}). The \\textbf{AECL} consists of two modules which are the\npseudo-label generation module and the contrastive learning module. Both\nmodules build a sample-level attention mechanism to capture similarity\nrelationships between samples and aggregate cross-sample features to generate\nconsistent representations. Then, the former module uses the more\ndiscriminative consistent representation to produce reliable supervision\ninformation for assist clustering, while the latter module explores similarity\nrelationships and consistent representations optimize the construction of\npositive samples to perform similarity-guided contrastive learning, effectively\naddressing the false negative separation issue. Experimental results\ndemonstrate that the proposed \\textbf{AECL} outperforms state-of-the-art\nmethods. If the paper is accepted, we will open-source the code.", "published": "2025-01-07 07:17:04", "link": "http://arxiv.org/abs/2501.03584v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment", "abstract": "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work.", "published": "2025-01-07 08:49:04", "link": "http://arxiv.org/abs/2501.03624v1", "categories": ["cs.HC", "cs.CL", "I.2.7"], "primary_category": "cs.HC"}
{"title": "A Diversity-Enhanced Knowledge Distillation Model for Practical Math\n  Word Problem Solving", "abstract": "Math Word Problem (MWP) solving is a critical task in natural language\nprocessing, has garnered significant research interest in recent years. Various\nrecent studies heavily rely on Seq2Seq models and their extensions (e.g.,\nSeq2Tree and Graph2Tree) to generate mathematical equations. While effective,\nthese models struggle to generate diverse but counterpart solution equations,\nlimiting their generalization across various math problem scenarios. In this\npaper, we introduce a novel Diversity-enhanced Knowledge Distillation (DivKD)\nmodel for practical MWP solving. Our approach proposes an adaptive diversity\ndistillation method, in which a student model learns diverse equations by\nselectively transferring high-quality knowledge from a teacher model.\nAdditionally, we design a diversity prior-enhanced student model to better\ncapture the diversity distribution of equations by incorporating a conditional\nvariational auto-encoder. Extensive experiments on {four} MWP benchmark\ndatasets demonstrate that our approach achieves higher answer accuracy than\nstrong baselines while maintaining high efficiency for practical applications.", "published": "2025-01-07 10:18:22", "link": "http://arxiv.org/abs/2501.03670v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language\n  Alignment", "abstract": "Despite the significant improvements achieved by large language models (LLMs)\nin English reasoning tasks, these models continue to struggle with multilingual\nreasoning. Recent studies leverage a full-parameter and two-stage training\nparadigm to teach models to first understand non-English questions and then\nreason. However, this method suffers from both substantial computational\nresource computing and catastrophic forgetting. The fundamental cause is that,\nwith the primary goal of enhancing multilingual comprehension, an excessive\nnumber of irrelevant layers and parameters are tuned during the first stage.\nGiven our findings that the representation learning of languages is merely\nconducted in lower-level layers, we propose an efficient multilingual reasoning\nalignment approach that precisely identifies and fine-tunes the layers\nresponsible for handling multilingualism. Experimental results show that our\nmethod, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of\nall parameters within 7B and 13B LLMs, achieving superior average performance\nthan all strong baselines across 10 languages. Meanwhile, SLAM only involves\none training stage, reducing training time by 4.1-11.9 compared to the\ntwo-stage method.", "published": "2025-01-07 10:29:43", "link": "http://arxiv.org/abs/2501.03681v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Select Pre-Trained Code Models for Reuse? A Learning Perspective", "abstract": "Pre-training a language model and then fine-tuning it has shown to be an\nefficient and effective technique for a wide range of code intelligence tasks,\nsuch as code generation, code summarization, and vulnerability detection.\nHowever, pretraining language models on a large-scale code corpus is\ncomputationally expensive. Fortunately, many off-the-shelf Pre-trained Code\nModels (PCMs), such as CodeBERT, CodeT5, CodeGen, and Code Llama, have been\nreleased publicly. These models acquire general code understanding and\ngeneration capability during pretraining, which enhances their performance on\ndownstream code intelligence tasks. With an increasing number of these public\npre-trained models, selecting the most suitable one to reuse for a specific\ntask is essential. In this paper, we systematically investigate the reusability\nof PCMs. We first explore three intuitive model selection methods that select\nby size, training data, or brute-force fine-tuning. Experimental results show\nthat these straightforward techniques either perform poorly or suffer high\ncosts. Motivated by these findings, we explore learning-based model selection\nstrategies that utilize pre-trained models without altering their parameters.\nSpecifically, we train proxy models to gauge the performance of pre-trained\nmodels, and measure the distribution deviation between a model's latent\nfeatures and the task's labels, using their closeness as an indicator of model\ntransferability. We conduct experiments on 100 widely-used opensource PCMs for\ncode intelligence tasks, with sizes ranging from 42.5 million to 3 billion\nparameters. The results demonstrate that learning-based selection methods\nreduce selection time to 100 seconds, compared to 2,700 hours with brute-force\nfine-tuning, with less than 6% performance degradation across related tasks.", "published": "2025-01-07 13:45:24", "link": "http://arxiv.org/abs/2501.03783v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Investigating the Impact of Data Selection Strategies on Language Model\n  Performance", "abstract": "Data selection is critical for enhancing the performance of language models,\nparticularly when aligning training datasets with a desired target\ndistribution. This study explores the effects of different data selection\nmethods and feature types on model performance. We evaluate whether selecting\ndata subsets can influence downstream tasks, whether n-gram features improve\nalignment with target distributions, and whether embedding-based neural\nfeatures provide complementary benefits. Through comparative experiments using\nbaseline random selection methods and distribution aligned approaches, we\nprovide insights into the interplay between data selection strategies and model\ntraining efficacy. All code for this study can be found on\n\\href{https://github.com/jgu13/HIR-Hybrid-Importance-Resampling-for-Language-Models}{github\nrepository}.", "published": "2025-01-07 14:38:49", "link": "http://arxiv.org/abs/2501.03826v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides", "abstract": "Automatically generating presentations from documents is a challenging task\nthat requires accommodating content quality, visual appeal, and structural\ncoherence. Existing methods primarily focus on improving and evaluating the\ncontent quality in isolation, overlooking visual appeal and structural\ncoherence, which limits their practical applicability. To address these\nlimitations, we propose PPTAgent, which comprehensively improves presentation\ngeneration through a two-stage, edit-based approach inspired by human\nworkflows. PPTAgent first analyzes reference presentations to extract\nslide-level functional types and content schemas, then drafts an outline and\niteratively generates editing actions based on selected reference slides to\ncreate new slides. To comprehensively evaluate the quality of generated\npresentations, we further introduce PPTEval, an evaluation framework that\nassesses presentations across three dimensions: Content, Design, and Coherence.\nResults demonstrate that PPTAgent significantly outperforms existing automatic\npresentation generation methods across all three dimensions.", "published": "2025-01-07 16:53:01", "link": "http://arxiv.org/abs/2501.03936v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection", "abstract": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.", "published": "2025-01-07 17:00:49", "link": "http://arxiv.org/abs/2501.03940v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Localizing AI: Evaluating Open-Weight Language Models for Languages of\n  Baltic States", "abstract": "Although large language models (LLMs) have transformed our expectations of\nmodern language technologies, concerns over data privacy often restrict the use\nof commercially available LLMs hosted outside of EU jurisdictions. This limits\ntheir application in governmental, defence, and other data-sensitive sectors.\nIn this work, we evaluate the extent to which locally deployable open-weight\nLLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.\nWe examine various size and precision variants of the top-performing\nmultilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine\ntranslation, multiple-choice question answering, and free-form text generation.\nThe results indicate that while certain models like Gemma~2 perform close to\nthe top commercially available models, many LLMs struggle with these languages.\nMost surprisingly, however, we find that these models, while showing close to\nstate-of-the-art translation performance, are still prone to lexical\nhallucinations with errors in at least 1 in 20 words for all open-weight\nmultilingual LLMs.", "published": "2025-01-07 17:24:17", "link": "http://arxiv.org/abs/2501.03952v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Open QA on the MIA Shared Task", "abstract": "Cross-lingual information retrieval (CLIR) ~\\cite{shi2021cross, asai2021one,\njiang2020cross} for example, can find relevant text in any language such as\nEnglish(high resource) or Telugu (low resource) even when the query is posed in\na different, possibly low-resource, language. In this work, we aim to develop\nuseful CLIR models for this constrained, yet important, setting where we do not\nrequire any kind of additional supervision or labelled data for retrieval task\nand hence can work effectively for low-resource languages.\n  \\par We propose a simple and effective re-ranking method for improving\npassage retrieval in open question answering. The re-ranker re-scores retrieved\npassages with a zero-shot multilingual question generation model, which is a\npre-trained language model, to compute the probability of the input question in\nthe target language conditioned on a retrieved passage, which can be possibly\nin a different language. We evaluate our method in a completely zero shot\nsetting and doesn't require any training. Thus the main advantage of our method\nis that our approach can be used to re-rank results obtained by any sparse\nretrieval methods like BM-25. This eliminates the need for obtaining expensive\nlabelled corpus required for the retrieval tasks and hence can be used for low\nresource languages.", "published": "2025-01-07 21:43:09", "link": "http://arxiv.org/abs/2501.04153v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Multihop Source Retrieval for Web Question Answering", "abstract": "This work deals with the challenge of learning and reasoning over multi-modal\nmulti-hop question answering (QA). We propose a graph reasoning network based\non the semantic structure of the sentences to learn multi-source reasoning\npaths and find the supporting facts across both image and text modalities for\nanswering the question. In this paper, we investigate the importance of graph\nstructure for multi-modal multi-hop question answering. Our analysis is\ncentered on WebQA. We construct a strong baseline model, that finds relevant\nsources using a pairwise classification task. We establish that, with the\nproper use of feature representations from pre-trained models, graph structure\nhelps in improving multi-modal multi-hop question answering. We point out that\nboth graph structure and adjacency matrix are task-related prior knowledge, and\ngraph structure can be leveraged to improve the retrieval performance for the\ntask. Experiments and visualized analysis demonstrate that message propagation\nover graph networks or the entire graph structure can replace massive\nmultimodal transformers with token-wise cross-attention. We demonstrated the\napplicability of our method and show a performance gain of \\textbf{4.6$\\%$}\nretrieval F1score over the transformer baselines, despite being a very light\nmodel. We further demonstrated the applicability of our model to a large scale\nretrieval setting.", "published": "2025-01-07 22:53:56", "link": "http://arxiv.org/abs/2501.04173v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IntegrityAI at GenAI Detection Task 2: Detecting Machine-Generated\n  Academic Essays in English and Arabic Using ELECTRA and Stylometry", "abstract": "Recent research has investigated the problem of detecting machine-generated\nessays for academic purposes. To address this challenge, this research utilizes\npre-trained, transformer-based models fine-tuned on Arabic and English academic\nessays with stylometric features. Custom models based on ELECTRA for English\nand AraELECTRA for Arabic were trained and evaluated using a benchmark dataset.\nProposed models achieved excellent results with an F1-score of 99.7%, ranking\n2nd among of 26 teams in the English subtask, and 98.4%, finishing 1st out of\n23 teams in the Arabic one.", "published": "2025-01-07 10:19:56", "link": "http://arxiv.org/abs/2501.05476v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Practical Design and Benchmarking of Generative AI Applications for\n  Surgical Billing and Coding", "abstract": "Background: Healthcare has many manual processes that can benefit from\nautomation and augmentation with Generative Artificial Intelligence (AI), the\nmedical billing and coding process. However, current foundational Large\nLanguage Models (LLMs) perform poorly when tasked with generating accurate\nInternational Classification of Diseases, 10th edition, Clinical Modification\n(ICD-10-CM) and Current Procedural Terminology (CPT) codes. Additionally, there\nare many security and financial challenges in the application of generative AI\nto healthcare. We present a strategy for developing generative AI tools in\nhealthcare, specifically for medical billing and coding, that balances\naccuracy, accessibility, and patient privacy.\n  Methods: We fine tune the PHI-3 Mini and PHI-3 Medium LLMs using\ninstitutional data and compare the results against the PHI-3 base model, a\nPHI-3 RAG application, and GPT-4o. We use the post operative surgical report as\ninput and the patients billing claim the associated ICD-10, CPT, and Modifier\ncodes as the target result. Performance is measured by accuracy of code\ngeneration, proportion of invalid codes, and the fidelity of the billing claim\nformat.\n  Results: Both fine-tuned models performed better or as well as GPT-4o. The\nPhi-3 Medium fine-tuned model showed the best performance (ICD-10 Recall and\nPrecision: 72%, 72%; CPT Recall and Precision: 77%, 79%; Modifier Recall and\nPrecision: 63%, 64%). The Phi-3 Medium fine-tuned model only fabricated 1% of\nICD-10 codes and 0.6% of CPT codes generated.\n  Conclusions: Our study shows that a small model that is fine-tuned on\ndomain-specific data for specific tasks using a simple set of open-source tools\nand minimal technological and monetary requirements performs as well as the\nlarger contemporary consumer models.", "published": "2025-01-07 17:11:12", "link": "http://arxiv.org/abs/2501.05479v1", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.1; J.3"], "primary_category": "cs.CL"}
{"title": "The \\textit{Questio de aqua et terra}: A Computational Authorship\n  Verification Study", "abstract": "The Questio de aqua et terra is a cosmological treatise traditionally\nattributed to Dante Alighieri. However, the authenticity of this text is\ncontroversial, due to discrepancies with Dante's established works and to the\nabsence of contemporary references. This study investigates the authenticity of\nthe Questio via computational authorship verification (AV), a class of\ntechniques which combine supervised machine learning and stylometry. We build a\nfamily of AV systems and assemble a corpus of 330 13th- and 14th-century Latin\ntexts, which we use to comparatively evaluate the AV systems through\nleave-one-out cross-validation. Our best-performing system achieves high\nverification accuracy (F1=0.970) despite the heterogeneity of the corpus in\nterms of textual genre. The key contribution to the accuracy of this system is\nshown to come from Distributional Random Oversampling (DRO), a technique\nspecially tailored to text classification which is here used for the first time\nin AV.\n  The application of the AV system to the Questio returns a highly confident\nprediction concerning its authenticity. These findings contribute to the debate\non the authorship of the Questio, and highlight DRO's potential in the\napplication of AV to cultural heritage.", "published": "2025-01-07 18:42:05", "link": "http://arxiv.org/abs/2501.05480v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "HP-BERT: A framework for longitudinal study of Hinduphobia on social\n  media via LLMs", "abstract": "During the COVID-19 pandemic, community tensions intensified, fuelling\nHinduphobic sentiments and discrimination against individuals of Hindu descent\nwithin India and worldwide. Large language models (LLMs) have become prominent\nin natural language processing (NLP) tasks and social media analysis, enabling\nlongitudinal studies of platforms like X (formerly Twitter) for specific issues\nduring COVID-19. We present an abuse detection and sentiment analysis framework\nthat offers a longitudinal analysis of Hinduphobia on X (Twitter) during and\nafter the COVID-19 pandemic. This framework assesses the prevalence and\nintensity of Hinduphobic discourse, capturing elements such as derogatory jokes\nand racist remarks through sentiment analysis and abuse detection from\npre-trained and fine-tuned LLMs. Additionally, we curate and publish a\n\"Hinduphobic COVID-19 X (Twitter) Dataset\" of 8,000 tweets annotated for\nHinduphobic abuse detection, which is used to fine-tune a BERT model, resulting\nin the development of the Hinduphobic BERT (HP-BERT) model. We then further\nfine-tune HP-BERT using the SenWave dataset for multi-label sentiment analysis.\nOur study encompasses approximately 27.4 million tweets from six countries,\nincluding Australia, Brazil, India, Indonesia, Japan, and the United Kingdom.\nOur findings reveal a strong correlation between spikes in COVID-19 cases and\nsurges in Hinduphobic rhetoric, highlighting how political narratives,\nmisinformation, and targeted jokes contributed to communal polarisation. These\ninsights provide valuable guidance for developing strategies to mitigate\ncommunal tensions in future crises, both locally and globally. We advocate\nimplementing automated monitoring and removal of such content on social media\nto curb divisive discourse.", "published": "2025-01-07 23:22:05", "link": "http://arxiv.org/abs/2501.05482v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Zoning in American Cities: Are Reforms Making a Difference? An AI-based\n  Analysis", "abstract": "Cities are at the forefront of addressing global sustainability challenges,\nparticularly those exacerbated by climate change. Traditional zoning codes,\nwhich often segregate land uses, have been linked to increased vehicular\ndependence, urban sprawl, and social disconnection, undermining broader social\nand environmental sustainability objectives. This study investigates the\nadoption and impact of form-based codes (FBCs), which aim to promote\nsustainable, compact, and mixed-use urban forms as a solution to these issues.\nUsing Natural Language Processing (NLP) techniques, we analyzed zoning\ndocuments from over 2000 U.S. census-designated places to identify linguistic\npatterns indicative of FBC principles. Our findings reveal widespread adoption\nof FBCs across the country, with notable variations within regions. FBCs are\nassociated with higher floor-to-area ratios, narrower and more consistent\nstreet setbacks, and smaller plots. We also find that places with FBCs have\nimproved walkability, shorter commutes, and a higher share of multi-family\nhousing. Our findings highlight the utility of NLP for evaluating zoning codes\nand underscore the potential benefits of form-based zoning reforms for\nenhancing urban sustainability.", "published": "2025-01-07 01:03:38", "link": "http://arxiv.org/abs/2502.00008v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Reading with Intent -- Neutralizing Intent", "abstract": "Queries to large language models (LLMs) can be divided into two parts: the\ninstruction/question and the accompanying context. The context for\nretrieval-augmented generation (RAG) systems in most benchmarks comes from\nWikipedia or Wikipedia-like texts which are written in a neutral and factual\ntone. However, when RAG systems retrieve internet-based content, they encounter\ntext with diverse tones and linguistic styles, introducing challenges for\ndownstream tasks. The Reading with Intent task addresses this issue by\nevaluating how varying tones in context passages affect model performance.\nBuilding on prior work that focused on sarcasm, we extend this paradigm by\nconstructing a dataset where context passages are transformed to $11$ distinct\nemotions using a better synthetic data generation approach. Using this dataset,\nwe train an emotion translation model to systematically adapt passages to\nspecified emotional tones. The human evaluation shows that the LLM fine-tuned\nto become the emotion-translator benefited from the synthetically generated\ndata. Finally, the emotion-translator is used in the Reading with Intent task\nto transform the passages to a neutral tone. By neutralizing the passages, it\nmitigates the challenges posed by sarcastic passages and improves overall\nresults on this task by about $3\\%$.", "published": "2025-01-07 02:33:25", "link": "http://arxiv.org/abs/2501.03475v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KG-TRICK: Unifying Textual and Relational Information Completion of\n  Knowledge for Multilingual Knowledge Graphs", "abstract": "Multilingual knowledge graphs (KGs) provide high-quality relational and\ntextual information for various NLP applications, but they are often\nincomplete, especially in non-English languages. Previous research has shown\nthat combining information from KGs in different languages aids either\nKnowledge Graph Completion (KGC), the task of predicting missing relations\nbetween entities, or Knowledge Graph Enhancement (KGE), the task of predicting\nmissing textual information for entities. Although previous efforts have\nconsidered KGC and KGE as independent tasks, we hypothesize that they are\ninterdependent and mutually beneficial. To this end, we introduce KG-TRICK, a\nnovel sequence-to-sequence framework that unifies the tasks of textual and\nrelational information completion for multilingual KGs. KG-TRICK demonstrates\nthat: i) it is possible to unify the tasks of KGC and KGE into a single\nframework, and ii) combining textual information from multiple languages is\nbeneficial to improve the completeness of a KG. As part of our contributions,\nwe also introduce WikiKGE10++, the largest manually-curated benchmark for\ntextual information completion of KGs, which features over 25,000 entities\nacross 10 diverse languages.", "published": "2025-01-07 06:21:40", "link": "http://arxiv.org/abs/2501.03560v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Code to Compliance: Assessing ChatGPT's Utility in Designing an\n  Accessible Webpage -- A Case Study", "abstract": "Web accessibility ensures that individuals with disabilities can access and\ninteract with digital content without barriers, yet a significant majority of\nmost used websites fail to meet accessibility standards. This study evaluates\nChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web\nContent Accessibility Guidelines (WCAG). While ChatGPT can effectively address\naccessibility issues when prompted, its default code often lacks compliance,\nreflecting limitations in its training data and prevailing inaccessible web\npractices. Automated and manual testing revealed strengths in resolving simple\nissues but challenges with complex tasks, requiring human oversight and\nadditional iterations. Unlike prior studies, we incorporate manual evaluation,\ndynamic elements, and use the visual reasoning capability of ChatGPT along with\nthe prompts to fix accessibility issues. Providing screenshots alongside\nprompts enhances the LLM's ability to address accessibility issues by allowing\nit to analyze surrounding components, such as determining appropriate contrast\ncolors. We found that effective prompt engineering, such as providing concise,\nstructured feedback and incorporating visual aids, significantly enhances\nChatGPT's performance. These findings highlight the potential and limitations\nof large language models for accessible web development, offering practical\nguidance for developers to create more inclusive websites.", "published": "2025-01-07 06:51:46", "link": "http://arxiv.org/abs/2501.03572v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "D.1.2; F.3.1; F.4.1; D.3.2; H.1.2; H.5.2; D.2.2; H.1.2; I.3.6;\n  H.5.4; H.5.1"], "primary_category": "cs.HC"}
{"title": "Context-Alignment: Activating and Enhancing LLM Capabilities in Time\n  Series", "abstract": "Recently, leveraging pre-trained Large Language Models (LLMs) for time series\n(TS) tasks has gained increasing attention, which involves activating and\nenhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities\nbased on token-level alignment but overlook LLMs' inherent strength on natural\nlanguage processing -- their deep understanding of linguistic logic and\nstructure rather than superficial embedding processing. We propose\nContext-Alignment, a new paradigm that aligns TS with a linguistic component in\nthe language environments familiar to LLMs to enable LLMs to contextualize and\ncomprehend TS data, thereby activating their capabilities. Specifically, such\ncontext-level alignment comprises structural alignment and logical alignment,\nwhich is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to\nTS-language multimodal inputs. Structural alignment utilizes dual-scale nodes\nto describe hierarchical structure in TS-language, enabling LLMs treat long TS\ndata as a whole linguistic component while preserving intrinsic token features.\nLogical alignment uses directed edges to guide logical relationships, ensuring\ncoherence in the contextual semantics. Demonstration examples prompt are\nemployed to construct Demonstration Examples based Context-Alignment (DECA)\nfollowing DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated\ninto various layers of pre-trained LLMs to improve awareness of logic and\nstructure, thereby enhancing performance. Extensive experiments show the\neffectiveness of DECA and the importance of Context-Alignment across tasks,\nparticularly in few-shot and zero-shot forecasting, confirming that\nContext-Alignment provide powerful prior knowledge on context.", "published": "2025-01-07 12:40:35", "link": "http://arxiv.org/abs/2501.03747v2", "categories": ["cs.LG", "cs.CL", "stat.AP"], "primary_category": "cs.LG"}
{"title": "Detecting the Undetectable: Assessing the Efficacy of Current Spoof\n  Detection Methods Against Seamless Speech Edits", "abstract": "Neural speech editing advancements have raised concerns about their misuse in\nspoofing attacks. Traditional partially edited speech corpora primarily focus\non cut-and-paste edits, which, while maintaining speaker consistency, often\nintroduce detectable discontinuities. Recent methods, like\nA\\textsuperscript{3}T and Voicebox, improve transitions by leveraging\ncontextual information. To foster spoofing detection research, we introduce the\nSpeech INfilling Edit (SINE) dataset, created with Voicebox. We detailed the\nprocess of re-implementing Voicebox training and dataset creation. Subjective\nevaluations confirm that speech edited using this novel technique is more\nchallenging to detect than conventional cut-and-paste methods. Despite human\ndifficulty, experimental results demonstrate that self-supervised-based\ndetectors can achieve remarkable performance in detection, localization, and\ngeneralization across different edit methods. The dataset and related models\nwill be made publicly available.", "published": "2025-01-07 14:17:47", "link": "http://arxiv.org/abs/2501.03805v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial\n  Product Attribute Value Identification", "abstract": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendations, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity to the item embedding. It leverages contrastive training with\ntaxonomy-aware hard negative sampling and employs adaptive inference with\ndynamic thresholds. TACLR offers three key advantages: (1) it effectively\nhandles implicit and OOD values while producing normalized outputs; (2) it\nscales to thousands of categories, tens of thousands of attributes, and\nmillions of values; and (3) it supports efficient inference for high-load\nindustrial scenarios. Extensive experiments on proprietary and public datasets\nvalidate the effectiveness and efficiency of TACLR. Moreover, it has been\nsuccessfully deployed in a real-world e-commerce platform, processing millions\nof product listings daily while supporting dynamic, large-scale attribute\ntaxonomies.", "published": "2025-01-07 14:45:30", "link": "http://arxiv.org/abs/2501.03835v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "BERTopic for Topic Modeling of Hindi Short Texts: A Comparative Study", "abstract": "As short text data in native languages like Hindi increasingly appear in\nmodern media, robust methods for topic modeling on such data have gained\nimportance. This study investigates the performance of BERTopic in modeling\nHindi short texts, an area that has been under-explored in existing research.\nUsing contextual embeddings, BERTopic can capture semantic relationships in\ndata, making it potentially more effective than traditional models, especially\nfor short and diverse texts. We evaluate BERTopic using 6 different document\nembedding models and compare its performance against 8 established topic\nmodeling techniques, such as Latent Dirichlet Allocation (LDA), Non-negative\nMatrix Factorization (NMF), Latent Semantic Indexing (LSI), Additive\nRegularization of Topic Models (ARTM), Probabilistic Latent Semantic Analysis\n(PLSA), Embedded Topic Model (ETM), Combined Topic Model (CTM), and Top2Vec.\nThe models are assessed using coherence scores across a range of topic counts.\nOur results reveal that BERTopic consistently outperforms other models in\ncapturing coherent topics from short Hindi texts.", "published": "2025-01-07 14:53:35", "link": "http://arxiv.org/abs/2501.03843v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "I.2.7; H.3.3; H.2.8"], "primary_category": "cs.IR"}
{"title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\n  Vision Token", "abstract": "The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.", "published": "2025-01-07 16:03:14", "link": "http://arxiv.org/abs/2501.03895v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Dolphin: Moving Towards Closed-loop Auto-research through Thinking,\n  Practice, and Feedback", "abstract": "The scientific research paradigm is undergoing a profound transformation\nowing to the development of Artificial Intelligence (AI). Recent works\ndemonstrate that various AI-assisted research methods can largely improve\nresearch efficiency by improving data analysis, accelerating computation, and\nfostering novel idea generation. To further move towards the ultimate goal\n(i.e., automatic scientific research), in this paper, we introduce Dolphin, a\nclosed-loop LLM-driven framework to enhance the automation level of scientific\nresearch. Dolphin first generates novel ideas based on feedback from previous\nexperiments and relevant papers ranked by the topic and task attributes. Then,\nthe generated ideas can be implemented using a code template refined and\ndebugged with the designed exception-traceback-guided local code structure.\nFinally, Dolphin automatically analyzes the results of each idea and feeds the\nresults back to the next round of idea generation. Experiments are conducted on\nthe benchmark datasets of different topics and a subset of MLE-bench. Results\nshow that Dolphin can continuously improve the performance of the input topic\nin a loop. We highlight that Dolphin can automatically propose methods that are\ncomparable to the state-of-the-art in some tasks such as 3D point\nclassification.", "published": "2025-01-07 16:31:10", "link": "http://arxiv.org/abs/2501.03916v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "From Newswire to Nexus: Using text-based actor embeddings and\n  transformer networks to forecast conflict dynamics", "abstract": "This study advances the field of conflict forecasting by using text-based\nactor embeddings with transformer models to predict dynamic changes in violent\nconflict patterns at the actor level. More specifically, we combine newswire\ntexts with structured conflict event data and leverage recent advances in\nNatural Language Processing (NLP) techniques to forecast escalations and\nde-escalations among conflicting actors, such as governments, militias,\nseparatist movements, and terrorists. This new approach accurately and promptly\ncaptures the inherently volatile patterns of violent conflicts, which existing\nmethods have not been able to achieve. To create this framework, we began by\ncurating and annotating a vast international newswire corpus, leveraging\nhand-labeled event data from the Uppsala Conflict Data Program. By using this\nhybrid dataset, our models can incorporate the textual context of news sources\nalong with the precision and detail of structured event data. This combination\nenables us to make both dynamic and granular predictions about conflict\ndevelopments. We validate our approach through rigorous back-testing against\nhistorical events, demonstrating superior out-of-sample predictive power. We\nfind that our approach is quite effective in identifying and predicting phases\nof conflict escalation and de-escalation, surpassing the capabilities of\ntraditional models. By focusing on actor interactions, our explicit goal is to\nprovide actionable insights to policymakers, humanitarian organizations, and\npeacekeeping operations in order to enable targeted and effective intervention\nstrategies.", "published": "2025-01-07 16:45:37", "link": "http://arxiv.org/abs/2501.03928v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "68", "I.2.7; J.4"], "primary_category": "cs.CY"}
{"title": "More is not always better? Enhancing Many-Shot In-Context Learning with\n  Differentiated and Reweighting Objectives", "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL)\nwithout requiring parameter updates. However, as the number of ICL\ndemonstrations increases from a few to many, performance tends to plateau and\neventually decline. We identify two primary causes for this trend: the\nsuboptimal negative log-likelihood (NLL) optimization objective and the\nincremental data noise. To address these issues, we introduce DrICL, a novel\noptimization method that enhances model performance through Differentiated\nLearning and advantage-based Reweighting objectives. Globally, DrICL utilizes\ndifferentiated learning to optimize the NLL objective, ensuring that many-shot\nperformance surpasses zero-shot levels. Locally, it dynamically adjusts the\nweighting of many-shot demonstrations by leveraging cumulative advantages\ninspired by reinforcement learning, thereby improving generalization. This\napproach allows the model to handle varying numbers of shots effectively,\nmitigating the impact of noisy data. Recognizing the lack of multi-task\ndatasets with diverse many-shot distributions, we develop the Many-Shot ICL\nBenchmark (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers\nfrom 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes.\nICL-50 facilitates the evaluation of many-shot ICL strategies across seven\nprominent NLP tasks and 50 distinct datasets. Experimental results demonstrate\nthat LLMs enhanced with DrICL achieve significant improvements in many-shot\nsetups across various tasks, including both in-domain and out-of-domain\nscenarios. We release the code and benchmark dataset hoping to facilitate\nfurther research in many-shot ICL.", "published": "2025-01-07 14:57:08", "link": "http://arxiv.org/abs/2501.04070v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data\n  Curation", "abstract": "Vision-language models (VLMs) are highly effective but often underperform on\nspecialized tasks; for example, Llava-1.5 struggles with chart and diagram\nunderstanding due to scarce task-specific training data. Existing training\ndata, sourced from general-purpose datasets, fails to capture the nuanced\ndetails needed for these tasks. We introduce MM-Gen, a scalable method that\ngenerates task-specific, high-quality synthetic text for candidate images by\nleveraging stronger models. MM-Gen employs a three-stage targeted process:\npartitioning data into subgroups, generating targeted text based on task\ndescriptions, and filtering out redundant and outlier data. Fine-tuning VLMs\nwith data generated by MM-Gen leads to significant performance gains, including\n29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B).\nCompared to human-curated caption data, MM-Gen achieves up to 1.6x better\nimprovements for the original models, proving its effectiveness in enhancing\ntask-specific VLM performance and bridging the gap between general-purpose\ndatasets and specialized requirements. Code available at\nhttps://github.com/sjoshi804/MM-Gen.", "published": "2025-01-07 21:55:56", "link": "http://arxiv.org/abs/2501.04155v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Reasoning-Enhanced Self-Training for Long-Form Personalized Text\n  Generation", "abstract": "Personalized text generation requires a unique ability of large language\nmodels (LLMs) to learn from context that they often do not encounter during\ntheir standard training. One way to encourage LLMs to better use personalized\ncontext for generating outputs that better align with the user's expectations\nis to instruct them to reason over the user's past preferences, background\nknowledge, or writing style. To achieve this, we propose Reasoning-Enhanced\nSelf-Training for Personalized Text Generation (REST-PG), a framework that\ntrains LLMs to reason over personal data during response generation. REST-PG\nfirst generates reasoning paths to train the LLM's reasoning abilities and then\nemploys Expectation-Maximization Reinforced Self-Training to iteratively train\nthe LLM based on its own high-reward outputs. We evaluate REST-PG on the\nLongLaMP benchmark, consisting of four diverse personalized long-form text\ngeneration tasks. Our experiments demonstrate that REST-PG achieves significant\nimprovements over state-of-the-art baselines, with an average relative\nperformance gain of 14.5% on the benchmark.", "published": "2025-01-07 22:29:08", "link": "http://arxiv.org/abs/2501.04167v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Generation by Evidence Retroactivity in LLMs", "abstract": "Retrieval-augmented generation has gained significant attention due to its\nability to integrate relevant external knowledge, enhancing the accuracy and\nreliability of the LLMs' responses. Most of the existing methods apply a\ndynamic multiple retrieval-generating process, to address multi-hop complex\nquestions by decomposing them into sub-problems. However, these methods rely on\nan unidirectional forward reasoning paradigm, where errors from insufficient\nreasoning steps or inherent flaws in current retrieval systems are\nirreversible, potentially derailing the entire reasoning chain. For the first\ntime, this work introduces Retroactive Retrieval-Augmented Generation\n(RetroRAG), a novel framework to build a retroactive reasoning paradigm.\nRetroRAG revises and updates the evidence, redirecting the reasoning chain to\nthe correct direction. RetroRAG constructs an evidence-collation-discovery\nframework to search, generate, and refine credible evidence. It synthesizes\ninferential evidence related to the key entities in the question from the\nexisting source knowledge and formulates search queries to uncover additional\ninformation. As new evidence is found, RetroRAG continually updates and\norganizes this information, enhancing its ability to locate further necessary\nevidence. Paired with an Answerer to generate and evaluate outputs, RetroRAG is\ncapable of refining its reasoning process iteratively until a reliable answer\nis obtained. Empirical evaluations show that RetroRAG significantly outperforms\nexisting methods.", "published": "2025-01-07 08:57:42", "link": "http://arxiv.org/abs/2501.05475v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Agreeing to Interact in Human-Robot Interaction using Large Language\n  Models and Vision Language Models", "abstract": "In human-robot interaction (HRI), the beginning of an interaction is often\ncomplex. Whether the robot should communicate with the human is dependent on\nseveral situational factors (e.g., the current human's activity, urgency of the\ninteraction, etc.). We test whether large language models (LLM) and vision\nlanguage models (VLM) can provide solutions to this problem. We compare four\ndifferent system-design patterns using LLMs and VLMs, and test on a test set\ncontaining 84 human-robot situations. The test set mixes several publicly\navailable datasets and also includes situations where the appropriate action to\ntake is open-ended. Our results using the GPT-4o and Phi-3 Vision model\nindicate that LLMs and VLMs are capable of handling interaction beginnings when\nthe desired actions are clear, however, challenge remains in the open-ended\nsituations where the model must balance between the human and robot situation.", "published": "2025-01-07 07:26:49", "link": "http://arxiv.org/abs/2503.15491v1", "categories": ["cs.HC", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.HC"}
{"title": "Unsupervised Speech Segmentation: A General Approach Using Speech\n  Language Models", "abstract": "In this paper, we introduce an unsupervised approach for Speech Segmentation,\nwhich builds on previously researched approaches, e.g., Speaker Diarization,\nwhile being applicable to an inclusive set of acoustic-semantic distinctions,\npaving a path towards a general Unsupervised Speech Segmentation approach.\nUnlike traditional speech and audio segmentation, which mainly focuses on\nspectral changes in the input signal, e.g., phone segmentation, our approach\ntries to segment the spoken utterance into chunks with differing\nacoustic-semantic styles, focusing on acoustic-semantic information that does\nnot translate well into text, e.g., emotion or speaker. While most Speech\nSegmentation tasks only handle one style change, e.g., emotion diarization, our\napproach tries to handle multiple acoustic-semantic style changes. Leveraging\nrecent advances in Speech Language Models (SLMs), we propose a simple\nunsupervised method to segment a given speech utterance. We empirically\ndemonstrate the effectiveness of the proposed approach by considering several\nsetups. Results suggest that the proposed method is superior to the evaluated\nbaselines on boundary detection, segment purity, and over-segmentation. Code is\navailable at\nhttps://github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.", "published": "2025-01-07 11:32:13", "link": "http://arxiv.org/abs/2501.03711v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Modality-Invariant Bidirectional Temporal Representation Distillation\n  Network for Missing Multimodal Sentiment Analysis", "abstract": "Multimodal Sentiment Analysis (MSA) integrates diverse modalities(text,\naudio, and video) to comprehensively analyze and understand individuals'\nemotional states. However, the real-world prevalence of incomplete data poses\nsignificant challenges to MSA, mainly due to the randomness of modality\nmissing. Moreover, the heterogeneity issue in multimodal data has yet to be\neffectively addressed. To tackle these challenges, we introduce the\nModality-Invariant Bidirectional Temporal Representation Distillation Network\n(MITR-DNet) for Missing Multimodal Sentiment Analysis. MITR-DNet employs a\ndistillation approach, wherein a complete modality teacher model guides a\nmissing modality student model, ensuring robustness in the presence of modality\nmissing. Simultaneously, we developed the Modality-Invariant Bidirectional\nTemporal Representation Learning Module (MIB-TRL) to mitigate heterogeneity.", "published": "2025-01-07 07:57:16", "link": "http://arxiv.org/abs/2501.05474v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language and Planning in Robotic Navigation: A Multilingual Evaluation\n  of State-of-the-Art Models", "abstract": "Large Language Models (LLMs) such as GPT-4, trained on huge amount of\ndatasets spanning multiple domains, exhibit significant reasoning,\nunderstanding, and planning capabilities across various tasks. This study\npresents the first-ever work in Arabic language integration within the\nVision-and-Language Navigation (VLN) domain in robotics, an area that has been\nnotably underexplored in existing research. We perform a comprehensive\nevaluation of state-of-the-art multi-lingual Small Language Models (SLMs),\nincluding GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the\nArabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure\nLLM-based instruction-following navigation agent, to assess the impact of\nlanguage on navigation reasoning through zero-shot sequential action prediction\nusing the R2R dataset. Through comprehensive experiments, we demonstrate that\nour framework is capable of high-level planning for navigation tasks when\nprovided with instructions in both English and Arabic. However, certain models\nstruggled with reasoning and planning in the Arabic language due to inherent\nlimitations in their capabilities, sub-optimal performance, and parsing issues.\nThese findings highlight the importance of enhancing planning and reasoning\ncapabilities in language models for effective navigation, emphasizing this as a\nkey area for further development while also unlocking the potential of\nArabic-language models for impactful real-world applications.", "published": "2025-01-07 16:01:25", "link": "http://arxiv.org/abs/2501.05478v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Deep Learning for Pathological Speech: A Survey", "abstract": "Advancements in spoken language technologies for neurodegenerative speech\ndisorders are crucial for meeting both clinical and technological needs. This\noverview paper is vital for advancing the field, as it presents a comprehensive\nreview of state-of-the-art methods in pathological speech detection, automatic\nspeech recognition, pathological speech intelligibility enhancement,\nintelligibility and severity assessment, and data augmentation approaches for\npathological speech. It also high-lights key challenges, such as ensuring\nrobustness, privacy, and interpretability. The paper concludes by exploring\npromising future directions, including the adoption of multimodal approaches\nand the integration of graph neural networks and large language models to\nfurther advance speech technology for neurodegenerative speech disorders", "published": "2025-01-07 05:16:30", "link": "http://arxiv.org/abs/2501.03536v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Pseudo Strong Labels from Frame-Level Predictions for Weakly Supervised\n  Sound Event Detection", "abstract": "Weakly Supervised Sound Event Detection (WSSED), which relies on audio tags\nwithout precise onset and offset times, has become prevalent due to the\nscarcity of strongly labeled data that includes exact temporal boundaries for\nevents. This study introduces Frame-level Pseudo Strong Labeling (FPSL) to\novercome the lack of temporal information in WSSED by generating pseudo strong\nlabels from frame-level predictions. This enhances temporal localization during\ntraining and addresses the limitations of clip-wise weak supervision. We\nvalidate our approach across three benchmark datasets (DCASE2017 Task 4,\nDCASE2018 Task 4, and UrbanSED) and demonstrate significant improvements in key\nmetrics such as the Polyphonic Sound Detection Scores (PSDS), event-based F1\nscores, and intersection-based F1 scores. For example, Convolutional Recurrent\nNeural Networks (CRNNs) trained with FPSL outperform baseline models by 4.9% in\nPSDS1 on DCASE2017, 7.6% on DCASE2018, and 1.8% on UrbanSED, confirming the\neffectiveness of our method in enhancing model performance.", "published": "2025-01-07 12:31:55", "link": "http://arxiv.org/abs/2501.03740v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards a Generalizable Speech Marker for Parkinson's Disease Diagnosis", "abstract": "Parkinson's Disease (PD) is a neurodegenerative disorder characterized by\nmotor symptoms, including altered voice production in the early stages. Early\ndiagnosis is crucial not only to improve PD patients' quality of life but also\nto enhance the efficacy of potential disease-modifying therapies during early\nneurodegeneration, a window often missed by current diagnostic tools. In this\npaper, we propose a more generalizable approach to PD recognition through\ndomain adaptation and self-supervised learning. We demonstrate the\ngeneralization capabilities of the proposed approach across diverse datasets in\ndifferent languages. Our approach leverages HuBERT, a large deep neural network\noriginally trained for speech recognition and further trains it on unlabeled\nspeech data from a population that is similar to the target group, i.e., the\nelderly, in a self-supervised manner. The model is then fine-tuned and adapted\nfor use across different datasets in multiple languages, including English,\nItalian, and Spanish. Evaluations on four publicly available PD datasets\ndemonstrate the model's efficacy, achieving an average specificity of 92.1% and\nan average sensitivity of 91.2%. This method offers objective and consistent\nevaluations across large populations, addressing the variability inherent in\nhuman assessments and providing a non-invasive, cost-effective and accessible\ndiagnostic option.", "published": "2025-01-07 07:10:50", "link": "http://arxiv.org/abs/2501.03581v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Universal Speaker Embedding Free Target Speaker Extraction and Personal\n  Voice Activity Detection", "abstract": "Determining 'who spoke what and when' remains challenging in real-world\napplications. In typical scenarios, Speaker Diarization (SD) is employed to\naddress the problem of 'who spoke when,' while Target Speaker Extraction (TSE)\nor Target Speaker Automatic Speech Recognition (TSASR) techniques are utilized\nto resolve the issue of 'who spoke what.' Although some works have achieved\npromising results by combining SD and TSE systems, inconsistencies remain\nbetween SD and TSE regarding both output inconsistency and scenario mismatch.\nTo address these limitations, we propose a Universal Speaker Embedding Free\nTarget Speaker Extraction and Personal Voice Activity Detection (USEF-TP) model\nthat jointly performs TSE and Personal Voice Activity Detection (PVAD). USEF-TP\nleverages frame-level features obtained through a cross-attention mechanism as\nspeaker-related features instead of using speaker embeddings as in traditional\napproaches. Additionally, a multi-task learning algorithm with a scenario-aware\ndifferentiated loss function is applied to ensure robust performance across\nvarious levels of speaker overlap. The experimental results show that our\nproposed USEF-TP model achieves superior performance in TSE and PVAD tasks on\nthe LibriMix and SparseLibriMix datasets.", "published": "2025-01-07 08:27:45", "link": "http://arxiv.org/abs/2501.03612v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Guitar-TECHS: An Electric Guitar Dataset Covering Techniques, Musical\n  Excerpts, Chords and Scales Using a Diverse Array of Hardware", "abstract": "Guitar-related machine listening research involves tasks like timbre\ntransfer, performance generation, and automatic transcription. However, small\ndatasets often limit model robustness due to insufficient acoustic diversity\nand musical content. To address these issues, we introduce Guitar-TECHS, a\ncomprehensive dataset featuring a variety of guitar techniques, musical\nexcerpts, chords, and scales. These elements are performed by diverse musicians\nacross various recording settings. Guitar-TECHS incorporates recordings from\ntwo stereo microphones: an egocentric microphone positioned on the performer's\nhead and an exocentric microphone placed in front of the performer. It also\nincludes direct input recordings and microphoned amplifier outputs, offering a\nwide spectrum of audio inputs and recording qualities. All signals and MIDI\nlabels are properly synchronized. Its multi-perspective and multi-modal content\nmakes Guitar-TECHS a valuable resource for advancing data-driven guitar\nresearch, and to develop robust guitar listening algorithms. We provide\nempirical data to demonstrate the dataset's effectiveness in training robust\nmodels for Guitar Tablature Transcription.", "published": "2025-01-07 12:01:09", "link": "http://arxiv.org/abs/2501.03720v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detecting Neurocognitive Disorders through Analyses of Topic Evolution\n  and Cross-modal Consistency in Visual-Stimulated Narratives", "abstract": "Early detection of neurocognitive disorders (NCDs) is crucial for timely\nintervention and disease management. Speech analysis offers a non-intrusive and\nscalable screening method, particularly through narrative tasks in\nneuropsychological assessment tools. Traditional narrative analysis often\nfocuses on local indicators in microstructure, such as word usage and syntax.\nWhile these features provide insights into language production abilities, they\noften fail to capture global narrative patterns, or microstructures.\nMacrostructures include coherence, thematic organization, and logical\nprogressions, reflecting essential cognitive skills potentially critical for\nrecognizing NCDs. Addressing this gap, we propose to investigate specific\ncognitive and linguistic challenges by analyzing topical shifts, temporal\ndynamics, and the coherence of narratives over time, aiming to reveal cognitive\ndeficits by identifying narrative impairments, and exploring their impact on\ncommunication and cognition. The investigation is based on the CU-MARVEL Rabbit\nStory corpus, which comprises recordings of a story-telling task from 758 older\nadults. We developed two approaches: the Dynamic Topic Models (DTM)-based\ntemporal analysis to examine the evolution of topics over time, and the\nText-Image Temporal Alignment Network (TITAN) to evaluate the coherence between\nspoken narratives and visual stimuli. DTM-based approach validated the\neffectiveness of dynamic topic consistency as a macrostructural metric\n(F1=0.61, AUC=0.78). The TITAN approach achieved the highest performance\n(F1=0.72, AUC=0.81), surpassing established microstructural and macrostructural\nfeature sets. Cross-comparison and regression tasks further demonstrated the\neffectiveness of proposed dynamic macrostructural modeling approaches for NCD\ndetection.", "published": "2025-01-07 12:16:26", "link": "http://arxiv.org/abs/2501.03727v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Spectral-Aware Low-Rank Adaptation for Speaker Verification", "abstract": "Previous research has shown that the principal singular vectors of a\npre-trained model's weight matrices capture critical knowledge. In contrast,\nthose associated with small singular values may contain noise or less reliable\ninformation. As a result, the LoRA-based parameter-efficient fine-tuning (PEFT)\napproach, which does not constrain the use of the spectral space, may not be\neffective for tasks that demand high representation capacity. In this study, we\nenhance existing PEFT techniques by incorporating the spectral information of\npre-trained weight matrices into the fine-tuning process. We investigate\nspectral adaptation strategies with a particular focus on the additive\nadjustment of top singular vectors. This is accomplished by applying singular\nvalue decomposition (SVD) to the pre-trained weight matrices and restricting\nthe fine-tuning within the top spectral space. Extensive speaker verification\nexperiments on VoxCeleb1 and CN-Celeb1 demonstrate enhanced tuning performance\nwith the proposed approach. Code is released at\nhttps://github.com/lizhepolyu/SpectralFT.", "published": "2025-01-07 14:41:09", "link": "http://arxiv.org/abs/2501.03829v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Artifact-free Sound Quality in DNN-based Closed-loop Systems for Audio\n  Processing", "abstract": "Recent advances in deep neural networks (DNNs) have significantly improved\nvarious audio processing applications, including speech enhancement, synthesis,\nand hearing aid algorithms. DNN-based closed-loop systems have gained\npopularity in these applications due to their robust performance and ability to\nadapt to diverse conditions. Despite their effectiveness, current DNN-based\nclosed-loop systems often suffer from sound quality degradation caused by\nartifacts introduced by suboptimal sampling methods. To address this challenge,\nwe introduce dCoNNear, a novel DNN architecture designed for seamless\nintegration into closed-loop frameworks. This architecture specifically aims to\nprevent the generation of spurious artifacts. We demonstrate the effectiveness\nof dCoNNear through a proof-of-principle example within a closed-loop framework\nthat employs biophysically realistic models of auditory processing for both\nnormal and hearing-impaired profiles to design personalized hearing aid\nalgorithms. Our results show that dCoNNear not only accurately simulates all\nprocessing stages of existing non-DNN biophysical models but also eliminates\naudible artifacts, thereby enhancing the sound quality of the resulting hearing\naid algorithms. This study presents a novel, artifact-free closed-loop\nframework that improves the sound quality of audio processing systems, offering\na promising solution for high-fidelity applications in audio and hearing\ntechnologies.", "published": "2025-01-07 19:52:24", "link": "http://arxiv.org/abs/2501.04116v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LHGNN: Local-Higher Order Graph Neural Networks For Audio Classification\n  and Tagging", "abstract": "Transformers have set new benchmarks in audio processing tasks, leveraging\nself-attention mechanisms to capture complex patterns and dependencies within\naudio data. However, their focus on pairwise interactions limits their ability\nto process the higher-order relations essential for identifying distinct audio\nobjects. To address this limitation, this work introduces the Local- Higher\nOrder Graph Neural Network (LHGNN), a graph based model that enhances feature\nunderstanding by integrating local neighbourhood information with higher-order\ndata from Fuzzy C-Means clusters, thereby capturing a broader spectrum of audio\nrelationships. Evaluation of the model on three publicly available audio\ndatasets shows that it outperforms Transformer-based models across all\nbenchmarks while operating with substantially fewer parameters. Moreover, LHGNN\ndemonstrates a distinct advantage in scenarios lacking ImageNet pretraining,\nestablishing its effectiveness and efficiency in environments where extensive\npretraining data is unavailable.", "published": "2025-01-07 01:45:39", "link": "http://arxiv.org/abs/2501.03464v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vocal Tract Length Warped Features for Spoken Keyword Spotting", "abstract": "In this paper, we propose several methods that incorporate vocal tract length\n(VTL) warped features for spoken keyword spotting (KWS). The first method,\nVTL-independent KWS, involves training a single deep neural network (DNN) that\nutilizes VTL features with various warping factors. During training, a specific\nVTL feature is randomly selected per epoch, allowing the exploration of VTL\nvariations. During testing, the VTL features with different warping factors of\na test utterance are scored against the DNN and combined with equal weight. In\nthe second method scores the conventional features of a test utterance (without\nVTL warping) against the DNN. The third method, VTL-concatenation KWS,\nconcatenates VTL warped features to form high-dimensional features for KWS.\nEvaluations carried out on the English Google Command dataset demonstrate that\nthe proposed methods improve the accuracy of KWS.", "published": "2025-01-07 04:38:28", "link": "http://arxiv.org/abs/2501.03523v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AADNet: Exploring EEG Spatiotemporal Information for Fast and Accurate\n  Orientation and Timbre Detection of Auditory Attention Based on A Cue-Masked\n  Paradigm", "abstract": "Auditory attention decoding from electroencephalogram (EEG) could infer to\nwhich source the user is attending in noisy environments. Decoding algorithms\nand experimental paradigm designs are crucial for the development of technology\nin practical applications. To simulate real-world scenarios, this study\nproposed a cue-masked auditory attention paradigm to avoid information leakage\nbefore the experiment. To obtain high decoding accuracy with low latency, an\nend-to-end deep learning model, AADNet, was proposed to exploit the\nspatiotemporal information from the short time window of EEG signals. The\nresults showed that with a 0.5-second EEG window, AADNet achieved an average\naccuracy of 93.46% and 91.09% in decoding auditory orientation attention (OA)\nand timbre attention (TA), respectively. It significantly outperformed five\nprevious methods and did not need the knowledge of the original audio source.\nThis work demonstrated that it was possible to detect the orientation and\ntimbre of auditory attention from EEG signals fast and accurately. The results\nare promising for the real-time multi-property auditory attention decoding,\nfacilitating the application of the neuro-steered hearing aids and other\nassistive listening devices.", "published": "2025-01-07 06:51:17", "link": "http://arxiv.org/abs/2501.03571v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.LG"}
{"title": "Effective and Efficient Mixed Precision Quantization of Speech\n  Foundation Models", "abstract": "This paper presents a novel mixed-precision quantization approach for speech\nfoundation models that tightly integrates mixed-precision learning and\nquantized model parameter estimation into one single model compression stage.\nExperiments conducted on LibriSpeech dataset with fine-tuned wav2vec2.0-base\nand HuBERT-large models suggest the resulting mixed-precision quantized models\nincreased the lossless compression ratio by factors up to 1.7x and 1.9x over\nthe respective uniform-precision and two-stage mixed-precision quantized\nbaselines that perform precision learning and model parameters quantization in\nseparate and disjointed stages, while incurring no statistically word error\nrate (WER) increase over the 32-bit full-precision models. The system\ncompression time of wav2vec2.0-base and HuBERT-large models is reduced by up to\n1.9 and 1.5 times over the two-stage mixed-precision baselines, while both\nproduce lower WERs. The best-performing 3.5-bit mixed-precision quantized\nHuBERT-large model produces a lossless compression ratio of 8.6x over the\n32-bit full-precision system.", "published": "2025-01-07 09:21:52", "link": "http://arxiv.org/abs/2501.03643v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MAJL: A Model-Agnostic Joint Learning Framework for Music Source\n  Separation and Pitch Estimation", "abstract": "Music source separation and pitch estimation are two vital tasks in music\ninformation retrieval. Typically, the input of pitch estimation is obtained\nfrom the output of music source separation. Therefore, existing methods have\ntried to perform these two tasks simultaneously, so as to leverage the mutually\nbeneficial relationship between both tasks. However, these methods still face\ntwo critical challenges that limit the improvement of both tasks: the lack of\nlabeled data and joint learning optimization. To address these challenges, we\npropose a Model-Agnostic Joint Learning (MAJL) framework for both tasks. MAJL\nis a generic framework and can use variant models for each task. It includes a\ntwo-stage training method and a dynamic weighting method named Dynamic Weights\non Hard Samples (DWHS), which addresses the lack of labeled data and joint\nlearning optimization, respectively. Experimental results on public music\ndatasets show that MAJL outperforms state-of-the-art methods on both tasks,\nwith significant improvements of 0.92 in Signal-to-Distortion Ratio (SDR) for\nmusic source separation and 2.71% in Raw Pitch Accuracy (RPA) for pitch\nestimation. Furthermore, comprehensive studies not only validate the\neffectiveness of each component of MAJL, but also indicate the great generality\nof MAJL in adapting to different model architectures.", "published": "2025-01-07 10:38:51", "link": "http://arxiv.org/abs/2501.03689v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NeuroIncept Decoder for High-Fidelity Speech Reconstruction from Neural\n  Activity", "abstract": "This paper introduces a novel algorithm designed for speech synthesis from\nneural activity recordings obtained using invasive electroencephalography (EEG)\ntechniques. The proposed system offers a promising communication solution for\nindividuals with severe speech impairments. Central to our approach is the\nintegration of time-frequency features in the high-gamma band computed from EEG\nrecordings with an advanced NeuroIncept Decoder architecture. This neural\nnetwork architecture combines Convolutional Neural Networks (CNNs) and Gated\nRecurrent Units (GRUs) to reconstruct audio spectrograms from neural patterns.\nOur model demonstrates robust mean correlation coefficients between predicted\nand actual spectrograms, though inter-subject variability indicates distinct\nneural processing mechanisms among participants. Overall, our study highlights\nthe potential of neural decoding techniques to restore communicative abilities\nin individuals with speech disorders and paves the way for future advancements\nin brain-computer interface technologies.", "published": "2025-01-07 12:56:51", "link": "http://arxiv.org/abs/2501.03757v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-label Cross-lingual automatic music genre classification from\n  lyrics with Sentence BERT", "abstract": "Music genres are shaped by both the stylistic features of songs and the\ncultural preferences of artists' audiences. Automatic classification of music\ngenres using lyrics can be useful in several applications such as\nrecommendation systems, playlist creation, and library organization. We present\na multi-label, cross-lingual genre classification system based on multilingual\nsentence embeddings generated by sBERT. Using a bilingual Portuguese-English\ndataset with eight overlapping genres, we demonstrate the system's ability to\ntrain on lyrics in one language and predict genres in another. Our approach\noutperforms the baseline approach of translating lyrics and using a\nbag-of-words representation, improving the genrewise average F1-Score from 0.35\nto 0.69. The classifier uses a one-vs-all architecture, enabling it to assign\nmultiple genre labels to a single lyric. Experimental results reveal that\ndataset centralization notably improves cross-lingual performance. This\napproach offers a scalable solution for genre classification across\nunderrepresented languages and cultural domains, advancing the capabilities of\nmusic information retrieval systems.", "published": "2025-01-07 13:22:35", "link": "http://arxiv.org/abs/2501.03769v1", "categories": ["cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
