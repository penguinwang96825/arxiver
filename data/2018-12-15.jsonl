{"title": "Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the\n  Embeddings of Words and Entities from Wikipedia", "abstract": "The embeddings of entities in a large knowledge base (e.g., Wikipedia) are\nhighly beneficial for solving various natural language tasks that involve real\nworld knowledge. In this paper, we present Wikipedia2Vec, a Python-based\nopen-source tool for learning the embeddings of words and entities from\nWikipedia. The proposed tool enables users to learn the embeddings efficiently\nby issuing a single command with a Wikipedia dump file as an argument. We also\nintroduce a web-based demonstration of our tool that allows users to visualize\nand explore the learned embeddings. In our experiments, our tool achieved a\nstate-of-the-art result on the KORE entity relatedness dataset, and competitive\nresults on various standard benchmark datasets. Furthermore, our tool has been\nused as a key component in various recent studies. We publicize the source\ncode, demonstration, and the pretrained embeddings for 12 languages at\nhttps://wikipedia2vec.github.io.", "published": "2018-12-15 12:51:39", "link": "http://arxiv.org/abs/1812.06280v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages", "abstract": "Cross-lingual speech emotion recognition is an important task for practical\napplications. The performance of automatic speech emotion recognition systems\ndegrades in cross-corpus scenarios, particularly in scenarios involving\nmultiple languages or a previously unseen language such as Urdu for which\nlimited or no data is available. In this study, we investigate the problem of\ncross-lingual emotion recognition for Urdu language and contribute URDU---the\nfirst ever spontaneous Urdu-language speech emotion database. Evaluations are\nperformed using three different Western languages against Urdu and experimental\nresults on different possible scenarios suggest various interesting aspects for\ndesigning more adaptive emotion recognition system for such limited languages.\nIn results, selecting training instances of multiple languages can deliver\ncomparable results to baseline and augmentation a fraction of testing language\ndata while training can help to boost accuracy for speech emotion recognition.\nURDU data is publicly available for further research.", "published": "2018-12-15 01:04:18", "link": "http://arxiv.org/abs/1812.10411v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "InverSynth: Deep Estimation of Synthesizer Parameter Configurations from\n  Audio Signals", "abstract": "Sound synthesis is a complex field that requires domain expertise. Manual\ntuning of synthesizer parameters to match a specific sound can be an exhaustive\ntask, even for experienced sound engineers. In this paper, we introduce\nInverSynth - an automatic method for synthesizer parameters tuning to match a\ngiven input sound. InverSynth is based on strided convolutional neural networks\nand is capable of inferring the synthesizer parameters configuration from the\ninput spectrogram and even from the raw audio. The effectiveness InverSynth is\ndemonstrated on a subtractive synthesizer with four frequency modulated\noscillators, envelope generator and a gater effect. We present extensive\nquantitative and qualitative results that showcase the superiority InverSynth\nover several baselines. Furthermore, we show that the network depth is an\nimportant factor that contributes to the prediction accuracy.", "published": "2018-12-15 20:23:51", "link": "http://arxiv.org/abs/1812.06349v2", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
