{"title": "Self-supervised Text-to-SQL Learning with Header Alignment Training", "abstract": "Since we can leverage a large amount of unlabeled data without any human\nsupervision to train a model and transfer the knowledge to target tasks,\nself-supervised learning is a de-facto component for the recent success of deep\nlearning in various fields. However, in many cases, there is a discrepancy\nbetween a self-supervised learning objective and a task-specific objective. In\norder to tackle such discrepancy in Text-to-SQL task, we propose a novel\nself-supervised learning framework. We utilize the task-specific properties of\nText-to-SQL task and the underlying structures of table contents to train the\nmodels to learn useful knowledge of the \\textit{header-column} alignment task\nfrom unlabeled table data. We are able to transfer the knowledge to the\nsupervised Text-to-SQL training with annotated samples, so that the model can\nleverage the knowledge to better perform the \\textit{header-span} alignment\ntask to predict SQL statements. Experimental results show that our\nself-supervised learning framework significantly improves the performance of\nthe existing strong BERT based models without using large external corpora. In\nparticular, our method is effective for training the model with scarce labeled\ndata. The source code of this work is available in GitHub.", "published": "2021-03-11 01:09:59", "link": "http://arxiv.org/abs/2103.06402v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MediaSum: A Large-scale Media Interview Dataset for Dialogue\n  Summarization", "abstract": "MediaSum, a large-scale media interview dataset consisting of 463.6K\ntranscripts with abstractive summaries. To create this dataset, we collect\ninterview transcripts from NPR and CNN and employ the overview and topic\ndescriptions as summaries. Compared with existing public corpora for dialogue\nsummarization, our dataset is an order of magnitude larger and contains complex\nmulti-party conversations from multiple domains. We conduct statistical\nanalysis to demonstrate the unique positional bias exhibited in the transcripts\nof televised and radioed interviews. We also show that MediaSum can be used in\ntransfer learning to improve a model's performance on other dialogue\nsummarization tasks.", "published": "2021-03-11 01:47:42", "link": "http://arxiv.org/abs/2103.06410v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LightMBERT: A Simple Yet Effective Method for Multilingual BERT\n  Distillation", "abstract": "The multilingual pre-trained language models (e.g, mBERT, XLM and XLM-R) have\nshown impressive performance on cross-lingual natural language understanding\ntasks. However, these models are computationally intensive and difficult to be\ndeployed on resource-restricted devices. In this paper, we propose a simple yet\neffective distillation method (LightMBERT) for transferring the cross-lingual\ngeneralization ability of the multilingual BERT to a small student model. The\nexperiment results empirically demonstrate the efficiency and effectiveness of\nLightMBERT, which is significantly better than the baselines and performs\ncomparable to the teacher mBERT.", "published": "2021-03-11 02:24:41", "link": "http://arxiv.org/abs/2103.06418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Answer Generation and Factuality for Reading\n  Comprehension Question-Answering", "abstract": "Question answering (QA) is an important use case on voice assistants. A\npopular approach to QA is extractive reading comprehension (RC) which finds an\nanswer span in a text passage. However, extractive answers are often unnatural\nin a conversational context which results in suboptimal user experience. In\nthis work, we investigate conversational answer generation for QA. We propose\nAnswerBART, an end-to-end generative RC model which combines answer generation\nfrom multiple passages with passage ranking and answerability. Moreover, a\nhurdle in applying generative RC are hallucinations where the answer is\nfactually inconsistent with the passage text. We leverage recent work from\nsummarization to evaluate factuality. Experiments show that AnswerBART\nsignificantly improves over previous best published results on MS MARCO 2.1\nNLGEN by 2.5 ROUGE-L and NarrativeQA by 9.4 ROUGE-L.", "published": "2021-03-11 06:53:07", "link": "http://arxiv.org/abs/2103.06500v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does the Magic of BERT Apply to Medical Code Assignment? A Quantitative\n  Study", "abstract": "Unsupervised pretraining is an integral part of many natural language\nprocessing systems, and transfer learning with language models has achieved\nremarkable results in many downstream tasks. In the clinical application of\nmedical code assignment, diagnosis and procedure codes are inferred from\nlengthy clinical notes such as hospital discharge summaries. However, it is not\nclear if pretrained models are useful for medical code prediction without\nfurther architecture engineering. This paper conducts a comprehensive\nquantitative analysis of various contextualized language models' performance,\npretrained in different domains, for medical code assignment from clinical\nnotes. We propose a hierarchical fine-tuning architecture to capture\ninteractions between distant words and adopt label-wise attention to exploit\nlabel information. Contrary to current trends, we demonstrate that a carefully\ntrained classical CNN outperforms attention-based models on a MIMIC-III subset\nwith frequent codes. Our empirical findings suggest directions for improving\nthe medical code assignment application.", "published": "2021-03-11 07:23:45", "link": "http://arxiv.org/abs/2103.06511v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DebIE: A Platform for Implicit and Explicit Debiasing of Word Embedding\n  Spaces", "abstract": "Recent research efforts in NLP have demonstrated that distributional word\nvector spaces often encode stereotypical human biases, such as racism and\nsexism. With word representations ubiquitously used in NLP models and\npipelines, this raises ethical issues and jeopardizes the fairness of language\ntechnologies. While there exists a large body of work on bias measures and\ndebiasing methods, to date, there is no platform that would unify these\nresearch efforts and make bias measuring and debiasing of representation spaces\nwidely accessible. In this work, we present DebIE, the first integrated\nplatform for (1) measuring and (2) mitigating bias in word embeddings. Given an\n(i) embedding space (users can choose between the predefined spaces or upload\ntheir own) and (ii) a bias specification (users can choose between existing\nbias specifications or create their own), DebIE can (1) compute several\nmeasures of implicit and explicit bias and modify the embedding space by\nexecuting two (mutually composable) debiasing models. DebIE's functionality can\nbe accessed through four different interfaces: (a) a web application, (b) a\ndesktop application, (c) a REST-ful API, and (d) as a command-line application.\nDebIE is available at: debie.informatik.uni-mannheim.de.", "published": "2021-03-11 10:55:00", "link": "http://arxiv.org/abs/2103.06598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASAP: A Chinese Review Dataset Towards Aspect Category Sentiment\n  Analysis and Rating Prediction", "abstract": "Sentiment analysis has attracted increasing attention in e-commerce. The\nsentiment polarities underlying user reviews are of great value for business\nintelligence. Aspect category sentiment analysis (ACSA) and review rating\nprediction (RP) are two essential tasks to detect the fine-to-coarse sentiment\npolarities. %Considering the sentiment of the aspects(ACSA) and the overall\nreview rating(RP) simultaneously has the potential to improve the overall\nperformance. ACSA and RP are highly correlated and usually employed jointly in\nreal-world e-commerce scenarios. While most public datasets are constructed for\nACSA and RP separately, which may limit the further exploitation of both tasks.\nTo address the problem and advance related researches, we present a large-scale\nChinese restaurant review dataset \\textbf{ASAP} including $46,730$ genuine\nreviews from a leading online-to-offline (O2O) e-commerce platform in China.\nBesides a $5$-star scale rating, each review is manually annotated according to\nits sentiment polarities towards $18$ pre-defined aspect categories. We hope\nthe release of the dataset could shed some light on the fields of sentiment\nanalysis. Moreover, we propose an intuitive yet effective joint model for ACSA\nand RP. Experimental results demonstrate that the joint model outperforms\nstate-of-the-art baselines on both tasks.", "published": "2021-03-11 11:00:59", "link": "http://arxiv.org/abs/2103.06605v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain State Tracking for a Simplified Dialogue System", "abstract": "Task-oriented dialogue systems aim to help users achieve their goals in\nspecific domains. Recent neural dialogue systems use the entire dialogue\nhistory for abundant contextual information accumulated over multiple\nconversational turns. However, the dialogue history becomes increasingly longer\nas the number of turns increases, thereby increasing memory usage and\ncomputational costs. In this paper, we present DoTS (Domain State Tracking for\na Simplified Dialogue System), a task-oriented dialogue system that uses a\nsimplified input context instead of the entire dialogue history. However,\nneglecting the dialogue history can result in a loss of contextual information\nfrom previous conversational turns. To address this issue, DoTS tracks the\ndomain state in addition to the belief state and uses it for the input context.\nUsing this simplified input, DoTS improves the inform rate and success rate by\n1.09 points and 1.24 points, respectively, compared to the previous\nstate-of-the-art model on MultiWOZ, which is a well-known benchmark.", "published": "2021-03-11 13:00:54", "link": "http://arxiv.org/abs/2103.06648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained\n  Language Models", "abstract": "In this paper, we explore the effects of language variants, data sizes, and\nfine-tuning task types in Arabic pre-trained language models. To do so, we\nbuild three pre-trained language models across three variants of Arabic: Modern\nStandard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a\nfourth language model which is pre-trained on a mix of the three. We also\nexamine the importance of pre-training data size by building additional models\nthat are pre-trained on a scaled-down set of the MSA variant. We compare our\ndifferent models to each other, as well as to eight publicly available models\nby fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest\nthat the variant proximity of pre-training data to fine-tuning data is more\nimportant than the pre-training data size. We exploit this insight in defining\nan optimized system selection model for the studied tasks.", "published": "2021-03-11 14:11:43", "link": "http://arxiv.org/abs/2103.06678v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Transfer Learning in Multilingual Neural Machine\n  Translation with Cross-Lingual Word Embeddings", "abstract": "In this work we look into adding a new language to a multilingual NMT system\nin an unsupervised fashion. Under the utilization of pre-trained cross-lingual\nword embeddings we seek to exploit a language independent multilingual sentence\nrepresentation to easily generalize to a new language. While using\ncross-lingual embeddings for word lookup we decode from a yet entirely unseen\nsource language in a process we call blind decoding. Blindly decoding from\nPortuguese using a basesystem containing several Romance languages we achieve\nscores of 36.4 BLEU for Portuguese-English and 12.8 BLEU for Russian-English.\nIn an attempt to train the mapping from the encoder sentence representation to\na new target language we use our model as an autoencoder. Merely training to\ntranslate from Portuguese to Portuguese while freezing the encoder we achieve\n26 BLEU on English-Portuguese, and up to 28 BLEU when adding artificial noise\nto the input. Lastly we explore a more practical adaptation approach through\nnon-iterative backtranslation, exploiting our model's ability to produce high\nquality translations through blind decoding. This yields us up to 34.6 BLEU on\nEnglish-Portuguese, attaining near parity with a model adapted on real\nbilingual data.", "published": "2021-03-11 14:22:08", "link": "http://arxiv.org/abs/2103.06689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding", "abstract": "Generating metaphors is a challenging task as it requires a proper\nunderstanding of abstract concepts, making connections between unrelated\nconcepts, and deviating from the literal meaning. In this paper, we aim to\ngenerate a metaphoric sentence given a literal expression by replacing relevant\nverbs. Based on a theoretically-grounded connection between metaphors and\nsymbols, we propose a method to automatically construct a parallel corpus by\ntransforming a large number of metaphorical sentences from the Gutenberg Poetry\ncorpus (Jacobs, 2018) to their literal counterpart using recent advances in\nmasked language modeling coupled with commonsense inference. For the generation\ntask, we incorporate a metaphor discriminator to guide the decoding of a\nsequence to sequence model fine-tuned on our parallel data to generate\nhigh-quality metaphors. Human evaluation on an independent test set of literal\nstatements shows that our best model generates metaphors better than three\nwell-crafted baselines 66% of the time on average. A task-based evaluation\nshows that human-written poems enhanced with metaphors proposed by our model\nare preferred 68% of the time compared to poems without metaphors.", "published": "2021-03-11 16:39:19", "link": "http://arxiv.org/abs/2103.06779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Continual Learning for Multilingual Machine Translation via\n  Vocabulary Substitution", "abstract": "We propose a straightforward vocabulary adaptation scheme to extend the\nlanguage capacity of multilingual machine translation models, paving the way\ntowards efficient continual learning for multilingual machine translation. Our\napproach is suitable for large-scale datasets, applies to distant languages\nwith unseen scripts, incurs only minor degradation on the translation\nperformance for the original language pairs and provides competitive\nperformance even in the case where we only possess monolingual data for the new\nlanguages.", "published": "2021-03-11 17:10:21", "link": "http://arxiv.org/abs/2103.06799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Morphological Embeddings for English and Russian Languages", "abstract": "This paper evaluates morphology-based embeddings for English and Russian\nlanguages. Despite the interest and introduction of several morphology-based\nword embedding models in the past and acclaimed performance improvements on\nword similarity and language modeling tasks, in our experiments, we did not\nobserve any stable preference over two of our baseline models - SkipGram and\nFastText. The performance exhibited by morphological embeddings is the average\nof the two baselines mentioned above.", "published": "2021-03-11 09:34:57", "link": "http://arxiv.org/abs/2103.06884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Anaphoric Binding: an integrated overview", "abstract": "The interpretation of anaphors depends on their antecedents as the semantic\nvalue that an anaphor eventually conveys is co-specified by the value of its\nantecedent. Interestingly, when occurring in a given syntactic position,\ndifferent anaphors may have different sets of admissible antecedents. Such\ndifferences are the basis for the categorization of anaphoric expressions\naccording to their anaphoric capacity, being important to determine what are\nthe sets of admissible antecedents and how to represent and process this\nanaphoric capacity for each type of anaphor.\n  From an empirical perspective, these constraints stem from what appears as\nquite cogent generalisations and exhibit a universal character, given their\ncross linguistic validity. From a conceptual point of view, in turn, the\nrelations among binding constraints involve non-trivial cross symmetry, which\nlends them a modular nature and provides further strength to the plausibility\nof their universal character. This kind of anaphoric binding constraints\nappears thus as a most significant subset of natural language knowledge,\nusually referred to as binding theory.\n  This paper provides an integrated overview of these constraints holding on\nthe pairing of nominal anaphors with their admissible antecedents that are\nbased on grammatical relations and structure. Along with the increasing\ninterest on neuro-symbolic approaches to natural language, this paper seeks to\ncontribute to revive the interest on this most intriguing research topic.", "published": "2021-03-11 19:48:36", "link": "http://arxiv.org/abs/2103.06924v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preregistering NLP Research", "abstract": "Preregistration refers to the practice of specifying what you are going to\ndo, and what you expect to find in your study, before carrying out the study.\nThis practice is increasingly common in medicine and psychology, but is rarely\ndiscussed in NLP. This paper discusses preregistration in more detail, explores\nhow NLP researchers could preregister their work, and presents several\npreregistration questions for different kinds of studies. Finally, we argue in\nfavour of registered reports, which could provide firmer grounds for slow\nscience in NLP research. The goal of this paper is to elicit a discussion in\nthe NLP community, which we hope to synthesise into a general NLP\npreregistration form in future research.", "published": "2021-03-11 20:44:31", "link": "http://arxiv.org/abs/2103.06944v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Policies for Multilingual Training of Neural Machine\n  Translation Systems", "abstract": "Low-resource Multilingual Neural Machine Translation (MNMT) is typically\ntasked with improving the translation performance on one or more language pairs\nwith the aid of high-resource language pairs. In this paper, we propose two\nsimple search based curricula -- orderings of the multilingual training data --\nwhich help improve translation performance in conjunction with existing\ntechniques such as fine-tuning. Additionally, we attempt to learn a curriculum\nfor MNMT from scratch jointly with the training of the translation system with\nthe aid of contextual multi-arm bandits. We show on the FLORES low-resource\ntranslation dataset that these learned curricula can provide better starting\npoints for fine tuning and improve overall performance of the translation\nsystem.", "published": "2021-03-11 21:38:04", "link": "http://arxiv.org/abs/2103.06964v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Feature Weights using Reward Modeling for Denoising Parallel\n  Corpora", "abstract": "Large web-crawled corpora represent an excellent resource for improving the\nperformance of Neural Machine Translation (NMT) systems across several language\npairs. However, since these corpora are typically extremely noisy, their use is\nfairly limited. Current approaches to dealing with this problem mainly focus on\nfiltering using heuristics or single features such as language model scores or\nbi-lingual similarity. This work presents an alternative approach which learns\nweights for multiple sentence-level features. These feature weights which are\noptimized directly for the task of improving translation performance, are used\nto score and filter sentences in the noisy corpora more effectively. We provide\nresults of applying this technique to building NMT systems using the Paracrawl\ncorpus for Estonian-English and show that it beats strong single feature\nbaselines and hand designed combinations. Additionally, we analyze the\nsensitivity of this method to different types of noise and explore if the\nlearned weights generalize to other language pairs using the Maltese-English\nParacrawl corpus.", "published": "2021-03-11 21:45:45", "link": "http://arxiv.org/abs/2103.06968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text\n  Encoders", "abstract": "Pretrained text encoders, such as BERT, have been applied increasingly in\nvarious natural language processing (NLP) tasks, and have recently demonstrated\nsignificant performance gains. However, recent studies have demonstrated the\nexistence of social bias in these pretrained NLP models. Although prior works\nhave made progress on word-level debiasing, improved sentence-level fairness of\npretrained encoders still lacks exploration. In this paper, we proposed the\nfirst neural debiasing method for a pretrained sentence encoder, which\ntransforms the pretrained encoder outputs into debiased representations via a\nfair filter (FairFil) network. To learn the FairFil, we introduce a contrastive\nlearning framework that not only minimizes the correlation between filtered\nembeddings and bias words but also preserves rich semantic information of the\noriginal sentences. On real-world datasets, our FairFil effectively reduces the\nbias degree of pretrained text encoders, while continuously showing desirable\nperformance on downstream tasks. Moreover, our post-hoc method does not require\nany retraining of the text encoders, further enlarging FairFil's application\nspace.", "published": "2021-03-11 02:01:14", "link": "http://arxiv.org/abs/2103.06413v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Topical Language Generation using Transformers", "abstract": "Large-scale transformer-based language models (LMs) demonstrate impressive\ncapabilities in open text generation. However, controlling the generated text's\nproperties such as the topic, style, and sentiment is challenging and often\nrequires significant changes to the model architecture or retraining and\nfine-tuning the model on new supervised data. This paper presents a novel\napproach for Topical Language Generation (TLG) by combining a pre-trained LM\nwith topic modeling information. We cast the problem using Bayesian probability\nformulation with topic probabilities as a prior, LM probabilities as the\nlikelihood, and topical language generation probability as the posterior. In\nlearning the model, we derive the topic probability distribution from the\nuser-provided document's natural structure. Furthermore, we extend our model by\nintroducing new parameters and functions to influence the quantity of the\ntopical features presented in the generated text. This feature would allow us\nto easily control the topical properties of the generated text. Our\nexperimental results demonstrate that our model outperforms the\nstate-of-the-art results on coherency, diversity, and fluency while being\nfaster in decoding.", "published": "2021-03-11 03:45:24", "link": "http://arxiv.org/abs/2103.06434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings", "abstract": "Cross-lingual word embeddings (CLWE) have been proven useful in many\ncross-lingual tasks. However, most existing approaches to learn CLWE including\nthe ones with contextual embeddings are sense agnostic. In this work, we\npropose a novel framework to align contextual embeddings at the sense level by\nleveraging cross-lingual signal from bilingual dictionaries only. We\noperationalize our framework by first proposing a novel sense-aware cross\nentropy loss to model word senses explicitly. The monolingual ELMo and BERT\nmodels pretrained with our sense-aware cross entropy loss demonstrate\nsignificant performance improvement for word sense disambiguation tasks. We\nthen propose a sense alignment objective on top of the sense-aware cross\nentropy loss for cross-lingual model pretraining, and pretrain cross-lingual\nmodels for several language pairs (English to German/Spanish/Japanese/Chinese).\nCompared with the best baseline results, our cross-lingual models achieve\n0.52%, 2.09% and 1.29% average performance improvements on zero-shot\ncross-lingual NER, sentiment classification and XNLI tasks, respectively.", "published": "2021-03-11 04:55:35", "link": "http://arxiv.org/abs/2103.06459v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of Morphological Embeddings for the Russian Language", "abstract": "A number of morphology-based word embedding models were introduced in recent\nyears. However, their evaluation was mostly limited to English, which is known\nto be a morphologically simple language. In this paper, we explore whether and\nto what extent incorporating morphology into word embeddings improves\nperformance on downstream NLP tasks, in the case of morphologically rich\nRussian language. NLP tasks of our choice are POS tagging, Chunking, and NER --\nfor Russian language, all can be mostly solved using only morphology without\nunderstanding the semantics of words. Our experiments show that\nmorphology-based embeddings trained with Skipgram objective do not outperform\nexisting embedding model -- FastText. Moreover, a more complex, but morphology\nunaware model, BERT, allows to achieve significantly greater performance on the\ntasks that presumably require understanding of a word's morphology.", "published": "2021-03-11 11:59:11", "link": "http://arxiv.org/abs/2103.06628v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Question Answering using Graph-Pattern Isomorphism", "abstract": "Knowledge Graph Question Answering (KGQA) systems are based on machine\nlearning algorithms, requiring thousands of question-answer pairs as training\nexamples or natural language processing pipelines that need module fine-tuning.\nIn this paper, we present a novel QA approach, dubbed TeBaQA. Our approach\nlearns to answer questions based on graph isomorphisms from basic graph\npatterns of SPARQL queries. Learning basic graph patterns is efficient due to\nthe small number of possible patterns. This novel paradigm reduces the amount\nof training data necessary to achieve state-of-the-art performance. TeBaQA also\nspeeds up the domain adaption process by transforming the QA system development\ntask into a much smaller and easier data compilation task. In our evaluation,\nTeBaQA achieves state-of-the-art performance on QALD-8 and delivers comparable\nresults on QALD-9 and LC-QuAD v1. Additionally, we performed a fine-grained\nevaluation on complex queries that deal with aggregation and superlative\nquestions as well as an ablation study, highlighting future research\nchallenges.", "published": "2021-03-11 16:03:24", "link": "http://arxiv.org/abs/2103.06752v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ENTRUST: Argument Reframing with Language Models and Entailment", "abstract": "Framing involves the positive or negative presentation of an argument or\nissue depending on the audience and goal of the speaker (Entman 1983).\nDifferences in lexical framing, the focus of our work, can have large effects\non peoples' opinions and beliefs. To make progress towards reframing arguments\nfor positive effects, we create a dataset and method for this task. We use a\nlexical resource for \"connotations\" to create a parallel corpus and propose a\nmethod for argument reframing that combines controllable text generation\n(positive connotation) with a post-decoding entailment component (same\ndenotation). Our results show that our method is effective compared to strong\nbaselines along the dimensions of fluency, meaning, and\ntrustworthiness/reduction of fear.", "published": "2021-03-11 16:15:13", "link": "http://arxiv.org/abs/2103.06758v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COVID-19 Smart Chatbot Prototype for Patient Monitoring", "abstract": "Many COVID-19 patients developed prolonged symptoms after the infection,\nincluding fatigue, delirium, and headache. The long-term health impact of these\nconditions is still not clear. It is necessary to develop a way to follow up\nwith these patients for monitoring their health status to support timely\nintervention and treatment. In the lack of sufficient human resources to follow\nup with patients, we propose a novel smart chatbot solution backed with machine\nlearning to collect information (i.e., generating digital diary) in a\npersonalized manner. In this article, we describe the design framework and\ncomponents of our prototype.", "published": "2021-03-11 17:37:55", "link": "http://arxiv.org/abs/2103.06816v2", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1; J.3"], "primary_category": "cs.CL"}
{"title": "CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language\n  Representation", "abstract": "Pipelined NLP systems have largely been superseded by end-to-end neural\nmodeling, yet nearly all commonly-used models still require an explicit\ntokenization step. While recent tokenization approaches based on data-derived\nsubword lexicons are less brittle than manually engineered tokenizers, these\ntechniques are not equally suited to all languages, and the use of any fixed\nvocabulary may limit a model's ability to adapt. In this paper, we present\nCANINE, a neural encoder that operates directly on character sequences, without\nexplicit tokenization or vocabulary, and a pre-training strategy that operates\neither directly on characters or optionally uses subwords as a soft inductive\nbias. To use its finer-grained input effectively and efficiently, CANINE\ncombines downsampling, which reduces the input sequence length, with a deep\ntransformer stack, which encodes context. CANINE outperforms a comparable mBERT\nmodel by 2.8 F1 on TyDi QA, a challenging multilingual benchmark, despite\nhaving 28% fewer model parameters.", "published": "2021-03-11 18:57:44", "link": "http://arxiv.org/abs/2103.06874v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU\n  Models", "abstract": "Recent studies indicate that NLU models are prone to rely on shortcut\nfeatures for prediction, without achieving true language understanding. As a\nresult, these models fail to generalize to real-world out-of-distribution data.\nIn this work, we show that the words in the NLU training set can be modeled as\na long-tailed distribution. There are two findings: 1) NLU models have strong\npreference for features located at the head of the long-tailed distribution,\nand 2) Shortcut features are picked up during very early few iterations of the\nmodel training. These two observations are further employed to formulate a\nmeasurement which can quantify the shortcut degree of each training sample.\nBased on this shortcut measurement, we propose a shortcut mitigation framework\nLTGR, to suppress the model from making overconfident predictions for samples\nwith large shortcut degree. Experimental results on three NLU benchmarks\ndemonstrate that our long-tailed distribution explanation accurately reflects\nthe shortcut learning behavior of NLU models. Experimental analysis further\nindicates that LTGR can improve the generalization accuracy on OOD data, while\npreserving the accuracy on in-distribution data.", "published": "2021-03-11 19:39:56", "link": "http://arxiv.org/abs/2103.06922v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterizing Partisan Political Narrative Frameworks about COVID-19 on\n  Twitter", "abstract": "The COVID-19 pandemic is a global crisis that has been testing every society\nand exposing the critical role of local politics in crisis response. In the\nUnited States, there has been a strong partisan divide between the Democratic\nand Republican party's narratives about the pandemic which resulted in\npolarization of individual behaviors and divergent policy adoption across\nregions. As shown in this case, as well as in most major social issues,\nstrongly polarized narrative frameworks facilitate such narratives. To\nunderstand polarization and other social chasms, it is critical to dissect\nthese diverging narratives. Here, taking the Democratic and Republican\npolitical social media posts about the pandemic as a case study, we demonstrate\nthat a combination of computational methods can provide useful insights into\nthe different contexts, framing, and characters and relationships that\nconstruct their narrative frameworks which individual posts source from.\nLeveraging a dataset of tweets from elite politicians in the U.S., we found\nthat the Democrats' narrative tends to be more concerned with the pandemic as\nwell as financial and social support, while the Republicans discuss more about\nother political entities such as China. We then perform an automatic framing\nanalysis to characterize the ways in which they frame their narratives, where\nwe found that the Democrats emphasize the government's role in responding to\nthe pandemic, and the Republicans emphasize the roles of individuals and\nsupport for small businesses. Finally, we present a semantic role analysis that\nuncovers the important characters and relationships in their narratives as well\nas how they facilitate a membership categorization process. Our findings\nconcretely expose the gaps in the \"elusive consensus\" between the two parties.\nOur methodologies may be applied to computationally study narratives in various\ndomains.", "published": "2021-03-11 21:24:41", "link": "http://arxiv.org/abs/2103.06960v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Full Page Handwriting Recognition via Image to Sequence Extraction", "abstract": "We present a Neural Network based Handwritten Text Recognition (HTR) model\narchitecture that can be trained to recognize full pages of handwritten or\nprinted text without image segmentation. Being based on Image to Sequence\narchitecture, it can extract text present in an image and then sequence it\ncorrectly without imposing any constraints regarding orientation, layout and\nsize of text and non-text. Further, it can also be trained to generate\nauxiliary markup related to formatting, layout and content. We use character\nlevel vocabulary, thereby enabling language and terminology of any subject. The\nmodel achieves a new state-of-art in paragraph level recognition on the IAM\ndataset. When evaluated on scans of real world handwritten free form test\nanswers - beset with curved and slanted lines, drawings, tables, math,\nchemistry and other symbols - it performs better than all commercially\navailable HTR cloud APIs. It is deployed in production as part of a commercial\nweb application.", "published": "2021-03-11 04:37:29", "link": "http://arxiv.org/abs/2103.06450v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Learning Word-Level Confidence For Subword End-to-End ASR", "abstract": "We study the problem of word-level confidence estimation in subword-based\nend-to-end (E2E) models for automatic speech recognition (ASR). Although prior\nworks have proposed training auxiliary confidence models for ASR systems, they\ndo not extend naturally to systems that operate on word-pieces (WP) as their\nvocabulary. In particular, ground truth WP correctness labels are needed for\ntraining confidence models, but the non-unique tokenization from word to WP\ncauses inaccurate labels to be generated. This paper proposes and studies two\nconfidence models of increasing complexity to solve this problem. The final\nmodel uses self-attention to directly learn word-level confidence without\nneeding subword tokenization, and exploits full context features from multiple\nhypotheses to improve confidence accuracy. Experiments on Voice Search and\nlong-tail test sets show standard metrics (e.g., NCE, AUC, RMSE) improving\nsubstantially. The proposed confidence module also enables a model selection\napproach to combine an on-device E2E model with a hybrid model on the server to\naddress the rare word recognition problem for the E2E model.", "published": "2021-03-11 15:03:33", "link": "http://arxiv.org/abs/2103.06716v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Intelligent behavior depends on the ecological niche: Scaling up AI to\n  human-like intelligence in socio-cultural environments", "abstract": "This paper outlines a perspective on the future of AI, discussing directions\nfor machines models of human-like intelligence. We explain how developmental\nand evolutionary theories of human cognition should further inform artificial\nintelligence. We emphasize the role of ecological niches in sculpting\nintelligent behavior, and in particular that human intelligence was\nfundamentally shaped to adapt to a constantly changing socio-cultural\nenvironment. We argue that a major limit of current work in AI is that it is\nmissing this perspective, both theoretically and experimentally. Finally, we\ndiscuss the promising approach of developmental artificial intelligence,\nmodeling infant development through multi-scale interaction between\nintrinsically motivated learning, embodiment and a fastly changing\nsocio-cultural environment. This paper takes the form of an interview of\nPierre-Yves Oudeyer by Mandred Eppe, organized within the context of a KI -\nK{\\\"{u}}nstliche Intelligenz special issue in developmental robotics.", "published": "2021-03-11 16:24:00", "link": "http://arxiv.org/abs/2103.06769v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Imagined-Trailing-Whitespace-Agnostic Levenshtein Distance For Plaintext\n  Table Detection", "abstract": "The standard algorithm for Levenshtein distance, treats trailing whitespace\nthe same as any other letter or symbol. However, when humans compare 2 strings,\nwe implicitly assume that both strings are padded by infinite trailing\nwhitespace. This informs our expectations for what the costs for insertion,\ndeletion and replacement, should be. This violation of our expectations results\nin non-intuitive edit distance values. To account for this specific human\nintuition, a naive approach which considers \"all possible\" substrings of\ntrailing whitespace would yield an $O(n^3)$ algorithm. In this work, we provide\nan efficient $O(n^2)$ algorithm to compute the same. Keywords: Imagined\nInfinite Trailing Whitespace, Human Friendly, Intuitive Edit Distance, Table\nDetection, Table Alignment", "published": "2021-03-11 20:39:40", "link": "http://arxiv.org/abs/2103.06942v1", "categories": ["cs.DS", "cs.CL", "cs.IR"], "primary_category": "cs.DS"}
{"title": "Active$^2$ Learning: Actively reducing redundancies in Active Learning\n  methods for Sequence Tagging and Machine Translation", "abstract": "While deep learning is a powerful tool for natural language processing (NLP)\nproblems, successful solutions to these problems rely heavily on large amounts\nof annotated samples. However, manually annotating data is expensive and\ntime-consuming. Active Learning (AL) strategies reduce the need for huge\nvolumes of labeled data by iteratively selecting a small number of examples for\nmanual annotation based on their estimated utility in training the given model.\nIn this paper, we argue that since AL strategies choose examples independently,\nthey may potentially select similar examples, all of which may not contribute\nsignificantly to the learning process. Our proposed approach,\nActive$\\mathbf{^2}$ Learning (A$\\mathbf{^2}$L), actively adapts to the deep\nlearning model being trained to eliminate further such redundant examples\nchosen by an AL strategy. We show that A$\\mathbf{^2}$L is widely applicable by\nusing it in conjunction with several different AL strategies and NLP tasks. We\nempirically demonstrate that the proposed approach is further able to reduce\nthe data requirements of state-of-the-art AL strategies by an absolute\npercentage reduction of $\\approx\\mathbf{3-25\\%}$ on multiple NLP tasks while\nachieving the same performance with no additional computation overhead.", "published": "2021-03-11 06:27:31", "link": "http://arxiv.org/abs/2103.06490v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Forward-Backward Convolutional Recurrent Neural Networks and\n  Tag-Conditioned Convolutional Neural Networks for Weakly Labeled\n  Semi-supervised Sound Event Detection", "abstract": "In this paper we present our system for the detection and classification of\nacoustic scenes and events (DCASE) 2020 Challenge Task 4: Sound event detection\nand separation in domestic environments. We introduce two new models: the\nforward-backward convolutional recurrent neural network (FBCRNN) and the\ntag-conditioned convolutional neural network (CNN). The FBCRNN employs two\nrecurrent neural network (RNN) classifiers sharing the same CNN for\npreprocessing. With one RNN processing a recording in forward direction and the\nother in backward direction, the two networks are trained to jointly predict\naudio tags, i.e., weak labels, at each time step within a recording, given that\nat each time step they have jointly processed the whole recording. The proposed\ntraining encourages the classifiers to tag events as soon as possible.\nTherefore, after training, the networks can be applied to shorter audio\nsegments of, e.g., 200 ms, allowing sound event detection (SED). Further, we\npropose a tag-conditioned CNN to complement SED. It is trained to predict\nstrong labels while using (predicted) tags, i.e., weak labels, as additional\ninput. For training pseudo strong labels from a FBCRNN ensemble are used. The\npresented system scored the fourth and third place in the systems and teams\nrankings, respectively. Subsequent improvements allow our system to even\noutperform the challenge baseline and winner systems in average by,\nrespectively, 18.0% and 2.2% event-based F1-score on the validation set. Source\ncode is publicly available at https://github.com/fgnt/pb_sed.", "published": "2021-03-11 10:18:22", "link": "http://arxiv.org/abs/2103.06581v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Multiway Canonical Correlation Analysis for Multi-Subject EEG\n  Normalization", "abstract": "The normalization of brain recordings from multiple subjects responding to\nthe natural stimuli is one of the key challenges in auditory neuroscience. The\nobjective of this normalization is to transform the brain data in such a way as\nto remove the inter-subject redundancies and to boost the component related to\nthe stimuli. In this paper, we propose a deep learning framework to improve the\ncorrelation of electroencephalography (EEG) data recorded from multiple\nsubjects engaged in an audio listening task. The proposed model extends the\nlinear multi-way canonical correlation analysis (CCA) for audio-EEG analysis\nusing an auto-encoder network with a shared encoder layer. The model is trained\nto optimize a combined loss involving correlation and reconstruction. The\nexperiments are performed on EEG data collected from subjects listening to\nnatural speech and music. In these experiments, we show that the proposed deep\nmulti-way CCA (DMCCA) based model significantly improves the correlations over\nthe linear multi-way CCA approach with absolute improvements of 0.08 and 0.29\nin terms of the Pearson correlation values for speech and music tasks\nrespectively.", "published": "2021-03-11 05:49:42", "link": "http://arxiv.org/abs/2103.06478v1", "categories": ["eess.AS", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Multi-Format Contrastive Learning of Audio Representations", "abstract": "Recent advances suggest the advantage of multi-modal training in comparison\nwith single-modal methods. In contrast to this view, in our work we find that\nsimilar gain can be obtained from training with different formats of a single\nmodality. In particular, we investigate the use of the contrastive learning\nframework to learn audio representations by maximizing the agreement between\nthe raw audio and its spectral representation. We find a significant gain using\nthis multi-format strategy against the single-format counterparts. Moreover, on\nthe downstream AudioSet and ESC-50 classification task, our audio-only approach\nachieves new state-of-the-art results with a mean average precision of 0.376\nand an accuracy of 90.5%, respectively.", "published": "2021-03-11 07:13:05", "link": "http://arxiv.org/abs/2103.06508v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Topological Data Analysis of Korean Music in Jeongganbo: A Cycle\n  Structure", "abstract": "Jeongganbo is a unique music representation invented by Sejong the Great.\nContrary to the western music notation, the pitch of each note is encrypted and\nthe length is visualized directly in a matrix form in Jeongganbo. We use\ntopological data analysis (TDA) to analyze the Korean music written in\nJeongganbo for Suyeonjang, Songuyeo, and Taryong, those well-known pieces\nplayed at the palace and among noble community. We are particularly interested\nin the cycle structure. We first define and determine the node elements of each\nmusic, characterized uniquely with its pitch and length. Then we transform the\nmusic into a graph and define the distance between the nodes as their adjacent\noccurrence rate. The graph is used as a point cloud whose homological structure\nis investigated by measuring the hole structure in each dimension. We identify\ncycles of each music, match those in Jeongganbo, and show how those cycles are\ninterconnected. The main discovery of this work is that the cycles of\nSuyeonjang and Songuyeo, categorized as a special type of cyclic music known as\nDodeuri, frequently overlap each other when appearing in the music while the\ncycles found in Taryong, which does not belong to Dodeuri class, appear\nindividually.", "published": "2021-03-11 11:42:02", "link": "http://arxiv.org/abs/2103.06620v2", "categories": ["cs.SD", "cs.CG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BYOL for Audio: Self-Supervised Learning for General-Purpose Audio\n  Representation", "abstract": "Inspired by the recent progress in self-supervised learning for computer\nvision that generates supervision using data augmentations, we explore a new\ngeneral-purpose audio representation learning approach. We propose learning\ngeneral-purpose audio representation from a single audio segment without\nexpecting relationships between different time segments of audio samples. To\nimplement this principle, we introduce Bootstrap Your Own Latent (BYOL) for\nAudio (BYOL-A, pronounced \"viola\"), an audio self-supervised learning method\nbased on BYOL for learning general-purpose audio representation. Unlike most\nprevious audio self-supervised learning methods that rely on agreement of\nvicinity audio segments or disagreement of remote ones, BYOL-A creates\ncontrasts in an augmented audio segment pair derived from a single audio\nsegment. With a combination of normalization and augmentation techniques,\nBYOL-A achieves state-of-the-art results in various downstream tasks. Extensive\nablation studies also clarified the contribution of each component and their\ncombinations.", "published": "2021-03-11 14:32:33", "link": "http://arxiv.org/abs/2103.06695v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "68T07"], "primary_category": "eess.AS"}
