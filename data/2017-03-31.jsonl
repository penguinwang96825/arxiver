{"title": "N-gram Language Modeling using Recurrent Neural Network Estimation", "abstract": "We investigate the effective memory depth of RNN models by using them for\n$n$-gram language model (LM) smoothing.\n  Experiments on a small corpus (UPenn Treebank, one million words of training\ndata and 10k vocabulary) have found the LSTM cell with dropout to be the best\nmodel for encoding the $n$-gram state when compared with feed-forward and\nvanilla RNN models. When preserving the sentence independence assumption the\nLSTM $n$-gram matches the LSTM LM performance for $n=9$ and slightly\noutperforms it for $n=13$. When allowing dependencies across sentence\nboundaries, the LSTM $13$-gram almost matches the perplexity of the unlimited\nhistory LSTM LM.\n  LSTM $n$-gram smoothing also has the desirable property of improving with\nincreasing $n$-gram order, unlike the Katz or Kneser-Ney back-off estimators.\nUsing multinomial distributions as targets in training instead of the usual\none-hot target is only slightly beneficial for low $n$-gram orders.\n  Experiments on the One Billion Words benchmark show that the results hold at\nlarger scale: while LSTM smoothing for short $n$-gram contexts does not provide\nsignificant advantages over classic N-gram models, it becomes effective with\nlong contexts ($n > 5$); depending on the task and amount of data it can match\nfully recurrent LSTM models at about $n=13$. This may have implications when\nmodeling short-format text, e.g. voice search/query LMs.\n  Building LSTM $n$-gram LMs may be appealing for some practical situations:\nthe state in a $n$-gram LM can be succinctly represented with $(n-1)*4$ bytes\nstoring the identity of the words in the context and batches of $n$-gram\ncontexts can be processed in parallel. On the downside, the $n$-gram context\nencoding computed by the LSTM is discarded, making the model more expensive\nthan a regular recurrent LSTM LM.", "published": "2017-03-31 01:21:40", "link": "http://arxiv.org/abs/1703.10724v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joining Hands: Exploiting Monolingual Treebanks for Parsing of\n  Code-mixing Data", "abstract": "In this paper, we propose efficient and less resource-intensive strategies\nfor parsing of code-mixed data. These strategies are not constrained by\nin-domain annotations, rather they leverage pre-existing monolingual annotated\nresources for training. We show that these methods can produce significantly\nbetter results as compared to an informed baseline. Besides, we also present a\ndata set of 450 Hindi and English code-mixed tweets of Hindi multilingual\nspeakers for evaluation. The data set is manually annotated with Universal\nDependencies.", "published": "2017-03-31 07:10:30", "link": "http://arxiv.org/abs/1703.10772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reading Wikipedia to Answer Open-Domain Questions", "abstract": "This paper proposes to tackle open- domain question answering using Wikipedia\nas the unique knowledge source: the answer to any factoid question is a text\nspan in a Wikipedia article. This task of machine reading at scale combines the\nchallenges of document retrieval (finding the relevant articles) with that of\nmachine comprehension of text (identifying the answer spans from those\narticles). Our approach combines a search component based on bigram hashing and\nTF-IDF matching with a multi-layer recurrent neural network model trained to\ndetect answers in Wikipedia paragraphs. Our experiments on multiple existing QA\ndatasets indicate that (1) both modules are highly competitive with respect to\nexisting counterparts and (2) multitask learning using distant supervision on\ntheir combination is an effective complete system on this challenging task.", "published": "2017-03-31 20:39:10", "link": "http://arxiv.org/abs/1704.00051v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One-Shot Neural Cross-Lingual Transfer for Paradigm Completion", "abstract": "We present a novel cross-lingual transfer method for paradigm completion, the\ntask of mapping a lemma to its inflected forms, using a neural encoder-decoder\nmodel, the state of the art for the monolingual task. We use labeled data from\na high-resource language to increase performance on a low-resource language. In\nexperiments on 21 language pairs from four different language families, we\nobtain up to 58% higher accuracy than without transfer and show that even\nzero-shot and one-shot learning are possible. We further find that the degree\nof language relatedness strongly influences the ability to transfer\nmorphological knowledge.", "published": "2017-03-31 20:39:38", "link": "http://arxiv.org/abs/1704.00052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frames: A Corpus for Adding Memory to Goal-Oriented Dialogue Systems", "abstract": "This paper presents the Frames dataset (Frames is available at\nhttp://datasets.maluuba.com/Frames), a corpus of 1369 human-human dialogues\nwith an average of 15 turns per dialogue. We developed this dataset to study\nthe role of memory in goal-oriented dialogue systems. Based on Frames, we\nintroduce a task called frame tracking, which extends state tracking to a\nsetting where several states are tracked simultaneously. We propose a baseline\nmodel for this task. We show that Frames can also be used to study memory in\ndialogue management and information presentation through natural language\ngeneration.", "published": "2017-03-31 21:03:58", "link": "http://arxiv.org/abs/1704.00057v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Simplification with Deep Reinforcement Learning", "abstract": "Sentence simplification aims to make sentences easier to read and understand.\nMost recent approaches draw on insights from machine translation to learn\nsimplification rewrites from monolingual corpora of complex and simple\nsentences. We address the simplification problem with an encoder-decoder model\ncoupled with a deep reinforcement learning framework. Our model, which we call\n{\\sc Dress} (as shorthand for {\\bf D}eep {\\bf RE}inforcement {\\bf S}entence\n{\\bf S}implification), explores the space of possible simplifications while\nlearning to optimize a reward function that encourages outputs which are\nsimple, fluent, and preserve the meaning of the input. Experiments on three\ndatasets demonstrate that our model outperforms competitive simplification\nsystems.", "published": "2017-03-31 15:05:45", "link": "http://arxiv.org/abs/1703.10931v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Discourse-level Diversity for Neural Dialog Models using\n  Conditional Variational Autoencoders", "abstract": "While recent neural encoder-decoder models have shown great promise in\nmodeling open-domain conversations, they often generate dull and generic\nresponses. Unlike past work that has focused on diversifying the output of the\ndecoder at word-level to alleviate this problem, we present a novel framework\nbased on conditional variational autoencoders that captures the discourse-level\ndiversity in the encoder. Our model uses latent variables to learn a\ndistribution over potential conversational intents and generates diverse\nresponses using only greedy decoders. We have further developed a novel variant\nthat is integrated with linguistic prior knowledge for better performance.\nFinally, the training procedure is improved by introducing a bag-of-word loss.\nOur proposed models have been validated to generate significantly more diverse\nresponses than baseline approaches and exhibit competence in discourse-level\ndecision-making.", "published": "2017-03-31 15:55:00", "link": "http://arxiv.org/abs/1703.10960v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Opinion Mining on Non-English Short Text", "abstract": "As the type and the number of such venues increase, automated analysis of\nsentiment on textual resources has become an essential data mining task. In\nthis paper, we investigate the problem of mining opinions on the collection of\ninformal short texts. Both positive and negative sentiment strength of texts\nare detected. We focus on a non-English language that has few resources for\ntext mining. This approach would help enhance the sentiment analysis in\nlanguages where a list of opinionated words does not exist. We propose a new\nmethod projects the text into dense and low dimensional feature vectors\naccording to the sentiment strength of the words. We detect the mixture of\npositive and negative sentiments on a multi-variant scale. Empirical evaluation\nof the proposed framework on Turkish tweets shows that our approach gets good\nresults for opinion mining.", "published": "2017-03-31 18:05:44", "link": "http://arxiv.org/abs/1704.00016v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Factorization tricks for LSTM networks", "abstract": "We present two simple ways of reducing the number of parameters and\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\ninputs and states into the independent groups. Both approaches allow us to\ntrain large LSTM networks significantly faster to the near state-of the art\nperplexity while using significantly less RNN parameters.", "published": "2017-03-31 00:50:37", "link": "http://arxiv.org/abs/1703.10722v3", "categories": ["cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
