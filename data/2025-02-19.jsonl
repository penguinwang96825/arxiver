{"title": "Slant/Gokigen Naname is NP-complete, and Some Variations are in P", "abstract": "In this paper we show that a generalized version of the Nikoli puzzle Slant\nis NP-complete. We also give polynomial time algorithms for versions of the\npuzzle where some constraints are omitted. These problems correspond to\nsimultaneously satisfying connectivity and vertex degree constraints in a grid\ngraph and its dual.", "published": "2025-02-19 08:41:55", "link": "http://arxiv.org/abs/2502.13536v1", "categories": ["cs.DM", "cs.CG", "cs.DS"], "primary_category": "cs.DM"}
{"title": "Elementary Cellular Automata as Multiplicative Automata", "abstract": "Elementary cellular automata (ECA) are converted into multiplicative versions\nby using permuted n-dim Galois fields and octonion multiplication tables as\nbinary pointers to each rule's Wolfram code truth table. This enables an\nextension of the binary ECA to complex numbers, identity solutions are found,\nproduces a polynomial, and is implemented in Java.", "published": "2025-02-19 01:49:34", "link": "http://arxiv.org/abs/2502.13360v1", "categories": ["nlin.CG", "cs.DM"], "primary_category": "nlin.CG"}
{"title": "Gaining efficiency in deep policy gradient method for continuous-time optimal control problems", "abstract": "In this paper, we propose an efficient implementation of deep policy gradient\nmethod (PGM) for optimal control problems in continuous time. The proposed\nmethod has the ability to manage the allocation of computational resources,\nnumber of trajectories, and complexity of architecture of the neural network.\nThis is, in particular, important for continuous-time problems that require a\nfine time discretization. Each step of this method focuses on a different time\nscale and learns a policy, modeled by a neural network, for a discretized\noptimal control problem. The first step has the coarsest time discretization.\nAs we proceed to other steps, the time discretization becomes finer. The\noptimal trained policy in each step is also used to provide data for the next\nstep. We accompany the multi-scale deep PGM with a theoretical result on\nallocation of computational resources to obtain a targeted efficiency and test\nour methods on the linear-quadratic stochastic optimal control problem.", "published": "2025-02-19 22:56:44", "link": "http://arxiv.org/abs/2502.14141v2", "categories": ["math.OC", "q-fin.CP", "49M25, 90-08"], "primary_category": "math.OC"}
{"title": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty", "abstract": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximisation frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-uncertainties and by switching\nattention from 'preference' to 'constraints'. Economic asset-pricing in this\nsetting is found to decompose into the viable pricing of model-risk and of\nnon-model risk separately such that the former has a unique and intuitive\nrisk-neutral equivalent formulation with convenient properties. Its parameter,\na dynamically conserved constant of model-risk inference, allows an integrated\nrepresentation of ex-ante risk-pricing and bias, such that their ex-post\nprice-effects can be disentangled, through well-known price anomalies such as\nMomentum and Low-Risk.", "published": "2025-02-19 14:07:37", "link": "http://arxiv.org/abs/2502.13744v3", "categories": ["q-fin.MF", "econ.EM"], "primary_category": "q-fin.MF"}
{"title": "Decentralized Annuity: A Quest for the Holy Grail of Lifetime Financial Security", "abstract": "This paper presents a novel framework for decentralized annuities, aiming to\naddress the limitations of traditional pension systems such as defined\ncontribution (DC) and defined benefit (DB) plans, while providing lifetime\nfinancial support. It sheds light on often ignored pitfalls within current\nretirement schemes and introduces individual rationality properties. The\nresearch delves into various fairness concepts that underpin existing plans,\nemphasizing that decentralized annuities, while meeting similar fairness\ncriteria, offer enhanced flexibility for individual rationality and improved\nsocial welfare for all participants. Using theoretical models and examples, we\ndemonstrate the potential of decentralized annuities to outperform self-managed\nplans (DC) and to produce effects comparable to defined benefit (DB) plans,\nparticularly within larger participant pools. The paper concludes by exploring\nthe managerial implications of decentralized annuities and laying the\ngroundwork for the further advancement of equitable and sustainable\ndecentralized annuity systems.", "published": "2025-02-19 14:06:29", "link": "http://arxiv.org/abs/2502.13742v1", "categories": ["q-fin.MF", "C61, D15, G11, G22"], "primary_category": "q-fin.MF"}
{"title": "Dual Formulation of the Optimal Consumption problem with Multiplicative Habit Formation", "abstract": "This paper provides a dual formulation of the optimal consumption problem\nwith internal multiplicative habit formation. In this problem, the agent\nderives utility from the ratio of consumption to the internal habit component.\nDue to this multiplicative specification of the habit model, the optimal\nconsumption problem is not strictly concave and incorporates irremovable\npath-dependency. As a consequence, standard Lagrangian techniques fail to\nsupply a candidate for the corresponding dual formulation. Using Fenchel's\nDuality Theorem, we manage to identify a candidate formulation and prove that\nit satisfies strong duality. On the basis of this strong duality result, we are\nable to derive duality relations that stipulate how the optimal primal controls\ndepend on the optimal dual controls and vice versa. {Moreover, using the dual\nformulation, we develop an analytical evaluation mechanism to bound the\naccuracy of approximations to the optimal solutions.", "published": "2025-02-19 12:37:26", "link": "http://arxiv.org/abs/2502.13678v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Stock Price Prediction Using a Hybrid LSTM-GNN Model: Integrating Time-Series and Graph-Based Analysis", "abstract": "This paper presents a novel hybrid model that integrates long-short-term\nmemory (LSTM) networks and Graph Neural Networks (GNNs) to significantly\nenhance the accuracy of stock market predictions. The LSTM component adeptly\ncaptures temporal patterns in stock price data, effectively modeling the time\nseries dynamics of financial markets. Concurrently, the GNN component leverages\nPearson correlation and association analysis to model inter-stock relational\ndata, capturing complex nonlinear polyadic dependencies influencing stock\nprices. The model is trained and evaluated using an expanding window validation\napproach, enabling continuous learning from increasing amounts of data and\nadaptation to evolving market conditions. Extensive experiments conducted on\nhistorical stock data demonstrate that our hybrid LSTM-GNN model achieves a\nmean square error (MSE) of 0.00144, representing a substantial reduction of\n10.6% compared to the MSE of the standalone LSTM model of 0.00161. Furthermore,\nthe hybrid model outperforms traditional and advanced benchmarks, including\nlinear regression, convolutional neural networks (CNN), and dense networks.\nThese compelling results underscore the significant potential of combining\ntemporal and relational data through a hybrid approach, offering a powerful\ntool for real-time trading and financial analysis.", "published": "2025-02-19 15:09:13", "link": "http://arxiv.org/abs/2502.15813v1", "categories": ["q-fin.ST", "cs.AI", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Deep Learning for VWAP Execution in Crypto Markets: Beyond the Volume Curve", "abstract": "Volume-Weighted Average Price (VWAP) is arguably the most prevalent benchmark\nfor trade execution as it provides an unbiased standard for comparing\nperformance across market participants. However, achieving VWAP is inherently\nchallenging due to its dependence on two dynamic factors, volumes and prices.\nTraditional approaches typically focus on forecasting the market's volume\ncurve, an assumption that may hold true under steady conditions but becomes\nsuboptimal in more volatile environments or markets such as cryptocurrency\nwhere prediction error margins are higher. In this study, I propose a deep\nlearning framework that directly optimizes the VWAP execution objective by\nbypassing the intermediate step of volume curve prediction. Leveraging\nautomatic differentiation and custom loss functions, my method calibrates order\nallocation to minimize VWAP slippage, thereby fully addressing the complexities\nof the execution problem. My results demonstrate that this direct optimization\napproach consistently achieves lower VWAP slippage compared to conventional\nmethods, even when utilizing a naive linear model presented in\narXiv:2410.21448. They validate the observation that strategies optimized for\nVWAP performance tend to diverge from accurate volume curve predictions and\nthus underscore the advantage of directly modeling the execution objective.\nThis research contributes a more efficient and robust framework for VWAP\nexecution in volatile markets, illustrating the potential of deep learning in\ncomplex financial systems where direct objective optimization is crucial.\nAlthough my empirical analysis focuses on cryptocurrency markets, the\nunderlying principles of the framework are readily applicable to other asset\nclasses such as equities.", "published": "2025-02-19 13:49:51", "link": "http://arxiv.org/abs/2502.13722v2", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Craw4LLM: Efficient Web Crawling for LLM Pretraining", "abstract": "Web crawl is a main source of large language models' (LLMs) pretraining data,\nbut the majority of crawled web pages are discarded in pretraining due to low\ndata quality. This paper presents Craw4LLM, an efficient web crawling method\nthat explores the web graph based on the preference of LLM pretraining.\nSpecifically, it leverages the influence of a webpage in LLM pretraining as the\npriority score of the web crawler's scheduler, replacing the standard graph\nconnectivity based priority. Our experiments on a web graph containing 900\nmillion webpages from a commercial search engine's index demonstrate the\nefficiency of Craw4LLM in obtaining high-quality pretraining data. With just\n21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream\nperformances of previous crawls, significantly reducing the crawling waste and\nalleviating the burdens on websites. Our code is publicly available at\nhttps://github.com/cxcscmu/Craw4LLM.", "published": "2025-02-19 00:31:43", "link": "http://arxiv.org/abs/2502.13347v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Segmentation Applications in Large Language Model Enabled\n  Automated Recall Assessments", "abstract": "Understanding how individuals perceive and recall information in their\nnatural environments is critical to understanding potential failures in\nperception (e.g., sensory loss) and memory (e.g., dementia). Event\nsegmentation, the process of identifying distinct events within dynamic\nenvironments, is central to how we perceive, encode, and recall experiences.\nThis cognitive process not only influences moment-to-moment comprehension but\nalso shapes event specific memory. Despite the importance of event segmentation\nand event memory, current research methodologies rely heavily on human\njudgements for assessing segmentation patterns and recall ability, which are\nsubjective and time-consuming. A few approaches have been introduced to\nautomate event segmentation and recall scoring, but validity with human\nresponses and ease of implementation require further advancements. To address\nthese concerns, we leverage Large Language Models (LLMs) to automate event\nsegmentation and assess recall, employing chat completion and text-embedding\nmodels, respectively. We validated these models against human annotations and\ndetermined that LLMs can accurately identify event boundaries, and that human\nevent segmentation is more consistent with LLMs than among humans themselves.\nUsing this framework, we advanced an automated approach for recall assessments\nwhich revealed semantic similarity between segmented narrative events and\nparticipant recall can estimate recall performance. Our findings demonstrate\nthat LLMs can effectively simulate human segmentation patterns and provide\nrecall evaluations that are a scalable alternative to manual scoring. This\nresearch opens novel avenues for studying the intersection between perception,\nmemory, and cognitive impairment using methodologies driven by artificial\nintelligence.", "published": "2025-02-19 00:48:51", "link": "http://arxiv.org/abs/2502.13349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text\n  Modifications", "abstract": "Large Language Models (LLMs) have transformed natural language processing,\nyet they still struggle with direct text editing tasks that demand precise,\ncontext-aware modifications. While models like ChatGPT excel in text generation\nand analysis, their editing abilities often fall short, addressing only\nsuperficial issues rather than deeper structural or logical inconsistencies. In\nthis work, we introduce a dual approach to enhance LLMs editing performance.\nFirst, we present InstrEditBench, a high-quality benchmark dataset comprising\nover 20,000 structured editing tasks spanning Wiki articles, LaTeX documents,\ncode, and database Domain-specific Languages (DSL). InstrEditBench is generated\nusing an innovative automated workflow that accurately identifies and evaluates\ntargeted edits, ensuring that modifications adhere strictly to specified\ninstructions without altering unrelated content. Second, we propose FineEdit, a\nspecialized model trained on this curated benchmark. Experimental results\ndemonstrate that FineEdit achieves significant improvements around {10\\%}\ncompared with Gemini on direct editing tasks, convincingly validating its\neffectiveness.", "published": "2025-02-19 01:41:44", "link": "http://arxiv.org/abs/2502.13358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Hallucinations in Language Model-based SPARQL Query Generation\n  Using Post-Generation Memory Retrieval", "abstract": "The ability to generate SPARQL queries from natural language questions is\ncrucial for ensuring efficient and accurate retrieval of structured data from\nknowledge graphs (KG). While large language models (LLMs) have been widely\nadopted for SPARQL query generation, they are often susceptible to\nhallucinations and out-of-distribution errors when producing KG elements like\nUniform Resource Identifiers (URIs) based on internal parametric knowledge.\nThis often results in content that appears plausible but is factually\nincorrect, posing significant challenges for their use in real-world\ninformation retrieval (IR) applications. This has led to increased research\naimed at detecting and mitigating such errors. In this paper, we introduce PGMR\n(Post-Generation Memory Retrieval), a modular framework that incorporates a\nnon-parametric memory module to retrieve KG elements and enhance LLM-based\nSPARQL query generation. Our experimental results indicate that PGMR\nconsistently delivers strong performance across diverse datasets, data\ndistributions, and LLMs. Notably, PGMR significantly mitigates URI\nhallucinations, nearly eliminating the problem in several scenarios.", "published": "2025-02-19 02:08:13", "link": "http://arxiv.org/abs/2502.13369v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-agnostic Prompt Compression with Context-aware Sentence Embedding\n  and Reward-guided Task Descriptor", "abstract": "The rise of Large Language Models (LLMs) has led to significant interest in\nprompt compression, a technique aimed at reducing the length of input prompts\nwhile preserving critical information. However, the prominent approaches in\nprompt compression often require explicit questions or handcrafted templates\nfor compression, limiting their generalizability. We propose Task-agnostic\nPrompt Compression (TPC), a novel framework that generalizes compression across\ntasks and domains without requiring input questions or templates. TPC generates\na context-relevant task description using a task descriptor trained on a\ncurated dataset of context and query pairs, and fine-tuned via reinforcement\nlearning with a reward function designed to capture the most relevant\ninformation. The task descriptor is then utilized to compute the relevance of\neach sentence in the prompt to generate the compressed prompt. We introduce 3\nmodel sizes (Base, Large, and Huge), where the largest model outperforms the\nexisting state-of-the-art methods on LongBench and ZeroSCROLLS benchmarks, and\nour smallest model performs comparable to the existing solutions while being\nconsiderably smaller.", "published": "2025-02-19 02:16:29", "link": "http://arxiv.org/abs/2502.13374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting a Weighting Mechanism into LLM-as-a-Judge in Two-Step: A Case\n  Study", "abstract": "While Large Language Models (LLMs) have emerged as promising tools for\nevaluating Natural Language Generation (NLG) tasks, their effectiveness is\nlimited by their inability to appropriately weigh the importance of different\ntopics, often overemphasizing minor details while undervaluing critical\ninformation, leading to misleading assessments. Our work proposes an efficient\nprompt design mechanism to address this specific limitation and provide a case\nstudy. Through strategic prompt engineering that incorporates explicit\nimportance weighting mechanisms, we enhance using LLM-as-a-Judge ability to\nprioritize relevant information effectively, as demonstrated by an average\nimprovement of 6% in the Human Alignment Rate (HAR) metric.", "published": "2025-02-19 03:09:55", "link": "http://arxiv.org/abs/2502.13396v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting LLM Fact-conflicting Hallucinations Enhanced by\n  Temporal-logic-based Reasoning", "abstract": "Large language models (LLMs) face the challenge of hallucinations -- outputs\nthat seem coherent but are actually incorrect. A particularly damaging type is\nfact-conflicting hallucination (FCH), where generated content contradicts\nestablished facts. Addressing FCH presents three main challenges: 1)\nAutomatically constructing and maintaining large-scale benchmark datasets is\ndifficult and resource-intensive; 2) Generating complex and efficient test\ncases that the LLM has not been trained on -- especially those involving\nintricate temporal features -- is challenging, yet crucial for eliciting\nhallucinations; and 3) Validating the reasoning behind LLM outputs is\ninherently difficult, particularly with complex logical relationships, as it\nrequires transparency in the model's decision-making process.\n  This paper presents Drowzee, an innovative end-to-end metamorphic testing\nframework that utilizes temporal logic to identify fact-conflicting\nhallucinations (FCH) in large language models (LLMs). Drowzee builds a\ncomprehensive factual knowledge base by crawling sources like Wikipedia and\nuses automated temporal-logic reasoning to convert this knowledge into a large,\nextensible set of test cases with ground truth answers. LLMs are tested using\nthese cases through template-based prompts, which require them to generate both\nanswers and reasoning steps. To validate the reasoning, we propose two\nsemantic-aware oracles that compare the semantic structure of LLM outputs to\nthe ground truths. Across nine LLMs in nine different knowledge domains,\nexperimental results show that Drowzee effectively identifies rates of\nnon-temporal-related hallucinations ranging from 24.7% to 59.8%, and rates of\ntemporal-related hallucinations ranging from 16.7% to 39.2%.", "published": "2025-02-19 04:21:46", "link": "http://arxiv.org/abs/2502.13416v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Lightweight, Adaptive and Attribute-Aware Multi-Aspect\n  Controllable Text Generation with Large Language Models", "abstract": "Multi-aspect controllable text generation aims to control text generation in\nattributes from multiple aspects, making it a complex but powerful task in\nnatural language processing. Supervised fine-tuning methods are often employed\nfor this task due to their simplicity and effectiveness. However, they still\nhave some limitations: low rank adaptation (LoRA) only fine-tunes a few\nparameters and has suboptimal control effects, while full fine-tuning (FFT)\nrequires significant computational resources and is susceptible to overfitting,\nparticularly when data is limited. Moreover, existing works typically train\nmulti-aspect controllable text generation models using only single-aspect\nannotated data, which results in discrepancies in data distribution; at the\nsame time, accurately generating text with specific attributes is a challenge\nthat requires strong attribute-aware capabilities. To address these\nlimitations, we propose a lightweight, adaptive and attribute-aware framework\nfor multi-aspect controllable text generation. Our framework can dynamically\nadjust model parameters according to different aspects of data to achieve\ncontrollable text generation, aiming to optimize performance across multiple\naspects. Experimental results show that our framework outperforms other strong\nbaselines, achieves state-of-the-art performance, adapts well to data\ndiscrepancies, and is more accurate in attribute perception.", "published": "2025-02-19 06:56:02", "link": "http://arxiv.org/abs/2502.13474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shall Your Data Strategy Work? Perform a Swift Study", "abstract": "This work presents a swift method to assess the efficacy of particular types\nof instruction-tuning data, utilizing just a handful of probe examples and\neliminating the need for model retraining. This method employs the idea of\ngradient-based data influence estimation, analyzing the gradient projections of\nprobe examples from the chosen strategy onto evaluation examples to assess its\nadvantages. Building upon this method, we conducted three swift studies to\ninvestigate the potential of Chain-of-thought (CoT) data, query clarification\ndata, and response evaluation data in enhancing model generalization.\nSubsequently, we embarked on a validation study to corroborate the findings of\nthese swift studies. In this validation study, we developed training datasets\ntailored to each studied strategy and compared model performance with and\nwithout the use of these datasets. The results of the validation study aligned\nwith the findings of the swift studies, validating the efficacy of our proposed\nmethod.", "published": "2025-02-19 08:08:35", "link": "http://arxiv.org/abs/2502.13514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large and Balanced Corpus for Fine-grained Arabic Readability\n  Assessment", "abstract": "This paper introduces the Balanced Arabic Readability Evaluation Corpus\nBAREC, a large-scale, fine-grained dataset for Arabic readability assessment.\nBAREC consists of 68,182 sentences spanning 1+ million words, carefully curated\nto cover 19 readability levels, from kindergarten to postgraduate\ncomprehension. The corpus balances genre diversity, topical coverage, and\ntarget audiences, offering a comprehensive resource for evaluating Arabic text\ncomplexity. The corpus was fully manually annotated by a large team of\nannotators. The average pairwise inter-annotator agreement, measured by\nQuadratic Weighted Kappa, is 81.3%, reflecting a high level of substantial\nagreement. Beyond presenting the corpus, we benchmark automatic readability\nassessment across different granularity levels, comparing a range of\ntechniques. Our results highlight the challenges and opportunities in Arabic\nreadability modeling, demonstrating competitive performance across various\nmethods. To support research and education, we will make BAREC openly\navailable, along with detailed annotation guidelines and benchmark results.", "published": "2025-02-19 08:16:11", "link": "http://arxiv.org/abs/2502.13520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Linguistic Bias in Government Documents Using Large language\n  Models", "abstract": "This paper addresses the critical need for detecting bias in government\ndocuments, an underexplored area with significant implications for governance.\nExisting methodologies often overlook the unique context and far-reaching\nimpacts of governmental documents, potentially obscuring embedded biases that\nshape public policy and citizen-government interactions. To bridge this gap, we\nintroduce the Dutch Government Data for Bias Detection (DGDB), a dataset\nsourced from the Dutch House of Representatives and annotated for bias by\nexperts. We fine-tune several BERT-based models on this dataset and compare\ntheir performance with that of generative language models. Additionally, we\nconduct a comprehensive error analysis that includes explanations of the\nmodels' predictions. Our findings demonstrate that fine-tuned models achieve\nstrong performance and significantly outperform generative language models,\nindicating the effectiveness of DGDB for bias detection. This work underscores\nthe importance of labeled datasets for bias detection in various languages and\ncontributes to more equitable governance practices.", "published": "2025-02-19 08:56:16", "link": "http://arxiv.org/abs/2502.13548v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STaR-SQL: Self-Taught Reasoner for Text-to-SQL", "abstract": "Generating step-by-step \"chain-of-thought\" rationales has proven effective\nfor improving the performance of large language models on complex reasoning\ntasks. However, applying such techniques to structured tasks, such as\ntext-to-SQL, remains largely unexplored. In this paper, we introduce\nSelf-Taught Reasoner for text-to-SQL (STaR-SQL), a novel approach that reframes\nSQL query generation as a reasoning-driven process. Our method prompts the LLM\nto produce detailed reasoning steps for SQL queries and fine-tunes it on\nrationales that lead to correct outcomes. Unlike traditional methods, STaR-SQL\ndedicates additional test-time computation to reasoning, thereby positioning\nLLMs as spontaneous reasoners rather than mere prompt-based agents. To further\nscale the inference process, we incorporate an outcome-supervised reward model\n(ORM) as a verifier, which enhances SQL query accuracy. Experimental results on\nthe challenging Spider benchmark demonstrate that STaR-SQL significantly\nimproves text-to-SQL performance, achieving an execution accuracy of 86.6%.\nThis surpasses a few-shot baseline by 31.6% and a baseline fine-tuned to\npredict answers directly by 18.0%. Additionally, STaR-SQL outperforms\nagent-like prompting methods that leverage more powerful yet closed-source\nmodels such as GPT-4. These findings underscore the potential of\nreasoning-augmented training for structured tasks and open the door to\nextending self-improving reasoning models to text-to-SQL generation and beyond.", "published": "2025-02-19 08:58:44", "link": "http://arxiv.org/abs/2502.13550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language\n  Models", "abstract": "The rapid development of large language models (LLMs) is redefining the\nlandscape of human-computer interaction, and their integration into various\nuser-service applications is becoming increasingly prevalent. However,\ntransmitting user data to cloud-based LLMs presents significant risks of data\nbreaches and unauthorized access to personal identification information. In\nthis paper, we propose a privacy preservation pipeline for protecting privacy\nand sensitive information during interactions between users and LLMs in\npractical LLM usage scenarios. We construct SensitiveQA, the first privacy\nopen-ended question-answering dataset. It comprises 57k interactions in Chinese\nand English, encompassing a diverse range of user-sensitive information within\nthe conversations. Our proposed solution employs a multi-stage strategy aimed\nat preemptively securing user information while simultaneously preserving the\nresponse quality of cloud-based LLMs. Experimental validation underscores our\nmethod's efficacy in balancing privacy protection with maintaining robust\ninteraction quality. The code and dataset are available at\nhttps://github.com/ligw1998/PRIV-QA.", "published": "2025-02-19 09:17:07", "link": "http://arxiv.org/abs/2502.13564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Social Connections from Finnish Karelian Refugee Interviews\n  Using LLMs", "abstract": "We performed a zero-shot information extraction study on a historical\ncollection of 89,339 brief Finnish-language interviews of refugee families\nrelocated post-WWII from Finnish Eastern Karelia. Our research objective is\ntwo-fold. First, we aim to extract social organizations and hobbies from the\nfree text of the interviews, separately for each family member. These can act\nas a proxy variable indicating the degree of social integration of refugees in\ntheir new environment. Second, we aim to evaluate several alternative ways to\napproach this task, comparing a number of generative models and a supervised\nlearning approach, to gain a broader insight into the relative merits of these\ndifferent approaches and their applicability in similar studies.\n  We find that the best generative model (GPT-4) is roughly on par with human\nperformance, at an F-score of 88.8%. Interestingly, the best open generative\nmodel (Llama-3-70B-Instruct) reaches almost the same performance, at 87.7%\nF-score, demonstrating that open models are becoming a viable alternative for\nsome practical tasks even on non-English data. Additionally, we test a\nsupervised learning alternative, where we fine-tune a Finnish BERT model\n(FinBERT) using GPT-4 generated training data. By this method, we achieved an\nF-score of 84.1% already with 6K interviews up to an F-score of 86.3% with 30k\ninterviews. Such an approach would be particularly appealing in cases where the\ncomputational resources are limited, or there is a substantial mass of data to\nprocess.", "published": "2025-02-19 09:17:41", "link": "http://arxiv.org/abs/2502.13566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Stop the Multi-Party! On Generating Synthetic Multi-Party\n  Conversations with Constraints", "abstract": "Multi-Party Conversations (MPCs) are widely studied across disciplines, with\nsocial media as a primary data source due to their accessibility. However,\nthese datasets raise privacy concerns and often reflect platform-specific\nproperties. For example, interactions between speakers may be limited due to\nrigid platform structures (e.g., threads, tree-like discussions), which yield\noverly simplistic interaction patterns (e.g., as a consequence of ``reply-to''\nlinks). This work explores the feasibility of generating diverse MPCs with\ninstruction-tuned Large Language Models (LLMs) by providing deterministic\nconstraints such as dialogue structure and participants' stance. We investigate\ntwo complementary strategies of leveraging LLMs in this context: (i.) LLMs as\nMPC generators, where we task the LLM to generate a whole MPC at once and (ii.)\nLLMs as MPC parties, where the LLM generates one turn of the conversation at a\ntime, provided the conversation history. We next introduce an analytical\nframework to evaluate compliance with the constraints, content quality, and\ninteraction complexity for both strategies. Finally, we assess the quality of\nobtained MPCs via human annotation and LLM-as-a-judge evaluations. We find\nstark differences among LLMs, with only some being able to generate\nhigh-quality MPCs. We also find that turn-by-turn generation yields better\nconformance to constraints and higher linguistic variability than generating\nMPCs in one pass. Nonetheless, our structural and qualitative evaluation\nindicates that both generation strategies can yield high-quality MPCs.", "published": "2025-02-19 10:10:43", "link": "http://arxiv.org/abs/2502.13592v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BeamLoRA: Beam-Constraint Low-Rank Adaptation", "abstract": "Due to the demand for efficient fine-tuning of large language models,\nLow-Rank Adaptation (LoRA) has been widely adopted as one of the most effective\nparameter-efficient fine-tuning methods. Nevertheless, while LoRA improves\nefficiency, there remains room for improvement in accuracy. Herein, we adopt a\nnovel perspective to assess the characteristics of LoRA ranks. The results\nreveal that different ranks within the LoRA modules not only exhibit varying\nlevels of importance but also evolve dynamically throughout the fine-tuning\nprocess, which may limit the performance of LoRA. Based on these findings, we\npropose BeamLoRA, which conceptualizes each LoRA module as a beam where each\nrank naturally corresponds to a potential sub-solution, and the fine-tuning\nprocess becomes a search for the optimal sub-solution combination. BeamLoRA\ndynamically eliminates underperforming sub-solutions while expanding the\nparameter space for promising ones, enhancing performance with a fixed rank.\nExtensive experiments across three base models and 12 datasets spanning math\nreasoning, code generation, and commonsense reasoning demonstrate that BeamLoRA\nconsistently enhances the performance of LoRA, surpassing the other baseline\nmethods.", "published": "2025-02-19 10:33:22", "link": "http://arxiv.org/abs/2502.13604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Euclidean Hierarchical Representational Learning using Hyperbolic\n  Graph Neural Networks for Environmental Claim Detection", "abstract": "Transformer-based models dominate NLP tasks like sentiment analysis, machine\ntranslation, and claim verification. However, their massive computational\ndemands and lack of interpretability pose challenges for real-world\napplications requiring efficiency and transparency. In this work, we explore\nGraph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as\nlightweight yet effective alternatives for Environmental Claim Detection,\nreframing it as a graph classification problem. We construct dependency parsing\ngraphs to explicitly model syntactic structures, using simple word embeddings\n(word2vec) for node features with dependency relations encoded as edge\nfeatures. Our results demonstrate that these graph-based models achieve\ncomparable or superior performance to state-of-the-art transformers while using\n30x fewer parameters. This efficiency highlights the potential of structured,\ninterpretable, and computationally efficient graph-based approaches.", "published": "2025-02-19 11:04:59", "link": "http://arxiv.org/abs/2502.13628v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts", "abstract": "Large language models (LLMs) are known to have the potential to generate\nharmful content, posing risks to users. While significant progress has been\nmade in developing taxonomies for LLM risks and safety evaluation prompts, most\nstudies have focused on monolingual contexts, primarily in English. However,\nlanguage- and region-specific risks in bilingual contexts are often overlooked,\nand core findings can diverge from those in monolingual settings. In this\npaper, we introduce Qorgau, a novel dataset specifically designed for safety\nevaluation in Kazakh and Russian, reflecting the unique bilingual context in\nKazakhstan, where both Kazakh (a low-resource language) and Russian (a\nhigh-resource language) are spoken. Experiments with both multilingual and\nlanguage-specific LLMs reveal notable differences in safety performance,\nemphasizing the need for tailored, region-specific datasets to ensure the\nresponsible and safe deployment of LLMs in countries like Kazakhstan. Warning:\nthis paper contains example data that may be offensive, harmful, or biased.", "published": "2025-02-19 11:33:22", "link": "http://arxiv.org/abs/2502.13640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring the Effect of Transcription Noise on Downstream Language\n  Understanding Tasks", "abstract": "With the increasing prevalence of recorded human speech, spoken language\nunderstanding (SLU) is essential for its efficient processing. In order to\nprocess the speech, it is commonly transcribed using automatic speech\nrecognition technology. This speech-to-text transition introduces errors into\nthe transcripts, which subsequently propagate to downstream NLP tasks, such as\ndialogue summarization. While it is known that transcript noise affects\ndownstream tasks, a systematic approach to analyzing its effects across\ndifferent noise severities and types has not been addressed. We propose a\nconfigurable framework for assessing task models in diverse noisy settings, and\nfor examining the impact of transcript-cleaning techniques. The framework\nfacilitates the investigation of task model behavior, which can in turn support\nthe development of effective SLU solutions. We exemplify the utility of our\nframework on three SLU tasks and four task models, offering insights regarding\nthe effect of transcript noise on tasks in general and models in particular.\nFor instance, we find that task models can tolerate a certain level of noise,\nand are affected differently by the types of errors in the transcript.", "published": "2025-02-19 11:37:59", "link": "http://arxiv.org/abs/2502.13645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D.Va: Validate Your Demonstration First Before You Use It", "abstract": "In-context learning (ICL) has demonstrated significant potential in enhancing\nthe capabilities of large language models (LLMs) during inference. It's\nwell-established that ICL heavily relies on selecting effective demonstrations\nto generate outputs that better align with the expected results. As for\ndemonstration selection, previous approaches have typically relied on intuitive\nmetrics to evaluate the effectiveness of demonstrations, which often results in\nlimited robustness and poor cross-model generalization capabilities. To tackle\nthese challenges, we propose a novel method, \\textbf{D}emonstration\n\\textbf{VA}lidation (\\textbf{D.Va}), which integrates a demonstration\nvalidation perspective into this field. By introducing the demonstration\nvalidation mechanism, our method effectively identifies demonstrations that are\nboth effective and highly generalizable. \\textbf{D.Va} surpasses all existing\ndemonstration selection techniques across both natural language understanding\n(NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate\nthe robustness and generalizability of our approach across various language\nmodels with different retrieval models.", "published": "2025-02-19 11:41:40", "link": "http://arxiv.org/abs/2502.13646v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction Tuning on Public Government and Cultural Data for\n  Low-Resource Language: a Case Study in Kazakh", "abstract": "Instruction tuning in low-resource languages remains underexplored due to\nlimited text data, particularly in government and cultural domains. To address\nthis, we introduce and open-source a large-scale (10,600 samples)\ninstruction-following (IFT) dataset, covering key institutional and cultural\nknowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of\nprocedural, legal, and structural governance topics. We employ LLM-assisted\ndata generation, comparing open-weight and closed-weight models for dataset\nconstruction, and select GPT-4o as the backbone. Each entity of our dataset\nundergoes full manual verification to ensure high quality. We also show that\nfine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent\nperformance improvements in both multiple-choice and generative tasks,\ndemonstrating the potential of LLM-assisted instruction tuning for low-resource\nlanguages.", "published": "2025-02-19 11:44:27", "link": "http://arxiv.org/abs/2502.13647v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reliability Across Parametric and External Knowledge: Understanding\n  Knowledge Handling in LLMs", "abstract": "Large Language Models (LLMs) enhance their problem-solving capability by\nleveraging both parametric and external knowledge. Beyond leveraging external\nknowledge to improve response accuracy, they require key capabilities for\nreliable knowledge-handling: resolving conflicts between knowledge sources,\navoiding distraction from uninformative external knowledge, and abstaining when\nsufficient knowledge is unavailable. Prior studies have examined these\nscenarios in isolation or with limited scope. To systematically evaluate these\ncapabilities, we introduce a comprehensive framework for analyzing\nknowledge-handling based on two key dimensions: the presence of parametric\nknowledge and the informativeness of external knowledge. Through analysis, we\nidentify biases in knowledge utilization and examine how the ability to handle\none scenario impacts performance in others. Furthermore, we demonstrate that\ntraining on data constructed based on the knowledge-handling scenarios improves\nLLMs' reliability in integrating and utilizing knowledge.", "published": "2025-02-19 11:49:23", "link": "http://arxiv.org/abs/2502.13648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Refining Sentence Embedding Model through Ranking Sentences Generation\n  with Large Language Models", "abstract": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis.", "published": "2025-02-19 12:07:53", "link": "http://arxiv.org/abs/2502.13656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCOPE: A Self-supervised Framework for Improving Faithfulness in\n  Conditional Text Generation", "abstract": "Large Language Models (LLMs), when used for conditional text generation,\noften produce hallucinations, i.e., information that is unfaithful or not\ngrounded in the input context. This issue arises in typical conditional text\ngeneration tasks, such as text summarization and data-to-text generation, where\nthe goal is to produce fluent text based on contextual input. When fine-tuned\non specific domains, LLMs struggle to provide faithful answers to a given\ncontext, often adding information or generating errors. One underlying cause of\nthis issue is that LLMs rely on statistical patterns learned from their\ntraining data. This reliance can interfere with the model's ability to stay\nfaithful to a provided context, leading to the generation of ungrounded\ninformation. We build upon this observation and introduce a novel\nself-supervised method for generating a training set of unfaithful samples. We\nthen refine the model using a training process that encourages the generation\nof grounded outputs over unfaithful ones, drawing on preference-based training.\nOur approach leads to significantly more grounded text generation,\noutperforming existing self-supervised techniques in faithfulness, as evaluated\nthrough automatic metrics, LLM-based assessments, and human evaluations.", "published": "2025-02-19 12:31:58", "link": "http://arxiv.org/abs/2502.13674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is This Collection Worth My LLM's Time? Automatically Measuring\n  Information Potential in Text Corpora", "abstract": "As large language models (LLMs) converge towards similar capabilities, the\nkey to advancing their performance lies in identifying and incorporating\nvaluable new information sources. However, evaluating which text collections\nare worth the substantial investment required for digitization, preprocessing,\nand integration into LLM systems remains a significant challenge. We present a\nnovel approach to this challenge: an automated pipeline that evaluates the\npotential information gain from text collections without requiring model\ntraining or fine-tuning. Our method generates multiple choice questions (MCQs)\nfrom texts and measures an LLM's performance both with and without access to\nthe source material. The performance gap between these conditions serves as a\nproxy for the collection's information potential. We validate our approach\nusing three strategically selected datasets: EPFL PhD manuscripts (likely\ncontaining novel specialized knowledge), Wikipedia articles (presumably part of\ntraining data), and a synthetic baseline dataset. Our results demonstrate that\nthis method effectively identifies collections containing valuable novel\ninformation, providing a practical tool for prioritizing data acquisition and\nintegration efforts.", "published": "2025-02-19 13:03:06", "link": "http://arxiv.org/abs/2502.13691v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Scale and Multi-Objective Optimization for Cross-Lingual\n  Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) is a sequence labeling task that has\ngarnered growing research interest in multilingual contexts. However, recent\nstudies lack more robust feature alignment and finer aspect-level alignment. In\nthis paper, we propose a novel framework, Multi-Scale and Multi-Objective\noptimization (MSMO) for cross-lingual ABSA. During multi-scale alignment, we\nachieve cross-lingual sentence-level and aspect-level alignment, aligning\nfeatures of aspect terms in different contextual environments. Specifically, we\nintroduce code-switched bilingual sentences into the language discriminator and\nconsistency training modules to enhance the model's robustness. During\nmulti-objective optimization, we design two optimization objectives: supervised\ntraining and consistency training, aiming to enhance cross-lingual semantic\nalignment. To further improve model performance, we incorporate distilled\nknowledge of the target language into the model. Results show that MSMO\nsignificantly enhances cross-lingual ABSA by achieving state-of-the-art\nperformance across multiple languages and models.", "published": "2025-02-19 13:43:33", "link": "http://arxiv.org/abs/2502.13718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Large Language Models for Time Series Modeling via a Novel\n  Parameter-efficient Adaptation Method", "abstract": "Time series modeling holds significant importance in many real-world\napplications and has been extensively studied. While pre-trained foundation\nmodels have made impressive strides in the fields of natural language\nprocessing (NLP) and computer vision (CV), their development in time series\ndomains has been constrained by data sparsity. A series of recent studies have\ndemonstrated that large language models (LLMs) possess robust pattern\nrecognition and reasoning abilities over complex sequences of tokens. However,\nthe current literature have yet striked a high-quality balance between (a)\neffectively aligning the time series and natural language modalities, and (b)\nkeeping the inference efficiency. To address the above issues, we now propose\nthe Time-LlaMA framework. Time-LlaMA first converts the time series input into\ntoken embeddings through a linear tokenization mechanism. Second, the time\nseries token embeddings are aligned with the text prompts. Third, to further\nadapt the LLM backbone for time series modeling, we have developed a dynamic\nlow-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most\nsuitable LoRA modules at each layer of the Transformer backbone for each time\nseries input, enhancing the model's predictive capabilities. Our experimental\nresults on an extensive collection of challenging real-world time series tasks\nconfirm that our proposed method achieves the state-of-the-art (SOTA)\nperformance.", "published": "2025-02-19 13:52:26", "link": "http://arxiv.org/abs/2502.13725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Input-Label Mapping in In-Context Learning with Contrastive\n  Decoding", "abstract": "Large language models (LLMs) excel at a range of tasks through in-context\nlearning (ICL), where only a few task examples guide their predictions.\nHowever, prior research highlights that LLMs often overlook input-label mapping\ninformation in ICL, relying more on their pre-trained knowledge. To address\nthis issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method\nthat emphasizes input-label mapping by contrasting the output distributions\nbetween positive and negative in-context examples. Experiments on 7 natural\nlanguage understanding (NLU) tasks show that our ICCD method brings consistent\nand significant improvement (up to +2.1 improvement on average) upon 6\ndifferent scales of LLMs without requiring additional training. Our approach is\nversatile, enhancing performance with various demonstration selection methods,\ndemonstrating its broad applicability and effectiveness. The code and scripts\nwill be publicly released.", "published": "2025-02-19 14:04:46", "link": "http://arxiv.org/abs/2502.13738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCALAR: Scientific Citation-based Live Assessment of Long-context\n  Academic Reasoning", "abstract": "Evaluating large language models' (LLMs) long-context understanding\ncapabilities remains challenging. We present SCALAR (Scientific Citation-based\nLive Assessment of Long-context Academic Reasoning), a novel benchmark that\nleverages academic papers and their citation networks. SCALAR features\nautomatic generation of high-quality ground truth labels without human\nannotation, controllable difficulty levels, and a dynamic updating mechanism\nthat prevents data contamination. Using ICLR 2025 papers, we evaluate 8\nstate-of-the-art LLMs, revealing key insights about their capabilities and\nlimitations in processing long scientific documents across different context\nlengths and reasoning types. Our benchmark provides a reliable and sustainable\nway to track progress in long-context understanding as LLM capabilities evolve.", "published": "2025-02-19 14:15:49", "link": "http://arxiv.org/abs/2502.13753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge\n  Benchmarking", "abstract": "Large Vision-Language Models (LVLMs) have recently gained attention due to\ntheir distinctive performance and broad applicability. While it has been\npreviously shown that their efficacy in usage scenarios involving non-Western\ncontexts falls short, existing studies are limited in scope, covering just a\nnarrow range of cultures, focusing exclusively on a small number of cultural\naspects, or evaluating a limited selection of models on a single task only.\nTowards globally inclusive LVLM research, we introduce GIMMICK, an extensive\nmultimodal benchmark designed to assess a broad spectrum of cultural knowledge\nacross 144 countries representing six global macro-regions. GIMMICK comprises\nsix tasks built upon three new datasets that span 728 unique cultural events or\nfacets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary\nand 26 open-weight models of all sizes. We systematically examine (1) regional\ncultural biases, (2) the influence of model size, (3) input modalities, and (4)\nexternal cues. Our analyses reveal strong biases toward Western cultures across\nmodels and tasks and highlight strong correlations between model size and\nperformance, as well as the effectiveness of multimodal input and external\ngeographic cues. We further find that models have more knowledge of tangible\nthan intangible aspects (e.g., food vs. rituals) and that they excel in\nrecognizing broad cultural origins but struggle with a more nuanced\nunderstanding.", "published": "2025-02-19 14:27:40", "link": "http://arxiv.org/abs/2502.13766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Tools to Teammates: Evaluating LLMs in Multi-Session Coding\n  Interactions", "abstract": "Large Language Models (LLMs) are increasingly used in working environments\nfor a wide range of tasks, excelling at solving individual problems in\nisolation. However, are they also able to effectively collaborate over\nlong-term interactions? To investigate this, we introduce MemoryCode, a\nsynthetic multi-session dataset designed to test LLMs' ability to track and\nexecute simple coding instructions amid irrelevant information, simulating a\nrealistic setting. While all the models we tested handle isolated instructions\nwell, even the performance of state-of-the-art models like GPT-4o deteriorates\nwhen instructions are spread across sessions. Our analysis suggests this is due\nto their failure to retrieve and integrate information over long instruction\nchains. Our results highlight a fundamental limitation of current LLMs,\nrestricting their ability to collaborate effectively in long interactions.", "published": "2025-02-19 14:58:04", "link": "http://arxiv.org/abs/2502.13791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster\n  Adaptive Internal Thinking", "abstract": "Large language models (LLMs) face inherent performance bottlenecks under\nparameter constraints, particularly in processing critical tokens that demand\ncomplex reasoning. Empirical analysis reveals challenging tokens induce abrupt\ngradient spikes across layers, exposing architectural stress points in standard\nTransformers. Building on this insight, we propose Inner Thinking Transformer\n(ITT), which reimagines layer computations as implicit thinking steps. ITT\ndynamically allocates computation through Adaptive Token Routing, iteratively\nrefines representations via Residual Thinking Connections, and distinguishes\nreasoning phases using Thinking Step Encoding. ITT enables deeper processing of\ncritical tokens without parameter expansion. Evaluations across 162M-466M\nparameter models show ITT achieves 96.5\\% performance of a 466M Transformer\nusing only 162M parameters, reduces training data by 43.2\\%, and outperforms\nTransformer/Loop variants in 11 benchmarks. By enabling elastic computation\nallocation during inference, ITT balances performance and efficiency through\narchitecture-aware optimization of implicit thinking pathways.", "published": "2025-02-19 16:02:23", "link": "http://arxiv.org/abs/2502.13842v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Fallacy Detection with Human Label Variation", "abstract": "We introduce Faina, the first dataset for fallacy detection that embraces\nmultiple plausible answers and natural disagreement. Faina includes over 11K\nspan-level annotations with overlaps across 20 fallacy types on social media\nposts in Italian about migration, climate change, and public health given by\ntwo expert annotators. Through an extensive annotation study that allowed\ndiscussion over multiple rounds, we minimize annotation errors whilst keeping\nsignals of human label variation. Moreover, we devise a framework that goes\nbeyond \"single ground truth\" evaluation and simultaneously accounts for\nmultiple (equally reliable) test sets and the peculiarities of the task, i.e.,\npartial span matches, overlaps, and the varying severity of labeling errors.\nOur experiments across four fallacy detection setups show that multi-task and\nmulti-label transformer-based approaches are strong baselines across all\nsettings. We release our data, code, and annotation guidelines to foster\nresearch on fallacy detection and human label variation more broadly.", "published": "2025-02-19 16:18:44", "link": "http://arxiv.org/abs/2502.13853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TESS 2: A Large-Scale Generalist Diffusion Language Model", "abstract": "We introduce TESS 2, a general instruction-following diffusion language model\nthat outperforms contemporary instruction-tuned diffusion models, as well as\nmatches and sometimes exceeds strong autoregressive (AR) models. We train TESS\n2 by first adapting a strong AR model via continued pretraining with the usual\ncross-entropy as diffusion loss, and then performing further instruction\ntuning. We find that adaptation training as well as the choice of the base\nmodel is crucial for training good instruction-following diffusion models. We\nfurther propose reward guidance, a novel and modular inference-time guidance\nprocedure to align model outputs without needing to train the underlying model.\nFinally, we show that TESS 2 further improves with increased inference-time\ncompute, highlighting the utility of diffusion LMs in having fine-grained\ncontrollability over the amount of compute used at inference time. Code and\nmodels are available at https://github.com/hamishivi/tess-2.", "published": "2025-02-19 17:50:31", "link": "http://arxiv.org/abs/2502.13917v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual\n  Narratives in Image Sequences?", "abstract": "Large Multimodal Models (LMMs) have achieved remarkable success across\nvarious visual-language tasks. However, existing benchmarks predominantly focus\non single-image understanding, leaving the analysis of image sequences largely\nunexplored. To address this limitation, we introduce StripCipher, a\ncomprehensive benchmark designed to evaluate capabilities of LMMs to comprehend\nand reason over sequential images. StripCipher comprises a human-annotated\ndataset and three challenging subtasks: visual narrative comprehension,\ncontextual frame prediction, and temporal narrative reordering. Our evaluation\nof $16$ state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a\nsignificant performance gap compared to human capabilities, particularly in\ntasks that require reordering shuffled sequential images. For instance, GPT-4o\nachieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower\nthan human performance. Further quantitative analysis discuss several factors,\nsuch as input format of images, affecting the performance of LLMs in sequential\nunderstanding, underscoring the fundamental challenges that remain in the\ndevelopment of LMMs.", "published": "2025-02-19 18:04:44", "link": "http://arxiv.org/abs/2502.13925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIDDIA: Language-based Intelligent Drug Discovery Agent", "abstract": "Drug discovery is a long, expensive, and complex process, relying heavily on\nhuman medicinal chemists, who can spend years searching the vast space of\npotential therapies. Recent advances in artificial intelligence for chemistry\nhave sought to expedite individual drug discovery tasks; however, there remains\na critical need for an intelligent agent that can navigate the drug discovery\nprocess. Towards this end, we introduce LIDDiA, an autonomous agent capable of\nintelligently navigating the drug discovery process in silico. By leveraging\nthe reasoning capabilities of large language models, LIDDiA serves as a\nlow-cost and highly-adaptable tool for autonomous drug discovery. We\ncomprehensively examine LIDDiA, demonstrating that (1) it can generate\nmolecules meeting key pharmaceutical criteria on over 70% of 30 clinically\nrelevant targets, (2) it intelligently balances exploration and exploitation in\nthe chemical space, and (3) it can identify promising novel drug candidates on\nEGFR, a critical target for cancers.", "published": "2025-02-19 18:56:12", "link": "http://arxiv.org/abs/2502.13959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question\n  Answering", "abstract": "Scaling the test-time compute of large language models has demonstrated\nimpressive performance on reasoning benchmarks. However, existing evaluations\nof test-time scaling make the strong assumption that a reasoning system should\nalways give an answer to any question provided. This overlooks concerns about\nwhether a model is confident in its answer, and whether it is appropriate to\nalways provide a response. To address these concerns, we extract confidence\nscores during reasoning for thresholding model responses. We find that\nincreasing compute budget at inference time not only helps models answer more\nquestions correctly, but also increases confidence in correct responses. We\nthen extend the current paradigm of zero-risk responses during evaluation by\nconsidering settings with non-zero levels of response risk, and suggest a\nrecipe for reporting evaluations under these settings.", "published": "2025-02-19 18:58:31", "link": "http://arxiv.org/abs/2502.13962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuDAF: Long-Context Multi-Document Attention Focusing through\n  Contrastive Learning on Attention Heads", "abstract": "Large Language Models (LLMs) frequently show distracted attention due to\nirrelevant information in the input, which severely impairs their long-context\ncapabilities. Inspired by recent studies on the effectiveness of retrieval\nheads in long-context factutality, we aim at addressing this distraction issue\nthrough improving such retrieval heads directly. We propose Multi-Document\nAttention Focusing (MuDAF), a novel method that explicitly optimizes the\nattention distribution at the head level through contrastive learning.\nAccording to the experimental results, MuDAF can significantly improve the\nlong-context question answering performance of LLMs, especially in\nmulti-document question answering. Extensive evaluations on retrieval scores\nand attention visualizations show that MuDAF possesses great potential in\nmaking attention heads more focused on relevant information and reducing\nattention distractions.", "published": "2025-02-19 18:59:15", "link": "http://arxiv.org/abs/2502.13963v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning\n  as a Computational Pipeline with UniMoral", "abstract": "Moral reasoning is a complex cognitive process shaped by individual\nexperiences and cultural contexts and presents unique challenges for\ncomputational analysis. While natural language processing (NLP) offers\npromising tools for studying this phenomenon, current research lacks cohesion,\nemploying discordant datasets and tasks that examine isolated aspects of moral\nreasoning. We bridge this gap with UniMoral, a unified dataset integrating\npsychologically grounded and social-media-derived moral dilemmas annotated with\nlabels for action choices, ethical principles, contributing factors, and\nconsequences, alongside annotators' moral and cultural profiles. Recognizing\nthe cultural relativity of moral reasoning, UniMoral spans six languages,\nArabic, Chinese, English, Hindi, Russian, and Spanish, capturing diverse\nsocio-cultural contexts. We demonstrate UniMoral's utility through a benchmark\nevaluations of three large language models (LLMs) across four tasks: action\nprediction, moral typology classification, factor attribution analysis, and\nconsequence generation. Key findings reveal that while implicitly embedded\nmoral contexts enhance the moral reasoning capability of LLMs, there remains a\ncritical need for increasingly specialized approaches to further advance moral\nreasoning in these models.", "published": "2025-02-19 20:13:24", "link": "http://arxiv.org/abs/2502.14083v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning", "abstract": "A key aspect of alignment is the proper use of within-document evidence to\nconstruct document-level decisions. We analyze the relationship between the\nretrieval and interpretation of within-document evidence for large language\nmodel in a few-shot setting. Specifically, we measure the extent to which model\nprediction errors are associated with evidence retrieval errors with respect to\ngold-standard human-annotated extractive evidence for five datasets, using two\npopular closed proprietary models. We perform two ablation studies to\ninvestigate when both label prediction and evidence retrieval errors can be\nattributed to qualities of the relevant evidence. We find that there is a\nstrong empirical relationship between model prediction and evidence retrieval\nerror, but that evidence retrieval error is mostly not associated with evidence\ninterpretation error--a hopeful sign for downstream applications built on this\nmechanism.", "published": "2025-02-19 20:48:09", "link": "http://arxiv.org/abs/2502.14095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meaning Beyond Truth Conditions: Evaluating Discourse Level\n  Understanding via Anaphora Accessibility", "abstract": "We present a hierarchy of natural language understanding abilities and argue\nfor the importance of moving beyond assessments of understanding at the lexical\nand sentence levels to the discourse level. We propose the task of anaphora\naccessibility as a diagnostic for assessing discourse understanding, and to\nthis end, present an evaluation dataset inspired by theoretical research in\ndynamic semantics. We evaluate human and LLM performance on our dataset and\nfind that LLMs and humans align on some tasks and diverge on others. Such\ndivergence can be explained by LLMs' reliance on specific lexical items during\nlanguage comprehension, in contrast to human sensitivity to structural\nabstractions.", "published": "2025-02-19 21:45:26", "link": "http://arxiv.org/abs/2502.14119v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which of These Best Describes Multiple Choice Evaluation with LLMs? A)\n  Forced B) Flawed C) Fixable D) All of the Above", "abstract": "Multiple choice question answering (MCQA) is popular for LLM evaluation due\nto its simplicity and human-like testing, but we argue for its reform. We first\nreveal flaws in MCQA's format, as it struggles to: 1) test\ngeneration/subjectivity; 2) match LLM use cases; and 3) fully test knowledge.\nWe instead advocate for generative formats based on human testing-where LLMs\nconstruct and explain answers-better capturing user needs and knowledge while\nremaining easy to score. We then show even when MCQA is a useful format, its\ndatasets suffer from: leakage; unanswerability; shortcuts; and saturation. In\neach issue, we give fixes from education, like rubrics to guide MCQ writing;\nscoring methods to bridle guessing; and Item Response Theory to build harder\nMCQs. Lastly, we discuss LLM errors in MCQA-robustness, biases, and unfaithful\nexplanations-showing how our prior solutions better measure or address these\nissues. While we do not need to desert MCQA, we encourage more efforts in\nrefining the task based on educational testing, advancing evaluations.", "published": "2025-02-19 22:11:52", "link": "http://arxiv.org/abs/2502.14127v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Regularization with Latent Space Explanations for Controllable\n  LLM-based Classification", "abstract": "Modern text classification methods heavily rely on contextual embeddings from\nlarge language models (LLMs). Compared to human-engineered features, these\nembeddings provide automatic and effective representations for classification\nmodel training. However, they also introduce a challenge: we lose the ability\nto manually remove unintended features, such as sensitive or task-irrelevant\nfeatures, to guarantee regulatory compliance or improve the generalizability of\nclassification models. This limitation arises because LLM embeddings are opaque\nand difficult to interpret. In this paper, we propose a novel framework to\nidentify and regularize unintended features in the LLM latent space.\nSpecifically, we first pre-train a sparse autoencoder (SAE) to extract\ninterpretable features from LLM latent spaces. To ensure the SAE can capture\ntask-specific features, we further fine-tune it on task-specific datasets. In\ntraining the classification model, we propose a simple and effective\nregularizer, by minimizing the similarity between the classifier weights and\nthe identified unintended feature, to remove the impacts of these unintended\nfeatures toward classification. We evaluate the proposed framework on three\nreal-world tasks, including toxic chat detection, reward modeling, and disease\ndiagnosis. Results show that the proposed framework can significantly improve\nthe classifier's generalizability by regularizing those features that are not\nsemantically correlated to each task. This work pioneers controllable text\nclassification on LLM latent spaces by leveraging interpreted features to\naddress generalizability, fairness, and privacy challenges. We will release our\ncode and data once accepted.", "published": "2025-02-19 22:27:59", "link": "http://arxiv.org/abs/2502.14133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent\n  approach for plain language adaptations of biomedical text", "abstract": "This paper describes our submissions to the TREC 2024 PLABA track with the\naim to simplify biomedical abstracts for a K8-level audience (13-14 years old\nstudents). We tested three approaches using OpenAI's gpt-4o and gpt-4o-mini\nmodels: baseline prompt engineering, a two-AI agent approach, and fine-tuning.\nAdaptations were evaluated using qualitative metrics (5-point Likert scales for\nsimplicity, accuracy, completeness, and brevity) and quantitative readability\nscores (Flesch-Kincaid grade level, SMOG Index). Results indicated that the\ntwo-agent approach and baseline prompt engineering with gpt-4o-mini models show\nsuperior qualitative performance, while fine-tuned models excelled in accuracy\nand completeness but were less simple. The evaluation results demonstrated that\nprompt engineering with gpt-4o-mini outperforms iterative improvement\nstrategies via two-agent approach as well as fine-tuning with gpt-4o. We intend\nto expand our investigation of the results and explore advanced evaluations.", "published": "2025-02-19 23:07:16", "link": "http://arxiv.org/abs/2502.14144v1", "categories": ["cs.CL", "68T50, 92-08", "I.2.7"], "primary_category": "cs.CL"}
{"title": "RGAR: Recurrence Generation-augmented Retrieval for Factual-aware\n  Medical Question Answering", "abstract": "Medical question answering requires extensive access to specialized\nconceptual knowledge. The current paradigm, Retrieval-Augmented Generation\n(RAG), acquires expertise medical knowledge through large-scale corpus\nretrieval and uses this knowledge to guide a general-purpose large language\nmodel (LLM) for generating answers. However, existing retrieval approaches\noften overlook the importance of factual knowledge, which limits the relevance\nof retrieved conceptual knowledge and restricts its applicability in real-world\nscenarios, such as clinical decision-making based on Electronic Health Records\n(EHRs). This paper introduces RGAR, a recurrence generation-augmented retrieval\nframework that retrieves both relevant factual and conceptual knowledge from\ndual sources (i.e., EHRs and the corpus), allowing them to interact and refine\neach another. Through extensive evaluation across three factual-aware medical\nquestion answering benchmarks, RGAR establishes a new state-of-the-art\nperformance among medical RAG systems. Notably, the Llama-3.1-8B-Instruct model\nwith RGAR surpasses the considerably larger, RAG-enhanced GPT-3.5. Our findings\ndemonstrate the benefit of extracting factual knowledge for retrieval, which\nconsistently yields improved generation quality.", "published": "2025-02-19 01:50:10", "link": "http://arxiv.org/abs/2502.13361v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MCTS-KBQA: Monte Carlo Tree Search for Knowledge Base Question Answering", "abstract": "This study explores how to enhance the reasoning capabilities of large\nlanguage models (LLMs) in knowledge base question answering (KBQA) by\nleveraging Monte Carlo Tree Search (MCTS). Semantic parsing-based KBQA methods\nare particularly challenging as these approaches require locating elements from\nknowledge bases and generating logical forms, demanding not only extensive\nannotated data but also strong reasoning capabilities. Although recent\napproaches leveraging LLMs as agents have demonstrated considerable potential,\nthese studies are inherently constrained by their linear decision-making\nprocesses. To address this limitation, we propose a MCTS-based framework that\nenhances LLMs' reasoning capabilities through tree search methodology. We\ndesign a carefully designed step-wise reward mechanism that requires only\ndirect prompting of open-source instruction LLMs without additional\nfine-tuning. Experimental results demonstrate that our approach significantly\noutperforms linear decision-making methods, particularly in low-resource\nscenarios. Additionally, we contribute new data resources to the KBQA community\nby annotating intermediate reasoning processes for existing question-SPARQL\ndatasets using distant supervision. Experimental results on the extended\ndataset demonstrate that our method achieves comparable performance to fully\nsupervised models while using significantly less training data.", "published": "2025-02-19 04:58:39", "link": "http://arxiv.org/abs/2502.13428v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning\n  Capabilities without External Scaffolding?", "abstract": "Self-improving large language models (LLMs) -- i.e., to improve the\nperformance of an LLM by fine-tuning it with synthetic data generated by itself\n-- is a promising way to advance the capabilities of LLMs while avoiding\nextensive supervision. Existing approaches to self-improvement often rely on\nexternal supervision signals in the form of seed data and/or assistance from\nthird-party models. This paper presents Crescent -- a simple yet effective\nframework for generating high-quality synthetic question-answer data in a fully\nautonomous manner. Crescent first elicits the LLM to generate raw questions via\na bait prompt, then diversifies these questions leveraging a rejection\nsampling-based self-deduplication, and finally feeds the questions to the LLM\nand collects the corresponding answers by means of majority voting. We show\nthat Crescent sheds light on the potential of true self-improvement with zero\nexternal supervision signals for math reasoning; in particular,\nCrescent-generated question-answer pairs suffice to (i) improve the reasoning\ncapabilities of an LLM while preserving its general performance (especially in\nthe 0-shot setting); and (ii) distil LLM knowledge to weaker models more\neffectively than existing methods based on seed-dataset augmentation.", "published": "2025-02-19 05:37:08", "link": "http://arxiv.org/abs/2502.13441v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Chest X-ray Classification through Knowledge Injection in\n  Cross-Modality Learning", "abstract": "The integration of artificial intelligence in medical imaging has shown\ntremendous potential, yet the relationship between pre-trained knowledge and\nperformance in cross-modality learning remains unclear. This study investigates\nhow explicitly injecting medical knowledge into the learning process affects\nthe performance of cross-modality classification, focusing on Chest X-ray (CXR)\nimages. We introduce a novel Set Theory-based knowledge injection framework\nthat generates captions for CXR images with controllable knowledge granularity.\nUsing this framework, we fine-tune CLIP model on captions with varying levels\nof medical information. We evaluate the model's performance through zero-shot\nclassification on the CheXpert dataset, a benchmark for CXR classification. Our\nresults demonstrate that injecting fine-grained medical knowledge substantially\nimproves classification accuracy, achieving 72.5\\% compared to 49.9\\% when\nusing human-generated captions. This highlights the crucial role of\ndomain-specific knowledge in medical cross-modality learning. Furthermore, we\nexplore the influence of knowledge density and the use of domain-specific Large\nLanguage Models (LLMs) for caption generation, finding that denser knowledge\nand specialized LLMs contribute to enhanced performance. This research advances\nmedical image analysis by demonstrating the effectiveness of knowledge\ninjection for improving automated CXR classification, paving the way for more\naccurate and reliable diagnostic tools.", "published": "2025-02-19 05:45:56", "link": "http://arxiv.org/abs/2502.13447v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Estimating Commonsense Plausibility through Semantic Shifts", "abstract": "Commonsense plausibility estimation is critical for evaluating language\nmodels (LMs), yet existing generative approaches--reliant on likelihoods or\nverbalized judgments--struggle with fine-grained discrimination. In this paper,\nwe propose ComPaSS, a novel discriminative framework that quantifies\ncommonsense plausibility by measuring semantic shifts when augmenting sentences\nwith commonsense-related information. Plausible augmentations induce minimal\nshifts in semantics, while implausible ones result in substantial deviations.\nEvaluations on two types of fine-grained commonsense plausibility estimation\ntasks across different backbones, including LLMs and vision-language models\n(VLMs), show that ComPaSS consistently outperforms baselines. It demonstrates\nthe advantage of discriminative approaches over generative methods in\nfine-grained commonsense plausibility evaluation. Experiments also show that\n(1) VLMs yield superior performance to LMs, when integrated with ComPaSS, on\nvision-grounded commonsense tasks. (2) contrastive pre-training sharpens\nbackbone models' ability to capture semantic nuances, thereby further enhancing\nComPaSS.", "published": "2025-02-19 06:31:06", "link": "http://arxiv.org/abs/2502.13464v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in\n  Speech Dialogue Systems", "abstract": "Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly\nenhanced the naturalness of human-machine interaction by enabling real-time\nbidirectional communication. However, existing approaches face challenges such\nas difficulties in independent module optimization and contextual noise\ninterference due to highly coupled architectural designs and oversimplified\nbinary state modeling. This paper proposes FlexDuo, a flexible full-duplex\ncontrol module that decouples duplex control from spoken dialogue systems\nthrough a plug-and-play architectural design. Furthermore, inspired by human\ninformation-filtering mechanisms in conversations, we introduce an explicit\nIdle state. On one hand, the Idle state filters redundant noise and irrelevant\naudio to enhance dialogue quality. On the other hand, it establishes a semantic\nintegrity-based buffering mechanism, reducing the risk of mutual interruptions\nwhile ensuring accurate response transitions. Experimental results on the\nFisher corpus demonstrate that FlexDuo reduces the false interruption rate by\n24.9% and improves response accuracy by 7.6% compared to integrated full-duplex\ndialogue system baselines. It also outperforms voice activity detection (VAD)\ncontrolled baseline systems in both Chinese and English dialogue quality. The\nproposed modular architecture and state-based dialogue model provide a novel\ntechnical pathway for building flexible and efficient duplex dialogue systems.", "published": "2025-02-19 06:51:34", "link": "http://arxiv.org/abs/2502.13472v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LLM should think and action as a human", "abstract": "It is popular lately to train large language models to be used as chat\nassistants, but in the conversation between the user and the chat assistant,\nthere are prompts, require multi-turns between the chat assistant and the user.\nHowever, there are a number of issues with the multi-turns conversation: The\nresponse of the chat assistant is prone to errors and can't help users achieve\ntheir goals, and as the number of conversation turns increases, the probability\nof errors will also increase; It is difficult for chat assistant to generate\nresponses with different processes based on actual needs for the same prompt;\nChat assistant require the use of tools, but the current approach is not\nelegant and efficient, and the number of tool calls is limited. The main reason\nfor these issues is that large language models don't have the thinking ability\nas a human, lack the reasoning ability and planning ability, and lack the\nability to execute plans. To solve these issues, we propose a thinking method\nbased on a built-in chain of thought: In the multi-turns conversation, for each\nuser prompt, the large language model thinks based on elements such as chat\nhistory, thinking context, action calls, memory and knowledge, makes detailed\nreasoning and planning, and actions according to the plan. We also explored how\nthe large language model enhances thinking ability through this thinking\nmethod: Collect training datasets according to the thinking method and fine\ntune the large language model through supervised learning; Train a consistency\nreward model and use it as a reward function to fine tune the large language\nmodel using reinforcement learning, and the reinforced large language model\noutputs according to this way of thinking. Our experimental results show that\nthe reasoning ability and planning ability of the large language model are\nenhanced, and the issues in the multi-turns conversation are solved.", "published": "2025-02-19 06:58:34", "link": "http://arxiv.org/abs/2502.13475v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What are Models Thinking about? Understanding Large Language Model\n  Hallucinations \"Psychology\" through Model Inner State Analysis", "abstract": "Large language model (LLM) systems suffer from the models' unstable ability\nto generate valid and factual content, resulting in hallucination generation.\nCurrent hallucination detection methods heavily rely on out-of-model\ninformation sources, such as RAG to assist the detection, thus bringing heavy\nadditional latency. Recently, internal states of LLMs' inference have been\nwidely used in numerous research works, such as prompt injection detection,\netc. Considering the interpretability of LLM internal states and the fact that\nthey do not require external information sources, we introduce such states into\nLLM hallucination detection. In this paper, we systematically analyze different\ninternal states' revealing features during inference forward and\ncomprehensively evaluate their ability in hallucination detection.\nSpecifically, we cut the forward process of a large language model into three\nstages: understanding, query, generation, and extracting the internal state\nfrom these stages. By analyzing these states, we provide a deep understanding\nof why the hallucinated content is generated and what happened in the internal\nstate of the models. Then, we introduce these internal states into\nhallucination detection and conduct comprehensive experiments to discuss the\nadvantages and limitations.", "published": "2025-02-19 07:23:18", "link": "http://arxiv.org/abs/2502.13490v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Geo-Culturally Grounded LLM Generations", "abstract": "Generative large language models (LLMs) have been demonstrated to have gaps\nin diverse, cultural knowledge across the globe. We investigate the effect of\nretrieval augmented generation and search-grounding techniques on the ability\nof LLMs to display familiarity with a diverse range of national cultures.\nSpecifically, we compare the performance of standard LLMs, LLMs augmented with\nretrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs\naugmented with retrievals from a web search (i.e., search grounding) on a\nseries of cultural familiarity benchmarks. We find that search grounding\nsignificantly improves the LLM performance on multiple-choice benchmarks that\ntest propositional knowledge (e.g., the norms, artifacts, and institutions of\nnational cultures), while KB grounding's effectiveness is limited by inadequate\nknowledge base coverage and a suboptimal retriever. However, search grounding\nalso increases the risk of stereotypical judgments by language models, while\nfailing to improve evaluators' judgments of cultural familiarity in a human\nevaluation with adequate statistical power. These results highlight the\ndistinction between propositional knowledge about a culture and open-ended\ncultural fluency when it comes to evaluating the cultural familiarity of\ngenerative LLMs.", "published": "2025-02-19 07:29:58", "link": "http://arxiv.org/abs/2502.13497v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference", "abstract": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.", "published": "2025-02-19 08:50:44", "link": "http://arxiv.org/abs/2502.13542v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap\n  for Text Length Control via MARKERGEN", "abstract": "Despite the rapid progress of large language models (LLMs), their\nlength-controllable text generation (LCTG) ability remains below expectations,\nposing a major limitation for practical applications. Existing methods mainly\nfocus on end-to-end training to reinforce adherence to length constraints.\nHowever, the lack of decomposition and targeted enhancement of LCTG\nsub-abilities restricts further progress. To bridge this gap, we conduct a\nbottom-up decomposition of LCTG sub-abilities with human patterns as reference\nand perform a detailed error analysis. On this basis, we propose MarkerGen, a\nsimple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental\ndeficiencies via external tool integration;(2) conducts explicit length\nmodeling with dynamically inserted markers;(3) employs a three-stage generation\nscheme to better align length constraints while maintaining content quality.\nComprehensive experiments demonstrate that MarkerGen significantly improves\nLCTG across various settings, exhibiting outstanding effectiveness and\ngeneralizability.", "published": "2025-02-19 08:52:45", "link": "http://arxiv.org/abs/2502.13544v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LSR-Adapt: Ultra-Efficient Parameter Tuning with Matrix Low Separation\n  Rank Kernel Adaptation", "abstract": "Imposing an effective structural assumption on neural network weight matrices\nhas been the major paradigm for designing Parameter-Efficient Fine-Tuning\n(PEFT) systems for adapting modern large pre-trained models to various\ndownstream tasks. However, low rank based adaptation has become increasingly\nchallenging due to the sheer scale of modern large language models. In this\npaper, we propose an effective kernelization to further reduce the number of\nparameters required for adaptation tasks. Specifically, from the classical idea\nin numerical analysis regarding matrix Low-Separation-Rank (LSR)\nrepresentations, we develop a kernel using this representation for the low rank\nadapter matrices of the linear layers from large networks, named the Low\nSeparation Rank Adaptation (LSR-Adapt) kernel. With the ultra-efficient kernel\nrepresentation of the low rank adapter matrices, we manage to achieve\nstate-of-the-art performance with even higher accuracy with almost half the\nnumber of parameters as compared to conventional low rank based methods. This\nstructural assumption also opens the door to further GPU-side optimizations due\nto the highly parallelizable nature of Kronecker computations.", "published": "2025-02-19 09:20:47", "link": "http://arxiv.org/abs/2502.13568v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Complex Ontology Matching with Large Language Model Embeddings", "abstract": "Ontology, and more broadly, Knowledge Graph Matching is a challenging task in\nwhich expressiveness has not been fully addressed. Despite the increasing use\nof embeddings and language models for this task, approaches for generating\nexpressive correspondences still do not take full advantage of these models, in\nparticular, large language models (LLMs). This paper proposes to integrate LLMs\ninto an approach for generating expressive correspondences based on alignment\nneed and ABox-based relation discovery. The generation of correspondences is\nperformed by matching similar surroundings of instance sub-graphs. The\nintegration of LLMs results in different architectural modifications, including\nlabel similarity, sub-graph matching, and entity matching. The performance word\nembeddings, sentence embeddings, and LLM-based embeddings, was compared. The\nresults demonstrate that integrating LLMs surpasses all other models, enhancing\nthe baseline version of the approach with a 45\\% increase in F-measure.", "published": "2025-02-19 10:56:27", "link": "http://arxiv.org/abs/2502.13619v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "REFIND at SemEval-2025 Task 3: Retrieval-Augmented Factuality\n  Hallucination Detection in Large Language Models", "abstract": "Hallucinations in large language model (LLM) outputs severely limit their\nreliability in knowledge-intensive tasks such as question answering. To address\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\nhallucINation Detection), a novel framework that detects hallucinated spans\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\ninnovative approach enables REFIND to efficiently and accurately detect\nhallucinations, setting it apart from existing methods. In the evaluation,\nREFIND demonstrated robustness across nine languages, including low-resource\nsettings, and significantly outperformed baseline models, achieving superior\nIoU scores in identifying hallucinated spans. This work highlights the\neffectiveness of quantifying context sensitivity for hallucination detection,\nthereby paving the way for more reliable and trustworthy LLM applications\nacross diverse languages. Our code is available at\nhttps://github.com/oneonlee/REFIND.", "published": "2025-02-19 10:59:05", "link": "http://arxiv.org/abs/2502.13622v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "C2T: A Classifier-Based Tree Construction Method in Speculative Decoding", "abstract": "The growing scale of Large Language Models (LLMs) has exacerbated inference\nlatency and computational costs. Speculative decoding methods, which aim to\nmitigate these issues, often face inefficiencies in the construction of token\ntrees and the verification of candidate tokens. Existing strategies, including\nchain mode, static tree, and dynamic tree approaches, have limitations in\naccurately preparing candidate token trees for verification. We propose a novel\nmethod named C2T that adopts a lightweight classifier to generate and prune\ntoken trees dynamically. Our classifier considers additional feature variables\nbeyond the commonly used joint probability to predict the confidence score for\neach draft token to determine whether it is the candidate token for\nverification. This method outperforms state-of-the-art (SOTA) methods such as\nEAGLE-2 on multiple benchmarks, by reducing the total number of candidate\ntokens by 25% while maintaining or even improving the acceptance length.", "published": "2025-02-19 11:57:02", "link": "http://arxiv.org/abs/2502.13652v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Novel Transformer Architecture for Time-series Forecasting", "abstract": "Despite the success of Transformer-based models in the time-series prediction\n(TSP) tasks, the existing Transformer architecture still face limitations and\nthe literature lacks comprehensive explorations into alternative architectures.\nTo address these challenges, we propose AutoFormer-TS, a novel framework that\nleverages a comprehensive search space for Transformer architectures tailored\nto TSP tasks. Our framework introduces a differentiable neural architecture\nsearch (DNAS) method, AB-DARTS, which improves upon existing DNAS approaches by\nenhancing the identification of optimal operations within the architecture.\nAutoFormer-TS systematically explores alternative attention mechanisms,\nactivation functions, and encoding operations, moving beyond the traditional\nTransformer design. Extensive experiments demonstrate that AutoFormer-TS\nconsistently outperforms state-of-the-art baselines across various TSP\nbenchmarks, achieving superior forecasting accuracy while maintaining\nreasonable training efficiency.", "published": "2025-02-19 13:49:20", "link": "http://arxiv.org/abs/2502.13721v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs\n  with Refined Values", "abstract": "We introduce Direct Value Optimization (DVO), an innovative reinforcement\nlearning framework for enhancing large language models in complex reasoning\ntasks. Unlike traditional methods relying on preference labels, DVO utilizes\nvalue signals at individual reasoning steps, optimizing models via a mean\nsquared error loss. The key benefit of DVO lies in its fine-grained\nsupervision, circumventing the need for labor-intensive human annotations.\nTarget values within the DVO are estimated using either Monte Carlo Tree Search\nor an outcome value model. Our empirical analysis on both mathematical and\ncommonsense reasoning tasks shows that DVO consistently outperforms existing\noffline preference optimization techniques, even with fewer training steps.\nThese findings underscore the importance of value signals in advancing\nreasoning capabilities and highlight DVO as a superior methodology under\nscenarios lacking explicit human preference information.", "published": "2025-02-19 13:51:05", "link": "http://arxiv.org/abs/2502.13723v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EHOP: A Dataset of Everyday NP-Hard Optimization Problems", "abstract": "We introduce the dataset of Everyday Hard Optimization Problems (EHOP), a\ncollection of NP-hard optimization problems expressed in natural language. EHOP\nincludes problem formulations that could be found in computer science\ntextbooks, versions that are dressed up as problems that could arise in real\nlife, and variants of well-known problems with inverted rules. We find that\nstate-of-the-art LLMs, across multiple prompting strategies, systematically\nsolve textbook problems more accurately than their real-life and inverted\ncounterparts. We argue that this constitutes evidence that LLMs adapt solutions\nseen during training, rather than leveraging reasoning abilities that would\nenable them to generalize to novel problems.", "published": "2025-02-19 14:39:59", "link": "http://arxiv.org/abs/2502.13776v1", "categories": ["cs.CL", "cs.CC", "68Q15", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Translation in the Hands of Many:Centering Lay Users in Machine\n  Translation Interactions", "abstract": "Converging societal and technical factors have transformed language\ntechnologies into user-facing applications employed across languages. Machine\nTranslation (MT) has become a global tool, with cross-lingual services now also\nsupported by dialogue systems powered by multilingual Large Language Models\n(LLMs). This accessibility has expanded MT's reach to a vast base of lay users,\noften with little to no expertise in the languages or the technology itself.\nDespite this, the understanding of MT consumed by this diverse group of users\n-- their needs, experiences, and interactions with these systems -- remains\nlimited. This paper traces the shift in MT user profiles, focusing on\nnon-expert users and how their engagement with these systems may change with\nLLMs. We identify three key factors -- usability, trust, and literacy -- that\nshape these interactions and must be addressed to align MT with user needs. By\nexploring these dimensions, we offer insights to guide future MT with a\nuser-centered approach.", "published": "2025-02-19 14:45:17", "link": "http://arxiv.org/abs/2502.13780v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "On the Duality between Gradient Transformations and Adapters", "abstract": "We study memory-efficient optimization of neural networks with linear\ngradient transformations, where the gradients are linearly mapped to a lower\ndimensional space than the full parameter space, thus saving memory required\nfor gradient accumulation and optimizer state persistence. The model parameters\nare updated by first performing an optimization step in the lower dimensional\nspace and then going back into the original parameter space via the linear\nmap's transpose. We show that optimizing the model in this transformed space is\nequivalent to reparameterizing the original model through a linear adapter that\nadditively modifies the model parameters, and then only optimizing the\nadapter's parameters. When the transformation is Kronecker-factored, this\nestablishes an equivalence between GaLore and one-sided LoRA. We show that this\nduality between gradient transformations and adapter-based reparameterizations\nunifies existing approaches to memory-efficient training and suggests new\ntechniques for improving training efficiency and memory use.", "published": "2025-02-19 15:26:18", "link": "http://arxiv.org/abs/2502.13811v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GroundCap: A Visually Grounded Image Captioning Dataset", "abstract": "Current image captioning systems lack the ability to link descriptive text to\nspecific visual elements, making their outputs difficult to verify. While\nrecent approaches offer some grounding capabilities, they cannot track object\nidentities across multiple references or ground both actions and objects\nsimultaneously. We propose a novel ID-based grounding system that enables\nconsistent object reference tracking and action-object linking, and present\nGroundCap, a dataset containing 52,016 images from 77 movies, with 344\nhuman-annotated and 52,016 automatically generated captions. Each caption is\ngrounded on detected objects (132 classes) and actions (51 classes) using a tag\nsystem that maintains object identity while linking actions to the\ncorresponding objects. Our approach features persistent object IDs for\nreference tracking, explicit action-object linking, and segmentation of\nbackground elements through K-means clustering. We propose gMETEOR, a metric\ncombining caption quality with grounding accuracy, and establish baseline\nperformance by fine-tuning Pixtral-12B. Human evaluation demonstrates our\napproach's effectiveness in producing verifiable descriptions with coherent\nobject references.", "published": "2025-02-19 17:31:59", "link": "http://arxiv.org/abs/2502.13898v2", "categories": ["cs.CV", "cs.CL", "I.2.10; I.2.7"], "primary_category": "cs.CV"}
{"title": "How Do LLMs Perform Two-Hop Reasoning in Context?", "abstract": "\"Socrates is human. All humans are mortal. Therefore, Socrates is mortal.\"\nThis classical example demonstrates two-hop reasoning, where a conclusion\nlogically follows from two connected premises. While transformer-based Large\nLanguage Models (LLMs) can make two-hop reasoning, they tend to collapse to\nrandom guessing when faced with distracting premises. To understand the\nunderlying mechanism, we train a three-layer transformer on synthetic two-hop\nreasoning tasks. The training dynamics show two stages: a slow learning phase,\nwhere the 3-layer transformer performs random guessing like LLMs, followed by\nan abrupt phase transitions, where the 3-layer transformer suddenly reaches\n$100%$ accuracy. Through reverse engineering, we explain the inner mechanisms\nfor how models learn to randomly guess between distractions initially, and how\nthey learn to ignore distractions eventually. We further propose a\nthree-parameter model that supports the causal claims for the mechanisms to the\ntraining dynamics of the transformer. Finally, experiments on LLMs suggest that\nthe discovered mechanisms generalize across scales. Our methodologies provide\nnew perspectives for scientific understandings of LLMs and our findings provide\nnew insights into how reasoning emerges during training.", "published": "2025-02-19 17:46:30", "link": "http://arxiv.org/abs/2502.13913v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Personalized Health Support through Data-Driven, Theory-Guided\n  LLMs: A Case Study in Sleep Health", "abstract": "Despite the prevalence of sleep-tracking devices, many individuals struggle\nto translate data into actionable improvements in sleep health. Current methods\noften provide data-driven suggestions but may not be feasible and adaptive to\nreal-life constraints and individual contexts. We present HealthGuru, a novel\nlarge language model-powered chatbot to enhance sleep health through\ndata-driven, theory-guided, and adaptive recommendations with conversational\nbehavior change support. HealthGuru's multi-agent framework integrates wearable\ndevice data, contextual information, and a contextual multi-armed bandit model\nto suggest tailored sleep-enhancing activities. The system facilitates natural\nconversations while incorporating data-driven insights and theoretical behavior\nchange techniques. Our eight-week in-the-wild deployment study with 16\nparticipants compared HealthGuru to a baseline chatbot. Results show improved\nmetrics like sleep duration and activity scores, higher quality responses, and\nincreased user motivation for behavior change with HealthGuru. We also identify\nchallenges and design considerations for personalization and user engagement in\nhealth chatbots.", "published": "2025-02-19 17:53:43", "link": "http://arxiv.org/abs/2502.13920v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference Optimization", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superior short-context LLMs may\nunderperform in long-context scenarios due to insufficient long-context\nalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- and long-context performance. To address these challenges, we introduce\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\ntasks by internally transferring short-context capabilities. LongPO harnesses\nLLMs to learn from self-generated short-to-long preference data, comprising\npaired responses generated for identical instructions with long-context inputs\nand their compressed short-context counterparts, respectively. This preference\nreveals capabilities and potentials of LLMs cultivated during short-context\nalignment that may be diminished in under-aligned long-context scenarios.\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\nshort-context performance decline during long-context alignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\nretains short-context performance and largely outperforms naive SFT and DPO in\nboth long- and short-context tasks. Specifically, LongPO-trained models can\nachieve results on long-context benchmarks comparable to, or even surpassing,\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\nannotation and larger parameter scales. Our code is available at\nhttps://github.com/DAMO-NLP-SG/LongPO.", "published": "2025-02-19 17:59:03", "link": "http://arxiv.org/abs/2502.13922v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Qwen2.5-VL Technical Report", "abstract": "We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robust document parsing, and\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects using bounding boxes or points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceive spatial scales and temporal dynamics without relying on\ntraditional normalization techniques. By training a native dynamic-resolution\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an interactive visual agent capable of reasoning, tool usage, and task\nexecution in real-world scenarios such as operating computers and mobile\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\nmaintains robust linguistic performance, preserving the core language\ncompetencies of the Qwen2.5 LLM.", "published": "2025-02-19 18:00:14", "link": "http://arxiv.org/abs/2502.13923v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Latent Distribution Decoupling: A Probabilistic Framework for\n  Uncertainty-Aware Multimodal Emotion Recognition", "abstract": "Multimodal multi-label emotion recognition (MMER) aims to identify the\nconcurrent presence of multiple emotions in multimodal data. Existing studies\nprimarily focus on improving fusion strategies and modeling modality-to-label\ndependencies. However, they often overlook the impact of \\textbf{aleatoric\nuncertainty}, which is the inherent noise in the multimodal data and hinders\nthe effectiveness of modality fusion by introducing ambiguity into feature\nrepresentations. To address this issue and effectively model aleatoric\nuncertainty, this paper proposes Latent emotional Distribution Decomposition\nwith Uncertainty perception (LDDU) framework from a novel perspective of latent\nemotional space probabilistic modeling. Specifically, we introduce a\ncontrastive disentangled distribution mechanism within the emotion space to\nmodel the multimodal data, allowing for the extraction of semantic features and\nuncertainty. Furthermore, we design an uncertainty-aware fusion multimodal\nmethod that accounts for the dispersed distribution of uncertainty and\nintegrates distribution information. Experimental results show that LDDU\nachieves state-of-the-art performance on the CMU-MOSEI and M$^3$ED datasets,\nhighlighting the importance of uncertainty modeling in MMER. Code is available\nat https://github.com/201983290498/lddu\\_mmer.git.", "published": "2025-02-19 18:53:23", "link": "http://arxiv.org/abs/2502.13954v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision", "abstract": "Retrieval-augmented generation (RAG) has shown great potential for\nknowledge-intensive tasks, but its traditional architectures rely on static\nretrieval, limiting their effectiveness for complex questions that require\nsequential information-seeking. While agentic reasoning and search offer a more\nadaptive approach, most existing methods depend heavily on prompt engineering.\nIn this work, we introduce RAG-Gym, a unified optimization framework that\nenhances information-seeking agents through fine-grained process supervision at\neach search step. We also propose ReSearch, a novel agent architecture that\nsynergizes answer reasoning and search query generation within the RAG-Gym\nframework. Experiments on four challenging datasets show that RAG-Gym improves\nperformance by up to 25.6\\% across various agent architectures, with ReSearch\nconsistently outperforming existing baselines. Further analysis highlights the\neffectiveness of advanced LLMs as process reward judges and the transferability\nof trained reward models as verifiers for different LLMs. Additionally, we\nexamine the scaling properties of training and inference in agentic RAG. The\nproject homepage is available at https://rag-gym.github.io/.", "published": "2025-02-19 18:56:03", "link": "http://arxiv.org/abs/2502.13957v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression", "abstract": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.", "published": "2025-02-19 19:12:46", "link": "http://arxiv.org/abs/2502.14051v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Navigating Semantic Relations: Challenges for Language Models in\n  Abstract Common-Sense Reasoning", "abstract": "Large language models (LLMs) have achieved remarkable performance in\ngenerating human-like text and solving reasoning tasks of moderate complexity,\nsuch as question-answering and mathematical problem-solving. However, their\ncapabilities in tasks requiring deeper cognitive skills, such as common-sense\nunderstanding and abstract reasoning, remain under-explored. In this paper, we\nsystematically evaluate abstract common-sense reasoning in LLMs using the\nConceptNet knowledge graph. We propose two prompting approaches: instruct\nprompting, where models predict plausible semantic relationships based on\nprovided definitions, and few-shot prompting, where models identify relations\nusing examples as guidance. Our experiments with the gpt-4o-mini model show\nthat in instruct prompting, consistent performance is obtained when ranking\nmultiple relations but with substantial decline when the model is restricted to\npredicting only one relation. In few-shot prompting, the model's accuracy\nimproves significantly when selecting from five relations rather than the full\nset, although with notable bias toward certain relations. These results suggest\nsignificant gaps still, even in commercially used LLMs' abstract common-sense\nreasoning abilities, compared to human-level understanding. However, the\nfindings also highlight the promise of careful prompt engineering, based on\nselective retrieval, for obtaining better performance.", "published": "2025-02-19 20:20:24", "link": "http://arxiv.org/abs/2502.14086v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach", "abstract": "Large Language Models (LLMs) enhanced with external contexts, such as through\nretrieval-augmented generation (RAG), often face challenges in handling\nimperfect evidence. They tend to over-rely on external knowledge, making them\nvulnerable to misleading and unhelpful contexts. To address this, we propose\nthe concept of context-robust LLMs, which can effectively balance internal\nknowledge with external context, similar to human cognitive processes.\nSpecifically, context-robust LLMs should rely on external context only when\nlacking internal knowledge, identify contradictions between internal and\nexternal knowledge, and disregard unhelpful contexts. To achieve this goal, we\nintroduce Grft, a lightweight and plug-and-play gated representation\nfine-tuning approach. Grft consists of two key components: a gating mechanism\nto detect and filter problematic inputs, and low-rank representation adapters\nto adjust hidden representations. By training a lightweight intervention\nfunction with only 0.0004\\% of model size on fewer than 200 examples, Grft can\neffectively adapt LLMs towards context-robust behaviors.", "published": "2025-02-19 20:59:35", "link": "http://arxiv.org/abs/2502.14100v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Can Community Notes Replace Professional Fact-Checkers?", "abstract": "Two commonly-employed strategies to combat the rise of misinformation on\nsocial media are (i) fact-checking by professional organisations and (ii)\ncommunity moderation by platform users. Policy changes by Twitter/X and, more\nrecently, Meta, signal a shift away from partnerships with fact-checking\norganisations and towards an increased reliance on crowdsourced community\nnotes. However, the extent and nature of dependencies between fact-checking and\nhelpful community notes remain unclear. To address these questions, we use\nlanguage models to annotate a large corpus of Twitter/X community notes with\nattributes such as topic, cited sources, and whether they refute claims tied to\nbroader misinformation narratives. Our analysis reveals that community notes\ncite fact-checking sources up to five times more than previously reported.\nFact-checking is especially crucial for notes on posts linked to broader\nnarratives, which are twice as likely to reference fact-checking sources\ncompared to other sources. In conclusion, our results show that successful\ncommunity moderation heavily relies on professional fact-checking.", "published": "2025-02-19 22:26:39", "link": "http://arxiv.org/abs/2502.14132v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems", "abstract": "Achieving full-duplex communication in spoken dialogue systems (SDS) requires\nreal-time coordination between listening, speaking, and thinking. This paper\nproposes a semantic voice activity detection (VAD) module as a dialogue manager\n(DM) to efficiently manage turn-taking in full-duplex SDS. Implemented as a\nlightweight (0.5B) LLM fine-tuned on full-duplex conversation data, the\nsemantic VAD predicts four control tokens to regulate turn-switching and\nturn-keeping, distinguishing between intentional and unintentional barge-ins\nwhile detecting query completion for handling user pauses and hesitations. By\nprocessing input speech in short intervals, the semantic VAD enables real-time\ndecision-making, while the core dialogue engine (CDE) is only activated for\nresponse generation, reducing computational overhead. This design allows\nindependent DM optimization without retraining the CDE, balancing interaction\naccuracy and inference efficiency for scalable, next-generation full-duplex\nSDS.", "published": "2025-02-19 23:15:13", "link": "http://arxiv.org/abs/2502.14145v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "GneissWeb: Preparing High Quality Data for LLMs at Scale", "abstract": "Data quantity and quality play a vital role in determining the performance of\nLarge Language Models (LLMs). High-quality data, in particular, can\nsignificantly boost the LLM's ability to generalize on a wide range of\ndownstream tasks. Large pre-training datasets for leading LLMs remain\ninaccessible to the public, whereas many open datasets are small in size (less\nthan 5 trillion tokens), limiting their suitability for training large models.\n  In this paper, we introduce GneissWeb, a large dataset yielding around 10\ntrillion tokens that caters to the data quality and quantity requirements of\ntraining LLMs. Our GneissWeb recipe that produced the dataset consists of\nsharded exact sub-string deduplication and a judiciously constructed ensemble\nof quality filters. GneissWeb achieves a favorable trade-off between data\nquality and quantity, producing models that outperform models trained on\nstate-of-the-art open large datasets (5+ trillion tokens).\n  We show that models trained using GneissWeb dataset outperform those trained\non FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed\non a set of 11 commonly used benchmarks (both zero-shot and few-shot) for\npre-training dataset evaluation. When the evaluation set is extended to 20\nbenchmarks (both zero-shot and few-shot), models trained using GneissWeb still\nachieve a 1.75 percentage points advantage over those trained on\nFineWeb-V1.1.0.", "published": "2025-02-19 00:14:29", "link": "http://arxiv.org/abs/2502.14907v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EvoP: Robust LLM Inference via Evolutionary Pruning", "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing\nstructured pruning methods address this issue by removing redundant structures\n(e.g., elements, channels, layers) from the model. However, these methods\nemploy a heuristic pruning strategy, which leads to suboptimal performance.\nBesides, they also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting structured pruning techniques, EvoP achieves the best performance\nwhile maintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications.", "published": "2025-02-19 06:33:59", "link": "http://arxiv.org/abs/2502.14910v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Batayan: A Filipino NLP benchmark for evaluating Large Language Models", "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities on widely benchmarked high-resource languages; however, linguistic\nnuances of under-resourced languages remain unexplored. We introduce Batayan, a\nholistic Filipino benchmark designed to systematically evaluate LLMs across\nthree key natural language processing (NLP) competencies: understanding,\nreasoning, and generation. Batayan consolidates eight tasks, covering both\nTagalog and code-switched Taglish utterances. Our rigorous,\nnative-speaker-driven annotation process ensures fluency and authenticity to\nthe complex morphological and syntactic structures of Filipino, alleviating a\npervasive translationese bias in existing Filipino corpora. We report empirical\nresults on a variety of multilingual LLMs, highlighting significant performance\ngaps that signal the under-representation of Filipino in pretraining corpora,\nthe unique hurdles in modeling Filipino's rich morphology and construction, and\nthe importance of explicit Filipino language support and instruction tuning.\nMoreover, we discuss the practical challenges encountered in dataset\nconstruction and propose principled solutions for building culturally and\nlinguistically-faithful resources in under-represented languages. We also\nprovide a public benchmark and leaderboard as a clear foundation for iterative,\ncommunity-driven progress in Filipino NLP.", "published": "2025-02-19 07:03:15", "link": "http://arxiv.org/abs/2502.14911v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD\n  Coding for Chinese EMRs", "abstract": "The task of automatically coding the International Classification of Diseases\n(ICD) in the medical field has been well-established and has received much\nattention. Automatic coding of the ICD in the medical field has been successful\nin English but faces challenges when dealing with Chinese electronic medical\nrecords (EMRs). The first issue lies in the difficulty of extracting disease\ncode-related information from Chinese EMRs, primarily due to the concise\nwriting style and specific internal structure of the EMRs. The second problem\nis that previous methods have failed to leverage the disease-based multi-axial\nknowledge and lack of association with the corresponding clinical evidence.\nThis paper introduces a novel framework called MKE-Coder: Multi-axial Knowledge\nwith Evidence verification in ICD coding for Chinese EMRs. Initially, we\nidentify candidate codes for the diagnosis and categorize each of them into\nknowledge under four coding axes.Subsequently, we retrieve corresponding\nclinical evidence from the comprehensive content of EMRs and filter credible\nevidence through a scoring model. Finally, to ensure the validity of the\ncandidate code, we propose an inference module based on the masked language\nmodeling strategy. This module verifies that all the axis knowledge associated\nwith the candidate code is supported by evidence and provides recommendations\naccordingly. To evaluate the performance of our framework, we conduct\nexperiments using a large-scale Chinese EMR dataset collected from various\nhospitals. The experimental results demonstrate that MKE-Coder exhibits\nsignificant superiority in the task of automatic ICD coding based on Chinese\nEMRs. In the practical evaluation of our method within simulated real coding\nscenarios, it has been demonstrated that our approach significantly aids coders\nin enhancing both their coding accuracy and speed.", "published": "2025-02-19 08:08:53", "link": "http://arxiv.org/abs/2502.14916v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers", "abstract": "This paper identifies the misinterpretation of the context can be a\nsignificant issue during the reasoning process of large language models,\nspanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones\nlike DeepSeek-R1. For example, in the phrase \"10 dollars per kilo,\" LLMs might\nnot recognize that \"per\" means \"for each,\" leading to calculation errors. We\nintroduce a novel, post-training approach called **Stick to the Facts (SIFT)**\nto tackle this. SIFT leverages increasing inference-time compute to ground LLM\nreasoning in contexts. At the core of SIFT lies the *Sticker*, which is\ngenerated by the model itself to explicitly emphasize the key information\nwithin the context. Given the curated Sticker, SIFT generates two predictions\n-- one from the original query and one from the query augmented with the\nSticker. If they differ, the Sticker is sequentially refined via *forward*\noptimization (to better align the extracted facts with the query) and *inverse*\ngeneration (to conform with the model's inherent tendencies) for more faithful\nreasoning outcomes. Studies across diverse models (from 3B to 100B+) and\nbenchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements.\nNotably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from\n78.33% to **85.67**%, establishing a new state-of-the-art in the open-source\ncommunity. The code is available at https://github.com/zhijie-group/SIFT.", "published": "2025-02-19 17:38:46", "link": "http://arxiv.org/abs/2502.14922v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AI Thinking as a Meaning-Centered Framework: Reimagining Language\n  Technologies Through Community Agency", "abstract": "While language technologies have advanced significantly, current approaches\nfail to address the complex sociocultural dimensions of linguistic\npreservation. AI Thinking proposes a meaning-centered framework that would\ntransform technological development from creating tools FOR communities to\nco-creating solutions WITH them. This approach recognizes that meaningful\nsolutions emerge through the interplay of cultural understanding, community\nagency, and technological innovation. The proposal articulates a holistic\nmethodology and a five-layer technological ecosystem where communities maintain\ncontrol over their linguistic and cultural knowledge representation. This\nsystematic integration of community needs, cultural preservation, and advanced\ncapabilities could revolutionize how we approach linguistic diversity\npreservation in the digital age.", "published": "2025-02-19 18:09:24", "link": "http://arxiv.org/abs/2502.14923v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of\n  Language?", "abstract": "Language exhibits a fractal structure in its information-theoretic complexity\n(i.e. bits per token), with self-similarity across scales and long-range\ndependence (LRD). In this work, we investigate whether large language models\n(LLMs) can replicate such fractal characteristics and identify conditions-such\nas temperature setting and prompting method-under which they may fail.\nMoreover, we find that the fractal parameters observed in natural language are\ncontained within a narrow range, whereas those of LLMs' output vary widely,\nsuggesting that fractal parameters might prove helpful in detecting a\nnon-trivial portion of LLM-generated texts. Notably, these findings, and many\nothers reported in this work, are robust to the choice of the architecture;\ne.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset\ncomprising of over 240,000 articles generated by various LLMs (both pretrained\nand instruction-tuned) with different decoding temperatures and prompting\nmethods, along with their corresponding human-generated texts. We hope that\nthis work highlights the complex interplay between fractal properties,\nprompting, and statistical mimicry in LLMs, offering insights for generating,\nevaluating and detecting synthetic texts.", "published": "2025-02-19 18:15:57", "link": "http://arxiv.org/abs/2502.14924v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Megrez-Omni Technical Report", "abstract": "In this work, we present the Megrez models, comprising a language model\n(Megrez-3B-Instruct) and a multimodal model (Megrez-3B-Omni). These models are\ndesigned to deliver fast inference, compactness, and robust edge-side\nintelligence through a software-hardware co-design approach. Megrez-3B-Instruct\noffers several advantages, including high accuracy, high speed, ease of use,\nand a wide range of applications. Building on Megrez-3B-Instruct,\nMegrez-3B-Omni is an on-device multimodal understanding LLM that supports\nimage, text, and audio analysis. It achieves state-of-the-art accuracy across\nall three modalities and demonstrates strong versatility and robustness,\nsetting a new benchmark for multimodal AI models.", "published": "2025-02-19 06:14:14", "link": "http://arxiv.org/abs/2502.15803v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Zero-Shot Commonsense Validation and Reasoning with Large Language\n  Models: An Evaluation on SemEval-2020 Task 4 Dataset", "abstract": "This study evaluates the performance of Large Language Models (LLMs) on\nSemEval-2020 Task 4 dataset, focusing on commonsense validation and\nexplanation. Our methodology involves evaluating multiple LLMs, including\nLLaMA3-70B, Gemma2-9B, and Mixtral-8x7B, using zero-shot prompting techniques.\nThe models are tested on two tasks: Task A (Commonsense Validation), where\nmodels determine whether a statement aligns with commonsense knowledge, and\nTask B (Commonsense Explanation), where models identify the reasoning behind\nimplausible statements. Performance is assessed based on accuracy, and results\nare compared to fine-tuned transformer-based models. The results indicate that\nlarger models outperform previous models and perform closely to human\nevaluation for Task A, with LLaMA3-70B achieving the highest accuracy of 98.40%\nin Task A whereas, lagging behind previous models with 93.40% in Task B.\nHowever, while models effectively identify implausible statements, they face\nchallenges in selecting the most relevant explanation, highlighting limitations\nin causal and inferential reasoning.", "published": "2025-02-19 12:40:49", "link": "http://arxiv.org/abs/2502.15810v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought\n  Verification", "abstract": "According to the Test-Time Scaling, the integration of External Slow-Thinking\nwith the Verify mechanism has been demonstrated to enhance multi-round\nreasoning in large language models (LLMs). However, in the multimodal (MM)\ndomain, there is still a lack of a strong MM-Verifier. In this paper, we\nintroduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through\nlonger inference and more robust verification. First, we propose a two-step MM\nverification data synthesis method, which combines a simulation-based tree\nsearch with verification and uses rejection sampling to generate high-quality\nChain-of-Thought (COT) data. This data is then used to fine-tune the\nverification model, MM-Verifier. Additionally, we present a more efficient\nmethod for synthesizing MMCOT data, bridging the gap between text-based and\nmultimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner.\nOur MM-Verifier outperforms all larger models on the MathCheck, MathVista, and\nMathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness\nand scalability, with performance improving as data size increases. Finally,\nour approach achieves strong performance when combining MM-Reasoner and\nMM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o\n(63.8) with 12 rollouts.", "published": "2025-02-19 02:46:52", "link": "http://arxiv.org/abs/2502.13383v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RLTHF: Targeted Human Feedback for LLM Alignment", "abstract": "Fine-tuning large language models (LLMs) to align with user preferences is\nchallenging due to the high cost of quality human annotations in Reinforcement\nLearning from Human Feedback (RLHF) and the generalizability limitations of AI\nFeedback. To address these challenges, we propose RLTHF, a human-AI hybrid\nframework that combines LLM-based initial alignment with selective human\nannotations to achieve full-human annotation alignment with minimal effort.\nRLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward\nmodel's reward distribution and iteratively enhances alignment by integrating\nstrategic human corrections while leveraging LLM's correctly labeled samples.\nEvaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human\nannotation-level alignment with only 6-7% of the human annotation effort.\nFurthermore, models trained on RLTHF's curated datasets for downstream tasks\noutperform those trained on fully human-annotated datasets, underscoring the\neffectiveness of RLTHF's strategic data curation.", "published": "2025-02-19 04:25:11", "link": "http://arxiv.org/abs/2502.13417v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TabSD: Large Free-Form Table Question Answering with SQL-Based Table\n  Decomposition", "abstract": "Question answering on free-form tables (TableQA) is challenging due to the\nabsence of predefined schemas and the presence of noise in large tables. While\nLarge Language Models (LLMs) have shown promise in TableQA, they struggle with\nlarge free-form tables and noise sensitivity. To address these challenges, we\npropose TabSD, a SQL-based decomposition model that enhances LLMs' ability to\nprocess large free-form tables. TabSD generates SQL queries to guide the table\ndecomposition, remove noise, and processes sub-tables for better answer\ngeneration. Additionally, SQL Verifier refines SQL outputs to enhance\ndecomposition accuracy. We introduce two TableQA datasets with large free-form\ntables, SLQA and SEQA, which consist solely of large free-form tables and will\nbe publicly available. Experimental results on four benchmark datasets\ndemonstrate that TABSD outperforms the best-existing baseline models by 23.07%,\n2.84%, 23.24% and 9.32% in accuracy, respectively, highlighting its\neffectiveness in handling large and noisy free-form tables.", "published": "2025-02-19 04:45:05", "link": "http://arxiv.org/abs/2502.13422v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM\n  Hallucination Evaluation", "abstract": "Large language models (LLMs) now achieve near-human performance on standard\nmath word problem benchmarks (e.g., GSM8K), yet their true reasoning ability\nremains disputed. A key concern is that models often produce confident, yet\nunfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic\ndataset that systematically generates infinite unanswerable math word problems\nand their answerable counterparts, by representing each question as a tree and\nremoving chosen necessary conditions. Experiments show TreeCut effectively\ninduce hallucinations in large language models, including GPT-4o and o3-mini,\nwith rates of 61% and 42% in their respective worst-case scenarios. Further\nanalysis highlights that deeper or more complex trees, composite item names,\nand removing necessary condition near the middle of a path all increase the\nlikelihood of hallucinations, underscoring the persistent challenges LLMs face\nin identifying unanswerable math problems.", "published": "2025-02-19 05:38:45", "link": "http://arxiv.org/abs/2502.13442v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails", "abstract": "Ensuring the safety of large language models (LLMs) is critical as they are\ndeployed in real-world applications. Existing guardrails rely on rule-based\nfiltering or single-pass classification, limiting their ability to handle\nnuanced safety violations. To address this, we propose ThinkGuard, a\ncritique-augmented guardrail model that distills knowledge from high-capacity\nLLMs by generating structured critiques alongside safety labels. Fine-tuned on\ncritique-augmented data, the captured deliberative thinking ability drastically\nenhances the guardrail's cautiousness and interpretability. Evaluated on\nmultiple safety benchmarks, ThinkGuard achieves the highest average F1 and\nAUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard\nimproves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses\nlabel-only fine-tuned models, confirming that structured critiques enhance both\nclassification precision and nuanced safety reasoning while maintaining\ncomputational efficiency.", "published": "2025-02-19 06:09:58", "link": "http://arxiv.org/abs/2502.13458v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HawkBench: Investigating Resilience of RAG Methods on Stratified\n  Information-Seeking Tasks", "abstract": "In real-world information-seeking scenarios, users have dynamic and diverse\nneeds, requiring RAG systems to demonstrate adaptable resilience. To\ncomprehensively evaluate the resilience of current RAG methods, we introduce\nHawkBench, a human-labeled, multi-domain benchmark designed to rigorously\nassess RAG performance across categorized task types. By stratifying tasks\nbased on information-seeking behaviors, HawkBench provides a systematic\nevaluation of how well RAG systems adapt to diverse user needs.\n  Unlike existing benchmarks, which focus primarily on specific task types\n(mostly factoid queries) and rely on varying knowledge bases, HawkBench offers:\n(1) systematic task stratification to cover a broad range of query types,\nincluding both factoid and rationale queries, (2) integration of multi-domain\ncorpora across all task types to mitigate corpus bias, and (3) rigorous\nannotation for high-quality evaluation.\n  HawkBench includes 1,600 high-quality test samples, evenly distributed across\ndomains and task types. Using this benchmark, we evaluate representative RAG\nmethods, analyzing their performance in terms of answer quality and response\nlatency. Our findings highlight the need for dynamic task strategies that\nintegrate decision-making, query interpretation, and global knowledge\nunderstanding to improve RAG generalizability. We believe HawkBench serves as a\npivotal benchmark for advancing the resilience of RAG methods and their ability\nto achieve general-purpose information seeking.", "published": "2025-02-19 06:33:39", "link": "http://arxiv.org/abs/2502.13465v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Transferring Textual Preferences to Vision-Language Understanding\n  through Model Merging", "abstract": "Large vision-language models (LVLMs) perform outstandingly across various\nmultimodal tasks. However, their ability to evaluate generated content remains\nlimited, and training vision-language reward models (VLRMs) with preference\ndata is computationally expensive. This paper explores a training-free\nalternative by merging text-based reward models (RMs) with LVLMs to create\nVLRMs. Our approach shows that integrating these models leads to improved\nperformance over LVLMs' scoring and text-based RMs, offering an efficient\nmethod for incorporating textual preferences into LVLMs.", "published": "2025-02-19 07:20:07", "link": "http://arxiv.org/abs/2502.13487v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference", "abstract": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.", "published": "2025-02-19 07:43:36", "link": "http://arxiv.org/abs/2502.13502v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework\n  for Language and Time Series Fusion", "abstract": "Large language models (LLMs) have shown remarkable performance in\nvision-language tasks, but their application in the medical field remains\nunderexplored, particularly for integrating structured time series data with\nunstructured clinical notes. In clinical practice, dynamic time series data\nsuch as lab test results capture critical temporal patterns, while clinical\nnotes provide rich semantic context. Merging these modalities is challenging\ndue to the inherent differences between continuous signals and discrete text.\nTo bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal\nframework that employs prompt-guided learning to unify these heterogeneous data\ntypes. Our approach leverages lightweight anomaly detection to generate anomaly\ncaptions that serve as prompts, guiding the encoding of raw time series data\ninto informative embeddings. These embeddings are aligned with textual\nrepresentations in a shared latent space, preserving fine-grained temporal\nnuances alongside semantic insights. Furthermore, our framework incorporates\ntailored self-supervised objectives to enhance both intra- and inter-modal\nalignment. We evaluate ProMedTS on disease diagnosis tasks using real-world\ndatasets, and the results demonstrate that our method consistently outperforms\nstate-of-the-art approaches.", "published": "2025-02-19 07:56:48", "link": "http://arxiv.org/abs/2502.13509v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Train Small, Infer Large: Memory-Efficient LoRA Training for Large\n  Language Models", "abstract": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing with exceptional task generalization capabilities. Low-Rank Adaption\n(LoRA) offers a cost-effective fine-tuning solution, freezing the original\nmodel parameters and training only lightweight, low-rank adapter matrices.\nHowever, the memory footprint of LoRA is largely dominated by the original\nmodel parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA\ntraining scheme founded on the intuition that many neurons in\nover-parameterized LLMs have low training utility but are essential for\ninference. LoRAM presents a unique twist: it trains on a pruned (small) model\nto obtain pruned low-rank matrices, which are then recovered and utilized with\nthe original (large) model for inference. Additionally, minimal-cost continual\npre-training, performed by the model publishers in advance, aligns the\nknowledge discrepancy between pruned and original models. Our extensive\nexperiments demonstrate the efficacy of LoRAM across various pruning strategies\nand downstream tasks. For a model with 70 billion parameters, LoRAM enables\ntraining on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA\ntraining and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by\nstructured pruning combined with 4-bit quantization, for LLaMA-3.1-70B\n(LLaMA-2-70B), reduces the parameter storage cost that dominates the memory\nusage in low-rank matrix training by 15.81$\\times$ (16.95$\\times$), while\nachieving dominant performance gains over both the original LLaMA-3.1-70B\n(LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B). Code is available at\nhttps://github.com/junzhang-zj/LoRAM.", "published": "2025-02-19 08:39:15", "link": "http://arxiv.org/abs/2502.13533v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MMTEB: Massive Multilingual Text Embedding Benchmark", "abstract": "Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost.", "published": "2025-02-19 10:13:43", "link": "http://arxiv.org/abs/2502.13595v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Efficient Safety Retrofitting Against Jailbreaking for LLMs", "abstract": "Direct Preference Optimization (DPO) is an efficient alignment technique that\nsteers LLMs towards preferable outputs by training on preference data,\nbypassing the need for explicit reward models. Its simplicity enables easy\nadaptation to various domains and safety requirements. This paper examines\nDPO's effectiveness in model safety against jailbreaking attacks while\nminimizing data requirements and training costs. We introduce Egida, a dataset\nexpanded from multiple sources, which includes 27 different safety topics and\n18 different attack styles, complemented with synthetic and human labels. This\ndata is used to boost the safety of state-of-the-art LLMs\n(Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack\nstyles. In addition to safety evaluations, we assess their post-alignment\nperformance degradation in general purpose tasks, and their tendency to over\nrefusal. Following the proposed methodology, trained models reduce their Attack\nSuccess Rate by 10%-30%, using small training efforts (2,000 samples) with low\ncomputational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned\nmodels generalize to unseen topics and attack styles, with the most successful\nattack style reaching a success rate around 5%. Size and family are found to\nstrongly influence model malleability towards safety, pointing at the\nimportance of pre-training choices. To validate our findings, a large\nindependent assessment of human preference agreement with Llama-Guard-3-8B is\nconducted by the authors and the associated dataset Egida-HSafe is released.\nOverall, this study illustrates how affordable and accessible it is to enhance\nLLM safety using DPO while outlining its current limitations. All datasets and\nmodels are released to enable reproducibility and further research.", "published": "2025-02-19 10:33:18", "link": "http://arxiv.org/abs/2502.13603v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Concept Layers: Enhancing Interpretability and Intervenability via LLM\n  Conceptualization", "abstract": "The opaque nature of Large Language Models (LLMs) has led to significant\nresearch efforts aimed at enhancing their interpretability, primarily through\npost-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck\nModels (CBMs), offer both interpretability and intervenability by incorporating\nexplicit concept representations. However, these methods suffer from key\nlimitations, including reliance on labeled concept datasets and significant\narchitectural modifications that challenges re-integration into existing system\npipelines. In this work, we introduce a new methodology for incorporating\ninterpretability and intervenability into an existing model by integrating\nConcept Layers (CLs) into its architecture. Our approach projects the model's\ninternal vector representations into a conceptual, explainable vector space\nbefore reconstructing and feeding them back into the model. Furthermore, we\neliminate the need for a human-selected concept set by algorithmically\nsearching an ontology for a set of concepts that can be either task-specific or\ntask-agnostic. We evaluate CLs across multiple tasks, demonstrating that they\nmaintain the original model's performance and agreement while enabling\nmeaningful interventions. Additionally, we present a proof of concept\nshowcasing an intervenability interface, allowing users to adjust model\nbehavior dynamically, such as mitigating biases during inference.", "published": "2025-02-19 11:10:19", "link": "http://arxiv.org/abs/2502.13632v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PeerQA: A Scientific Question Answering Dataset from Peer Reviews", "abstract": "We present PeerQA, a real-world, scientific, document-level Question\nAnswering (QA) dataset. PeerQA questions have been sourced from peer reviews,\nwhich contain questions that reviewers raised while thoroughly examining the\nscientific article. Answers have been annotated by the original authors of each\npaper. The dataset contains 579 QA pairs from 208 academic articles, with a\nmajority from ML and NLP, as well as a subset of other scientific communities\nlike Geoscience and Public Health. PeerQA supports three critical tasks for\ndeveloping practical QA systems: Evidence retrieval, unanswerable question\nclassification, and answer generation. We provide a detailed analysis of the\ncollected dataset and conduct experiments establishing baseline systems for all\nthree tasks. Our experiments and analyses reveal the need for\ndecontextualization in document-level retrieval, where we find that even simple\ndecontextualization approaches consistently improve retrieval performance\nacross architectures. On answer generation, PeerQA serves as a challenging\nbenchmark for long-context modeling, as the papers have an average size of 12k\ntokens. Our code and data is available at https://github.com/UKPLab/peerqa.", "published": "2025-02-19 12:24:46", "link": "http://arxiv.org/abs/2502.13668v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "An LLM-based Agent for Reliable Docker Environment Configuration", "abstract": "Environment configuration is a critical yet time-consuming step in software\ndevelopment, especially when dealing with unfamiliar code repositories. While\nLarge Language Models (LLMs) demonstrate the potential to accomplish software\nengineering tasks, existing methods for environment configuration often rely on\nmanual efforts or fragile scripts, leading to inefficiencies and unreliable\noutcomes. We introduce Repo2Run, the first LLM-based agent designed to fully\nautomate environment configuration and generate executable Dockerfiles for\narbitrary Python repositories. We address two major challenges: (1) enabling\nthe LLM agent to configure environments within isolated Docker containers, and\n(2) ensuring the successful configuration process is recorded and accurately\ntransferred to a Dockerfile without error. To achieve this, we propose atomic\nconfiguration synthesis, featuring a dual-environment architecture (internal\nand external environment) with a rollback mechanism to prevent environment\n\"pollution\" from failed commands, guaranteeing atomic execution (execute fully\nor not at all) and a Dockerfile generator to transfer successful configuration\nsteps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark\nof 420 recent Python repositories with unit tests, where it achieves an 86.0%\nsuccess rate, outperforming the best baseline by 63.9%. Repo2Run is available\nat https://github.com/bytedance/Repo2Run.", "published": "2025-02-19 12:51:35", "link": "http://arxiv.org/abs/2502.13681v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "MoM: Linear Sequence Modeling with Mixture-of-Memories", "abstract": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE.", "published": "2025-02-19 12:53:55", "link": "http://arxiv.org/abs/2502.13685v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in\n  Healthcare", "abstract": "Alignment techniques have become central to ensuring that Large Language\nModels (LLMs) generate outputs consistent with human values. However, existing\nalignment paradigms often model an averaged or monolithic preference, failing\nto account for the diversity of perspectives across cultures, demographics, and\ncommunities. This limitation is particularly critical in health-related\nscenarios, where plurality is essential due to the influence of culture,\nreligion, personal values, and conflicting opinions. Despite progress in\npluralistic alignment, no prior work has focused on health, likely due to the\nunavailability of publicly available datasets. To address this gap, we\nintroduce VITAL, a new benchmark dataset comprising 13.1K value-laden\nsituations and 5.4K multiple-choice questions focused on health, designed to\nassess and benchmark pluralistic alignment methodologies. Through extensive\nevaluation of eight LLMs of varying sizes, we demonstrate that existing\npluralistic alignment techniques fall short in effectively accommodating\ndiverse healthcare beliefs, underscoring the need for tailored AI alignment in\nspecific domains. This work highlights the limitations of current approaches\nand lays the groundwork for developing health-specific alignment solutions.", "published": "2025-02-19 14:38:57", "link": "http://arxiv.org/abs/2502.13775v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LESA: Learnable LLM Layer Scaling-Up", "abstract": "Training Large Language Models (LLMs) from scratch requires immense\ncomputational resources, making it prohibitively expensive. Model scaling-up\noffers a promising solution by leveraging the parameters of smaller models to\ncreate larger ones. However, existing depth scaling-up methods rely on\nempirical heuristic rules for layer duplication, which result in poorer\ninitialization and slower convergence during continual pre-training. We propose\n\\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating\nparameters from each layer and applying Singular Value Decomposition, we\nuncover latent patterns between layers, suggesting that inter-layer parameters\ncan be learned. LESA uses a neural network to predict the parameters inserted\nbetween adjacent layers, enabling better initialization and faster training.\nExperiments show that LESA outperforms existing baselines, achieving superior\nperformance with less than half the computational cost during continual\npre-training. Extensive analyses demonstrate its effectiveness across different\nmodel sizes and tasks.", "published": "2025-02-19 14:58:48", "link": "http://arxiv.org/abs/2502.13794v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Scoring Verifiers: Evaluating Synthetic Verification for Code and\n  Reasoning", "abstract": "Synthetic verification techniques such as generating test cases and reward\nmodelling are common ways to enhance the coding capabilities of large language\nmodels (LLM) beyond predefined tests. Additionally, code verification has\nrecently found great success as a critical component in improving reasoning\ncapability of LLMs via reinforcement learning. In this paper, we propose a an\napproach which can transform existing coding benchmarks into scoring and\nranking datasets to evaluate the effectiveness of synthetic verifiers. We also\npropose multiple metrics to measure different aspects of the synthetic\nverifiers with the proposed benchmarks. By employing the proposed approach, we\nrelease four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed\nsynthetic verification methods with standard, reasoning-based, and reward-based\nLLMs. Our experiments show that reasoning can significantly improve test case\ngeneration and that scaling the number of test cases enhances the verification\naccuracy.", "published": "2025-02-19 15:32:11", "link": "http://arxiv.org/abs/2502.13820v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.AI"}
{"title": "DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented\n  Generation Method for Multi-Turn Dialogue", "abstract": "Retrieval-Augmented Generation (RAG) systems have shown substantial benefits\nin applications such as question answering and multi-turn dialogue\n\\citep{lewis2020retrieval}. However, traditional RAG methods, while leveraging\nstatic knowledge bases, often overlook the potential of dynamic historical\ninformation in ongoing conversations. To bridge this gap, we introduce DH-RAG,\na Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for\nMulti-Turn Dialogue. DH-RAG is inspired by human cognitive processes that\nutilize both long-term memory and immediate historical context in\nconversational responses \\citep{stafford1987conversational}. DH-RAG is\nstructured around two principal components: a History-Learning based Query\nReconstruction Module, designed to generate effective queries by synthesizing\ncurrent and prior interactions, and a Dynamic History Information Updating\nModule, which continually refreshes historical context throughout the dialogue.\nThe center of DH-RAG is a Dynamic Historical Information database, which is\nfurther refined by three strategies within the Query Reconstruction Module:\nHistorical Query Clustering, Hierarchical Matching, and Chain of Thought\nTracking. Experimental evaluations show that DH-RAG significantly surpasses\nconventional models on several benchmarks, enhancing response relevance,\ncoherence, and dialogue quality.", "published": "2025-02-19 16:10:43", "link": "http://arxiv.org/abs/2502.13847v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PSCon: Toward Conversational Product Search", "abstract": "Conversational Product Search (CPS) is confined to simulated conversations\ndue to the lack of real-world CPS datasets that reflect human-like language.\nAdditionally, current conversational datasets are limited to support\ncross-market and multi-lingual usage. In this paper, we introduce a new CPS\ndata collection protocol and present PSCon, a novel CPS dataset designed to\nassist product search via human-like conversations. The dataset is constructed\nusing a coached human-to-human data collection protocol and supports two\nlanguages and dual markets. Also, the dataset enables thorough exploration of\nsix subtasks of CPS: user intent detection, keyword extraction, system action\nprediction, question selection, item ranking, and response generation.\nFurthermore, we also offer an analysis of the dataset and propose a benchmark\nmodel on the proposed CPS dataset.", "published": "2025-02-19 17:05:42", "link": "http://arxiv.org/abs/2502.13881v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "DataSciBench: An LLM Agent Benchmark for Data Science", "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating\nLarge Language Model (LLM) capabilities in data science. Recent related\nbenchmarks have primarily focused on single tasks, easily obtainable ground\ntruth, and straightforward evaluation metrics, which limits the scope of tasks\nthat can be evaluated. In contrast, DataSciBench is constructed based on a more\ncomprehensive and curated collection of natural and challenging prompts for\nuncertain ground truth and evaluation metrics. We develop a semi-automated\npipeline for generating ground truth (GT) and validating evaluation metrics.\nThis pipeline utilizes and implements an LLM-based self-consistency and human\nverification strategy to produce accurate GT by leveraging collected prompts,\npredefined task types, and aggregate functions (metrics). Furthermore, we\npropose an innovative Task - Function - Code (TFC) framework to assess each\ncode execution outcome based on precisely defined metrics and programmatic\nrules. Our experimental framework involves testing 6 API-based models, 8\nopen-source general models, and 9 open-source code generation models using the\ndiverse set of prompts we have gathered. This approach aims to provide a more\ncomprehensive and rigorous evaluation of LLMs in data science, revealing their\nstrengths and weaknesses. Experimental results demonstrate that API-based\nmodels outperform open-sourced models on all metrics and\nDeepseek-Coder-33B-Instruct achieves the highest score among open-sourced\nmodels. We release all code and data at https://github.com/THUDM/DataSciBench.", "published": "2025-02-19 17:31:51", "link": "http://arxiv.org/abs/2502.13897v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Symmetrical Visual Contrastive Optimization: Aligning Vision-Language\n  Models with Minimal Contrastive Images", "abstract": "Recent studies have shown that Large Vision-Language Models (VLMs) tend to\nneglect image content and over-rely on language-model priors, resulting in\nerrors in visually grounded tasks and hallucinations. We hypothesize that this\nissue arises because existing VLMs are not explicitly trained to generate texts\nthat are accurately grounded in fine-grained image details. To enhance visual\nfeedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive\nOptimization), a novel finetuning objective that steers the model toward\ncapturing important visual details and aligning them with corresponding text\ntokens. To further facilitate this detailed alignment, we introduce MVC, a\npaired image-text dataset built by automatically filtering and augmenting\nvisual counterfactual data to challenge the model with hard contrastive cases\ninvolving Minimal Visual Contrasts. Experiments show that our method\nconsistently improves VLM performance across diverse benchmarks covering\nvarious abilities and domains, achieving up to a 22% reduction in\nhallucinations, and significant gains in vision-centric and general tasks.\nNotably, these improvements become increasingly pronounced in benchmarks with\nhigher visual dependency. In short, S-VCO offers a significant enhancement of\nVLM's visually-dependent task performance while retaining or even improving the\nmodel's general abilities. We opensource our code at https://s-vco.github.io/", "published": "2025-02-19 18:05:42", "link": "http://arxiv.org/abs/2502.13928v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "AdaptiveStep: Automatically Dividing Reasoning Step through Model\n  Confidence", "abstract": "Current approaches for training Process Reward Models (PRMs) often involve\nbreaking down responses into multiple reasoning steps using rule-based\ntechniques, such as using predefined placeholder tokens or setting the\nreasoning step's length into a fixed size. These approaches overlook the fact\nthat specific words do not typically mark true decision points in a text. To\naddress this, we propose AdaptiveStep, a method that divides reasoning steps\nbased on the model's confidence in predicting the next word. This division\nmethod provides more decision-making information at each step, enhancing\ndownstream tasks, such as reward model learning. Moreover, our method does not\nrequire manual annotation. We demonstrate its effectiveness through experiments\nwith AdaptiveStep-trained PRMs in mathematical reasoning and code generation\ntasks. Experimental results indicate that the outcome PRM achieves\nstate-of-the-art Best-of-N performance, surpassing greedy search strategy with\ntoken-level value-guided decoding, while also reducing construction costs by\nover 30% compared to existing open-source PRMs. In addition, we provide a\nthorough analysis and case study on the PRM's performance, transferability, and\ngeneralization capabilities.", "published": "2025-02-19 18:35:55", "link": "http://arxiv.org/abs/2502.13943v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety\n  Mechanisms Tend to Be Anchored in The Template Region", "abstract": "The safety alignment of large language models (LLMs) remains vulnerable, as\ntheir initial behavior can be easily jailbroken by even relatively simple\nattacks. Since infilling a fixed template between the input instruction and\ninitial model output is a common practice for existing LLMs, we hypothesize\nthat this template is a key factor behind their vulnerabilities: LLMs'\nsafety-related decision-making overly relies on the aggregated information from\nthe template region, which largely influences these models' safety behavior. We\nrefer to this issue as template-anchored safety alignment. In this paper, we\nconduct extensive experiments and verify that template-anchored safety\nalignment is widespread across various aligned LLMs. Our mechanistic analyses\ndemonstrate how it leads to models' susceptibility when encountering\ninference-time jailbreak attacks. Furthermore, we show that detaching safety\nmechanisms from the template region is promising in mitigating vulnerabilities\nto jailbreak attacks. We encourage future research to develop more robust\nsafety alignment techniques that reduce reliance on the template region.", "published": "2025-02-19 18:42:45", "link": "http://arxiv.org/abs/2502.13946v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Erasing with Precision: Evaluating Specific Concept Erasure from\n  Text-to-Image Generative Models", "abstract": "Studies have been conducted to prevent specific concepts from being generated\nfrom pretrained text-to-image generative models, achieving concept erasure in\nvarious ways. However, the performance evaluation of these studies is still\nlargely reliant on visualization, with the superiority of studies often\ndetermined by human subjectivity. The metrics of quantitative evaluation also\nvary, making comprehensive comparisons difficult. We propose EraseEval, an\nevaluation method that differs from previous evaluation methods in that it\ninvolves three fundamental evaluation criteria: (1) How well does the prompt\ncontaining the target concept be reflected, (2) To what extent the concepts\nrelated to the erased concept can reduce the impact of the erased concept, and\n(3) Whether other concepts are preserved. These criteria are evaluated and\nintegrated into a single metric, such that a lower score is given if any of the\nevaluations are low, leading to a more robust assessment. We experimentally\nevaluated baseline concept erasure methods, organized their characteristics,\nand identified challenges with them. Despite being fundamental evaluation\ncriteria, some concept erasure methods failed to achieve high scores, which\npoint toward future research directions for concept erasure methods. Our code\nis available at https://github.com/fmp453/erase-eval.", "published": "2025-02-19 02:19:38", "link": "http://arxiv.org/abs/2502.13989v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures", "abstract": "The remarkable performance of large language models (LLMs) in various\nlanguage tasks has attracted considerable attention. However, the\never-increasing size of these models presents growing challenges for deployment\nand inference. Structured pruning, an effective model compression technique, is\ngaining increasing attention due to its ability to enhance inference\nefficiency. Nevertheless, most previous optimization-based structured pruning\nmethods sacrifice the uniform structure across layers for greater flexibility\nto maintain performance. The heterogeneous structure hinders the effective\nutilization of off-the-shelf inference acceleration techniques and impedes\nefficient configuration for continued training. To address this issue, we\npropose a novel masking learning paradigm based on minimax optimization to\nobtain the uniform pruned structure by optimizing the masks under sparsity\nregularization. Extensive experimental results demonstrate that our method can\nmaintain high performance while ensuring the uniformity of the pruned model\nstructure, thereby outperforming existing SOTA methods.", "published": "2025-02-19 11:57:31", "link": "http://arxiv.org/abs/2502.14008v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Which Attention Heads Matter for In-Context Learning?", "abstract": "Large language models (LLMs) exhibit impressive in-context learning (ICL)\ncapability, enabling them to perform new tasks using only a few demonstrations\nin the prompt. Two different mechanisms have been proposed to explain ICL:\ninduction heads that find and copy relevant tokens, and function vector (FV)\nheads whose activations compute a latent encoding of the ICL task. To better\nunderstand which of the two distinct mechanisms drives ICL, we study and\ncompare induction heads and FV heads in 12 language models.\n  Through detailed ablations, we discover that few-shot ICL performance depends\nprimarily on FV heads, especially in larger models. In addition, we uncover\nthat FV and induction heads are connected: many FV heads start as induction\nheads during training before transitioning to the FV mechanism. This leads us\nto speculate that induction facilitates learning the more complex FV mechanism\nthat ultimately drives ICL.", "published": "2025-02-19 12:25:02", "link": "http://arxiv.org/abs/2502.14010v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text\n  Generation Systems", "abstract": "As text generation systems' outputs are increasingly anthropomorphic --\nperceived as human-like -- scholars have also raised increasing concerns about\nhow such outputs can lead to harmful outcomes, such as users over-relying or\ndeveloping emotional dependence on these systems. How to intervene on such\nsystem outputs to mitigate anthropomorphic behaviors and their attendant\nharmful outcomes, however, remains understudied. With this work, we aim to\nprovide empirical and theoretical grounding for developing such interventions.\nTo do so, we compile an inventory of interventions grounded both in prior\nliterature and a crowdsourced study where participants edited system outputs to\nmake them less human-like. Drawing on this inventory, we also develop a\nconceptual framework to help characterize the landscape of possible\ninterventions, articulate distinctions between different types of\ninterventions, and provide a theoretical basis for evaluating the effectiveness\nof different interventions.", "published": "2025-02-19 18:06:37", "link": "http://arxiv.org/abs/2502.14019v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation", "abstract": "Despite their increasing performance, large language models still tend to\nreproduce training data, generate several repetitions, and focus on the most\ncommon grammatical structures and words. A possible cause is the decoding\nstrategy adopted: the most common ones either consider only the most probable\ntokens, reducing output diversity, or increase the likelihood of unlikely\ntokens at the cost of output accuracy and correctness. In this paper, we\npropose a family of three new decoding methods by leveraging a mathematical\nanalysis of the token probability distribution. In particular, the difference\nbetween consecutive, sorted probabilities can be used to avoid incorrect tokens\nand increase the chance of low-probable but accurate words. Experiments\nconcerning math problem solving, extreme summarization, and the divergent\nassociation task show that our approach consistently performs at least as well\nas current alternatives in terms of quality and diversity.", "published": "2025-02-19 19:00:02", "link": "http://arxiv.org/abs/2502.14037v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Decomposition and Selective Context Filtering -- Text\n  Processing Techniques for Context-Aware NLP-Based Systems", "abstract": "In this paper, we present two techniques for use in context-aware systems:\nSemantic Decomposition, which sequentially decomposes input prompts into a\nstructured and hierarchal information schema in which systems can parse and\nprocess easily, and Selective Context Filtering, which enables systems to\nsystematically filter out specific irrelevant sections of contextual\ninformation that is fed through a system's NLP-based pipeline. We will explore\nhow context-aware systems and applications can utilize these two techniques in\norder to implement dynamic LLM-to-system interfaces, improve an LLM's ability\nto generate more contextually cohesive user-facing responses, and optimize\ncomplex automated workflows and pipelines.", "published": "2025-02-19 19:09:40", "link": "http://arxiv.org/abs/2502.14048v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.7; I.7.0"], "primary_category": "cs.CL"}
{"title": "Diversity-driven Data Selection for Language Model Tuning through Sparse\n  Autoencoder", "abstract": "Instruction tuning data are often quantity-saturated due to the large volume\nof data collection and fast model iteration, leaving data selection important\nbut underexplored. Existing quality-driven data selection methods, such as LIMA\n(NeurIPS 2023 \\citep{zhou2024lima}) and AlpaGasus (ICLR 2024\n\\citep{chenalpagasus}) generally ignore the equal importance of data diversity\nand complexity. In this work, we aim to design a diversity-aware data selection\nstrategy and creatively propose using sparse autoencoders (SAEs) to tackle the\nchallenge of data diversity measure. In addition, SAEs can also provide more\ninterpretability of model behavior and explain, e.g., the surprising\neffectiveness of selecting the longest response (ICML 2024 \\citep{zhaolong}).\nUsing effective data selection, we experimentally prove that models trained on\nour selected data can outperform other methods in terms of model capabilities,\nreduce training cost, and potentially gain more control over model behaviors.\nWe prove that SAEs can serve as a good alternative to diversity measure and\ndesign our method to be scalable for potential industrial large-scale pruning,\nand we will also release our trained SAEs for use by the broader community.", "published": "2025-02-19 19:12:34", "link": "http://arxiv.org/abs/2502.14050v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating Non-Transitivity in LLM-as-a-Judge", "abstract": "Automatic evaluation methods based on large language models (LLMs) are\nemerging as the standard tool for assessing the instruction-following abilities\nof LLM-based agents. The most common method in this paradigm, pairwise\ncomparisons with a baseline model, critically depends on the assumption of\ntransitive preferences. However, the validity of this assumption remains\nlargely unexplored. In this study, we investigate the presence of\nnon-transitivity within the AlpacaEval framework and analyze its effects on\nmodel rankings. We find that LLM judges exhibit non-transitive preferences,\nleading to rankings that are sensitive to the choice of the baseline model. To\nmitigate this issue, we show that round-robin tournaments combined with\nBradley-Terry models of preference can produce more reliable rankings. Notably,\nour method increases both the Spearman correlation and the Kendall correlation\nwith Chatbot Arena (95.0% -> 96.4% and 82.1% -> 86.3% respectively). To address\nthe computational cost of round-robin tournaments, we propose Swiss-Wise\nIterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to\ncapture the benefits of round-robin tournaments while maintaining computational\nefficiency.", "published": "2025-02-19 19:59:16", "link": "http://arxiv.org/abs/2502.14074v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Benchmarking LLMs for Political Science: A United Nations Perspective", "abstract": "Large Language Models (LLMs) have achieved significant advances in natural\nlanguage processing, yet their potential for high-stake political\ndecision-making remains largely unexplored. This paper addresses the gap by\nfocusing on the application of LLMs to the United Nations (UN) decision-making\nprocess, where the stakes are particularly high and political decisions can\nhave far-reaching consequences. We introduce a novel dataset comprising\npublicly available UN Security Council (UNSC) records from 1994 to 2024,\nincluding draft resolutions, voting records, and diplomatic speeches. Using\nthis dataset, we propose the United Nations Benchmark (UNBench), the first\ncomprehensive benchmark designed to evaluate LLMs across four interconnected\npolitical science tasks: co-penholder judgment, representative voting\nsimulation, draft adoption prediction, and representative statement generation.\nThese tasks span the three stages of the UN decision-making process--drafting,\nvoting, and discussing--and aim to assess LLMs' ability to understand and\nsimulate political dynamics. Our experimental analysis demonstrates the\npotential and challenges of applying LLMs in this domain, providing insights\ninto their strengths and limitations in political science. This work\ncontributes to the growing intersection of AI and political science, opening\nnew avenues for research and practical applications in global governance. The\nUNBench Repository can be accessed at:\nhttps://github.com/yueqingliang1/UNBench.", "published": "2025-02-19 21:51:01", "link": "http://arxiv.org/abs/2502.14122v1", "categories": ["cs.CL", "cs.CY", "cs.ET"], "primary_category": "cs.CL"}
{"title": "Giving AI Personalities Leads to More Human-Like Reasoning", "abstract": "In computational cognitive modeling, capturing the full spectrum of human\njudgment and decision-making processes, beyond just optimal behaviors, is a\nsignificant challenge. This study explores whether Large Language Models (LLMs)\ncan emulate the breadth of human reasoning by predicting both intuitive, fast\nSystem 1 and deliberate, slow System 2 processes. We investigate the potential\nof AI to mimic diverse reasoning behaviors across a human population,\naddressing what we call the \"full reasoning spectrum problem\". We designed\nreasoning tasks using a novel generalization of the Natural Language Inference\n(NLI) format to evaluate LLMs' ability to replicate human reasoning. The\nquestions were crafted to elicit both System 1 and System 2 responses. Human\nresponses were collected through crowd-sourcing and the entire distribution was\nmodeled, rather than just the majority of the answers. We used\npersonality-based prompting inspired by the Big Five personality model to\nelicit AI responses reflecting specific personality traits, capturing the\ndiversity of human reasoning, and exploring how personality traits influence\nLLM outputs. Combined with genetic algorithms to optimize the weighting of\nthese prompts, this method was tested alongside traditional machine learning\nmodels. The results show that LLMs can mimic human response distributions, with\nopen-source models like Llama and Mistral outperforming proprietary GPT models.\nPersonality-based prompting, especially when optimized with genetic algorithms,\nsignificantly enhanced LLMs' ability to predict human response distributions,\nsuggesting that capturing suboptimal, naturalistic reasoning may require\nmodeling techniques incorporating diverse reasoning styles and psychological\nprofiles. The study concludes that personality-based prompting combined with\ngenetic algorithms is promising for enhancing AI's 'human-ness' in reasoning.", "published": "2025-02-19 23:51:23", "link": "http://arxiv.org/abs/2502.14155v2", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "KOALA: Knowledge Conflict Augmentations for Robustness in Vision\n  Language Models", "abstract": "The robustness of large language models (LLMs) against knowledge conflicts in\nunimodal question answering systems has been well studied. However, the effect\nof conflicts in information sources on vision language models (VLMs) in\nmultimodal settings has not yet been explored. In this work, we propose\n\\segsub, a framework that applies targeted perturbations to image sources to\nstudy and improve the robustness of VLMs against three different types of\nknowledge conflicts, namely parametric, source, and counterfactual conflicts.\nContrary to prior findings that showed that LLMs are sensitive to parametric\nconflicts arising from textual perturbations, we find VLMs are largely robust\nto image perturbation. On the other hand, VLMs perform poorly on counterfactual\nexamples (<30% accuracy) and fail to reason over source conflicts (<1%\naccuracy). We also find a link between hallucinations and image context, with\nGPT-4o prone to hallucination when presented with highly contextualized\ncounterfactual examples. While challenges persist with source conflicts,\nfinetuning models significantly improves reasoning over counterfactual samples.\nOur findings highlight the need for VLM training methodologies that enhance\ntheir reasoning capabilities, particularly in addressing complex knowledge\nconflicts between multimodal sources.", "published": "2025-02-19 00:26:38", "link": "http://arxiv.org/abs/2502.14908v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Universal Semantic Embeddings of Chemical Elements for Enhanced\n  Materials Inference and Discovery", "abstract": "We present a framework for generating universal semantic embeddings of\nchemical elements to advance materials inference and discovery. This framework\nleverages ElementBERT, a domain-specific BERT-based natural language processing\nmodel trained on 1.29 million abstracts of alloy-related scientific papers, to\ncapture latent knowledge and contextual relationships specific to alloys. These\nsemantic embeddings serve as robust elemental descriptors, consistently\noutperforming traditional empirical descriptors with significant improvements\nacross multiple downstream tasks. These include predicting mechanical and\ntransformation properties, classifying phase structures, and optimizing\nmaterials properties via Bayesian optimization. Applications to titanium\nalloys, high-entropy alloys, and shape memory alloys demonstrate up to 23%\ngains in prediction accuracy. Our results show that ElementBERT surpasses\ngeneral-purpose BERT variants by encoding specialized alloy knowledge. By\nbridging contextual insights from scientific literature with quantitative\ninference, our framework accelerates the discovery and optimization of advanced\nmaterials, with potential applications extending beyond alloys to other\nmaterial classes.", "published": "2025-02-19 07:26:03", "link": "http://arxiv.org/abs/2502.14912v1", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and\n  Consistency Alignment", "abstract": "Although multi-agent collaborative Large Language Models (LLMs) have achieved\nsignificant breakthroughs in the Text-to-SQL task, their performance is still\nconstrained by various factors. These factors include the incompleteness of the\nframework, failure to follow instructions, and model hallucination problems. To\naddress these problems, we propose OpenSearch-SQL, which divides the\nText-to-SQL task into four main modules: Preprocessing, Extraction, Generation,\nand Refinement, along with an Alignment module based on a consistency alignment\nmechanism. This architecture aligns the inputs and outputs of agents through\nthe Alignment module, reducing failures in instruction following and\nhallucination. Additionally, we designed an intermediate language called\nSQL-Like and optimized the structured CoT based on SQL-Like. Meanwhile, we\ndeveloped a dynamic few-shot strategy in the form of self-taught Query-CoT-SQL.\nThese methods have significantly improved the performance of LLMs in the\nText-to-SQL task.\n  In terms of model selection, we directly applied the base LLMs without any\npost-training, thereby simplifying the task chain and enhancing the framework's\nportability. Experimental results show that OpenSearch-SQL achieves an\nexecution accuracy(EX) of 69.3% on the BIRD development set, 72.28% on the test\nset, and a reward-based validity efficiency score (R-VES) of 69.36%, with all\nthree metrics ranking first at the time of submission. These results\ndemonstrate the comprehensive advantages of the proposed method in both\neffectiveness and efficiency.", "published": "2025-02-19 07:51:50", "link": "http://arxiv.org/abs/2502.14913v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "What Is a Good Caption? A Comprehensive Visual Caption Benchmark for\n  Evaluating Both Correctness and Coverage of MLLMs", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have rendered\ntraditional visual captioning benchmarks obsolete, as they primarily evaluate\nshort descriptions with outdated metrics. While recent benchmarks address these\nlimitations by decomposing captions into visual elements and adopting\nmodel-based evaluation, they remain incomplete-overlooking critical aspects,\nwhile providing vague, non-explanatory scores. To bridge this gap, we propose\nCV-CapBench, a Comprehensive Visual Caption Benchmark that systematically\nevaluates caption quality across 6 views and 13 dimensions. CV-CapBench\nintroduces precision, recall, and hit rate metrics for each dimension, uniquely\nassessing both correctness and coverage. Experiments on leading MLLMs reveal\nsignificant capability gaps, particularly in dynamic and knowledge-intensive\ndimensions. These findings provide actionable insights for future research. The\ncode and data will be released.", "published": "2025-02-19 07:55:51", "link": "http://arxiv.org/abs/2502.14914v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic\n  Text", "abstract": "How much information about training samples can be gleaned from synthetic\ndata generated by Large Language Models (LLMs)? Overlooking the subtleties of\ninformation flow in synthetic data generation pipelines can lead to a false\nsense of privacy. In this paper, we design membership inference attacks (MIAs)\nthat target data used to fine-tune pre-trained LLMs that are then used to\nsynthesize data, particularly when the adversary does not have access to the\nfine-tuned model but only to the synthetic data. We show that such data-based\nMIAs do significantly better than a random guess, meaning that synthetic data\nleaks information about the training data. Further, we find that canaries\ncrafted to maximize vulnerability to model-based MIAs are sub-optimal for\nprivacy auditing when only synthetic data is released. Such out-of-distribution\ncanaries have limited influence on the model's output when prompted to generate\nuseful, in-distribution synthetic data, which drastically reduces their\nvulnerability. To tackle this problem, we leverage the mechanics of\nauto-regressive models to design canaries with an in-distribution prefix and a\nhigh-perplexity suffix that leave detectable traces in synthetic data. This\nenhances the power of data-based MIAs and provides a better assessment of the\nprivacy risks of releasing synthetic data generated by LLMs.", "published": "2025-02-19 15:30:30", "link": "http://arxiv.org/abs/2502.14921v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An explainable transformer circuit for compositional generalization", "abstract": "Compositional generalization-the systematic combination of known components\ninto novel structures-remains a core challenge in cognitive science and machine\nlearning. Although transformer-based large language models can exhibit strong\nperformance on certain compositional tasks, the underlying mechanisms driving\nthese abilities remain opaque, calling into question their interpretability. In\nthis work, we identify and mechanistically interpret the circuit responsible\nfor compositional induction in a compact transformer. Using causal ablations,\nwe validate the circuit and formalize its operation using a program-like\ndescription. We further demonstrate that this mechanistic understanding enables\nprecise activation edits to steer the model's behavior predictably. Our\nfindings advance the understanding of complex behaviors in transformers and\nhighlight such insights can provide a direct pathway for model control.", "published": "2025-02-19 02:30:41", "link": "http://arxiv.org/abs/2502.15801v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of\n  Iterative Chaos", "abstract": "Large Reasoning Models (LRMs) have significantly advanced beyond traditional\nLarge Language Models (LLMs) with their exceptional logical reasoning\ncapabilities, yet these improvements introduce heightened safety risks. When\nsubjected to jailbreak attacks, their ability to generate more targeted and\norganized content can lead to greater harm. Although some studies claim that\nreasoning enables safer LRMs against existing LLM attacks, they overlook the\ninherent flaws within the reasoning process itself. To address this gap, we\npropose the first jailbreak attack targeting LRMs, exploiting their unique\nvulnerabilities stemming from the advanced reasoning capabilities.\nSpecifically, we introduce a Chaos Machine, a novel component to transform\nattack prompts with diverse one-to-one mappings. The chaos mappings iteratively\ngenerated by the machine are embedded into the reasoning chain, which\nstrengthens the variability and complexity and also promotes a more robust\nattack. Based on this, we construct the Mousetrap framework, which makes\nattacks projected into nonlinear-like low sample spaces with mismatched\ngeneralization enhanced. Also, due to the more competing objectives, LRMs\ngradually maintain the inertia of unpredictable iterative reasoning and fall\ninto our trap. Success rates of the Mousetrap attacking o1-mini, claude-sonnet\nand gemini-thinking are as high as 96%, 86% and 98% respectively on our toxic\ndataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench,\nattacking claude-sonnet, well-known for its safety, Mousetrap can astonishingly\nachieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This\npaper contains inappropriate, offensive and harmful content.", "published": "2025-02-19 07:23:36", "link": "http://arxiv.org/abs/2502.15806v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "ChatWise: AI-Powered Engaging Conversations for Enhancing Senior\n  Cognitive Wellbeing", "abstract": "Cognitive health in older adults presents a growing challenge. While\nconversational interventions show feasibility in improving cognitive wellness,\nhuman caregiver resources remain overburdened. AI-based methods have shown\npromise in providing conversational support, yet existing work is limited to\nimplicit strategy while lacking multi-turn support tailored to seniors. We\nimprove prior art with an LLM-driven chatbot named ChatWise for older adults.\nIt follows dual-level conversation reasoning at the inference phase to provide\nengaging companionship. ChatWise thrives in long-turn conversations, in\ncontrast to conventional LLMs that primarily excel in short-turn exchanges.\nGrounded experiments show that ChatWise significantly enhances simulated users'\ncognitive and emotional status, including those with Mild Cognitive Impairment.", "published": "2025-02-19 21:32:09", "link": "http://arxiv.org/abs/2503.05740v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "$\\mathtt{GeLLM^3O}$: Generalizing Large Language Models for\n  Multi-property Molecule Optimization", "abstract": "Despite recent advancements, most computational methods for molecule\noptimization are constrained to single- or double-property optimization tasks\nand suffer from poor scalability and generalizability to novel optimization\ntasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable\nout-of-domain generalizability to novel tasks. To demonstrate LLMs' potential\nfor molecule optimization, we introduce $\\mathtt{MoMUInstruct}$, the first\nhigh-quality instruction-tuning dataset specifically focused on complex\nmulti-property molecule optimization tasks. Leveraging $\\mathtt{MoMUInstruct}$,\nwe develop $\\mathtt{GeLLM^3O}$s, a series of instruction-tuned LLMs for\nmolecule optimization. Extensive evaluations across 5 in-domain and 5\nout-of-domain tasks demonstrate that $\\mathtt{GeLLM^3O}$s consistently\noutperform state-of-the-art baselines. $\\mathtt{GeLLM^3O}$s also exhibit\noutstanding zero-shot generalization to unseen tasks, significantly\noutperforming powerful closed-source LLMs. Such strong generalizability\ndemonstrates the tremendous potential of $\\mathtt{GeLLM^3O}$s as foundational\nmodels for molecule optimization, thereby tackling novel optimization tasks\nwithout resource-intensive retraining. $\\mathtt{MoMUInstruct}$, models, and\ncode are accessible through https://github.com/ninglab/GeLLMO.", "published": "2025-02-19 03:14:11", "link": "http://arxiv.org/abs/2502.13398v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.chem-ph", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "LaVCa: LLM-assisted Visual Cortex Captioning", "abstract": "Understanding the property of neural populations (or voxels) in the human\nbrain can advance our comprehension of human perceptual and cognitive\nprocessing capabilities and contribute to developing brain-inspired computer\nmodels. Recent encoding models using deep neural networks (DNNs) have\nsuccessfully predicted voxel-wise activity. However, interpreting the\nproperties that explain voxel responses remains challenging because of the\nblack-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex\nCaptioning (LaVCa), a data-driven approach that uses large language models\n(LLMs) to generate natural-language captions for images to which voxels are\nselective. By applying LaVCa for image-evoked brain activity, we demonstrate\nthat LaVCa generates captions that describe voxel selectivity more accurately\nthan the previously proposed method. Furthermore, the captions generated by\nLaVCa quantitatively capture more detailed properties than the existing method\nat both the inter-voxel and intra-voxel levels. Furthermore, a more detailed\nanalysis of the voxel-specific properties generated by LaVCa reveals\nfine-grained functional differentiation within regions of interest (ROIs) in\nthe visual cortex and voxels that simultaneously represent multiple distinct\nconcepts. These findings offer profound insights into human visual\nrepresentations by assigning detailed captions throughout the visual cortex\nwhile highlighting the potential of LLM-based methods in understanding brain\nrepresentations. Please check out our webpage at\nhttps://sites.google.com/view/lavca-llm/", "published": "2025-02-19 10:37:04", "link": "http://arxiv.org/abs/2502.13606v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "SPEX: Scaling Feature Interaction Explanations for LLMs", "abstract": "Large language models (LLMs) have revolutionized machine learning due to\ntheir ability to capture complex interactions between input features. Popular\npost-hoc explanation methods like SHAP provide marginal feature attributions,\nwhile their extensions to interaction importances only scale to small input\nlengths ($\\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic\ninteraction attribution algorithm that efficiently scales to large input\nlengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among\ninteractions -- common in real-world data -- and applies a sparse Fourier\ntransform using a channel decoding algorithm to efficiently identify important\ninteractions. We perform experiments across three difficult long-context\ndatasets that require LLMs to utilize interactions between inputs to complete\nthe task. For large inputs, SPEX outperforms marginal attribution methods by up\nto 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX\nsuccessfully identifies key features and interactions that strongly influence\nmodel output. For one of our datasets, HotpotQA, SPEX provides interactions\nthat align with human annotations. Finally, we use our model-agnostic approach\nto generate explanations to demonstrate abstract reasoning in closed-source\nLLMs (GPT-4o mini) and compositional reasoning in vision-language models.", "published": "2025-02-19 16:49:55", "link": "http://arxiv.org/abs/2502.13870v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Slamming: Training a Speech Language Model on One GPU in a Day", "abstract": "We introduce Slam, a recipe for training high-quality Speech Language Models\n(SLMs) on a single academic GPU in 24 hours. We do so through empirical\nanalysis of model initialisation and architecture, synthetic training data,\npreference optimisation with synthetic data and tweaking all other components.\nWe empirically demonstrate that this training recipe also scales well with more\ncompute getting results on par with leading SLMs in a fraction of the compute\ncost. We hope these insights will make SLM training and research more\naccessible. In the context of SLM scaling laws, our results far outperform\npredicted compute optimal performance, giving an optimistic view to SLM\nfeasibility. See code, data, models, samples at -\nhttps://pages.cs.huji.ac.il/adiyoss-lab/slamming .", "published": "2025-02-19 17:21:15", "link": "http://arxiv.org/abs/2502.15814v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "MATS: An Audio Language Model under Text-only Supervision", "abstract": "Large audio-language models (LALMs), built upon powerful Large Language\nModels (LLMs), have exhibited remarkable audio comprehension and reasoning\ncapabilities. However, the training of LALMs demands a large corpus of\naudio-language pairs, which requires substantial costs in both data collection\nand training resources. In this paper, we propose MATS, an audio-language\nmultimodal LLM designed to handle Multiple Audio task using solely Text-only\nSupervision. By leveraging pre-trained audio-language alignment models such as\nCLAP, we develop a text-only training strategy that projects the shared\naudio-language latent space into LLM latent space, endowing the LLM with audio\ncomprehension capabilities without relying on audio data during training. To\nfurther bridge the modality gap between audio and language embeddings within\nCLAP, we propose the Strongly-related noisy text with audio (Santa) mechanism.\nSanta maps audio embeddings into CLAP language embedding space while preserving\nessential information from the audio input. Extensive experiments demonstrate\nthat MATS, despite being trained exclusively on text data, achieves competitive\nperformance compared to recent LALMs trained on large-scale audio-language\npairs.", "published": "2025-02-19 05:07:56", "link": "http://arxiv.org/abs/2502.13433v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adopting Whisper for Confidence Estimation", "abstract": "Recent research on word-level confidence estimation for speech recognition\nsystems has primarily focused on lightweight models known as Confidence\nEstimation Modules (CEMs), which rely on hand-engineered features derived from\nAutomatic Speech Recognition (ASR) outputs. In contrast, we propose a novel\nend-to-end approach that leverages the ASR model itself (Whisper) to generate\nword-level confidence scores. Specifically, we introduce a method in which the\nWhisper model is fine-tuned to produce scalar confidence scores given an audio\ninput and its corresponding hypothesis transcript. Our experiments demonstrate\nthat the fine-tuned Whisper-tiny model, comparable in size to a strong CEM\nbaseline, achieves similar performance on the in-domain dataset and surpasses\nthe CEM baseline on eight out-of-domain datasets, whereas the fine-tuned\nWhisper-large model consistently outperforms the CEM baseline by a substantial\nmargin across all datasets.", "published": "2025-02-19 05:45:28", "link": "http://arxiv.org/abs/2502.13446v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Multi-channel Replay Speech Detection using an Adaptive Learnable\n  Beamformer", "abstract": "Replay attacks belong to the class of severe threats against voice-controlled\nsystems, exploiting the easy accessibility of speech signals by recorded and\nreplayed speech to grant unauthorized access to sensitive data. In this work,\nwe propose a multi-channel neural network architecture called M-ALRAD for the\ndetection of replay attacks based on spatial audio features. This approach\nintegrates a learnable adaptive beamformer with a convolutional recurrent\nneural network, allowing for joint optimization of spatial filtering and\nclassification. Experiments have been carried out on the ReMASC dataset, which\nis a state-of-the-art multi-channel replay speech detection dataset\nencompassing four microphones with diverse array configurations and four\nenvironments. Results on the ReMASC dataset show the superiority of the\napproach compared to the state-of-the-art and yield substantial improvements\nfor challenging acoustic environments. In addition, we demonstrate that our\napproach is able to better generalize to unseen environments with respect to\nprior studies.", "published": "2025-02-19 06:55:37", "link": "http://arxiv.org/abs/2502.13473v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Audio-Based Classification of Insect Species Using Machine Learning\n  Models: Cicada, Beetle, Termite, and Cricket", "abstract": "This project addresses the challenge of classifying insect species: Cicada,\nBeetle, Termite, and Cricket using sound recordings. Accurate species\nidentification is crucial for ecological monitoring and pest management. We\nemploy machine learning models such as XGBoost, Random Forest, and K Nearest\nNeighbors (KNN) to analyze audio features, including Mel Frequency Cepstral\nCoefficients (MFCC). The potential novelty of this work lies in the combination\nof diverse audio features and machine learning models to tackle insect\nclassification, specifically focusing on capturing subtle acoustic variations\nbetween species that have not been fully leveraged in previous research. The\ndataset is compiled from various open sources, and we anticipate achieving high\nclassification accuracy, contributing to improved automated insect detection\nsystems.", "published": "2025-02-19 17:26:18", "link": "http://arxiv.org/abs/2502.13893v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the application of Visibility Graphs in the Spectral Domain for\n  Speaker Recognition", "abstract": "In this study, we explore the potential of visibility graphs in the spectral\ndomain for speaker recognition. Adult participants were instructed to record\nvocalizations of the five Spanish vowels. For each vocalization, we computed\nthe frequency spectrum considering the source-filter model of speech\nproduction, where formants are shaped by the vocal tract acting as a passive\nfilter with resonant frequencies. Spectral profiles exhibited consistent\nintra-speaker characteristics, reflecting individual vocal tract anatomies,\nwhile showing variation between speakers. We then constructed visibility graphs\nfrom these spectral profiles and extracted various graph-theoretic metrics to\ncapture their topological features. These metrics were assembled into feature\nvectors representing the five vowels for each speaker. Using an ensemble of\ndecision trees trained on these features, we achieved high accuracy in speaker\nidentification. Our analysis identified key topological features that were\ncritical in distinguishing between speakers. This study demonstrates the\neffectiveness of visibility graphs for spectral analysis and their potential in\nspeaker recognition. We also discuss the robustness of this approach, offering\ninsights into its applicability for real-world speaker recognition systems.\nThis research contributes to expanding the feature extraction toolbox for\nspeaker recognition by leveraging the topological properties of speech signals\nin the spectral domain.", "published": "2025-02-19 21:26:07", "link": "http://arxiv.org/abs/2502.14110v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion\n  Models with Jointly Learned Prior", "abstract": "Denoising diffusion probabilistic models (DDPMs) can be utilized for\nrecovering a clean signal from its degraded observation(s) by conditioning the\nmodel on the degraded signal. The degraded signals are themselves contaminated\nversions of the clean signals; due to this correlation, they may encompass\ncertain useful information about the target clean data distribution. However,\nexisting adoption of the standard Gaussian as the prior distribution in turn\ndiscards such information, resulting in sub-optimal performance. In this paper,\nwe propose to improve conditional DDPMs for signal restoration by leveraging a\nmore informative prior that is jointly learned with the diffusion model. The\nproposed framework, called RestoreGrad, seamlessly integrates DDPMs into the\nvariational autoencoder framework and exploits the correlation between the\ndegraded and clean signals to encode a better diffusion prior. On speech and\nimage restoration tasks, we show that RestoreGrad demonstrates faster\nconvergence (5-10 times fewer training steps) to achieve better quality of\nrestored signals over existing DDPM baselines, and improved robustness to using\nfewer sampling steps in inference time (2-2.5 times fewer), advocating the\nadvantages of leveraging jointly learned prior for efficiency improvements in\nthe diffusion process.", "published": "2025-02-19 09:29:46", "link": "http://arxiv.org/abs/2502.13574v1", "categories": ["eess.IV", "cs.LG", "eess.AS"], "primary_category": "eess.IV"}
{"title": "TALKPLAY: Multimodal Music Recommendation with Large Language Models", "abstract": "We present TalkPlay, a multimodal music recommendation system that\nreformulates the recommendation task as large language model token generation.\nTalkPlay represents music through an expanded token vocabulary that encodes\nmultiple modalities - audio, lyrics, metadata, semantic tags, and playlist\nco-occurrence. Using these rich representations, the model learns to generate\nrecommendations through next-token prediction on music recommendation\nconversations, that requires learning the associations natural language query\nand response, as well as music items. In other words, the formulation\ntransforms music recommendation into a natural language understanding task,\nwhere the model's ability to predict conversation tokens directly optimizes\nquery-item relevance. Our approach eliminates traditional\nrecommendation-dialogue pipeline complexity, enabling end-to-end learning of\nquery-aware music recommendations. In the experiment, TalkPlay is successfully\ntrained and outperforms baseline methods in various aspects, demonstrating\nstrong context understanding as a conversational music recommender.", "published": "2025-02-19 13:28:20", "link": "http://arxiv.org/abs/2502.13713v3", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise", "abstract": "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.", "published": "2025-02-19 03:09:49", "link": "http://arxiv.org/abs/2502.13395v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP", "physics.optics"], "primary_category": "cs.SD"}
{"title": "Semi-supervised classification of bird vocalizations", "abstract": "Changes in bird populations can indicate broader changes in ecosystems,\nmaking birds one of the most important animal groups to monitor. Combining\nmachine learning and passive acoustics enables continuous monitoring over\nextended periods without direct human involvement. However, most existing\ntechniques require extensive expert-labeled datasets for training and cannot\neasily detect time-overlapping calls in busy soundscapes. We propose a\nsemi-supervised acoustic bird detector designed to allow both the detection of\ntime-overlapping calls (when separated in frequency) and the use of few labeled\ntraining samples. The classifier is trained and evaluated on a combination of\ncommunity-recorded open-source data and long-duration soundscape recordings\nfrom Singapore. It achieves a mean F0.5 score of 0.701 across 315 classes from\n110 bird species on a hold-out test set, with an average of 11 labeled training\nsamples per class. It outperforms the state-of-the-art BirdNET classifier on a\ntest set of 103 bird species despite significantly fewer labeled training\nsamples. The detector is further tested on 144 microphone-hours of continuous\nsoundscape data. The rich soundscape in Singapore makes suppression of false\npositives a challenge on raw, continuous data streams. Nevertheless, we\ndemonstrate that achieving high precision in such environments with minimal\nlabeled training data is possible.", "published": "2025-02-19 05:31:13", "link": "http://arxiv.org/abs/2502.13440v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
