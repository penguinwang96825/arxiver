{"title": "A Benchmark Study of Contrastive Learning for Arabic Social Meaning", "abstract": "Contrastive learning (CL) brought significant progress to various NLP tasks.\nDespite this progress, CL has not been applied to Arabic NLP to date. Nor is it\nclear how much benefits it could bring to particular classes of tasks such as\nthose involved in Arabic social meaning (e.g., sentiment analysis, dialect\nidentification, hate speech detection). In this work, we present a\ncomprehensive benchmark study of state-of-the-art supervised CL methods on a\nwide array of Arabic social meaning tasks. Through extensive empirical\nanalyses, we show that CL methods outperform vanilla finetuning on most tasks\nwe consider. We also show that CL can be data efficient and quantify this\nefficiency. Overall, our work allows us to demonstrate the promise of CL\nmethods, including in low-resource settings.", "published": "2022-10-22 00:36:07", "link": "http://arxiv.org/abs/2210.12314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Comparison of Neural Networks as Cognitive Models of\n  Inflection", "abstract": "Neural networks have long been at the center of a debate around the cognitive\nmechanism by which humans process inflectional morphology. This debate has\ngravitated into NLP by way of the question: Are neural networks a feasible\naccount for human behavior in morphological inflection? We address that\nquestion by measuring the correlation between human judgments and neural\nnetwork probabilities for unknown word inflections. We test a larger range of\narchitectures than previously studied on two important tasks for the cognitive\nprocessing debate: English past tense, and German number inflection. We find\nevidence that the Transformer may be a better account of human behavior than\nLSTMs on these datasets, and that LSTM features known to increase inflection\naccuracy do not always result in more human-like behavior.", "published": "2022-10-22 00:59:40", "link": "http://arxiv.org/abs/2210.12321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "R$^2$F: A General Retrieval, Reading and Fusion Framework for\n  Document-level Natural Language Inference", "abstract": "Document-level natural language inference (DOCNLI) is a new challenging task\nin natural language processing, aiming at judging the entailment relationship\nbetween a pair of hypothesis and premise documents. Current datasets and\nbaselines largely follow sentence-level settings, but fail to address the\nissues raised by longer documents. In this paper, we establish a general\nsolution, named Retrieval, Reading and Fusion (R2F) framework, and a new\nsetting, by analyzing the main challenges of DOCNLI: interpretability,\nlong-range dependency, and cross-sentence inference. The basic idea of the\nframework is to simplify document-level task into a set of sentence-level\ntasks, and improve both performance and interpretability with the power of\nevidence. For each hypothesis sentence, the framework retrieves evidence\nsentences from the premise, and reads to estimate its credibility. Then the\nsentence-level results are fused to judge the relationship between the\ndocuments. For the setting, we contribute complementary evidence and entailment\nlabel annotation on hypothesis sentences, for interpretability study. Our\nexperimental results show that R2F framework can obtain state-of-the-art\nperformance and is robust for diverse evidence retrieval methods. Moreover, it\ncan give more interpretable prediction results. Our model and code are released\nat https://github.com/phoenixsecularbird/R2F.", "published": "2022-10-22 02:02:35", "link": "http://arxiv.org/abs/2210.12328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-domain Question Answering via Chain of Reasoning over Heterogeneous\n  Knowledge", "abstract": "We propose a novel open-domain question answering (ODQA) framework for\nanswering single/multi-hop questions across heterogeneous knowledge sources.\nThe key novelty of our method is the introduction of the intermediary modules\ninto the current retriever-reader pipeline. Unlike previous methods that solely\nrely on the retriever for gathering all evidence in isolation, our intermediary\nperforms a chain of reasoning over the retrieved set. Specifically, our method\nlinks the retrieved evidence with its related global context into graphs and\norganizes them into a candidate list of evidence chains. Built upon pretrained\nlanguage models, our system achieves competitive performance on two ODQA\ndatasets, OTT-QA and NQ, against tables and passages from Wikipedia. In\nparticular, our model substantially outperforms the previous state-of-the-art\non OTT-QA with an exact match score of 47.3 (45 % relative gain).", "published": "2022-10-22 03:21:32", "link": "http://arxiv.org/abs/2210.12338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "P$^3$LM: Probabilistically Permuted Prophet Language Modeling for\n  Generative Pre-Training", "abstract": "Conventional autoregressive left-to-right (L2R) sequence generation faces two\nissues during decoding: limited to unidirectional target sequence modeling, and\nconstrained on strong local dependencies. To address the aforementioned\nproblem, we propose P$^3$LM, a probabilistically permuted prophet language\nmodel, which strengthens the modeling of bidirectional information and long\ntoken dependencies for sequence generation. Specifically, P$^3$LM learns to\ngenerate tokens in permuted order upon an order-aware transformer decoder, as\nwell as to generate the corresponding future $N$ tokens with a multi-stream\nattention mechanism. Extensive experiments are conducted on the GLGE benchmark,\nwhich includes four datasets for summarization, two for question generation,\none for conversational question answering, and one for dialog response\ngeneration, where P$^3$LM achieves state-of-the-art results compared with\nstrong publicly available generative pre-training methods.", "published": "2022-10-22 03:50:59", "link": "http://arxiv.org/abs/2210.12339v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EnDex: Evaluation of Dialogue Engagingness at Scale", "abstract": "We propose EnDex, the first human-reaction based model to evaluate dialogue\nengagingness. EnDex is trained on 80k Reddit-based Engagement Dataset (RED)\ncurated using a novel distant-supervision framework. Engagingness is a key\nmeasure that captures high-level quality of AI dialogue systems and closely\nreflects actual user experience. However, data shortage, plus the abstract and\nextensive definition of engagingness makes it challenging to develop an\nautomatic metric. Our work departs from mainstream approaches that use\nsynthetic negative examples to train binary classifiers, and instead, proposes\na solution using distant-supervision from human-reaction feedback. To support\nthe soundness of our EnDex metric, we offer a theoretical foundation for\nengagement, an extensive ablation study, and empirical evidence of high\ncorrelation on five engagingness related datasets. We will release code,\noff-the-shelf EnDex model, and a large-scale dataset upon paper publication to\nfacilitate future research.", "published": "2022-10-22 06:09:43", "link": "http://arxiv.org/abs/2210.12362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FCGEC: Fine-Grained Corpus for Chinese Grammatical Error Correction", "abstract": "Grammatical Error Correction (GEC) has been broadly applied in automatic\ncorrection and proofreading system recently. However, it is still immature in\nChinese GEC due to limited high-quality data from native speakers in terms of\ncategory and scale. In this paper, we present FCGEC, a fine-grained corpus to\ndetect, identify and correct the grammatical errors. FCGEC is a human-annotated\ncorpus with multiple references, consisting of 41,340 sentences collected\nmainly from multi-choice questions in public school Chinese examinations.\nFurthermore, we propose a Switch-Tagger-Generator (STG) baseline model to\ncorrect the grammatical errors in low-resource settings. Compared to other GEC\nbenchmark models, experimental results illustrate that STG outperforms them on\nour FCGEC. However, there exists a significant gap between benchmark models and\nhumans that encourages future models to bridge it.", "published": "2022-10-22 06:29:05", "link": "http://arxiv.org/abs/2210.12364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer\n  Data Augmentation", "abstract": "While counterfactual data augmentation offers a promising step towards robust\ngeneralization in natural language processing, producing a set of\ncounterfactuals that offer valuable inductive bias for models remains a\nchallenge. Most existing approaches for producing counterfactuals, manual or\nautomated, rely on small perturbations via minimal edits, resulting in\nsimplistic changes. We introduce NeuroCounterfactuals, designed as loose\ncounterfactuals, allowing for larger edits which result in naturalistic\ngenerations containing linguistic diversity, while still bearing similarity to\nthe original document. Our novel generative approach bridges the benefits of\nconstrained decoding, with those of language model adaptation for sentiment\nsteering. Training data augmentation with our generations results in both\nin-domain and out-of-domain improvements for sentiment classification,\noutperforming even manually curated counterfactuals, under select settings. We\nfurther present detailed analyses to show the advantages of\nNeuroCounterfactuals over approaches involving simple, minimal edits.", "published": "2022-10-22 06:29:21", "link": "http://arxiv.org/abs/2210.12365v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Precisely the Point: Adversarial Augmentations for Faithful and\n  Informative Text Generation", "abstract": "Though model robustness has been extensively studied in language\nunderstanding, the robustness of Seq2Seq generation remains understudied. In\nthis paper, we conduct the first quantitative analysis on the robustness of\npre-trained Seq2Seq models. We find that even current SOTA pre-trained Seq2Seq\nmodel (BART) is still vulnerable, which leads to significant degeneration in\nfaithfulness and informativeness for text generation tasks. This motivated us\nto further propose a novel adversarial augmentation framework, namely AdvSeq,\nfor generally improving faithfulness and informativeness of Seq2Seq models via\nenhancing their robustness. AdvSeq automatically constructs two types of\nadversarial augmentations during training, including implicit adversarial\nsamples by perturbing word representations and explicit adversarial samples by\nword swapping, both of which effectively improve Seq2Seq robustness. Extensive\nexperiments on three popular text generation tasks demonstrate that AdvSeq\nsignificantly improves both the faithfulness and informativeness of Seq2Seq\ngeneration under both automatic and human evaluation settings.", "published": "2022-10-22 06:38:28", "link": "http://arxiv.org/abs/2210.12367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReasTAP: Injecting Table Reasoning Skills During Pre-training via\n  Synthetic Reasoning Examples", "abstract": "Reasoning over tabular data requires both table structure understanding and a\nbroad set of table reasoning skills. Current models with table-specific\narchitectures and pre-training methods perform well on understanding table\nstructures, but they still struggle with tasks that require various table\nreasoning skills. In this work, we develop ReasTAP to show that high-level\ntable reasoning skills can be injected into models during pre-training without\na complex table-specific architecture design. We define 7 table reasoning\nskills, such as numerical operation, temporal comparison, and conjunction. Each\nreasoning skill is associated with one example generator, which synthesizes\nquestions over semi-structured tables according to the sampled templates. We\nmodel the table pre-training task as a sequence generation task and pre-train\nReasTAP to generate precise answers to the synthetic examples. ReasTAP is\nevaluated on four benchmarks covering three downstream tasks including: 1)\nWikiSQL and WTQ for Table Question Answering; 2) TabFact for Table Fact\nVerification; and 3) LogicNLG for Faithful Table-to-Text Generation.\nExperimental results demonstrate that ReasTAP achieves new state-of-the-art\nperformance on all benchmarks and delivers a significant improvement on\nlow-resource setting. Our code is publicly available at\nhttps://github.com/Yale-LILY/ReasTAP.", "published": "2022-10-22 07:04:02", "link": "http://arxiv.org/abs/2210.12374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Correcting Diverse Factual Errors in Abstractive Summarization via\n  Post-Editing and Language Model Infilling", "abstract": "Abstractive summarization models often generate inconsistent summaries\ncontaining factual errors or hallucinated content. Recent works focus on\ncorrecting factual errors in generated summaries via post-editing. Such\ncorrection models are trained using adversarial non-factual summaries\nconstructed using heuristic rules for injecting errors. However, generating\nnon-factual summaries using heuristics often does not generalize well to actual\nmodel errors. In this work, we propose to generate hard, representative\nsynthetic examples of non-factual summaries through infilling language models.\nWith this data, we train a more robust fact-correction model to post-edit the\nsummaries to improve factual consistency. Through quantitative and qualitative\nexperiments on two popular summarization datasets -- CNN/DM and XSum -- we show\nthat our approach vastly outperforms prior methods in correcting erroneous\nsummaries. Our model -- FactEdit -- improves factuality scores by over ~11\npoints on CNN/DM and over ~31 points on XSum on average across multiple\nsummarization models, producing more factual summaries while maintaining\ncompetitive summarization quality.", "published": "2022-10-22 07:16:19", "link": "http://arxiv.org/abs/2210.12378v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stance Detection and Open Research Avenues", "abstract": "This tutorial aims to cover the state-of-the-art on stance detection and\naddress open research avenues for interested researchers and practitioners.\nStance detection is a recent research topic where the stance towards a given\ntarget or target set is determined based on the given content and there are\nsignificant application opportunities of stance detection in various domains.\nThe tutorial comprises two parts where the first part outlines the fundamental\nconcepts, problems, approaches, and resources of stance detection, while the\nsecond part covers open research avenues and application areas of stance\ndetection. The tutorial will be a useful guide for researchers and\npractitioners of stance detection, social media analysis, information\nretrieval, and natural language processing.", "published": "2022-10-22 08:18:09", "link": "http://arxiv.org/abs/2210.12383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity\n  Recognition", "abstract": "African languages are spoken by over a billion people, but are\nunderrepresented in NLP research and development. The challenges impeding\nprogress include the limited availability of annotated datasets, as well as a\nlack of understanding of the settings where current methods are effective. In\nthis paper, we make progress towards solutions for these challenges, focusing\non the task of named entity recognition (NER). We create the largest\nhuman-annotated NER dataset for 20 African languages, and we study the behavior\nof state-of-the-art cross-lingual transfer methods in an Africa-centric\nsetting, demonstrating that the choice of source language significantly affects\nperformance. We show that choosing the best transfer language improves\nzero-shot F1 scores by an average of 14 points across 20 languages compared to\nusing English. Our results highlight the need for benchmark datasets and models\nthat cover typologically-diverse African languages.", "published": "2022-10-22 08:53:14", "link": "http://arxiv.org/abs/2210.12391v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaASSIST: Robust Dialogue State Tracking with Meta Learning", "abstract": "Existing dialogue datasets contain lots of noise in their state annotations.\nSuch noise can hurt model training and ultimately lead to poor generalization\nperformance. A general framework named ASSIST has recently been proposed to\ntrain robust dialogue state tracking (DST) models. It introduces an auxiliary\nmodel to generate pseudo labels for the noisy training set. These pseudo labels\nare combined with vanilla labels by a common fixed weighting parameter to train\nthe primary DST model. Notwithstanding the improvements of ASSIST on DST,\ntuning the weighting parameter is challenging. Moreover, a single parameter\nshared by all slots and all instances may be suboptimal. To overcome these\nlimitations, we propose a meta learning-based framework MetaASSIST to\nadaptively learn the weighting parameter. Specifically, we propose three\nschemes with varying degrees of flexibility, ranging from slot-wise to both\nslot-wise and instance-wise, to convert the weighting parameter into learnable\nfunctions. These functions are trained in a meta-learning manner by taking the\nvalidation set as meta data. Experimental results demonstrate that all three\nschemes can achieve competitive performance. Most impressively, we achieve a\nstate-of-the-art joint goal accuracy of 80.10% on MultiWOZ 2.4.", "published": "2022-10-22 09:14:45", "link": "http://arxiv.org/abs/2210.12397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Varifocal Question Generation for Fact-checking", "abstract": "Fact-checking requires retrieving evidence related to a claim under\ninvestigation. The task can be formulated as question generation based on a\nclaim, followed by question answering. However, recent question generation\napproaches assume that the answer is known and typically contained in a passage\ngiven as input, whereas such passages are what is being sought when verifying a\nclaim. In this paper, we present {\\it Varifocal}, a method that generates\nquestions based on different focal points within a given claim, i.e.\\ different\nspans of the claim and its metadata, such as its source and date. Our method\noutperforms previous work on a fact-checking question generation dataset on a\nwide range of automatic evaluation metrics. These results are corroborated by\nour manual evaluation, which indicates that our method generates more relevant\nand informative questions. We further demonstrate the potential of focal points\nin generating sets of clarification questions for product descriptions.", "published": "2022-10-22 09:41:47", "link": "http://arxiv.org/abs/2210.12400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PcMSP: A Dataset for Scientific Action Graphs Extraction from\n  Polycrystalline Materials Synthesis Procedure Text", "abstract": "Scientific action graphs extraction from materials synthesis procedures is\nimportant for reproducible research, machine automation, and material\nprediction. But the lack of annotated data has hindered progress in this field.\nWe demonstrate an effort to annotate Polycrystalline Materials Synthesis\nProcedures (PcMSP) from 305 open access scientific articles for the\nconstruction of synthesis action graphs. This is a new dataset for material\nscience information extraction that simultaneously contains the synthesis\nsentences extracted from the experimental paragraphs, as well as the entity\nmentions and intra-sentence relations. A two-step human annotation and\ninter-annotator agreement study guarantee the high quality of the PcMSP corpus.\nWe introduce four natural language processing tasks: sentence classification,\nnamed entity recognition, relation classification, and joint extraction of\nentities and relations. Comprehensive experiments validate the effectiveness of\nseveral state-of-the-art models for these challenges while leaving large space\nfor improvement. We also perform the error analysis and point out some unique\nchallenges that require further investigation. We will release our annotation\nscheme, the corpus, and codes to the research community to alleviate the\nscarcity of labeled data in this domain.", "published": "2022-10-22 09:43:54", "link": "http://arxiv.org/abs/2210.12401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PATS: Sensitivity-aware Noisy Learning for Pretrained Language Models", "abstract": "A wide range of NLP tasks benefit from the fine-tuning of pretrained language\nmodels (PLMs). However, a number of redundant parameters which contribute less\nto the downstream task are observed in a directly fine-tuned model. We consider\nthe gap between pretraining and downstream tasks hinders the training of these\nredundant parameters, and results in a suboptimal performance of the overall\nmodel. In this paper, we present PATS (Perturbation According To Sensitivity),\na noisy training mechanism which considers each parameter's importance in the\ndownstream task to help fine-tune PLMs. The main idea of PATS is to add bigger\nnoise to parameters with lower sensitivity and vice versa, in order to activate\nmore parameters' contributions to downstream tasks without affecting the\nsensitive ones much. Extensive experiments conducted on different tasks of the\nGLUE benchmark show PATS can consistently empower the fine-tuning of different\nsizes of PLMs, and the parameters in the well-performing models always have\nmore concentrated distributions of sensitivities, which experimentally proves\nthe effectiveness of our method.", "published": "2022-10-22 10:05:14", "link": "http://arxiv.org/abs/2210.12403v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in\n  Transformer-Based Variational AutoEncoder for Diverse Text Generation", "abstract": "Variational Auto-Encoder (VAE) has been widely adopted in text generation.\nAmong many variants, recurrent VAE learns token-wise latent variables with each\nconditioned on the preceding ones, which captures sequential variability better\nin the era of RNN. However, it is unclear how to incorporate such recurrent\ndynamics into the recently dominant Transformer due to its parallelism. In this\nwork, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE\nimposes recurrence on segment-wise latent variables with arbitrarily separated\ntext segments and constructs the posterior distribution with residual\nparameterization. Besides, we design an acceleration method by approximating\nidempotent matrices, which allows parallelism while maintaining the conditional\ndependence of latent variables. We demonstrate that TRACE could enhance the\nentanglement of each segment and preceding latent variables and deduce a\nnon-zero lower bound of the KL term, providing a theoretical guarantee of\ngeneration diversity. Experiments on two unconditional and one conditional\ngeneration tasks show that TRACE achieves significantly improved diversity\nwhile maintaining satisfactory generation quality.", "published": "2022-10-22 10:25:35", "link": "http://arxiv.org/abs/2210.12409v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Shared Task on Gender Rewriting", "abstract": "In this paper, we present the results and findings of the Shared Task on\nGender Rewriting, which was organized as part of the Seventh Arabic Natural\nLanguage Processing Workshop. The task of gender rewriting refers to generating\nalternatives of a given sentence to match different target user gender contexts\n(e.g., female speaker with a male listener, a male speaker with a male\nlistener, etc.). This requires changing the grammatical gender (masculine or\nfeminine) of certain words referring to the users. In this task, we focus on\nArabic, a gender-marking morphologically rich language. A total of five teams\nfrom four countries participated in the shared task.", "published": "2022-10-22 10:27:53", "link": "http://arxiv.org/abs/2210.12410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in\n  Dialog Systems", "abstract": "Dialog systems are often designed or trained to output human-like responses.\nHowever, some responses may be impossible for a machine to truthfully say (e.g.\n\"that movie made me cry\"). Highly anthropomorphic responses might make users\nuncomfortable or implicitly deceive them into thinking they are interacting\nwith a human. We collect human ratings on the feasibility of approximately 900\ntwo-turn dialogs sampled from 9 diverse data sources. Ratings are for two\nhypothetical machine embodiments: a futuristic humanoid robot and a digital\nassistant. We find that for some data-sources commonly used to train dialog\nsystems, 20-30% of utterances are not viewed as possible for a machine. Rating\nis marginally affected by machine embodiment. We explore qualitative and\nquantitative reasons for these ratings. Finally, we build classifiers and\nexplore how modeling configuration might affect output permissibly, and discuss\nimplications for building less falsely anthropomorphic dialog systems.", "published": "2022-10-22 12:10:44", "link": "http://arxiv.org/abs/2210.12429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structure-Unified M-Tree Coding Solver for MathWord Problem", "abstract": "As one of the challenging NLP tasks, designing math word problem (MWP)\nsolvers has attracted increasing research attention for the past few years. In\nprevious work, models designed by taking into account the properties of the\nbinary tree structure of mathematical expressions at the output side have\nachieved better performance. However, the expressions corresponding to a MWP\nare often diverse (e.g., $n_1+n_2 \\times n_3-n_4$, $n_3\\times n_2-n_4+n_1$,\netc.), and so are the corresponding binary trees, which creates difficulties in\nmodel learning due to the non-deterministic output space. In this paper, we\npropose the Structure-Unified M-Tree Coding Solver (SUMC-Solver), which applies\na tree with any M branches (M-tree) to unify the output structures. To learn\nthe M-tree, we use a mapping to convert the M-tree into the M-tree codes, where\ncodes store the information of the paths from tree root to leaf nodes and the\ninformation of leaf nodes themselves, and then devise a Sequence-to-Code\n(seq2code) model to generate the codes. Experimental results on the widely used\nMAWPS and Math23K datasets have demonstrated that SUMC-Solver not only\noutperforms several state-of-the-art models under similar experimental settings\nbut also performs much better under low-resource conditions.", "published": "2022-10-22 12:20:36", "link": "http://arxiv.org/abs/2210.12432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Prompt Tuning for Relation Classification", "abstract": "Using prompts to explore the knowledge contained within pre-trained language\nmodels for downstream tasks has now become an active topic. Current prompt\ntuning methods mostly convert the downstream tasks to masked language modeling\nproblems by adding cloze-style phrases and mapping all labels to verbalizations\nwith fixed length, which has proven effective for tasks with simple label\nspaces. However, when applied to relation classification exhibiting complex\nlabel spaces, vanilla prompt tuning methods may struggle with label\nverbalizations with arbitrary lengths due to rigid prompt restrictions.\nInspired by the text infilling task for pre-training generative models that can\nflexibly predict missing spans, we propose a novel generative prompt tuning\nmethod to reformulate relation classification as an infilling problem, which\nfrees our approach from limitations of current prompt based approaches and thus\nfully exploits rich semantics of entity and relation types. In addition, we\ndesign entity-guided decoding and discriminative relation scoring to generate\nand align relations effectively and efficiently during inference. Extensive\nexperiments under fully supervised settings and low-resource settings\ndemonstrate the effectiveness of our approach.", "published": "2022-10-22 12:40:23", "link": "http://arxiv.org/abs/2210.12435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extractive Summarization of Legal Decisions using Multi-task Learning\n  and Maximal Marginal Relevance", "abstract": "Summarizing legal decisions requires the expertise of law practitioners,\nwhich is both time- and cost-intensive. This paper presents techniques for\nextractive summarization of legal decisions in a low-resource setting using\nlimited expert annotated data. We test a set of models that locate relevant\ncontent using a sequential model and tackle redundancy by leveraging maximal\nmarginal relevance to compose summaries. We also demonstrate an implicit\napproach to help train our proposed models generate more informative summaries.\nOur multi-task learning model variant leverages rhetorical role identification\nas an auxiliary task to further improve the summarizer. We perform extensive\nexperiments on datasets containing legal decisions from the US Board of\nVeterans' Appeals and conduct quantitative and expert-ranked evaluations of our\nmodels. Our results show that the proposed approaches can achieve ROUGE scores\nvis-\\`a-vis expert extracted summaries that match those achieved by\ninter-annotator comparison.", "published": "2022-10-22 12:51:52", "link": "http://arxiv.org/abs/2210.12437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-domain Generalization for AMR Parsing", "abstract": "Abstract Meaning Representation (AMR) parsing aims to predict an AMR graph\nfrom textual input. Recently, there has been notable growth in AMR parsing\nperformance. However, most existing work focuses on improving the performance\nin the specific domain, ignoring the potential domain dependence of AMR parsing\nsystems. To address this, we extensively evaluate five representative AMR\nparsers on five domains and analyze challenges to cross-domain AMR parsing. We\nobserve that challenges to cross-domain AMR parsing mainly arise from the\ndistribution shift of words and AMR concepts. Based on our observation, we\ninvestigate two approaches to reduce the domain distribution divergence of text\nand AMR features, respectively. Experimental results on two out-of-domain test\nsets show the superiority of our method.", "published": "2022-10-22 13:24:13", "link": "http://arxiv.org/abs/2210.12445v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "There Is No Standard Answer: Knowledge-Grounded Dialogue Generation with\n  Adversarial Activated Multi-Reference Learning", "abstract": "Knowledge-grounded conversation (KGC) shows excellent potential to deliver an\nengaging and informative response. However, existing approaches emphasize\nselecting one golden knowledge given a particular dialogue context, overlooking\nthe one-to-many phenomenon in dialogue. As a result, the existing paradigm\nlimits the diversity of knowledge selection and generation. To this end, we\nestablish a multi-reference KGC dataset and propose a series of metrics to\nsystematically assess the one-to-many efficacy of existing KGC models.\nFurthermore, to extend the hypothesis space of knowledge selection to enhance\nthe mapping relationship between multiple knowledge and multiple responses, we\ndevise a span-based variational model and optimize the model in a wake-sleep\nstyle with an ameliorated evidence lower bound objective to learn the\none-to-many generalization. Both automatic and human evaluations demonstrate\nthe efficacy of our approach.", "published": "2022-10-22 14:43:33", "link": "http://arxiv.org/abs/2210.12459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collaborative Reasoning on Multi-Modal Semantic Graphs for\n  Video-Grounded Dialogue Generation", "abstract": "We study video-grounded dialogue generation, where a response is generated\nbased on the dialogue context and the associated video. The primary challenges\nof this task lie in (1) the difficulty of integrating video data into\npre-trained language models (PLMs) which presents obstacles to exploiting the\npower of large-scale pre-training; and (2) the necessity of taking into account\nthe complementarity of various modalities throughout the reasoning process.\nAlthough having made remarkable progress in video-grounded dialogue generation,\nexisting methods still fall short when it comes to integrating with PLMs in a\nway that allows information from different modalities to complement each other.\nTo alleviate these issues, we first propose extracting pertinent information\nfrom videos and turning it into reasoning paths that are acceptable to PLMs.\nAdditionally, we propose a multi-agent reinforcement learning method to\ncollaboratively perform reasoning on different modalities (i.e., video and\ndialogue context). Empirical experiment results on two public datasets indicate\nthat the proposed model can significantly outperform state-of-the-art models by\nlarge margins on both automatic and human evaluations.", "published": "2022-10-22 14:45:29", "link": "http://arxiv.org/abs/2210.12460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Efficient Dialogue Pre-training with Transferable and\n  Interpretable Latent Structure", "abstract": "With the availability of massive general-domain dialogue data, pre-trained\ndialogue generation appears to be super appealing to transfer knowledge from\nthe general domain to downstream applications. In most existing work, such\ntransferable ability is mainly obtained by fitting a large model with hundreds\nof millions of parameters on massive data in an exhaustive way, leading to\ninefficient running and poor interpretability. This paper proposes a novel\ndialogue generation model with a latent structure that is easily transferable\nfrom the general domain to downstream tasks in a lightweight and transparent\nway. Experiments on two benchmarks validate the effectiveness of the proposed\nmodel. Thanks to the transferable latent structure, our model is able to yield\nbetter dialogue responses than four strong baselines in terms of both automatic\nand human evaluations, and our model with about 22% parameters particularly\ndelivers a 5x speedup in running time compared with the strongest baseline.\nMoreover, the proposed model is explainable by interpreting the discrete latent\nvariables.", "published": "2022-10-22 14:46:43", "link": "http://arxiv.org/abs/2210.12461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long\n  Earnings Call Transcripts", "abstract": "Despite tremendous progress in automatic summarization, state-of-the-art\nmethods are predominantly trained to excel in summarizing short newswire\narticles, or documents with strong layout biases such as scientific articles or\ngovernment reports. Efficient techniques to summarize financial documents,\nincluding facts and figures, have largely been unexplored, majorly due to the\nunavailability of suitable datasets. In this work, we present ECTSum, a new\ndataset with transcripts of earnings calls (ECTs), hosted by publicly traded\ncompanies, as documents, and short experts-written telegram-style bullet point\nsummaries derived from corresponding Reuters articles. ECTs are long\nunstructured documents without any prescribed length limit or format. We\nbenchmark our dataset with state-of-the-art summarizers across various metrics\nevaluating the content quality and factual consistency of the generated\nsummaries. Finally, we present a simple-yet-effective approach, ECT-BPS, to\ngenerate a set of bullet points that precisely capture the important facts\ndiscussed in the calls.", "published": "2022-10-22 15:02:41", "link": "http://arxiv.org/abs/2210.12467v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DiscoSense: Commonsense Reasoning with Discourse Connectives", "abstract": "We present DiscoSense, a benchmark for commonsense reasoning via\nunderstanding a wide variety of discourse connectives. We generate compelling\ndistractors in DiscoSense using Conditional Adversarial Filtering, an extension\nof Adversarial Filtering that employs conditional generation. We show that\nstate-of-the-art pre-trained language models struggle to perform well on\nDiscoSense, which makes this dataset ideal for evaluating next-generation\ncommonsense reasoning systems.", "published": "2022-10-22 15:33:38", "link": "http://arxiv.org/abs/2210.12478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SynGEC: Syntax-Enhanced Grammatical Error Correction with a Tailored\n  GEC-Oriented Parser", "abstract": "This work proposes a syntax-enhanced grammatical error correction (GEC)\napproach named SynGEC that effectively incorporates dependency syntactic\ninformation into the encoder part of GEC models. The key challenge for this\nidea is that off-the-shelf parsers are unreliable when processing ungrammatical\nsentences. To confront this challenge, we propose to build a tailored\nGEC-oriented parser (GOPar) using parallel GEC training data as a pivot. First,\nwe design an extended syntax representation scheme that allows us to represent\nboth grammatical errors and syntax in a unified tree structure. Then, we obtain\nparse trees of the source incorrect sentences by projecting trees of the target\ncorrect sentences. Finally, we train GOPar with such projected trees. For GEC,\nwe employ the graph convolution network to encode source-side syntactic\ninformation produced by GOPar, and fuse them with the outputs of the\nTransformer encoder. Experiments on mainstream English and Chinese GEC datasets\nshow that our proposed SynGEC approach consistently and substantially\noutperforms strong baselines and achieves competitive performance. Our code and\ndata are all publicly available at https://github.com/HillZhang1999/SynGEC.", "published": "2022-10-22 15:54:29", "link": "http://arxiv.org/abs/2210.12484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and\n  Cross-lingual NLU", "abstract": "Curriculum Learning (CL) is a technique of training models via ranking\nexamples in a typically increasing difficulty trend with the aim of\naccelerating convergence and improving generalisability. Current approaches for\nNatural Language Understanding (NLU) tasks use CL to improve in-distribution\ndata performance often via heuristic-oriented or task-agnostic difficulties. In\nthis work, instead, we employ CL for NLU by taking advantage of training\ndynamics as difficulty metrics, i.e., statistics that measure the behavior of\nthe model at hand on specific task-data instances during training and propose\nmodifications of existing CL schedulers based on these statistics. Differently\nfrom existing works, we focus on evaluating models on in-distribution (ID),\nout-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer\ndatasets. We show across several NLU tasks that CL with training dynamics can\nresult in better performance mostly on zero-shot cross-lingual transfer and OOD\nsettings with improvements up by 8.5% in certain cases. Overall, experiments\nindicate that training dynamics can lead to better performing models with\nsmoother training compared to other difficulty metrics while being 20% faster\non average. In addition, through analysis we shed light on the correlations of\ntask-specific versus task-agnostic metrics.", "published": "2022-10-22 17:10:04", "link": "http://arxiv.org/abs/2210.12499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric\n  Code Switching", "abstract": "Accurate alignment between languages is fundamental for improving\ncross-lingual pre-trained language models (XLMs). Motivated by the natural\nphenomenon of code-switching (CS) in multilingual speakers, CS has been used as\nan effective data augmentation method that offers language alignment at the\nword- or phrase-level, in contrast to sentence-level via parallel instances.\nExisting approaches either use dictionaries or parallel sentences with word\nalignment to generate CS data by randomly switching words in a sentence.\nHowever, such methods can be suboptimal as dictionaries disregard semantics,\nand syntax might become invalid after random word switching. In this work, we\npropose EntityCS, a method that focuses on Entity-level Code-Switching to\ncapture fine-grained cross-lingual semantics without corrupting syntax. We use\nWikidata and English Wikipedia to construct an entity-centric CS corpus by\nswitching entities to their counterparts in other languages. We further propose\nentity-oriented masking strategies during intermediate model training on the\nEntityCS corpus for improving entity prediction. Evaluation of the trained\nmodels on four entity-centric downstream tasks shows consistent improvements\nover the baseline with a notable increase of 10% in Fact Retrieval. We release\nthe corpus and models to assist research on code-switching and enriching XLMs\nwith external knowledge.", "published": "2022-10-22 20:05:40", "link": "http://arxiv.org/abs/2210.12540v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PHEE: A Dataset for Pharmacovigilance Event Extraction from Text", "abstract": "The primary goal of drug safety researchers and regulators is to promptly\nidentify adverse drug reactions. Doing so may in turn prevent or reduce the\nharm to patients and ultimately improve public health. Evaluating and\nmonitoring drug safety (i.e., pharmacovigilance) involves analyzing an ever\ngrowing collection of spontaneous reports from health professionals,\nphysicians, and pharmacists, and information voluntarily submitted by patients.\nIn this scenario, facilitating analysis of such reports via automation has the\npotential to rapidly identify safety signals. Unfortunately, public resources\nfor developing natural language models for this task are scant. We present\nPHEE, a novel dataset for pharmacovigilance comprising over 5000 annotated\nevents from medical case reports and biomedical literature, making it the\nlargest such public dataset to date. We describe the hierarchical event schema\ndesigned to provide coarse and fine-grained information about patients'\ndemographics, treatments and (side) effects. Along with the discussion of the\ndataset, we present a thorough experimental evaluation of current\nstate-of-the-art approaches for biomedical event extraction, point out their\nlimitations, and highlight open challenges to foster future research in this\narea.", "published": "2022-10-22 21:57:42", "link": "http://arxiv.org/abs/2210.12560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Limitations of Reference-Free Evaluations of Generated Text", "abstract": "There is significant interest in developing evaluation metrics which\naccurately estimate the quality of generated text without the aid of a\nhuman-written reference text, which can be time consuming and expensive to\ncollect or entirely unavailable in online applications. However, in this work,\nwe demonstrate that these reference-free metrics are inherently biased and\nlimited in their ability to evaluate generated text, and we argue that they\nshould not be used to measure progress on tasks like machine translation or\nsummarization. We show how reference-free metrics are equivalent to using one\ngeneration model to evaluate another, which has several limitations: (1) the\nmetrics can be optimized at test time to find the approximate best-possible\noutput, (2) they are inherently biased toward models which are more similar to\ntheir own, and (3) they can be biased against higher-quality outputs, including\nthose written by humans. Therefore, we recommend that reference-free metrics\nshould be used as diagnostic tools for analyzing and understanding model\nbehavior instead of measures of how well models perform a task, in which the\ngoal is to achieve as high of a score as possible.", "published": "2022-10-22 22:12:06", "link": "http://arxiv.org/abs/2210.12563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-Based Conditioned Variational Autoencoder for Dialogue\n  Generation", "abstract": "In human dialogue, a single query may elicit numerous appropriate responses.\nThe Transformer-based dialogue model produces frequently occurring sentences in\nthe corpus since it is a one-to-one mapping function. CVAE is a technique for\nreducing generic replies. In this paper, we create a new dialogue model\n(CVAE-T) based on the Transformer with CVAE structure. We use a pre-trained MLM\nmodel to rewrite some key n-grams in responses to obtain a series of negative\nexamples, and introduce a regularization term during training to explicitly\nguide the latent variable in learning the semantic differences between each\npair of positive and negative examples. Experiments suggest that the method we\ndesign is capable of producing more informative replies.", "published": "2022-10-22 01:57:16", "link": "http://arxiv.org/abs/2210.12326v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback", "abstract": "Recently, dataset-generation-based zero-shot learning has shown promising\nresults by training a task-specific model with a dataset synthesized from large\npre-trained language models (PLMs). The final task-specific model often\nachieves compatible or even better performance than PLMs under the zero-shot\nsetting, with orders of magnitude fewer parameters. However, synthetic datasets\nhave their drawbacks. They have long been suffering from low-quality issues\n(e.g., low informativeness and redundancy). This explains why the massive\nsynthetic data does not lead to better performance -- a scenario we would\nexpect in the human-labeled data. To improve the quality of dataset synthesis,\nwe propose a progressive zero-shot dataset generation framework, ProGen, which\nleverages the feedback from the task-specific model to guide the generation of\nnew training data via in-context examples. Extensive experiments on five text\nclassification datasets demonstrate the effectiveness of the proposed approach.\nWe also show ProGen achieves on-par or superior performance with only 1\\%\nsynthetic dataset size compared to baseline methods without in-context\nfeedback.", "published": "2022-10-22 02:07:10", "link": "http://arxiv.org/abs/2210.12329v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AI-based Arabic Language and Speech Tutor", "abstract": "In the past decade, we have observed a growing interest in using technologies\nsuch as artificial intelligence (AI), machine learning, and chatbots to provide\nassistance to language learners, especially in second language learning. By\nusing AI and natural language processing (NLP) and chatbots, we can create an\nintelligent self-learning environment that goes beyond multiple-choice\nquestions and/or fill in the blank exercises. In addition, NLP allows for\nlearning to be adaptive in that it offers more than an indication that an error\nhas occurred. It also provides a description of the error, uses linguistic\nanalysis to isolate the source of the error, and then suggests additional\ndrills to achieve optimal individualized learning outcomes. In this paper, we\npresent our approach for developing an Artificial Intelligence-based Arabic\nLanguage and Speech Tutor (AI-ALST) for teaching the Moroccan Arabic dialect.\nThe AI-ALST system is an intelligent tutor that provides analysis and\nassessment of students learning the Moroccan dialect at University of Arizona\n(UA). The AI-ALST provides a self-learned environment to practice each lesson\nfor pronunciation training. In this paper, we present our initial experimental\nevaluation of the AI-ALST that is based on MFCC (Mel frequency cepstrum\ncoefficient) feature extraction, bidirectional LSTM (Long Short-Term Memory),\nattention mechanism, and a cost-based strategy for dealing with class-imbalance\nlearning. We evaluated our tutor on the word pronunciation of lesson 1 of the\nMoroccan Arabic dialect class. The experimental results show that the AI-ALST\ncan effectively and successfully detect pronunciation errors and evaluate its\nperformance by using F_1-score, accuracy, precision, and recall.", "published": "2022-10-22 04:22:16", "link": "http://arxiv.org/abs/2210.12346v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Multiple Choice Question Answering", "abstract": "While large language models (LLMs) like GPT-3 have achieved impressive\nresults on multiple choice question answering (MCQA) tasks in the zero, one,\nand few-shot settings, they generally lag behind the MCQA state of the art\n(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.\nAn LLM is conditioned on a question (without the associated answer options) and\nits chosen option is the one assigned the highest probability after\nnormalization (for length, etc.). A more natural prompting approach is to\npresent the question and answer options to the LLM jointly and have it output\nthe symbol (e.g., \"A\") associated with its chosen answer option. This approach\nallows the model to explicitly compare answer options, reduces computational\ncosts, and mitigates the effects of tokenization scheme and answer option\nrepresentations on answer selection. For the natural approach to be effective,\nthe LLM it is used with must be able to associate answer options with the\nsymbols that represent them. The LLM needs what we term multiple choice symbol\nbinding (MCSB) ability. This ability varies greatly by model. We show that a\nmodel with high MCSB ability performs much better with the natural approach\nthan with the traditional approach across 20 diverse datasets and largely\ncloses the gap with the SOTA, suggesting that the MCQA ability of LLMs has been\npreviously underestimated.", "published": "2022-10-22 05:04:54", "link": "http://arxiv.org/abs/2210.12353v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual\n  Understanding With Multilingual Language Models", "abstract": "Pre-trained multilingual language models show significant performance gains\nfor zero-shot cross-lingual model transfer on a wide range of natural language\nunderstanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation,\npre-trained models are only fine-tuned on English data and tested on a variety\nof target languages. In this paper, we do cross-lingual evaluation on various\nNLU tasks (sentence classification, sequence labeling, question answering)\nusing prompt-tuning and compare it with fine-tuning. The results show that\nprompt tuning achieves much better cross-lingual transfer than fine-tuning\nacross datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we\ndemonstrate through the analysis that prompt tuning can have better\ncross-lingual transferability of representations on downstream tasks with\nbetter aligned decision boundaries.", "published": "2022-10-22 05:48:02", "link": "http://arxiv.org/abs/2210.12360v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ADDMU: Detection of Far-Boundary Adversarial Examples with Data and\n  Model Uncertainty Estimation", "abstract": "Adversarial Examples Detection (AED) is a crucial defense technique against\nadversarial attacks and has drawn increasing attention from the Natural\nLanguage Processing (NLP) community. Despite the surge of new AED methods, our\nstudies show that existing methods heavily rely on a shortcut to achieve good\nperformance. In other words, current search-based adversarial attacks in NLP\nstop once model predictions change, and thus most adversarial examples\ngenerated by those attacks are located near model decision boundaries. To\nsurpass this shortcut and fairly evaluate AED methods, we propose to test AED\nmethods with \\textbf{F}ar \\textbf{B}oundary (\\textbf{FB}) adversarial examples.\nExisting methods show worse than random guess performance under this scenario.\nTo overcome this limitation, we propose a new technique, \\textbf{ADDMU},\n\\textbf{a}dversary \\textbf{d}etection with \\textbf{d}ata and \\textbf{m}odel\n\\textbf{u}ncertainty, which combines two types of uncertainty estimation for\nboth regular and FB adversarial example detection. Our new method outperforms\nprevious methods by 3.6 and 6.0 \\emph{AUC} points under each scenario. Finally,\nour analysis shows that the two types of uncertainty provided by \\textbf{ADDMU}\ncan be leveraged to characterize adversarial examples and identify the ones\nthat contribute most to model's robustness in adversarial training.", "published": "2022-10-22 09:11:12", "link": "http://arxiv.org/abs/2210.12396v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hard Gate Knowledge Distillation -- Leverage Calibration for Robust and\n  Reliable Language Model", "abstract": "In knowledge distillation, a student model is trained with supervisions from\nboth knowledge from a teacher and observations drawn from a training data\ndistribution. Knowledge of a teacher is considered a subject that holds\ninter-class relations which send a meaningful supervision to a student; hence,\nmuch effort has been put to find such knowledge to be distilled. In this paper,\nwe explore a question that has been given little attention: \"when to distill\nsuch knowledge.\" The question is answered in our work with the concept of model\ncalibration; we view a teacher model not only as a source of knowledge but also\nas a gauge to detect miscalibration of a student. This simple and yet novel\nview leads to a hard gate knowledge distillation scheme that switches between\nlearning from a teacher model and training data. We verify the gating mechanism\nin the context of natural language generation at both the token-level and the\nsentence-level. Empirical comparisons with strong baselines show that hard gate\nknowledge distillation not only improves model generalization, but also\nsignificantly lowers model calibration error.", "published": "2022-10-22 11:57:10", "link": "http://arxiv.org/abs/2210.12427v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EtriCA: Event-Triggered Context-Aware Story Generation Augmented by\n  Cross Attention", "abstract": "One of the key challenges of automatic story generation is how to generate a\nlong narrative that can maintain fluency, relevance, and coherence. Despite\nrecent progress, current story generation systems still face the challenge of\nhow to effectively capture contextual and event features, which has a profound\nimpact on a model's generation performance. To address these challenges, we\npresent EtriCA, a novel neural generation model, which improves the relevance\nand coherence of the generated stories through residually mapping context\nfeatures to event sequences with a cross-attention mechanism. Such a feature\ncapturing mechanism allows our model to better exploit the logical relatedness\nbetween events when generating stories. Extensive experiments based on both\nautomatic and human evaluations show that our model significantly outperforms\nstate-of-the-art baselines, demonstrating the effectiveness of our model in\nleveraging context and event features.", "published": "2022-10-22 14:51:12", "link": "http://arxiv.org/abs/2210.12463v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring The Landscape of Distributional Robustness for Question\n  Answering Models", "abstract": "We conduct a large empirical evaluation to investigate the landscape of\ndistributional robustness in question answering. Our investigation spans over\n350 models and 16 question answering datasets, including a diverse set of\narchitectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter\ntuning, in-context learning, etc.). We find that, in many cases, model\nvariations do not affect robustness and in-distribution performance alone\ndetermines out-of-distribution performance. Moreover, our findings indicate\nthat i) zero-shot and in-context learning methods are more robust to\ndistribution shifts than fully fine-tuned models; ii) few-shot prompt\nfine-tuned models exhibit better robustness than few-shot fine-tuned span\nprediction models; iii) parameter-efficient and robustness enhancing training\nmethods provide no significant robustness improvements. In addition, we\npublicly release all evaluations to encourage researchers to further analyze\nrobustness trends for question answering models.", "published": "2022-10-22 18:17:31", "link": "http://arxiv.org/abs/2210.12517v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Why Do You Feel This Way? Summarizing Triggers of Emotions in Social\n  Media Posts", "abstract": "Crises such as the COVID-19 pandemic continuously threaten our world and\nemotionally affect billions of people worldwide in distinct ways. Understanding\nthe triggers leading to people's emotions is of crucial importance. Social\nmedia posts can be a good source of such analysis, yet these texts tend to be\ncharged with multiple emotions, with triggers scattering across multiple\nsentences. This paper takes a novel angle, namely, emotion detection and\ntrigger summarization, aiming to both detect perceived emotions in text, and\nsummarize events and their appraisals that trigger each emotion. To support\nthis goal, we introduce CovidET (Emotions and their Triggers during Covid-19),\na dataset of ~1,900 English Reddit posts related to COVID-19, which contains\nmanual annotations of perceived emotions and abstractive summaries of their\ntriggers described in the post. We develop strong baselines to jointly detect\nemotions and summarize emotion triggers. Our analyses show that CovidET\npresents new challenges in emotion-specific summarization, as well as\nmulti-emotion detection in long social media posts.", "published": "2022-10-22 19:10:26", "link": "http://arxiv.org/abs/2210.12531v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Understanding Domain Learning in Language Models Through Subpopulation\n  Analysis", "abstract": "We investigate how different domains are encoded in modern neural network\narchitectures. We analyze the relationship between natural language domains,\nmodel size, and the amount of training data used. The primary analysis tool we\ndevelop is based on subpopulation analysis with Singular Vector Canonical\nCorrelation Analysis (SVCCA), which we apply to Transformer-based language\nmodels (LMs). We compare the latent representations of such a language model at\nits different layers from a pair of models: a model trained on multiple domains\n(an experimental model) and a model trained on a single domain (a control\nmodel). Through our method, we find that increasing the model capacity impacts\nhow domain information is stored in upper and lower layers differently. In\naddition, we show that larger experimental models simultaneously embed\ndomain-specific information as if they were conjoined control models. These\nfindings are confirmed qualitatively, demonstrating the validity of our method.", "published": "2022-10-22 21:12:57", "link": "http://arxiv.org/abs/2210.12553v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Visual Tour Of Current Challenges In Multimodal Language Models", "abstract": "Transformer models trained on massive text corpora have become the de facto\nmodels for a wide range of natural language processing tasks. However, learning\neffective word representations for function words remains challenging.\nMultimodal learning, which visually grounds transformer models in imagery, can\novercome the challenges to some extent; however, there is still much work to be\ndone. In this study, we explore the extent to which visual grounding\nfacilitates the acquisition of function words using stable diffusion models\nthat employ multimodal models for text-to-image generation. Out of seven\ncategories of function words, along with numerous subcategories, we find that\nstable diffusion models effectively model only a small fraction of function\nwords -- a few pronoun subcategories and relatives. We hope that our findings\nwill stimulate the development of new datasets and approaches that enable\nmultimodal models to learn better representations of function words.", "published": "2022-10-22 22:53:55", "link": "http://arxiv.org/abs/2210.12565v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Meta-learning Pathologies from Radiology Reports using Variance Aware\n  Prototypical Networks", "abstract": "Large pretrained Transformer-based language models like BERT and GPT have\nchanged the landscape of Natural Language Processing (NLP). However, fine\ntuning such models still requires a large number of training examples for each\ntarget task, thus annotating multiple datasets and training these models on\nvarious downstream tasks becomes time consuming and expensive. In this work, we\npropose a simple extension of the Prototypical Networks for few-shot text\nclassification. Our main idea is to replace the class prototypes by Gaussians\nand introduce a regularization term that encourages the examples to be\nclustered near the appropriate class centroids. Experimental results show that\nour method outperforms various strong baselines on 13 public and 4 internal\ndatasets. Furthermore, we use the class distributions as a tool for detecting\npotential out-of-distribution (OOD) data points during deployment.", "published": "2022-10-22 05:22:29", "link": "http://arxiv.org/abs/2210.13979v2", "categories": ["cs.LG", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Learning a Grammar Inducer from Massive Uncurated Instructional Videos", "abstract": "Video-aided grammar induction aims to leverage video information for finding\nmore accurate syntactic grammars for accompanying text. While previous work\nfocuses on building systems for inducing grammars on text that are well-aligned\nwith video content, we investigate the scenario, in which text and video are\nonly in loose correspondence. Such data can be found in abundance online, and\nthe weak correspondence is similar to the indeterminacy problem studied in\nlanguage acquisition. Furthermore, we build a new model that can better learn\nvideo-span correlation without manually designed features adopted by previous\nwork. Experiments show that our model trained only on large-scale YouTube data\nwith no text-video alignment reports strong and robust performances across\nthree unseen datasets, despite domain shift and noisy label issues. Furthermore\nour model yields higher F1 scores than the previous state-of-the-art systems\ntrained on in-domain data.", "published": "2022-10-22 00:22:55", "link": "http://arxiv.org/abs/2210.12309v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Salience Allocation as Guidance for Abstractive Summarization", "abstract": "Abstractive summarization models typically learn to capture the salient\ninformation from scratch implicitly. Recent literature adds extractive\nsummaries as guidance for abstractive summarization models to provide hints of\nsalient content and achieves better performance. However, extractive summaries\nas guidance could be over strict, leading to information loss or noisy signals.\nFurthermore, it cannot easily adapt to documents with various abstractiveness.\nAs the number and allocation of salience content pieces vary, it is hard to\nfind a fixed threshold deciding which content should be included in the\nguidance. In this paper, we propose a novel summarization approach with a\nflexible and reliable salience guidance, namely SEASON (SaliencE Allocation as\nGuidance for Abstractive SummarizatiON). SEASON utilizes the allocation of\nsalience expectation to guide abstractive summarization and adapts well to\narticles in different abstractiveness. Automatic and human evaluations on two\nbenchmark datasets show that the proposed method is effective and reliable.\nEmpirical results on more than one million news articles demonstrate a natural\nfifteen-fifty salience split for news article sentences, providing a useful\ninsight for composing news articles.", "published": "2022-10-22 02:13:44", "link": "http://arxiv.org/abs/2210.12330v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Guided contrastive self-supervised pre-training for automatic speech\n  recognition", "abstract": "Contrastive Predictive Coding (CPC) is a representation learning method that\nmaximizes the mutual information between intermediate latent representations\nand the output of a given model. It can be used to effectively initialize the\nencoder of an Automatic Speech Recognition (ASR) model. We present a novel\nmodification of CPC called Guided Contrastive Predictive Coding (GCPC). Our\nproposed method maximizes the mutual information between representations from a\nprior-knowledge model and the output of the model being pre-trained, allowing\nprior knowledge injection during pre-training. We validate our method on 3 ASR\ntasks: German, French and English. Our method outperforms CPC pre-training on\nall three datasets, reducing the Word Error Rate (WER) by 4.44%, 6.55% and\n15.43% relative on the German, French and English (Librispeech) tasks\nrespectively, compared to training from scratch, while CPC pre-training only\nbrings 2.96%, 1.01% and 14.39% relative WER reduction respectively.", "published": "2022-10-22 02:38:43", "link": "http://arxiv.org/abs/2210.12335v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Information-Transport-based Policy for Simultaneous Translation", "abstract": "Simultaneous translation (ST) outputs translation while receiving the source\ninputs, and hence requires a policy to determine whether to translate a target\ntoken or wait for the next source token. The major challenge of ST is that each\ntarget token can only be translated based on the current received source\ntokens, where the received source information will directly affect the\ntranslation quality. So naturally, how much source information is received for\nthe translation of the current target token is supposed to be the pivotal\nevidence for the ST policy to decide between translating and waiting. In this\npaper, we treat the translation as information transport from source to target\nand accordingly propose an Information-Transport-based Simultaneous Translation\n(ITST). ITST quantifies the transported information weight from each source\ntoken to the current target token, and then decides whether to translate the\ntarget token according to its accumulated received information. Experiments on\nboth text-to-text ST and speech-to-text ST (a.k.a., streaming speech\ntranslation) tasks show that ITST outperforms strong baselines and achieves\nstate-of-the-art performance.", "published": "2022-10-22 05:26:45", "link": "http://arxiv.org/abs/2210.12357v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Weakly-Supervised Temporal Article Grounding", "abstract": "Given a long untrimmed video and natural language queries, video grounding\n(VG) aims to temporally localize the semantically-aligned video segments.\nAlmost all existing VG work holds two simple but unrealistic assumptions: 1)\nAll query sentences can be grounded in the corresponding video. 2) All query\nsentences for the same video are always at the same semantic scale.\nUnfortunately, both assumptions make today's VG models fail to work in\npractice. For example, in real-world multimodal assets (eg, news articles),\nmost of the sentences in the article can not be grounded in their affiliated\nvideos, and they typically have rich hierarchical relations (ie, at different\nsemantic scales). To this end, we propose a new challenging grounding task:\nWeakly-Supervised temporal Article Grounding (WSAG). Specifically, given an\narticle and a relevant video, WSAG aims to localize all ``groundable''\nsentences to the video, and these sentences are possibly at different semantic\nscales. Accordingly, we collect the first WSAG dataset to facilitate this task:\nYouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow\narticles and plentiful YouTube videos. In addition, we propose a simple but\neffective method DualMIL for WSAG, which consists of a two-level MIL loss and a\nsingle-/cross- sentence constraint loss. These training objectives are\ncarefully designed for these relaxed assumptions. Extensive ablations have\nverified the effectiveness of DualMIL.", "published": "2022-10-22 13:23:02", "link": "http://arxiv.org/abs/2210.12444v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "DANLI: Deliberative Agent for Following Natural Language Instructions", "abstract": "Recent years have seen an increasing amount of work on embodied AI agents\nthat can perform tasks by following human language instructions. However, most\nof these agents are reactive, meaning that they simply learn and imitate\nbehaviors encountered in the training data. These reactive agents are\ninsufficient for long-horizon complex tasks. To address this limitation, we\npropose a neuro-symbolic deliberative agent that, while following language\ninstructions, proactively applies reasoning and planning based on its neural\nand symbolic representations acquired from past experience (e.g., natural\nlanguage and egocentric vision). We show that our deliberative agent achieves\ngreater than 70% improvement over reactive baselines on the challenging TEACh\nbenchmark. Moreover, the underlying reasoning and planning processes, together\nwith our modular framework, offer impressive transparency and explainability to\nthe behaviors of the agent. This enables an in-depth understanding of the\nagent's capabilities, which shed light on challenges and opportunities for\nfuture embodied agents for instruction following. The code is available at\nhttps://github.com/sled-group/DANLI.", "published": "2022-10-22 15:57:01", "link": "http://arxiv.org/abs/2210.12485v1", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure", "abstract": "In this paper, we propose a comprehensive benchmark to investigate models'\nlogical reasoning capabilities in complex real-life scenarios. Current\nexplanation datasets often employ synthetic data with simple reasoning\nstructures. Therefore, it cannot express more complex reasoning processes, such\nas the rebuttal to a reasoning step and the degree of certainty of the\nevidence. To this end, we propose a comprehensive logical reasoning explanation\nform. Based on the multi-hop chain of reasoning, the explanation form includes\nthree main components: (1) The condition of rebuttal that the reasoning node\ncan be challenged; (2) Logical formulae that uncover the internal texture of\nreasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The\nfine-grained structure conforms to the real logical reasoning scenario, better\nfitting the human cognitive process but, simultaneously, is more challenging\nfor the current models. We evaluate the current best models' performance on\nthis new explanation form. The experimental results show that generating\nreasoning graphs remains a challenging task for current models, even with the\nhelp of giant pre-trained language models.", "published": "2022-10-22 16:01:13", "link": "http://arxiv.org/abs/2210.12487v1", "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.AI"}
{"title": "DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in\n  Interactive Autonomous Driving Agents", "abstract": "In the real world, autonomous driving agents navigate in highly dynamic\nenvironments full of unexpected situations where pre-trained models are\nunreliable. In these situations, what is immediately available to vehicles is\noften only human operators. Empowering autonomous driving agents with the\nability to navigate in a continuous and dynamic environment and to communicate\nwith humans through sensorimotor-grounded dialogue becomes critical. To this\nend, we introduce Dialogue On the ROad To Handle Irregular Events (DOROTHIE), a\nnovel interactive simulation platform that enables the creation of unexpected\nsituations on the fly to support empirical studies on situated communication\nwith autonomous driving agents. Based on this platform, we created the Situated\nDialogue Navigation (SDN), a navigation benchmark of 183 trials with a total of\n8415 utterances, around 18.7 hours of control streams, and 2.9 hours of trimmed\naudio. SDN is developed to evaluate the agent's ability to predict dialogue\nmoves from humans as well as generate its own dialogue moves and physical\nnavigation actions. We further developed a transformer-based baseline model for\nthese SDN tasks. Our empirical results indicate that language guided-navigation\nin a highly dynamic environment is an extremely difficult task for end-to-end\nmodels. These results will provide insight towards future work on robust\nautonomous driving agents. The DOROTHIE platform, SDN benchmark, and code for\nthe baseline model are available at https://github.com/sled-group/DOROTHIE.", "published": "2022-10-22 17:52:46", "link": "http://arxiv.org/abs/2210.12511v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.AI"}
{"title": "LMPriors: Pre-Trained Language Models as Task-Specific Priors", "abstract": "Particularly in low-data regimes, an outstanding challenge in machine\nlearning is developing principled techniques for augmenting our models with\nsuitable priors. This is to encourage them to learn in ways that are compatible\nwith our understanding of the world. But in contrast to generic priors such as\nshrinkage or sparsity, we draw inspiration from the recent successes of\nlarge-scale language models (LMs) to construct task-specific priors distilled\nfrom the rich knowledge of LMs. Our method, Language Model Priors (LMPriors),\nincorporates auxiliary natural language metadata about the task -- such as\nvariable names and descriptions -- to encourage downstream model outputs to be\nconsistent with the LM's common-sense reasoning based on the metadata.\nEmpirically, we demonstrate that LMPriors improve model performance in settings\nwhere such natural language descriptions are available, and perform well on\nseveral tasks that benefit from such prior knowledge, such as feature\nselection, causal inference, and safe reinforcement learning.", "published": "2022-10-22 19:09:18", "link": "http://arxiv.org/abs/2210.12530v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adaptive Label Smoothing with Self-Knowledge in Natural Language\n  Generation", "abstract": "Overconfidence has been shown to impair generalization and calibration of a\nneural network. Previous studies remedy this issue by adding a regularization\nterm to a loss function, preventing a model from making a peaked distribution.\nLabel smoothing smoothes target labels with a pre-defined prior label\ndistribution; as a result, a model is learned to maximize the likelihood of\npredicting the soft label. Nonetheless, the amount of smoothing is the same in\nall samples and remains fixed in training. In other words, label smoothing does\nnot reflect the change in probability distribution mapped by a model over the\ncourse of training. To address this issue, we propose a regularization scheme\nthat brings dynamic nature into the smoothing parameter by taking model\nprobability distribution into account, thereby varying the parameter per\ninstance. A model in training self-regulates the extent of smoothing on the fly\nduring forward propagation. Furthermore, inspired by recent work in bridging\nlabel smoothing and knowledge distillation, our work utilizes self-knowledge as\na prior label distribution in softening target labels, and presents theoretical\nsupport for the regularization effect by knowledge distillation and the dynamic\nsmoothing parameter. Our regularizer is validated comprehensively, and the\nresult illustrates marked improvements in model generalization and calibration,\nenhancing robustness and trustworthiness of a model.", "published": "2022-10-22 11:52:38", "link": "http://arxiv.org/abs/2210.13459v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "chowdsp_wdf: An Advanced C++ Library for Wave Digital Circuit Modelling", "abstract": "chowdsp_wdf is a C++ library for implementing real-time wave digital models\nof analog circuits. chowdsp_wdf differs from existing wave digital modelling\nlibraries by providing a template meta-programming interface for modelling\ncircuits with a fixed topology, and providing support for explicit SIMD\nacceleration. The motivation and design of the library are described, as well\nas real-world use-cases, and performance comparisons with other wave digital\nmodelling libraries.", "published": "2022-10-22 21:20:46", "link": "http://arxiv.org/abs/2210.12554v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Neural Sound Field Decomposition with Super-resolution of Sound\n  Direction", "abstract": "Sound field decomposition predicts waveforms in arbitrary directions using\nsignals from a limited number of microphones as inputs. Sound field\ndecomposition is fundamental to downstream tasks, including source\nlocalization, source separation, and spatial audio reproduction. Conventional\nsound field decomposition methods such as Ambisonics have limited spatial\ndecomposition resolution. This paper proposes a learning-based Neural Sound\nfield Decomposition (NeSD) framework to allow sound field decomposition with\nfine spatial direction resolution, using recordings from microphone capsules of\na few microphones at arbitrary positions. The inputs of a NeSD system include\nmicrophone signals, microphone positions, and queried directions. The outputs\nof a NeSD include the waveform and the presence probability of a queried\nposition. We model the NeSD systems respectively with different neural\nnetworks, including fully connected, time delay, and recurrent neural networks.\nWe show that the NeSD systems outperform conventional Ambisonics and DOANet\nmethods in sound field decomposition and source localization on speech, music,\nand sound events datasets. Demos are available at\nhttps://www.youtube.com/watch?v=0GIr6doj3BQ.", "published": "2022-10-22 04:20:29", "link": "http://arxiv.org/abs/2210.12345v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep domain adaptation for polyphonic melody extraction", "abstract": "Extraction of the predominant pitch from polyphonic audio is one of the\nfundamental tasks in the field of music information retrieval and computational\nmusicology. To accomplish this task using machine learning, a large amount of\nlabeled audio data is required to train the model that predicts the pitch\ncontour. But a classical model pre-trained on data from one domain (source),\ne.g, songs of a particular singer or genre, may not perform comparatively well\nin extracting melody from other domains (target). The performance of such\nmodels can be boosted by adapting the model using some annotated data in the\ntarget domain. In this work, we study various adaptation techniques applied to\nmachine learning models for polyphonic melody extraction. Experimental results\nshow that meta-learning-based adaptation performs better than simple\nfine-tuning. In addition to this, we find that this method outperforms the\nexisting state-of-the-art non-adaptive polyphonic melody extraction algorithms.", "published": "2022-10-22 19:11:42", "link": "http://arxiv.org/abs/2210.12532v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GCT: Gated Contextual Transformer for Sequential Audio Tagging", "abstract": "Audio tagging aims to assign predefined tags to audio clips to indicate the\nclass information of audio events. Sequential audio tagging (SAT) means\ndetecting both the class information of audio events, and the order in which\nthey occur within the audio clip. Most existing methods for SAT are based on\nconnectionist temporal classification (CTC). However, CTC cannot effectively\ncapture connections between events due to the conditional independence\nassumption between outputs at different times. The contextual Transformer\n(cTransformer) addresses this issue by exploiting contextual information in\nSAT. Nevertheless, cTransformer is also limited in exploiting contextual\ninformation as it only uses forward information in inference. This paper\nproposes a gated contextual Transformer (GCT) with forward-backward inference\n(FBI). In addition, a gated contextual multi-layer perceptron (GCMLP) block is\nproposed in GCT to improve the performance of cTransformer structurally.\nExperiments on two real-life audio datasets show that the proposed GCT with\nGCMLP and FBI performs better than the CTC-based methods and cTransformer. To\npromote research on SAT, the manually annotated sequential labels for the two\ndatasets are released.", "published": "2022-10-22 20:07:57", "link": "http://arxiv.org/abs/2210.12541v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Emotion Recognition via an Attentive Time-Frequency Neural\n  Network", "abstract": "Spectrogram is commonly used as the input feature of deep neural networks to\nlearn the high(er)-level time-frequency pattern of speech signal for speech\nemotion recognition (SER). \\textcolor{black}{Generally, different emotions\ncorrespond to specific energy activations both within frequency bands and time\nframes on spectrogram, which indicates the frequency and time domains are both\nessential to represent the emotion for SER. However, recent spectrogram-based\nworks mainly focus on modeling the long-term dependency in time domain, leading\nto these methods encountering the following two issues: (1) neglecting to model\nthe emotion-related correlations within frequency domain during the\ntime-frequency joint learning; (2) ignoring to capture the specific frequency\nbands associated with emotions.} To cope with the issues, we propose an\nattentive time-frequency neural network (ATFNN) for SER, including a\ntime-frequency neural network (TFNN) and time-frequency attention.\nSpecifically, aiming at the first issue, we design a TFNN with a\nfrequency-domain encoder (F-Encoder) based on the Transformer encoder and a\ntime-domain encoder (T-Encoder) based on the Bidirectional Long Short-Term\nMemory (Bi-LSTM). The F-Encoder and T-Encoder model the correlations within\nfrequency bands and time frames, respectively, and they are embedded into a\ntime-frequency joint learning strategy to obtain the time-frequency patterns\nfor speech emotions. Moreover, to handle the second issue, we also adopt\ntime-frequency attention with a frequency-attention network (F-Attention) and a\ntime-attention network (T-Attention) to focus on the emotion-related frequency\nband ranges and time frame ranges, which can enhance the discriminability of\nspeech emotion features.", "published": "2022-10-22 12:18:26", "link": "http://arxiv.org/abs/2210.12430v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
