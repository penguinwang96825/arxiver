{"title": "Riposte! A Large Corpus of Counter-Arguments", "abstract": "Constructive feedback is an effective method for improving critical thinking\nskills. Counter-arguments (CAs), one form of constructive feedback, have been\nproven to be useful for critical thinking skills. However, little work has been\ndone for constructing a large-scale corpus of them which can drive research on\nautomatic generation of CAs for fallacious micro-level arguments (i.e. a single\nclaim and premise pair). In this work, we cast providing constructive feedback\nas a natural language processing task and create Riposte!, a corpus of CAs,\ntowards this goal. Produced by crowdworkers, Riposte! contains over 18k CAs. We\ninstruct workers to first identify common fallacy types and produce a CA which\nidentifies the fallacy. We analyze how workers create CAs and construct a\nbaseline model based on our analysis.", "published": "2019-10-08 07:18:58", "link": "http://arxiv.org/abs/1910.03246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Interactive Machine Translation Framework for Modernizing Historical\n  Documents", "abstract": "Due to the nature of human language, historical documents are hard to\ncomprehend by contemporary people. This limits their accessibility to scholars\nspecialized in the time period in which the documents were written.\nModernization aims at breaking this language barrier by generating a new\nversion of a historical document, written in the modern version of the\ndocument's original language. However, while it is able to increase the\ndocument's comprehension, modernization is still far from producing an\nerror-free version. In this work, we propose a collaborative framework in which\na scholar can work together with the machine to generate the new version. We\ntested our approach on a simulated environment, achieving significant\nreductions of the human effort needed to produce the modernized version of the\ndocument.", "published": "2019-10-08 12:15:52", "link": "http://arxiv.org/abs/1910.03355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In Search for Linear Relations in Sentence Embedding Spaces", "abstract": "We present an introductory investigation into continuous-space vector\nrepresentations of sentences. We acquire pairs of very similar sentences\ndiffering only by a small alterations (such as change of a noun, adding an\nadjective, noun or punctuation) from datasets for natural language inference\nusing a simple pattern method. We look into how such a small change within the\nsentence text affects its representation in the continuous space and how such\nalterations are reflected by some of the popular sentence embedding models. We\nfound that vector differences of some embeddings actually reflect small changes\nwithin a sentence.", "published": "2019-10-08 13:06:01", "link": "http://arxiv.org/abs/1910.03375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistically Informed Relation Extraction and Neural Architectures for\n  Nested Named Entity Recognition in BioNLP-OST 2019", "abstract": "Named Entity Recognition (NER) and Relation Extraction (RE) are essential\ntools in distilling knowledge from biomedical literature. This paper presents\nour findings from participating in BioNLP Shared Tasks 2019. We addressed Named\nEntity Recognition including nested entities extraction, Entity Normalization\nand Relation Extraction. Our proposed approach of Named Entities can be\ngeneralized to different languages and we have shown it's effectiveness for\nEnglish and Spanish text. We investigated linguistic features, hybrid loss\nincluding ranking and Conditional Random Fields (CRF), multi-task objective and\ntoken-level ensembling strategy to improve NER. We employed dictionary based\nfuzzy and semantic search to perform Entity Normalization. Finally, our RE\nsystem employed Support Vector Machine (SVM) with linguistic features.\n  Our NER submission (team:MIC-CIS) ranked first in BB-2019 norm+NER task with\nstandard error rate (SER) of 0.7159 and showed competitive performance on\nPharmaCo NER task with F1-score of 0.8662. Our RE system ranked first in the\nSeeDev-binary Relation Extraction Task with F1-score of 0.3738.", "published": "2019-10-08 13:33:48", "link": "http://arxiv.org/abs/1910.03385v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Highly Relevant Questions", "abstract": "The neural seq2seq based question generation (QG) is prone to generating\ngeneric and undiversified questions that are poorly relevant to the given\npassage and target answer. In this paper, we propose two methods to address the\nissue. (1) By a partial copy mechanism, we prioritize words that are\nmorphologically close to words in the input passage when generating questions;\n(2) By a QA-based reranker, from the n-best list of question candidates, we\nselect questions that are preferred by both the QA and QG model. Experiments\nand analyses demonstrate that the proposed two methods substantially improve\nthe relevance of generated questions to passages and answers.", "published": "2019-10-08 13:57:02", "link": "http://arxiv.org/abs/1910.03401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation from Internal Representations", "abstract": "Knowledge distillation is typically conducted by training a small model (the\nstudent) to mimic a large and cumbersome model (the teacher). The idea is to\ncompress the knowledge from the teacher by using its output probabilities as\nsoft-labels to optimize the student. However, when the teacher is considerably\nlarge, there is no guarantee that the internal knowledge of the teacher will be\ntransferred into the student; even if the student closely matches the\nsoft-labels, its internal representations may be considerably different. This\ninternal mismatch can undermine the generalization capabilities originally\nintended to be transferred from the teacher to the student. In this paper, we\npropose to distill the internal representations of a large model such as BERT\ninto a simplified version of it. We formulate two ways to distill such\nrepresentations and various algorithms to conduct the distillation. We\nexperiment with datasets from the GLUE benchmark and consistently show that\nadding knowledge distillation from internal representations is a more powerful\nmethod than only using soft-label distillation.", "published": "2019-10-08 23:56:27", "link": "http://arxiv.org/abs/1910.03723v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SesameBERT: Attention for Anywhere", "abstract": "Fine-tuning with pre-trained models has achieved exceptional results for many\nlanguage tasks. In this study, we focused on one such self-attention network\nmodel, namely BERT, which has performed well in terms of stacking layers across\ndiverse language-understanding benchmarks. However, in many downstream tasks,\ninformation between layers is ignored by BERT for fine-tuning. In addition,\nalthough self-attention networks are well-known for their ability to capture\nglobal dependencies, room for improvement remains in terms of emphasizing the\nimportance of local contexts. In light of these advantages and disadvantages,\nthis paper proposes SesameBERT, a generalized fine-tuning method that (1)\nenables the extraction of global information among all layers through Squeeze\nand Excitation and (2) enriches local information by capturing neighboring\ncontexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness\nof our approach in the HANS dataset, which is used to determine whether models\nhave adopted shallow heuristics instead of learning underlying generalizations.\nThe experiments revealed that SesameBERT outperformed BERT with respect to GLUE\nbenchmark and the HANS evaluation set.", "published": "2019-10-08 02:31:35", "link": "http://arxiv.org/abs/1910.03176v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Look before you Hop: Conversational Question Answering over Knowledge\n  Graphs Using Judicious Context Expansion", "abstract": "Fact-centric information needs are rarely one-shot; users typically ask\nfollow-up questions to explore a topic. In such a conversational setting, the\nuser's inputs are often incomplete, with entities or predicates left out, and\nungrammatical phrases. This poses a huge challenge to question answering (QA)\nsystems that typically rely on cues in full-fledged interrogative sentences. As\na solution, we develop CONVEX: an unsupervised method that can answer\nincomplete questions over a knowledge graph (KG) by maintaining conversation\ncontext using entities and predicates seen so far and automatically inferring\nmissing or ambiguous pieces for follow-up questions. The core of our method is\na graph exploration algorithm that judiciously expands a frontier to find\ncandidate answers for the current question. To evaluate CONVEX, we release\nConvQuestions, a crowdsourced benchmark with 11,200 distinct conversations from\nfive different domains. We show that CONVEX: (i) adds conversational support to\nany stand-alone QA system, and (ii) outperforms state-of-the-art baselines and\nquestion completion strategies.", "published": "2019-10-08 07:57:48", "link": "http://arxiv.org/abs/1910.03262v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CONAN -- COunter NArratives through Nichesourcing: a Multilingual\n  Dataset of Responses to Fight Online Hate Speech", "abstract": "Although there is an unprecedented effort to provide adequate responses in\nterms of laws and policies to hate content on social media platforms, dealing\nwith hatred online is still a tough problem. Tackling hate speech in the\nstandard way of content deletion or user suspension may be charged with\ncensorship and overblocking. One alternate strategy, that has received little\nattention so far by the research community, is to actually oppose hate content\nwith counter-narratives (i.e. informed textual responses). In this paper, we\ndescribe the creation of the first large-scale, multilingual, expert-based\ndataset of hate speech/counter-narrative pairs. This dataset has been built\nwith the effort of more than 100 operators from three different NGOs that\napplied their training and expertise to the task. Together with the collected\ndata we also provide additional annotations about expert demographics, hate and\nresponse type, and data augmentation through translation and paraphrasing.\nFinally, we provide initial experiments to assess the quality of our data.", "published": "2019-10-08 08:33:56", "link": "http://arxiv.org/abs/1910.03270v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "One-To-Many Multilingual End-to-end Speech Translation", "abstract": "Nowadays, training end-to-end neural models for spoken language translation\n(SLT) still has to confront with extreme data scarcity conditions. The existing\nSLT parallel corpora are indeed orders of magnitude smaller than those\navailable for the closely related tasks of automatic speech recognition (ASR)\nand machine translation (MT), which usually comprise tens of millions of\ninstances. To cope with data paucity, in this paper we explore the\neffectiveness of transfer learning in end-to-end SLT by presenting a\nmultilingual approach to the task. Multilingual solutions are widely studied in\nMT and usually rely on ``\\textit{target forcing}'', in which multilingual\nparallel data are combined to train a single model by prepending to the input\nsequences a language token that specifies the target language. However, when\ntested in speech translation, our experiments show that MT-like \\textit{target\nforcing}, used as is, is not effective in discriminating among the target\nlanguages. Thus, we propose a variant that uses target-language embeddings to\nshift the input representations in different portions of the space according to\nthe language, so to better support the production of output in the desired\ntarget language. Our experiments on end-to-end SLT from English into six\nlanguages show important improvements when translating into similar languages,\nespecially when these are supported by scarce data. Further improvements are\nobtained when using English ASR data as an additional language (up to $+2.5$\nBLEU points).", "published": "2019-10-08 10:29:09", "link": "http://arxiv.org/abs/1910.03320v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "When Specialization Helps: Using Pooled Contextualized Embeddings to\n  Detect Chemical and Biomedical Entities in Spanish", "abstract": "The recognition of pharmacological substances, compounds and proteins is an\nessential preliminary work for the recognition of relations between chemicals\nand other biomedically relevant units. In this paper, we describe an approach\nto Task 1 of the PharmaCoNER Challenge, which involves the recognition of\nmentions of chemicals and drugs in Spanish medical texts. We train a\nstate-of-the-art BiLSTM-CRF sequence tagger with stacked Pooled Contextualized\nEmbeddings, word and sub-word embeddings using the open-source framework FLAIR.\nWe present a new corpus composed of articles and papers from Spanish health\nscience journals, termed the Spanish Health Corpus, and use it to train\ndomain-specific embeddings which we incorporate in our model training. We\nachieve a result of 89.76% F1-score using pre-trained embeddings and are able\nto improve these results to 90.52% F1-score using specialized embeddings.", "published": "2019-10-08 13:38:39", "link": "http://arxiv.org/abs/1910.03387v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Federated Learning of N-gram Language Models", "abstract": "We propose algorithms to train production-quality n-gram language models\nusing federated learning. Federated learning is a distributed computation\nplatform that can be used to train global models for portable devices such as\nsmart phones. Federated learning is especially relevant for applications\nhandling privacy-sensitive data, such as virtual keyboards, because training is\nperformed without the users' data ever leaving their devices. While the\nprinciples of federated learning are fairly generic, its methodology assumes\nthat the underlying models are neural networks. However, virtual keyboards are\ntypically powered by n-gram language models for latency reasons.\n  We propose to train a recurrent neural network language model using the\ndecentralized FederatedAveraging algorithm and to approximate this federated\nmodel server-side with an n-gram model that can be deployed to devices for fast\ninference. Our technical contributions include ways of handling large\nvocabularies, algorithms to correct capitalization errors in user data, and\nefficient finite state transducer algorithms to convert word language models to\nword-piece language models and vice versa. The n-gram language models trained\nwith federated learning are compared to n-grams trained with traditional\nserver-based algorithms using A/B tests on tens of millions of users of virtual\nkeyboard. Results are presented for two languages, American English and\nBrazilian Portuguese. This work demonstrates that high-quality n-gram language\nmodels can be trained directly on client mobile devices without sensitive\ntraining data ever leaving the devices.", "published": "2019-10-08 14:48:43", "link": "http://arxiv.org/abs/1910.03432v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Find or Classify? Dual Strategy for Slot-Value Predictions on\n  Multi-Domain Dialog State Tracking", "abstract": "Dialog state tracking (DST) is a core component in task-oriented dialog\nsystems. Existing approaches for DST mainly fall into one of two categories,\nnamely, ontology-based and ontology-free methods. An ontology-based method\nselects a value from a candidate-value list for each target slot, while an\nontology-free method extracts spans from dialog contexts. Recent work\nintroduced a BERT-based model to strike a balance between the two methods by\npre-defining categorical and non-categorical slots. However, it is not clear\nenough which slots are better handled by either of the two slot types, and the\nway to use the pre-trained model has not been well investigated. In this paper,\nwe propose a simple yet effective dual-strategy model for DST, by adapting a\nsingle BERT-style reading comprehension model to jointly handle both the\ncategorical and non-categorical slots. Our experiments on the MultiWOZ datasets\nshow that our method significantly outperforms the BERT-based counterpart,\nfinding that the key is a deep interaction between the domain-slot and context\ninformation. When evaluated on noisy (MultiWOZ 2.0) and cleaner (MultiWOZ 2.1)\nsettings, our method performs competitively and robustly across the two\ndifferent settings. Our method sets the new state of the art in the noisy\nsetting, while performing more robustly than the best model in the cleaner\nsetting. We also conduct a comprehensive error analysis on the dataset,\nincluding the effects of the dual strategy for each slot, to facilitate future\nresearch.", "published": "2019-10-08 17:08:39", "link": "http://arxiv.org/abs/1910.03544v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-based Biomedical Data Science 2019", "abstract": "Knowledge-based biomedical data science (KBDS) involves the design and\nimplementation of computer systems that act as if they knew about biomedicine.\nSuch systems depend on formally represented knowledge in computer systems,\noften in the form of knowledge graphs. Here we survey the progress in the last\nyear in systems that use formally represented knowledge to address data science\nproblems in both clinical and biological domains, as well as on approaches for\ncreating knowledge graphs. Major themes include the relationships between\nknowledge graphs and machine learning, the use of natural language processing,\nand the expansion of knowledge-based approaches to novel domains, such as\nChinese Traditional Medicine and biodiversity.", "published": "2019-10-08 17:28:16", "link": "http://arxiv.org/abs/1910.06710v1", "categories": ["cs.AI", "cs.CL", "I.2.0; I.2.1; I.2.4; I.2.7; I.2.m; I.5.0; I.7.0; J.3"], "primary_category": "cs.AI"}
{"title": "Read, Highlight and Summarize: A Hierarchical Neural Semantic\n  Encoder-based Approach", "abstract": "Traditional sequence-to-sequence (seq2seq) models and other variations of the\nattention-mechanism such as hierarchical attention have been applied to the\ntext summarization problem. Though there is a hierarchy in the way humans use\nlanguage by forming paragraphs from sentences and sentences from words,\nhierarchical models have usually not worked that much better than their\ntraditional seq2seq counterparts. This effect is mainly because either the\nhierarchical attention mechanisms are too sparse using hard attention or noisy\nusing soft attention. In this paper, we propose a method based on extracting\nthe highlights of a document; a key concept that is conveyed in a few\nsentences. In a typical text summarization dataset consisting of documents that\nare 800 tokens in length (average), capturing long-term dependencies is very\nimportant, e.g., the last sentence can be grouped with the first sentence of a\ndocument to form a summary. LSTMs (Long Short-Term Memory) proved useful for\nmachine translation. However, they often fail to capture long-term dependencies\nwhile modeling long sequences. To address these issues, we have adapted Neural\nSemantic Encoders (NSE) to text summarization, a class of memory-augmented\nneural networks by improving its functionalities and proposed a novel\nhierarchical NSE that outperforms similar previous models significantly. The\nquality of summarization was improved by augmenting linguistic factors, namely\nlemma, and Part-of-Speech (PoS) tags, to each word in the dataset for improved\nvocabulary coverage and generalization. The hierarchical NSE model on factored\ndataset outperformed the state-of-the-art by nearly 4 ROUGE points. We further\ndesigned and used the first GPU-based self-critical Reinforcement Learning\nmodel.", "published": "2019-10-08 02:36:02", "link": "http://arxiv.org/abs/1910.03177v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Voice for the Voiceless: Active Sampling to Detect Comments Supporting\n  the Rohingyas", "abstract": "The Rohingya refugee crisis is one of the biggest humanitarian crises of\nmodern times with more than 600,000 Rohingyas rendered homeless according to\nthe United Nations High Commissioner for Refugees. While it has received\nsustained press attention globally, no comprehensive research has been\nperformed on social media pertaining to this large evolving crisis. In this\nwork, we construct a substantial corpus of YouTube video comments (263,482\ncomments from 113,250 users in 5,153 relevant videos) with an aim to analyze\nthe possible role of AI in helping a marginalized community. Using a novel\ncombination of multiple Active Learning strategies and a novel active sampling\nstrategy based on nearest-neighbors in the comment-embedding space, we\nconstruct a classifier that can detect comments defending the Rohingyas among\nlarger numbers of disparaging and neutral ones. We advocate that beyond the\nburgeoning field of hate-speech detection, automatic detection of\n\\emph{help-speech} can lend voice to the voiceless people and make the internet\nsafer for marginalized communities.", "published": "2019-10-08 04:17:33", "link": "http://arxiv.org/abs/1910.03206v2", "categories": ["cs.CY", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task", "abstract": "In this paper, we propose a new approach to learn multimodal multilingual\nembeddings for matching images and their relevant captions in two languages. We\ncombine two existing objective functions to make images and captions close in a\njoint embedding space while adapting the alignment of word embeddings between\nexisting languages in our model. We show that our approach enables better\ngeneralization, achieving state-of-the-art performance in text-to-image and\nimage-to-text retrieval task, and caption-caption similarity task. Two\nmultimodal multilingual datasets are used for evaluation: Multi30k with German\nand English captions and Microsoft-COCO with English and Japanese captions.", "published": "2019-10-08 09:13:39", "link": "http://arxiv.org/abs/1910.03291v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modulated Self-attention Convolutional Network for VQA", "abstract": "As new data-sets for real-world visual reasoning and compositional question\nanswering are emerging, it might be needed to use the visual feature extraction\nas a end-to-end process during training. This small contribution aims to\nsuggest new ideas to improve the visual processing of traditional convolutional\nnetwork for visual question answering (VQA). In this paper, we propose to\nmodulate by a linguistic input a CNN augmented with self-attention. We show\nencouraging relative improvements for future research in this direction.", "published": "2019-10-08 11:28:38", "link": "http://arxiv.org/abs/1910.03343v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Prose for a Painting", "abstract": "Painting captions are often dry and simplistic which motivates us to describe\na painting creatively in the style of Shakespearean prose. This is a difficult\nproblem, since there does not exist a large supervised dataset from paintings\nto Shakespearean prose. Our solution is to use an intermediate English poem\ndescription of the painting and then apply language style transfer which\nresults in Shakespearean prose describing the painting. We rate our results by\nhuman evaluation on a Likert scale, and evaluate the quality of language style\ntransfer using BLEU score as a function of prose length. We demonstrate the\napplicability and limitations of our approach by generating Shakespearean prose\nfor famous paintings. We make our models and code publicly available.", "published": "2019-10-08 18:39:49", "link": "http://arxiv.org/abs/1910.03634v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Linking emotions to behaviors through deep transfer learning", "abstract": "Human behavior refers to the way humans act and interact. Understanding human\nbehavior is a cornerstone of observational practice, especially in\npsychotherapy. An important cue of behavior analysis is the dynamical changes\nof emotions during the conversation. Domain experts integrate emotional\ninformation in a highly nonlinear manner, thus, it is challenging to explicitly\nquantify the relationship between emotions and behaviors. In this work, we\nemploy deep transfer learning to analyze their inferential capacity and\ncontextual importance. We first train a network to quantify emotions from\nacoustic signals and then use information from the emotion recognition network\nas features for behavior recognition. We treat this emotion-related information\nas behavioral primitives and further train higher level layers towards behavior\nquantification. Through our analysis, we find that emotion-related information\nis an important cue for behavior recognition. Further, we investigate the\nimportance of emotional-context in the expression of behavior by constraining\n(or not) the neural networks' contextual view of the data. This demonstrates\nthat the sequence of emotions is critical in behavior expression. To achieve\nthese frameworks we employ hybrid architectures of convolutional networks and\nrecurrent networks to extract emotion-related behavior primitives and\nfacilitate automatic behavior recognition from speech.", "published": "2019-10-08 18:55:08", "link": "http://arxiv.org/abs/1910.03641v1", "categories": ["cs.LG", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Executing Instructions in Situated Collaborative Interactions", "abstract": "We study a collaborative scenario where a user not only instructs a system to\ncomplete tasks, but also acts alongside it. This allows the user to adapt to\nthe system abilities by changing their language or deciding to simply\naccomplish some tasks themselves, and requires the system to effectively\nrecover from errors as the user strategically assigns it new goals. We build a\ngame environment to study this scenario, and learn to map user instructions to\nsystem actions. We introduce a learning approach focused on recovery from\ncascading errors between instructions, and modeling methods to explicitly\nreason about instructions with multiple goals. We evaluate with a new\nevaluation protocol using recorded interactions and online games with human\nusers, and observe how users adapt to the system abilities.", "published": "2019-10-08 19:22:58", "link": "http://arxiv.org/abs/1910.03655v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoML using Metadata Language Embeddings", "abstract": "As a human choosing a supervised learning algorithm, it is natural to begin\nby reading a text description of the dataset and documentation for the\nalgorithms you might use. We demonstrate that the same idea improves the\nperformance of automated machine learning methods. We use language embeddings\nfrom modern NLP to improve state-of-the-art AutoML systems by augmenting their\nrecommendations with vector embeddings of datasets and of algorithms. We use\nthese embeddings in a neural architecture to learn the distance between\nbest-performing pipelines. The resulting (meta-)AutoML framework improves on\nthe performance of existing AutoML frameworks. Our zero-shot AutoML system\nusing dataset metadata embeddings provides good solutions instantaneously,\nrunning in under one second of computation. Performance is competitive with\nAutoML systems OBOE, AutoSklearn, AlphaD3M, and TPOT when each framework is\nallocated a minute of computation. We make our data, models, and code publicly\navailable.", "published": "2019-10-08 21:28:47", "link": "http://arxiv.org/abs/1910.03698v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Do People Prefer \"Natural\" code?", "abstract": "Natural code is known to be very repetitive (much more so than natural\nlanguage corpora); furthermore, this repetitiveness persists, even after\naccounting for the simpler syntax of code. However, programming languages are\nvery expressive, allowing a great many different ways (all clear and\nunambiguous) to express even very simple computations. So why is natural code\nrepetitive? We hypothesize that the reasons for this lie in fact that code is\nbimodal: it is executed by machines, but also read by humans. This bimodality,\nwe argue, leads developers to write code in certain preferred ways that would\nbe familiar to code readers. To test this theory, we 1) model familiarity using\na language model estimated over a large training corpus and 2) run an\nexperiment applying several meaning preserving transformations to Java and\nPython expressions in a distinct test corpus to see if forms more familiar to\nreaders (as predicted by the language models) are in fact the ones actually\nwritten. We find that these transformations generally produce program\nstructures that are less common in practice, supporting the theory that the\nhigh repetitiveness in code is a matter of deliberate preference. Finally, 3)\nwe use a human subject study to show alignment between language model score and\nhuman preference for the first time in code, providing support for using this\nmeasure to improve code.", "published": "2019-10-08 22:11:55", "link": "http://arxiv.org/abs/1910.03704v1", "categories": ["cs.CL", "cs.IT", "cs.PL", "math.IT"], "primary_category": "cs.CL"}
{"title": "MelGAN: Generative Adversarial Networks for Conditional Waveform\n  Synthesis", "abstract": "Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that\ngenerating coherent raw audio waveforms with GANs is challenging. In this\npaper, we show that it is possible to train GANs reliably to generate high\nquality coherent waveforms by introducing a set of architectural changes and\nsimple training techniques. Subjective evaluation metric (Mean Opinion Score,\nor MOS) shows the effectiveness of the proposed approach for high quality\nmel-spectrogram inversion. To establish the generality of the proposed\ntechniques, we show qualitative results of our model in speech synthesis, music\ndomain translation and unconditional music synthesis. We evaluate the various\ncomponents of the model through ablation studies and suggest a set of\nguidelines to design general purpose discriminators and generators for\nconditional sequence synthesis tasks. Our model is non-autoregressive, fully\nconvolutional, with significantly fewer parameters than competing models and\ngeneralizes to unseen speakers for mel-spectrogram inversion. Our pytorch\nimplementation runs at more than 100x faster than realtime on GTX 1080Ti GPU\nand more than 2x faster than real-time on CPU, without any hardware specific\noptimization tricks.", "published": "2019-10-08 15:03:08", "link": "http://arxiv.org/abs/1910.06711v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long\n  samples using Spectrograms", "abstract": "Traditional voice conversion methods rely on parallel recordings of multiple\nspeakers pronouncing the same sentences. For real-world applications however,\nparallel data is rarely available. We propose MelGAN-VC, a voice conversion\nmethod that relies on non-parallel speech data and is able to convert audio\nsignals of arbitrary length from a source voice to a target voice. We firstly\ncompute spectrograms from waveform data and then perform a domain translation\nusing a Generative Adversarial Network (GAN) architecture. An additional\nsiamese network helps preserving speech information in the translation process,\nwithout sacrificing the ability to flexibly model the style of the target\nspeaker. We test our framework with a dataset of clean speech recordings, as\nwell as with a collection of noisy real-world speech examples. Finally, we\napply the same method to perform music style transfer, translating arbitrarily\nlong music samples from one genre to another, and showing that our framework is\nflexible and can be used for audio manipulation applications different from\nvoice conversion.", "published": "2019-10-08 23:22:50", "link": "http://arxiv.org/abs/1910.03713v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
