{"title": "Automatic Quality Estimation for ASR System Combination", "abstract": "Recognizer Output Voting Error Reduction (ROVER) has been widely used for\nsystem combination in automatic speech recognition (ASR). In order to select\nthe most appropriate words to insert at each position in the output\ntranscriptions, some ROVER extensions rely on critical information such as\nconfidence scores and other ASR decoder features. This information, which is\nnot always available, highly depends on the decoding process and sometimes\ntends to over estimate the real quality of the recognized words. In this paper\nwe propose a novel variant of ROVER that takes advantage of ASR quality\nestimation (QE) for ranking the transcriptions at \"segment level\" instead of:\ni) relying on confidence scores, or ii) feeding ROVER with randomly ordered\nhypotheses. We first introduce an effective set of features to compensate for\nthe absence of ASR decoder information. Then, we apply QE techniques to perform\naccurate hypothesis ranking at segment-level before starting the fusion\nprocess. The evaluation is carried out on two different tasks, in which we\nrespectively combine hypotheses coming from independent ASR systems and\nmulti-microphone recordings. In both tasks, it is assumed that the ASR decoder\ninformation is not available. The proposed approach significantly outperforms\nstandard ROVER and it is competitive with two strong oracles that e xploit\nprior knowledge about the real quality of the hypotheses to be combined.\nCompared to standard ROVER, the abs olute WER improvements in the two\nevaluation scenarios range from 0.5% to 7.3%.", "published": "2017-06-22 10:11:34", "link": "http://arxiv.org/abs/1706.07238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Conversation Modeling Track in DSTC6", "abstract": "End-to-end training of neural networks is a promising approach to automatic\nconstruction of dialog systems using a human-to-human dialog corpus. Recently,\nVinyals et al. tested neural conversation models using OpenSubtitles. Lowe et\nal. released the Ubuntu Dialogue Corpus for researching unstructured multi-turn\ndialogue systems. Furthermore, the approach has been extended to accomplish\ntask oriented dialogs to provide information properly with natural\nconversation. For example, Ghazvininejad et al. proposed a knowledge grounded\nneural conversation model [3], where the research is aiming at combining\nconversational dialogs with task-oriented knowledge using unstructured data\nsuch as Twitter data for conversation and Foursquare data for external\nknowledge.However, the task is still limited to a restaurant information\nservice, and has not yet been tested with a wide variety of dialog tasks. In\naddition, it is still unclear how to create intelligent dialog systems that can\nrespond like a human agent.\n  In consideration of these problems, we proposed a challenge track to the 6th\ndialog system technology challenges (DSTC6) using human-to-human dialog data to\nmimic human dialog behaviors. The focus of the challenge track is to train\nend-to-end conversation models from human-to-human conversation and accomplish\nend-to-end dialog tasks in various situations assuming a customer service, in\nwhich a system plays a role of human agent and generates natural and\ninformative sentences in response to user's questions or comments given dialog\ncontext.", "published": "2017-06-22 18:00:34", "link": "http://arxiv.org/abs/1706.07440v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Gumbel-Greedy Decoding", "abstract": "Previous neural machine translation models used some heuristic search\nalgorithms (e.g., beam search) in order to avoid solving the maximum a\nposteriori problem over translation sentences at test time. In this paper, we\npropose the Gumbel-Greedy Decoding which trains a generative network to predict\ntranslation under a trained model. We solve such a problem using the\nGumbel-Softmax reparameterization, which makes our generative network\ndifferentiable and trainable through standard stochastic gradient methods. We\nempirically demonstrate that our proposed model is effective for generating\nsequences of discrete words.", "published": "2017-06-22 22:54:25", "link": "http://arxiv.org/abs/1706.07518v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RelNet: End-to-End Modeling of Entities & Relations", "abstract": "We introduce RelNet: a new model for relational reasoning. RelNet is a memory\naugmented neural network which models entities as abstract memory slots and is\nequipped with an additional relational memory which models relations between\nall memory pairs. The model thus builds an abstract knowledge graph on the\nentities and relations present in a document which can then be used to answer\nquestions about the document. It is trained end-to-end: only supervision to the\nmodel is in the form of correct answers to the questions. We test the model on\nthe 20 bAbI question-answering tasks with 10k examples per task and find that\nit solves all the tasks with a mean error of 0.3%, achieving 0% error on 11 of\nthe 20 tasks.", "published": "2017-06-22 06:59:07", "link": "http://arxiv.org/abs/1706.07179v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Personalization in Goal-Oriented Dialog", "abstract": "The main goal of modeling human conversation is to create agents which can\ninteract with people in both open-ended and goal-oriented scenarios. End-to-end\ntrained neural dialog systems are an important line of research for such\ngeneralized dialog models as they do not resort to any situation-specific\nhandcrafting of rules. However, incorporating personalization into such systems\nis a largely unexplored topic as there are no existing corpora to facilitate\nsuch work. In this paper, we present a new dataset of goal-oriented dialogs\nwhich are influenced by speaker profiles attached to them. We analyze the\nshortcomings of an existing end-to-end dialog system based on Memory Networks\nand propose modifications to the architecture which enable personalization. We\nalso investigate personalization in dialog as a multi-task learning problem,\nand show that a single model which shares features among various profiles\noutperforms separate models for each profile.", "published": "2017-06-22 22:09:14", "link": "http://arxiv.org/abs/1706.07503v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Speaker Verification: Do We Need End to End?", "abstract": "End-to-end learning treats the entire system as a whole adaptable black box,\nwhich, if sufficient data are available, may learn a system that works very\nwell for the target task. This principle has recently been applied to several\nprototype research on speaker verification (SV), where the feature learning and\nclassifier are learned together with an objective function that is consistent\nwith the evaluation metric. An opposite approach to end-to-end is feature\nlearning, which firstly trains a feature learning model, and then constructs a\nback-end classifier separately to perform SV. Recently, both approaches\nachieved significant performance gains on SV, mainly attributed to the smart\nutilization of deep neural networks. However, the two approaches have not been\ncarefully compared, and their respective advantages have not been well\ndiscussed. In this paper, we compare the end-to-end and feature learning\napproaches on a text-independent SV task. Our experiments on a dataset sampled\nfrom the Fisher database and involving 5,000 speakers demonstrated that the\nfeature learning approach outperformed the end-to-end approach. This is a\nstrong support for the feature learning approach, at least with data and\ncomputation resources similar to ours.", "published": "2017-06-22 04:33:59", "link": "http://arxiv.org/abs/1706.07859v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Speaker Recognition with Cough, Laugh and \"Wei\"", "abstract": "This paper proposes a speaker recognition (SRE) task with trivial speech\nevents, such as cough and laugh. These trivial events are ubiquitous in\nconversations and less subjected to intentional change, therefore offering\nvaluable particularities to discover the genuine speaker from disguised speech.\nHowever, trivial events are often short and idiocratic in spectral patterns,\nmaking SRE extremely difficult. Fortunately, we found a very powerful deep\nfeature learning structure that can extract highly speaker-sensitive features.\nBy employing this tool, we studied the SRE performance on three types of\ntrivial events: cough, laugh and \"Wei\" (a short Chinese \"Hello\"). The results\nshow that there is rich speaker information within these trivial events, even\nfor cough that is intuitively less speaker distinguishable. With the deep\nfeature approach, the EER can reach 10%-14% with the three trivial events,\ndespite their extremely short durations (0.2-1.0 seconds).", "published": "2017-06-22 04:26:39", "link": "http://arxiv.org/abs/1706.07860v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Cross-lingual Speaker Verification with Deep Feature Learning", "abstract": "Existing speaker verification (SV) systems often suffer from performance\ndegradation if there is any language mismatch between model training, speaker\nenrollment, and test. A major cause of this degradation is that most existing\nSV methods rely on a probabilistic model to infer the speaker factor, so any\nsignificant change on the distribution of the speech signal will impact the\ninference. Recently, we proposed a deep learning model that can learn how to\nextract the speaker factor by a deep neural network (DNN). By this feature\nlearning, an SV system can be constructed with a very simple back-end model. In\nthis paper, we investigate the robustness of the feature-based SV system in\nsituations with language mismatch. Our experiments were conducted on a complex\ncross-lingual scenario, where the model training was in English, and the\nenrollment and test were in Chinese or Uyghur. The experiments demonstrated\nthat the feature-based system outperformed the i-vector system with a large\nmargin, particularly with language mismatch between enrollment and test.", "published": "2017-06-22 04:32:40", "link": "http://arxiv.org/abs/1706.07861v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Explaining Recurrent Neural Network Predictions in Sentiment Analysis", "abstract": "Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown\nto deliver insightful explanations in the form of input space relevances for\nunderstanding feed-forward neural network classification decisions. In the\npresent work, we extend the usage of LRP to recurrent neural networks. We\npropose a specific propagation rule applicable to multiplicative connections as\nthey arise in recurrent network architectures such as LSTMs and GRUs. We apply\nour technique to a word-based bi-directional LSTM model on a five-class\nsentiment prediction task, and evaluate the resulting LRP relevances both\nqualitatively and quantitatively, obtaining better results than a\ngradient-based related method which was used in previous work.", "published": "2017-06-22 08:24:59", "link": "http://arxiv.org/abs/1706.07206v2", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Gated-Attention Architectures for Task-Oriented Language Grounding", "abstract": "To perform tasks specified by natural language instructions, autonomous\nagents need to extract semantically meaningful representations of language and\nmap it to visual elements and actions in the environment. This problem is\ncalled task-oriented language grounding. We propose an end-to-end trainable\nneural architecture for task-oriented language grounding in 3D environments\nwhich assumes no prior linguistic or perceptual knowledge and requires only raw\npixels from the environment and the natural language instruction as input. The\nproposed model combines the image and text representations using a\nGated-Attention mechanism and learns a policy to execute the natural language\ninstruction using standard reinforcement and imitation learning methods. We\nshow the effectiveness of the proposed model on unseen instructions as well as\nunseen maps, both quantitatively and qualitatively. We also introduce a novel\nenvironment based on a 3D game engine to simulate the challenges of\ntask-oriented language grounding over a rich set of instructions and\nenvironment states.", "published": "2017-06-22 09:39:17", "link": "http://arxiv.org/abs/1706.07230v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.LG"}
