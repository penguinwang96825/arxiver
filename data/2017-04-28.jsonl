{"title": "Understanding and Detecting Supporting Arguments of Diverse Types", "abstract": "We investigate the problem of sentence-level supporting argument detection\nfrom relevant documents for user-specified claims. A dataset containing claims\nand associated citation articles is collected from online debate website\nidebate.org. We then manually label sentence-level supporting arguments from\nthe documents along with their types as study, factual, opinion, or reasoning.\nWe further characterize arguments of different types, and explore whether\nleveraging type information can facilitate the supporting arguments detection\ntask. Experimental results show that LambdaMART (Burges, 2010) ranker that uses\nfeatures informed by argument types yields better performance than the same\nranker trained without type information.", "published": "2017-04-28 19:29:54", "link": "http://arxiv.org/abs/1705.00045v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping Instructions and Visual Observations to Actions with\n  Reinforcement Learning", "abstract": "We propose to directly map raw visual observations and text input to actions\nfor instruction execution. While existing approaches assume access to\nstructured environment representations or use a pipeline of separately trained\nmodels, we learn a single model to jointly reason about linguistic and visual\ninput. We use reinforcement learning in a contextual bandit setting to train a\nneural network agent. To guide the agent's exploration, we use reward shaping\nwith different forms of supervision. Our approach does not require intermediate\nrepresentations, planning procedures, or training different models. We evaluate\nin a simulated environment, and show significant improvements over supervised\nlearning and common reinforcement learning variants.", "published": "2017-04-28 03:12:57", "link": "http://arxiv.org/abs/1704.08795v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Affect Intensities", "abstract": "Words often convey affect -- emotions, feelings, and attitudes. Further,\ndifferent words can convey affect to various degrees (intensities). However,\nexisting manually created lexicons for basic emotions (such as anger and fear)\nindicate only coarse categories of affect association (for example, associated\nwith anger or not associated with anger). Automatic lexicons of affect provide\nfine degrees of association, but they tend not to be accurate as human-created\nlexicons. Here, for the first time, we present a manually created affect\nintensity lexicon with real-valued scores of intensity for four basic emotions:\nanger, fear, joy, and sadness. (We will subsequently add entries for more\nemotions such as disgust, anticipation, trust, and surprise.) We refer to this\ndataset as the NRC Affect Intensity Lexicon, or AIL for short. AIL has entries\nfor close to 6,000 English words. We used a technique called best-worst scaling\n(BWS) to create the lexicon. BWS improves annotation consistency and obtains\nreliable fine-grained scores (split-half reliability > 0.91). We also compare\nthe entries in AIL with the entries in the NRC VAD Lexicon, which has valence,\narousal, and dominance (VAD) scores for 20K English words. We find that anger,\nfear, and sadness words, on average, have very similar VAD scores. However,\nsadness words tend to have slightly lower dominance scores than fear and anger\nwords. The Affect Intensity Lexicon has applications in automatic emotion\nanalysis in a number of domains such as commerce, education, intelligence, and\npublic health. AIL is also useful in the building of natural language\ngeneration systems.", "published": "2017-04-28 03:33:16", "link": "http://arxiv.org/abs/1704.08798v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How compatible are our discourse annotations? Insights from mapping\n  RST-DT and PDTB annotations", "abstract": "Discourse-annotated corpora are an important resource for the community, but\nthey are often annotated according to different frameworks. This makes\ncomparison of the annotations difficult, thereby also preventing researchers\nfrom searching the corpora in a unified way, or using all annotated data\njointly to train computational systems. Several theoretical proposals have\nrecently been made for mapping the relational labels of different frameworks to\neach other, but these proposals have so far not been validated against existing\nannotations. The two largest discourse relation annotated resources, the Penn\nDiscourse Treebank and the Rhetorical Structure Theory Discourse Treebank, have\nhowever been annotated on the same text, allowing for a direct comparison of\nthe annotation layers. We propose a method for automatically aligning the\ndiscourse segments, and then evaluate existing mapping proposals by comparing\nthe empirically observed against the proposed mappings. Our analysis highlights\nthe influence of segmentation on subsequent discourse relation labeling, and\nshows that while agreement between frameworks is reasonable for explicit\nrelations, agreement on implicit relations is low. We identify several sources\nof systematic discrepancies between the two annotation schemes and discuss\nconsequences of these discrepancies for future annotation and for the training\nof automatic discourse relation labellers.", "published": "2017-04-28 12:09:31", "link": "http://arxiv.org/abs/1704.08893v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Word Segmentation with Rich Pretraining", "abstract": "Neural word segmentation research has benefited from large-scale raw texts by\nleveraging them for pretraining character and word embeddings. On the other\nhand, statistical segmentation research has exploited richer sources of\nexternal information, such as punctuation, automatic segmentation and POS. We\ninvestigate the effectiveness of a range of external training sources for\nneural word segmentation by building a modular segmentation model, pretraining\nthe most important submodule using rich external sources. Results show that\nsuch pretraining significantly improves the model, leading to accuracies\ncompetitive to the best methods on six benchmarks.", "published": "2017-04-28 14:46:25", "link": "http://arxiv.org/abs/1704.08960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not All Dialogues are Created Equal: Instance Weighting for Neural\n  Conversational Models", "abstract": "Neural conversational models require substantial amounts of dialogue data for\ntheir parameter estimation and are therefore usually learned on large corpora\nsuch as chat forums or movie subtitles. These corpora are, however, often\nchallenging to work with, notably due to their frequent lack of turn\nsegmentation and the presence of multiple references external to the dialogue\nitself. This paper shows that these challenges can be mitigated by adding a\nweighting model into the architecture. The weighting model, which is itself\nestimated from dialogue data, associates each training example to a numerical\nweight that reflects its intrinsic quality for dialogue modelling. At training\ntime, these sample weights are included into the empirical loss to be\nminimised. Evaluation results on retrieval-based models trained on movie and TV\nsubtitles demonstrate that the inclusion of such a weighting model improves the\nmodel performance on unsupervised metrics.", "published": "2017-04-28 14:57:29", "link": "http://arxiv.org/abs/1704.08966v2", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Neural Ranking Models with Weak Supervision", "abstract": "Despite the impressive improvements achieved by unsupervised deep neural\nnetworks in computer vision and NLP tasks, such improvements have not yet been\nobserved in ranking for information retrieval. The reason may be the complexity\nof the ranking problem, as it is not obvious how to learn from queries and\ndocuments when no supervised signal is available. Hence, in this paper, we\npropose to train a neural ranking model using weak supervision, where labels\nare obtained automatically without human annotators or any external resources\n(e.g., click data). To this aim, we use the output of an unsupervised ranking\nmodel, such as BM25, as a weak supervision signal. We further train a set of\nsimple yet effective ranking models based on feed-forward neural networks. We\nstudy their effectiveness under various learning scenarios (point-wise and\npair-wise models) and using different input representations (i.e., from\nencoding query-document pairs into dense/sparse vectors to using word embedding\nrepresentation). We train our networks using tens of millions of training\ninstances and evaluate it on two standard collections: a homogeneous news\ncollection(Robust) and a heterogeneous large-scale web collection (ClueWeb).\nOur experiments indicate that employing proper objective functions and letting\nthe networks to learn the input representation based on weakly supervised data\nleads to impressive performance, with over 13% and 35% MAP improvements over\nthe BM25 model on the Robust and the ClueWeb collections. Our findings also\nsuggest that supervised neural ranking models can greatly benefit from\npre-training on large amounts of weakly labeled data that can be easily\nobtained from unsupervised IR models.", "published": "2017-04-28 04:08:47", "link": "http://arxiv.org/abs/1704.08803v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Past, Present, Future: A Computational Investigation of the Typology of\n  Tense in 1000 Languages", "abstract": "We present SuperPivot, an analysis method for low-resource languages that\noccur in a superparallel corpus, i.e., in a corpus that contains an order of\nmagnitude more languages than parallel corpora currently in use. We show that\nSuperPivot performs well for the crosslingual analysis of the linguistic\nphenomenon of tense. We produce analysis results for more than 1000 languages,\nconducting - to the best of our knowledge - the largest crosslingual\ncomputational study performed to date. We extend existing methodology for\nleveraging parallel corpora for typological analysis by overcoming a limiting\nassumption of earlier work: We only require that a linguistic feature is\novertly marked in a few of thousands of languages as opposed to requiring that\nit be marked in all languages under investigation.", "published": "2017-04-28 13:11:09", "link": "http://arxiv.org/abs/1704.08914v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
