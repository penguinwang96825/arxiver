{"title": "Be More with Less: Hypergraph Attention Networks for Inductive Text\n  Classification", "abstract": "Text classification is a critical research topic with broad applications in\nnatural language processing. Recently, graph neural networks (GNNs) have\nreceived increasing attention in the research community and demonstrated their\npromising results on this canonical task. Despite the success, their\nperformance could be largely jeopardized in practice since they are: (1) unable\nto capture high-order interaction between words; (2) inefficient to handle\nlarge datasets and new documents. To address those issues, in this paper, we\npropose a principled model -- hypergraph attention networks (HyperGAT), which\ncan obtain more expressive power with less computational consumption for text\nrepresentation learning. Extensive experiments on various benchmark datasets\ndemonstrate the efficacy of the proposed approach on the text classification\ntask.", "published": "2020-11-01 00:21:59", "link": "http://arxiv.org/abs/2011.00387v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigation of BERT Model on Biomedical Relation Extraction Based on\n  Revised Fine-tuning Mechanism", "abstract": "With the explosive growth of biomedical literature, designing automatic tools\nto extract information from the literature has great significance in biomedical\nresearch. Recently, transformer-based BERT models adapted to the biomedical\ndomain have produced leading results. However, all the existing BERT models for\nrelation classification only utilize partial knowledge from the last layer. In\nthis paper, we will investigate the method of utilizing the entire layer in the\nfine-tuning process of BERT model. To the best of our knowledge, we are the\nfirst to explore this method. The experimental results illustrate that our\nmethod improves the BERT model performance and outperforms the state-of-the-art\nmethods on three benchmark datasets for different relation extraction tasks. In\naddition, further analysis indicates that the key knowledge about the relations\ncan be learned from the last layer of BERT model.", "published": "2020-11-01 01:47:16", "link": "http://arxiv.org/abs/2011.00398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Autoregressive Predictive Coding for Learning Speech Representations\n  from Local Dependencies", "abstract": "Self-supervised speech representations have been shown to be effective in a\nvariety of speech applications. However, existing representation learning\nmethods generally rely on the autoregressive model and/or observed global\ndependencies while generating the representation. In this work, we propose\nNon-Autoregressive Predictive Coding (NPC), a self-supervised method, to learn\na speech representation in a non-autoregressive manner by relying only on local\ndependencies of speech. NPC has a conceptually simple objective and can be\nimplemented easily with the introduced Masked Convolution Blocks. NPC offers a\nsignificant speedup for inference since it is parallelizable in time and has a\nfixed inference time for each time step regardless of the input sequence\nlength. We discuss and verify the effectiveness of NPC by theoretically and\nempirically comparing it with other methods. We show that the NPC\nrepresentation is comparable to other methods in speech experiments on phonetic\nand speaker classification while being more efficient.", "published": "2020-11-01 02:48:37", "link": "http://arxiv.org/abs/2011.00406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fake or Real? A Study of Arabic Satirical Fake News", "abstract": "One very common type of fake news is satire which comes in a form of a news\nwebsite or an online platform that parodies reputable real news agencies to\ncreate a sarcastic version of reality. This type of fake news is often\ndisseminated by individuals on their online platforms as it has a much stronger\neffect in delivering criticism than through a straightforward message. However,\nwhen the satirical text is disseminated via social media without mention of its\nsource, it can be mistaken for real news. This study conducts several\nexploratory analyses to identify the linguistic properties of Arabic fake news\nwith satirical content. We exploit these features to build a number of machine\nlearning models capable of identifying satirical fake news with an accuracy of\nup to 98.6%.", "published": "2020-11-01 08:56:56", "link": "http://arxiv.org/abs/2011.00452v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeing Both the Forest and the Trees: Multi-head Attention for Joint\n  Classification on Different Compositional Levels", "abstract": "In natural languages, words are used in association to construct sentences.\nIt is not words in isolation, but the appropriate combination of hierarchical\nstructures that conveys the meaning of the whole sentence. Neural networks can\ncapture expressive language features; however, insights into the link between\nwords and sentences are difficult to acquire automatically. In this work, we\ndesign a deep neural network architecture that explicitly wires lower and\nhigher linguistic components; we then evaluate its ability to perform the same\ntask at different hierarchical levels. Settling on broad text classification\ntasks, we show that our model, MHAL, learns to simultaneously solve them at\ndifferent levels of granularity by fluidly transferring knowledge between\nhierarchies. Using a multi-head attention mechanism to tie the representations\nbetween single words and full sentences, MHAL systematically outperforms\nequivalent models that are not incentivized towards developing compositional\nrepresentations. Moreover, we demonstrate that, with the proposed architecture,\nthe sentence information flows naturally to individual words, allowing the\nmodel to behave like a sequence labeller (which is a lower, word-level task)\neven without any word supervision, in a zero-shot fashion.", "published": "2020-11-01 10:44:46", "link": "http://arxiv.org/abs/2011.00470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Opinion Transmission Network for Jointly Improving Aspect-oriented\n  Opinion Words Extraction and Sentiment Classification", "abstract": "Aspect-level sentiment classification (ALSC) and aspect oriented opinion\nwords extraction (AOWE) are two highly relevant aspect-based sentiment analysis\n(ABSA) subtasks. They respectively aim to detect the sentiment polarity and\nextract the corresponding opinion words toward a given aspect in a sentence.\nPrevious works separate them and focus on one of them by training neural models\non small-scale labeled data, while neglecting the connections between them. In\nthis paper, we propose a novel joint model, Opinion Transmission Network (OTN),\nto exploit the potential bridge between ALSC and AOWE to achieve the goal of\nfacilitating them simultaneously. Specifically, we design two tailor-made\nopinion transmission mechanisms to control opinion clues flow bidirectionally,\nrespectively from ALSC to AOWE and AOWE to ALSC. Experiment results on two\nbenchmark datasets show that our joint model outperforms strong baselines on\nthe two tasks. Further analysis also validates the effectiveness of opinion\ntransmission mechanisms.", "published": "2020-11-01 11:00:19", "link": "http://arxiv.org/abs/2011.00474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Multi-Aspect Modeling for Multi-Aspect Multi-Sentiment\n  Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) aims at analyzing the sentiment of a\ngiven aspect in a sentence. Recently, neural network-based methods have\nachieved promising results in existing ABSA datasets. However, these datasets\ntend to degenerate to sentence-level sentiment analysis because most sentences\ncontain only one aspect or multiple aspects with the same sentiment polarity.\nTo facilitate the research of ABSA, NLPCC 2020 Shared Task 2 releases a new\nlarge-scale Multi-Aspect Multi-Sentiment (MAMS) dataset. In the MAMS dataset,\neach sentence contains at least two different aspects with different sentiment\npolarities, which makes ABSA more complex and challenging. To address the\nchallenging dataset, we re-formalize ABSA as a problem of multi-aspect\nsentiment analysis, and propose a novel Transformer-based Multi-aspect Modeling\nscheme (TMM), which can capture potential relations between multiple aspects\nand simultaneously detect the sentiment of all aspects in a sentence.\nExperiment results on the MAMS dataset show that our method achieves noticeable\nimprovements compared with strong baselines such as BERT and RoBERTa, and\nfinally ranks the 2nd in NLPCC 2020 Shared Task 2 Evaluation.", "published": "2020-11-01 11:06:31", "link": "http://arxiv.org/abs/2011.00476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHIME: Cross-passage Hierarchical Memory Network for Generative Review\n  Question Answering", "abstract": "We introduce CHIME, a cross-passage hierarchical memory network for question\nanswering (QA) via text generation. It extends XLNet introducing an auxiliary\nmemory module consisting of two components: the context memory collecting\ncross-passage evidences, and the answer memory working as a buffer continually\nrefining the generated answers. Empirically, we show the efficacy of the\nproposed architecture in the multi-passage generative QA, outperforming the\nstate-of-the-art baselines with better syntactically well-formed answers and\nincreased precision in addressing the questions of the AmazonQA review dataset.\nAn additional qualitative analysis revealed the interpretability introduced by\nthe memory module.", "published": "2020-11-01 14:48:49", "link": "http://arxiv.org/abs/2011.00519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic coordinates analysis reveals language changes in the AI field", "abstract": "Semantic shifts can reflect changes in beliefs across hundreds of years, but\nit is less clear whether trends in fast-changing communities across a short\ntime can be detected. We propose semantic coordinates analysis, a method based\non semantic shifts, that reveals changes in language within publications of a\nfield (we use AI as example) across a short time span. We use GloVe-style\nprobability ratios to quantify the shifting directions and extents from\nmultiple viewpoints. We show that semantic coordinates analysis can detect\nshifts echoing changes of research interests (e.g., \"deep\" shifted further from\n\"rigorous\" to \"neural\"), and developments of research activities (e,g.,\n\"collaboration\" contains less \"competition\" than \"collaboration\"), based on\npublications spanning as short as 10 years.", "published": "2020-11-01 15:59:24", "link": "http://arxiv.org/abs/2011.00543v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMRT Chatbots: Improving Non-Task-Oriented Dialog with Simulated\n  Multiple Reference Training", "abstract": "Non-task-oriented dialog models suffer from poor quality and non-diverse\nresponses. To overcome limited conversational data, we apply Simulated Multiple\nReference Training (SMRT; Khayrallah et al., 2020), and use a paraphraser to\nsimulate multiple responses per training prompt. We find SMRT improves over a\nstrong Transformer baseline as measured by human and automatic quality scores\nand lexical diversity. We also find SMRT is comparable to pretraining in human\nevaluation quality, and outperforms pretraining on automatic quality and\nlexical diversity, without requiring related-domain dialog data.", "published": "2020-11-01 16:11:17", "link": "http://arxiv.org/abs/2011.00547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recent Neural Methods on Slot Filling and Intent Classification for\n  Task-Oriented Dialogue Systems: A Survey", "abstract": "In recent years, fostered by deep learning technologies and by the high\ndemand for conversational AI, various approaches have been proposed that\naddress the capacity to elicit and understand user's needs in task-oriented\ndialogue systems. We focus on two core tasks, slot filling (SF) and intent\nclassification (IC), and survey how neural-based models have rapidly evolved to\naddress natural language understanding in dialogue systems. We introduce three\nneural architectures: independent model, which model SF and IC separately,\njoint models, which exploit the mutual benefit of the two tasks simultaneously,\nand transfer learning models, that scale the model to new domains. We discuss\nthe current state of the research in SF and IC and highlight challenges that\nstill require attention.", "published": "2020-11-01 17:15:42", "link": "http://arxiv.org/abs/2011.00564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vec2Sent: Probing Sentence Embeddings with Natural Language Generation", "abstract": "We introspect black-box sentence embeddings by conditionally generating from\nthem with the objective to retrieve the underlying discrete sentence. We\nperceive of this as a new unsupervised probing task and show that it correlates\nwell with downstream task performance. We also illustrate how the language\ngenerated from different encoders differs. We apply our approach to generate\nsentence analogies from sentence embeddings.", "published": "2020-11-01 18:46:53", "link": "http://arxiv.org/abs/2011.00592v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bracketing Encodings for 2-Planar Dependency Parsing", "abstract": "We present a bracketing-based encoding that can be used to represent any\n2-planar dependency tree over a sentence of length n as a sequence of n labels,\nhence providing almost total coverage of crossing arcs in sequence labeling\nparsing. First, we show that existing bracketing encodings for parsing as\nlabeling can only handle a very mild extension of projective trees. Second, we\novercome this limitation by taking into account the well-known property of\n2-planarity, which is present in the vast majority of dependency syntactic\nstructures in treebanks, i.e., the arcs of a dependency tree can be split into\ntwo planes such that arcs in a given plane do not cross. We take advantage of\nthis property to design a method that balances the brackets and that encodes\nthe arcs belonging to each of those planes, allowing for almost unrestricted\nnon-projectivity (round 99.9% coverage) in sequence labeling parsing. The\nexperiments show that our linearizations improve over the accuracy of the\noriginal bracketing encoding in highly non-projective treebanks (on average by\n0.4 LAS), while achieving a similar speed. Also, they are especially suitable\nwhen PoS tags are not used as input parameters to the models.", "published": "2020-11-01 18:53:32", "link": "http://arxiv.org/abs/2011.00596v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Conversational Question Answering Systems after Deployment\n  using Feedback-Weighted Learning", "abstract": "The interaction of conversational systems with users poses an exciting\nopportunity for improving them after deployment, but little evidence has been\nprovided of its feasibility. In most applications, users are not able to\nprovide the correct answer to the system, but they are able to provide binary\n(correct, incorrect) feedback. In this paper we propose feedback-weighted\nlearning based on importance sampling to improve upon an initial supervised\nsystem using binary user feedback. We perform simulated experiments on document\nclassification (for development) and Conversational Question Answering datasets\nlike QuAC and DoQA, where binary user feedback is derived from gold\nannotations. The results show that our method is able to improve over the\ninitial supervised system, getting close to a fully-supervised system that has\naccess to the same labeled examples in in-domain experiments (QuAC), and even\nmatching in out-of-domain experiments (DoQA). Our work opens the prospect to\nexploit interactions with real users and improve conversational systems after\ndeployment.", "published": "2020-11-01 19:50:34", "link": "http://arxiv.org/abs/2011.00615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-Based Argument Mining", "abstract": "Computational Argumentation in general and Argument Mining in particular are\nimportant research fields. In previous works, many of the challenges to\nautomatically extract and to some degree reason over natural language arguments\nwere addressed. The tools to extract argument units are increasingly available\nand further open problems can be addressed. In this work, we are presenting the\ntask of Aspect-Based Argument Mining (ABAM), with the essential subtasks of\nAspect Term Extraction (ATE) and Nested Segmentation (NS). At the first\ninstance, we create and release an annotated corpus with aspect information on\nthe token-level. We consider aspects as the main point(s) argument units are\naddressing. This information is important for further downstream tasks such as\nargument ranking, argument summarization and generation, as well as the search\nfor counter-arguments on the aspect-level. We present several experiments using\nstate-of-the-art supervised architectures and demonstrate their performance for\nboth of the subtasks. The annotated benchmark is available at\nhttps://github.com/trtm/ABAM.", "published": "2020-11-01 21:57:51", "link": "http://arxiv.org/abs/2011.00633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Effect of Multi-task Learning for Biomedical Named Entity\n  Recognition", "abstract": "Developing high-performing systems for detecting biomedical named entities\nhas major implications. State-of-the-art deep-learning based solutions for\nentity recognition often require large annotated datasets, which is not\navailable in the biomedical domain. Transfer learning and multi-task learning\nhave been shown to improve performance for low-resource domains. However, the\napplications of these methods are relatively scarce in the biomedical domain,\nand a theoretical understanding of why these methods improve the performance is\nlacking. In this study, we performed an extensive analysis to understand the\ntransferability between different biomedical entity datasets. We found useful\nmeasures to predict transferability between these datasets. Besides, we propose\ncombining transfer learning and multi-task learning to improve the performance\nof biomedical named entity recognition systems, which is not applied before to\nthe best of our knowledge.", "published": "2020-11-01 04:52:56", "link": "http://arxiv.org/abs/2011.00425v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deconstruct to Reconstruct a Configurable Evaluation Metric for\n  Open-Domain Dialogue Systems", "abstract": "Many automatic evaluation metrics have been proposed to score the overall\nquality of a response in open-domain dialogue. Generally, the overall quality\nis comprised of various aspects, such as relevancy, specificity, and empathy,\nand the importance of each aspect differs according to the task. For instance,\nspecificity is mandatory in a food-ordering dialogue task, whereas fluency is\npreferred in a language-teaching dialogue system. However, existing metrics are\nnot designed to cope with such flexibility. For example, BLEU score\nfundamentally relies only on word overlapping, whereas BERTScore relies on\nsemantic similarity between reference and candidate response. Thus, they are\nnot guaranteed to capture the required aspects, i.e., specificity. To design a\nmetric that is flexible to a task, we first propose making these qualities\nmanageable by grouping them into three groups: understandability, sensibleness,\nand likability, where likability is a combination of qualities that are\nessential for a task. We also propose a simple method to composite metrics of\neach aspect to obtain a single metric called USL-H, which stands for\nUnderstandability, Sensibleness, and Likability in Hierarchy. We demonstrated\nthat USL-H score achieves good correlations with human judgment and maintains\nits configurability towards different aspects and metrics.", "published": "2020-11-01 11:34:50", "link": "http://arxiv.org/abs/2011.00483v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Diacritization: Efficient Hierarchical Recurrence for Improved\n  Arabic Diacritization", "abstract": "We propose a novel architecture for labelling character sequences that\nachieves state-of-the-art results on the Tashkeela Arabic diacritization\nbenchmark. The core is a two-level recurrence hierarchy that operates on the\nword and character levels separately---enabling faster training and inference\nthan comparable traditional models. A cross-level attention module further\nconnects the two, and opens the door for network interpretability. The task\nmodule is a softmax classifier that enumerates valid combinations of\ndiacritics. This architecture can be extended with a recurrent decoder that\noptionally accepts priors from partially diacritized text, which improves\nresults. We employ extra tricks such as sentence dropout and majority voting to\nfurther boost the final result. Our best model achieves a WER of 5.34%,\noutperforming the previous state-of-the-art with a 30.56% relative error\nreduction.", "published": "2020-11-01 15:33:43", "link": "http://arxiv.org/abs/2011.00538v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ASAD: A Twitter-based Benchmark Arabic Sentiment Analysis Dataset", "abstract": "This paper provides a detailed description of a new Twitter-based benchmark\ndataset for Arabic Sentiment Analysis (ASAD), which is launched in a\ncompetition3, sponsored by KAUST for awarding 10000 USD, 5000 USD and 2000 USD\nto the first, second and third place winners, respectively. Compared to other\npublicly released Arabic datasets, ASAD is a large, high-quality annotated\ndataset(including 95K tweets), with three-class sentiment labels (positive,\nnegative and neutral). We presents the details of the data collection process\nand annotation process. In addition, we implement several baseline models for\nthe competition task and report the results as a reference for the participants\nto the competition.", "published": "2020-11-01 18:03:22", "link": "http://arxiv.org/abs/2011.00578v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Unifying Theory of Transition-based and Sequence Labeling Parsing", "abstract": "We define a mapping from transition-based parsing algorithms that read\nsentences from left to right to sequence labeling encodings of syntactic trees.\nThis not only establishes a theoretical relation between transition-based\nparsing and sequence-labeling parsing, but also provides a method to obtain new\nencodings for fast and simple sequence labeling parsing from the many existing\ntransition-based parsers for different formalisms. Applying it to dependency\nparsing, we implement sequence labeling versions of four algorithms, showing\nthat they are learnable and obtain comparable performance to existing\nencodings.", "published": "2020-11-01 18:25:15", "link": "http://arxiv.org/abs/2011.00584v1", "categories": ["cs.CL", "cs.FL", "68T50, 68Q45", "F.4.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "MixKD: Towards Efficient Distillation of Large-scale Language Models", "abstract": "Large-scale language models have recently demonstrated impressive empirical\nperformance. Nevertheless, the improved results are attained at the price of\nbigger models, more power consumption, and slower inference, which hinder their\napplicability to low-resource (both memory and computation) platforms.\nKnowledge distillation (KD) has been demonstrated as an effective framework for\ncompressing such big models. However, large-scale neural network systems are\nprone to memorize training instances, and thus tend to make inconsistent\npredictions when the data distribution is altered slightly. Moreover, the\nstudent model has few opportunities to request useful information from the\nteacher model when there is limited task-specific data available. To address\nthese issues, we propose MixKD, a data-agnostic distillation framework that\nleverages mixup, a simple yet efficient data augmentation approach, to endow\nthe resulting model with stronger generalization ability. Concretely, in\naddition to the original training examples, the student model is encouraged to\nmimic the teacher's behavior on the linear interpolation of example pairs as\nwell. We prove from a theoretical perspective that under reasonable conditions\nMixKD gives rise to a smaller gap between the generalization error and the\nempirical error. To verify its effectiveness, we conduct experiments on the\nGLUE benchmark, where MixKD consistently leads to significant gains over the\nstandard KD training, and outperforms several competitive baselines.\nExperiments under a limited-data setting and ablation studies further\ndemonstrate the advantages of the proposed approach.", "published": "2020-11-01 18:47:51", "link": "http://arxiv.org/abs/2011.00593v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Social Chemistry 101: Learning to Reason about Social and Moral Norms", "abstract": "Social norms -- the unspoken commonsense rules about acceptable social\nbehavior -- are crucial in understanding the underlying causes and intents of\npeople's actions in narratives. For example, underlying an action such as\n\"wanting to call cops on my neighbors\" are social norms that inform our\nconduct, such as \"It is expected that you report crimes.\"\n  We present Social Chemistry, a new conceptual formalism to study people's\neveryday social norms and moral judgments over a rich spectrum of real life\nsituations described in natural language. We introduce Social-Chem-101, a\nlarge-scale corpus that catalogs 292k rules-of-thumb such as \"it is rude to run\na blender at 5am\" as the basic conceptual units. Each rule-of-thumb is further\nbroken down with 12 different dimensions of people's judgments, including\nsocial judgments of good and bad, moral foundations, expected cultural\npressure, and assumed legality, which together amount to over 4.5 million\nannotations of categorical labels and free-text descriptions.\n  Comprehensive empirical results based on state-of-the-art neural models\ndemonstrate that computational modeling of social norms is a promising research\ndirection. Our model framework, Neural Norm Transformer, learns and generalizes\nSocial-Chem-101 to successfully reason about previously unseen situations,\ngenerating relevant (and potentially novel) attribute-aware social\nrules-of-thumb.", "published": "2020-11-01 20:16:45", "link": "http://arxiv.org/abs/2011.00620v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards A Friendly Online Community: An Unsupervised Style Transfer\n  Framework for Profanity Redaction", "abstract": "Offensive and abusive language is a pressing problem on social media\nplatforms. In this work, we propose a method for transforming offensive\ncomments, statements containing profanity or offensive language, into\nnon-offensive ones. We design a RETRIEVE, GENERATE and EDIT unsupervised style\ntransfer pipeline to redact the offensive comments in a word-restricted manner\nwhile maintaining a high level of fluency and preserving the content of the\noriginal text. We extensively evaluate our method's performance and compare it\nto previous style transfer models using both automatic metrics and human\nevaluations. Experimental results show that our method outperforms other models\non human evaluations and is the only approach that consistently performs well\non all automatic evaluation metrics.", "published": "2020-11-01 02:10:25", "link": "http://arxiv.org/abs/2011.00403v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Deep Learning for Text Style Transfer: A Survey", "abstract": "Text style transfer is an important task in natural language generation,\nwhich aims to control certain attributes in the generated text, such as\npoliteness, emotion, humor, and many others. It has a long history in the field\nof natural language processing, and recently has re-gained significant\nattention thanks to the promising performance brought by deep neural models. In\nthis paper, we present a systematic survey of the research on neural text style\ntransfer, spanning over 100 representative articles since the first neural text\nstyle transfer work in 2017. We discuss the task formulation, existing datasets\nand subtasks, evaluation, as well as the rich methodologies in the presence of\nparallel and non-parallel data. We also provide discussions on a variety of\nimportant topics regarding the future development of this task. Our curated\npaper list is at https://github.com/zhijing-jin/Text_Style_Transfer_Survey", "published": "2020-11-01 04:04:43", "link": "http://arxiv.org/abs/2011.00416v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Cyberbully Detection with User Interaction", "abstract": "Cyberbullying, identified as intended and repeated online bullying behavior,\nhas become increasingly prevalent in the past few decades. Despite the\nsignificant progress made thus far, the focus of most existing work on\ncyberbullying detection lies in the independent content analysis of different\ncomments within a social media session. We argue that such leading notions of\nanalysis suffer from three key limitations: they overlook the temporal\ncorrelations among different comments; they only consider the content within a\nsingle comment rather than the topic coherence across comments; they remain\ngeneric and exploit limited interactions between social media users. In this\nwork, we observe that user comments in the same session may be inherently\nrelated, e.g., discussing similar topics, and their interaction may evolve over\ntime. We also show that modeling such topic coherence and temporal interaction\nare critical to capture the repetitive characteristics of bullying behavior,\nthus leading to better predicting performance. To achieve the goal, we first\nconstruct a unified temporal graph for each social media session. Drawing on\nrecent advances in graph neural network, we then propose a principled\ngraph-based approach for modeling the temporal dynamics and topic coherence\nthroughout user interactions. We empirically evaluate the effectiveness of our\napproach with the tasks of session-level bullying detection and comment-level\ncase study. Our code is released to public.", "published": "2020-11-01 08:47:33", "link": "http://arxiv.org/abs/2011.00449v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "WLV-RIT at HASOC-Dravidian-CodeMix-FIRE2020: Offensive Language\n  Identification in Code-switched YouTube Comments", "abstract": "This paper describes the WLV-RIT entry to the Hate Speech and Offensive\nContent Identification in Indo-European Languages (HASOC) shared task 2020. The\nHASOC 2020 organizers provided participants with annotated datasets containing\nsocial media posts of code-mixed in Dravidian languages (Malayalam-English and\nTamil-English). We participated in task 1: Offensive comment identification in\nCode-mixed Malayalam Youtube comments. In our methodology, we take advantage of\navailable English data by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions to Malayalam data. We further improve the\nresults using various fine tuning strategies. Our system achieved 0.89 weighted\naverage F1 score for the test set and it ranked 5th place out of 12\nparticipants.", "published": "2020-11-01 16:52:08", "link": "http://arxiv.org/abs/2011.00559v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COOT: Cooperative Hierarchical Transformer for Video-Text Representation\n  Learning", "abstract": "Many real-world video-text tasks involve different levels of granularity,\nsuch as frames and words, clip and sentences or videos and paragraphs, each\nwith distinct semantics. In this paper, we propose a Cooperative hierarchical\nTransformer (COOT) to leverage this hierarchy information and model the\ninteractions between different levels of granularity and different modalities.\nThe method consists of three major components: an attention-aware feature\naggregation layer, which leverages the local temporal context (intra-level,\ne.g., within a clip), a contextual transformer to learn the interactions\nbetween low-level and high-level semantics (inter-level, e.g. clip-video,\nsentence-paragraph), and a cross-modal cycle-consistency loss to connect video\nand text. The resulting method compares favorably to the state of the art on\nseveral benchmarks while having few parameters. All code is available\nopen-source at https://github.com/gingsi/coot-videotext", "published": "2020-11-01 18:54:09", "link": "http://arxiv.org/abs/2011.00597v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "TransQuest: Translation Quality Estimation with Cross-lingual\n  Transformers", "abstract": "Recent years have seen big advances in the field of sentence-level quality\nestimation (QE), largely as a result of using neural-based architectures.\nHowever, the majority of these methods work only on the language pair they are\ntrained on and need retraining for new language pairs. This process can prove\ndifficult from a technical point of view and is usually computationally\nexpensive. In this paper we propose a simple QE framework based on\ncross-lingual transformers, and we use it to implement and evaluate two\ndifferent neural architectures. Our evaluation shows that the proposed methods\nachieve state-of-the-art results outperforming current open-source quality\nestimation frameworks when trained on datasets from WMT. In addition, the\nframework proves very useful in transfer learning settings, especially when\ndealing with low-resourced languages, allowing us to obtain very competitive\nresults.", "published": "2020-11-01 16:34:44", "link": "http://arxiv.org/abs/2011.01536v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepOpht: Medical Report Generation for Retinal Images via Deep Models\n  and Visual Explanation", "abstract": "In this work, we propose an AI-based method that intends to improve the\nconventional retinal disease treatment procedure and help ophthalmologists\nincrease diagnosis efficiency and accuracy. The proposed method is composed of\na deep neural networks-based (DNN-based) module, including a retinal disease\nidentifier and clinical description generator, and a DNN visual explanation\nmodule. To train and validate the effectiveness of our DNN-based module, we\npropose a large-scale retinal disease image dataset. Also, as ground truth, we\nprovide a retinal image dataset manually labeled by ophthalmologists to\nqualitatively show, the proposed AI-based method is effective. With our\nexperimental results, we show that the proposed method is quantitatively and\nqualitatively effective. Our method is capable of creating meaningful retinal\nimage descriptions and visual explanations that are clinically relevant.", "published": "2020-11-01 17:28:12", "link": "http://arxiv.org/abs/2011.00569v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Focusing Phenomena in Linear Discrete Inverse Problems in Acoustics", "abstract": "The focusing operation inherent to the linear discrete inverse problem is\nformalised. The development is given in the context of sound-field reproduction\nwhere the source strengths are the inverse solution needed to recreate a\nprescribed pressure field at discrete locations. The behaviour of the system is\nfundamentally tied to the amount of acoustic crosstalk at each control point as\na result of the focusing operation inherent to the pseudoinverse. The\nmaximisation of the crosstalk at just one point leads to linear dependence in\nthe system. On the other hand, its minimisation leads to the ideal focusing\nstate wherein the sources can selectively focus at each point, while a null is\ncreated at all other points. Two theoretical case studies are presented that\ndemonstrate ideal and super ideal focusing, wherein the latter the condition\nnumber is unitary. First, the application of binaural audio reproduction using\nan array of loudspeakers is examined and several cases of ideal focusing are\npresented. In the process, the Optimal Source Distribution is re-derived and\nshown to be a case of super ideal focusing. Secondly, the application of\nrecreating multiple sound zones is examined using a uniform linear array. The\nconditions are derived to achieve ideal focusing at control points positioned\narbitrarily in the far-field. In all cases, the ability to maintain ideal\nfocusing as a function of frequency requires proportional changes in the source\nor control point geometry.", "published": "2020-11-01 13:51:21", "link": "http://arxiv.org/abs/2011.00502v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
