{"title": "HERALD: An Annotation Efficient Method to Detect User Disengagement in\n  Social Conversations", "abstract": "Open-domain dialog systems have a user-centric goal: to provide humans with\nan engaging conversation experience. User engagement is one of the most\nimportant metrics for evaluating open-domain dialog systems, and could also be\nused as real-time feedback to benefit dialog policy learning. Existing work on\ndetecting user disengagement typically requires hand-labeling many dialog\nsamples. We propose HERALD, an efficient annotation framework that reframes the\ntraining data annotation process as a denoising problem. Specifically, instead\nof manually labeling training samples, we first use a set of labeling\nheuristics to label training samples automatically. We then denoise the weakly\nlabeled data using the Shapley algorithm. Finally, we use the denoised data to\ntrain a user engagement detector. Our experiments show that HERALD improves\nannotation efficiency significantly and achieves 86% user disengagement\ndetection accuracy in two dialog corpora.", "published": "2021-06-01 01:09:55", "link": "http://arxiv.org/abs/2106.00162v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Bias Amplification During Speed-Quality Optimization in Neural\n  Machine Translation", "abstract": "Is bias amplified when neural machine translation (NMT) models are optimized\nfor speed and evaluated on generic test sets using BLEU? We investigate\narchitectures and techniques commonly used to speed up decoding in\nTransformer-based models, such as greedy search, quantization, average\nattention networks (AANs) and shallow decoder models and show their effect on\ngendered noun translation. We construct a new gender bias test set, SimpleGEN,\nbased on gendered noun phrases in which there is a single, unambiguous, correct\nanswer. While we find minimal overall BLEU degradation as we apply speed\noptimizations, we observe that gendered noun translation performance degrades\nat a much faster rate.", "published": "2021-06-01 01:32:08", "link": "http://arxiv.org/abs/2106.00169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese\n  Adjectives", "abstract": "Gender bias in word embeddings gradually becomes a vivid research field in\nrecent years. Most studies in this field aim at measurement and debiasing\nmethods with English as the target language. This paper investigates gender\nbias in static word embeddings from a unique perspective, Chinese adjectives.\nBy training word representations with different models, the gender bias behind\nthe vectors of adjectives is assessed. Through a comparison between the\nproduced results and a human-scored data set, we demonstrate how gender bias\nencoded in word embeddings differentiates from people's attitudes.", "published": "2021-06-01 02:12:45", "link": "http://arxiv.org/abs/2106.00181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Formality Style Transfer with Context-Aware Rule Injection", "abstract": "Models pre-trained on large-scale regular text corpora often do not work well\nfor user-generated data where the language styles differ significantly from the\nmainstream text. Here we present Context-Aware Rule Injection (CARI), an\ninnovative method for formality style transfer (FST). CARI injects multiple\nrules into an end-to-end BERT-based encoder and decoder model. It learns to\nselect optimal rules based on context. The intrinsic evaluation showed that\nCARI achieved the new highest performance on the FST benchmark dataset. Our\nextrinsic evaluation showed that CARI can greatly improve the regular\npre-trained models' performance on several tweet sentiment analysis tasks.", "published": "2021-06-01 03:59:07", "link": "http://arxiv.org/abs/2106.00210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discontinuous Named Entity Recognition as Maximal Clique Discovery", "abstract": "Named entity recognition (NER) remains challenging when entity mentions can\nbe discontinuous. Existing methods break the recognition process into several\nsequential steps. In training, they predict conditioned on the golden\nintermediate results, while at inference relying on the model output of the\nprevious steps, which introduces exposure bias. To solve this problem, we first\nconstruct a segment graph for each sentence, in which each node denotes a\nsegment (a continuous entity on its own, or a part of discontinuous entities),\nand an edge links two nodes that belong to the same entity. The nodes and edges\ncan be generated respectively in one stage with a grid tagging scheme and\nlearned jointly using a novel architecture named Mac. Then discontinuous NER\ncan be reformulated as a non-parametric process of discovering maximal cliques\nin the graph and concatenating the spans in each clique. Experiments on three\nbenchmarks show that our method outperforms the state-of-the-art (SOTA)\nresults, with up to 3.5 percentage points improvement on F1, and achieves 5x\nspeedup over the SOTA model.", "published": "2021-06-01 04:13:39", "link": "http://arxiv.org/abs/2106.00218v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-aware Transformer Models for Consumer Health Question\n  Summarization", "abstract": "Searching for health information online is becoming customary for more and\nmore consumers every day, which makes the need for efficient and reliable\nquestion answering systems more pressing. An important contributor to the\nsuccess rates of these systems is their ability to fully understand the\nconsumers' questions. However, these questions are frequently longer than\nneeded and mention peripheral information that is not useful in finding\nrelevant answers. Question summarization is one of the potential solutions to\nsimplifying long and complex consumer questions before attempting to find an\nanswer. In this paper, we study the task of abstractive summarization for\nreal-world consumer health questions. We develop an abstractive question\nsummarization model that leverages the semantic interpretation of a question\nvia recognition of medical entities, which enables the generation of\ninformative summaries. Towards this, we propose multiple Cloze tasks (i.e. the\ntask of filing missing words in a given context) to identify the key medical\nentities that enforce the model to have better coverage in question-focus\nrecognition. Additionally, we infuse the decoder inputs with question-type\ninformation to generate question-type driven summaries. When evaluated on the\nMeQSum benchmark corpus, our framework outperformed the state-of-the-art method\nby 10.2 ROUGE-L points. We also conducted a manual evaluation to assess the\ncorrectness of the generated summaries.", "published": "2021-06-01 04:21:31", "link": "http://arxiv.org/abs/2106.00219v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Automatic Hate Speech Detection with Multiword Expression\n  Features", "abstract": "The task of automatically detecting hate speech in social media is gaining\nmore and more attention. Given the enormous volume of content posted daily,\nhuman monitoring of hate speech is unfeasible. In this work, we propose new\nword-level features for automatic hate speech detection (HSD): multiword\nexpressions (MWEs). MWEs are lexical units greater than a word that have\nidiomatic and compositional meanings. We propose to integrate MWE features in a\ndeep neural network-based HSD framework. Our baseline HSD system relies on\nUniversal Sentence Encoder (USE). To incorporate MWE features, we create a\nthree-branch deep neural network: one branch for USE, one for MWE categories,\nand one for MWE embeddings. We conduct experiments on two hate speech tweet\ncorpora with different MWE categories and with two types of MWE embeddings,\nword2vec and BERT. Our experiments demonstrate that the proposed HSD system\nwith MWE features significantly outperforms the baseline system in terms of\nmacro-F1.", "published": "2021-06-01 05:30:29", "link": "http://arxiv.org/abs/2106.00237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Volta at SemEval-2021 Task 9: Statement Verification and Evidence\n  Finding with Tables using TAPAS and Transfer Learning", "abstract": "Tables are widely used in various kinds of documents to present information\nconcisely. Understanding tables is a challenging problem that requires an\nunderstanding of language and table structure, along with numerical and logical\nreasoning. In this paper, we present our systems to solve Task 9 of\nSemEval-2021: Statement Verification and Evidence Finding with Tables\n(SEM-TAB-FACTS). The task consists of two subtasks: (A) Given a table and a\nstatement, predicting whether the table supports the statement and (B)\nPredicting which cells in the table provide evidence for/against the statement.\nWe fine-tune TAPAS (a model which extends BERT's architecture to capture\ntabular structure) for both the subtasks as it has shown state-of-the-art\nperformance in various table understanding tasks. In subtask A, we evaluate how\ntransfer learning and standardizing tables to have a single header row improves\nTAPAS' performance. In subtask B, we evaluate how different fine-tuning\nstrategies can improve TAPAS' performance. Our systems achieve an F1 score of\n67.34 in subtask A three-way classification, 72.89 in subtask A two-way\nclassification, and 62.95 in subtask B.", "published": "2021-06-01 06:06:29", "link": "http://arxiv.org/abs/2106.00248v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Dynamic Selection of Branch Expansion Orders for Code\n  Generation", "abstract": "Due to the great potential in facilitating software development, code\ngeneration has attracted increasing attention recently. Generally, dominant\nmodels are Seq2Tree models, which convert the input natural language\ndescription into a sequence of tree-construction actions corresponding to the\npre-order traversal of an Abstract Syntax Tree (AST). However, such a traversal\norder may not be suitable for handling all multi-branch nodes. In this paper,\nwe propose to equip the Seq2Tree model with a context-based Branch Selector,\nwhich is able to dynamically determine optimal expansion orders of branches for\nmulti-branch nodes. Particularly, since the selection of expansion orders is a\nnon-differentiable multi-step operation, we optimize the selector through\nreinforcement learning, and formulate the reward function as the difference of\nmodel losses obtained through different expansion orders. Experimental results\nand in-depth analysis on several commonly-used datasets demonstrate the\neffectiveness and generality of our approach. We have released our code at\nhttps://github.com/DeepLearnXMU/CG-RL.", "published": "2021-06-01 06:52:41", "link": "http://arxiv.org/abs/2106.00261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preview, Attend and Review: Schema-Aware Curriculum Learning for\n  Multi-Domain Dialog State Tracking", "abstract": "Existing dialog state tracking (DST) models are trained with dialog data in a\nrandom order, neglecting rich structural information in a dataset. In this\npaper, we propose to use curriculum learning (CL) to better leverage both the\ncurriculum structure and schema structure for task-oriented dialogs.\nSpecifically, we propose a model-agnostic framework called Schema-aware\nCurriculum Learning for Dialog State Tracking (SaCLog), which consists of a\npreview module that pre-trains a DST model with schema information, a\ncurriculum module that optimizes the model with CL, and a review module that\naugments mispredicted data to reinforce the CL training. We show that our\nproposed approach improves DST performance over both a transformer-based and\nRNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art\nresults on WOZ2.0 and MultiWOZ2.1.", "published": "2021-06-01 07:52:35", "link": "http://arxiv.org/abs/2106.00291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LenAtten: An Effective Length Controlling Unit For Text Summarization", "abstract": "Fixed length summarization aims at generating summaries with a preset number\nof words or characters. Most recent researches incorporate length information\nwith word embeddings as the input to the recurrent decoding unit, causing a\ncompromise between length controllability and summary quality. In this work, we\npresent an effective length controlling unit Length Attention (LenAtten) to\nbreak this trade-off. Experimental results show that LenAtten not only brings\nimprovements in length controllability and ROGUE scores but also has great\ngeneralization ability. In the task of generating a summary with the target\nlength, our model is 732 times better than the best-performing length\ncontrollable summarizer in length controllability on the CNN/Daily Mail\ndataset.", "published": "2021-06-01 08:45:41", "link": "http://arxiv.org/abs/2106.00316v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distribution Matching for Rationalization", "abstract": "The task of rationalization aims to extract pieces of input text as\nrationales to justify neural network predictions on text classification tasks.\nBy definition, rationales represent key text pieces used for prediction and\nthus should have similar classification feature distribution compared to the\noriginal input text. However, previous methods mainly focused on maximizing the\nmutual information between rationales and labels while neglecting the\nrelationship between rationales and input text. To address this issue, we\npropose a novel rationalization method that matches the distributions of\nrationales and input text in both the feature space and output space.\nEmpirically, the proposed distribution matching approach consistently\noutperforms previous methods by a large margin. Our data and code are\navailable.", "published": "2021-06-01 08:49:32", "link": "http://arxiv.org/abs/2106.00320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An In-depth Study on Internal Structure of Chinese Words", "abstract": "Unlike English letters, Chinese characters have rich and specific meanings.\nUsually, the meaning of a word can be derived from its constituent characters\nin some way. Several previous works on syntactic parsing propose to annotate\nshallow word-internal structures for better utilizing character-level\ninformation. This work proposes to model the deep internal structures of\nChinese words as dependency trees with 11 labels for distinguishing syntactic\nrelationships. First, based on newly compiled annotation guidelines, we\nmanually annotate a word-internal structure treebank (WIST) consisting of over\n30K multi-char words from Chinese Penn Treebank. To guarantee quality, each\nword is independently annotated by two annotators and inconsistencies are\nhandled by a third senior annotator. Second, we present detailed and\ninteresting analysis on WIST to reveal insights on Chinese word formation.\nThird, we propose word-internal structure parsing as a new task, and conduct\nbenchmark experiments using a competitive dependency parser. Finally, we\npresent two simple ways to encode word-internal structures, leading to\npromising gains on the sentence-level syntactic parsing task.", "published": "2021-06-01 09:09:51", "link": "http://arxiv.org/abs/2106.00334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sub-Character Tokenization for Chinese Pretrained Language Models", "abstract": "Tokenization is fundamental to pretrained language models (PLMs). Existing\ntokenization methods for Chinese PLMs typically treat each character as an\nindivisible token. However, they ignore the unique feature of the Chinese\nwriting system where additional linguistic information exists below the\ncharacter level, i.e., at the sub-character level. To utilize such information,\nwe propose sub-character (SubChar for short) tokenization. Specifically, we\nfirst encode the input text by converting each Chinese character into a short\nsequence based on its glyph or pronunciation, and then construct the vocabulary\nbased on the encoded text with sub-word segmentation. Experimental results show\nthat SubChar tokenizers have two main advantages over existing tokenizers: 1)\nThey can tokenize inputs into much shorter sequences, thus improving the\ncomputational efficiency. 2) Pronunciation-based SubChar tokenizers can encode\nChinese homophones into the same transliteration sequences and produce the same\ntokenization output, hence being robust to homophone typos. At the same time,\nmodels trained with SubChar tokenizers perform competitively on downstream\ntasks. We release our code and models at\nhttps://github.com/thunlp/SubCharTokenization to facilitate future work.", "published": "2021-06-01 11:20:02", "link": "http://arxiv.org/abs/2106.00400v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue-oriented Pre-training", "abstract": "Pre-trained language models (PrLM) has been shown powerful in enhancing a\nbroad range of downstream tasks including various dialogue related ones.\nHowever, PrLMs are usually trained on general plain text with common language\nmodel (LM) training objectives, which cannot sufficiently capture dialogue\nexclusive features due to the limitation of such training setting, so that\nthere is an immediate need to fill the gap between a specific dialogue task and\nthe LM task. As it is unlikely to collect huge dialogue data for\ndialogue-oriented pre-training, in this paper, we propose three strategies to\nsimulate the conversation features on general plain text. Our proposed method\ndiffers from existing post-training methods that it may yield a general-purpose\nPrLM and does not individualize to any detailed task while keeping the\ncapability of learning dialogue related features including speaker awareness,\ncontinuity and consistency. The resulted Dialog-PrLM is fine-tuned on three\npublic multi-turn dialogue datasets and helps achieve significant and\nconsistent improvement over the plain PrLMs.", "published": "2021-06-01 12:02:46", "link": "http://arxiv.org/abs/2106.00420v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2021 Task 1: Lexical Complexity Prediction", "abstract": "This paper presents the results and main findings of SemEval-2021 Task 1 -\nLexical Complexity Prediction. We provided participants with an augmented\nversion of the CompLex Corpus (Shardlow et al 2020). CompLex is an English\nmulti-domain corpus in which words and multi-word expressions (MWEs) were\nannotated with respect to their complexity using a five point Likert scale.\nSemEval-2021 Task 1 featured two Sub-tasks: Sub-task 1 focused on single words\nand Sub-task 2 focused on MWEs. The competition attracted 198 teams in total,\nof which 54 teams submitted official runs on the test data to Sub-task 1 and 37\nto Sub-task 2.", "published": "2021-06-01 13:22:36", "link": "http://arxiv.org/abs/2106.00473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DoT: An efficient Double Transformer for NLP tasks with tables", "abstract": "Transformer-based approaches have been successfully used to obtain\nstate-of-the-art accuracy on natural language processing (NLP) tasks with\nsemi-structured tables. These model architectures are typically deep, resulting\nin slow training and inference, especially for long inputs. To improve\nefficiency while maintaining a high accuracy, we propose a new architecture,\nDoT, a double transformer model, that decomposes the problem into two\nsub-tasks: A shallow pruning transformer that selects the top-K tokens,\nfollowed by a deep task-specific transformer that takes as input those K\ntokens. Additionally, we modify the task-specific attention to incorporate the\npruning scores. The two transformers are jointly trained by optimizing the\ntask-specific loss. We run experiments on three benchmarks, including\nentailment and question-answering. We show that for a small drop of accuracy,\nDoT improves training and inference time by at least 50%. We also show that the\npruning transformer effectively selects relevant tokens enabling the end-to-end\nmodel to maintain similar accuracy as slower baseline models. Finally, we\nanalyse the pruning and give some insight into its impact on the task model.", "published": "2021-06-01 13:33:53", "link": "http://arxiv.org/abs/2106.00479v1", "categories": ["cs.CL", "68-06", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Quantifiable Dialogue Coherence Evaluation", "abstract": "Automatic dialogue coherence evaluation has attracted increasing attention\nand is crucial for developing promising dialogue systems. However, existing\nmetrics have two major limitations: (a) they are mostly trained in a simplified\ntwo-level setting (coherent vs. incoherent), while humans give Likert-type\nmulti-level coherence scores, dubbed as \"quantifiable\"; (b) their predicted\ncoherence scores cannot align with the actual human rating standards due to the\nabsence of human guidance during training. To address these limitations, we\npropose Quantifiable Dialogue Coherence Evaluation (QuantiDCE), a novel\nframework aiming to train a quantifiable dialogue coherence metric that can\nreflect the actual human rating standards. Specifically, QuantiDCE includes two\ntraining stages, Multi-Level Ranking (MLR) pre-training and Knowledge\nDistillation (KD) fine-tuning. During MLR pre-training, a new MLR loss is\nproposed for enabling the model to learn the coarse judgement of coherence\ndegrees. Then, during KD fine-tuning, the pretrained model is further finetuned\nto learn the actual human rating standards with only very few human-annotated\ndata. To advocate the generalizability even with limited fine-tuning data, a\nnovel KD regularization is introduced to retain the knowledge learned at the\npre-training stage. Experimental results show that the model trained by\nQuantiDCE presents stronger correlations with human judgements than the other\nstate-of-the-art metrics.", "published": "2021-06-01 14:11:17", "link": "http://arxiv.org/abs/2106.00507v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpanNER: Named Entity Re-/Recognition as Span Prediction", "abstract": "Recent years have seen the paradigm shift of Named Entity Recognition (NER)\nsystems from sequence labeling to span prediction. Despite its preliminary\neffectiveness, the span prediction model's architectural bias has not been\nfully understood. In this paper, we first investigate the strengths and\nweaknesses when the span prediction model is used for named entity recognition\ncompared with the sequence labeling framework and how to further improve it,\nwhich motivates us to make complementary advantages of systems based on\ndifferent paradigms. We then reveal that span prediction, simultaneously, can\nserve as a system combiner to re-recognize named entities from different\nsystems' outputs. We experimentally implement 154 systems on 11 datasets,\ncovering three languages, comprehensive results show the effectiveness of span\nprediction models that both serve as base NER systems and system combiners. We\nmake all code and datasets available: \\url{https://github.com/neulab/spanner},\nas well as an online system demo: \\url{http://spanner.sh}. Our model also has\nbeen deployed into the ExplainaBoard platform, which allows users to flexibly\nperform a system combination of top-scoring systems in an interactive way:\n\\url{http://explainaboard.nlpedia.ai/leaderboard/task-ner/}.", "published": "2021-06-01 17:11:42", "link": "http://arxiv.org/abs/2106.00641v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit Representations of Meaning in Neural Language Models", "abstract": "Does the effectiveness of neural language models derive entirely from\naccurate modeling of surface word co-occurrence statistics, or do these models\nrepresent and reason about the world they describe? In BART and T5 transformer\nlanguage models, we identify contextual word representations that function as\nmodels of entities and situations as they evolve throughout a discourse. These\nneural representations have functional similarities to linguistic models of\ndynamic semantics: they support a linear readout of each entity's current\nproperties and relations, and can be manipulated with predictable effects on\nlanguage generation. Our results indicate that prediction in pretrained neural\nlanguage models is supported, at least in part, by dynamic representations of\nmeaning and implicit simulation of entity state, and that this behavior can be\nlearned with only text as training data. Code and data are available at\nhttps://github.com/belindal/state-probes .", "published": "2021-06-01 19:23:20", "link": "http://arxiv.org/abs/2106.00737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Higher-order Derivatives of Weighted Finite-state Machines", "abstract": "Weighted finite-state machines are a fundamental building block of NLP\nsystems. They have withstood the test of time -- from their early use in noisy\nchannel models in the 1990s up to modern-day neurally parameterized conditional\nrandom fields. This work examines the computation of higher-order derivatives\nwith respect to the normalization constant for weighted finite-state machines.\nWe provide a general algorithm for evaluating derivatives of all orders, which\nhas not been previously described in the literature. In the case of\nsecond-order derivatives, our scheme runs in the optimal $\\mathcal{O}(A^2 N^4)$\ntime where $A$ is the alphabet size and $N$ is the number of states. Our\nalgorithm is significantly faster than prior algorithms. Additionally, our\napproach leads to a significantly faster algorithm for computing second-order\nexpectations, such as covariance matrices and gradients of first-order\nexpectations.", "published": "2021-06-01 19:51:55", "link": "http://arxiv.org/abs/2106.00749v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Finding the $K$-best Non-projective Dependency Trees", "abstract": "The connection between the maximum spanning tree in a directed graph and the\nbest dependency tree of a sentence has been exploited by the NLP community.\nHowever, for many dependency parsing schemes, an important detail of this\napproach is that the spanning tree must have exactly one edge emanating from\nthe root. While work has been done to efficiently solve this problem for\nfinding the one-best dependency tree, no research has attempted to extend this\nsolution to finding the $K$-best dependency trees. This is arguably a more\nimportant extension as a larger proportion of decoded trees will not be subject\nto the root constraint of dependency trees. Indeed, we show that the rate of\nroot constraint violations increases by an average of $13$ times when decoding\nwith $K\\!=\\!50$ as opposed to $K\\!=\\!1$. In this paper, we provide a\nsimplification of the $K$-best spanning tree algorithm of Camerini et al.\n(1980). Our simplification allows us to obtain a constant time speed-up over\nthe original algorithm. Furthermore, we present a novel extension of the\nalgorithm for decoding the $K$-best dependency trees of a graph which are\nsubject to a root constraint.", "published": "2021-06-01 20:23:41", "link": "http://arxiv.org/abs/2106.00780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text\n  Generation", "abstract": "We study the task of long-form opinion text generation, which faces at least\ntwo distinct challenges. First, existing neural generation models fall short of\ncoherence, thus requiring efficient content planning. Second, diverse types of\ninformation are needed to guide the generator to cover both subjective and\nobjective content. To this end, we propose DYPLOC, a generation framework that\nconducts dynamic planning of content while generating the output based on a\nnovel design of mixed language models. To enrich the generation with diverse\ncontent, we further propose to use large pre-trained models to predict relevant\nconcepts and to generate claims. We experiment with two challenging tasks on\nnewly collected datasets: (1) argument generation with Reddit ChangeMyView, and\n(2) writing articles using New York Times' Opinion section. Automatic\nevaluation shows that our model significantly outperforms competitive\ncomparisons. Human judges further confirm that our generations are more\ncoherent with richer content.", "published": "2021-06-01 20:56:10", "link": "http://arxiv.org/abs/2106.00791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoRI: Collective Relation Integration with Data Augmentation for Open\n  Information Extraction", "abstract": "Integrating extracted knowledge from the Web to knowledge graphs (KGs) can\nfacilitate tasks like question answering. We study relation integration that\naims to align free-text relations in subject-relation-object extractions to\nrelations in a target KG. To address the challenge that free-text relations are\nambiguous, previous methods exploit neighbor entities and relations for\nadditional context. However, the predictions are made independently, which can\nbe mutually inconsistent. We propose a two-stage Collective Relation\nIntegration (CoRI) model, where the first stage independently makes candidate\npredictions, and the second stage employs a collective model that accesses all\ncandidate predictions to make globally coherent predictions. We further improve\nthe collective model with augmented data from the portion of the target KG that\nis otherwise unused. Experiment results on two datasets show that CoRI can\nsignificantly outperform the baselines, improving AUC from .677 to .748 and\nfrom .716 to .780, respectively.", "published": "2021-06-01 21:01:43", "link": "http://arxiv.org/abs/2106.00793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive\n  Summarization with Argument Mining", "abstract": "While online conversations can cover a vast amount of information in many\ndifferent formats, abstractive text summarization has primarily focused on\nmodeling solely news articles. This research gap is due, in part, to the lack\nof standardized datasets for summarizing online discussions. To address this\ngap, we design annotation protocols motivated by an\nissues--viewpoints--assertions framework to crowdsource four new datasets on\ndiverse online conversation forms of news comments, discussion forums,\ncommunity question answering forums, and email threads. We benchmark\nstate-of-the-art models on our datasets and analyze characteristics associated\nwith the data. To create a comprehensive benchmark, we also evaluate these\nmodels on widely-used conversation summarization datasets to establish strong\nbaselines in this domain. Furthermore, we incorporate argument mining through\ngraph construction to directly model the issues, viewpoints, and assertions\npresent in a conversation and filter noisy input, showing comparable or\nimproved results according to automatic and human evaluations.", "published": "2021-06-01 22:17:13", "link": "http://arxiv.org/abs/2106.00829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Test Sets with Item Response Theory", "abstract": "Recent years have seen numerous NLP datasets introduced to evaluate the\nperformance of fine-tuned models on natural language understanding tasks.\nRecent results from large pretrained models, though, show that many of these\ndatasets are largely saturated and unlikely to be able to detect further\nprogress. What kind of datasets are still effective at discriminating among\nstrong models, and what kind of datasets should we expect to be able to detect\nfuture improvements? To measure this uniformly across datasets, we draw on Item\nResponse Theory and evaluate 29 datasets using predictions from 18 pretrained\nTransformer models on individual test examples. We find that Quoref, HellaSwag,\nand MC-TACO are best suited for distinguishing among state-of-the-art models,\nwhile SNLI, MNLI, and CommitmentBank seem to be saturated for current strong\nmodels. We also observe span selection task format, which is used for QA\ndatasets like QAMR or SQuAD2.0, is effective in differentiating between strong\nand weak models.", "published": "2021-06-01 22:33:53", "link": "http://arxiv.org/abs/2106.00840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Neural Question Answering Models via Graph-Enriched\n  Document Representations", "abstract": "As the computational footprint of modern NLP systems grows, it becomes\nincreasingly important to arrive at more efficient models. We show that by\nemploying graph convolutional document representation, we can arrive at a\nquestion answering system that performs comparably to, and in some cases\nexceeds the SOTA solutions, while using less than 5\\% of their resources in\nterms of trainable parameters. As it currently stands, a major issue in\napplying GCNs to NLP is document representation. In this paper, we show that a\nGCN enriched document representation greatly improves the results seen in\nHotPotQA, even when using a trivial topology. Our model (gQA), performs\nadmirably when compared to the current SOTA, and requires little to no\npreprocessing. In Shao et al. 2020, the authors suggest that graph networks are\nnot necessary for good performance in multi-hop QA. In this paper, we suggest\nthat large language models are not necessary for good performance by showing a\nna\\\"{i}ve implementation of a GCN performs comparably to SoTA models based on\npretrained language models.", "published": "2021-06-01 23:24:51", "link": "http://arxiv.org/abs/2106.00851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Claim Matching Beyond English to Scale Global Fact-Checking", "abstract": "Manual fact-checking does not scale well to serve the needs of the internet.\nThis issue is further compounded in non-English contexts. In this paper, we\ndiscuss claim matching as a possible solution to scale fact-checking. We define\nclaim matching as the task of identifying pairs of textual messages containing\nclaims that can be served with one fact-check. We construct a novel dataset of\nWhatsApp tipline and public group messages alongside fact-checked claims that\nare first annotated for containing \"claim-like statements\" and then matched\nwith potentially similar items and annotated for claim matching. Our dataset\ncontains content in high-resource (English, Hindi) and lower-resource (Bengali,\nMalayalam, Tamil) languages. We train our own embedding model using knowledge\ndistillation and a high-quality \"teacher\" model in order to address the\nimbalance in embedding quality between the low- and high-resource languages in\nour dataset. We provide evaluations on the performance of our solution and\ncompare with baselines and existing state-of-the-art multilingual embedding\nmodels, namely LASER and LaBSE. We demonstrate that our performance exceeds\nLASER and LaBSE in all settings. We release our annotated datasets, codebooks,\nand trained embedding model to allow for further research.", "published": "2021-06-01 23:28:05", "link": "http://arxiv.org/abs/2106.00853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Integrated Gradients and Constituency Parse Trees to explain\n  Linguistic Acceptability learnt by BERT", "abstract": "Linguistic Acceptability is the task of determining whether a sentence is\ngrammatical or ungrammatical. It has applications in several use cases like\nQuestion-Answering, Natural Language Generation, Neural Machine Translation,\nwhere grammatical correctness is crucial. In this paper we aim to understand\nthe decision-making process of BERT (Devlin et al., 2019) in distinguishing\nbetween Linguistically Acceptable sentences (LA) and Linguistically\nUnacceptable sentences (LUA). We leverage Layer Integrated Gradients\nAttribution Scores (LIG) to explain the Linguistic Acceptability criteria that\nare learnt by BERT on the Corpus of Linguistic Acceptability (CoLA) (Warstadt\net al., 2018) benchmark dataset. Our experiments on 5 categories of sentences\nlead to the following interesting findings: 1) LIG for LA are significantly\nsmaller in comparison to LUA, 2) There are specific subtrees of the\nConstituency Parse Tree (CPT) for LA and LUA which contribute larger LIG, 3)\nAcross the different categories of sentences we observed around 88% to 100% of\nthe Correctly classified sentences had positive LIG, indicating a strong\npositive relationship to the prediction confidence of the model, and 4) Around\n43% of the Misclassified sentences had negative LIG, which we believe can\nbecome correctly classified sentences if the LIG are parameterized in the loss\nfunction of the model.", "published": "2021-06-01 15:17:45", "link": "http://arxiv.org/abs/2106.07349v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D\n  World", "abstract": "We propose PIGLeT: a model that learns physical commonsense knowledge through\ninteraction, and then uses this knowledge to ground language. We factorize\nPIGLeT into a physical dynamics model, and a separate language model. Our\ndynamics model learns not just what objects are but also what they do: glass\ncups break when thrown, plastic ones don't. We then use it as the interface to\nour language model, giving us a unified model of linguistic form and grounded\nmeaning. PIGLeT can read a sentence, simulate neurally what might happen next,\nand then communicate that result through a literal symbolic representation, or\nnatural language.\n  Experimental results show that our model effectively learns world dynamics,\nalong with how to communicate them. It is able to correctly forecast \"what\nhappens next\" given an English sentence over 80% of the time, outperforming a\n100x larger, text-to-text approach by over 10%. Likewise, its natural language\nsummaries of physical interactions are also judged by humans as more accurate\nthan LM alternatives. We present comprehensive analysis showing room for future\nwork.", "published": "2021-06-01 02:32:12", "link": "http://arxiv.org/abs/2106.00188v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Iterative Hierarchical Attention for Answering Complex Questions over\n  Long Documents", "abstract": "We propose a new model, DocHopper, that iteratively attends to different\nparts of long, hierarchically structured documents to answer complex questions.\nSimilar to multi-hop question-answering (QA) systems, at each step, DocHopper\nuses a query $q$ to attend to information from a document, combines this\n``retrieved'' information with $q$ to produce the next query. However, in\ncontrast to most previous multi-hop QA systems, DocHopper is able to\n``retrieve'' either short passages or long sections of the document, thus\nemulating a multi-step process of ``navigating'' through a long document to\nanswer a question. To enable this novel behavior, DocHopper does not combine\ndocument information with $q$ by concatenating text to the text of $q$, but by\ncombining a compact neural representation of $q$ with a compact neural\nrepresentation of a hierarchical part of the document, which can potentially be\nquite large. We experiment with DocHopper on four different QA tasks that\nrequire reading long and complex documents to answer multi-hop questions, and\nshow that DocHopper achieves state-of-the-art results on three of the datasets.\nAdditionally, DocHopper is efficient at inference time, being 3--10 times\nfaster than the baselines.", "published": "2021-06-01 03:13:35", "link": "http://arxiv.org/abs/2106.00200v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Volta at SemEval-2021 Task 6: Towards Detecting Persuasive Texts and\n  Images using Textual and Multimodal Ensemble", "abstract": "Memes are one of the most popular types of content used to spread information\nonline. They can influence a large number of people through rhetorical and\npsychological techniques. The task, Detection of Persuasion Techniques in Texts\nand Images, is to detect these persuasive techniques in memes. It consists of\nthree subtasks: (A) Multi-label classification using textual content, (B)\nMulti-label classification and span identification using textual content, and\n(C) Multi-label classification using visual and textual content. In this paper,\nwe propose a transfer learning approach to fine-tune BERT-based models in\ndifferent modalities. We also explore the effectiveness of ensembles of models\ntrained in different modalities. We achieve an F1-score of 57.0, 48.2, and 52.1\nin the corresponding subtasks.", "published": "2021-06-01 05:41:03", "link": "http://arxiv.org/abs/2106.00240v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Reinforced Iterative Knowledge Distillation for Cross-Lingual Named\n  Entity Recognition", "abstract": "Named entity recognition (NER) is a fundamental component in many\napplications, such as Web Search and Voice Assistants. Although deep neural\nnetworks greatly improve the performance of NER, due to the requirement of\nlarge amounts of training data, deep neural networks can hardly scale out to\nmany languages in an industry setting. To tackle this challenge, cross-lingual\nNER transfers knowledge from a rich-resource language to languages with low\nresources through pre-trained multilingual language models. Instead of using\ntraining data in target languages, cross-lingual NER has to rely on only\ntraining data in source languages, and optionally adds the translated training\ndata derived from source languages. However, the existing cross-lingual NER\nmethods do not make good use of rich unlabeled data in target languages, which\nis relatively easy to collect in industry applications. To address the\nopportunities and challenges, in this paper we describe our novel practice in\nMicrosoft to leverage such large amounts of unlabeled data in target languages\nin real production settings. To effectively extract weak supervision signals\nfrom the unlabeled data, we develop a novel approach based on the ideas of\nsemi-supervised learning and reinforcement learning. The empirical study on\nthree benchmark data sets verifies that our approach establishes the new\nstate-of-the-art performance with clear edges. Now, the NER techniques reported\nin this paper are on their way to become a fundamental component for Web\nranking, Entity Pane, Answers Triggering, and Question Answering in the\nMicrosoft Bing search engine. Moreover, our techniques will also serve as part\nof the Spoken Language Understanding module for a commercial voice assistant.\nWe plan to open source the code of the prototype framework after deployment.", "published": "2021-06-01 05:46:22", "link": "http://arxiv.org/abs/2106.00241v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA\n  Models", "abstract": "Benefiting from large-scale pre-training, we have witnessed significant\nperformance boost on the popular Visual Question Answering (VQA) task. Despite\nrapid progress, it remains unclear whether these state-of-the-art (SOTA) models\nare robust when encountering examples in the wild. To study this, we introduce\nAdversarial VQA, a new large-scale VQA benchmark, collected iteratively via an\nadversarial human-and-model-in-the-loop procedure. Through this new benchmark,\nwe discover several interesting findings. (i) Surprisingly, we find that during\ndataset collection, non-expert annotators can easily attack SOTA VQA models\nsuccessfully. (ii) Both large-scale pre-trained models and adversarial training\nmethods achieve far worse performance on the new benchmark than over standard\nVQA v2 dataset, revealing the fragility of these models while demonstrating the\neffectiveness of our adversarial dataset. (iii) When used for data\naugmentation, our dataset can effectively boost model performance on other\nrobust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light\non robustness study in the community and serve as a valuable benchmark for\nfuture work.", "published": "2021-06-01 05:54:41", "link": "http://arxiv.org/abs/2106.00245v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags", "abstract": "Multimodal Machine Translation (MMT) enriches the source text with visual\ninformation for translation. It has gained popularity in recent years, and\nseveral pipelines have been proposed in the same direction. Yet, the task lacks\nquality datasets to illustrate the contribution of visual modality in the\ntranslation systems. In this paper, we propose our system under the team name\nVolta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We\nalso participate in the textual-only subtask of the same language pair for\nwhich we use mBART, a pretrained multilingual sequence-to-sequence model. For\nmultimodal translation, we propose to enhance the textual input by bringing the\nvisual information to a textual domain by extracting object tags from the\nimage. We also explore the robustness of our system by systematically degrading\nthe source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test\nset and challenge set of the multimodal task.", "published": "2021-06-01 06:19:29", "link": "http://arxiv.org/abs/2106.00250v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Coarse to Fine Question Answering System based on Reinforcement\n  Learning", "abstract": "In this paper, we present a coarse to fine question answering (CFQA) system\nbased on reinforcement learning which can efficiently processes documents with\ndifferent lengths by choosing appropriate actions. The system is designed using\nan actor-critic based deep reinforcement learning model to achieve multi-step\nquestion answering. Compared to previous QA models targeting on datasets mainly\ncontaining either short or long documents, our multi-step coarse to fine model\ntakes the merits from multiple system modules, which can handle both short and\nlong documents. The system hence obtains a much better accuracy and faster\ntrainings speed compared to the current state-of-the-art models. We test our\nmodel on four QA datasets, WIKEREADING, WIKIREADING LONG, CNN and SQuAD, and\ndemonstrate 1.3$\\%$-1.7$\\%$ accuracy improvements with 1.5x-3.4x training\nspeed-ups in comparison to the baselines using state-of-the-art models.", "published": "2021-06-01 06:41:48", "link": "http://arxiv.org/abs/2106.00257v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Replicating and Extending \"Because Their Treebanks Leak\": Graph\n  Isomorphism, Covariants, and Parser Performance", "abstract": "S{\\o}gaard (2020) obtained results suggesting the fraction of trees occurring\nin the test data isomorphic to trees in the training set accounts for a\nnon-trivial variation in parser performance. Similar to other statistical\nanalyses in NLP, the results were based on evaluating linear regressions.\nHowever, the study had methodological issues and was undertaken using a small\nsample size leading to unreliable results. We present a replication study in\nwhich we also bin sentences by length and find that only a small subset of\nsentences vary in performance with respect to graph isomorphism. Further, the\ncorrelation observed between parser performance and graph isomorphism in the\nwild disappears when controlling for covariants. However, in a controlled\nexperiment, where covariants are kept fixed, we do observe a strong\ncorrelation. We suggest that conclusions drawn from statistical analyses like\nthis need to be tempered and that controlled experiments can complement them by\nmore readily teasing factors apart.", "published": "2021-06-01 10:00:46", "link": "http://arxiv.org/abs/2106.00352v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KGPool: Dynamic Knowledge Graph Context Selection for Relation\n  Extraction", "abstract": "We present a novel method for relation extraction (RE) from a single\nsentence, mapping the sentence and two given entities to a canonical fact in a\nknowledge graph (KG). Especially in this presumed sentential RE setting, the\ncontext of a single sentence is often sparse. This paper introduces the KGPool\nmethod to address this sparsity, dynamically expanding the context with\nadditional facts from the KG. It learns the representation of these facts\n(entity alias, entity descriptions, etc.) using neural methods, supplementing\nthe sentential context. Unlike existing methods that statically use all\nexpanded facts, KGPool conditions this expansion on the sentence. We study the\nefficacy of KGPool by evaluating it with different neural models and KGs\n(Wikidata and NYT Freebase). Our experimental evaluation on standard datasets\nshows that by feeding the KGPool representation into a Graph Neural Network,\nthe overall method is significantly more accurate than state-of-the-art\nmethods.", "published": "2021-06-01 13:12:24", "link": "http://arxiv.org/abs/2106.00459v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VILA: Improving Structured Content Extraction from Scientific PDFs Using\n  Visual Layout Groups", "abstract": "Accurately extracting structured content from PDFs is a critical first step\nfor NLP over scientific papers. Recent work has improved extraction accuracy by\nincorporating elementary layout information, e.g., each token's 2D position on\nthe page, into language model pretraining. We introduce new methods that\nexplicitly model VIsual LAyout (VILA) groups, i.e., text lines or text blocks,\nto further improve performance. In our I-VILA approach, we show that simply\ninserting special tokens denoting layout group boundaries into model inputs can\nlead to a 1.9% Macro F1 improvement in token classification. In the H-VILA\napproach, we show that hierarchical encoding of layout-groups can result in\nup-to 47% inference time reduction with less than 0.8% Macro F1 loss. Unlike\nprior layout-aware approaches, our methods do not require expensive additional\npretraining, only fine-tuning, which we show can reduce training cost by up to\n95%. Experiments are conducted on a newly curated evaluation suite, S2-VLUE,\nthat unifies existing automatically-labeled datasets and includes a new dataset\nof manual annotations covering diverse papers from 19 scientific disciplines.\nPre-trained weights, benchmark datasets, and source code are available at\nhttps://github.com/allenai/VILA.", "published": "2021-06-01 17:59:00", "link": "http://arxiv.org/abs/2106.00676v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Is it a click bait? Let's predict using Machine Learning", "abstract": "In this era of digitisation, news reader tend to read news online. This is\nbecause, online media instantly provides access to a wide variety of content.\nThus, people don't have to wait for tomorrow's newspaper to know what's\nhappening today. Along with these virtues, online news have some vices as well.\nOne such vice is presence of social media posts (tweets) relating to news\narticles whose sole purpose is to draw attention of the users rather than\ndirecting them to read the actual content. Such posts are referred to as\nclickbaits. The objective of this project is to develop a system which would be\ncapable of predicting how likely are the social media posts (tweets) relating\nto new articles tend to be clickbait.", "published": "2021-06-01 08:07:28", "link": "http://arxiv.org/abs/2106.07348v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "THG: Transformer with Hyperbolic Geometry", "abstract": "Transformer model architectures have become an indispensable staple in deep\nlearning lately for their effectiveness across a range of tasks. Recently, a\nsurge of \"X-former\" models have been proposed which improve upon the original\nTransformer architecture. However, most of these variants make changes only\naround the quadratic time and memory complexity of self-attention, i.e. the dot\nproduct between the query and the key. What's more, they are calculate solely\nin Euclidean space. In this work, we propose a novel Transformer with\nHyperbolic Geometry (THG) model, which take the advantage of both Euclidean\nspace and Hyperbolic space. THG makes improvements in linear transformations of\nself-attention, which are applied on the input sequence to get the query and\nthe key, with the proposed hyperbolic linear. Extensive experiments on sequence\nlabeling task, machine reading comprehension task and classification task\ndemonstrate the effectiveness and generalizability of our model. It also\ndemonstrates THG could alleviate overfitting.", "published": "2021-06-01 14:09:33", "link": "http://arxiv.org/abs/2106.07350v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Speech Translation with Unified Transformer: Huawei Noah's\n  Ark Lab at IWSLT 2021", "abstract": "This paper describes the system submitted to the IWSLT 2021 Multilingual\nSpeech Translation (MultiST) task from Huawei Noah's Ark Lab. We use a unified\ntransformer architecture for our MultiST model, so that the data from different\nmodalities (i.e., speech and text) and different tasks (i.e., Speech\nRecognition, Machine Translation, and Speech Translation) can be exploited to\nenhance the model's ability. Specifically, speech and text inputs are firstly\nfed to different feature extractors to extract acoustic and textual features,\nrespectively. Then, these features are processed by a shared encoder--decoder\narchitecture. We apply several training techniques to improve the performance,\nincluding multi-task learning, task-level curriculum learning, data\naugmentation, etc. Our final system achieves significantly better results than\nbilingual baselines on supervised language pairs and yields reasonable results\non zero-shot language pairs.", "published": "2021-06-01 02:50:49", "link": "http://arxiv.org/abs/2106.00197v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Nora: The Well-Being Coach", "abstract": "The current pandemic has forced people globally to remain in isolation and\npractice social distancing, which creates the need for a system to combat the\nresulting loneliness and negative emotions. In this paper we propose Nora, a\nvirtual coaching platform designed to utilize natural language understanding in\nits dialogue system and suggest other recommendations based on user\ninteractions. It is intended to provide assistance and companionship to people\nundergoing self-quarantine or work-from-home routines. Nora helps users gauge\ntheir well-being by detecting and recording the user's emotion, sentiment, and\nstress. Nora also recommends various workout, meditation, or yoga exercises to\nusers in support of developing a healthy daily routine. In addition, we provide\na social community inside Nora, where users can connect and share their\nexperiences with others undergoing a similar isolation procedure. Nora can be\naccessed from anywhere via a web link and has support for both English and\nMandarin.", "published": "2021-06-01 11:42:07", "link": "http://arxiv.org/abs/2106.00410v1", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CIDER: Commonsense Inference for Dialogue Explanation and Reasoning", "abstract": "Commonsense inference to understand and explain human language is a\nfundamental research problem in natural language processing. Explaining human\nconversations poses a great challenge as it requires contextual understanding,\nplanning, inference, and several aspects of reasoning including causal,\ntemporal, and commonsense reasoning. In this work, we introduce CIDER -- a\nmanually curated dataset that contains dyadic dialogue explanations in the form\nof implicit and explicit knowledge triplets inferred using contextual\ncommonsense inference. Extracting such rich explanations from conversations can\nbe conducive to improving several downstream applications. The annotated\ntriplets are categorized by the type of commonsense knowledge present (e.g.,\ncausal, conditional, temporal). We set up three different tasks conditioned on\nthe annotated dataset: Dialogue-level Natural Language Inference, Span\nExtraction, and Multi-choice Span Selection. Baseline results obtained with\ntransformer-based models reveal that the tasks are difficult, paving the way\nfor promising future research. The dataset and the baseline implementations are\npublicly available at https://cider-task.github.io/cider/.", "published": "2021-06-01 14:14:46", "link": "http://arxiv.org/abs/2106.00510v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NewsEmbed: Modeling News through Pre-trained Document Representations", "abstract": "Effectively modeling text-rich fresh content such as news articles at\ndocument-level is a challenging problem. To ensure a content-based model\ngeneralize well to a broad range of applications, it is critical to have a\ntraining dataset that is large beyond the scale of human labels while achieving\ndesired quality. In this work, we address those two challenges by proposing a\nnovel approach to mine semantically-relevant fresh documents, and their topic\nlabels, with little human supervision. Meanwhile, we design a multitask model\ncalled NewsEmbed that alternatively trains a contrastive learning with a\nmulti-label classification to derive a universal document encoder. We show that\nthe proposed approach can provide billions of high quality organic training\nexamples and can be naturally extended to multilingual setting where texts in\ndifferent languages are encoded in the same semantic space. We experimentally\ndemonstrate NewsEmbed's competitive performance across multiple natural\nlanguage understanding tasks, both supervised and unsupervised.", "published": "2021-06-01 15:59:40", "link": "http://arxiv.org/abs/2106.00590v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Validating GAN-BioBERT: A Methodology For Assessing Reporting Trends In\n  Clinical Trials", "abstract": "In the past decade, there has been much discussion about the issue of biased\nreporting in clinical research. Despite this attention, there have been limited\ntools developed for the systematic assessment of qualitative statements made in\nclinical research, with most studies assessing qualitative statements relying\non the use of manual expert raters, which limits their size. Also, previous\nattempts to develop larger scale tools, such as those using natural language\nprocessing, were limited by both their accuracy and the number of categories\nused for the classification of their findings. With these limitations in mind,\nthis study's goal was to develop a classification algorithm that was both\nsuitably accurate and finely grained to be applied on a large scale for\nassessing the qualitative sentiment expressed in clinical trial abstracts.\nAdditionally, this study seeks to compare the performance of the proposed\nalgorithm, GAN-BioBERT, to previous studies as well as to expert manual rating\nof clinical trial abstracts. This study develops a three-class sentiment\nclassification algorithm for clinical trial abstracts using a semi-supervised\nnatural language process model based on the Bidirectional Encoder\nRepresentation from Transformers (BERT) model, from a series of clinical trial\nabstracts annotated by a group of experts in academic medicine. Results: The\nuse of this algorithm was found to have a classification accuracy of 91.3%,\nwith a macro F1-Score of 0.92, which is a significant improvement in accuracy\nwhen compared to previous methods and expert ratings, while also making the\nsentiment classification finer grained than previous studies. The proposed\nalgorithm, GAN-BioBERT, is a suitable classification model for the large-scale\nassessment of qualitative statements in clinical trial literature, providing an\naccurate, reproducible tool for the large-scale study of clinical publication\ntrends.", "published": "2021-06-01 17:51:54", "link": "http://arxiv.org/abs/2106.00665v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Part of Speech and Universal Dependency effects on English Arabic\n  Machine Translation", "abstract": "In this research paper, I will elaborate on a method to evaluate machine\ntranslation models based on their performance on underlying syntactical\nphenomena between English and Arabic languages. This method is especially\nimportant as such \"neural\" and \"machine learning\" are hard to fine-tune and\nchange. Thus, finding a way to evaluate them easily and diversely would greatly\nhelp the task of bettering them.", "published": "2021-06-01 19:48:23", "link": "http://arxiv.org/abs/2106.00745v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Out-of-Distribution Problem in Explainability and Search Methods for\n  Feature Importance Explanations", "abstract": "Feature importance (FI) estimates are a popular form of explanation, and they\nare commonly created and evaluated by computing the change in model confidence\ncaused by removing certain input features at test time. For example, in the\nstandard Sufficiency metric, only the top-k most important tokens are kept. In\nthis paper, we study several under-explored dimensions of FI explanations,\nproviding conceptual and empirical improvements for this form of explanation.\nFirst, we advance a new argument for why it can be problematic to remove\nfeatures from an input when creating or evaluating explanations: the fact that\nthese counterfactual inputs are out-of-distribution (OOD) to models implies\nthat the resulting explanations are socially misaligned. The crux of the\nproblem is that the model prior and random weight initialization influence the\nexplanations (and explanation metrics) in unintended ways. To resolve this\nissue, we propose a simple alteration to the model training process, which\nresults in more socially aligned explanations and metrics. Second, we compare\namong five approaches for removing features from model inputs. We find that\nsome methods produce more OOD counterfactuals than others, and we make\nrecommendations for selecting a feature-replacement function. Finally, we\nintroduce four search-based methods for identifying FI explanations and compare\nthem to strong baselines, including LIME, Anchors, and Integrated Gradients.\nThrough experiments with six diverse text classification datasets, we find that\nthe only method that consistently outperforms random search is a Parallel Local\nSearch (PLS) that we introduce. Improvements over the second-best method are as\nlarge as 5.4 points for Sufficiency and 17 points for Comprehensiveness. All\nsupporting code for experiments in this paper is publicly available at\nhttps://github.com/peterbhase/ExplanationSearch.", "published": "2021-06-01 20:36:48", "link": "http://arxiv.org/abs/2106.00786v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "What Ingredients Make for an Effective Crowdsourcing Protocol for\n  Difficult NLU Data Collection Tasks?", "abstract": "Crowdsourcing is widely used to create data for common natural language\nunderstanding tasks. Despite the importance of these datasets for measuring and\nrefining model understanding of language, there has been little focus on the\ncrowdsourcing methods used for collecting the datasets. In this paper, we\ncompare the efficacy of interventions that have been proposed in prior work as\nways of improving data quality. We use multiple-choice question answering as a\ntestbed and run a randomized trial by assigning crowdworkers to write questions\nunder one of four different data collection protocols. We find that asking\nworkers to write explanations for their examples is an ineffective stand-alone\nstrategy for boosting NLU example difficulty. However, we find that training\ncrowdworkers, and then using an iterative process of collecting data, sending\nfeedback, and qualifying workers based on expert judgments is an effective\nmeans of collecting challenging data. But using crowdsourced, instead of expert\njudgments, to qualify workers and send feedback does not prove to be effective.\nWe observe that the data from the iterative protocol with expert assessments is\nmore challenging by several measures. Notably, the human--model gap on the\nunanimous agreement portion of this data is, on average, twice as large as the\ngap for the baseline protocol data.", "published": "2021-06-01 21:05:52", "link": "http://arxiv.org/abs/2106.00794v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Supervised Speech Representation Learning for Parkinson's Disease\n  Classification", "abstract": "Recently proposed automatic pathological speech classification techniques use\nunsupervised auto-encoders to obtain a high-level abstract representation of\nspeech. Since these representations are learned based on reconstructing the\ninput, there is no guarantee that they are robust to pathology-unrelated cues\nsuch as speaker identity information. Further, these representations are not\nnecessarily discriminative for pathology detection. In this paper, we exploit\nsupervised auto-encoders to extract robust and discriminative speech\nrepresentations for Parkinson's disease classification. To reduce the influence\nof speaker variabilities unrelated to pathology, we propose to obtain speaker\nidentity-invariant representations by adversarial training of an auto-encoder\nand a speaker identification task. To obtain a discriminative representation,\nwe propose to jointly train an auto-encoder and a pathological speech\nclassifier. Experimental results on a Spanish database show that the proposed\nsupervised representation learning methods yield more robust and discriminative\nrepresentations for automatically classifying Parkinson's disease speech,\noutperforming the baseline unsupervised representation learning system.", "published": "2021-06-01 14:48:08", "link": "http://arxiv.org/abs/2106.00531v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Exotic Counterpoint Compositions", "abstract": "In this paper, first musical compositions are presented, which are created\nusing the mathematical counterpoint theory of Guerino Mazzola and his\ncollaborators. These compositions also use the RUBATO(R) software's components\nfor counterpoint constructions. The present work aims at opening new \"exotic\"\ndirections of contrapuntal composition in non-Fuxian worlds. The authors would\nlike to receive first impressions about these compositions, which are available\nas scores and audio files.", "published": "2021-06-01 21:17:53", "link": "http://arxiv.org/abs/2106.00806v1", "categories": ["cs.SD", "eess.AS", "00A65, 13P99"], "primary_category": "cs.SD"}
{"title": "Sparse, Efficient, and Semantic Mixture Invariant Training: Taming\n  In-the-Wild Unsupervised Sound Separation", "abstract": "Supervised neural network training has led to significant progress on\nsingle-channel sound separation. This approach relies on ground truth isolated\nsources, which precludes scaling to widely available mixture data and limits\nprogress on open-domain tasks. The recent mixture invariant training (MixIT)\nmethod enables training on in-the-wild data; however, it suffers from two\noutstanding problems. First, it produces models which tend to over-separate,\nproducing more output sources than are present in the input. Second, the\nexponential computational complexity of the MixIT loss limits the number of\nfeasible output sources. In this paper we address both issues. To combat\nover-separation we introduce new losses: sparsity losses that favor fewer\noutput sources and a covariance loss that discourages correlated outputs. We\nalso experiment with a semantic classification loss by predicting weak class\nlabels for each mixture. To handle larger numbers of sources, we introduce an\nefficient approximation using a fast least-squares solution, projected onto the\nMixIT constraint set. Our experiments show that the proposed losses curtail\nover-separation and improve overall performance. The best performance is\nachieved using larger numbers of output sources, enabled by our efficient MixIT\nloss, combined with sparsity losses to prevent over-separation. On the FUSS\ntest set, we achieve over 13 dB in multi-source SI-SNR improvement, while\nboosting single-source reconstruction SI-SNR by over 17 dB.", "published": "2021-06-01 22:58:47", "link": "http://arxiv.org/abs/2106.00847v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Neural Acoustic Echo Canceller Optimized Using An Automatic Speech\n  Recognizer And Large Scale Synthetic Data", "abstract": "We consider the problem of recognizing speech utterances spoken to a device\nwhich is generating a known sound waveform; for example, recognizing queries\nissued to a digital assistant which is generating responses to previous user\ninputs. Previous work has proposed building acoustic echo cancellation (AEC)\nmodels for this task that optimize speech enhancement metrics using both neural\nnetwork as well as signal processing approaches.\n  Since our goal is to recognize the input speech, we consider enhancements\nwhich improve word error rates (WERs) when the predicted speech signal is\npassed to an automatic speech recognition (ASR) model. First, we augment the\nloss function with a term that produces outputs useful to a pre-trained ASR\nmodel and show that this augmented loss function improves WER metrics. Second,\nwe demonstrate that augmenting our training dataset of real world examples with\na large synthetic dataset improves performance. Crucially, applying SpecAugment\nstyle masks to the reference channel during training aids the model in adapting\nfrom synthetic to real domains. In experimental evaluations, we find the\nproposed approaches improve performance, on average, by 57% over a signal\nprocessing baseline and 45% over the neural AEC model without the proposed\nchanges.", "published": "2021-06-01 23:39:08", "link": "http://arxiv.org/abs/2106.00856v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dual Normalization Multitasking for Audio-Visual Sounding Object\n  Localization", "abstract": "Although several research works have been reported on audio-visual sound\nsource localization in unconstrained videos, no datasets and metrics have been\nproposed in the literature to quantitatively evaluate its performance. Defining\nthe ground truth for sound source localization is difficult, because the\nlocation where the sound is produced is not limited to the range of the source\nobject, but the vibrations propagate and spread through the surrounding\nobjects. Therefore we propose a new concept, Sounding Object, to reduce the\nambiguity of the visual location of sound, making it possible to annotate the\nlocation of the wide range of sound sources. With newly proposed metrics for\nquantitative evaluation, we formulate the problem of Audio-Visual Sounding\nObject Localization (AVSOL). We also created the evaluation dataset (AVSOL-E\ndataset) by manually annotating the test set of well-known Audio-Visual Event\n(AVE) dataset. To tackle this new AVSOL problem, we propose a novel multitask\ntraining strategy and architecture called Dual Normalization Multitasking\n(DNM), which aggregates the Audio-Visual Correspondence (AVC) task and the\nclassification task for video events into a single audio-visual similarity map.\nBy efficiently utilize both supervisions by DNM, our proposed architecture\nsignificantly outperforms the baseline methods.", "published": "2021-06-01 02:02:52", "link": "http://arxiv.org/abs/2106.00180v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Improving the Adversarial Robustness for Speaker Verification by\n  Self-Supervised Learning", "abstract": "Previous works have shown that automatic speaker verification (ASV) is\nseriously vulnerable to malicious spoofing attacks, such as replay, synthetic\nspeech, and recently emerged adversarial attacks. Great efforts have been\ndedicated to defending ASV against replay and synthetic speech; however, only a\nfew approaches have been explored to deal with adversarial attacks. All the\nexisting approaches to tackle adversarial attacks for ASV require the knowledge\nfor adversarial samples generation, but it is impractical for defenders to know\nthe exact attack algorithms that are applied by the in-the-wild attackers. This\nwork is among the first to perform adversarial defense for ASV without knowing\nthe specific attack algorithms. Inspired by self-supervised learning models\n(SSLMs) that possess the merits of alleviating the superficial noise in the\ninputs and reconstructing clean samples from the interrupted ones, this work\nregards adversarial perturbations as one kind of noise and conducts adversarial\ndefense for ASV by SSLMs. Specifically, we propose to perform adversarial\ndefense from two perspectives: 1) adversarial perturbation purification and 2)\nadversarial perturbation detection. Experimental results show that our\ndetection module effectively shields the ASV by detecting adversarial samples\nwith an accuracy of around 80%. Moreover, since there is no common metric for\nevaluating the adversarial defense performance for ASV, this work also\nformalizes evaluation metrics for adversarial defense considering both\npurification and detection based approaches into account. We sincerely\nencourage future works to benchmark their approaches based on the proposed\nevaluation framework.", "published": "2021-06-01 07:10:54", "link": "http://arxiv.org/abs/2106.00273v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Omnizart: A General Toolbox for Automatic Music Transcription", "abstract": "We present and release Omnizart, a new Python library that provides a\nstreamlined solution to automatic music transcription (AMT). Omnizart\nencompasses modules that construct the life-cycle of deep learning-based AMT,\nand is designed for ease of use with a compact command-line interface. To the\nbest of our knowledge, Omnizart is the first transcription toolkit which offers\nmodels covering a wide class of instruments ranging from solo, instrument\nensembles, percussion instruments to vocal, as well as models for chord\nrecognition and beat/downbeat tracking, two music information retrieval (MIR)\ntasks highly related to AMT.", "published": "2021-06-01 14:00:14", "link": "http://arxiv.org/abs/2106.00497v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-modal Point-of-Care Diagnostics for COVID-19 Based On Acoustics\n  and Symptoms", "abstract": "The research direction of identifying acoustic bio-markers of respiratory\ndiseases has received renewed interest following the onset of COVID-19\npandemic. In this paper, we design an approach to COVID-19 diagnostic using\ncrowd-sourced multi-modal data. The data resource, consisting of acoustic\nsignals like cough, breathing, and speech signals, along with the data of\nsymptoms, are recorded using a web-application over a period of ten months. We\ninvestigate the use of statistical descriptors of simple time-frequency\nfeatures for acoustic signals and binary features for the presence of symptoms.\nUnlike previous works, we primarily focus on the application of simple linear\nclassifiers like logistic regression and support vector machines for acoustic\ndata while decision tree models are employed on the symptoms data. We show that\na multi-modal integration of acoustics and symptoms classifiers achieves an\narea-under-curve (AUC) of 92.40, a significant improvement over any individual\nmodality. Several ablation experiments are also provided which highlight the\nacoustic and symptom dimensions that are important for the task of COVID-19\ndiagnostics.", "published": "2021-06-01 17:10:07", "link": "http://arxiv.org/abs/2106.00639v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Online Detection of Vibration Anomalies Using Balanced Spiking Neural\n  Networks", "abstract": "Vibration patterns yield valuable information about the health state of a\nrunning machine, which is commonly exploited in predictive maintenance tasks\nfor large industrial systems. However, the overhead, in terms of size,\ncomplexity and power budget, required by classical methods to exploit this\ninformation is often prohibitive for smaller-scale applications such as\nautonomous cars, drones or robotics. Here we propose a neuromorphic approach to\nperform vibration analysis using spiking neural networks that can be applied to\na wide range of scenarios. We present a spike-based end-to-end pipeline able to\ndetect system anomalies from vibration data, using building blocks that are\ncompatible with analog-digital neuromorphic circuits. This pipeline operates in\nan online unsupervised fashion, and relies on a cochlea model, on feedback\nadaptation and on a balanced spiking neural network. We show that the proposed\nmethod achieves state-of-the-art performance or better against two publicly\navailable data sets. Further, we demonstrate a working proof-of-concept\nimplemented on an asynchronous neuromorphic processor device. This work\nrepresents a significant step towards the design and implementation of\nautonomous low-power edge-computing devices for online vibration monitoring.", "published": "2021-06-01 18:00:02", "link": "http://arxiv.org/abs/2106.00687v1", "categories": ["cs.NE", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.NE"}
