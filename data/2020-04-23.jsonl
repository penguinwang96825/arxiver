{"title": "Semi-Supervised Models via Data Augmentationfor Classifying Interactive\n  Affective Responses", "abstract": "We present semi-supervised models with data augmentation (SMDA), a\nsemi-supervised text classification system to classify interactive affective\nresponses. SMDA utilizes recent transformer-based models to encode each\nsentence and employs back translation techniques to paraphrase given sentences\nas augmented data. For labeled sentences, we performed data augmentations to\nuniform the label distributions and computed supervised loss during training\nprocess. For unlabeled sentences, we explored self-training by regarding\nlow-entropy predictions over unlabeled sentences as pseudo labels, assuming\nhigh-confidence predictions as labeled data for training. We further introduced\nconsistency regularization as unsupervised loss after data augmentations on\nunlabeled data, based on the assumption that the model should predict similar\nclass distributions with original unlabeled sentences as input and augmented\nsentences as input. Via a set of experiments, we demonstrated that our system\noutperformed baseline models in terms of F1-score and accuracy.", "published": "2020-04-23 05:02:31", "link": "http://arxiv.org/abs/2004.10972v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog", "abstract": "Recent studies have shown remarkable success in end-to-end task-oriented\ndialog system. However, most neural models rely on large training data, which\nare only available for a certain number of task domains, such as navigation and\nscheduling.\n  This makes it difficult to scalable for a new domain with limited labeled\ndata. However, there has been relatively little research on how to effectively\nuse data from all domains to improve the performance of each domain and also\nunseen domains. To this end, we investigate methods that can make explicit use\nof domain knowledge and introduce a shared-private network to learn shared and\nspecific knowledge. In addition, we propose a novel Dynamic Fusion Network\n(DF-Net) which automatically exploit the relevance between the target domain\nand each domain. Results show that our model outperforms existing methods on\nmulti-domain dialogue, giving the state-of-the-art in the literature. Besides,\nwith little training data, we show its transferability by outperforming prior\nbest model by 13.9\\% on average.", "published": "2020-04-23 08:17:22", "link": "http://arxiv.org/abs/2004.11019v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QURIOUS: Question Generation Pretraining for Text Generation", "abstract": "Recent trends in natural language processing using pretraining have shifted\nfocus towards pretraining and fine-tuning approaches for text generation. Often\nthe focus has been on task-agnostic approaches that generalize the language\nmodeling objective. We propose question generation as a pretraining method,\nwhich better aligns with the text generation objectives. Our text generation\nmodels pretrained with this method are better at understanding the essence of\nthe input and are better language models for the target task. When evaluated on\ntwo text generation tasks, abstractive summarization and answer-focused\nquestion generation, our models result in state-of-the-art performances in\nterms of automatic metrics. Human evaluators also found our summaries and\ngenerated questions to be more natural, concise and informative.", "published": "2020-04-23 08:41:52", "link": "http://arxiv.org/abs/2004.11026v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DuReader_robust: A Chinese Dataset Towards Evaluating Robustness and\n  Generalization of Machine Reading Comprehension in Real-World Applications", "abstract": "Machine reading comprehension (MRC) is a crucial task in natural language\nprocessing and has achieved remarkable advancements. However, most of the\nneural MRC models are still far from robust and fail to generalize well in\nreal-world applications. In order to comprehensively verify the robustness and\ngeneralization of MRC models, we introduce a real-world Chinese dataset --\nDuReader_robust. It is designed to evaluate the MRC models from three aspects:\nover-sensitivity, over-stability and generalization. Comparing to previous\nwork, the instances in DuReader_robust are natural texts, rather than the\naltered unnatural texts. It presents the challenges when applying MRC models to\nreal-world applications. The experimental results show that MRC models do not\nperform well on the challenge test set. Moreover, we analyze the behavior of\nexisting models on the challenge test set, which may provide suggestions for\nfuture model development. The dataset and codes are publicly available at\nhttps://github.com/baidu/DuReader.", "published": "2020-04-23 13:38:18", "link": "http://arxiv.org/abs/2004.11142v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Attention Attribution: Interpreting Information Interactions Inside\n  Transformer", "abstract": "The great success of Transformer-based models benefits from the powerful\nmulti-head self-attention mechanism, which learns token dependencies and\nencodes contextual information from the input. Prior work strives to attribute\nmodel decisions to individual input features with different saliency measures,\nbut they fail to explain how these input features interact with each other to\nreach predictions. In this paper, we propose a self-attention attribution\nmethod to interpret the information interactions inside Transformer. We take\nBERT as an example to conduct extensive studies. Firstly, we apply\nself-attention attribution to identify the important attention heads, while\nothers can be pruned with marginal performance degradation. Furthermore, we\nextract the most salient dependencies in each layer to construct an attribution\ntree, which reveals the hierarchical interactions inside Transformer. Finally,\nwe show that the attribution results can be used as adversarial patterns to\nimplement non-targeted attacks towards BERT.", "published": "2020-04-23 14:58:22", "link": "http://arxiv.org/abs/2004.11207v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Correct Me If You Can: Learning from Error Corrections and Markings", "abstract": "Sequence-to-sequence learning involves a trade-off between signal strength\nand annotation cost of training data. For example, machine translation data\nrange from costly expert-generated translations that enable supervised\nlearning, to weak quality-judgment feedback that facilitate reinforcement\nlearning. We present the first user study on annotation cost and machine\nlearnability for the less popular annotation mode of error markings. We show\nthat error markings for translations of TED talks from English to German allow\nprecise credit assignment while requiring significantly less human effort than\ncorrecting/post-editing, and that error-marked data can be used successfully to\nfine-tune neural machine translation models.", "published": "2020-04-23 15:17:37", "link": "http://arxiv.org/abs/2004.11222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transliteration of Judeo-Arabic Texts into Arabic Script Using Recurrent\n  Neural Networks", "abstract": "We trained a model to automatically transliterate Judeo-Arabic texts into\nArabic script, enabling Arabic readers to access those writings. We employ a\nrecurrent neural network (RNN), combined with the connectionist temporal\nclassification (CTC) loss to deal with unequal input/output lengths. This\nobligates adjustments in the training data to avoid input sequences that are\nshorter than their corresponding outputs. We also utilize a pretraining stage\nwith a different loss function to improve network converge. Since only a single\nsource of parallel text was available for training, we take advantage of the\npossibility of generating data synthetically. We train a model that has the\ncapability to memorize words in the output language, and that also utilizes\ncontext for distinguishing ambiguities in the transliteration. We obtain an\nimprovement over the baseline 9.5% character error, achieving 2% error with our\nbest configuration. To measure the contribution of context to learning, we also\ntested word-shuffled data, for which the error rises to 2.5%.", "published": "2020-04-23 18:03:41", "link": "http://arxiv.org/abs/2004.11405v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Tool for Facilitating OCR Postediting in Historical Documents", "abstract": "Optical character recognition (OCR) for historical documents is a complex\nprocedure subject to a unique set of material issues, including inconsistencies\nin typefaces and low quality scanning. Consequently, even the most\nsophisticated OCR engines produce errors. This paper reports on a tool built\nfor postediting the output of Tesseract, more specifically for correcting\ncommon errors in digitized historical documents. The proposed tool suggests\nalternatives for word forms not found in a specified vocabulary. The assumed\nerror is replaced by a presumably correct alternative in the post-edition based\non the scores of a Language Model (LM). The tool is tested on a chapter of the\nbook An Essay Towards Regulating the Trade and Employing the Poor of this\nKingdom (Cary ,1719). As demonstrated below, the tool is successful in\ncorrecting a number of common errors. If sometimes unreliable, it is also\ntransparent and subject to human intervention.", "published": "2020-04-23 21:40:30", "link": "http://arxiv.org/abs/2004.11471v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple Segmentations of Thai Sentences for Neural Machine Translation", "abstract": "Thai is a low-resource language, so it is often the case that data is not\navailable in sufficient quantities to train an Neural Machine Translation (NMT)\nmodel which perform to a high level of quality. In addition, the Thai script\ndoes not use white spaces to delimit the boundaries between words, which adds\nmore complexity when building sequence to sequence models. In this work, we\nexplore how to augment a set of English--Thai parallel data by replicating\nsentence-pairs with different word segmentation methods on Thai, as training\ndata for NMT model training. Using different merge operations of Byte Pair\nEncoding, different segmentations of Thai sentences can be obtained. The\nexperiments show that combining these datasets, performance is improved for NMT\nmodels trained with a dataset that has been split using a supervised splitting\ntool.", "published": "2020-04-23 21:48:58", "link": "http://arxiv.org/abs/2004.11472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UHH-LT at SemEval-2020 Task 12: Fine-Tuning of Pre-Trained Transformer\n  Networks for Offensive Language Detection", "abstract": "Fine-tuning of pre-trained transformer networks such as BERT yield\nstate-of-the-art results for text classification tasks. Typically, fine-tuning\nis performed on task-specific training datasets in a supervised manner. One can\nalso fine-tune in unsupervised manner beforehand by further pre-training the\nmasked language modeling (MLM) task. Hereby, in-domain data for unsupervised\nMLM resembling the actual classification target dataset allows for domain\nadaptation of the model. In this paper, we compare current pre-trained\ntransformer networks with and without MLM fine-tuning on their performance for\noffensive language detection. Our MLM fine-tuned RoBERTa-based classifier\nofficially ranks 1st in the SemEval 2020 Shared Task~12 for the English\nlanguage. Further experiments with the ALBERT model even surpass this result.", "published": "2020-04-23 23:59:58", "link": "http://arxiv.org/abs/2004.11493v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TCNN: Triple Convolutional Neural Network Models for Retrieval-based\n  Question Answering System in E-commerce", "abstract": "Automatic question-answering (QA) systems have boomed during last few years,\nand commonly used techniques can be roughly categorized into Information\nRetrieval (IR)-based and generation-based. A key solution to the IR based\nmodels is to retrieve the most similar knowledge entries of a given query from\na QA knowledge base, and then rerank those knowledge entries with semantic\nmatching models. In this paper, we aim to improve an IR based e-commerce QA\nsystem-AliMe with proposed text matching models, including a basic Triple\nConvolutional Neural Network (TCNN) model and two Attention-based TCNN (ATCNN)\nmodels. Experimental results show their effect.", "published": "2020-04-23 01:02:15", "link": "http://arxiv.org/abs/2004.10919v1", "categories": ["cs.LG", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "abstract": "Language models pretrained on text from a wide variety of sources form the\nfoundation of today's NLP. In light of the success of these broad-coverage\nmodels, we investigate whether it is still helpful to tailor a pretrained model\nto the domain of a target task. We present a study across four domains\n(biomedical and computer science publications, news, and reviews) and eight\nclassification tasks, showing that a second phase of pretraining in-domain\n(domain-adaptive pretraining) leads to performance gains, under both high- and\nlow-resource settings. Moreover, adapting to the task's unlabeled data\n(task-adaptive pretraining) improves performance even after domain-adaptive\npretraining. Finally, we show that adapting to a task corpus augmented using\nsimple data selection strategies is an effective alternative, especially when\nresources for domain-adaptive pretraining might be unavailable. Overall, we\nconsistently find that multi-phase adaptive pretraining offers large gains in\ntask performance.", "published": "2020-04-23 04:21:19", "link": "http://arxiv.org/abs/2004.10964v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilling Knowledge for Fast Retrieval-based Chat-bots", "abstract": "Response retrieval is a subset of neural ranking in which a model selects a\nsuitable response from a set of candidates given a conversation history.\nRetrieval-based chat-bots are typically employed in information seeking\nconversational systems such as customer support agents. In order to make\npairwise comparisons between a conversation history and a candidate response,\ntwo approaches are common: cross-encoders performing full self-attention over\nthe pair and bi-encoders encoding the pair separately. The former gives better\nprediction quality but is too slow for practical use. In this paper, we propose\na new cross-encoder architecture and transfer knowledge from this model to a\nbi-encoder model using distillation. This effectively boosts bi-encoder\nperformance at no cost during inference time. We perform a detailed analysis of\nthis approach on three response retrieval datasets.", "published": "2020-04-23 09:41:37", "link": "http://arxiv.org/abs/2004.11045v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Coupling semantic and statistical techniques for dynamically enriching\n  web ontologies", "abstract": "With the development of the Semantic Web technology, the use of ontologies to\nstore and retrieve information covering several domains has increased. However,\nvery few ontologies are able to cope with the ever-growing need of frequently\nupdated semantic information or specific user requirements in specialized\ndomains. As a result, a critical issue is related to the unavailability of\nrelational information between concepts, also coined missing background\nknowledge. One solution to address this issue relies on the manual enrichment\nof ontologies by domain experts which is however a time consuming and costly\nprocess, hence the need for dynamic ontology enrichment. In this paper we\npresent an automatic coupled statistical/semantic framework for dynamically\nenriching large-scale generic ontologies from the World Wide Web. Using the\nmassive amount of information encoded in texts on the Web as a corpus, missing\nbackground knowledge can therefore be discovered through a combination of\nsemantic relatedness measures and pattern acquisition techniques and\nsubsequently exploited. The benefits of our approach are: (i) proposing the\ndynamic enrichment of large-scale generic ontologies with missing background\nknowledge, and thus, enabling the reuse of such knowledge, (ii) dealing with\nthe issue of costly ontological manual enrichment by domain experts.\nExperimental results in a precision-based evaluation setting demonstrate the\neffectiveness of the proposed techniques.", "published": "2020-04-23 11:21:30", "link": "http://arxiv.org/abs/2004.11081v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Coupled intrinsic and extrinsic human language resource-based query\n  expansion", "abstract": "Poor information retrieval performance has often been attributed to the\nquery-document vocabulary mismatch problem which is defined as the difficulty\nfor human users to formulate precise natural language queries that are in line\nwith the vocabulary of the documents deemed relevant to a specific search goal.\nTo alleviate this problem, query expansion processes are applied in order to\nspawn and integrate additional terms to an initial query. This requires\naccurate identification of main query concepts to ensure the intended search\ngoal is duly emphasized and relevant expansion concepts are extracted and\nincluded in the enriched query. Natural language queries have intrinsic\nlinguistic properties such as parts-of-speech labels and grammatical relations\nwhich can be utilized in determining the intended search goal. Additionally,\nextrinsic language-based resources such as ontologies are needed to suggest\nexpansion concepts semantically coherent with the query content. We present\nhere a query expansion framework which capitalizes on both linguistic\ncharacteristics of user queries and ontology resources for query constituent\nencoding, expansion concept extraction and concept weighting. A thorough\nempirical evaluation on real-world datasets validates our approach against\nunigram language model, relevance model and a sequential dependence based\ntechnique.", "published": "2020-04-23 11:22:38", "link": "http://arxiv.org/abs/2004.11083v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Natural language technology and query expansion: issues,\n  state-of-the-art and perspectives", "abstract": "The availability of an abundance of knowledge sources has spurred a large\namount of effort in the development and enhancement of Information Retrieval\ntechniques. Users information needs are expressed in natural language and\nsuccessful retrieval is very much dependent on the effective communication of\nthe intended purpose. Natural language queries consist of multiple linguistic\nfeatures which serve to represent the intended search goal. Linguistic\ncharacteristics that cause semantic ambiguity and misinterpretation of queries\nas well as additional factors such as the lack of familiarity with the search\nenvironment affect the users ability to accurately represent their information\nneeds, coined by the concept intention gap. The latter directly affects the\nrelevance of the returned search results which may not be to the users\nsatisfaction and therefore is a major issue impacting the effectiveness of\ninformation retrieval systems. Central to our discussion is the identification\nof the significant constituents that characterize the query intent and their\nenrichment through the addition of meaningful terms, phrases or even latent\nrepresentations, either manually or automatically to capture their intended\nmeaning. Specifically, we discuss techniques to achieve the enrichment and in\nparticular those utilizing the information gathered from statistical processing\nof term dependencies within a document corpus or from external knowledge\nsources such as ontologies. We lay down the anatomy of a generic linguistic\nbased query expansion framework and propose its module-based decomposition,\ncovering topical issues from query processing, information retrieval,\ncomputational linguistics and ontology engineering. For each of the modules we\nreview state-of-the-art solutions in the literature categorized and analyzed\nunder the light of the techniques used.", "published": "2020-04-23 11:39:07", "link": "http://arxiv.org/abs/2004.11093v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On Adversarial Examples for Biomedical NLP Tasks", "abstract": "The success of pre-trained word embeddings has motivated its use in tasks in\nthe biomedical domain. The BERT language model has shown remarkable results on\nstandard performance metrics in tasks such as Named Entity Recognition (NER)\nand Semantic Textual Similarity (STS), which has brought significant progress\nin the field of NLP. However, it is unclear whether these systems work\nseemingly well in critical domains, such as legal or medical. For that reason,\nin this work, we propose an adversarial evaluation scheme on two well-known\ndatasets for medical NER and STS. We propose two types of attacks inspired by\nnatural spelling errors and typos made by humans. We also propose another type\nof attack that uses synonyms of medical terms. Under these adversarial\nsettings, the accuracy of the models drops significantly, and we quantify the\nextent of this performance loss. We also show that we can significantly improve\nthe robustness of the models by training them with adversarial examples. We\nhope our work will motivate the use of adversarial examples to evaluate and\ndevelop models with increased robustness for medical tasks.", "published": "2020-04-23 13:46:11", "link": "http://arxiv.org/abs/2004.11157v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Same Side Stance Classification Task: Facilitating Argument Stance\n  Classification by Fine-tuning a BERT Model", "abstract": "Research on computational argumentation is currently being intensively\ninvestigated. The goal of this community is to find the best pro and con\narguments for a user given topic either to form an opinion for oneself, or to\npersuade others to adopt a certain standpoint. While existing argument mining\nmethods can find appropriate arguments for a topic, a correct classification\ninto pro and con is not yet reliable. The same side stance classification task\nprovides a dataset of argument pairs classified by whether or not both\narguments share the same stance and does not need to distinguish between\ntopic-specific pro and con vocabulary but only the argument similarity within a\nstance needs to be assessed. The results of our contribution to the task are\nbuild on a setup based on the BERT architecture. We fine-tuned a pre-trained\nBERT model for three epochs and used the first 512 tokens of each argument to\npredict if two arguments share the same stance.", "published": "2020-04-23 13:54:31", "link": "http://arxiv.org/abs/2004.11163v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Forgetting Curves for Spaced Repetition Language Learning", "abstract": "The forgetting curve has been extensively explored by psychologists,\neducationalists and cognitive scientists alike. In the context of Intelligent\nTutoring Systems, modelling the forgetting curve for each user and knowledge\ncomponent (e.g. vocabulary word) should enable us to develop optimal revision\nstrategies that counteract memory decay and ensure long-term retention. In this\nstudy we explore a variety of forgetting curve models incorporating\npsychological and linguistic features, and we use these models to predict the\nprobability of word recall by learners of English as a second language. We\nevaluate the impact of the models and their features using data from an online\nvocabulary teaching platform and find that word complexity is a highly\ninformative feature which may be successfully learned by a neural network\nmodel.", "published": "2020-04-23 17:22:38", "link": "http://arxiv.org/abs/2004.11327v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterising User Content on a Multi-lingual Social Network", "abstract": "Social media has been on the vanguard of political information diffusion in\nthe 21st century. Most studies that look into disinformation, political\ninfluence and fake-news focus on mainstream social media platforms. This has\ninevitably made English an important factor in our current understanding of\npolitical activity on social media. As a result, there has only been a limited\nnumber of studies into a large portion of the world, including the largest,\nmultilingual and multi-cultural democracy: India. In this paper we present our\ncharacterisation of a multilingual social network in India called ShareChat. We\ncollect an exhaustive dataset across 72 weeks before and during the Indian\ngeneral elections of 2019, across 14 languages. We investigate the cross\nlingual dynamics by clustering visually similar images together, and exploring\nhow they move across language barriers. We find that Telugu, Malayalam, Tamil\nand Kannada languages tend to be dominant in soliciting political images (often\nreferred to as memes), and posts from Hindi have the largest cross-lingual\ndiffusion across ShareChat (as well as images containing text in English). In\nthe case of images containing text that cross language barriers, we see that\nlanguage translation is used to widen the accessibility. That said, we find\ncases where the same image is associated with very different text (and\ntherefore meanings). This initial characterisation paves the way for more\nadvanced pipelines to understand the dynamics of fake and political content in\na multi-lingual and non-textual setting.", "published": "2020-04-23 22:25:48", "link": "http://arxiv.org/abs/2004.11480v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A Review of Winograd Schema Challenge Datasets and Approaches", "abstract": "The Winograd Schema Challenge is both a commonsense reasoning and natural\nlanguage understanding challenge, introduced as an alternative to the Turing\ntest. A Winograd schema is a pair of sentences differing in one or two words\nwith a highly ambiguous pronoun, resolved differently in the two sentences,\nthat appears to require commonsense knowledge to be resolved correctly. The\nexamples were designed to be easily solvable by humans but difficult for\nmachines, in principle requiring a deep understanding of the content of the\ntext and the situation it describes. This paper reviews existing Winograd\nSchema Challenge benchmark datasets and approaches that have been published\nsince its introduction.", "published": "2020-04-23 08:40:11", "link": "http://arxiv.org/abs/2004.13831v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Question Answering Using Semantic Information from Image\n  Descriptions", "abstract": "In this work, we propose a deep neural architecture that uses an attention\nmechanism which utilizes region based image features, the natural language\nquestion asked, and semantic knowledge extracted from the regions of an image\nto produce open-ended answers for questions asked in a visual question\nanswering (VQA) task. The combination of both region based features and region\nbased textual information about the image bolsters a model to more accurately\nrespond to questions and potentially do so with less required training data. We\nevaluate our proposed architecture on a VQA task against a strong baseline and\nshow that our method achieves excellent results on this task.", "published": "2020-04-23 04:35:04", "link": "http://arxiv.org/abs/2004.10966v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Love, Joy, Anger, Sadness, Fear, and Surprise: SE Needs Special Kinds of\n  AI: A Case Study on Text Mining and SE", "abstract": "Do you like your code? What kind of code makes developers happiest? What\nmakes them angriest? Is it possible to monitor the mood of a large team of\ncoders to determine when and where a codebase needs additional help?", "published": "2020-04-23 07:11:12", "link": "http://arxiv.org/abs/2004.11005v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Learning Dialog Policies from Weak Demonstrations", "abstract": "Deep reinforcement learning is a promising approach to training a dialog\nmanager, but current methods struggle with the large state and action spaces of\nmulti-domain dialog systems. Building upon Deep Q-learning from Demonstrations\n(DQfD), an algorithm that scores highly in difficult Atari games, we leverage\ndialog data to guide the agent to successfully respond to a user's requests. We\nmake progressively fewer assumptions about the data needed, using labeled,\nreduced-labeled, and even unlabeled data to train expert demonstrators. We\nintroduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to\novercome the domain gap between the datasets and the environment. Experiments\nin a challenging multi-domain dialog system framework validate our approaches,\nand get high success rates even when trained on out-of-domain data.", "published": "2020-04-23 10:22:16", "link": "http://arxiv.org/abs/2004.11054v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Rapidly Bootstrapping a Question Answering Dataset for COVID-19", "abstract": "We present CovidQA, the beginnings of a question answering dataset\nspecifically designed for COVID-19, built by hand from knowledge gathered from\nKaggle's COVID-19 Open Research Dataset Challenge. To our knowledge, this is\nthe first publicly available resource of its type, and intended as a stopgap\nmeasure for guiding research until more substantial evaluation resources become\navailable. While this dataset, comprising 124 question-article pairs as of the\npresent version 0.1 release, does not have sufficient examples for supervised\nmachine learning, we believe that it can be helpful for evaluating the\nzero-shot or transfer capabilities of existing models on topics specifically\nrelated to COVID-19. This paper describes our methodology for constructing the\ndataset and presents the effectiveness of a number of baselines, including\nterm-based techniques and various transformer-based models. The dataset is\navailable at http://covidqa.ai/", "published": "2020-04-23 17:35:11", "link": "http://arxiv.org/abs/2004.11339v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "End-to-end speech-to-dialog-act recognition", "abstract": "Spoken language understanding, which extracts intents and/or semantic\nconcepts in utterances, is conventionally formulated as a post-processing of\nautomatic speech recognition. It is usually trained with oracle transcripts,\nbut needs to deal with errors by ASR. Moreover, there are acoustic features\nwhich are related with intents but not represented with the transcripts. In\nthis paper, we present an end-to-end model which directly converts speech into\ndialog acts without the deterministic transcription process. In the proposed\nmodel, the dialog act recognition network is conjunct with an acoustic-to-word\nASR model at its latent layer before the softmax layer, which provides a\ndistributed representation of word-level ASR decoding information. Then, the\nentire network is fine-tuned in an end-to-end manner. This allows for stable\ntraining as well as robustness against ASR errors. The model is further\nextended to conduct DA segmentation jointly. Evaluations with the Switchboard\ncorpus demonstrate that the proposed method significantly improves dialog act\nrecognition accuracy from the conventional pipeline framework.", "published": "2020-04-23 18:44:27", "link": "http://arxiv.org/abs/2004.11419v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Upgrading the Newsroom: An Automated Image Selection System for News\n  Articles", "abstract": "We propose an automated image selection system to assist photo editors in\nselecting suitable images for news articles. The system fuses multiple textual\nsources extracted from news articles and accepts multilingual inputs. It is\nequipped with char-level word embeddings to help both modeling morphologically\nrich languages, e.g. German, and transferring knowledge across nearby\nlanguages. The text encoder adopts a hierarchical self-attention mechanism to\nattend more to both keywords within a piece of text and informative components\nof a news article. We extensively experiment with our system on a large-scale\ntext-image database containing multimodal multilingual news articles collected\nfrom Swiss local news media websites. The system is compared with multiple\nbaselines with ablation studies and is shown to beat existing text-image\nretrieval methods in a weakly-supervised learning setting. Besides, we also\noffer insights on the advantage of using multiple textual sources and\nmultilingual data.", "published": "2020-04-23 20:29:26", "link": "http://arxiv.org/abs/2004.11449v1", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.MM"}
{"title": "A Gamma-Poisson Mixture Topic Model for Short Text", "abstract": "Most topic models are constructed under the assumption that documents follow\na multinomial distribution. The Poisson distribution is an alternative\ndistribution to describe the probability of count data. For topic modelling,\nthe Poisson distribution describes the number of occurrences of a word in\ndocuments of fixed length. The Poisson distribution has been successfully\napplied in text classification, but its application to topic modelling is not\nwell documented, specifically in the context of a generative probabilistic\nmodel. Furthermore, the few Poisson topic models in literature are admixture\nmodels, making the assumption that a document is generated from a mixture of\ntopics. In this study, we focus on short text. Many studies have shown that the\nsimpler assumption of a mixture model fits short text better. With mixture\nmodels, as opposed to admixture models, the generative assumption is that a\ndocument is generated from a single topic. One topic model, which makes this\none-topic-per-document assumption, is the Dirichlet-multinomial mixture model.\nThe main contributions of this work are a new Gamma-Poisson mixture model, as\nwell as a collapsed Gibbs sampler for the model. The benefit of the collapsed\nGibbs sampler derivation is that the model is able to automatically select the\nnumber of topics contained in the corpus. The results show that the\nGamma-Poisson mixture model performs better than the Dirichlet-multinomial\nmixture model at selecting the number of topics in labelled corpora.\nFurthermore, the Gamma-Poisson mixture produces better topic coherence scores\nthan the Dirichlet-multinomial mixture model, thus making it a viable option\nfor the challenging task of topic modelling of short text.", "published": "2020-04-23 21:13:53", "link": "http://arxiv.org/abs/2004.11464v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards an evolutionary-based approach for natural language processing", "abstract": "Tasks related to Natural Language Processing (NLP) have recently been the\nfocus of a large research endeavor by the machine learning community. The\nincreased interest in this area is mainly due to the success of deep learning\nmethods. Genetic Programming (GP), however, was not under the spotlight with\nrespect to NLP tasks. Here, we propose a first proof-of-concept that combines\nGP with the well established NLP tool word2vec for the next word prediction\ntask. The main idea is that, once words have been moved into a vector space,\ntraditional GP operators can successfully work on vectors, thus producing\nmeaningful words as the output. To assess the suitability of this approach, we\nperform an experimental evaluation on a set of existing newspaper headlines.\nIndividuals resulting from this (pre-)training phase can be employed as the\ninitial population in other NLP tasks, like sentence generation, which will be\nthe focus of future investigations, possibly employing adversarial\nco-evolutionary approaches.", "published": "2020-04-23 18:44:12", "link": "http://arxiv.org/abs/2004.13832v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "ByteSing: A Chinese Singing Voice Synthesis System Using Duration\n  Allocated Encoder-Decoder Acoustic Models and WaveRNN Vocoders", "abstract": "This paper presents ByteSing, a Chinese singing voice synthesis (SVS) system\nbased on duration allocated Tacotron-like acoustic models and WaveRNN neural\nvocoders. Different from the conventional SVS models, the proposed ByteSing\nemploys Tacotron-like encoder-decoder structures as the acoustic models, in\nwhich the CBHG models and recurrent neural networks (RNNs) are explored as\nencoders and decoders respectively. Meanwhile an auxiliary phoneme duration\nprediction model is utilized to expand the input sequence, which can enhance\nthe model controllable capacity, model stability and tempo prediction accuracy.\nWaveRNN neural vocoders are also adopted as neural vocoders to further improve\nthe voice quality of synthesized songs. Both objective and subjective\nexperimental results prove that the SVS method proposed in this paper can\nproduce quite natural, expressive and high-fidelity songs by improving the\npitch and spectrogram prediction accuracy and the models using attention\nmechanism can achieve best performance.", "published": "2020-04-23 07:48:09", "link": "http://arxiv.org/abs/2004.11012v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Flexible framework for audio reconstruction", "abstract": "The paper presents a unified, flexible framework for the tasks of audio\ninpainting, declipping, and dequantization. The concept is further extended to\ncover analogous degradation models in a transformed domain, e.g. quantization\nof the signal's time-frequency coefficients. The task of reconstructing an\naudio signal from degraded observations in two different domains is formulated\nas an inverse problem, and several algorithmic solutions are developed. The\nviability of the presented concept is demonstrated on an example where audio\nreconstruction from partial and quantized observations of both the time-domain\nsignal and its time-frequency coefficients is carried out.", "published": "2020-04-23 13:53:45", "link": "http://arxiv.org/abs/2004.11162v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Speech Decomposition via Triple Information Bottleneck", "abstract": "Speech information can be roughly decomposed into four components: language\ncontent, timbre, pitch, and rhythm. Obtaining disentangled representations of\nthese components is useful in many speech analysis and generation applications.\nRecently, state-of-the-art voice conversion systems have led to speech\nrepresentations that can disentangle speaker-dependent and independent\ninformation. However, these systems can only disentangle timbre, while\ninformation about pitch, rhythm and content is still mixed together. Further\ndisentangling the remaining speech components is an under-determined problem in\nthe absence of explicit annotations for each component, which are difficult and\nexpensive to obtain. In this paper, we propose SpeechSplit, which can blindly\ndecompose speech into its four components by introducing three carefully\ndesigned information bottlenecks. SpeechSplit is among the first algorithms\nthat can separately perform style transfer on timbre, pitch and rhythm without\ntext labels. Our code is publicly available at\nhttps://github.com/auspicious3000/SpeechSplit.", "published": "2020-04-23 16:12:42", "link": "http://arxiv.org/abs/2004.11284v6", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
