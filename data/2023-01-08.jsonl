{"title": "Analyzing the Representational Geometry of Acoustic Word Embeddings", "abstract": "Acoustic word embeddings (AWEs) are vector representations such that\ndifferent acoustic exemplars of the same word are projected nearby in the\nembedding space. In addition to their use in speech technology applications\nsuch as spoken term discovery and keyword spotting, AWE models have been\nadopted as models of spoken-word processing in several cognitively motivated\nstudies and have been shown to exhibit human-like performance in some auditory\nprocessing tasks. Nevertheless, the representational geometry of AWEs remains\nan under-explored topic that has not been studied in the literature. In this\npaper, we take a closer analytical look at AWEs learned from English speech and\nstudy how the choice of the learning objective and the architecture shapes\ntheir representational profile. To this end, we employ a set of analytic\ntechniques from machine learning and neuroscience in three different analyses:\nembedding space uniformity, word discriminability, and representational\nconsistency. Our main findings highlight the prominent role of the learning\nobjective on shaping the representation profile compared to the model\narchitecture.", "published": "2023-01-08 10:22:50", "link": "http://arxiv.org/abs/2301.03012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of German Drama Texts Using Fine Tuned GPT-2 Models", "abstract": "This study is devoted to the automatic generation of German drama texts. We\nsuggest an approach consisting of two key steps: fine-tuning a GPT-2 model (the\noutline model) to generate outlines of scenes based on keywords and fine-tuning\na second model (the generation model) to generate scenes from the scene\noutline. The input for the neural model comprises two datasets: the German\nDrama Corpus (GerDraCor) and German Text Archive (Deutsches Textarchiv or DTA).\nIn order to estimate the effectiveness of the proposed method, our models are\ncompared with baseline GPT-2 models. Our models perform well according to\nautomatic quantitative evaluation, but, conversely, manual qualitative analysis\nreveals a poor quality of generated texts. This may be due to the quality of\nthe dataset or training inputs.", "published": "2023-01-08 23:12:46", "link": "http://arxiv.org/abs/2301.03119v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Traditional Readability Formulas Compared for English", "abstract": "Traditional English readability formulas, or equations, were largely\ndeveloped in the 20th century. Nonetheless, many researchers still rely on them\nfor various NLP applications. This phenomenon is presumably due to the\nconvenience and straightforwardness of readability formulas. In this work, we\ncontribute to the NLP community by 1. introducing New English Readability\nFormula (NERF), 2. recalibrating the coefficients of old readability formulas\n(Flesch-Kincaid Grade Level, Fog Index, SMOG Index, Coleman-Liau Index, and\nAutomated Readability Index), 3. evaluating the readability formulas, for use\nin text simplification studies and medical texts, and 4. developing a\nPython-based program for the wide application to various NLP projects.", "published": "2023-01-08 04:33:43", "link": "http://arxiv.org/abs/2301.02975v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Topic Modelling of Swedish Newspaper Articles about Coronavirus: a Case\n  Study using Latent Dirichlet Allocation Method", "abstract": "Topic Modelling (TM) is from the research branches of natural language\nunderstanding (NLU) and natural language processing (NLP) that is to facilitate\ninsightful analysis from large documents and datasets, such as a summarisation\nof main topics and the topic changes. This kind of discovery is getting more\npopular in real-life applications due to its impact on big data analytics. In\nthis study, from the social-media and healthcare domain, we apply popular\nLatent Dirichlet Allocation (LDA) methods to model the topic changes in Swedish\nnewspaper articles about Coronavirus. We describe the corpus we created\nincluding 6515 articles, methods applied, and statistics on topic changes over\napproximately 1 year and two months period of time from 17th January 2020 to\n13th March 2021. We hope this work can be an asset for grounding applications\nof topic modelling and can be inspiring for similar case studies in an era with\npandemics, to support socio-economic impact research as well as clinical and\nhealthcare analytics. Our data and source code are openly available at\nhttps://github. com/poethan/Swed_Covid_TM Keywords: Latent Dirichlet Allocation\n(LDA); Topic Modelling; Coronavirus; Pandemics; Natural Language Understanding;\nBERT-topic", "published": "2023-01-08 12:33:58", "link": "http://arxiv.org/abs/2301.03029v6", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MEGAnno: Exploratory Labeling for NLP in Computational Notebooks", "abstract": "We present MEGAnno, a novel exploratory annotation framework designed for NLP\nresearchers and practitioners. Unlike existing labeling tools that focus on\ndata labeling only, our framework aims to support a broader, iterative ML\nworkflow including data exploration and model development. With MEGAnno's API,\nusers can programmatically explore the data through sophisticated search and\nautomated suggestion functions and incrementally update task schema as their\nproject evolve. Combined with our widget, the users can interactively sort,\nfilter, and assign labels to multiple items simultaneously in the same notebook\nwhere the rest of the NLP project resides. We demonstrate MEGAnno's flexible,\nexploratory, efficient, and seamless labeling experience through a sentiment\nanalysis use case.", "published": "2023-01-08 19:16:22", "link": "http://arxiv.org/abs/2301.03095v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "SpeeChain: A Speech Toolkit for Large-Scale Machine Speech Chain", "abstract": "This paper introduces SpeeChain, an open-source Pytorch-based toolkit\ndesigned to develop the machine speech chain for large-scale use. This first\nrelease focuses on the TTS-to-ASR chain, a core component of the machine speech\nchain, that refers to the TTS data augmentation by unspoken text for ASR. To\nbuild an efficient pipeline for the large-scale TTS-to-ASR chain, we implement\neasy-to-use multi-GPU batch-level model inference, multi-dataloader batch\ngeneration, and on-the-fly data selection techniques. In this paper, we first\nexplain the overall procedure of the TTS-to-ASR chain and the difficulties of\neach step. Then, we present a detailed ablation study on different types of\nunlabeled data, data filtering thresholds, batch composition, and\nreal-synthetic data ratios. Our experimental results on train_clean_460 of\nLibriSpeech demonstrate that our TTS-to-ASR chain can significantly improve WER\nin a semi-supervised setting.", "published": "2023-01-08 03:16:56", "link": "http://arxiv.org/abs/2301.02966v1", "categories": ["cs.CL", "cs.LG", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers", "abstract": "We carried out a reproducibility study of InPars, which is a method for\nunsupervised training of neural rankers (Bonifacio et al., 2022). As a\nby-product, we developed InPars-light, which is a simple-yet-effective\nmodification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller\nranking models and only a freely available language model BLOOM, which -- as we\nfound out -- produced more accurate rankers compared to a proprietary GPT-3\nmodel. On all five English retrieval collections (used in the original InPars\nstudy) we obtained substantial (7%-30%) and statistically significant\nimprovements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer\nMiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars\nstudy only a 100x larger monoT5-3B model consistently outperformed BM25,\nwhereas their smaller monoT5-220M model (which is still 7x larger than our\nMiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same\nthree-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par\nwith the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact,\non three out of five datasets, DeBERTA slightly outperformed monoT5-3B.\nFinally, these good results were achieved by re-ranking only 100 candidate\ndocuments compared to 1000 used by Bonifacio et al. (2022). We believe that\nInPars-light is the first truly cost-effective prompt-based unsupervised recipe\nto train and deploy neural ranking models that outperform BM25. Our code and\ndata is publicly available. https://github.com/searchivarius/inpars_light/", "published": "2023-01-08 08:03:46", "link": "http://arxiv.org/abs/2301.02998v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The State of Human-centered NLP Technology for Fact-checking", "abstract": "Misinformation threatens modern society by promoting distrust in science,\nchanging narratives in public health, heightening social polarization, and\ndisrupting democratic elections and financial markets, among a myriad of other\nsocietal harms. To address this, a growing cadre of professional fact-checkers\nand journalists provide high-quality investigations into purported facts.\nHowever, these largely manual efforts have struggled to match the enormous\nscale of the problem. In response, a growing body of Natural Language\nProcessing (NLP) technologies have been proposed for more scalable\nfact-checking. Despite tremendous growth in such research, however, practical\nadoption of NLP technologies for fact-checking still remains in its infancy\ntoday.\n  In this work, we review the capabilities and limitations of the current NLP\ntechnologies for fact-checking. Our particular focus is to further chart the\ndesign space for how these technologies can be harnessed and refined in order\nto better meet the needs of human fact-checkers. To do so, we review key\naspects of NLP-based fact-checking: task formulation, dataset construction,\nmodeling, and human-centered strategies, such as explainable models and\nhuman-in-the-loop approaches. Next, we review the efficacy of applying\nNLP-based fact-checking tools to assist human fact-checkers. We recommend that\nfuture research include collaboration with fact-checker stakeholders early on\nin NLP research, as well as incorporation of human-centered design practices in\nmodel development, in order to further guide technology development for human\nuse and practical adoption. Finally, we advocate for more research on benchmark\ndevelopment supporting extrinsic evaluation of human-centered fact-checking\ntechnologies.", "published": "2023-01-08 15:13:13", "link": "http://arxiv.org/abs/2301.03056v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
