{"title": "Fast ASR-free and almost zero-resource keyword spotting using DTW and\n  CNNs for humanitarian monitoring", "abstract": "We use dynamic time warping (DTW) as supervision for training a convolutional\nneural network (CNN) based keyword spotting system using a small set of spoken\nisolated keywords. The aim is to allow rapid deployment of a keyword spotting\nsystem in a new language to support urgent United Nations (UN) relief\nprogrammes in parts of Africa where languages are extremely under-resourced and\nthe development of annotated speech resources is infeasible. First, we use 1920\nrecorded keywords (40 keyword types, 34 minutes of speech) as exemplars in a\nDTW-based template matching system and apply it to untranscribed broadcast\nspeech. Then, we use the resulting DTW scores as targets to train a CNN on the\nsame unlabelled speech. In this way we use just 34 minutes of labelled speech,\nbut leverage a large amount of unlabelled data for training. While the\nresulting CNN keyword spotter cannot match the performance of the DTW-based\nsystem, it substantially outperforms a CNN classifier trained only on the\nkeywords, improving the area under the ROC curve from 0.54 to 0.64. Because our\nCNN system is several orders of magnitude faster at runtime than the DTW\nsystem, it represents the most viable keyword spotter on this extremely limited\ndataset.", "published": "2018-06-25 10:41:29", "link": "http://arxiv.org/abs/1806.09374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prior Attention for Style-aware Sequence-to-Sequence Models", "abstract": "We extend sequence-to-sequence models with the possibility to control the\ncharacteristics or style of the generated output, via attention that is\ngenerated a priori (before decoding) from a latent code vector. After training\nan initial attention-based sequence-to-sequence model, we use a variational\nauto-encoder conditioned on representations of input sequences and a latent\ncode vector space to generate attention matrices. By sampling the code vector\nfrom specific regions of this latent space during decoding and imposing prior\nattention generated from it in the seq2seq model, output can be steered towards\nhaving certain attributes. This is demonstrated for the task of sentence\nsimplification, where the latent code vector allows control over output length\nand lexical simplification, and enables fine-tuning to optimize for different\nevaluation metrics.", "published": "2018-06-25 13:17:24", "link": "http://arxiv.org/abs/1806.09439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation for Low Resource Languages using Bilingual\n  Lexicon Induced from Comparable Corpora", "abstract": "Resources for the non-English languages are scarce and this paper addresses\nthis problem in the context of machine translation, by automatically extracting\nparallel sentence pairs from the multilingual articles available on the\nInternet. In this paper, we have used an end-to-end Siamese bidirectional\nrecurrent neural network to generate parallel sentences from comparable\nmultilingual articles in Wikipedia. Subsequently, we have showed that using the\nharvested dataset improved BLEU scores on both NMT and phrase-based SMT systems\nfor the low-resource language pairs: English--Hindi and English--Tamil, when\ncompared to training exclusively on the limited bilingual corpora collected for\nthese language pairs.", "published": "2018-06-25 18:22:27", "link": "http://arxiv.org/abs/1806.09652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Framework for Opinion Mining Approach to Augment Education System\n  Performance", "abstract": "The extensive expansion growth of social networking sites allows the people\nto share their views and experiences freely with their peers on internet. Due\nto this, huge amount of data is generated on everyday basis which can be used\nfor the opinion mining to extract the views of people in a particular field.\nOpinion mining finds its applications in many areas such as Tourism, Politics,\neducation and entertainment, etc. It has not been extensively implemented in\narea of education system. This paper discusses the malpractices in the present\nexamination system. In the present scenario, Opinion mining is vastly used for\ndecision making. The authors of this paper have designed a framework by\napplying Na\\\"ive Bayes approach to the education dataset. The various phases of\nNa\\\"ive Bayes approach include three steps: conversion of data into frequency\ntable, making classes of dataset and apply the Na\\\"ive Bayes algorithm equation\nto calculate the probabilities of classes. Finally the highest probability\nclass is the outcome of this prediction. These predictions are used to make\nimprovements in the education system and help to provide better education.", "published": "2018-06-25 04:17:44", "link": "http://arxiv.org/abs/1806.09279v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Single-channel Speech Dereverberation via Generative Adversarial\n  Training", "abstract": "In this paper, we propose a single-channel speech dereverberation system\n(DeReGAT) based on convolutional, bidirectional long short-term memory and deep\nfeed-forward neural network (CBLDNN) with generative adversarial training\n(GAT). In order to obtain better speech quality instead of only minimizing a\nmean square error (MSE), GAT is employed to make the dereverberated speech\nindistinguishable form the clean samples. Besides, our system can deal with\nwide range reverberation and be well adapted to variant environments. The\nexperimental results show that the proposed model outperforms weighted\nprediction error (WPE) and deep neural network-based systems. In addition,\nDeReGAT is extended to an online speech dereverberation scenario, which reports\ncomparable performance with the offline case.", "published": "2018-06-25 08:35:56", "link": "http://arxiv.org/abs/1806.09325v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Hierarchical Deep Learning Natural Language Parser for Fashion", "abstract": "This work presents a hierarchical deep learning natural language parser for\nfashion. Our proposal intends not only to recognize fashion-domain entities but\nalso to expose syntactic and morphologic insights. We leverage the usage of an\narchitecture of specialist models, each one for a different task (from parsing\nto entity recognition). Such architecture renders a hierarchical model able to\ncapture the nuances of the fashion language. The natural language parser is\nable to deal with textual ambiguities which are left unresolved by our\ncurrently existing solution. Our empirical results establish a robust baseline,\nwhich justifies the use of hierarchical architectures of deep learning models\nwhile opening new research avenues to explore.", "published": "2018-06-25 14:57:52", "link": "http://arxiv.org/abs/1806.09511v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The Emotional Voices Database: Towards Controlling the Emotion Dimension\n  in Voice Generation Systems", "abstract": "In this paper, we present a database of emotional speech intended to be\nopen-sourced and used for synthesis and generation purpose. It contains data\nfor male and female actors in English and a male actor in French. The database\ncovers 5 emotion classes so it could be suitable to build synthesis and voice\ntransformation systems with the potential to control the emotional dimension in\na continuous way. We show the data's efficiency by building a simple MLP system\nconverting neutral to angry speech style and evaluate it via a CMOS perception\ntest. Even though the system is a very simple one, the test show the efficiency\nof the data which is promising for future work.", "published": "2018-06-25 15:01:54", "link": "http://arxiv.org/abs/1806.09514v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mapping Unparalleled Clinical Professional and Consumer Languages with\n  Embedding Alignment", "abstract": "Mapping and translating professional but arcane clinical jargons to consumer\nlanguage is essential to improve the patient-clinician communication.\nResearchers have used the existing biomedical ontologies and consumer health\nvocabulary dictionary to translate between the languages. However, such\napproaches are limited by expert efforts to manually build the dictionary,\nwhich is hard to be generalized and scalable. In this work, we utilized the\nembeddings alignment method for the word mapping between unparalleled clinical\nprofessional and consumer language embeddings. To map semantically similar\nwords in two different word embeddings, we first independently trained word\nembeddings on both the corpus with abundant clinical professional terms and the\nother with mainly healthcare consumer terms. Then, we aligned the embeddings by\nthe Procrustes algorithm. We also investigated the approach with the\nadversarial training with refinement. We evaluated the quality of the alignment\nthrough the similar words retrieval both by computing the model precision and\nas well as judging qualitatively by human. We show that the Procrustes\nalgorithm can be performant for the professional consumer language embeddings\nalignment, whereas adversarial training with refinement may find some relations\nbetween two languages.", "published": "2018-06-25 15:54:45", "link": "http://arxiv.org/abs/1806.09542v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "EMPHASIS: An Emotional Phoneme-based Acoustic Model for Speech Synthesis\n  System", "abstract": "We present EMPHASIS, an emotional phoneme-based acoustic model for speech\nsynthesis system. EMPHASIS includes a phoneme duration prediction model and an\nacoustic parameter prediction model. It uses a CBHG-based regression network to\nmodel the dependencies between linguistic features and acoustic features. We\nmodify the input and output layer structures of the network to improve the\nperformance. For the linguistic features, we apply a feature grouping strategy\nto enhance emotional and prosodic features. The acoustic parameters are\ndesigned to be suitable for the regression task and waveform reconstruction.\nEMPHASIS can synthesize speech in real-time and generate expressive\ninterrogative and exclamatory speech with high audio quality. EMPHASIS is\ndesigned to be a multi-lingual model and can synthesize Mandarin-English speech\nfor now. In the experiment of emotional speech synthesis, it achieves better\nsubjective results than other real-time speech synthesis systems.", "published": "2018-06-25 03:42:38", "link": "http://arxiv.org/abs/1806.09276v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Feature Clustering for Unsupervised Speech Activity Detection", "abstract": "In certain applications such as zero-resource speech processing or very-low\nresource speech-language systems, it might not be feasible to collect speech\nactivity detection (SAD) annotations. However, the state-of-the-art supervised\nSAD techniques based on neural networks or other machine learning methods\nrequire annotated training data matched to the target domain. This paper\nestablish a clustering approach for fully unsupervised SAD useful for cases\nwhere SAD annotations are not available. The proposed approach leverages\nHartigan dip test in a recursive strategy for segmenting the feature space into\nprominent modes. Statistical dip is invariant to distortions that lends\nrobustness to the proposed method. We evaluate the method on NIST OpenSAD 2015\nand NIST OpenSAT 2017 public safety communications data. The results showed the\nsuperiority of proposed approach over the two-component GMM baseline. Index\nTerms: Clustering, Hartigan dip test, NIST OpenSAD, NIST OpenSAT, speech\nactivity detection, zero-resource speech processing, unsupervised learning.", "published": "2018-06-25 06:53:54", "link": "http://arxiv.org/abs/1806.09301v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Convolutional Neural Networks to Enhance Coded Speech", "abstract": "Enhancing coded speech suffering from far-end acoustic background noise,\nquantization noise, and potentially transmission errors, is a challenging task.\nIn this work we propose two postprocessing approaches applying convolutional\nneural networks (CNNs) either in the time domain or the cepstral domain to\nenhance the coded speech without any modification of the codecs. The time\ndomain approach follows an end-to-end fashion, while the cepstral domain\napproach uses analysis-synthesis with cepstral domain features. The proposed\npostprocessors in both domains are evaluated for various narrowband and\nwideband speech codecs in a wide range of conditions. The proposed\npostprocessor improves speech quality (PESQ) by up to 0.25 MOS-LQO points for\nG.711, 0.30 points for G.726, 0.82 points for G.722, and 0.26 points for\nadaptive multirate wideband codec (AMR-WB). In a subjective CCR listening test,\nthe proposed postprocessor on G.711-coded speech exceeds the speech quality of\nan ITU-T-standardized postfilter by 0.36 CMOS points, and obtains a clear\npreference of 1.77 CMOS points compared to legacy G.711, even better than\nuncoded speech with statistical significance. The source code for the cepstral\ndomain approach to enhance G.711-coded speech is made available.", "published": "2018-06-25 12:20:55", "link": "http://arxiv.org/abs/1806.09411v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Frame-level Instrument Recognition by Timbre and Pitch", "abstract": "Instrument recognition is a fundamental task in music information retrieval,\nyet little has been done to predict the presence of instruments in\nmulti-instrument music for each time frame. This task is important for not only\nautomatic transcription but also many retrieval problems. In this paper, we use\nthe newly released MusicNet dataset to study this front, by building and\nevaluating a convolutional neural network for making frame-level instrument\nprediction. We consider it as a multi-label classification problem for each\nframe and use frame-level annotations as the supervisory signal in training the\nnetwork. Moreover, we experiment with different ways to incorporate pitch\ninformation to our model, with the premise that doing so informs the model the\nnotes that are active per frame, and also encourages the model to learn\nrelative rates of energy buildup in the harmonic partials of different\ninstruments. Experiments show salient performance improvement over baseline\nmethods. We also report an analysis probing how pitch information helps the\ninstrument prediction task. Code and experiment details can be found at\n\\url{https://biboamy.github.io/instrument-recognition/}.", "published": "2018-06-25 17:33:04", "link": "http://arxiv.org/abs/1806.09587v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sounderfeit: Cloning a Physical Model using a Conditional Adversarial\n  Autoencoder", "abstract": "An adversarial autoencoder conditioned on known parameters of a physical\nmodeling bowed string synthesizer is evaluated for use in parameter estimation\nand resynthesis tasks. Latent dimensions are provided to capture variance not\nexplained by the conditional parameters. Results are compared with and without\nthe adversarial training, and a system capable of \"copying\" a given\nparameter-signal bidirectional relationship is examined. A real-time synthesis\nsystem built on a generative, conditioned and regularized neural network is\npresented, allowing to construct engaging sound synthesizers based purely on\nrecorded data.", "published": "2018-06-25 14:21:37", "link": "http://arxiv.org/abs/1806.09617v1", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
