{"title": "MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and\n  Contextualized Masked Language Model Score", "abstract": "This paper proposes a new natural language processing (NLP) application for\nidentifying medical jargon terms potentially difficult for patients to\ncomprehend from electronic health record (EHR) notes. We first present a novel\nand publicly available dataset with expert-annotated medical jargon terms from\n18K+ EHR note sentences ($MedJ$). Then, we introduce a novel medical jargon\nextraction ($MedJEx$) model which has been shown to outperform existing\nstate-of-the-art NLP models. First, MedJEx improved the overall performance\nwhen it was trained on an auxiliary Wikipedia hyperlink span dataset, where\nhyperlink spans provide additional Wikipedia articles to explain the spans (or\nterms), and then fine-tuned on the annotated MedJ data. Secondly, we found that\na contextualized masked language model score was beneficial for detecting\ndomain-specific unfamiliar jargon terms. Moreover, our results show that\ntraining on the auxiliary Wikipedia hyperlink span datasets improved six out of\neight biomedical named entity recognition benchmark datasets. Both MedJ and\nMedJEx are publicly available.", "published": "2022-10-12 02:27:32", "link": "http://arxiv.org/abs/2210.05875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AD-DROP: Attribution-Driven Dropout for Robust Language Model\n  Fine-Tuning", "abstract": "Fine-tuning large pre-trained language models on downstream tasks is apt to\nsuffer from overfitting when limited training data is available. While dropout\nproves to be an effective antidote by randomly dropping a proportion of units,\nexisting research has not examined its effect on the self-attention mechanism.\nIn this paper, we investigate this problem through self-attention attribution\nand find that dropping attention positions with low attribution scores can\naccelerate training and increase the risk of overfitting. Motivated by this\nobservation, we propose Attribution-Driven Dropout (AD-DROP), which randomly\ndiscards some high-attribution positions to encourage the model to make\npredictions by relying more on low-attribution positions to reduce overfitting.\nWe also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to\navoid dropping high-attribution positions excessively. Extensive experiments on\nvarious benchmarks show that AD-DROP yields consistent improvements over\nbaselines. Analysis further confirms that AD-DROP serves as a strategic\nregularizer to prevent overfitting during fine-tuning.", "published": "2022-10-12 02:54:41", "link": "http://arxiv.org/abs/2210.05883v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Prompting for Implicit Intent Prediction and Recommendation\n  with Commonsense Reasoning", "abstract": "Intelligent virtual assistants are currently designed to perform tasks or\nservices explicitly mentioned by users, so multiple related domains or tasks\nneed to be performed one by one through a long conversation with many explicit\nintents. Instead, human assistants are capable of reasoning (multiple) implicit\nintents based on user utterances via commonsense knowledge, reducing complex\ninteractions and improving practicality. Therefore, this paper proposes a\nframework of multi-domain dialogue systems, which can automatically infer\nimplicit intents based on user utterances and then perform zero-shot prompting\nusing a large pre-trained language model to trigger suitable single\ntask-oriented bots. The proposed framework is demonstrated effective to realize\nimplicit intents and recommend associated bots in a zero-shot manner.", "published": "2022-10-12 03:33:49", "link": "http://arxiv.org/abs/2210.05901v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse Analysis via Questions and Answers: Parsing Dependency\n  Structures of Questions Under Discussion", "abstract": "Automatic discourse processing is bottlenecked by data: current discourse\nformalisms pose highly demanding annotation tasks involving large taxonomies of\ndiscourse relations, making them inaccessible to lay annotators. This work\ninstead adopts the linguistic framework of Questions Under Discussion (QUD) for\ndiscourse analysis and seeks to derive QUD structures automatically. QUD views\neach sentence as an answer to a question triggered in prior context; thus, we\ncharacterize relationships between sentences as free-form questions, in\ncontrast to exhaustive fine-grained taxonomies. We develop the\nfirst-of-its-kind QUD parser that derives a dependency structure of questions\nover full documents, trained using a large, crowdsourced question-answering\ndataset DCQA (Ko et al., 2022). Human evaluation results show that QUD\ndependency parsing is possible for language models trained with this\ncrowdsourced, generalizable annotation scheme. We illustrate how our QUD\nstructure is distinct from RST trees, and demonstrate the utility of QUD\nanalysis in the context of document simplification. Our findings show that QUD\nparsing is an appealing alternative for automatic discourse processing.", "published": "2022-10-12 03:53:12", "link": "http://arxiv.org/abs/2210.05905v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Step out of KG: Knowledge Graph Completion via Knowledgeable Retrieval\n  and Reading Comprehension", "abstract": "Knowledge graphs, as the cornerstone of many AI applications, usually face\nserious incompleteness problems. In recent years, there have been many efforts\nto study automatic knowledge graph completion (KGC), most of which use existing\nknowledge to infer new knowledge. However, in our experiments, we find that not\nall relations can be obtained by inference, which constrains the performance of\nexisting models. To alleviate this problem, we propose a new model based on\ninformation retrieval and reading comprehension, namely IR4KGC. Specifically,\nwe pre-train a knowledge-based information retrieval module that can retrieve\ndocuments related to the triples to be completed. Then, the retrieved documents\nare handed over to the reading comprehension module to generate the predicted\nanswers. In experiments, we find that our model can well solve relations that\ncannot be inferred from existing knowledge, and achieve good results on KGC\ndatasets.", "published": "2022-10-12 04:50:55", "link": "http://arxiv.org/abs/2210.05921v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Graph-Based Text Representations with Character and Word Level\n  N-grams", "abstract": "Graph-based text representation focuses on how text documents are represented\nas graphs for exploiting dependency information between tokens and documents\nwithin a corpus. Despite the increasing interest in graph representation\nlearning, there is limited research in exploring new ways for graph-based text\nrepresentation, which is important in downstream natural language processing\ntasks. In this paper, we first propose a new heterogeneous word-character text\ngraph that combines word and character n-gram nodes together with document\nnodes, allowing us to better learn dependencies among these entities.\nAdditionally, we propose two new graph-based neural models, WCTextGCN and\nWCTextGAT, for modeling our proposed text graph. Extensive experiments in text\nclassification and automatic text summarization benchmarks demonstrate that our\nproposed models consistently outperform competitive baselines and\nstate-of-the-art graph-based models.", "published": "2022-10-12 08:07:54", "link": "http://arxiv.org/abs/2210.05999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating Translation Memories into Non-Autoregressive Machine\n  Translation", "abstract": "Non-autoregressive machine translation (NAT) has recently made great\nprogress. However, most works to date have focused on standard translation\ntasks, even though some edit-based NAT models, such as the Levenshtein\nTransformer (LevT), seem well suited to translate with a Translation Memory\n(TM). This is the scenario considered here. We first analyze the vanilla LevT\nmodel and explain why it does not do well in this setting. We then propose a\nnew variant, TM-LevT, and show how to effectively train this model. By\nmodifying the data presentation and introducing an extra deletion operation, we\nobtain performance that are on par with an autoregressive approach, while\nreducing the decoding load. We also show that incorporating TMs during training\ndispenses to use knowledge distillation, a well-known trick used to mitigate\nthe multimodality issue.", "published": "2022-10-12 08:51:35", "link": "http://arxiv.org/abs/2210.06020v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EduQG: A Multi-format Multiple Choice Dataset for the Educational Domain", "abstract": "We introduce a high-quality dataset that contains 3,397 samples comprising\n(i) multiple choice questions, (ii) answers (including distractors), and (iii)\ntheir source documents, from the educational domain. Each question is phrased\nin two forms, normal and close. Correct answers are linked to source documents\nwith sentence-level annotations. Thus, our versatile dataset can be used for\nboth question and distractor generation, as well as to explore new challenges\nsuch as question format conversion. Furthermore, 903 questions are accompanied\nby their cognitive complexity level as per Bloom's taxonomy. All questions have\nbeen generated by educational experts rather than crowd workers to ensure they\nare maintaining educational and learning standards. Our analysis and\nexperiments suggest distinguishable differences between our dataset and\ncommonly used ones for question generation for educational purposes. We believe\nthis new dataset can serve as a valuable resource for research and evaluation\nin the educational domain. The dataset and baselines will be released to\nsupport further research in question generation.", "published": "2022-10-12 11:28:34", "link": "http://arxiv.org/abs/2210.06104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Data Augmentation for Translation Suggestion", "abstract": "Translation suggestion (TS) models are used to automatically provide\nalternative suggestions for incorrect spans in sentences generated by machine\ntranslation. This paper introduces the system used in our submission to the\nWMT'22 Translation Suggestion shared task. Our system is based on the ensemble\nof different translation architectures, including Transformer, SA-Transformer,\nand DynamicConv. We use three strategies to construct synthetic data from\nparallel corpora to compensate for the lack of supervised data. In addition, we\nintroduce a multi-phase pre-training strategy, adding an additional\npre-training phase with in-domain data. We rank second and third on the\nEnglish-German and English-Chinese bidirectional tasks, respectively.", "published": "2022-10-12 12:46:43", "link": "http://arxiv.org/abs/2210.06138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotating Norwegian Language Varieties on Twitter for Part-of-Speech", "abstract": "Norwegian Twitter data poses an interesting challenge for Natural Language\nProcessing (NLP) tasks. These texts are difficult for models trained on\nstandardized text in one of the two Norwegian written forms (Bokm{\\aa}l and\nNynorsk), as they contain both the typical variation of social media text, as\nwell as a large amount of dialectal variety. In this paper we present a novel\nNorwegian Twitter dataset annotated with POS-tags. We show that models trained\non Universal Dependency (UD) data perform worse when evaluated against this\ndataset, and that models trained on Bokm{\\aa}l generally perform better than\nthose trained on Nynorsk. We also see that performance on dialectal tweets is\ncomparable to the written standards for some models. Finally we perform a\ndetailed analysis of the errors that models commonly make on this data.", "published": "2022-10-12 12:53:30", "link": "http://arxiv.org/abs/2210.06150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SilverAlign: MT-Based Silver Data Algorithm For Evaluating Word\n  Alignment", "abstract": "Word alignments are essential for a variety of NLP tasks. Therefore, choosing\nthe best approaches for their creation is crucial. However, the scarce\navailability of gold evaluation data makes the choice difficult. We propose\nSilverAlign, a new method to automatically create silver data for the\nevaluation of word aligners by exploiting machine translation and minimal\npairs. We show that performance on our silver data correlates well with gold\nbenchmarks for 9 language pairs, making our approach a valid resource for\nevaluation of different domains and languages when gold data are not available.\nThis addresses the important scenario of missing gold data alignments for\nlow-resource languages.", "published": "2022-10-12 13:48:59", "link": "http://arxiv.org/abs/2210.06207v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Back to the Future: On Potential Histories in NLP", "abstract": "Machine learning and NLP require the construction of datasets to train and\nfine-tune models. In this context, previous work has demonstrated the\nsensitivity of these data sets. For instance, potential societal biases in this\ndata are likely to be encoded and to be amplified in the models we deploy. In\nthis work, we draw from developments in the field of history and take a novel\nperspective on these problems: considering datasets and models through the lens\nof historical fiction surfaces their political nature, and affords\nre-configuring how we view the past, such that marginalized discourses are\nsurfaced. Building on such insights, we argue that contemporary methods for\nmachine learning are prejudiced towards dominant and hegemonic histories.\nEmploying the example of neopronouns, we show that by surfacing marginalized\nhistories within contemporary conditions, we can create models that better\nrepresent the lived realities of traditionally marginalized and excluded\ncommunities.", "published": "2022-10-12 14:32:25", "link": "http://arxiv.org/abs/2210.06245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CIKQA: Learning Commonsense Inference with a Unified\n  Knowledge-in-the-loop QA Paradigm", "abstract": "Recently, the community has achieved substantial progress on many commonsense\nreasoning benchmarks. However, it is still unclear what is learned from the\ntraining process: the knowledge, inference capability, or both? We argue that\ndue to the large scale of commonsense knowledge, it is infeasible to annotate a\nlarge enough training set for each task to cover all commonsense for learning.\nThus we should separate the commonsense knowledge acquisition and inference\nover commonsense knowledge as two separate tasks. In this work, we focus on\ninvestigating models' commonsense inference capabilities from two perspectives:\n(1) Whether models can know if the knowledge they have is enough to solve the\ntask; (2) Whether models can develop commonsense inference capabilities that\ngeneralize across commonsense tasks. We first align commonsense tasks with\nrelevant knowledge from commonsense knowledge bases and ask humans to annotate\nwhether the knowledge is enough or not. Then, we convert different commonsense\ntasks into a unified question answering format to evaluate models'\ngeneralization capabilities. We name the benchmark as Commonsense Inference\nwith Knowledge-in-the-loop Question Answering (CIKQA).", "published": "2022-10-12 14:32:39", "link": "http://arxiv.org/abs/2210.06246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot On-the-Fly Event Schema Induction", "abstract": "What are the events involved in a pandemic outbreak? What steps should be\ntaken when planning a wedding? The answers to these questions can be found by\ncollecting many documents on the complex event of interest, extracting relevant\ninformation, and analyzing it. We present a new approach in which large\nlanguage models are utilized to generate source documents that allow\npredicting, given a high-level event definition, the specific events,\narguments, and relations between them to construct a schema that describes the\ncomplex event in its entirety. Using our model, complete schemas on any topic\ncan be generated on-the-fly without any manual data collection, i.e., in a\nzero-shot manner. Moreover, we develop efficient methods to extract pertinent\ninformation from texts and demonstrate in a series of experiments that these\nschemas are considered to be more complete than human-curated ones in the\nmajority of examined scenarios. Finally, we show that this framework is\ncomparable in performance with previous supervised schema induction methods\nthat rely on collecting real texts while being more general and flexible\nwithout the need for a predefined ontology.", "published": "2022-10-12 14:37:00", "link": "http://arxiv.org/abs/2210.06254v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialoGen: Generalized Long-Range Context Representation for Dialogue\n  Systems", "abstract": "Long-range context modeling is crucial to both dialogue understanding and\ngeneration. The most popular method for dialogue context representation is to\nconcatenate the last-$k$ utterances in chronological order. However, this\nmethod may not be ideal for conversations containing long-range dependencies,\ni.e., when there is a need to look beyond last-$k$ utterances to generate a\nmeaningful response. In this work, we propose DialoGen, a novel encoder-decoder\nbased framework for dialogue generation with a generalized context\nrepresentation that can look beyond the last-$k$ utterances. The main idea of\nour approach is to identify and utilize the most relevant historical utterances\ninstead of last-$k$, which also enables the compact representation of dialogue\nhistory with fewer tokens. We study the effectiveness of our proposed method on\nboth dialogue generation (open-domain) and understanding (DST). Even with a\ncompact context representation, DialoGen performs comparably to the\nstate-of-the-art models on the open-domain DailyDialog dataset. We observe a\nsimilar behavior on the DST task of the MultiWOZ dataset when the proposed\ncontext representation is applied to existing DST models. We also discuss the\ngeneralizability and interpretability of DialoGen and show that the relevance\nscore of previous utterances agrees well with human cognition.", "published": "2022-10-12 15:05:28", "link": "http://arxiv.org/abs/2210.06282v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims\n  on Social Media", "abstract": "We present Reddit Health Online Talk (RedHOT), a corpus of 22,000 richly\nannotated social media posts from Reddit spanning 24 health conditions.\nAnnotations include demarcations of spans corresponding to medical claims,\npersonal experiences, and questions. We collect additional granular annotations\non identified claims. Specifically, we mark snippets that describe patient\nPopulations, Interventions, and Outcomes (PIO elements) within these. Using\nthis corpus, we introduce the task of retrieving trustworthy evidence relevant\nto a given claim made on social media. We propose a new method to automatically\nderive (noisy) supervision for this task which we use to train a dense\nretrieval model; this outperforms baseline models. Manual evaluation of\nretrieval results performed by medical doctors indicate that while our system\nperformance is promising, there is considerable room for improvement. Collected\nannotations (and scripts to assemble the dataset), are available at\nhttps://github.com/sominw/redhot.", "published": "2022-10-12 15:50:32", "link": "http://arxiv.org/abs/2210.06331v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GMP*: Well-Tuned Gradual Magnitude Pruning Can Outperform Most\n  BERT-Pruning Methods", "abstract": "We revisit the performance of the classic gradual magnitude pruning (GMP)\nbaseline for large language models, focusing on the classic BERT benchmark on\nvarious popular tasks. Despite existing evidence in the literature that GMP\nperforms poorly, we show that a simple and general variant, which we call GMP*,\ncan match and sometimes outperform more complex state-of-the-art methods. Our\nresults provide a simple yet strong baseline for future work, highlight the\nimportance of parameter tuning for baselines, and even improve the performance\nof the state-of-the-art second-order pruning method in this setting.", "published": "2022-10-12 16:35:47", "link": "http://arxiv.org/abs/2210.06384v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Text Style Transfer via Style Masked Language Models", "abstract": "Text Style Transfer (TST) is performable through approaches such as latent\nspace disentanglement, cycle-consistency losses, prototype editing etc. The\nprototype editing approach, which is known to be quite successful in TST,\ninvolves two key phases a) Masking of source style-associated tokens and b)\nReconstruction of this source-style masked sentence conditioned with the target\nstyle. We follow a similar transduction method, in which we transpose the more\ndifficult direct source to target TST task to a simpler Style-Masked Language\nModel (SMLM) Task, wherein, similar to BERT \\cite{bert}, the goal of our model\nis now to reconstruct the source sentence from its style-masked version. We\narrive at the SMLM mechanism naturally by formulating prototype editing/\ntransduction methods in a probabilistic framework, where TST resolves into\nestimating a hypothetical parallel dataset from a partially observed parallel\ndataset, wherein each domain is assumed to have a common latent style-masked\nprior. To generate this style-masked prior, we use \"Explainable Attention\" as\nour choice of attribution for a more precise style-masking step and also\nintroduce a cost-effective and accurate \"Attribution-Surplus\" method of\ndetermining the position of masks from any arbitrary attribution model in O(1)\ntime. We empirically show that this non-generational approach well suites the\n\"content preserving\" criteria for a task like TST, even for a complex style\nlike Discourse Manipulation. Our model, the Style MLM, outperforms strong TST\nbaselines and is on par with state-of-the-art TST models, which use complex\narchitectures and orders of more parameters.", "published": "2022-10-12 16:44:06", "link": "http://arxiv.org/abs/2210.06394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus", "abstract": "In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.", "published": "2022-10-12 17:01:53", "link": "http://arxiv.org/abs/2210.06405v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EleutherAI: Going Beyond \"Open Science\" to \"Science in the Open\"", "abstract": "Over the past two years, EleutherAI has established itself as a radically\nnovel initiative aimed at both promoting open-source research and conducting\nresearch in a transparent, openly accessible and collaborative manner.\nEleutherAI's approach to research goes beyond transparency: by doing research\nentirely in public, anyone in the world can observe and contribute at every\nstage. Our work has been received positively and has resulted in several\nhigh-impact projects in Natural Language Processing and other fields. In this\npaper, we describe our experience doing public-facing machine learning\nresearch, the benefits we believe this approach brings, and the pitfalls we\nhave encountered.", "published": "2022-10-12 17:06:53", "link": "http://arxiv.org/abs/2210.06413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Devil is in the Details: On Models and Training Regimes for Few-Shot\n  Intent Classification", "abstract": "Few-shot Intent Classification (FSIC) is one of the key challenges in modular\ntask-oriented dialog systems. While advanced FSIC methods are similar in using\npretrained language models to encode texts and nearest neighbour-based\ninference for classification, these methods differ in details. They start from\ndifferent pretrained text encoders, use different encoding architectures with\nvarying similarity functions, and adopt different training regimes. Coupling\nthese mostly independent design decisions and the lack of accompanying ablation\nstudies are big obstacle to identify the factors that drive the reported FSIC\nperformance. We study these details across three key dimensions: (1) Encoding\narchitectures: Cross-Encoder vs Bi-Encoders; (2) Similarity function:\nParameterized (i.e., trainable) functions vs non-parameterized function; (3)\nTraining regimes: Episodic meta-learning vs the straightforward (i.e.,\nnon-episodic) training. Our experimental results on seven FSIC benchmarks\nreveal three important findings. First, the unexplored combination of the\ncross-encoder architecture (with parameterized similarity scoring function) and\nepisodic meta-learning consistently yields the best FSIC performance. Second,\nEpisodic training yields a more robust FSIC classifier than non-episodic one.\nThird, in meta-learning methods, splitting an episode to support and query sets\nis not a must. Our findings paves the way for conducting state-of-the-art\nresearch in FSIC and more importantly raise the community's attention to\ndetails of FSIC methods. We release our code and data publicly.", "published": "2022-10-12 17:37:54", "link": "http://arxiv.org/abs/2210.06440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Pretrained Language Models (Yet) Reason Deductively?", "abstract": "Acquiring factual knowledge with Pretrained Language Models (PLMs) has\nattracted increasing attention, showing promising performance in many\nknowledge-intensive tasks. Their good performance has led the community to\nbelieve that the models do possess a modicum of reasoning competence rather\nthan merely memorising the knowledge. In this paper, we conduct a comprehensive\nevaluation of the learnable deductive (also known as explicit) reasoning\ncapability of PLMs. Through a series of controlled experiments, we posit two\nmain findings. (i) PLMs inadequately generalise learned logic rules and perform\ninconsistently against simple adversarial surface form edits. (ii) While the\ndeductive reasoning fine-tuning of PLMs does improve their performance on\nreasoning over unseen knowledge facts, it results in catastrophically\nforgetting the previously learnt knowledge. Our main results suggest that PLMs\ncannot yet perform reliable deductive reasoning, demonstrating the importance\nof controlled examinations and probing of PLMs' reasoning abilities; we reach\nbeyond (misleading) task performance, revealing that PLMs are still far from\nhuman-level reasoning capabilities, even for simple deductive tasks.", "published": "2022-10-12 17:44:15", "link": "http://arxiv.org/abs/2210.06442v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Tracking via Effective Use of Multi-Task Learning Model and\n  Mention-guided Decoding", "abstract": "Cross-task knowledge transfer via multi-task learning has recently made\nremarkable progress in general NLP tasks. However, entity tracking on the\nprocedural text has not benefited from such knowledge transfer because of its\ndistinct formulation, i.e., tracking the event flow while following structural\nconstraints. State-of-the-art entity tracking approaches either design\ncomplicated model architectures or rely on task-specific pre-training to\nachieve good results. To this end, we propose MeeT, a Multi-task\nlearning-enabled entity Tracking approach, which utilizes knowledge gained from\ngeneral domain tasks to improve entity tracking. Specifically, MeeT first\nfine-tunes T5, a pre-trained multi-task learning model, with entity\ntracking-specialized QA formats, and then employs our customized decoding\nstrategy to satisfy the structural constraints. MeeT achieves state-of-the-art\nperformances on two popular entity tracking datasets, even though it does not\nrequire any task-specific architecture design or pre-training.", "published": "2022-10-12 17:46:16", "link": "http://arxiv.org/abs/2210.06444v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subword Segmental Language Modelling for Nguni Languages", "abstract": "Subwords have become the standard units of text in NLP, enabling efficient\nopen-vocabulary models. With algorithms like byte-pair encoding (BPE), subword\nsegmentation is viewed as a preprocessing step applied to the corpus before\ntraining. This can lead to sub-optimal segmentations for low-resource languages\nwith complex morphologies. We propose a subword segmental language model (SSLM)\nthat learns how to segment words while being trained for autoregressive\nlanguage modelling. By unifying subword segmentation and language modelling,\nour model learns subwords that optimise LM performance. We train our model on\nthe 4 Nguni languages of South Africa. These are low-resource agglutinative\nlanguages, so subword information is critical. As an LM, SSLM outperforms\nexisting approaches such as BPE-based models on average across the 4 languages.\nFurthermore, it outperforms standard subword segmenters on unsupervised\nmorphological segmentation. We also train our model as a word-level sequence\nmodel, resulting in an unsupervised morphological segmenter that outperforms\nexisting methods by a large margin for all 4 languages. Our results show that\nlearning subword segmentation is an effective alternative to existing subword\nsegmenters, enabling the model to discover morpheme-like subwords that improve\nits LM capabilities.", "published": "2022-10-12 18:41:00", "link": "http://arxiv.org/abs/2210.06525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing a general-purpose clinical language inference model from a\n  large corpus of clinical notes", "abstract": "Several biomedical language models have already been developed for clinical\nlanguage inference. However, these models typically utilize general\nvocabularies and are trained on relatively small clinical corpora. We sought to\nevaluate the impact of using a domain-specific vocabulary and a large clinical\ntraining corpus on the performance of these language models in clinical\nlanguage inference. We trained a Bidirectional Encoder Decoder from\nTransformers (BERT) model using a diverse, deidentified corpus of 75 million\ndeidentified clinical notes authored at the University of California, San\nFrancisco (UCSF). We evaluated this model on several clinical language\ninference benchmark tasks: clinical and temporal concept recognition, relation\nextraction and medical language inference. We also evaluated our model on two\ntasks using discharge summaries from UCSF: diagnostic code assignment and\ntherapeutic class inference. Our model performs at par with the best publicly\navailable biomedical language models of comparable sizes on the public\nbenchmark tasks, and is significantly better than these models in a\nwithin-system evaluation on the two tasks using UCSF data. The use of in-domain\nvocabulary appears to improve the encoding of longer documents. The use of\nlarge clinical corpora appears to enhance document encoding and inferential\naccuracy. However, further research is needed to improve abbreviation\nresolution, and numerical, temporal, and implicitly causal inference.", "published": "2022-10-12 20:08:45", "link": "http://arxiv.org/abs/2210.06566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DATScore: Evaluating Translation with Data Augmented Translations", "abstract": "The rapid development of large pretrained language models has revolutionized\nnot only the field of Natural Language Generation (NLG) but also its\nevaluation. Inspired by the recent work of BARTScore: a metric leveraging the\nBART language model to evaluate the quality of generated text from various\naspects, we introduce DATScore. DATScore uses data augmentation techniques to\nimprove the evaluation of machine translation. Our main finding is that\nintroducing data augmented translations of the source and reference texts is\ngreatly helpful in evaluating the quality of the generated translation. We also\npropose two novel score averaging and term weighting strategies to improve the\noriginal score computing process of BARTScore. Experimental results on WMT show\nthat DATScore correlates better with human meta-evaluations than the other\nrecent state-of-the-art metrics, especially for low-resource languages.\nAblation studies demonstrate the value added by our new scoring strategies.\nMoreover, we report in our extended experiments the performance of DATScore on\n3 NLG tasks other than translation.", "published": "2022-10-12 20:31:42", "link": "http://arxiv.org/abs/2210.06576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Question Answering with Generation of NQ-like Questions", "abstract": "Question Answering (QA) systems require a large amount of annotated data\nwhich is costly and time-consuming to gather. Converting datasets of existing\nQA benchmarks are challenging due to different formats and complexities. To\naddress these issues, we propose an algorithm to automatically generate shorter\nquestions resembling day-to-day human communication in the Natural Questions\n(NQ) dataset from longer trivia questions in Quizbowl (QB) dataset by\nleveraging conversion in style among the datasets. This provides an automated\nway to generate more data for our QA systems. To ensure quality as well as\nquantity of data, we detect and remove ill-formed questions using a neural\nclassifier. We demonstrate that in a low resource setting, using the generated\ndata improves the QA performance over the baseline system on both NQ and QB\ndata. Our algorithm improves the scalability of training data while maintaining\nquality of data for QA systems.", "published": "2022-10-12 21:36:20", "link": "http://arxiv.org/abs/2210.06599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Document-level Information Extraction via Imitation Learning", "abstract": "We present a novel iterative extraction model, IterX, for extracting complex\nrelations, or templates (i.e., N-tuples representing a mapping from named slots\nto spans of text) within a document. Documents may feature zero or more\ninstances of a template of any given type, and the task of template extraction\nentails identifying the templates in a document and extracting each template's\nslot values. Our imitation learning approach casts the problem as a Markov\ndecision process (MDP), and relieves the need to use predefined template orders\nto train an extractor. It leads to state-of-the-art results on two established\nbenchmarks -- 4-ary relation extraction on SciREX and template extraction on\nMUC-4 -- as well as a strong baseline on the new BETTER Granular task.", "published": "2022-10-12 21:46:04", "link": "http://arxiv.org/abs/2210.06600v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis", "abstract": "Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis\ntask which involves four elements from user-generated texts: aspect term,\naspect category, opinion term, and sentiment polarity. Most computational\napproaches focus on some of the ABSA sub-tasks such as tuple (aspect term,\nsentiment polarity) or triplet (aspect term, opinion term, sentiment polarity)\nextraction using either pipeline or joint modeling approaches. Recently,\ngenerative approaches have been proposed to extract all four elements as (one\nor more) quadruplets from text as a single task. In this work, we take a step\nfurther and propose a unified framework for solving ABSA, and the associated\nsub-tasks to improve the performance in few-shot scenarios. To this end, we\nfine-tune a T5 model with instructional prompts in a multi-task learning\nfashion covering all the sub-tasks, as well as the entire quadruple prediction\ntask. In experiments with multiple benchmark datasets, we show that the\nproposed multi-task prompting approach brings performance boost (by absolute\n8.29 F1) in the few-shot learning setting.", "published": "2022-10-12 23:38:57", "link": "http://arxiv.org/abs/2210.06629v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perplexity from PLM Is Unreliable for Evaluating Text Quality", "abstract": "Recently, amounts of works utilize perplexity~(PPL) to evaluate the quality\nof the generated text. They suppose that if the value of PPL is smaller, the\nquality(i.e. fluency) of the text to be evaluated is better. However, we find\nthat the PPL referee is unqualified and it cannot evaluate the generated text\nfairly for the following reasons: (i) The PPL of short text is larger than long\ntext, which goes against common sense, (ii) The repeated text span could damage\nthe performance of PPL, and (iii) The punctuation marks could affect the\nperformance of PPL heavily. Experiments show that the PPL is unreliable for\nevaluating the quality of given text. Last, we discuss the key problems with\nevaluating text quality using language models.", "published": "2022-10-12 03:13:28", "link": "http://arxiv.org/abs/2210.05892v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval\n  on Predefined Topics", "abstract": "In this paper, we consider the task of retrieving documents with predefined\ntopics from an unlabeled document dataset using an unsupervised approach. The\nproposed unsupervised approach requires only a small number of keywords\ndescribing the respective topics and no labeled document. Existing approaches\neither heavily relied on a large amount of additionally encoded world knowledge\nor on term-document frequencies. Contrariwise, we introduce a method that\nlearns jointly embedded document and word vectors solely from the unlabeled\ndocument dataset in order to find documents that are semantically similar to\nthe topics described by the keywords. The proposed method requires almost no\ntext preprocessing but is simultaneously effective at retrieving relevant\ndocuments with high probability. When successively retrieving documents on\ndifferent predefined topics from publicly available and commonly used datasets,\nwe achieved an average area under the receiver operating characteristic curve\nvalue of 0.95 on one dataset and 0.92 on another. Further, our method can be\nused for multiclass document classification, without the need to assign labels\nto the dataset in advance. Compared with an unsupervised classification\nbaseline, we increased F1 scores from 76.6 to 82.7 and from 61.0 to 75.1 on the\nrespective datasets. For easy replication of our approach, we make the\ndeveloped Lbl2Vec code publicly available as a ready-to-use tool under the\n3-Clause BSD license.", "published": "2022-10-12 08:57:01", "link": "http://arxiv.org/abs/2210.06023v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating Massive Multilingual Pre-Trained Machine Translation\n  Models for Clinical Domain via Transfer Learning", "abstract": "Massively multilingual pre-trained language models (MMPLMs) are developed in\nrecent years demonstrating superpowers and the pre-knowledge they acquire for\ndownstream tasks. This work investigates whether MMPLMs can be applied to\nclinical domain machine translation (MT) towards entirely unseen languages via\ntransfer learning. We carry out an experimental investigation using Meta-AI's\nMMPLMs ``wmt21-dense-24-wide-en-X and X-en (WMT21fb)'' which were pre-trained\non 7 language pairs and 14 translation directions including English to Czech,\nGerman, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite\ndirection. We fine-tune these MMPLMs towards English-\\textit{Spanish} language\npair which \\textit{did not exist at all} in their original pre-trained corpora\nboth implicitly and explicitly. We prepare carefully aligned \\textit{clinical}\ndomain data for this fine-tuning, which is different from their original mixed\ndomain knowledge. Our experimental result shows that the fine-tuning is very\nsuccessful using just 250k well-aligned in-domain EN-ES segments for three\nsub-task translation testings: clinical cases, clinical terms, and ontology\nconcepts. It achieves very close evaluation scores to another MMPLM NLLB from\nMeta-AI, which included Spanish as a high-resource setting in the pre-training.\nTo the best of our knowledge, this is the first work on using MMPLMs towards\n\\textit{clinical domain transfer-learning NMT} successfully for totally unseen\nlanguages during pre-training.", "published": "2022-10-12 10:19:44", "link": "http://arxiv.org/abs/2210.06068v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Federated Continual Learning for Text Classification via Selective\n  Inter-client Transfer", "abstract": "In this work, we combine the two paradigms: Federated Learning (FL) and\nContinual Learning (CL) for text classification task in cloud-edge continuum.\nThe objective of Federated Continual Learning (FCL) is to improve deep learning\nmodels over life time at each client by (relevant and efficient) knowledge\ntransfer without sharing data. Here, we address challenges in minimizing\ninter-client interference while knowledge sharing due to heterogeneous tasks\nacross clients in FCL setup. In doing so, we propose a novel framework,\nFederated Selective Inter-client Transfer (FedSeIT) which selectively combines\nmodel parameters of foreign clients. To further maximize knowledge transfer, we\nassess domain overlap and select informative tasks from the sequence of\nhistorical tasks at each foreign client while preserving privacy. Evaluating\nagainst the baselines, we show improved performance, a gain of (average) 12.4\\%\nin text classification over a sequence of tasks using five datasets from\ndiverse domains. To the best of our knowledge, this is the first work that\napplies FCL to NLP.", "published": "2022-10-12 11:24:13", "link": "http://arxiv.org/abs/2210.06101v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich\n  Document Understanding", "abstract": "Recent years have witnessed the rise and success of pre-training techniques\nin visually-rich document understanding. However, most existing methods lack\nthe systematic mining and utilization of layout-centered knowledge, leading to\nsub-optimal performances. In this paper, we propose ERNIE-Layout, a novel\ndocument pre-training solution with layout knowledge enhancement in the whole\nworkflow, to learn better representations that combine the features from text,\nlayout, and image. Specifically, we first rearrange input sequences in the\nserialization stage, and then present a correlative pre-training task, reading\norder prediction, to learn the proper reading order of documents. To improve\nthe layout awareness of the model, we integrate a spatial-aware disentangled\nattention into the multi-modal transformer and a replaced regions prediction\ntask into the pre-training phase. Experimental results show that ERNIE-Layout\nachieves superior performance on various downstream tasks, setting new\nstate-of-the-art on key information extraction, document image classification,\nand document question answering datasets. The code and models are publicly\navailable at\nhttp://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-layout.", "published": "2022-10-12 12:59:24", "link": "http://arxiv.org/abs/2210.06155v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Focusing on Context is NICE: Improving Overshadowed Entity\n  Disambiguation", "abstract": "Entity disambiguation (ED) is the task of mapping an ambiguous entity mention\nto the corresponding entry in a structured knowledge base. Previous research\nshowed that entity overshadowing is a significant challenge for existing ED\nmodels: when presented with an ambiguous entity mention, the models are much\nmore likely to rank a more frequent yet less contextually relevant entity at\nthe top. Here, we present NICE, an iterative approach that uses entity type\ninformation to leverage context and avoid over-relying on the frequency-based\nprior. Our experiments show that NICE achieves the best performance results on\nthe overshadowed entities while still performing competitively on the frequent\nentities.", "published": "2022-10-12 13:05:37", "link": "http://arxiv.org/abs/2210.06164v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Pruning Pre-trained Language Models Without Fine-Tuning", "abstract": "To overcome the overparameterized problem in Pre-trained Language Models\n(PLMs), pruning is widely used as a simple and straightforward compression\nmethod by directly removing unimportant weights. Previous first-order methods\nsuccessfully compress PLMs to extremely high sparsity with little performance\ndrop. These methods, such as movement pruning, use first-order information to\nprune PLMs while fine-tuning the remaining weights. In this work, we argue\nfine-tuning is redundant for first-order pruning, since first-order pruning is\nsufficient to converge PLMs to downstream tasks without fine-tuning. Under this\nmotivation, we propose Static Model Pruning (SMP), which only uses first-order\npruning to adapt PLMs to downstream tasks while achieving the target sparsity\nlevel. In addition, we also design a new masking function and training\nobjective to further improve SMP. Extensive experiments at various sparsity\nlevels show SMP has significant improvements over first-order and zero-order\nmethods. Unlike previous first-order methods, SMP is also applicable to low\nsparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter\nefficient than other methods due to it does not require fine-tuning.", "published": "2022-10-12 13:58:38", "link": "http://arxiv.org/abs/2210.06210v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Formal Semantic Geometry over Transformer-based Variational AutoEncoder", "abstract": "Formal/symbolic semantics can provide canonical, rigid controllability and\ninterpretability to sentence representations due to their \\textit{localisation}\nor \\textit{composition} property. How can we deliver such property to the\ncurrent distributional sentence representations to control and interpret the\ngeneration of language models (LMs)? In this work, we theoretically frame the\nsentence semantics as the composition of \\textit{semantic role - word content}\nfeatures and propose the formal semantic geometry. To inject such geometry into\nTransformer-based LMs (i.e. GPT2), we deploy Transformer-based Variational\nAutoEncoder with a supervision approach, where the sentence generation can be\nmanipulated and explained over low-dimensional latent Gaussian space. In\naddition, we propose a new probing algorithm to guide the movement of sentence\nvectors over such geometry. Experimental results reveal that the formal\nsemantic geometry can potentially deliver better control and interpretation to\nsentence generation.", "published": "2022-10-12 14:20:33", "link": "http://arxiv.org/abs/2210.06230v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TwiRGCN: Temporally Weighted Graph Convolution for Question Answering\n  over Temporal Knowledge Graphs", "abstract": "Recent years have witnessed much interest in temporal reasoning over\nknowledge graphs (KG) for complex question answering (QA), but there remains a\nsubstantial gap in human capabilities. We explore how to generalize relational\ngraph convolutional networks (RGCN) for temporal KGQA. Specifically, we propose\na novel, intuitive and interpretable scheme to modulate the messages passed\nthrough a KG edge during convolution, based on the relevance of its associated\ntime period to the question. We also introduce a gating device to predict if\nthe answer to a complex temporal question is likely to be a KG entity or time\nand use this prediction to guide our scoring mechanism. We evaluate the\nresulting system, which we call TwiRGCN, on TimeQuestions, a recently released,\nchallenging dataset for multi-hop complex temporal QA. We show that TwiRGCN\nsignificantly outperforms state-of-the-art systems on this dataset across\ndiverse question types. Notably, TwiRGCN improves accuracy by 9--10 percentage\npoints for the most difficult ordinal and implicit question types.", "published": "2022-10-12 15:03:49", "link": "http://arxiv.org/abs/2210.06281v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Context Generation Improves Open Domain Question Answering", "abstract": "Closed-book question answering (QA) requires a model to directly answer an\nopen-domain question without access to any external knowledge. Prior work on\nclosed-book QA either directly finetunes or prompts a pretrained language model\n(LM) to leverage the stored knowledge. However, they do not fully exploit the\nparameterized knowledge. To address this issue, we propose a two-stage,\nclosed-book QA framework which employs a coarse-to-fine approach to extract\nrelevant knowledge and answer a question. Our approach first generates a\nrelated context for a given question by prompting a pretrained LM. We then\nprompt the same LM for answer prediction using the generated context and the\nquestion. Additionally, to eliminate failure caused by context uncertainty, we\nmarginalize over generated contexts. Experimental results on three QA\nbenchmarks show that our method significantly outperforms previous closed-book\nQA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book\nmethods that exploit external knowledge sources (e.g. 68.6% vs. 68.0%). Our\nmethod is able to better exploit the stored knowledge in pretrained LMs without\nadding extra learnable parameters or needing finetuning, and paves the way for\nhybrid models that integrate pretrained LMs with external knowledge.", "published": "2022-10-12 16:00:50", "link": "http://arxiv.org/abs/2210.06349v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Probing Commonsense Knowledge in Pre-trained Language Models with\n  Sense-level Precision and Expanded Vocabulary", "abstract": "Progress on commonsense reasoning is usually measured from performance\nimprovements on Question Answering tasks designed to require commonsense\nknowledge. However, fine-tuning large Language Models (LMs) on these specific\ntasks does not directly evaluate commonsense learned during pre-training. The\nmost direct assessments of commonsense knowledge in pre-trained LMs are\narguably cloze-style tasks targeting commonsense assertions (e.g., A pen is\nused for [MASK].). However, this approach is restricted by the LM's vocabulary\navailable for masked predictions, and its precision is subject to the context\nprovided by the assertion. In this work, we present a method for enriching LMs\nwith a grounded sense inventory (i.e., WordNet) available at the vocabulary\nlevel, without further training. This modification augments the prediction\nspace of cloze-style prompts to the size of a large ontology while enabling\nfiner-grained (sense-level) queries and predictions. In order to evaluate LMs\nwith higher precision, we propose SenseLAMA, a cloze-style task featuring\nverbalized relations from disambiguated triples sourced from WordNet, WikiData,\nand ConceptNet. Applying our method to BERT, producing a WordNet-enriched\nversion named SynBERT, we find that LMs can learn non-trivial commonsense\nknowledge from self-supervision, covering numerous relations, and more\neffectively than comparable similarity-based approaches.", "published": "2022-10-12 16:26:59", "link": "http://arxiv.org/abs/2210.06376v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "One does not fit all! On the Complementarity of Vision Encoders for\n  Vision and Language Tasks", "abstract": "Current multimodal models, aimed at solving Vision and Language (V+L) tasks,\npredominantly repurpose Vision Encoders (VE) as feature extractors. While many\nVEs -- of different architectures, trained on different data and objectives --\nare publicly available, they are not designed for the downstream V+L tasks.\nNonetheless, most current work assumes that a \\textit{single} pre-trained VE\ncan serve as a general-purpose encoder. In this work, we focus on analysis and\naim to understand whether the information stored within different VEs is\ncomplementary, i.e. if providing the model with features from multiple VEs can\nimprove the performance on a target task, and how they are combined. We\nexhaustively experiment with three popular VEs on six downstream V+L tasks and\nanalyze the attention and VE-dropout patterns. Our analyses suggest that\ndiverse VEs complement each other, resulting in improved downstream V+L task\nperformance, where the improvements are not due to simple ensemble effects\n(i.e. the performance does not always improve when increasing the number of\nencoders). We demonstrate that future VEs, which are not \\textit{repurposed},\nbut explicitly \\textit{designed} for V+L tasks, have the potential of improving\nperformance on the target V+L tasks.", "published": "2022-10-12 16:31:39", "link": "http://arxiv.org/abs/2210.06379v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PriMeSRL-Eval: A Practical Quality Metric for Semantic Role Labeling\n  Systems Evaluation", "abstract": "Semantic role labeling (SRL) identifies the predicate-argument structure in a\nsentence. This task is usually accomplished in four steps: predicate\nidentification, predicate sense disambiguation, argument identification, and\nargument classification. Errors introduced at one step propagate to later\nsteps. Unfortunately, the existing SRL evaluation scripts do not consider the\nfull effect of this error propagation aspect. They either evaluate arguments\nindependent of predicate sense (CoNLL09) or do not evaluate predicate sense at\nall (CoNLL05), yielding an inaccurate SRL model performance on the argument\nclassification task. In this paper, we address key practical issues with\nexisting evaluation scripts and propose a more strict SRL evaluation metric\nPriMeSRL. We observe that by employing PriMeSRL, the quality evaluation of all\nSoTA SRL models drops significantly, and their relative rankings also change.\nWe also show that PriMeSRLsuccessfully penalizes actual failures in SoTA SRL\nmodels.", "published": "2022-10-12 17:04:28", "link": "http://arxiv.org/abs/2210.06408v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MiniALBERT: Model Distillation via Parameter-Efficient Recursive\n  Transformers", "abstract": "Pre-trained Language Models (LMs) have become an integral part of Natural\nLanguage Processing (NLP) in recent years, due to their superior performance in\ndownstream applications. In spite of this resounding success, the usability of\nLMs is constrained by computational and time complexity, along with their\nincreasing size; an issue that has been referred to as `overparameterisation'.\nDifferent strategies have been proposed in the literature to alleviate these\nproblems, with the aim to create effective compact models that nearly match the\nperformance of their bloated counterparts with negligible performance losses.\nOne of the most popular techniques in this area of research is model\ndistillation. Another potent but underutilised technique is cross-layer\nparameter sharing. In this work, we combine these two strategies and present\nMiniALBERT, a technique for converting the knowledge of fully parameterised LMs\n(such as BERT) into a compact recursive student. In addition, we investigate\nthe application of bottleneck adapters for layer-wise adaptation of our\nrecursive student, and also explore the efficacy of adapter tuning for\nfine-tuning of compact models. We test our proposed models on a number of\ngeneral and biomedical NLP tasks to demonstrate their viability and compare\nthem with the state-of-the-art and other existing compact models. All the codes\nused in the experiments are available at\nhttps://github.com/nlpie-research/MiniALBERT. Our pre-trained compact models\ncan be accessed from https://huggingface.co/nlpie.", "published": "2022-10-12 17:23:21", "link": "http://arxiv.org/abs/2210.06425v2", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Are Sample-Efficient NLP Models More Robust?", "abstract": "Recent results in image classification and extractive question answering have\nobserved that pre-trained models trained on less in-distribution data have\nbetter out-of-distribution performance. However, it is unclear how broadly\nthese trends hold. We conduct a large empirical study across three tasks, three\nbroadly-applicable modeling interventions (increasing model size, using a\ndifferent adaptation method, and pre-training on more data), and 14 diverse\ndatasets to investigate the relationship between sample efficiency (amount of\ndata needed to reach a given ID accuracy) and robustness (how models fare on\nOOD evaluation). We find that higher sample efficiency is only correlated with\nbetter average OOD robustness on some modeling interventions and tasks, but not\nothers. On individual datasets, models with lower sample efficiency can even be\nmore robust. These results suggest that general-purpose methods for improving\nsample efficiency are unlikely to yield universal OOD robustness improvements,\nsince such improvements are highly dataset- and task-dependent. Even in an era\nof large, multi-purpose pretrained models, task-specific decisions may often be\nnecessary for OOD generalization.", "published": "2022-10-12 17:54:59", "link": "http://arxiv.org/abs/2210.06456v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Better Smatch = Better Parser? AMR evaluation is not so simple anymore", "abstract": "Recently, astonishing advances have been observed in AMR parsing, as measured\nby the structural Smatch metric. In fact, today's systems achieve performance\nlevels that seem to surpass estimates of human inter annotator agreement (IAA).\nTherefore, it is unclear how well Smatch (still) relates to human estimates of\nparse quality, as in this situation potentially fine-grained errors of similar\nweight may impact the AMR's meaning to different degrees.\n  We conduct an analysis of two popular and strong AMR parsers that --\naccording to Smatch -- reach quality levels on par with human IAA, and assess\nhow human quality ratings relate to Smatch and other AMR metrics. Our main\nfindings are: i) While high Smatch scores indicate otherwise, we find that AMR\nparsing is far from being solved: we frequently find structurally small, but\nsemantically unacceptable errors that substantially distort sentence meaning.\nii) Considering high-performance parsers, better Smatch scores may not\nnecessarily indicate consistently better parsing quality. To obtain a\nmeaningful and comprehensive assessment of quality differences of parse(r)s, we\nrecommend augmenting evaluations with macro statistics, use of additional\nmetrics, and more human analysis.", "published": "2022-10-12 17:57:48", "link": "http://arxiv.org/abs/2210.06461v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual textual data: an approach through multiple factor analysis", "abstract": "This paper focuses on the analysis of open-ended questions answered in\ndifferent languages. Closed-ended questions, called contextual variables, are\nasked to all respondents in order to understand the relationships between the\nfree and the closed responses among the different samples since the latter\nassumably affect the word choices. We have developed \"Multiple Factor Analysis\non Generalized Aggregated Lexical Tables\" (MFA-GALT) to jointly study the\nopen-ended responses in different languages through the relationships between\nthe choice of words and the variables that drive this choice. MFA-GALT studies\nif variability among words is structured in the same way by variability among\nvariables, and inversely, from one sample to another. An application on an\ninternational satisfaction survey shows the easy-to-interpret results that are\nproposed.", "published": "2022-10-12 18:46:00", "link": "http://arxiv.org/abs/2210.06527v1", "categories": ["cs.CL", "stat.ME"], "primary_category": "cs.CL"}
{"title": "OpenCQA: Open-ended Question Answering with Charts", "abstract": "Charts are very popular to analyze data and convey important insights. People\noften analyze visualizations to answer open-ended questions that require\nexplanatory answers. Answering such questions are often difficult and\ntime-consuming as it requires a lot of cognitive and perceptual efforts. To\naddress this challenge, we introduce a new task called OpenCQA, where the goal\nis to answer an open-ended question about a chart with descriptive texts. We\npresent the annotation process and an in-depth analysis of our dataset. We\nimplement and evaluate a set of baselines under three practical settings. In\nthe first setting, a chart and the accompanying article is provided as input to\nthe model. The second setting provides only the relevant paragraph(s) to the\nchart instead of the entire article, whereas the third setting requires the\nmodel to generate an answer solely based on the chart. Our analysis of the\nresults show that the top performing models generally produce fluent and\ncoherent text while they struggle to perform complex logical and arithmetic\nreasoning.", "published": "2022-10-12 23:37:30", "link": "http://arxiv.org/abs/2210.06628v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Language Agnostic Multilingual Information Retrieval with Contrastive\n  Learning", "abstract": "Multilingual information retrieval (IR) is challenging since annotated\ntraining data is costly to obtain in many languages. We present an effective\nmethod to train multilingual IR systems when only English IR training data and\nsome parallel corpora between English and other languages are available. We\nleverage parallel and non-parallel corpora to improve the pretrained\nmultilingual language models' cross-lingual transfer ability. We design a\nsemantic contrastive loss to align representations of parallel sentences that\nshare the same semantics in different languages, and a new language contrastive\nloss to leverage parallel sentence pairs to remove language-specific\ninformation in sentence representations from non-parallel corpora. When trained\non English IR data with these losses and evaluated zero-shot on non-English\ndata, our model demonstrates significant improvement to prior work on retrieval\nperformance, while it requires much less computational effort. We also\ndemonstrate the value of our model for a practical setting when a parallel\ncorpus is only available for a few languages, but a lack of parallel corpora\nresources persists for many other low-resource languages. Our model can work\nwell even with a small number of parallel sentences, and be used as an add-on\nmodule to any backbones and other tasks.", "published": "2022-10-12 23:53:50", "link": "http://arxiv.org/abs/2210.06633v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses", "abstract": "Recently, substantial progress has been made in text ranking based on\npretrained language models such as BERT. However, there are limited studies on\nhow to leverage more powerful sequence-to-sequence models such as T5. Existing\nattempts usually formulate text ranking as classification and rely on\npostprocessing to obtain a ranked list. In this paper, we propose RankT5 and\nstudy two T5-based ranking model structures, an encoder-decoder and an\nencoder-only one, so that they not only can directly output ranking scores for\neach query-document pair, but also can be fine-tuned with \"pairwise\" or\n\"listwise\" ranking losses to optimize ranking performances. Our experiments\nshow that the proposed models with ranking losses can achieve substantial\nranking performance gains on different public text ranking data sets. Moreover,\nwhen fine-tuned with listwise ranking losses, the ranking model appears to have\nbetter zero-shot ranking performance on out-of-domain data sets compared to the\nmodel fine-tuned with classification losses.", "published": "2022-10-12 20:51:49", "link": "http://arxiv.org/abs/2210.10634v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Hate-CLIPper: Multimodal Hateful Meme Classification based on\n  Cross-modal Interaction of CLIP Features", "abstract": "Hateful memes are a growing menace on social media. While the image and its\ncorresponding text in a meme are related, they do not necessarily convey the\nsame meaning when viewed individually. Hence, detecting hateful memes requires\ncareful consideration of both visual and textual information. Multimodal\npre-training can be beneficial for this task because it effectively captures\nthe relationship between the image and the text by representing them in a\nsimilar feature space. Furthermore, it is essential to model the interactions\nbetween the image and text features through intermediate fusion. Most existing\nmethods either employ multimodal pre-training or intermediate fusion, but not\nboth. In this work, we propose the Hate-CLIPper architecture, which explicitly\nmodels the cross-modal interactions between the image and text representations\nobtained using Contrastive Language-Image Pre-training (CLIP) encoders via a\nfeature interaction matrix (FIM). A simple classifier based on the FIM\nrepresentation is able to achieve state-of-the-art performance on the Hateful\nMemes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the\nhuman performance of 82.65. Experiments on other meme datasets such as\nPropaganda Memes and TamilMemes also demonstrate the generalizability of the\nproposed approach. Finally, we analyze the interpretability of the FIM\nrepresentation and show that cross-modal interactions can indeed facilitate the\nlearning of meaningful concepts. The code for this work is available at\nhttps://github.com/gokulkarthik/hateclipper.", "published": "2022-10-12 04:34:54", "link": "http://arxiv.org/abs/2210.05916v3", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Text-Derived Knowledge Helps Vision: A Simple Cross-modal Distillation\n  for Video-based Action Anticipation", "abstract": "Anticipating future actions in a video is useful for many autonomous and\nassistive technologies. Most prior action anticipation work treat this as a\nvision modality problem, where the models learn the task information primarily\nfrom the video features in the action anticipation datasets. However, knowledge\nabout action sequences can also be obtained from external textual data. In this\nwork, we show how knowledge in pretrained language models can be adapted and\ndistilled into vision-based action anticipation models. We show that a simple\ndistillation technique can achieve effective knowledge transfer and provide\nconsistent gains on a strong vision model (Anticipative Vision Transformer) for\ntwo action anticipation datasets (3.5% relative gain on EGTEA-GAZE+ and 7.2%\nrelative gain on EPIC-KITCHEN 55), giving a new state-of-the-art result.", "published": "2022-10-12 08:02:11", "link": "http://arxiv.org/abs/2210.05991v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-Granularity Cross-modal Alignment for Generalized Medical Visual\n  Representation Learning", "abstract": "Learning medical visual representations directly from paired radiology\nreports has become an emerging topic in representation learning. However,\nexisting medical image-text joint learning methods are limited by instance or\nlocal supervision analysis, ignoring disease-level semantic correspondences. In\nthis paper, we present a novel Multi-Granularity Cross-modal Alignment (MGCA)\nframework for generalized medical visual representation learning by harnessing\nthe naturally exhibited semantic correspondences between medical image and\nradiology reports at three different levels, i.e., pathological region-level,\ninstance-level, and disease-level. Specifically, we first incorporate the\ninstance-wise alignment module by maximizing the agreement between image-report\npairs. Further, for token-wise alignment, we introduce a bidirectional\ncross-attention strategy to explicitly learn the matching between fine-grained\nvisual tokens and text tokens, followed by contrastive learning to align them.\nMore important, to leverage the high-level inter-subject relationship semantic\n(e.g., disease) correspondences, we design a novel cross-modal disease-level\nalignment paradigm to enforce the cross-modal cluster assignment consistency.\nExtensive experimental results on seven downstream medical image datasets\ncovering image classification, object detection, and semantic segmentation\ntasks demonstrate the stable and superior performance of our framework.", "published": "2022-10-12 09:31:39", "link": "http://arxiv.org/abs/2210.06044v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Summary on the ISCSLP 2022 Chinese-English Code-Switching ASR Challenge", "abstract": "Code-switching automatic speech recognition becomes one of the most\nchallenging and the most valuable scenarios of automatic speech recognition,\ndue to the code-switching phenomenon between multilingual language and the\nfrequent occurrence of code-switching phenomenon in daily life. The ISCSLP 2022\nChinese-English Code-Switching Automatic Speech Recognition (CSASR) Challenge\naims to promote the development of code-switching automatic speech recognition.\nThe ISCSLP 2022 CSASR challenge provided two training sets, TAL_CSASR corpus\nand MagicData-RAMC corpus, a development and a test set for participants, which\nare used for CSASR model training and evaluation. Along with the challenge, we\nalso provide the baseline system performance for reference. As a result, more\nthan 40 teams participated in this challenge, and the winner team achieved\n16.70% Mixture Error Rate (MER) performance on the test set and has achieved\n9.8% MER absolute improvement compared with the baseline system. In this paper,\nwe will describe the datasets, the associated baselines system and the\nrequirements, and summarize the CSASR challenge results and major techniques\nand tricks used in the submitted systems.", "published": "2022-10-12 11:05:13", "link": "http://arxiv.org/abs/2210.06091v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards visually prompted keyword localisation for zero-resource spoken\n  languages", "abstract": "Imagine being able to show a system a visual depiction of a keyword and\nfinding spoken utterances that contain this keyword from a zero-resource speech\ncorpus. We formalise this task and call it visually prompted keyword\nlocalisation (VPKL): given an image of a keyword, detect and predict where in\nan utterance the keyword occurs. To do VPKL, we propose a speech-vision model\nwith a novel localising attention mechanism which we train with a new keyword\nsampling scheme. We show that these innovations give improvements in VPKL over\nan existing speech-vision model. We also compare to a visual bag-of-words (BoW)\nmodel where images are automatically tagged with visual labels and paired with\nunlabelled speech. Although this visual BoW can be queried directly with a\nwritten keyword (while our's takes image queries), our new model still\noutperforms the visual BoW in both detection and localisation, giving a 16%\nrelative improvement in localisation F1.", "published": "2022-10-12 14:17:34", "link": "http://arxiv.org/abs/2210.06229v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A context-aware knowledge transferring strategy for CTC-based ASR", "abstract": "Non-autoregressive automatic speech recognition (ASR) modeling has received\nincreasing attention recently because of its fast decoding speed and superior\nperformance. Among representatives, methods based on the connectionist temporal\nclassification (CTC) are still a dominating stream. However, the theoretically\ninherent flaw, the assumption of independence between tokens, creates a\nperformance barrier for the school of works. To mitigate the challenge, we\npropose a context-aware knowledge transferring strategy, consisting of a\nknowledge transferring module and a context-aware training strategy, for\nCTC-based ASR. The former is designed to distill linguistic information from a\npre-trained language model, and the latter is framed to modulate the\nlimitations caused by the conditional independence assumption. As a result, a\nknowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is\npresented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2\ndatasets demonstrate the effectiveness of the proposed method.", "published": "2022-10-12 14:31:38", "link": "http://arxiv.org/abs/2210.06244v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Task Compass: Scaling Multi-task Pre-training with Task Prefix", "abstract": "Leveraging task-aware annotated data as supervised signals to assist with\nself-supervised learning on large-scale unlabeled data has become a new trend\nin pre-training language models. Existing studies show that multi-task learning\nwith large-scale supervised tasks suffers from negative effects across tasks.\nTo tackle the challenge, we propose a task prefix guided multi-task\npre-training framework to explore the relationships among tasks. We conduct\nextensive experiments on 40 datasets, which show that our model can not only\nserve as the strong foundation backbone for a wide range of tasks but also be\nfeasible as a probing tool for analyzing task relationships. The task\nrelationships reflected by the prefixes align transfer learning performance\nbetween tasks. They also suggest directions for data augmentation with\ncomplementary tasks, which help our model achieve human-parity results on\ncommonsense reasoning leaderboards. Code is available at\nhttps://github.com/cooelf/CompassMTL", "published": "2022-10-12 15:02:04", "link": "http://arxiv.org/abs/2210.06277v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in\n  Transformers", "abstract": "This paper studies the curious phenomenon for machine learning models with\nTransformer architectures that their activation maps are sparse. By activation\nmap we refer to the intermediate output of the multi-layer perceptrons (MLPs)\nafter a ReLU activation function, and by sparse we mean that on average very\nfew entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each\ninput to MLP. Moreover, larger Transformers with more layers and wider MLP\nhidden dimensions are sparser as measured by the percentage of nonzero entries.\nThrough extensive experiments we demonstrate that the emergence of sparsity is\na prevalent phenomenon that occurs for both natural language processing and\nvision tasks, on both training and evaluation data, for Transformers of various\nconfigurations, at layers of all depth levels, as well as for other\narchitectures including MLP-mixers and 2-layer MLPs. We show that sparsity also\nemerges using training datasets with random labels, or with random inputs, or\nwith infinite amount of data, demonstrating that sparsity is not a result of a\nspecific family of datasets. We discuss how sparsity immediately implies a way\nto significantly reduce the FLOP count and improve efficiency for Transformers.\nMoreover, we demonstrate perhaps surprisingly that enforcing an even sparser\nactivation via Top-k thresholding with a small value of k brings a collection\nof desired but missing properties for Transformers, namely less sensitivity to\nnoisy training data, more robustness to input corruptions, and better\ncalibration for their prediction confidence.", "published": "2022-10-12 15:25:19", "link": "http://arxiv.org/abs/2210.06313v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SQuId: Measuring Speech Naturalness in Many Languages", "abstract": "Much of text-to-speech research relies on human evaluation, which incurs\nheavy costs and slows down the development process. The problem is particularly\nacute in heavily multilingual applications, where recruiting and polling judges\ncan take weeks. We introduce SQuId (Speech Quality Identification), a\nmultilingual naturalness prediction model trained on over a million ratings and\ntested in 65 locales-the largest effort of this type to date. The main insight\nis that training one model on many locales consistently outperforms mono-locale\nbaselines. We present our task, the model, and show that it outperforms a\ncompetitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then\ndemonstrate the effectiveness of cross-locale transfer during fine-tuning and\nhighlight its effect on zero-shot locales, i.e., locales for which there is no\nfine-tuning data. Through a series of analyses, we highlight the role of\nnon-linguistic effects such as sound artifacts in cross-locale transfer.\nFinally, we present the effect of our design decision, e.g., model size,\npre-training diversity, and language rebalancing with several ablation\nexperiments.", "published": "2022-10-12 15:43:09", "link": "http://arxiv.org/abs/2210.06324v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Relational Graph Convolutional Neural Networks for Multihop Reasoning: A\n  Comparative Study", "abstract": "Multihop Question Answering is a complex Natural Language Processing task\nthat requires multiple steps of reasoning to find the correct answer to a given\nquestion. Previous research has explored the use of models based on Graph\nNeural Networks for tackling this task. Various architectures have been\nproposed, including Relational Graph Convolutional Networks (RGCN). For these\nmany node types and relations between them have been introduced, such as simple\nentity co-occurrences, modelling coreferences, or \"reasoning paths\" from\nquestions to answers via intermediary entities. Nevertheless, a thoughtful\nanalysis on which relations, node types, embeddings and architecture are the\nmost beneficial for this task is still missing. In this paper we explore a\nnumber of RGCN-based Multihop QA models, graph relations, and node embeddings,\nand empirically explore the influence of each on Multihop QA performance on the\nWikiHop dataset.", "published": "2022-10-12 17:13:30", "link": "http://arxiv.org/abs/2210.06418v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Foundation Transformers", "abstract": "A big convergence of model architectures across language, vision, speech, and\nmultimodal is emerging. However, under the same name \"Transformers\", the above\nareas use different implementations for better performance, e.g.,\nPost-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We\ncall for the development of Foundation Transformer for true general-purpose\nmodeling, which serves as a go-to architecture for various tasks and modalities\nwith guaranteed training stability. In this work, we introduce a Transformer\nvariant, named Magneto, to fulfill the goal. Specifically, we propose\nSub-LayerNorm for good expressivity, and the initialization strategy\ntheoretically derived from DeepNet for stable scaling up. Extensive experiments\ndemonstrate its superior performance and better stability than the de facto\nTransformer variants designed for various applications, including language\nmodeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e.,\nBEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).", "published": "2022-10-12 17:16:27", "link": "http://arxiv.org/abs/2210.06423v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "SUMBot: Summarizing Context in Open-Domain Dialogue Systems", "abstract": "In this paper, we investigate the problem of including relevant information\nas context in open-domain dialogue systems. Most models struggle to identify\nand incorporate important knowledge from dialogues and simply use the entire\nturns as context, which increases the size of the input fed to the model with\nunnecessary information. Additionally, due to the input size limitation of a\nfew hundred tokens of large pre-trained models, regions of the history are not\nincluded and informative parts from the dialogue may be omitted. In order to\nsurpass this problem, we introduce a simple method that substitutes part of the\ncontext with a summary instead of the whole history, which increases the\nability of models to keep track of all the previous relevant information. We\nshow that the inclusion of a summary may improve the answer generation task and\ndiscuss some examples to further understand the system's weaknesses.", "published": "2022-10-12 18:00:07", "link": "http://arxiv.org/abs/2210.06496v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A generative grammar of cooking", "abstract": "Cooking is a uniquely human endeavor for transforming raw ingredients into\ndelicious dishes. Over centuries, cultures worldwide have evolved diverse\ncooking practices ingrained in their culinary traditions. Recipes, thus, are\ncultural capsules that capture culinary knowledge in elaborate cooking\nprotocols. While simple quantitative models have probed the patterns in recipe\ncomposition and the process of cuisine evolution, unlike other cultural quirks\nsuch as language, the principles of cooking remain hitherto unexplored. The\nfundamental rules that drive the act of cooking, shaping recipe composition and\ncuisine architecture, are unclear. Here we present a generative grammar of\ncooking that captures the underlying culinary logic. By studying an extensive\nrepository of structured recipes, we identify core concepts and rules that\ntogether forge a combinatorial system for culinary synthesis. Building on the\nbody of work done in the context of language, the demonstration of a logically\nconsistent generative framework offers profound insights into the act of\ncooking. Given the central role of food in nutrition and lifestyle disorders,\nculinary grammar provides leverage to improve public health through dietary\ninterventions beyond applications for creative pursuits such as novel recipe\ngeneration.", "published": "2022-10-12 06:34:23", "link": "http://arxiv.org/abs/2211.09059v1", "categories": ["physics.soc-ph", "cs.AI", "cs.CL", "J.5; J.3"], "primary_category": "physics.soc-ph"}
{"title": "Cross-dataset COVID-19 Transfer Learning with Cough Detection, Cough\n  Segmentation, and Data Augmentation", "abstract": "This paper addresses issues on cough-based COVID-19 detection. We propose a\ncross-dataset transfer learning approach to improve the performance of COVID-19\ndetection by incorporating cough detection, cough segmentation, and data\naugmentation. The first aimed at removing non-cough signals and cough signals\nwith low probability. The second aimed at segregating several coughs in a\nwaveform into individual coughs. The third aimed at increasing the number of\nsamples for the deep learning model. These three processing blocks are\nimportant as our finding revealed a large margin of improvement relative to the\nbaseline methods without these blocks. An ablation study is conducted to\noptimize hyperparameters and it was found that alpha mixup is an important\nfactor among others in improving the model performance via this augmentation\nmethod. A summary of this study with previous studies on the same evaluation\nset was given to gain insights into different methods of cough-based COVID-19\ndetection.", "published": "2022-10-12 00:23:13", "link": "http://arxiv.org/abs/2210.05843v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adversarial Speaker-Consistency Learning Using Untranscribed Speech Data\n  for Zero-Shot Multi-Speaker Text-to-Speech", "abstract": "Several recently proposed text-to-speech (TTS) models achieved to generate\nthe speech samples with the human-level quality in the single-speaker and\nmulti-speaker TTS scenarios with a set of pre-defined speakers. However,\nsynthesizing a new speaker's voice with a single reference audio, commonly\nknown as zero-shot multi-speaker text-to-speech (ZSM-TTS), is still a very\nchallenging task. The main challenge of ZSM-TTS is the speaker domain shift\nproblem upon the speech generation of a new speaker. To mitigate this problem,\nwe propose adversarial speaker-consistency learning (ASCL). The proposed method\nfirst generates an additional speech of a query speaker using the external\nuntranscribed datasets at each training iteration. Then, the model learns to\nconsistently generate the speech sample of the same speaker as the\ncorresponding speaker embedding vector by employing an adversarial learning\nscheme. The experimental results show that the proposed method is effective\ncompared to the baseline in terms of the quality and speaker similarity in\nZSM-TTS.", "published": "2022-10-12 07:40:15", "link": "http://arxiv.org/abs/2210.05979v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Can we use Common Voice to train a Multi-Speaker TTS system?", "abstract": "Training of multi-speaker text-to-speech (TTS) systems relies on curated\ndatasets based on high-quality recordings or audiobooks. Such datasets often\nlack speaker diversity and are expensive to collect. As an alternative, recent\nstudies have leveraged the availability of large, crowdsourced automatic speech\nrecognition (ASR) datasets. A major problem with such datasets is the presence\nof noisy and/or distorted samples, which degrade TTS quality. In this paper, we\npropose to automatically select high-quality training samples using a\nnon-intrusive mean opinion score (MOS) estimator, WV-MOS. We show the viability\nof this approach for training a multi-speaker GlowTTS model on the Common Voice\nEnglish dataset. Our approach improves the overall quality of generated\nutterances by 1.26 MOS point with respect to training on all the samples and by\n0.35 MOS point with respect to training on the LibriTTS dataset. This opens the\ndoor to automatic TTS dataset curation for a wider range of languages.", "published": "2022-10-12 16:20:54", "link": "http://arxiv.org/abs/2210.06370v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enemy Spotted: in-game gun sound dataset for gunshot classification and\n  localization", "abstract": "Recently, deep learning-based methods have drawn huge attention due to their\nsimple yet high performance without domain knowledge in sound classification\nand localization tasks. However, a lack of gun sounds in existing datasets has\nbeen a major obstacle to implementing a support system to spot criminals from\ntheir gunshots by leveraging deep learning models. Since the occurrence of\ngunshot is rare and unpredictable, it is impractical to collect gun sounds in\nthe real world. As an alternative, gun sounds can be obtained from an FPS game\nthat is designed to mimic real-world warfare. The recent FPS game offers a\nrealistic environment where we can safely collect gunshot data while simulating\neven dangerous situations. By exploiting the advantage of the game environment,\nwe construct a gunshot dataset, namely BGG, for the firearm classification and\ngunshot localization tasks. The BGG dataset consists of 37 different types of\nfirearms, distances, and directions between the sound source and a receiver. We\ncarefully verify that the in-game gunshot data has sufficient information to\nidentify the location and type of gunshots by training several sound\nclassification and localization baselines on the BGG dataset. Afterward, we\ndemonstrate that the accuracy of real-world firearm classification and\nlocalization tasks can be enhanced by utilizing the BGG dataset.", "published": "2022-10-12 04:36:56", "link": "http://arxiv.org/abs/2210.05917v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment\n  Generation via Transformer VQ-VAE", "abstract": "This paper proposes a model that generates a drum track in the audio domain\nto play along to a user-provided drum-free recording. Specifically, using\npaired data of drumless tracks and the corresponding human-made drum tracks, we\ntrain a Transformer model to improvise the drum part of an unseen drumless\nrecording. We combine two approaches to encode the input audio. First, we train\na vector-quantized variational autoencoder (VQ-VAE) to represent the input\naudio with discrete codes, which can then be readily used in a Transformer.\nSecond, using an audio-domain beat tracking model, we compute beat-related\nfeatures of the input audio and use them as embeddings in the Transformer.\nInstead of generating the drum track directly as waveforms, we use a separate\nVQ-VAE to encode the mel-spectrogram of a drum track into another set of\ndiscrete codes, and train the Transformer to predict the sequence of\ndrum-related discrete codes. The output codes are then converted to a\nmel-spectrogram with a decoder, and then to the waveform with a vocoder. We\nreport both objective and subjective evaluations of variants of the proposed\nmodel, demonstrating that the model with beat information generates drum\naccompaniment that is rhythmically and stylistically consistent with the input\naudio.", "published": "2022-10-12 08:23:20", "link": "http://arxiv.org/abs/2210.06007v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpecRNet: Towards Faster and More Accessible Audio DeepFake Detection", "abstract": "Audio DeepFakes are utterances generated with the use of deep neural\nnetworks. They are highly misleading and pose a threat due to use in fake news,\nimpersonation, or extortion. In this work, we focus on increasing accessibility\nto the audio DeepFake detection methods by providing SpecRNet, a neural network\narchitecture characterized by a quick inference time and low computational\nrequirements. Our benchmark shows that SpecRNet, requiring up to about 40% less\ntime to process an audio sample, provides performance comparable to LCNN\narchitecture - one of the best audio DeepFake detection models. Such a method\ncan not only be used by online multimedia services to verify a large bulk of\ncontent uploaded daily but also, thanks to its low requirements, by average\ncitizens to evaluate materials on their devices. In addition, we provide\nbenchmarks in three unique settings that confirm the correctness of our model.\nThey reflect scenarios of low-resource datasets, detection on short utterances\nand limited attacks benchmark in which we take a closer look at the influence\nof particular attacks on given architectures.", "published": "2022-10-12 11:36:14", "link": "http://arxiv.org/abs/2210.06105v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "THUEE system description for NIST 2020 SRE CTS challenge", "abstract": "This paper presents the system description of the THUEE team for the NIST\n2020 Speaker Recognition Evaluation (SRE) conversational telephone speech (CTS)\nchallenge. The subsystems including ResNet74, ResNet152, and RepVGG-B2 are\ndeveloped as speaker embedding extractors in this evaluation. We used combined\nAM-Softmax and AAM-Softmax based loss functions, namely CM-Softmax. We adopted\na two-staged training strategy to further improve system performance. We fused\nall individual systems as our final submission. Our approach leads to excellent\nperformance and ranks 1st in the challenge.", "published": "2022-10-12 12:01:59", "link": "http://arxiv.org/abs/2210.06111v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Individualized Conditioning and Negative Distances for Speaker\n  Separation", "abstract": "Speaker separation aims to extract multiple voices from a mixed signal. In\nthis paper, we propose two speaker-aware designs to improve the existing\nspeaker separation solutions. The first model is a speaker conditioning network\nthat integrates speech samples to generate individualized speaker conditions,\nwhich then provide informed guidance for a separation module to produce\nwell-separated outputs.\n  The second design aims to reduce non-target voices in the separated speech.\nTo this end, we propose negative distances to penalize the appearance of any\nnon-target voice in the channel outputs, and positive distances to drive the\nseparated voices closer to the clean targets. We explore two different setups,\nweighted-sum and triplet-like, to integrate these two distances to form a\ncombined auxiliary loss for the separation networks. Experiments conducted on\nLibriMix demonstrate the effectiveness of our proposed models.", "published": "2022-10-12 16:18:41", "link": "http://arxiv.org/abs/2210.06368v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Ensemble Teacher-Student Learning Approach with Poisson Sub-sampling\n  to Differential Privacy Preserving Speech Recognition", "abstract": "We propose an ensemble learning framework with Poisson sub-sampling to\neffectively train a collection of teacher models to issue some differential\nprivacy (DP) guarantee for training data. Through boosting under DP, a student\nmodel derived from the training data suffers little model degradation from the\nmodels trained with no privacy protection. Our proposed solution leverages upon\ntwo mechanisms, namely: (i) a privacy budget amplification via Poisson\nsub-sampling to train a target prediction model that requires less noise to\nachieve a same level of privacy budget, and (ii) a combination of the\nsub-sampling technique and an ensemble teacher-student learning framework that\nintroduces DP-preserving noise at the output of the teacher models and\ntransfers DP-preserving properties via noisy labels. Privacy-preserving student\nmodels are then trained with the noisy labels to learn the knowledge with\nDP-protection from the teacher model ensemble. Experimental evidences on spoken\ncommand recognition and continuous speech recognition of Mandarin speech show\nthat our proposed framework greatly outperforms existing DP-preserving\nalgorithms in both speech processing tasks.", "published": "2022-10-12 16:34:08", "link": "http://arxiv.org/abs/2210.06382v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
