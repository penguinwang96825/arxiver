{"title": "Learning Disentangled Semantic Representations for Zero-Shot\n  Cross-Lingual Transfer in Multilingual Machine Reading Comprehension", "abstract": "Multilingual pre-trained models are able to zero-shot transfer knowledge from\nrich-resource to low-resource languages in machine reading comprehension (MRC).\nHowever, inherent linguistic discrepancies in different languages could make\nanswer spans predicted by zero-shot transfer violate syntactic constraints of\nthe target language. In this paper, we propose a novel multilingual MRC\nframework equipped with a Siamese Semantic Disentanglement Model (SSDM) to\ndisassociate semantics from syntax in representations learned by multilingual\npre-trained models. To explicitly transfer only semantic knowledge to the\ntarget language, we propose two groups of losses tailored for semantic and\nsyntactic encoding and disentanglement. Experimental results on three\nmultilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the\neffectiveness of our proposed approach over models based on mBERT and XLM-100.\nCode is available at:https://github.com/wulinjuan/SSDM_MRC.", "published": "2022-04-03 05:26:42", "link": "http://arxiv.org/abs/2204.00996v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PERFECT: Prompt-free and Efficient Few-shot Learning with Language\n  Models", "abstract": "Current methods for few-shot fine-tuning of pretrained masked language models\n(PLMs) require carefully engineered prompts and verbalizers for each new task\nto convert examples into a cloze-format that the PLM can score. In this work,\nwe propose PERFECT, a simple and efficient method for few-shot fine-tuning of\nPLMs without relying on any such handcrafting, which is highly effective given\nas few as 32 data points. PERFECT makes two key design choices: First, we show\nthat manually engineered task prompts can be replaced with task-specific\nadapters that enable sample-efficient fine-tuning and reduce memory and storage\ncosts by roughly factors of 5 and 100, respectively. Second, instead of using\nhandcrafted verbalizers, we learn new multi-token label embeddings during\nfine-tuning, which are not tied to the model vocabulary and which allow us to\navoid complex auto-regressive decoding. These embeddings are not only learnable\nfrom limited data but also enable nearly 100x faster training and inference.\nExperiments on a wide range of few-shot NLP tasks demonstrate that PERFECT,\nwhile being simple and efficient, also outperforms existing state-of-the-art\nfew-shot learning methods. Our code is publicly available at\nhttps://github.com/facebookresearch/perfect.git.", "published": "2022-04-03 22:31:25", "link": "http://arxiv.org/abs/2204.01172v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Part-of-Speech Tagger for Yiddish", "abstract": "We describe the construction and evaluation of a part-of-speech tagger for\nYiddish. This is the first step in a larger project of automatically assigning\npart-of-speech tags and syntactic structure to Yiddish text for purposes of\nlinguistic research. We combine two resources for the current work - an\n80K-word subset of the Penn Parsed Corpus of Historical Yiddish (PPCHY) and 650\nmillion words of OCR'd Yiddish text from the Yiddish Book Center (YBC). Yiddish\northography in the YBC corpus has many spelling inconsistencies, and we present\nsome evidence that even simple non-contextualized embeddings trained on YBC are\nable to capture the relationships among spelling variants without the need to\nfirst \"standardize\" the corpus. We also use YBC for continued pretraining of\ncontexualized embeddings, which are then integrated into a tagger model trained\nand evaluated on the PPCHY. We evaluate the tagger performance on a 10-fold\ncross-validation split, showing that the use of the YBC text for the\ncontextualized embeddings improves tagger performance. We conclude by\ndiscussing some next steps, including the need for additional annotated\ntraining and test data.", "published": "2022-04-03 22:53:36", "link": "http://arxiv.org/abs/2204.01175v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-Driven Graph Fusion Network For Visual Question Answering", "abstract": "Existing Visual Question Answering (VQA) models have explored various visual\nrelationships between objects in the image to answer complex questions, which\ninevitably introduces irrelevant information brought by inaccurate object\ndetection and text grounding. To address the problem, we propose a\nQuestion-Driven Graph Fusion Network (QD-GFN). It first models semantic,\nspatial, and implicit visual relations in images by three graph attention\nnetworks, then question information is utilized to guide the aggregation\nprocess of the three graphs, further, our QD-GFN adopts an object filtering\nmechanism to remove question-irrelevant objects contained in the image.\nExperiment results demonstrate that our QD-GFN outperforms the prior\nstate-of-the-art on both VQA 2.0 and VQA-CP v2 datasets. Further analysis shows\nthat both the novel graph aggregation method and object filtering mechanism\nplay a significant role in improving the performance of the model.", "published": "2022-04-03 03:02:03", "link": "http://arxiv.org/abs/2204.00975v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "On Efficiently Acquiring Annotations for Multilingual Models", "abstract": "When tasked with supporting multiple languages for a given problem, two\napproaches have arisen: training a model for each language with the annotation\nbudget divided equally among them, and training on a high-resource language\nfollowed by zero-shot transfer to the remaining languages. In this work, we\nshow that the strategy of joint learning across multiple languages using a\nsingle model performs substantially better than the aforementioned\nalternatives. We also demonstrate that active learning provides additional,\ncomplementary benefits. We show that this simple approach enables the model to\nbe data efficient by allowing it to arbitrate its annotation budget to query\nlanguages it is less certain on. We illustrate the effectiveness of our\nproposed method on a diverse set of tasks: a classification task with 4\nlanguages, a sequence tagging task with 4 languages and a dependency parsing\ntask with 5 languages. Our proposed method, whilst simple, substantially\noutperforms the other viable alternatives for building a model in a\nmultilingual setting under constrained budgets.", "published": "2022-04-03 07:42:13", "link": "http://arxiv.org/abs/2204.01016v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Task2Dial: A Novel Task and Dataset for Commonsense enhanced Task-based\n  Dialogue Grounded in Documents", "abstract": "This paper proposes a novel task on commonsense-enhanced task-based dialogue\ngrounded in documents and describes the Task2Dial dataset, a novel dataset of\ndocument-grounded task-based dialogues, where an Information Giver (IG)\nprovides instructions (by consulting a document) to an Information Follower\n(IF), so that the latter can successfully complete the task. In this unique\nsetting, the IF can ask clarification questions which may not be grounded in\nthe underlying document and require commonsense knowledge to be answered. The\nTask2Dial dataset poses new challenges: (1) its human reference texts show more\nlexical richness and variation than other document-grounded dialogue datasets;\n(2) generating from this set requires paraphrasing as instructional responses\nmight have been modified from the underlying document; (3) requires commonsense\nknowledge, since questions might not necessarily be grounded in the document;\n(4) generating requires planning based on context, as task steps need to be\nprovided in order. The Task2Dial dataset contains dialogues with an average\n$18.15$ number of turns and 19.79 tokens per turn, as compared to 12.94 and 12\nrespectively in existing datasets. As such, learning from this dataset promises\nmore natural, varied and less template-like system utterances.", "published": "2022-04-03 12:15:56", "link": "http://arxiv.org/abs/2204.01061v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A sequence-to-sequence approach for document-level relation extraction", "abstract": "Motivated by the fact that many relations cross the sentence boundary, there\nhas been increasing interest in document-level relation extraction (DocRE).\nDocRE requires integrating information within and across sentences, capturing\ncomplex interactions between mentions of entities. Most existing methods are\npipeline-based, requiring entities as input. However, jointly learning to\nextract entities and relations can improve performance and be more efficient\ndue to shared parameters and training steps. In this paper, we develop a\nsequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE\n(entity extraction, coreference resolution and relation extraction) end-to-end,\nreplacing a pipeline of task-specific components. Using a simple strategy we\ncall entity hinting, we compare our approach to existing pipeline-based methods\non several popular biomedical datasets, in some cases exceeding their\nperformance. We also report the first end-to-end results on these datasets for\nfuture comparison. Finally, we demonstrate that, under our model, an end-to-end\napproach outperforms a pipeline-based approach. Our code, data and trained\nmodels are available at {\\url{https://github.com/johngiorgi/seq2rel}}. An\nonline demo is available at\n{\\url{https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py}}.", "published": "2022-04-03 16:03:19", "link": "http://arxiv.org/abs/2204.01098v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pragmatic constraints and pronoun reference disambiguation: the possible\n  and the impossible", "abstract": "Pronoun disambiguation in understanding text and discourse often requires the\napplication of both general pragmatic knowledge and context-specific\ninformation. In AI and linguistics research, this has mostly been studied in\ncases where the referent is explicitly stated in the preceding text nearby.\nHowever, pronouns in natural text often refer to entities, collections, or\nevents that are only implicitly mentioned previously; in those cases the need\nto use pragmatic knowledge to disambiguate becomes much more acute and the\ncharacterization of the knowledge becomes much more difficult. Extended\nliterary texts at times employ both extremely complex patterns of reference and\nextremely rich and subtle forms of knowledge. Indeed, it is occasionally\npossible to have a pronoun that is far separated from its referent in a text.\nIn the opposite direction, pronoun use is affected by considerations of focus\nof attention and by formal constraints such as a preference for parallel\nsyntactic structures; these can be so strong that no pragmatic knowledge\nsuffices to overrule them.", "published": "2022-04-03 21:57:58", "link": "http://arxiv.org/abs/2204.01166v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Dialect Density Estimation for African American English", "abstract": "In this paper, we explore automatic prediction of dialect density of the\nAfrican American English (AAE) dialect, where dialect density is defined as the\npercentage of words in an utterance that contain characteristics of the\nnon-standard dialect. We investigate several acoustic and language modeling\nfeatures, including the commonly used X-vector representation and ComParE\nfeature set, in addition to information extracted from ASR transcripts of the\naudio files and prosodic information. To address issues of limited labeled\ndata, we use a weakly supervised model to project prosodic and X-vector\nfeatures into low-dimensional task-relevant representations. An XGBoost model\nis then used to predict the speaker's dialect density from these features and\nshow which are most significant during inference. We evaluate the utility of\nthese features both alone and in combination for the given task. This work,\nwhich does not rely on hand-labeled transcripts, is performed on audio segments\nfrom the CORAAL database. We show a significant correlation between our\npredicted and ground truth dialect density measures for AAE speech in this\ndatabase and propose this work as a tool for explaining and mitigating bias in\nspeech technology.", "published": "2022-04-03 01:34:48", "link": "http://arxiv.org/abs/2204.00967v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "F.2.2, I.2.7"], "primary_category": "eess.AS"}
{"title": "Deep Speech Based End-to-End Automated Speech Recognition (ASR) for\n  Indian-English Accents", "abstract": "Automated Speech Recognition (ASR) is an interdisciplinary application of\ncomputer science and linguistics that enable us to derive the transcription\nfrom the uttered speech waveform. It finds several applications in Military\nlike High-performance fighter aircraft, helicopters, air-traffic controller.\nOther than military speech recognition is used in healthcare, persons with\ndisabilities and many more. ASR has been an active research area. Several\nmodels and algorithms for speech to text (STT) have been proposed. One of the\nmost recent is Mozilla Deep Speech, it is based on the Deep Speech research\npaper by Baidu. Deep Speech is a state-of-art speech recognition system is\ndeveloped using end-to-end deep learning, it is trained using well-optimized\nRecurrent Neural Network (RNN) training system utilizing multiple Graphical\nProcessing Units (GPUs). This training is mostly done using American-English\naccent datasets, which results in poor generalizability to other English\naccents. India is a land of vast diversity. This can even be seen in the\nspeech, there are several English accents which vary from state to state. In\nthis work, we have used transfer learning approach using most recent Deep\nSpeech model i.e., deepspeech-0.9.3 to develop an end-to-end speech recognition\nsystem for Indian-English accents. This work utilizes fine-tuning and data\nargumentation to further optimize and improve the Deep Speech ASR system. Indic\nTTS data of Indian-English accents is used for transfer learning and\nfine-tuning the pre-trained Deep Speech model. A general comparison is made\namong the untrained model, our trained model and other available speech\nrecognition services for Indian-English Accents.", "published": "2022-04-03 03:11:21", "link": "http://arxiv.org/abs/2204.00977v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Why Exposure Bias Matters: An Imitation Learning Perspective of Error\n  Accumulation in Language Generation", "abstract": "Current language generation models suffer from issues such as repetition,\nincoherence, and hallucinations. An often-repeated hypothesis is that this\nbrittleness of generation models is caused by the training and the generation\nprocedure mismatch, also referred to as exposure bias. In this paper, we verify\nthis hypothesis by analyzing exposure bias from an imitation learning\nperspective. We show that exposure bias leads to an accumulation of errors,\nanalyze why perplexity fails to capture this accumulation, and empirically show\nthat this accumulation results in poor generation quality. Source code to\nreproduce these experiments is available at\nhttps://github.com/kushalarora/quantifying_exposure_bias", "published": "2022-04-03 22:28:31", "link": "http://arxiv.org/abs/2204.01171v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual and Multimodal Abuse Detection", "abstract": "The presence of abusive content on social media platforms is undesirable as\nit severely impedes healthy and safe social media interactions. While automatic\nabuse detection has been widely explored in textual domain, audio abuse\ndetection still remains unexplored. In this paper, we attempt abuse detection\nin conversational audio from a multimodal perspective in a multilingual social\nmedia setting. Our key hypothesis is that along with the modelling of audio,\nincorporating discriminative information from other modalities can be highly\nbeneficial for this task. Our proposed method, MADA, explicitly focuses on two\nmodalities other than the audio itself, namely, the underlying emotions\nexpressed in the abusive audio and the semantic information encapsulated in the\ncorresponding textual form. Observations prove that MADA demonstrates gains\nover audio-only approaches on the ADIMA dataset. We test the proposed approach\non 10 different languages and observe consistent gains in the range 0.6%-5.2%\nby leveraging multiple modalities. We also perform extensive ablation\nexperiments for studying the contributions of every modality and observe the\nbest results while leveraging all the modalities together. Additionally, we\nperform experiments to empirically confirm that there is a strong correlation\nbetween underlying emotions and abusive behaviour.", "published": "2022-04-03 13:28:58", "link": "http://arxiv.org/abs/2204.02263v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker\n  Adaptation in Text-to-Speech Synthesis", "abstract": "Zero-shot speaker adaptation aims to clone an unseen speaker's voice without\nany adaptation time and parameters. Previous researches usually use a speaker\nencoder to extract a global fixed speaker embedding from reference speech, and\nseveral attempts have tried variable-length speaker embedding. However, they\nneglect to transfer the personal pronunciation characteristics related to\nphoneme content, leading to poor speaker similarity in terms of detailed\nspeaking styles and pronunciation habits. To improve the ability of the speaker\nencoder to model personal pronunciation characteristics, we propose\ncontent-dependent fine-grained speaker embedding for zero-shot speaker\nadaptation. The corresponding local content embeddings and speaker embeddings\nare extracted from a reference speech, respectively. Instead of modeling the\ntemporal relations, a reference attention module is introduced to model the\ncontent relevance between the reference speech and the input text, and to\ngenerate the fine-grained speaker embedding for each phoneme encoder output.\nThe experimental results show that our proposed method can improve speaker\nsimilarity of synthesized speeches, especially for unseen speakers.", "published": "2022-04-03 04:55:00", "link": "http://arxiv.org/abs/2204.00990v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Frequency and Multi-Scale Selective Kernel Attention for Speaker\n  Verification", "abstract": "The majority of recent state-of-the-art speaker verification architectures\nadopt multi-scale processing and frequency-channel attention mechanisms.\nConvolutional layers of these models typically have a fixed kernel size, e.g.,\n3 or 5. In this study, we further contribute to this line of research utilising\na selective kernel attention (SKA) mechanism. The SKA mechanism allows each\nconvolutional layer to adaptively select the kernel size in a data-driven\nfashion. It is based on an attention mechanism which exploits both frequency\nand channel domain. We first apply existing SKA module to our baseline. Then we\npropose two SKA variants where the first variant is applied in front of the\nECAPA-TDNN model and the other is combined with the Res2net backbone block.\nThrough extensive experiments, we demonstrate that our two proposed SKA\nvariants consistently improves the performance and are complementary when\ntested on three different evaluation protocols.", "published": "2022-04-03 06:26:50", "link": "http://arxiv.org/abs/2204.01005v4", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "A Computational Analysis of Pitch Drift in Unaccompanied Solo Singing\n  using DBSCAN Clustering", "abstract": "Unaccompanied vocalists usually change the tuning unintentionally and end up\nwith a higher or lower pitch than the starting point during a long performance.\nThis phenomenon is called pitch drift, which is dependent on various elements,\nsuch as the skill of the performer, and the length and difficulty of the\nperformance. In this paper, we propose a computational method for measuring\npitch drift in the course of an unaccompanied vocal performance, using pitch\nhistogram and DBSCAN clustering.", "published": "2022-04-03 06:54:13", "link": "http://arxiv.org/abs/2204.01009v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On incorporating social speaker characteristics in synthetic speech", "abstract": "In our previous work, we derived the acoustic features, that contribute to\nthe perception of warmth and competence in synthetic speech. As an extension,\nin our current work, we investigate the impact of the derived vocal features in\nthe generation of the desired characteristics. The acoustic features, spectral\nflux, F1 mean and F2 mean and their convex combinations were explored for the\ngeneration of higher warmth in female speech. The voiced slope, spectral flux,\nand their convex combinations were investigated for the generation of higher\ncompetence in female speech. We have employed a feature quantization approach\nin the traditional end-to-end tacotron based speech synthesis model. The\nlistening tests have shown that the convex combination of acoustic features\ndisplays higher Mean Opinion Scores of warmth and competence when compared to\nthat of individual features.", "published": "2022-04-03 16:51:21", "link": "http://arxiv.org/abs/2204.01115v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
