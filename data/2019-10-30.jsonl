{"title": "Ensembling Strategies for Answering Natural Questions", "abstract": "Many of the top question answering systems today utilize ensembling to\nimprove their performance on tasks such as the Stanford Question Answering\nDataset (SQuAD) and Natural Questions (NQ) challenges. Unfortunately most of\nthese systems do not publish their ensembling strategies used in their\nleaderboard submissions. In this work, we investigate a number of ensembling\ntechniques and demonstrate a strategy which improves our F1 score for short\nanswers on the dev set for NQ by 2.3 F1 points over our single model (which\noutperforms the previous SOTA by 1.9 F1 points).", "published": "2019-10-30 19:18:14", "link": "http://arxiv.org/abs/1911.00337v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phenotyping of Clinical Notes with Improved Document Classification\n  Models Using Contextualized Neural Language Models", "abstract": "Clinical notes contain an extensive record of a patient's health status, such\nas smoking status or the presence of heart conditions. However, this detail is\nnot replicated within the structured data of electronic health systems.\nPhenotyping, the extraction of patient conditions from free clinical text, is a\ncritical task which supports avariety of downstream applications such as\ndecision support and secondary use of medical records. Previous work has\nresulted in systems which are high performing but require hand engineering,\noften of rules. Recent work in pretrained contextualized language models have\nenabled advances in representing text for a variety of tasks. We therefore\nexplore several architectures for modeling pheno-typing that rely solely on\nBERT representations of the clinical note, removing the need for manual\nengineering. We find these architectures are competitive with or outperform\nexisting state of the art methods on two phenotyping tasks.", "published": "2019-10-30 04:47:17", "link": "http://arxiv.org/abs/1910.13664v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LSTM Easy-first Dependency Parsing with Pre-trained Word Embeddings and\n  Character-level Word Embeddings in Vietnamese", "abstract": "In Vietnamese dependency parsing, several methods have been proposed.\nDependency parser which uses deep neural network model has been reported that\nachieved state-of-the-art results. In this paper, we proposed a new method\nwhich applies LSTM easy-first dependency parsing with pre-trained word\nembeddings and character-level word embeddings. Our method achieves an accuracy\nof 80.91% of unlabeled attachment score and 72.98% of labeled attachment score\non the Vietnamese Dependency Treebank (VnDT).", "published": "2019-10-30 09:28:49", "link": "http://arxiv.org/abs/1910.13732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Time to Take Emoji Seriously: They Vastly Improve Casual Conversational\n  Models", "abstract": "Graphical emoji are ubiquitous in modern-day online conversations. So is a\nsingle thumbs-up emoji able to signify an agreement, without any words. We\nargue that the current state-of-the-art systems are ill-equipped to correctly\ninterpret these emoji, especially in a conversational context. However, in a\ncasual context, the benefits might be high: a better understanding of users'\nutterances and more natural, emoji-rich responses.\n  With this in mind, we modify BERT to fully support emoji, both from the\nUnicode Standard and custom emoji. This modified BERT is then trained on a\ncorpus of question-answer (QA) tuples with a high number of emoji, where we're\nable to increase the 1-of-100 accuracy from 12.7% for the current\nstate-of-the-art to 17.8% for our model with emoji support.", "published": "2019-10-30 12:11:36", "link": "http://arxiv.org/abs/1910.13793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation", "abstract": "Question Generation (QG) is a Natural Language Processing (NLP) task that\naids advances in Question Answering (QA) and conversational assistants.\nExisting models focus on generating a question based on a text and possibly the\nanswer to the generated question. They need to determine the type of\ninterrogative word to be generated while having to pay attention to the grammar\nand vocabulary of the question. In this work, we propose\nInterrogative-Word-Aware Question Generation (IWAQG), a pipelined system\ncomposed of two modules: an interrogative word classifier and a QG model. The\nfirst module predicts the interrogative word that is provided to the second\nmodule to create the question. Owing to an increased recall of deciding the\ninterrogative words to be used for the generated questions, the proposed model\nachieves new state-of-the-art results on the task of QG in SQuAD, improving\nfrom 46.58 to 47.69 in BLEU-1, 17.55 to 18.53 in BLEU-4, 21.24 to 22.33 in\nMETEOR, and from 44.53 to 46.94 in ROUGE-L.", "published": "2019-10-30 12:14:11", "link": "http://arxiv.org/abs/1910.13794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Framework for Building Closed-Domain Chat Dialogue Systems", "abstract": "This paper presents HRIChat, a framework for developing closed-domain chat\ndialogue systems. Being able to engage in chat dialogues has been found\neffective for improving communication between humans and dialogue systems. This\npaper focuses on closed-domain systems because they would be useful when\ncombined with task-oriented dialogue systems in the same domain. HRIChat\nenables domain-dependent language understanding so that it can deal well with\ndomain-specific utterances. In addition, HRIChat makes it possible to integrate\nstate transition network-based dialogue management and reaction-based dialogue\nmanagement. FoodChatbot, which is an application in the food and restaurant\ndomain, has been developed and evaluated through a user study. Its results\nsuggest that reasonably good systems can be developed with HRIChat. This paper\nalso reports lessons learned from the development and evaluation of\nFoodChatbot.", "published": "2019-10-30 13:06:23", "link": "http://arxiv.org/abs/1910.13826v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Latent Morphology Model for Open-Vocabulary Neural Machine Translation", "abstract": "Translation into morphologically-rich languages challenges neural machine\ntranslation (NMT) models with extremely sparse vocabularies where atomic\ntreatment of surface forms is unrealistic. This problem is typically addressed\nby either pre-processing words into subword units or performing translation\ndirectly at the level of characters. The former is based on word segmentation\nalgorithms optimized using corpus-level statistics with no regard to the\ntranslation task. The latter learns directly from translation data but requires\nrather deep architectures. In this paper, we propose to translate words by\nmodeling word formation through a hierarchical latent variable model which\nmimics the process of morphological inflection. Our model generates words one\ncharacter at a time by composing two latent representations: a continuous one,\naimed at capturing the lexical semantics, and a set of (approximately) discrete\nfeatures, aimed at capturing the morphosyntactic function, which are shared\namong different surface forms. Our model achieves better accuracy in\ntranslation into three morphologically-rich languages than conventional\nopen-vocabulary NMT methods, while also demonstrating a better generalization\ncapacity under low to mid-resource settings.", "published": "2019-10-30 14:29:47", "link": "http://arxiv.org/abs/1910.13890v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Gender-Inclusive Coreference Resolution", "abstract": "Correctly resolving textual mentions of people fundamentally entails making\ninferences about those people. Such inferences raise the risk of systemic\nbiases in coreference resolution systems, including biases that can harm binary\nand non-binary trans and cis stakeholders. To better understand such biases, we\nforeground nuanced conceptualizations of gender from sociology and\nsociolinguistics, and develop two new datasets for interrogating bias in crowd\nannotations and in existing coreference resolution systems. Through these\nstudies, conducted on English text, we confirm that without acknowledging and\nbuilding systems that recognize the complexity of gender, we build systems that\nlead to many potential harms.", "published": "2019-10-30 14:59:56", "link": "http://arxiv.org/abs/1910.13913v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let's FACE it. Finnish Poetry Generation with Aesthetics and Framing", "abstract": "We present a creative poem generator for the morphologically rich Finnish\nlanguage. Our method falls into the master-apprentice paradigm, where a\ncomputationally creative genetic algorithm teaches a BRNN model to generate\npoetry. We model several parts of poetic aesthetics in the fitness function of\nthe genetic algorithm, such as sonic features, semantic coherence, imagery and\nmetaphor. Furthermore, we justify the creativity of our method based on the\nFACE theory on computational creativity and take additional care in evaluating\nour system by automatic metrics for concepts together with human evaluation for\naesthetics, framing and expressions.", "published": "2019-10-30 16:00:54", "link": "http://arxiv.org/abs/1910.13946v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Multilingual Neural Machine Translation to Unseen Languages", "abstract": "Multilingual Neural Machine Translation (MNMT) for low-resource languages\n(LRL) can be enhanced by the presence of related high-resource languages (HRL),\nbut the relatedness of HRL usually relies on predefined linguistic assumptions\nabout language similarity. Recently, adapting MNMT to a LRL has shown to\ngreatly improve performance. In this work, we explore the problem of adapting\nan MNMT model to an unseen LRL using data selection and model adaptation. In\norder to improve NMT for LRL, we employ perplexity to select HRL data that are\nmost similar to the LRL on the basis of language distance. We extensively\nexplore data selection in popular multilingual NMT settings, namely in\n(zero-shot) translation, and in adaptation from a multilingual pre-trained\nmodel, for both directions (LRL-en). We further show that dynamic adaptation of\nthe model's vocabulary results in a more favourable segmentation for the LRL in\ncomparison with direct adaptation. Experiments show reductions in training time\nand significant performance gains over LRL baselines, even with zero LRL data\n(+13.0 BLEU), up to +17.0 BLEU for pre-trained multilingual model dynamic\nadaptation with related data selection. Our method outperforms current\napproaches, such as massively multilingual models and data augmentation, on\nfour LRL.", "published": "2019-10-30 17:34:46", "link": "http://arxiv.org/abs/1910.13998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural\n  Machine Translation", "abstract": "Most neural machine translation systems still translate sentences in\nisolation. To make further progress, a promising line of research additionally\nconsiders the surrounding context in order to provide the model potentially\nmissing source-side information, as well as to maintain a coherent output. One\ndifficulty in training such larger-context (i.e. document-level) machine\ntranslation systems is that context may be missing from many parallel examples.\nTo circumvent this issue, two-stage approaches, in which sentence-level\ntranslations are post-edited in context, have recently been proposed. In this\npaper, we instead consider the viability of filling in the missing context. In\nparticular, we consider three distinct approaches to generate the missing\ncontext: using random contexts, applying a copy heuristic or generating it with\na language model. In particular, the copy heuristic significantly helps with\nlexical coherence, while using completely random contexts hurts performance on\nmany long-distance linguistic phenomena. We also validate the usefulness of\ntagged back-translation. In addition to improving BLEU scores as expected,\nusing back-translated data helps larger-context machine translation systems to\nbetter capture long-range phenomena.", "published": "2019-10-30 18:38:17", "link": "http://arxiv.org/abs/1910.14075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Topic-Attention Model for Medical Term Abbreviation\n  Disambiguation", "abstract": "Automated analysis of clinical notes is attracting increasing attention.\nHowever, there has not been much work on medical term abbreviation\ndisambiguation. Such abbreviations are abundant, and highly ambiguous, in\nclinical documents. One of the main obstacles is the lack of large scale,\nbalance labeled data sets. To address the issue, we propose a few-shot learning\napproach to take advantage of limited labeled data. Specifically, a neural\ntopic-attention model is applied to learn improved contextualized sentence\nrepresentations for medical term abbreviation disambiguation. Another vital\nissue is that the existing scarce annotations are noisy and missing. We\nre-examine and correct an existing dataset for training and collect a test set\nto evaluate the models fairly especially for rare senses. We train our model on\nthe training set which contains 30 abbreviation terms as categories (on\naverage, 479 samples and 3.24 classes in each term) selected from a public\nabbreviation disambiguation dataset, and then test on a manually-created\nbalanced dataset (each class in each term has 15 samples). We show that\nenhancing the sentence representation with topic information improves the\nperformance on small-scale unbalanced training datasets by a large margin,\ncompared to a number of baseline models.", "published": "2019-10-30 18:39:46", "link": "http://arxiv.org/abs/1910.14076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Generalizable Neuro-Symbolic Systems for Commonsense Question\n  Answering", "abstract": "Non-extractive commonsense QA remains a challenging AI task, as it requires\nsystems to reason about, synthesize, and gather disparate pieces of\ninformation, in order to generate responses to queries. Recent approaches on\nsuch tasks show increased performance, only when models are either pre-trained\nwith additional information or when domain-specific heuristics are used,\nwithout any special consideration regarding the knowledge resource type. In\nthis paper, we perform a survey of recent commonsense QA methods and we provide\na systematic analysis of popular knowledge resources and knowledge-integration\nmethods, across benchmarks from multiple commonsense datasets. Our results and\nanalysis show that attention-based injection seems to be a preferable choice\nfor knowledge integration and that the degree of domain overlap, between\nknowledge bases and datasets, plays a crucial role in determining model\nsuccess.", "published": "2019-10-30 19:07:31", "link": "http://arxiv.org/abs/1910.14087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse-Aware Neural Extractive Text Summarization", "abstract": "Recently BERT has been adopted for document encoding in state-of-the-art text\nsummarization models. However, sentence-based extractive models often result in\nredundant or uninformative phrases in the extracted summaries. Also, long-range\ndependencies throughout a document are not well captured by BERT, which is\npre-trained on sentence pairs instead of documents. To address these issues, we\npresent a discourse-aware neural summarization model - DiscoBert. DiscoBert\nextracts sub-sentential discourse units (instead of sentences) as candidates\nfor extractive selection on a finer granularity. To capture the long-range\ndependencies among discourse units, structural discourse graphs are constructed\nbased on RST trees and coreference mentions, encoded with Graph Convolutional\nNetworks. Experiments show that the proposed model outperforms state-of-the-art\nmethods by a significant margin on popular summarization benchmarks compared to\nother BERT-base models.", "published": "2019-10-30 21:17:26", "link": "http://arxiv.org/abs/1910.14142v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How does Grammatical Gender Affect Noun Representations in\n  Gender-Marking Languages?", "abstract": "Many natural languages assign grammatical gender also to inanimate nouns in\nthe language. In such languages, words that relate to the gender-marked nouns\nare inflected to agree with the noun's gender. We show that this affects the\nword representations of inanimate nouns, resulting in nouns with the same\ngender being closer to each other than nouns with different gender. While\n\"embedding debiasing\" methods fail to remove the effect, we demonstrate that a\ncareful application of methods that neutralize grammatical gender signals from\nthe words' context when training word embeddings is effective in removing it.\nFixing the grammatical gender bias yields a positive effect on the quality of\nthe resulting word embeddings, both in monolingual and cross-lingual settings.\nWe note that successfully removing gender signals, while achievable, is not\ntrivial to do and that a language-specific morphological analyzer, together\nwith careful usage of it, are essential for achieving good results.", "published": "2019-10-30 22:28:14", "link": "http://arxiv.org/abs/1910.14161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Discourse Structure using Distant Supervision from Sentiment", "abstract": "Discourse parsing could not yet take full advantage of the neural NLP\nrevolution, mostly due to the lack of annotated datasets. We propose a novel\napproach that uses distant supervision on an auxiliary task (sentiment\nclassification), to generate abundant data for RST-style discourse structure\nprediction. Our approach combines a neural variant of multiple-instance\nlearning, using document-level supervision, with an optimal CKY-style tree\ngeneration algorithm. In a series of experiments, we train a discourse parser\n(for only structure prediction) on our automatically generated dataset and\ncompare it with parsers trained on human-annotated corpora (news domain RST-DT\nand Instructional domain). Results indicate that while our parser does not yet\nmatch the performance of a parser trained and tested on the same dataset\n(intra-domain), it does perform remarkably well on the much more difficult and\narguably more useful task of inter-domain discourse structure prediction, where\nthe parser is trained on one domain and tested/applied on another one.", "published": "2019-10-30 23:15:24", "link": "http://arxiv.org/abs/1910.14176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Augmented Transformer Architecture for Natural Language Generation\n  Tasks", "abstract": "The Transformer based neural networks have been showing significant\nadvantages on most evaluations of various natural language processing and other\nsequence-to-sequence tasks due to its inherent architecture based\nsuperiorities. Although the main architecture of the Transformer has been\ncontinuously being explored, little attention was paid to the positional\nencoding module. In this paper, we enhance the sinusoidal positional encoding\nalgorithm by maximizing the variances between encoded consecutive positions to\nobtain additional promotion. Furthermore, we propose an augmented Transformer\narchitecture encoded with additional linguistic knowledge, such as the\nPart-of-Speech (POS) tagging, to boost the performance on some natural language\ngeneration tasks, e.g., the automatic translation and summarization tasks.\nExperiments show that the proposed architecture attains constantly superior\nresults compared to the vanilla Transformer.", "published": "2019-10-30 02:46:04", "link": "http://arxiv.org/abs/1910.13634v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contextual Text Denoising with Masked Language Models", "abstract": "Recently, with the help of deep learning models, significant advances have\nbeen made in different Natural Language Processing (NLP) tasks. Unfortunately,\nstate-of-the-art models are vulnerable to noisy texts. We propose a new\ncontextual text denoising algorithm based on the ready-to-use masked language\nmodel. The proposed algorithm does not require retraining of the model and can\nbe integrated into any NLP system without additional training on paired\ncleaning training data. We evaluate our method under synthetic noise and\nnatural noise and show that the proposed algorithm can use context information\nto correct noise text and improve the performance of noisy inputs in several\ndownstream tasks.", "published": "2019-10-30 18:47:37", "link": "http://arxiv.org/abs/1910.14080v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Higher Criticism for Discriminating Word-Frequency Tables and Testing\n  Authorship", "abstract": "We adapt the Higher Criticism (HC) goodness-of-fit test to measure the\ncloseness between word-frequency tables. We apply this measure to authorship\nattribution challenges, where the goal is to identify the author of a document\nusing other documents whose authorship is known. The method is simple yet\nperforms well without handcrafting and tuning; reporting accuracy at the state\nof the art level in various current challenges. As an inherent side effect, the\nHC calculation identifies a subset of discriminating words. In practice, the\nidentified words have low variance across documents belonging to a corpus of\nhomogeneous authorship. We conclude that in comparing the similarity of a new\ndocument and a corpus of a single author, HC is mostly affected by words\ncharacteristic of the author and is relatively unaffected by topic structure.", "published": "2019-10-30 23:47:25", "link": "http://arxiv.org/abs/1911.01208v5", "categories": ["cs.CL", "cs.LG", "stat.CO", "stat.ML", "62G, 62P", "J.5"], "primary_category": "cs.CL"}
{"title": "Scrambled Translation Problem: A Problem of Denoising UNMT", "abstract": "In this paper, we identify an interesting kind of error in the output of\nUnsupervised Neural Machine Translation (UNMT) systems like\n\\textit{Undreamt}(footnote). We refer to this error type as \\textit{Scrambled\nTranslation problem}. We observe that UNMT models which use \\textit{word\nshuffle} noise (as in case of Undreamt) can generate correct words, but fail to\nstitch them together to form phrases. As a result, words of the translated\nsentence look \\textit{scrambled}, resulting in decreased BLEU. We hypothesise\nthat the reason behind \\textit{scrambled translation problem} is 'shuffling\nnoise' which is introduced in every input sentence as a denoising strategy. To\ntest our hypothesis, we experiment by retraining UNMT models with a simple\n\\textit{retraining} strategy. We stop the training of the Denoising UNMT model\nafter a pre-decided number of iterations and resume the training for the\nremaining iterations -- which number is also pre-decided -- using original\nsentence as input without adding any noise. Our proposed solution achieves\nsignificant performance improvement UNMT models that train conventionally. We\ndemonstrate these performance gains on four language pairs, \\textit{viz.},\nEnglish-French, English-German, English-Spanish, Hindi-Punjabi. Our qualitative\nand quantitative analysis shows that the retraining strategy helps achieve\nbetter alignment as observed by attention heatmap and better phrasal\ntranslation, leading to statistically significant improvement in BLEU scores.", "published": "2019-10-30 12:22:37", "link": "http://arxiv.org/abs/1911.01212v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Relative contributions of Shakespeare and Fletcher in Henry VIII: An\n  Analysis Based on Most Frequent Words and Most Frequent Rhythmic Patterns", "abstract": "The versified play Henry VIII is nowadays widely recognized to be a\ncollaborative work not written solely by William Shakespeare. We employ\ncombined analysis of vocabulary and versification together with machine\nlearning techniques to determine which authors also took part in the writing of\nthe play and what were their relative contributions. Unlike most previous\nstudies, we go beyond the attribution of particular scenes and use the rolling\nattribution approach to determine the probabilities of authorship of pieces of\ntexts, without respecting the scene boundaries. Our results highly support the\ncanonical division of the play between William Shakespeare and John Fletcher\nproposed by James Spedding, but also bring new evidence supporting the\nmodifications proposed later by Thomas Merriam.", "published": "2019-10-30 22:40:05", "link": "http://arxiv.org/abs/1911.05652v1", "categories": ["cs.CL", "cs.LG", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "ON-TRAC Consortium End-to-End Speech Translation Systems for the IWSLT\n  2019 Shared Task", "abstract": "This paper describes the ON-TRAC Consortium translation systems developed for\nthe end-to-end model task of IWSLT Evaluation 2019 for the\nEnglish-to-Portuguese language pair. ON-TRAC Consortium is composed of\nresearchers from three French academic laboratories: LIA (Avignon\nUniversit\\'e), LIG (Universit\\'e Grenoble Alpes), and LIUM (Le Mans\nUniversit\\'e). A single end-to-end model built as a neural encoder-decoder\narchitecture with attention mechanism was used for two primary submissions\ncorresponding to the two EN-PT evaluations sets: (1) TED (MuST-C) and (2) How2.\nIn this paper, we notably investigate impact of pooling heterogeneous corpora\nfor training, impact of target tokenization (characters or BPEs), impact of\nspeech input segmentation and we also compare our best end-to-end model (BLEU\nof 26.91 on MuST-C and 43.82 on How2 validation sets) to a pipeline (ASR+MT)\napproach.", "published": "2019-10-30 06:11:15", "link": "http://arxiv.org/abs/1910.13689v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Jointly optimal dereverberation and beamforming", "abstract": "We previously proposed an optimal (in the maximum likelihood sense)\nconvolutional beamformer that can perform simultaneous denoising and\ndereverberation, and showed its superiority over the widely used cascade of a\nWPE dereverberation filter and a conventional MPDR beamformer. However, it has\nnot been fully investigated which components in the convolutional beamformer\nyield such superiority. To this end, this paper presents a new derivation of\nthe convolutional beamformer that allows us to factorize it into a WPE\ndereverberation filter, and a special type of a (non-convolutional) beamformer,\nreferred to as a wMPDR beamformer, without loss of optimality. With\nexperiments, we show that the superiority of the convolutional beamformer in\nfact comes from its wMPDR part.", "published": "2019-10-30 08:11:05", "link": "http://arxiv.org/abs/1910.13707v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank\n  Transformer", "abstract": "Highly performing deep neural networks come at the cost of computational\ncomplexity that limits their practicality for deployment on portable devices.\nWe propose the low-rank transformer (LRT), a memory-efficient and fast neural\narchitecture that significantly reduces the parameters and boosts the speed of\ntraining and inference for end-to-end speech recognition. Our approach reduces\nthe number of parameters of the network by more than 50% and speeds up the\ninference time by around 1.35x compared to the baseline transformer model. The\nexperiments show that our LRT model generalizes better and yields lower error\nrates on both validation and test sets compared to an uncompressed transformer\nmodel. The LRT model outperforms those from existing works on several datasets\nin an end-to-end setting without using an external language model or acoustic\ndata.", "published": "2019-10-30 15:20:07", "link": "http://arxiv.org/abs/1910.13923v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SMS-WSJ: Database, performance measures, and baseline recipe for\n  multi-channel source separation and recognition", "abstract": "We present a multi-channel database of overlapping speech for training,\nevaluation, and detailed analysis of source separation and extraction\nalgorithms: SMS-WSJ -- Spatialized Multi-Speaker Wall Street Journal. It\nconsists of artificially mixed speech taken from the WSJ database, but unlike\nearlier databases we consider all WSJ0+1 utterances and take care of strictly\nseparating the speaker sets present in the training, validation and test sets.\nWhen spatializing the data we ensure a high degree of randomness w.r.t. room\nsize, array center and rotation, as well as speaker position. Furthermore, this\npaper offers a critical assessment of recently proposed measures of source\nseparation performance. Alongside the code to generate the database we provide\na source separation baseline and a Kaldi recipe with competitive word error\nrates to provide common ground for evaluation.", "published": "2019-10-30 15:39:31", "link": "http://arxiv.org/abs/1910.13934v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Graph Neural News Recommendation with Long-term and Short-term Interest\n  Modeling", "abstract": "With the information explosion of news articles, personalized news\nrecommendation has become important for users to quickly find news that they\nare interested in. Existing methods on news recommendation mainly include\ncollaborative filtering methods which rely on direct user-item interactions and\ncontent based methods which characterize the content of user reading history.\nAlthough these methods have achieved good performances, they still suffer from\ndata sparse problem, since most of them fail to extensively exploit high-order\nstructure information (similar users tend to read similar news articles) in\nnews recommendation systems. In this paper, we propose to build a heterogeneous\ngraph to explicitly model the interactions among users, news and latent topics.\nThe incorporated topic information would help indicate a user's interest and\nalleviate the sparsity of user-item interactions. Then we take advantage of\ngraph neural networks to learn user and news representations that encode\nhigh-order structure information by propagating embeddings over the graph. The\nlearned user embeddings with complete historic user clicks capture the users'\nlong-term interests. We also consider a user's short-term interest using the\nrecent reading history with an attention based LSTM model. Experimental results\non real-world datasets show that our proposed model significantly outperforms\nstate-of-the-art methods on news recommendation.", "published": "2019-10-30 08:04:43", "link": "http://arxiv.org/abs/1910.14025v2", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Building an Application Independent Natural Language Interface", "abstract": "Traditional approaches to building natural language (NL) interfaces typically\nuse a semantic parser to parse the user command and convert it to a logical\nform, which is then translated to an executable action in an application.\nHowever, it is still challenging for a semantic parser to correctly parse\nnatural language. For a different domain, the parser may need to be retrained\nor tuned, and a new translator also needs to be written to convert the logical\nforms to executable actions. In this work, we propose a novel and application\nindependent approach to building NL interfaces that does not need a semantic\nparser or a translator. It is based on natural language to natural language\nmatching and learning, where the representation of each action and each user\ncommand are both in natural language. To perform a user intended action, the\nsystem only needs to match the user command with the correct action\nrepresentation, and then execute the corresponding action. The system also\ninteractively learns new (paraphrased) commands for actions to expand the\naction representations over time. Our experimental results show the\neffectiveness of the proposed approach.", "published": "2019-10-30 18:57:28", "link": "http://arxiv.org/abs/1910.14084v2", "categories": ["cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Temporal Feedback Convolutional Recurrent Neural Networks for Speech\n  Command Recognition", "abstract": "End-to-end learning models using raw waveforms as input have shown superior\nperformances in many audio recognition tasks. However, most model architectures\nare based on convolutional neural networks (CNN) which were mainly developed\nfor visual recognition tasks. In this paper, we propose an extension of\nsqueeze-and-excitation networks (SENets) which adds temporal feedback control\nfrom the top-layer features to channel-wise feature activations in lower layers\nusing a recurrent module. This is analogous to the adaptive gain control\nmechanism of outer hair-cell in the human auditory system. We apply the\nproposed model to speech command recognition and show that it slightly\noutperforms the SENets and other CNN-based models. We also investigate the\ndetails of the performance improvement by conducting failure analysis and\nvisualizing the channel-wise feature scaling induced by the temporal feedback.", "published": "2019-10-30 04:11:29", "link": "http://arxiv.org/abs/1911.01803v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Overlapped speech recognition from a jointly learned multi-channel\n  neural speech extraction and representation", "abstract": "We propose an end-to-end joint optimization framework of a multi-channel\nneural speech extraction and deep acoustic model without mel-filterbank (FBANK)\nextraction for overlapped speech recognition. First, based on a multi-channel\nconvolutional TasNet with STFT kernel, we unify the multi-channel target speech\nenhancement front-end network and a convolutional, long short-term memory and\nfully connected deep neural network (CLDNN) based acoustic model (AM) with the\nFBANK extraction layer to build a hybrid neural network, which is thus jointly\nupdated only by the recognition loss. The proposed framework achieves 28% word\nerror rate reduction (WERR) over a separately optimized system on AISHELL-1 and\nshows consistent robustness to signal to interference ratio (SIR) and angle\ndifference between overlapping speakers. Next, a further exploration shows that\nthe speech recognition is improved with a simplified structure by replacing the\nFBANK extraction layer in the joint model with a learnable feature projection.\nFinally, we also perform the objective measurement of speech quality on the\nreconstructed waveform from the enhancement network in the joint model.", "published": "2019-10-30 13:05:50", "link": "http://arxiv.org/abs/1910.13825v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Mixture factorized auto-encoder for unsupervised hierarchical deep\n  factorization of speech signal", "abstract": "Speech signal is constituted and contributed by various informative factors,\nsuch as linguistic content and speaker characteristic. There have been notable\nrecent studies attempting to factorize speech signal into these individual\nfactors without requiring any annotation. These studies typically assume\ncontinuous representation for linguistic content, which is not in accordance\nwith general linguistic knowledge and may make the extraction of speaker\ninformation less successful. This paper proposes the mixture factorized\nauto-encoder (mFAE) for unsupervised deep factorization. The encoder part of\nmFAE comprises a frame tokenizer and an utterance embedder. The frame tokenizer\nmodels linguistic content of input speech with a discrete categorical\ndistribution. It performs frame clustering by assigning each frame a soft\nmixture label. The utterance embedder generates an utterance-level vector\nrepresentation. A frame decoder serves to reconstruct speech features from the\nencoders'outputs. The mFAE is evaluated on speaker verification (SV) task and\nunsupervised subword modeling (USM) task. The SV experiments on VoxCeleb 1 show\nthat the utterance embedder is capable of extracting speaker-discriminative\nembeddings with performance comparable to a x-vector baseline. The USM\nexperiments on ZeroSpeech 2017 dataset verify that the frame tokenizer is able\nto capture linguistic content and the utterance embedder can acquire\nspeaker-related information.", "published": "2019-10-30 08:54:34", "link": "http://arxiv.org/abs/1911.01806v1", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Metric Learning with Background Noise Class for Few-shot Detection of\n  Rare Sound Events", "abstract": "Few-shot learning systems for sound event recognition have gained interests\nsince they require only a few examples to adapt to new target classes without\nfine-tuning. However, such systems have only been applied to chunks of sounds\nfor classification or verification. In this paper, we aim to achieve few-shot\ndetection of rare sound events, from query sequence that contain not only the\ntarget events but also the other events and background noise. Therefore, it is\nrequired to prevent false positive reactions to both the other events and\nbackground noise. We propose metric learning with background noise class for\nthe few-shot detection. The contribution is to present the explicit inclusion\nof background noise as an independent class, a suitable loss function that\nemphasizes this additional class, and a corresponding sampling strategy that\nassists training. It provides a feature space where the event classes and the\nbackground noise class are sufficiently separated. Evaluations on few-shot\ndetection tasks, using DCASE 2017 task2 and ESC-50, show that our proposed\nmethod outperforms metric learning without considering the background noise\nclass. The few-shot detection performance is also comparable to that of the\nDCASE 2017 task2 baseline system, which requires huge amount of annotated audio\ndata.", "published": "2019-10-30 09:00:59", "link": "http://arxiv.org/abs/1910.13724v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-end Microphone Permutation and Number Invariant Multi-channel\n  Speech Separation", "abstract": "An important problem in ad-hoc microphone speech separation is how to\nguarantee the robustness of a system with respect to the locations and numbers\nof microphones. The former requires the system to be invariant to different\nindexing of the microphones with the same locations, while the latter requires\nthe system to be able to process inputs with varying dimensions. Conventional\noptimization-based beamforming techniques satisfy these requirements by\ndefinition, while for deep learning-based end-to-end systems those constraints\nare not fully addressed. In this paper, we propose\ntransform-average-concatenate (TAC), a simple design paradigm for channel\npermutation and number invariant multi-channel speech separation. Based on the\nfilter-and-sum network (FaSNet), a recently proposed end-to-end time-domain\nbeamforming system, we show how TAC significantly improves the separation\nperformance across various numbers of microphones in noisy reverberant\nseparation tasks with ad-hoc arrays. Moreover, we show that TAC also\nsignificantly improves the separation performance with fixed geometry array\nconfiguration, further proving the effectiveness of the proposed paradigm in\nthe general problem of multi-microphone speech separation.", "published": "2019-10-30 19:45:34", "link": "http://arxiv.org/abs/1910.14104v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fast acoustic scattering using convolutional neural networks", "abstract": "Diffracted scattering and occlusion are important acoustic effects in\ninteractive auralization and noise control applications, typically requiring\nexpensive numerical simulation. We propose training a convolutional neural\nnetwork to map from a convex scatterer's cross-section to a 2D slice of the\nresulting spatial loudness distribution. We show that employing a\nfull-resolution residual network for the resulting image-to-image regression\nproblem yields spatially detailed loudness fields with a root-mean-squared\nerror of less than 1 dB, at over 100x speedup compared to full wave simulation.", "published": "2019-10-30 23:53:18", "link": "http://arxiv.org/abs/1911.01802v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.IV", "eess.SP"], "primary_category": "eess.AS"}
