{"title": "MarkBERT: Marking Word Boundaries Improves Chinese BERT", "abstract": "We present a Chinese BERT model dubbed MarkBERT that uses word information in\nthis work. Existing word-based BERT models regard words as basic units,\nhowever, due to the vocabulary limit of BERT, they only cover high-frequency\nwords and fall back to character level when encountering out-of-vocabulary\n(OOV) words. Different from existing works, MarkBERT keeps the vocabulary being\nChinese characters and inserts boundary markers between contiguous words. Such\ndesign enables the model to handle any words in the same way, no matter they\nare OOV words or not. Besides, our model has two additional benefits: first, it\nis convenient to add word-level learning objectives over markers, which is\ncomplementary to traditional character and sentence-level pretraining tasks;\nsecond, it can easily incorporate richer semantics such as POS tags of words by\nreplacing generic markers with POS tag-specific markers. With the simple\nmarkers insertion, MarkBERT can improve the performances of various downstream\ntasks including language understanding and sequence labeling. \\footnote{All the\ncodes and models will be made publicly available at\n\\url{https://github.com/daiyongya/markbert}}", "published": "2022-03-12 08:43:06", "link": "http://arxiv.org/abs/2203.06378v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiBERT: Accurate Fully Binarized BERT", "abstract": "The large pre-trained BERT has achieved remarkable performance on Natural\nLanguage Processing (NLP) tasks but is also computation and memory expensive.\nAs one of the powerful compression approaches, binarization extremely reduces\nthe computation and memory consumption by utilizing 1-bit parameters and\nbitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit\nweight, embedding, and activation) usually suffer a significant performance\ndrop, and there is rare study addressing this problem. In this paper, with the\ntheoretical justification and empirical analysis, we identify that the severe\nperformance drop can be mainly attributed to the information degradation and\noptimization direction mismatch respectively in the forward and backward\npropagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate\nthe performance bottlenecks. Specifically, BiBERT introduces an efficient\nBi-Attention structure for maximizing representation information statistically\nand a Direction-Matching Distillation (DMD) scheme to optimize the full\nbinarized BERT accurately. Extensive experiments show that BiBERT outperforms\nboth the straightforward baseline and existing state-of-the-art quantized BERTs\nwith ultra-low bit activations by convincing margins on the NLP benchmark. As\nthe first fully binarized BERT, our method yields impressive 56.3 times and\n31.2 times saving on FLOPs and model size, demonstrating the vast advantages\nand potential of the fully binarized BERT model in real-world\nresource-constrained scenarios.", "published": "2022-03-12 09:46:13", "link": "http://arxiv.org/abs/2203.06390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A combined approach to the analysis of speech conversations in a contact\n  center domain", "abstract": "The ever more accurate search for deep analysis in customer data is a really\nstrong technological trend nowadays, quite appealing to both private and public\ncompanies. This is particularly true in the contact center domain, where speech\nanalytics is an extremely powerful methodology for gaining insights from\nunstructured data, coming from customer and human agent conversations. In this\nwork, we describe an experimentation with a speech analytics process for an\nItalian contact center, that deals with call recordings extracted from inbound\nor outbound flows. First, we illustrate in detail the development of an\nin-house speech-to-text solution, based on Kaldi framework, and evaluate its\nperformance (and compare it to Google Cloud Speech API). Then, we evaluate and\ncompare different approaches to the semantic tagging of call transcripts,\nranging from classic regular expressions to machine learning models based on\nngrams and logistic regression, and propose a combination of them, which is\nshown to provide a consistent benefit. Finally, a decision tree inducer, called\nJ48S, is applied to the problem of tagging. Such an algorithm is natively\ncapable of exploiting sequential data, such as texts, for classification\npurposes. The solution is compared with the other approaches and is shown to\nprovide competitive classification performances, while generating highly\ninterpretable models and reducing the complexity of the data preparation phase.\nThe potential operational impact of the whole process is thoroughly examined.", "published": "2022-03-12 10:03:20", "link": "http://arxiv.org/abs/2203.06396v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Adversarial Defences and Robustness in NLP", "abstract": "In the past few years, it has become increasingly evident that deep neural\nnetworks are not resilient enough to withstand adversarial perturbations in\ninput data, leaving them vulnerable to attack. Various authors have proposed\nstrong adversarial attacks for computer vision and Natural Language Processing\n(NLP) tasks. As a response, many defense mechanisms have also been proposed to\nprevent these networks from failing. The significance of defending neural\nnetworks against adversarial attacks lies in ensuring that the model's\npredictions remain unchanged even if the input data is perturbed. Several\nmethods for adversarial defense in NLP have been proposed, catering to\ndifferent NLP tasks such as text classification, named entity recognition, and\nnatural language inference. Some of these methods not only defend neural\nnetworks against adversarial attacks but also act as a regularization mechanism\nduring training, saving the model from overfitting. This survey aims to review\nthe various methods proposed for adversarial defenses in NLP over the past few\nyears by introducing a novel taxonomy. The survey also highlights the fragility\nof advanced deep neural networks in NLP and the challenges involved in\ndefending them.", "published": "2022-03-12 11:37:17", "link": "http://arxiv.org/abs/2203.06414v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FiNER: Financial Numeric Entity Recognition for XBRL Tagging", "abstract": "Publicly traded companies are required to submit periodic reports with\neXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging\nthe reports is tedious and costly. We, therefore, introduce XBRL tagging as a\nnew entity extraction task for the financial domain and release FiNER-139, a\ndataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction\ndatasets, FiNER-139 uses a much larger label set of 139 entity types. Most\nannotated tokens are numeric, with the correct tag per token depending mostly\non context, rather than the token itself. We show that subword fragmentation of\nnumeric expressions harms BERT's performance, allowing word-level BILSTMs to\nperform better. To improve BERT's performance, we propose two simple and\neffective solutions that replace numeric expressions with pseudo-tokens\nreflecting original token shapes and numeric magnitudes. We also experiment\nwith FIN-BERT, an existing BERT model for the financial domain, and release our\nown BERT (SEC-BERT), pre-trained on financial filings, which performs best.\nThrough data and error analysis, we finally identify possible limitations to\ninspire future work on XBRL tagging.", "published": "2022-03-12 16:43:57", "link": "http://arxiv.org/abs/2203.06482v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization", "abstract": "Charts are commonly used for exploring data and communicating insights.\nGenerating natural language summaries from charts can be very helpful for\npeople in inferring key insights that would otherwise require a lot of\ncognitive and perceptual efforts. We present Chart-to-text, a large-scale\nbenchmark with two datasets and a total of 44,096 charts covering a wide range\nof topics and chart types. We explain the dataset construction process and\nanalyze the datasets. We also introduce a number of state-of-the-art neural\nmodels as baselines that utilize image captioning and data-to-text generation\ntechniques to tackle two problem variations: one assumes the underlying data\ntable of the chart is available while the other needs to extract data from\nchart images. Our analysis with automatic and human evaluation shows that while\nour best models usually generate fluent summaries and yield reasonable BLEU\nscores, they also suffer from hallucinations and factual errors as well as\ndifficulties in correctly explaining complex patterns and trends in charts.", "published": "2022-03-12 17:01:38", "link": "http://arxiv.org/abs/2203.06486v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Information Hiding in Natural Language Systems", "abstract": "With data privacy becoming more of a necessity than a luxury in today's\ndigital world, research on more robust models of privacy preservation and\ninformation security is on the rise. In this paper, we take a look at Natural\nLanguage Steganography (NLS) methods, which perform information hiding in\nnatural language systems, as a means to achieve data security as well as\nconfidentiality. We summarize primary challenges regarding the secrecy and\nimperceptibility requirements of these systems and propose potential directions\nof improvement, specifically targeting steganographic text quality. We believe\nthat this study will act as an appropriate framework to build more resilient\nmodels of Natural Language Steganography, working towards instilling security\nwithin natural language-based neural models.", "published": "2022-03-12 20:34:05", "link": "http://arxiv.org/abs/2203.06512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Topic Modeling with Deep Mutual Information Estimation", "abstract": "The emerging neural topic models make topic modeling more easily adaptable\nand extendable in unsupervised text mining. However, the existing neural topic\nmodels is difficult to retain representative information of the documents\nwithin the learnt topic representation. In this paper, we propose a neural\ntopic model which incorporates deep mutual information estimation, i.e., Neural\nTopic Modeling with Deep Mutual Information Estimation(NTM-DMIE). NTM-DMIE is a\nneural network method for topic learning which maximizes the mutual information\nbetween the input documents and their latent topic representation. To learn\nrobust topic representation, we incorporate the discriminator to discriminate\nnegative examples and positive examples via adversarial learning. Moreover, we\nuse both global and local mutual information to preserve the rich information\nof the input documents in the topic representation. We evaluate NTM-DMIE on\nseveral metrics, including accuracy of text clustering, with topic\nrepresentation, topic uniqueness and topic coherence. Compared to the existing\nmethods, the experimental results show that NTM-DMIE can outperform in all the\nmetrics on the four datasets.", "published": "2022-03-12 01:08:10", "link": "http://arxiv.org/abs/2203.06298v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ensemble Semi-supervised Entity Alignment via Cycle-teaching", "abstract": "Entity alignment is to find identical entities in different knowledge graphs.\nAlthough embedding-based entity alignment has recently achieved remarkable\nprogress, training data insufficiency remains a critical challenge.\nConventional semi-supervised methods also suffer from the incorrect entity\nalignment in newly proposed training data. To resolve these issues, we design\nan iterative cycle-teaching framework for semi-supervised entity alignment. The\nkey idea is to train multiple entity alignment models (called aligners)\nsimultaneously and let each aligner iteratively teach its successor the\nproposed new entity alignment. We propose a diversity-aware alignment selection\nmethod to choose reliable entity alignment for each aligner. We also design a\nconflict resolution mechanism to resolve the alignment conflict when combining\nthe new alignment of an aligner and that from its teacher. Besides, considering\nthe influence of cycle-teaching order, we elaborately design a strategy to\narrange the optimal order that can maximize the overall performance of multiple\naligners. The cycle-teaching process can break the limitations of each model's\nlearning capability and reduce the noise in new training data, leading to\nimproved performance. Extensive experiments on benchmark datasets demonstrate\nthe effectiveness of the proposed cycle-teaching framework, which significantly\noutperforms the state-of-the-art models when the training data is insufficient\nand the new entity alignment has much noise.", "published": "2022-03-12 01:43:50", "link": "http://arxiv.org/abs/2203.06308v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ELLE: Efficient Lifelong Pre-training for Emerging Data", "abstract": "Current pre-trained language models (PLM) are typically trained with static\ndata, ignoring that in real-world scenarios, streaming data of various sources\nmay continuously grow. This requires PLMs to integrate the information from all\nthe sources in a lifelong manner. Although this goal could be achieved by\nexhaustive pre-training on all the existing data, such a process is known to be\ncomputationally expensive. To this end, we propose ELLE, aiming at efficient\nlifelong pre-training for emerging data. Specifically, ELLE consists of (1)\nfunction preserved model expansion, which flexibly expands an existing PLM's\nwidth and depth to improve the efficiency of knowledge acquisition; and (2)\npre-trained domain prompts, which disentangle the versatile knowledge learned\nduring pre-training and stimulate the proper knowledge for downstream tasks. We\nexperiment ELLE with streaming data from 5 domains on BERT and GPT. The results\nshow the superiority of ELLE over various lifelong learning baselines in both\npre-training efficiency and downstream performances. The codes are publicly\navailable at https://github.com/thunlp/ELLE.", "published": "2022-03-12 01:53:53", "link": "http://arxiv.org/abs/2203.06311v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Equal Opportunity Fairness through Adversarial Learning", "abstract": "Adversarial training is a common approach for bias mitigation in natural\nlanguage processing. Although most work on debiasing is motivated by equal\nopportunity, it is not explicitly captured in standard adversarial training. In\nthis paper, we propose an augmented discriminator for adversarial training,\nwhich takes the target class as input to create richer features and more\nexplicitly model equal opportunity. Experimental results over two datasets show\nthat our method substantially improves over standard adversarial debiasing\nmethods, in terms of the performance--fairness trade-off.", "published": "2022-03-12 02:22:58", "link": "http://arxiv.org/abs/2203.06317v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Makes Reading Comprehension Questions Difficult?", "abstract": "For a natural language understanding benchmark to be useful in research, it\nhas to consist of examples that are diverse and difficult enough to\ndiscriminate among current and near-future state-of-the-art systems. However,\nwe do not yet know how best to select text sources to collect a variety of\nchallenging examples. In this study, we crowdsource multiple-choice reading\ncomprehension questions for passages taken from seven qualitatively distinct\nsources, analyzing what attributes of passages contribute to the difficulty and\nquestion types of the collected examples. To our surprise, we find that passage\nsource, length, and readability measures do not significantly affect question\ndifficulty. Through our manual annotation of seven reasoning types, we observe\nseveral trends between passage sources and reasoning types, e.g., logical\nreasoning is more often required in questions written for technical passages.\nThese results suggest that when creating a new benchmark dataset, selecting a\ndiverse set of passages can help ensure a diverse range of question types, but\nthat passage difficulty need not be a priority.", "published": "2022-03-12 04:23:28", "link": "http://arxiv.org/abs/2203.06342v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Proposal to Study \"Is High Quality Data All We Need?\"", "abstract": "Even though deep neural models have achieved superhuman performance on many\npopular benchmarks, they have failed to generalize to OOD or adversarial\ndatasets. Conventional approaches aimed at increasing robustness include\ndeveloping increasingly large models and augmentation with large scale\ndatasets. However, orthogonal to these trends, we hypothesize that a smaller,\nhigh quality dataset is what we need. Our hypothesis is based on the fact that\ndeep neural networks are data driven models, and data is what leads/misleads\nmodels. In this work, we propose an empirical study that examines how to select\na subset of and/or create high quality benchmark data, for a model to learn\neffectively. We seek to answer if big datasets are truly needed to learn a\ntask, and whether a smaller subset of high quality data can replace big\ndatasets. We plan to investigate both data pruning and data creation paradigms\nto generate high quality datasets.", "published": "2022-03-12 10:50:13", "link": "http://arxiv.org/abs/2203.06404v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "When did you become so smart, oh wise one?! Sarcasm Explanation in\n  Multi-modal Multi-party Dialogues", "abstract": "Indirect speech such as sarcasm achieves a constellation of discourse goals\nin human communication. While the indirectness of figurative language warrants\nspeakers to achieve certain pragmatic goals, it is challenging for AI agents to\ncomprehend such idiosyncrasies of human communication. Though sarcasm\nidentification has been a well-explored topic in dialogue analysis, for\nconversational systems to truly grasp a conversation's innate meaning and\ngenerate appropriate responses, simply detecting sarcasm is not enough; it is\nvital to explain its underlying sarcastic connotation to capture its true\nessence. In this work, we study the discourse structure of sarcastic\nconversations and propose a novel task - Sarcasm Explanation in Dialogue (SED).\nSet in a multimodal and code-mixed setting, the task aims to generate natural\nlanguage explanations of satirical conversations. To this end, we curate WITS,\na new dataset to support our task. We propose MAF (Modality Aware Fusion), a\nmultimodal context-aware attention and global information fusion module to\ncapture multimodality and use it to benchmark WITS. The proposed attention\nmodule surpasses the traditional multimodal fusion baselines and reports the\nbest performance on almost all metrics. Lastly, we carry out detailed analyses\nboth quantitatively and qualitatively.", "published": "2022-03-12 12:16:07", "link": "http://arxiv.org/abs/2203.06419v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in\n  Practice", "abstract": "Classifiers in natural language processing (NLP) often have a large number of\noutput classes. For example, neural language models (LMs) and machine\ntranslation (MT) models both predict tokens from a vocabulary of thousands. The\nSoftmax output layer of these models typically receives as input a dense\nfeature representation, which has much lower dimensionality than the output. In\ntheory, the result is some words may be impossible to be predicted via argmax,\nirrespective of input features, and empirically, there is evidence this happens\nin small language models. In this paper we ask whether it can happen in\npractical large language models and translation models. To do so, we develop\nalgorithms to detect such \\emph{unargmaxable} tokens in public models. We find\nthat 13 out of 150 models do indeed have such tokens; however, they are very\ninfrequent and unlikely to impact model quality. We release our code so that\nothers can inspect their models.", "published": "2022-03-12 15:34:54", "link": "http://arxiv.org/abs/2203.06462v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enabling Multimodal Generation on CLIP via Vision-Language Knowledge\n  Distillation", "abstract": "The recent large-scale vision-language pre-training (VLP) of dual-stream\narchitectures (e.g., CLIP) with a tremendous amount of image-text pair data,\nhas shown its superiority on various multimodal alignment tasks. Despite its\nsuccess, the resulting models are not capable of multimodal generative tasks\ndue to the weak text encoder. To tackle this problem, we propose to augment the\ndual-stream VLP model with a textual pre-trained language model (PLM) via\nvision-language knowledge distillation (VLKD), enabling the capability for\nmultimodal generation. VLKD is pretty data- and computation-efficient compared\nto the pre-training from scratch. Experimental results show that the resulting\nmodel has strong zero-shot performance on multimodal generation tasks, such as\nopen-ended visual question answering and image captioning. For example, it\nachieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous\nstate-of-the-art zero-shot model with $7\\times$ fewer parameters. Furthermore,\nthe original textual language understanding and generation ability of the PLM\nis maintained after VLKD, which makes our model versatile for both multimodal\nand unimodal tasks.", "published": "2022-03-12 09:33:37", "link": "http://arxiv.org/abs/2203.06386v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SA-SASV: An End-to-End Spoof-Aggregated Spoofing-Aware Speaker\n  Verification System", "abstract": "Research in the past several years has boosted the performance of automatic\nspeaker verification systems and countermeasure systems to deliver low Equal\nError Rates (EERs) on each system. However, research on joint optimization of\nboth systems is still limited. The Spoofing-Aware Speaker Verification (SASV)\n2022 challenge was proposed to encourage the development of integrated SASV\nsystems with new metrics to evaluate joint model performance. This paper\nproposes an ensemble-free end-to-end solution, known as Spoof-Aggregated-SASV\n(SA-SASV) to build a SASV system with multi-task classifiers, which are\noptimized by multiple losses and has more flexible requirements in training\nset. The proposed system is trained on the ASVSpoof 2019 LA dataset, a spoof\nverification dataset with small number of bonafide speakers. Results of\nSASV-EER indicate that the model performance can be further improved by\ntraining in complete automatic speaker verification and countermeasure\ndatasets.", "published": "2022-03-12 21:15:59", "link": "http://arxiv.org/abs/2203.06517v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
