{"title": "Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue\n  System", "abstract": "Understanding dynamic scenes and dialogue contexts in order to converse with\nusers has been challenging for multimodal dialogue systems. The 8-th Dialog\nSystem Technology Challenge (DSTC8) proposed an Audio Visual Scene-Aware Dialog\n(AVSD) task, which contains multiple modalities including audio, vision, and\nlanguage, to evaluate how dialogue systems understand different modalities and\nresponse to users. In this paper, we proposed a multi-step joint-modality\nattention network (JMAN) based on recurrent neural network (RNN) to reason on\nvideos. Our model performs a multi-step attention mechanism and jointly\nconsiders both visual and textual representations in each reasoning process to\nbetter integrate information from the two different modalities. Compared to the\nbaseline released by AVSD organizers, our model achieves a relative 12.1% and\n22.4% improvement over the baseline on ROUGE-L score and CIDEr score.", "published": "2020-01-17 09:18:00", "link": "http://arxiv.org/abs/2001.06206v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Common Semantic Space for Monolingual and Cross-Lingual\n  Meta-Embeddings", "abstract": "This paper presents a new technique for creating monolingual and\ncross-lingual meta-embeddings. Our method integrates multiple word embeddings\ncreated from complementary techniques, textual sources, knowledge bases and\nlanguages. Existing word vectors are projected to a common semantic space using\nlinear transformations and averaging. With our method the resulting\nmeta-embeddings maintain the dimensionality of the original embeddings without\nlosing information while dealing with the out-of-vocabulary problem. An\nextensive empirical evaluation demonstrates the effectiveness of our technique\nwith respect to previous work on various intrinsic and extrinsic multilingual\nevaluations, obtaining competitive results for Semantic Textual Similarity and\nstate-of-the-art performance for word similarity and POS tagging (English and\nSpanish). The resulting cross-lingual meta-embeddings also exhibit excellent\ncross-lingual transfer learning capabilities. In other words, we can leverage\npre-trained source embeddings from a resource-rich language in order to improve\nthe word representations for under-resourced languages.", "published": "2020-01-17 15:42:29", "link": "http://arxiv.org/abs/2001.06381v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generaci\u00f3n autom\u00e1tica de frases literarias en espa\u00f1ol", "abstract": "In this work we present a state of the art in the area of Computational\nCreativity (CC). In particular, we address the automatic generation of literary\nsentences in Spanish. We propose three models of text generation based mainly\non statistical algorithms and shallow parsing analysis. We also present some\nrather encouraging preliminary results.", "published": "2020-01-17 15:42:14", "link": "http://arxiv.org/abs/2001.11381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RobBERT: a Dutch RoBERTa-based Language Model", "abstract": "Pre-trained language models have been dominating the field of natural\nlanguage processing in recent years, and have led to significant performance\ngains for various complex natural language tasks. One of the most prominent\npre-trained language models is BERT, which was released as an English as well\nas a multilingual version. Although multilingual BERT performs well on many\ntasks, recent studies show that BERT models trained on a single language\nsignificantly outperform the multilingual version. Training a Dutch BERT model\nthus has a lot of potential for a wide range of Dutch NLP tasks. While previous\napproaches have used earlier implementations of BERT to train a Dutch version\nof BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch\nlanguage model called RobBERT. We measured its performance on various tasks as\nwell as the importance of the fine-tuning dataset size. We also evaluated the\nimportance of language-specific tokenizers and the model's fairness. We found\nthat RobBERT improves state-of-the-art results for various tasks, and\nespecially significantly outperforms other models when dealing with smaller\ndatasets. These results indicate that it is a powerful pre-trained model for a\nlarge variety of Dutch language tasks. The pre-trained and fine-tuned models\nare publicly available to support further downstream Dutch NLP applications.", "published": "2020-01-17 13:25:44", "link": "http://arxiv.org/abs/2001.06286v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modality-Balanced Models for Visual Dialogue", "abstract": "The Visual Dialog task requires a model to exploit both image and\nconversational context information to generate the next response to the\ndialogue. However, via manual analysis, we find that a large number of\nconversational questions can be answered by only looking at the image without\nany access to the context history, while others still need the conversation\ncontext to predict the correct answers. We demonstrate that due to this reason,\nprevious joint-modality (history and image) models over-rely on and are more\nprone to memorizing the dialogue history (e.g., by extracting certain keywords\nor patterns in the context information), whereas image-only models are more\ngeneralizable (because they cannot memorize or extract keywords from history)\nand perform substantially better at the primary normalized discounted\ncumulative gain (NDCG) task metric which allows multiple correct answers.\nHence, this observation encourages us to explicitly maintain two models, i.e.,\nan image-only model and an image-history joint model, and combine their\ncomplementary abilities for a more balanced multimodal model. We present\nmultiple methods for this integration of the two models, via ensemble and\nconsensus dropout fusion with shared parameters. Empirically, our models\nachieve strong results on the Visual Dialog challenge 2019 (rank 3 on NDCG and\nhigh balance across metrics), and substantially outperform the winner of the\nVisual Dialog challenge 2018 on most metrics.", "published": "2020-01-17 14:57:12", "link": "http://arxiv.org/abs/2001.06354v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Plato Dialogue System: A Flexible Conversational AI Research Platform", "abstract": "As the field of Spoken Dialogue Systems and Conversational AI grows, so does\nthe need for tools and environments that abstract away implementation details\nin order to expedite the development process, lower the barrier of entry to the\nfield, and offer a common test-bed for new ideas. In this paper, we present\nPlato, a flexible Conversational AI platform written in Python that supports\nany kind of conversational agent architecture, from standard architectures to\narchitectures with jointly-trained components, single- or multi-party\ninteractions, and offline or online training of any conversational agent\ncomponent. Plato has been designed to be easy to understand and debug and is\nagnostic to the underlying learning frameworks that train each component.", "published": "2020-01-17 18:27:29", "link": "http://arxiv.org/abs/2001.06463v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
