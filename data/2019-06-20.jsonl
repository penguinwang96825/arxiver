{"title": "Hierarchical Document Encoder for Parallel Corpus Mining", "abstract": "We explore using multilingual document embeddings for nearest neighbor mining\nof parallel data. Three document-level representations are investigated: (i)\ndocument embeddings generated by simply averaging multilingual sentence\nembeddings; (ii) a neural bag-of-words (BoW) document encoding model; (iii) a\nhierarchical multilingual document encoder (HiDE) that builds on our\nsentence-level model. The results show document embeddings derived from\nsentence-level averaging are surprisingly effective for clean datasets, but\nsuggest models trained hierarchically at the document-level are more effective\non noisy data. Analysis experiments demonstrate our hierarchical models are\nvery robust to variations in the underlying sentence embedding quality. Using\ndocument embeddings trained with HiDE achieves state-of-the-art performance on\nUnited Nations (UN) parallel document mining, 94.9% P@1 for en-fr and 97.3% P@1\nfor en-es.", "published": "2019-06-20 00:59:22", "link": "http://arxiv.org/abs/1906.08401v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Grained Named Entity Recognition", "abstract": "This paper presents a novel framework, MGNER, for Multi-Grained Named Entity\nRecognition where multiple entities or entity mentions in a sentence could be\nnon-overlapping or totally nested. Different from traditional approaches\nregarding NER as a sequential labeling task and annotate entities\nconsecutively, MGNER detects and recognizes entities on multiple granularities:\nit is able to recognize named entities without explicitly assuming\nnon-overlapping or totally nested structures. MGNER consists of a Detector that\nexamines all possible word segments and a Classifier that categorizes entities.\nIn addition, contextual information and a self-attention mechanism are utilized\nthroughout the framework to improve the NER performance. Experimental results\nshow that MGNER outperforms current state-of-the-art baselines up to 4.4% in\nterms of the F1 score among nested/non-overlapping NER tasks.", "published": "2019-06-20 05:33:30", "link": "http://arxiv.org/abs/1906.08449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hindi Question Generation Using Dependency Structures", "abstract": "Hindi question answering systems suffer from a lack of data. To address the\nsame, this paper presents an approach towards automatic question generation. We\npresent a rule-based system for question generation in Hindi by formalizing\nquestion transformation methods based on karaka-dependency theory. We use a\nHindi dependency parser to mark the karaka roles and use IndoWordNet a Hindi\nontology to detect the semantic category of the karaka role heads to generate\nthe interrogatives. We analyze how one sentence can have multiple generations\nfrom the same karaka role's rule. The generations are manually annotated by\nmultiple annotators on a semantic and syntactic scale for evaluation. Further,\nwe constrain our generation with the help of various semantic and syntactic\nfilters so as to improve the generation quality. Using these methods, we are\nable to generate diverse questions, significantly more than number of sentences\nfed to the system.", "published": "2019-06-20 12:05:13", "link": "http://arxiv.org/abs/1906.08570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-shot Translation with Language-Independent Constraints", "abstract": "An important concern in training multilingual neural machine translation\n(NMT) is to translate between language pairs unseen during training, i.e\nzero-shot translation. Improving this ability kills two birds with one stone by\nproviding an alternative to pivot translation which also allows us to better\nunderstand how the model captures information between languages.\n  In this work, we carried out an investigation on this capability of the\nmultilingual NMT models. First, we intentionally create an encoder architecture\nwhich is independent with respect to the source language. Such experiments shed\nlight on the ability of NMT encoders to learn multilingual representations, in\ngeneral. Based on such proof of concept, we were able to design regularization\nmethods into the standard Transformer model, so that the whole architecture\nbecomes more robust in zero-shot conditions. We investigated the behaviour of\nsuch models on the standard IWSLT 2017 multilingual dataset. We achieved an\naverage improvement of 2.23 BLEU points across 12 language pairs compared to\nthe zero-shot performance of a state-of-the-art multilingual system.\nAdditionally, we carry out further experiments in which the effect is confirmed\neven for language pairs with multiple intermediate pivots.", "published": "2019-06-20 12:49:17", "link": "http://arxiv.org/abs/1906.08584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conflict as an Inverse of Attention in Sequence Relationship", "abstract": "Attention is a very efficient way to model the relationship between two\nsequences by comparing how similar two intermediate representations are.\nInitially demonstrated in NMT, it is a standard in all NLU tasks today when\nefficient interaction between sequences is considered. However, we show that\nattention, by virtue of its composition, works best only when it is given that\nthere is a match somewhere between two sequences. It does not very well adapt\nto cases when there is no similarity between two sequences or if the\nrelationship is contrastive. We propose an Conflict model which is very similar\nto how attention works but which emphasizes mostly on how well two sequences\nrepel each other and finally empirically show how this method in conjunction\nwith attention can boost the overall performance.", "published": "2019-06-20 13:16:37", "link": "http://arxiv.org/abs/1906.08593v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Statistical Approach for Comparing Algorithms for Lexicon Based\n  Sentiment Analysis", "abstract": "Lexicon based sentiment analysis usually relies on the identification of\nvarious words to which a numerical value corresponding to sentiment can be\nassigned. In principle, classifiers can be obtained from these algorithms by\ncomparison with human annotation, which is considered the gold standard. In\npractise this is difficult in languages such as Portuguese where there is a\npaucity of human annotated texts. Thus in order to compare algorithms, a next\nbest step is to directly compare different algorithms with each other without\nreferring to human annotation. In this paper we develop methods for a\nstatistical comparison of algorithms which does not rely on human annotation or\non known class labels. We will motivate the use of marginal homogeneity tests,\nas well as log linear models within the framework of maximum likelihood\nestimation We will also show how some uncertainties present in lexicon based\nsentiment analysis may be similar to those which occur in human annotated\ntweets. We will also show how the variability in the output of different\nalgorithms is lexicon dependent, and quantify this variability in the output\nwithin the framework of log linear models.", "published": "2019-06-20 16:04:38", "link": "http://arxiv.org/abs/1906.08717v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Low-Resource Corpus Filtering using Multilingual Sentence Embeddings", "abstract": "In this paper, we describe our submission to the WMT19 low-resource parallel\ncorpus filtering shared task. Our main approach is based on the LASER toolkit\n(Language-Agnostic SEntence Representations), which uses an encoder-decoder\narchitecture trained on a parallel corpus to obtain multilingual sentence\nrepresentations. We then use the representations directly to score and filter\nthe noisy parallel sentences without additionally training a scoring function.\nWe contrast our approach to other promising methods and show that LASER yields\nstrong results. Finally, we produce an ensemble of different scoring methods\nand obtain additional gains. Our submission achieved the best overall\nperformance for both the Nepali-English and Sinhala-English 1M tasks by a\nmargin of 1.3 and 1.4 BLEU respectively, as compared to the second best\nsystems. Moreover, our experiments show that this technique is promising for\nlow and even no-resource scenarios.", "published": "2019-06-20 22:39:44", "link": "http://arxiv.org/abs/1906.08885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Collective Entity Linking Based on Recurrent Random Walk Network\n  Learning", "abstract": "Benefiting from the excellent ability of neural networks on learning semantic\nrepresentations, existing studies for entity linking (EL) have resorted to\nneural networks to exploit both the local mention-to-entity compatibility and\nthe global interdependence between different EL decisions for target entity\ndisambiguation. However, most neural collective EL methods depend entirely upon\nneural networks to automatically model the semantic dependencies between\ndifferent EL decisions, which lack of the guidance from external knowledge. In\nthis paper, we propose a novel end-to-end neural network with recurrent\nrandom-walk layers for collective EL, which introduces external knowledge to\nmodel the semantic interdependence between different EL decisions.\nSpecifically, we first establish a model based on local context features, and\nthen stack random-walk layers to reinforce the evidence for related EL\ndecisions into high-probability decisions, where the semantic interdependence\nbetween candidate entities is mainly induced from an external knowledge base.\nFinally, a semantic regularizer that preserves the collective EL decisions\nconsistency is incorporated into the conventional objective function, so that\nthe external knowledge base can be fully exploited in collective EL decisions.\nExperimental results and in-depth analysis on various datasets show that our\nmodel achieves better performance than other state-of-the-art models. Our code\nand data are released at \\url{https://github.com/DeepLearnXMU/RRWEL}.", "published": "2019-06-20 11:09:12", "link": "http://arxiv.org/abs/1906.09320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Empathetic Responses by Looking Ahead the User's Sentiment", "abstract": "An important aspect of human conversation difficult for machines is\nconversing with empathy, which is to understand the user's emotion and respond\nappropriately. Recent neural conversation models that attempted to generate\nempathetic responses either focused on conditioning the output to a given\nemotion, or incorporating the current user emotional state. However, these\napproaches do not factor in how the user would feel towards the generated\nresponse. Hence, in this paper, we propose Sentiment Look-ahead, which is a\nnovel perspective for empathy that models the future user emotional state. In\nshort, Sentiment Look-ahead is a reward function under a reinforcement learning\nframework that provides a higher reward to the generative model when the\ngenerated utterance improves the user's sentiment. We implement and evaluate\nthree different possible implementations of sentiment look-ahead and\nempirically show that our proposed approach can generate significantly more\nempathetic, relevant, and fluent responses than other competitive baselines\nsuch as multitask learning.", "published": "2019-06-20 08:03:58", "link": "http://arxiv.org/abs/1906.08487v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-Shot Sequence Labeling with Label Dependency Transfer and Pair-wise\n  Embedding", "abstract": "While few-shot classification has been widely explored with similarity based\nmethods, few-shot sequence labeling poses a unique challenge as it also calls\nfor modeling the label dependencies. To consider both the item similarity and\nlabel dependency, we propose to leverage the conditional random fields (CRFs)\nin few-shot sequence labeling. It calculates emission score with similarity\nbased methods and obtains transition score with a specially designed transfer\nmechanism. When applying CRF in the few-shot scenarios, the discrepancy of\nlabel sets among different domains makes it hard to use the label dependency\nlearned in prior domains. To tackle this, we introduce the dependency transfer\nmechanism that transfers abstract label transition patterns. In addition, the\nsimilarity methods rely on the high quality sample representation, which is\nchallenging for sequence labeling, because sense of a word is different when\nmeasuring its similarity to words in different sentences. To remedy this, we\ntake advantage of recent contextual embedding technique, and further propose a\npair-wise embedder. It provides additional certainty for word sense by\nembedding query and support sentence pairwisely. Experimental results on slot\ntagging and named entity recognition show that our model significantly\noutperforms the strongest few-shot learning baseline by 11.76 (21.2%) and 12.18\n(97.7%) F1 scores respectively in the one-shot setting.", "published": "2019-06-20 15:57:26", "link": "http://arxiv.org/abs/1906.08711v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Autonomous Haiku Generation", "abstract": "Artificial Intelligence is an excellent tool to improve efficiency and lower\ncost in many quantitative real world applications, but what if the task is not\neasily defined? What if the task is generating creativity? Poetry is a creative\nendeavor that is highly difficult to both grasp and achieve with any level of\ncompetence. As Rita Dove, a famous American poet and author states, \"Poetry is\nlanguage at its most distilled and most powerful.\" Taking Doves quote as an\ninspiration, our task was to generate high quality haikus using artificial\nintelligence and deep learning.", "published": "2019-06-20 16:25:47", "link": "http://arxiv.org/abs/1906.08733v1", "categories": ["cs.CL", "cs.AI", "97R40"], "primary_category": "cs.CL"}
{"title": "Informative Image Captioning with External Sources of Information", "abstract": "An image caption should fluently present the essential information in a given\nimage, including informative, fine-grained entity mentions and the manner in\nwhich these entities interact. However, current captioning models are usually\ntrained to generate captions that only contain common object names, thus\nfalling short on an important \"informativeness\" dimension. We present a\nmechanism for integrating image information together with fine-grained labels\n(assumed to be generated by some upstream models) into a caption that describes\nthe image in a fluent and informative manner. We introduce a multimodal,\nmulti-encoder model based on Transformer that ingests both image features and\nmultiple sources of entity labels. We demonstrate that we can learn to control\nthe appearance of these entity labels in the output, resulting in captions that\nare both fluent and informative.", "published": "2019-06-20 21:51:48", "link": "http://arxiv.org/abs/1906.08876v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Adversarial Regularization for Visual Question Answering: Strengths,\n  Shortcomings, and Side Effects", "abstract": "Visual question answering (VQA) models have been shown to over-rely on\nlinguistic biases in VQA datasets, answering questions \"blindly\" without\nconsidering visual context. Adversarial regularization (AdvReg) aims to address\nthis issue via an adversary sub-network that encourages the main model to learn\na bias-free representation of the question. In this work, we investigate the\nstrengths and shortcomings of AdvReg with the goal of better understanding how\nit affects inference in VQA models. Despite achieving a new state-of-the-art on\nVQA-CP, we find that AdvReg yields several undesirable side-effects, including\nunstable gradients and sharply reduced performance on in-domain examples. We\ndemonstrate that gradual introduction of regularization during training helps\nto alleviate, but not completely solve, these issues. Through error analyses,\nwe observe that AdvReg improves generalization to binary questions, but impairs\nperformance on questions with heterogeneous answer distributions.\nQualitatively, we also find that regularized models tend to over-rely on visual\nfeatures, while ignoring important linguistic cues in the question. Our results\nsuggest that AdvReg requires further refinement before it can be considered a\nviable bias mitigation technique for VQA.", "published": "2019-06-20 03:28:09", "link": "http://arxiv.org/abs/1906.08430v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Semi-supervised acoustic model training for five-lingual code-switched\n  ASR", "abstract": "This paper presents recent progress in the acoustic modelling of\nunder-resourced code-switched (CS) speech in multiple South African languages.\nWe consider two approaches. The first constructs separate bilingual acoustic\nmodels corresponding to language pairs (English-isiZulu, English-isiXhosa,\nEnglish-Setswana and English-Sesotho). The second constructs a single unified\nfive-lingual acoustic model representing all the languages (English, isiZulu,\nisiXhosa, Setswana and Sesotho). For these two approaches we consider the\neffectiveness of semi-supervised training to increase the size of the very\nsparse acoustic training sets. Using approximately 11 hours of untranscribed\nspeech, we show that both approaches benefit from semi-supervised training. The\nbilingual TDNN-F acoustic models also benefit from the addition of CNN layers\n(CNN-TDNN-F), while the five-lingual system does not show any significant\nimprovement. Furthermore, because English is common to all language pairs in\nour data, it dominates when training a unified language model, leading to\nimproved English ASR performance at the expense of the other languages.\nNevertheless, the five-lingual model offers flexibility because it can process\nmore than two languages simultaneously, and is therefore an attractive option\nas an automatic transcription system in a semi-supervised training pipeline.", "published": "2019-06-20 14:11:55", "link": "http://arxiv.org/abs/1906.08647v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Deep Eyedentification: Biometric Identification using Micro-Movements of\n  the Eye", "abstract": "We study involuntary micro-movements of the eye for biometric identification.\nWhile prior studies extract lower-frequency macro-movements from the output of\nvideo-based eye-tracking systems and engineer explicit features of these\nmacro-movements, we develop a deep convolutional architecture that processes\nthe raw eye-tracking signal. Compared to prior work, the network attains a\nlower error rate by one order of magnitude and is faster by two orders of\nmagnitude: it identifies users accurately within seconds.", "published": "2019-06-20 10:36:40", "link": "http://arxiv.org/abs/1906.11889v5", "categories": ["cs.CV", "cs.CL", "cs.HC", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "A Signal Subspace Rotation Method for Localization of Multiple Wideband\n  Sound Sources", "abstract": "In this paper, the problem of extending narrowband multichannel sound source\nlocalization algorithms to the wideband case is addressed. The DOA estimation\nof narrowband algorithms is based on the estimate of inter-channel phase\ndifferences (IPD) between microphones of the sound sources. A new method for\nwideband sound source DOA estimation based on signal subspace rotation is\npresent. The proposed algorithm normalizes the narrowband signal statistics by\nrotating the estimated signal subspace to the wideband counterpart in the\neigenvector domain. Then the wideband DOA estimate can be obtained by\nestimating the normalized IPD from these wideband signal statistics. In\naddition to requiring less computational complexity compared to repeating the\nnarrowband algorithms for all relevant frequencies of wideband signals, the\nproposed method also does not require any additional prior knowledge. The\nexperimental results demonstrate the efficacy and the robustness of the\nproposed method.", "published": "2019-06-20 20:49:31", "link": "http://arxiv.org/abs/1906.08847v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Parameter Enhancement for MELP Speech Codec in Noisy Communication\n  Environment", "abstract": "In this paper, we propose a deep learning (DL)-based parameter enhancement\nmethod for a mixed excitation linear prediction (MELP) speech codec in noisy\ncommunication environment. Unlike conventional speech enhancement modules that\nare designed to obtain clean speech signal by removing noise components before\nspeech codec processing, the proposed method directly enhances codec parameters\non either the encoder or decoder side. As the proposed method has been\nimplemented by a small network without any additional processes required in\nconventional enhancement systems, e.g., time-frequency (T-F) analysis/synthesis\nmodules, its computational complexity is very low. By enhancing the\nnoise-corrupted codec parameters with the proposed DL framework, we achieved an\nenhancement system that is much simpler and faster than conventional T-F\nmask-based speech enhancement methods, while the quality of its performance\nremains similar.", "published": "2019-06-20 01:20:50", "link": "http://arxiv.org/abs/1906.08407v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Monaural Speech Enhancement Method for Robust Small-Footprint Keyword\n  Spotting", "abstract": "Robustness against noise is critical for keyword spotting (KWS) in real-world\nenvironments. To improve the robustness, a speech enhancement front-end is\ninvolved. Instead of treating the speech enhancement as a separated\npreprocessing before the KWS system, in this study, a pre-trained speech\nenhancement front-end and a convolutional neural networks (CNNs) based KWS\nsystem are concatenated, where a feature transformation block is used to\ntransform the output from the enhancement front-end into the KWS system's\ninput. The whole model is trained jointly, thus the linguistic and other useful\ninformation from the KWS system can be back-propagated to the enhancement\nfront-end to improve its performance. To fit the small-footprint device, a\nnovel convolution recurrent network is proposed, which needs fewer parameters\nand computation and does not degrade performance. Furthermore, by changing the\ninput features from the power spectrogram to Mel-spectrogram, less computation\nand better performance are obtained. our experimental results demonstrate that\nthe proposed method significantly improves the KWS system with respect to noise\nrobustness.", "published": "2019-06-20 02:25:45", "link": "http://arxiv.org/abs/1906.08415v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Learning for Improved Onsets and Frames Music Transcription", "abstract": "Automatic music transcription is considered to be one of the hardest problems\nin music information retrieval, yet recent deep learning approaches have\nachieved substantial improvements on transcription performance. These\napproaches commonly employ supervised learning models that predict various\ntime-frequency representations, by minimizing element-wise losses such as the\ncross entropy function. However, applying the loss in this manner assumes\nconditional independence of each label given the input, and thus cannot\naccurately express inter-label dependencies. To address this issue, we\nintroduce an adversarial training scheme that operates directly on the\ntime-frequency representations and makes the output distribution closer to the\nground-truth. Through adversarial learning, we achieve a consistent improvement\nin both frame-level and note-level metrics over Onsets and Frames, a\nstate-of-the-art music transcription model. Our results show that adversarial\nlearning can significantly reduce the error rate while increasing the\nconfidence of the model estimations. Our approach is generic and applicable to\nany transcription model based on multi-label predictions, which are very common\nin music signal analysis.", "published": "2019-06-20 09:17:36", "link": "http://arxiv.org/abs/1906.08512v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Unleashing the Unused Potential of I-Vectors Enabled by GPU Acceleration", "abstract": "Speaker embeddings are continuous-value vector representations that allow\neasy comparison between voices of speakers with simple geometric operations.\nAmong others, i-vector and x-vector have emerged as the mainstream methods for\nspeaker embedding. In this paper, we illustrate the use of modern computation\nplatform to harness the benefit of GPU acceleration for i-vector extraction. In\nparticular, we achieve an acceleration of 3000 times in frame posterior\ncomputation compared to real time and 25 times in training the i-vector\nextractor compared to the CPU baseline from Kaldi toolkit. This significant\nspeed-up allows the exploration of ideas that were hitherto impossible. In\nparticular, we show that it is beneficial to update the universal background\nmodel (UBM) and re-compute frame alignments while training the i-vector\nextractor. Additionally, we are able to study different variations of i-vector\nextractors more rigorously than before. In this process, we reveal some\nundocumented details of Kaldi's i-vector extractor and show that it outperforms\nthe standard formulation by a margin of 1 to 2% when tested with VoxCeleb\nspeaker verification protocol. All of our findings are asserted by ensemble\naveraging the results from multiple runs with random start.", "published": "2019-06-20 11:09:39", "link": "http://arxiv.org/abs/1906.08556v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Low-dimensional Embodied Semantics for Music and Language", "abstract": "Embodied cognition states that semantics is encoded in the brain as firing\npatterns of neural circuits, which are learned according to the statistical\nstructure of human multimodal experience. However, each human brain is\nidiosyncratically biased, according to its subjective experience history,\nmaking this biological semantic machinery noisy with respect to the overall\nsemantics inherent to media artifacts, such as music and language excerpts. We\npropose to represent shared semantics using low-dimensional vector embeddings\nby jointly modeling several brains from human subjects. We show these\nunsupervised efficient representations outperform the original high-dimensional\nfMRI voxel spaces in proxy music genre and language topic classification tasks.\nWe further show that joint modeling of several subjects increases the semantic\nrichness of the learned latent vector spaces.", "published": "2019-06-20 11:09:01", "link": "http://arxiv.org/abs/1906.11759v1", "categories": ["q-bio.NC", "cs.IR", "cs.LG", "cs.SD", "eess.AS", "stat.ML", "I.2.6; H.5.5; H.5.1; I.2.7"], "primary_category": "q-bio.NC"}
