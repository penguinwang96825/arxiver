{"title": "Market information of the fractional stochastic regularity model", "abstract": "The Fractional Stochastic Regularity Model (FSRM) is an extension of\nBlack-Scholes model describing the multifractal nature of prices. It is based\non a multifractional process with a random Hurst exponent $H_t$, driven by a\nfractional Ornstein-Uhlenbeck (fOU) process. When the regularity parameter\n$H_t$ is equal to $1/2$, the efficient market hypothesis holds, but when\n$H_t\\neq 1/2$ past price returns contain some information on a future trend or\nmean-reversion of the log-price process. In this paper, we investigate some\nproperties of the fOU process and, thanks to information theory and Shannon's\nentropy, we determine theoretically the serial information of the regularity\nprocess $H_t$ of the FSRM, giving some insight into one's ability to forecast\nfuture price increments and to build statistical arbitrages with this model.", "published": "2024-09-11 10:14:02", "link": "http://arxiv.org/abs/2409.07159v1", "categories": ["q-fin.MF", "q-fin.ST"], "primary_category": "q-fin.MF"}
{"title": "Automated Speaking Assessment of Conversation Tests with Novel\n  Graph-based Modeling on Spoken Response Coherence", "abstract": "Automated speaking assessment in conversation tests (ASAC) aims to evaluate\nthe overall speaking proficiency of an L2 (second-language) speaker in a\nsetting where an interlocutor interacts with one or more candidates. Although\nprior ASAC approaches have shown promising performance on their respective\ndatasets, there is still a dearth of research specifically focused on\nincorporating the coherence of the logical flow within a conversation into the\ngrading model. To address this critical challenge, we propose a hierarchical\ngraph model that aptly incorporates both broad inter-response interactions\n(e.g., discourse relations) and nuanced semantic information (e.g., semantic\nwords and speaker intents), which is subsequently fused with contextual\ninformation for the final prediction. Extensive experimental results on the\nNICT-JLE benchmark dataset suggest that our proposed modeling approach can\nyield considerable improvements in prediction accuracy with respect to various\nassessment metrics, as compared to some strong baselines. This also sheds light\non the importance of investigating coherence-related facets of spoken responses\nin ASAC.", "published": "2024-09-11 07:24:07", "link": "http://arxiv.org/abs/2409.07064v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Space Interpretation for Stylistic Analysis and Explainable\n  Authorship Attribution", "abstract": "Recent state-of-the-art authorship attribution methods learn authorship\nrepresentations of texts in a latent, non-interpretable space, hindering their\nusability in real-world applications. Our work proposes a novel approach to\ninterpreting these learned embeddings by identifying representative points in\nthe latent space and utilizing LLMs to generate informative natural language\ndescriptions of the writing style of each point. We evaluate the alignment of\nour interpretable space with the latent one and find that it achieves the best\nprediction agreement compared to other baselines. Additionally, we conduct a\nhuman evaluation to assess the quality of these style descriptions, validating\ntheir utility as explanations for the latent space. Finally, we investigate\nwhether human performance on the challenging AA task improves when aided by our\nsystem's explanations, finding an average improvement of around +20% in\naccuracy.", "published": "2024-09-11 07:48:06", "link": "http://arxiv.org/abs/2409.07072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling", "abstract": "Linear attention Transformers and their gated variants, celebrated for\nenabling parallel training and efficient recurrent inference, still fall short\nin recall-intensive tasks compared to traditional Transformers and demand\nsignificant resources for training from scratch. This paper introduces Gated\nSlot Attention (GSA), which enhances Attention with Bounded-memory-Control\n(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention\n(GLA). Essentially, GSA comprises a two-layer GLA linked via\n$\\operatorname{softmax}$, utilizing context-aware memory reading and adaptive\nforgetting to improve memory capacity while maintaining compact recurrent state\nsize. This design greatly enhances both training and inference efficiency\nthrough GLA's hardware-efficient training algorithm and reduced state size.\nAdditionally, retaining the $\\operatorname{softmax}$ operation is particularly\nbeneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings,\nreducing the need for extensive training from scratch. Extensive experiments\nconfirm GSA's superior performance in scenarios requiring in-context recall and\nin T2R settings.", "published": "2024-09-11 09:49:50", "link": "http://arxiv.org/abs/2409.07146v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Efficient Recursive Numeral Systems via Reinforcement Learning", "abstract": "It has previously been shown that by using reinforcement learning (RL),\nagents can derive simple approximate and exact-restricted numeral systems that\nare similar to human ones (Carlsson, 2021). However, it is a major challenge to\nshow how more complex recursive numeral systems, similar to for example\nEnglish, could arise via a simple learning mechanism such as RL. Here, we\nintroduce an approach towards deriving a mechanistic explanation of the\nemergence of efficient recursive number systems. We consider pairs of agents\nlearning how to communicate about numerical quantities through a meta-grammar\nthat can be gradually modified throughout the interactions. %We find that the\nseminal meta-grammar of Hurford (Hurford, 1975) is not suitable for this\napplication as its optimization results in systems that deviate from standard\nconventions observed within human numeral systems. We propose a simple\nmodification which addresses this issue. Utilising a slightly modified version\nof the meta-grammar of Hurford, we demonstrate that our RL agents, shaped by\nthe pressures for efficient communication, can effectively modify their lexicon\ntowards Pareto-optimal configurations which are comparable to those observed\nwithin human numeral systems in terms of their efficiency.", "published": "2024-09-11 10:33:30", "link": "http://arxiv.org/abs/2409.07170v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud\n  Outcomes for Effective Text Evaluation", "abstract": "This study introduces \\textbf{InteractEval}, a framework that integrates\nhuman expertise and Large Language Models (LLMs) using the Think-Aloud (TA)\nmethod to generate attributes for checklist-based text evaluation. By combining\nhuman flexibility and reasoning with LLM consistency, InteractEval outperforms\ntraditional non-LLM-based and LLM-based baselines across four distinct\ndimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The\nexperiment also investigates the effectiveness of the TA method, showing that\nit promotes divergent thinking in both humans and LLMs, leading to the\ngeneration of a wider range of relevant attributes and enhance text evaluation\nperformance. Comparative analysis reveals that humans excel at identifying\nattributes related to internal quality (Coherence and Fluency), but LLMs\nperform better at those attributes related to external alignment (Consistency\nand Relevance). Consequently, leveraging both humans and LLMs together produces\nthe best evaluation outcomes. In other words, this study emphasizes the\nnecessity of effectively combining humans and LLMs in an automated\nchecklist-based text evaluation framework. The code is available at\n\\textbf{\\url{https://github.com/BBeeChu/InteractEval.git}}.", "published": "2024-09-11 15:40:07", "link": "http://arxiv.org/abs/2409.07355v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recent Trends of Multimodal Affective Computing: A Survey from NLP\n  Perspective", "abstract": "Multimodal affective computing (MAC) has garnered increasing attention due to\nits broad applications in analyzing human behaviors and intentions, especially\nin text-dominated multimodal affective computing field. This survey presents\nthe recent trends of multimodal affective computing from NLP perspective\nthrough four hot tasks: multimodal sentiment analysis, multimodal emotion\nrecognition in conversation, multimodal aspect-based sentiment analysis and\nmultimodal multi-label emotion recognition. The goal of this survey is to\nexplore the current landscape of multimodal affective research, identify\ndevelopment trends, and highlight the similarities and differences across\nvarious tasks, offering a comprehensive report on the recent progress in\nmultimodal affective computing from an NLP perspective. This survey covers the\nformalization of tasks, provides an overview of relevant works, describes\nbenchmark datasets, and details the evaluation metrics for each task.\nAdditionally, it briefly discusses research in multimodal affective computing\ninvolving facial expressions, acoustic signals, physiological signals, and\nemotion causes. Additionally, we discuss the technical approaches, challenges,\nand future directions in multimodal affective computing. To support further\nresearch, we released a repository that compiles related works in multimodal\naffective computing, providing detailed resources and references for the\ncommunity.", "published": "2024-09-11 16:24:06", "link": "http://arxiv.org/abs/2409.07388v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and\n  Parametric Knowledge", "abstract": "Knowledge conflict arises from discrepancies between information in the\ncontext of a large language model (LLM) and the knowledge stored in its\nparameters. This can hurt performance when using standard decoding techniques,\nwhich tend to ignore the context. Existing test-time contrastive methods seek\nto address this by comparing the LLM's output distribution with and without the\ncontext and adjust the model according to the contrast between them. However,\nwe find that these methods frequently misjudge the degree of conflict and\nstruggle to handle instances that vary in their amount of conflict, with static\nmethods over-adjusting when conflict is absent. We propose a fine-grained,\ninstance-level approach called AdaCAD, which dynamically infers the weight of\nadjustment based on the degree of conflict, as measured by the Jensen-Shannon\ndivergence between distributions representing contextual and parametric\nknowledge. Our experiments across four models on six diverse question-answering\n(QA) datasets and three summarization tasks demonstrate that our training-free\nadaptive method consistently outperforms other decoding methods on QA, with\naverage accuracy gains of 14.21% (absolute) over a static contrastive baseline,\nand improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our\nanalysis shows that while decoding with contrastive baselines hurts performance\nwhen conflict is absent, AdaCAD mitigates these losses, making it more\napplicable to real-world datasets in which some examples have conflict and\nothers do not.", "published": "2024-09-11 16:35:18", "link": "http://arxiv.org/abs/2409.07394v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing adversarial robustness in Natural Language Inference using\n  explanations", "abstract": "The surge of state-of-the-art Transformer-based models has undoubtedly pushed\nthe limits of NLP model performance, excelling in a variety of tasks. We cast\nthe spotlight on the underexplored task of Natural Language Inference (NLI),\nsince models trained on popular well-suited datasets are susceptible to\nadversarial attacks, allowing subtle input interventions to mislead the model.\nIn this work, we validate the usage of natural language explanation as a\nmodel-agnostic defence strategy through extensive experimentation: only by\nfine-tuning a classifier on the explanation rather than premise-hypothesis\ninputs, robustness under various adversarial attacks is achieved in comparison\nto explanation-free baselines. Moreover, since there is no standard strategy of\ntesting the semantic validity of the generated explanations, we research the\ncorrelation of widely used language generation metrics with human perception,\nin order for them to serve as a proxy towards robust NLI models. Our approach\nis resource-efficient and reproducible without significant computational\nlimitations.", "published": "2024-09-11 17:09:49", "link": "http://arxiv.org/abs/2409.07423v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agent Workflow Memory", "abstract": "Despite the potential of language model-based agents to solve real-world\ntasks such as web navigation, current methods still struggle with long-horizon\ntasks with complex action trajectories. In contrast, humans can flexibly solve\ncomplex tasks by learning reusable task workflows from past experiences and\nusing them to guide future actions. To build agents that can similarly benefit\nfrom this process, we introduce Agent Workflow Memory (AWM), a method for\ninducing commonly reused routines, i.e., workflows, and selectively providing\nworkflows to the agent to guide subsequent generations. AWM flexibly applies to\nboth offline and online scenarios, where agents induce workflows from training\nexamples beforehand or from test queries on the fly. We experiment on two major\nweb navigation benchmarks -- Mind2Web and WebArena -- that collectively cover\n1000+ tasks from 200+ domains across travel, shopping, and social media, among\nothers. AWM substantially improves the baseline results by 24.6% and 51.1%\nrelative success rate on Mind2Web and WebArena while reducing the number of\nsteps taken to solve WebArena tasks successfully. Furthermore, online AWM\nrobustly generalizes in cross-task, website, and domain evaluations, surpassing\nbaselines from 8.9 to 14.0 absolute points as train-test task distribution gaps\nwiden.", "published": "2024-09-11 17:21:00", "link": "http://arxiv.org/abs/2409.07429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MOSAIC: Multiple Observers Spotting AI Content, a Robust Approach to\n  Machine-Generated Text Detection", "abstract": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities has vastly increased the\nthreats posed by generative AI technologies by reducing the cost of producing\nharmful, toxic, faked or forged content. In response, various proposals have\nbeen made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a classification problem.\nMost approaches evaluate an input document by a well-chosen detector LLM,\nassuming that low-perplexity scores reliably signal machine-made content. As\nusing one single detector can induce brittleness of performance, we instead\nconsider several and derive a new, theoretically grounded approach to combine\ntheir respective strengths. Our experiments, using a variety of generator LLMs,\nsuggest that our method effectively leads to robust detection performances. An\nearly version of the code is available at\nhttps://github.com/BaggerOfWords/MOSAIC.", "published": "2024-09-11 20:55:12", "link": "http://arxiv.org/abs/2409.07615v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks", "abstract": "We introduce SimulBench, a benchmark designed to evaluate large language\nmodels (LLMs) across a diverse collection of creative simulation scenarios,\nsuch as acting as a Linux terminal or playing text games with users. While\nthese simulation tasks serve as effective measures of an LLM's general\nintelligence, they are seldom incorporated into existing benchmarks. A major\nchallenge is to develop an evaluation framework for testing different LLMs\nfairly while preserving the multi-round interactive nature of simulation tasks\nbetween users and AI. To tackle this issue, we suggest using a fixed LLM as a\nuser agent to engage with an LLM to collect dialogues first under different\ntasks. Then, challenging dialogue scripts are extracted for evaluating\ndifferent target LLMs. To facilitate automatic assessment on \\DataName{}, GPT-4\nis employed as the evaluator, tasked with reviewing the quality of the final\nresponse generated by the target LLMs given multi-turn dialogue scripts. Our\ncomprehensive experiments indicate that these simulation tasks continue to pose\na significant challenge with their unique natures and show the gap between\nproprietary models and the most advanced open LLMs. For example, GPT-4-turbo\noutperforms LLaMA-3-70b-Chat on 18.55\\% more cases.", "published": "2024-09-11 21:53:20", "link": "http://arxiv.org/abs/2409.07641v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representation Tuning", "abstract": "Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, we extend the idea of\ninference-time steering with vectors that represent a behavioral direction of\ninterest to tuning those vectors directly into the model, obviating the need\nfor online control. First, we identify activation vectors related to honesty in\nan open-source LLM (Llama-2-13b-chat). Next, we demonstrate that model output\ncan be made more or less honest by adding positive or negative multiples of\nthese vectors to residual stream activations during generation. Then, we show\nthat a similar effect can be achieved by fine-tuning the vectors directly into\nthe model, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, we compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning. Tuned\nmodels are available at\nhttps://huggingface.co/collections/cackerman/representation-tuning-66da1e5ab41cd1b824687d9f.", "published": "2024-09-11 00:56:02", "link": "http://arxiv.org/abs/2409.06927v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "You Have Thirteen Hours in Which to Solve the Labyrinth: Enhancing AI\n  Game Masters with Function Calling", "abstract": "Developing a consistent and reliable AI game master for text-based games is a\nchallenging task due to the limitations of large language models (LLMs) and the\ncomplexity of the game master's role. This paper presents a novel approach to\nenhance AI game masters by leveraging function calling in the context of the\ntable-top role-playing game \"Jim Henson's Labyrinth: The Adventure Game.\" Our\nmethodology involves integrating game-specific controls through functions,\nwhich we show improves the narrative quality and state update consistency of\nthe AI game master. The experimental results, based on human evaluations and\nunit tests, demonstrate the effectiveness of our approach in enhancing gameplay\nexperience and maintaining coherence with the game state. This work contributes\nto the advancement of game AI and interactive storytelling, offering insights\ninto the design of more engaging and consistent AI-driven game masters.", "published": "2024-09-11 02:03:51", "link": "http://arxiv.org/abs/2409.06949v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond IID: Optimizing Instruction Learning from the Perspective of\n  Instruction Interaction and Dependency", "abstract": "With the availability of various instruction datasets, a pivotal challenge is\nhow to effectively select and integrate these instructions to fine-tune large\nlanguage models (LLMs). Previous research mainly focuses on selecting\nindividual high-quality instructions. However, these works overlooked the joint\ninteractions and dependencies between different categories of instructions,\nleading to suboptimal selection strategies. Moreover, the nature of these\ninteraction patterns remains largely unexplored, let alone optimize the\ninstruction set with regard to them. To fill these gaps, in this paper, we: (1)\nsystemically investigate interaction and dependency patterns between different\ncategories of instructions, (2) manage to optimize the instruction set\nconcerning the interaction patterns using a linear programming-based method,\nand optimize the learning schema of SFT using an instruction dependency\ntaxonomy guided curriculum learning. Experimental results across different LLMs\ndemonstrate improved performance over strong baselines on widely adopted\nbenchmarks.", "published": "2024-09-11 06:27:50", "link": "http://arxiv.org/abs/2409.07045v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Native vs Non-Native Language Prompting: A Comparative Analysis", "abstract": "Large language models (LLMs) have shown remarkable abilities in different\nfields, including standard Natural Language Processing (NLP) tasks. To elicit\nknowledge from LLMs, prompts play a key role, consisting of natural language\ninstructions. Most open and closed source LLMs are trained on available labeled\nand unlabeled resources--digital content such as text, images, audio, and\nvideos. Hence, these models have better knowledge for high-resourced languages\nbut struggle with low-resourced languages. Since prompts play a crucial role in\nunderstanding their capabilities, the language used for prompts remains an\nimportant research question. Although there has been significant research in\nthis area, it is still limited, and less has been explored for medium to\nlow-resourced languages. In this study, we investigate different prompting\nstrategies (native vs. non-native) on 11 different NLP tasks associated with 12\ndifferent Arabic datasets (9.7K data points). In total, we conducted 197\nexperiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our\nfindings suggest that, on average, the non-native prompt performs the best,\nfollowed by mixed and native prompts.", "published": "2024-09-11 06:59:37", "link": "http://arxiv.org/abs/2409.07054v2", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Understanding Knowledge Drift in LLMs through Misinformation", "abstract": "Large Language Models (LLMs) have revolutionized numerous applications,\nmaking them an integral part of our digital ecosystem. However, their\nreliability becomes critical, especially when these models are exposed to\nmisinformation. We primarily analyze the susceptibility of state-of-the-art\nLLMs to factual inaccuracies when they encounter false information in a QnA\nscenario, an issue that can lead to a phenomenon we refer to as *knowledge\ndrift*, which significantly undermines the trustworthiness of these models. We\nevaluate the factuality and the uncertainty of the models' responses relying on\nEntropy, Perplexity, and Token Probability metrics. Our experiments reveal that\nan LLM's uncertainty can increase up to 56.6% when the question is answered\nincorrectly due to the exposure to false information. At the same time,\nrepeated exposure to the same false information can decrease the models\nuncertainty again (-52.8% w.r.t. the answers on the untainted prompts),\npotentially manipulating the underlying model's beliefs and introducing a drift\nfrom its original knowledge. These findings provide insights into LLMs'\nrobustness and vulnerability to adversarial inputs, paving the way for\ndeveloping more reliable LLM applications across various domains. The code is\navailable at https://github.com/afastowski/knowledge_drift.", "published": "2024-09-11 08:11:16", "link": "http://arxiv.org/abs/2409.07085v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset\n  Synthesis using Large Language Model", "abstract": "Knowledge Graph-to-Text (G2T) generation involves verbalizing structured\nknowledge graphs into natural language text. Recent advancements in Pretrained\nLanguage Models (PLMs) have improved G2T performance, but their effectiveness\ndepends on datasets with precise graph-text alignment. However, the scarcity of\nhigh-quality, general-domain G2T generation datasets restricts progress in the\ngeneral-domain G2T generation research. To address this issue, we introduce\nWikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T\ndataset generated using a novel method that leverages Large Language Model\n(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain\ngraph-text pairs, offers high graph-text consistency without relying on\nexternal ontologies. Experimental results demonstrate that PLM fine-tuned on\nWikiOFGraph outperforms those trained on other datasets across various\nevaluation metrics. Our method proves to be a scalable and effective solution\nfor generating high-quality G2T data, significantly advancing the field of G2T\ngeneration.", "published": "2024-09-11 08:16:20", "link": "http://arxiv.org/abs/2409.07088v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem", "abstract": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German.", "published": "2024-09-11 09:21:20", "link": "http://arxiv.org/abs/2409.07123v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM-based feature generation from text for interpretable machine\n  learning", "abstract": "Existing text representations such as embeddings and bag-of-words are not\nsuitable for rule learning due to their high dimensionality and absent or\nquestionable feature-level interpretability. This article explores whether\nlarge language models (LLMs) could address this by extracting a small number of\ninterpretable features from text. We demonstrate this process on two datasets\n(CORD-19 and M17+) containing several thousand scientific articles from\nmultiple disciplines and a target being a proxy for research impact. An\nevaluation based on testing for the statistically significant correlation with\nresearch impact has shown that LLama 2-generated features are semantically\nmeaningful. We consequently used these generated features in text\nclassification to predict the binary target variable representing the citation\nrate for the CORD-19 dataset and the ordinal 5-class target representing an\nexpert-awarded grade in the M17+ dataset. Machine-learning models trained on\nthe LLM-generated features provided similar predictive performance to the\nstate-of-the-art embedding model SciBERT for scientific text. The LLM used only\n62 features compared to 768 features in SciBERT embeddings, and these features\nwere directly interpretable, corresponding to notions such as article\nmethodological rigor, novelty, or grammatical correctness. As the final step,\nwe extract a small number of well-interpretable action rules. Consistently\ncompetitive results obtained with the same LLM feature set across both\nthematically diverse datasets show that this approach generalizes across\ndomains.", "published": "2024-09-11 09:29:28", "link": "http://arxiv.org/abs/2409.07132v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How Effectively Do LLMs Extract Feature-Sentiment Pairs from App\n  Reviews?", "abstract": "Automatic analysis of user reviews to understand user sentiments toward app\nfunctionality (i.e. app features) helps align development efforts with user\nexpectations and needs. Recent advances in Large Language Models (LLMs) such as\nChatGPT have shown impressive performance on several new tasks without updating\nthe model's parameters i.e. using zero or a few labeled examples, but the\ncapabilities of LLMs are yet unexplored for feature-specific sentiment\nanalysis. The goal of our study is to explore the capabilities of LLMs to\nperform feature-specific sentiment analysis of user reviews. This study\ncompares the performance of state-of-the-art LLMs, including GPT-4, ChatGPT,\nand different variants of Llama-2 chat, against previous approaches for\nextracting app features and associated sentiments in zero-shot, 1-shot, and\n5-shot scenarios. The results indicate that GPT-4 outperforms the rule-based\nSAFE by 17% in f1-score for extracting app features in the zero-shot scenario,\nwith 5-shot further improving it by 6%. However, the fine-tuned RE-BERT exceeds\nGPT-4 by 6% in f1-score. For predicting positive and neutral sentiments, GPT-4\nachieves f1-scores of 76% and 45% in the zero-shot setting, which improve by 7%\nand 23% in the 5-shot setting, respectively. Our study conducts a thorough\nevaluation of both proprietary and open-source LLMs to provide an objective\nassessment of their performance in extracting feature-sentiment pairs.", "published": "2024-09-11 10:21:13", "link": "http://arxiv.org/abs/2409.07162v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Propaganda to Hate: A Multimodal Analysis of Arabic Memes with\n  Multi-Agent LLMs", "abstract": "In the past decade, social media platforms have been used for information\ndissemination and consumption. While a major portion of the content is posted\nto promote citizen journalism and public awareness, some content is posted to\nmislead users. Among different content types such as text, images, and videos,\nmemes (text overlaid on images) are particularly prevalent and can serve as\npowerful vehicles for propaganda, hate, and humor. In the current literature,\nthere have been efforts to individually detect such content in memes. However,\nthe study of their intersection is very limited. In this study, we explore the\nintersection between propaganda and hate in memes using a multi-agent LLM-based\napproach. We extend the propagandistic meme dataset with coarse and\nfine-grained hate labels. Our finding suggests that there is an association\nbetween propaganda and hate in memes. We provide detailed experimental results\nthat can serve as a baseline for future studies. We will make the experimental\nresources publicly available to the community\n(https://github.com/firojalam/propaganda-and-hateful-memes).", "published": "2024-09-11 13:04:34", "link": "http://arxiv.org/abs/2409.07246v2", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical\n  Applications", "abstract": "The rapid development of Large Language Models (LLMs) for healthcare\napplications has spurred calls for holistic evaluation beyond frequently-cited\nbenchmarks like USMLE, to better reflect real-world performance. While\nreal-world assessments are valuable indicators of utility, they often lag\nbehind the pace of LLM evolution, likely rendering findings obsolete upon\ndeployment. This temporal disconnect necessitates a comprehensive upfront\nevaluation that can guide model selection for specific clinical applications.\nWe introduce MEDIC, a framework assessing LLMs across five critical dimensions\nof clinical competence: medical reasoning, ethics and bias, data and language\nunderstanding, in-context learning, and clinical safety. MEDIC features a novel\ncross-examination framework quantifying LLM performance across areas like\ncoverage and hallucination detection, without requiring reference outputs. We\napply MEDIC to evaluate LLMs on medical question-answering, safety,\nsummarization, note generation, and other tasks. Our results show performance\ndisparities across model sizes, baseline vs medically finetuned models, and\nhave implications on model selection for applications requiring specific model\nstrengths, such as low hallucination or lower cost of inference. MEDIC's\nmultifaceted evaluation reveals these performance trade-offs, bridging the gap\nbetween theoretical capabilities and practical implementation in healthcare\nsettings, ensuring that the most promising models are identified and adapted\nfor diverse healthcare applications.", "published": "2024-09-11 14:44:51", "link": "http://arxiv.org/abs/2409.07314v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explanation, Debate, Align: A Weak-to-Strong Framework for Language\n  Model Generalization", "abstract": "The rapid advancement of artificial intelligence systems has brought the\nchallenge of AI alignment to the forefront of research, particularly in complex\ndecision-making and task execution. As these systems surpass human-level\nperformance in sophisticated problems, ensuring their alignment with human\nvalues, intentions, and ethical guidelines becomes crucial. Building on\nprevious work in explanation generation for human-agent alignment, we address\nthe more complex dynamics of multi-agent systems and human-AI teams. This paper\nintroduces a novel approach to model alignment through weak-to-strong\ngeneralization in the context of language models. We present a framework where\na strong model facilitates the improvement of a weaker model, bridging the gap\nbetween explanation generation and model alignment. Our method, formalized as a\nfacilitation function, allows for the transfer of capabilities from advanced\nmodels to less capable ones without direct access to extensive training data.\nOur results suggest that this facilitation-based approach not only enhances\nmodel performance but also provides insights into the nature of model alignment\nand the potential for scalable oversight of AI systems.", "published": "2024-09-11 15:16:25", "link": "http://arxiv.org/abs/2409.07335v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Evaluation of GPT-4V for Transcribing the Urban Renewal Hand-Written\n  Collection", "abstract": "Between 1960 and 1980, urban renewal transformed many cities, creating vast\nhandwritten records. These documents posed a significant challenge for\nresearchers due to their volume and handwritten nature. The launch of GPT-4V in\nNovember 2023 offered a breakthrough, enabling large-scale, efficient\ntranscription and analysis of these historical urban renewal documents.", "published": "2024-09-11 20:55:59", "link": "http://arxiv.org/abs/2409.09090v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "When Less Is Not More: Large Language Models Normalize Less-Frequent\n  Terms with Lower Accuracy", "abstract": "Term normalization is the process of mapping a term from free text to a\nstandardized concept and its machine-readable code in an ontology. Accurate\nnormalization of terms that capture phenotypic differences between patients and\ndiseases is critical to the success of precision medicine initiatives. A large\nlanguage model (LLM), such as GPT-4o, can normalize terms to the Human\nPhenotype Ontology (HPO), but it may retrieve incorrect HPO IDs. Reported\naccuracy rates for LLMs on these tasks may be inflated due to imbalanced test\ndatasets skewed towards high-frequency terms. In our study, using a\ncomprehensive dataset of 268,776 phenotype annotations for 12,655 diseases from\nthe HPO, GPT-4o achieved an accuracy of 13.1% in normalizing 11,225 unique\nterms. However, the accuracy was unevenly distributed, with higher-frequency\nand shorter terms normalized more accurately than lower-frequency and longer\nterms. Feature importance analysis, using SHAP and permutation methods,\nidentified low-term frequency as the most significant predictor of\nnormalization errors. These findings suggest that training and evaluation\ndatasets for LLM-based term normalization should balance low- and\nhigh-frequency terms to improve model performance, particularly for infrequent\nterms critical to precision medicine.", "published": "2024-09-11 21:34:46", "link": "http://arxiv.org/abs/2409.13746v1", "categories": ["cs.CL", "cs.AI", "I.2"], "primary_category": "cs.CL"}
{"title": "Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction", "abstract": "Legal judgment prediction (LJP), which enables litigants and their lawyers to\nforecast judgment outcomes and refine litigation strategies, has emerged as a\ncrucial legal NLP task. Existing studies typically utilize legal facts, i.e.,\nfacts that have been established by evidence and determined by the judge, to\npredict the judgment. However, legal facts are often difficult to obtain in the\nearly stages of litigation, significantly limiting the practical applicability\nof fact-based LJP. To address this limitation, we propose a novel legal NLP\ntask: \\textit{legal fact prediction} (LFP), which takes the evidence submitted\nby litigants for trial as input to predict legal facts, thereby empowering\nfact-based LJP technologies to perform prediction in the absence of\nground-truth legal facts. We also propose the first benchmark dataset,\nLFPBench, for evaluating the LFP task. Our extensive experiments on LFPBench\ndemonstrate the effectiveness of LFP-empowered LJP and highlight promising\nresearch directions for LFP. Our code and data are available at\nhttps://github.com/HPRCEST/LFPBench.", "published": "2024-09-11 07:01:08", "link": "http://arxiv.org/abs/2409.07055v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Reranking Laws for Language Generation: A Communication-Theoretic\n  Perspective", "abstract": "To ensure large language models (LLMs) are used safely, one must reduce their\npropensity to hallucinate or to generate unacceptable answers. A simple and\noften used strategy is to first let the LLM generate multiple hypotheses and\nthen employ a reranker to choose the best one. In this paper, we draw a\nparallel between this strategy and the use of redundancy to decrease the error\nrate in noisy communication channels. We conceptualize the generator as a\nsender transmitting multiple descriptions of a message through parallel noisy\nchannels. The receiver decodes the message by ranking the (potentially\ncorrupted) descriptions and selecting the one found to be most reliable. We\nprovide conditions under which this protocol is asymptotically error-free\n(i.e., yields an acceptable answer almost surely) even in scenarios where the\nreranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the\nchannel distributions are statistically dependent. We use our framework to\nobtain reranking laws which we validate empirically on two real-world tasks\nusing LLMs: text-to-code generation with DeepSeek-Coder 7B and machine\ntranslation of medical data with TowerInstruct 13B.", "published": "2024-09-11 09:27:50", "link": "http://arxiv.org/abs/2409.07131v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Leveraging Unstructured Text Data for Federated Instruction Tuning of\n  Large Language Models", "abstract": "Federated instruction tuning enables multiple clients to collaboratively\nfine-tune a shared large language model (LLM) that can follow humans'\ninstructions without directly sharing raw data. However, existing literature\nimpractically requires that all the clients readily hold instruction-tuning\ndata (i.e., structured instruction-response pairs), which necessitates massive\nhuman annotations since clients' data is usually unstructured text instead.\nAddressing this, we propose a novel and flexible framework FedIT-U2S, which can\nautomatically transform unstructured corpus into structured data for federated\ninstruction tuning. FedIT-U2S consists two key steps: (1) few-shot\ninstruction-tuning data generation, where each unstructured data piece together\nwith several examples is combined to prompt an LLM in generating an\ninstruction-response pair. To further enhance the flexibility, a\nretrieval-based example selection technique is proposed, where the examples are\nautomatically selected based on the relatedness between the client's data piece\nand example pool, bypassing the need of determining examples in advance. (2) A\ntypical federated instruction tuning process based on the generated data.\nOverall, FedIT-U2S can be applied to diverse scenarios as long as the client\nholds valuable text corpus, broadening the application scope of federated\ninstruction tuning. We conduct a series of experiments on three domains\n(medicine, knowledge, and math), showing that our proposed FedIT-U2S can\nconsistently and significantly brings improvement over the base LLM.", "published": "2024-09-11 09:31:44", "link": "http://arxiv.org/abs/2409.07136v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Cross-Dialect Text-To-Speech in Pitch-Accent Language Incorporating\n  Multi-Dialect Phoneme-Level BERT", "abstract": "We explore cross-dialect text-to-speech (CD-TTS), a task to synthesize\nlearned speakers' voices in non-native dialects, especially in pitch-accent\nlanguages. CD-TTS is important for developing voice agents that naturally\ncommunicate with people across regions. We present a novel TTS model comprising\nthree sub-modules to perform competitively at this task. We first train a\nbackbone TTS model to synthesize dialect speech from a text conditioned on\nphoneme-level accent latent variables (ALVs) extracted from speech by a\nreference encoder. Then, we train an ALV predictor to predict ALVs tailored to\na target dialect from input text leveraging our novel multi-dialect\nphoneme-level BERT. We conduct multi-dialect TTS experiments and evaluate the\neffectiveness of our model by comparing it with a baseline derived from\nconventional dialect TTS methods. The results show that our model improves the\ndialectal naturalness of synthetic speech in CD-TTS.", "published": "2024-09-11 13:40:27", "link": "http://arxiv.org/abs/2409.07265v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Using Generative Agents to Create Tip Sheets for Investigative Data\n  Reporting", "abstract": "This paper introduces a system using generative AI agents to create tip\nsheets for investigative data reporting. Our system employs three specialized\nagents--an analyst, a reporter, and an editor--to collaboratively generate and\nrefine tips from datasets. We validate this approach using real-world\ninvestigative stories, demonstrating that our agent-based system generally\ngenerates more newsworthy and accurate insights compared to a baseline model\nwithout agents, although some variability was noted between different stories.\nOur findings highlight the potential of generative AI to provide leads for\ninvestigative data reporting.", "published": "2024-09-11 14:14:15", "link": "http://arxiv.org/abs/2409.07286v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring\n  System via Language Model Coordination", "abstract": "The vast pre-existing slides serve as rich and important materials to carry\nlecture knowledge. However, effectively leveraging lecture slides to serve\nstudents is difficult due to the multi-modal nature of slide content and the\nheterogeneous teaching actions. We study the problem of discovering effective\ndesigns that convert a slide into an interactive lecture. We develop\nSlide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring\nsystem that can (1) effectively convert an input lecture slide into a\nstructured teaching agenda consisting of a set of heterogeneous teaching\nactions; (2) create and manage an interactive lecture that generates responsive\ninteractions catering to student learning demands while regulating the\ninteractions to follow teaching actions. Slide2Lecture contains a complete\npipeline for learners to obtain an interactive classroom experience to learn\nthe slide. For teachers and developers, Slide2Lecture enables customization to\ncater to personalized demands. The evaluation rated by annotators and students\nshows that Slide2Lecture is effective in outperforming the remaining\nimplementation. Slide2Lecture's online deployment has made more than 200K\ninteraction with students in the 3K lecture sessions. We open source\nSlide2Lecture's implementation in\nhttps://anonymous.4open.science/r/slide2lecture-4210/.", "published": "2024-09-11 16:03:09", "link": "http://arxiv.org/abs/2409.07372v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "What to align in multimodal contrastive learning?", "abstract": "Humans perceive the world through multisensory integration, blending the\ninformation of different modalities to adapt their behavior. Contrastive\nlearning offers an appealing solution for multimodal self-supervised learning.\nIndeed, by considering each modality as a different view of the same entity, it\nlearns to align features of different modalities in a shared representation\nspace. However, this approach is intrinsically limited as it only learns shared\nor redundant information between modalities, while multimodal interactions can\narise in other ways. In this work, we introduce CoMM, a Contrastive MultiModal\nlearning strategy that enables the communication between modalities in a single\nmultimodal space. Instead of imposing cross- or intra- modality constraints, we\npropose to align multimodal representations by maximizing the mutual\ninformation between augmented versions of these multimodal features. Our\ntheoretical analysis shows that shared, synergistic and unique terms of\ninformation naturally emerge from this formulation, allowing us to estimate\nmultimodal interactions beyond redundancy. We test CoMM both in a controlled\nand in a series of real-world settings: in the former, we demonstrate that CoMM\neffectively captures redundant, unique and synergistic information between\nmodalities. In the latter, CoMM learns complex multimodal interactions and\nachieves state-of-the-art results on the seven multimodal benchmarks. Code is\navailable at https://github.com/Duplums/CoMM", "published": "2024-09-11 16:42:22", "link": "http://arxiv.org/abs/2409.07402v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Towards Fairer Health Recommendations: finding informative unbiased\n  samples via Word Sense Disambiguation", "abstract": "There have been growing concerns around high-stake applications that rely on\nmodels trained with biased data, which consequently produce biased predictions,\noften harming the most vulnerable. In particular, biased medical data could\ncause health-related applications and recommender systems to create outputs\nthat jeopardize patient care and widen disparities in health outcomes. A recent\nframework titled Fairness via AI posits that, instead of attempting to correct\nmodel biases, researchers must focus on their root causes by using AI to debias\ndata. Inspired by this framework, we tackle bias detection in medical curricula\nusing NLP models, including LLMs, and evaluate them on a gold standard dataset\ncontaining 4,105 excerpts annotated by medical experts for bias from a large\ncorpus. We build on previous work by coauthors which augments the set of\nnegative samples with non-annotated text containing social identifier terms.\nHowever, some of these terms, especially those related to race and ethnicity,\ncan carry different meanings (e.g., \"white matter of spinal cord\"). To address\nthis issue, we propose the use of Word Sense Disambiguation models to refine\ndataset quality by removing irrelevant sentences. We then evaluate fine-tuned\nvariations of BERT models as well as GPT models with zero- and few-shot\nprompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for\nbias detection, while fine-tuned BERT models generally perform well across all\nevaluated metrics.", "published": "2024-09-11 17:10:20", "link": "http://arxiv.org/abs/2409.07424v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.7; J.3; K.4"], "primary_category": "cs.CL"}
{"title": "Synthetic continued pretraining", "abstract": "Pretraining on large-scale, unstructured internet text enables language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient--to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining with EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If, instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.", "published": "2024-09-11 17:21:59", "link": "http://arxiv.org/abs/2409.07431v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Salmon: A Suite for Acoustic Language Model Evaluation", "abstract": "Speech language models have recently demonstrated great potential as\nuniversal speech processing systems. Such models have the ability to model the\nrich acoustic information existing in audio signals, beyond spoken content,\nsuch as emotion, background noise, etc. Despite this, evaluation benchmarks\nwhich evaluate awareness to a wide range of acoustic aspects, are lacking. To\nhelp bridge this gap, we introduce SALMon, a novel evaluation suite\nencompassing background noise, emotion, speaker identity and room impulse\nresponse. The proposed benchmarks both evaluate the consistency of the\ninspected element and how much it matches the spoken text. We follow a\nmodelling based approach, measuring whether a model gives correct samples\nhigher scores than incorrect ones. This approach makes the benchmark fast to\ncompute even for large models. We evaluated several speech language models on\nSALMon, thus highlighting the strengths and weaknesses of each evaluated\nmethod. We make the code and data publicly available at\nhttps://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .", "published": "2024-09-11 17:34:52", "link": "http://arxiv.org/abs/2409.07437v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research\n  Repositories", "abstract": "Given that Large Language Models (LLMs) have made significant progress in\nwriting code, can they now be used to autonomously reproduce results from\nresearch repositories? Such a capability would be a boon to the research\ncommunity, helping researchers validate, understand, and extend prior work. To\nadvance towards this goal, we introduce SUPER, the first benchmark designed to\nevaluate the capability of LLMs in setting up and executing tasks from research\nrepositories. SUPERaims to capture the realistic challenges faced by\nresearchers working with Machine Learning (ML) and Natural Language Processing\n(NLP) research repositories. Our benchmark comprises three distinct problem\nsets: 45 end-to-end problems with annotated expert solutions, 152 sub problems\nderived from the expert set that focus on specific challenges (e.g.,\nconfiguring a trainer), and 602 automatically generated problems for\nlarger-scale development. We introduce various evaluation measures to assess\nboth task success and progress, utilizing gold solutions when available or\napproximations otherwise. We show that state-of-the-art approaches struggle to\nsolve these problems with the best model (GPT-4o) solving only 16.3% of the\nend-to-end set, and 46.1% of the scenarios. This illustrates the challenge of\nthis task, and suggests that SUPER can serve as a valuable resource for the\ncommunity to make and measure progress.", "published": "2024-09-11 17:37:48", "link": "http://arxiv.org/abs/2409.07440v1", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting\n  LLMs", "abstract": "Jailbreak vulnerabilities in Large Language Models (LLMs) refer to methods\nthat extract malicious content from the model by carefully crafting prompts or\nsuffixes, which has garnered significant attention from the research community.\nHowever, traditional attack methods, which primarily focus on the semantic\nlevel, are easily detected by the model. These methods overlook the difference\nin the model's alignment protection capabilities at different output stages. To\naddress this issue, we propose an adaptive position pre-fill jailbreak attack\napproach for executing jailbreak attacks on LLMs. Our method leverages the\nmodel's instruction-following capabilities to first output pre-filled safe\ncontent, then exploits its narrative-shifting abilities to generate harmful\ncontent. Extensive black-box experiments demonstrate our method can improve the\nattack success rate by 47% on the widely recognized secure model (Llama2)\ncompared to existing approaches. Our code can be found at:\nhttps://github.com/Yummy416/AdaPPA.", "published": "2024-09-11 00:00:58", "link": "http://arxiv.org/abs/2409.07503v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Leveraging User-Generated Reviews for Recommender Systems with Dynamic\n  Headers", "abstract": "E-commerce platforms have a vast catalog of items to cater to their\ncustomers' shopping interests. Most of these platforms assist their customers\nin the shopping process by offering optimized recommendation carousels,\ndesigned to help customers quickly locate their desired items. Many models have\nbeen proposed in academic literature to generate and enhance the ranking and\nrecall set of items in these carousels. Conventionally, the accompanying\ncarousel title text (header) of these carousels remains static. In most\ninstances, a generic text such as \"Items similar to your current viewing\" is\nutilized. Fixed variations such as the inclusion of specific attributes \"Other\nitems from a similar seller\" or \"Items from a similar brand\" in addition to\n\"frequently bought together\" or \"considered together\" are observed as well.\nThis work proposes a novel approach to customize the header generation process\nof these carousels. Our work leverages user-generated reviews that lay focus on\nspecific attributes (aspects) of an item that were favorably perceived by users\nduring their interaction with the given item. We extract these aspects from\nreviews and train a graph neural network-based model under the framework of a\nconditional ranking task. We refer to our innovative methodology as Dynamic\nText Snippets (DTS) which generates multiple header texts for an anchor item\nand its recall set. Our approach demonstrates the potential of utilizing\nuser-generated reviews and presents a unique paradigm for exploring\nincreasingly context-aware recommendation systems.", "published": "2024-09-11 21:18:21", "link": "http://arxiv.org/abs/2409.07627v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4\n  Capabilities", "abstract": "In this paper we explore evaluation of LLM capabilities. We present\nmeasurements of GPT-4 performance on several deterministic tasks; each task\ninvolves a basic calculation and takes as input parameter some element drawn\nfrom a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform\nenough trials so that statistically significant differences can be detected.\nThis allows us to investigate the sensitivity of task-accuracy both to query\nphrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far\nlarger than can be explained by sampling effects. For example, performance on a\nsimple list-counting task varies with query-phrasing and list-length, but also\nwith list composition (i.e., the thing-to-be-counted) and object frequency\n(e.g., success when an element accounts for $\\approx$ 50\\% of a list is\ndifferent from when it accounts for $\\approx$ 70\\% etc).\n  We conclude that efforts to quantify LLM capabilities easily succumb to the\nlanguage-as-fixed-effect fallacy, where experimental observations are\nimproperly generalized beyond what the data supports. A consequence appears to\nbe that intuitions that have been formed based on interactions with humans form\na very unreliable guide as to which input modifications should ``make no\ndifference'' to LLM performance.", "published": "2024-09-11 21:48:33", "link": "http://arxiv.org/abs/2409.07638v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Simplified Retriever to Improve Accuracy of Phenotype Normalizations\n  by Large Language Models", "abstract": "Large language models (LLMs) have shown improved accuracy in phenotype term\nnormalization tasks when augmented with retrievers that suggest candidate\nnormalizations based on term definitions. In this work, we introduce a\nsimplified retriever that enhances LLM accuracy by searching the Human\nPhenotype Ontology (HPO) for candidate matches using contextual word embeddings\nfrom BioBERT without the need for explicit term definitions. Testing this\nmethod on terms derived from the clinical synopses of Online Mendelian\nInheritance in Man (OMIM), we demonstrate that the normalization accuracy of a\nstate-of-the-art LLM increases from a baseline of 62.3% without augmentation to\n90.3% with retriever augmentation. This approach is potentially generalizable\nto other biomedical term normalization tasks and offers an efficient\nalternative to more complex retrieval methods.", "published": "2024-09-11 00:16:17", "link": "http://arxiv.org/abs/2409.13744v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "68", "I.2"], "primary_category": "cs.CL"}
{"title": "Contextualization of ASR with LLM using phonetic retrieval-based\n  augmentation", "abstract": "Large language models (LLMs) have shown superb capability of modeling\nmultimodal signals including audio and text, allowing the model to generate\nspoken or textual response given a speech input. However, it remains a\nchallenge for the model to recognize personal named entities, such as contacts\nin a phone book, when the input modality is speech. In this work, we start with\na speech recognition task and propose a retrieval-based solution to\ncontextualize the LLM: we first let the LLM detect named entities in speech\nwithout any context, then use this named entity as a query to retrieve\nphonetically similar named entities from a personal database and feed them to\nthe LLM, and finally run context-aware LLM decoding. In a voice assistant task,\nour solution achieved up to 30.2% relative word error rate reduction and 73.6%\nrelative named entity error rate reduction compared to a baseline system\nwithout contextualization. Notably, our solution by design avoids prompting the\nLLM with the full named entity database, making it highly efficient and\napplicable to large named entity databases.", "published": "2024-09-11 18:32:38", "link": "http://arxiv.org/abs/2409.15353v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Context-Aware Membership Inference Attacks against Pre-trained Large\n  Language Models", "abstract": "Prior Membership Inference Attacks (MIAs) on pre-trained Large Language\nModels (LLMs), adapted from classification model attacks, fail due to ignoring\nthe generative process of LLMs across token sequences. In this paper, we\npresent a novel attack that adapts MIA statistical tests to the perplexity\ndynamics of subsequences within a data point. Our method significantly\noutperforms prior loss-based approaches, revealing context-dependent\nmemorization patterns in pre-trained LLMs.", "published": "2024-09-11 01:56:35", "link": "http://arxiv.org/abs/2409.13745v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Neural Ambisonic Encoding For Multi-Speaker Scenarios Using A Circular\n  Microphone Array", "abstract": "Spatial audio formats like Ambisonics are playback device layout-agnostic and\nwell-suited for applications such as teleconferencing and virtual reality.\nConventional Ambisonic encoding methods often rely on spherical microphone\narrays for efficient sound field capture, which limits their flexibility in\npractical scenarios. We propose a deep learning (DL)-based approach, leveraging\na two-stage network architecture for encoding circular microphone array signals\ninto second-order Ambisonics (SOA) in multi-speaker environments. In addition,\nwe introduce: (i) a novel loss function based on spatial power maps to\nregularize inter-channel correlations of the Ambisonic signals, and (ii) a\nchannel permutation technique to resolve the ambiguity of encoding vertical\ninformation using a horizontal circular array. Evaluation on simulated speech\nand noise datasets shows that our approach consistently outperforms traditional\nsignal processing (SP) and DL-based methods, providing significantly better\ntimbral and spatial quality and higher source localization accuracy. Binaural\naudio demos with visualizations are available at\nhttps://bridgoon97.github.io/NeuralAmbisonicEncoding/.", "published": "2024-09-11 02:32:18", "link": "http://arxiv.org/abs/2409.06954v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Rethinking Mamba in Speech Processing by Self-Supervised Models", "abstract": "The Mamba-based model has demonstrated outstanding performance across tasks\nin computer vision, natural language processing, and speech processing.\nHowever, in the realm of speech processing, the Mamba-based model's performance\nvaries across different tasks. For instance, in tasks such as speech\nenhancement and spectrum reconstruction, the Mamba model performs well when\nused independently. However, for tasks like speech recognition, additional\nmodules are required to surpass the performance of attention-based models. We\npropose the hypothesis that the Mamba-based model excels in \"reconstruction\"\ntasks within speech processing. However, for \"classification tasks\" such as\nSpeech Recognition, additional modules are necessary to accomplish the\n\"reconstruction\" step. To validate our hypothesis, we analyze the previous\nMamba-based Speech Models from an information theory perspective. Furthermore,\nwe leveraged the properties of HuBERT in our study. We trained a Mamba-based\nHuBERT model, and the mutual information patterns, along with the model's\nperformance metrics, confirmed our assumptions.", "published": "2024-09-11 13:48:23", "link": "http://arxiv.org/abs/2409.07273v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "The VoiceMOS Challenge 2024: Beyond Speech Quality Prediction", "abstract": "We present the third edition of the VoiceMOS Challenge, a scientific\ninitiative designed to advance research into automatic prediction of human\nspeech ratings. There were three tracks. The first track was on predicting the\nquality of ``zoomed-in'' high-quality samples from speech synthesis systems.\nThe second track was to predict ratings of samples from singing voice synthesis\nand voice conversion with a large variety of systems, listeners, and languages.\nThe third track was semi-supervised quality prediction for noisy, clean, and\nenhanced speech, where a very small amount of labeled training data was\nprovided. Among the eight teams from both academia and industry, we found that\nmany were able to outperform the baseline systems. Successful techniques\nincluded retrieval-based methods and the use of non-self-supervised\nrepresentations like spectrograms and pitch histograms. These results showed\nthat the challenge has advanced the field of subjective speech rating\nprediction.", "published": "2024-09-11 04:26:38", "link": "http://arxiv.org/abs/2409.07001v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic\n  Framework and its Applicability in Automatic Pronunciation Assessment", "abstract": "Second language (L2) learners can improve their pronunciation by imitating\ngolden speech, especially when the speech that aligns with their respective\nspeech characteristics. This study explores the hypothesis that\nlearner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS)\ntechniques can be harnessed as an effective metric for measuring the\npronunciation proficiency of L2 learners. Building on this exploration, the\ncontributions of this study are at least two-fold: 1) design and development of\na systematic framework for assessing the ability of a synthesis model to\ngenerate golden speech, and 2) in-depth investigations of the effectiveness of\nusing golden speech in automatic pronunciation assessment (APA). Comprehensive\nexperiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets\nsuggest that our proposed modeling can yield significant performance\nimprovements with respect to various assessment metrics in relation to some\nprior arts. To our knowledge, this study is the first to explore the role of\ngolden speech in both ZS-TTS and APA, offering a promising regime for\ncomputer-assisted pronunciation training (CAPT).", "published": "2024-09-11 09:55:57", "link": "http://arxiv.org/abs/2409.07151v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Analytic Class Incremental Learning for Sound Source Localization with\n  Privacy Protection", "abstract": "Sound Source Localization (SSL) enabling technology for applications such as\nsurveillance and robotics. While traditional Signal Processing (SP)-based SSL\nmethods provide analytic solutions under specific signal and noise assumptions,\nrecent Deep Learning (DL)-based methods have significantly outperformed them.\nHowever, their success depends on extensive training data and substantial\ncomputational resources. Moreover, they often rely on large-scale annotated\nspatial data and may struggle when adapting to evolving sound classes. To\nmitigate these challenges, we propose a novel Class Incremental Learning (CIL)\napproach, termed SSL-CIL, which avoids serious accuracy degradation due to\ncatastrophic forgetting by incrementally updating the DL-based SSL model\nthrough a closed-form analytic solution. In particular, data privacy is ensured\nsince the learning process does not revisit any historical data\n(exemplar-free), which is more suitable for smart home scenarios. Empirical\nresults in the public SSLR dataset demonstrate the superior performance of our\nproposal, achieving a localization accuracy of 90.9%, surpassing other\ncompetitive methods.", "published": "2024-09-11 12:31:07", "link": "http://arxiv.org/abs/2409.07224v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Muskits-ESPnet: A Comprehensive Toolkit for Singing Voice Synthesis in\n  New Paradigm", "abstract": "This research presents Muskits-ESPnet, a versatile toolkit that introduces\nnew paradigms to Singing Voice Synthesis (SVS) through the application of\npretrained audio models in both continuous and discrete approaches.\nSpecifically, we explore discrete representations derived from SSL models and\naudio codecs and offer significant advantages in versatility and intelligence,\nsupporting multi-format inputs and adaptable data processing workflows for\nvarious SVS models. The toolkit features automatic music score error detection\nand correction, as well as a perception auto-evaluation module to imitate human\nsubjective evaluating scores. Muskits-ESPnet is available at\n\\url{https://github.com/espnet/espnet}.", "published": "2024-09-11 12:36:13", "link": "http://arxiv.org/abs/2409.07226v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ManaTTS Persian: a recipe for creating TTS datasets for lower resource\n  languages", "abstract": "In this study, we introduce ManaTTS, the most extensive publicly accessible\nsingle-speaker Persian corpus, and a comprehensive framework for collecting\ntranscribed speech datasets for the Persian language. ManaTTS, released under\nthe open CC-0 license, comprises approximately 86 hours of audio with a\nsampling rate of 44.1 kHz. Alongside ManaTTS, we also generated the\nVirgoolInformal dataset to evaluate Persian speech recognition models used for\nforced alignment, extending over 5 hours of audio. The datasets are supported\nby a fully transparent, MIT-licensed pipeline, a testament to innovation in the\nfield. It includes unique tools for sentence tokenization, bounded audio\nsegmentation, and a novel forced alignment method. This alignment technique is\nspecifically designed for low-resource languages, addressing a crucial need in\nthe field. With this dataset, we trained a Tacotron2-based TTS model, achieving\na Mean Opinion Score (MOS) of 3.76, which is remarkably close to the MOS of\n3.86 for the utterances generated by the same vocoder and natural spectrogram,\nand the MOS of 4.01 for the natural waveform, demonstrating the exceptional\nquality and effectiveness of the corpus.", "published": "2024-09-11 13:28:41", "link": "http://arxiv.org/abs/2409.07259v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech\n  Editing and Synthesis", "abstract": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model\ndesigned for stable, safe, and robust zero-shot textbased speech editing and\ntext-to-speech synthesis. SSR-Speech is built on a Transformer decoder and\nincorporates classifier-free guidance to enhance the stability of the\ngeneration process. A watermark Encodec is proposed to embed frame-level\nwatermarks into the edited regions of the speech so that which parts were\nedited can be detected. In addition, the waveform reconstruction leverages the\noriginal unedited speech segments, providing superior recovery compared to the\nEncodec model. Our approach achieves state-of-the-art performance in the\nRealEdit speech editing task and the LibriTTS text-to-speech task, surpassing\nprevious methods. Furthermore, SSR-Speech excels in multi-span speech editing\nand also demonstrates remarkable robustness to background sounds. The source\ncode and demos are released.", "published": "2024-09-11 18:24:07", "link": "http://arxiv.org/abs/2409.07556v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FlowSep: Language-Queried Sound Separation with Rectified Flow Matching", "abstract": "Language-queried audio source separation (LASS) focuses on separating sounds\nusing textual descriptions of the desired sources. Current methods mainly use\ndiscriminative approaches, such as time-frequency masking, to separate target\nsounds and minimize interference from other sources. However, these models face\nchallenges when separating overlapping soundtracks, which may lead to artifacts\nsuch as spectral holes or incomplete separation. Rectified flow matching (RFM),\na generative model that establishes linear relations between the distribution\nof data and noise, offers superior theoretical properties and simplicity, but\nhas not yet been explored in sound separation. In this work, we introduce\nFlowSep, a new generative model based on RFM for LASS tasks. FlowSep learns\nlinear flow trajectories from noise to target source features within the\nvariational autoencoder (VAE) latent space. During inference, the RFM-generated\nlatent features are reconstructed into a mel-spectrogram via the pre-trained\nVAE decoder, followed by a pre-trained vocoder to synthesize the waveform.\nTrained on 1,680 hours of audio data, FlowSep outperforms the state-of-the-art\nmodels across multiple benchmarks, as evaluated with subjective and objective\nmetrics. Additionally, our results show that FlowSep surpasses a\ndiffusion-based LASS model in both separation quality and inference efficiency,\nhighlighting its strong potential for audio source separation tasks. Code,\npre-trained models and demos can be found at:\nhttps://audio-agi.github.io/FlowSep_demo/ .", "published": "2024-09-11 20:54:23", "link": "http://arxiv.org/abs/2409.07614v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Anomalous Sound Detection via Low-Rank Adaptation Fine-Tuning\n  of Pre-Trained Audio Models", "abstract": "Anomalous Sound Detection (ASD) has gained significant interest through the\napplication of various Artificial Intelligence (AI) technologies in industrial\nsettings. Though possessing great potential, ASD systems can hardly be readily\ndeployed in real production sites due to the generalization problem, which is\nprimarily caused by the difficulty of data collection and the complexity of\nenvironmental factors. This paper introduces a robust ASD model that leverages\naudio pre-trained models. Specifically, we fine-tune these models using machine\noperation data, employing SpecAug as a data augmentation strategy.\nAdditionally, we investigate the impact of utilizing Low-Rank Adaptation (LoRA)\ntuning instead of full fine-tuning to address the problem of limited data for\nfine-tuning. Our experiments on the DCASE2023 Task 2 dataset establish a new\nbenchmark of 77.75% on the evaluation set, with a significant improvement of\n6.48% compared with previous state-of-the-art (SOTA) models, including top-tier\ntraditional convolutional networks and speech pre-trained models, which\ndemonstrates the effectiveness of audio pre-trained models with LoRA tuning.\nAblation studies are also conducted to showcase the efficacy of the proposed\nscheme.", "published": "2024-09-11 05:19:38", "link": "http://arxiv.org/abs/2409.07016v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Linear Time Complexity Conformers with SummaryMixing for Streaming\n  Speech Recognition", "abstract": "Automatic speech recognition (ASR) with an encoder equipped with\nself-attention, whether streaming or non-streaming, takes quadratic time in the\nlength of the speech utterance. This slows down training and decoding, increase\ntheir cost, and limit the deployment of the ASR in constrained devices.\nSummaryMixing is a promising linear-time complexity alternative to\nself-attention for non-streaming speech recognition that, for the first time,\npreserves or outperforms the accuracy of self-attention models. Unfortunately,\nthe original definition of SummaryMixing is not suited to streaming speech\nrecognition. Hence, this work extends SummaryMixing to a Conformer Transducer\nthat works in both a streaming and an offline mode. It shows that this new\nlinear-time complexity speech encoder outperforms self-attention in both\nscenarios while requiring less compute and memory during training and decoding.", "published": "2024-09-11 10:24:43", "link": "http://arxiv.org/abs/2409.07165v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing CTC-Based Visual Speech Recognition", "abstract": "This paper presents LiteVSR2, an enhanced version of our previously\nintroduced efficient approach to Visual Speech Recognition (VSR). Building upon\nour knowledge distillation framework from a pre-trained Automatic Speech\nRecognition (ASR) model, we introduce two key improvements: a stabilized video\npreprocessing technique and feature normalization in the distillation process.\nThese improvements yield substantial performance gains on the LRS2 and LRS3\nbenchmarks, positioning LiteVSR2 as the current best CTC-based VSR model\nwithout increasing the volume of training data or computational resources\nutilized. Furthermore, we explore the scalability of our approach by examining\nperformance metrics across varying model complexities and training data\nvolumes. LiteVSR2 maintains the efficiency of its predecessor while\nsignificantly enhancing accuracy, thereby demonstrating the potential for\nresource-efficient advancements in VSR technology.", "published": "2024-09-11 12:02:42", "link": "http://arxiv.org/abs/2409.07210v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "D-CAPTCHA++: A Study of Resilience of Deepfake CAPTCHA under\n  Transferable Imperceptible Adversarial Attack", "abstract": "The advancements in generative AI have enabled the improvement of audio\nsynthesis models, including text-to-speech and voice conversion. This raises\nconcerns about its potential misuse in social manipulation and political\ninterference, as synthetic speech has become indistinguishable from natural\nhuman speech. Several speech-generation programs are utilized for malicious\npurposes, especially impersonating individuals through phone calls. Therefore,\ndetecting fake audio is crucial to maintain social security and safeguard the\nintegrity of information. Recent research has proposed a D-CAPTCHA system based\non the challenge-response protocol to differentiate fake phone calls from real\nones. In this work, we study the resilience of this system and introduce a more\nrobust version, D-CAPTCHA++, to defend against fake calls. Specifically, we\nfirst expose the vulnerability of the D-CAPTCHA system under transferable\nimperceptible adversarial attack. Secondly, we mitigate such vulnerability by\nimproving the robustness of the system by using adversarial training in\nD-CAPTCHA deepfake detectors and task classifiers.", "published": "2024-09-11 16:25:02", "link": "http://arxiv.org/abs/2409.07390v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "VMAS: Video-to-Music Generation via Semantic Alignment in Web Music\n  Videos", "abstract": "We present a framework for learning to generate background music from video\ninputs. Unlike existing works that rely on symbolic musical annotations, which\nare limited in quantity and diversity, our method leverages large-scale web\nvideos accompanied by background music. This enables our model to learn to\ngenerate realistic and diverse music. To accomplish this goal, we develop a\ngenerative video-music Transformer with a novel semantic video-music alignment\nscheme. Our model uses a joint autoregressive and contrastive learning\nobjective, which encourages the generation of music aligned with high-level\nvideo content. We also introduce a novel video-beat alignment scheme to match\nthe generated music beats with the low-level motions in the video. Lastly, to\ncapture fine-grained visual cues in a video needed for realistic background\nmusic generation, we introduce a new temporal video encoder architecture,\nallowing us to efficiently process videos consisting of many densely sampled\nframes. We train our framework on our newly curated DISCO-MV dataset,\nconsisting of 2.2M video-music samples, which is orders of magnitude larger\nthan any prior datasets used for video music generation. Our method outperforms\nexisting approaches on the DISCO-MV and MusicCaps datasets according to various\nmusic generation evaluation metrics, including human evaluation. Results are\navailable at https://genjib.github.io/project_page/VMAs/index.html", "published": "2024-09-11 17:56:48", "link": "http://arxiv.org/abs/2409.07450v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "The Unreliability of Acoustic Systems in Alzheimer's Speech Datasets\n  with Heterogeneous Recording Conditions", "abstract": "Automated speech analysis is a thriving approach to detect early markers of\nAlzheimer's disease (AD). Yet, recording conditions in most AD datasets are\nheterogeneous, with patients and controls often evaluated in different acoustic\nsettings. While this is not a problem for analyses based on speech\ntranscription or features obtained from manual alignment, it does cast serious\ndoubts on the validity of acoustic features, which are strongly influenced by\nacquisition conditions. We examined this issue in the ADreSSo dataset, derived\nfrom the widely used Pitt corpus. We show that systems based on two acoustic\nfeatures, MFCCs and Wav2vec 2.0 embeddings, can discriminate AD patients from\ncontrols with above-chance performance when using only the non-speech part of\nthe audio signals. We replicated this finding in a separate dataset of Spanish\nspeakers. Thus, in these datasets, the class can be partly predicted by\nrecording conditions. Our results are a warning against the use of acoustic\nsystems for identifying patients based on non-standardized recordings. We\npropose that acoustically heterogeneous datasets for dementia studies should be\neither (a) analyzed using only transcripts or other features derived from\nmanual annotations, or (b) replaced by datasets collected with strictly\ncontrolled acoustic conditions.", "published": "2024-09-11 20:50:45", "link": "http://arxiv.org/abs/2409.12170v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Developing a Framework for Sonifying Variational Quantum Algorithms:\n  Implications for Music Composition", "abstract": "This chapter examines the Variational Quantum Harmonizer, a software tool and\nmusical interface that focuses on the problem of sonification of the\nminimization steps of Variational Quantum Algorithms (VQA), used for simulating\nproperties of quantum systems and optimization problems assisted by quantum\nhardware. Particularly, it details the sonification of Quadratic Unconstrained\nBinary Optimization (QUBO) problems using VQA. A flexible design enables its\nfuture applications both as a sonification tool for auditory displays in\nscientific investigation, and as a hybrid quantum-digital musical instrument\nfor artistic endeavours. In turn, sonification can help researchers understand\ncomplex systems better and can serve for the training of quantum physics and\nquantum computing. The VQH structure, including its software implementation,\ncontrol mechanisms, and sonification mappings are detailed. Moreover, it guides\nthe design of QUBO cost functions in VQH as a music compositional object. The\ndiscussion is extended to the implications of applying quantum-assisted\nsimulation in quantum-computer aided composition and live-coding performances.\nAn artistic output is showcased by the piece \\textit{Hexagonal Chambers}\n(Thomas and Itabora\\'i, 2023).", "published": "2024-09-11 08:50:43", "link": "http://arxiv.org/abs/2409.07104v1", "categories": ["cs.SD", "cs.ET", "cs.HC", "eess.AS", "quant-ph"], "primary_category": "cs.SD"}
