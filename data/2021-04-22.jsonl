{"title": "Provable Limitations of Acquiring Meaning from Ungrounded Form: What\n  Will Future Language Models Understand?", "abstract": "Language models trained on billions of tokens have recently led to\nunprecedented results on many NLP tasks. This success raises the question of\nwhether, in principle, a system can ever ``understand'' raw text without access\nto some form of grounding. We formally investigate the abilities of ungrounded\nsystems to acquire meaning. Our analysis focuses on the role of ``assertions'':\ntextual contexts that provide indirect clues about the underlying semantics. We\nstudy whether assertions enable a system to emulate representations preserving\nsemantic relations like equivalence. We find that assertions enable semantic\nemulation of languages that satisfy a strong notion of semantic transparency.\nHowever, for classes of languages where the same expression can take different\nvalues in different contexts, we show that emulation can become uncomputable.\nFinally, we discuss differences between our formal model and natural language,\nexploring how our results generalize to a modal setting and other semantic\nrelations. Together, our results suggest that assertions in code or language do\nnot provide sufficient signal to fully emulate semantic representations. We\nformalize ways in which ungrounded language models appear to be fundamentally\nlimited in their ability to ``understand''.", "published": "2021-04-22 01:00:17", "link": "http://arxiv.org/abs/2104.10809v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Fuzziness in Neural Network Models of Language Processing", "abstract": "Humans often communicate by using imprecise language, suggesting that fuzzy\nconcepts with unclear boundaries are prevalent in language use. In this paper,\nwe test the extent to which models trained to capture the distributional\nstatistics of language show correspondence to fuzzy-membership patterns. Using\nthe task of natural language inference, we test a recent state of the art model\non the classical case of temperature, by examining its mapping of temperature\ndata to fuzzy-perceptions such as \"cool\", \"hot\", etc. We find the model to show\npatterns that are similar to classical fuzzy-set theoretic formulations of\nlinguistic hedges, albeit with a substantial amount of noise, suggesting that\nmodels trained solely on language show promise in encoding fuzziness.", "published": "2021-04-22 01:06:14", "link": "http://arxiv.org/abs/2104.10813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fuzzy Classification of Multi-intent Utterances", "abstract": "Current intent classification approaches assign binary intent class\nmemberships to natural language utterances while disregarding the inherent\nvagueness in language and the corresponding vagueness in intent class\nboundaries. In this work, we propose a scheme to address the ambiguity in\nsingle-intent as well as multi-intent natural language utterances by creating\ndegree memberships over fuzzified intent classes. To our knowledge, this is the\nfirst work to address and quantify the impact of the fuzzy nature of natural\nlanguage utterances over intent category memberships. Additionally, our\napproach overcomes the sparsity of multi-intent utterance data to train\nclassification models by using a small database of single intent utterances to\ngenerate class memberships over multi-intent utterances. We evaluate our\napproach over two task-oriented dialog datasets, across different fuzzy\nmembership generation techniques and approximate string similarity measures.\nOur results reveal the impact of lexical overlap between utterances of\ndifferent intents, and the underlying data distributions, on the fuzzification\nof intent memberships. Moreover, we evaluate the accuracy of our approach by\ncomparing the defuzzified memberships to their binary counterparts, across\ndifferent combinations of membership functions and string similarity measures.", "published": "2021-04-22 02:15:56", "link": "http://arxiv.org/abs/2104.10830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low Anisotropy Sense Retrofitting (LASeR) : Towards Isotropic and Sense\n  Enriched Representations", "abstract": "Contextual word representation models have shown massive improvements on a\nmultitude of NLP tasks, yet their word sense disambiguation capabilities remain\npoorly explained. To address this gap, we assess whether contextual word\nrepresentations extracted from deep pretrained language models create\ndistinguishable representations for different senses of a given word. We\nanalyze the representation geometry and find that most layers of deep\npretrained language models create highly anisotropic representations, pointing\ntowards the existence of representation degeneration problem in contextual word\nrepresentations. After accounting for anisotropy, our study further reveals\nthat there is variability in sense learning capabilities across different\nlanguage models. Finally, we propose LASeR, a 'Low Anisotropy Sense\nRetrofitting' approach that renders off-the-shelf representations isotropic and\nsemantically more meaningful, resolving the representation degeneration problem\nas a post-processing step, and conducting sense-enrichment of contextualized\nrepresentations extracted from deep neural language models.", "published": "2021-04-22 02:44:49", "link": "http://arxiv.org/abs/2104.10833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriched Attention for Robust Relation Extraction", "abstract": "The performance of relation extraction models has increased considerably with\nthe rise of neural networks. However, a key issue of neural relation extraction\nis robustness: the models do not scale well to long sentences with multiple\nentities and relations. In this work, we address this problem with an enriched\nattention mechanism. Attention allows the model to focus on parts of the input\nsentence that are relevant to relation extraction. We propose to enrich the\nattention function with features modeling knowledge about the relation\narguments and the shortest dependency path between them. Thus, for different\nrelation arguments, the model can pay attention to different parts of the\nsentence. Our model outperforms prior work using comparable setups on two\npopular benchmarks, and our analysis confirms that it indeed scales to long\nsentences with many entities.", "published": "2021-04-22 07:17:19", "link": "http://arxiv.org/abs/2104.10899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of\n  Media Frames", "abstract": "Understanding how news media frame political issues is important due to its\nimpact on public attitudes, yet hard to automate. Computational approaches have\nlargely focused on classifying the frame of a full news article while framing\nsignals are often subtle and local. Furthermore, automatic news analysis is a\nsensitive domain, and existing classifiers lack transparency in their\npredictions. This paper addresses both issues with a novel semi-supervised\nmodel, which jointly learns to embed local information about the events and\nrelated actors in a news article through an auto-encoding framework, and to\nleverage this signal for document-level frame classification. Our experiments\nshow that: our model outperforms previous models of frame prediction; we can\nfurther improve performance with unlabeled training data leveraging the\nsemi-supervised nature of our model; and the learnt event and actor embeddings\nintuitively corroborate the document-level predictions, providing a nuanced and\ninterpretable article frame representation.", "published": "2021-04-22 13:05:53", "link": "http://arxiv.org/abs/2104.11030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COVID-19 and Big Data: Multi-faceted Analysis for Spatio-temporal\n  Understanding of the Pandemic with Social Media Conversations", "abstract": "COVID-19 has been devastating the world since the end of 2019 and has\ncontinued to play a significant role in major national and worldwide events,\nand consequently, the news. In its wake, it has left no life unaffected. Having\nearned the world's attention, social media platforms have served as a vehicle\nfor the global conversation about COVID-19. In particular, many people have\nused these sites in order to express their feelings, experiences, and\nobservations about the pandemic. We provide a multi-faceted analysis of\ncritical properties exhibited by these conversations on social media regarding\nthe novel coronavirus pandemic. We present a framework for analysis, mining,\nand tracking the critical content and characteristics of social media\nconversations around the pandemic. Focusing on Twitter and Reddit, we have\ngathered a large-scale dataset on COVID-19 social media conversations. Our\nanalyses cover tracking potential reports on virus acquisition, symptoms,\nconversation topics, and language complexity measures through time and by\nregion across the United States. We also present a BERT-based model for\nrecognizing instances of hateful tweets in COVID-19 conversations, which\nachieves a lower error-rate than the state-of-the-art performance. Our results\nprovide empirical validation for the effectiveness of our proposed framework\nand further demonstrate that social media data can be efficiently leveraged to\nprovide public health experts with inexpensive but thorough insight over the\ncourse of an outbreak.", "published": "2021-04-22 00:45:50", "link": "http://arxiv.org/abs/2104.10807v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "On Geodesic Distances and Contextual Embedding Compression for Text\n  Classification", "abstract": "In some memory-constrained settings like IoT devices and over-the-network\ndata pipelines, it can be advantageous to have smaller contextual embeddings.\nWe investigate the efficacy of projecting contextual embedding data (BERT) onto\na manifold, and using nonlinear dimensionality reduction techniques to compress\nthese embeddings. In particular, we propose a novel post-processing approach,\napplying a combination of Isomap and PCA. We find that the geodesic distance\nestimations, estimates of the shortest path on a Riemannian manifold, from\nIsomap's k-Nearest Neighbors graph bolstered the performance of the compressed\nembeddings to be comparable to the original BERT embeddings. On one dataset, we\nfind that despite a 12-fold dimensionality reduction, the compressed embeddings\nperformed within 0.1% of the original BERT embeddings on a downstream\nclassification task. In addition, we find that this approach works particularly\nwell on tasks reliant on syntactic data, when compared with linear\ndimensionality reduction. These results show promise for a novel geometric\napproach to achieve lower dimensional text embeddings from existing\ntransformers and pave the way for data-specific and application-specific\nembedding compressions.", "published": "2021-04-22 19:30:06", "link": "http://arxiv.org/abs/2104.11295v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Short Survey of Pre-trained Language Models for Conversational AI-A\n  NewAge in NLP", "abstract": "Building a dialogue system that can communicate naturally with humans is a\nchallenging yet interesting problem of agent-based computing. The rapid growth\nin this area is usually hindered by the long-standing problem of data scarcity\nas these systems are expected to learn syntax, grammar, decision making, and\nreasoning from insufficient amounts of task-specific dataset. The recently\nintroduced pre-trained language models have the potential to address the issue\nof data scarcity and bring considerable advantages by generating contextualized\nword embeddings. These models are considered counterpart of ImageNet in NLP and\nhave demonstrated to capture different facets of language such as hierarchical\nrelations, long-term dependency, and sentiment. In this short survey paper, we\ndiscuss the recent progress made in the field of pre-trained language models.\nWe also deliberate that how the strengths of these language models can be\nleveraged in designing more engaging and more eloquent conversational agents.\nThis paper, therefore, intends to establish whether these pre-trained models\ncan overcome the challenges pertinent to dialogue systems, and how their\narchitecture could be exploited in order to overcome these challenges. Open\nchallenges in the field of dialogue systems have also been deliberated.", "published": "2021-04-22 01:00:56", "link": "http://arxiv.org/abs/2104.10810v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Efficient Relation-aware Scoring Function Search for Knowledge Graph\n  Embedding", "abstract": "The scoring function, which measures the plausibility of triplets in\nknowledge graphs (KGs), is the key to ensure the excellent performance of KG\nembedding, and its design is also an important problem in the literature.\nAutomated machine learning (AutoML) techniques have recently been introduced\ninto KG to design task-aware scoring functions, which achieve state-of-the-art\nperformance in KG embedding. However, the effectiveness of searched scoring\nfunctions is still not as good as desired. In this paper, observing that\nexisting scoring functions can exhibit distinct performance on different\nsemantic patterns, we are motivated to explore such semantics by searching\nrelation-aware scoring functions. But the relation-aware search requires a much\nlarger search space than the previous one. Hence, we propose to encode the\nspace as a supernet and propose an efficient alternative minimization algorithm\nto search through the supernet in a one-shot manner. Finally, experimental\nresults on benchmark datasets demonstrate that the proposed method can\nefficiently search relation-aware scoring functions, and achieve better\nembedding performance than state-of-the-art methods.", "published": "2021-04-22 06:05:13", "link": "http://arxiv.org/abs/2104.10880v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Hybrid Encoder: Towards Efficient and Precise Native AdsRecommendation\n  via Hybrid Transformer Encoding Networks", "abstract": "Transformer encoding networks have been proved to be a powerful tool of\nunderstanding natural languages. They are playing a critical role in native ads\nservice, which facilitates the recommendation of appropriate ads based on\nuser's web browsing history. For the sake of efficient recommendation,\nconventional methods would generate user and advertisement embeddings\nindependently with a siamese transformer encoder, such that approximate nearest\nneighbour search (ANN) can be leveraged. Given that the underlying semantic\nabout user and ad can be complicated, such independently generated embeddings\nare prone to information loss, which leads to inferior recommendation quality.\nAlthough another encoding strategy, the cross encoder, can be much more\naccurate, it will lead to huge running cost and become infeasible for realtime\nservices, like native ads recommendation. In this work, we propose hybrid\nencoder, which makes efficient and precise native ads recommendation through\ntwo consecutive steps: retrieval and ranking. In the retrieval step, user and\nad are encoded with a siamese component, which enables relevant candidates to\nbe retrieved via ANN search. In the ranking step, it further represents each ad\nwith disentangled embeddings and each user with ad-related embeddings, which\ncontributes to the fine-grained selection of high-quality ads from the\ncandidate set. Both steps are light-weighted, thanks to the pre-computed and\ncached intermedia results. To optimize the hybrid encoder's performance in this\ntwo-stage workflow, a progressive training pipeline is developed, which builds\nup the model's capability in the retrieval and ranking task step-by-step. The\nhybrid encoder's effectiveness is experimentally verified: with very little\nadditional cost, it outperforms the siamese encoder significantly and achieves\ncomparable recommendation quality as the cross encoder.", "published": "2021-04-22 08:42:07", "link": "http://arxiv.org/abs/2104.10925v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Fast Text-Only Domain Adaptation of RNN-Transducer Prediction Network", "abstract": "Adaption of end-to-end speech recognition systems to new tasks is known to be\nchallenging. A number of solutions have been proposed which apply external\nlanguage models with various fusion methods, possibly with a combination of\ntwo-pass decoding. Also TTS systems have been used to generate adaptation data\nfor the end-to-end models. In this paper we show that RNN-transducer models can\nbe effectively adapted to new domains using only small amounts of textual data.\nBy taking advantage of model's inherent structure, where the prediction network\nis interpreted as a language model, we can apply fast adaptation to the model.\nAdapting the model avoids the need for complicated decoding time fusions and\nexternal language models. Using appropriate regularization, the prediction\nnetwork can be adapted to new domains while still retaining good generalization\ncapabilities. We show with multiple ASR evaluation tasks how this method can\nprovide relative gains of 10-45% in target task WER. We also share insights how\nRNN-transducer prediction network performs as a language model.", "published": "2021-04-22 15:21:41", "link": "http://arxiv.org/abs/2104.11127v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Earnings-21: A Practical Benchmark for ASR in the Wild", "abstract": "Commonly used speech corpora inadequately challenge academic and commercial\nASR systems. In particular, speech corpora lack metadata needed for detailed\nanalysis and WER measurement. In response, we present Earnings-21, a 39-hour\ncorpus of earnings calls containing entity-dense speech from nine different\nfinancial sectors. This corpus is intended to benchmark ASR systems in the wild\nwith special attention towards named entity recognition. We benchmark four\ncommercial ASR models, two internal models built with open-source tools, and an\nopen-source LibriSpeech model and discuss their differences in performance on\nEarnings-21. Using our recently released fstalign tool, we provide a candid\nanalysis of each model's recognition capabilities under different partitions.\nOur analysis finds that ASR accuracy for certain NER categories is poor,\npresenting a significant impediment to transcript comprehension and usage.\nEarnings-21 bridges academic and commercial ASR system evaluation and enables\nfurther research on entity modeling and WER on real world audio.", "published": "2021-04-22 23:04:28", "link": "http://arxiv.org/abs/2104.11348v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Building Bilingual and Code-Switched Voice Conversion with Limited\n  Training Data Using Embedding Consistency Loss", "abstract": "Building cross-lingual voice conversion (VC) systems for multiple speakers\nand multiple languages has been a challenging task for a long time. This paper\ndescribes a parallel non-autoregressive network to achieve bilingual and\ncode-switched voice conversion for multiple speakers when there are only\nmono-lingual corpora for each language. We achieve cross-lingual VC between\nMandarin speech with multiple speakers and English speech with multiple\nspeakers by applying bilingual bottleneck features. To boost voice cloning\nperformance, we use an adversarial speaker classifier with a gradient reversal\nlayer to reduce the source speaker's information from the output of encoder.\nFurthermore, in order to improve speaker similarity between reference speech\nand converted speech, we adopt an embedding consistency loss between the\nsynthesized speech and its natural reference speech in our network.\nExperimental results show that our proposed method can achieve high quality\nconverted speech with mean opinion score (MOS) around 4. The conversion system\nperforms well in terms of speaker similarity for both in-set speaker conversion\nand out-set-of one-shot conversion.", "published": "2021-04-22 02:43:26", "link": "http://arxiv.org/abs/2104.10832v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Nonlinear Spatial Filtering in Multichannel Speech Enhancement", "abstract": "The majority of multichannel speech enhancement algorithms are two-step\nprocedures that first apply a linear spatial filter, a so-called beamformer,\nand combine it with a single-channel approach for postprocessing. However, the\nserial concatenation of a linear spatial filter and a postfilter is not\ngenerally optimal in the minimum mean square error (MMSE) sense for noise\ndistributions other than a Gaussian distribution. Rather, the MMSE optimal\nfilter is a joint spatial and spectral nonlinear function. While estimating the\nparameters of such a filter with traditional methods is challenging, modern\nneural networks may provide an efficient way to learn the nonlinear function\ndirectly from data. To see if further research in this direction is worthwhile,\nin this work we examine the potential performance benefit of replacing the\ncommon two-step procedure with a joint spatial and spectral nonlinear filter.\n  We analyze three different forms of non-Gaussianity: First, we evaluate on\nsuper-Gaussian noise with a high kurtosis. Second, we evaluate on inhomogeneous\nnoise fields created by five interfering sources using two microphones, and\nthird, we evaluate on real-world recordings from the CHiME3 database. In all\nscenarios, considerable improvements may be obtained. Most prominently, our\nanalyses show that a nonlinear spatial filter uses the available spatial\ninformation more effectively than a linear spatial filter as it is capable of\nsuppressing more than $D-1$ directional interfering sources with a\n$D$-dimensional microphone array without spatial adaptation.", "published": "2021-04-22 13:07:02", "link": "http://arxiv.org/abs/2104.11033v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Protecting gender and identity with disentangled speech representations", "abstract": "Besides its linguistic content, our speech is rich in biometric information\nthat can be inferred by classifiers. Learning privacy-preserving\nrepresentations for speech signals enables downstream tasks without sharing\nunnecessary, private information about an individual. In this paper, we show\nthat protecting gender information in speech is more effective than modelling\nspeaker-identity information only when generating a non-sensitive\nrepresentation of speech. Our method relies on reconstructing speech by\ndecoding linguistic content along with gender information using a variational\nautoencoder. Specifically, we exploit disentangled representation learning to\nencode information about different attributes into separate subspaces that can\nbe factorised independently. We present a novel way to encode gender\ninformation and disentangle two sensitive biometric identifiers, namely gender\nand identity, in a privacy-protecting setting. Experiments on the LibriSpeech\ndataset show that gender recognition and speaker verification can be reduced to\na random guess, protecting against classification-based attacks.", "published": "2021-04-22 13:31:41", "link": "http://arxiv.org/abs/2104.11051v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Restoring degraded speech via a modified diffusion model", "abstract": "There are many deterministic mathematical operations (e.g. compression,\nclipping, downsampling) that degrade speech quality considerably. In this paper\nwe introduce a neural network architecture, based on a modification of the\nDiffWave model, that aims to restore the original speech signal. DiffWave, a\nrecently published diffusion-based vocoder, has shown state-of-the-art\nsynthesized speech quality and relatively shorter waveform generation times,\nwith only a small set of parameters. We replace the mel-spectrum upsampler in\nDiffWave with a deep CNN upsampler, which is trained to alter the degraded\nspeech mel-spectrum to match that of the original speech. The model is trained\nusing the original speech waveform, but conditioned on the degraded speech\nmel-spectrum. Post-training, only the degraded mel-spectrum is used as input\nand the model generates an estimate of the original speech. Our model results\nin improved speech quality (original DiffWave model as baseline) on several\ndifferent experiments. These include improving the quality of speech degraded\nby LPC-10 compression, AMR-NB compression, and signal clipping. Compared to the\noriginal DiffWave architecture, our scheme achieves better performance on\nseveral objective perceptual metrics and in subjective comparisons.\nImprovements over baseline are further amplified in a out-of-corpus evaluation\nsetting.", "published": "2021-04-22 23:03:23", "link": "http://arxiv.org/abs/2104.11347v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pose-Controllable Talking Face Generation by Implicitly Modularized\n  Audio-Visual Representation", "abstract": "While accurate lip synchronization has been achieved for arbitrary-subject\naudio-driven talking face generation, the problem of how to efficiently drive\nthe head pose remains. Previous methods rely on pre-estimated structural\ninformation such as landmarks and 3D parameters, aiming to generate\npersonalized rhythmic movements. However, the inaccuracy of such estimated\ninformation under extreme conditions would lead to degradation problems. In\nthis paper, we propose a clean yet effective framework to generate\npose-controllable talking faces. We operate on raw face images, using only a\nsingle photo as an identity reference. The key is to modularize audio-visual\nrepresentations by devising an implicit low-dimension pose code. Substantially,\nboth speech content and head pose information lie in a joint non-identity\nembedding space. While speech content information can be defined by learning\nthe intrinsic synchronization between audio-visual modalities, we identify that\na pose code will be complementarily learned in a modulated convolution-based\nreconstruction framework.\n  Extensive experiments show that our method generates accurately lip-synced\ntalking faces whose poses are controllable by other videos. Moreover, our model\nhas multiple advanced capabilities including extreme view robustness and\ntalking face frontalization. Code, models, and demo videos are available at\nhttps://hangz-nju-cuhk.github.io/projects/PC-AVS.", "published": "2021-04-22 15:10:26", "link": "http://arxiv.org/abs/2104.11116v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
