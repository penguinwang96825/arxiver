{"title": "Multilingual Neural Machine Translation with Deep Encoder and Multiple\n  Shallow Decoders", "abstract": "Recent work in multilingual translation advances translation quality\nsurpassing bilingual baselines using deep transformer models with increased\ncapacity. However, the extra latency and memory costs introduced by this\napproach may make it unacceptable for efficiency-constrained applications. It\nhas recently been shown for bilingual translation that using a deep encoder and\nshallow decoder (DESD) can reduce inference latency while maintaining\ntranslation quality, so we study similar speed-accuracy trade-offs for\nmultilingual translation. We find that for many-to-one translation we can\nindeed increase decoder speed without sacrificing quality using this approach,\nbut for one-to-many translation, shallow decoders cause a clear quality drop.\nTo ameliorate this drop, we propose a deep encoder with multiple shallow\ndecoders (DEMSD) where each shallow decoder is responsible for a disjoint\nsubset of target languages. Specifically, the DEMSD model with 2-layer decoders\nis able to obtain a 1.8x speedup on average compared to a standard transformer\nmodel with no drop in translation quality.", "published": "2022-06-05 01:15:04", "link": "http://arxiv.org/abs/2206.02079v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech Detection Task Against Asian Hate: BERT the Central, While\n  Data-Centric Studies the Crucial", "abstract": "With the COVID-19 pandemic continuing, hatred against Asians is intensifying\nin countries outside Asia, especially among the Chinese. There is an urgent\nneed to detect and prevent hate speech towards Asians effectively. In this\nwork, we first create COVID-HATE-2022, an annotated dataset including 2,025\nannotated tweets fetched in early February 2022, which are labeled based on\nspecific criteria, and we present the comprehensive collection of scenarios of\nhate and non-hate tweets in the dataset. Second, we fine-tune the BERT model\nbased on the relevant datasets and demonstrate several strategies related to\nthe \"cleaning\" of the tweets. Third, we investigate the performance of advanced\nfine-tuning strategies with various model-centric and data-centric approaches,\nand we show that both strategies generally improve the performance, while\ndata-centric ones outperform the others, and it demonstrates the feasibility\nand effectiveness of the data-centric approaches in the associated tasks.", "published": "2022-06-05 07:41:24", "link": "http://arxiv.org/abs/2206.02114v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multimodal Corpus for Emotion Recognition in Sarcasm", "abstract": "While sentiment and emotion analysis have been studied extensively, the\nrelationship between sarcasm and emotion has largely remained unexplored. A\nsarcastic expression may have a variety of underlying emotions. For example, \"I\nlove being ignored\" belies sadness, while \"my mobile is fabulous with a battery\nbackup of only 15 minutes!\" expresses frustration. Detecting the emotion behind\na sarcastic expression is non-trivial yet an important task. We undertake the\ntask of detecting the emotion in a sarcastic statement, which to the best of\nour knowledge, is hitherto unexplored. We start with the recently released\nmultimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions.\nWe identify and correct 343 incorrect emotion labels (out of 690). We double\nthe size of the dataset, label it with emotions along with valence and arousal\nwhich are important indicators of emotional intensity. Finally, we label each\nsarcastic utterance with one of the four sarcasm types-Propositional, Embedded,\nLikeprefixed and Illocutionary, with the goal of advancing sarcasm detection\nresearch. Exhaustive experimentation with multimodal (text, audio, and video)\nfusion models establishes a benchmark for exact emotion recognition in sarcasm\nand outperforms the state-of-art sarcasm detection. We release the dataset\nenriched with various annotations and the code for research purposes:\nhttps://github.com/apoorva-nunna/MUStARD_Plus_Plus", "published": "2022-06-05 08:01:09", "link": "http://arxiv.org/abs/2206.02119v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Meta-learning Paradigm for Zero-shot Intent Classification with\n  Mixture Attention Mechanism", "abstract": "Zero-shot intent classification is a vital and challenging task in dialogue\nsystems, which aims to deal with numerous fast-emerging unacquainted intents\nwithout annotated training data. To obtain more satisfactory performance, the\ncrucial points lie in two aspects: extracting better utterance features and\nstrengthening the model generalization ability. In this paper, we propose a\nsimple yet effective meta-learning paradigm for zero-shot intent\nclassification. To learn better semantic representations for utterances, we\nintroduce a new mixture attention mechanism, which encodes the pertinent word\noccurrence patterns by leveraging the distributional signature attention and\nmulti-layer perceptron attention simultaneously. To strengthen the transfer\nability of the model from seen classes to unseen classes, we reformulate\nzero-shot intent classification with a meta-learning strategy, which trains the\nmodel by simulating multiple zero-shot classification tasks on seen categories,\nand promotes the model generalization ability with a meta-adapting procedure on\nmimic unseen categories. Extensive experiments on two real-world dialogue\ndatasets in different languages show that our model outperforms other strong\nbaselines on both standard and generalized zero-shot intent classification\ntasks.", "published": "2022-06-05 13:37:51", "link": "http://arxiv.org/abs/2206.02179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stylistic Fingerprints, POS-tags and Inflected Languages: A Case Study\n  in Polish", "abstract": "In stylometric investigations, frequencies of the most frequent words (MFWs)\nand character n-grams outperform other style-markers, even if their performance\nvaries significantly across languages. In inflected languages, word endings\nplay a prominent role, and hence different word forms cannot be recognized\nusing generic text tokenization. Countless inflected word forms make\nfrequencies sparse, making most statistical procedures complicated. Presumably,\napplying one of the NLP techniques, such as lemmatization and/or parsing, might\nincrease the performance of classification. The aim of this paper is to examine\nthe usefulness of grammatical features (as assessed via POS-tag n-grams) and\nlemmatized forms in recognizing authorial profiles, in order to address the\nunderlying issue of the degree of freedom of choice within lexis and grammar.\nUsing a corpus of Polish novels, we performed a series of supervised authorship\nattribution benchmarks, in order to compare the classification accuracy for\ndifferent types of lexical and syntactic style-markers. Even if the performance\nof POS-tags as well as lemmatized forms was notoriously worse than that of\nlexical markers, the difference was not substantial and never exceeded ca. 15%.", "published": "2022-06-05 15:48:16", "link": "http://arxiv.org/abs/2206.02208v1", "categories": ["cs.CL", "62H30", "I.7"], "primary_category": "cs.CL"}
{"title": "Exploring Cross-lingual Textual Style Transfer with Large Multilingual\n  Language Models", "abstract": "Detoxification is a task of generating text in polite style while preserving\nmeaning and fluency of the original toxic text. Existing detoxification methods\nare designed to work in one exact language. This work investigates multilingual\nand cross-lingual detoxification and the behavior of large multilingual models\nlike in this setting. Unlike previous works we aim to make large language\nmodels able to perform detoxification without direct fine-tuning in given\nlanguage. Experiments show that multilingual models are capable of performing\nmultilingual style transfer. However, models are not able to perform\ncross-lingual detoxification and direct fine-tuning on exact language is\ninevitable.", "published": "2022-06-05 20:02:30", "link": "http://arxiv.org/abs/2206.02252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation Error Detection: Analyzing the Past and Present for a More\n  Coherent Future", "abstract": "Annotated data is an essential ingredient in natural language processing for\ntraining and evaluating machine learning models. It is therefore very desirable\nfor the annotations to be of high quality. Recent work, however, has shown that\nseveral popular datasets contain a surprising amount of annotation errors or\ninconsistencies. To alleviate this issue, many methods for annotation error\ndetection have been devised over the years. While researchers show that their\napproaches work well on their newly introduced datasets, they rarely compare\ntheir methods to previous work or on the same datasets. This raises strong\nconcerns on methods' general performance and makes it difficult to asses their\nstrengths and weaknesses. We therefore reimplement 18 methods for detecting\npotential annotation errors and evaluate them on 9 English datasets for text\nclassification as well as token and span labeling. In addition, we define a\nuniform evaluation setup including a new formalization of the annotation error\ndetection task, evaluation protocol and general best practices. To facilitate\nfuture research and reproducibility, we release our datasets and\nimplementations in an easy-to-use and open source software package.", "published": "2022-06-05 22:31:45", "link": "http://arxiv.org/abs/2206.02280v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LAE: Language-Aware Encoder for Monolingual and Multilingual ASR", "abstract": "Despite the rapid progress in automatic speech recognition (ASR) research,\nrecognizing multilingual speech using a unified ASR system remains highly\nchallenging. Previous works on multilingual speech recognition mainly focus on\ntwo directions: recognizing multiple monolingual speech or recognizing\ncode-switched speech that uses different languages interchangeably within a\nsingle utterance. However, a pragmatic multilingual recognizer is expected to\nbe compatible with both directions. In this work, a novel language-aware\nencoder (LAE) architecture is proposed to handle both situations by\ndisentangling language-specific information and generating frame-level\nlanguage-aware representations during encoding. In the LAE, the primary\nencoding is implemented by the shared block while the language-specific blocks\nare used to extract specific representations for each language. To learn\nlanguage-specific information discriminatively, a language-aware training\nmethod is proposed to optimize the language-specific blocks in LAE. Experiments\nconducted on Mandarin-English code-switched speech suggest that the proposed\nLAE is capable of discriminating different languages in frame-level and shows\nsuperior performance on both monolingual and multilingual ASR tasks. With\neither a real-recorded or simulated code-switched dataset, the proposed LAE\nachieves statistically significant improvements on both CTC and neural\ntransducer systems. Code is released", "published": "2022-06-05 04:03:12", "link": "http://arxiv.org/abs/2206.02093v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of Online Travel Reviews Based on Capsule Network and\n  Sentiment Lexicon", "abstract": "With the development of online travel services, it has great application\nprospects to timely mine users' evaluation emotions for travel services and use\nthem as indicators to guide the improvement of online travel service quality.\nIn this paper, we study the text sentiment classification of online travel\nreviews based on social media online comments and propose the SCCL model based\non capsule network and sentiment lexicon. SCCL model aims at the lack of\nconsideration of local features and emotional semantic features of the text in\nthe language model that can efficiently extract text context features like BERT\nand GRU. Then make the following improvements to their shortcomings. On the one\nhand, based on BERT-BiGRU, the capsule network is introduced to extract local\nfeatures while retaining good context features. On the other hand, the\nsentiment lexicon is introduced to extract the emotional sequence of the text\nto provide richer emotional semantic features for the model. To enhance the\nuniversality of the sentiment lexicon, the improved SO-PMI algorithm based on\nTF-IDF is used to expand the lexicon, so that the lexicon can also perform well\nin the field of online travel reviews.", "published": "2022-06-05 12:17:46", "link": "http://arxiv.org/abs/2206.02160v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Near-Term Advances in Quantum Natural Language Processing", "abstract": "This paper describes experiments showing that some tasks in natural language\nprocessing (NLP) can already be performed using quantum computers, though so\nfar only with small datasets.\n  We demonstrate various approaches to topic classification. The first uses an\nexplicit word-based approach, in which word-topic scoring weights are\nimplemented as fractional rotations of individual qubit, and a new phrase is\nclassified based on the accumulation of these weights in a scoring qubit using\nentangling controlled-NOT gates. This is compared with more scalable quantum\nencodings of word embedding vectors, which are used in the computation of\nkernel values in a quantum support vector machine: this approach achieved an\naverage of 62% accuracy on classification tasks involving over 10000 words,\nwhich is the largest such quantum computing experiment to date.\n  We describe a quantum probability approach to bigram modeling that can be\napplied to sequences of words and formal concepts, investigating a generative\napproximation to these distributions using a quantum circuit Born machine, and\nan approach to ambiguity resolution in verb-noun composition using single-qubit\nrotations for simple nouns and 2-qubit controlled-NOT gates for simple verbs.\n  The smaller systems described have been run successfully on physical quantum\ncomputers, and the larger ones have been simulated. We show that statistically\nmeaningful results can be obtained using real datasets, but this is much more\ndifficult to predict than with easier artificial language examples used\npreviously in developing quantum NLP systems.\n  Other approaches to quantum NLP are compared, partly with respect to\ncontemporary issues including informal language, fluency, and truthfulness.", "published": "2022-06-05 13:10:46", "link": "http://arxiv.org/abs/2206.02171v3", "categories": ["cs.CL", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Performance Comparison of Simple Transformer and Res-CNN-BiLSTM for\n  Cyberbullying Classification", "abstract": "The task of text classification using Bidirectional based LSTM architectures\nis computationally expensive and time consuming to train. For this,\ntransformers were discovered which effectively give good performance as\ncompared to the traditional deep learning architectures. In this paper we\npresent a performance based comparison between simple transformer based network\nand Res-CNN-BiLSTM based network for cyberbullying text classification problem.\nThe results obtained show that transformer we trained with 0.65 million\nparameters has significantly being able to beat the performance of\nRes-CNN-BiLSTM with 48.82 million parameters for faster training speeds and\nmore generalized metrics. The paper also compares the 1-dimensional character\nlevel embedding network and 100-dimensional glove embedding network with\ntransformer.", "published": "2022-06-05 15:46:21", "link": "http://arxiv.org/abs/2206.02206v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Finetuning a Kalaallisut-English machine translation system using\n  web-crawled data", "abstract": "West Greenlandic, known by native speakers as Kalaallisut, is an extremely\nlow-resource polysynthetic language spoken by around 56,000 people in\nGreenland. Here, we attempt to finetune a pretrained Kalaallisut-to-English\nneural machine translation (NMT) system using web-crawled pseudoparallel\nsentences from around 30 multilingual websites. We compile a corpus of over\n93,000 Kalaallisut sentences and over 140,000 Danish sentences, then use\ncross-lingual sentence embeddings and approximate nearest-neighbors search in\nan attempt to mine near-translations from these corpora. Finally, we translate\nthe Danish sentence to English to obtain a synthetic Kalaallisut-English\naligned corpus. Although the resulting dataset is too small and noisy to\nimprove the pretrained MT model, we believe that with additional resources, we\ncould construct a better pseudoparallel corpus and achieve more promising\nresults on MT. We also note other possible uses of the monolingual Kalaallisut\ndata and discuss directions for future work. We make the code and data for our\nexperiments publicly available.", "published": "2022-06-05 17:56:55", "link": "http://arxiv.org/abs/2206.02230v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chat, Shift and Perform: Bridging the Gap between Task-oriented and\n  Non-task-oriented Dialog Systems", "abstract": "We propose CASPER (ChAt, Shift and PERform), a novel dialog system consisting\nof three types of dialog models: chatter, shifter, and performer. Shifter,\nwhich is designed for topic switching, enables a seamless flow of dialog from\nopen-domain chat- to task-oriented dialog. In a user study, CASPER gave a\nbetter impression in terms of naturalness of response, lack of forced topic\nswitching, and satisfaction compared with a baseline dialog system trained in\nan end-to-end manner. In an ablation study, we found that naturalness of\nresponse, dialog satisfaction, and task-elicitation rate improved compared with\nwhen shifter was removed from CASPER, indicating that topic shift with shifter\nsupports the introduction of natural task-oriented dialog.", "published": "2022-06-05 05:00:18", "link": "http://arxiv.org/abs/2206.11813v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Offline RL for Natural Language Generation with Implicit Language Q\n  Learning", "abstract": "Large language models distill broad knowledge from text corpora. However,\nthey can be inconsistent when it comes to completing user specified tasks. This\nissue can be addressed by finetuning such models via supervised learning on\ncurated datasets, or via reinforcement learning. In this work, we propose a\nnovel offline RL method, implicit language Q-learning (ILQL), designed for use\non language models, that combines both the flexible utility maximization\nframework of RL algorithms with the ability of supervised learning to leverage\npreviously collected data, as well as its simplicity and stability. Our method\nemploys a combination of value conservatism alongside an implicit dataset\nsupport constraint in learning value functions, which are then used to guide\nlanguage model generations towards maximizing user-specified utility functions.\nIn addition to empirically validating ILQL, we present a detailed empirical\nanalysis of situations where offline RL can be useful in natural language\ngeneration settings, demonstrating how it can be a more effective utility\noptimizer than prior approaches for end-to-end dialogue, and how it can\neffectively optimize high variance reward functions based on subjective\njudgement, such as whether to label a comment as toxic or not.", "published": "2022-06-05 18:38:42", "link": "http://arxiv.org/abs/2206.11871v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lip-Listening: Mixing Senses to Understand Lips using Cross Modality\n  Knowledge Distillation for Word-Based Models", "abstract": "In this work, we propose a technique to transfer speech recognition\ncapabilities from audio speech recognition systems to visual speech\nrecognizers, where our goal is to utilize audio data during lipreading model\ntraining. Impressive progress in the domain of speech recognition has been\nexhibited by audio and audio-visual systems. Nevertheless, there is still much\nto be explored with regards to visual speech recognition systems due to the\nvisual ambiguity of some phonemes. To this end, the development of visual\nspeech recognition models is crucial given the instability of audio models. The\nmain contributions of this work are i) building on recent state-of-the-art\nword-based lipreading models by integrating sequence-level and frame-level\nKnowledge Distillation (KD) to their systems; ii) leveraging audio data during\ntraining visual models, a feat which has not been utilized in prior word-based\nwork; iii) proposing the Gaussian-shaped averaging in frame-level KD, as an\nefficient technique that aids the model in distilling knowledge at the sequence\nmodel encoder. This work proposes a novel and competitive architecture for\nlip-reading, as we demonstrate a noticeable improvement in performance, setting\na new benchmark equals to 88.64% on the LRW dataset.", "published": "2022-06-05 15:47:54", "link": "http://arxiv.org/abs/2207.05692v1", "categories": ["cs.MM", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for\n  Text-to-Speech", "abstract": "Polyphone disambiguation aims to capture accurate pronunciation knowledge\nfrom natural text sequences for reliable Text-to-speech (TTS) systems. However,\nprevious approaches require substantial annotated training data and additional\nefforts from language experts, making it difficult to extend high-quality\nneural TTS systems to out-of-domain daily conversations and countless languages\nworldwide. This paper tackles the polyphone disambiguation problem from a\nconcise and novel perspective: we propose Dict-TTS, a semantic-aware generative\ntext-to-speech model with an online website dictionary (the existing prior\ninformation in the natural language). Specifically, we design a\nsemantics-to-pronunciation attention (S2PA) module to match the semantic\npatterns between the input text sequence and the prior semantics in the\ndictionary and obtain the corresponding pronunciations; The S2PA module can be\neasily trained with the end-to-end TTS model without any annotated phoneme\nlabels. Experimental results in three languages show that our model outperforms\nseveral strong baseline models in terms of pronunciation accuracy and improves\nthe prosody modeling of TTS systems. Further extensive analyses demonstrate\nthat each design in Dict-TTS is effective. The code is available at\n\\url{https://github.com/Zain-Jiang/Dict-TTS}.", "published": "2022-06-05 10:50:34", "link": "http://arxiv.org/abs/2206.02147v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Variable-rate hierarchical CPC leads to acoustic unit discovery in\n  speech", "abstract": "The success of deep learning comes from its ability to capture the\nhierarchical structure of data by learning high-level representations defined\nin terms of low-level ones. In this paper we explore self-supervised learning\nof hierarchical representations of speech by applying multiple levels of\nContrastive Predictive Coding (CPC). We observe that simply stacking two CPC\nmodels does not yield significant improvements over single-level architectures.\nInspired by the fact that speech is often described as a sequence of discrete\nunits unevenly distributed in time, we propose a model in which the output of a\nlow-level CPC module is non-uniformly downsampled to directly minimize the loss\nof a high-level CPC module. The latter is designed to also enforce a prior of\nseparability and discreteness in its representations by enforcing dissimilarity\nof successive high-level representations through focused negative sampling, and\nby quantization of the prediction targets. Accounting for the structure of the\nspeech signal improves upon single-level CPC features and enhances the\ndisentanglement of the learned representations, as measured by downstream\nspeech recognition tasks, while resulting in a meaningful segmentation of the\nsignal that closely resembles phone boundaries.", "published": "2022-06-05 16:18:27", "link": "http://arxiv.org/abs/2206.02211v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sampling Frequency Independent Dialogue Separation", "abstract": "In some DNNs for audio source separation, the relevant model parameters are\nindependent of the sampling frequency of the audio used for training.\nConsidering the application of dialogue separation, this is shown for two DNN\narchitectures: a U-Net and a fully-convolutional model. The models are trained\nwith audio sampled at 8 kHz. The learned parameters are transferred to models\nfor processing audio at 48 kHz. The separated audio sources are compared with\nthe ones produced by the same model architectures trained with 48 kHz versions\nof the same training data. A listening test and computational measures show\nthat there is no significant perceptual difference between the models trained\nwith 8 kHz or with 48 kHz. This transferability of the learned parameters\nallows for a faster and computationally less costly training. It also enables\nusing training datasets available at a lower sampling frequency than the one\nneeded by the application at hand, or using data collections with multiple\nsampling frequencies.", "published": "2022-06-05 08:40:07", "link": "http://arxiv.org/abs/2206.02124v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Geometrically-Motivated Primary-Ambient Decomposition With\n  Center-Channel Extraction", "abstract": "A geometrically-motivated method for primary-ambient decomposition is\nproposed and evaluated in an up-mixing application. The method consists of two\nsteps, accommodating a particularly intuitive explanation. The first step\nconsists of signal-adaptive rotations applied on the input stereo scene, which\ntranslate the primary sound sources into the center of the rotated scene. The\nsecond step applies a center-channel extraction method, based on a simple\nsignal model and optimal in the mean-squared-error sense. The performance is\nevaluated by using the estimated ambient component to enable surround sound\nstarting from real-world stereo signals. The participants in the reported\nlistening test are asked to adjust the audio scene envelopment and find the\naudio settings that pleases them the most. The possibility for up-mixing\nenabled by the proposed method is used extensively, and the user satisfaction\nis significantly increased compared to the original stereo mix.", "published": "2022-06-05 08:45:00", "link": "http://arxiv.org/abs/2206.02125v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "M2FNet: Multi-modal Fusion Network for Emotion Recognition in\n  Conversation", "abstract": "Emotion Recognition in Conversations (ERC) is crucial in developing\nsympathetic human-machine interaction. In conversational videos, emotion can be\npresent in multiple modalities, i.e., audio, video, and transcript. However,\ndue to the inherent characteristics of these modalities, multi-modal ERC has\nalways been considered a challenging undertaking. Existing ERC research focuses\nmainly on using text information in a discussion, ignoring the other two\nmodalities. We anticipate that emotion recognition accuracy can be improved by\nemploying a multi-modal approach. Thus, in this study, we propose a Multi-modal\nFusion Network (M2FNet) that extracts emotion-relevant features from visual,\naudio, and text modality. It employs a multi-head attention-based fusion\nmechanism to combine emotion-rich latent representations of the input data. We\nintroduce a new feature extractor to extract latent features from the audio and\nvisual modality. The proposed feature extractor is trained with a novel\nadaptive margin-based triplet loss function to learn emotion-relevant features\nfrom the audio and visual data. In the domain of ERC, the existing methods\nperform well on one benchmark dataset but not on others. Our results show that\nthe proposed M2FNet architecture outperforms all other methods in terms of\nweighted average F1 score on well-known MELD and IEMOCAP datasets and sets a\nnew state-of-the-art performance in ERC.", "published": "2022-06-05 14:18:58", "link": "http://arxiv.org/abs/2206.02187v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention\n  Guided Heterogeneous Translator", "abstract": "Understanding the underlying relationship between tongue and oropharyngeal\nmuscle deformation seen in tagged-MRI and intelligible speech plays an\nimportant role in advancing speech motor control theories and treatment of\nspeech related-disorders. Because of their heterogeneous representations,\nhowever, direct mapping between the two modalities -- i.e., two-dimensional\n(mid-sagittal slice) plus time tagged-MRI sequence and its corresponding\none-dimensional waveform -- is not straightforward. Instead, we resort to\ntwo-dimensional spectrograms as an intermediate representation, which contains\nboth pitch and resonance, from which to develop an end-to-end deep learning\nframework to translate from a sequence of tagged-MRI to its corresponding audio\nwaveform with limited dataset size.~Our framework is based on a novel fully\nconvolutional asymmetry translator with guidance of a self residual attention\nstrategy to specifically exploit the moving muscular structures during\nspeech.~In addition, we leverage a pairwise correlation of the samples with the\nsame utterances with a latent space representation disentanglement\nstrategy.~Furthermore, we incorporate an adversarial training approach with\ngenerative adversarial networks to offer improved realism on our generated\nspectrograms.~Our experimental results, carried out with a total of 63\ntagged-MRI sequences alongside speech acoustics, showed that our framework\nenabled the generation of clear audio waveforms from a sequence of tagged-MRI,\nsurpassing competing methods. Thus, our framework provides the great potential\nto help better understand the relationship between the two modalities.", "published": "2022-06-05 23:08:34", "link": "http://arxiv.org/abs/2206.02284v3", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models", "abstract": "We present a novel way of conditioning a pretrained denoising diffusion\nspeech model to produce speech in the voice of a novel person unseen during\ntraining. The method requires a short (~3 seconds) sample from the target\nperson, and generation is steered at inference time, without any training\nsteps. At the heart of the method lies a sampling process that combines the\nestimation of the denoising model with a low-pass version of the new speaker's\nsample. The objective and subjective evaluations show that our sampling method\ncan generate a voice similar to that of the target speaker in terms of\nfrequency, with an accuracy comparable to state-of-the-art methods, and without\ntraining.", "published": "2022-06-05 19:45:29", "link": "http://arxiv.org/abs/2206.02246v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
