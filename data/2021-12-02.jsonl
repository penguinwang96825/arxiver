{"title": "Context-Dependent Semantic Parsing for Temporal Relation Extraction", "abstract": "Extracting temporal relations among events from unstructured text has\nextensive applications, such as temporal reasoning and question answering.\nWhile it is difficult, recent development of Neural-symbolic methods has shown\npromising results on solving similar tasks. Current temporal relation\nextraction methods usually suffer from limited expressivity and inconsistent\nrelation inference. For example, in TimeML annotations, the concept of\nintersection is absent. Additionally, current methods do not guarantee the\nconsistency among the predicted annotations. In this work, we propose SMARTER,\na neural semantic parser, to extract temporal information in text effectively.\nSMARTER parses natural language to an executable logical form representation,\nbased on a custom typed lambda calculus. In the training phase, dynamic\nprogramming on denotations (DPD) technique is used to provide weak supervision\non logical forms. In the inference phase, SMARTER generates a temporal relation\ngraph by executing the logical form. As a result, our neural semantic parser\nproduces logical forms capturing the temporal information of text precisely.\nThe accurate logical form representations of an event given the context ensure\nthe correctness of the extracted relations.", "published": "2021-12-02 00:29:21", "link": "http://arxiv.org/abs/2112.00894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DKPLM: Decomposable Knowledge-enhanced Pre-trained Language Model for\n  Natural Language Understanding", "abstract": "Knowledge-Enhanced Pre-trained Language Models (KEPLMs) are pre-trained\nmodels with relation triples injecting from knowledge graphs to improve\nlanguage understanding abilities. To guarantee effective knowledge injection,\nprevious studies integrate models with knowledge encoders for representing\nknowledge retrieved from knowledge graphs. The operations for knowledge\nretrieval and encoding bring significant computational burdens, restricting the\nusage of such models in real-world applications that require high inference\nspeed. In this paper, we propose a novel KEPLM named DKPLM that Decomposes\nKnowledge injection process of the Pre-trained Language Models in pre-training,\nfine-tuning and inference stages, which facilitates the applications of KEPLMs\nin real-world scenarios. Specifically, we first detect knowledge-aware\nlong-tail entities as the target for knowledge injection, enhancing the KEPLMs'\nsemantic understanding abilities and avoiding injecting redundant information.\nThe embeddings of long-tail entities are replaced by \"pseudo token\nrepresentations\" formed by relevant knowledge triples. We further design the\nrelational knowledge decoding task for pre-training to force the models to\ntruly understand the injected knowledge by relation triple reconstruction.\nExperiments show that our model outperforms other KEPLMs significantly over\nzero-shot knowledge probing tasks and multiple knowledge-aware language\nunderstanding tasks. We further show that DKPLM has a higher inference speed\nthan other competing models due to the decomposing mechanism.", "published": "2021-12-02 08:19:42", "link": "http://arxiv.org/abs/2112.01047v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Consensus to Disagreement: Multi-Teacher Distillation for\n  Semi-Supervised Relation Extraction", "abstract": "Lack of labeled data is a main obstacle in relation extraction.\nSemi-supervised relation extraction (SSRE) has been proven to be a promising\nway for this problem through annotating unlabeled samples as additional\ntraining data. Almost all prior researches along this line adopt multiple\nmodels to make the annotations more reliable by taking the intersection set of\npredicted results from these models. However, the difference set, which\ncontains rich information about unlabeled data, has been long neglected by\nprior studies.\n  In this paper, we propose to learn not only from the consensus but also the\ndisagreement among different models in SSRE. To this end, we develop a simple\nand general multi-teacher distillation (MTD) framework, which can be easily\nintegrated into any existing SSRE methods. Specifically, we first let the\nteachers correspond to the multiple models and select the samples in the\nintersection set of the last iteration in SSRE methods to augment labeled data\nas usual. We then transfer the class distributions for samples in the\ndifference set as soft labels to guide the student. We finally perform\nprediction using the trained student model. Experimental results on two public\ndatasets demonstrate that our framework significantly promotes the performance\nof the base SSRE methods with pretty low computational cost.", "published": "2021-12-02 08:20:23", "link": "http://arxiv.org/abs/2112.01048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotions are Subtle: Learning Sentiment Based Text Representations Using\n  Contrastive Learning", "abstract": "Contrastive learning techniques have been widely used in the field of\ncomputer vision as a means of augmenting datasets. In this paper, we extend the\nuse of these contrastive learning embeddings to sentiment analysis tasks and\ndemonstrate that fine-tuning on these embeddings provides an improvement over\nfine-tuning on BERT-based embeddings to achieve higher benchmarks on the task\nof sentiment analysis when evaluated on the DynaSent dataset. We also explore\nhow our fine-tuned models perform on cross-domain benchmark datasets.\nAdditionally, we explore upsampling techniques to achieve a more balanced class\ndistribution to make further improvements on our benchmark tasks.", "published": "2021-12-02 08:29:26", "link": "http://arxiv.org/abs/2112.01054v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CO2Sum:Contrastive Learning for Factual-Consistent Abstractive\n  Summarization", "abstract": "Generating factual-consistent summaries is a challenging task for abstractive\nsummarization. Previous works mainly encode factual information or perform\npost-correct/rank after decoding. In this paper, we provide a\nfactual-consistent solution from the perspective of contrastive learning, which\nis a natural extension of previous works. We propose CO2Sum (Contrastive for\nConsistency), a contrastive learning scheme that can be easily applied on\nsequence-to-sequence models for factual-consistent abstractive summarization,\nproving that the model can be fact-aware without modifying the architecture.\nCO2Sum applies contrastive learning on the encoder, which can help the model be\naware of the factual information contained in the input article, or performs\ncontrastive learning on the decoder, which makes the model to generate\nfactual-correct output summary. What's more, these two schemes are orthogonal\nand can be combined to further improve faithfulness. Comprehensive experiments\non public benchmarks demonstrate that CO2Sum improves the faithfulness on large\npre-trained language models and reaches competitive results compared to other\nstrong factual-consistent summarization baselines.", "published": "2021-12-02 11:52:01", "link": "http://arxiv.org/abs/2112.01147v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Generating Citation Sentences for Multiple References with\n  Intent Control", "abstract": "Machine-generated citation sentences can aid automated scientific literature\nreview and assist article writing. Current methods in generating citation text\nwere limited to single citation generation using the citing document and a\ncited document as input. However, in real-world situations, writers often\nsummarize several studies in one sentence or discuss relevant information\nacross the entire paragraph. In addition, multiple citation intents have been\npreviously identified, implying that writers may need control over the intents\nof generated sentences to cover different scenarios. Therefore, this work\nfocuses on generating multiple citations and releasing a newly collected\ndataset named CiteMI to drive the future research. We first build a novel\ngeneration model with the Fusion-in-Decoder approach to cope with multiple long\ninputs. Second, we incorporate the predicted citation intents into training for\nintent control. The experiments demonstrate that the proposed approaches\nprovide much more comprehensive features for generating citation sentences.", "published": "2021-12-02 15:32:24", "link": "http://arxiv.org/abs/2112.01332v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KPDrop: Improving Absent Keyphrase Generation", "abstract": "Keyphrase generation is the task of generating phrases (keyphrases) that\nsummarize the main topics of a given document. Keyphrases can be either present\nor absent from the given document. While the extraction of present keyphrases\nhas received much attention in the past, only recently a stronger focus has\nbeen placed on the generation of absent keyphrases. However, generating absent\nkeyphrases is challenging; even the best methods show only a modest degree of\nsuccess. In this paper, we propose a model-agnostic approach called keyphrase\ndropout (or KPDrop) to improve absent keyphrase generation. In this approach,\nwe randomly drop present keyphrases from the document and turn them into\nartificial absent keyphrases during training. We test our approach extensively\nand show that it consistently improves the absent performance of strong\nbaselines in both supervised and resource-constrained semi-supervised settings.", "published": "2021-12-02 18:25:56", "link": "http://arxiv.org/abs/2112.01476v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Impact of 9/11 on The Simpsons through Natural\n  Language Processing", "abstract": "The impact of real world events on fictional media is particularly apparent\nin the American cartoon series The Simpsons. While there are often very direct\npop culture references evident in the dialogue and visual gags of the show,\nsubtle changes in tone or sentiment may not be so obvious. Our aim was to use\nNatural Language Processing to attempt to search for changes in word frequency,\ntopic, and sentiment before and after the September 11 terrorist attacks in New\nYork. No clear trend change was seen, there was a slight decrease in the\naverage sentiment over time around the relevant period between 2000 and 2002,\nbut the scripts still maintained an overall positive value, indicating that the\ncomedic nature of The Simpsons did not wane particularly significantly. The\nexploration of other social issues and even specific character statistics is\nneeded to bolster the findings here.", "published": "2021-12-02 03:45:10", "link": "http://arxiv.org/abs/2112.03025v1", "categories": ["cs.CL", "68T50 (Primary) 03B65, 91F20 (Secondary)", "I.7"], "primary_category": "cs.CL"}
{"title": "Inferring Prototypes for Multi-Label Few-Shot Image Classification with\n  Word Vector Guided Attention", "abstract": "Multi-label few-shot image classification (ML-FSIC) is the task of assigning\ndescriptive labels to previously unseen images, based on a small number of\ntraining examples. A key feature of the multi-label setting is that images\noften have multiple labels, which typically refer to different regions of the\nimage. When estimating prototypes, in a metric-based setting, it is thus\nimportant to determine which regions are relevant for which labels, but the\nlimited amount of training data makes this highly challenging. As a solution,\nin this paper we propose to use word embeddings as a form of prior knowledge\nabout the meaning of the labels. In particular, visual prototypes are obtained\nby aggregating the local feature maps of the support images, using an attention\nmechanism that relies on the label embeddings. As an important advantage, our\nmodel can infer prototypes for unseen labels without the need for fine-tuning\nany model parameters, which demonstrates its strong generalization abilities.\nExperiments on COCO and PASCAL VOC furthermore show that our model\nsubstantially improves the current state-of-the-art.", "published": "2021-12-02 07:59:11", "link": "http://arxiv.org/abs/2112.01037v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Perform Like an Engine: A Closed-Loop Neural-Symbolic Learning Framework\n  for Knowledge Graph Inference", "abstract": "Knowledge graph (KG) inference aims to address the natural incompleteness of\nKGs, including rule learning-based and KG embedding (KGE) models. However, the\nrule learning-based models suffer from low efficiency and generalization while\nKGE models lack interpretability. To address these challenges, we propose a\nnovel and effective closed-loop neural-symbolic learning framework EngineKG via\nincorporating our developed KGE and rule learning modules. KGE module exploits\nsymbolic rules and paths to enhance the semantic association between entities\nand relations for improving KG embeddings and interpretability. A novel rule\npruning mechanism is proposed in the rule learning module by leveraging paths\nas initial candidate rules and employing KG embeddings together with concepts\nfor extracting more high-quality rules. Experimental results on four real-world\ndatasets show that our model outperforms the relevant baselines on link\nprediction tasks, demonstrating the superiority of our KG inference model in a\nneural-symbolic learning fashion.", "published": "2021-12-02 08:02:59", "link": "http://arxiv.org/abs/2112.01040v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Syntax Customized Video Captioning by Imitating Exemplar Sentences", "abstract": "Enhancing the diversity of sentences to describe video contents is an\nimportant problem arising in recent video captioning research. In this paper,\nwe explore this problem from a novel perspective of customizing video captions\nby imitating exemplar sentence syntaxes. Specifically, given a video and any\nsyntax-valid exemplar sentence, we introduce a new task of Syntax Customized\nVideo Captioning (SCVC) aiming to generate one caption which not only\nsemantically describes the video contents but also syntactically imitates the\ngiven exemplar sentence. To tackle the SCVC task, we propose a novel video\ncaptioning model, where a hierarchical sentence syntax encoder is firstly\ndesigned to extract the syntactic structure of the exemplar sentence, then a\nsyntax conditioned caption decoder is devised to generate the syntactically\nstructured caption expressing video semantics. As there is no available syntax\ncustomized groundtruth video captions, we tackle such a challenge by proposing\na new training strategy, which leverages the traditional pairwise video\ncaptioning data and our collected exemplar sentences to accomplish the model\nlearning. Extensive experiments, in terms of semantic, syntactic, fluency, and\ndiversity evaluations, clearly demonstrate our model capability to generate\nsyntax-varied and semantics-coherent video captions that well imitate different\nexemplar sentences with enriched diversities.", "published": "2021-12-02 09:08:09", "link": "http://arxiv.org/abs/2112.01062v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Extract Free Dense Labels from CLIP", "abstract": "Contrastive Language-Image Pre-training (CLIP) has made a remarkable\nbreakthrough in open-vocabulary zero-shot image recognition. Many recent\nstudies leverage the pre-trained CLIP models for image-level classification and\nmanipulation. In this paper, we wish examine the intrinsic potential of CLIP\nfor pixel-level dense prediction, specifically in semantic segmentation. To\nthis end, with minimal modification, we show that MaskCLIP yields compelling\nsegmentation results on open concepts across various datasets in the absence of\nannotations and fine-tuning. By adding pseudo labeling and self-training,\nMaskCLIP+ surpasses SOTA transductive zero-shot semantic segmentation methods\nby large margins, e.g., mIoUs of unseen classes on PASCAL VOC/PASCAL\nContext/COCO Stuff are improved from 35.6/20.7/30.3 to 86.1/66.7/54.7. We also\ntest the robustness of MaskCLIP under input corruption and evaluate its\ncapability in discriminating fine-grained objects and novel concepts. Our\nfinding suggests that MaskCLIP can serve as a new reliable source of\nsupervision for dense prediction tasks to achieve annotation-free segmentation.\nSource code is available at https://github.com/chongzhou96/MaskCLIP.", "published": "2021-12-02 09:23:01", "link": "http://arxiv.org/abs/2112.01071v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Controllable Video Captioning with an Exemplar Sentence", "abstract": "In this paper, we investigate a novel and challenging task, namely\ncontrollable video captioning with an exemplar sentence. Formally, given a\nvideo and a syntactically valid exemplar sentence, the task aims to generate\none caption which not only describes the semantic contents of the video, but\nalso follows the syntactic form of the given exemplar sentence. In order to\ntackle such an exemplar-based video captioning task, we propose a novel Syntax\nModulated Caption Generator (SMCG) incorporated in an\nencoder-decoder-reconstructor architecture. The proposed SMCG takes video\nsemantic representation as an input, and conditionally modulates the gates and\ncells of long short-term memory network with respect to the encoded syntactic\ninformation of the given exemplar sentence. Therefore, SMCG is able to control\nthe states for word prediction and achieve the syntax customized caption\ngeneration. We conduct experiments by collecting auxiliary exemplar sentences\nfor two public video captioning datasets. Extensive experimental results\ndemonstrate the effectiveness of our approach on generating syntax controllable\nand semantic preserved video captions. By providing different exemplar\nsentences, our approach is capable of producing different captions with various\nsyntactic structures, thus indicating a promising way to strengthen the\ndiversity of video captioning.", "published": "2021-12-02 09:24:45", "link": "http://arxiv.org/abs/2112.01073v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AST-Transformer: Encoding Abstract Syntax Trees Efficiently for Code\n  Summarization", "abstract": "Code summarization aims to generate brief natural language descriptions for\nsource code. As source code is highly structured and follows strict programming\nlanguage grammars, its Abstract Syntax Tree (AST) is often leveraged to inform\nthe encoder about the structural information. However, ASTs are usually much\nlonger than the source code. Current approaches ignore the size limit and\nsimply feed the whole linearized AST into the encoder. To address this problem,\nwe propose AST-Transformer to efficiently encode tree-structured ASTs.\nExperiments show that AST-Transformer outperforms the state-of-arts by a\nsubstantial margin while being able to reduce $90\\sim95\\%$ of the computational\ncomplexity in the encoding process.", "published": "2021-12-02 12:57:22", "link": "http://arxiv.org/abs/2112.01184v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "How not to Lie with a Benchmark: Rearranging NLP Leaderboards", "abstract": "Comparison with a human is an essential requirement for a benchmark for it to\nbe a reliable measurement of model capabilities. Nevertheless, the methods for\nmodel comparison could have a fundamental flaw - the arithmetic mean of\nseparate metrics is used for all tasks of different complexity, different size\nof test and training sets.\n  In this paper, we examine popular NLP benchmarks' overall scoring methods and\nrearrange the models by geometric and harmonic mean (appropriate for averaging\nrates) according to their reported results. We analyze several popular\nbenchmarks including GLUE, SuperGLUE, XGLUE, and XTREME. The analysis shows\nthat e.g. human level on SuperGLUE is still not reached, and there is still\nroom for improvement for the current models.", "published": "2021-12-02 15:40:52", "link": "http://arxiv.org/abs/2112.01342v1", "categories": ["cs.CL", "cs.AI", "68-06, 68T50, 68T01", "G.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late\n  Interaction", "abstract": "Neural information retrieval (IR) has greatly advanced search and other\nknowledge-intensive language tasks. While many neural IR methods encode queries\nand documents into single-vector representations, late interaction models\nproduce multi-vector representations at the granularity of each token and\ndecompose relevance modeling into scalable token-level computations. This\ndecomposition has been shown to make late interaction more effective, but it\ninflates the space footprint of these models by an order of magnitude. In this\nwork, we introduce ColBERTv2, a retriever that couples an aggressive residual\ncompression mechanism with a denoised supervision strategy to simultaneously\nimprove the quality and space footprint of late interaction. We evaluate\nColBERTv2 across a wide range of benchmarks, establishing state-of-the-art\nquality within and outside the training domain while reducing the space\nfootprint of late interaction models by 6--10$\\times$.", "published": "2021-12-02 18:38:50", "link": "http://arxiv.org/abs/2112.01488v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation", "abstract": "Assessing the quality of natural language generation systems through human\nannotation is very expensive. Additionally, human annotation campaigns are\ntime-consuming and include non-reusable human labour. In practice, researchers\nrely on automatic metrics as a proxy of quality. In the last decade, many\nstring-based metrics (e.g., BLEU) have been introduced. However, such metrics\nusually rely on exact matches and thus, do not robustly handle synonyms. In\nthis paper, we introduce InfoLM a family of untrained metrics that can be\nviewed as a string-based metric that addresses the aforementioned flaws thanks\nto a pre-trained masked language model. This family of metrics also makes use\nof information measures allowing the adaptation of InfoLM to various evaluation\ncriteria. Using direct assessment, we demonstrate that InfoLM achieves\nstatistically significant improvement and over $10$ points of correlation gains\nin many configurations on both summarization and data2text generation.", "published": "2021-12-02 20:09:29", "link": "http://arxiv.org/abs/2112.01589v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PLSUM: Generating PT-BR Wikipedia by Summarizing Multiple Websites", "abstract": "Wikipedia is an important free source of intelligible knowledge. Despite\nthat, Brazilian Portuguese Wikipedia still lacks descriptions for many\nsubjects. In an effort to expand the Brazilian Wikipedia, we contribute PLSum,\na framework for generating wiki-like abstractive summaries from multiple\ndescriptive websites. The framework has an extractive stage followed by an\nabstractive one. In particular, for the abstractive stage, we fine-tune and\ncompare two recent variations of the Transformer neural network, PTT5, and\nLongformer. To fine-tune and evaluate the model, we created a dataset with\nthousands of examples, linking reference websites to Wikipedia. Our results\nshow that it is possible to generate meaningful abstractive summaries from\nBrazilian Portuguese web content.", "published": "2021-12-02 20:16:17", "link": "http://arxiv.org/abs/2112.01591v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluator for Emotionally Consistent Chatbots", "abstract": "One challenge for evaluating current sequence- or dialogue-level chatbots,\nsuch as Empathetic Open-domain Conversation Models, is to determine whether the\nchatbot performs in an emotionally consistent way. The most recent work only\nevaluates on the aspects of context coherence, language fluency, response\ndiversity, or logical self-consistency between dialogues. This work proposes\ntraining an evaluator to determine the emotional consistency of chatbots.", "published": "2021-12-02 21:47:29", "link": "http://arxiv.org/abs/2112.01616v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MultiVerS: Improving scientific claim verification with weak supervision\n  and full-document context", "abstract": "The scientific claim verification task requires an NLP system to label\nscientific documents which Support or Refute an input claim, and to select\nevidentiary sentences (or rationales) justifying each predicted label. In this\nwork, we present MultiVerS, which predicts a fact-checking label and identifies\nrationales in a multitask fashion based on a shared encoding of the claim and\nfull document context. This approach accomplishes two key modeling goals.\nFirst, it ensures that all relevant contextual information is incorporated into\neach labeling decision. Second, it enables the model to learn from instances\nannotated with a document-level fact-checking label, but lacking sentence-level\nrationales. This allows MultiVerS to perform weakly-supervised domain\nadaptation by training on scientific documents labeled using high-precision\nheuristics. Our approach outperforms two competitive baselines on three\nscientific claim verification datasets, with particularly strong performance in\nzero / few-shot domain adaptation experiments. Our code and data are available\nat https://github.com/dwadden/multivers.", "published": "2021-12-02 23:37:16", "link": "http://arxiv.org/abs/2112.01640v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transfer Learning in Conversational Analysis through Reusing\n  Preprocessing Data as Supervisors", "abstract": "Conversational analysis systems are trained using noisy human labels and\noften require heavy preprocessing during multi-modal feature extraction. Using\nnoisy labels in single-task learning increases the risk of over-fitting.\nAuxiliary tasks could improve the performance of the primary task learning\nduring the same training -- this approach sits in the intersection of transfer\nlearning and multi-task learning (MTL). In this paper, we explore how the\npreprocessed data used for feature engineering can be re-used as auxiliary\ntasks, thereby promoting the productive use of data. Our main contributions\nare: (1) the identification of sixteen beneficially auxiliary tasks, (2)\nstudying the method of distributing learning capacity between the primary and\nauxiliary tasks, and (3) studying the relative supervision hierarchy between\nthe primary and auxiliary tasks. Extensive experiments on IEMOCAP and SEMAINE\ndata validate the improvements over single-task approaches, and suggest that it\nmay generalize across multiple primary tasks.", "published": "2021-12-02 08:40:42", "link": "http://arxiv.org/abs/2112.03032v1", "categories": ["cs.CL", "cs.LG", "I.2.6"], "primary_category": "cs.CL"}
{"title": "Relational Graph Learning for Grounded Video Description Generation", "abstract": "Grounded video description (GVD) encourages captioning models to attend to\nappropriate video regions (e.g., objects) dynamically and generate a\ndescription. Such a setting can help explain the decisions of captioning models\nand prevents the model from hallucinating object words in its description.\nHowever, such design mainly focuses on object word generation and thus may\nignore fine-grained information and suffer from missing visual concepts.\nMoreover, relational words (e.g., \"jump left or right\") are usual\nspatio-temporal inference results, i.e., these words cannot be grounded on\ncertain spatial regions. To tackle the above limitations, we design a novel\nrelational graph learning framework for GVD, in which a language-refined scene\ngraph representation is designed to explore fine-grained visual concepts.\nFurthermore, the refined graph can be regarded as relational inductive\nknowledge to assist captioning models in selecting the relevant information it\nneeds to generate correct words. We validate the effectiveness of our model\nthrough automatic metrics and human evaluation, and the results indicate that\nour approach can generate more fine-grained and accurate description, and it\nsolves the problem of object hallucination to some extent.", "published": "2021-12-02 03:48:45", "link": "http://arxiv.org/abs/2112.00967v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Object-Centric Unsupervised Image Captioning", "abstract": "Image captioning is a longstanding problem in the field of computer vision\nand natural language processing. To date, researchers have produced impressive\nstate-of-the-art performance in the age of deep learning. Most of these\nstate-of-the-art, however, requires large volume of annotated image-caption\npairs in order to train their models. When given an image dataset of interests,\npractitioner needs to annotate the caption for each image in the training set\nand this process needs to happen for each newly collected image dataset. In\nthis paper, we explore the task of unsupervised image captioning which utilizes\nunpaired images and texts to train the model so that the texts can come from\ndifferent sources than the images. A main school of research on this topic that\nhas been shown to be effective is to construct pairs from the images and texts\nin the training set according to their overlap of objects. Unlike in the\nsupervised setting, these constructed pairings are however not guaranteed to\nhave fully overlapping set of objects. Our work in this paper overcomes this by\nharvesting objects corresponding to a given sentence from the training set,\neven if they don't belong to the same image. When used as input to a\ntransformer, such mixture of objects enables larger if not full object\ncoverage, and when supervised by the corresponding sentence, produced results\nthat outperform current state of the art unsupervised methods by a significant\nmargin. Building upon this finding, we further show that (1) additional\ninformation on relationship between objects and attributes of objects also\nhelps in boosting performance; and (2) our method also extends well to\nnon-English image captioning, which usually suffers from a scarcer level of\nannotations. Our findings are supported by strong empirical results. Our code\nis available at https://github.com/zihangm/obj-centric-unsup-caption.", "published": "2021-12-02 03:56:09", "link": "http://arxiv.org/abs/2112.00969v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Consensus Graph Representation Learning for Better Grounded Image\n  Captioning", "abstract": "The contemporary visual captioning models frequently hallucinate objects that\nare not actually in a scene, due to the visual misclassification or\nover-reliance on priors that resulting in the semantic inconsistency between\nthe visual information and the target lexical words. The most common way is to\nencourage the captioning model to dynamically link generated object words or\nphrases to appropriate regions of the image, i.e., the grounded image\ncaptioning (GIC). However, GIC utilizes an auxiliary task (grounding objects)\nthat has not solved the key issue of object hallucination, i.e., the semantic\ninconsistency. In this paper, we take a novel perspective on the issue above -\nexploiting the semantic coherency between the visual and language modalities.\nSpecifically, we propose the Consensus Rraph Representation Learning framework\n(CGRL) for GIC that incorporates a consensus representation into the grounded\ncaptioning pipeline. The consensus is learned by aligning the visual graph\n(e.g., scene graph) to the language graph that consider both the nodes and\nedges in a graph. With the aligned consensus, the captioning model can capture\nboth the correct linguistic characteristics and visual relevance, and then\ngrounding appropriate image regions further. We validate the effectiveness of\nour model, with a significant decline in object hallucination (-9% CHAIRi) on\nthe Flickr30k Entities dataset. Besides, our CGRL also evaluated by several\nautomatic metrics and human evaluation, the results indicate that the proposed\napproach can simultaneously improve the performance of image captioning (+2.9\nCider) and grounding (+2.3 F1LOC).", "published": "2021-12-02 04:17:01", "link": "http://arxiv.org/abs/2112.00974v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Controllability of Educational Question Generation by Keyword\n  Provision", "abstract": "Question Generation (QG) receives increasing research attention in NLP\ncommunity. One motivation for QG is that QG significantly facilitates the\npreparation of educational reading practice and assessments. While the\nsignificant advancement of QG techniques was reported, current QG results are\nnot ideal for educational reading practice assessment in terms of\n\\textit{controllability} and \\textit{question difficulty}. This paper reports\nour results toward the two issues. First, we report a state-of-the-art\nexam-like QG model by advancing the current best model from 11.96 to 20.19 (in\nterms of BLEU 4 score). Second, we propose to investigate a variant of QG\nsetting by allowing users to provide keywords for guiding QG direction. We also\npresent a simple but effective model toward the QG controllability task.\nExperiments are also performed and the results demonstrate the feasibility and\npotentials of improving QG diversity and controllability by the proposed\nkeyword provision QG model.", "published": "2021-12-02 06:54:44", "link": "http://arxiv.org/abs/2112.01012v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Mixture of Expert Based Deep Neural Network for Improved ASR", "abstract": "This paper presents a novel deep learning architecture for acoustic model in\nthe context of Automatic Speech Recognition (ASR), termed as MixNet. Besides\nthe conventional layers, such as fully connected layers in DNN-HMM and memory\ncells in LSTM-HMM, the model uses two additional layers based on Mixture of\nExperts (MoE). The first MoE layer operating at the input is based on\npre-defined broad phonetic classes and the second layer operating at the\npenultimate layer is based on automatically learned acoustic classes. In\nnatural speech, overlap in distribution across different acoustic classes is\ninevitable, which leads to inter-class mis-classification. The ASR accuracy is\nexpected to improve if the conventional architecture of acoustic model is\nmodified to make them more suitable to account for such overlaps. MixNet is\ndeveloped keeping this in mind. Analysis conducted by means of scatter diagram\nverifies that MoE indeed improves the separation between classes that\ntranslates to better ASR accuracy. Experiments are conducted on a large\nvocabulary ASR task which show that the proposed architecture provides 13.6%\nand 10.0% relative reduction in word error rates compared to the conventional\nmodels, namely, DNN and LSTM respectively, trained using sMBR criteria. In\ncomparison to an existing method developed for phone-classification (by Eigen\net al), our proposed method yields a significant improvement.", "published": "2021-12-02 07:26:34", "link": "http://arxiv.org/abs/2112.01025v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Local Citation Recommendation with Hierarchical-Attention Text Encoder\n  and SciBERT-based Reranking", "abstract": "The goal of local citation recommendation is to recommend a missing reference\nfrom the local citation context and optionally also from the global context. To\nbalance the tradeoff between speed and accuracy of citation recommendation in\nthe context of a large-scale paper database, a viable approach is to first\nprefetch a limited number of relevant documents using efficient ranking methods\nand then to perform a fine-grained reranking using more sophisticated models.\nIn that vein, BM25 has been found to be a tough-to-beat approach to\nprefetching, which is why recent work has focused mainly on the reranking step.\nEven so, we explore prefetching with nearest neighbor search among text\nembeddings constructed by a hierarchical attention network. When coupled with a\nSciBERT reranker fine-tuned on local citation recommendation tasks, our\nhierarchical Attention encoder (HAtten) achieves high prefetch recall for a\ngiven number of candidates to be reranked. Consequently, our reranker requires\nfewer prefetch candidates to rerank, yet still achieves state-of-the-art\nperformance on various local citation recommendation datasets such as ACL-200,\nFullTextPeerRead, RefSeer, and arXiv.", "published": "2021-12-02 13:20:26", "link": "http://arxiv.org/abs/2112.01206v3", "categories": ["cs.IR", "cs.CL", "cs.LG", "H.3.3; I.7"], "primary_category": "cs.IR"}
{"title": "ScaleVLAD: Improving Multimodal Sentiment Analysis via Multi-Scale\n  Fusion of Locally Descriptors", "abstract": "Fusion technique is a key research topic in multimodal sentiment analysis.\nThe recent attention-based fusion demonstrates advances over simple\noperation-based fusion. However, these fusion works adopt single-scale, i.e.,\ntoken-level or utterance-level, unimodal representation. Such single-scale\nfusion is suboptimal because that different modality should be aligned with\ndifferent granularities. This paper proposes a fusion model named ScaleVLAD to\ngather multi-Scale representation from text, video, and audio with shared\nVectors of Locally Aggregated Descriptors to improve unaligned multimodal\nsentiment analysis. These shared vectors can be regarded as shared topics to\nalign different modalities. In addition, we propose a self-supervised shifted\nclustering loss to keep the fused feature differentiation among samples. The\nbackbones are three Transformer encoders corresponding to three modalities, and\nthe aggregated features generated from the fusion module are feed to a\nTransformer plus a full connection to finish task predictions. Experiments on\nthree popular sentiment analysis benchmarks, IEMOCAP, MOSI, and MOSEI,\ndemonstrate significant gains over baselines.", "published": "2021-12-02 16:09:33", "link": "http://arxiv.org/abs/2112.01368v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with\n  Self-training", "abstract": "Natural language generation from structured data mainly focuses on\nsurface-level descriptions, suffering from uncontrollable content selection and\nlow fidelity. Previous works leverage logical forms to facilitate logical\nknowledge-conditioned text generation. Though achieving remarkable progress,\nthey are data-hungry, which makes the adoption for real-world applications\nchallenging with limited data. To this end, this paper proposes a unified\nframework for logical knowledge-conditioned text generation in the few-shot\nsetting. With only a few seeds logical forms (e.g., 20/100 shot), our approach\nleverages self-training and samples pseudo logical forms based on content and\nstructure consistency. Experimental results demonstrate that our approach can\nobtain better few-shot performance than baselines.", "published": "2021-12-02 16:49:41", "link": "http://arxiv.org/abs/2112.01404v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+GAN\n  Space Optimization", "abstract": "Generating images from natural language instructions is an intriguing yet\nhighly challenging task. We approach text-to-image generation by combining the\npower of the retrained CLIP representation with an off-the-shelf image\ngenerator (GANs), optimizing in the latent space of GAN to find images that\nachieve maximum CLIP score with the given input text. Compared to traditional\nmethods that train generative models from text to image starting from scratch,\nthe CLIP+GAN approach is training-free, zero shot and can be easily customized\nwith different generators.\n  However, optimizing CLIP score in the GAN space casts a highly challenging\noptimization problem and off-the-shelf optimizers such as Adam fail to yield\nsatisfying results. In this work, we propose a FuseDream pipeline, which\nimproves the CLIP+GAN approach with three key techniques: 1) an AugCLIP score\nwhich robustifies the CLIP objective by introducing random augmentation on\nimage. 2) a novel initialization and over-parameterization strategy for\noptimization which allows us to efficiently navigate the non-convex landscape\nin GAN space. 3) a composed generation technique which, by leveraging a novel\nbi-level optimization formulation, can compose multiple images to extend the\nGAN space and overcome the data-bias.\n  When promoted by different input text, FuseDream can generate high-quality\nimages with varying objects, backgrounds, artistic styles, even novel\ncounterfactual concepts that do not appear in the training data of the GAN we\nuse. Quantitatively, the images generated by FuseDream yield top-level\nInception score and FID score on MS COCO dataset, without additional\narchitecture design or training. Our code is publicly available at\n\\url{https://github.com/gnobitab/FuseDream}.", "published": "2021-12-02 19:27:27", "link": "http://arxiv.org/abs/2112.01573v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Unsupervised Law Article Mining based on Deep Pre-Trained Language\n  Representation Models with Application to the Italian Civil Code", "abstract": "Modeling law search and retrieval as prediction problems has recently emerged\nas a predominant approach in law intelligence. Focusing on the law article\nretrieval task, we present a deep learning framework named LamBERTa, which is\ndesigned for civil-law codes, and specifically trained on the Italian civil\ncode. To our knowledge, this is the first study proposing an advanced approach\nto law article prediction for the Italian legal system based on a BERT\n(Bidirectional Encoder Representations from Transformers) learning framework,\nwhich has recently attracted increased attention among deep learning\napproaches, showing outstanding effectiveness in several natural language\nprocessing and learning tasks. We define LamBERTa models by fine-tuning an\nItalian pre-trained BERT on the Italian civil code or its portions, for law\narticle retrieval as a classification task. One key aspect of our LamBERTa\nframework is that we conceived it to address an extreme classification\nscenario, which is characterized by a high number of classes, the few-shot\nlearning problem, and the lack of test query benchmarks for Italian legal\nprediction tasks. To solve such issues, we define different methods for the\nunsupervised labeling of the law articles, which can in principle be applied to\nany law article code system. We provide insights into the explainability and\ninterpretability of our LamBERTa models, and we present an extensive\nexperimental analysis over query sets of different type, for single-label as\nwell as multi-label evaluation tasks. Empirical evidence has shown the\neffectiveness of LamBERTa, and also its superiority against widely used\ndeep-learning text classifiers and a few-shot learner conceived for an\nattribute-aware prediction task.", "published": "2021-12-02 11:02:00", "link": "http://arxiv.org/abs/2112.03033v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "A higher order Minkowski loss for improved prediction ability of\n  acoustic model in ASR", "abstract": "Conventional automatic speech recognition (ASR) system uses second-order\nminkowski loss during inference time which is suboptimal as it incorporates\nonly first order statistics in posterior estimation [2]. In this paper we have\nproposed higher order minkowski loss (4th Order and 6th Order) during inference\ntime, without any changes during training time. The main contribution of the\npaper is to show that higher order loss uses higher order statistics in\nposterior estimation, which improves the prediction ability of acoustic model\nin ASR system. We have shown mathematically that posterior probability obtained\ndue to higher order loss is function of second order posterior and thus the\nmethod can be incorporated in standard ASR system in an easy manner. It is to\nbe noted that all changes are proposed during test(inference) time, we do not\nmake any change in any training pipeline. Multiple baseline systems namely,\nTDNN1, TDNN2, DNN and LSTM are developed to verify the improvement incurred due\nto higher order minkowski loss. All experiments are conducted on LibriSpeech\ndataset and performance metrics are word error rate (WER) on \"dev-clean\",\n\"test-clean\", \"dev-other\" and \"test-other\" datasets.", "published": "2021-12-02 07:20:08", "link": "http://arxiv.org/abs/2112.01023v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
