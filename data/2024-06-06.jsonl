{"title": "What Makes Language Models Good-enough?", "abstract": "Psycholinguistic research suggests that humans may build a representation of\nlinguistic input that is 'good-enough' for the task at hand. This study\nexamines what architectural features make language models learn human-like\ngood-enough language processing. We focus on the number of layers and\nself-attention heads in Transformers. We create a good-enough language\nprocessing (GELP) evaluation dataset (7,680 examples), which is designed to\ntest the effects of two plausibility types, eight construction types, and three\ndegrees of memory cost on language processing. To annotate GELP, we first\nconduct a crowdsourcing experiment whose design follows prior psycholinguistic\nstudies. Our model evaluation against the annotated GELP then reveals that the\nfull model as well as models with fewer layers and/or self-attention heads\nexhibit a good-enough performance. This result suggests that models with\nshallower depth and fewer heads can learn good-enough language processing.", "published": "2024-06-06 00:51:28", "link": "http://arxiv.org/abs/2406.03666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M-QALM: A Benchmark to Assess Clinical Reading Comprehension and\n  Knowledge Recall in Large Language Models via Question Answering", "abstract": "There is vivid research on adapting Large Language Models (LLMs) to perform a\nvariety of tasks in high-stakes domains such as healthcare. Despite their\npopularity, there is a lack of understanding of the extent and contributing\nfactors that allow LLMs to recall relevant knowledge and combine it with\npresented information in the clinical and biomedical domain: a fundamental\npre-requisite for success on down-stream tasks. Addressing this gap, we use\nMultiple Choice and Abstractive Question Answering to conduct a large-scale\nempirical study on 22 datasets in three generalist and three specialist\nbiomedical sub-domains. Our multifaceted analysis of the performance of 15\nLLMs, further broken down by sub-domain, source of knowledge and model\narchitecture, uncovers success factors such as instruction tuning that lead to\nimproved recall and comprehension. We further show that while recently proposed\ndomain-adapted models may lack adequate knowledge, directly fine-tuning on our\ncollected medical knowledge datasets shows encouraging results, even\ngeneralising to unseen specialist sub-domains. We complement the quantitative\nresults with a skill-oriented manual error analysis, which reveals a\nsignificant gap between the models' capabilities to simply recall necessary\nknowledge and to integrate it with the presented context. To foster research\nand collaboration in this field we share M-QALM, our resources, standardised\nmethodology, and evaluation results, with the research community to facilitate\nfurther advancements in clinical knowledge representation learning within\nlanguage models.", "published": "2024-06-06 02:43:21", "link": "http://arxiv.org/abs/2406.03699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text\n  Classification", "abstract": "With the booming of Large Language Models (LLMs), prompt-learning has become\na promising method mainly researched in various research areas. Recently, many\nattempts based on prompt-learning have been made to improve the performance of\ntext classification. However, most of these methods are based on heuristic\nChain-of-Thought (CoT), and tend to be more complex but less efficient. In this\npaper, we rethink the LLM-based text classification methodology, propose a\nsimple and effective transfer learning strategy, namely LLMEmbed, to address\nthis classical but challenging task. To illustrate, we first study how to\nproperly extract and fuse the text embeddings via various lightweight LLMs at\ndifferent network depths to improve their robustness and discrimination, then\nadapt such embeddings to train the classifier. We perform extensive experiments\non publicly available datasets, and the results show that LLMEmbed achieves\nstrong performance while enjoys low training overhead using lightweight LLM\nbackbones compared to recent methods based on larger LLMs, i.e. GPT-3, and\nsophisticated prompt-based strategies. Our LLMEmbed achieves adequate accuracy\non publicly available benchmarks without any fine-tuning while merely use 4%\nmodel parameters, 1.8% electricity consumption and 1.5% runtime compared to its\ncounterparts. Code is available at:\nhttps://github.com/ChunLiu-cs/LLMEmbed-ACL2024.", "published": "2024-06-06 03:46:59", "link": "http://arxiv.org/abs/2406.03725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting\n  by Learning from Human", "abstract": "Increasing concerns about privacy leakage issues in academia and industry\narise when employing NLP models from third-party providers to process sensitive\ntexts. To protect privacy before sending sensitive data to those models, we\nsuggest sanitizing sensitive text using two common strategies used by humans:\ni) deleting sensitive expressions, and ii) obscuring sensitive details by\nabstracting them. To explore the issues and develop a tool for text rewriting,\nwe curate the first corpus, coined NAP^2, through both crowdsourcing and the\nuse of large language models (LLMs). Compared to the prior works based on\ndifferential privacy, which lead to a sharp drop in information utility and\nunnatural texts, the human-inspired approaches result in more natural rewrites\nand offer an improved balance between privacy protection and data utility, as\ndemonstrated by our extensive experiments.", "published": "2024-06-06 05:07:44", "link": "http://arxiv.org/abs/2406.03749v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character-Level Chinese Dependency Parsing via Modeling Latent\n  Intra-Word Structure", "abstract": "Revealing the syntactic structure of sentences in Chinese poses significant\nchallenges for word-level parsers due to the absence of clear word boundaries.\nTo facilitate a transition from word-level to character-level Chinese\ndependency parsing, this paper proposes modeling latent internal structures\nwithin words. In this way, each word-level dependency tree is interpreted as a\nforest of character-level trees. A constrained Eisner algorithm is implemented\nto ensure the compatibility of character-level trees, guaranteeing a single\nroot for intra-word structures and establishing inter-word dependencies between\nthese roots. Experiments on Chinese treebanks demonstrate the superiority of\nour method over both the pipeline framework and previous joint models. A\ndetailed analysis reveals that a coarse-to-fine parsing strategy empowers the\nmodel to predict more linguistically plausible intra-word structures.", "published": "2024-06-06 06:23:02", "link": "http://arxiv.org/abs/2406.03772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Trainable Retrieval-Augmented Generation for Relation\n  Extraction", "abstract": "This paper addresses a crucial challenge in retrieval-augmented\ngeneration-based relation extractors; the end-to-end training is not applicable\nto conventional retrieval-augmented generation due to the non-differentiable\nnature of instance retrieval. This problem prevents the instance retrievers\nfrom being optimized for the relation extraction task, and conventionally it\nmust be trained with an objective different from that for relation extraction.\nTo address this issue, we propose a novel End-to-end Trainable\nRetrieval-Augmented Generation (ETRAG), which allows end-to-end optimization of\nthe entire model, including the retriever, for the relation extraction\nobjective by utilizing a differentiable selection of the $k$ nearest instances.\nWe evaluate the relation extraction performance of ETRAG on the TACRED dataset,\nwhich is a standard benchmark for relation extraction. ETRAG demonstrates\nconsistent improvements against the baseline model as retrieved instances are\nadded. Furthermore, the analysis of instances retrieved by the end-to-end\ntrained retriever confirms that the retrieved instances contain common relation\nlabels or entities with the query and are specialized for the target task. Our\nfindings provide a promising foundation for future research on\nretrieval-augmented generation and the broader applications of text generation\nin Natural Language Processing.", "published": "2024-06-06 07:01:50", "link": "http://arxiv.org/abs/2406.03790v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning", "abstract": "Parameter-efficient fine-tuning (PEFT) has emerged as the predominant\ntechnique for fine-tuning in the era of large language models. However,\nexisting PEFT methods still have inadequate training efficiency. Firstly, the\nutilization of large-scale foundation models during the training process is\nexcessively redundant for certain fine-tuning tasks. Secondly, as the model\nsize increases, the growth in trainable parameters of empirically added PEFT\nmodules becomes non-negligible and redundant, leading to inefficiency. To\nachieve task-specific efficient fine-tuning, we propose the Light-PEFT\nframework, which includes two methods: Masked Early Pruning of the Foundation\nModel and Multi-Granularity Early Pruning of PEFT. The Light-PEFT framework\nallows for the simultaneous estimation of redundant parameters in both the\nfoundation model and PEFT modules during the early stage of training. These\nparameters can then be pruned for more efficient fine-tuning. We validate our\napproach on GLUE, SuperGLUE, QA tasks, and various models. With Light-PEFT,\nparameters of the foundation model can be pruned by up to over 40%, while still\ncontrolling trainable parameters to be only 25% of the original PEFT method.\nCompared to utilizing the PEFT method directly, Light-PEFT achieves training\nand inference speedup, reduces memory usage, and maintains comparable\nperformance and the plug-and-play feature of PEFT.", "published": "2024-06-06 07:03:29", "link": "http://arxiv.org/abs/2406.03792v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search", "abstract": "Recent methodologies in LLM self-training mostly rely on LLM generating\nresponses and filtering those with correct output answers as training data.\nThis approach often yields a low-quality fine-tuning training set (e.g.,\nincorrect plans or intermediate reasoning). In this paper, we develop a\nreinforced self-training approach, called ReST-MCTS*, based on integrating\nprocess reward guidance with tree search MCTS* for collecting higher-quality\nreasoning traces as well as per-step value to train policy and reward models.\nReST-MCTS* circumvents the per-step manual annotation typically used to train\nprocess rewards by tree-search-based reinforcement learning: Given oracle final\ncorrect answers, ReST-MCTS* is able to infer the correct process rewards by\nestimating the probability this step can help lead to the correct answer. These\ninferred rewards serve dual purposes: they act as value targets for further\nrefining the process reward model and also facilitate the selection of\nhigh-quality traces for policy model self-training. We first show that the\ntree-search policy in ReST-MCTS* achieves higher accuracy compared with prior\nLLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same\nsearch budget. We then show that by using traces searched by this tree-search\npolicy as training data, we can continuously enhance the three language models\nfor multiple iterations, and outperform other self-training algorithms such as\nReST$^\\text{EM}$ and Self-Rewarding LM. We release all code at\nhttps://github.com/THUDM/ReST-MCTS.", "published": "2024-06-06 07:40:00", "link": "http://arxiv.org/abs/2406.03816v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chaos with Keywords: Exposing Large Language Models Sycophantic\n  Hallucination to Misleading Keywords and Evaluating Defense Strategies", "abstract": "This study explores the sycophantic tendencies of Large Language Models\n(LLMs), where these models tend to provide answers that match what users want\nto hear, even if they are not entirely correct. The motivation behind this\nexploration stems from the common behavior observed in individuals searching\nthe internet for facts with partial or misleading knowledge. Similar to using\nweb search engines, users may recall fragments of misleading keywords and\nsubmit them to an LLM, hoping for a comprehensive response. Our empirical\nanalysis of several LLMs shows the potential danger of these models amplifying\nmisinformation when presented with misleading keywords. Additionally, we\nthoroughly assess four existing hallucination mitigation strategies to reduce\nLLMs sycophantic behavior. Our experiments demonstrate the effectiveness of\nthese strategies for generating factually correct statements. Furthermore, our\nanalyses delve into knowledge-probing experiments on factual keywords and\ndifferent categories of sycophancy mitigation.", "published": "2024-06-06 08:03:05", "link": "http://arxiv.org/abs/2406.03827v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lean Workbook: A large-scale Lean problem set formalized from natural\n  language math problems", "abstract": "Large language models have demonstrated impressive capabilities across\nvarious natural language processing tasks, especially in solving mathematical\nproblems. However, large language models are not good at math theorem proving\nusing formal languages like Lean. A significant challenge in this area is the\nscarcity of training data available in these formal languages. To address this\nissue, we propose a novel pipeline that iteratively generates and filters\nsynthetic data to translate natural language mathematical problems into Lean 4\nstatements, and vice versa. Our results indicate that the synthetic data\npipeline can provide useful training data and improve the performance of LLMs\nin translating and understanding complex mathematical problems and proofs. Our\nfinal dataset contains about 57K formal-informal question pairs along with\nsearched proof from the math contest forum and 21 new IMO questions. We\nopen-source our code at https://github.com/InternLM/InternLM-Math and our data\nat https://huggingface.co/datasets/InternLM/Lean-Workbook.", "published": "2024-06-06 08:25:43", "link": "http://arxiv.org/abs/2406.03847v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speculative Decoding via Early-exiting for Faster LLM Inference with\n  Thompson Sampling Control Mechanism", "abstract": "The recent advancements in large language models (LLMs) have been\nextraordinary, yet the escalating inference costs associated with them present\nchallenges in real-world applications. To address these challenges, we propose\na novel approach called Early-exiting Speculative Decoding (EESD) with lossless\nacceleration. Specifically, EESD utilizes a segment of the LLM to generate\ndraft tokens, incorporating Early-exiting structures after the first N layers.\nTo enhance the quality of draft tokens, a self-distillation method is\nintegrated. This early-exiting design not only reduces deployment and training\ncosts but also significantly accelerates the token generation speed. Moreover,\nwe introduce a novel sampling mechanism that leverages Thompson Sampling to\nregulate the generation processes, automatically determining the quantity of\ndraft tokens in each round. The original LLM is then employed to validate these\ndraft tokens through a single forward pass, and thus guarantees that the final\noutput text maintains a distribution consistent with vanilla auto-regressive\ndecoding. The experimental results on both 13B and 70B models demonstrate that\nour approach decodes tokens at a markedly accelerated rate compared to prior\nmethods, showing the effectiveness of our approach.", "published": "2024-06-06 08:40:28", "link": "http://arxiv.org/abs/2406.03853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance of large language models in numerical vs. semantic medical\n  knowledge: Benchmarking on evidence-based Q&As", "abstract": "Clinical problem-solving requires processing of semantic medical knowledge\nsuch as illness scripts and numerical medical knowledge of diagnostic tests for\nevidence-based decision-making. As large language models (LLMs) show promising\nresults in many aspects of language-based clinical practice, their ability to\ngenerate non-language evidence-based answers to clinical questions is\ninherently limited by tokenization. Therefore, we evaluated LLMs' performance\non two question types: numeric (correlating findings) and semantic\n(differentiating entities) while examining differences within and between LLMs\nin medical aspects and comparing their performance to humans. To generate\nstraightforward multi-choice questions and answers (QAs) based on\nevidence-based medicine (EBM), we used a comprehensive medical knowledge graph\n(encompassed data from more than 50,00 peer-reviewed articles) and created the\n\"EBMQA\". EBMQA contains 105,000 QAs labeled with medical and non-medical topics\nand classified into numerical or semantic questions. We benchmarked this\ndataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and\nClaude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question\ntypes and according to sub-labeled topics. For validation, six medical experts\nwere tested on 100 numerical EBMQA questions. We found that both LLMs excelled\nmore in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical\nQAs. However, both LLMs showed inter and intra gaps in different medical\naspects and remained inferior to humans. Thus, their medical advice should be\naddressed carefully.", "published": "2024-06-06 08:41:46", "link": "http://arxiv.org/abs/2406.03855v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recovering document annotations for sentence-level bitext", "abstract": "Data availability limits the scope of any given task. In machine translation,\nhistorical models were incapable of handling longer contexts, so the lack of\ndocument-level datasets was less noticeable. Now, despite the emergence of\nlong-sequence methods, we remain within a sentence-level paradigm and without\ndata to adequately approach context-aware machine translation. Most large-scale\ndatasets have been processed through a pipeline that discards document-level\nmetadata. In this work, we reconstruct document-level information for three\n(ParaCrawl, News Commentary, and Europarl) large datasets in German, French,\nSpanish, Italian, Polish, and Portuguese (paired with English). We then\nintroduce a document-level filtering technique as an alternative to traditional\nbitext filtering. We present this filtering with analysis to show that this\nmethod prefers context-consistent translations rather than those that may have\nbeen sentence-level machine translated. Last we train models on these longer\ncontexts and demonstrate improvement in document-level translation without\ndegradation of sentence-level translation. We release our dataset, ParaDocs,\nand resulting models as a resource to the community.", "published": "2024-06-06 08:58:14", "link": "http://arxiv.org/abs/2406.03869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoder-only Streaming Transformer for Simultaneous Translation", "abstract": "Simultaneous Machine Translation (SiMT) generates translation while reading\nsource tokens, essentially producing the target prefix based on the source\nprefix. To achieve good performance, it leverages the relationship between\nsource and target prefixes to exact a policy to guide the generation of\ntranslations. Although existing SiMT methods primarily focus on the\nEncoder-Decoder architecture, we explore the potential of Decoder-only\narchitecture, owing to its superior performance in various tasks and its\ninherent compatibility with SiMT. However, directly applying the Decoder-only\narchitecture to SiMT poses challenges in terms of training and inference. To\nalleviate the above problems, we propose the first Decoder-only SiMT model,\nnamed Decoder-only Streaming Transformer (DST). Specifically, DST separately\nencodes the positions of the source and target prefixes, ensuring that the\nposition of the target prefix remains unaffected by the expansion of the source\nprefix. Furthermore, we propose a Streaming Self-Attention (SSA) mechanism\ntailored for the Decoder-only architecture. It is capable of obtaining\ntranslation policy by assessing the sufficiency of input source information and\nintegrating with the soft-attention mechanism to generate translations.\nExperiments demonstrate that our approach achieves state-of-the-art performance\non three translation tasks.", "published": "2024-06-06 09:13:13", "link": "http://arxiv.org/abs/2406.03878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the IWSLT2023 Speech Translation Tasks: Human Annotations,\n  Automatic Metrics, and Segmentation", "abstract": "Human evaluation is a critical component in machine translation system\ndevelopment and has received much attention in text translation research.\nHowever, little prior work exists on the topic of human evaluation for speech\ntranslation, which adds additional challenges such as noisy data and\nsegmentation mismatches. We take first steps to fill this gap by conducting a\ncomprehensive human evaluation of the results of several shared tasks from the\nlast International Workshop on Spoken Language Translation (IWSLT 2023). We\npropose an effective evaluation strategy based on automatic resegmentation and\ndirect assessment with segment context. Our analysis revealed that: 1) the\nproposed evaluation strategy is robust and scores well-correlated with other\ntypes of human judgements; 2) automatic metrics are usually, but not always,\nwell-correlated with direct assessment scores; and 3) COMET as a slightly\nstronger automatic metric than chrF, despite the segmentation noise introduced\nby the resegmentation step systems. We release the collected human-annotated\ndata in order to encourage further investigation.", "published": "2024-06-06 09:18:42", "link": "http://arxiv.org/abs/2406.03881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages?", "abstract": "While machine translation evaluation has been studied primarily for\nhigh-resource languages, there has been a recent interest in evaluation for\nlow-resource languages due to the increasing availability of data and models.\nIn this paper, we focus on a zero-shot evaluation setting focusing on\nlow-resource Indian languages, namely Assamese, Kannada, Maithili, and Punjabi.\nWe collect sufficient Multi-Dimensional Quality Metrics (MQM) and Direct\nAssessment (DA) annotations to create test sets and meta-evaluate a plethora of\nautomatic evaluation metrics. We observe that even for learned metrics, which\nare known to exhibit zero-shot performance, the Kendall Tau and Pearson\ncorrelations with human annotations are only as high as 0.32 and 0.45.\nSynthetic data approaches show mixed results and overall do not help close the\ngap by much for these languages. This indicates that there is still a long way\nto go for low-resource evaluation.", "published": "2024-06-06 09:28:08", "link": "http://arxiv.org/abs/2406.03893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Culturally Aware and Adapted NLP: A Taxonomy and a Survey of the State\n  of the Art", "abstract": "The surge of interest in \"culture\" in NLP has inspired much recent research,\nbut a shared understanding of \"culture\" remains unclear, making it difficult to\nevaluate progress in this emerging area. Drawing on prior research in NLP and\nrelated fields, we propose a fine-grained taxonomy of elements in culture that\ncan provide a systematic framework for analyzing and understanding research\nprogress. Using the taxonomy, we survey existing resources and methods for\nculturally aware and adapted NLP, providing an overview of the state of the art\nand the research gaps that still need to be filled.", "published": "2024-06-06 10:16:43", "link": "http://arxiv.org/abs/2406.03930v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UltraMedical: Building Specialized Generalists in Biomedicine", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains and are moving towards more specialized areas. Recent advanced\nproprietary models such as GPT-4 and Gemini have achieved significant\nadvancements in biomedicine, which have also raised privacy and security\nchallenges. The construction of specialized generalists hinges largely on\nhigh-quality datasets, enhanced by techniques like supervised fine-tuning and\nreinforcement learning from human or AI feedback, and direct preference\noptimization. However, these leading technologies (e.g., preference learning)\nare still significantly limited in the open source community due to the\nscarcity of specialized data. In this paper, we present the UltraMedical\ncollections, which consist of high-quality manual and synthetic datasets in the\nbiomedicine domain, featuring preference annotations across multiple advanced\nLLMs. By utilizing these datasets, we fine-tune a suite of specialized medical\nmodels based on Llama-3 series, demonstrating breathtaking capabilities across\nvarious medical benchmarks. Moreover, we develop powerful reward models skilled\nin biomedical and general reward benchmark, enhancing further online preference\nlearning within the biomedical LLM community. Datasets and models are available\nat https://github.com/TsinghuaC3I/UltraMedical", "published": "2024-06-06 10:50:26", "link": "http://arxiv.org/abs/2406.03949v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tox-BART: Leveraging Toxicity Attributes for Explanation Generation of\n  Implicit Hate Speech", "abstract": "Employing language models to generate explanations for an incoming implicit\nhate post is an active area of research. The explanation is intended to make\nexplicit the underlying stereotype and aid content moderators. The training\noften combines top-k relevant knowledge graph (KG) tuples to provide world\nknowledge and improve performance on standard metrics. Interestingly, our study\npresents conflicting evidence for the role of the quality of KG tuples in\ngenerating implicit explanations. Consequently, simpler models incorporating\nexternal toxicity signals outperform KG-infused models. Compared to the\nKG-based setup, we observe a comparable performance for SBIC (LatentHatred)\ndatasets with a performance variation of +0.44 (+0.49), +1.83 (-1.56), and\n-4.59 (+0.77) in BLEU, ROUGE-L, and BERTScore. Further human evaluation and\nerror analysis reveal that our proposed setup produces more precise\nexplanations than zero-shot GPT-3.5, highlighting the intricate nature of the\ntask.", "published": "2024-06-06 10:54:44", "link": "http://arxiv.org/abs/2406.03953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A + B: A General Generator-Reader Framework for Optimizing LLMs to\n  Unleash Synergy Potential", "abstract": "Retrieval-Augmented Generation (RAG) is an effective solution to supplement\nnecessary knowledge to large language models (LLMs). Targeting its bottleneck\nof retriever performance, \"generate-then-read\" pipeline is proposed to replace\nthe retrieval stage with generation from the LLM itself. Although promising,\nthis research direction is underexplored and still cannot work in the scenario\nwhen source knowledge is given. In this paper, we formalize a general \"A + B\"\nframework with varying combinations of foundation models and types for\nsystematic investigation. We explore the efficacy of the base and chat versions\nof LLMs and found their different functionalities suitable for generator A and\nreader B, respectively. Their combinations consistently outperform single\nmodels, especially in complex scenarios. Furthermore, we extend the application\nof the \"A + B\" framework to scenarios involving source documents through\ncontinuous learning, enabling the direct integration of external knowledge into\nLLMs. This approach not only facilitates effective acquisition of new knowledge\nbut also addresses the challenges of safety and helpfulness post-adaptation.\nThe paper underscores the versatility of the \"A + B\" framework, demonstrating\nits potential to enhance the practical application of LLMs across various\ndomains.", "published": "2024-06-06 11:14:27", "link": "http://arxiv.org/abs/2406.03963v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing LLMs for Zero-shot Abstractive Summarization Through the Lens\n  of Relevance Paraphrasing", "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance at\nzero-shot generation of abstractive summaries for given articles. However,\nlittle is known about the robustness of such a process of zero-shot\nsummarization. To bridge this gap, we propose relevance paraphrasing, a simple\nstrategy that can be used to measure the robustness of LLMs as summarizers. The\nrelevance paraphrasing approach identifies the most relevant sentences that\ncontribute to generating an ideal summary, and then paraphrases these inputs to\nobtain a minimally perturbed dataset. Then, by evaluating model performance for\nsummarization on both the original and perturbed datasets, we can assess the\nLLM's one aspect of robustness. We conduct extensive experiments with relevance\nparaphrasing on 4 diverse datasets, as well as 4 LLMs of different sizes\n(GPT-3.5-Turbo, Llama-2-13B, Mistral-7B, and Dolly-v2-7B). Our results indicate\nthat LLMs are not consistent summarizers for the minimally perturbed articles,\nnecessitating further improvements.", "published": "2024-06-06 12:08:43", "link": "http://arxiv.org/abs/2406.03993v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The syntax-semantics interface in a child's path: A study of 3- to\n  11-year-olds' elicited production of Mandarin recursive relative clauses", "abstract": "There have been apparently conflicting claims over the syntax-semantics\nrelationship in child acquisition. However, few of them have assessed the\nchild's path toward the acquisition of recursive relative clauses (RRCs). The\nauthors of the current paper did experiments to investigate 3- to 11-year-olds'\nmost-structured elicited production of eight Mandarin RRCs in a 4 (syntactic\ntypes)*2 (semantic conditions) design. The four syntactic types were RRCs with\na subject-gapped RC embedded in an object-gapped RC (SORRCs), RRCs with an\nobject-gapped RC embedded in another object-gapped RC (OORRCs), RRCs with an\nobject-gapped RC embedded in a subject-gapped RC (OSRRCs), and RRCs with a\nsubject-gapped RC embedded in another subject-gapped RC (SSRRCs). Each\nsyntactic type was put in two conditions differing in internal semantics:\nirreversible internal semantics (IIS) and reversible internal semantics (RIS).\nFor example, \"the balloon that [the girl that _ eats the banana] holds _\" is\nSORRCs in the IIS condition; \"the monkey that [the dog that _ bites the pig]\nhits_\" is SORRCs in the RIS condition. For each target, the participants were\nprovided with a speech-visual stimulus constructing a condition of irreversible\nexternal semantics (IES). The results showed that SSRRCs, OSRRCs and SORRCs in\nthe IIS-IES condition were produced two years earlier than their counterparts\nin the RIS-IES condition. Thus, a 2-stage development path is proposed: the\nlanguage acquisition device starts with the interface between (irreversible)\nsyntax and IIS, and ends with the interface between syntax and IES, both\nabiding by the syntax-semantic interface principle.", "published": "2024-06-06 12:51:14", "link": "http://arxiv.org/abs/2406.04025v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainability and Hate Speech: Structured Explanations Make Social\n  Media Moderators Faster", "abstract": "Content moderators play a key role in keeping the conversation on social\nmedia healthy. While the high volume of content they need to judge represents a\nbottleneck to the moderation pipeline, no studies have explored how models\ncould support them to make faster decisions. There is, by now, a vast body of\nresearch into detecting hate speech, sometimes explicitly motivated by a desire\nto help improve content moderation, but published research using real content\nmoderators is scarce. In this work we investigate the effect of explanations on\nthe speed of real-world moderators. Our experiments show that while generic\nexplanations do not affect their speed and are often ignored, structured\nexplanations lower moderators' decision making time by 7.4%.", "published": "2024-06-06 14:23:10", "link": "http://arxiv.org/abs/2406.04106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intention and Face in Dialog", "abstract": "The notion of face described by Brown and Levinson (1987) has been studied in\ngreat detail, but a critical aspect of the framework, that which focuses on how\nintentions mediate the planning of turns which impose upon face, has received\nfar less attention. We present an analysis of three computational systems\ntrained for classifying both intention and politeness, focusing on how the\nformer influences the latter. In politeness theory, agents attend to the desire\nto have their wants appreciated (positive face), and a complementary desire to\nact unimpeded and maintain freedom (negative face). Similar to speech acts,\nutterances can perform so-called face acts which can either raise or threaten\nthe positive or negative face of the speaker or hearer. We begin by using an\nexisting corpus to train a model which classifies face acts, achieving a new\nSoTA in the process. We then observe that every face act has an underlying\nintention that motivates it and perform additional experiments integrating\ndialog act annotations to provide these intentions by proxy. Our analysis finds\nthat dialog acts improve performance on face act detection for minority classes\nand points to a close relationship between aspects of face and intent.", "published": "2024-06-06 14:26:35", "link": "http://arxiv.org/abs/2406.04109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering Limitations of Large Language Models in Information Seeking\n  from Tables", "abstract": "Tables are recognized for their high information density and widespread\nusage, serving as essential sources of information. Seeking information from\ntables (TIS) is a crucial capability for Large Language Models (LLMs), serving\nas the foundation of knowledge-based Q&A systems. However, this field presently\nsuffers from an absence of thorough and reliable evaluation. This paper\nintroduces a more reliable benchmark for Table Information Seeking (TabIS). To\navoid the unreliable evaluation caused by text similarity-based metrics, TabIS\nadopts a single-choice question format (with two options per question) instead\nof a text generation format. We establish an effective pipeline for generating\noptions, ensuring their difficulty and quality. Experiments conducted on 12\nLLMs reveal that while the performance of GPT-4-turbo is marginally\nsatisfactory, both other proprietary and open-source models perform\ninadequately. Further analysis shows that LLMs exhibit a poor understanding of\ntable structures, and struggle to balance between TIS performance and\nrobustness against pseudo-relevant tables (common in retrieval-augmented\nsystems). These findings uncover the limitations and potential challenges of\nLLMs in seeking information from tables. We release our data and code to\nfacilitate further research in this field.", "published": "2024-06-06 14:30:59", "link": "http://arxiv.org/abs/2406.04113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Task-agnostic Debiasing Through the Lenses of\n  Intrinsic Bias and Forgetfulness", "abstract": "While task-agnostic debiasing provides notable generalizability and reduced\nreliance on downstream data, its impact on language modeling ability and the\nrisk of relearning social biases from downstream task-specific data remain as\nthe two most significant challenges when debiasing Pretrained Language Models\n(PLMs). The impact on language modeling ability can be alleviated given a\nhigh-quality and long-contextualized debiasing corpus, but there remains a\ndeficiency in understanding the specifics of relearning biases. We empirically\nascertain that the effectiveness of task-agnostic debiasing hinges on the\nquantitative bias level of both the task-specific data used for downstream\napplications and the debiased model. We empirically show that the lower bound\nof the bias level of the downstream fine-tuned model can be approximated by the\nbias level of the debiased model, in most practical cases. To gain more\nin-depth understanding about how the parameters of PLMs change during\nfine-tuning due to the forgetting issue of PLMs, we propose a novel framework\nwhich can Propagate Socially-fair Debiasing to Downstream Fine-tuning,\nProSocialTuning. Our proposed framework can push the fine-tuned model to\napproach the bias lower bound during downstream fine-tuning, indicating that\nthe ineffectiveness of debiasing can be alleviated by overcoming the forgetting\nissue through regularizing successfully debiased attention heads based on the\nPLMs' bias levels from stages of pretraining and debiasing.", "published": "2024-06-06 15:11:11", "link": "http://arxiv.org/abs/2406.04146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase\n  for Math Reasoning", "abstract": "The advancement of large language models (LLMs) relies on evaluation using\npublic benchmarks, but data contamination can lead to overestimated\nperformance. Previous researches focus on detecting contamination by\ndetermining whether the model has seen the exact same data during training.\nBesides, prior work has already shown that even training on data similar to\nbenchmark data inflates performance, namely \\emph{In-distribution\ncontamination}. In this work, we argue that in-distribution contamination can\nlead to the performance drop on OOD benchmarks. To effectively detect\nin-distribution contamination, we propose DICE, a novel method that leverages\nthe internal states of LLMs to locate-then-detect the contamination. DICE first\nidentifies the most sensitive layer to contamination, then trains a classifier\nbased on the internal states of that layer. Experiments reveal DICE's high\naccuracy in detecting in-distribution contamination across various LLMs and\nmath reasoning datasets. We also show the generalization capability of the\ntrained DICE detector, which is able to detect contamination across multiple\nbenchmarks with similar distributions. Additionally, we find that DICE's\npredictions correlate with the performance of LLMs fine-tuned by either us or\nother organizations, achieving a coefficient of determination ($R^2$) between\n0.61 and 0.75. The code and data are available at\nhttps://github.com/THU-KEG/DICE.", "published": "2024-06-06 15:55:53", "link": "http://arxiv.org/abs/2406.04197v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ValueBench: Towards Comprehensively Evaluating Value Orientations and\n  Understanding of Large Language Models", "abstract": "Large Language Models (LLMs) are transforming diverse fields and gaining\nincreasing influence as human proxies. This development underscores the urgent\nneed for evaluating value orientations and understanding of LLMs to ensure\ntheir responsible integration into public-facing applications. This work\nintroduces ValueBench, the first comprehensive psychometric benchmark for\nevaluating value orientations and value understanding in LLMs. ValueBench\ncollects data from 44 established psychometric inventories, encompassing 453\nmultifaceted value dimensions. We propose an evaluation pipeline grounded in\nrealistic human-AI interactions to probe value orientations, along with novel\ntasks for evaluating value understanding in an open-ended value space. With\nextensive experiments conducted on six representative LLMs, we unveil their\nshared and distinctive value orientations and exhibit their ability to\napproximate expert conclusions in value-related extraction and generation\ntasks. ValueBench is openly accessible at\nhttps://github.com/Value4AI/ValueBench.", "published": "2024-06-06 16:14:16", "link": "http://arxiv.org/abs/2406.04214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Steganalysis via LLMs: Two Modes for Efficient Detection of\n  Strongly Concealed Stego", "abstract": "To detect stego (steganographic text) in complex scenarios, linguistic\nsteganalysis (LS) with various motivations has been proposed and achieved\nexcellent performance. However, with the development of generative\nsteganography, some stegos have strong concealment, especially after the\nemergence of LLMs-based steganography, the existing LS has low detection or\ncannot detect them. We designed a novel LS with two modes called LSGC. In the\ngeneration mode, we created an LS-task \"description\" and used the generation\nability of LLM to explain whether texts to be detected are stegos. On this\nbasis, we rethought the principle of LS and LLMs, and proposed the\nclassification mode. In this mode, LSGC deleted the LS-task \"description\" and\nused the \"causalLM\" LLMs to extract steganographic features. The LS features\ncan be extracted by only one pass of the model, and a linear layer with\ninitialization weights is added to obtain the classification probability.\nExperiments on strongly concealed stegos show that LSGC significantly improves\ndetection and reaches SOTA performance. Additionally, LSGC in classification\nmode greatly reduces training time while maintaining high performance.", "published": "2024-06-06 16:18:02", "link": "http://arxiv.org/abs/2406.04218v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmark Data Contamination of Large Language Models: A Survey", "abstract": "The rapid development of Large Language Models (LLMs) like GPT-4, Claude-3,\nand Gemini has transformed the field of natural language processing. However,\nit has also resulted in a significant issue known as Benchmark Data\nContamination (BDC). This occurs when language models inadvertently incorporate\nevaluation benchmark information from their training data, leading to\ninaccurate or unreliable performance during the evaluation phase of the\nprocess. This paper reviews the complex challenge of BDC in LLM evaluation and\nexplores alternative assessment methods to mitigate the risks associated with\ntraditional benchmarks. The paper also examines challenges and future\ndirections in mitigating BDC risks, highlighting the complexity of the issue\nand the need for innovative solutions to ensure the reliability of LLM\nevaluation in real-world applications.", "published": "2024-06-06 16:41:39", "link": "http://arxiv.org/abs/2406.04244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Buffer of Thoughts: Thought-Augmented Reasoning with Large Language\n  Models", "abstract": "We introduce Buffer of Thoughts (BoT), a novel and versatile\nthought-augmented reasoning approach for enhancing accuracy, efficiency and\nrobustness of large language models (LLMs). Specifically, we propose\nmeta-buffer to store a series of informative high-level thoughts, namely\nthought-template, distilled from the problem-solving processes across various\ntasks. Then for each problem, we retrieve a relevant thought-template and\nadaptively instantiate it with specific reasoning structures to conduct\nefficient reasoning. To guarantee the scalability and stability, we further\npropose buffer-manager to dynamically update the meta-buffer, thus enhancing\nthe capacity of meta-buffer as more tasks are solved. We conduct extensive\nexperiments on 10 challenging reasoning-intensive tasks, and achieve\nsignificant performance improvements over previous SOTA methods: 11% on Game of\n24, 20% on Geometric Shapes and 51% on Checkmate-in-One. Further analysis\ndemonstrate the superior generalization ability and model robustness of our\nBoT, while requiring only 12% of the cost of multi-query prompting methods\n(e.g., tree/graph of thoughts) on average. Notably, we find that our\nLlama3-8B+BoT has the potential to surpass Llama3-70B model. Our project is\navailable at: https://github.com/YangLing0818/buffer-of-thought-llm", "published": "2024-06-06 17:22:08", "link": "http://arxiv.org/abs/2406.04271v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Languages are Easy to Language-Model? A Perspective from Learning\n  Probabilistic Regular Languages", "abstract": "What can large language models learn? By definition, language models (LM) are\ndistributions over strings. Therefore, an intuitive way of addressing the above\nquestion is to formalize it as a matter of learnability of classes of\ndistributions over strings. While prior work in this direction focused on\nassessing the theoretical limits, in contrast, we seek to understand the\nempirical learnability. Unlike prior empirical work, we evaluate neural LMs on\ntheir home turf-learning probabilistic languages-rather than as classifiers of\nformal languages. In particular, we investigate the learnability of regular LMs\n(RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs\nas a function of various complexity parameters of the RLM and the hidden state\nsize of the neural LM. We find that the RLM rank, which corresponds to the size\nof linear space spanned by the logits of its conditional distributions, and the\nexpected length of sampled strings are strong and significant predictors of\nlearnability for both RNNs and Transformers. Several other predictors also\nreach significance, but with differing patterns between RNNs and Transformers.", "published": "2024-06-06 17:34:24", "link": "http://arxiv.org/abs/2406.04289v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Smooth Control of Attribute Intensity in Text Generation\n  with LLMs", "abstract": "Controlling the attribute intensity of text generation is crucial across\nscenarios (e.g., writing conciseness, chatting emotion, and explanation\nclarity). The remarkable capabilities of large language models (LLMs) have\nrevolutionized text generation, prompting us to explore such \\emph{smooth\ncontrol} of LLM generation. Specifically, we propose metrics to assess the\nrange, calibration, and consistency of the generated text's attribute intensity\nin response to varying control values, as well as its relevance to the intended\ncontext. To quantify the attribute intensity and context relevance, we propose\nan effective evaluation framework leveraging the Elo rating system and GPT4,\nboth renowned for their robust alignment with human judgment. We look into two\nviable training-free methods for achieving smooth control of LLMs: (1)\nPrompting with semantic shifters, and (2) Modifying internal model\nrepresentations. The evaluations of these two methods are conducted on $5$\ndifferent attributes with various models. Our code and dataset can be obtained\nfrom \\url{https://github.com/ShangDataLab/Smooth-Control}.", "published": "2024-06-06 19:35:51", "link": "http://arxiv.org/abs/2406.04460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Label Classification for Implicit Discourse Relation Recognition", "abstract": "Discourse relations play a pivotal role in establishing coherence within\ntextual content, uniting sentences and clauses into a cohesive narrative. The\nPenn Discourse Treebank (PDTB) stands as one of the most extensively utilized\ndatasets in this domain. In PDTB-3, the annotators can assign multiple labels\nto an example, when they believe that multiple relations are present. Prior\nresearch in discourse relation recognition has treated these instances as\nseparate examples during training, and only one example needs to have its label\npredicted correctly for the instance to be judged as correct. However, this\napproach is inadequate, as it fails to account for the interdependence of\nlabels in real-world contexts and to distinguish between cases where only one\nsense relation holds and cases where multiple relations hold simultaneously. In\nour work, we address this challenge by exploring various multi-label\nclassification frameworks to handle implicit discourse relation recognition. We\nshow that multi-label classification methods don't depress performance for\nsingle-label prediction. Additionally, we give comprehensive analysis of\nresults and data. Our work contributes to advancing the understanding and\napplication of discourse relations and provide a foundation for the future\nstudy", "published": "2024-06-06 19:37:25", "link": "http://arxiv.org/abs/2406.04461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "llmNER: (Zero|Few)-Shot Named Entity Recognition, Exploiting the Power\n  of Large Language Models", "abstract": "Large language models (LLMs) allow us to generate high-quality human-like\ntext. One interesting task in natural language processing (NLP) is named entity\nrecognition (NER), which seeks to detect mentions of relevant information in\ndocuments. This paper presents llmNER, a Python library for implementing\nzero-shot and few-shot NER with LLMs; by providing an easy-to-use interface,\nllmNER can compose prompts, query the model, and parse the completion returned\nby the LLM. Also, the library enables the user to perform prompt engineering\nefficiently by providing a simple interface to test multiple variables. We\nvalidated our software on two NER tasks to show the library's flexibility.\nllmNER aims to push the boundaries of in-context learning research by removing\nthe barrier of the prompting and parsing steps.", "published": "2024-06-06 22:01:59", "link": "http://arxiv.org/abs/2406.04528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Context Selection in LLM-based Leaderboard Generation: An\n  Empirical Study", "abstract": "This paper explores the impact of context selection on the efficiency of\nLarge Language Models (LLMs) in generating Artificial Intelligence (AI)\nresearch leaderboards, a task defined as the extraction of (Task, Dataset,\nMetric, Score) quadruples from scholarly articles. By framing this challenge as\na text generation objective and employing instruction finetuning with the\nFLAN-T5 collection, we introduce a novel method that surpasses traditional\nNatural Language Inference (NLI) approaches in adapting to new developments\nwithout a predefined taxonomy. Through experimentation with three distinct\ncontext types of varying selectivity and length, our study demonstrates the\nimportance of effective context selection in enhancing LLM accuracy and\nreducing hallucinations, providing a new pathway for the reliable and efficient\ngeneration of AI leaderboards. This contribution not only advances the state of\nthe art in leaderboard generation but also sheds light on strategies to\nmitigate common challenges in LLM-based information extraction.", "published": "2024-06-06 06:05:39", "link": "http://arxiv.org/abs/2407.02409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistically Conditioned Semantic Textual Similarity", "abstract": "Semantic textual similarity (STS) is a fundamental NLP task that measures the\nsemantic similarity between a pair of sentences. In order to reduce the\ninherent ambiguity posed from the sentences, a recent work called Conditional\nSTS (C-STS) has been proposed to measure the sentences' similarity conditioned\non a certain aspect. Despite the popularity of C-STS, we find that the current\nC-STS dataset suffers from various issues that could impede proper evaluation\non this task. In this paper, we reannotate the C-STS validation set and observe\nan annotator discrepancy on 55% of the instances resulting from the annotation\nerrors in the original label, ill-defined conditions, and the lack of clarity\nin the task definition. After a thorough dataset analysis, we improve the C-STS\ntask by leveraging the models' capability to understand the conditions under a\nQA task setting. With the generated answers, we present an automatic error\nidentification pipeline that is able to identify annotation errors from the\nC-STS data with over 80% F1 score. We also propose a new method that largely\nimproves the performance over baselines on the C-STS data by training the\nmodels with the answers. Finally we discuss the conditionality annotation based\non the typed-feature structure (TFS) of entity types. We show in examples that\nthe TFS is able to provide a linguistic foundation for constructing C-STS data\nwith new conditions.", "published": "2024-06-06 01:23:45", "link": "http://arxiv.org/abs/2406.03673v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating the World Model Implicit in a Generative Model", "abstract": "Recent work suggests that large language models may implicitly learn world\nmodels. How should we assess this possibility? We formalize this question for\nthe case where the underlying reality is governed by a deterministic finite\nautomaton. This includes problems as diverse as simple logical reasoning,\ngeographic navigation, game-playing, and chemistry. We propose new evaluation\nmetrics for world model recovery inspired by the classic Myhill-Nerode theorem\nfrom language theory. We illustrate their utility in three domains: game\nplaying, logic puzzles, and navigation. In all domains, the generative models\nwe consider do well on existing diagnostics for assessing world models, but our\nevaluation metrics reveal their world models to be far less coherent than they\nappear. Such incoherence creates fragility: using a generative model to solve\nrelated but subtly different tasks can lead to failures. Building generative\nmodels that meaningfully capture the underlying logic of the domains they model\nwould be immensely valuable; our results suggest new ways to assess how close a\ngiven model is to that goal.", "published": "2024-06-06 02:20:31", "link": "http://arxiv.org/abs/2406.03689v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synthesizing Conversations from Unlabeled Documents using Automatic\n  Response Segmentation", "abstract": "In this study, we tackle the challenge of inadequate and costly training data\nthat has hindered the development of conversational question answering (ConvQA)\nsystems. Enterprises have a large corpus of diverse internal documents. Instead\nof relying on a searching engine, a more compelling approach for people to\ncomprehend these documents is to create a dialogue system. In this paper, we\npropose a robust dialog synthesising method. We learn the segmentation of data\nfor the dialog task instead of using segmenting at sentence boundaries. The\nsynthetic dataset generated by our proposed method achieves superior quality\nwhen compared to WikiDialog, as assessed through machine and human evaluations.\nBy employing our inpainted data for ConvQA retrieval system pre-training, we\nobserved a notable improvement in performance across OR-QuAC benchmarks.", "published": "2024-06-06 02:52:45", "link": "http://arxiv.org/abs/2406.03703v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Medical Large Language Models: Technology, Application,\n  Trustworthiness, and Future Directions", "abstract": "With the advent of Large Language Models (LLMs), medical artificial\nintelligence (AI) has experienced substantial technological progress and\nparadigm shifts, highlighting the potential of LLMs to streamline healthcare\ndelivery and improve patient outcomes. Considering this rapid technical\nprogress, in this survey, we trace the recent advances of Medical Large\nLanguage Models (Med-LLMs), including the background, key findings, and\nmainstream techniques, especially for the evolution from general-purpose models\nto medical-specialized applications. Firstly, we delve into the foundational\ntechnology of Med-LLMs, indicating how general models can be progressively\nadapted and refined for the complicated medical tasks. Secondly, the\nwide-ranging applications of Med-LLMs are investigated across various\nhealthcare domains, as well as an up-to-date review of existing Med-LLMs. The\ntransformative impact of these models on daily medical practice is evident\nthrough their ability to assist clinicians, educators, and patients.\nRecognizing the importance of responsible innovation, we discuss the challenges\nassociated with ensuring fairness, accountability, privacy, and robustness.\nEthical considerations, rigorous evaluation methodologies, and the\nestablishment of regulatory frameworks are crucial for building trustworthiness\nin the real-world system. We emphasize the need for ongoing scrutiny and\ndevelopment to maintain high standards of safety and reliability. Finally, we\nanticipate possible future trajectories for Med-LLMs, identifying key avenues\nfor prudent expansion. By consolidating these insights, our review aims to\nprovide professionals and researchers with a thorough understanding of the\nstrengths and limitations of Med-LLMs, fostering a balanced and ethical\napproach to their integration into the healthcare ecosystem.", "published": "2024-06-06 03:15:13", "link": "http://arxiv.org/abs/2406.03712v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data", "abstract": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.", "published": "2024-06-06 04:22:11", "link": "http://arxiv.org/abs/2406.03736v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Efficient Knowledge Infusion via KG-LLM Alignment", "abstract": "To tackle the problem of domain-specific knowledge scarcity within large\nlanguage models (LLMs), knowledge graph-retrievalaugmented method has been\nproven to be an effective and efficient technique for knowledge infusion.\nHowever, existing approaches face two primary challenges: knowledge mismatch\nbetween public available knowledge graphs and the specific domain of the task\nat hand, and poor information compliance of LLMs with knowledge graphs. In this\npaper, we leverage a small set of labeled samples and a large-scale corpus to\nefficiently construct domain-specific knowledge graphs by an LLM, addressing\nthe issue of knowledge mismatch. Additionally, we propose a three-stage KG-LLM\nalignment strategyto enhance the LLM's capability to utilize information from\nknowledge graphs. We conduct experiments with a limited-sample setting on two\nbiomedical question-answering datasets, and the results demonstrate that our\napproach outperforms existing baselines.", "published": "2024-06-06 04:55:55", "link": "http://arxiv.org/abs/2406.03746v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew", "abstract": "While large language models (LLMs) excel in various natural language tasks in\nEnglish, their performance in lower-resourced languages like Hebrew, especially\nfor generative tasks such as abstractive summarization, remains unclear. The\nhigh morphological richness in Hebrew adds further challenges due to the\nambiguity in sentence comprehension and the complexities in meaning\nconstruction. In this paper, we address this resource and evaluation gap by\nintroducing HeSum, a novel benchmark specifically designed for abstractive text\nsummarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs\nsourced from Hebrew news websites written by professionals. Linguistic analysis\nconfirms HeSum's high abstractness and unique morphological challenges. We show\nthat HeSum presents distinct difficulties for contemporary state-of-the-art\nLLMs, establishing it as a valuable testbed for generative language technology\nin Hebrew, and MRLs generative challenges in general.", "published": "2024-06-06 09:36:14", "link": "http://arxiv.org/abs/2406.03897v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On The Persona-based Summarization of Domain-Specific Documents", "abstract": "In an ever-expanding world of domain-specific knowledge, the increasing\ncomplexity of consuming, and storing information necessitates the generation of\nsummaries from large information repositories. However, every persona of a\ndomain has different requirements of information and hence their summarization.\nFor example, in the healthcare domain, a persona-based (such as Doctor, Nurse,\nPatient etc.) approach is imperative to deliver targeted medical information\nefficiently. Persona-based summarization of domain-specific information by\nhumans is a high cognitive load task and is generally not preferred. The\nsummaries generated by two different humans have high variability and do not\nscale in cost and subject matter expertise as domains and personas grow.\nFurther, AI-generated summaries using generic Large Language Models (LLMs) may\nnot necessarily offer satisfactory accuracy for different domains unless they\nhave been specifically trained on domain-specific data and can also be very\nexpensive to use in day-to-day operations. Our contribution in this paper is\ntwo-fold: 1) We present an approach to efficiently fine-tune a domain-specific\nsmall foundation LLM using a healthcare corpus and also show that we can\neffectively evaluate the summarization quality using AI-based critiquing. 2) We\nfurther show that AI-based critiquing has good concordance with Human-based\ncritiquing of the summaries. Hence, such AI-based pipelines to generate\ndomain-specific persona-based summaries can be easily scaled to other domains\nsuch as legal, enterprise documents, education etc. in a very efficient and\ncost-effective manner.", "published": "2024-06-06 12:00:41", "link": "http://arxiv.org/abs/2406.03986v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "American Sign Language Handshapes Reflect Pressures for Communicative\n  Efficiency", "abstract": "Communicative efficiency is a key topic in linguistics and cognitive\npsychology, with many studies demonstrating how the pressure to communicate\nwith minimal effort guides the form of natural language. However, this\nphenomenon is rarely explored in signed languages. This paper shows how\nhandshapes in American Sign Language (ASL) reflect these efficiency pressures\nand provides new evidence of communicative efficiency in the visual-gestural\nmodality.\n  We focus on hand configurations in native ASL signs and signs borrowed from\nEnglish to compare efficiency pressures from both ASL and English usage. First,\nwe develop new methodologies to quantify the articulatory effort needed to\nproduce handshapes and the perceptual effort required to recognize them. Then,\nwe analyze correlations between communicative effort and usage statistics in\nASL or English. Our findings reveal that frequent ASL handshapes are easier to\nproduce and that pressures for communicative efficiency mostly come from ASL\nusage, rather than from English lexical borrowing.", "published": "2024-06-06 12:46:21", "link": "http://arxiv.org/abs/2406.04024v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Promoting the Responsible Development of Speech Datasets for Mental\n  Health and Neurological Disorders Research", "abstract": "Current research in machine learning and artificial intelligence is largely\ncentered on modeling and performance evaluation, less so on data collection.\nHowever, recent research demonstrated that limitations and biases in data may\nnegatively impact trustworthiness and reliability. These aspects are\nparticularly impactful on sensitive domains such as mental health and\nneurological disorders, where speech data are used to develop AI applications\nfor patients and healthcare providers. In this paper, we chart the landscape of\navailable speech datasets for this domain, to highlight possible pitfalls and\nopportunities for improvement and promote fairness and diversity. We present a\ncomprehensive list of desiderata for building speech datasets for mental health\nand neurological disorders and distill it into an actionable checklist focused\non ethical concerns to foster more responsible research.", "published": "2024-06-06 14:36:07", "link": "http://arxiv.org/abs/2406.04116v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Are We Done with MMLU?", "abstract": "Maybe not. We identify and analyse errors in the popular Massive Multitask\nLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,\nour analysis demonstrates numerous ground truth errors that obscure the true\ncapabilities of LLMs. For example, we find that 57% of the analysed questions\nin the Virology subset contain errors. To address this issue, we introduce a\ncomprehensive framework for identifying dataset errors using a novel error\nannotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700\nmanually re-annotated questions across all 57 MMLU subjects. We estimate that\n6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate\nsignificant discrepancies with the model performance metrics that were\noriginally reported. Our results strongly advocate for revising MMLU's\nerror-ridden questions to enhance its future utility and reliability as a\nbenchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.", "published": "2024-06-06 14:49:06", "link": "http://arxiv.org/abs/2406.04127v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Every Answer Matters: Evaluating Commonsense with Probabilistic Measures", "abstract": "Large language models have demonstrated impressive performance on commonsense\ntasks; however, these tasks are often posed as multiple-choice questions,\nallowing models to exploit systematic biases. Commonsense is also inherently\nprobabilistic with multiple correct answers. The purpose of \"boiling water\"\ncould be making tea and cooking, but it also could be killing germs. Existing\ntasks do not capture the probabilistic nature of common sense. To this end, we\npresent commonsense frame completion (CFC), a new generative task that\nevaluates common sense via multiple open-ended generations. We also propose a\nmethod of probabilistic evaluation that strongly correlates with human\njudgments. Humans drastically outperform strong language model baselines on our\ndataset, indicating this approach is both a challenging and useful evaluation\nof machine common sense.", "published": "2024-06-06 15:10:27", "link": "http://arxiv.org/abs/2406.04145v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AgentGym: Evolving Large Language Model-based Agents across Diverse\n  Environments", "abstract": "Building generalist agents that can handle diverse tasks and evolve\nthemselves across different environments is a long-term goal in the AI\ncommunity. Large language models (LLMs) are considered a promising foundation\nto build such agents due to their generalized capabilities. Current approaches\neither have LLM-based agents imitate expert-provided trajectories step-by-step,\nrequiring human supervision, which is hard to scale and limits environmental\nexploration; or they let agents explore and learn in isolated environments,\nresulting in specialist agents with limited generalization. In this paper, we\ntake the first step towards building generally-capable LLM-based agents with\nself-evolution ability. We identify a trinity of ingredients: 1) diverse\nenvironments for agent exploration and learning, 2) a trajectory set to equip\nagents with basic capabilities and prior knowledge, and 3) an effective and\nscalable evolution method. We propose AgentGym, a new framework featuring a\nvariety of environments and tasks for broad, real-time, uni-format, and\nconcurrent agent exploration. AgentGym also includes a database with expanded\ninstructions, a benchmark suite, and high-quality trajectories across\nenvironments. Next, we propose a novel method, AgentEvol, to investigate the\npotential of agent self-evolution beyond previously seen data across tasks and\nenvironments. Experimental results show that the evolved agents can achieve\nresults comparable to SOTA models. We release the AgentGym suite, including the\nplatform, dataset, benchmark, checkpoints, and algorithm implementations. The\nAgentGym suite is available on https://github.com/WooooDyy/AgentGym.", "published": "2024-06-06 15:15:41", "link": "http://arxiv.org/abs/2406.04151v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Confabulation: The Surprising Value of Large Language Model\n  Hallucinations", "abstract": "This paper presents a systematic defense of large language model (LLM)\nhallucinations or 'confabulations' as a potential resource instead of a\ncategorically negative pitfall. The standard view is that confabulations are\ninherently problematic and AI research should eliminate this flaw. In this\npaper, we argue and empirically demonstrate that measurable semantic\ncharacteristics of LLM confabulations mirror a human propensity to utilize\nincreased narrativity as a cognitive resource for sense-making and\ncommunication. In other words, it has potential value. Specifically, we analyze\npopular hallucination benchmarks and reveal that hallucinated outputs display\nincreased levels of narrativity and semantic coherence relative to veridical\noutputs. This finding reveals a tension in our usually dismissive\nunderstandings of confabulation. It suggests, counter-intuitively, that the\ntendency for LLMs to confabulate may be intimately associated with a positive\ncapacity for coherent narrative-text generation.", "published": "2024-06-06 15:32:29", "link": "http://arxiv.org/abs/2406.04175v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language\n  Model", "abstract": "With the development of large-scale Language Models (LLM), fine-tuning\npre-trained LLM has become a mainstream paradigm for solving downstream tasks\nof natural language processing. However, training a language model in the legal\nfield requires a large number of legal documents so that the language model can\nlearn legal terminology and the particularity of the format of legal documents.\nThe typical NLP approaches usually rely on many manually annotated data sets\nfor training. However, in the legal field application, it is difficult to\nobtain a large number of manually annotated data sets, which restricts the\ntypical method applied to the task of drafting legal documents. The\nexperimental results of this paper show that not only can we leverage a large\nnumber of annotation-free legal documents without Chinese word segmentation to\nfine-tune a large-scale language model, but more importantly, it can fine-tune\na pre-trained LLM on the local computer to achieve the generating legal\ndocument drafts task, and at the same time achieve the protection of\ninformation privacy and to improve information security issues.", "published": "2024-06-06 16:00:20", "link": "http://arxiv.org/abs/2406.04202v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Do Language Models Learn in Context? The Structured Task Hypothesis", "abstract": "Large language models (LLMs) exhibit an intriguing ability to learn a novel\ntask from in-context examples presented in a demonstration, termed in-context\nlearning (ICL). Understandably, a swath of research has been dedicated to\nuncovering the theories underpinning ICL. One popular hypothesis explains ICL\nby task selection. LLMs identify the task based on the demonstration and\ngeneralize it to the prompt. Another popular hypothesis is that ICL is a form\nof meta-learning, i.e., the models learn a learning algorithm at pre-training\ntime and apply it to the demonstration. Finally, a third hypothesis argues that\nLLMs use the demonstration to select a composition of tasks learned during\npre-training to perform ICL. In this paper, we empirically explore these three\nhypotheses that explain LLMs' ability to learn in context with a suite of\nexperiments derived from common text classification tasks. We invalidate the\nfirst two hypotheses with counterexamples and provide evidence in support of\nthe last hypothesis. Our results suggest an LLM could learn a novel task in\ncontext via composing tasks learned during pre-training.", "published": "2024-06-06 16:15:34", "link": "http://arxiv.org/abs/2406.04216v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BEADs: Bias Evaluation Across Domains", "abstract": "Recent advancements in large language models (LLMs) have greatly enhanced\nnatural language processing (NLP) applications. Nevertheless, these models\noften inherit biases from their training data. Despite the availability of\nvarious datasets for bias detection, most are limited to one or two NLP tasks\n(typically classification or evaluation) and lack comprehensive evaluations\nacross a broader range of NLP tasks. To address this gap, we introduce the Bias\nEvaluations Across Domains BEADs dataset, designed to support a wide array of\nNLP tasks, including text classification, token classification, bias\nquantification, and benign language generation. A key focus of this paper is\nthe gold label dataset that is annotated by GPT4 for scalabilty and verified by\nexperts to ensure high reliability. BEADs provides data for both fine-tuning,\nincluding classification and language generation tasks, and for evaluating\nLLMs. Our findings indicate that BEADs effectively identifies numerous biases\nwhen fine-tuned on this dataset. It also reduces biases when used for\nfine-tuning language generation task, while preserving language quality. The\nresults also reveal some prevalent demographic biases in LLMs when BEADs is\nused for evaluation in demographic task. We provide the BEADs dataset for\ndetecting biases in various domains, and this dataset is readily usable for\nresponsible AI development and application. The dataset can be accessed at\nhttps://huggingface.co/datasets/shainar/BEAD .", "published": "2024-06-06 16:18:30", "link": "http://arxiv.org/abs/2406.04220v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FairytaleQA Translated: Enabling Educational Question and Answer\n  Generation in Less-Resourced Languages", "abstract": "Question Answering (QA) datasets are crucial in assessing reading\ncomprehension skills for both machines and humans. While numerous datasets have\nbeen developed in English for this purpose, a noticeable void exists in\nless-resourced languages. To alleviate this gap, our paper introduces\nmachine-translated versions of FairytaleQA, a renowned QA dataset designed to\nassess and enhance narrative comprehension skills in young children. By\nemploying fine-tuned, modest-scale models, we establish benchmarks for both\nQuestion Generation (QG) and QA tasks within the translated datasets. In\naddition, we present a case study proposing a model for generating\nquestion-answer pairs, with an evaluation incorporating quality metrics such as\nquestion well-formedness, answerability, relevance, and children suitability.\nOur evaluation prioritizes quantifying and describing error cases, along with\nproviding directions for future work. This paper contributes to the advancement\nof QA and QG research in less-resourced languages, promoting accessibility and\ninclusivity in the development of these models for reading comprehension. The\ncode and data is publicly available at\ngithub.com/bernardoleite/fairytaleqa-translated.", "published": "2024-06-06 16:31:47", "link": "http://arxiv.org/abs/2406.04233v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hypernetworks for Personalizing ASR to Atypical Speech", "abstract": "Parameter-efficient fine-tuning (PEFT) for personalizing automatic speech\nrecognition (ASR) has recently shown promise for adapting general population\nmodels to atypical speech. However, these approaches assume a priori knowledge\nof the atypical speech disorder being adapted for -- the diagnosis of which\nrequires expert knowledge that is not always available. Even given this\nknowledge, data scarcity and high inter/intra-speaker variability further limit\nthe effectiveness of traditional fine-tuning. To circumvent these challenges,\nwe first identify the minimal set of model parameters required for ASR\nadaptation. Our analysis of each individual parameter's effect on adaptation\nperformance allows us to reduce Word Error Rate (WER) by half while adapting\n0.03% of all weights. Alleviating the need for cohort-specific models, we next\npropose the novel use of a meta-learned hypernetwork to generate highly\nindividualized, utterance-level adaptations on-the-fly for a diverse set of\natypical speech characteristics. Evaluating adaptation at the global, cohort\nand individual-level, we show that hypernetworks generalize better to\nout-of-distribution speakers, while maintaining an overall relative WER\nreduction of 75.2% using 0.1% of the full parameter budget.", "published": "2024-06-06 16:39:00", "link": "http://arxiv.org/abs/2406.04240v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Transformers need glasses! Information over-squashing in language tasks", "abstract": "We study how information propagates in decoder-only Transformers, which are\nthe architectural backbone of most existing frontier large language models\n(LLMs). We rely on a theoretical signal propagation analysis -- specifically,\nwe analyse the representations of the last token in the final layer of the\nTransformer, as this is the representation used for next-token prediction. Our\nanalysis reveals a representational collapse phenomenon: we prove that certain\ndistinct sequences of inputs to the Transformer can yield arbitrarily close\nrepresentations in the final token. This effect is exacerbated by the\nlow-precision floating-point formats frequently used in modern LLMs. As a\nresult, the model is provably unable to respond to these sequences in different\nways -- leading to errors in, e.g., tasks involving counting or copying.\nFurther, we show that decoder-only Transformer language models can lose\nsensitivity to specific tokens in the input, which relates to the well-known\nphenomenon of over-squashing in graph neural networks. We provide empirical\nevidence supporting our claims on contemporary LLMs. Our theory also points to\nsimple solutions towards ameliorating these issues.", "published": "2024-06-06 17:14:44", "link": "http://arxiv.org/abs/2406.04267v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterizing Similarities and Divergences in Conversational Tones in\n  Humans and LLMs by Sampling with People", "abstract": "Conversational tones -- the manners and attitudes in which speakers\ncommunicate -- are essential to effective communication. Amidst the increasing\npopularization of Large Language Models (LLMs) over recent years, it becomes\nnecessary to characterize the divergences in their conversational tones\nrelative to humans. However, existing investigations of conversational\nmodalities rely on pre-existing taxonomies or text corpora, which suffer from\nexperimenter bias and may not be representative of real-world distributions for\nthe studies' psycholinguistic domains. Inspired by methods from cognitive\nscience, we propose an iterative method for simultaneously eliciting\nconversational tones and sentences, where participants alternate between two\ntasks: (1) one participant identifies the tone of a given sentence and (2) a\ndifferent participant generates a sentence based on that tone. We run 100\niterations of this process with human participants and GPT-4, then obtain a\ndataset of sentences and frequent conversational tones. In an additional\nexperiment, humans and GPT-4 annotated all sentences with all tones. With data\nfrom 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4\nqueries, we show how our approach can be used to create an interpretable\ngeometric representation of relations between conversational tones in humans\nand GPT-4. This work demonstrates how combining ideas from machine learning and\ncognitive science can address challenges in human-computer interactions.", "published": "2024-06-06 17:26:00", "link": "http://arxiv.org/abs/2406.04278v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract\n  Descriptions", "abstract": "We present ABEX, a novel and effective generative data augmentation\nmethodology for low-resource Natural Language Understanding (NLU) tasks. ABEX\nis based on ABstract-and-EXpand, a novel paradigm for generating diverse forms\nof an input document -- we first convert a document into its concise, abstract\ndescription and then generate new documents based on expanding the resultant\nabstraction. To learn the task of expanding abstract descriptions, we first\ntrain BART on a large-scale synthetic dataset with abstract-document pairs.\nNext, to generate abstract descriptions for a document, we propose a simple,\ncontrollable, and training-free method based on editing AMR graphs. ABEX brings\nthe best of both worlds: by expanding from abstract representations, it\npreserves the original semantic properties of the documents, like style and\nmeaning, thereby maintaining alignment with the original label and data\ndistribution. At the same time, the fundamental process of elaborating on\nabstract descriptions facilitates diverse generations. We demonstrate the\neffectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource\nsettings. ABEX outperforms all our baselines qualitatively with improvements of\n0.04% - 38.8%. Qualitatively, ABEX outperforms all prior methods from\nliterature in terms of context and length diversity.", "published": "2024-06-06 17:29:57", "link": "http://arxiv.org/abs/2406.04286v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring and Addressing Indexical Bias in Information Retrieval", "abstract": "Information Retrieval (IR) systems are designed to deliver relevant content,\nbut traditional systems may not optimize rankings for fairness, neutrality, or\nthe balance of ideas. Consequently, IR can often introduce indexical biases, or\nbiases in the positional order of documents. Although indexical bias can\ndemonstrably affect people's opinion, voting patterns, and other behaviors,\nthese issues remain understudied as the field lacks reliable metrics and\nprocedures for automatically measuring indexical bias. Towards this end, we\nintroduce the PAIR framework, which supports automatic bias audits for ranked\ndocuments or entire IR systems. After introducing DUO, the first\ngeneral-purpose automatic bias metric, we run an extensive evaluation of 8 IR\nsystems on a new corpus of 32k synthetic and 4.7k natural documents, with 4k\nqueries spanning 1.4k controversial issue topics. A human behavioral study\nvalidates our approach, showing that our bias metric can help predict when and\nhow indexical bias will shift a reader's opinion.", "published": "2024-06-06 17:42:37", "link": "http://arxiv.org/abs/2406.04298v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploring the Latest LLMs for Leaderboard Extraction", "abstract": "The rapid advancements in Large Language Models (LLMs) have opened new\navenues for automating complex tasks in AI research. This paper investigates\nthe efficacy of different LLMs-Mistral 7B, Llama-2, GPT-4-Turbo and GPT-4.o in\nextracting leaderboard information from empirical AI research articles. We\nexplore three types of contextual inputs to the models: DocTAET (Document\nTitle, Abstract, Experimental Setup, and Tabular Information), DocREC (Results,\nExperiments, and Conclusions), and DocFULL (entire document). Our comprehensive\nstudy evaluates the performance of these models in generating (Task, Dataset,\nMetric, Score) quadruples from research papers. The findings reveal significant\ninsights into the strengths and limitations of each model and context type,\nproviding valuable guidance for future AI research automation efforts.", "published": "2024-06-06 05:54:45", "link": "http://arxiv.org/abs/2406.04383v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoralBench: Moral Evaluation of LLMs", "abstract": "In the rapidly evolving field of artificial intelligence, large language\nmodels (LLMs) have emerged as powerful tools for a myriad of applications, from\nnatural language processing to decision-making support systems. However, as\nthese models become increasingly integrated into societal frameworks, the\nimperative to ensure they operate within ethical and moral boundaries has never\nbeen more critical. This paper introduces a novel benchmark designed to measure\nand compare the moral reasoning capabilities of LLMs. We present the first\ncomprehensive dataset specifically curated to probe the moral dimensions of LLM\noutputs, addressing a wide range of ethical dilemmas and scenarios reflective\nof real-world complexities.\n  The main contribution of this work lies in the development of benchmark\ndatasets and metrics for assessing the moral identity of LLMs, which accounts\nfor nuance, contextual sensitivity, and alignment with human ethical standards.\nOur methodology involves a multi-faceted approach, combining quantitative\nanalysis with qualitative insights from ethics scholars to ensure a thorough\nevaluation of model performance. By applying our benchmark across several\nleading LLMs, we uncover significant variations in moral reasoning capabilities\nof different models. These findings highlight the importance of considering\nmoral reasoning in the development and evaluation of LLMs, as well as the need\nfor ongoing research to address the biases and limitations uncovered in our\nstudy. We publicly release the benchmark at\nhttps://drive.google.com/drive/u/0/folders/1k93YZJserYc2CkqP8d4B3M3sgd3kA8W7\nand also open-source the code of the project at\nhttps://github.com/agiresearch/MoralBench.", "published": "2024-06-06 18:15:01", "link": "http://arxiv.org/abs/2406.04428v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TexIm FAST: Text-to-Image Representation for Semantic Similarity\n  Evaluation using Transformers", "abstract": "One of the principal objectives of Natural Language Processing (NLP) is to\ngenerate meaningful representations from text. Improving the informativeness of\nthe representations has led to a tremendous rise in the dimensionality and the\nmemory footprint. It leads to a cascading effect amplifying the complexity of\nthe downstream model by increasing its parameters. The available techniques\ncannot be applied to cross-modal applications such as text-to-image. To\nameliorate these issues, a novel Text-to-Image methodology for generating\nfixed-length representations through a self-supervised Variational Auto-Encoder\n(VAE) for semantic evaluation applying transformers (TexIm FAST) has been\nproposed in this paper. The pictorial representations allow oblivious inference\nwhile retaining the linguistic intricacies, and are potent in cross-modal\napplications. TexIm FAST deals with variable-length sequences and generates\nfixed-length representations with over 75% reduced memory footprint. It\nenhances the efficiency of the models for downstream tasks by reducing its\nparameters. The efficacy of TexIm FAST has been extensively analyzed for the\ntask of Semantic Textual Similarity (STS) upon the MSRPC, CNN/ Daily Mail, and\nXSum data-sets. The results demonstrate 6% improvement in accuracy compared to\nthe baseline and showcase its exceptional ability to compare disparate length\nsequences such as a text with its summary.", "published": "2024-06-06 18:28:50", "link": "http://arxiv.org/abs/2406.04438v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAIRA-2: Grounded Radiology Report Generation", "abstract": "Radiology reporting is a complex task requiring detailed medical image\nunderstanding and precise language generation, for which generative multimodal\nmodels offer a promising solution. However, to impact clinical practice, models\nmust achieve a high level of both verifiable performance and utility. We\naugment the utility of automated report generation by incorporating\nlocalisation of individual findings on the image - a task we call grounded\nreport generation - and enhance performance by incorporating realistic\nreporting context as inputs. We design a novel evaluation framework (RadFact)\nleveraging the logical inference capabilities of large language models (LLMs)\nto quantify report correctness and completeness at the level of individual\nsentences, while supporting the new task of grounded reporting. We develop\nMAIRA-2, a large radiology-specific multimodal model designed to generate chest\nX-ray reports with and without grounding. MAIRA-2 achieves state of the art on\nexisting report generation benchmarks and establishes the novel task of\ngrounded report generation.", "published": "2024-06-06 19:12:41", "link": "http://arxiv.org/abs/2406.04449v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning", "abstract": "Pre-trained language models (PLMs) have attracted enormous attention over the\npast few years with their unparalleled performances. Meanwhile, the soaring\ncost to train PLMs as well as their amazing generalizability have jointly\ncontributed to few-shot fine-tuning and prompting as the most popular training\nparadigms for natural language processing (NLP) models. Nevertheless, existing\nstudies have shown that these NLP models can be backdoored such that model\nbehavior is manipulated when trigger tokens are presented. In this paper, we\npropose PromptFix, a novel backdoor mitigation strategy for NLP models via\nadversarial prompt-tuning in few-shot settings. Unlike existing NLP backdoor\nremoval methods, which rely on accurate trigger inversion and subsequent model\nfine-tuning, PromptFix keeps the model parameters intact and only utilizes two\nextra sets of soft tokens which approximate the trigger and counteract it\nrespectively. The use of soft tokens and adversarial optimization eliminates\nthe need to enumerate possible backdoor configurations and enables an adaptive\nbalance between trigger finding and preservation of performance. Experiments\nwith various backdoor attacks validate the effectiveness of the proposed method\nand the performances when domain shift is present further shows PromptFix's\napplicability to models pretrained on unknown data source which is the common\ncase in prompt tuning scenarios.", "published": "2024-06-06 20:06:42", "link": "http://arxiv.org/abs/2406.04478v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CORU: Comprehensive Post-OCR Parsing and Receipt Understanding Dataset", "abstract": "In the fields of Optical Character Recognition (OCR) and Natural Language\nProcessing (NLP), integrating multilingual capabilities remains a critical\nchallenge, especially when considering languages with complex scripts such as\nArabic. This paper introduces the Comprehensive Post-OCR Parsing and Receipt\nUnderstanding Dataset (CORU), a novel dataset specifically designed to enhance\nOCR and information extraction from receipts in multilingual contexts involving\nArabic and English. CORU consists of over 20,000 annotated receipts from\ndiverse retail settings, including supermarkets and clothing stores, alongside\n30,000 annotated images for OCR that were utilized to recognize each detected\nline, and 10,000 items annotated for detailed information extraction. These\nannotations capture essential details such as merchant names, item\ndescriptions, total prices, receipt numbers, and dates. They are structured to\nsupport three primary computational tasks: object detection, OCR, and\ninformation extraction. We establish the baseline performance for a range of\nmodels on CORU to evaluate the effectiveness of traditional methods, like\nTesseract OCR, and more advanced neural network-based approaches. These\nbaselines are crucial for processing the complex and noisy document layouts\ntypical of real-world receipts and for advancing the state of automated\nmultilingual document processing. Our datasets are publicly accessible\n(https://github.com/Update-For-Integrated-Business-AI/CORU).", "published": "2024-06-06 20:38:15", "link": "http://arxiv.org/abs/2406.04493v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "NATURAL PLAN: Benchmarking LLMs on Natural Language Planning", "abstract": "We introduce NATURAL PLAN, a realistic planning benchmark in natural language\ncontaining 3 key tasks: Trip Planning, Meeting Planning, and Calendar\nScheduling. We focus our evaluation on the planning capabilities of LLMs with\nfull information on the task, by providing outputs from tools such as Google\nFlights, Google Maps, and Google Calendar as contexts to the models. This\neliminates the need for a tool-use environment for evaluating LLMs on Planning.\nWe observe that NATURAL PLAN is a challenging benchmark for state of the art\nmodels. For example, in Trip Planning, GPT-4 and Gemini 1.5 Pro could only\nachieve 31.1% and 34.8% solve rate respectively. We find that model performance\ndrops drastically as the complexity of the problem increases: all models\nperform below 5% when there are 10 cities, highlighting a significant gap in\nplanning in natural language for SoTA LLMs. We also conduct extensive ablation\nstudies on NATURAL PLAN to further shed light on the (in)effectiveness of\napproaches such as self-correction, few-shot generalization, and in-context\nplanning with long-contexts on improving LLM planning.", "published": "2024-06-06 21:27:35", "link": "http://arxiv.org/abs/2406.04520v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Proofread: Fixes All Errors with One Tap", "abstract": "The impressive capabilities in Large Language Models (LLMs) provide a\npowerful approach to reimagine users' typing experience. This paper\ndemonstrates Proofread, a novel Gboard feature powered by a server-side LLM in\nGboard, enabling seamless sentence-level and paragraph-level corrections with a\nsingle tap. We describe the complete system in this paper, from data\ngeneration, metrics design to model tuning and deployment. To obtain models\nwith sufficient quality, we implement a careful data synthetic pipeline\ntailored to online use cases, design multifaceted metrics, employ a two-stage\ntuning approach to acquire the dedicated LLM for the feature: the Supervised\nFine Tuning (SFT) for foundational quality, followed by the Reinforcement\nLearning (RL) tuning approach for targeted refinement. Specifically, we find\nsequential tuning on Rewrite and proofread tasks yields the best quality in SFT\nstage, and propose global and direct rewards in the RL tuning stage to seek\nfurther improvement. Extensive experiments on a human-labeled golden set showed\nour tuned PaLM2-XS model achieved 85.56\\% good ratio. We launched the feature\nto Pixel 8 devices by serving the model on TPU v5 in Google Cloud, with\nthousands of daily active users. Serving latency was significantly reduced by\nquantization, bucket inference, text segmentation, and speculative decoding.\nOur demo could be seen in \\href{https://youtu.be/4ZdcuiwFU7I}{Youtube}.", "published": "2024-06-06 21:38:08", "link": "http://arxiv.org/abs/2406.04523v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Label-Synchronous Neural Transducer for E2E Simultaneous Speech\n  Translation", "abstract": "While the neural transducer is popular for online speech recognition,\nsimultaneous speech translation (SST) requires both streaming and re-ordering\ncapabilities. This paper presents the LS-Transducer-SST, a label-synchronous\nneural transducer for SST, which naturally possesses these two properties. The\nLS-Transducer-SST dynamically decides when to emit translation tokens based on\nan Auto-regressive Integrate-and-Fire (AIF) mechanism. A latency-controllable\nAIF is also proposed, which can control the quality-latency trade-off either\nonly during decoding, or it can be used in both decoding and training. The\nLS-Transducer-SST can naturally utilise monolingual text-only data via its\nprediction network which helps alleviate the key issue of data sparsity for E2E\nSST. During decoding, a chunk-based incremental joint decoding technique is\ndesigned to refine and expand the search space. Experiments on the\nFisher-CallHome Spanish (Es-En) and MuST-C En-De data show that the\nLS-Transducer-SST gives a better quality-latency trade-off than existing\npopular methods. For example, the LS-Transducer-SST gives a 3.1/2.9 point BLEU\nincrease (Es-En/En-De) relative to CAAT at a similar latency and a 1.4 s\nreduction in average lagging latency with similar BLEU scores relative to\nWait-k.", "published": "2024-06-06 22:39:43", "link": "http://arxiv.org/abs/2406.04541v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Qabas: An Open-Source Arabic Lexicographic Database", "abstract": "We present Qabas, a novel open-source Arabic lexicon designed for NLP\napplications. The novelty of Qabas lies in its synthesis of 110 lexicons.\nSpecifically, Qabas lexical entries (lemmas) are assembled by linking lemmas\nfrom 110 lexicons. Furthermore, Qabas lemmas are also linked to 12\nmorphologically annotated corpora (about 2M tokens), making it the first Arabic\nlexicon to be linked to lexicons and corpora. Qabas was developed\nsemi-automatically, utilizing a mapping framework and a web-based tool.\nCompared with other lexicons, Qabas stands as the most extensive Arabic\nlexicon, encompassing about 58K lemmas (45K nominal lemmas, 12.5K verbal\nlemmas, and 473 functional-word lemmas). Qabas is open-source and accessible\nonline at https://sina.birzeit.edu/qabas.", "published": "2024-06-06 09:25:36", "link": "http://arxiv.org/abs/2406.06598v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prototypical Reward Network for Data-Efficient RLHF", "abstract": "The reward model for Reinforcement Learning from Human Feedback (RLHF) has\nproven effective in fine-tuning Large Language Models (LLMs). Notably,\ncollecting human feedback for RLHF can be resource-intensive and lead to\nscalability issues for LLMs and complex tasks. Our proposed framework Proto-RM\nleverages prototypical networks to enhance reward models under limited human\nfeedback. By enabling stable and reliable structural learning from fewer\nsamples, Proto-RM significantly enhances LLMs' adaptability and accuracy in\ninterpreting human preferences. Extensive experiments on various datasets\ndemonstrate that Proto-RM significantly improves the performance of reward\nmodels and LLMs in human feedback tasks, achieving comparable and usually\nbetter results than traditional methods, while requiring significantly less\ndata. in data-limited scenarios. This research offers a promising direction for\nenhancing the efficiency of reward models and optimizing the fine-tuning of\nlanguage models under restricted feedback conditions.", "published": "2024-06-06 15:23:30", "link": "http://arxiv.org/abs/2406.06606v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Prompt Report: A Systematic Survey of Prompt Engineering Techniques", "abstract": "Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date.", "published": "2024-06-06 18:10:11", "link": "http://arxiv.org/abs/2406.06608v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with\n  Multi-Modal Context and Large Language Model", "abstract": "Recent advances in large language models (LLMs) and development of audio\ncodecs greatly propel the zero-shot TTS. They can synthesize personalized\nspeech with only a 3-second speech of an unseen speaker as acoustic prompt.\nHowever, they only support short speech prompts and cannot leverage longer\ncontext information, as required in audiobook and conversational TTS scenarios.\nIn this paper, we introduce a novel audio codec-based TTS model to adapt\ncontext features with multiple enhancements. Inspired by the success of\nQformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to\nutilize additional multi-modal context information. Besides, we adapt a\npretrained LLM to leverage its understanding ability to predict semantic\ntokens, and use a SoundStorm to generate acoustic tokens thereby enhancing\naudio quality and speaker similarity. The extensive objective and subjective\nevaluations show that our proposed method outperforms baselines across various\ncontext TTS scenarios.", "published": "2024-06-06 03:06:45", "link": "http://arxiv.org/abs/2406.03706v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "What Should Embeddings Embed? Autoregressive Models Represent Latent\n  Generating Distributions", "abstract": "Autoregressive language models have demonstrated a remarkable ability to\nextract latent structure from text. The embeddings from large language models\nhave been shown to capture aspects of the syntax and semantics of language. But\nwhat {\\em should} embeddings represent? We connect the autoregressive\nprediction objective to the idea of constructing predictive sufficient\nstatistics to summarize the information contained in a sequence of\nobservations, and use this connection to identify three settings where the\noptimal content of embeddings can be identified: independent identically\ndistributed data, where the embedding should capture the sufficient statistics\nof the data; latent state models, where the embedding should encode the\nposterior distribution over states given the data; and discrete hypothesis\nspaces, where the embedding should reflect the posterior distribution over\nhypotheses given the data. We then conduct empirical probing studies to show\nthat transformers encode these three kinds of latent generating distributions,\nand that they perform well in out-of-distribution cases and without token\nmemorization in these settings.", "published": "2024-06-06 03:06:46", "link": "http://arxiv.org/abs/2406.03707v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML", "I.2; I.5"], "primary_category": "cs.LG"}
{"title": "Generalization-Enhanced Code Vulnerability Detection via Multi-Task\n  Instruction Fine-Tuning", "abstract": "Code Pre-trained Models (CodePTMs) based vulnerability detection have\nachieved promising results over recent years. However, these models struggle to\ngeneralize as they typically learn superficial mapping from source code to\nlabels instead of understanding the root causes of code vulnerabilities,\nresulting in poor performance in real-world scenarios beyond the training\ninstances. To tackle this challenge, we introduce VulLLM, a novel framework\nthat integrates multi-task learning with Large Language Models (LLMs) to\neffectively mine deep-seated vulnerability features. Specifically, we construct\ntwo auxiliary tasks beyond the vulnerability detection task. First, we utilize\nthe vulnerability patches to construct a vulnerability localization task.\nSecond, based on the vulnerability features extracted from patches, we leverage\nGPT-4 to construct a vulnerability interpretation task. VulLLM innovatively\naugments vulnerability classification by leveraging generative LLMs to\nunderstand complex vulnerability patterns, thus compelling the model to capture\nthe root causes of vulnerabilities rather than overfitting to spurious features\nof a single task. The experiments conducted on six large datasets demonstrate\nthat VulLLM surpasses seven state-of-the-art models in terms of effectiveness,\ngeneralization, and robustness.", "published": "2024-06-06 03:29:05", "link": "http://arxiv.org/abs/2406.03718v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "XL-HeadTags: Leveraging Multimodal Retrieval Augmentation for the\n  Multilingual Generation of News Headlines and Tags", "abstract": "Millions of news articles published online daily can overwhelm readers.\nHeadlines and entity (topic) tags are essential for guiding readers to decide\nif the content is worth their time. While headline generation has been\nextensively studied, tag generation remains largely unexplored, yet it offers\nreaders better access to topics of interest. The need for conciseness in\ncapturing readers' attention necessitates improved content selection strategies\nfor identifying salient and relevant segments within lengthy articles, thereby\nguiding language models effectively. To address this, we propose to leverage\nauxiliary information such as images and captions embedded in the articles to\nretrieve relevant sentences and utilize instruction tuning with variations to\ngenerate both headlines and tags for news articles in a multilingual context.\nTo make use of the auxiliary information, we have compiled a dataset named\nXL-HeadTags, which includes 20 languages across 6 diverse language families.\nThrough extensive evaluation, we demonstrate the effectiveness of our\nplug-and-play multimodal-multilingual retrievers for both tasks. Additionally,\nwe have developed a suite of tools for processing and evaluating multilingual\ntexts, significantly contributing to the research community by enabling more\naccurate and efficient analysis across languages.", "published": "2024-06-06 06:40:19", "link": "http://arxiv.org/abs/2406.03776v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Tool-Planner: Task Planning with Clusters across Multiple Tools", "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\nhttps://github.com/OceannTwT/Tool-Planner", "published": "2024-06-06 07:30:14", "link": "http://arxiv.org/abs/2406.03807v3", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and\n  Gated Monolingual Datastores", "abstract": "The kNN-CTC model has proven to be effective for monolingual automatic speech\nrecognition (ASR). However, its direct application to multilingual scenarios\nlike code-switching, presents challenges. Although there is potential for\nperformance improvement, a kNN-CTC model utilizing a single bilingual datastore\ncan inadvertently introduce undesirable noise from the alternative language. To\naddress this, we propose a novel kNN-CTC-based code-switching ASR (CS-ASR)\nframework that employs dual monolingual datastores and a gated datastore\nselection mechanism to reduce noise interference. Our method selects the\nappropriate datastore for decoding each frame, ensuring the injection of\nlanguage-specific information into the ASR process. We apply this framework to\ncutting-edge CTC-based models, developing an advanced CS-ASR system. Extensive\nexperiments demonstrate the remarkable effectiveness of our gated datastore\nmechanism in enhancing the performance of zero-shot Chinese-English CS-ASR.", "published": "2024-06-06 07:39:17", "link": "http://arxiv.org/abs/2406.03814v5", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MuJo: Multimodal Joint Feature Space Learning for Human Activity\n  Recognition", "abstract": "Human activity recognition (HAR) is a long-standing problem in artificial\nintelligence with applications in a broad range of areas, including healthcare,\nsports and fitness, security, and more. The performance of HAR in real-world\nsettings is strongly dependent on the type and quality of the input signal that\ncan be acquired. Given an unobstructed, high-quality camera view of a scene,\ncomputer vision systems, in particular in conjunction with foundation models,\ncan today fairly reliably distinguish complex activities. On the other hand,\nrecognition using modalities such as wearable sensors (which are often more\nbroadly available, e.g., in mobile phones and smartwatches) is a more difficult\nproblem, as the signals often contain less information and labeled training\ndata is more difficult to acquire. To alleviate the need for labeled data, we\nintroduce our comprehensive Fitness Multimodal Activity Dataset (FiMAD) in this\nwork, which can be used with the proposed pre-training method MuJo (Multimodal\nJoint Feature Space Learning) to enhance HAR performance across various\nmodalities. FiMAD was created using YouTube fitness videos and contains\nparallel video, language, pose, and simulated IMU sensor data. MuJo utilizes\nthis dataset to learn a joint feature space for these modalities. We show that\nclassifiers pre-trained on FiMAD can increase the performance on real HAR\ndatasets such as MM-Fit, MyoGym, MotionSense, and MHEALTH. For instance, on\nMM-Fit, we achieve a Macro F1-Score of up to 0.855 when fine-tuning on only 2%\nof the training data and 0.942 when utilizing the complete training set for\nclassification tasks. We compare our approach with other self-supervised ones\nand show that, unlike them, ours consistently improves compared to the baseline\nnetwork performance while also providing better data efficiency.", "published": "2024-06-06 08:42:36", "link": "http://arxiv.org/abs/2406.03857v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "abstract": "The recent release of GPT-4o showcased the potential of end-to-end multimodal\nmodels, not just in terms of low latency but also in their ability to\nunderstand and generate expressive speech with rich emotions. While the details\nare unknown to the open research community, it likely involves significant\namounts of curated data and compute, neither of which is readily accessible. In\nthis paper, we present BLSP-Emo (Bootstrapped Language-Speech Pretraining with\nEmotion support), a novel approach to developing an end-to-end speech-language\nmodel capable of understanding both semantics and emotions in speech and\ngenerate empathetic responses. BLSP-Emo utilizes existing speech recognition\n(ASR) and speech emotion recognition (SER) datasets through a two-stage\nprocess. The first stage focuses on semantic alignment, following recent work\non pretraining speech-language models using ASR data. The second stage performs\nemotion alignment with the pretrained speech-language model on an emotion-aware\ncontinuation task constructed from SER data. Our experiments demonstrate that\nthe BLSP-Emo model excels in comprehending speech and delivering empathetic\nresponses, both in instruction-following tasks and conversations.", "published": "2024-06-06 09:02:31", "link": "http://arxiv.org/abs/2406.03872v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Spontaneous Speech-Based Suicide Risk Detection Using Whisper and Large\n  Language Models", "abstract": "The early detection of suicide risk is important since it enables the\nintervention to prevent potential suicide attempts. This paper studies the\nautomatic detection of suicide risk based on spontaneous speech from\nadolescents, and collects a Mandarin dataset with 15 hours of suicide speech\nfrom more than a thousand adolescents aged from ten to eighteen for our\nexperiments. To leverage the diverse acoustic and linguistic features embedded\nin spontaneous speech, both the Whisper speech model and textual large language\nmodels (LLMs) are used for suicide risk detection. Both all-parameter\nfinetuning and parameter-efficient finetuning approaches are used to adapt the\npre-trained models for suicide risk detection, and multiple audio-text fusion\napproaches are evaluated to combine the representations of Whisper and the LLM.\nThe proposed system achieves a detection accuracy of 0.807 and an F1-score of\n0.846 on the test set with 119 subjects, indicating promising potential for\nreal suicide risk detection applications.", "published": "2024-06-06 09:21:13", "link": "http://arxiv.org/abs/2406.03882v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ArMeme: Propagandistic Content in Arabic Memes", "abstract": "With the rise of digital communication, memes have become a significant\nmedium for cultural and political expression that is often used to mislead\naudiences. Identification of such misleading and persuasive multimodal content\nhas become more important among various stakeholders, including social media\nplatforms, policymakers, and the broader society as they often cause harm to\nindividuals, organizations, and/or society. While there has been effort to\ndevelop AI-based automatic systems for resource-rich languages (e.g., English),\nit is relatively little to none for medium to low resource languages. In this\nstudy, we focused on developing an Arabic memes dataset with manual annotations\nof propagandistic content. We annotated ~6K Arabic memes collected from various\nsocial media platforms, which is a first resource for Arabic multimodal\nresearch. We provide a comprehensive analysis aiming to develop computational\ntools for their detection. We will make them publicly available for the\ncommunity.", "published": "2024-06-06 09:56:49", "link": "http://arxiv.org/abs/2406.03916v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Ask LLMs Directly, \"What shapes your bias?\": Measuring Social Bias in\n  Large Language Models", "abstract": "Social bias is shaped by the accumulation of social perceptions towards\ntargets across various demographic identities. To fully understand such social\nbias in large language models (LLMs), it is essential to consider the composite\nof social perceptions from diverse perspectives among identities. Previous\nstudies have either evaluated biases in LLMs by indirectly assessing the\npresence of sentiments towards demographic identities in the generated text or\nmeasuring the degree of alignment with given stereotypes. These methods have\nlimitations in directly quantifying social biases at the level of distinct\nperspectives among identities. In this paper, we aim to investigate how social\nperceptions from various viewpoints contribute to the development of social\nbias in LLMs. To this end, we propose a novel strategy to intuitively quantify\nthese social perceptions and suggest metrics that can evaluate the social\nbiases within LLMs by aggregating diverse social perceptions. The experimental\nresults show the quantitative demonstration of the social attitude in LLMs by\nexamining social perception. The analysis we conducted shows that our proposed\nmetrics capture the multi-dimensional aspects of social bias, enabling a\nfine-grained and comprehensive investigation of bias in LLMs.", "published": "2024-06-06 13:32:09", "link": "http://arxiv.org/abs/2406.04064v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Legal Judgment Reimagined: PredEx and the Rise of Intelligent AI\n  Interpretation in Indian Courts", "abstract": "In the era of Large Language Models (LLMs), predicting judicial outcomes\nposes significant challenges due to the complexity of legal proceedings and the\nscarcity of expert-annotated datasets. Addressing this, we introduce\n\\textbf{Pred}iction with \\textbf{Ex}planation (\\texttt{PredEx}), the largest\nexpert-annotated dataset for legal judgment prediction and explanation in the\nIndian context, featuring over 15,000 annotations. This groundbreaking corpus\nsignificantly enhances the training and evaluation of AI models in legal\nanalysis, with innovations including the application of instruction tuning to\nLLMs. This method has markedly improved the predictive accuracy and explanatory\ndepth of these models for legal judgments. We employed various\ntransformer-based models, tailored for both general and Indian legal contexts.\nThrough rigorous lexical, semantic, and expert assessments, our models\neffectively leverage \\texttt{PredEx} to provide precise predictions and\nmeaningful explanations, establishing it as a valuable benchmark for both the\nlegal profession and the NLP community.", "published": "2024-06-06 14:57:48", "link": "http://arxiv.org/abs/2406.04136v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Language Models Understand Morality? Towards a Robust Detection of\n  Moral Content", "abstract": "The task of detecting moral values in text has significant implications in\nvarious fields, including natural language processing, social sciences, and\nethical decision-making. Previously proposed supervised models often suffer\nfrom overfitting, leading to hyper-specialized moral classifiers that struggle\nto perform well on data from different domains. To address this issue, we\nintroduce novel systems that leverage abstract concepts and common-sense\nknowledge acquired from Large Language Models and Natural Language Inference\nmodels during previous stages of training on multiple data sources. By doing\nso, we aim to develop versatile and robust methods for detecting moral values\nin real-world scenarios. Our approach uses the GPT 3.5 model as a zero-shot\nready-made unsupervised multi-label classifier for moral values detection,\neliminating the need for explicit training on labeled data. We compare it with\na smaller NLI-based zero-shot model. The results show that the NLI approach\nachieves competitive results compared to the Davinci model. Furthermore, we\nconduct an in-depth investigation of the performance of supervised systems in\nthe context of cross-domain multi-label moral value detection. This involves\ntraining supervised models on different domains to explore their effectiveness\nin handling data from different sources and comparing their performance with\nthe unsupervised methods. Our contributions encompass a thorough analysis of\nboth supervised and unsupervised methodologies for cross-domain value\ndetection. We introduce the Davinci model as a state-of-the-art zero-shot\nunsupervised moral values classifier, pushing the boundaries of moral value\ndetection without the need for explicit training on labeled data. Additionally,\nwe perform a comparative evaluation of our approach with the supervised models,\nshedding light on their respective strengths and weaknesses.", "published": "2024-06-06 15:08:16", "link": "http://arxiv.org/abs/2406.04143v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pointer-Guided Pre-Training: Infusing Large Language Models with\n  Paragraph-Level Contextual Awareness", "abstract": "We introduce \"pointer-guided segment ordering\" (SO), a novel pre-training\ntechnique aimed at enhancing the contextual understanding of paragraph-level\ntext representations in large language models. Our methodology leverages a\nself-attention-driven pointer network to restore the original sequence of\nshuffled text segments, addressing the challenge of capturing the structural\ncoherence and contextual dependencies within documents. This pre-training\napproach is complemented by a fine-tuning methodology that incorporates dynamic\nsampling, augmenting the diversity of training instances and improving sample\nefficiency for various downstream applications. We evaluate our method on a\ndiverse set of datasets, demonstrating its efficacy in tasks requiring\nsequential text classification across scientific literature and financial\nreporting domains. Our experiments show that pointer-guided pre-training\nsignificantly enhances the model's ability to understand complex document\nstructures, leading to state-of-the-art performance in downstream\nclassification tasks.", "published": "2024-06-06 15:17:51", "link": "http://arxiv.org/abs/2406.04156v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation\n  Strategy by Language Models and Humans", "abstract": "It is very challenging to curate a dataset for language-specific knowledge\nand common sense in order to evaluate natural language understanding\ncapabilities of language models. Due to the limitation in the availability of\nannotators, most current multilingual datasets are created through translation,\nwhich cannot evaluate such language-specific aspects. Therefore, we propose\nMultilingual CommonsenseQA (mCSQA) based on the construction process of CSQA\nbut leveraging language models for a more efficient construction, e.g., by\nasking LM to generate questions/answers, refine answers and verify QAs followed\nby reduced human efforts for verification. Constructed dataset is a benchmark\nfor cross-lingual language-transfer capabilities of multilingual LMs, and\nexperimental results showed high language-transfer capabilities for questions\nthat LMs could easily solve, but lower transfer capabilities for questions\nrequiring deep knowledge or commonsense. This highlights the necessity of\nlanguage-specific datasets for evaluation and training. Finally, our method\ndemonstrated that multilingual LMs could create QA including language-specific\nknowledge, significantly reducing the dataset creation cost compared to manual\ncreation. The datasets are available at\nhttps://huggingface.co/datasets/yusuke1997/mCSQA.", "published": "2024-06-06 16:14:54", "link": "http://arxiv.org/abs/2406.04215v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MLVU: Benchmarking Multi-task Long Video Understanding", "abstract": "The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs.", "published": "2024-06-06 17:09:32", "link": "http://arxiv.org/abs/2406.04264v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Self-Play with Adversarial Critic: Provable and Scalable Offline\n  Alignment for Language Models", "abstract": "This work studies the challenge of aligning large language models (LLMs) with\noffline preference data. We focus on alignment by Reinforcement Learning from\nHuman Feedback (RLHF) in particular. While popular preference optimization\nmethods exhibit good empirical performance in practice, they are not\ntheoretically guaranteed to converge to the optimal policy and can provably\nfail when the data coverage is sparse by classical offline reinforcement\nlearning (RL) results. On the other hand, a recent line of work has focused on\ntheoretically motivated preference optimization methods with provable\nguarantees, but these are not computationally efficient for large-scale\napplications like LLM alignment. To bridge this gap, we propose SPAC, a new\noffline preference optimization method with self-play, inspired by the\non-average pessimism technique from the offline RL literature, to be the first\nprovable and scalable approach to LLM alignment. We both provide theoretical\nanalysis for its convergence under single-policy concentrability for the\ngeneral function approximation setting and demonstrate its competitive\nempirical performance for LLM alignment on a 7B Mistral model with Open LLM\nLeaderboard evaluations.", "published": "2024-06-06 17:23:49", "link": "http://arxiv.org/abs/2406.04274v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval", "abstract": "Multi-modal retrieval becomes increasingly popular in practice. However, the\nexisting retrievers are mostly text-oriented, which lack the capability to\nprocess visual information. Despite the presence of vision-language models like\nCLIP, the current methods are severely limited in representing the text-only\nand image-only data. In this work, we present a new embedding model VISTA for\nuniversal multi-modal retrieval. Our work brings forth threefold technical\ncontributions. Firstly, we introduce a flexible architecture which extends a\npowerful text encoder with the image understanding capability by introducing\nvisual token embeddings. Secondly, we develop two data generation strategies,\nwhich bring high-quality composed image-text to facilitate the training of the\nembedding model. Thirdly, we introduce a multi-stage training algorithm, which\nfirst aligns the visual token embedding with the text encoder using massive\nweakly labeled data, and then develops multi-modal representation capability\nusing the generated composed image-text data. In our experiments, VISTA\nachieves superior performances across a variety of multi-modal retrieval tasks\nin both zero-shot and supervised settings. Our model, data, and source code are\navailable at https://github.com/FlagOpen/FlagEmbedding.", "published": "2024-06-06 17:37:47", "link": "http://arxiv.org/abs/2406.04292v1", "categories": ["cs.IR", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "PaCE: Parsimonious Concept Engineering for Large Language Models", "abstract": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable outputs via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Given\nany alignment task, we instruct a concept partitioner to efficiently annotate\nthe concepts as benign or undesirable. Then, at inference time, we decompose\nthe LLM activations along the concept dictionary via sparse coding, to\naccurately represent the activations as linear combinations of benign and\nundesirable components. By removing the latter ones from the activations, we\nreorient the behavior of the LLM towards the alignment goal. We conduct\nexperiments on tasks such as response detoxification, faithfulness enhancement,\nand sentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.", "published": "2024-06-06 17:59:10", "link": "http://arxiv.org/abs/2406.04331v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models", "abstract": "Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbalized machine learning (VML). In contrast to conventional\nmachine learning (ML) models that are typically optimized over a continuous\nparameter space, VML constrains the parameter space to be human-interpretable\nnatural language. Such a constraint leads to a new perspective of function\napproximation, where an LLM with a text prompt can be viewed as a function\nparameterized by the text prompt. Guided by this perspective, we revisit\nclassical ML problems, such as regression and classification, and find that\nthese problems can be solved by an LLM-parameterized learner and optimizer. The\nmajor advantages of VML include (1) easy encoding of inductive bias: prior\nknowledge about the problem and hypothesis class can be encoded in natural\nlanguage and fed into the LLM-parameterized learner; (2) automatic model class\nselection: the optimizer can automatically select a model class based on data\nand verbalized prior knowledge, and it can update the model class during\ntraining; and (3) interpretable learner updates: the LLM-parameterized\noptimizer can provide explanations for why an update is performed. We\nempirically verify the effectiveness of VML, and hope that VML can serve as a\nstepping stone to stronger interpretability.", "published": "2024-06-06 17:59:56", "link": "http://arxiv.org/abs/2406.04344v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "VHDL-Eval: A Framework for Evaluating Large Language Models in VHDL Code\n  Generation", "abstract": "With the unprecedented advancements in Large Language Models (LLMs), their\napplication domains have expanded to include code generation tasks across\nvarious programming languages. While significant progress has been made in\nenhancing LLMs for popular programming languages, there exists a notable gap in\ncomprehensive evaluation frameworks tailored for Hardware Description Languages\n(HDLs), particularly VHDL. This paper addresses this gap by introducing a\ncomprehensive evaluation framework designed specifically for assessing LLM\nperformance in VHDL code generation task. We construct a dataset for evaluating\nLLMs on VHDL code generation task. This dataset is constructed by translating a\ncollection of Verilog evaluation problems to VHDL and aggregating publicly\navailable VHDL problems, resulting in a total of 202 problems. To assess the\nfunctional correctness of the generated VHDL code, we utilize a curated set of\nself-verifying testbenches specifically designed for those aggregated VHDL\nproblem set. We conduct an initial evaluation of different LLMs and their\nvariants, including zero-shot code generation, in-context learning (ICL), and\nParameter-efficient fine-tuning (PEFT) methods. Our findings underscore the\nconsiderable challenges faced by existing LLMs in VHDL code generation,\nrevealing significant scope for improvement. This study emphasizes the\nnecessity of supervised fine-tuning code generation models specifically for\nVHDL, offering potential benefits to VHDL designers seeking efficient code\ngeneration solutions.", "published": "2024-06-06 00:06:50", "link": "http://arxiv.org/abs/2406.04379v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Why Has Predicting Downstream Capabilities of Frontier AI Models with\n  Scale Remained Elusive?", "abstract": "Predicting changes from scaling advanced AI systems is a desirable property\nfor engineers, economists, governments and industry alike, and, while a\nwell-established literature exists on how pretraining performance scales,\npredictable scaling behavior on downstream capabilities remains elusive. While\nmany factors are certainly responsible, this paper identifies a significant\nfactor that makes predicting scaling behavior on widely used multiple-choice\nquestion answering benchmarks challenging and illuminates a path towards making\nsuch downstream evaluations predictable with scale. Using five model families\nand twelve well-established multiple-choice benchmarks, we demonstrate that\ndownstream performance is computed from negative log likelihoods via a sequence\nof transformations that progressively degrades the statistical relationship\nbetween performance and scale. We then pinpoint the mechanism causing this\ndegradation: downstream metrics require comparing the correct choice against a\nsmall number of specific incorrect choices, meaning accurately predicting\ndownstream capabilities requires predicting not just how probability mass\nconcentrates on the correct choice with scale, but also how probability mass\nfluctuates on the alternative incorrect choices with scale. We empirically\nstudy how probability mass on the correct choice co-varies with probability\nmass on incorrect choices with increasing compute, suggesting that scaling laws\nfor \\textit{incorrect} choices might be achievable. Our work also explains why\npretraining scaling laws are commonly regarded as more predictable than\ndownstream capabilities and contributes towards establishing\nscaling-predictable evaluations of frontier AI models.", "published": "2024-06-06 17:46:56", "link": "http://arxiv.org/abs/2406.04391v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Spread Preference Annotation: Direct Preference Judgment for Efficient\n  LLM Alignment", "abstract": "Aligning large language models (LLMs) with human preferences becomes a key\ncomponent to obtaining state-of-the-art performance, but it yields a huge cost\nto construct a large human-annotated preference dataset. To tackle this\nproblem, we propose a new framework, Spread Preference Annotation with direct\npreference judgment (SPA), that boosts the alignment of LLMs using only a very\nsmall amount of human-annotated preference data. Our key idea is leveraging the\nhuman prior knowledge within the small (seed) data and progressively improving\nthe alignment of LLM, by iteratively generating the responses and learning from\nthem with the self-annotated preference data. To be specific, we propose to\nderive the preference label from the logits of LLM to explicitly extract the\nmodel's inherent preference. Compared to the previous approaches using external\nreward models or implicit in-context learning, we observe that the proposed\napproach is significantly more effective. In addition, we introduce a\nnoise-aware preference learning algorithm to mitigate the risk of low quality\nwithin generated preference data. Our experimental results demonstrate that the\nproposed framework significantly boosts the alignment of LLMs. For example, we\nachieve superior alignment performance on AlpacaEval 2.0 with only 3.3% of the\nground-truth preference labels in the Ultrafeedback data compared to the cases\nusing the entire data or state-of-the-art baselines.", "published": "2024-06-06 18:01:02", "link": "http://arxiv.org/abs/2406.04412v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LipGER: Visually-Conditioned Generative Error Correction for Robust\n  Automatic Speech Recognition", "abstract": "Visual cues, like lip motion, have been shown to improve the performance of\nAutomatic Speech Recognition (ASR) systems in noisy environments. We propose\nLipGER (Lip Motion aided Generative Error Correction), a novel framework for\nleveraging visual cues for noise-robust ASR. Instead of learning the\ncross-modal correlation between the audio and visual modalities, we make an LLM\nlearn the task of visually-conditioned (generative) ASR error correction.\nSpecifically, we instruct an LLM to predict the transcription from the N-best\nhypotheses generated using ASR beam-search. This is further conditioned on lip\nmotions. This approach addresses key challenges in traditional AVSR learning,\nsuch as the lack of large-scale paired datasets and difficulties in adapting to\nnew domains. We experiment on 4 datasets in various settings and show that\nLipGER improves the Word Error Rate in the range of 1.1%-49.2%. We also release\nLipHyp, a large-scale dataset with hypothesis-transcription pairs that is\nadditionally equipped with lip motion cues to promote further research in this\nspace", "published": "2024-06-06 18:17:59", "link": "http://arxiv.org/abs/2406.04432v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Small-E: Small Language Model with Linear Attention for Efficient Speech\n  Synthesis", "abstract": "Recent advancements in text-to-speech (TTS) powered by language models have\nshowcased remarkable capabilities in achieving naturalness and zero-shot voice\ncloning. Notably, the decoder-only transformer is the prominent architecture in\nthis domain. However, transformers face challenges stemming from their\nquadratic complexity in sequence length, impeding training on lengthy sequences\nand resource-constrained hardware. Moreover they lack specific inductive bias\nwith regards to the monotonic nature of TTS alignments. In response, we propose\nto replace transformers with emerging recurrent architectures and introduce\nspecialized cross-attention mechanisms for reducing repeating and skipping\nissues. Consequently our architecture can be efficiently trained on long\nsamples and achieve state-of-the-art zero-shot voice cloning against baselines\nof comparable size. Our implementation and demos are available at\nhttps://github.com/theodorblackbird/lina-speech.", "published": "2024-06-06 19:48:17", "link": "http://arxiv.org/abs/2406.04467v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic Bug Detection in LLM-Powered Text-Based Games Using LLMs", "abstract": "Advancements in large language models (LLMs) are revolutionizing interactive\ngame design, enabling dynamic plotlines and interactions between players and\nnon-player characters (NPCs). However, LLMs may exhibit flaws such as\nhallucinations, forgetfulness, or misinterpretations of prompts, causing\nlogical inconsistencies and unexpected deviations from intended designs.\nAutomated techniques for detecting such game bugs are still lacking. To address\nthis, we propose a systematic LLM-based method for automatically identifying\nsuch bugs from player game logs, eliminating the need for collecting additional\ndata such as post-play surveys. Applied to a text-based game DejaBoom!, our\napproach effectively identifies bugs inherent in LLM-powered interactive games,\nsurpassing unstructured LLM-powered bug-catching methods and filling the gap in\nautomated detection of logical and design flaws.", "published": "2024-06-06 20:11:08", "link": "http://arxiv.org/abs/2406.04482v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Time Sensitive Knowledge Editing through Efficient Finetuning", "abstract": "Large Language Models (LLMs) have demonstrated impressive capability in\ndifferent tasks and are bringing transformative changes to many domains.\nHowever, keeping the knowledge in LLMs up-to-date remains a challenge once\npretraining is complete. It is thus essential to design effective methods to\nboth update obsolete knowledge and induce new knowledge into LLMs. Existing\nlocate-and-edit knowledge editing (KE) method suffers from two limitations.\nFirst, the post-edit LLMs by such methods generally have poor capability in\nanswering complex queries that require multi-hop reasoning. Second, the long\nrun-time of such locate-and-edit methods to perform knowledge edits make it\ninfeasible for large scale KE in practice. In this paper, we explore\nParameter-Efficient Fine-Tuning (PEFT) techniques as an alternative for KE. We\ncurate a more comprehensive temporal KE dataset with both knowledge update and\nknowledge injection examples for KE performance benchmarking. We further probe\nthe effect of fine-tuning on a range of layers in an LLM for the multi-hop QA\ntask. We find that PEFT performs better than locate-and-edit techniques for\ntime-sensitive knowledge edits.", "published": "2024-06-06 20:41:36", "link": "http://arxiv.org/abs/2406.04496v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FLUID-LLM: Learning Computational Fluid Dynamics with\n  Spatiotemporal-aware Large Language Models", "abstract": "Learning computational fluid dynamics (CFD) traditionally relies on\ncomputationally intensive simulations of the Navier-Stokes equations. Recently,\nlarge language models (LLMs) have shown remarkable pattern recognition and\nreasoning abilities in natural language processing (NLP) and computer vision\n(CV). However, these models struggle with the complex geometries inherent in\nfluid dynamics. We introduce FLUID-LLM, a novel framework combining pre-trained\nLLMs with spatiotemporal-aware encoding to predict unsteady fluid dynamics. Our\napproach leverages the temporal autoregressive abilities of LLMs alongside\nspatial-aware layers, bridging the gap between previous CFD prediction methods.\nEvaluations on standard benchmarks reveal significant performance improvements\nacross various fluid datasets. Our results demonstrate that FLUID-LLM\neffectively integrates spatiotemporal information into pre-trained LLMs,\nenhancing CFD task performance.", "published": "2024-06-06 20:55:40", "link": "http://arxiv.org/abs/2406.04501v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "To Distill or Not to Distill? On the Robustness of Robust Knowledge\n  Distillation", "abstract": "Arabic is known to present unique challenges for Automatic Speech Recognition\n(ASR). On one hand, its rich linguistic diversity and wide range of dialects\ncomplicate the development of robust, inclusive models. On the other, current\nmultilingual ASR models are compute-intensive and lack proper comprehensive\nevaluations. In light of these challenges, we distill knowledge from large\nteacher models into smaller student variants that are more efficient. We also\nintroduce a novel human-annotated dataset covering five under-represented\nArabic dialects for evaluation. We further evaluate both our models and\nexisting SoTA multilingual models on both standard available benchmarks and our\nnew dialectal data. Our best-distilled model's overall performance ($45.0$\\%\nWER) surpasses that of a SoTA model twice its size (SeamlessM4T-large-v2,\nWER=$47.0$\\%) and its teacher model (Whisper-large-v2, WER=$55.1$\\%), and its\naverage performance on our new dialectal data ($56.9$\\% WER) outperforms all\nother models. To gain more insight into the poor performance of these models on\ndialectal data, we conduct an error analysis and report the main types of\nerrors the different models tend to make. The GitHub repository for the project\nis available at \\url{https://github.com/UBC-NLP/distill-whisper-ar}.", "published": "2024-06-06 21:11:53", "link": "http://arxiv.org/abs/2406.04512v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models the New Interface for Data Pipelines?", "abstract": "A Language Model is a term that encompasses various types of models designed\nto understand and generate human communication. Large Language Models (LLMs)\nhave gained significant attention due to their ability to process text with\nhuman-like fluency and coherence, making them valuable for a wide range of\ndata-related tasks fashioned as pipelines. The capabilities of LLMs in natural\nlanguage understanding and generation, combined with their scalability,\nversatility, and state-of-the-art performance, enable innovative applications\nacross various AI-related fields, including eXplainable Artificial Intelligence\n(XAI), Automated Machine Learning (AutoML), and Knowledge Graphs (KG).\nFurthermore, we believe these models can extract valuable insights and make\ndata-driven decisions at scale, a practice commonly referred to as Big Data\nAnalytics (BDA). In this position paper, we provide some discussions in the\ndirection of unlocking synergies among these technologies, which can lead to\nmore powerful and intelligent AI solutions, driving improvements in data\npipelines across a wide range of applications and domains integrating humans,\ncomputers, and knowledge.", "published": "2024-06-06 08:10:32", "link": "http://arxiv.org/abs/2406.06596v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "HORAE: A Domain-Agnostic Modeling Language for Automating Multimodal\n  Service Regulation", "abstract": "Artificial intelligence is rapidly encroaching on the field of service\nregulation. This work-in-progress article presents the design principles behind\nHORAE, a unified specification language to model multimodal regulation rules\nacross a diverse set of domains. We show how HORAE facilitates an intelligent\nservice regulation pipeline by further exploiting a fine-tuned large language\nmodel named HORAE that automates the HORAE modeling process, thereby yielding\nan end-to-end framework for fully automated intelligent service regulation.", "published": "2024-06-06 13:44:57", "link": "http://arxiv.org/abs/2406.06600v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Human-in-the-Loop Approach to Improving Cross-Text Prosody Transfer", "abstract": "Text-To-Speech (TTS) prosody transfer models can generate varied prosodic\nrenditions, for the same text, by conditioning on a reference utterance. These\nmodels are trained with a reference that is identical to the target utterance.\nBut when the reference utterance differs from the target text, as in cross-text\nprosody transfer, these models struggle to separate prosody from text,\nresulting in reduced perceived naturalness. To address this, we propose a\nHuman-in-the-Loop (HitL) approach. HitL users adjust salient correlates of\nprosody to make the prosody more appropriate for the target text, while\nmaintaining the overall reference prosodic effect. Human adjusted renditions\nmaintain the reference prosody while being rated as more appropriate for the\ntarget text $57.8\\%$ of the time. Our analysis suggests that limited user\neffort suffices for these improvements, and that closeness in the latent\nreference space is not a reliable prosodic similarity metric for the cross-text\ncondition.", "published": "2024-06-06 14:01:53", "link": "http://arxiv.org/abs/2406.06601v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Reinterpreting 'the Company a Word Keeps': Towards Explainable and\n  Ontologically Grounded Language Models", "abstract": "We argue that the relative success of large language models (LLMs) is not a\nreflection on the symbolic vs. subsymbolic debate but a reflection on employing\na successful bottom-up strategy of a reverse engineering of language at scale.\nHowever, and due to their subsymbolic nature whatever knowledge these systems\nacquire about language will always be buried in millions of weights none of\nwhich is meaningful on its own, rendering such systems utterly unexplainable.\nFurthermore, and due to their stochastic nature, LLMs will often fail in making\nthe correct inferences in various linguistic contexts that require reasoning in\nintensional, temporal, or modal contexts. To remedy these shortcomings we\nsuggest employing the same successful bottom-up strategy employed in LLMs but\nin a symbolic setting, resulting in explainable, language-agnostic, and\nontologically grounded language models.", "published": "2024-06-06 20:38:35", "link": "http://arxiv.org/abs/2406.06610v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The CLRS-Text Algorithmic Reasoning Language Benchmark", "abstract": "Eliciting reasoning capabilities from language models (LMs) is a critical\ndirection on the path towards building intelligent systems. Most recent studies\ndedicated to reasoning focus on out-of-distribution performance on\nprocedurally-generated synthetic benchmarks, bespoke-built to evaluate specific\nskills only. This trend makes results hard to transfer across publications,\nslowing down progress. Three years ago, a similar issue was identified and\nrectified in the field of neural algorithmic reasoning, with the advent of the\nCLRS benchmark. CLRS is a dataset generator comprising graph execution traces\nof classical algorithms from the Introduction to Algorithms textbook. Inspired\nby this, we propose CLRS-Text -- a textual version of these algorithmic traces.\nOut of the box, CLRS-Text is capable of procedurally generating trace data for\nthirty diverse, challenging algorithmic tasks across any desirable input\ndistribution, while offering a standard pipeline in which any additional\nalgorithmic tasks may be created in the benchmark. We fine-tune and evaluate\nvarious LMs as generalist executors on this benchmark, validating prior work\nand revealing a novel, interesting challenge for the LM reasoning community.\nOur code is available at\nhttps://github.com/google-deepmind/clrs/tree/master/clrs/_src/clrs_text.", "published": "2024-06-06 16:29:25", "link": "http://arxiv.org/abs/2406.04229v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Improving Alignment and Robustness with Circuit Breakers", "abstract": "AI systems can take harmful actions and are highly vulnerable to adversarial\nattacks. We present an approach, inspired by recent advances in representation\nengineering, that interrupts the models as they respond with harmful outputs\nwith \"circuit breakers.\" Existing techniques aimed at improving alignment, such\nas refusal training, are often bypassed. Techniques such as adversarial\ntraining try to plug these holes by countering specific attacks. As an\nalternative to refusal training and adversarial training, circuit-breaking\ndirectly controls the representations that are responsible for harmful outputs\nin the first place. Our technique can be applied to both text-only and\nmultimodal language models to prevent the generation of harmful outputs without\nsacrificing utility -- even in the presence of powerful unseen attacks.\nNotably, while adversarial robustness in standalone image recognition remains\nan open challenge, circuit breakers allow the larger multimodal system to\nreliably withstand image \"hijacks\" that aim to produce harmful content.\nFinally, we extend our approach to AI agents, demonstrating considerable\nreductions in the rate of harmful actions when they are under attack. Our\napproach represents a significant step forward in the development of reliable\nsafeguards to harmful behavior and adversarial attacks.", "published": "2024-06-06 17:57:04", "link": "http://arxiv.org/abs/2406.04313v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Anna Karenina Strikes Again: Pre-Trained LLM Embeddings May Favor\n  High-Performing Learners", "abstract": "Unsupervised clustering of student responses to open-ended questions into\nbehavioral and cognitive profiles using pre-trained LLM embeddings is an\nemerging technique, but little is known about how well this captures\npedagogically meaningful information. We investigate this in the context of\nstudent responses to open-ended questions in biology, which were previously\nanalyzed and clustered by experts into theory-driven Knowledge Profiles (KPs).\nComparing these KPs to ones discovered by purely data-driven clustering\ntechniques, we report poor discoverability of most KPs, except for the ones\nincluding the correct answers. We trace this \"discoverability bias\" to the\nrepresentations of KPs in the pre-trained LLM embeddings space.", "published": "2024-06-06 10:36:48", "link": "http://arxiv.org/abs/2406.06599v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Helsinki Speech Challenge 2024", "abstract": "The Helsinki Speech Challenge 2024 (HSC2024) invites researchers to enhance\nand deconvolve speech audio recordings. We recorded a dataset that challenges\nparticipants to apply speech enhancement and inverse problems techniques to\nrecorded speech data. This dataset includes paired samples of AI-generated\nclean speech and corresponding recordings, which feature varying levels of\ncorruption, including frequency attenuation and reverberation. The challenge\nfocuses on developing innovative deconvolution methods to accurately recover\nthe original audio. The effectiveness of these methods will be quantitatively\nassessed using a speech recognition model, providing a relevant metric for\nevaluating enhancements in real-world scenarios.", "published": "2024-06-06 14:47:39", "link": "http://arxiv.org/abs/2406.04123v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Total-Duration-Aware Duration Modeling for Text-to-Speech Systems", "abstract": "Accurate control of the total duration of generated speech by adjusting the\nspeech rate is crucial for various text-to-speech (TTS) applications. However,\nthe impact of adjusting the speech rate on speech quality, such as\nintelligibility and speaker characteristics, has been underexplored. In this\nwork, we propose a novel total-duration-aware (TDA) duration model for TTS,\nwhere phoneme durations are predicted not only from the text input but also\nfrom an additional input of the total target duration. We also propose a\nMaskGIT-based duration model that enhances the diversity and quality of the\npredicted phoneme durations. Our results demonstrate that the proposed TDA\nduration models achieve better intelligibility and speaker similarity for\nvarious speech rate configurations compared to the baseline models. We also\nshow that the proposed MaskGIT-based model can generate phoneme durations with\nhigher quality and diversity compared to its regression or flow-matching\ncounterparts.", "published": "2024-06-06 17:27:09", "link": "http://arxiv.org/abs/2406.04281v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards Naturalistic Voice Conversion: NaturalVoices Dataset with an\n  Automatic Processing Pipeline", "abstract": "Voice conversion (VC) research traditionally depends on scripted or acted\nspeech, which lacks the natural spontaneity of real-life conversations. While\nnatural speech data is limited for VC, our study focuses on filling in this\ngap. We introduce a novel data-sourcing pipeline that makes the release of a\nnatural speech dataset for VC, named NaturalVoices. The pipeline extracts rich\ninformation in speech such as emotion and signal-to-noise ratio (SNR) from raw\npodcast data, utilizing recent deep learning methods and providing flexibility\nand ease of use. NaturalVoices marks a large-scale, spontaneous, expressive,\nand emotional speech dataset, comprising over 3,800 hours speech sourced from\nthe original podcasts in the MSP-Podcast dataset. Objective and subjective\nevaluations demonstrate the effectiveness of using our pipeline for providing\nnatural and expressive data for VC, suggesting the potential of NaturalVoices\nfor broader speech generation tasks.", "published": "2024-06-06 20:38:19", "link": "http://arxiv.org/abs/2406.04494v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Flexible Multichannel Speech Enhancement for Noise-Robust Frontend", "abstract": "This paper proposes a flexible multichannel speech enhancement system with\nthe main goal of improving robustness of automatic speech recognition (ASR) in\nnoisy conditions. The proposed system combines a flexible neural mask estimator\napplicable to different channel counts and configurations and a multichannel\nfilter with automatic reference selection. A transform-attend-concatenate layer\nis proposed to handle cross-channel information in the mask estimator, which is\nshown to be effective for arbitrary microphone configurations. The presented\nevaluation demonstrates the effectiveness of the flexible system for several\nseen and unseen compact array geometries, matching the performance of fixed\nconfiguration-specific systems. Furthermore, a significantly improved ASR\nperformance is observed for configurations with randomly-placed microphones.", "published": "2024-06-06 23:42:14", "link": "http://arxiv.org/abs/2406.04552v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis\n  with Context-Aware Contrastive Language-Audio Pretraining", "abstract": "Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker\nusing only a short speech prompt. They leverage a strong in-context ability to\nmimic the speech prompts, including speaker style, prosody, and emotion.\nTherefore, the selection of a speech prompt greatly influences the generated\nspeech, akin to the importance of a prompt in large language models (LLMs).\nHowever, current prompt-based TTS models choose the speech prompt manually or\nsimply at random. Hence, in this paper, we adapt retrieval augmented generation\n(RAG) from LLMs to prompt-based TTS. Unlike traditional RAG methods, we\nadditionally consider contextual information during the retrieval process and\npresent a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model\nto extract context-aware, style-related features. The objective and subjective\nevaluations demonstrate that our proposed RAG method outperforms baselines, and\nour CA-CLAP achieves better results than text-only retrieval methods.", "published": "2024-06-06 03:17:44", "link": "http://arxiv.org/abs/2406.03714v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PLDNet: PLD-Guided Lightweight Deep Network Boosted by Efficient\n  Attention for Handheld Dual-Microphone Speech Enhancement", "abstract": "Low-complexity speech enhancement on mobile phones is crucial in the era of\n5G. Thus, focusing on handheld mobile phone communication scenario, based on\npower level difference (PLD) algorithm and lightweight U-Net, we propose\nPLD-guided lightweight deep network (PLDNet), an extremely lightweight\ndual-microphone speech enhancement method that integrates the guidance of\nsignal processing algorithm and lightweight attention-augmented U-Net. For the\nguidance information, we employ PLD algorithm to pre-process dual-microphone\nspectrum, and feed the output into subsequent deep neural network, which\nutilizes a lightweight U-Net with our proposed gated convolution augmented\nfrequency attention (GCAFA) module to extract desired clean speech.\nExperimental results demonstrate that our proposed method achieves competitive\nperformance with recent top-performing models while reducing computational cost\nby over 90%, highlighting the potential for low-complexity speech enhancement\non mobile phones.", "published": "2024-06-06 09:36:38", "link": "http://arxiv.org/abs/2406.03899v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "STraDa: A Singer Traits Dataset", "abstract": "There is a limited amount of large-scale public datasets that contain\ndownloadable music audio files and rich lead singer metadata. To provide such a\ndataset to benefit research in singing voices, we created Singer Traits Dataset\n(STraDa) with two subsets: automatic-strada and annotated-strada. The\nautomatic-strada contains twenty-five thousand tracks across numerous genres\nand languages of more than five thousand unique lead singers, which includes\ncross-validated lead singer metadata as well as other track metadata. The\nannotated-strada consists of two hundred tracks that are balanced in terms of 2\ngenders, 5 languages, and 4 age groups. To show its use for model training and\nbias analysis thanks to its metadata's richness and downloadable audio files,\nwe benchmarked singer sex classification (SSC) and conducted bias analysis.", "published": "2024-06-06 15:03:32", "link": "http://arxiv.org/abs/2406.04140v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Event Bounding Boxes", "abstract": "Sound event detection is the task of recognizing sounds and determining their\nextent (onset/offset times) within an audio clip. Existing systems commonly\npredict sound presence confidence in short time frames. Then, thresholding\nproduces binary frame-level presence decisions, with the extent of individual\nevents determined by merging consecutive positive frames. In this paper, we\nshow that frame-level thresholding degrades the prediction of the event extent\nby coupling it with the system's sound presence confidence. We propose to\ndecouple the prediction of event extent and confidence by introducing SEBBs,\nwhich format each sound event prediction as a tuple of a class type, extent,\nand overall confidence. We also propose a change-detection-based algorithm to\nconvert legacy frame-level outputs into SEBBs. We find the algorithm\nsignificantly improves the performance of DCASE 2023 Challenge systems,\nboosting the state of the art from .644 to .686 PSDS1.", "published": "2024-06-06 16:11:39", "link": "http://arxiv.org/abs/2406.04212v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Beyond Performance Plateaus: A Comprehensive Study on Scalability in\n  Speech Enhancement", "abstract": "Deep learning-based speech enhancement (SE) models have achieved impressive\nperformance in the past decade. Numerous advanced architectures have been\ndesigned to deliver state-of-the-art performance; however, their scalability\npotential remains unrevealed. Meanwhile, the majority of research focuses on\nsmall-sized datasets with restricted diversity, leading to a plateau in\nperformance improvement. In this paper, we aim to provide new insights for\naddressing the above issues by exploring the scalability of SE models in terms\nof architectures, model sizes, compute budgets, and dataset sizes. Our\ninvestigation involves several popular SE architectures and speech data from\ndifferent domains. Experiments reveal both similarities and distinctions\nbetween the scaling effects in SE and other tasks such as speech recognition.\nThese findings further provide insights into the under-explored SE directions,\ne.g., larger-scale multi-domain corpora and efficiently scalable architectures.", "published": "2024-06-06 17:20:21", "link": "http://arxiv.org/abs/2406.04269v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SilentCipher: Deep Audio Watermarking", "abstract": "In the realm of audio watermarking, it is challenging to simultaneously\nencode imperceptible messages while enhancing the message capacity and\nrobustness. Although recent advancements in deep learning-based methods bolster\nthe message capacity and robustness over traditional methods, the encoded\nmessages introduce audible artefacts that restricts their usage in professional\nsettings. In this study, we introduce three key innovations. Firstly, our work\nis the first deep learning-based model to integrate psychoacoustic model based\nthresholding to achieve imperceptible watermarks. Secondly, we introduce\npsuedo-differentiable compression layers, enhancing the robustness of our\nwatermarking algorithm. Lastly, we introduce a method to eliminate the need for\nperceptual losses, enabling us to achieve SOTA in both robustness as well as\nimperceptible watermarking. Our contributions lead us to SilentCipher, a model\nenabling users to encode messages within audio signals sampled at 44.1kHz.", "published": "2024-06-06 07:58:31", "link": "http://arxiv.org/abs/2406.03822v2", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "InaGVAD : a Challenging French TV and Radio Corpus Annotated for Speech\n  Activity Detection and Speaker Gender Segmentation", "abstract": "InaGVAD is an audio corpus collected from 10 French radio and 18 TV channels\ncategorized into 4 groups: generalist radio, music radio, news TV, and\ngeneralist TV. It contains 277 1-minute-long annotated recordings aimed at\nrepresenting the acoustic diversity of French audiovisual programs and was\nprimarily designed to build systems able to monitor men's and women's speaking\ntime in media. inaGVAD is provided with Voice Activity Detection (VAD) and\nSpeaker Gender Segmentation (SGS) annotations extended with overlap, speaker\ntraits (gender, age, voice quality), and 10 non-speech event categories.\nAnnotation distributions are detailed for each channel category. This dataset\nis partitioned into a 1h development and a 3h37 test subset, allowing fair and\nreproducible system evaluation. A benchmark of 6 freely available VAD software\nis presented, showing diverse abilities based on channel and non-speech event\ncategories. Two existing SGS systems are evaluated on the corpus and compared\nagainst a baseline X-vector transfer learning strategy, trained on the\ndevelopment subset. Results demonstrate that our proposal, trained on a single\n- but diverse - hour of data, achieved competitive SGS results. The entire\ninaGVAD package; including corpus, annotations, evaluation scripts, and\nbaseline training code; is made freely accessible, fostering future advancement\nin the domain.", "published": "2024-06-06 18:16:14", "link": "http://arxiv.org/abs/2406.04429v1", "categories": ["eess.AS", "cs.DL", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound", "abstract": "Generating combined visual and auditory sensory experiences is critical for\nthe consumption of immersive content. Recent advances in neural generative\nmodels have enabled the creation of high-resolution content across multiple\nmodalities such as images, text, speech, and videos. Despite these successes,\nthere remains a significant gap in the generation of high-quality spatial audio\nthat complements generated visual content. Furthermore, current audio\ngeneration models excel in either generating natural audio or speech or music\nbut fall short in integrating spatial audio cues necessary for immersive\nexperiences. In this work, we introduce SEE-2-SOUND, a zero-shot approach that\ndecomposes the task into (1) identifying visual regions of interest; (2)\nlocating these elements in 3D space; (3) generating mono-audio for each; and\n(4) integrating them into spatial audio. Using our framework, we demonstrate\ncompelling results for generating spatial audio for high-quality videos,\nimages, and dynamic images from the internet, as well as media generated by\nlearned approaches.", "published": "2024-06-06 22:55:01", "link": "http://arxiv.org/abs/2406.06612v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Flowers Revisited: A Preliminary Replication of Flowers et al. 1997", "abstract": "In 1997, Flowers, Buhman, and Turnage published a paper titled ``Cross-Modal\nEquivalence of Visual and Auditory Scatterplots for Exploring Bivariate Data\nSamples.'' This paper examined our capacity to assess the relationship between\ntwo data variables when presented through visual or auditory scatterplots.\nTwenty-seven years later, we have replicated the first part of this influential\nstudy and present the preliminary findings of our replication, initially\ninvolving 21 participants. In addition to purely auditory and visual\nscatterplots, we introduced audiovisual scatterplots as a third condition in\nour experiment. Our initial findings mirror those of Flowers et al.'s original\nresearch. With this extended abstract, we also aim to spark a discussion about\nthe significance of replication studies for our research community in general.", "published": "2024-06-06 17:16:33", "link": "http://arxiv.org/abs/2407.11992v1", "categories": ["cs.HC", "cs.GR", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
