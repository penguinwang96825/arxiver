{"title": "Knowledge-based Review Generation by Coherence Enhanced Text Planning", "abstract": "As a natural language generation task, it is challenging to generate\ninformative and coherent review text. In order to enhance the informativeness\nof the generated text, existing solutions typically learn to copy entities or\ntriples from knowledge graphs (KGs). However, they lack overall consideration\nto select and arrange the incorporated knowledge, which tends to cause text\nincoherence.\n  To address the above issue, we focus on improving entity-centric coherence of\nthe generated reviews by leveraging the semantic structure of KGs. In this\npaper, we propose a novel Coherence Enhanced Text Planning model (CETP) based\non knowledge graphs (KGs) to improve both global and local coherence for review\ngeneration. The proposed model learns a two-level text plan for generating a\ndocument: (1) the document plan is modeled as a sequence of sentence plans in\norder, and (2) the sentence plan is modeled as an entity-based subgraph from\nKG. Local coherence can be naturally enforced by KG subgraphs through\nintra-sentence correlations between entities. For global coherence, we design a\nhierarchical self-attentive architecture with both subgraph- and node-level\nattention to enhance the correlations between subgraphs. To our knowledge, we\nare the first to utilize a KG-based text planning model to enhance text\ncoherence for review generation. Extensive experiments on three datasets\nconfirm the effectiveness of our model on improving the content coherence of\ngenerated texts.", "published": "2021-05-09 02:12:05", "link": "http://arxiv.org/abs/2105.03815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents", "abstract": "Legal artificial intelligence (LegalAI) aims to benefit legal systems with\nthe technology of artificial intelligence, especially natural language\nprocessing (NLP). Recently, inspired by the success of pre-trained language\nmodels (PLMs) in the generic domain, many LegalAI researchers devote their\neffort to apply PLMs to legal tasks. However, utilizing PLMs to address legal\ntasks is still challenging, as the legal documents usually consist of thousands\nof tokens, which is far longer than the length that mainstream PLMs can\nprocess. In this paper, we release the Longformer-based pre-trained language\nmodel, named as Lawformer, for Chinese legal long documents understanding. We\nevaluate Lawformer on a variety of LegalAI tasks, including judgment\nprediction, similar case retrieval, legal reading comprehension, and legal\nquestion answering. The experimental results demonstrate that our model can\nachieve promising improvement on tasks with long documents as inputs.", "published": "2021-05-09 09:39:25", "link": "http://arxiv.org/abs/2105.03887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Patent Mining and Relevance Classification using Transformers", "abstract": "Patent analysis and mining are time-consuming and costly processes for\ncompanies, but nevertheless essential if they are willing to remain\ncompetitive. To face the overload induced by numerous patents, the idea is to\nautomatically filter them, bringing only few to read to experts. This paper\nreports a successful application of fine-tuning and retraining on pre-trained\ndeep Natural Language Processing models on patent classification. The solution\nthat we propose combines several state-of-the-art treatments to achieve our\ngoal - decrease the workload while preserving recall and precision metrics.", "published": "2021-05-09 17:57:55", "link": "http://arxiv.org/abs/2105.03979v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dispatcher: A Message-Passing Approach To Language Modelling", "abstract": "This paper proposes a message-passing mechanism to address language\nmodelling. A new layer type is introduced that aims to substitute\nself-attention for unidirectional sequence generation tasks. The system is\nshown to be competitive with existing methods: Given N tokens, the\ncomputational complexity is O(N logN) and the memory complexity is O(N) under\nreasonable assumptions. In the end, the Dispatcher layer is seen to achieve\ncomparable perplexity to prior results while being more efficient.", "published": "2021-05-09 18:57:34", "link": "http://arxiv.org/abs/2105.03994v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocSCAN: Unsupervised Text Classification via Learning from Neighbors", "abstract": "We introduce DocSCAN, a completely unsupervised text classification approach\nusing Semantic Clustering by Adopting Nearest-Neighbors (SCAN). For each\ndocument, we obtain semantically informative vectors from a large pre-trained\nlanguage model. Similar documents have proximate vectors, so neighbors in the\nrepresentation space tend to share topic labels. Our learnable clustering\napproach uses pairs of neighboring datapoints as a weak learning signal. The\nproposed approach learns to assign classes to the whole dataset without\nprovided ground-truth labels. On five topic classification benchmarks, we\nimprove on various unsupervised baselines by a large margin. In datasets with\nrelatively few and balanced outcome classes, DocSCAN approaches the performance\nof supervised classification. The method fails for other types of\nclassification, such as sentiment analysis, pointing to important conceptual\nand practical differences between classifying images and texts.", "published": "2021-05-09 21:20:31", "link": "http://arxiv.org/abs/2105.04024v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Online Political Advertisements", "abstract": "Online political advertising is a central aspect of modern election\ncampaigning for influencing public opinion. Computational analysis of political\nads is of utmost importance in political science to understand the\ncharacteristics of digital campaigning. It is also important in computational\nlinguistics to study features of political discourse and communication on a\nlarge scale. In this work, we present the first computational study on online\npolitical ads with the aim to (1) infer the political ideology of an ad\nsponsor; and (2) identify whether the sponsor is an official political party or\na third-party organization. We develop two new large datasets for the two tasks\nconsisting of ads from the U.S.. Evaluation results show that our approach that\ncombines textual and visual information from pre-trained neural models\noutperforms a state-of-the-art method for generic commercial ad classification.\nFinally, we provide an in-depth analysis of the limitations of our\nbest-performing models and linguistic analysis to study the characteristics of\npolitical ads discourse.", "published": "2021-05-09 23:18:37", "link": "http://arxiv.org/abs/2105.04047v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Sentiment Analysis by Transferring Multi-source Knowledge", "abstract": "Sentiment analysis (SA) is an important research area in cognitive\ncomputation-thus in-depth studies of patterns of sentiment analysis are\nnecessary. At present, rich resource data-based SA has been well developed,\nwhile the more challenging and practical multi-source unsupervised SA (i.e. a\ntarget domain SA by transferring from multiple source domains) is seldom\nstudied. The challenges behind this problem mainly locate in the lack of\nsupervision information, the semantic gaps among domains (i.e., domain shifts),\nand the loss of knowledge. However, existing methods either lack the\ndistinguishable capacity of the semantic gaps among domains or lose private\nknowledge. To alleviate these problems, we propose a two-stage domain\nadaptation framework. In the first stage, a multi-task methodology-based\nshared-private architecture is employed to explicitly model the domain common\nfeatures and the domain-specific features for the labeled source domains. In\nthe second stage, two elaborate mechanisms are embedded in the shared private\narchitecture to transfer knowledge from multiple source domains. The first\nmechanism is a selective domain adaptation (SDA) method, which transfers\nknowledge from the closest source domain. And the second mechanism is a\ntarget-oriented ensemble (TOE) method, in which knowledge is transferred\nthrough a well-designed ensemble method. Extensive experiment evaluations\nverify that the performance of the proposed framework outperforms unsupervised\nstate-of-the-art competitors. What can be concluded from the experiments is\nthat transferring from very different distributed source domains may degrade\nthe target-domain performance, and it is crucial to choose the proper source\ndomains to transfer from.", "published": "2021-05-09 03:02:19", "link": "http://arxiv.org/abs/2105.11902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FNet: Mixing Tokens with Fourier Transforms", "abstract": "We show that Transformer encoder architectures can be sped up, with limited\naccuracy costs, by replacing the self-attention sublayers with simple linear\ntransformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling\nsemantic relationships in several text classification tasks. Most surprisingly,\nwe find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the\naccuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on\nGPUs and 70% faster on TPUs at standard 512 input lengths. At longer input\nlengths, our FNet model is significantly faster: when compared to the\n\"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the\naccuracy of the most accurate models, while outpacing the fastest models across\nall sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\nFinally, FNet has a light memory footprint and is particularly efficient at\nsmaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.", "published": "2021-05-09 03:32:48", "link": "http://arxiv.org/abs/2105.03824v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Which transformer architecture fits my data? A vocabulary bottleneck in\n  self-attention", "abstract": "After their successful debut in natural language processing, Transformer\narchitectures are now becoming the de-facto standard in many domains. An\nobstacle for their deployment over new modalities is the architectural\nconfiguration: the optimal depth-to-width ratio has been shown to dramatically\nvary across data types (e.g., $10$x larger over images than over language). We\ntheoretically predict the existence of an embedding rank bottleneck that limits\nthe contribution of self-attention width to the Transformer expressivity. We\nthus directly tie the input vocabulary size and rank to the optimal\ndepth-to-width ratio, since a small vocabulary size or rank dictates an added\nadvantage of depth over width. We empirically demonstrate the existence of this\nbottleneck and its implications on the depth-to-width interplay of Transformer\narchitectures, linking the architecture variability across domains to the often\nglossed-over usage of different vocabulary sizes or embedding ranks in\ndifferent domains. As an additional benefit, our rank bottlenecking framework\nallows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models\nsuch as ALBERT and T5.", "published": "2021-05-09 13:08:26", "link": "http://arxiv.org/abs/2105.03928v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "gComm: An environment for investigating generalization in Grounded\n  Language Acquisition", "abstract": "gComm is a step towards developing a robust platform to foster research in\ngrounded language acquisition in a more challenging and realistic setting. It\ncomprises a 2-d grid environment with a set of agents (a stationary speaker and\na mobile listener connected via a communication channel) exposed to a\ncontinuous array of tasks in a partially observable setting. The key to solving\nthese tasks lies in agents developing linguistic abilities and utilizing them\nfor efficiently exploring the environment. The speaker and listener have access\nto information provided in different modalities, i.e. the speaker's input is a\nnatural language instruction that contains the target and task specifications\nand the listener's input is its grid-view. Each must rely on the other to\ncomplete the assigned task, however, the only way they can achieve the same, is\nto develop and use some form of communication. gComm provides several tools for\nstudying different forms of communication and assessing their generalization.", "published": "2021-05-09 13:44:55", "link": "http://arxiv.org/abs/2105.03943v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural\n  Machine Translation", "abstract": "The data scarcity in low-resource languages has become a bottleneck to\nbuilding robust neural machine translation systems. Fine-tuning a multilingual\npre-trained model (e.g., mBART (Liu et al., 2020)) on the translation task is a\ngood approach for low-resource languages; however, its performance will be\ngreatly limited when there are unseen languages in the translation pairs. In\nthis paper, we present a continual pre-training (CPT) framework on mBART to\neffectively adapt it to unseen languages. We first construct noisy\nmixed-language text from the monolingual corpus of the target language in the\ntranslation pair to cover both the source and target languages, and then, we\ncontinue pre-training mBART to reconstruct the original monolingual text.\nResults show that our method can consistently improve the fine-tuning\nperformance upon the mBART baseline, as well as other strong baselines, across\nall tested low-resource translation pairs containing unseen languages.\nFurthermore, our approach also boosts the performance on translation pairs\nwhere both languages are seen in the original mBART's pre-training. The code is\navailable at https://github.com/zliucr/cpt-nmt.", "published": "2021-05-09 14:49:07", "link": "http://arxiv.org/abs/2105.03953v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding the Role of Affect Dimensions in Detecting Emotions from\n  Tweets: A Multi-task Approach", "abstract": "We propose VADEC, a multi-task framework that exploits the correlation\nbetween the categorical and dimensional models of emotion representation for\nbetter subjectivity analysis. Focusing primarily on the effective detection of\nemotions from tweets, we jointly train multi-label emotion classification and\nmulti-dimensional emotion regression, thereby utilizing the inter-relatedness\nbetween the tasks. Co-training especially helps in improving the performance of\nthe classification task as we outperform the strongest baselines with 3.4%,\n11%, and 3.9% gains in Jaccard Accuracy, Macro-F1, and Micro-F1 scores\nrespectively on the AIT dataset. We also achieve state-of-the-art results with\n11.3% gains averaged over six different metrics on the SenWave dataset. For the\nregression task, VADEC, when trained with SenWave, achieves 7.6% and 16.5%\ngains in Pearson Correlation scores over the current state-of-the-art on the\nEMOBANK dataset for the Valence (V) and Dominance (D) affect dimensions\nrespectively. We conclude our work with a case study on COVID-19 tweets posted\nby Indians that further helps in establishing the efficacy of our proposed\nsolution.", "published": "2021-05-09 18:07:04", "link": "http://arxiv.org/abs/2105.03983v1", "categories": ["cs.IR", "cs.CL", "I.2.7; J.4"], "primary_category": "cs.IR"}
{"title": "FastCorrect: Fast Error Correction with Edit Alignment for Automatic\n  Speech Recognition", "abstract": "Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the popular NAR models adopted in neural machine translation and\ntext edition by a large margin.", "published": "2021-05-09 05:35:36", "link": "http://arxiv.org/abs/2105.03842v6", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "High-performance symbolic-numerics via multiple dispatch", "abstract": "As mathematical computing becomes more democratized in high-level languages,\nhigh-performance symbolic-numeric systems are necessary for domain scientists\nand engineers to get the best performance out of their machine without deep\nknowledge of code optimization. Naturally, users need different term types\neither to have different algebraic properties for them, or to use efficient\ndata structures. To this end, we developed Symbolics.jl, an extendable symbolic\nsystem which uses dynamic multiple dispatch to change behavior depending on the\ndomain needs. In this work we detail an underlying abstract term interface\nwhich allows for speed without sacrificing generality. We show that by\nformalizing a generic API on actions independent of implementation, we can\nretroactively add optimized data structures to our system without changing the\npre-existing term rewriters. We showcase how this can be used to optimize term\nconstruction and give a 113x acceleration on general symbolic transformations.\nFurther, we show that such a generic API allows for complementary\nterm-rewriting implementations. We demonstrate the ability to swap between\nclassical term-rewriting simplifiers and e-graph-based term-rewriting\nsimplifiers. We showcase an e-graph ruleset which minimizes the number of CPU\ncycles during expression evaluation, and demonstrate how it simplifies a\nreal-world reaction-network simulation to halve the runtime. Additionally, we\nshow a reaction-diffusion partial differential equation solver which is able to\nbe automatically converted into symbolic expressions via multiple dispatch\ntracing, which is subsequently accelerated and parallelized to give a 157x\nsimulation speedup. Together, this presents Symbolics.jl as a next-generation\nsymbolic-numeric computing environment geared towards modeling and simulation.", "published": "2021-05-09 14:22:43", "link": "http://arxiv.org/abs/2105.03949v3", "categories": ["cs.CL", "cs.MS", "cs.PL", "cs.SC", "D.3.3; I.1.1; I.1.3"], "primary_category": "cs.CL"}
{"title": "Advising Agent for Service-Providing Live-Chat Operators", "abstract": "Call centers, in which human operators attend clients using textual chat, are\nvery common in modern e-commerce. Training enough skilled operators who are\nable to provide good service is a challenge. We suggest an algorithm and a\nmethod to train and implement an assisting agent that provides on-line advice\nto operators while they attend clients. The agent is domain-independent and can\nbe introduced to new domains without major efforts in design, training and\norganizing structured knowledge of the professional discipline. We demonstrate\nthe applicability of the system in an experiment that realizes its full\nlife-cycle on a specific domain and analyze its capabilities.", "published": "2021-05-09 18:10:54", "link": "http://arxiv.org/abs/2105.03986v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "English Accent Accuracy Analysis in a State-of-the-Art Automatic Speech\n  Recognition System", "abstract": "Nowadays, research in speech technologies has gotten a lot out thanks to\nrecently created public domain corpora that contain thousands of recording\nhours. These large amounts of data are very helpful for training the new\ncomplex models based on deep learning technologies. However, the lack of\ndialectal diversity in a corpus is known to cause performance biases in speech\nsystems, mainly for underrepresented dialects. In this work, we propose to\nevaluate a state-of-the-art automatic speech recognition (ASR) deep\nlearning-based model, using unseen data from a corpus with a wide variety of\nlabeled English accents from different countries around the world. The model\nhas been trained with 44.5K hours of English speech from an open access corpus\ncalled Multilingual LibriSpeech, showing remarkable results in popular\nbenchmarks. We test the accuracy of such ASR against samples extracted from\nanother public corpus that is continuously growing, the Common Voice dataset.\nThen, we present graphically the accuracy in terms of Word Error Rate of each\nof the different English included accents, showing that there is indeed an\naccuracy bias in terms of accentual variety, favoring the accents most\nprevalent in the training corpus.", "published": "2021-05-09 08:24:33", "link": "http://arxiv.org/abs/2105.05041v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Superresolution photoacoustic tomography using random speckle\n  illumination and second order moments", "abstract": "Idier et al. [IEEE Trans. Comput. Imaging 4(1), 2018] propose a method which\nachieves superresolution in the microscopy setting by leveraging random speckle\nillumination and knowledge about statistical second order moments for the\nillumination patterns and model noise. This is achieved without any assumptions\non the sparsity of the imaged object. In this paper, we show that their\ntechnique can be extended to photoacoustic tomography. We propose a simple\nalgorithm for doing the reconstruction which only requires a small number of\nlinear algebra steps. It is therefore much faster than the iterative method\nused by Idier et al. We also propose a new representation of the imaged object\nbased on Dirac delta expansion functions.", "published": "2021-05-09 01:23:16", "link": "http://arxiv.org/abs/2105.03809v2", "categories": ["eess.SP", "eess.AS", "eess.IV", "physics.med-ph"], "primary_category": "eess.SP"}
