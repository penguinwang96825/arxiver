{"title": "Long Short-Term Memory Based Recurrent Neural Network Architectures for\n  Large Vocabulary Speech Recognition", "abstract": "Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)\narchitecture that has been designed to address the vanishing and exploding\ngradient problems of conventional RNNs. Unlike feedforward neural networks,\nRNNs have cyclic connections making them powerful for modeling sequences. They\nhave been successfully used for sequence labeling and sequence prediction\ntasks, such as handwriting recognition, language modeling, phonetic labeling of\nacoustic frames. However, in contrast to the deep neural networks, the use of\nRNNs in speech recognition has been limited to phone recognition in small scale\ntasks. In this paper, we present novel LSTM based RNN architectures which make\nmore effective use of model parameters to train acoustic models for large\nvocabulary speech recognition. We train and compare LSTM, RNN and DNN models at\nvarious numbers of parameters and configurations. We show that LSTM models\nconverge quickly and give state of the art speech recognition performance for\nrelatively small sized models.", "published": "2014-02-05 19:01:51", "link": "http://arxiv.org/abs/1402.1128v1", "categories": ["cs.NE", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.NE"}
