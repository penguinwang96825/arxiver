{"title": "Large Language Model Agent in Financial Trading: A Survey", "abstract": "Trading is a highly competitive task that requires a combination of strategy,\nknowledge, and psychological fortitude. With the recent success of large\nlanguage models(LLMs), it is appealing to apply the emerging intelligence of\nLLM agents in this competitive arena and understanding if they can outperform\nprofessional traders. In this survey, we provide a comprehensive review of the\ncurrent research on using LLMs as agents in financial trading. We summarize the\ncommon architecture used in the agent, the data inputs, and the performance of\nLLM trading agents in backtesting as well as the challenges presented in these\nresearch. This survey aims to provide insights into the current state of\nLLM-based financial trading agents and outline future research directions in\nthis field.", "published": "2024-07-26 08:53:05", "link": "http://arxiv.org/abs/2408.06361v1", "categories": ["q-fin.TR", "cs.CL"], "primary_category": "q-fin.TR"}
{"title": "Guidance-Based Prompt Data Augmentation in Specialized Domains for Named\n  Entity Recognition", "abstract": "While the abundance of rich and vast datasets across numerous fields has\nfacilitated the advancement of natural language processing, sectors in need of\nspecialized data types continue to struggle with the challenge of finding\nquality data. Our study introduces a novel guidance data augmentation technique\nutilizing abstracted context and sentence structures to produce varied\nsentences while maintaining context-entity relationships, addressing data\nscarcity challenges. By fostering a closer relationship between context,\nsentence structure, and role of entities, our method enhances data\naugmentation's effectiveness. Consequently, by showcasing diversification in\nboth entity-related vocabulary and overall sentence structure, and\nsimultaneously improving the training performance of named entity recognition\ntask.", "published": "2024-07-26 00:48:28", "link": "http://arxiv.org/abs/2407.18442v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-turn Response Selection with Commonsense-enhanced Language Models", "abstract": "As a branch of advanced artificial intelligence, dialogue systems are\nprospering. Multi-turn response selection is a general research problem in\ndialogue systems. With the assistance of background information and pre-trained\nlanguage models, the performance of state-of-the-art methods on this problem\ngains impressive improvement. However, existing studies neglect the importance\nof external commonsense knowledge. Hence, we design a Siamese network where a\npre-trained Language model merges with a Graph neural network (SinLG). SinLG\ntakes advantage of Pre-trained Language Models (PLMs) to catch the word\ncorrelations in the context and response candidates and utilizes a Graph Neural\nNetwork (GNN) to reason helpful common sense from an external knowledge graph.\nThe GNN aims to assist the PLM in fine-tuning, and arousing its related\nmemories to attain better performance. Specifically, we first extract related\nconcepts as nodes from an external knowledge graph to construct a subgraph with\nthe context response pair as a super node for each sample. Next, we learn two\nrepresentations for the context response pair via both the PLM and GNN. A\nsimilarity loss between the two representations is utilized to transfer the\ncommonsense knowledge from the GNN to the PLM. Then only the PLM is used to\ninfer online so that efficiency can be guaranteed. Finally, we conduct\nextensive experiments on two variants of the PERSONA-CHAT dataset, which proves\nthat our solution can not only improve the performance of the PLM but also\nachieve an efficient inference.", "published": "2024-07-26 03:13:47", "link": "http://arxiv.org/abs/2407.18479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Multidimensional Evaluation Framework for Empathetic\n  Conversational Systems", "abstract": "Empathetic Conversational Systems (ECS) are built to respond empathetically\nto the user's emotions and sentiments, regardless of the application domain.\nCurrent ECS studies evaluation approaches are restricted to offline evaluation\nexperiments primarily for gold standard comparison & benchmarking, and user\nevaluation studies for collecting human ratings on specific constructs. These\nmethods are inadequate in measuring the actual quality of empathy in\nconversations. In this paper, we propose a multidimensional empathy evaluation\nframework with three new methods for measuring empathy at (i) structural level\nusing three empathy-related dimensions, (ii) behavioral level using empathy\nbehavioral types, and (iii) overall level using an empathy lexicon, thereby\nfortifying the evaluation process. Experiments were conducted with the\nstate-of-the-art ECS models and large language models (LLMs) to show the\nframework's usefulness.", "published": "2024-07-26 06:34:55", "link": "http://arxiv.org/abs/2407.18538v1", "categories": ["cs.CL", "I.2"], "primary_category": "cs.CL"}
{"title": "The BIAS Detection Framework: Bias Detection in Word Embeddings and\n  Language Models for European Languages", "abstract": "The project BIAS: Mitigating Diversity Biases of AI in the Labor Market is a\nfour-year project funded by the European commission and supported by the Swiss\nState Secretariat for Education, Research and Innovation (SERI). As part of the\nproject, novel bias detection methods to identify societal bias in language\nmodels and word embeddings in European languages are developed, with particular\nattention to linguistic and geographic particularities. This technical report\ndescribes the overall architecture and components of the BIAS Detection\nFramework. The code described in this technical report is available and will be\nupdated and expanded continuously with upcoming results from the BIAS project.\nThe details about the datasets for the different languages are described in\ncorresponding papers at scientific venues.", "published": "2024-07-26 12:13:45", "link": "http://arxiv.org/abs/2407.18689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatSchema: A pipeline of extracting structured information with Large\n  Multimodal Models based on schema", "abstract": "Objective: This study introduces ChatSchema, an effective method for\nextracting and structuring information from unstructured data in medical paper\nreports using a combination of Large Multimodal Models (LMMs) and Optical\nCharacter Recognition (OCR) based on the schema. By integrating predefined\nschema, we intend to enable LMMs to directly extract and standardize\ninformation according to the schema specifications, facilitating further data\nentry. Method: Our approach involves a two-stage process, including\nclassification and extraction for categorizing report scenarios and structuring\ninformation. We established and annotated a dataset to verify the effectiveness\nof ChatSchema, and evaluated key extraction using precision, recall, F1-score,\nand accuracy metrics. Based on key extraction, we further assessed value\nextraction. We conducted ablation studies on two LMMs to illustrate the\nimprovement of structured information extraction with different input modals\nand methods. Result: We analyzed 100 medical reports from Peking University\nFirst Hospital and established a ground truth dataset with 2,945 key-value\npairs. We evaluated ChatSchema using GPT-4o and Gemini 1.5 Pro and found a\nhigher overall performance of GPT-4o. The results are as follows: For the\nresult of key extraction, key-precision was 98.6%, key-recall was 98.5%,\nkey-F1-score was 98.6%. For the result of value extraction based on correct key\nextraction, the overall accuracy was 97.2%, precision was 95.8%, recall was\n95.8%, and F1-score was 95.8%. An ablation study demonstrated that ChatSchema\nachieved significantly higher overall accuracy and overall F1-score of\nkey-value extraction, compared to the Baseline, with increases of 26.9% overall\naccuracy and 27.4% overall F1-score, respectively.", "published": "2024-07-26 13:05:24", "link": "http://arxiv.org/abs/2407.18716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating an Aligned Corpus of Sound and Text: The Multimodal Corpus of\n  Shakespeare and Milton", "abstract": "In this work we present a corpus of poems by William Shakespeare and John\nMilton that have been enriched with readings from the public domain. We have\naligned all the lines with their respective audio segments, at the line, word,\nsyllable and phone level, and we have included their scansion. We make a basic\nvisualization platform for these poems and we conclude by conjecturing possible\nfuture directions.", "published": "2024-07-26 13:30:24", "link": "http://arxiv.org/abs/2407.18730v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Effective and Efficient Continual Pre-training of Large Language\n  Models", "abstract": "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.", "published": "2024-07-26 13:55:21", "link": "http://arxiv.org/abs/2407.18743v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The power of Prompts: Evaluating and Mitigating Gender Bias in MT with\n  LLMs", "abstract": "This paper studies gender bias in machine translation through the lens of\nLarge Language Models (LLMs). Four widely-used test sets are employed to\nbenchmark various base LLMs, comparing their translation quality and gender\nbias against state-of-the-art Neural Machine Translation (NMT) models for\nEnglish to Catalan (En $\\rightarrow$ Ca) and English to Spanish (En\n$\\rightarrow$ Es) translation directions. Our findings reveal pervasive gender\nbias across all models, with base LLMs exhibiting a higher degree of bias\ncompared to NMT models. To combat this bias, we explore prompting engineering\ntechniques applied to an instruction-tuned LLM. We identify a prompt structure\nthat significantly reduces gender bias by up to 12% on the WinoMT evaluation\ndataset compared to more straightforward prompts. These results significantly\nreduce the gender bias accuracy gap between LLMs and traditional NMT systems.", "published": "2024-07-26 14:47:31", "link": "http://arxiv.org/abs/2407.18786v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Granularity is crucial when applying differential privacy to text: An\n  investigation for neural machine translation", "abstract": "Applying differential privacy (DP) by means of the DP-SGD algorithm to\nprotect individual data points during training is becoming increasingly popular\nin NLP. However, the choice of granularity at which DP is applied is often\nneglected. For example, neural machine translation (NMT) typically operates on\nthe sentence-level granularity. From the perspective of DP, this setup assumes\nthat each sentence belongs to a single person and any two sentences in the\ntraining dataset are independent. This assumption is however violated in many\nreal-world NMT datasets, e.g., those including dialogues. For proper\napplication of DP we thus must shift from sentences to entire documents. In\nthis paper, we investigate NMT at both the sentence and document levels,\nanalyzing the privacy/utility trade-off for both scenarios, and evaluating the\nrisks of not using the appropriate privacy granularity in terms of leaking\npersonally identifiable information (PII). Our findings indicate that the\ndocument-level NMT system is more resistant to membership inference attacks,\nemphasizing the significance of using the appropriate granularity when working\nwith DP.", "published": "2024-07-26 14:52:37", "link": "http://arxiv.org/abs/2407.18789v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OfficeBench: Benchmarking Language Agents across Multiple Applications\n  for Office Automation", "abstract": "Office automation significantly enhances human productivity by automatically\nfinishing routine tasks in the workflow. Beyond the basic information\nextraction studied in much of the prior document AI literature, the office\nautomation research should be extended to more realistic office tasks which\nrequire to integrate various information sources in the office system and\nproduce outputs through a series of decision-making processes. We introduce\nOfficeBench, one of the first office automation benchmarks for evaluating\ncurrent LLM agents' capability to address office tasks in realistic office\nworkflows. OfficeBench requires LLM agents to perform feasible long-horizon\nplanning, proficiently switch between applications in a timely manner, and\naccurately ground their actions within a large combined action space, based on\nthe contextual demands of the workflow. Applying our customized evaluation\nmethods on each task, we find that GPT-4 Omni achieves the highest pass rate of\n47.00%, demonstrating a decent performance in handling office tasks. However,\nthis is still far below the human performance and accuracy standards required\nby real-world office workflows. We further observe that most issues are related\nto operation redundancy and hallucinations, as well as limitations in switching\nbetween multiple applications, which may provide valuable insights for\ndeveloping effective agent frameworks for office automation.", "published": "2024-07-26 19:27:17", "link": "http://arxiv.org/abs/2407.19056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Role-specific Guided Large Language Model for Ophthalmic Consultation\n  Based on Stylistic Differentiation", "abstract": "Ophthalmology consultations are crucial for diagnosing, treating, and\npreventing eye diseases. However, the growing demand for consultations exceeds\nthe availability of ophthalmologists. By leveraging large pre-trained language\nmodels, we can design effective dialogues for specific scenarios, aiding in\nconsultations. Traditional fine-tuning strategies for question-answering tasks\nare impractical due to increasing model size and often ignoring patient-doctor\nrole function during consultations. In this paper, we propose EyeDoctor, an\nophthalmic medical questioning large language model that enhances accuracy\nthrough doctor-patient role perception guided and an augmented knowledge base\nwith external disease information. Experimental results show EyeDoctor achieves\nhigher question-answering precision in ophthalmology consultations. Notably,\nEyeDoctor demonstrated a 7.25% improvement in Rouge-1 scores and a 10.16%\nimprovement in F1 scores on multi-round datasets compared to second best model\nChatGPT, highlighting the importance of doctor-patient role differentiation and\ndynamic knowledge base expansion for intelligent medical consultations. EyeDoc\nalso serves as a free available web based service and souce code is available\nat https://github.com/sperfu/EyeDoc.", "published": "2024-07-26 03:23:31", "link": "http://arxiv.org/abs/2407.18483v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards More Accurate Prediction of Human Empathy and Emotion in Text\n  and Multi-turn Conversations by Combining Advanced NLP, Transformers-based\n  Networks, and Linguistic Methodologies", "abstract": "Based on the WASSA 2022 Shared Task on Empathy Detection and Emotion\nClassification, we predict the level of empathic concern and personal distress\ndisplayed in essays. For the first stage of this project we implemented a\nFeed-Forward Neural Network using sentence-level embeddings as features. We\nexperimented with four different embedding models for generating the inputs to\nthe neural network. The subsequent stage builds upon the previous work and we\nhave implemented three types of revisions. The first revision focuses on the\nenhancements to the model architecture and the training approach. The second\nrevision focuses on handling class imbalance using stratified data sampling.\nThe third revision focuses on leveraging lexical resources, where we apply four\ndifferent resources to enrich the features associated with the dataset. During\nthe final stage of this project, we have created the final end-to-end system\nfor the primary task using an ensemble of models to revise primary task\nperformance. Additionally, as part of the final stage, these approaches have\nbeen adapted to the WASSA 2023 Shared Task on Empathy Emotion and Personality\nDetection in Interactions, in which the empathic concern, emotion polarity, and\nemotion intensity in dyadic text conversations are predicted.", "published": "2024-07-26 04:01:27", "link": "http://arxiv.org/abs/2407.18496v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Universal Prompting Strategy for Extracting Process Model Information\n  from Natural Language Text using Large Language Models", "abstract": "Over the past decade, extensive research efforts have been dedicated to the\nextraction of information from textual process descriptions. Despite the\nremarkable progress witnessed in natural language processing (NLP), information\nextraction within the Business Process Management domain remains predominantly\nreliant on rule-based systems and machine learning methodologies. Data scarcity\nhas so far prevented the successful application of deep learning techniques.\nHowever, the rapid progress in generative large language models (LLMs) makes it\npossible to solve many NLP tasks with very high quality without the need for\nextensive data. Therefore, we systematically investigate the potential of LLMs\nfor extracting information from textual process descriptions, targeting the\ndetection of process elements such as activities and actors, and relations\nbetween them. Using a heuristic algorithm, we demonstrate the suitability of\nthe extracted information for process model generation. Based on a novel\nprompting strategy, we show that LLMs are able to outperform state-of-the-art\nmachine learning approaches with absolute performance improvements of up to 8\\%\n$F_1$ score across three different datasets. We evaluate our prompting strategy\non eight different LLMs, showing it is universally applicable, while also\nanalyzing the impact of certain prompt parts on extraction quality. The number\nof example texts, the specificity of definitions, and the rigour of format\ninstructions are identified as key for improving the accuracy of extracted\ninformation. Our code, prompts, and data are publicly available.", "published": "2024-07-26 06:39:35", "link": "http://arxiv.org/abs/2407.18540v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Robust Named Entity Recognizers From Noisy Data With Retrieval\n  Augmentation", "abstract": "Named entity recognition (NER) models often struggle with noisy inputs, such\nas those with spelling mistakes or errors generated by Optical Character\nRecognition processes, and learning a robust NER model is challenging. Existing\nrobust NER models utilize both noisy text and its corresponding gold text for\ntraining, which is infeasible in many real-world applications in which gold\ntext is not available. In this paper, we consider a more realistic setting in\nwhich only noisy text and its NER labels are available. We propose to retrieve\nrelevant text of the noisy text from a knowledge corpus and use it to enhance\nthe representation of the original noisy input. We design three retrieval\nmethods: sparse retrieval based on lexicon similarity, dense retrieval based on\nsemantic similarity, and self-retrieval based on task-specific text. After\nretrieving relevant text, we concatenate the retrieved text with the original\nnoisy text and encode them with a transformer network, utilizing self-attention\nto enhance the contextual token representations of the noisy text using the\nretrieved text. We further employ a multi-view training framework that improves\nrobust NER without retrieving text during inference. Experiments show that our\nretrieval-augmented model achieves significant improvements in various noisy\nNER settings.", "published": "2024-07-26 07:30:41", "link": "http://arxiv.org/abs/2407.18562v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech\n  Recognition with Hierarchical Routing", "abstract": "The Mixture of Experts (MoE) model is a promising approach for handling\ncode-switching speech recognition (CS-ASR) tasks. However, the existing CS-ASR\nwork on MoE has yet to leverage the advantages of MoE's parameter scaling\nability fully. This work proposes DLG-MoE, a Dynamic Language Group-based MoE,\nwhich can effectively handle the CS-ASR task and leverage the advantages of\nparameter scaling. DLG-MoE operates based on a hierarchical routing mechanism.\nFirst, the language router explicitly models the language attribute and\ndispatches the representations to the corresponding language expert groups.\nSubsequently, the unsupervised router within each language group implicitly\nmodels attributes beyond language and coordinates expert routing and\ncollaboration. DLG-MoE outperforms the existing MoE methods on CS-ASR tasks\nwhile demonstrating great flexibility. It supports different top-$k$ inference\nand streaming capabilities and can also prune the model parameters flexibly to\nobtain a monolingual sub-model. The code has been released.", "published": "2024-07-26 08:03:07", "link": "http://arxiv.org/abs/2407.18581v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Generalized Offensive Language Identification", "abstract": "The prevalence of offensive content on the internet, encompassing hate speech\nand cyberbullying, is a pervasive issue worldwide. Consequently, it has\ngarnered significant attention from the machine learning (ML) and natural\nlanguage processing (NLP) communities. As a result, numerous systems have been\ndeveloped to automatically identify potentially harmful content and mitigate\nits impact. These systems can follow two approaches; (1) Use publicly available\nmodels and application endpoints, including prompting large language models\n(LLMs) (2) Annotate datasets and train ML models on them. However, both\napproaches lack an understanding of how generalizable they are. Furthermore,\nthe applicability of these systems is often questioned in off-domain and\npractical environments. This paper empirically evaluates the generalizability\nof offensive language detection models and datasets across a novel generalized\nbenchmark. We answer three research questions on generalizability. Our findings\nwill be useful in creating robust real-world offensive language detection\nsystems.", "published": "2024-07-26 13:50:22", "link": "http://arxiv.org/abs/2407.18738v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Structure as Prompt: Improving Small Language Models\n  Capabilities for Knowledge-based Causal Discovery", "abstract": "Causal discovery aims to estimate causal structures among variables based on\nobservational data. Large Language Models (LLMs) offer a fresh perspective to\ntackle the causal discovery problem by reasoning on the metadata associated\nwith variables rather than their actual data values, an approach referred to as\nknowledge-based causal discovery. In this paper, we investigate the\ncapabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1\nbillion parameters) with prompt-based learning for knowledge-based causal\ndiscovery. Specifically, we present KG Structure as Prompt, a novel approach\nfor integrating structural information from a knowledge graph, such as common\nneighbor nodes and metapaths, into prompt-based learning to enhance the\ncapabilities of SLMs. Experimental results on three types of biomedical and\nopen-domain datasets under few-shot settings demonstrate the effectiveness of\nour approach, surpassing most baselines and even conventional fine-tuning\napproaches trained on full datasets. Our findings further highlight the strong\ncapabilities of SLMs: in combination with knowledge graphs and prompt-based\nlearning, SLMs demonstrate the potential to surpass LLMs with larger number of\nparameters. Our code and datasets are available on GitHub.", "published": "2024-07-26 14:07:00", "link": "http://arxiv.org/abs/2407.18752v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Embedding And Clustering Your Data Can Improve Contrastive Pretraining", "abstract": "Recent studies of large-scale contrastive pretraining in the text embedding\ndomain show that using single-source minibatches, rather than mixed-source\nminibatches, can substantially improve overall model accuracy. In this work, we\nexplore extending training data stratification beyond source granularity by\nleveraging a pretrained text embedding model and the classic k-means clustering\nalgorithm to further split training data apart by the semantic clusters within\neach source. Experimentally, we observe a notable increase in NDCG@10 when\npretraining a BERT-based text embedding model on query-passage pairs from the\nMSMARCO passage retrieval dataset. Additionally, we conceptually connect our\nclustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B\nmethodology and the nearest-neighbor-based hard-negative mining aspect of the\nANCE methodology and discuss how this unified view motivates future lines of\nresearch on the organization of contrastive pretraining data.", "published": "2024-07-26 17:36:40", "link": "http://arxiv.org/abs/2407.18887v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Optimizing Numerical Estimation and Operational Efficiency in the Legal\n  Domain through Large Language Models", "abstract": "The legal landscape encompasses a wide array of lawsuit types, presenting\nlawyers with challenges in delivering timely and accurate information to\nclients, particularly concerning critical aspects like potential imprisonment\nduration or financial repercussions. Compounded by the scarcity of legal\nexperts, there's an urgent need to enhance the efficiency of traditional legal\nworkflows. Recent advances in deep learning, especially Large Language Models\n(LLMs), offer promising solutions to this challenge. Leveraging LLMs'\nmathematical reasoning capabilities, we propose a novel approach integrating\nLLM-based methodologies with specially designed prompts to address precision\nrequirements in legal Artificial Intelligence (LegalAI) applications. The\nproposed work seeks to bridge the gap between traditional legal practices and\nmodern technological advancements, paving the way for a more accessible,\nefficient, and equitable legal system. To validate this method, we introduce a\ncurated dataset tailored to precision-oriented LegalAI tasks, serving as a\nbenchmark for evaluating LLM-based approaches. Extensive experimentation\nconfirms the efficacy of our methodology in generating accurate numerical\nestimates within the legal domain, emphasizing the role of LLMs in streamlining\nlegal processes and meeting the evolving demands of LegalAI.", "published": "2024-07-26 18:46:39", "link": "http://arxiv.org/abs/2407.19041v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Many-Shot In-Context Learning for Molecular Inverse Design", "abstract": "Large Language Models (LLMs) have demonstrated great performance in few-shot\nIn-Context Learning (ICL) for a variety of generative and discriminative\nchemical design tasks. The newly expanded context windows of LLMs can further\nimprove ICL capabilities for molecular inverse design and lead optimization. To\ntake full advantage of these capabilities we developed a new semi-supervised\nlearning method that overcomes the lack of experimental data available for\nmany-shot ICL. Our approach involves iterative inclusion of LLM generated\nmolecules with high predicted performance, along with experimental data. We\nfurther integrated our method in a multi-modal LLM which allows for the\ninteractive modification of generated molecular structures using text\ninstructions. As we show, the new method greatly improves upon existing ICL\nmethods for molecular design while being accessible and easy to use for\nscientists.", "published": "2024-07-26 21:10:50", "link": "http://arxiv.org/abs/2407.19089v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Large Language Models for the Interpretation of Building\n  Regulations", "abstract": "Compliance checking is an essential part of a construction project. The\nrecent rapid uptake of building information models (BIM) in the construction\nindustry has created more opportunities for automated compliance checking\n(ACC). BIM enables sharing of digital building design data that can be used for\ncompliance checking with legal requirements, which are conventionally conveyed\nin natural language and not intended for machine processing. Creating a\ncomputable representation of legal requirements suitable for ACC is complex,\ncostly, and time-consuming. Large language models (LLMs) such as the generative\npre-trained transformers (GPT), GPT-3.5 and GPT-4, powering OpenAI's ChatGPT,\ncan generate logically coherent text and source code responding to user\nprompts. This capability could be used to automate the conversion of building\nregulations into a semantic and computable representation. This paper evaluates\nthe performance of LLMs in translating building regulations into LegalRuleML in\na few-shot learning setup. By providing GPT-3.5 with only a few example\ntranslations, it can learn the basic structure of the format. Using a system\nprompt, we further specify the LegalRuleML representation and explore the\nexistence of expert domain knowledge in the model. Such domain knowledge might\nbe ingrained in GPT-3.5 through the broad pre-training but needs to be brought\nforth by careful contextualisation. Finally, we investigate whether strategies\nsuch as chain-of-thought reasoning and self-consistency could apply to this use\ncase. As LLMs become more sophisticated, the increased common sense, logical\ncoherence, and means to domain adaptation can significantly support ACC,\nleading to more efficient and effective checking processes.", "published": "2024-07-26 08:30:47", "link": "http://arxiv.org/abs/2407.21060v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fairness Definitions in Language Models Explained", "abstract": "Language Models (LMs) have demonstrated exceptional performance across\nvarious Natural Language Processing (NLP) tasks. Despite these advancements,\nLMs can inherit and amplify societal biases related to sensitive attributes\nsuch as gender and race, limiting their adoption in real-world applications.\nTherefore, fairness has been extensively explored in LMs, leading to the\nproposal of various fairness notions. However, the lack of clear agreement on\nwhich fairness definition to apply in specific contexts (\\textit{e.g.,}\nmedium-sized LMs versus large-sized LMs) and the complexity of understanding\nthe distinctions between these definitions can create confusion and impede\nfurther progress. To this end, this paper proposes a systematic survey that\nclarifies the definitions of fairness as they apply to LMs. Specifically, we\nbegin with a brief introduction to LMs and fairness in LMs, followed by a\ncomprehensive, up-to-date overview of existing fairness notions in LMs and the\nintroduction of a novel taxonomy that categorizes these concepts based on their\nfoundational principles and operational distinctions. We further illustrate\neach definition through experiments, showcasing their practical implications\nand outcomes. Finally, we discuss current research challenges and open\nquestions, aiming to foster innovative ideas and advance the field. The\nimplementation and additional resources are publicly available at\nhttps://github.com/LavinWong/Fairness-in-Large-Language-Models/tree/main/definitions.", "published": "2024-07-26 01:21:25", "link": "http://arxiv.org/abs/2407.18454v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Dysarthric Speech Recognition for Unseen Speakers via\n  Prototype-Based Adaptation", "abstract": "Dysarthric speech recognition (DSR) presents a formidable challenge due to\ninherent inter-speaker variability, leading to severe performance degradation\nwhen applying DSR models to new dysarthric speakers. Traditional speaker\nadaptation methodologies typically involve fine-tuning models for each speaker,\nbut this strategy is cost-prohibitive and inconvenient for disabled users,\nrequiring substantial data collection. To address this issue, we introduce a\nprototype-based approach that markedly improves DSR performance for unseen\ndysarthric speakers without additional fine-tuning. Our method employs a\nfeature extractor trained with HuBERT to produce per-word prototypes that\nencapsulate the characteristics of previously unseen speakers. These prototypes\nserve as the basis for classification. Additionally, we incorporate supervised\ncontrastive learning to refine feature extraction. By enhancing representation\nquality, we further improve DSR performance, enabling effective personalized\nDSR. We release our code at https://github.com/NKU-HLT/PB-DSR.", "published": "2024-07-26 02:03:23", "link": "http://arxiv.org/abs/2407.18461v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Constructing the CORD-19 Vaccine Dataset", "abstract": "We introduce new dataset 'CORD-19-Vaccination' to cater to scientists\nspecifically looking into COVID-19 vaccine-related research. This dataset is\nextracted from CORD-19 dataset [Wang et al., 2020] and augmented with new\ncolumns for language detail, author demography, keywords, and topic per paper.\nFacebook's fastText model is used to identify languages [Joulin et al., 2016].\nTo establish author demography (author affiliation, lab/institution location,\nand lab/institution country columns) we processed the JSON file for each paper\nand then further enhanced using Google's search API to determine country\nvalues. 'Yake' was used to extract keywords from the title, abstract, and body\nof each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to\nadd topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset,\nwe demonstrate a question-answering task like the one used in the CORD-19\nKaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential\nsentence classification was performed on each paper's abstract using the model\nfrom Dernoncourt et al. [2016]. We partially hand annotated the training\ndataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination'\ncontains 30k research papers and can be immensely valuable for NLP research\nsuch as text mining, information extraction, and question answering, specific\nto the domain of COVID-19 vaccine research.", "published": "2024-07-26 02:44:55", "link": "http://arxiv.org/abs/2407.18471v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Reliable Common-Sense Reasoning Socialbot Built Using LLMs and\n  Goal-Directed ASP", "abstract": "The development of large language models (LLMs), such as GPT, has enabled the\nconstruction of several socialbots, like ChatGPT, that are receiving a lot of\nattention for their ability to simulate a human conversation. However, the\nconversation is not guided by a goal and is hard to control. In addition,\nbecause LLMs rely more on pattern recognition than deductive reasoning, they\ncan give confusing answers and have difficulty integrating multiple topics into\na cohesive response. These limitations often lead the LLM to deviate from the\nmain topic to keep the conversation interesting. We propose AutoCompanion, a\nsocialbot that uses an LLM model to translate natural language into predicates\n(and vice versa) and employs commonsense reasoning based on Answer Set\nProgramming (ASP) to hold a social conversation with a human. In particular, we\nrely on s(CASP), a goal-directed implementation of ASP as the backend. This\npaper presents the framework design and how an LLM is used to parse user\nmessages and generate a response from the s(CASP) engine output. To validate\nour proposal, we describe (real) conversations in which the chatbot's goal is\nto keep the user entertained by talking about movies and books, and s(CASP)\nensures (i) correctness of answers, (ii) coherence (and precision) during the\nconversation, which it dynamically regulates to achieve its specific purpose,\nand (iii) no deviation from the main topic.", "published": "2024-07-26 04:13:43", "link": "http://arxiv.org/abs/2407.18498v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "The formation of perceptual space in early phonetic acquisition: a\n  cross-linguistic modeling approach", "abstract": "This study investigates how learners organize perceptual space in early\nphonetic acquisition by advancing previous studies in two key aspects. Firstly,\nit examines the shape of the learned hidden representation as well as its\nability to categorize phonetic categories. Secondly, it explores the impact of\ntraining models on context-free acoustic information, without involving\ncontextual cues, on phonetic acquisition, closely mimicking the early language\nlearning stage. Using a cross-linguistic modeling approach, autoencoder models\nare trained on English and Mandarin and evaluated in both native and non-native\nconditions, following experimental conditions used in infant language\nperception studies. The results demonstrate that unsupervised bottom-up\ntraining on context-free acoustic information leads to comparable learned\nrepresentations of perceptual space between native and non-native conditions\nfor both English and Mandarin, resembling the early stage of universal\nlistening in infants. These findings provide insights into the organization of\nperceptual space during early phonetic acquisition and contribute to our\nunderstanding of the formation and representation of phonetic categories.", "published": "2024-07-26 04:18:36", "link": "http://arxiv.org/abs/2407.18501v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Is larger always better? Evaluating and prompting large language models\n  for non-generative medical tasks", "abstract": "The use of Large Language Models (LLMs) in medicine is growing, but their\nability to handle both structured Electronic Health Record (EHR) data and\nunstructured clinical notes is not well-studied. This study benchmarks various\nmodels, including GPT-based LLMs, BERT-based models, and traditional clinical\npredictive models, for non-generative medical tasks utilizing renowned\ndatasets. We assessed 14 language models (9 GPT-based and 5 BERT-based) and 7\ntraditional predictive models using the MIMIC dataset (ICU patient records) and\nthe TJH dataset (early COVID-19 EHR data), focusing on tasks such as mortality\nand readmission prediction, disease hierarchy reconstruction, and biomedical\nsentence matching, comparing both zero-shot and finetuned performance. Results\nindicated that LLMs exhibited robust zero-shot predictive capabilities on\nstructured EHR data when using well-designed prompting strategies, frequently\nsurpassing traditional models. However, for unstructured medical texts, LLMs\ndid not outperform finetuned BERT models, which excelled in both supervised and\nunsupervised tasks. Consequently, while LLMs are effective for zero-shot\nlearning on structured data, finetuned BERT models are more suitable for\nunstructured texts, underscoring the importance of selecting models based on\nspecific task requirements and data characteristics to optimize the application\nof NLP technology in healthcare.", "published": "2024-07-26 06:09:10", "link": "http://arxiv.org/abs/2407.18525v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended\n  Text Generation", "abstract": "Decoding from the output distributions of large language models to produce\nhigh-quality text is a complex challenge in language modeling. Various\napproaches, such as beam search, sampling with temperature, $k-$sampling,\nnucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive\nsearch, have been proposed to address this problem, aiming to improve\ncoherence, diversity, as well as resemblance to human-generated text. In this\nstudy, we introduce adaptive contrastive search, a novel decoding strategy\nextending contrastive search by incorporating an adaptive degeneration penalty,\nguided by the estimated uncertainty of the model at each generation step. This\nstrategy is designed to enhance both the creativity and diversity of the\nlanguage modeling process while at the same time producing coherent and\nhigh-quality generated text output. Our findings indicate performance\nenhancement in both aspects, across different model architectures and datasets,\nunderscoring the effectiveness of our method in text generation tasks. Our code\nbase, datasets, and models are publicly available.", "published": "2024-07-26 12:23:54", "link": "http://arxiv.org/abs/2407.18698v2", "categories": ["cs.CL", "cs.LG", "stat.ME", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Cluster-norm for Unsupervised Probing of Knowledge", "abstract": "The deployment of language models brings challenges in generating reliable\ninformation, especially when these models are fine-tuned using human\npreferences. To extract encoded knowledge without (potentially) biased human\nlabels, unsupervised probing techniques like Contrast-Consistent Search (CCS)\nhave been developed (Burns et al., 2022). However, salient but unrelated\nfeatures in a given dataset can mislead these probes (Farquhar et al., 2023).\nAddressing this, we propose a cluster normalization method to minimize the\nimpact of such features by clustering and normalizing activations of contrast\npairs before applying unsupervised probing techniques. While this approach does\nnot address the issue of differentiating between knowledge in general and\nsimulated knowledge - a major issue in the literature of latent knowledge\nelicitation (Christiano et al., 2021) - it significantly improves the ability\nof unsupervised probes to identify the intended knowledge amidst distractions.", "published": "2024-07-26 12:57:54", "link": "http://arxiv.org/abs/2407.18712v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "AppWorld: A Controllable World of Apps and People for Benchmarking\n  Interactive Coding Agents", "abstract": "Autonomous agents that address day-to-day digital tasks (e.g., ordering\ngroceries for a household), must not only operate multiple apps (e.g., notes,\nmessaging, shopping app) via APIs, but also generate rich code with complex\ncontrol flow in an iterative manner based on their interaction with the\nenvironment. However, existing benchmarks for tool use are inadequate, as they\nonly cover tasks that require a simple sequence of API calls.\n  To remedy this gap, we built $\\textbf{AppWorld Engine}$, a high-quality\nexecution environment (60K lines of code) of 9 day-to-day apps operable via 457\nAPIs and populated with realistic digital activities simulating the lives of\n~100 fictitious users. We then created $\\textbf{AppWorld Benchmark}$ (40K lines\nof code), a suite of 750 natural, diverse, and challenging autonomous agent\ntasks requiring rich and interactive code generation. It supports robust\nprogrammatic evaluation with state-based unit tests, allowing for different\nways of completing a task while also checking for unexpected changes, i.e.,\ncollateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our\n'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least\n16% fewer. This highlights the benchmark's difficulty and AppWorld's potential\nto push the frontiers of interactive coding agents. The project website is\navailable at https://appworld.dev/.", "published": "2024-07-26 17:55:45", "link": "http://arxiv.org/abs/2407.18901v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Wolf: Dense Video Captioning with a World Summarization Framework", "abstract": "We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Webpage: https://wolfv0.github.io/.", "published": "2024-07-26 17:59:09", "link": "http://arxiv.org/abs/2407.18908v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable\n  Frameworks", "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\nincreasing demands of application scenarios have driven the evolution of RAG,\nleading to the integration of advanced retrievers, LLMs and other complementary\ntechnologies, which in turn has amplified the intricacy of RAG systems.\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\nwith many methods struggling to be unified under the process of\n\"retrieve-then-generate\". In this context, this paper examines the limitations\nof the existing RAG paradigm and introduces the modular RAG framework. By\ndecomposing complex RAG systems into independent modules and specialized\noperators, it facilitates a highly reconfigurable framework. Modular RAG\ntranscends the traditional linear architecture, embracing a more advanced\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\nextensive research, this paper further identifies prevalent RAG\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\nanalysis of their respective implementation nuances. Modular RAG presents\ninnovative opportunities for the conceptualization and deployment of RAG\nsystems. Finally, the paper explores the potential emergence of new operators\nand paradigms, establishing a solid theoretical foundation and a practical\nroadmap for the continued evolution and practical deployment of RAG\ntechnologies.", "published": "2024-07-26 03:45:30", "link": "http://arxiv.org/abs/2407.21059v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving noisy student training for low-resource languages in\n  End-to-End ASR using CycleGAN and inter-domain losses", "abstract": "Training a semi-supervised end-to-end speech recognition system using noisy\nstudent training has significantly improved performance. However, this approach\nrequires a substantial amount of paired speech-text and unlabeled speech, which\nis costly for low-resource languages. Therefore, this paper considers a more\nextreme case of semi-supervised end-to-end automatic speech recognition where\nthere are limited paired speech-text, unlabeled speech (less than five hours),\nand abundant external text. Firstly, we observe improved performance by\ntraining the model using our previous work on semi-supervised learning\n\"CycleGAN and inter-domain losses\" solely with external text. Secondly, we\nenhance \"CycleGAN and inter-domain losses\" by incorporating automatic\nhyperparameter tuning, calling it \"enhanced CycleGAN inter-domain losses.\"\nThirdly, we integrate it into the noisy student training approach pipeline for\nlow-resource scenarios. Our experimental results, conducted on six non-English\nlanguages from Voxforge and Common Voice, show a 20% word error rate reduction\ncompared to the baseline teacher model and a 10% word error rate reduction\ncompared to the baseline best student model, highlighting the significant\nimprovements achieved through our proposed method.", "published": "2024-07-26 10:57:06", "link": "http://arxiv.org/abs/2407.21061v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Every Part Matters: Integrity Verification of Scientific Figures Based\n  on Multimodal Large Language Models", "abstract": "This paper tackles a key issue in the interpretation of scientific figures:\nthe fine-grained alignment of text and figures. It advances beyond prior\nresearch that primarily dealt with straightforward, data-driven visualizations\nsuch as bar and pie charts and only offered a basic understanding of diagrams\nthrough captioning and classification. We introduce a novel task, Figure\nIntegrity Verification, designed to evaluate the precision of technologies in\naligning textual knowledge with visual elements in scientific figures. To\nsupport this, we develop a semi-automated method for constructing a large-scale\ndataset, Figure-seg, specifically designed for this task. Additionally, we\npropose an innovative framework, Every Part Matters (EPM), which leverages\nMultimodal Large Language Models (MLLMs) to not only incrementally improve the\nalignment and verification of text-figure integrity but also enhance integrity\nthrough analogical reasoning. Our comprehensive experiments show that these\ninnovations substantially improve upon existing methods, allowing for more\nprecise and thorough analysis of complex scientific figures. This progress not\nonly enhances our understanding of multimodal technologies but also stimulates\nfurther research and practical applications across fields requiring the\naccurate interpretation of complex visual data.", "published": "2024-07-26 09:35:36", "link": "http://arxiv.org/abs/2407.18626v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.DL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Multimodal Emotion Recognition using Audio-Video Transformer Fusion with\n  Cross Attention", "abstract": "Understanding emotions is a fundamental aspect of human communication.\nIntegrating audio and video signals offers a more comprehensive understanding\nof emotional states compared to traditional methods that rely on a single data\nsource, such as speech or facial expressions. Despite its potential, multimodal\nemotion recognition faces significant challenges, particularly in\nsynchronization, feature extraction, and fusion of diverse data sources. To\naddress these issues, this paper introduces a novel transformer-based model\nnamed Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA\nmodel employs a transformer fusion approach to effectively capture and\nsynchronize interlinked features from both audio and video inputs, thereby\nresolving synchronization problems. Additionally, the Cross Attention mechanism\nwithin AVT-CA selectively extracts and emphasizes critical features while\ndiscarding irrelevant ones from both modalities, addressing feature extraction\nand fusion challenges. Extensive experimental analysis conducted on the\nCMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the\nproposed model. The results underscore the importance of AVT-CA in developing\nprecise and reliable multimodal emotion recognition systems for practical\napplications.", "published": "2024-07-26 07:05:04", "link": "http://arxiv.org/abs/2407.18552v3", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS", "F.2.2; I.2.7"], "primary_category": "cs.MM"}
{"title": "Matlab-based Epoch Extraction for Speaker Differentiation", "abstract": "Epoch extraction has become increasingly popular in recent years for speech\nanalysis research because accurately detecting the location of the Epoch is\ncrucial for analyzing speech signals. The Epoch, occurring at the instant of\nexcitation in the vocal tract system, particularly during glottal closure,\nplays a significant role in differentiating speakers in multi-speaker\nconversations. However, the extraction of the Epoch poses a challenge due to\nthe time-varying factors in the vocal tract system, which makes deconvolution\nfor obtaining the original excitation location more complex. In this paper,\nvarious methods for Epoch extraction, including Zero Frequency Filtering (ZFF)\nand Zero Frequency Resonator (ZFR), will be discussed, and their pros and cons\nevaluated. In addition, the stability, accuracy, and feasibility of each method\nwill be compared. The evaluation will involve a Matlab-based locking algorithm,\nand a proposed hardware implementation using Raspberry pi for speaker\ndifferentiation. The experiment includes six individuals uttering the phrase\n\"The University of Mississippi,\" with one person acting as the reference or\n\"lock\" speaker. The number of epochs occurring at similar positions to the\nreference speaker will be counted as Delta, with larger Delta values indicating\ngreater speaker similarity. Experimental results demonstrate that when the\nspeaker remains the same, the average number of Delta is 7.5, while for\ndifferent speakers, the average number of Delta decreases to 3, 2, 2, and 1,\nrespectively, representing a decrease of approximately 73% in the number of\nepochs at similar positions compared to the reference speaker.", "published": "2024-07-26 01:01:32", "link": "http://arxiv.org/abs/2407.18447v1", "categories": ["eess.AS", "94A12 (Primary), 68T10 (Secondary)", "I.5.4; H.5.5"], "primary_category": "eess.AS"}
{"title": "VoxSim: A perceptual voice similarity dataset", "abstract": "This paper introduces VoxSim, a dataset of perceptual voice similarity\nratings. Recent efforts to automate the assessment of speech synthesis\ntechnologies have primarily focused on predicting mean opinion score of\nnaturalness, leaving speaker voice similarity relatively unexplored due to a\nlack of extensive training data. To address this, we generate about 41k\nutterance pairs from the VoxCeleb dataset, a widely utilised speech dataset for\nspeaker recognition, and collect nearly 70k speaker similarity scores through a\nlistening test. VoxSim offers a valuable resource for the development and\nbenchmarking of speaker similarity prediction models. We provide baseline\nresults of speaker similarity prediction models on the VoxSim test set and\nfurther demonstrate that the model trained on our dataset generalises to the\nout-of-domain VCC2018 dataset.", "published": "2024-07-26 04:27:13", "link": "http://arxiv.org/abs/2407.18505v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Integrating Posture Control in Speech Motor Models: A\n  Parallel-Structured Simulation Approach", "abstract": "Posture is an essential aspect of motor behavior, necessitating continuous\nmuscle activation to counteract gravity. It remains stable under perturbation,\naiding in maintaining bodily balance and enabling movement execution.\nSimilarities have been observed between gross body postures and speech\npostures, such as those involving the jaw, tongue, and lips, which also exhibit\nresilience to perturbations and assist in equilibrium and movement. Although\npostural control is a recognized element of human movement and balance,\nparticularly in broader motor skills, it has not been adequately incorporated\ninto existing speech motor control models, which typically concentrate on the\ngestures or motor commands associated with specific speech movements,\noverlooking the influence of postural control and gravity. Here we introduce a\nmodel that aligns speech posture and movement, using simulations to explore\nwhether speech posture within this framework mirrors the principles of bodily\npostural control. Our findings indicate that, akin to body posture, speech\nposture is also robust to perturbation and plays a significant role in\nmaintaining local segment balance and enhancing speech production.", "published": "2024-07-26 05:12:31", "link": "http://arxiv.org/abs/2407.18516v1", "categories": ["eess.AS", "cs.SY", "eess.SY"], "primary_category": "eess.AS"}
{"title": "SLIM: Style-Linguistics Mismatch Model for Generalized Audio Deepfake\n  Detection", "abstract": "Audio deepfake detection (ADD) is crucial to combat the misuse of speech\nsynthesized from generative AI models. Existing ADD models suffer from\ngeneralization issues, with a large performance discrepancy between in-domain\nand out-of-domain data. Moreover, the black-box nature of existing models\nlimits their use in real-world scenarios, where explanations are required for\nmodel decisions. To alleviate these issues, we introduce a new ADD model that\nexplicitly uses the StyleLInguistics Mismatch (SLIM) in fake speech to separate\nthem from real speech. SLIM first employs self-supervised pretraining on only\nreal samples to learn the style-linguistics dependency in the real class. The\nlearned features are then used in complement with standard pretrained acoustic\nfeatures (e.g., Wav2vec) to learn a classifier on the real and fake classes.\nWhen the feature encoders are frozen, SLIM outperforms benchmark methods on\nout-of-domain datasets while achieving competitive results on in-domain data.\nThe features learned by SLIM allow us to quantify the (mis)match between style\nand linguistic content in a sample, hence facilitating an explanation of the\nmodel decision.", "published": "2024-07-26 05:23:41", "link": "http://arxiv.org/abs/2407.18517v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Improving NAM-to-Speech Synthesis Intelligibility using\n  Self-Supervised Speech Models", "abstract": "We propose a novel approach to significantly improve the intelligibility in\nthe Non-Audible Murmur (NAM)-to-speech conversion task, leveraging\nself-supervision and sequence-to-sequence (Seq2Seq) learning techniques. Unlike\nconventional methods that explicitly record ground-truth speech, our\nmethodology relies on self-supervision and speech-to-speech synthesis to\nsimulate ground-truth speech. Despite utilizing simulated speech, our method\nsurpasses the current state-of-the-art (SOTA) by 29.08% improvement in the\nMel-Cepstral Distortion (MCD) metric. Additionally, we present error rates and\ndemonstrate our model's proficiency to synthesize speech in novel voices of\ninterest. Moreover, we present a methodology for augmenting the existing CSTR\nNAM TIMIT Plus corpus, setting a benchmark with a Word Error Rate (WER) of\n42.57% to gauge the intelligibility of the synthesized speech. Speech samples\ncan be found at https://nam2speech.github.io/NAM2Speech/", "published": "2024-07-26 06:44:01", "link": "http://arxiv.org/abs/2407.18541v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Bandwidth Expansion Via High Fidelity Generative Adversarial\n  Networks", "abstract": "Speech bandwidth expansion is crucial for expanding the frequency range of\nlow-bandwidth speech signals, thereby improving audio quality, clarity and\nperceptibility in digital applications. Its applications span telephony,\ncompression, text-to-speech synthesis, and speech recognition. This paper\npresents a novel approach using a high-fidelity generative adversarial network,\nunlike cascaded systems, our system is trained end-to-end on paired narrowband\nand wideband speech signals. Our method integrates various bandwidth upsampling\nratios into a single unified model specifically designed for speech bandwidth\nexpansion applications. Our approach exhibits robust performance across various\nbandwidth expansion factors, including those not encountered during training,\ndemonstrating zero-shot capability. To the best of our knowledge, this is the\nfirst work to showcase this capability. The experimental results demonstrate\nthat our method outperforms previous end-to-end approaches, as well as\ninterpolation and traditional techniques, showcasing its effectiveness in\npractical speech enhancement applications.", "published": "2024-07-26 07:54:47", "link": "http://arxiv.org/abs/2407.18571v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Physics-Informed Neural Network-Based Approach for the Spatial\n  Upsampling of Spherical Microphone Arrays", "abstract": "Spherical microphone arrays are convenient tools for capturing the spatial\ncharacteristics of a sound field. However, achieving superior spatial\nresolution requires arrays with numerous capsules, consequently leading to\nexpensive devices. To address this issue, we present a method for spatially\nupsampling spherical microphone arrays with a limited number of capsules. Our\napproach exploits a physics-informed neural network with Rowdy activation\nfunctions, leveraging physical constraints to provide high-order microphone\narray signals, starting from low-order devices. Results show that, within its\ndomain of application, our approach outperforms a state of the art method based\non signal processing for spherical microphone arrays upsampling.", "published": "2024-07-26 13:35:06", "link": "http://arxiv.org/abs/2407.18732v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Utilizing TTS Synthesized Data for Efficient Development of Keyword\n  Spotting Model", "abstract": "This paper explores the use of TTS synthesized training data for KWS (keyword\nspotting) task while minimizing development cost and time. Keyword spotting\nmodels require a huge amount of training data to be accurate, and obtaining\nsuch training data can be costly. In the current state of the art, TTS models\ncan generate large amounts of natural-sounding data, which can help reducing\ncost and time for KWS model development. Still, TTS generated data can be\nlacking diversity compared to real data. To pursue maximizing KWS model\naccuracy under the constraint of limited resources and current TTS capability,\nwe explored various strategies to mix TTS data and real human speech data, with\na focus on minimizing real data use and maximizing diversity of TTS output. Our\nexperimental results indicate that relatively small amounts of real audio data\nwith speaker diversity (100 speakers, 2k utterances) and large amounts of TTS\nsynthesized data can achieve reasonably high accuracy (within 3x error rate of\nbaseline), compared to the baseline (trained with 3.8M real positive\nutterances).", "published": "2024-07-26 17:24:50", "link": "http://arxiv.org/abs/2407.18879v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
