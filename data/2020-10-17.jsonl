{"title": "Incorporate Semantic Structures into Machine Translation Evaluation via\n  UCCA", "abstract": "Copying mechanism has been commonly used in neural paraphrasing networks and\nother text generation tasks, in which some important words in the input\nsequence are preserved in the output sequence. Similarly, in machine\ntranslation, we notice that there are certain words or phrases appearing in all\ngood translations of one source text, and these words tend to convey important\nsemantic information. Therefore, in this work, we define words carrying\nimportant semantic meanings in sentences as semantic core words. Moreover, we\npropose an MT evaluation approach named Semantically Weighted Sentence\nSimilarity (SWSS). It leverages the power of UCCA to identify semantic core\nwords, and then calculates sentence similarity scores on the overlap of\nsemantic core words. Experimental results show that SWSS can consistently\nimprove the performance of popular MT evaluation metrics which are based on\nlexical similarity.", "published": "2020-10-17 06:47:58", "link": "http://arxiv.org/abs/2010.08728v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich\n  Semantic Annotations for Task-Oriented Dialogue Modeling", "abstract": "In order to alleviate the shortage of multi-domain data and to capture\ndiscourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a\nlarge-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic\nAnnotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn\nsemantically annotated dialogues, with more than 150K utterances spanning over\n12 domains, which is larger than all previous annotated H2H conversational\ndatasets. Both single- and multi-domain dialogues are constructed, accounting\nfor 65% and 35%, respectively. Each dialogue is labeled with comprehensive\ndialogue annotations, including dialogue goal in the form of natural language\ndescription, domain, dialogue states and acts at both the user and system side.\nIn addition to traditional dialogue annotations, we especially provide\nlinguistic annotations on discourse phenomena, e.g., ellipsis and coreference,\nin dialogues, which are useful for dialogue coreference and ellipsis resolution\ntasks. Apart from the fully annotated dataset, we also present a detailed\ndescription of the data collection procedure, statistics and analysis of the\ndataset. A series of benchmark models and results are reported, including\nnatural language understanding (intent detection & slot filling), dialogue\nstate tracking and dialogue context-to-text generation, as well as coreference\nand ellipsis resolution, which facilitate the baseline comparison for future\nresearch on this corpus.", "published": "2020-10-17 08:18:59", "link": "http://arxiv.org/abs/2010.08738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUSATNLP@HASOC-Dravidian-CodeMix-FIRE2020:Identifying Offensive Language\n  from ManglishTweets", "abstract": "With the popularity of social media, communications through blogs, Facebook,\nTwitter, and other plat-forms have increased. Initially, English was the only\nmedium of communication. Fortunately, now we can communicate in any language.\nIt has led to people using English and their own native or mother tongue\nlanguage in a mixed form. Sometimes, comments in other languages have English\ntransliterated format or other cases; people use the intended language scripts.\nIdentifying sentiments and offensive content from such code mixed tweets is a\nnecessary task in these times. We present a working model submitted for Task2\nof the sub-track HASOC Offensive Language Identification- DravidianCodeMix in\nForum for Information Retrieval Evaluation, 2020. It is a message level\nclassification task. An embedding model-based classifier identifies offensive\nand not offensive comments in our approach. We applied this method in the\nManglish dataset provided along with the sub-track.", "published": "2020-10-17 10:11:41", "link": "http://arxiv.org/abs/2010.08756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistency and Coherency Enhanced Story Generation", "abstract": "Story generation is a challenging task, which demands to maintain consistency\nof the plots and characters throughout the story. Previous works have shown\nthat GPT2, a large-scale language model, has achieved good performance on story\ngeneration. However, we observe that several serious issues still exist in the\nstories generated by GPT2 which can be categorized into two folds: consistency\nand coherency. In terms of consistency, on one hand, GPT2 cannot guarantee the\nconsistency of the plots explicitly. On the other hand, the generated stories\nusually contain coreference errors. In terms of coherency, GPT2 does not take\naccount of the discourse relations between sentences of stories directly. To\nenhance the consistency and coherency of the generated stories, we propose a\ntwo-stage generation framework, where the first stage is to organize the story\noutline which depicts the story plots and events, and the second stage is to\nexpand the outline into a complete story. Therefore the plots consistency can\nbe controlled and guaranteed explicitly. In addition, coreference supervision\nsignals are incorporated to reduce coreference errors and improve the\ncoreference consistency. Moreover, we design an auxiliary task of discourse\nrelation modeling to improve the coherency of the generated stories.\nExperimental results on a story dataset show that our model outperforms the\nbaseline approaches in terms of both automatic metrics and human evaluation.", "published": "2020-10-17 16:40:37", "link": "http://arxiv.org/abs/2010.08822v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Grounded Dialogue Generation with Pre-trained Language Models", "abstract": "We study knowledge-grounded dialogue generation with pre-trained language\nmodels. To leverage the redundant external knowledge under capacity constraint,\nwe propose equipping response generation defined by a pre-trained language\nmodel with a knowledge selection module, and an unsupervised approach to\njointly optimizing knowledge selection and response generation with unlabeled\ndialogues. Empirical results on two benchmarks indicate that our model can\nsignificantly outperform state-of-the-art methods in both automatic evaluation\nand human judgment.", "published": "2020-10-17 16:49:43", "link": "http://arxiv.org/abs/2010.08824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TweetBERT: A Pretrained Language Representation Model for Twitter Text\n  Analysis", "abstract": "Twitter is a well-known microblogging social site where users express their\nviews and opinions in real-time. As a result, tweets tend to contain valuable\ninformation. With the advancements of deep learning in the domain of natural\nlanguage processing, extracting meaningful information from tweets has become a\ngrowing interest among natural language researchers. Applying existing language\nrepresentation models to extract information from Twitter does not often\nproduce good results. Moreover, there is no existing language representation\nmodels for text analysis specific to the social media domain. Hence, in this\narticle, we introduce two TweetBERT models, which are domain specific language\npresentation models, pre-trained on millions of tweets. We show that the\nTweetBERT models significantly outperform the traditional BERT models in\nTwitter text mining tasks by more than 7% on each Twitter dataset. We also\nprovide an extensive analysis by evaluating seven BERT models on 31 different\ndatasets. Our results validate our hypothesis that continuously training\nlanguage models on twitter corpus help performance with Twitter.", "published": "2020-10-17 00:45:02", "link": "http://arxiv.org/abs/2010.11091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Multitask Learning Approach for BERT", "abstract": "Recent works show that learning contextualized embeddings for words is\nbeneficial for downstream tasks. BERT is one successful example of this\napproach. It learns embeddings by solving two tasks, which are masked language\nmodel (masked LM) and the next sentence prediction (NSP). The pre-training of\nBERT can also be framed as a multitask learning problem. In this work, we adopt\nhierarchical multitask learning approaches for BERT pre-training. Pre-training\ntasks are solved at different layers instead of the last layer, and information\nfrom the NSP task is transferred to the masked LM task. Also, we propose a new\npre-training task bigram shift to encode word order information. We choose two\ndownstream tasks, one of which requires sentence-level embeddings (textual\nentailment), and the other requires contextualized embeddings of words\n(question answering). Due to computational restrictions, we use the downstream\ntask data instead of a large dataset for the pre-training to see the\nperformance of proposed models when given a restricted dataset. We test their\nperformance on several probing tasks to analyze learned embeddings. Our results\nshow that imposing a task hierarchy in pre-training improves the performance of\nembeddings.", "published": "2020-10-17 09:23:04", "link": "http://arxiv.org/abs/2011.04451v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Example-Driven Intent Prediction with Observers", "abstract": "A key challenge of dialog systems research is to effectively and efficiently\nadapt to new domains. A scalable paradigm for adaptation necessitates the\ndevelopment of generalizable models that perform well in few-shot settings. In\nthis paper, we focus on the intent classification problem which aims to\nidentify user intents given utterances addressed to the dialog system. We\npropose two approaches for improving the generalizability of utterance\nclassification models: (1) observers and (2) example-driven training. Prior\nwork has shown that BERT-like models tend to attribute a significant amount of\nattention to the [CLS] token, which we hypothesize results in diluted\nrepresentations. Observers are tokens that are not attended to, and are an\nalternative to the [CLS] token as a semantic representation of utterances.\nExample-driven training learns to classify utterances by comparing to examples,\nthereby using the underlying encoder as a sentence similarity model. These\nmethods are complementary; improving the representation through observers\nallows the example-driven model to better measure sentence similarities. When\ncombined, the proposed methods attain state-of-the-art results on three intent\nprediction datasets (\\textsc{banking77}, \\textsc{clinc150}, \\textsc{hwu64}) in\nboth the full data and few-shot (10 examples per intent) settings. Furthermore,\nwe demonstrate that the proposed approach can transfer to new intents and\nacross datasets without any additional training.", "published": "2020-10-17 01:03:06", "link": "http://arxiv.org/abs/2010.08684v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Answer-checking in Context: A Multi-modal FullyAttention Network for\n  Visual Question Answering", "abstract": "Visual Question Answering (VQA) is challenging due to the complex cross-modal\nrelations. It has received extensive attention from the research community.\nFrom the human perspective, to answer a visual question, one needs to read the\nquestion and then refer to the image to generate an answer. This answer will\nthen be checked against the question and image again for the final\nconfirmation. In this paper, we mimic this process and propose a fully\nattention based VQA architecture. Moreover, an answer-checking module is\nproposed to perform a unified attention on the jointly answer, question and\nimage representation to update the answer. This mimics the human answer\nchecking process to consider the answer in the context. With answer-checking\nmodules and transferred BERT layers, our model achieves the state-of-the-art\naccuracy 71.57\\% using fewer parameters on VQA-v2.0 test-standard split.", "published": "2020-10-17 03:37:16", "link": "http://arxiv.org/abs/2010.08708v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Factual Error Correction for Abstractive Summarization Models", "abstract": "Neural abstractive summarization systems have achieved promising progress,\nthanks to the availability of large-scale datasets and models pre-trained with\nself-supervised methods. However, ensuring the factual consistency of the\ngenerated summaries for abstractive summarization systems is a challenge. We\npropose a post-editing corrector module to address this issue by identifying\nand correcting factual errors in generated summaries. The neural corrector\nmodel is pre-trained on artificial examples that are created by applying a\nseries of heuristic transformations on reference summaries. These\ntransformations are inspired by an error analysis of state-of-the-art\nsummarization model outputs. Experimental results show that our model is able\nto correct factual errors in summaries generated by other neural summarization\nmodels and outperforms previous models on factual consistency evaluation on the\nCNN/DailyMail dataset. We also find that transferring from artificial error\ncorrection to downstream settings is still very challenging.", "published": "2020-10-17 04:24:16", "link": "http://arxiv.org/abs/2010.08712v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Active Testing: An Unbiased Evaluation Method for Distantly Supervised\n  Relation Extraction", "abstract": "Distant supervision has been a widely used method for neural relation\nextraction for its convenience of automatically labeling datasets. However,\nexisting works on distantly supervised relation extraction suffer from the low\nquality of test set, which leads to considerable biased performance evaluation.\nThese biases not only result in unfair evaluations but also mislead the\noptimization of neural relation extraction. To mitigate this problem, we\npropose a novel evaluation method named active testing through utilizing both\nthe noisy test set and a few manual annotations. Experiments on a widely used\nbenchmark show that our proposed approach can yield approximately unbiased\nevaluations for distantly supervised relation extractors.", "published": "2020-10-17 12:29:09", "link": "http://arxiv.org/abs/2010.08777v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Question Answering over Knowledge Base using Language Model Embeddings", "abstract": "Knowledge Base, represents facts about the world, often in some form of\nsubsumption ontology, rather than implicitly, embedded in procedural code, the\nway a conventional computer program does. While there is a rapid growth in\nknowledge bases, it poses a challenge of retrieving information from them.\nKnowledge Base Question Answering is one of the promising approaches for\nextracting substantial knowledge from Knowledge Bases. Unlike web search,\nQuestion Answering over a knowledge base gives accurate and concise results,\nprovided that natural language questions can be understood and mapped precisely\nto an answer in the knowledge base. However, some of the existing\nembedding-based methods for knowledge base question answering systems ignore\nthe subtle correlation between the question and the Knowledge Base (e.g.,\nentity types, relation paths, and context) and suffer from the Out Of\nVocabulary problem. In this paper, we focused on using a pre-trained language\nmodel for the Knowledge Base Question Answering task. Firstly, we used Bert\nbase uncased for the initial experiments. We further fine-tuned these\nembeddings with a two-way attention mechanism from the knowledge base to the\nasked question and from the asked question to the knowledge base answer\naspects. Our method is based on a simple Convolutional Neural Network\narchitecture with a Multi-Head Attention mechanism to represent the asked\nquestion dynamically in multiple aspects. Our experimental results show the\neffectiveness and the superiority of the Bert pre-trained language model\nembeddings for question answering systems on knowledge bases over other\nwell-known embedding methods.", "published": "2020-10-17 22:59:34", "link": "http://arxiv.org/abs/2010.08883v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learning from similarity and information extraction from structured\n  documents", "abstract": "The automation of document processing is gaining recent attention due to the\ngreat potential to reduce manual work through improved methods and hardware.\nNeural networks have been successfully applied before - even though they have\nbeen trained only on relatively small datasets with hundreds of documents so\nfar. To successfully explore deep learning techniques and improve the\ninformation extraction results, a dataset with more than twenty-five thousand\ndocuments has been compiled, anonymized and is published as a part of this\nwork. We will expand our previous work where we proved that convolutions, graph\nconvolutions and self-attention can work together and exploit all the\ninformation present in a structured document. Taking the fully trainable method\none step further, we will now design and examine various approaches to using\nsiamese networks, concepts of similarity, one-shot learning and context/memory\nawareness. The aim is to improve micro F1 of per-word classification on the\nhuge real-world document dataset. The results verify the hypothesis that\ntrainable access to a similar (yet still different) page together with its\nalready known target information improves the information extraction.\nFurthermore, the experiments confirm that all proposed architecture parts are\nall required to beat the previous results. The best model improves the previous\nstate-of-the-art results by an 8.25 gain in F1 score. Qualitative analysis is\nprovided to verify that the new model performs better for all target classes.\nAdditionally, multiple structural observations about the causes of the\nunderperformance of some architectures are revealed. All the source codes,\nparameters and implementation details are published together with the dataset\nin the hope to push the research boundaries since all the techniques used in\nthis work are not problem-specific and can be generalized for other tasks and\ncontexts.", "published": "2020-10-17 21:34:52", "link": "http://arxiv.org/abs/2011.07964v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Corpus for English-Japanese Multimodal Neural Machine Translation with\n  Comparable Sentences", "abstract": "Multimodal neural machine translation (NMT) has become an increasingly\nimportant area of research over the years because additional modalities, such\nas image data, can provide more context to textual data. Furthermore, the\nviability of training multimodal NMT models without a large parallel corpus\ncontinues to be investigated due to low availability of parallel sentences with\nimages, particularly for English-Japanese data. However, this void can be\nfilled with comparable sentences that contain bilingual terms and parallel\nphrases, which are naturally created through media such as social network posts\nand e-commerce product descriptions. In this paper, we propose a new multimodal\nEnglish-Japanese corpus with comparable sentences that are compiled from\nexisting image captioning datasets. In addition, we supplement our comparable\nsentences with a smaller parallel corpus for validation and test purposes. To\ntest the performance of this comparable sentence translation scenario, we train\nseveral baseline NMT models with our comparable corpus and evaluate their\nEnglish-Japanese translation performance. Due to low translation scores in our\nbaseline experiments, we believe that current multimodal NMT models are not\ndesigned to effectively utilize comparable sentence data. Despite this, we hope\nfor our corpus to be used to further research into multimodal NMT with\ncomparable sentences.", "published": "2020-10-17 06:12:25", "link": "http://arxiv.org/abs/2010.08725v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Drink Bleach or Do What Now? Covid-HeRA: A Study of Risk-Informed Health\n  Decision Making in the Presence of COVID-19 Misinformation", "abstract": "Given the widespread dissemination of inaccurate medical advice related to\nthe 2019 coronavirus pandemic (COVID-19), such as fake remedies, treatments and\nprevention suggestions, misinformation detection has emerged as an open problem\nof high importance and interest for the research community. Several works study\nhealth misinformation detection, yet little attention has been given to the\nperceived severity of misinformation posts. In this work, we frame health\nmisinformation as a risk assessment task. More specifically, we study the\nseverity of each misinformation story and how readers perceive this severity,\ni.e., how harmful a message believed by the audience can be and what type of\nsignals can be used to recognize potentially malicious fake news and detect\nrefuted claims. To address our research questions, we introduce a new benchmark\ndataset, accompanied by detailed data analysis. We evaluate several traditional\nand state-of-the-art models and show there is a significant gap in performance\nwhen applying traditional misinformation classification models to this task. We\nconclude with open challenges and future directions.", "published": "2020-10-17 08:34:57", "link": "http://arxiv.org/abs/2010.08743v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ArCOV19-Rumors: Arabic COVID-19 Twitter Dataset for Misinformation\n  Detection", "abstract": "In this paper we introduce ArCOV19-Rumors, an Arabic COVID-19 Twitter dataset\nfor misinformation detection composed of tweets containing claims from 27th\nJanuary till the end of April 2020. We collected 138 verified claims, mostly\nfrom popular fact-checking websites, and identified 9.4K relevant tweets to\nthose claims. Tweets were manually-annotated by veracity to support research on\nmisinformation detection, which is one of the major problems faced during a\npandemic. ArCOV19-Rumors supports two levels of misinformation detection over\nTwitter: verifying free-text claims (called claim-level verification) and\nverifying claims expressed in tweets (called tweet-level verification). Our\ndataset covers, in addition to health, claims related to other topical\ncategories that were influenced by COVID-19, namely, social, politics, sports,\nentertainment, and religious. Moreover, we present benchmarking results for\ntweet-level verification on the dataset. We experimented with SOTA models of\nversatile approaches that either exploit content, user profiles features,\ntemporal features and propagation structure of the conversational threads for\ntweet verification.", "published": "2020-10-17 11:21:40", "link": "http://arxiv.org/abs/2010.08768v2", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Studying the Similarity of COVID-19 Sounds based on Correlation Analysis\n  of MFCC", "abstract": "Recently there has been a formidable work which has been put up from the\npeople who are working in the frontlines such as hospitals, clinics, and labs\nalongside researchers and scientists who are also putting tremendous efforts in\nthe fight against COVID-19 pandemic. Due to the preposterous spread of the\nvirus, the integration of the artificial intelligence has taken a considerable\npart in the health sector, by implementing the fundamentals of Automatic Speech\nRecognition (ASR) and deep learning algorithms. In this paper, we illustrate\nthe importance of speech signal processing in the extraction of the\nMel-Frequency Cepstral Coefficients (MFCCs) of the COVID-19 and non-COVID-19\nsamples and find their relationship using Pearson correlation coefficients. Our\nresults show high similarity in MFCCs between different COVID-19 cough and\nbreathing sounds, while MFCC of voice is more robust between COVID-19 and\nnon-COVID-19 samples. Moreover, our results are preliminary, and there is a\npossibility to exclude the voices of COVID-19 patients from further processing\nin diagnosing the disease.", "published": "2020-10-17 11:38:05", "link": "http://arxiv.org/abs/2010.08770v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HABERTOR: An Efficient and Effective Deep Hatespeech Detector", "abstract": "We present our HABERTOR model for detecting hatespeech in large scale\nuser-generated content. Inspired by the recent success of the BERT model, we\npropose several modifications to BERT to enhance the performance on the\ndownstream hatespeech classification task. HABERTOR inherits BERT's\narchitecture, but is different in four aspects: (i) it generates its own\nvocabularies and is pre-trained from the scratch using the largest scale\nhatespeech dataset; (ii) it consists of Quaternion-based factorized components,\nresulting in a much smaller number of parameters, faster training and\ninferencing, as well as less memory usage; (iii) it uses our proposed\nmulti-source ensemble heads with a pooling layer for separate input sources, to\nfurther enhance its effectiveness; and (iv) it uses a regularized adversarial\ntraining with our proposed fine-grained and adaptive noise magnitude to enhance\nits robustness. Through experiments on the large-scale real-world hatespeech\ndataset with 1.4M annotated comments, we show that HABERTOR works better than\n15 state-of-the-art hatespeech detection methods, including fine-tuning\nLanguage Models. In particular, comparing with BERT, our HABERTOR is 4~5 times\nfaster in the training/inferencing phase, uses less than 1/3 of the memory, and\nhas better performance, even though we pre-train it by using less than 1% of\nthe number of words. Our generalizability analysis shows that HABERTOR\ntransfers well to other unseen hatespeech datasets and is a more efficient and\neffective alternative to BERT for the hatespeech classification.", "published": "2020-10-17 21:10:08", "link": "http://arxiv.org/abs/2010.08865v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio-based Near-Duplicate Video Retrieval with Audio Similarity\n  Learning", "abstract": "In this work, we address the problem of audio-based near-duplicate video\nretrieval. We propose the Audio Similarity Learning (AuSiL) approach that\neffectively captures temporal patterns of audio similarity between video pairs.\nFor the robust similarity calculation between two videos, we first extract\nrepresentative audio-based video descriptors by leveraging transfer learning\nbased on a Convolutional Neural Network (CNN) trained on a large scale dataset\nof audio events, and then we calculate the similarity matrix derived from the\npairwise similarity of these descriptors. The similarity matrix is subsequently\nfed to a CNN network that captures the temporal structures existing within its\ncontent. We train our network following a triplet generation process and\noptimizing the triplet loss function. To evaluate the effectiveness of the\nproposed approach, we have manually annotated two publicly available video\ndatasets based on the audio duplicity between their videos. The proposed\napproach achieves very competitive results compared to three state-of-the-art\nmethods. Also, unlike the competing methods, it is very robust to the retrieval\nof audio duplicates generated with speed transformations.", "published": "2020-10-17 08:12:18", "link": "http://arxiv.org/abs/2010.08737v2", "categories": ["cs.MM", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
