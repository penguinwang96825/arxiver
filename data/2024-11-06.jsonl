{"title": "Robust and Fast Bass local volatility", "abstract": "The Bass Local Volatility Model (Bass-LV), as studied in\n\\citep{henry2021bass}, stands out for its ability to eliminate the need for\ninterpolation between maturities. This offers a significant advantage over\ntraditional LV models. However, its performance highly depends on accurate\nconstruction of state price densities and the corresponding marginal\ndistributions and efficient numerical convolutions which are necessary when\nsolving the associated fixed point problems. In this paper, we propose a new\napproach combining local quadratic estimation and lognormal mixture tails for\nthe construction of state price densities. We investigate computational\nefficiency of trapezoidal rule based schemes for numerical convolutions and\nshow that they outperform commonly used Gauss-Hermite quadrature. We\ndemonstrate the performance of the proposed method, both in standard option\npricing models, as well as through a detailed market case study.", "published": "2024-11-06 23:49:52", "link": "http://arxiv.org/abs/2411.04321v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Finding the nonnegative minimal solutions of Cauchy PDEs in a volatility-stabilized market", "abstract": "The strong relative arbitrage problem in Stochastic Portfolio Theory seeks to\ngenerate an investment strategy that almost surely outperforms a benchmark\nportfolio at the end of a given time horizon. The highest relative return in\nrelative arbitrage opportunities is characterized by the smallest nonnegative\ncontinuous solution of a Cauchy problem for a partial differential equation\n(PDE). However, solving this type of PDE poses analytical and numerical\nchallenges, due to the high dimensionality and its non-unique solutions. In\nthis paper, we discuss numerical methods to address the relative arbitrage\nproblem and the associated PDE in a volatility-stabilized market, using\ntime-changed Bessel bridges. We present a practical algorithm and demonstrate\nnumerical results through an example in volatility-stabilized markets.", "published": "2024-11-06 18:08:02", "link": "http://arxiv.org/abs/2411.13558v2", "categories": ["q-fin.CP", "math.PR", "q-fin.MF", "60H10, 91G10"], "primary_category": "q-fin.CP"}
{"title": "Market efficiency, informational asymmetry and pseudo-collusion of adaptively learning agents", "abstract": "We examine the dynamics of informational efficiency in a market with\nasymmetrically informed, boundedly rational traders who adaptively learn\noptimal strategies using simple multiarmed bandit (MAB) algorithms. The\nstrategies available to the traders have two dimensions: on the one hand, the\ntraders must endogenously choose whether to acquire a costly information\nsignal, on the other, they must determine how aggressively they trade by\nchoosing the share of their wealth to be invested in the risky asset. Our study\ncontributes to two strands of literature: the literature comparing the effects\nof competitive and strategic behavior on asset price efficiency under costly\ninformation as well as the actively growing literature on algorithmic tacit\ncollusion and pseudo-collusion in financial markets. We find that for certain\nmarket environments (with low information costs) our model reproduces the\nresults of Kyle [1989] in that the ability of traders to trade strategically\nleads to worse price efficiency compared to the purely competitive case. For\nother environments (with high information costs), on the other hand, our\nresults show that a market with strategically acting traders can be more\nefficient than a purely competitive one. Furthermore, we obtain novel results\non the ability of independently learning traders to coordinate on a\npseudo-collusive behavior, leading to non-competitive pricing. Contrary to some\nrecent contributions (see e.g. [Cartea et al. 2022]), we find that the\npseudo-collusive behavior in our model is robust to a large number of agents,\ndemonstrating that even in the setting of financial markets with a large number\nof independently learning traders non-competitive pricing and pseudo-collusive\nbehavior can frequently arise.", "published": "2024-11-06 10:06:42", "link": "http://arxiv.org/abs/2411.05032v1", "categories": ["econ.TH", "cs.GT", "q-fin.CP"], "primary_category": "econ.TH"}
{"title": "Volatility Parametrizations with Random Coefficients: Analytic Flexibility for Implied Volatility Surfaces", "abstract": "It is a market practice to express market-implied volatilities in some\nparametric form. The most popular parametrizations are based on or inspired by\nan underlying stochastic model, like the Heston model (SVI method) or the SABR\nmodel (SABR parametrization). Their popularity is often driven by a closed-form\nrepresentation enabling efficient calibration. However, these representations\nindirectly impose a model-specific volatility structure on observable market\nquotes. When the market's volatility does not follow the parametric model\nregime, the calibration procedure will fail or lead to extreme parameters,\nindicating inconsistency. This article addresses this critical limitation - we\npropose an arbitrage-free framework for letting the parameters from the\nparametric implied volatility formula be random. The method enhances the\nexisting parametrizations and enables a significant widening of the spectrum of\npermissible shapes of implied volatilities while preserving analyticity and,\ntherefore, computation efficiency. We demonstrate the effectiveness of the\nnovel method on real data from short-term index and equity options, where the\nstandard parametrizations fail to capture market dynamics. Our results show\nthat the proposed method is particularly powerful in modeling the implied\nvolatility curves of short expiry options preceding an earnings announcement,\nwhen the risk-neutral probability density function exhibits a bimodal form.", "published": "2024-11-06 16:40:55", "link": "http://arxiv.org/abs/2411.04041v2", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Corporate Fundamentals and Stock Price Co-Movement", "abstract": "We introduce an innovative framework that leverages advanced big data\ntechniques to analyze dynamic co-movement between stocks and their underlying\nfundamentals using high-frequency stock market data. Our method identifies\nleading co-movement stocks through four distinct regression models: Forecast\nError Variance Decomposition, transaction volume-normalized FEVD, Granger\ncausality test frequency, and Granger causality test days. Validated using\nChinese banking sector stocks, our framework uncovers complex relationships\nbetween stock price co-movements and fundamental characteristics, demonstrating\nits robustness and wide applicability across various sectors and markets. This\napproach not only enhances our understanding of market dynamics but also\nprovides actionable insights for investors and policymakers, helping to\nmitigate broader market volatilities and improve financial stability. Our model\nindicates that banks' influence on their peers is significantly affected by\ntheir wealth management business, interbank activities, equity multiplier,\nnon-performing loans, regulatory requirements, and reserve requirement ratios.\nThis aids in mitigating the impact of broader market volatilities and provides\ndeep insights into the unique influence of banks within the financial\necosystem.", "published": "2024-11-06 13:53:49", "link": "http://arxiv.org/abs/2411.03922v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Zero-Coupon Treasury Rates and Returns using the Volatility Index", "abstract": "We study a multivariate autoregressive stochastic volatility model for the\nfirst 3 principal components (level, slope, curvature) of 10 series of\nzero-coupon Treasury bond rates with maturities from 1 to 10 years. We fit this\nmodel using monthly data from 1990. Unlike classic models with hidden\nstochastic volatility, here it is observed as VIX: the volatility index for the\nS&P 500 stock market index. Surprisingly, this stock index volatility works for\nTreasury bonds, too. Next, we prove long-term stability and the Law of Large\nNumbers. We express total returns of zero-coupon bonds using these principal\ncomponents. We prove the Law of Large Numbers for these returns. All results\nare done for discrete and continuous time.", "published": "2024-11-06 06:46:04", "link": "http://arxiv.org/abs/2411.03699v4", "categories": ["q-fin.ST", "math.PR", "stat.AP", "60H10, 60J20, 60J60, 62J05, 62M10, 91B70, 91G30"], "primary_category": "q-fin.ST"}
{"title": "Composing Ensembles of Instrument-Model Pairs for Optimizing Profitability in Algorithmic Trading", "abstract": "Financial markets are nonlinear with complexity, where different types of\nassets are traded between buyers and sellers, each having a view to maximize\ntheir Return on Investment (ROI). Forecasting market trends is a challenging\ntask since various factors like stock-specific news, company profiles, public\nsentiments, and global economic conditions influence them. This paper describes\na daily price directional predictive system of financial instruments,\naddressing the difficulty of predicting short-term price movements. This paper\nwill introduce the development of a novel trading system methodology by\nproposing a two-layer Composing Ensembles architecture, optimized through grid\nsearch, to predict whether the price will rise or fall the next day. This\nstrategy was back-tested on a wide range of financial instruments and time\nframes, demonstrating an improvement of 20% over the benchmark, representing a\nstandard investment strategy.", "published": "2024-11-06 18:17:26", "link": "http://arxiv.org/abs/2411.13559v1", "categories": ["q-fin.TR", "cs.AI", "cs.LG"], "primary_category": "q-fin.TR"}
{"title": "Supervised Autoencoders with Fractionally Differentiated Features and Triple Barrier Labelling Enhance Predictions on Noisy Data", "abstract": "This paper investigates the enhancement of financial time series forecasting\nwith the use of neural networks through supervised autoencoders (SAE), to\nimprove investment strategy performance. Using the Sharpe and Information\nRatios, it specifically examines the impact of noise augmentation and triple\nbarrier labeling on risk-adjusted returns. The study focuses on Bitcoin,\nLitecoin, and Ethereum as the traded assets from January 1, 2016, to April 30,\n2022. Findings indicate that supervised autoencoders, with balanced noise\naugmentation and bottleneck size, significantly boost strategy effectiveness.\nHowever, excessive noise and large bottleneck sizes can impair performance.", "published": "2024-11-06 08:43:03", "link": "http://arxiv.org/abs/2411.12753v2", "categories": ["q-fin.TR", "cs.LG", "stat.CO", "stat.ML"], "primary_category": "q-fin.TR"}
{"title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical\n  Challenge Problems and Beyond", "abstract": "Run-time steering strategies like Medprompt are valuable for guiding large\nlanguage models (LLMs) to top performance on challenging tasks. Medprompt\ndemonstrates that a general LLM can be focused to deliver state-of-the-art\nperformance on specialized domains like medicine by using a prompt to elicit a\nrun-time strategy involving chain of thought reasoning and ensembling. OpenAI's\no1-preview model represents a new paradigm, where a model is designed to do\nrun-time reasoning before generating final responses. We seek to understand the\nbehavior of o1-preview on a diverse set of medical challenge problem\nbenchmarks. Following on the Medprompt study with GPT-4, we systematically\nevaluate the o1-preview model across various medical benchmarks. Notably, even\nwithout prompting techniques, o1-preview largely outperforms the GPT-4 series\nwith Medprompt. We further systematically study the efficacy of classic prompt\nengineering strategies, as represented by Medprompt, within the new paradigm of\nreasoning models. We found that few-shot prompting hinders o1's performance,\nsuggesting that in-context learning may no longer be an effective steering\napproach for reasoning-native models. While ensembling remains viable, it is\nresource-intensive and requires careful cost-performance optimization. Our cost\nand accuracy analysis across run-time strategies reveals a Pareto frontier,\nwith GPT-4o representing a more affordable option and o1-preview achieving\nstate-of-the-art performance at higher cost. Although o1-preview offers top\nperformance, GPT-4o with steering strategies like Medprompt retains value in\nspecific contexts. Moreover, we note that the o1-preview model has reached\nnear-saturation on many existing medical benchmarks, underscoring the need for\nnew, challenging benchmarks. We close with reflections on general directions\nfor inference-time computation with LLMs.", "published": "2024-11-06 01:09:17", "link": "http://arxiv.org/abs/2411.03590v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms\n  in Aligned Language Models", "abstract": "Natural-language assistants are designed to provide users with helpful\nresponses while avoiding harmful outputs, largely achieved through alignment to\nhuman preferences. Yet there is limited understanding of whether alignment\ntechniques may inadvertently perpetuate or even amplify harmful biases\ninherited from their pre-aligned base models. This issue is compounded by the\nchoice of bias evaluation benchmarks in popular preference-finetuned models,\nwhich predominantly focus on dominant social categories, such as binary gender,\nthereby limiting insights into biases affecting underrepresented groups.\nTowards addressing this gap, we center transgender, nonbinary, and other\ngender-diverse identities to investigate how alignment procedures interact with\npre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a\ncomprehensive survey of bias evaluation modalities across leading\npreference-finetuned LLMs, highlighting critical gaps in gender-diverse\nrepresentation, 2) systematic evaluation of gender-diverse biases across 12\nmodels spanning Direct Preference Optimization (DPO) stages, uncovering harms\npopular bias benchmarks fail to detect, and 3) a flexible framework for\nmeasuring harmful biases in implicit reward signals applicable to other social\ncontexts. Our findings reveal that DPO-aligned models are particularly\nsensitive to supervised finetuning (SFT), and can amplify two forms of\nreal-world gender-diverse harms from their base models: stigmatization and\ngender non-affirmative language. We conclude with recommendations tailored to\nDPO and broader alignment practices, advocating for the adoption of\ncommunity-informed bias evaluation frameworks to more effectively identify and\naddress underrepresented harms in LLMs.", "published": "2024-11-06 06:50:50", "link": "http://arxiv.org/abs/2411.03700v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Recent Large Language Models on Generating\n  Hospital Discharge Summaries for Lung Cancer Patients", "abstract": "Generating discharge summaries is a crucial yet time-consuming task in\nclinical practice, essential for conveying pertinent patient information and\nfacilitating continuity of care. Recent advancements in large language models\n(LLMs) have significantly enhanced their capability in understanding and\nsummarizing complex medical texts. This research aims to explore how LLMs can\nalleviate the burden of manual summarization, streamline workflow efficiencies,\nand support informed decision-making in healthcare settings. Clinical notes\nfrom a cohort of 1,099 lung cancer patients were utilized, with a subset of 50\npatients for testing purposes, and 102 patients used for model fine-tuning.\nThis study evaluates the performance of multiple LLMs, including GPT-3.5,\nGPT-4, GPT-4o, and LLaMA 3 8b, in generating discharge summaries. Evaluation\nmetrics included token-level analysis (BLEU, ROUGE-1, ROUGE-2, ROUGE-L) and\nsemantic similarity scores between model-generated summaries and\nphysician-written gold standards. LLaMA 3 8b was further tested on clinical\nnotes of varying lengths to examine the stability of its performance. The study\nfound notable variations in summarization capabilities among LLMs. GPT-4o and\nfine-tuned LLaMA 3 demonstrated superior token-level evaluation metrics, while\nLLaMA 3 consistently produced concise summaries across different input lengths.\nSemantic similarity scores indicated GPT-4o and LLaMA 3 as leading models in\ncapturing clinical relevance. This study contributes insights into the efficacy\nof LLMs for generating discharge summaries, highlighting LLaMA 3's robust\nperformance in maintaining clarity and relevance across varying clinical\ncontexts. These findings underscore the potential of automated summarization\ntools to enhance documentation precision and efficiency, ultimately improving\npatient care and operational capability in healthcare settings.", "published": "2024-11-06 10:02:50", "link": "http://arxiv.org/abs/2411.03805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Effects of Human-written Paraphrases in LLM-generated\n  Text Detection", "abstract": "Natural Language Generation has been rapidly developing with the advent of\nlarge language models (LLMs). While their usage has sparked significant\nattention from the general public, it is important for readers to be aware when\na piece of text is LLM-generated. This has brought about the need for building\nmodels that enable automated LLM-generated text detection, with the aim of\nmitigating potential negative outcomes of such content. Existing LLM-generated\ndetectors show competitive performances in telling apart LLM-generated and\nhuman-written text, but this performance is likely to deteriorate when\nparaphrased texts are considered. In this study, we devise a new data\ncollection strategy to collect Human & LLM Paraphrase Collection (HLPC), a\nfirst-of-its-kind dataset that incorporates human-written texts and\nparaphrases, as well as LLM-generated texts and paraphrases. With the aim of\nunderstanding the effects of human-written paraphrases on the performance of\nstate-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark\ndetectors, we perform classification experiments that incorporate human-written\nparaphrases, watermarked and non-watermarked LLM-generated documents from GPT\nand OPT, and LLM-generated paraphrases from DIPPER and BART. The results show\nthat the inclusion of human-written paraphrases has a significant impact of\nLLM-generated detector performance, promoting TPR@1%FPR with a possible\ntrade-off of AUROC and accuracy.", "published": "2024-11-06 10:06:21", "link": "http://arxiv.org/abs/2411.03806v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The natural stability of autonomous morphology", "abstract": "Autonomous morphology, such as inflection class systems and paradigmatic\ndistribution patterns, is widespread and diachronically resilient in natural\nlanguage. Why this should be so has remained unclear given that autonomous\nmorphology imposes learning costs, offers no clear benefit relative to its\nabsence and could easily be removed by the analogical forces which are\nconstantly reshaping it. Here we propose an explanation for the resilience of\nautonomous morphology, in terms of a diachronic dynamic of attraction and\nrepulsion between morphomic categories, which emerges spontaneously from a\nsimple paradigm cell filling process. Employing computational evolutionary\nmodels, our key innovation is to bring to light the role of `dissociative\nevidence', i.e., evidence for inflectional distinctiveness which a rational\nreasoner will have access to during analogical inference. Dissociative evidence\ncreates a repulsion dynamic which prevents morphomic classes from collapsing\ntogether entirely, i.e., undergoing complete levelling. As we probe alternative\nmodels, we reveal the limits of conditional entropy as a measure for\npredictability in systems that are undergoing change. Finally, we demonstrate\nthat autonomous morphology, far from being `unnatural' (e.g.\n\\citealt{Aronoff1994}), is rather the natural (emergent) consequence of a\nnatural (rational) process of inference applied to inflectional systems.", "published": "2024-11-06 10:14:58", "link": "http://arxiv.org/abs/2411.03811v1", "categories": ["cs.CL", "I.6.m; J.5"], "primary_category": "cs.CL"}
{"title": "Multi3Hate: Multimodal, Multilingual, and Multicultural Hate Speech\n  Detection with Vision-Language Models", "abstract": "Warning: this paper contains content that may be offensive or upsetting\n  Hate speech moderation on global platforms poses unique challenges due to the\nmultimodal and multilingual nature of content, along with the varying cultural\nperceptions. How well do current vision-language models (VLMs) navigate these\nnuances? To investigate this, we create the first multimodal and multilingual\nparallel hate speech dataset, annotated by a multicultural set of annotators,\ncalled Multi3Hate. It contains 300 parallel meme samples across 5 languages:\nEnglish, German, Spanish, Hindi, and Mandarin. We demonstrate that cultural\nbackground significantly affects multimodal hate speech annotation in our\ndataset. The average pairwise agreement among countries is just 74%,\nsignificantly lower than that of randomly selected annotator groups. Our\nqualitative analysis indicates that the lowest pairwise label agreement-only\n67% between the USA and India-can be attributed to cultural factors. We then\nconduct experiments with 5 large VLMs in a zero-shot setting, finding that\nthese models align more closely with annotations from the US than with those\nfrom other cultures, even when the memes and prompts are presented in the\ndominant language of the other culture. Code and dataset are available at\nhttps://github.com/MinhDucBui/Multi3Hate.", "published": "2024-11-06 13:06:43", "link": "http://arxiv.org/abs/2411.03888v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Analysis of Gender Depiction in the Comedias of Calder\u00f3n\n  de la Barca", "abstract": "In theatre, playwrights use the portrayal of characters to explore culturally\nbased gender norms. In this paper, we develop quantitative methods to study\ngender depiction in the non-religious works (comedias) of Pedro Calder\\'on de\nla Barca, a prolific Spanish 17th century author. We gather insights from a\ncorpus of more than 100 plays by using a gender classifier and applying model\nexplainability (attribution) methods to determine which text features are most\ninfluential in the model's decision to classify speech as 'male' or 'female',\nindicating the most gendered elements of dialogue in Calder\\'on's comedias in a\nhuman accessible manner. We find that female and male characters are portrayed\ndifferently and can be identified by the gender prediction model at practically\nuseful accuracies (up to f=0.83). Analysis reveals semantic aspects of gender\nportrayal, and demonstrates that the model is even useful in providing a\nrelatively accurate scene-by-scene prediction of cross-dressing characters.", "published": "2024-11-06 13:13:33", "link": "http://arxiv.org/abs/2411.03895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAGulator: Lightweight Out-of-Context Detectors for Grounded Text\n  Generation", "abstract": "Real-time detection of out-of-context LLM outputs is crucial for enterprises\nlooking to safely adopt RAG applications. In this work, we train lightweight\nmodels to discriminate LLM-generated text that is semantically out-of-context\nfrom retrieved text documents. We preprocess a combination of summarisation and\nsemantic textual similarity datasets to construct training data using minimal\nresources. We find that DeBERTa is not only the best-performing model under\nthis pipeline, but it is also fast and does not require additional text\npreprocessing or feature engineering. While emerging work demonstrates that\ngenerative LLMs can also be fine-tuned and used in complex data pipelines to\nachieve state-of-the-art performance, we note that speed and resource limits\nare important considerations for on-premise deployment.", "published": "2024-11-06 13:51:42", "link": "http://arxiv.org/abs/2411.03920v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation data contamination in LLMs: how do we measure it and (when)\n  does it matter?", "abstract": "Hampering the interpretation of benchmark scores, evaluation data\ncontamination has become a growing concern in the evaluation of LLMs, and an\nactive area of research studies its effects. While evaluation data\ncontamination is easily understood intuitively, it is surprisingly difficult to\ndefine precisely which samples should be considered contaminated and,\nconsequently, how it impacts benchmark scores. We propose that these questions\nshould be addressed together and that contamination metrics can be assessed\nbased on whether models benefit from the examples they mark contaminated. We\npropose a novel analysis method called ConTAM, and show with a large scale\nsurvey of existing and novel n-gram based contamination metrics across 13\nbenchmarks and 7 models from 2 different families that ConTAM can be used to\nbetter understand evaluation data contamination and its effects. We find that\ncontamination may have a much larger effect than reported in recent LLM\nreleases and benefits models differently at different scales. We also find that\nconsidering only the longest contaminated substring provides a better signal\nthan considering a union of all contaminated substrings, and that doing model\nand benchmark specific threshold analysis greatly increases the specificity of\nthe results. Lastly, we investigate the impact of hyperparameter choices,\nfinding that, among other things, both using larger values of n and\ndisregarding matches that are infrequent in the pre-training data lead to many\nfalse negatives. With ConTAM, we provide a method to empirically ground\nevaluation data contamination metrics in downstream effects. With our\nexploration, we shed light on how evaluation data contamination can impact LLMs\nand provide insight into the considerations important when doing contamination\nanalysis. We end our paper by discussing these in more detail and providing\nconcrete suggestions for future work.", "published": "2024-11-06 13:54:08", "link": "http://arxiv.org/abs/2411.03923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?", "abstract": "The classic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the\nunwanted false mappings caused by Phase 2 text preprocessing, we propose a\nnovel context-based pipeline repair approach that employs a post hoc check to\nfind common words that cause false mappings. These words are stored in a\nreserved word set and applied in text preprocessing. The experimental results\nshow that our approach improves the matching correctness and the overall\nmatching performance. We then consider the broader integration of the classic\ntext preprocessing pipeline with modern large language models (LLMs) for OM. We\nrecommend that (1) the text preprocessing pipeline be injected via function\ncalling into LLMs to avoid the tendency towards unstable true mappings produced\nby LLM prompting; or (2) LLMs be used to repair non-existent and\ncounter-intuitive false mappings generated by the text preprocessing pipeline.", "published": "2024-11-06 14:51:02", "link": "http://arxiv.org/abs/2411.03962v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WorryWords: Norms of Anxiety Association for over 44k English Words", "abstract": "Anxiety, the anticipatory unease about a potential negative outcome, is a\ncommon and beneficial human emotion. However, there is still much that is not\nknown, such as how anxiety relates to our body and how it manifests in\nlanguage. This is especially pertinent given the increasing impact of\nanxiety-related disorders. In this work, we introduce WorryWords, the first\nlarge-scale repository of manually derived word--anxiety associations for over\n44,450 English words. We show that the anxiety associations are highly\nreliable. We use WorryWords to study the relationship between anxiety and other\nemotion constructs, as well as the rate at which children acquire anxiety words\nwith age. Finally, we show that using WorryWords alone, one can accurately\ntrack the change of anxiety in streams of text. The lexicon enables a wide\nvariety of anxiety-related research in psychology, NLP, public health, and\nsocial sciences. WorryWords (and its translations to over 100 languages) is\nfreely available. http://saifmohammad.com/worrywords.html", "published": "2024-11-06 15:03:47", "link": "http://arxiv.org/abs/2411.03966v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Engineering Using GPT for Word-Level Code-Mixed Language\n  Identification in Low-Resource Dravidian Languages", "abstract": "Language Identification (LI) is crucial for various natural language\nprocessing tasks, serving as a foundational step in applications such as\nsentiment analysis, machine translation, and information retrieval. In\nmultilingual societies like India, particularly among the youth engaging on\nsocial media, text often exhibits code-mixing, blending local languages with\nEnglish at different linguistic levels. This phenomenon presents formidable\nchallenges for LI systems, especially when languages intermingle within single\nwords. Dravidian languages, prevalent in southern India, possess rich\nmorphological structures yet suffer from under-representation in digital\nplatforms, leading to the adoption of Roman or hybrid scripts for\ncommunication. This paper introduces a prompt based method for a shared task\naimed at addressing word-level LI challenges in Dravidian languages. In this\nwork, we leveraged GPT-3.5 Turbo to understand whether the large language\nmodels is able to correctly classify words into correct categories. Our\nfindings show that the Kannada model consistently outperformed the Tamil model\nacross most metrics, indicating a higher accuracy and reliability in\nidentifying and categorizing Kannada language instances. In contrast, the Tamil\nmodel showed moderate performance, particularly needing improvement in\nprecision and recall.", "published": "2024-11-06 16:20:37", "link": "http://arxiv.org/abs/2411.04025v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beemo: Benchmark of Expert-edited Machine-generated Outputs", "abstract": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available.", "published": "2024-11-06 16:31:28", "link": "http://arxiv.org/abs/2411.04032v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarization of Opinionated Political Documents with Varied\n  Perspectives", "abstract": "Global partisan hostility and polarization has increased, and this\npolarization is heightened around presidential elections. Models capable of\ngenerating accurate summaries of diverse perspectives can help reduce such\npolarization by exposing users to alternative perspectives. In this work, we\nintroduce a novel dataset and task for independently summarizing each political\nperspective in a set of passages from opinionated news articles. For this task,\nwe propose a framework for evaluating different dimensions of perspective\nsummary performance. We benchmark 10 models of varying sizes and architectures\nthrough both automatic and human evaluation. While recent models like GPT-4o\nperform well on this task, we find that all models struggle to generate\nsummaries faithful to the intended perspective. Our analysis of summaries\nfocuses on how extraction behavior depends on the features of the input\ndocuments.", "published": "2024-11-06 18:14:48", "link": "http://arxiv.org/abs/2411.04093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diversity Helps Jailbreak Large Language Models", "abstract": "We have uncovered a powerful jailbreak technique that leverages large\nlanguage models' ability to diverge from prior context, enabling them to bypass\nsafety constraints and generate harmful outputs. By simply instructing the LLM\nto deviate and obfuscate previous attacks, our method dramatically outperforms\nexisting approaches, achieving up to a 62% higher success rate in compromising\nnine leading chatbots, including GPT-4, Gemini, and Llama, while using only 13%\nof the queries. This revelation exposes a critical flaw in current LLM safety\ntraining, suggesting that existing methods may merely mask vulnerabilities\nrather than eliminate them. Our findings sound an urgent alarm for the need to\nrevolutionize testing methodologies to ensure robust and reliable LLM security.", "published": "2024-11-06 19:39:48", "link": "http://arxiv.org/abs/2411.04223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On-Device Emoji Classifier Trained with GPT-based Data Augmentation for\n  a Mobile Keyboard", "abstract": "Emojis improve communication quality among smart-phone users that use mobile\nkeyboards to exchange text. To predict emojis for users based on input text, we\nshould consider the on-device low memory and time constraints, ensure that the\non-device emoji classifier covers a wide range of emoji classes even though the\nemoji dataset is typically imbalanced, and adapt the emoji classifier output to\nuser favorites. This paper proposes an on-device emoji classifier based on\nMobileBert with reasonable memory and latency requirements for SwiftKey. To\naccount for the data imbalance, we utilize the widely used GPT to generate one\nor more tags for each emoji class. For each emoji and corresponding tags, we\nmerge the original set with GPT-generated sentences and label them with this\nemoji without human intervention to alleviate the data imbalance. At inference\ntime, we interpolate the emoji output with the user history for emojis for\nbetter emoji classifications. Results show that the proposed on-device emoji\nclassifier deployed for SwiftKey increases the accuracy performance of emoji\nprediction particularly on rare emojis and emoji engagement.", "published": "2024-11-06 09:52:29", "link": "http://arxiv.org/abs/2411.05031v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Word Vectors to Multimodal Embeddings: Techniques, Applications,\n  and Future Directions For Large Language Models", "abstract": "Word embeddings and language models have transformed natural language\nprocessing (NLP) by facilitating the representation of linguistic elements in\ncontinuous vector spaces. This review visits foundational concepts such as the\ndistributional hypothesis and contextual similarity, tracing the evolution from\nsparse representations like one-hot encoding to dense embeddings including\nWord2Vec, GloVe, and fastText. We examine both static and contextualized\nembeddings, underscoring advancements in models such as ELMo, BERT, and GPT and\ntheir adaptations for cross-lingual and personalized applications. The\ndiscussion extends to sentence and document embeddings, covering aggregation\nmethods and generative topic models, along with the application of embeddings\nin multimodal domains, including vision, robotics, and cognitive science.\nAdvanced topics such as model compression, interpretability, numerical\nencoding, and bias mitigation are analyzed, addressing both technical\nchallenges and ethical implications. Additionally, we identify future research\ndirections, emphasizing the need for scalable training techniques, enhanced\ninterpretability, and robust grounding in non-textual modalities. By\nsynthesizing current methodologies and emerging trends, this survey offers\nresearchers and practitioners an in-depth resource to push the boundaries of\nembedding-based language models.", "published": "2024-11-06 15:40:02", "link": "http://arxiv.org/abs/2411.05036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The American Sign Language Knowledge Graph: Infusing ASL Models with\n  Linguistic Knowledge", "abstract": "Language models for American Sign Language (ASL) could make language\ntechnologies substantially more accessible to those who sign. To train models\non tasks such as isolated sign recognition (ISR) and ASL-to-English\ntranslation, datasets provide annotated video examples of ASL signs. To\nfacilitate the generalizability and explainability of these models, we\nintroduce the American Sign Language Knowledge Graph (ASLKG), compiled from\ntwelve sources of expert linguistic knowledge. We use the ASLKG to train\nneuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of\n91% on ISR, 14% for predicting the semantic features of unseen signs, and 36%\nfor classifying the topic of Youtube-ASL videos.", "published": "2024-11-06 00:16:16", "link": "http://arxiv.org/abs/2411.03568v1", "categories": ["cs.CL", "cs.CV", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Deploying Multi-task Online Server with Large Language Model", "abstract": "In the industry, numerous tasks are deployed online. Traditional approaches\noften tackle each task separately by its own network, which leads to excessive\ncosts for developing and scaling models, especially in the context of large\nlanguage models. Although multi-task methods can save costs through parameter\nsharing, they often struggle to outperform single-task methods in real-world\napplications. To tackle these challenges, we present a three-stage multi-task\nlearning framework for large language models. It involves task filtering,\nfollowed by fine-tuning on high-resource tasks, and finally fine-tuning on all\ntasks. We conducted comprehensive experiments in single-task and multi-task\nsettings. Our approach, exemplified on different benchmarks, demonstrates that\nit is able to achieve performance comparable to the single-task method while\nreducing up to 90.9\\% of its overhead.", "published": "2024-11-06 03:48:41", "link": "http://arxiv.org/abs/2411.03644v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework", "abstract": "Proper moral beliefs are fundamental for language models, yet assessing these\nbeliefs poses a significant challenge. This study introduces a novel\nthree-module framework to evaluate the moral beliefs of four prominent large\nlanguage models. Initially, we constructed a dataset containing 472 moral\nchoice scenarios in Chinese, derived from moral words. The decision-making\nprocess of the models in these scenarios reveals their moral principle\npreferences. By ranking these moral choices, we discern the varying moral\nbeliefs held by different language models. Additionally, through moral debates,\nwe investigate the firmness of these models to their moral choices. Our\nfindings indicate that English language models, namely ChatGPT and Gemini,\nclosely mirror moral decisions of the sample of Chinese university students,\ndemonstrating strong adherence to their choices and a preference for\nindividualistic moral beliefs. In contrast, Chinese models such as Ernie and\nChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their\nmoral choices and debates. This study also uncovers gender bias embedded within\nthe moral beliefs of all examined language models. Our methodology offers an\ninnovative means to assess moral beliefs in both artificial and human\nintelligence, facilitating a comparison of moral values across different\ncultures.", "published": "2024-11-06 04:52:38", "link": "http://arxiv.org/abs/2411.03665v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QUILL: Quotation Generation Enhancement of Large Language Models", "abstract": "While Large language models (LLMs) have become excellent writing assistants,\nthey still struggle with quotation generation. This is because they either\nhallucinate when providing factual quotations or fail to provide quotes that\nexceed human expectations. To bridge the gap, we systematically study how to\nevaluate and improve LLMs' performance in quotation generation tasks. We first\nestablish a holistic and automatic evaluation system for quotation generation\ntask, which consists of five criteria each with corresponding automatic metric.\nTo improve the LLMs' quotation generation abilities, we construct a bilingual\nknowledge base that is broad in scope and rich in dimensions, containing up to\n32,022 quotes. Moreover, guided by our critiria, we further design a\nquotation-specific metric to rerank the retrieved quotations from the knowledge\nbase. Extensive experiments show that our metrics strongly correlate with human\npreferences. Existing LLMs struggle to generate desired quotes, but our\nquotation knowledge base and reranking metric help narrow this gap. Our dataset\nand code are publicly available at https://github.com/GraceXiaoo/QUILL.", "published": "2024-11-06 05:24:09", "link": "http://arxiv.org/abs/2411.03675v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It", "abstract": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.", "published": "2024-11-06 08:59:44", "link": "http://arxiv.org/abs/2411.03766v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Really is Commonsense Knowledge?", "abstract": "Commonsense datasets have been well developed in Natural Language Processing,\nmainly through crowdsource human annotation. However, there are debates on the\ngenuineness of commonsense reasoning benchmarks. In specific, a significant\nportion of instances in some commonsense benchmarks do not concern commonsense\nknowledge. That problem would undermine the measurement of the true commonsense\nreasoning ability of evaluated models. It is also suggested that the problem\noriginated from a blurry concept of commonsense knowledge, as distinguished\nfrom other types of knowledge. To demystify all of the above claims, in this\nstudy, we survey existing definitions of commonsense knowledge, ground into the\nthree frameworks for defining concepts, and consolidate them into a\nmulti-framework unified definition of commonsense knowledge (so-called\nconsolidated definition). We then use the consolidated definition for\nannotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets\nto examine the above claims. Our study shows that there exists a large portion\nof non-commonsense-knowledge instances in the two datasets, and a large\nperformance gap on these two subsets where Large Language Models (LLMs) perform\nworse on commonsense-knowledge instances.", "published": "2024-11-06 14:54:19", "link": "http://arxiv.org/abs/2411.03964v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for\n  Evaluating Foundation Models", "abstract": "Existing benchmarks for evaluating foundation models mainly focus on\nsingle-document, text-only tasks. However, they often fail to fully capture the\ncomplexity of research workflows, which typically involve interpreting\nnon-textual data and gathering information across multiple documents. To\naddress this gap, we introduce M3SciQA, a multi-modal, multi-document\nscientific question answering benchmark designed for a more comprehensive\nevaluation of foundation models. M3SciQA consists of 1,452 expert-annotated\nquestions spanning 70 natural language processing paper clusters, where each\ncluster represents a primary paper along with all its cited documents,\nmirroring the workflow of comprehending a single paper by requiring multi-modal\nand multi-document data. With M3SciQA, we conduct a comprehensive evaluation of\n18 foundation models. Our results indicate that current foundation models still\nsignificantly underperform compared to human experts in multi-modal information\nretrieval and in reasoning across multiple scientific documents. Additionally,\nwe explore the implications of these findings for the future advancement of\napplying foundation models in multi-modal scientific literature analysis.", "published": "2024-11-06 17:52:01", "link": "http://arxiv.org/abs/2411.04075v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unfair Alignment: Examining Safety Alignment Across Vision Encoder\n  Layers in Vision-Language Models", "abstract": "Vision-language models (VLMs) have improved significantly in multi-modal\ntasks, but their more complex architecture makes their safety alignment more\nchallenging than the alignment of large language models (LLMs). In this paper,\nwe reveal an unfair distribution of safety across the layers of VLM's vision\nencoder, with earlier and middle layers being disproportionately vulnerable to\nmalicious inputs compared to the more robust final layers. This 'cross-layer'\nvulnerability stems from the model's inability to generalize its safety\ntraining from the default architectural settings used during training to unseen\nor out-of-distribution scenarios, leaving certain layers exposed. We conduct a\ncomprehensive analysis by projecting activations from various intermediate\nlayers and demonstrate that these layers are more likely to generate harmful\noutputs when exposed to malicious inputs. Our experiments with LLaVA-1.5 and\nLlama 3.2 show discrepancies in attack success rates and toxicity scores across\nlayers, indicating that current safety alignment strategies focused on a single\ndefault layer are insufficient.", "published": "2024-11-06 22:19:32", "link": "http://arxiv.org/abs/2411.04291v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Capabilities Approach to Studying Bias and Harm in Language\n  Technologies", "abstract": "Mainstream Natural Language Processing (NLP) research has ignored the\nmajority of the world's languages. In moving from excluding the majority of the\nworld's languages to blindly adopting what we make for English, we first risk\nimporting the same harms we have at best mitigated and at least measured for\nEnglish. However, in evaluating and mitigating harms arising from adopting new\ntechnologies into such contexts, we often disregard (1) the actual community\nneeds of Language Technologies, and (2) biases and fairness issues within the\ncontext of the communities. In this extended abstract, we consider fairness,\nbias, and inclusion in Language Technologies through the lens of the\nCapabilities Approach. The Capabilities Approach centers on what people are\ncapable of achieving, given their intersectional social, political, and\neconomic contexts instead of what resources are (theoretically) available to\nthem. We detail the Capabilities Approach, its relationship to multilingual and\nmulticultural evaluation, and how the framework affords meaningful\ncollaboration with community members in defining and measuring the harms of\nLanguage Technologies.", "published": "2024-11-06 22:46:13", "link": "http://arxiv.org/abs/2411.04298v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Improving Bilingual Capabilities of Language Models to Support Diverse\n  Linguistic Practices in Education", "abstract": "Large language models (LLMs) offer promise in generating educational content,\nproviding instructor feedback, and reducing teacher workload on assessments.\nWhile prior studies have focused on studying LLM-powered learning analytics,\nlimited research has examined how effective LLMs are in a bilingual context. In\nthis paper, we study the effectiveness of multilingual large language models\n(MLLMs) across monolingual (English-only, Spanish-only) and bilingual\n(Spanglish) student writing. We present a learning analytics use case that\ndetails LLM performance in assessing acceptable and unacceptable explanations\nof Science and Social Science concepts. Our findings reveal a significant bias\nin the grading performance of pre-trained models for bilingual writing compared\nto English-only and Spanish-only writing. Following this, we fine-tune\nopen-source MLLMs including Llama 3.1 and Mistral NeMo using synthetic datasets\ngenerated in English, Spanish, and Spanglish. Our experiments indicate that the\nmodels perform significantly better for all three languages after fine-tuning\nwith bilingual data. This study highlights the potential of enhancing MLLM\neffectiveness to support authentic language practices amongst bilingual\nlearners. It also aims to illustrate the value of incorporating non-English\nlanguages into the design and implementation of language models in education.", "published": "2024-11-06 23:16:25", "link": "http://arxiv.org/abs/2411.04308v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "YouTube Comments Decoded: Leveraging LLMs for Low Resource Language\n  Classification", "abstract": "Sarcasm detection is a significant challenge in sentiment analysis,\nparticularly due to its nature of conveying opinions where the intended meaning\ndeviates from the literal expression. This challenge is heightened in social\nmedia contexts where code-mixing, especially in Dravidian languages, is\nprevalent. Code-mixing involves the blending of multiple languages within a\nsingle utterance, often with non-native scripts, complicating the task for\nsystems trained on monolingual data. This shared task introduces a novel gold\nstandard corpus designed for sarcasm and sentiment detection within code-mixed\ntexts, specifically in Tamil-English and Malayalam-English languages. The\nprimary objective of this task is to identify sarcasm and sentiment polarity\nwithin a code-mixed dataset of Tamil-English and Malayalam-English comments and\nposts collected from social media platforms. Each comment or post is annotated\nat the message level for sentiment polarity, with particular attention to the\nchallenges posed by class imbalance, reflecting real-world scenarios.In this\nwork, we experiment with state-of-the-art large language models like GPT-3.5\nTurbo via prompting to classify comments into sarcastic or non-sarcastic\ncategories. We obtained a macro-F1 score of 0.61 for Tamil language. We\nobtained a macro-F1 score of 0.50 for Malayalam language.", "published": "2024-11-06 17:58:01", "link": "http://arxiv.org/abs/2411.05039v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bottom-Up and Top-Down Analysis of Values, Agendas, and Observations in\n  Corpora and LLMs", "abstract": "Large language models (LLMs) generate diverse, situated, persuasive texts\nfrom a plurality of potential perspectives, influenced heavily by their prompts\nand training data. As part of LLM adoption, we seek to characterize - and\nideally, manage - the socio-cultural values that they express, for reasons of\nsafety, accuracy, inclusion, and cultural fidelity. We present a validated\napproach to automatically (1) extracting heterogeneous latent value\npropositions from texts, (2) assessing resonance and conflict of values with\ntexts, and (3) combining these operations to characterize the pluralistic value\nalignment of human-sourced and LLM-sourced textual data.", "published": "2024-11-06 18:51:04", "link": "http://arxiv.org/abs/2411.05040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Radiology Report Conciseness and Structure via Local Large\n  Language Models", "abstract": "In this study, we aim to enhance radiology reporting by improving both the\nconciseness and structured organization of findings (also referred to as\ntemplating), specifically by organizing information according to anatomical\nregions. This structured approach allows physicians to locate relevant\ninformation quickly, increasing the report's utility. We utilize Large Language\nModels (LLMs) such as Mixtral, Mistral, and Llama to generate concise,\nwell-structured reports. Among these, we primarily focus on the Mixtral model\ndue to its superior adherence to specific formatting requirements compared to\nother models. To maintain data security and privacy, we run these LLMs locally\nbehind our institution's firewall. We leverage the LangChain framework and\napply five distinct prompting strategies to enforce a consistent structure in\nradiology reports, aiming to eliminate extraneous language and achieve a high\nlevel of conciseness. We also introduce a novel metric, the Conciseness\nPercentage (CP) score, to evaluate report brevity. Our dataset comprises 814\nradiology reports authored by seven board-certified body radiologists at our\ncancer center. In evaluating the different prompting methods, we discovered\nthat the most effective approach for generating concise, well-structured\nreports involves first instructing the LLM to condense the report, followed by\na prompt to structure the content according to specific guidelines. We assessed\nall prompting strategies based on their ability to handle formatting issues,\nreduce report length, and adhere to formatting instructions. Our findings\ndemonstrate that open-source, locally deployed LLMs can significantly improve\nradiology report conciseness and structure while conforming to specified\nformatting standards.", "published": "2024-11-06 19:00:57", "link": "http://arxiv.org/abs/2411.05042v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Library Perspective on Supervised Text Processing in Digital\n  Libraries: An Investigation in the Biomedical Domain", "abstract": "Digital libraries that maintain extensive textual collections may want to\nfurther enrich their content for certain downstream applications, e.g.,\nbuilding knowledge graphs, semantic enrichment of documents, or implementing\nnovel access paths. All of these applications require some text processing,\neither to identify relevant entities, extract semantic relationships between\nthem, or to classify documents into some categories. However, implementing\nreliable, supervised workflows can become quite challenging for a digital\nlibrary because suitable training data must be crafted, and reliable models\nmust be trained. While many works focus on achieving the highest accuracy on\nsome benchmarks, we tackle the problem from a digital library practitioner. In\nother words, we also consider trade-offs between accuracy and application\ncosts, dive into training data generation through distant supervision and large\nlanguage models such as ChatGPT, LLama, and Olmo, and discuss how to design\nfinal pipelines. Therefore, we focus on relation extraction and text\nclassification, using the showcase of eight biomedical benchmarks.", "published": "2024-11-06 07:54:10", "link": "http://arxiv.org/abs/2411.12752v1", "categories": ["cs.DL", "cs.CL", "H.4"], "primary_category": "cs.DL"}
{"title": "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with\n  Captions in 28 Languages", "abstract": "Research in vision and language has made considerable progress thanks to\nbenchmarks such as COCO. COCO captions focused on unambiguous facts in English;\nArtEmis introduced subjective emotions and ArtELingo introduced some\nmultilinguality (Chinese and Arabic). However we believe there should be more\nmultilinguality. Hence, we present ArtELingo-28, a vision-language benchmark\nthat spans $\\textbf{28}$ languages and encompasses approximately\n$\\textbf{200,000}$ annotations ($\\textbf{140}$ annotations per image).\nTraditionally, vision research focused on unambiguous class labels, whereas\nArtELingo-28 emphasizes diversity of opinions over languages and cultures. The\nchallenge is to build machine learning systems that assign emotional captions\nto images. Baseline results will be presented for three novel conditions:\nZero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual\ntransfer is more successful for culturally-related languages. Data and code are\nprovided at www.artelingo.org.", "published": "2024-11-06 09:05:17", "link": "http://arxiv.org/abs/2411.03769v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue", "abstract": "Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon.", "published": "2024-11-06 10:32:09", "link": "http://arxiv.org/abs/2411.03814v2", "categories": ["cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.AI"}
{"title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning", "abstract": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.", "published": "2024-11-06 10:35:11", "link": "http://arxiv.org/abs/2411.03817v3", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination", "abstract": "The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting models' contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is quite effective and sensitive in identifying varying degrees\nof contamination, and can highlight significant performance improvements due to\nthe leakage of multimodal benchmark training sets. Furthermore, we explore\nwhether the contamination originates from the base LLMs used by MLLMs or the\nmultimodal training phase, providing new insights into the stages at which\ncontamination may be introduced.", "published": "2024-11-06 10:44:15", "link": "http://arxiv.org/abs/2411.03823v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba", "abstract": "An ecosystem of Transformer-based models has been established by building\nlarge models with extensive data. Parameter-efficient fine-tuning (PEFT) is a\ncrucial technology for deploying these models to downstream tasks with minimal\ncost while achieving effective performance. Recently, Mamba, a State Space\nModel (SSM)-based model, has attracted attention as a potential alternative to\nTransformers. While many large-scale Mamba-based models have been proposed,\nefficiently adapting pre-trained Mamba-based models to downstream tasks remains\nunexplored. In this paper, we conduct an exploratory analysis of PEFT methods\nfor Mamba. We investigate the effectiveness of existing PEFT methods for\nTransformers when applied to Mamba. We also modify these methods to better\nalign with the Mamba architecture. Additionally, we propose new Mamba-specific\nPEFT methods that leverage the distinctive structure of Mamba. Our experiments\nindicate that PEFT performs more effectively for Mamba than Transformers.\nLastly, we demonstrate how to effectively combine multiple PEFT methods and\nprovide a framework that outperforms previous works. To ensure reproducibility,\nwe will release the code after publication.", "published": "2024-11-06 11:57:55", "link": "http://arxiv.org/abs/2411.03855v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward", "abstract": "Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\nspeech perturbations. In this paper, we address these questions by conducting\nvarious ablation experiments using a recent and widely adopted approach called\nSLAM-ASR. We present novel empirical findings that offer insights on how to\neffectively utilize the SLAM-ASR architecture across a wide range of settings.\nOur main findings indicate that SLAM-ASR exhibits poor performance in\ncross-domain evaluation settings. Additionally, speech perturbations on\nin-domain data, such as changes in speech rate or additive noise, can\nsignificantly degrade performance. Our findings offer critical insights for\nfine-tuning and configuring robust LLM-based ASR models, tailored to different\ndata characteristics and computational resources.", "published": "2024-11-06 12:22:04", "link": "http://arxiv.org/abs/2411.03866v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering", "abstract": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder.", "published": "2024-11-06 12:57:58", "link": "http://arxiv.org/abs/2411.03883v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models", "abstract": "Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom.", "published": "2024-11-06 13:00:34", "link": "http://arxiv.org/abs/2411.03884v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System", "abstract": "In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality.", "published": "2024-11-06 13:37:28", "link": "http://arxiv.org/abs/2411.03906v2", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Interactions Across Blocks in Post-Training Quantization of Large\n  Language Models", "abstract": "Post-training quantization is widely employed to reduce the computational\ndemands of neural networks. Typically, individual substructures, such as layers\nor blocks of layers, are quantized with the objective of minimizing\nquantization errors in their pre-activations by fine-tuning the corresponding\nweights. Deriving this local objective from the global objective of minimizing\ntask loss involves two key simplifications: assuming substructures are mutually\nindependent and ignoring the knowledge of subsequent substructures as well as\nthe task loss. In this work, we assess the effects of these simplifications on\nweight-only quantization of large language models. We introduce two multi-block\nfine-tuning strategies and compare them against the baseline of fine-tuning\nsingle transformer blocks. The first captures correlations of weights across\nblocks by jointly optimizing multiple quantized blocks. The second incorporates\nknowledge of subsequent blocks by minimizing the error in downstream\npre-activations rather than focusing solely on the quantized block. Our\nfindings indicate that the effectiveness of these methods depends on the\nspecific network model, with no impact on some models but demonstrating\nsignificant benefits for others.", "published": "2024-11-06 14:11:39", "link": "http://arxiv.org/abs/2411.03934v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Collaborative Content Moderation Framework for Toxicity Detection\n  based on Conformalized Estimates of Annotation Disagreement", "abstract": "Content moderation typically combines the efforts of human moderators and\nmachine learning models. However, these systems often rely on data where\nsignificant disagreement occurs during moderation, reflecting the subjective\nnature of toxicity perception. Rather than dismissing this disagreement as\nnoise, we interpret it as a valuable signal that highlights the inherent\nambiguity of the content,an insight missed when only the majority label is\nconsidered. In this work, we introduce a novel content moderation framework\nthat emphasizes the importance of capturing annotation disagreement. Our\napproach uses multitask learning, where toxicity classification serves as the\nprimary task and annotation disagreement is addressed as an auxiliary task.\nAdditionally, we leverage uncertainty estimation techniques, specifically\nConformal Prediction, to account for both the ambiguity in comment annotations\nand the model's inherent uncertainty in predicting toxicity and\ndisagreement.The framework also allows moderators to adjust thresholds for\nannotation disagreement, offering flexibility in determining when ambiguity\nshould trigger a review. We demonstrate that our joint approach enhances model\nperformance, calibration, and uncertainty estimation, while offering greater\nparameter efficiency and improving the review process in comparison to\nsingle-task methods.", "published": "2024-11-06 18:08:57", "link": "http://arxiv.org/abs/2411.04090v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "68T50 (Primary) 68T37 (Secondary)", "I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis", "abstract": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve. We perform our study on two\nfronts. First, we pursue an understanding of precisely how a three-layer\ntransformer, trained from scratch and attains perfect test accuracy, solves\nthis problem. We are able to identify certain \"planning\" and \"reasoning\"\nmechanisms in the network that necessitate cooperation between the attention\nblocks to implement the desired logic. Second, we study how pretrained LLMs,\nnamely Mistral-7B and Gemma-2-9B, solve this problem. We characterize their\nreasoning circuits through causal intervention experiments, providing necessity\nand sufficiency evidence for the circuits. We find evidence suggesting that the\ntwo models' latent reasoning strategies are surprisingly similar, and\nhuman-like. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.", "published": "2024-11-06 18:35:32", "link": "http://arxiv.org/abs/2411.04105v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Self-Consistency Preference Optimization", "abstract": "Self-alignment, whereby models learn to improve themselves without human\nannotation, is a rapidly growing research area. However, existing techniques\noften fail to improve complex reasoning tasks due to the difficulty of\nassigning correct rewards. An orthogonal approach that is known to improve\ncorrectness is self-consistency, a method applied at inference time based on\nmultiple sampling in order to find the most consistent answer. In this work, we\nextend the self-consistency concept to help train models. We thus introduce\nself-consistency preference optimization (ScPO), which iteratively trains\nconsistent answers to be preferred over inconsistent ones on unsupervised new\nproblems. We show ScPO leads to large improvements over conventional reward\nmodel training on reasoning tasks such as GSM8K and MATH, closing the gap with\nsupervised training with gold answers or preferences, and that combining ScPO\nwith standard supervised learning improves results even further. On ZebraLogic,\nScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and\nClaude-3 Haiku.", "published": "2024-11-06 18:36:22", "link": "http://arxiv.org/abs/2411.04109v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Medical Adaptation of Large Language and Vision-Language Models: Are We\n  Making Progress?", "abstract": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.", "published": "2024-11-06 18:51:02", "link": "http://arxiv.org/abs/2411.04118v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crystal: Illuminating LLM Abilities on Language and Code", "abstract": "Large Language Models (LLMs) specializing in code generation (which are also\noften referred to as code LLMs), e.g., StarCoder and Code Llama, play\nincreasingly critical roles in various software development scenarios. It is\nalso crucial for code LLMs to possess both code generation and natural language\nabilities for many specific applications, such as code snippet retrieval using\nnatural language or code explanations. The intricate interaction between\nacquiring language and coding skills complicates the development of strong code\nLLMs. Furthermore, there is a lack of thorough prior studies on the LLM\npretraining strategy that mixes code and natural language. In this work, we\npropose a pretraining strategy to enhance the integration of natural language\nand coding capabilities within a single LLM. Specifically, it includes two\nphases of training with appropriately adjusted code/language ratios. The\nresulting model, Crystal, demonstrates remarkable capabilities in both domains.\nSpecifically, it has natural language and coding performance comparable to that\nof Llama 2 and Code Llama, respectively. Crystal exhibits better data\nefficiency, using 1.4 trillion tokens compared to the more than 2 trillion\ntokens used by Llama 2 and Code Llama. We verify our pretraining strategy by\nanalyzing the training process and observe consistent improvements in most\nbenchmarks. We also adopted a typical application adaptation phase with a\ncode-centric data mixture, only to find that it did not lead to enhanced\nperformance or training efficiency, underlining the importance of a carefully\ndesigned data recipe. To foster research within the community, we commit to\nopen-sourcing every detail of the pretraining, including our training datasets,\ncode, loggings and 136 checkpoints throughout the training.", "published": "2024-11-06 10:28:46", "link": "http://arxiv.org/abs/2411.04156v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Analyzing Multimodal Features of Spontaneous Voice Assistant Commands\n  for Mild Cognitive Impairment Detection", "abstract": "Mild cognitive impairment (MCI) is a major public health concern due to its\nhigh risk of progressing to dementia. This study investigates the potential of\ndetecting MCI with spontaneous voice assistant (VA) commands from 35 older\nadults in a controlled setting. Specifically, a command-generation task is\ndesigned with pre-defined intents for participants to freely generate commands\nthat are more associated with cognitive ability than read commands. We develop\nMCI classification and regression models with audio, textual, intent, and\nmultimodal fusion features. We find the command-generation task outperforms the\ncommand-reading task with an average classification accuracy of 82%, achieved\nby leveraging multimodal fusion features. In addition, generated commands\ncorrelate more strongly with memory and attention subdomains than read\ncommands. Our results confirm the effectiveness of the command-generation task\nand imply the promise of using longitudinal in-home commands for MCI detection.", "published": "2024-11-06 13:50:50", "link": "http://arxiv.org/abs/2411.04158v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning\n  Capabilities via Self-Rewarding", "abstract": "Large language models (LLMs) have shown impressive capabilities, but still\nstruggle with complex reasoning tasks requiring multiple steps. While\nprompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at\ninference time, optimizing reasoning capabilities during training remains\nchallenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled\nframework that formulates reasoning as sampling from a latent distribution and\noptimizes it via variational approaches. LaTRO enables LLMs to concurrently\nimprove both their reasoning process and ability to evaluate reasoning quality,\nwithout requiring external feedback or reward models. We validate LaTRO through\nexperiments on GSM8K and ARC-Challenge datasets using multiple model\narchitectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of\n12.5% over base models and 9.6% over supervised fine-tuning across\nPhi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that\npre-trained LLMs possess latent reasoning capabilities that can be unlocked and\nenhanced through our proposed optimization approach in a self-improvement\nmanner. The code of LaTRO is available at\n\\url{https://github.com/SalesforceAIResearch/LaTRO}.", "published": "2024-11-06 22:02:30", "link": "http://arxiv.org/abs/2411.04282v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML", "I.2.7"], "primary_category": "cs.AI"}
{"title": "A Multilingual Sentiment Lexicon for Low-Resource Language Translation\n  using Large Languages Models and Explainable AI", "abstract": "South Africa and the Democratic Republic of Congo (DRC) present a complex\nlinguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French,\nEnglish, and Tshiluba (Ciluba), which creates unique challenges for AI-driven\ntranslation and sentiment analysis systems due to a lack of accurately labeled\ndata. This study seeks to address these challenges by developing a multilingual\nlexicon designed for French and Tshiluba, now expanded to include translations\nin English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural\nrelevance in sentiment classification by integrating language-specific\nsentiment scores. A comprehensive testing corpus is created to support\ntranslation and sentiment analysis tasks, with machine learning models such as\nRandom Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive\nBayes (GNB) trained to predict sentiment across low resource languages (LRLs).\nAmong them, the Random Forest model performed particularly well, capturing\nsentiment polarity and handling language-specific nuances effectively.\nFurthermore, Bidirectional Encoder Representations from Transformers (BERT), a\nLarge Language Model (LLM), is applied to predict context-based sentiment with\nhigh accuracy, achieving 99% accuracy and 98% precision, outperforming other\nmodels. The BERT predictions were clarified using Explainable AI (XAI),\nimproving transparency and fostering confidence in sentiment classification.\nOverall, findings demonstrate that the proposed lexicon and machine learning\nmodels significantly enhance translation and sentiment analysis for LRLs in\nSouth Africa and the DRC, laying a foundation for future AI models that support\nunderrepresented languages, with applications across education, governance, and\nbusiness in multilingual contexts.", "published": "2024-11-06 23:41:18", "link": "http://arxiv.org/abs/2411.04316v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion", "abstract": "Embeddings have become a cornerstone in the functionality of large language\nmodels (LLMs) due to their ability to transform text data into rich, dense\nnumerical representations that capture semantic and syntactic properties. These\nembedding vector databases serve as the long-term memory of LLMs, enabling\nefficient handling of a wide range of natural language processing tasks.\nHowever, the surge in popularity of embedding vector databases in LLMs has been\naccompanied by significant concerns about privacy leakage. Embedding vector\ndatabases are particularly vulnerable to embedding inversion attacks, where\nadversaries can exploit the embeddings to reverse-engineer and extract\nsensitive information from the original text data. Existing defense mechanisms\nhave shown limitations, often struggling to balance security with the\nperformance of downstream tasks. To address these challenges, we introduce\nEguard, a novel defense mechanism designed to mitigate embedding inversion\nattacks. Eguard employs a transformer-based projection network and text mutual\ninformation optimization to safeguard embeddings while preserving the utility\nof LLMs. Our approach significantly reduces privacy risks, protecting over 95%\nof tokens from inversion while maintaining high performance across downstream\ntasks consistent with original embeddings.", "published": "2024-11-06 14:42:41", "link": "http://arxiv.org/abs/2411.05034v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Towards Interpreting Language Models: A Case Study in Multi-Hop\n  Reasoning", "abstract": "Answering multi-hop reasoning questions requires retrieving and synthesizing\ninformation from diverse sources. Language models (LMs) struggle to perform\nsuch reasoning consistently. We propose an approach to pinpoint and rectify\nmulti-hop reasoning failures through targeted memory injections on LM attention\nheads. First, we analyze the per-layer activations of GPT-2 models in response\nto single- and multi-hop prompts. We then propose a mechanism that allows users\nto inject relevant prompt-specific information, which we refer to as\n\"memories,\" at critical LM locations during inference. By thus enabling the LM\nto incorporate additional relevant information during inference, we enhance the\nquality of multi-hop prompt completions. We empirically show that a simple,\nefficient, and targeted memory injection into a key attention layer often\nincreases the probability of the desired next token in multi-hop tasks, by up\nto 424%. We observe that small subsets of attention heads can significantly\nimpact the model prediction during multi-hop reasoning. To more faithfully\ninterpret these heads, we develop Attention Lens: an open source tool that\ntranslates the outputs of attention heads into vocabulary tokens via learned\ntransformations called lenses. We demonstrate the use of lenses to reveal how a\nmodel arrives at its answer and use them to localize sources of model failures\nsuch as in the case of biased and malicious language generation.", "published": "2024-11-06 16:30:26", "link": "http://arxiv.org/abs/2411.05037v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\n  for Design Space Exploration", "abstract": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\nimprove reasoning accuracy and contextual relevance. Despite its promising\napplications and strong relevance to multiple research communities, such as\ndatabases and natural language processing, GraphRAG currently lacks modular\nworkflow analysis, systematic solution frameworks, and insightful empirical\nstudies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\nthat enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\nsystematic classification of existing techniques and implemented GraphRAG\ninstances, and 3) creation of new GraphRAG instances. Our framework facilitates\ncomprehensive empirical studies of GraphRAG on large-scale real-world graphs\nand diverse query sets, revealing insights into balancing reasoning quality,\nruntime efficiency, and token or GPU cost, that are essential for building\nadvanced GraphRAG systems.", "published": "2024-11-06 15:32:28", "link": "http://arxiv.org/abs/2411.05844v2", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "PhDGPT: Introducing a psychometric and linguistic dataset about how\n  large language models perceive graduate students and professors in psychology", "abstract": "Machine psychology aims to reconstruct the mindset of Large Language Models\n(LLMs), i.e. how these artificial intelligences perceive and associate ideas.\nThis work introduces PhDGPT, a prompting framework and synthetic dataset that\nencapsulates the machine psychology of PhD researchers and professors as\nperceived by OpenAI's GPT-3.5. The dataset consists of 756,000 datapoints,\ncounting 300 iterations repeated across 15 academic events, 2 biological\ngenders, 2 career levels and 42 unique item responses of the Depression,\nAnxiety, and Stress Scale (DASS-42). PhDGPT integrates these psychometric\nscores with their explanations in plain language. This synergy of scores and\ntexts offers a dual, comprehensive perspective on the emotional well-being of\nsimulated academics, e.g. male/female PhD students or professors. By combining\nnetwork psychometrics and psycholinguistic dimensions, this study identifies\nseveral similarities and distinctions between human and LLM data. The\npsychometric networks of simulated male professors do not differ between\nphysical and emotional anxiety subscales, unlike humans. Other LLMs'\npersonification can reconstruct human DASS factors with a purity up to 80%.\nFurthemore, LLM-generated personifications across different scenarios are found\nto elicit explanations lower in concreteness and imageability in items coding\nfor anxiety, in agreement with past studies about human psychology. Our\nfindings indicate an advanced yet incomplete ability for LLMs to reproduce the\ncomplexity of human psychometric data, unveiling convenient advantages and\nlimitations in using LLMs to replace human participants. PhDGPT also\nintriguingly capture the ability for LLMs to adapt and change language patterns\naccording to prompted mental distress contextual features, opening new\nquantitative opportunities for assessing the machine psychology of these\nartificial intelligences.", "published": "2024-11-06 20:04:20", "link": "http://arxiv.org/abs/2411.10473v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Mobile Recording Device Recognition Based Cross-Scale and Multi-Level\n  Representation Learning", "abstract": "This paper introduces a modeling approach that employs multi-level global\nprocessing, encompassing both short-term frame-level and long-term sample-level\nfeature scales. In the initial stage of shallow feature extraction, various\nscales are employed to extract multi-level features, including Mel-Frequency\nCepstral Coefficients (MFCC) and pre-Fbank log energy spectrum. The\nconstruction of the identification network model involves considering the input\ntwo-dimensional temporal features from both frame and sample levels.\nSpecifically, the model initially employs one-dimensional convolution-based\nConvolutional Long Short-Term Memory (ConvLSTM) to fuse spatiotemporal\ninformation and extract short-term frame-level features. Subsequently,\nbidirectional long Short-Term Memory (BiLSTM) is utilized to learn long-term\nsample-level sequential representations. The transformer encoder then performs\ncross-scale, multi-level processing on global frame-level and sample-level\nfeatures, facilitating deep feature representation and fusion at both levels.\nFinally, recognition results are obtained through Softmax. Our method achieves\nan impressive 99.6% recognition accuracy on the CCNU_Mobile dataset, exhibiting\na notable improvement of 2% to 12% compared to the baseline system.\nAdditionally, we thoroughly investigate the transferability of our model,\nachieving an 87.9% accuracy in a classification task on a new dataset.", "published": "2024-11-06 05:04:04", "link": "http://arxiv.org/abs/2411.03668v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MOS-Bench: Benchmarking Generalization Abilities of Subjective Speech\n  Quality Assessment Models", "abstract": "Subjective speech quality assessment (SSQA) is critical for evaluating speech\nsamples as perceived by human listeners. While model-based SSQA has enjoyed\ngreat success thanks to the development of deep neural networks (DNNs),\ngeneralization remains a key challenge, especially for unseen, out-of-domain\ndata. To benchmark the generalization abilities of SSQA models, we present\nMOS-Bench, a diverse collection of datasets. In addition, we also introduce\nSHEET, an open-source toolkit containing complete recipes to conduct SSQA\nexperiments. We provided benchmark results for MOS-Bench, and we also explored\nmulti-dataset training to enhance generalization. Additionally, we proposed a\nnew performance metric, best score difference/ratio, and used latent space\nvisualizations to explain model behavior, offering valuable insights for future\nresearch.", "published": "2024-11-06 07:29:28", "link": "http://arxiv.org/abs/2411.03715v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Contrastive Self-Supervised Learning scheme for beat tracking amenable\n  to few-shot learning", "abstract": "In this paper, we propose a novel Self-Supervised-Learning scheme to train\nrhythm analysis systems and instantiate it for few-shot beat tracking. Taking\ninspiration from the Contrastive Predictive Coding paradigm, we propose to\ntrain a Log-Mel-Spectrogram Transformer encoder to contrast observations at\ntimes separated by hypothesized beat intervals from those that are not. We do\nthis without the knowledge of ground-truth tempo or beat positions, as we rely\non the local maxima of a Predominant Local Pulse function, considered as a\nproxy for Tatum positions, to define candidate anchors, candidate positives\n(located at a distance of a power of two from the anchor) and negatives\n(remaining time positions). We show that a model pre-trained using this\napproach on the unlabeled FMA, MTT and MTG-Jamendo datasets can successfully be\nfine-tuned in the few-shot regime, i.e. with just a few annotated examples to\nget a competitive beat-tracking performance.", "published": "2024-11-06 09:28:46", "link": "http://arxiv.org/abs/2411.04152v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of\n  Study in Tabletop Role-Playing Games Soundtracks", "abstract": "This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness.", "published": "2024-11-06 14:29:49", "link": "http://arxiv.org/abs/2411.03948v2", "categories": ["cs.SD", "cs.AI", "cs.MM", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
