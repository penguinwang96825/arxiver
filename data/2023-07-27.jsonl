{"title": "Improving Natural Language Inference in Arabic using Transformer Models\n  and Linguistically Informed Pre-Training", "abstract": "This paper addresses the classification of Arabic text data in the field of\nNatural Language Processing (NLP), with a particular focus on Natural Language\nInference (NLI) and Contradiction Detection (CD). Arabic is considered a\nresource-poor language, meaning that there are few data sets available, which\nleads to limited availability of NLP methods. To overcome this limitation, we\ncreate a dedicated data set from publicly available resources. Subsequently,\ntransformer-based machine learning models are being trained and evaluated. We\nfind that a language-specific model (AraBERT) performs competitively with\nstate-of-the-art multilingual approaches, when we apply linguistically informed\npre-training methods such as Named Entity Recognition (NER). To our knowledge,\nthis is the first large-scale evaluation for this task in Arabic, as well as\nthe first application of multi-task pre-training in this context.", "published": "2023-07-27 07:40:11", "link": "http://arxiv.org/abs/2307.14666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turning Whisper into Real-Time Transcription System", "abstract": "Whisper is one of the recent state-of-the-art multilingual speech recognition\nand translation models, however, it is not designed for real time\ntranscription. In this paper, we build on top of Whisper and create\nWhisper-Streaming, an implementation of real-time speech transcription and\ntranslation of Whisper-like models. Whisper-Streaming uses local agreement\npolicy with self-adaptive latency to enable streaming transcription. We show\nthat Whisper-Streaming achieves high quality and 3.3 seconds latency on\nunsegmented long-form speech transcription test set, and we demonstrate its\nrobustness and practical usability as a component in live transcription service\nat a multilingual conference.", "published": "2023-07-27 10:00:05", "link": "http://arxiv.org/abs/2307.14743v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling\n  Model", "abstract": "This paper presents a series of approaches aimed at enhancing the performance\nof Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic\ninformation from a Semantic Role Labeling (SRL) model. We propose a novel\nend-to-end Semantic Role Labeling model that effectively captures most of the\nstructured semantic information within the Transformer hidden state. We believe\nthat this end-to-end model is well-suited for our newly proposed models that\nincorporate semantic information. We evaluate the proposed models in two\nlanguages, English and Czech, employing ELECTRA-small models. Our combined\nmodels improve ABSA performance in both languages. Moreover, we achieved new\nstate-of-the-art results on the Czech ABSA.", "published": "2023-07-27 11:28:16", "link": "http://arxiv.org/abs/2307.14785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Models of reference production: How do they withstand the test of time?", "abstract": "In recent years, many NLP studies have focused solely on performance\nimprovement. In this work, we focus on the linguistic and scientific aspects of\nNLP. We use the task of generating referring expressions in context\n(REG-in-context) as a case study and start our analysis from GREC, a\ncomprehensive set of shared tasks in English that addressed this topic over a\ndecade ago. We ask what the performance of models would be if we assessed them\n(1) on more realistic datasets, and (2) using more advanced methods. We test\nthe models using different evaluation metrics and feature selection\nexperiments. We conclude that GREC can no longer be regarded as offering a\nreliable assessment of models' ability to mimic human reference production,\nbecause the results are highly impacted by the choice of corpus and evaluation\nmetrics. Our results also suggest that pre-trained language models are less\ndependent on the choice of corpus than classic Machine Learning models, and\ntherefore make more robust class predictions.", "published": "2023-07-27 12:46:38", "link": "http://arxiv.org/abs/2307.14817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Makes a Good Paraphrase: Do Automated Evaluations Work?", "abstract": "Paraphrasing is the task of expressing an essential idea or meaning in\ndifferent words. But how different should the words be in order to be\nconsidered an acceptable paraphrase? And can we exclusively use automated\nmetrics to evaluate the quality of a paraphrase? We attempt to answer these\nquestions by conducting experiments on a German data set and performing\nautomatic and expert linguistic evaluation.", "published": "2023-07-27 12:51:16", "link": "http://arxiv.org/abs/2307.14818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turkish Native Language Identification", "abstract": "In this paper, we present the first application of Native Language\nIdentification (NLI) for the Turkish language. NLI involves predicting the\nwriter's first language by analysing their writing in different languages.\nWhile most NLI research has focused on English, our study extends its scope to\nTurkish. We used the recently constructed Turkish Learner Corpus and employed a\ncombination of three syntactic features (CFG production rules, part-of-speech\nn-grams, and function words) with L2 texts to demonstrate their effectiveness\nin this task.", "published": "2023-07-27 13:28:31", "link": "http://arxiv.org/abs/2307.14850v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArcGPT: A Large Language Model Tailored for Real-world Archival\n  Applications", "abstract": "Archives play a crucial role in preserving information and knowledge, and the\nexponential growth of such data necessitates efficient and automated tools for\nmanaging and utilizing archive information resources. Archival applications\ninvolve managing massive data that are challenging to process and analyze.\nAlthough LLMs have made remarkable progress in diverse domains, there are no\npublicly available archives tailored LLM. Addressing this gap, we introduce\nArcGPT, to our knowledge, the first general-purpose LLM tailored to the\narchival field. To enhance model performance on real-world archival tasks,\nArcGPT has been pre-trained on massive and extensive archival domain data.\nAlongside ArcGPT, we release AMBLE, a benchmark comprising four real-world\narchival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing\nstate-of-the-art models, marking a substantial step forward in effective\narchival data management. Ultimately, ArcGPT aims to better serve the archival\ncommunity, aiding archivists in their crucial role of preserving and harnessing\nour collective information and knowledge.", "published": "2023-07-27 13:31:45", "link": "http://arxiv.org/abs/2307.14852v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MESED: A Multi-modal Entity Set Expansion Dataset with Fine-grained\n  Semantic Classes and Hard Negative Entities", "abstract": "The Entity Set Expansion (ESE) task aims to expand a handful of seed entities\nwith new entities belonging to the same semantic class. Conventional ESE\nmethods are based on mono-modality (i.e., literal modality), which struggle to\ndeal with complex entities in the real world such as: (1) Negative entities\nwith fine-grained semantic differences. (2) Synonymous entities. (3) Polysemous\nentities. (4) Long-tailed entities. These challenges prompt us to propose\nMulti-modal Entity Set Expansion (MESE), where models integrate information\nfrom multiple modalities to represent entities. Intuitively, the benefits of\nmulti-modal information for ESE are threefold: (1) Different modalities can\nprovide complementary information. (2) Multi-modal information provides a\nunified signal via common visual properties for the same semantic class or\nentity. (3) Multi-modal information offers robust alignment signal for\nsynonymous entities. To assess the performance of model in MESE and facilitate\nfurther research, we constructed the MESED dataset which is the first\nmulti-modal dataset for ESE with large-scale and elaborate manual calibration.\nA powerful multi-modal model MultiExpan is proposed which is pre-trained on\nfour multimodal pre-training tasks. The extensive experiments and analyses on\nMESED demonstrate the high quality of the dataset and the effectiveness of our\nMultiExpan, as well as pointing the direction for future research.", "published": "2023-07-27 14:09:59", "link": "http://arxiv.org/abs/2307.14878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-based Text Selection for Addressing Class-Imbalanced Data in\n  Classification", "abstract": "This paper addresses the problem of selecting of a set of texts for\nannotation in text classification using retrieval methods when there are limits\non the number of annotations due to constraints on human resources. An\nadditional challenge addressed is dealing with binary categories that have a\nsmall number of positive instances, reflecting severe class imbalance. In our\nsituation, where annotation occurs over a long time period, the selection of\ntexts to be annotated can be made in batches, with previous annotations guiding\nthe choice of the next set. To address these challenges, the paper proposes\nleveraging SHAP to construct a quality set of queries for Elasticsearch and\nsemantic search, to try to identify optimal sets of texts for annotation that\nwill help with class imbalance. The approach is tested on sets of cue texts\ndescribing possible future events, constructed by participants involved in\nstudies aimed to help with the management of obesity and diabetes. We introduce\nan effective method for selecting a small set of texts for annotation and\nbuilding high-quality classifiers. We integrate vector search, semantic search,\nand machine learning classifiers to yield a good solution. Our experiments\ndemonstrate improved F1 scores for the minority classes in binary\nclassification.", "published": "2023-07-27 14:42:16", "link": "http://arxiv.org/abs/2307.14899v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for\n  Writing Style Detection", "abstract": "The task of multi-author writing style detection aims at finding any\npositions of writing style change in a given text document. We formulate the\ntask as a natural language inference problem where two consecutive paragraphs\nare paired. Our approach focuses on transitions between paragraphs while\ntruncating input tokens for the task. As backbone models, we employ different\nTransformer-based encoders with warmup phase during training. We submit the\nmodel version that outperforms baselines and other proposed model versions in\nour experiments. For the easy and medium setups, we submit transition-focused\nnatural language inference based on DeBERTa with warmup training, and the same\nmodel without transition for the hard setup.", "published": "2023-07-27 14:56:06", "link": "http://arxiv.org/abs/2307.14913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransNormerLLM: A Faster and Better Large Language Model with Improved\n  TransNormer", "abstract": "We present TransNormerLLM, the first linear attention-based Large Language\nModel (LLM) that outperforms conventional softmax attention-based models in\nterms of both accuracy and efficiency. TransNormerLLM evolves from the previous\nlinear attention architecture TransNormer by making advanced modifications that\ninclude positional embedding, linear attention acceleration, gating mechanisms,\ntensor normalization, and inference acceleration and stabilization.\nSpecifically, we use LRPE together with an exponential decay to avoid attention\ndilution issues while allowing the model to retain global interactions between\ntokens. Additionally, we propose Lightning Attention, a cutting-edge technique\nthat accelerates linear attention by more than twice in runtime and reduces\nmemory usage by a remarkable four times. To further enhance the performance of\nTransNormer, we leverage a gating mechanism for smooth training and a new\ntensor normalization scheme to accelerate the model, resulting in an impressive\nacceleration of over $20\\%$. Furthermore, we develop a robust inference\nalgorithm that ensures numerical stability and consistent inference speed,\nregardless of the sequence length, showcasing superior efficiency during both\ntraining and inference stages. We also implement an efficient model parallel\nschema for TransNormerLLM, enabling seamless deployment on large-scale clusters\nand facilitating expansion to even more extensive models, i.e., LLMs with 175B\nparameters. We validate our model design through a series of ablations and\ntrain models with sizes of 385M, 1B, and 7B on our self-collected corpus.\nBenchmark results demonstrate that our models not only match the performance of\nstate-of-the-art LLMs with Transformer but are also significantly faster. Code\nis released at: https://github.com/OpenNLPLab/TransnormerLLM.", "published": "2023-07-27 16:45:33", "link": "http://arxiv.org/abs/2307.14995v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gzip versus bag-of-words for text classification", "abstract": "The effectiveness of compression in text classification ('gzip') has recently\ngarnered lots of attention. In this note we show that `bag-of-words' approaches\ncan achieve similar or better results, and are more efficient.", "published": "2023-07-27 16:57:32", "link": "http://arxiv.org/abs/2307.15002v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Geometric Notion of Causal Probing", "abstract": "The linear subspace hypothesis (Bolukbasi et al., 2016) states that, in a\nlanguage model's representation space, all information about a concept such as\nverbal number is encoded in a linear subspace. Prior work has relied on\nauxiliary classification tasks to identify and evaluate candidate subspaces\nthat might give support for this hypothesis. We instead give a set of intrinsic\ncriteria which characterize an ideal linear concept subspace and enable us to\nidentify the subspace using only the language model distribution. Our\ninformation-theoretic framework accounts for spuriously correlated features in\nthe representation space (Kumar et al., 2022) by reconciling the statistical\nnotion of concept information and the geometric notion of how concepts are\nencoded in the representation space. As a byproduct of this analysis, we\nhypothesize a causal process for how a language model might leverage concepts\nduring generation. Empirically, we find that linear concept erasure is\nsuccessful in erasing most concept information under our framework for verbal\nnumber as well as some complex aspect-level sentiment concepts from a\nrestaurant review dataset. Our causal intervention for controlled generation\nshows that, for at least one concept across two languages models, the concept\nsubspace can be used to manipulate the concept value of the generated word with\nprecision.", "published": "2023-07-27 17:57:57", "link": "http://arxiv.org/abs/2307.15054v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metric-Based In-context Learning: A Case Study in Text Simplification", "abstract": "In-context learning (ICL) for large language models has proven to be a\npowerful approach for many natural language processing tasks. However,\ndetermining the best method to select examples for ICL is nontrivial as the\nresults can vary greatly depending on the quality, quantity, and order of\nexamples used. In this paper, we conduct a case study on text simplification\n(TS) to investigate how to select the best and most robust examples for ICL. We\npropose Metric-Based in-context Learning (MBL) method that utilizes commonly\nused TS metrics such as SARI, compression ratio, and BERT-Precision for\nselection. Through an extensive set of experiments with various-sized GPT\nmodels on standard TS benchmarks such as TurkCorpus and ASSET, we show that\nexamples selected by the top SARI scores perform the best on larger models such\nas GPT-175B, while the compression ratio generally performs better on smaller\nmodels such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is\ngenerally robust to example orderings and out-of-domain test sets, and\noutperforms strong baselines and state-of-the-art finetuned language models.\nFinally, we show that the behaviour of large GPT models can be implicitly\ncontrolled by the chosen metric. Our research provides a new framework for\nselecting examples in ICL, and demonstrates its effectiveness in text\nsimplification tasks, breaking new ground for more accurate and efficient NLG\nsystems.", "published": "2023-07-27 05:45:35", "link": "http://arxiv.org/abs/2307.14632v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Generative Models for Graph-to-Text Generation", "abstract": "Large language models (LLMs) have been widely employed for graph-to-text\ngeneration tasks. However, the process of finetuning LLMs requires significant\ntraining resources and annotation work. In this paper, we explore the\ncapability of generative models to generate descriptive text from graph data in\na zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two\ngraph-to-text datasets and compare their performance with that of finetuned LLM\nmodels such as T5 and BART. Our results demonstrate that generative models are\ncapable of generating fluent and coherent text, achieving BLEU scores of 10.57\nand 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error\nanalysis reveals that generative models still struggle with understanding the\nsemantic relations between entities, and they also tend to generate text with\nhallucinations or irrelevant information. As a part of error analysis, we\nutilize BERT to detect machine-generated text and achieve high macro-F1 scores.\nWe have made the text generated by generative models publicly available.", "published": "2023-07-27 09:03:05", "link": "http://arxiv.org/abs/2307.14712v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners", "abstract": "In-context learning, which offers substantial advantages over fine-tuning, is\npredominantly observed in decoder-only models, while encoder-decoder (i.e.,\nseq2seq) models excel in methods that rely on weight updates. Recently, a few\nstudies have demonstrated the feasibility of few-shot learning with seq2seq\nmodels; however, this has been limited to tasks that align well with the\nseq2seq architecture, such as summarization and translation. Inspired by these\ninitial studies, we provide a first-ever extensive experiment comparing the\nin-context few-shot learning capabilities of decoder-only and encoder-decoder\nmodels on a broad range of tasks. Furthermore, we propose two methods to more\neffectively elicit in-context learning ability in seq2seq models:\nobjective-aligned prompting and a fusion-based approach. Remarkably, our\napproach outperforms a decoder-only model that is six times larger and exhibits\nsignificant performance improvements compared to conventional seq2seq models\nacross a variety of settings. We posit that, with the right configuration and\nprompt design, seq2seq models can be highly effective few-shot learners for a\nwide spectrum of applications.", "published": "2023-07-27 13:37:06", "link": "http://arxiv.org/abs/2307.14856v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger\n  Detection", "abstract": "Fanfiction, a popular form of creative writing set within established\nfictional universes, has gained a substantial online following. However,\nensuring the well-being and safety of participants has become a critical\nconcern in this community. The detection of triggering content, material that\nmay cause emotional distress or trauma to readers, poses a significant\nchallenge. In this paper, we describe our approach for the Trigger Detection\nshared task at PAN CLEF 2023, where we want to detect multiple triggering\ncontent in a given Fanfiction document. For this, we build a hierarchical model\nthat uses recurrence over Transformer-based language models. In our approach,\nwe first split long documents into smaller sized segments and use them to\nfine-tune a Transformer model. Then, we extract feature embeddings from the\nfine-tuned Transformer model, which are used as input in the training of\nmultiple LSTM models for trigger detection in a multi-label setting. Our model\nachieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the\nvalidation set, which are higher than the baseline results shared at PAN CLEF\n2023.", "published": "2023-07-27 14:55:10", "link": "http://arxiv.org/abs/2307.14912v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark", "abstract": "Large language models (LLMs) have shown the potential to be integrated into\nhuman daily lives. Therefore, user preference is the most critical criterion\nfor assessing LLMs' performance in real-world scenarios. However, existing\nbenchmarks mainly focus on measuring models' accuracy using multi-choice\nquestions, which limits the understanding of their capabilities in real\napplications. We fill this gap by proposing a comprehensive Chinese benchmark\nSuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE\nencompasses three sub-tasks: actual users' queries and ratings derived from an\nLLM battle platform (CArena), open-ended questions with single and\nmultiple-turn dialogues (OPEN), and closed-ended questions with the same stems\nas open-ended single-turn ones (CLOSE). Our study shows that accuracy on\nclosed-ended questions is insufficient to reflect human preferences achieved on\nopen-ended ones. At the same time, they can complement each other to predict\nactual user preferences. We also demonstrate that GPT-4 is a reliable judge to\nautomatically evaluate human preferences on open-ended questions in a Chinese\ncontext. Our benchmark will be released at https://www.CLUEbenchmarks.com", "published": "2023-07-27 17:24:09", "link": "http://arxiv.org/abs/2307.15020v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Matching Patients to Clinical Trials with Large Language Models", "abstract": "Patient recruitment is challenging for clinical trials. We introduce\nTrialGPT, an end-to-end framework for zero-shot patient-to-trial matching with\nlarge language models. TrialGPT comprises three modules: it first performs\nlarge-scale filtering to retrieve candidate trials (TrialGPT-Retrieval); then\npredicts criterion-level patient eligibility (TrialGPT-Matching); and finally\ngenerates trial-level scores (TrialGPT-Ranking). We evaluate TrialGPT on three\ncohorts of 183 synthetic patients with over 75,000 trial annotations.\nTrialGPT-Retrieval can recall over 90% of relevant trials using less than 6% of\nthe initial collection. Manual evaluations on 1,015 patient-criterion pairs\nshow that TrialGPT-Matching achieves an accuracy of 87.3% with faithful\nexplanations, close to the expert performance. The TrialGPT-Ranking scores are\nhighly correlated with human judgments and outperform the best-competing models\nby 43.8% in ranking and excluding trials. Furthermore, our user study reveals\nthat TrialGPT can reduce the screening time by 42.6% in patient recruitment.\nOverall, these results have demonstrated promising opportunities for\npatient-to-trial matching with TrialGPT.", "published": "2023-07-27 17:56:56", "link": "http://arxiv.org/abs/2307.15051v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "f-Divergence Minimization for Sequence-Level Knowledge Distillation", "abstract": "Knowledge distillation (KD) is the process of transferring knowledge from a\nlarge model to a small one. It has gained increasing attention in the natural\nlanguage processing community, driven by the demands of compressing\never-growing language models. In this work, we propose an f-DISTILL framework,\nwhich formulates sequence-level knowledge distillation as minimizing a\ngeneralized f-divergence function. We propose four distilling variants under\nour framework and show that existing SeqKD and ENGINE approaches are\napproximations of our f-DISTILL methods. We further derive step-wise\ndecomposition for our f-DISTILL, reducing intractable sequence-level divergence\nto word-level losses that can be computed in a tractable manner. Experiments\nacross four datasets show that our methods outperform existing KD approaches,\nand that our symmetric distilling losses can better force the student to learn\nfrom the teacher distribution.", "published": "2023-07-27 20:39:06", "link": "http://arxiv.org/abs/2307.15190v1", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6; I.2.m; I.5.1; I.7.m"], "primary_category": "cs.CL"}
{"title": "Incrementally-Computable Neural Networks: Efficient Inference for\n  Dynamic Inputs", "abstract": "Deep learning often faces the challenge of efficiently processing dynamic\ninputs, such as sensor data or user inputs. For example, an AI writing\nassistant is required to update its suggestions in real time as a document is\nedited. Re-running the model each time is expensive, even with compression\ntechniques like knowledge distillation, pruning, or quantization. Instead, we\ntake an incremental computing approach, looking to reuse calculations as the\ninputs change. However, the dense connectivity of conventional architectures\nposes a major obstacle to incremental computation, as even minor input changes\ncascade through the network and restrict information reuse. To address this, we\nuse vector quantization to discretize intermediate values in the network, which\nfilters out noisy and unnecessary modifications to hidden neurons, facilitating\nthe reuse of their values. We apply this approach to the transformers\narchitecture, creating an efficient incremental inference algorithm with\ncomplexity proportional to the fraction of the modified inputs. Our experiments\nwith adapting the OPT-125M pre-trained language model demonstrate comparable\naccuracy on document classification while requiring 12.1X (median) fewer\noperations for processing sequences of atomic edits.", "published": "2023-07-27 16:30:27", "link": "http://arxiv.org/abs/2307.14988v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Universal and Transferable Adversarial Attacks on Aligned Language\n  Models", "abstract": "Because \"out-of-the-box\" large language models are capable of generating a\ngreat deal of objectionable content, recent work has focused on aligning these\nmodels in an attempt to prevent undesirable generation. While there has been\nsome success at circumventing these measures -- so-called \"jailbreaks\" against\nLLMs -- these attacks have required significant human ingenuity and are brittle\nin practice. In this paper, we propose a simple and effective attack method\nthat causes aligned language models to generate objectionable behaviors.\nSpecifically, our approach finds a suffix that, when attached to a wide range\nof queries for an LLM to produce objectionable content, aims to maximize the\nprobability that the model produces an affirmative response (rather than\nrefusing to answer). However, instead of relying on manual engineering, our\napproach automatically produces these adversarial suffixes by a combination of\ngreedy and gradient-based search techniques, and also improves over past\nautomatic prompt generation methods.\n  Surprisingly, we find that the adversarial prompts generated by our approach\nare quite transferable, including to black-box, publicly released LLMs.\nSpecifically, we train an adversarial attack suffix on multiple prompts (i.e.,\nqueries asking for many different types of objectionable content), as well as\nmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting\nattack suffix is able to induce objectionable content in the public interfaces\nto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,\nPythia, Falcon, and others. In total, this work significantly advances the\nstate-of-the-art in adversarial attacks against aligned language models,\nraising important questions about how such systems can be prevented from\nproducing objectionable information. Code is available at\ngithub.com/llm-attacks/llm-attacks.", "published": "2023-07-27 17:49:12", "link": "http://arxiv.org/abs/2307.15043v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cascaded Cross-Modal Transformer for Request and Complaint Detection", "abstract": "We propose a novel cascaded cross-modal transformer (CCMT) that combines\nspeech and text transcripts to detect customer requests and complaints in phone\nconversations. Our approach leverages a multimodal paradigm by transcribing the\nspeech using automatic speech recognition (ASR) models and translating the\ntranscripts into different languages. Subsequently, we combine\nlanguage-specific BERT-based models with Wav2Vec2.0 audio features in a novel\ncascaded cross-attention transformer model. We apply our system to the Requests\nSub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics\nChallenge, reaching unweighted average recalls (UAR) of 65.41% and 85.87% for\nthe complaint and request classes, respectively.", "published": "2023-07-27 13:45:42", "link": "http://arxiv.org/abs/2307.15097v1", "categories": ["cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News\n  Stories Leveraging BERT and Stacked Embeddings", "abstract": "Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion\nClassification from essays written in reaction to news articles. Emotion\ndetection from complex dialogues is challenging and often requires\ncontext/domain understanding. Therefore in this research, we have focused on\ndeveloping deep learning (DL) models using the combination of word embedding\nrepresentations with tailored prepossessing strategies to capture the nuances\nof emotions expressed. Our experiments used static and contextual embeddings\n(individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and\nTransformer based models. We occupied rank tenth in the emotion detection task\nby scoring a Macro F1-Score of 0.2717, validating the efficacy of our\nimplemented approaches for small and imbalanced datasets with mixed categories\nof target emotions.", "published": "2023-07-27 19:42:22", "link": "http://arxiv.org/abs/2307.15164v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RCT Rejection Sampling for Causal Estimation Evaluation", "abstract": "Confounding is a significant obstacle to unbiased estimation of causal\neffects from observational data. For settings with high-dimensional covariates\n-- such as text data, genomics, or the behavioral social sciences --\nresearchers have proposed methods to adjust for confounding by adapting machine\nlearning methods to the goal of causal estimation. However, empirical\nevaluation of these adjustment methods has been challenging and limited. In\nthis work, we build on a promising empirical evaluation strategy that\nsimplifies evaluation design and uses real data: subsampling randomized\ncontrolled trials (RCTs) to create confounded observational datasets while\nusing the average causal effects from the RCTs as ground-truth. We contribute a\nnew sampling algorithm, which we call RCT rejection sampling, and provide\ntheoretical guarantees that causal identification holds in the observational\ndata to allow for valid comparisons to the ground-truth RCT. Using synthetic\ndata, we show our algorithm indeed results in low bias when oracle estimators\nare evaluated on the confounded samples, which is not always the case for a\npreviously proposed algorithm. In addition to this identification result, we\nhighlight several finite data considerations for evaluation designers who plan\nto use RCT rejection sampling on their own datasets. As a proof of concept, we\nimplement an example evaluation pipeline and walk through these finite data\nconsiderations with a novel, real-world RCT -- which we release publicly --\nconsisting of approximately 70k observations and text data as high-dimensional\ncovariates. Together, these contributions build towards a broader agenda of\nimproved empirical evaluation for causal estimation.", "published": "2023-07-27 20:11:07", "link": "http://arxiv.org/abs/2307.15176v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.AI"}
{"title": "PromptStyler: Prompt-driven Style Generation for Source-free Domain\n  Generalization", "abstract": "In a joint vision-language space, a text feature (e.g., from \"a photo of a\ndog\") could effectively represent its relevant image features (e.g., from dog\nphotos). Also, a recent study has demonstrated the cross-modal transferability\nphenomenon of this joint space. From these observations, we propose\nPromptStyler which simulates various distribution shifts in the joint space by\nsynthesizing diverse styles via prompts without using any images to deal with\nsource-free domain generalization. The proposed method learns to generate a\nvariety of style features (from \"a S* style of a\") via learnable style word\nvectors for pseudo-words S*. To ensure that learned styles do not distort\ncontent information, we force style-content features (from \"a S* style of a\n[class]\") to be located nearby their corresponding content features (from\n\"[class]\") in the joint vision-language space. After learning style word\nvectors, we train a linear classifier using synthesized style-content features.\nPromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and\nDomainNet, even though it does not require any images for training.", "published": "2023-07-27 21:14:46", "link": "http://arxiv.org/abs/2307.15199v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Open Problems and Fundamental Limitations of Reinforcement Learning from\n  Human Feedback", "abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training\nAI systems to align with human goals. RLHF has emerged as the central method\nused to finetune state-of-the-art large language models (LLMs). Despite this\npopularity, there has been relatively little public work systematizing its\nflaws. In this paper, we (1) survey open problems and fundamental limitations\nof RLHF and related methods; (2) overview techniques to understand, improve,\nand complement RLHF in practice; and (3) propose auditing and disclosure\nstandards to improve societal oversight of RLHF systems. Our work emphasizes\nthe limitations of RLHF and highlights the importance of a multi-faceted\napproach to the development of safer AI systems.", "published": "2023-07-27 22:29:25", "link": "http://arxiv.org/abs/2307.15217v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "LLMediator: GPT-4 Assisted Online Dispute Resolution", "abstract": "In this article, we introduce LLMediator, an experimental platform designed\nto enhance online dispute resolution (ODR) by utilizing capabilities of\nstate-of-the-art large language models (LLMs) such as GPT-4. In the context of\nhigh-volume, low-intensity legal disputes, alternative dispute resolution\nmethods such as negotiation and mediation offer accessible and cooperative\nsolutions for laypeople. These approaches can be carried out online on ODR\nplatforms. LLMediator aims to improve the efficacy of such processes by\nleveraging GPT-4 to reformulate user messages, draft mediator responses, and\npotentially autonomously engage in the discussions. We present and discuss\nseveral features of LLMediator and conduct initial qualitative evaluations,\ndemonstrating the potential for LLMs to support ODR and facilitate amicable\nsettlements. The initial proof of concept is promising and opens up avenues for\nfurther research in AI-assisted negotiation and mediation.", "published": "2023-07-27 10:25:29", "link": "http://arxiv.org/abs/2307.16732v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset", "abstract": "We present a new large-scale emotion-labeled symbolic music dataset\nconsisting of 12k MIDI songs. To create this dataset, we first trained emotion\nclassification models on the GoEmotions dataset, achieving state-of-the-art\nresults with a model half the size of the baseline. We then applied these\nmodels to lyrics from two large-scale MIDI datasets. Our dataset covers a wide\nrange of fine-grained emotions, providing a valuable resource to explore the\nconnection between music and emotions and, especially, to develop models that\ncan generate music based on specific emotions. Our code for inference, trained\nmodels, and datasets are available online.", "published": "2023-07-27 11:24:47", "link": "http://arxiv.org/abs/2307.14783v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "eess.AS"}
{"title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking\n  Feedback", "abstract": "Large Language Models for Code (Code LLM) are flourishing. New and powerful\nmodels are released on a weekly basis, demonstrating remarkable performance on\nthe code generation task. Various approaches have been proposed to boost the\ncode generation performance of pre-trained Code LLMs, such as supervised\nfine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we\npropose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework,\nwhich can effectively and efficiently boost pre-trained large language models\nfor code generation. Under this framework, we present PanGu-Coder2, which\nachieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through\nan extensive evaluation on CoderEval and LeetCode benchmarks, we show that\nPanGu-Coder2 consistently outperforms all previous Code LLMs.", "published": "2023-07-27 15:28:29", "link": "http://arxiv.org/abs/2307.14936v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation", "abstract": "Self-supervised and language-supervised image models contain rich knowledge\nof the world that is important for generalization. Many robotic tasks, however,\nrequire a detailed understanding of 3D geometry, which is often lacking in 2D\nimage features. This work bridges this 2D-to-3D gap for robotic manipulation by\nleveraging distilled feature fields to combine accurate 3D geometry with rich\nsemantics from 2D foundation models. We present a few-shot learning method for\n6-DOF grasping and placing that harnesses these strong spatial and semantic\npriors to achieve in-the-wild generalization to unseen objects. Using features\ndistilled from a vision-language model, CLIP, we present a way to designate\nnovel objects for manipulation via free-text natural language, and demonstrate\nits ability to generalize to unseen expressions and novel categories of\nobjects.", "published": "2023-07-27 17:59:14", "link": "http://arxiv.org/abs/2308.07931v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Spatial Upsampling of Head-Related Transfer Functions Using a\n  Physics-Informed Neural Network", "abstract": "Head-related transfer function (HRTF) capture the information that a person\nuses to localize sound sources in space, and thus is crucial for creating\npersonalized virtual acoustic experiences. However, practical HRTF measurement\nsystems may only measure a person's HRTFs sparsely, and this necessitates HRTF\nupsampling. This paper proposes a physics-informed neural network (PINN) method\nfor HRTF upsampling. The PINN exploits the Helmholtz equation, the governing\nequation of acoustic wave propagation, for regularizing the upsampling process.\nThis helps the generation of physically valid upsamplings which generalize\nbeyond the measured HRTF. Furthermore, the size (width and depth) of the PINN\nis set according to the Helmholtz equation and its solutions, the spherical\nharmonics (SHs). This makes the PINN have an appropriate level of expressive\npower and thus does not suffer from the over-fitting problem. Since the PINN is\ndesigned independent of any specific HRTF dataset, it offers more\ngeneralizability compared to pure data-driven methods. Numerical experiments\nconfirm the better performance of the PINN method for HRTF upsampling in both\ninterpolation and extrapolation scenarios in comparison with the SH method and\nthe HRTF field method.", "published": "2023-07-27 06:55:10", "link": "http://arxiv.org/abs/2307.14650v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Mitigating Cross-Database Differences for Learning Unified HRTF\n  Representation", "abstract": "Individualized head-related transfer functions (HRTFs) are crucial for\naccurate sound positioning in virtual auditory displays. As the acoustic\nmeasurement of HRTFs is resource-intensive, predicting individualized HRTFs\nusing machine learning models is a promising approach at scale. Training such\nmodels require a unified HRTF representation across multiple databases to\nutilize their respectively limited samples. However, in addition to differences\non the spatial sampling locations, recent studies have shown that, even for the\ncommon location, HRTFs across databases manifest consistent differences that\nmake it trivial to tell which databases they come from. This poses a\nsignificant challenge for learning a unified HRTF representation across\ndatabases. In this work, we first identify the possible causes of these\ncross-database differences, attributing them to variations in the measurement\nsetup. Then, we propose a novel approach to normalize the frequency responses\nof HRTFs across databases. We show that HRTFs from different databases cannot\nbe classified by their database after normalization. We further show that these\nnormalized HRTFs can be used to learn a more unified HRTF representation across\ndatabases than the prior art. We believe that this normalization approach paves\nthe road to many data-intensive tasks on HRTF modeling.", "published": "2023-07-27 00:06:22", "link": "http://arxiv.org/abs/2307.14547v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Inputs for Active Speaker Detection and Localization via\n  Microphone Array", "abstract": "This study considers the problem of detecting and locating an active talker's\nhorizontal position from multichannel audio captured by a microphone array. We\nrefer to this as active speaker detection and localization (ASDL). Our goal was\nto investigate the performance of spatial acoustic features extracted from the\nmultichannel audio as the input of a convolutional recurrent neural network\n(CRNN), in relation to the number of channels employed and additive noise. To\nthis end, experiments were conducted to compare the generalized\ncross-correlation with phase transform (GCC-PHAT), the spatial cue-augmented\nlog-spectrogram (SALSA) features, and a recently-proposed beamforming method,\nevaluating their robustness to various noise intensities. The array aperture\nand sampling density were tested by taking subsets from the 16-microphone\narray. Results and tests of statistical significance demonstrate the\nmicrophones' contribution to performance on the TragicTalkers dataset, which\noffers opportunities to investigate audio-visual approaches in the future.", "published": "2023-07-27 09:52:14", "link": "http://arxiv.org/abs/2307.14739v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The Effect of Spoken Language on Speech Enhancement using\n  Self-Supervised Speech Representation Loss Functions", "abstract": "Recent work in the field of speech enhancement (SE) has involved the use of\nself-supervised speech representations (SSSRs) as feature transformations in\nloss functions. However, in prior work, very little attention has been paid to\nthe relationship between the language of the audio used to train the\nself-supervised representation and that used to train the SE system.\nEnhancement models trained using a loss function which incorporates a\nself-supervised representation that shares exactly the language of the noisy\ndata used to train the SE system show better performance than those which do\nnot match exactly. This may lead to enhancement systems which are language\nspecific and as such do not generalise well to unseen languages, unlike models\ntrained using traditional spectrogram or time domain loss functions. In this\nwork, SE models are trained and tested on a number of different languages, with\nself-supervised representations which themselves are trained using different\nlanguage combinations and with differing network structures as loss function\nrepresentations. These models are then tested across unseen languages and their\nperformances are analysed. It is found that the training language of the\nself-supervised representation appears to have a minor effect on enhancement\nperformance, the amount of training data of a particular language, however,\ngreatly affects performance.", "published": "2023-07-27 09:20:38", "link": "http://arxiv.org/abs/2307.14502v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Complete and separate: Conditional separation with missing target source\n  attribute completion", "abstract": "Recent approaches in source separation leverage semantic information about\ntheir input mixtures and constituent sources that when used in conditional\nseparation models can achieve impressive performance. Most approaches along\nthese lines have focused on simple descriptions, which are not always useful\nfor varying types of input mixtures. In this work, we present an approach in\nwhich a model, given an input mixture and partial semantic information about a\ntarget source, is trained to extract additional semantic data. We then leverage\nthis pre-trained model to improve the separation performance of an uncoupled\nmulti-conditional separation network. Our experiments demonstrate that the\nseparation performance of this multi-conditional model is significantly\nimproved, approaching the performance of an oracle model with complete semantic\ninformation. Furthermore, our approach achieves performance levels that are\ncomparable to those of the best performing specialized single conditional\nmodels, thus providing an easier to use alternative.", "published": "2023-07-27 03:53:53", "link": "http://arxiv.org/abs/2307.14609v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Graph-based Polyphonic Multitrack Music Generation", "abstract": "Graphs can be leveraged to model polyphonic multitrack symbolic music, where\nnotes, chords and entire sections may be linked at different levels of the\nmusical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a\nlack of works that consider graph representations in the context of deep\nlearning systems for music generation. This paper bridges this gap by\nintroducing a novel graph representation for music and a deep Variational\nAutoencoder that generates the structure and the content of musical graphs\nseparately, one after the other, with a hierarchical architecture that matches\nthe structural priors of music. By separating the structure and content of\nmusical graphs, it is possible to condition generation by specifying which\ninstruments are played at certain times. This opens the door to a new form of\nhuman-computer interaction in the context of music co-creation. After training\nthe model on existing MIDI datasets, the experiments show that the model is\nable to generate appealing short and long musical sequences and to\nrealistically interpolate between them, producing music that is tonally and\nrhythmically consistent. Finally, the visualization of the embeddings shows\nthat the model is able to organize its latent space in accordance with known\nmusical concepts.", "published": "2023-07-27 15:18:50", "link": "http://arxiv.org/abs/2307.14928v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Visual Acoustic Matching", "abstract": "Acoustic matching aims to re-synthesize an audio clip to sound as if it were\nrecorded in a target acoustic environment. Existing methods assume access to\npaired training data, where the audio is observed in both source and target\nenvironments, but this limits the diversity of training data or requires the\nuse of simulated data or heuristics to create paired samples. We propose a\nself-supervised approach to visual acoustic matching where training samples\ninclude only the target scene image and audio -- without acoustically\nmismatched source audio for reference. Our approach jointly learns to\ndisentangle room acoustics and re-synthesize audio into the target environment,\nvia a conditional GAN framework and a novel metric that quantifies the level of\nresidual acoustic information in the de-biased audio. Training with either\nin-the-wild web data or simulated data, we demonstrate it outperforms the\nstate-of-the-art on multiple challenging datasets and a wide variety of\nreal-world audio and environments.", "published": "2023-07-27 17:59:59", "link": "http://arxiv.org/abs/2307.15064v2", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Detection of Children Abuse by Voice and Audio Classification by\n  Short-Time Fourier Transform Machine Learning implemented on Nvidia Edge GPU\n  device", "abstract": "The safety of children in children home has become an increasing social\nconcern, and the purpose of this experiment is to use machine learning applied\nto detect the scenarios of child abuse to increase the safety of children. This\nexperiment uses machine learning to classify and recognize a child's voice and\npredict whether the current sound made by the child is crying, screaming or\nlaughing. If a child is found to be crying or screaming, an alert is\nimmediately sent to the relevant personnel so that they can perceive what the\nchild may be experiencing in a surveillance blind spot and respond in a timely\nmanner. Together with a hybrid use of video image classification, the accuracy\nof child abuse detection can be significantly increased. This greatly reduces\nthe likelihood that a child will receive violent abuse in the nursery and\nallows personnel to stop an imminent or incipient child abuse incident in time.\nThe datasets collected from this experiment is entirely from sounds recorded on\nsite at the children home, including crying, laughing, screaming sound and\nbackground noises. These sound files are transformed into spectrograms using\nShort-Time Fourier Transform, and then these image data are imported into a CNN\nneural network for classification, and the final trained model can achieve an\naccuracy of about 92% for sound detection.", "published": "2023-07-27 16:48:19", "link": "http://arxiv.org/abs/2307.15101v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "First level: 68"], "primary_category": "eess.AS"}
