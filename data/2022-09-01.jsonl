{"title": "Isotropic Representation Can Improve Dense Retrieval", "abstract": "The recent advancement in language representation modeling has broadly\naffected the design of dense retrieval models. In particular, many of the\nhigh-performing dense retrieval models evaluate representations of query and\ndocument using BERT, and subsequently apply a cosine-similarity based scoring\nto determine the relevance. BERT representations, however, are known to follow\nan anisotropic distribution of a narrow cone shape and such an anisotropic\ndistribution can be undesirable for the cosine-similarity based scoring. In\nthis work, we first show that BERT-based DR also follows an anisotropic\ndistribution. To cope with the problem, we introduce unsupervised\npost-processing methods of Normalizing Flow and whitening, and develop\ntoken-wise method in addition to the sequence-wise method for applying the\npost-processing methods to the representations of dense retrieval models. We\nshow that the proposed methods can effectively enhance the representations to\nbe isotropic, then we perform experiments with ColBERT and RepBERT to show that\nthe performance (NDCG at 10) of document re-ranking can be improved by\n5.17\\%$\\sim$8.09\\% for ColBERT and 6.88\\%$\\sim$22.81\\% for RepBERT. To examine\nthe potential of isotropic representation for improving the robustness of DR\nmodels, we investigate out-of-distribution tasks where the test dataset differs\nfrom the training dataset. The results show that isotropic representation can\nachieve a generally improved performance. For instance, when training dataset\nis MS-MARCO and test dataset is Robust04, isotropy post-processing can improve\nthe baseline performance by up to 24.98\\%. Furthermore, we show that an\nisotropic model trained with an out-of-distribution dataset can even outperform\na baseline model trained with the in-distribution dataset.", "published": "2022-09-01 04:29:38", "link": "http://arxiv.org/abs/2209.00218v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Less is More: Rethinking State-of-the-art Continual Relation Extraction\n  Models with a Frustratingly Easy but Effective Approach", "abstract": "Continual relation extraction (CRE) requires the model to continually learn\nnew relations from class-incremental data streams. In this paper, we propose a\nFrustratingly easy but Effective Approach (FEA) method with two learning stages\nfor CRE: 1) Fast Adaption (FA) warms up the model with only new data. 2)\nBalanced Tuning (BT) finetunes the model on the balanced memory data. Despite\nits simplicity, FEA achieves comparable (on TACRED or superior (on FewRel)\nperformance compared with the state-of-the-art baselines. With careful\nexaminations, we find that the data imbalance between new and old relations\nleads to a skewed decision boundary in the head classifiers over the pretrained\nencoders, thus hurting the overall performance. In FEA, the FA stage unleashes\nthe potential of memory data for the subsequent finetuning, while the BT stage\nhelps establish a more balanced decision boundary. With a unified view, we find\nthat two strong CRE baselines can be subsumed into the proposed training\npipeline. The success of FEA also provides actionable insights and suggestions\nfor future model designing in CRE.", "published": "2022-09-01 06:08:07", "link": "http://arxiv.org/abs/2209.00243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Effective Information Utilization in Multi-Turn Topic-Driven\n  Conversations", "abstract": "Conversations are always related to certain topics. However, it is\nchallenging to fuse dialogue history and topic information from various sources\nat the same time in current dialogue generation models because of the input\nlength limit of pre-trained language models (PLMs). In order to expand the\ninformation that PLMs can utilize, we encode topic and dialogue history\ninformation using certain prompts with multiple channels of Fusion-in-Decoder\n(FiD) and explore the influence of three different channel settings. In this\npaper, our experiments focus on a specific Chinese dataset named NaturalConv,\nwhere the conversation revolves around a piece of recent news. We thoroughly\ncompared different dialogue models and different FiD channel settings.\nEmpirical results show that by combining our proposed whole passage channel\nwith additional history channel, our methods can achieve competitive\nperformance on NaturalConv, making it possible to encode various information\nfrom excessively long texts.", "published": "2022-09-01 06:20:39", "link": "http://arxiv.org/abs/2209.00250v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which anonymization technique is best for which NLP task? -- It depends.\n  A Systematic Study on Clinical Text Processing", "abstract": "Clinical text processing has gained more and more attention in recent years.\nThe access to sensitive patient data, on the other hand, is still a big\nchallenge, as text cannot be shared without legal hurdles and without removing\npersonal information. There are many techniques to modify or remove patient\nrelated information, each with different strengths. This paper investigates the\ninfluence of different anonymization techniques on the performance of ML models\nusing multiple datasets corresponding to five different NLP tasks. Several\nlearnings and recommendations are presented. This work confirms that\nparticularly stronger anonymization techniques lead to a significant drop of\nperformance. In addition to that, most of the presented techniques are not\nsecure against a re-identification attack based on similarity search.", "published": "2022-09-01 07:00:54", "link": "http://arxiv.org/abs/2209.00262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Environmental Claim Detection", "abstract": "To transition to a green economy, environmental claims made by companies must\nbe reliable, comparable, and verifiable. To analyze such claims at scale,\nautomated methods are needed to detect them in the first place. However, there\nexist no datasets or models for this. Thus, this paper introduces the task of\nenvironmental claim detection. To accompany the task, we release an\nexpert-annotated dataset and models trained on this dataset. We preview one\npotential application of such models: We detect environmental claims made in\nquarterly earning calls and find that the number of environmental claims has\nsteadily increased since the Paris Agreement in 2015.", "published": "2022-09-01 14:51:07", "link": "http://arxiv.org/abs/2209.00507v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Do Neural Language Models Still Need Commonsense Knowledge to Handle\n  Semantic Variations in Question Answering?", "abstract": "Many contextualized word representations are now learned by intricate neural\nnetwork models, such as masked neural language models (MNLMs) which are made up\nof huge neural network structures and trained to restore the masked text. Such\nrepresentations demonstrate superhuman performance in some reading\ncomprehension (RC) tasks which extract a proper answer in the context given a\nquestion. However, identifying the detailed knowledge trained in MNLMs is\nchallenging owing to numerous and intermingled model parameters. This paper\nprovides new insights and empirical analyses on commonsense knowledge included\nin pretrained MNLMs. First, we use a diagnostic test that evaluates whether\ncommonsense knowledge is properly trained in MNLMs. We observe that a large\nproportion of commonsense knowledge is not appropriately trained in MNLMs and\nMNLMs do not often understand the semantic meaning of relations accurately. In\naddition, we find that the MNLM-based RC models are still vulnerable to\nsemantic variations that require commonsense knowledge. Finally, we discover\nthe fundamental reason why some knowledge is not trained. We further suggest\nthat utilizing an external commonsense knowledge repository can be an effective\nsolution. We exemplify the possibility to overcome the limitations of the\nMNLM-based RC models by enriching text with the required knowledge from an\nexternal commonsense knowledge repository in controlled experiments.", "published": "2022-09-01 17:15:02", "link": "http://arxiv.org/abs/2209.00599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Ion Exchange Mechanism Inspired Story Ending Generator for Different\n  Characters", "abstract": "Story ending generation aims at generating reasonable endings for a given\nstory context. Most existing studies in this area focus on generating coherent\nor diversified story endings, while they ignore that different characters may\nlead to different endings for a given story. In this paper, we propose a\nCharacter-oriented Story Ending Generator (CoSEG) to customize an ending for\neach character in a story. Specifically, we first propose a character modeling\nmodule to learn the personalities of characters from their descriptive\nexperiences extracted from the story context. Then, inspired by the ion\nexchange mechanism in chemical reactions, we design a novel vector\nbreaking/forming module to learn the intrinsic interactions between each\ncharacter and the corresponding context through an analogical information\nexchange procedure. Finally, we leverage the attention mechanism to learn\neffective character-specific interactions and feed each interaction into a\ndecoder to generate character-orient endings. Extensive experimental results\nand case studies demonstrate that CoSEG achieves significant improvements in\nthe quality of generated endings compared with state-of-the-art methods, and it\neffectively customizes the endings for different characters.", "published": "2022-09-01 03:32:36", "link": "http://arxiv.org/abs/2209.00200v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Semantic Understanding with Self-supervised Methods for\n  Abstractive Dialogue Summarization", "abstract": "Contextualized word embeddings can lead to state-of-the-art performances in\nnatural language understanding. Recently, a pre-trained deep contextualized\ntext encoder such as BERT has shown its potential in improving natural language\ntasks including abstractive summarization. Existing approaches in dialogue\nsummarization focus on incorporating a large language model into summarization\ntask trained on large-scale corpora consisting of news articles rather than\ndialogues of multiple speakers. In this paper, we introduce self-supervised\nmethods to compensate shortcomings to train a dialogue summarization model. Our\nprinciple is to detect incoherent information flows using pretext dialogue text\nto enhance BERT's ability to contextualize the dialogue text representations.\nWe build and fine-tune an abstractive dialogue summarization model on a shared\nencoder-decoder architecture using the enhanced BERT. We empirically evaluate\nour abstractive dialogue summarizer with the SAMSum corpus, a recently\nintroduced dataset with abstractive dialogue summaries. All of our methods have\ncontributed improvements to abstractive summary measured in ROUGE scores.\nThrough an extensive ablation study, we also present a sensitivity analysis to\ncritical model hyperparameters, probabilities of switching utterances and\nmasking interlocutors.", "published": "2022-09-01 07:51:46", "link": "http://arxiv.org/abs/2209.00278v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Find the Funding: Entity Linking with Incomplete Funding Knowledge Bases", "abstract": "Automatic extraction of funding information from academic articles adds\nsignificant value to industry and research communities, such as tracking\nresearch outcomes by funding organizations, profiling researchers and\nuniversities based on the received funding, and supporting open access\npolicies. Two major challenges of identifying and linking funding entities are:\n(i) sparse graph structure of the Knowledge Base (KB), which makes the commonly\nused graph-based entity linking approaches suboptimal for the funding domain,\n(ii) missing entities in KB, which (unlike recent zero-shot approaches)\nrequires marking entity mentions without KB entries as NIL. We propose an\nentity linking model that can perform NIL prediction and overcome data scarcity\nissues in a time and data-efficient manner. Our model builds on a\ntransformer-based mention detection and bi-encoder model to perform entity\nlinking. We show that our model outperforms strong existing baselines.", "published": "2022-09-01 10:41:42", "link": "http://arxiv.org/abs/2209.00351v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KoCHET: a Korean Cultural Heritage corpus for Entity-related Tasks", "abstract": "As digitized traditional cultural heritage documents have rapidly increased,\nresulting in an increased need for preservation and management, practical\nrecognition of entities and typification of their classes has become essential.\nTo achieve this, we propose KoCHET - a Korean cultural heritage corpus for the\ntypical entity-related tasks, i.e., named entity recognition (NER), relation\nextraction (RE), and entity typing (ET). Advised by cultural heritage experts\nbased on the data construction guidelines of government-affiliated\norganizations, KoCHET consists of respectively 112,362, 38,765, 113,198\nexamples for NER, RE, and ET tasks, covering all entity types related to Korean\ncultural heritage. Moreover, unlike the existing public corpora, modified\nredistribution can be allowed both domestic and foreign researchers. Our\nexperimental results make the practical usability of KoCHET more valuable in\nterms of cultural heritage. We also provide practical insights of KoCHET in\nterms of statistical and linguistic analysis. Our corpus is freely available at\nhttps://github.com/Gyeongmin47/KoCHET.", "published": "2022-09-01 11:23:03", "link": "http://arxiv.org/abs/2209.00367v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Focus-Driven Contrastive Learniang for Medical Question Summarization", "abstract": "Automatic medical question summarization can significantly help the system to\nunderstand consumer health questions and retrieve correct answers. The Seq2Seq\nmodel based on maximum likelihood estimation (MLE) has been applied in this\ntask, which faces two general problems: the model can not capture well question\nfocus and and the traditional MLE strategy lacks the ability to understand\nsentence-level semantics. To alleviate these problems, we propose a novel\nquestion focus-driven contrastive learning framework (QFCL). Specially, we\npropose an easy and effective approach to generate hard negative samples based\non the question focus, and exploit contrastive learning at both encoder and\ndecoder to obtain better sentence level representations. On three medical\nbenchmark datasets, our proposed model achieves new state-of-the-art results,\nand obtains a performance gain of 5.33, 12.85 and 3.81 points over the baseline\nBART model on three datasets respectively. Further human judgement and detailed\nanalysis prove that our QFCL model learns better sentence representations with\nthe ability to distinguish different sentence meanings, and generates\nhigh-quality summaries by capturing question focus.", "published": "2022-09-01 14:15:46", "link": "http://arxiv.org/abs/2209.00484v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In conversation with Artificial Intelligence: aligning language models\n  with human values", "abstract": "Large-scale language technologies are increasingly used in various forms of\ncommunication with humans across different contexts. One particular use case\nfor these technologies is conversational agents, which output natural language\ntext in response to prompts and queries. This mode of engagement raises a\nnumber of social and ethical questions. For example, what does it mean to align\nconversational agents with human norms or values? Which norms or values should\nthey be aligned with? And how can this be accomplished? In this paper, we\npropose a number of steps that help answer these questions. We start by\ndeveloping a philosophical analysis of the building blocks of linguistic\ncommunication between conversational agents and human interlocutors. We then\nuse this analysis to identify and formulate ideal norms of conversation that\ncan govern successful linguistic communication between humans and\nconversational agents. Furthermore, we explore how these norms can be used to\nalign conversational agents with human values across a range of different\ndiscursive domains. We conclude by discussing the practical implications of our\nproposal for the design of conversational agents that are aligned with these\nnorms and values.", "published": "2022-09-01 21:16:47", "link": "http://arxiv.org/abs/2209.00731v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Deep Sparse Conformer for Speech Recognition", "abstract": "Conformer has achieved impressive results in Automatic Speech Recognition\n(ASR) by leveraging transformer's capturing of content-based global\ninteractions and convolutional neural network's exploiting of local features.\nIn Conformer, two macaron-like feed-forward layers with half-step residual\nconnections sandwich the multi-head self-attention and convolution modules\nfollowed by a post layer normalization. We improve Conformer's long-sequence\nrepresentation ability in two directions, \\emph{sparser} and \\emph{deeper}. We\nadapt a sparse self-attention mechanism with $\\mathcal{O}(L\\text{log}L)$ in\ntime complexity and memory usage. A deep normalization strategy is utilized\nwhen performing residual connections to ensure our training of hundred-level\nConformer blocks. On the Japanese CSJ-500h dataset, this deep sparse Conformer\nachieves respectively CERs of 5.52\\%, 4.03\\% and 4.50\\% on the three evaluation\nsets and 4.16\\%, 2.84\\% and 3.20\\% when ensembling five deep sparse Conformer\nvariants from 12 to 16, 17, 50, and finally 100 encoder layers.", "published": "2022-09-01 06:56:11", "link": "http://arxiv.org/abs/2209.00260v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Attention Enhanced Citrinet for Speech Recognition", "abstract": "Citrinet is an end-to-end convolutional Connectionist Temporal Classification\n(CTC) based automatic speech recognition (ASR) model. To capture local and\nglobal contextual information, 1D time-channel separable convolutions combined\nwith sub-word encoding and squeeze-and-excitation (SE) are used in Citrinet,\nmaking the whole architecture to be as deep as including 23 blocks with 235\nconvolution layers and 46 linear layers. This pure convolutional and deep\narchitecture makes Critrinet relatively slow at convergence. In this paper, we\npropose to introduce multi-head attentions together with feed-forward networks\nin the convolution module in Citrinet blocks while keeping the SE module and\nresidual module unchanged. For speeding up, we remove 8 convolution layers in\neach attention-enhanced Citrinet block and reduce 23 blocks to 13. Experiments\non the Japanese CSJ-500h and Magic-1600h dataset show that the\nattention-enhanced Citrinet with less layers and blocks and converges faster\nwith lower character error rates than (1) Citrinet with 80\\% training time and\n(2) Conformer with 40\\% training time and 29.8\\% model size.", "published": "2022-09-01 06:59:50", "link": "http://arxiv.org/abs/2209.00261v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Video-Guided Curriculum Learning for Spoken Video Grounding", "abstract": "In this paper, we introduce a new task, spoken video grounding (SVG), which\naims to localize the desired video fragments from spoken language descriptions.\nCompared with using text, employing audio requires the model to directly\nexploit the useful phonemes and syllables related to the video from raw speech.\nMoreover, we randomly add environmental noises to this speech audio, further\nincreasing the difficulty of this task and better simulating real applications.\nTo rectify the discriminative phonemes and extract video-related information\nfrom noisy audio, we develop a novel video-guided curriculum learning (VGCL)\nduring the audio pre-training process, which can make use of the vital visual\nperceptions to help understand the spoken language and suppress the external\nnoise. Considering during inference the model can not obtain ground truth video\nsegments, we design a curriculum strategy that gradually shifts the input video\nfrom the ground truth to the entire video content during pre-training. Finally,\nthe model can learn how to extract critical visual information from the entire\nvideo clip to help understand the spoken language. In addition, we collect the\nfirst large-scale spoken video grounding dataset based on ActivityNet, which is\nnamed as ActivityNet Speech dataset. Extensive experiments demonstrate our\nproposed video-guided curriculum learning can facilitate the pre-training\nprocess to obtain a mutual audio encoder, significantly promoting the\nperformance of spoken video grounding tasks. Moreover, we prove that in the\ncase of noisy sound, our model outperforms the method that grounding video with\nASR transcripts, further demonstrating the effectiveness of our curriculum\nstrategy.", "published": "2022-09-01 07:47:01", "link": "http://arxiv.org/abs/2209.00277v1", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Negation detection in Dutch clinical texts: an evaluation of rule-based\n  and machine learning methods", "abstract": "As structured data are often insufficient, labels need to be extracted from\nfree text in electronic health records when developing models for clinical\ninformation retrieval and decision support systems. One of the most important\ncontextual properties in clinical text is negation, which indicates the absence\nof findings. We aimed to improve large scale extraction of labels by comparing\nthree methods for negation detection in Dutch clinical notes. We used the\nErasmus Medical Center Dutch Clinical Corpus to compare a rule-based method\nbased on ContextD, a biLSTM model using MedCAT and (finetuned) RoBERTa-based\nmodels. We found that both the biLSTM and RoBERTa models consistently\noutperform the rule-based model in terms of F1 score, precision and recall. In\naddition, we systematically categorized the classification errors for each\nmodel, which can be used to further improve model performance in particular\napplications. Combining the three models naively was not beneficial in terms of\nperformance. We conclude that the biLSTM and RoBERTa-based models in particular\nare highly accurate accurate in detecting clinical negations, but that\nultimately all three approaches can be viable depending on the use case at\nhand.", "published": "2022-09-01 14:00:13", "link": "http://arxiv.org/abs/2209.00470v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML", "68T50, 68P20", "I.2.7; J.3; H.3.3"], "primary_category": "cs.CL"}
{"title": "Unsupervised Simplification of Legal Texts", "abstract": "The processing of legal texts has been developing as an emerging field in\nnatural language processing (NLP). Legal texts contain unique jargon and\ncomplex linguistic attributes in vocabulary, semantics, syntax, and morphology.\nTherefore, the development of text simplification (TS) methods specific to the\nlegal domain is of paramount importance for facilitating comprehension of legal\ntext by ordinary people and providing inputs to high-level models for\nmainstream legal NLP applications. While a recent study proposed a rule-based\nTS method for legal text, learning-based TS in the legal domain has not been\nconsidered previously. Here we introduce an unsupervised simplification method\nfor legal texts (USLT). USLT performs domain-specific TS by replacing complex\nwords and splitting long sentences. To this end, USLT detects complex words in\na sentence, generates candidates via a masked-transformer model, and selects a\ncandidate for substitution based on a rank score. Afterward, USLT recursively\ndecomposes long sentences into a hierarchy of shorter core and context\nsentences while preserving semantic meaning. We demonstrate that USLT\noutperforms state-of-the-art domain-general TS methods in text simplicity while\nkeeping the semantics intact.", "published": "2022-09-01 15:58:12", "link": "http://arxiv.org/abs/2209.00557v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilling Multi-Scale Knowledge for Event Temporal Relation Extraction", "abstract": "Event Temporal Relation Extraction (ETRE) is paramount but challenging.\nWithin a discourse, event pairs are situated at different distances or the\nso-called proximity bands. The temporal ordering communicated about event pairs\nwhere at more remote (i.e., ``long'') or less remote (i.e., ``short'')\nproximity bands are encoded differently. SOTA models have tended to perform\nwell on events situated at either short or long proximity bands, but not both.\nNonetheless, real-world, natural texts contain all types of temporal\nevent-pairs. In this paper, we present MulCo: Distilling Multi-Scale Knowledge\nvia Contrastive Learning, a knowledge co-distillation approach that shares\nknowledge across multiple event pair proximity bands to improve performance on\nall types of temporal datasets. Our experimental results show that MulCo\nsuccessfully integrates linguistic cues pertaining to temporal reasoning across\nboth short and long proximity bands and achieves new state-of-the-art results\non several ETRE benchmark datasets.", "published": "2022-09-01 16:19:22", "link": "http://arxiv.org/abs/2209.00568v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lip-to-Speech Synthesis for Arbitrary Speakers in the Wild", "abstract": "In this work, we address the problem of generating speech from silent lip\nvideos for any speaker in the wild. In stark contrast to previous works, our\nmethod (i) is not restricted to a fixed number of speakers, (ii) does not\nexplicitly impose constraints on the domain or the vocabulary and (iii) deals\nwith videos that are recorded in the wild as opposed to within laboratory\nsettings. The task presents a host of challenges, with the key one being that\nmany features of the desired target speech, like voice, pitch and linguistic\ncontent, cannot be entirely inferred from the silent face video. In order to\nhandle these stochastic variations, we propose a new VAE-GAN architecture that\nlearns to associate the lip and speech sequences amidst the variations. With\nthe help of multiple powerful discriminators that guide the training process,\nour generator learns to synthesize speech sequences in any voice for the lip\nmovements of any person. Extensive experiments on multiple datasets show that\nwe outperform all baselines by a large margin. Further, our network can be\nfine-tuned on videos of specific identities to achieve a performance comparable\nto single-speaker models that are trained on $4\\times$ more data. We conduct\nnumerous ablation studies to analyze the effect of different modules of our\narchitecture. We also provide a demo video that demonstrates several\nqualitative results along with the code and trained models on our website:\n\\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/lip-to-speech-synthesis}}", "published": "2022-09-01 17:50:29", "link": "http://arxiv.org/abs/2209.00642v1", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment\n  Utterances and a Sampling Strategy for the SASV Challenge 2022", "abstract": "Current state-of-the-art automatic speaker verification (ASV) systems are\nvulnerable to presentation attacks, and several countermeasures (CMs), which\ndistinguish bona fide trials from spoofing ones, have been explored to protect\nASV. However, ASV systems and CMs are generally developed and optimized\nindependently without considering their inter-relationship. In this paper, we\npropose a new spoofing-aware ASV back-end module that efficiently computes a\ncombined ASV score based on speaker similarity and CM score. In addition to the\nlearnable fusion function of the two scores, the proposed back-end module has\ntwo types of attention components, scaled-dot and feed-forward self-attention,\nso that intra-relationship information of multiple enrollment utterances can\nalso be learned at the same time. Moreover, a new effective trials-sampling\nstrategy is designed for simulating new spoofing-aware verification scenarios\nintroduced in the Spoof-Aware Speaker Verification (SASV) challenge 2022.", "published": "2022-09-01 13:01:10", "link": "http://arxiv.org/abs/2209.00423v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On the potential of jointly-optimised solutions to spoofing attack\n  detection and automatic speaker verification", "abstract": "The spoofing-aware speaker verification (SASV) challenge was designed to\npromote the study of jointly-optimised solutions to accomplish the\ntraditionally separately-optimised tasks of spoofing detection and speaker\nverification. Jointly-optimised systems have the potential to operate in\nsynergy as a better performing solution to the single task of reliable speaker\nverification. However, none of the 23 submissions to SASV 2022 are jointly\noptimised. We have hence sought to determine why separately-optimised\nsub-systems perform best or why joint optimisation was not successful.\nExperiments reported in this paper show that joint optimisation is successful\nin improving robustness to spoofing but that it degrades speaker verification\nperformance. The findings suggest that spoofing detection and speaker\nverification sub-systems should be optimised jointly in a manner which reflects\nthe differences in how information provided by each sub-system is complementary\nto that provided by the other. Progress will also likely depend upon the\ncollection of data from a larger number of speakers.", "published": "2022-09-01 14:48:16", "link": "http://arxiv.org/abs/2209.00506v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "diaLogic: Non-Invasive Speaker-Focused Data Acquisition for Team\n  Behavior Modeling", "abstract": "This paper presents diaLogic system, a Human-In-A-Loop system for modeling\nthe behavior of teams during solving open-ended problems. Team behavior is\nmodeled through the hypotheses extracted from features computed from acquired\nvoice data. These features include speaker interactions, speaker emotions,\nfundamental frequencies, and the corresponding text and clauses. Hypotheses\nabout the invariant and differentiated situations are found based on the\nsimilarities and dissimilarities of the behavior of teams over time. To provide\nfull automation of data acquisition, the diaLogic system is executed within an\nintuitive, user-friendly GUI interface. Experiments present the performance of\nthe system for a broad set of cases featuring team behavior during problem\nsolving.", "published": "2022-09-01 17:33:11", "link": "http://arxiv.org/abs/2209.00619v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Joint Speaker Encoder and Neural Back-end Model for Fully End-to-End\n  Automatic Speaker Verification with Multiple Enrollment Utterances", "abstract": "Conventional automatic speaker verification systems can usually be decomposed\ninto a front-end model such as time delay neural network (TDNN) for extracting\nspeaker embeddings and a back-end model such as statistics-based probabilistic\nlinear discriminant analysis (PLDA) or neural network-based neural PLDA (NPLDA)\nfor similarity scoring. However, the sequential optimization of the front-end\nand back-end models may lead to a local minimum, which theoretically prevents\nthe whole system from achieving the best optimization. Although some methods\nhave been proposed for jointly optimizing the two models, such as the\ngeneralized end-to-end (GE2E) model and NPLDA E2E model, all of these methods\nare designed for use with a single enrollment utterance. In this paper, we\npropose a new E2E joint method for speaker verification especially designed for\nthe practical case of multiple enrollment utterances. In order to leverage the\nintra-relationship among multiple enrollment utterances, our model comes\nequipped with frame-level and utterance-level attention mechanisms. We also\nutilize several data augmentation techniques, including conventional noise\naugmentation using MUSAN and RIRs datasets and a unique speaker embedding-level\nmixup strategy for better optimization.", "published": "2022-09-01 14:15:47", "link": "http://arxiv.org/abs/2209.00485v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Wavelet Transform Based Scheme to Extract Speech Pitch and Formant\n  Frequencies", "abstract": "Pitch and Formant frequencies are important features in speech processing\napplications. The period of the vocal cord's output for vowels is known as the\npitch or the fundamental frequency, and formant frequencies are essentially\nresonance frequencies of the vocal tract. These features vary among different\npersons and even words, but they are within a certain frequency range. In\npractice, just the first three formants are enough for the most of speech\nprocessing. Feature extraction and classification are the main components of\neach speech recognition system. In this article, two wavelet based approaches\nare proposed to extract the mentioned features with help of the filter bank\nidea. By comparing the results of the presented feature extraction methods on\nseveral speech signals, it was found out that the wavelet transform has a good\naccuracy compared to the cepstrum method and it has no sensitivity to noise. In\naddition, several fuzzy based classification techniques for speech processing\nare reviewed.", "published": "2022-09-01 21:32:36", "link": "http://arxiv.org/abs/2209.00733v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "What is missing in deep music generation? A study of repetition and\n  structure in popular music", "abstract": "Structure is one of the most essential aspects of music, and music structure\nis commonly indicated through repetition. However, the nature of repetition and\nstructure in music is still not well understood, especially in the context of\nmusic generation, and much remains to be explored with Music Information\nRetrieval (MIR) techniques. Analyses of two popular music datasets (Chinese and\nAmerican) illustrate important music construction principles: (1) structure\nexists at multiple hierarchical levels, (2) songs use repetition and limited\nvocabulary so that individual songs do not follow general statistics of song\ncollections, (3) structure interacts with rhythm, melody, harmony, and\npredictability, and (4) over the course of a song, repetition is not random,\nbut follows a general trend as revealed by cross-entropy. These and other\nfindings offer challenges as well as opportunities for deep-learning music\ngeneration and suggest new formal music criteria and evaluation methods. Music\nfrom recent music generation systems is analyzed and compared to human-composed\nmusic in our datasets, often revealing striking differences from a structural\nperspective.", "published": "2022-09-01 02:22:11", "link": "http://arxiv.org/abs/2209.00182v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generating Coherent Drum Accompaniment With Fills And Improvisations", "abstract": "Creating a complex work of art like music necessitates profound creativity.\nWith recent advancements in deep learning and powerful models such as\ntransformers, there has been huge progress in automatic music generation. In an\naccompaniment generation context, creating a coherent drum pattern with\napposite fills and improvisations at proper locations in a song is a\nchallenging task even for an experienced drummer. Drum beats tend to follow a\nrepetitive pattern through stanzas with fills or improvisation at section\nboundaries. In this work, we tackle the task of drum pattern generation\nconditioned on the accompanying music played by four melodic instruments:\nPiano, Guitar, Bass, and Strings. We use the transformer sequence to sequence\nmodel to generate a basic drum pattern conditioned on the melodic accompaniment\nto find that improvisation is largely absent, attributed possibly to its\nexpectedly relatively low representation in the training data. We propose a\nnovelty function to capture the extent of improvisation in a bar relative to\nits neighbors. We train a model to predict improvisation locations from the\nmelodic accompaniment tracks. Finally, we use a novel BERT-inspired in-filling\narchitecture, to learn the structure of both the drums and melody to in-fill\nelements of improvised music.", "published": "2022-09-01 08:31:26", "link": "http://arxiv.org/abs/2209.00291v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AccoMontage2: A Complete Harmonization and Accompaniment Arrangement\n  System", "abstract": "We propose AccoMontage2, a system capable of doing full-length song\nharmonization and accompaniment arrangement based on a lead melody. Following\nAccoMontage, this study focuses on generating piano arrangements for\npopular/folk songs and it carries on the generalized template-based retrieval\nmethod. The novelties of this study are twofold. First, we invent a\nharmonization module (which AccoMontage does not have). This module generates\nstructured and coherent full-length chord progression by optimizing and\nbalancing three loss terms: a micro-level loss for note-wise dissonance, a\nmeso-level loss for phrase-template matching, and a macro-level loss for full\npiece coherency. Second, we develop a graphical user interface which allows\nusers to select different styles of chord progression and piano texture.\nCurrently, chord progression styles include Pop, R&B, and Dark, while piano\ntexture styles include several levels of voicing density and rhythmic\ncomplexity. Experimental results show that both our harmonization and\narrangement results significantly outperform the baselines. Lastly, we release\nAccoMontage2 as an online application as well as the organized chord\nprogression templates as a public dataset.", "published": "2022-09-01 10:42:56", "link": "http://arxiv.org/abs/2209.00353v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
