{"title": "Open-domain Event Extraction and Embedding for Natural Gas Market\n  Prediction", "abstract": "We propose an approach to predict the natural gas price in several days using\nhistorical price data and events extracted from news headlines. Most previous\nmethods treats price as an extrapolatable time series, those analyze the\nrelation between prices and news either trim their price data correspondingly\nto a public news dataset, manually annotate headlines or use off-the-shelf\ntools. In comparison to off-the-shelf tools, our event extraction method\ndetects not only the occurrence of phenomena but also the changes in\nattribution and characteristics from public sources. Instead of using sentence\nembedding as a feature, we use every word of the extracted events, encode and\norganize them before feeding to the learning models. Empirical results show\nfavorable results, in terms of prediction performance, money saved and\nscalability.", "published": "2019-12-08 10:19:40", "link": "http://arxiv.org/abs/1912.11334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi Purpose and Large Scale Speech Corpus in Persian and English for\n  Speaker and Speech Recognition: the DeepMine Database", "abstract": "DeepMine is a speech database in Persian and English designed to build and\nevaluate text-dependent, text-prompted, and text-independent speaker\nverification, as well as Persian speech recognition systems. It contains more\nthan 1850 speakers and 540 thousand recordings overall, more than 480 hours of\nspeech are transcribed. It is the first public large-scale speaker verification\ndatabase in Persian, the largest public text-dependent and text-prompted\nspeaker verification database in English, and the largest public evaluation\ndataset for text-independent speaker verification. It has a good coverage of\nage, gender, and accents. We provide several evaluation protocols for each part\nof the database to allow for research on different aspects of speaker\nverification. We also provide the results of several experiments that can be\nconsidered as baselines: HMM-based i-vectors for text-dependent speaker\nverification, and HMM-based as well as state-of-the-art deep neural network\nbased ASR. We demonstrate that the database can serve for training robust ASR\nmodels.", "published": "2019-12-08 07:05:36", "link": "http://arxiv.org/abs/1912.03627v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bidirectional Scene Text Recognition with a Single Decoder", "abstract": "Scene Text Recognition (STR) is the problem of recognizing the correct word\nor character sequence in a cropped word image. To obtain more robust output\nsequences, the notion of bidirectional STR has been introduced. So far,\nbidirectional STRs have been implemented by using two separate decoders; one\nfor left-to-right decoding and one for right-to-left. Having two separate\ndecoders for almost the same task with the same output space is undesirable\nfrom a computational and optimization point of view. We introduce the\nbidirectional Scene Text Transformer (Bi-STET), a novel bidirectional STR\nmethod with a single decoder for bidirectional text decoding. With its single\ndecoder, Bi-STET outperforms methods that apply bidirectional decoding by using\ntwo separate decoders while also being more efficient than those methods,\nFurthermore, we achieve or beat state-of-the-art (SOTA) methods on all STR\nbenchmarks with Bi-STET. Finally, we provide analyses and insights into the\nperformance of Bi-STET.", "published": "2019-12-08 11:20:35", "link": "http://arxiv.org/abs/1912.03656v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Attentive Representation Learning with Adversarial Training for Short\n  Text Clustering", "abstract": "Short text clustering has far-reaching effects on semantic analysis, showing\nits importance for multiple applications such as corpus summarization and\ninformation retrieval. However, it inevitably encounters the severe sparsity of\nshort text representations, making the previous clustering approaches still far\nfrom satisfactory. In this paper, we present a novel attentive representation\nlearning model for shot text clustering, wherein cluster-level attention is\nproposed to capture the correlations between text representations and cluster\nrepresentations. Relying on this, the representation learning and clustering\nfor short texts are seamlessly integrated into a unified model. To further\nensure robust model training for short texts, we apply adversarial training to\nthe unsupervised clustering setting, by injecting perturbations into the\ncluster representations. The model parameters and perturbations are optimized\nalternately through a minimax game. Extensive experiments on four real-world\nshort text datasets demonstrate the superiority of the proposed model over\nseveral strong competitors, verifying that robust adversarial training yields\nsubstantial performance gains.", "published": "2019-12-08 17:22:07", "link": "http://arxiv.org/abs/1912.03720v2", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "A Supervised Speech enhancement Approach with Residual Noise Control for\n  Voice Communication", "abstract": "For voice communication, it is important to extract the speech from its noisy\nversion without introducing unnaturally artificial noise. By studying the\nsubband mean-squared error (MSE) of the speech for unsupervised speech\nenhancement approaches and revealing its relationship with the existing loss\nfunction for supervised approaches, this paper derives a generalized loss\nfunction, when taking the residual noise control into account, for supervised\napproaches. Our generalized loss function contains the well-known MSE loss\nfunction and many other often-used loss functions as special cases. Compared\nwith traditional loss functions, our generalized loss function is more flexible\nto make a good trade-off between speech distortion and noise reduction. This is\nbecause a group of well-studied noise shaping schemes can be introduced to\ncontrol residual noise for practical applications. Objective and subjective\ntest results verify the importance of residual noise control for the supervised\nspeech enhancement approach.", "published": "2019-12-08 13:50:06", "link": "http://arxiv.org/abs/1912.03679v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
