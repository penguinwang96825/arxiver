{"title": "A preliminary study of Croatian Language Syllable Networks", "abstract": "This paper presents preliminary results of Croatian syllable networks\nanalysis. Syllable network is a network in which nodes are syllables and links\nbetween them are constructed according to their connections within words. In\nthis paper we analyze networks of syllables generated from texts collected from\nthe Croatian Wikipedia and Blogs. As a main tool we use complex network\nanalysis methods which provide mechanisms that can reveal new patterns in a\nlanguage structure. We aim to show that syllable networks have much higher\nclustering coefficient in comparison to Erd\\\"os-Renyi random networks. The\nresults indicate that Croatian syllable networks exhibit certain properties of\na small world networks. Furthermore, we compared Croatian syllable networks\nwith Portuguese and Chinese syllable networks and we showed that they have\nsimilar properties.", "published": "2014-05-16 08:57:40", "link": "http://arxiv.org/abs/1405.4097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Les math\u00e9matiques de la langue : l'approche formelle de Montague", "abstract": "We present a natural language modelization method which is strongely relying\non mathematics. This method, called \"Formal Semantics,\" has been initiated by\nthe American linguist Richard M. Montague in the 1970's. It uses mathematical\ntools such as formal languages and grammars, first-order logic, type theory and\n$\\lambda$-calculus. Our goal is to have the reader discover both Montagovian\nformal semantics and the mathematical tools that he used in his method.\n  -----\n  Nous pr\\'esentons une m\\'ethode de mod\\'elisation de la langue naturelle qui\nest fortement bas\\'ee sur les math\\'ematiques. Cette m\\'ethode, appel\\'ee\n{\\guillemotleft}s\\'emantique formelle{\\guillemotright}, a \\'et\\'e initi\\'ee par\nle linguiste am\\'ericain Richard M. Montague dans les ann\\'ees 1970. Elle\nutilise des outils math\\'ematiques tels que les langages et grammaires formels,\nla logique du 1er ordre, la th\\'eorie de types et le $\\lambda$-calcul. Nous\nnous proposons de faire d\\'ecouvrir au lecteur tant la s\\'emantique formelle de\nMontague que les outils math\\'ematiques dont il s'est servi.", "published": "2014-05-16 17:17:19", "link": "http://arxiv.org/abs/1405.4248v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Compositional Morphology for Word Representations and Language Modelling", "abstract": "This paper presents a scalable method for integrating compositional\nmorphological representations into a vector-based probabilistic language model.\nOur approach is evaluated in the context of log-bilinear language models,\nrendered suitably efficient for implementation inside a machine translation\ndecoder by factoring the vocabulary. We perform both intrinsic and extrinsic\nevaluations, presenting results on a range of languages which demonstrate that\nour model learns morphological representations that both perform well on word\nsimilarity tasks and lead to substantial reductions in perplexity. When used\nfor translation into morphologically rich languages with large vocabularies,\nour models obtain improvements of up to 1.2 BLEU points relative to a baseline\nsystem using back-off n-gram models.", "published": "2014-05-16 19:08:14", "link": "http://arxiv.org/abs/1405.4273v1", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Distributed Representations of Sentences and Documents", "abstract": "Many machine learning algorithms require the input to be represented as a\nfixed-length feature vector. When it comes to texts, one of the most common\nfixed-length features is bag-of-words. Despite their popularity, bag-of-words\nfeatures have two major weaknesses: they lose the ordering of the words and\nthey also ignore semantics of the words. For example, \"powerful,\" \"strong\" and\n\"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an\nunsupervised algorithm that learns fixed-length feature representations from\nvariable-length pieces of texts, such as sentences, paragraphs, and documents.\nOur algorithm represents each document by a dense vector which is trained to\npredict words in the document. Its construction gives our algorithm the\npotential to overcome the weaknesses of bag-of-words models. Empirical results\nshow that Paragraph Vectors outperform bag-of-words models as well as other\ntechniques for text representations. Finally, we achieve new state-of-the-art\nresults on several text classification and sentiment analysis tasks.", "published": "2014-05-16 07:12:16", "link": "http://arxiv.org/abs/1405.4053v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
