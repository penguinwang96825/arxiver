{"title": "Streaming Punctuation: A Novel Punctuation Technique Leveraging\n  Bidirectional Context for Continuous Speech Recognition", "abstract": "While speech recognition Word Error Rate (WER) has reached human parity for\nEnglish, continuous speech recognition scenarios such as voice typing and\nmeeting transcriptions still suffer from segmentation and punctuation problems,\nresulting from irregular pausing patterns or slow speakers. Transformer\nsequence tagging models are effective at capturing long bi-directional context,\nwhich is crucial for automatic punctuation. Automatic Speech Recognition (ASR)\nproduction systems, however, are constrained by real-time requirements, making\nit hard to incorporate the right context when making punctuation decisions.\nContext within the segments produced by ASR decoders can be helpful but\nlimiting in overall punctuation performance for a continuous speech session. In\nthis paper, we propose a streaming approach for punctuation or re-punctuation\nof ASR output using dynamic decoding windows and measure its impact on\npunctuation and segmentation accuracy across scenarios. The new system tackles\nover-segmentation issues, improving segmentation F0.5-score by 13.9%. Streaming\npunctuation achieves an average BLEUscore improvement of 0.66 for the\ndownstream task of Machine Translation (MT).", "published": "2023-01-10 07:07:20", "link": "http://arxiv.org/abs/2301.03819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Mandarin-Cantonese Machine Translation", "abstract": "Advancements in unsupervised machine translation have enabled the development\nof machine translation systems that can translate between languages for which\nthere is not an abundance of parallel data available. We explored unsupervised\nmachine translation between Mandarin Chinese and Cantonese. Despite the vast\nnumber of native speakers of Cantonese, there is still no large-scale corpus\nfor the language, due to the fact that Cantonese is primarily used for oral\ncommunication. The key contributions of our project include: 1. The creation of\na new corpus containing approximately 1 million Cantonese sentences, and 2. A\nlarge-scale comparison across different model architectures, tokenization\nschemes, and embedding structures. Our best model trained with character-based\ntokenization and a Transformer architecture achieved a character-level BLEU of\n25.1 when translating from Mandarin to Cantonese and of 24.4 when translating\nfrom Cantonese to Mandarin. In this paper we discuss our research process,\nexperiments, and results.", "published": "2023-01-10 14:09:40", "link": "http://arxiv.org/abs/2301.03971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Conversational Search Behavior For Domain Exploration", "abstract": "Conversational search has evolved as a new information retrieval paradigm,\nmarking a shift from traditional search systems towards interactive dialogues\nwith intelligent search agents. This change especially affects exploratory\ninformation-seeking contexts, where conversational search systems can guide the\ndiscovery of unfamiliar domains. In these scenarios, users find it often\ndifficult to express their information goals due to insufficient background\nknowledge. Conversational interfaces can provide assistance by eliciting\ninformation needs and narrowing down the search space. However, due to the\ncomplexity of information-seeking behavior, the design of conversational\ninterfaces for retrieving information remains a great challenge. Although prior\nwork has employed user studies to empirically ground the system design, most\nexisting studies are limited to well-defined search tasks or known domains,\nthus being less exploratory in nature. Therefore, we conducted a laboratory\nstudy to investigate open-ended search behavior for navigation through unknown\ninformation landscapes. The study comprised of 26 participants who were\nrestricted in their search to a text chat interface. Based on the collected\ndialogue transcripts, we applied statistical analyses and process mining\ntechniques to uncover general information-seeking patterns across five\ndifferent domains. We not only identify core dialogue acts and their\ninterrelations that enable users to discover domain knowledge, but also derive\ndesign suggestions for conversational search systems.", "published": "2023-01-10 17:43:03", "link": "http://arxiv.org/abs/2301.04098v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neighborhood-Regularized Self-Training for Learning with Few Labels", "abstract": "Training deep neural networks (DNNs) with limited supervision has been a\npopular research topic as it can significantly alleviate the annotation burden.\nSelf-training has been successfully applied in semi-supervised learning tasks,\nbut one drawback of self-training is that it is vulnerable to the label noise\nfrom incorrect pseudo labels. Inspired by the fact that samples with similar\nlabels tend to share similar representations, we develop a neighborhood-based\nsample selection approach to tackle the issue of noisy pseudo labels. We\nfurther stabilize self-training via aggregating the predictions from different\nrounds during sample selection. Experiments on eight tasks show that our\nproposed method outperforms the strongest self-training baseline with 1.83% and\n2.51% performance gain for text and graph datasets on average. Our further\nanalysis demonstrates that our proposed data selection strategy reduces the\nnoise of pseudo labels by 36.8% and saves 57.3% of the time when compared with\nthe best baseline. Our code and appendices will be uploaded to\nhttps://github.com/ritaranx/NeST.", "published": "2023-01-10 00:07:33", "link": "http://arxiv.org/abs/2301.03726v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The use of new technologies to support Public Administration. Sentiment\n  analysis and the case of the app IO", "abstract": "App IO is an app developed for the Italian PA. It is definitely useful for\ncitizens to interact with the PA and to get services that were not digitized\nyet. Nevertheless, it was not perceived in a good way by the citizens and it\nhas been criticized. As we wanted to find the root that caused all these bad\nreviews we scraped feedback from mobile app stores using custom-coded automated\ntools and - after that - we trained two machine learning models to perform both\nsentiment analysis and emotion detection to understand what caused the bad\nreviews.", "published": "2023-01-10 08:41:25", "link": "http://arxiv.org/abs/2301.03848v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "AI based approach to Trailer Generation for Online Educational Courses", "abstract": "In this paper, we propose an AI based approach to Trailer Generation in the\nform of short videos for online educational courses. Trailers give an overview\nof the course to the learners and help them make an informed choice about the\ncourses they want to learn. It also helps to generate curiosity and interest\namong the learners and encourages them to pursue a course. While it is possible\nto manually generate the trailers, it requires extensive human efforts and\nskills over a broad spectrum of design, span selection, video editing, domain\nknowledge, etc., thus making it time-consuming and expensive, especially in an\nacademic setting. The framework we propose in this work is a template based\nmethod for video trailer generation, where most of the textual content of the\ntrailer is auto-generated and the trailer video is automatically generated, by\nleveraging Machine Learning and Natural Language Processing techniques. The\nproposed trailer is in the form of a timeline consisting of various fragments\ncreated by selecting, para-phrasing or generating content using various\nproposed techniques. The fragments are further enhanced by adding voice-over\ntext, subtitles, animations, etc., to create a holistic experience. Finally, we\nperform user evaluation with 63 human evaluators for evaluating the trailers\ngenerated by our system and the results obtained were encouraging.", "published": "2023-01-10 13:33:08", "link": "http://arxiv.org/abs/2301.03957v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models sounds the Death Knell of Knowledge Graphs", "abstract": "Healthcare domain generates a lot of unstructured and semi-structured text.\nNatural Language processing (NLP) has been used extensively to process this\ndata. Deep Learning based NLP especially Large Language Models (LLMs) such as\nBERT have found broad acceptance and are used extensively for many\napplications. A Language Model is a probability distribution over a word\nsequence. Self-supervised Learning on a large corpus of data automatically\ngenerates deep learning-based language models. BioBERT and Med-BERT are\nlanguage models pre-trained for the healthcare domain. Healthcare uses typical\nNLP tasks such as question answering, information extraction, named entity\nrecognition, and search to simplify and improve processes. However, to ensure\nrobust application of the results, NLP practitioners need to normalize and\nstandardize them. One of the main ways of achieving normalization and\nstandardization is the use of Knowledge Graphs. A Knowledge Graph captures\nconcepts and their relationships for a specific domain, but their creation is\ntime-consuming and requires manual intervention from domain experts, which can\nprove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical\nTerms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are\npopular ontologies from the healthcare domain. SNOMED CT and UMLS capture\nconcepts such as disease, symptoms and diagnosis and GO is the world's largest\nsource of information on the functions of genes. Healthcare has been dealing\nwith an explosion in information about different types of drugs, diseases, and\nprocedures. This paper argues that using Knowledge Graphs is not the best\nsolution for solving problems in this domain. We present experiments using LLMs\nfor the healthcare domain to demonstrate that language models provide the same\nfunctionality as knowledge graphs, thereby making knowledge graphs redundant.", "published": "2023-01-10 14:20:15", "link": "http://arxiv.org/abs/2301.03980v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structured Case-based Reasoning for Inference-time Adaptation of\n  Text-to-SQL parsers", "abstract": "Inference-time adaptation methods for semantic parsing are useful for\nleveraging examples from newly-observed domains without repeated fine-tuning.\nExisting approaches typically bias the decoder by simply concatenating\ninput-output example pairs (cases) from the new domain at the encoder's input\nin a Seq-to-Seq model. Such methods cannot adequately leverage the structure of\nlogical forms in the case examples. We propose StructCBR, a structured\ncase-based reasoning approach, which leverages subtree-level similarity between\nlogical forms of cases and candidate outputs, resulting in better decoder\ndecisions. For the task of adapting Text-to-SQL models to unseen schemas, we\nshow that exploiting case examples in a structured manner via StructCBR offers\nconsistent performance improvements over prior inference-time adaptation\nmethods across five different databases. To the best of our knowledge, we are\nthe first to attempt inference-time adaptation of Text-to-SQL models, and\nharness trainable structured similarity between subqueries.", "published": "2023-01-10 18:20:42", "link": "http://arxiv.org/abs/2301.04110v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "User-Centered Security in Natural Language Processing", "abstract": "This dissertation proposes a framework of user-centered security in Natural\nLanguage Processing (NLP), and demonstrates how it can improve the\naccessibility of related research. Accordingly, it focuses on two security\ndomains within NLP with great public interest. First, that of author profiling,\nwhich can be employed to compromise online privacy through invasive inferences.\nWithout access and detailed insight into these models' predictions, there is no\nreasonable heuristic by which Internet users might defend themselves from such\ninferences. Secondly, that of cyberbullying detection, which by default\npresupposes a centralized implementation; i.e., content moderation across\nsocial platforms. As access to appropriate data is restricted, and the nature\nof the task rapidly evolves (both through lexical variation, and cultural\nshifts), the effectiveness of its classifiers is greatly diminished and thereby\noften misrepresented.\n  Under the proposed framework, we predominantly investigate the use of\nadversarial attacks on language; i.e., changing a given input (generating\nadversarial samples) such that a given model does not function as intended.\nThese attacks form a common thread between our user-centered security problems;\nthey are highly relevant for privacy-preserving obfuscation methods against\nauthor profiling, and adversarial samples might also prove useful to assess the\ninfluence of lexical variation and augmentation on cyberbullying detection.", "published": "2023-01-10 22:34:19", "link": "http://arxiv.org/abs/2301.04230v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Memory Augmented Large Language Models are Computationally Universal", "abstract": "We show that transformer-based large language models are computationally\nuniversal when augmented with an external memory. Any deterministic language\nmodel that conditions on strings of bounded length is equivalent to a finite\nautomaton, hence computationally limited. However, augmenting such models with\na read-write memory creates the possibility of processing arbitrarily large\ninputs and, potentially, simulating any algorithm. We establish that an\nexisting large language model, Flan-U-PaLM 540B, can be combined with an\nassociative read-write memory to exactly simulate the execution of a universal\nTuring machine, $U_{15,2}$. A key aspect of the finding is that it does not\nrequire any modification of the language model weights. Instead, the\nconstruction relies solely on designing a form of stored instruction computer\nthat can subsequently be programmed with a specific set of prompts.", "published": "2023-01-10 02:37:44", "link": "http://arxiv.org/abs/2301.04589v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Generative Mixed-Modal Language Models", "abstract": "Generative language models define distributions over sequences of tokens that\ncan represent essentially any combination of data modalities (e.g., any\npermutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens\nfor language or code, and so on). To better understand the scaling properties\nof such mixed-modal models, we conducted over 250 experiments using seven\ndifferent modalities and model sizes ranging from 8 million to 30 billion,\ntrained on 5-100 billion tokens. We report new mixed-modal scaling laws that\nunify the contributions of individual modalities and the interactions between\nthem. Specifically, we explicitly model the optimal synergy and competition due\nto data and model size as an additive term to previous uni-modal scaling laws.\nWe also find four empirical phenomena observed during the training, such as\nemergent coordinate-ascent style training that naturally alternates between\nmodalities, guidelines for selecting critical hyper-parameters, and connections\nbetween mixed-modal competition and training stability. Finally, we test our\nscaling law by training a 30B speech-text model, which significantly\noutperforms the corresponding unimodal models. Overall, our research provides\nvaluable insights into the design and training of mixed-modal generative\nmodels, an important new class of unified models that have unique\ndistributional properties.", "published": "2023-01-10 00:20:06", "link": "http://arxiv.org/abs/2301.03728v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language\n  Understanding", "abstract": "Current natural language understanding (NLU) models have been continuously\nscaling up, both in terms of model size and input context, introducing more\nhidden and input neurons. While this generally improves performance on average,\nthe extra neurons do not yield a consistent improvement for all instances. This\nis because some hidden neurons are redundant, and the noise mixed in input\nneurons tends to distract the model. Previous work mainly focuses on\nextrinsically reducing low-utility neurons by additional post- or\npre-processing, such as network pruning and context selection, to avoid this\nproblem. Beyond that, can we make the model reduce redundant parameters and\nsuppress input noise by intrinsically enhancing the utility of each neuron? If\na model can efficiently utilize neurons, no matter which neurons are ablated\n(disabled), the ablated submodel should perform no better than the original\nfull model. Based on such a comparison principle between models, we propose a\ncross-model comparative loss for a broad range of tasks. Comparative loss is\nessentially a ranking loss on top of the task-specific losses of the full and\nablated models, with the expectation that the task-specific loss of the full\nmodel is minimal. We demonstrate the universal effectiveness of comparative\nloss through extensive experiments on 14 datasets from 3 distinct NLU tasks\nbased on 5 widely used pretrained language models and find it particularly\nsuperior for models with few parameters or long input.", "published": "2023-01-10 03:04:27", "link": "http://arxiv.org/abs/2301.03765v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UnifySpeech: A Unified Framework for Zero-shot Text-to-Speech and Voice\n  Conversion", "abstract": "Text-to-speech (TTS) and voice conversion (VC) are two different tasks both\naiming at generating high quality speaking voice according to different input\nmodality. Due to their similarity, this paper proposes UnifySpeech, which\nbrings TTS and VC into a unified framework for the first time. The model is\nbased on the assumption that speech can be decoupled into three independent\ncomponents: content information, speaker information, prosody information. Both\nTTS and VC can be regarded as mining these three parts of information from the\ninput and completing the reconstruction of speech. For TTS, the speech content\ninformation is derived from the text, while in VC it's derived from the source\nspeech, so all the remaining units are shared except for the speech content\nextraction module in the two tasks. We applied vector quantization and domain\nconstrain to bridge the gap between the content domains of TTS and VC.\nObjective and subjective evaluation shows that by combining the two task, TTS\nobtains better speaker modeling ability while VC gets hold of impressive speech\ncontent decoupling capability.", "published": "2023-01-10 06:06:57", "link": "http://arxiv.org/abs/2301.03801v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Channel-aware Decoupling Network for Multi-turn Dialogue Comprehension", "abstract": "Training machines to understand natural language and interact with humans is\none of the major goals of artificial intelligence. Recent years have witnessed\nan evolution from matching networks to pre-trained language models (PrLMs). In\ncontrast to the plain-text modeling as the focus of the PrLMs, dialogue texts\ninvolve multiple speakers and reflect special characteristics such as topic\ntransitions and structure dependencies between distant utterances. However, the\nrelated PrLM models commonly represent dialogues sequentially by processing the\npairwise dialogue history as a whole. Thus the hierarchical information on\neither utterance interrelation or speaker roles coupled in such representations\nis not well addressed. In this work, we propose compositional learning for\nholistic interaction across the utterances beyond the sequential\ncontextualization from PrLMs, in order to capture the utterance-aware and\nspeaker-aware representations entailed in a dialogue history. We decouple the\ncontextualized word representations by masking mechanisms in Transformer-based\nPrLM, making each word only focus on the words in current utterance, other\nutterances, and two speaker roles (i.e., utterances of sender and utterances of\nthe receiver), respectively. In addition, we employ domain-adaptive training\nstrategies to help the model adapt to the dialogue domains. Experimental\nresults show that our method substantially boosts the strong PrLM baselines in\nfour public benchmark datasets, achieving new state-of-the-art performance over\nprevious methods.", "published": "2023-01-10 13:18:25", "link": "http://arxiv.org/abs/2301.03953v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "There is No Big Brother or Small Brother: Knowledge Infusion in Language\n  Models for Link Prediction and Question Answering", "abstract": "The integration of knowledge graphs with deep learning is thriving in\nimproving the performance of various natural language processing (NLP) tasks.\nIn this paper, we focus on knowledge-infused link prediction and question\nanswering using language models, T5, and BLOOM across three domains: Aviation,\nMovie, and Web. In this context, we infuse knowledge in large and small\nlanguage models and study their performance, and find the performance to be\nsimilar. For the link prediction task on the Aviation Knowledge Graph, we\nobtain a 0.2 hits@1 score using T5-small, T5-base, T5-large, and BLOOM. Using\ntemplate-based scripts, we create a set of 1 million synthetic factoid QA pairs\nin the aviation domain from National Transportation Safety Board (NTSB)\nreports. On our curated QA pairs, the three models of T5 achieve a 0.7 hits@1\nscore. We validate out findings with the paired student t-test and Cohen's\nkappa scores. For link prediction on Aviation Knowledge Graph using T5-small\nand T5-large, we obtain a Cohen's kappa score of 0.76, showing substantial\nagreement between the models. Thus, we infer that small language models perform\nsimilar to large language models with the infusion of knowledge.", "published": "2023-01-10 14:59:33", "link": "http://arxiv.org/abs/2301.04013v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Localization Inform Editing? Surprising Differences in\n  Causality-Based Localization vs. Knowledge Editing in Language Models", "abstract": "Language models learn a great quantity of factual information during\npretraining, and recent work localizes this information to specific model\nweights like mid-layer MLP weights. In this paper, we find that we can change\nhow a fact is stored in a model by editing weights that are in a different\nlocation than where existing methods suggest that the fact is stored. This is\nsurprising because we would expect that localizing facts to specific model\nparameters would tell us where to manipulate knowledge in models, and this\nassumption has motivated past work on model editing methods. Specifically, we\nshow that localization conclusions from representation denoising (also known as\nCausal Tracing) do not provide any insight into which model MLP layer would be\nbest to edit in order to override an existing stored fact with a new one. This\nfinding raises questions about how past work relies on Causal Tracing to select\nwhich model layers to edit. Next, we consider several variants of the editing\nproblem, including erasing and amplifying facts. For one of our editing\nproblems, editing performance does relate to localization results from\nrepresentation denoising, but we find that which layer we edit is a far better\npredictor of performance. Our results suggest, counterintuitively, that better\nmechanistic understanding of how pretrained language models work may not always\ntranslate to insights about how to best change their behavior. Our code is\navailable at https://github.com/google/belief-localization", "published": "2023-01-10 21:26:08", "link": "http://arxiv.org/abs/2301.04213v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Predicting Hateful Discussions on Reddit using Graph Transformer\n  Networks and Communal Context", "abstract": "We propose a system to predict harmful discussions on social media platforms.\nOur solution uses contextual deep language models and proposes the novel idea\nof integrating state-of-the-art Graph Transformer Networks to analyze all\nconversations that follow an initial post. This framework also supports\nadapting to future comments as the conversation unfolds. In addition, we study\nwhether a community-specific analysis of hate speech leads to more effective\ndetection of hateful discussions. We evaluate our approach on 333,487 Reddit\ndiscussions from various communities. We find that community-specific modeling\nimproves performance two-fold and that models which capture wider-discussion\ncontext improve accuracy by 28\\% (35\\% for the most hateful content) compared\nto limited context models.", "published": "2023-01-10 23:47:13", "link": "http://arxiv.org/abs/2301.04248v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "AI Insights into Theoretical Physics and the Swampland Program: A\n  Journey Through the Cosmos with ChatGPT", "abstract": "In this case study, we explore the capabilities and limitations of ChatGPT, a\nnatural language processing model developed by OpenAI, in the field of string\ntheoretical swampland conjectures. We find that it is effective at paraphrasing\nand explaining concepts in a variety of styles, but not at genuinely connecting\nconcepts. It will provide false information with full confidence and make up\nstatements when necessary. However, its ingenious use of language can be\nfruitful for identifying analogies and describing visual representations of\nabstract concepts.", "published": "2023-01-10 16:57:16", "link": "http://arxiv.org/abs/2301.08155v1", "categories": ["physics.pop-ph", "cs.AI", "cs.CL"], "primary_category": "physics.pop-ph"}
{"title": "Generative Emotional AI for Speech Emotion Recognition: The Case for\n  Synthetic Emotional Speech Augmentation", "abstract": "Despite advances in deep learning, current state-of-the-art speech emotion\nrecognition (SER) systems still have poor performance due to a lack of speech\nemotion datasets. This paper proposes augmenting SER systems with synthetic\nemotional speech generated by an end-to-end text-to-speech (TTS) system based\non an extended Tacotron architecture. The proposed TTS system includes encoders\nfor speaker and emotion embeddings, a sequence-to-sequence text generator for\ncreating Mel-spectrograms, and a WaveRNN to generate audio from the\nMel-spectrograms. Extensive experiments show that the quality of the generated\nemotional speech can significantly improve SER performance on multiple\ndatasets, as demonstrated by a higher mean opinion score (MOS) compared to the\nbaseline. The generated samples were also effective at augmenting SER\nperformance.", "published": "2023-01-10 02:03:26", "link": "http://arxiv.org/abs/2301.03751v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Driven Video Editing via an Audio-Conditioned Diffusion Model", "abstract": "Taking inspiration from recent developments in visual generative tasks using\ndiffusion models, we propose a method for end-to-end speech-driven video\nediting using a denoising diffusion model. Given a video of a talking person,\nand a separate auditory speech recording, the lip and jaw motions are\nre-synchronized without relying on intermediate structural representations such\nas facial landmarks or a 3D face model. We show this is possible by\nconditioning a denoising diffusion model on audio mel spectral features to\ngenerate synchronised facial motion. Proof of concept results are demonstrated\non both single-speaker and multi-speaker video editing, providing a baseline\nmodel on the CREMA-D audiovisual data set. To the best of our knowledge, this\nis the first work to demonstrate and validate the feasibility of applying\nend-to-end denoising diffusion models to the task of audio-driven video\nediting.", "published": "2023-01-10 12:01:20", "link": "http://arxiv.org/abs/2301.04474v3", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
