{"title": "ChatGPT and Corporate Policies", "abstract": "We create a firm-level ChatGPT investment score, based on conference calls,\nthat measures managers' anticipated changes in capital expenditures. We\nvalidate the score with interpretable textual content and its strong\ncorrelation with CFO survey responses. The investment score predicts future\ncapital expenditure for up to nine quarters, controlling for Tobin's $q$ and\nother determinants, implying the investment score provides incremental\ninformation about firms' future investment opportunities. The investment score\nalso separately forecasts future total, intangible, and R\\&D investments.\nConsistent with theoretical predictions, high-investment-score firms experience\nsignificant positive short-term returns upon disclosure, and negative long-run\nfuture abnormal returns. We demonstrate ChatGPT's applicability to measure\nother policies, such as dividends and employment.", "published": "2024-09-26 15:09:35", "link": "http://arxiv.org/abs/2409.17933v2", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Mamba Meets Financial Markets: A Graph-Mamba Approach for Stock Price Prediction", "abstract": "Stock markets play an important role in the global economy, where accurate\nstock price predictions can lead to significant financial returns. While\nexisting transformer-based models have outperformed long short-term memory\nnetworks and convolutional neural networks in financial time series prediction,\ntheir high computational complexity and memory requirements limit their\npracticality for real-time trading and long-sequence data processing. To\naddress these challenges, we propose SAMBA, an innovative framework for stock\nreturn prediction that builds on the Mamba architecture and integrates graph\nneural networks. SAMBA achieves near-linear computational complexity by\nutilizing a bidirectional Mamba block to capture long-term dependencies in\nhistorical price data and employing adaptive graph convolution to model\ndependencies between daily stock features. Our experimental results demonstrate\nthat SAMBA significantly outperforms state-of-the-art baseline models in\nprediction accuracy, maintaining low computational complexity. The code and\ndatasets are available at github.com/Ali-Meh619/SAMBA.", "published": "2024-09-26 03:29:26", "link": "http://arxiv.org/abs/2410.03707v2", "categories": ["q-fin.CP", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "Risk measures based on target risk profiles", "abstract": "We address the problem that classical risk measures may not detect the tail\nrisk adequately. This can occur for instance due to the averaging process when\ncomputing Expected Shortfall. The current literature proposes a solution, the\nso-called adjusted Expected Shortfall. This risk measure is the supremum of\nExpected Shortfalls for all possible levels, adjusted with a function $g$, the\nso-called target risk profile. We generalize this idea by using other risk\nmeasures instead of Expected Shortfall. Therefore, we introduce the concept of\ngeneral adjusted risk measures. For these the realization of the adjusted risk\nmeasure quantifies the minimal amount of capital that has to be raised and\ninjected in a financial position $X$ to ensure that the risk measure is always\nsmaller or equal to the adjustment function $g(p)$ for all levels $p\\in[0,1]$.\nWe discuss a variety of assumptions such that desirable properties for risk\nmeasures are satisfied in this setup. From a theoretical point of view, our\nmain contribution is the analysis of equivalent assumptions such that a general\nadjusted risk measure is positive homogeneous and subadditive. Furthermore, we\nshow that these conditions hold for a bunch of new risk measures, beyond the\nadjusted Expected Shortfall. For these risk measures, we derive their dual\nrepresentations. Finally, we test the performance of these new risk measures in\na case study based on the S$\\&$P $500$.", "published": "2024-09-26 09:35:32", "link": "http://arxiv.org/abs/2409.17676v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Enhancing Financial Sentiment Analysis with Expert-Designed Hint", "abstract": "This paper investigates the role of expert-designed hint in enhancing\nsentiment analysis on financial social media posts. We explore the capability\nof large language models (LLMs) to empathize with writer perspectives and\nanalyze sentiments. Our findings reveal that expert-designed hint, i.e.,\npointing out the importance of numbers, significantly improve performances\nacross various LLMs, particularly in cases requiring perspective-taking skills.\nFurther analysis on tweets containing different types of numerical data\ndemonstrates that the inclusion of expert-designed hint leads to notable\nimprovements in sentiment analysis performance, especially for tweets with\nmonetary-related numbers. Our findings contribute to the ongoing discussion on\nthe applicability of Theory of Mind in NLP and open new avenues for improving\nsentiment analysis in financial domains through the strategic use of expert\nknowledge.", "published": "2024-09-26 00:54:17", "link": "http://arxiv.org/abs/2409.17448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What is the social benefit of hate speech detection research? A\n  Systematic Review", "abstract": "While NLP research into hate speech detection has grown exponentially in the\nlast three decades, there has been minimal uptake or engagement from policy\nmakers and non-profit organisations. We argue the absence of ethical frameworks\nhave contributed to this rift between current practice and best practice. By\nadopting appropriate ethical frameworks, NLP researchers may enable the social\nimpact potential of hate speech research. This position paper is informed by\nreviewing forty-eight hate speech detection systems associated with\nthirty-seven publications from different venues.", "published": "2024-09-26 01:57:27", "link": "http://arxiv.org/abs/2409.17467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Proportion Detection for Optimized Data Management for Large\n  Language Models", "abstract": "Large language models (LLMs) have demonstrated exceptional performance across\na wide range of tasks and domains, with data preparation playing a critical\nrole in achieving these results. Pre-training data typically combines\ninformation from multiple domains. To maximize performance when integrating\ndata from various domains, determining the optimal data proportion is\nessential. However, state-of-the-art (SOTA) LLMs rarely disclose details about\ntheir pre-training data, making it difficult for researchers to identify ideal\ndata proportions. In this paper, we introduce a new topic, \\textit{data\nproportion detection}, which enables the automatic estimation of pre-training\ndata proportions by analyzing the generated outputs of LLMs. We provide\nrigorous theoretical proofs, practical algorithms, and preliminary experimental\nresults for data proportion detection. Based on these findings, we offer\nvaluable insights into the challenges and future directions for effective data\nproportion detection and data management.", "published": "2024-09-26 04:30:32", "link": "http://arxiv.org/abs/2409.17527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MUSE: Integrating Multi-Knowledge for Knowledge Graph Completion", "abstract": "Knowledge Graph Completion (KGC) aims to predict the missing [relation] part\nof (head entity)--[relation]->(tail entity) triplet. Most existing KGC methods\nfocus on single features (e.g., relation types) or sub-graph aggregation.\nHowever, they do not fully explore the Knowledge Graph (KG) features and\nneglect the guidance of external semantic knowledge. To address these\nshortcomings, we propose a knowledge-aware reasoning model (MUSE), which\ndesigns a novel multi-knowledge representation learning mechanism for missing\nrelation prediction. Our model develops a tailored embedding space through\nthree parallel components: 1) Prior Knowledge Learning for enhancing the\ntriplets' semantic representation by fine-tuning BERT; 2) Context Message\nPassing for enhancing the context messages of KG; 3) Relational Path\nAggregation for enhancing the path representation from the head entity to the\ntail entity. The experimental results show that MUSE significantly outperforms\nother baselines on four public datasets, achieving over 5.50% H@1 improvement\nand 4.20% MRR improvement on the NELL995 dataset. The code and datasets will be\nreleased via https://github.com/SUSTech-TP/ADMA2024-MUSE.git.", "published": "2024-09-26 04:48:20", "link": "http://arxiv.org/abs/2409.17536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in\n  Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks but their performance in complex logical reasoning tasks remains\nunsatisfactory. Although some prompting methods, such as Chain-of-Thought, can\nimprove the reasoning ability of LLMs to some extent, they suffer from an\nunfaithful issue where derived conclusions may not align with the generated\nreasoning chain. To address this issue, some studies employ the approach of\npropositional logic to further enhance logical reasoning abilities of LLMs.\nHowever, the potential omissions in the extraction of logical expressions in\nthese methods can cause information loss in the logical reasoning process,\nthereby generating incorrect results. To this end, we propose Logic-of-Thought\n(LoT) prompting which employs propositional logic to generate expanded logical\ninformation descriptions and utilizes them as an additional augmentation to\noriginal contexts, thereby ensuring information completeness and enhancing\nlogical reasoning ability. LoT is orthogonal to existing prompting methods and\ncan be seamlessly integrated with them. Extensive experiments demonstrate that\nLoT boosts the performance of various prompting methods with a striking margin\nacross five logical reasoning tasks. In particular, LoT enhances\nChain-of-Thought's performance on the ReClor dataset by +4.35%, improves\nChain-of-Thought with Self-Consistency's performance on the RuleTaker dataset\nby +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter\ndataset by +8%.", "published": "2024-09-26 04:59:45", "link": "http://arxiv.org/abs/2409.17539v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Annotator Disagreement for Text Classification", "abstract": "It is common practice in text classification to only use one majority label\nfor model training even if a dataset has been annotated by multiple annotators.\nDoing so can remove valuable nuances and diverse perspectives inherent in the\nannotators' assessments. This paper proposes and compares three different\nstrategies to leverage annotator disagreement for text classification: a\nprobability-based multi-label method, an ensemble system, and instruction\ntuning. All three approaches are evaluated on the tasks of hate speech and\nabusive conversation detection, which inherently entail a high degree of\nsubjectivity. Moreover, to evaluate the effectiveness of embracing annotation\ndisagreements for model training, we conduct an online survey that compares the\nperformance of the multi-label model against a baseline model, which is trained\nwith the majority label.\n  The results show that in hate speech detection, the multi-label method\noutperforms the other two approaches, while in abusive conversation detection,\ninstruction tuning achieves the best performance. The results of the survey\nalso show that the outputs from the multi-label models are considered a better\nrepresentation of the texts than the single-label model.", "published": "2024-09-26 06:46:53", "link": "http://arxiv.org/abs/2409.17577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DualCoTs: Dual Chain-of-Thoughts Prompting for Sentiment Lexicon\n  Expansion of Idioms", "abstract": "Idioms represent a ubiquitous vehicle for conveying sentiments in the realm\nof everyday discourse, rendering the nuanced analysis of idiom sentiment\ncrucial for a comprehensive understanding of emotional expression within\nreal-world texts. Nevertheless, the existing corpora dedicated to idiom\nsentiment analysis considerably limit research in text sentiment analysis. In\nthis paper, we propose an innovative approach to automatically expand the\nsentiment lexicon for idioms, leveraging the capabilities of large language\nmodels through the application of Chain-of-Thought prompting. To demonstrate\nthe effectiveness of this approach, we integrate multiple existing resources\nand construct an emotional idiom lexicon expansion dataset (called EmoIdiomE),\nwhich encompasses a comprehensive repository of Chinese and English idioms.\nThen we designed the Dual Chain-of-Thoughts (DualCoTs) method, which combines\ninsights from linguistics and psycholinguistics, to demonstrate the\neffectiveness of using large models to automatically expand the sentiment\nlexicon for idioms. Experiments show that DualCoTs is effective in idioms\nsentiment lexicon expansion in both Chinese and English. For reproducibility,\nwe will release the data and code upon acceptance.", "published": "2024-09-26 07:07:14", "link": "http://arxiv.org/abs/2409.17588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient In-Domain Question Answering for Resource-Constrained\n  Environments", "abstract": "Retrieval Augmented Generation (RAG) is a common method for integrating\nexternal knowledge into pretrained Large Language Models (LLMs) to enhance\naccuracy and relevancy in question answering (QA) tasks. However, prompt\nengineering and resource efficiency remain significant bottlenecks in\ndeveloping optimal and robust RAG solutions for real-world QA applications.\nRecent studies have shown success in using fine tuning to address these\nproblems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to\nsmaller 7B models has demonstrated superior performance compared to RAG setups\nwith much larger models such as GPT-3.5. The combination of RAFT with\nparameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation\n(LoRA), promises an even more efficient solution, yet remains an unexplored\narea. In this work, we combine RAFT with LoRA to reduce fine tuning and storage\nrequirements and gain faster inference times while maintaining comparable RAG\nperformance. This results in a more compute-efficient RAFT, or CRAFT, which is\nparticularly useful for knowledge-intensive QA tasks in resource-constrained\nenvironments where internet access may be restricted and hardware resources\nlimited.", "published": "2024-09-26 08:55:21", "link": "http://arxiv.org/abs/2409.17648v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Human-Preference Alignment for Neural Machine Translation\n  with Direct Quality Optimization", "abstract": "Reinforcement Learning from Human Feedback (RLHF) and derivative techniques\nlike Direct Preference Optimization (DPO) are task-alignment algorithms used to\nrepurpose general, foundational models for specific tasks. We show that\napplying task-alignment to neural machine translation (NMT) addresses an\nexisting task--data mismatch in NMT, leading to improvements across all\nlanguages of a multilingual model, even when task-alignment is only applied to\na subset of those languages. We do so by introducing Direct Quality\nOptimization (DQO), a variant of DPO leveraging a pre-trained translation\nquality estimation model as a proxy for human preferences, and verify the\nimprovements with both automatic metrics and human evaluation.", "published": "2024-09-26 09:32:12", "link": "http://arxiv.org/abs/2409.17673v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BeanCounter: A low-toxicity, large-scale, and open dataset of\n  business-oriented text", "abstract": "Many of the recent breakthroughs in language modeling have resulted from\nscaling effectively the same model architecture to larger datasets. In this\nvein, recent work has highlighted performance gains from increasing training\ndataset size and quality, suggesting a need for novel sources of large-scale\ndatasets. In this work, we introduce BeanCounter, a public dataset consisting\nof more than 159B tokens extracted from businesses' disclosures. We show that\nthis data is indeed novel: less than 0.1% of BeanCounter appears in Common\nCrawl-based datasets and it is an order of magnitude larger than datasets\nrelying on similar sources. Given the data's provenance, we hypothesize that\nBeanCounter is comparatively more factual and less toxic than web-based\ndatasets. Exploring this hypothesis, we find that many demographic identities\noccur with similar prevalence in BeanCounter but with significantly less toxic\ncontext relative to other datasets. To demonstrate the utility of BeanCounter,\nwe evaluate and compare two LLMs continually pre-trained on BeanCounter with\ntheir base models. We find an 18-33% reduction in toxic generation and improved\nperformance within the finance domain for the continually pretrained models.\nCollectively, our work suggests that BeanCounter is a novel source of\nlow-toxicity and high-quality domain-specific data with sufficient scale to\ntrain multi-billion parameter LLMs.", "published": "2024-09-26 13:26:46", "link": "http://arxiv.org/abs/2409.17827v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent\n  Representation MOdification", "abstract": "Due to their substantial sizes, large language models (LLMs) are typically\ndeployed within a single-backbone multi-tenant framework. In this setup, a\nsingle instance of an LLM backbone must cater to multiple users or tasks\nthrough the application of various parameter-efficient fine-tuning (PEFT)\nmodels. Despite the availability of numerous effective PEFT techniques such as\nLoRA, there remains a need for a PEFT approach that achieves both high\nefficiency during inference and competitive performance on downstream tasks. In\nthis research, we introduce a new and straightforward PEFT methodology named\n\\underline{P}rompt D\\underline{E}pen\\underline{D}ent \\underline{R}epresentation\nM\\underline{O}dification (PEDRO). The proposed method involves integrating a\nlightweight vector generator into each Transformer layer, which generates\nvectors contingent upon the input prompts. These vectors then modify the hidden\nrepresentations created by the LLM through a dot product operation, thereby\ninfluencing the semantic output and generated content of the model. Extensive\nexperimentation across a variety of tasks indicates that: (a) PEDRO surpasses\nrecent PEFT benchmarks when using a similar number of tunable parameters. (b)\nUnder the single-backbone multi-tenant deployment model, PEDRO exhibits\nsuperior efficiency compared to LoRA, indicating significant industrial\npotential.", "published": "2024-09-26 13:36:00", "link": "http://arxiv.org/abs/2409.17834v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language\n  Models", "abstract": "In this work, we introduce EMMA-500, a large-scale multilingual language\nmodel continue-trained on texts across 546 languages designed for enhanced\nmultilingual performance, focusing on improving language coverage for\nlow-resource languages. To facilitate continual pre-training, we compile the\nMaLA corpus, a comprehensive multilingual dataset enriched with curated\ndatasets across diverse domains. Leveraging this corpus, we conduct extensive\ncontinual pre-training of the Llama 2 7B model, resulting in EMMA-500, which\ndemonstrates robust performance across a wide collection of benchmarks,\nincluding a comprehensive set of multilingual tasks. Our results highlight the\neffectiveness of continual pre-training in expanding large language models'\nlanguage capacity, particularly for underrepresented languages, demonstrating\nsignificant gains in cross-lingual transfer, task generalization, and language\nadaptability. We release the MaLA corpus, EMMA-500 model weights, scripts, and\nmodel generations.", "published": "2024-09-26 14:40:45", "link": "http://arxiv.org/abs/2409.17892v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan\n  Arabic Dialect", "abstract": "We introduce Atlas-Chat, the first-ever collection of LLMs specifically\ndeveloped for dialectal Arabic. Focusing on Moroccan Arabic, also known as\nDarija, we construct our instruction dataset by consolidating existing Darija\nlanguage resources, creating novel datasets both manually and synthetically,\nand translating English instructions with stringent quality control.\nAtlas-Chat-2B, 9B, and 27B models, fine-tuned on the dataset, exhibit superior\nability in following Darija instructions and performing standard NLP tasks.\nNotably, our models outperform both state-of-the-art and Arabic-specialized\nLLMs like LLaMa, Jais, and AceGPT, e.g., our 9B model gains a 13% performance\nboost over a larger 13B model on DarijaMMLU, in our newly introduced evaluation\nsuite for Darija covering both discriminative and generative tasks.\nFurthermore, we perform an experimental analysis of various fine-tuning\nstrategies and base model choices to determine optimal configurations. All our\nresources are publicly accessible, and we believe our work offers comprehensive\ndesign methodologies of instruction-tuning for low-resource languages, which\nare often neglected in favor of data-rich languages by contemporary LLMs.", "published": "2024-09-26 14:56:38", "link": "http://arxiv.org/abs/2409.17912v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Lou Dataset -- Exploring the Impact of Gender-Fair Language in\n  German Text Classification", "abstract": "Gender-fair language, an evolving German linguistic variation, fosters\ninclusion by addressing all genders or using neutral forms. Nevertheless, there\nis a significant lack of resources to assess the impact of this linguistic\nshift on classification using language models (LMs), which are probably not\ntrained on such variations. To address this gap, we present Lou, the first\ndataset featuring high-quality reformulations for German text classification\ncovering seven tasks, like stance detection and toxicity classification.\nEvaluating 16 mono- and multi-lingual LMs on Lou shows that gender-fair\nlanguage substantially impacts predictions by flipping labels, reducing\ncertainty, and altering attention patterns. However, existing evaluations\nremain valid, as LM rankings of original and reformulated instances do not\nsignificantly differ. While we offer initial insights on the effect on German\ntext classification, the findings likely apply to other languages, as\nconsistent patterns were observed in multi-lingual and English LMs.", "published": "2024-09-26 15:08:17", "link": "http://arxiv.org/abs/2409.17929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Multilingual Long-Context Models for Retrieval and Reasoning", "abstract": "Recent large language models (LLMs) demonstrate impressive capabilities in\nhandling long contexts, some exhibiting near-perfect recall on synthetic\nretrieval tasks. However, these evaluations have mainly focused on English text\nand involved a single target sentence within lengthy contexts. Our work\ninvestigates how LLM performance generalizes to multilingual settings with\nmultiple hidden target sentences. We create a new dataset -- mLongRR -- to\ncomprehensively evaluate several multilingual long-context LLMs on retrieval\nand reasoning tasks across five languages: English, Vietnamese, Indonesian,\nSwahili, and Somali. These languages share the Latin script but belong to\ndistinct language families and resource levels. Our analysis reveals a\nsignificant performance gap between languages. The best-performing models such\nas Gemini-1.5 and GPT-4o, achieve around 96% accuracy in English to around 36%\nin Somali with a single target sentence. However, this accuracy drops to 40% in\nEnglish and 0% in Somali when dealing with three target sentences. Our findings\nhighlight the challenges long-context LLMs face when processing longer\ncontexts, an increase in the number of target sentences, or languages of lower\nresource levels.", "published": "2024-09-26 16:15:14", "link": "http://arxiv.org/abs/2409.18006v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DARE: Diverse Visual Question Answering with Robustness Evaluation", "abstract": "Vision Language Models (VLMs) extend remarkable capabilities of text-only\nlarge language models and vision-only models, and are able to learn from and\nprocess multi-modal vision-text input. While modern VLMs perform well on a\nnumber of standard image classification and image-text matching tasks, they\nstill struggle with a number of crucial vision-language (VL) reasoning\nabilities such as counting and spatial reasoning. Moreover, while they might be\nvery brittle to small variations in instructions and/or evaluation protocols,\nexisting benchmarks fail to evaluate their robustness (or rather the lack of\nit). In order to couple challenging VL scenarios with comprehensive robustness\nevaluation, we introduce DARE, Diverse Visual Question Answering with\nRobustness Evaluation, a carefully created and curated multiple-choice VQA\nbenchmark. DARE evaluates VLM performance on five diverse categories and\nincludes four robustness-oriented evaluations based on the variations of:\nprompts, the subsets of answer options, the output format and the number of\ncorrect answers. Among a spectrum of other findings, we report that\nstate-of-the-art VLMs still struggle with questions in most categories and are\nunable to consistently deliver their peak performance across the tested\nrobustness evaluations. The worst case performance across the subsets of\noptions is up to 34% below the performance in the standard case. The robustness\nof the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match the\nclosed-source models such as GPT-4 and Gemini, but even the latter remain very\nbrittle to different variations.", "published": "2024-09-26 16:31:50", "link": "http://arxiv.org/abs/2409.18023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Detection and Analysis of Power Words in Persuasive Text Using\n  Natural Language Processing", "abstract": "Power words are terms that evoke strong emotional responses and significantly\ninfluence readers' behavior, playing a crucial role in fields like marketing,\npolitics, and motivational writing. This study proposes a methodology for the\nautomated detection and analysis of power words in persuasive text using a\ncustom lexicon created from a comprehensive dataset scraped from online\nsources. A specialized Python package, The Text Monger, is created and employed\nto identify the presence and frequency of power words within a given text. By\nanalyzing diverse datasets, including fictional excerpts, speeches, and\nmarketing materials,the aim is to classify and assess the impact of power words\non sentiment and reader engagement. The findings provide valuable insights into\nthe effectiveness of power words across various domains, offering practical\napplications for content creators, advertisers, and policymakers looking to\nenhance their messaging and engagement strategies.", "published": "2024-09-26 16:38:56", "link": "http://arxiv.org/abs/2409.18033v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling the Role of Pretraining in Direct Speech Translation", "abstract": "Direct speech-to-text translation systems encounter an important drawback in\ndata scarcity. A common solution consists on pretraining the encoder on\nautomatic speech recognition, hence losing efficiency in the training process.\nIn this study, we compare the training dynamics of a system using a pretrained\nencoder, the conventional approach, and one trained from scratch. We observe\nthat, throughout the training, the randomly initialized model struggles to\nincorporate information from the speech inputs for its predictions. Hence, we\nhypothesize that this issue stems from the difficulty of effectively training\nan encoder for direct speech translation. While a model trained from scratch\nneeds to learn acoustic and semantic modeling simultaneously, a pretrained one\ncan just focus on the latter. Based on these findings, we propose a subtle\nchange in the decoder cross-attention to integrate source information from\nearlier steps in training. We show that with this change, the model trained\nfrom scratch can achieve comparable performance to the pretrained one, while\nreducing the training time.", "published": "2024-09-26 16:46:46", "link": "http://arxiv.org/abs/2409.18044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource\n  Languages Injected with Multilingual Graph Knowledge", "abstract": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Using LLMs for such languages is often difficult due to a high\ncomputational cost; not only during training, but also during inference. Static\nword embeddings are much more resource-efficient (\"green\"), and thus still\nprovide value, particularly for very low-resource languages. There is, however,\na notable lack of comprehensive repositories with such embeddings for diverse\nlanguages. To address this gap, we present GrEmLIn, a centralized repository of\ngreen, static baseline embeddings for 87 mid- and low-resource languages. We\ncompute GrEmLIn embeddings with a novel method that enhances GloVe embeddings\nby integrating multilingual graph knowledge, which makes our static embeddings\ncompetitive with LLM representations, while being parameter-free at inference\ntime. Our experiments demonstrate that GrEmLIn embeddings outperform\nstate-of-the-art contextualized embeddings from E5 on the task of lexical\nsimilarity. They remain competitive in extrinsic evaluation tasks like\nsentiment analysis and natural language inference, with average performance\ngaps of just 5-10\\% or less compared to state-of-the-art models, given a\nsufficient vocabulary overlap with the target task, and underperform only on\ntopic classification. Our code and embeddings are publicly available at\nhttps://huggingface.co/DFKI.", "published": "2024-09-26 18:10:26", "link": "http://arxiv.org/abs/2409.18193v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LangSAMP: Language-Script Aware Multilingual Pretraining", "abstract": "Recent multilingual pretrained language models (mPLMs) often avoid using\nlanguage embeddings -- learnable vectors assigned to different languages. These\nembeddings are discarded for two main reasons: (1) mPLMs are expected to have a\nsingle, unified parameter set across all languages, and (2) they need to\nfunction seamlessly as universal text encoders without requiring language IDs\nas input. However, this removal increases the burden on token embeddings to\nencode all language-specific information, which may hinder the model's ability\nto produce more language-neutral representations. To address this challenge, we\npropose Language-Script Aware Multilingual Pretraining (LangSAMP), a method\nthat incorporates both language and script embeddings to enhance representation\nlearning while maintaining a simple architecture. Specifically, we integrate\nthese embeddings into the output of the transformer blocks before passing the\nfinal representations to the language modeling head for prediction. We apply\nLangSAMP to the continual pretraining of XLM-R on a highly multilingual corpus\ncovering more than 500 languages. The resulting model consistently outperforms\nthe baseline. Extensive analysis further shows that language/script embeddings\nencode language/script-specific information, which improves the selection of\nsource languages for crosslingual transfer. We make our code and models\npublicly available at \\url{https://github.com/cisnlp/LangSAMP}.", "published": "2024-09-26 18:29:10", "link": "http://arxiv.org/abs/2409.18199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dealing with Controversy: An Emotion and Coping Strategy Corpus Based on\n  Role Playing", "abstract": "There is a mismatch between psychological and computational studies on\nemotions. Psychological research aims at explaining and documenting internal\nmechanisms of these phenomena, while computational work often simplifies them\ninto labels. Many emotion fundamentals remain under-explored in natural\nlanguage processing, particularly how emotions develop and how people cope with\nthem. To help reduce this gap, we follow theories on coping, and treat emotions\nas strategies to cope with salient situations (i.e., how people deal with\nemotion-eliciting events). This approach allows us to investigate the link\nbetween emotions and behavior, which also emerges in language. We introduce the\ntask of coping identification, together with a corpus to do so, constructed via\nrole-playing. We find that coping strategies realize in text even though they\nare challenging to recognize, both for humans and automatic systems trained and\nprompted on the same task. We thus open up a promising research direction to\nenhance the capability of models to better capture emotion mechanisms from\ntext.", "published": "2024-09-26 06:49:54", "link": "http://arxiv.org/abs/2409.19025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut\n  Learning in Text Classification by Language Models", "abstract": "Language models (LMs), despite their advances, often depend on spurious\ncorrelations, undermining their accuracy and generalizability. This study\naddresses the overlooked impact of subtler, more complex shortcuts that\ncompromise model reliability beyond oversimplified shortcuts. We introduce a\ncomprehensive benchmark that categorizes shortcuts into occurrence, style, and\nconcept, aiming to explore the nuanced ways in which these shortcuts influence\nthe performance of LMs. Through extensive experiments across traditional LMs,\nlarge language models, and state-of-the-art robust models, our research\nsystematically investigates models' resilience and susceptibilities to\nsophisticated shortcuts. Our benchmark and code can be found at:\nhttps://github.com/yuqing-zhou/shortcut-learning-in-text-classification.", "published": "2024-09-26 01:17:42", "link": "http://arxiv.org/abs/2409.17455v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with\n  Scoring-aware Multiple Rewards", "abstract": "Recent advances in automated essay scoring (AES) have shifted towards\nevaluating multiple traits to provide enriched feedback. Like typical AES\nsystems, multi-trait AES employs the quadratic weighted kappa (QWK) to measure\nagreement with human raters, aligning closely with the rating schema; however,\nits non-differentiable nature prevents its direct use in neural network\ntraining. In this paper, we propose Scoring-aware Multi-reward Reinforcement\nLearning (SaMRL), which integrates actual evaluation schemes into the training\nprocess by designing QWK-based rewards with a mean-squared error penalty for\nmulti-trait AES. Existing reinforcement learning (RL) applications in AES are\nlimited to classification models despite associated performance degradation, as\nRL requires probability distributions; instead, we adopt an autoregressive\nscore generation framework to leverage token generation probabilities for\nrobust multi-trait score predictions. Empirical analyses demonstrate that SaMRL\nfacilitates model training, notably enhancing scoring of previously inferior\nprompts.", "published": "2024-09-26 02:16:48", "link": "http://arxiv.org/abs/2409.17472v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reducing and Exploiting Data Augmentation Noise through Meta Reweighting\n  Contrastive Learning for Text Classification", "abstract": "Data augmentation has shown its effectiveness in resolving the data-hungry\nproblem and improving model's generalization ability. However, the quality of\naugmented data can be varied, especially compared with the raw/original data.\nTo boost deep learning models' performance given augmented data/samples in text\nclassification tasks, we propose a novel framework, which leverages both meta\nlearning and contrastive learning techniques as parts of our design for\nreweighting the augmented samples and refining their feature representations\nbased on their quality. As part of the framework, we propose novel\nweight-dependent enqueue and dequeue algorithms to utilize augmented samples'\nweight/quality information effectively. Through experiments, we show that our\nframework can reasonably cooperate with existing deep learning models (e.g.,\nRoBERTa-base and Text-CNN) and augmentation techniques (e.g., Wordnet and\nEasydata) for specific supervised learning tasks. Experiment results show that\nour framework achieves an average of 1.6%, up to 4.3% absolute improvement on\nText-CNN encoders and an average of 1.4%, up to 4.4% absolute improvement on\nRoBERTa-base encoders on seven GLUE benchmark datasets compared with the best\nbaseline. We present an indepth analysis of our framework design, revealing the\nnon-trivial contributions of our network components. Our code is publicly\navailable for better reproducibility.", "published": "2024-09-26 02:19:13", "link": "http://arxiv.org/abs/2409.17474v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HaloScope: Harnessing Unlabeled LLM Generations for Hallucination\n  Detection", "abstract": "The surge in applications of large language models (LLMs) has prompted\nconcerns about the generation of misleading or fabricated information, known as\nhallucinations. Therefore, detecting hallucinations has become critical to\nmaintaining trust in LLM-generated content. A primary challenge in learning a\ntruthfulness classifier is the lack of a large amount of labeled truthful and\nhallucinated data. To address the challenge, we introduce HaloScope, a novel\nlearning framework that leverages the unlabeled LLM generations in the wild for\nhallucination detection. Such unlabeled data arises freely upon deploying LLMs\nin the open world, and consists of both truthful and hallucinated information.\nTo harness the unlabeled data, we present an automated membership estimation\nscore for distinguishing between truthful and untruthful generations within\nunlabeled mixture data, thereby enabling the training of a binary truthfulness\nclassifier on top. Importantly, our framework does not require extra data\ncollection and human annotations, offering strong flexibility and practicality\nfor real-world applications. Extensive experiments show that HaloScope can\nachieve superior hallucination detection performance, outperforming the\ncompetitive rivals by a significant margin. Code is available at\nhttps://github.com/deeplearningwisc/haloscope.", "published": "2024-09-26 03:22:09", "link": "http://arxiv.org/abs/2409.17504v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "When A Man Says He Is Pregnant: ERP Evidence for A Rational Account of\n  Speaker-contextualized Language Comprehension", "abstract": "Spoken language is often, if not always, understood in a context formed by\nthe identity of the speaker. For example, we can easily make sense of an\nutterance such as \"I'm going to have a manicure this weekend\" or \"The first\ntime I got pregnant I had a hard time\" when spoken by a woman, but it would be\nharder to understand when it is spoken by a man. Previous event-related\npotential (ERP) studies have shown mixed results regarding the\nneurophysiological responses to such speaker-content mismatches, with some\nreporting an N400 effect and others a P600 effect. In an electroencephalography\n(EEG) experiment involving 64 participants, we used social and biological\nmismatches as test cases to demonstrate how these distinct ERP patterns reflect\ndifferent aspects of rational inference. We showed that when the mismatch\ninvolves social stereotypes (e.g., men getting a manicure), listeners can\narrive at a \"literal\" interpretation by integrating the content with their\nsocial knowledge, though this integration requires additional effort due to\nstereotype violations-resulting in an N400 effect. In contrast, when the\nmismatch involves biological knowledge (e.g., men getting pregnant), a\n\"literal\" interpretation becomes impossible, leading listeners to treat the\ninput as potentially containing errors and engage in correction\nprocesses-resulting in a P600 effect. Supporting this rational inference\nframework, we found that the social N400 effect decreased as a function of the\nlistener's personality trait of openness (as more open-minded individuals\nmaintain more flexible social expectations), while the biological P600 effect\nremained robust (as biological constraints are recognized regardless of\nindividual personalities). Our findings help to reconcile the empirical\ninconsistencies and show how rational inference shapes speaker-contextualized\nlanguage comprehension.", "published": "2024-09-26 04:24:52", "link": "http://arxiv.org/abs/2409.17525v2", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context\n  Information in Multi-Turn Multimodal Medical Dialogue", "abstract": "The rocketing prosperity of large language models (LLMs) in recent years has\nboosted the prevalence of vision-language models (VLMs) in the medical sector.\nIn our online medical consultation scenario, a doctor responds to the texts and\nimages provided by a patient in multiple rounds to diagnose her/his health\ncondition, forming a multi-turn multimodal medical dialogue format. Unlike\nhigh-quality images captured by professional equipment in traditional medical\nvisual question answering (Med-VQA), the images in our case are taken by\npatients' mobile phones. These images have poor quality control, with issues\nsuch as excessive background elements and the lesion area being significantly\noff-center, leading to degradation of vision-language alignment in the model\ntraining phase. In this paper, we propose ZALM3, a Zero-shot strategy to\nimprove vision-language ALignment in Multi-turn Multimodal Medical dialogue.\nSince we observe that the preceding text conversations before an image can\ninfer the regions of interest (RoIs) in the image, ZALM3 employs an LLM to\nsummarize the keywords from the preceding context and a visual grounding model\nto extract the RoIs. The updated images eliminate unnecessary background noise\nand provide more effective vision-language alignment. To better evaluate our\nproposed method, we design a new subjective assessment metric for multi-turn\nunimodal/multimodal medical dialogue to provide a fine-grained performance\ncomparison. Our experiments across three different clinical departments\nremarkably demonstrate the efficacy of ZALM3 with statistical significance.", "published": "2024-09-26 07:55:57", "link": "http://arxiv.org/abs/2409.17610v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task", "abstract": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations.", "published": "2024-09-26 08:44:38", "link": "http://arxiv.org/abs/2409.17640v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Digital Twin Ecosystem for Oncology Clinical Operations", "abstract": "Artificial Intelligence (AI) and Large Language Models (LLMs) hold\nsignificant promise in revolutionizing healthcare, especially in clinical\napplications. Simultaneously, Digital Twin technology, which models and\nsimulates complex systems, has gained traction in enhancing patient care.\nHowever, despite the advances in experimental clinical settings, the potential\nof AI and digital twins to streamline clinical operations remains largely\nuntapped. This paper introduces a novel digital twin framework specifically\ndesigned to enhance oncology clinical operations. We propose the integration of\nmultiple specialized digital twins, such as the Medical Necessity Twin, Care\nNavigator Twin, and Clinical History Twin, to enhance workflow efficiency and\npersonalize care for each patient based on their unique data. Furthermore, by\nsynthesizing multiple data sources and aligning them with the National\nComprehensive Cancer Network (NCCN) guidelines, we create a dynamic Cancer Care\nPath, a continuously evolving knowledge base that enables these digital twins\nto provide precise, tailored clinical recommendations.", "published": "2024-09-26 08:56:54", "link": "http://arxiv.org/abs/2409.17650v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Zero- and Few-shot Named Entity Recognition and Text Expansion in\n  Medication Prescriptions using ChatGPT", "abstract": "Introduction: Medication prescriptions are often in free text and include a\nmix of two languages, local brand names, and a wide range of idiosyncratic\nformats and abbreviations. Large language models (LLMs) have shown promising\nability to generate text in response to input prompts. We use ChatGPT 3.5 to\nautomatically structure and expand medication statements in discharge summaries\nand thus make them easier to interpret for people and machines. Methods:\nNamed-entity Recognition (NER) and Text Expansion (EX) are used in a zero- and\nfew-shot setting with different prompt strategies. 100 medication statements\nwere manually annotated and curated. NER performance was measured by using\nstrict and partial matching. For the task EX, two experts interpreted the\nresults by assessing semantic equivalence between original and expanded\nstatements. The model performance was measured by precision, recall, and F1\nscore. Results: For NER, the best-performing prompt reached an average F1 score\nof 0.94 in the test set. For EX, the few-shot prompt showed superior\nperformance among other prompts, with an average F1 score of 0.87. Conclusion:\nOur study demonstrates good performance for NER and EX tasks in free-text\nmedication statements using ChatGPT. Compared to a zero-shot baseline, a\nfew-shot approach prevented the system from hallucinating, which would be\nunacceptable when processing safety-relevant medication data.", "published": "2024-09-26 09:49:27", "link": "http://arxiv.org/abs/2409.17683v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Integrating Hierarchical Semantic into Iterative Generation Model for\n  Entailment Tree Explanation", "abstract": "Manifestly and logically displaying the line of reasoning from evidence to\nanswer is significant to explainable question answering (QA). The entailment\ntree exhibits the lines structurally, which is different from the\nself-explanation principle in large-scale language models. Existing methods\nrarely consider the semantic association of sentences between and within\nhierarchies within the tree structure, which is prone to apparent mistakes in\ncombinations. In this work, we propose an architecture of integrating the\nHierarchical Semantics of sentences under the framework of Controller-Generator\n(HiSCG) to explain answers. The HiSCG designs a hierarchical mapping between\nhypotheses and facts, discriminates the facts involved in tree constructions,\nand optimizes single-step entailments. To the best of our knowledge, We are the\nfirst to notice hierarchical semantics of sentences between the same layer and\nadjacent layers to yield improvements. The proposed method achieves comparable\nperformance on all three settings of the EntailmentBank dataset. The\ngeneralization results on two out-of-domain datasets also demonstrate the\neffectiveness of our method.", "published": "2024-09-26 11:46:58", "link": "http://arxiv.org/abs/2409.17757v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Faithfulness and the Notion of Adversarial Sensitivity in NLP\n  Explanations", "abstract": "Faithfulness is arguably the most critical metric to assess the reliability\nof explainable AI. In NLP, current methods for faithfulness evaluation are\nfraught with discrepancies and biases, often failing to capture the true\nreasoning of models. We introduce Adversarial Sensitivity as a novel approach\nto faithfulness evaluation, focusing on the explainer's response when the model\nis under adversarial attack. Our method accounts for the faithfulness of\nexplainers by capturing sensitivity to adversarial input changes. This work\naddresses significant limitations in existing evaluation techniques, and\nfurthermore, quantifies faithfulness from a crucial yet underexplored paradigm.", "published": "2024-09-26 12:11:28", "link": "http://arxiv.org/abs/2409.17774v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-supervised Preference Optimization: Enhance Your Language Model\n  with Preference Degree Awareness", "abstract": "Recently, there has been significant interest in replacing the reward model\nin Reinforcement Learning with Human Feedback (RLHF) methods for Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO) and its variants.\nThese approaches commonly use a binary cross-entropy mechanism on pairwise\nsamples, i.e., minimizing and maximizing the loss based on preferred or\ndis-preferred responses, respectively. However, while this training strategy\nomits the reward model, it also overlooks the varying preference degrees within\ndifferent responses. We hypothesize that this is a key factor hindering LLMs\nfrom sufficiently understanding human preferences. To address this problem, we\npropose a novel Self-supervised Preference Optimization (SPO) framework, which\nconstructs a self-supervised preference degree loss combined with the alignment\nloss, thereby helping LLMs improve their ability to understand the degree of\npreference. Extensive experiments are conducted on two widely used datasets of\ndifferent tasks. The results demonstrate that SPO can be seamlessly integrated\nwith existing preference optimization methods and significantly boost their\nperformance to achieve state-of-the-art performance. We also conduct detailed\nanalyses to offer comprehensive insights into SPO, which verifies its\neffectiveness. The code is available at https://github.com/lijian16/SPO.", "published": "2024-09-26 12:37:26", "link": "http://arxiv.org/abs/2409.17791v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inference-Time Language Model Alignment via Integrated Value Guidance", "abstract": "Large language models are typically fine-tuned to align with human\npreferences, but tuning large models is computationally intensive and complex.\nIn this work, we introduce $\\textit{Integrated Value Guidance}$ (IVG), a method\nthat uses implicit and explicit value functions to guide language model\ndecoding at token and chunk-level respectively, efficiently aligning large\nlanguage models purely at inference time. This approach circumvents the\ncomplexities of direct fine-tuning and outperforms traditional methods.\nEmpirically, we demonstrate the versatility of IVG across various tasks. In\ncontrolled sentiment generation and summarization tasks, our method\nsignificantly improves the alignment of large models using inference-time\nguidance from $\\texttt{gpt2}$-based value functions. Moreover, in a more\nchallenging instruction-following benchmark AlpacaEval 2.0, we show that both\nspecifically tuned and off-the-shelf value functions greatly improve the\nlength-controlled win rates of large models against $\\texttt{gpt-4-turbo}$\n(e.g., $19.51\\% \\rightarrow 26.51\\%$ for $\\texttt{Mistral-7B-Instruct-v0.2}$\nand $25.58\\% \\rightarrow 33.75\\%$ for $\\texttt{Mixtral-8x7B-Instruct-v0.1}$\nwith Tulu guidance).", "published": "2024-09-26 13:15:18", "link": "http://arxiv.org/abs/2409.17819v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing:\n  Leveraging a Fine-Grained Dataset and an Innovative Criterion", "abstract": "During pre-training, the Text-to-Image (T2I) diffusion models encode factual\nknowledge into their parameters. These parameterized facts enable realistic\nimage generation, but they may become obsolete over time, thereby\nmisrepresenting the current state of the world. Knowledge editing techniques\naim to update model knowledge in a targeted way. However, facing the dual\nchallenges posed by inadequate editing datasets and unreliable evaluation\ncriterion, the development of T2I knowledge editing encounter difficulties in\neffectively generalizing injected knowledge. In this work, we design a T2I\nknowledge editing framework by comprehensively spanning on three phases: First,\nwe curate a dataset \\textbf{CAKE}, comprising paraphrase and multi-object test,\nto enable more fine-grained assessment on knowledge generalization. Second, we\npropose a novel criterion, \\textbf{adaptive CLIP threshold}, to effectively\nfilter out false successful images under the current criterion and achieve\nreliable editing evaluation. Finally, we introduce \\textbf{MPE}, a simple but\neffective approach for T2I knowledge editing. Instead of tuning parameters, MPE\nprecisely recognizes and edits the outdated part of the conditioning\ntext-prompt to accommodate the up-to-date knowledge. A straightforward\nimplementation of MPE (Based on in-context learning) exhibits better overall\nperformance than previous model editors. We hope these efforts can further\npromote faithful evaluation of T2I knowledge editing methods.", "published": "2024-09-26 15:07:30", "link": "http://arxiv.org/abs/2409.17928v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Hard Positive Truth about Vision-Language Compositionality", "abstract": "Several benchmarks have concluded that our best vision-language models (e.g.,\nCLIP) are lacking in compositionality. Given an image, these benchmarks probe a\nmodel's ability to identify its associated caption amongst a set of\ncompositional distractors. In response, a surge of recent proposals show\nimprovements by finetuning CLIP with distractors as hard negatives. Our\ninvestigations reveal that these improvements have, in fact, been significantly\noverstated -- because existing benchmarks do not probe whether finetuned\nvision-language models remain invariant to hard positives. By curating an\nevaluation dataset with 112,382 hard negatives and hard positives, we uncover\nthat including hard positives decreases CLIP's performance by 12.9%, while\nhumans perform effortlessly at 99%. CLIP finetuned with hard negatives results\nin an even larger decrease, up to 38.7%. With this finding, we then produce a\n1,775,259 image-text training set with both hard negative and hard positive\ncaptions. By training with both, we see improvements on existing benchmarks\nwhile simultaneously improving performance on hard positives, indicating a more\nrobust improvement in compositionality. Our work suggests the need for future\nresearch to rigorously test and improve CLIP's understanding of semantic\nrelationships between related \"positive\" concepts.", "published": "2024-09-26 15:36:10", "link": "http://arxiv.org/abs/2409.17958v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and\n  Adaptive Disambiguate based Efficient Tree Search", "abstract": "Large Language Models (LLMs) have exhibited exceptional performance across a\nbroad range of tasks and domains. However, they still encounter difficulties in\nsolving mathematical problems due to the rigorous and logical nature of\nmathematics. Previous studies have employed techniques such as supervised\nfine-tuning (SFT), prompt engineering, and search-based methods to improve the\nmathematical problem-solving abilities of LLMs. Despite these efforts, their\nperformance remains suboptimal and demands substantial computational resources.\nTo address this issue, we propose a novel approach, BEATS, to enhance\nmathematical problem-solving abilities. Our method leverages newly designed\nprompts that guide the model to iteratively rewrite, advance by one step, and\ngenerate answers based on previous steps. Additionally, we introduce a new\nback-verification technique that uses LLMs to validate the correctness of the\ngenerated answers. Furthermore, we employ a pruning tree search to optimize\nsearch time while achieving strong performance. Notably, our method improves\nQwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on the\nMATH benchmark.", "published": "2024-09-26 15:47:42", "link": "http://arxiv.org/abs/2409.17972v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting Affect Aggregates from Longitudinal Social Media Data with\n  Temporal Adapters for Large Language Models", "abstract": "This paper proposes temporally aligned Large Language Models (LLMs) as a tool\nfor longitudinal analysis of social media data. We fine-tune Temporal Adapters\nfor Llama 3 8B on full timelines from a panel of British Twitter users, and\nextract longitudinal aggregates of emotions and attitudes with established\nquestionnaires. We focus our analysis on the beginning of the COVID-19 pandemic\nthat had a strong impact on public opinion and collective emotions. We validate\nour estimates against representative British survey data and find strong\npositive, significant correlations for several collective emotions. The\nobtained estimates are robust across multiple training seeds and prompt\nformulations, and in line with collective emotions extracted using a\ntraditional classification model trained on labeled data. We demonstrate the\nflexibility of our method on questions of public opinion for which no\npre-trained classifier is available. Our work extends the analysis of affect in\nLLMs to a longitudinal setting through Temporal Adapters. It enables flexible,\nnew approaches towards the longitudinal analysis of social media data.", "published": "2024-09-26 16:02:00", "link": "http://arxiv.org/abs/2409.17990v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective", "abstract": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.", "published": "2024-09-26 16:34:35", "link": "http://arxiv.org/abs/2409.18028v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid\n  Emotions", "abstract": "GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nfor the open-source community. Existing vision-language models rely on external\ntools for speech processing, while speech-language models still suffer from\nlimited or totally without vision-understanding capabilities. To address this\ngap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable\nLarge Language Models with end-to-end speech abilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we surprisingly notice that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the bi-modal aligned\ncounterparts. Moreover, a lightweight style module is introduced for the\nflexible speech style controls including emotions and pitches. For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.", "published": "2024-09-26 16:44:02", "link": "http://arxiv.org/abs/2409.18042v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Open-World Evaluation for Retrieving Diverse Perspectives", "abstract": "We study retrieving a set of documents that covers various perspectives on a\ncomplex and contentious question (e.g., will ChatGPT do more harm than good?).\nWe curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS),\nwhere each example consists of a question and diverse perspectives associated\nwith the question, sourced from survey questions and debate websites. On this\ndata, retrievers paired with a corpus are evaluated to surface a document set\nthat contains diverse perspectives. Our framing diverges from most retrieval\ntasks in that document relevancy cannot be decided by simple string matches to\nreferences. Instead, we build a language model based automatic evaluator that\ndecides whether each retrieved document contains a perspective. This allows us\nto evaluate the performance of three different types of corpus (Wikipedia, web\nsnapshot, and corpus constructed on the fly with retrieved pages from the\nsearch engine) paired with retrievers. Retrieving diverse documents remains\nchallenging, with the outputs from existing retrievers covering all\nperspectives on only 33.74% of the examples. We further study the impact of\nquery expansion and diversity-focused reranking approaches and analyze\nretriever sycophancy. Together, we lay the foundation for future studies in\nretrieval diversity handling complex queries.", "published": "2024-09-26 17:52:57", "link": "http://arxiv.org/abs/2409.18110v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Evaluation of Large Language Models for Summarization Tasks in the\n  Medical Domain: A Narrative Review", "abstract": "Large Language Models have advanced clinical Natural Language Generation,\ncreating opportunities to manage the volume of medical text. However, the\nhigh-stakes nature of medicine requires reliable evaluation, which remains a\nchallenge. In this narrative review, we assess the current evaluation state for\nclinical summarization tasks and propose future directions to address the\nresource constraints of expert human evaluation.", "published": "2024-09-26 17:58:26", "link": "http://arxiv.org/abs/2409.18170v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DisGeM: Distractor Generation for Multiple Choice Questions with Span\n  Masking", "abstract": "Recent advancements in Natural Language Processing (NLP) have impacted\nnumerous sub-fields such as natural language generation, natural language\ninference, question answering, and more. However, in the field of question\ngeneration, the creation of distractors for multiple-choice questions (MCQ)\nremains a challenging task. In this work, we present a simple, generic\nframework for distractor generation using readily available Pre-trained\nLanguage Models (PLMs). Unlike previous methods, our framework relies solely on\npre-trained language models and does not require additional training on\nspecific datasets. Building upon previous research, we introduce a two-stage\nframework consisting of candidate generation and candidate selection. Our\nproposed distractor generation framework outperforms previous methods without\nthe need for training or fine-tuning. Human evaluations confirm that our\napproach produces more effective and engaging distractors. The related codebase\nis publicly available at https://github.com/obss/disgem.", "published": "2024-09-26 20:15:46", "link": "http://arxiv.org/abs/2409.18263v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Development and Validation of a Dynamic-Template-Constrained Large\n  Language Model for Generating Fully-Structured Radiology Reports", "abstract": "Current LLMs for creating fully-structured reports face the challenges of\nformatting errors, content hallucinations, and privacy leakage issues when\nuploading data to external servers.We aim to develop an open-source, accurate\nLLM for creating fully-structured and standardized LCS reports from varying\nfree-text reports across institutions and demonstrate its utility in automatic\nstatistical analysis and individual lung nodule retrieval. With IRB approvals,\nour retrospective study included 5,442 de-identified LDCT LCS radiology reports\nfrom two institutions. We constructed two evaluation datasets by labeling 500\npairs of free-text and fully-structured radiology reports and one large-scale\nconsecutive dataset from January 2021 to December 2023. Two radiologists\ncreated a standardized template for recording 27 lung nodule features on LCS.\nWe designed a dynamic-template-constrained decoding method to enhance existing\nLLMs for creating fully-structured reports from free-text radiology reports.\nUsing consecutive structured reports, we automated descriptive statistical\nanalyses and a nodule retrieval prototype. Our best LLM for creating\nfully-structured reports achieved high performance on cross-institutional\ndatasets with an F1 score of about 97%, with neither formatting errors nor\ncontent hallucinations. Our method consistently improved the best open-source\nLLMs by up to 10.42%, and outperformed GPT-4o by 17.19%. The automatically\nderived statistical distributions were consistent with prior findings regarding\nattenuation, location, size, stability, and Lung-RADS. The retrieval system\nwith structured reports allowed flexible nodule-level search and complex\nstatistical analysis. Our developed software is publicly available for local\ndeployment and further research.", "published": "2024-09-26 21:59:11", "link": "http://arxiv.org/abs/2409.18319v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Fairness-Driven Method for Learning Human-Compatible Negotiation\n  Strategies", "abstract": "Despite recent advancements in AI and NLP, negotiation remains a difficult\ndomain for AI agents. Traditional game theoretic approaches that have worked\nwell for two-player zero-sum games struggle in the context of negotiation due\nto their inability to learn human-compatible strategies. On the other hand,\napproaches that only use human data tend to be domain-specific and lack the\ntheoretical guarantees provided by strategies grounded in game theory.\nMotivated by the notion of fairness as a criterion for optimality in general\nsum games, we propose a negotiation framework called FDHC which incorporates\nfairness into both the reward design and search to learn human-compatible\nnegotiation strategies. Our method includes a novel, RL+search technique called\nLGM-Zero which leverages a pre-trained language model to retrieve\nhuman-compatible offers from large action spaces. Our results show that our\nmethod is able to achieve more egalitarian negotiation outcomes and improve\nnegotiation quality.", "published": "2024-09-26 23:16:47", "link": "http://arxiv.org/abs/2409.18335v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language\n  Models", "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated great\nsuccess in many Natural Language Processing (NLP) tasks. In addition to their\ncognitive intelligence, exploring their capabilities in emotional intelligence\nis also crucial, as it enables more natural and empathetic conversational AI.\nRecent studies have shown LLMs' capability in recognizing emotions, but they\noften focus on single emotion labels and overlook the complex and ambiguous\nnature of human emotions. This study is the first to address this gap by\nexploring the potential of LLMs in recognizing ambiguous emotions, leveraging\ntheir strong generalization capabilities and in-context learning. We design\nzero-shot and few-shot prompting and incorporate past dialogue as context\ninformation for ambiguous emotion recognition. Experiments conducted using\nthree datasets indicate significant potential for LLMs in recognizing ambiguous\nemotions, and highlight the substantial benefits of including context\ninformation. Furthermore, our findings indicate that LLMs demonstrate a high\ndegree of effectiveness in recognizing less ambiguous emotions and exhibit\npotential for identifying more ambiguous emotions, paralleling human perceptual\ncapabilities.", "published": "2024-09-26 23:25:21", "link": "http://arxiv.org/abs/2409.18339v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MultiClimate: Multimodal Stance Detection on Climate Change Videos", "abstract": "Climate change (CC) has attracted increasing attention in NLP in recent\nyears. However, detecting the stance on CC in multimodal data is understudied\nand remains challenging due to a lack of reliable datasets. To improve the\nunderstanding of public opinions and communication strategies, this paper\npresents MultiClimate, the first open-source manually-annotated stance\ndetection dataset with $100$ CC-related YouTube videos and $4,209$\nframe-transcript pairs. We deploy state-of-the-art vision and language models,\nas well as multimodal models for MultiClimate stance detection. Results show\nthat text-only BERT significantly outperforms image-only ResNet50 and ViT.\nCombining both modalities achieves state-of-the-art, $0.747$/$0.749$ in\naccuracy/F1. Our 100M-sized fusion models also beat CLIP and BLIP, as well as\nthe much larger 9B-sized multimodal IDEFICS and text-only Llama3 and Gemma2,\nindicating that multimodal stance detection remains challenging for large\nlanguage models. Our code, dataset, as well as supplementary materials, are\navailable at https://github.com/werywjw/MultiClimate.", "published": "2024-09-26 23:48:08", "link": "http://arxiv.org/abs/2409.18346v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Elephant in the Room: Unveiling the Impact of Reward Model Quality in\n  Alignment", "abstract": "The demand for regulating potentially risky behaviors of large language\nmodels (LLMs) has ignited research on alignment methods. Since LLM alignment\nheavily relies on reward models for optimization or evaluation, neglecting the\nquality of reward models may cause unreliable results or even misalignment.\nDespite the vital role reward models play in alignment, previous works have\nconsistently overlooked their performance and used off-the-shelf reward models\narbitrarily without verification, rendering the reward model ``\\emph{an\nelephant in the room}''. To this end, this work first investigates the quality\nof the widely-used preference dataset, HH-RLHF, and curates a clean version,\nCHH-RLHF. Based on CHH-RLHF, we benchmark the accuracy of a broad range of\nreward models used in previous alignment works, unveiling the unreliability of\nusing them both for optimization and evaluation. Furthermore, we systematically\nstudy the impact of reward model quality on alignment performance in three\nreward utilization paradigms. Extensive experiments reveal that better reward\nmodels perform as better human preference proxies. This work aims to awaken\npeople to notice this huge elephant in alignment research. We call attention to\nthe following issues: (1) The reward model needs to be rigorously evaluated,\nwhether for alignment optimization or evaluation. (2) Considering the role of\nreward models, research efforts should not only concentrate on alignment\nalgorithm, but also on developing more reliable human proxy.", "published": "2024-09-26 04:28:35", "link": "http://arxiv.org/abs/2409.19024v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Code Generation and Algorithmic Problem Solving Using Llama 3.1 405B", "abstract": "Code generation by Llama 3.1 models, such as Meta's Llama 3.1 405B,\nrepresents a significant advancement in the field of artificial intelligence,\nparticularly in natural language processing and programming automation. This\npaper explores the capabilities and applications of Llama-driven code\ngeneration, highlighting its ability to translate natural language prompts into\nexecutable code across multiple programming languages. Key features include\ncontextual awareness, multi-language support, and enhanced debugging and\noptimization functionalities. By examining these aspects, we illustrate how\nLlama can serve as a versatile tool for developers of all skill levels,\nimproving productivity and efficiency in software development. The potential\nimplications for education, industry, and the future of coding practices are\nalso discussed, underscoring the transformative impact of AI in programming.\nExperimentation shows that while Llama 3.1 405B performs well with simple\nalgorithmic and data structure based problems, it still struggles with problems\non Quantum Computing, Bioinformatics, and Artificial Intelligence.", "published": "2024-09-26 13:29:20", "link": "http://arxiv.org/abs/2409.19027v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A Novel Spinor-Based Embedding Model for Transformers", "abstract": "This paper proposes a novel approach to word embeddings in Transformer models\nby utilizing spinors from geometric algebra. Spinors offer a rich mathematical\nframework capable of capturing complex relationships and transformations in\nhigh-dimensional spaces. By encoding words as spinors, we aim to enhance the\nexpressiveness and robustness of language representations. We present the\ntheoretical foundations of spinors, detail their integration into Transformer\narchitectures, and discuss potential advantages and challenges.", "published": "2024-09-26 01:18:45", "link": "http://arxiv.org/abs/2410.00038v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Description-based Controllable Text-to-Speech with Cross-Lingual Voice\n  Control", "abstract": "We propose a novel description-based controllable text-to-speech (TTS) method\nwith cross-lingual control capability. To address the lack of audio-description\npaired data in the target language, we combine a TTS model trained on the\ntarget language with a description control model trained on another language,\nwhich maps input text descriptions to the conditional features of the TTS\nmodel. These two models share disentangled timbre and style representations\nbased on self-supervised learning (SSL), allowing for disentangled voice\ncontrol, such as controlling speaking styles while retaining the original\ntimbre. Furthermore, because the SSL-based timbre and style representations are\nlanguage-agnostic, combining the TTS and description control models while\nsharing the same embedding space effectively enables cross-lingual control of\nvoice characteristics. Experiments on English and Japanese TTS demonstrate that\nour method achieves high naturalness and controllability for both languages,\neven though no Japanese audio-description pairs are used.", "published": "2024-09-26 01:08:09", "link": "http://arxiv.org/abs/2409.17452v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RED QUEEN: Safeguarding Large Language Models against Concealed\n  Multi-Turn Jailbreaking", "abstract": "The rapid progress of Large Language Models (LLMs) has opened up new\nopportunities across various domains and applications; yet it also presents\nchallenges related to potential misuse. To mitigate such risks, red teaming has\nbeen employed as a proactive security measure to probe language models for\nharmful outputs via jailbreak attacks. However, current jailbreak attack\napproaches are single-turn with explicit malicious queries that do not fully\ncapture the complexity of real-world interactions. In reality, users can engage\nin multi-turn interactions with LLM-based chat assistants, allowing them to\nconceal their true intentions in a more covert manner. To bridge this gap, we,\nfirst, propose a new jailbreak approach, RED QUEEN ATTACK. This method\nconstructs a multi-turn scenario, concealing the malicious intent under the\nguise of preventing harm. We craft 40 scenarios that vary in turns and select\n14 harmful categories to generate 56k multi-turn attack data points. We conduct\ncomprehensive experiments on the RED QUEEN ATTACK with four representative LLM\nfamilies of different sizes. Our experiments reveal that all LLMs are\nvulnerable to RED QUEEN ATTACK, reaching 87.62% attack success rate on GPT-4o\nand 75.4% on Llama3-70B. Further analysis reveals that larger models are more\nsusceptible to the RED QUEEN ATTACK, with multi-turn structures and concealment\nstrategies contributing to its success. To prioritize safety, we introduce a\nstraightforward mitigation strategy called RED QUEEN GUARD, which aligns LLMs\nto effectively counter adversarial attacks. This approach reduces the attack\nsuccess rate to below 1% while maintaining the model's performance across\nstandard benchmarks. Full implementation and dataset are publicly accessible at\nhttps://github.com/kriti-hippo/red_queen.", "published": "2024-09-26 01:24:17", "link": "http://arxiv.org/abs/2409.17458v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models", "abstract": "Large Language Models (LLMs) are distinguished by their massive parameter\ncounts, which typically result in significant redundancy. This work introduces\nMaskLLM, a learnable pruning method that establishes Semi-structured (or\n``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during\ninference. Instead of developing a new importance criterion, MaskLLM explicitly\nmodels N:M patterns as a learnable distribution through Gumbel Softmax\nsampling. This approach facilitates end-to-end training on large-scale datasets\nand offers two notable advantages: 1) High-quality Masks - our method\neffectively scales to large datasets and learns accurate masks; 2)\nTransferability - the probabilistic modeling of mask distribution enables the\ntransfer learning of sparsity across domains or tasks. We assessed MaskLLM\nusing 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3,\nwith sizes ranging from 843M to 15B parameters, and our empirical results show\nsubstantial improvements over state-of-the-art methods. For instance, leading\napproaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to\nthe dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL\nsolely by learning the masks with frozen weights. Furthermore, MaskLLM's\nlearnable nature allows customized masks for lossless application of 2:4\nsparsity to downstream tasks or domains. Code is available at\nhttps://github.com/NVlabs/MaskLLM.", "published": "2024-09-26 02:37:41", "link": "http://arxiv.org/abs/2409.17481v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Comparing Unidirectional, Bidirectional, and Word2vec Models for\n  Discovering Vulnerabilities in Compiled Lifted Code", "abstract": "Ransomware and other forms of malware cause significant financial and\noperational damage to organizations by exploiting long-standing and often\ndifficult-to-detect software vulnerabilities. To detect vulnerabilities such as\nbuffer overflows in compiled code, this research investigates the application\nof unidirectional transformer-based embeddings, specifically GPT-2. Using a\ndataset of LLVM functions, we trained a GPT-2 model to generate embeddings,\nwhich were subsequently used to build LSTM neural networks to differentiate\nbetween vulnerable and non-vulnerable code. Our study reveals that embeddings\nfrom the GPT-2 model significantly outperform those from bidirectional models\nof BERT and RoBERTa, achieving an accuracy of 92.5% and an F1-score of 89.7%.\nLSTM neural networks were developed with both frozen and unfrozen embedding\nmodel layers. The model with the highest performance was achieved when the\nembedding layers were unfrozen. Further, the research finds that, in exploring\nthe impact of different optimizers within this domain, the SGD optimizer\ndemonstrates superior performance over Adam. Overall, these findings reveal\nimportant insights into the potential of unidirectional transformer-based\napproaches in enhancing cybersecurity defenses.", "published": "2024-09-26 03:48:47", "link": "http://arxiv.org/abs/2409.17513v2", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE", "D.4.6; I.2.6; I.5.1"], "primary_category": "cs.CR"}
{"title": "On the Implicit Relation Between Low-Rank Adaptation and Differential\n  Privacy", "abstract": "A significant approach in natural language processing involves large-scale\npre-training of models on general domain data followed by their adaptation to\nspecific tasks or domains. As models grow in size, full fine-tuning all of\ntheir parameters becomes increasingly impractical. To address this, some\nmethods for low-rank task adaptation of language models have been proposed,\ne.g., LoRA and FLoRA. These methods keep the pre-trained model weights fixed\nand incorporate trainable low-rank decomposition matrices into some layers of\nthe transformer architecture, called adapters. This approach significantly\nreduces the number of trainable parameters required for downstream tasks\ncompared to full fine-tuning all parameters. In this work, we look at low-rank\nadaptation from the lens of data privacy. We show theoretically that the\nlow-rank adaptation used in LoRA and FLoRA leads to the injection of some\nrandom noise into the batch gradients w.r.t the adapter parameters. We quantify\nthe variance of the injected noise and show that the smaller the adaptation\nrank, the larger the noise variance. By establishing a Berry-Esseen type bound\non the total variation distance between distribution of the injected noise and\na Gaussian distribution with the same variance, we show that the dynamics of\nlow-rank adaptation is close to that of differentially private fine-tuning of\nthe adapters. Finally, using Johnson-Lindenstrauss lemma, we show that when\naugmented with gradient scaling, low-rank adaptation is very close to\nperforming DPSGD algorithm with a fixed noise scale to fine-tune the adapters.\nSuggested by our theoretical findings and approved by our experimental results,\nwe show that low-rank adaptation, besides mitigating the space and\ncomputational complexities, implicitly provides a privacy protection w.r.t the\nfine-tuning data, without inducing the high space complexity of DPSGD.", "published": "2024-09-26 04:56:49", "link": "http://arxiv.org/abs/2409.17538v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Modulated Intervention Preference Optimization (MIPO): Keep the Easy,\n  Refine the Difficult", "abstract": "Preference optimization methods typically begin training with a well-trained\nSFT model as a reference model. In RLHF and DPO, a regularization term is used\nduring the preference optimization process to prevent the policy model from\ndeviating too far from the reference model's distribution, thereby avoiding the\ngeneration of anomalous responses. When the reference model is already\nwell-aligned with the given data or only requires slight adjustments, this\napproach can produce a well-aligned model. However, if the reference model is\nnot aligned with the given data and requires significant deviation from its\ncurrent state, a regularization term may actually hinder the model alignment.\nIn this study, we propose \\textbf{Modulated Intervention Preference\nOptimization (MIPO)} to address this issue. MIPO modulates the degree of\nintervention from the reference model based on how well the given data is\naligned with it. If the data is well-aligned, the intervention is increased to\nprevent the policy model from diverging significantly from reference model.\nConversely, if the alignment is poor, the interference is reduced to facilitate\nmore extensive training. We compare the performance of MIPO and DPO using\nMistral-7B and Llama3-8B in Alpaca Eval 2.0 and MT-Bench. The experimental\nresults demonstrate that MIPO consistently outperforms DPO across various\nevaluation scenarios.", "published": "2024-09-26 05:24:14", "link": "http://arxiv.org/abs/2409.17545v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep CLAS: Deep Contextual Listen, Attend and Spell", "abstract": "Contextual-LAS (CLAS) has been shown effective in improving Automatic Speech\nRecognition (ASR) of rare words. It relies on phrase-level contextual modeling\nand attention-based relevance scoring without explicit contextual constraint\nwhich lead to insufficient use of contextual information. In this work, we\npropose deep CLAS to use contextual information better. We introduce bias loss\nforcing model to focus on contextual information. The query of bias attention\nis also enriched to improve the accuracy of the bias attention score. To get\nfine-grained contextual information, we replace phrase-level encoding with\ncharacter-level encoding and encode contextual information with conformer\nrather than LSTM. Moreover, we directly use the bias attention score to correct\nthe output probability distribution of the model. Experiments using the public\nAISHELL-1 and AISHELL-NER. On AISHELL-1, compared to CLAS baselines, deep CLAS\nobtains a 65.78% relative recall and a 53.49% relative F1-score increase in the\nnamed entity recognition scene.", "published": "2024-09-26 07:40:03", "link": "http://arxiv.org/abs/2409.17603v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MIO: A Foundation Model on Multimodal Tokens", "abstract": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.", "published": "2024-09-26 09:57:16", "link": "http://arxiv.org/abs/2409.17692v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric\n  Retrieval Model", "abstract": "A supervised ranking model, despite its advantage of being effective, usually\ninvolves complex processing - typically multiple stages of task-specific\npre-training and fine-tuning. This has motivated researchers to explore simpler\npipelines leveraging large language models (LLMs) that are capable of working\nin a zero-shot manner. However, since zero-shot inference does not make use of\na training set of pairs of queries and their relevant documents, its\nperformance is mostly worse than that of supervised models, which are trained\non such example pairs. Motivated by the existing findings that training\nexamples generally improve zero-shot performance, in our work, we explore if\nthis also applies to ranking models. More specifically, given a query and a\npair of documents, the preference prediction task is improved by augmenting\nexamples of preferences for similar queries from a training set. Our proposed\npairwise few-shot ranker demonstrates consistent improvements over the\nzero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)\nretrieval benchmarks. Our method also achieves a close performance to that of a\nsupervised model without requiring any complex training pipeline.", "published": "2024-09-26 11:19:09", "link": "http://arxiv.org/abs/2409.17745v3", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Are Transformers in Pre-trained LM A Good ASR Encoder? An Empirical\n  Study", "abstract": "In this study, we delve into the efficacy of transformers within pre-trained\nlanguage models (PLMs) when repurposed as encoders for Automatic Speech\nRecognition (ASR). Our underlying hypothesis posits that, despite being\ninitially trained on text-based corpora, these transformers possess a\nremarkable capacity to extract effective features from the input sequence. This\ninherent capability, we argue, is transferrable to speech data, thereby\naugmenting the acoustic modeling ability of ASR. Through rigorous empirical\nanalysis, our findings reveal a notable improvement in Character Error Rate\n(CER) and Word Error Rate (WER) across diverse ASR tasks when transformers from\npre-trained LMs are incorporated. Particularly, they serve as an advantageous\nstarting point for initializing ASR encoders. Furthermore, we uncover that\nthese transformers, when integrated into a well-established ASR encoder, can\nsignificantly boost performance, especially in scenarios where profound\nsemantic comprehension is pivotal. This underscores the potential of leveraging\nthe semantic prowess embedded within pre-trained transformers to advance ASR\nsystems' capabilities.", "published": "2024-09-26 11:31:18", "link": "http://arxiv.org/abs/2409.17750v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SECURE: Semantics-aware Embodied Conversation under Unawareness for\n  Lifelong Robot Learning", "abstract": "This paper addresses a challenging interactive task learning scenario we call\nrearrangement under unawareness: to manipulate a rigid-body environment in a\ncontext where the agent is unaware of a concept that is key to solving the\ninstructed task. We propose SECURE, an interactive task learning framework\ndesigned to solve such problems. It uses embodied conversation to fix its\ndeficient domain model -- through dialogue, the agent discovers and then learns\nto exploit unforeseen possibilities. In particular, SECURE learns from the\nuser's embodied corrective feedback when it makes a mistake, and it makes\nstrategic dialogue decisions to reveal useful evidence about novel concepts for\nsolving the instructed task. Together, these abilities allow the agent to\ngeneralise to subsequent tasks using newly acquired knowledge. We demonstrate\nthat learning to solve rearrangement under unawareness is more data efficient\nwhen the agent is semantics-aware -- that is, during both learning and\ninference it augments the evidence from the user's embodied conversation with\nits logical consequences, stemming from semantic analysis.", "published": "2024-09-26 11:40:07", "link": "http://arxiv.org/abs/2409.17755v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Implementing a Nordic-Baltic Federated Health Data Network: a case\n  report", "abstract": "Background: Centralized collection and processing of healthcare data across\nnational borders pose significant challenges, including privacy concerns, data\nheterogeneity and legal barriers. To address some of these challenges, we\nformed an interdisciplinary consortium to develop a feder-ated health data\nnetwork, comprised of six institutions across five countries, to facilitate\nNordic-Baltic cooperation on secondary use of health data. The objective of\nthis report is to offer early insights into our experiences developing this\nnetwork. Methods: We used a mixed-method ap-proach, combining both experimental\ndesign and implementation science to evaluate the factors affecting the\nimplementation of our network. Results: Technically, our experiments indicate\nthat the network functions without significant performance degradation compared\nto centralized simu-lation. Conclusion: While use of interdisciplinary\napproaches holds a potential to solve challeng-es associated with establishing\nsuch collaborative networks, our findings turn the spotlight on the uncertain\nregulatory landscape playing catch up and the significant operational costs.", "published": "2024-09-26 14:15:54", "link": "http://arxiv.org/abs/2409.17865v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Predicting Anchored Text from Translation Memories for Machine\n  Translation Using Deep Learning Methods", "abstract": "Translation memories (TMs) are the backbone for professional translation\ntools called computer-aided translation (CAT) tools. In order to perform a\ntranslation using a CAT tool, a translator uses the TM to gather translations\nsimilar to the desired segment to translate (s'). Many CAT tools offer a\nfuzzy-match algorithm to locate segments (s) in the TM that are close in\ndistance to s'. After locating two similar segments, the CAT tool will present\nparallel segments (s, t) that contain one segment in the source language along\nwith its translation in the target language. Additionally, CAT tools contain\nfuzzy-match repair (FMR) techniques that will automatically use the parallel\nsegments from the TM to create new TM entries containing a modified version of\nthe original with the idea in mind that it will be the translation of s'. Most\nFMR techniques use machine translation as a way of \"repairing\" those words that\nhave to be modified. In this article, we show that for a large part of those\nwords which are anchored, we can use other techniques that are based on machine\nlearning approaches such as Word2Vec. BERT, and even ChatGPT. Specifically, we\nshow that for anchored words that follow the continuous bag-of-words (CBOW)\nparadigm, Word2Vec, BERT, and GPT-4 can be used to achieve similar and, for\nsome cases, better results than neural machine translation for translating\nanchored words from French to English.", "published": "2024-09-26 15:12:59", "link": "http://arxiv.org/abs/2409.17939v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Translating Technical Terminology: A Translation Workflow for\n  Machine-Translated Acronyms", "abstract": "The typical workflow for a professional translator to translate a document\nfrom its source language (SL) to a target language (TL) is not always focused\non what many language models in natural language processing (NLP) do - predict\nthe next word in a series of words. While high-resource languages like English\nand French are reported to achieve near human parity using common metrics for\nmeasurement such as BLEU and COMET, we find that an important step is being\nmissed: the translation of technical terms, specifically acronyms. Some\nstate-of-the art machine translation systems like Google Translate which are\npublicly available can be erroneous when dealing with acronyms - as much as 50%\nin our findings. This article addresses acronym disambiguation for MT systems\nby proposing an additional step to the SL-TL (FR-EN) translation workflow where\nwe first offer a new acronym corpus for public consumption and then experiment\nwith a search-based thresholding algorithm that achieves nearly 10% increase\nwhen compared to Google Translate and OpusMT.", "published": "2024-09-26 15:18:34", "link": "http://arxiv.org/abs/2409.17943v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weak-to-Strong Backdoor Attack for Large Language Models", "abstract": "Despite being widely applied due to their exceptional capabilities, Large\nLanguage Models (LLMs) have been proven to be vulnerable to backdoor attacks.\nThese attacks introduce targeted vulnerabilities into LLMs by poisoning\ntraining samples and full-parameter fine-tuning. However, this kind of backdoor\nattack is limited since they require significant computational resources,\nespecially as the size of LLMs increases. Besides, parameter-efficient\nfine-tuning (PEFT) offers an alternative but the restricted parameter updating\nmay impede the alignment of triggers with target labels. In this study, we\nfirst verify that backdoor attacks with PEFT may encounter challenges in\nachieving feasible performance. To address these issues and improve the\neffectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack\nalgorithm from weak to strong based on feature alignment-enhanced knowledge\ndistillation (W2SAttack). Specifically, we poison small-scale language models\nthrough full-parameter fine-tuning to serve as the teacher model. The teacher\nmodel then covertly transfers the backdoor to the large-scale student model\nthrough feature alignment-enhanced knowledge distillation, which employs PEFT.\nTheoretical analysis reveals that W2SAttack has the potential to augment the\neffectiveness of backdoor attacks. We demonstrate the superior performance of\nW2SAttack on classification tasks across four language models, four backdoor\nattack algorithms, and two different architectures of teacher models.\nExperimental results indicate success rates close to 100% for backdoor attacks\ntargeting PEFT.", "published": "2024-09-26 15:20:37", "link": "http://arxiv.org/abs/2409.17946v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "An Adversarial Perspective on Machine Unlearning for AI Safety", "abstract": "Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining.", "published": "2024-09-26 16:32:19", "link": "http://arxiv.org/abs/2409.18025v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for\n  Zero-shot Captioning", "abstract": "Recent advancements in image captioning have explored text-only training\nmethods to overcome the limitations of paired image-text data. However,\nexisting text-only training methods often overlook the modality gap between\nusing text data during training and employing images during inference. To\naddress this issue, we propose a novel approach called Image-like Retrieval,\nwhich aligns text features with visually relevant features to mitigate the\nmodality gap. Our method further enhances the accuracy of generated captions by\ndesigning a Fusion Module that integrates retrieved captions with input\nfeatures. Additionally, we introduce a Frequency-based Entity Filtering\ntechnique that significantly improves caption quality. We integrate these\nmethods into a unified framework, which we refer to as IFCap\n($\\textbf{I}$mage-like Retrieval and $\\textbf{F}$requency-based Entity\nFiltering for Zero-shot $\\textbf{Cap}$tioning). Through extensive\nexperimentation, our straightforward yet powerful approach has demonstrated its\nefficacy, outperforming the state-of-the-art methods by a significant margin in\nboth image captioning and video captioning compared to zero-shot captioning\nbased on text-only training.", "published": "2024-09-26 16:47:32", "link": "http://arxiv.org/abs/2409.18046v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Infer Human's Intentions Before Following Natural Language Instructions", "abstract": "For AI agents to be helpful to humans, they should be able to follow natural\nlanguage instructions to complete everyday cooperative tasks in human\nenvironments. However, real human instructions inherently possess ambiguity,\nbecause the human speakers assume sufficient prior knowledge about their hidden\ngoals and intentions. Standard language grounding and planning methods fail to\naddress such ambiguities because they do not model human internal goals as\nadditional partially observable factors in the environment. We propose a new\nframework, Follow Instructions with Social and Embodied Reasoning (FISER),\naiming for better natural language instruction following in collaborative\nembodied tasks. Our framework makes explicit inferences about human goals and\nintentions as intermediate reasoning steps. We implement a set of\nTransformer-based models and evaluate them over a challenging benchmark,\nHandMeThat. We empirically demonstrate that using social reasoning to\nexplicitly infer human intentions before making action plans surpasses purely\nend-to-end approaches. We also compare our implementation with strong\nbaselines, including Chain of Thought prompting on the largest available\npre-trained language models, and find that FISER provides better performance on\nthe embodied social reasoning tasks under investigation, reaching the\nstate-of-the-art on HandMeThat.", "published": "2024-09-26 17:19:49", "link": "http://arxiv.org/abs/2409.18073v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Data-Prep-Kit: getting your data ready for LLM application development", "abstract": "Data preparation is the first and a very important step towards any Large\nLanguage Model (LLM) development. This paper introduces an easy-to-use,\nextensible, and scale-flexible open-source data preparation toolkit called Data\nPrep Kit (DPK). DPK is architected and designed to enable users to scale their\ndata preparation to their needs. With DPK they can prepare data on a local\nmachine or effortlessly scale to run on a cluster with thousands of CPU Cores.\nDPK comes with a highly scalable, yet extensible set of modules that transform\nnatural language and code data. If the user needs additional transforms, they\ncan be easily developed using extensive DPK support for transform creation.\nThese modules can be used independently or pipelined to perform a series of\noperations. In this paper, we describe DPK architecture and show its\nperformance from a small scale to a very large number of CPUs. The modules from\nDPK have been used for the preparation of Granite Models [1] [2]. We believe\nDPK is a valuable contribution to the AI community to easily prepare data to\nenhance the performance of their LLM models or to fine-tune models with\nRetrieval-Augmented Generation (RAG).", "published": "2024-09-26 17:30:28", "link": "http://arxiv.org/abs/2409.18164v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "AI Policy Projector: Grounding LLM Policy Design in Iterative Mapmaking", "abstract": "Whether a large language model policy is an explicit constitution or an\nimplicit reward model, it is challenging to assess coverage over the unbounded\nset of real-world situations that a policy must contend with. We introduce an\nAI policy design process inspired by mapmaking, which has developed tactics for\nvisualizing and iterating on maps even when full coverage is not possible. With\nPolicy Projector, policy designers can survey the landscape of model\ninput-output pairs, define custom regions (e.g., \"violence\"), and navigate\nthese regions with rules that can be applied to LLM outputs (e.g., if output\ncontains \"violence\" and \"graphic details,\" then rewrite without \"graphic\ndetails\"). Policy Projector supports interactive policy authoring using LLM\nclassification and steering and a map visualization reflecting the policy\ndesigner's work. In an evaluation with 12 AI safety experts, our system helps\npolicy designers to address problematic model behaviors extending beyond an\nexisting, comprehensive harm taxonomy.", "published": "2024-09-26 18:34:16", "link": "http://arxiv.org/abs/2409.18203v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "MMMT-IF: A Challenging Multimodal Multi-Turn Instruction Following\n  Benchmark", "abstract": "Evaluating instruction following capabilities for multimodal, multi-turn\ndialogue is challenging. With potentially multiple instructions in the input\nmodel context, the task is time-consuming for human raters and we show LLM\nbased judges are biased towards answers from the same model. We propose\nMMMT-IF, an image based multi-turn Q$\\&$A evaluation set with added global\ninstructions between questions, constraining the answer format. This challenges\nmodels to retrieve instructions dispersed across long dialogues and reason\nunder instruction constraints. All instructions are objectively verifiable\nthrough code execution. We introduce the Programmatic Instruction Following\n($\\operatorname{PIF}$) metric to measure the fraction of the instructions that\nare correctly followed while performing a reasoning task. The\n$\\operatorname{PIF-N-K}$ set of metrics further evaluates robustness by\nmeasuring the fraction of samples in a corpus where, for each sample, at least\nK out of N generated model responses achieve a $\\operatorname{PIF}$ score of\none. The $\\operatorname{PIF}$ metric aligns with human instruction following\nratings, showing 60 percent correlation. Experiments show Gemini 1.5 Pro,\nGPT-4o, and Claude 3.5 Sonnet, have a $\\operatorname{PIF}$ metric that drops\nfrom 0.81 on average at turn 1 across the models, to 0.64 at turn 20. Across\nall turns, when each response is repeated 4 times ($\\operatorname{PIF-4-4}$),\nGPT-4o and Gemini successfully follow all instructions only $11\\%$ of the time.\nWhen all the instructions are also appended to the end of the model input\ncontext, the $\\operatorname{PIF}$ metric improves by 22.3 points on average,\nshowing that the challenge with the task lies not only in following the\ninstructions, but also in retrieving the instructions spread out in the model\ncontext. We plan to open source the MMMT-IF dataset and metric computation\ncode.", "published": "2024-09-26 18:51:46", "link": "http://arxiv.org/abs/2409.18216v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2"], "primary_category": "cs.AI"}
{"title": "Advancing Object Detection in Transportation with Multimodal Large\n  Language Models (MLLMs): A Comprehensive Review and Empirical Testing", "abstract": "This study aims to comprehensively review and empirically evaluate the\napplication of multimodal large language models (MLLMs) and Large Vision Models\n(VLMs) in object detection for transportation systems. In the first fold, we\nprovide a background about the potential benefits of MLLMs in transportation\napplications and conduct a comprehensive review of current MLLM technologies in\nprevious studies. We highlight their effectiveness and limitations in object\ndetection within various transportation scenarios. The second fold involves\nproviding an overview of the taxonomy of end-to-end object detection in\ntransportation applications and future directions. Building on this, we\nproposed empirical analysis for testing MLLMs on three real-world\ntransportation problems that include object detection tasks namely, road safety\nattributes extraction, safety-critical event detection, and visual reasoning of\nthermal images. Our findings provide a detailed assessment of MLLM performance,\nuncovering both strengths and areas for improvement. Finally, we discuss\npractical limitations and challenges of MLLMs in enhancing object detection in\ntransportation, thereby offering a roadmap for future research and development\nin this critical area.", "published": "2024-09-26 20:58:11", "link": "http://arxiv.org/abs/2409.18286v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Realistic Evaluation of Model Merging for Compositional Generalization", "abstract": "Merging has become a widespread way to cheaply combine individual models into\na single model that inherits their capabilities and attains better performance.\nThis popularity has spurred rapid development of many new merging methods,\nwhich are typically validated in disparate experimental settings and frequently\ndiffer in the assumptions made about model architecture, data availability, and\ncomputational budget. In this work, we characterize the relative merits of\ndifferent merging methods by evaluating them in a shared experimental setting\nand precisely identifying the practical requirements of each method.\nSpecifically, our setting focuses on using merging for compositional\ngeneralization of capabilities in image classification, image generation, and\nnatural language processing. Additionally, we measure the computational costs\nof different merging methods as well as how they perform when scaling the\nnumber of models being merged. Taken together, our results clarify the state of\nthe field of model merging and provide a comprehensive and rigorous\nexperimental setup to test new methods.", "published": "2024-09-26 21:44:20", "link": "http://arxiv.org/abs/2409.18314v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A Generalized LLM-Augmented BIM Framework: Application to a\n  Speech-to-BIM system", "abstract": "Performing building information modeling (BIM) tasks is a complex process\nthat imposes a steep learning curve and a heavy cognitive load due to the\nnecessity of remembering sequences of numerous commands. With the rapid\nadvancement of large language models (LLMs), it is foreseeable that BIM tasks,\nincluding querying and managing BIM data, 4D and 5D BIM, design compliance\nchecking, or authoring a design, using written or spoken natural language\n(i.e., text-to-BIM or speech-to-BIM), will soon supplant traditional graphical\nuser interfaces. This paper proposes a generalized LLM-augmented BIM framework\nto expedite the development of LLM-enhanced BIM applications by providing a\nstep-by-step development process. The proposed framework consists of six steps:\ninterpret-fill-match-structure-execute-check. The paper demonstrates the\napplicability of the proposed framework through implementing a speech-to-BIM\napplication, NADIA-S (Natural-language-based Architectural Detailing through\nInteraction with Artificial Intelligence via Speech), using exterior wall\ndetailing as an example.", "published": "2024-09-26 23:46:15", "link": "http://arxiv.org/abs/2409.18345v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Exploring LLM-Driven Explanations for Quantum Algorithms", "abstract": "Background: Quantum computing is a rapidly growing new programming paradigm\nthat brings significant changes to the design and implementation of algorithms.\nUnderstanding quantum algorithms requires knowledge of physics and mathematics,\nwhich can be challenging for software developers.\n  Aims: In this work, we provide a first analysis of how LLMs can support\ndevelopers' understanding of quantum code. Method: We empirically analyse and\ncompare the quality of explanations provided by three widely adopted LLMs\n(Gpt3.5, Llama2, and Tinyllama) using two different human-written prompt styles\nfor seven state-of-the-art quantum algorithms. We also analyse how consistent\nLLM explanations are over multiple rounds and how LLMs can improve existing\ndescriptions of quantum algorithms.\n  Results: Llama2 provides the highest quality explanations from scratch, while\nGpt3.5 emerged as the LLM best suited to improve existing explanations. In\naddition, we show that adding a small amount of context to the prompt\nsignificantly improves the quality of explanations. Finally, we observe how\nexplanations are qualitatively and syntactically consistent over multiple\nrounds.\n  Conclusions: This work highlights promising results, and opens challenges for\nfuture research in the field of LLMs for quantum code explanation. Future work\nincludes refining the methods through prompt optimisation and parsing of\nquantum code explanations, as well as carrying out a systematic assessment of\nthe quality of explanations.", "published": "2024-09-26 18:16:57", "link": "http://arxiv.org/abs/2409.19028v1", "categories": ["cs.CL", "cs.SE", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Heuristics and Biases in AI Decision-Making: Implications for\n  Responsible AGI", "abstract": "We investigate the presence of cognitive biases in three large language\nmodels (LLMs): GPT-4o, Gemma 2, and Llama 3.1. The study uses 1,500 experiments\nacross nine established cognitive biases to evaluate the models' responses and\nconsistency. GPT-4o demonstrated the strongest overall performance. Gemma 2\nshowed strengths in addressing the sunk cost fallacy and prospect theory,\nhowever its performance varied across different biases. Llama 3.1 consistently\nunderperformed, relying on heuristics and exhibiting frequent inconsistencies\nand contradictions. The findings highlight the challenges of achieving robust\nand generalizable reasoning in LLMs, and underscore the need for further\ndevelopment to mitigate biases in artificial general intelligence (AGI). The\nstudy emphasizes the importance of integrating statistical reasoning and\nethical considerations in future AI development.", "published": "2024-09-26 05:34:00", "link": "http://arxiv.org/abs/2410.02820v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Revisiting Acoustic Similarity in Emotional Speech and Music via\n  Self-Supervised Representations", "abstract": "Emotion recognition from speech and music shares similarities due to their\nacoustic overlap, which has led to interest in transferring knowledge between\nthese domains. However, the shared acoustic cues between speech and music,\nparticularly those encoded by Self-Supervised Learning (SSL) models, remain\nlargely unexplored, given the fact that SSL models for speech and music have\nrarely been applied in cross-domain research. In this work, we revisit the\nacoustic similarity between emotion speech and music, starting with an analysis\nof the layerwise behavior of SSL models for Speech Emotion Recognition (SER)\nand Music Emotion Recognition (MER). Furthermore, we perform cross-domain\nadaptation by comparing several approaches in a two-stage fine-tuning process,\nexamining effective ways to utilize music for SER and speech for MER. Lastly,\nwe explore the acoustic similarities between emotional speech and music using\nFrechet audio distance for individual emotions, uncovering the issue of emotion\nbias in both speech and music SSL models. Our findings reveal that while speech\nand music SSL models do capture shared acoustic features, their behaviors can\nvary depending on different emotions due to their training strategies and\ndomain-specificities. Additionally, parameter-efficient fine-tuning can enhance\nSER and MER performance by leveraging knowledge from each other. This study\nprovides new insights into the acoustic similarity between emotional speech and\nmusic, and highlights the potential for cross-domain generalization to improve\nSER and MER systems.", "published": "2024-09-26 14:49:09", "link": "http://arxiv.org/abs/2409.17899v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Freeze and Learn: Continual Learning with Selective Freezing for Speech\n  Deepfake Detection", "abstract": "In speech deepfake detection, one of the critical aspects is developing\ndetectors able to generalize on unseen data and distinguish fake signals across\ndifferent datasets. Common approaches to this challenge involve incorporating\ndiverse data into the training process or fine-tuning models on unseen\ndatasets. However, these solutions can be computationally demanding and may\nlead to the loss of knowledge acquired from previously learned data. Continual\nlearning techniques offer a potential solution to this problem, allowing the\nmodels to learn from unseen data without losing what they have already learned.\nStill, the optimal way to apply these algorithms for speech deepfake detection\nremains unclear, and we do not know which is the best way to apply these\nalgorithms to the developed models. In this paper we address this aspect and\ninvestigate whether, when retraining a speech deepfake detector, it is more\neffective to apply continual learning across the entire model or to update only\nsome of its layers while freezing others. Our findings, validated across\nmultiple models, indicate that the most effective approach among the analyzed\nones is to update only the weights of the initial layers, which are responsible\nfor processing the input features of the detector.", "published": "2024-09-26 07:27:51", "link": "http://arxiv.org/abs/2409.17598v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Paraformer-v2: An improved non-autoregressive transformer for\n  noise-robust speech recognition", "abstract": "Attention-based encoder-decoder, e.g. transformer and its variants, generates\nthe output sequence in an autoregressive (AR) manner. Despite its superior\nperformance, AR model is computationally inefficient as its generation requires\nas many iterations as the output length. In this paper, we propose\nParaformer-v2, an improved version of Paraformer, for fast, accurate, and\nnoise-robust non-autoregressive speech recognition. In Paraformer-v2, we use a\nCTC module to extract the token embeddings, as the alternative to the\ncontinuous integrate-and-fire module in Paraformer. Extensive experiments\ndemonstrate that Paraformer-v2 outperforms Paraformer on multiple datasets,\nespecially on the English datasets (over 14% improvement on WER), and is more\nrobust in noisy environments.", "published": "2024-09-26 11:22:16", "link": "http://arxiv.org/abs/2409.17746v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MC-SEMamba: A Simple Multi-channel Extension of SEMamba", "abstract": "Transformer-based models have become increasingly popular and have impacted\nspeech-processing research owing to their exceptional performance in sequence\nmodeling. Recently, a promising model architecture, Mamba, has emerged as a\npotential alternative to transformer-based models because of its efficient\nmodeling of long sequences. In particular, models like SEMamba have\ndemonstrated the effectiveness of the Mamba architecture in single-channel\nspeech enhancement. This paper aims to adapt SEMamba for multi-channel\napplications with only a small increase in parameters. The resulting system,\nMC-SEMamba, achieved results on the CHiME3 dataset that were comparable or even\nsuperior to several previous baseline models. Additionally, we found that\nincreasing the number of microphones from 1 to 6 improved the speech\nenhancement performance of MC-SEMamba.", "published": "2024-09-26 14:48:21", "link": "http://arxiv.org/abs/2409.17898v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Fly on the Wall -- Exploiting Acoustic Side-Channels in Differential\n  Pressure Sensors", "abstract": "Differential Pressure Sensors are widely deployed to monitor critical\nenvironments. However, our research unveils a previously overlooked\nvulnerability: their high sensitivity to pressure variations makes them\nsusceptible to acoustic side-channel attacks. We demonstrate that the\npressure-sensing diaphragms in DPS can inadvertently capture subtle air\nvibrations caused by speech, which propagate through the sensor's components\nand affect the pressure readings. Exploiting this discovery, we introduce\nBaroVox, a novel attack that reconstructs speech from DPS readings, effectively\nturning DPS into a \"fly on the wall.\" We model the effect of sound on DPS,\nexploring the limits and challenges of acoustic leakage. To overcome these\nchallenges, we propose two solutions: a signal-processing approach using a\nunique spectral subtraction method and a deep learning-based approach for\nkeyword classification. Evaluations under various conditions demonstrate\nBaroVox's effectiveness, achieving a word error rate of 0.29 for manual\nrecognition and 90.51% accuracy for automatic recognition. Our findings\nhighlight the significant privacy implications of this vulnerability. We also\ndiscuss potential defense strategies to mitigate the risks posed by BaroVox.", "published": "2024-09-26 18:46:20", "link": "http://arxiv.org/abs/2409.18213v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Simple but Strong Baseline for Sounding Video Generation: Effective\n  Adaptation of Audio and Video Diffusion Models for Joint Generation", "abstract": "In this work, we build a simple but strong baseline for sounding video\ngeneration. Given base diffusion models for audio and video, we integrate them\nwith additional modules into a single model and train it to make the model\njointly generate audio and video. To enhance alignment between audio-video\npairs, we introduce two novel mechanisms in our model. The first one is\ntimestep adjustment, which provides different timestep information to each base\nmodel. It is designed to align how samples are generated along with timesteps\nacross modalities. The second one is a new design of the additional modules,\ntermed Cross-Modal Conditioning as Positional Encoding (CMC-PE). In CMC-PE,\ncross-modal information is embedded as if it represents temporal position\ninformation, and the embeddings are fed into the model like positional\nencoding. Compared with the popular cross-attention mechanism, CMC-PE provides\na better inductive bias for temporal alignment in the generated data.\nExperimental results validate the effectiveness of the two newly introduced\nmechanisms and also demonstrate that our method outperforms existing methods.", "published": "2024-09-26 05:39:52", "link": "http://arxiv.org/abs/2409.17550v3", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "FlowMAC: Conditional Flow Matching for Audio Coding at Low Bit Rates", "abstract": "This paper introduces FlowMAC, a novel neural audio codec for high-quality\ngeneral audio compression at low bit rates based on conditional flow matching\n(CFM). FlowMAC jointly learns a mel spectrogram encoder, quantizer and decoder.\nAt inference time the decoder integrates a continuous normalizing flow via an\nODE solver to generate a high-quality mel spectrogram. This is the first time\nthat a CFM-based approach is applied to general audio coding, enabling a\nscalable, simple and memory efficient training. Our subjective evaluations show\nthat FlowMAC at 3 kbps achieves similar quality as state-of-the-art GAN-based\nand DDPM-based neural audio codecs at double the bit rate. Moreover, FlowMAC\noffers a tunable inference pipeline, which permits to trade off complexity and\nquality. This enables real-time coding on CPU, while maintaining high\nperceptual quality.", "published": "2024-09-26 08:32:31", "link": "http://arxiv.org/abs/2409.17635v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Prototype based Masked Audio Model for Self-Supervised Learning of Sound\n  Event Detection", "abstract": "A significant challenge in sound event detection (SED) is the effective\nutilization of unlabeled data, given the limited availability of labeled data\ndue to high annotation costs. Semi-supervised algorithms rely on labeled data\nto learn from unlabeled data, and the performance is constrained by the quality\nand size of the former. In this paper, we introduce the Prototype based Masked\nAudio Model~(PMAM) algorithm for self-supervised representation learning in\nSED, to better exploit unlabeled data. Specifically, semantically rich\nframe-level pseudo labels are constructed from a Gaussian mixture model (GMM)\nbased prototypical distribution modeling. These pseudo labels supervise the\nlearning of a Transformer-based masked audio model, in which binary\ncross-entropy loss is employed instead of the widely used InfoNCE loss, to\nprovide independent loss contributions from different prototypes, which is\nimportant in real scenarios in which multiple labels may apply to unsupervised\ndata frames. A final stage of fine-tuning with just a small amount of labeled\ndata yields a very high performing SED model. On like-for-like tests using the\nDESED task, our method achieves a PSDS1 score of 62.5\\%, surpassing current\nstate-of-the-art models and demonstrating the superiority of the proposed\ntechnique.", "published": "2024-09-26 09:07:20", "link": "http://arxiv.org/abs/2409.17656v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Sub-millisecond Latency Real-Time Speech Enhancement Models on\n  Hearables", "abstract": "Low latency models are critical for real-time speech enhancement\napplications, such as hearing aids and hearables. However, the sub-millisecond\nlatency space for resource-constrained hearables remains underexplored. We\ndemonstrate speech enhancement using a computationally efficient minimum-phase\nFIR filter, enabling sample-by-sample processing to achieve mean algorithmic\nlatency of 0.32 ms to 1.25 ms. With a single microphone, we observe a mean\nSI-SDRi of 4.1 dB. The approach shows generalization with a DNSMOS increase of\n0.2 on unseen audio recordings. We use a lightweight LSTM-based model of 626k\nparameters to generate FIR taps. Using a real hardware implementation on a\nlow-power DSP, our system can run with 376 MIPS and a mean end-to-end latency\nof 3.35 ms. In addition, we provide a comparison with existing low-latency\nspectral masking techniques. We hope this work will enable a better\nunderstanding of latency and can be used to improve the comfort and usability\nof hearables.", "published": "2024-09-26 19:31:05", "link": "http://arxiv.org/abs/2409.18239v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
