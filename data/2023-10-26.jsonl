{"title": "FLEEK: Factual Error Detection and Correction with Evidence Retrieved\n  from External Knowledge", "abstract": "Detecting factual errors in textual information, whether generated by large\nlanguage models (LLM) or curated by humans, is crucial for making informed\ndecisions. LLMs' inability to attribute their claims to external knowledge and\ntheir tendency to hallucinate makes it difficult to rely on their responses.\nHumans, too, are prone to factual errors in their writing. Since manual\ndetection and correction of factual errors is labor-intensive, developing an\nautomatic approach can greatly reduce human effort. We present FLEEK, a\nprototype tool that automatically extracts factual claims from text, gathers\nevidence from external knowledge sources, evaluates the factuality of each\nclaim, and suggests revisions for identified errors using the collected\nevidence. Initial empirical evaluation on fact error detection (77-85\\% F1)\nshows the potential of FLEEK. A video demo of FLEEK can be found at\nhttps://youtu.be/NapJFUlkPdQ.", "published": "2023-10-26 03:28:30", "link": "http://arxiv.org/abs/2310.17119v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Test-time Augmentation for Factual Probing", "abstract": "Factual probing is a method that uses prompts to test if a language model\n\"knows\" certain world knowledge facts. A problem in factual probing is that\nsmall changes to the prompt can lead to large changes in model output. Previous\nwork aimed to alleviate this problem by optimizing prompts via text mining or\nfine-tuning. However, such approaches are relation-specific and do not\ngeneralize to unseen relation types. Here, we propose to use test-time\naugmentation (TTA) as a relation-agnostic method for reducing sensitivity to\nprompt variations by automatically augmenting and ensembling prompts at test\ntime. Experiments show improved model calibration, i.e., with TTA, model\nconfidence better reflects prediction accuracy. Improvements in prediction\naccuracy are observed for some models, but for other models, TTA leads to\ndegradation. Error analysis identifies the difficulty of producing high-quality\nprompt variations as the main challenge for TTA.", "published": "2023-10-26 03:41:32", "link": "http://arxiv.org/abs/2310.17121v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M2C: Towards Automatic Multimodal Manga Complement", "abstract": "Multimodal manga analysis focuses on enhancing manga understanding with\nvisual and textual features, which has attracted considerable attention from\nboth natural language processing and computer vision communities. Currently,\nmost comics are hand-drawn and prone to problems such as missing pages, text\ncontamination, and aging, resulting in missing comic text content and seriously\nhindering human comprehension. In other words, the Multimodal Manga Complement\n(M2C) task has not been investigated, which aims to handle the aforementioned\nissues by providing a shared semantic space for vision and language\nunderstanding. To this end, we first propose the Multimodal Manga Complement\ntask by establishing a new M2C benchmark dataset covering two languages. First,\nwe design a manga argumentation method called MCoT to mine event knowledge in\ncomics with large language models. Then, an effective baseline FVP-M$^{2}$\nusing fine-grained visual prompts is proposed to support manga complement.\nExtensive experimental results show the effectiveness of FVP-M$^{2}$ method for\nMultimodal Mange Complement.", "published": "2023-10-26 04:10:16", "link": "http://arxiv.org/abs/2310.17130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity", "abstract": "Cross-lingual transfer (XLT) is an emergent ability of multilingual language\nmodels that preserves their performance on a task to a significant extent when\nevaluated in languages that were not included in the fine-tuning process. While\nEnglish, due to its widespread usage, is typically regarded as the primary\nlanguage for model adaption in various tasks, recent studies have revealed that\nthe efficacy of XLT can be amplified by selecting the most appropriate source\nlanguages based on specific conditions. In this work, we propose the\nutilization of sub-network similarity between two languages as a proxy for\npredicting the compatibility of the languages in the context of XLT. Our\napproach is model-oriented, better reflecting the inner workings of foundation\nmodels. In addition, it requires only a moderate amount of raw text from\ncandidate languages, distinguishing it from the majority of previous methods\nthat rely on external resources. In experiments, we demonstrate that our method\nis more effective than baselines across diverse tasks. Specifically, it shows\nproficiency in ranking candidates for zero-shot XLT, achieving an improvement\nof 4.6% on average in terms of NDCG@3. We also provide extensive analyses that\nconfirm the utility of sub-networks for XLT prediction.", "published": "2023-10-26 05:39:49", "link": "http://arxiv.org/abs/2310.17166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual\n  Representation Learning", "abstract": "Expressing universal semantics common to all languages is helpful in\nunderstanding the meanings of complex and culture-specific sentences. The\nresearch theme underlying this scenario focuses on learning universal\nrepresentations across languages with the usage of massive parallel corpora.\nHowever, due to the sparsity and scarcity of parallel data, there is still a\nbig challenge in learning authentic ``universals'' for any two languages. In\nthis paper, we propose EMMA-X: an EM-like Multilingual pre-training Algorithm,\nto learn (X)Cross-lingual universals with the aid of excessive multilingual\nnon-parallel data. EMMA-X unifies the cross-lingual representation learning\ntask and an extra semantic relation prediction task within an EM framework.\nBoth the extra semantic classifier and the cross-lingual sentence encoder\napproximate the semantic relation of two sentences, and supervise each other\nuntil convergence. To evaluate EMMA-X, we conduct experiments on XRETE, a newly\nintroduced benchmark containing 12 widely studied cross-lingual tasks that\nfully depend on sentence-level representations. Results reveal that EMMA-X\nachieves state-of-the-art performance. Further geometric analysis of the built\nrepresentation space with three requirements demonstrates the superiority of\nEMMA-X over advanced models.", "published": "2023-10-26 08:31:00", "link": "http://arxiv.org/abs/2310.17233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Role of Input Token Characters in Language Models: How\n  Does Information Loss Affect Performance?", "abstract": "Understanding how and what pre-trained language models (PLMs) learn about\nlanguage is an open challenge in natural language processing. Previous work has\nfocused on identifying whether they capture semantic and syntactic information,\nand how the data or the pre-training objective affects their performance.\nHowever, to the best of our knowledge, no previous work has specifically\nexamined how information loss in input token characters affects the performance\nof PLMs. In this study, we address this gap by pre-training language models\nusing small subsets of characters from individual tokens. Surprisingly, we find\nthat pre-training even under extreme settings, i.e. using only one character of\neach token, the performance retention in standard NLU benchmarks and probing\ntasks compared to full-token models is high. For instance, a model pre-trained\nonly on single first characters from tokens achieves performance retention of\napproximately $90$\\% and $77$\\% of the full-token model in SuperGLUE and GLUE\ntasks, respectively.", "published": "2023-10-26 09:47:50", "link": "http://arxiv.org/abs/2310.17271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Logical Forms improve fidelity in Table-to-Text generation", "abstract": "Table-to-text systems generate natural language statements from structured\ndata like tables. While end-to-end techniques suffer from low factual\ncorrectness (fidelity), a previous study reported gains when using manual\nlogical forms (LF) that represent the selected content and the semantics of the\ntarget text. Given the manual step, it was not clear whether automatic LFs\nwould be effective, or whether the improvement came from content selection\nalone. We present TlT which, given a table and a selection of the content,\nfirst produces LFs and then the textual statement. We show for the first time\nthat automatic LFs improve quality, with an increase in fidelity of 30 points\nover a comparable system not using LFs. Our experiments allow to quantify the\nremaining challenges for high factual correctness, with automatic selection of\ncontent coming first, followed by better Logic-to-Text generation and, to a\nlesser extent, better Table-to-Logic parsing.", "published": "2023-10-26 10:00:24", "link": "http://arxiv.org/abs/2310.17279v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Abstract with Nonparametric Variational Information\n  Bottleneck", "abstract": "Learned representations at the level of characters, sub-words, words and\nsentences, have each contributed to advances in understanding different NLP\ntasks and linguistic phenomena. However, learning textual embeddings is costly\nas they are tokenization specific and require different models to be trained\nfor each level of abstraction. We introduce a novel language representation\nmodel which can learn to compress to different levels of abstraction at\ndifferent layers of the same model. We apply Nonparametric Variational\nInformation Bottleneck (NVIB) to stacked Transformer self-attention layers in\nthe encoder, which encourages an information-theoretic compression of the\nrepresentations through the model. We find that the layers within the model\ncorrespond to increasing levels of abstraction and that their representations\nare more linguistically informed. Finally, we show that NVIB compression\nresults in a model which is more robust to adversarial perturbations.", "published": "2023-10-26 10:04:31", "link": "http://arxiv.org/abs/2310.17284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Ensemble Method Based on the Combination of Transformers with\n  Convolutional Neural Networks to Detect Artificially Generated Text", "abstract": "Thanks to the state-of-the-art Large Language Models (LLMs), language\ngeneration has reached outstanding levels. These models are capable of\ngenerating high quality content, thus making it a challenging task to detect\ngenerated text from human-written content. Despite the advantages provided by\nNatural Language Generation, the inability to distinguish automatically\ngenerated text can raise ethical concerns in terms of authenticity.\nConsequently, it is important to design and develop methodologies to detect\nartificial content. In our work, we present some classification models\nconstructed by ensembling transformer models such as Sci-BERT, DeBERTa and\nXLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate\nthat the considered ensemble architectures surpass the performance of the\nindividual transformer models for classification. Furthermore, the proposed\nSciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared\ntask 2023 data.", "published": "2023-10-26 11:17:03", "link": "http://arxiv.org/abs/2310.17312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nabra: Syrian Arabic Dialects with Morphological Annotations", "abstract": "This paper presents Nabra, a corpora of Syrian Arabic dialects with\nmorphological annotations. A team of Syrian natives collected more than 6K\nsentences containing about 60K words from several sources including social\nmedia posts, scripts of movies and series, lyrics of songs and local proverbs\nto build Nabra. Nabra covers several local Syrian dialects including those of\nAleppo, Damascus, Deir-ezzur, Hama, Homs, Huran, Latakia, Mardin, Raqqah, and\nSuwayda. A team of nine annotators annotated the 60K tokens with full\nmorphological annotations across sentence contexts. We trained the annotators\nto follow methodological annotation guidelines to ensure unique morpheme\nannotations, and normalized the annotations. F1 and kappa agreement scores\nranged between 74% and 98% across features, showing the excellent quality of\nNabra annotations. Our corpora are open-source and publicly available as part\nof the Currasat portal https://sina.birzeit.edu/currasat.", "published": "2023-10-26 11:23:05", "link": "http://arxiv.org/abs/2310.17315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Fine-Grained Entity Recognition", "abstract": "Traditional NER systems are typically trained to recognize coarse-grained\nentities, and less attention is given to classifying entities into a hierarchy\nof fine-grained lower-level subtypes. This article aims to advance Arabic NER\nwith fine-grained entities. We chose to extend Wojood (an open-source Nested\nArabic Named Entity Corpus) with subtypes. In particular, four main entity\ntypes in Wojood, geopolitical entity (GPE), location (LOC), organization (ORG),\nand facility (FAC), are extended with 31 subtypes. To do this, we first revised\nWojood's annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC's\nACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC,\nORG, and FAC (~44K) in Wojood are manually annotated with the LDC's ACE\nsub-types. We refer to this extended version of Wojood as WojoodF ine. To\nevaluate our annotations, we measured the inter-annotator agreement (IAA) using\nboth Cohen's Kappa and F1 score, resulting in 0.9861 and 0.9889, respectively.\nTo compute the baselines of WojoodF ine, we fine-tune three pre-trained Arabic\nBERT encoders in three settings: flat NER, nested NER and nested NER with\nsubtypes and achieved F1 score of 0.920, 0.866, and 0.885, respectively. Our\ncorpus and models are open-source and available at\nhttps://sina.birzeit.edu/wojood/.", "published": "2023-10-26 11:59:45", "link": "http://arxiv.org/abs/2310.17333v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ACT-SQL: In-Context Learning for Text-to-SQL with\n  Automatically-Generated Chain-of-Thought", "abstract": "Recently Large Language Models (LLMs) have been proven to have strong\nabilities in various domains and tasks. We study the problem of prompt\ndesigning in the text-to-SQL task and attempt to improve the LLMs' reasoning\nability when generating SQL queries. Besides the trivial few-shot in-context\nlearning setting, we design our chain-of-thought (CoT) prompt with a similar\nmethod to schema linking. We provide a method named ACT-SQL to automatically\ngenerate auto-CoT exemplars and thus the whole process doesn't need manual\nlabeling. Our approach is cost-saving since we only use the LLMs' API call once\nwhen generating one SQL query. Furthermore, we extend our in-context learning\nmethod to the multi-turn text-to-SQL task. The experiment results show that the\nLLMs' performance can benefit from our ACT-SQL approach. Our approach achieves\nSOTA performance on the Spider dev set among existing in-context learning\napproaches.", "published": "2023-10-26 12:16:25", "link": "http://arxiv.org/abs/2310.17342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language and Mental Health: Measures of Emotion Dynamics from Text as\n  Linguistic Biosocial Markers", "abstract": "Research in psychopathology has shown that, at an aggregate level, the\npatterns of emotional change over time -- emotion dynamics -- are indicators of\none's mental health. One's patterns of emotion change have traditionally been\ndetermined through self-reports of emotions; however, there are known issues\nwith accuracy, bias, and ease of data collection. Recent approaches to\ndetermining emotion dynamics from one's everyday utterances addresses many of\nthese concerns, but it is not yet known whether these measures of utterance\nemotion dynamics (UED) correlate with mental health diagnoses. Here, for the\nfirst time, we study the relationship between tweet emotion dynamics and mental\nhealth disorders. We find that each of the UED metrics studied varied by the\nuser's self-disclosed diagnosis. For example: average valence was significantly\nhigher (i.e., more positive text) in the control group compared to users with\nADHD, MDD, and PTSD. Valence variability was significantly lower in the control\ngroup compared to ADHD, depression, bipolar disorder, MDD, PTSD, and OCD but\nnot PPD. Rise and recovery rates of valence also exhibited significant\ndifferences from the control. This work provides important early evidence for\nhow linguistic cues pertaining to emotion dynamics can play a crucial role as\nbiosocial markers for mental illnesses and aid in the understanding, diagnosis,\nand management of mental health disorders.", "published": "2023-10-26 13:00:26", "link": "http://arxiv.org/abs/2310.17369v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meaning and understanding in large language models", "abstract": "Can a machine understand the meanings of natural language? Recent\ndevelopments in the generative large language models (LLMs) of artificial\nintelligence have led to the belief that traditional philosophical assumptions\nabout machine understanding of language need to be revised. This article\ncritically evaluates the prevailing tendency to regard machine language\nperformance as mere syntactic manipulation and the simulation of understanding,\nwhich is only partial and very shallow, without sufficient referential\ngrounding in the world. The aim is to highlight the conditions crucial to\nattributing natural language understanding to state-of-the-art LLMs, where it\ncan be legitimately argued that LLMs not only use syntax but also semantics,\ntheir understanding not being simulated but duplicated; and determine how they\nground the meanings of linguistic expressions.", "published": "2023-10-26 14:06:14", "link": "http://arxiv.org/abs/2310.17407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases", "abstract": "We propose a comprehensive study of one-stage elicitation techniques for\nquerying a large pre-trained generative transformer (GPT-3.5-turbo) in the\nrhetorical role prediction task of legal cases. This task is known as requiring\ntextual context to be addressed. Our study explores strategies such as zero-few\nshots, task specification with definitions and clarification of annotation\nambiguities, textual context and reasoning with general prompts and specific\nquestions. We show that the number of examples, the definition of labels, the\npresentation of the (labelled) textual context and specific questions about\nthis context have a positive influence on the performance of the model. Given\nnon-equivalent test set configurations, we observed that prompting with a few\nlabelled examples from direct context can lead the model to a better\nperformance than a supervised fined-tuned multi-class classifier based on the\nBERT encoder (weighted F1 score of = 72%). But there is still a gap to reach\nthe performance of the best systems = 86%) in the LegalEval 2023 task which, on\nthe other hand, require dedicated resources, architectures and training.", "published": "2023-10-26 14:19:48", "link": "http://arxiv.org/abs/2310.17413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT\n  Generated English Text", "abstract": "Language serves as a powerful tool for the manifestation of societal belief\nsystems. In doing so, it also perpetuates the prevalent biases in our society.\nGender bias is one of the most pervasive biases in our society and is seen in\nonline and offline discourses. With LLMs increasingly gaining human-like\nfluency in text generation, gaining a nuanced understanding of the biases these\nsystems can generate is imperative. Prior work often treats gender bias as a\nbinary classification task. However, acknowledging that bias must be perceived\nat a relative scale; we investigate the generation and consequent receptivity\nof manual annotators to bias of varying degrees. Specifically, we create the\nfirst dataset of GPT-generated English text with normative ratings of gender\nbias. Ratings were obtained using Best--Worst Scaling -- an efficient\ncomparative annotation framework. Next, we systematically analyze the variation\nof themes of gender biases in the observed ranking and show that\nidentity-attack is most closely related to gender bias. Finally, we show the\nperformance of existing automated models trained on related concepts on our\ndataset.", "published": "2023-10-26 14:34:06", "link": "http://arxiv.org/abs/2310.17428v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Validity of Evaluation Results: Assessing Concurrence Across\n  Compositionality Benchmarks", "abstract": "NLP models have progressed drastically in recent years, according to numerous\ndatasets proposed to evaluate performance. Questions remain, however, about how\nparticular dataset design choices may impact the conclusions we draw about\nmodel capabilities. In this work, we investigate this question in the domain of\ncompositional generalization. We examine the performance of six modeling\napproaches across 4 datasets, split according to 8 compositional splitting\nstrategies, ranking models by 18 compositional generalization splits in total.\nOur results show that: i) the datasets, although all designed to evaluate\ncompositional generalization, rank modeling approaches differently; ii)\ndatasets generated by humans align better with each other than they with\nsynthetic datasets, or than synthetic datasets among themselves; iii)\ngenerally, whether datasets are sampled from the same source is more predictive\nof the resulting model ranking than whether they maintain the same\ninterpretation of compositionality; and iv) which lexical items are used in the\ndata can strongly impact conclusions. Overall, our results demonstrate that\nmuch work remains to be done when it comes to assessing whether popular\nevaluation datasets measure what they intend to measure, and suggest that\nelucidating more rigorous standards for establishing the validity of evaluation\nsets could benefit the field.", "published": "2023-10-26 16:11:04", "link": "http://arxiv.org/abs/2310.17514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct\n  Speech-to-Speech Translation", "abstract": "While Diffusion Generative Models have achieved great success on image\ngeneration tasks, how to efficiently and effectively incorporate them into\nspeech generation especially translation tasks remains a non-trivial problem.\nSpecifically, due to the low information density of speech data, the\ntransformed discrete speech unit sequence is much longer than the corresponding\ntext transcription, posing significant challenges to existing auto-regressive\nmodels. Furthermore, it is not optimal to brutally apply discrete diffusion on\nthe speech unit sequence while disregarding the continuous space structure,\nwhich will degrade the generation performance significantly. In this paper, we\npropose a novel diffusion model by applying the diffusion forward process in\nthe \\textit{continuous} speech representation space, while employing the\ndiffusion backward process in the \\textit{discrete} speech unit space. In this\nway, we preserve the semantic structure of the continuous speech representation\nspace in the diffusion process and integrate the continuous and discrete\ndiffusion models. We conduct extensive experiments on the textless direct\nspeech-to-speech translation task, where the proposed method achieves\ncomparable results to the computationally intensive auto-regressive baselines\n(500 steps on average) with significantly fewer decoding steps (50 steps).", "published": "2023-10-26 16:58:14", "link": "http://arxiv.org/abs/2310.17570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Global Voices, Local Biases: Socio-Cultural Prejudices across Languages", "abstract": "Human biases are ubiquitous but not uniform: disparities exist across\nlinguistic, cultural, and societal borders. As large amounts of recent\nliterature suggest, language models (LMs) trained on human data can reflect and\noften amplify the effects of these social biases. However, the vast majority of\nexisting studies on bias are heavily skewed towards Western and European\nlanguages. In this work, we scale the Word Embedding Association Test (WEAT) to\n24 languages, enabling broader studies and yielding interesting findings about\nLM bias. We additionally enhance this data with culturally relevant information\nfor each language, capturing local contexts on a global scale. Further, to\nencompass more widely prevalent societal biases, we examine new bias dimensions\nacross toxicity, ableism, and more. Moreover, we delve deeper into the Indian\nlinguistic landscape, conducting a comprehensive regional bias analysis across\nsix prevalent Indian languages. Finally, we highlight the significance of these\nsocial biases and the new dimensions through an extensive comparison of\nembedding methods, reinforcing the need to address them in pursuit of more\nequitable language models. All code, data and results are available here:\nhttps://github.com/iamshnoo/weathub.", "published": "2023-10-26 17:07:50", "link": "http://arxiv.org/abs/2310.17586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lil-Bevo: Explorations of Strategies for Training Language Models in\n  More Humanlike Ways", "abstract": "We present Lil-Bevo, our submission to the BabyLM Challenge. We pretrained\nour masked language models with three ingredients: an initial pretraining with\nmusic data, training on shorter sequences before training on longer ones, and\nmasking specific tokens to target some of the BLiMP subtasks. Overall, our\nbaseline models performed above chance, but far below the performance levels of\nlarger LLMs trained on more data. We found that training on short sequences\nperformed better than training on longer sequences.Pretraining on music may\nhelp performance marginally, but, if so, the effect seems small. Our targeted\nMasked Language Modeling augmentation did not seem to improve model performance\nin general, but did seem to help on some of the specific BLiMP tasks that we\nwere targeting (e.g., Negative Polarity Items). Training performant LLMs on\nsmall amounts of data is a difficult but potentially informative task. While\nsome of our techniques showed some promise, more work is needed to explore\nwhether they can improve performance more than the modest gains here. Our code\nis available at https://github.com/venkatasg/Lil-Bevo and out models at\nhttps://huggingface.co/collections/venkatasg/babylm-653591cdb66f4bf68922873a", "published": "2023-10-26 17:13:07", "link": "http://arxiv.org/abs/2310.17591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InstOptima: Evolutionary Multi-objective Instruction Optimization via\n  Large Language Model-based Instruction Operators", "abstract": "Instruction-based language modeling has received significant attention in\npretrained language models. However, the efficiency of instruction engineering\nremains low and hinders the development of instruction studies. Recent studies\nhave focused on automating instruction generation, but they primarily aim to\nimprove performance without considering other crucial objectives that impact\ninstruction quality, such as instruction length and perplexity. Therefore, we\npropose a novel approach (i.e., InstOptima) that treats instruction generation\nas an evolutionary multi-objective optimization problem. In contrast to text\nedition-based methods, our approach utilizes a large language model (LLM) to\nsimulate instruction operators, including mutation and crossover. Furthermore,\nwe introduce an objective-guided mechanism for these operators, allowing the\nLLM to comprehend the objectives and enhance the quality of the generated\ninstructions. Experimental results demonstrate improved fine-tuning performance\nand the generation of a diverse set of high-quality instructions.", "published": "2023-10-26 17:48:45", "link": "http://arxiv.org/abs/2310.17630v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-contrastive sentence representations via self-supervision", "abstract": "Sample contrastive methods, typically referred to simply as contrastive are\nthe foundation of most unsupervised methods to learn text and sentence\nembeddings. On the other hand, a different class of self-supervised loss\nfunctions and methods have been considered in the computer vision community and\nreferred to as dimension contrastive. In this paper, we thoroughly compare this\nclass of methods with the standard baseline for contrastive sentence\nembeddings, SimCSE. We find that self-supervised embeddings trained using\ndimension contrastive objectives can outperform SimCSE on downstream tasks\nwithout needing auxiliary loss functions.", "published": "2023-10-26 18:00:00", "link": "http://arxiv.org/abs/2310.17690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The impact of responding to patient messages with large language model\n  assistance", "abstract": "Documentation burden is a major contributor to clinician burnout, which is\nrising nationally and is an urgent threat to our ability to care for patients.\nArtificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician\nburden by assisting with documentation. Although many hospitals are actively\nintegrating such systems into electronic medical record systems, AI chatbots\nutility and impact on clinical decision-making have not been studied for this\nintended use. We are the first to examine the utility of large language models\nin assisting clinicians draft responses to patient questions. In our two-stage\ncross-sectional study, 6 oncologists responded to 100 realistic synthetic\ncancer patient scenarios and portal messages developed to reflect common\nmedical situations, first manually, then with AI assistance.\n  We find AI-assisted responses were longer, less readable, but provided\nacceptable drafts without edits 58% of time. AI assistance improved efficiency\n77% of time, with low harm risk (82% safe). However, 7.7% unedited AI responses\ncould severely harm. In 31% cases, physicians thought AI drafts were\nhuman-written. AI assistance led to more patient education recommendations,\nfewer clinical actions than manual responses. Results show promise for AI to\nimprove clinician efficiency and patient care through assisting documentation,\nif used judiciously. Monitoring model outputs and human-AI interaction remains\ncrucial for safe implementation.", "published": "2023-10-26 18:03:46", "link": "http://arxiv.org/abs/2310.17703v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Explanation the Cure? Misinformation Mitigation in the Short Term and\n  Long Term", "abstract": "With advancements in natural language processing (NLP) models, automatic\nexplanation generation has been proposed to mitigate misinformation on social\nmedia platforms in addition to adding warning labels to identified fake news.\nWhile many researchers have focused on generating good explanations, how these\nexplanations can really help humans combat fake news is under-explored. In this\nstudy, we compare the effectiveness of a warning label and the state-of-the-art\ncounterfactual explanations generated by GPT-4 in debunking misinformation. In\na two-wave, online human-subject study, participants (N = 215) were randomly\nassigned to a control group in which false contents are shown without any\nintervention, a warning tag group in which the false claims were labeled, or an\nexplanation group in which the false contents were accompanied by GPT-4\ngenerated explanations. Our results show that both interventions significantly\ndecrease participants' self-reported belief in fake claims in an equivalent\nmanner for the short-term and long-term. We discuss the implications of our\nfindings and directions for future NLP-based misinformation debunking\nstrategies.", "published": "2023-10-26 18:12:02", "link": "http://arxiv.org/abs/2310.17711v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Multilingual Coreference Resolution by Universal\n  Annotations", "abstract": "Multilingual coreference resolution (MCR) has been a long-standing and\nchallenging task. With the newly proposed multilingual coreference dataset,\nCorefUD (Nedoluzhko et al., 2022), we conduct an investigation into the task by\nusing its harmonized universal morphosyntactic and coreference annotations.\nFirst, we study coreference by examining the ground truth data at different\nlinguistic levels, namely mention, entity and document levels, and across\ndifferent genres, to gain insights into the characteristics of coreference\nacross multiple languages. Second, we perform an error analysis of the most\nchallenging cases that the SotA system fails to resolve in the CRAC 2022 shared\ntask using the universal annotations. Last, based on this analysis, we extract\nfeatures from universal morphosyntactic annotations and integrate these\nfeatures into a baseline system to assess their potential benefits for the MCR\ntask. Our results show that our best configuration of features improves the\nbaseline by 0.9% F1 score.", "published": "2023-10-26 18:50:04", "link": "http://arxiv.org/abs/2310.17734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural\n  Languages", "abstract": "Building multi-modal language models has been a trend in the recent years,\nwhere additional modalities such as image, video, speech, etc. are jointly\nlearned along with natural languages (i.e., textual information). Despite the\nsuccess of these multi-modal language models with different modalities, there\nis no existing solution for neural network architectures and natural languages.\nProviding neural architectural information as a new modality allows us to\nprovide fast architecture-2-text and text-2-architecture retrieval/generation\nservices on the cloud with a single inference. Such solution is valuable in\nterms of helping beginner and intermediate ML users to come up with better\nneural architectures or AutoML approaches with a simple text query. In this\npaper, we propose ArchBERT, a bi-modal model for joint learning and\nunderstanding of neural architectures and natural languages, which opens up new\navenues for research in this area. We also introduce a pre-training strategy\nnamed Masked Architecture Modeling (MAM) for a more generalized joint learning.\nMoreover, we introduce and publicly release two new bi-modal datasets for\ntraining and validating our methods. The ArchBERT's performance is verified\nthrough a set of numerical experiments on different downstream tasks such as\narchitecture-oriented reasoning, question answering, and captioning\n(summarization). Datasets, codes, and demos are available supplementary\nmaterials.", "published": "2023-10-26 18:58:52", "link": "http://arxiv.org/abs/2310.17737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StyleBART: Decorate Pretrained Model with Style Adapters for\n  Unsupervised Stylistic Headline Generation", "abstract": "Stylistic headline generation is the task to generate a headline that not\nonly summarizes the content of an article, but also reflects a desired style\nthat attracts users. As style-specific article-headline pairs are scarce,\nprevious researches focus on unsupervised approaches with a standard headline\ngeneration dataset and mono-style corpora. In this work, we follow this line\nand propose StyleBART, an unsupervised approach for stylistic headline\ngeneration. Our method decorates the pretrained BART model with adapters that\nare responsible for different styles and allows the generation of headlines\nwith diverse styles by simply switching the adapters. Different from previous\nworks, StyleBART separates the task of style learning and headline generation,\nmaking it possible to freely combine the base model and the style adapters\nduring inference. We further propose an inverse paraphrasing task to enhance\nthe style adapters. Extensive automatic and human evaluations show that\nStyleBART achieves new state-of-the-art performance in the unsupervised\nstylistic headline generation task, producing high-quality headlines with the\ndesired style.", "published": "2023-10-26 19:31:22", "link": "http://arxiv.org/abs/2310.17743v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Framework for Automated Measurement of Responsible AI Harms in\n  Generative AI Applications", "abstract": "We present a framework for the automated measurement of responsible AI (RAI)\nmetrics for large language models (LLMs) and associated products and services.\nOur framework for automatically measuring harms from LLMs builds on existing\ntechnical and sociotechnical expertise and leverages the capabilities of\nstate-of-the-art LLMs, such as GPT-4. We use this framework to run through\nseveral case studies investigating how different LLMs may violate a range of\nRAI-related principles. The framework may be employed alongside domain-specific\nsociotechnical expertise to create measurements for new harm areas in the\nfuture. By implementing this framework, we aim to enable more advanced harm\nmeasurement efforts and further the responsible use of LLMs.", "published": "2023-10-26 19:45:06", "link": "http://arxiv.org/abs/2310.17750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Words, Subwords, and Morphemes: What Really Matters in the\n  Surprisal-Reading Time Relationship?", "abstract": "An important assumption that comes with using LLMs on psycholinguistic data\nhas gone unverified. LLM-based predictions are based on subword tokenization,\nnot decomposition of words into morphemes. Does that matter? We carefully test\nthis by comparing surprisal estimates using orthographic, morphological, and\nBPE tokenization against reading time data. Our results replicate previous\nfindings and provide evidence that in the aggregate, predictions using BPE\ntokenization do not suffer relative to morphological and orthographic\nsegmentation. However, a finer-grained analysis points to potential issues with\nrelying on BPE-based tokenization, as well as providing promising results\ninvolving morphologically-aware surprisal estimates and suggesting a new method\nfor evaluating morphological prediction.", "published": "2023-10-26 20:55:29", "link": "http://arxiv.org/abs/2310.17774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the\n  Automatic Ordering of Events in News Articles", "abstract": "Temporal relation extraction models have thus far been hindered by a number\nof issues in existing temporal relation-annotated news datasets, including: (1)\nlow inter-annotator agreement due to the lack of specificity of their\nannotation guidelines in terms of what counts as a temporal relation; (2) the\nexclusion of long-distance relations within a given document (those spanning\nacross different paragraphs); and (3) the exclusion of events that are not\ncentred on verbs. This paper aims to alleviate these issues by presenting a new\nannotation scheme that clearly defines the criteria based on which temporal\nrelations should be annotated. Additionally, the scheme includes events even if\nthey are not expressed as verbs (e.g., nominalised events). Furthermore, we\npropose a method for annotating all temporal relations -- including\nlong-distance ones -- which automates the process, hence reducing time and\nmanual effort on the part of annotators. The result is a new dataset, the\nTIMELINE corpus, in which improved inter-annotator agreement was obtained, in\ncomparison with previously reported temporal relation datasets. We report the\nresults of training and evaluating baseline temporal relation extraction models\non the new corpus, and compare them with results obtained on the widely used\nMATRES corpus.", "published": "2023-10-26 22:23:38", "link": "http://arxiv.org/abs/2310.17802v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Probing Signals into Multimodal Machine Translation via\n  Visual Question-Answering Pairs", "abstract": "This paper presents an in-depth study of multimodal machine translation\n(MMT), examining the prevailing understanding that MMT systems exhibit\ndecreased sensitivity to visual information when text inputs are complete.\nInstead, we attribute this phenomenon to insufficient cross-modal interaction,\nrather than image information redundancy. A novel approach is proposed to\ngenerate parallel Visual Question-Answering (VQA) style pairs from the source\ntext, fostering more robust cross-modal interaction. Using Large Language\nModels (LLMs), we explicitly model the probing signal in MMT to convert it into\nVQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask\nlearning framework is introduced to incorporate explicit probing signals from\nthe dataset into the MMT training process. Experimental results on two\nwidely-used benchmarks demonstrate the effectiveness of this novel approach.\nOur code and data would be available at:\n\\url{https://github.com/libeineu/MMT-VQA}.", "published": "2023-10-26 04:13:49", "link": "http://arxiv.org/abs/2310.17133v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Symbolic Planning and Code Generation for Grounded Dialogue", "abstract": "Large language models (LLMs) excel at processing and generating both text and\ncode. However, LLMs have had limited applicability in grounded task-oriented\ndialogue as they are difficult to steer toward task objectives and fail to\nhandle novel grounding. We present a modular and interpretable grounded\ndialogue system that addresses these shortcomings by composing LLMs with a\nsymbolic planner and grounded code execution. Our system consists of a reader\nand planner: the reader leverages an LLM to convert partner utterances into\nexecutable code, calling functions that perform grounding. The translated\ncode's output is stored to track dialogue state, while a symbolic planner\ndetermines the next appropriate response. We evaluate our system's performance\non the demanding OneCommon dialogue task, involving collaborative reference\nresolution on abstract images of scattered dots. Our system substantially\noutperforms the previous state-of-the-art, including improving task success in\nhuman evaluations from 56% to 69% in the most challenging setting.", "published": "2023-10-26 04:22:23", "link": "http://arxiv.org/abs/2310.17140v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Techniques for supercharging academic writing with generative AI", "abstract": "Academic writing is an indispensable yet laborious part of the research\nenterprise. This Perspective maps out principles and methods for using\ngenerative artificial intelligence (AI), specifically large language models\n(LLMs), to elevate the quality and efficiency of academic writing. We introduce\na human-AI collaborative framework that delineates the rationale (why), process\n(how), and nature (what) of AI engagement in writing. The framework pinpoints\nboth short-term and long-term reasons for engagement and their underlying\nmechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals\nthe role of AI throughout the writing process, conceptualized through a\ntwo-stage model for human-AI collaborative writing, and the nature of AI\nassistance in writing, represented through a model of writing-assistance types\nand levels. Building on this framework, we describe effective prompting\ntechniques for incorporating AI into the writing routine (outlining, drafting,\nand editing) as well as strategies for maintaining rigorous scholarship,\nadhering to varied journal policies, and avoiding overreliance on AI.\nUltimately, the prudent integration of AI into academic writing can ease the\ncommunication burden, empower authors, accelerate discovery, and promote\ndiversity in science.", "published": "2023-10-26 04:35:00", "link": "http://arxiv.org/abs/2310.17143v3", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Efficient Data Fusion using the Tsetlin Machine", "abstract": "We propose a novel way of assessing and fusing noisy dynamic data using a\nTsetlin Machine. Our approach consists in monitoring how explanations in form\nof logical clauses that a TM learns changes with possible noise in dynamic\ndata. This way TM can recognize the noise by lowering weights of previously\nlearned clauses, or reflect it in the form of new clauses. We also perform a\ncomprehensive experimental study using notably different datasets that\ndemonstrated high performance of the proposed approach.", "published": "2023-10-26 07:49:25", "link": "http://arxiv.org/abs/2310.17207v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Codebook Features: Sparse and Discrete Interpretability for Neural\n  Networks", "abstract": "Understanding neural networks is challenging in part because of the dense,\ncontinuous nature of their hidden states. We explore whether we can train\nneural networks to have hidden states that are sparse, discrete, and more\ninterpretable by quantizing their continuous features into what we call\ncodebook features. Codebook features are produced by finetuning neural networks\nwith vector quantization bottlenecks at each layer, producing a network whose\nhidden features are the sum of a small number of discrete vector codes chosen\nfrom a larger codebook. Surprisingly, we find that neural networks can operate\nunder this extreme bottleneck with only modest degradation in performance. This\nsparse, discrete bottleneck also provides an intuitive way of controlling\nneural network behavior: first, find codes that activate when the desired\nbehavior is present, then activate those same codes during generation to elicit\nthat behavior. We validate our approach by training codebook Transformers on\nseveral different datasets. First, we explore a finite state machine dataset\nwith far more hidden states than neurons. In this setting, our approach\novercomes the superposition problem by assigning states to distinct codes, and\nwe find that we can make the neural network behave as if it is in a different\nstate by activating the code for that state. Second, we train Transformer\nlanguage models with up to 410M parameters on two natural language datasets. We\nidentify codes in these models representing diverse, disentangled concepts\n(ranging from negative emotions to months of the year) and find that we can\nguide the model to generate different topics by activating the appropriate\ncodes during inference. Overall, codebook features appear to be a promising\nunit of analysis and control for neural networks and interpretability. Our\ncodebase and models are open-sourced at\nhttps://github.com/taufeeque9/codebook-features.", "published": "2023-10-26 08:28:48", "link": "http://arxiv.org/abs/2310.17230v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Cultural Adaptation of Recipes", "abstract": "Building upon the considerable advances in Large Language Models (LLMs), we\nare now equipped to address more sophisticated tasks demanding a nuanced\nunderstanding of cross-cultural contexts. A key example is recipe adaptation,\nwhich goes beyond simple translation to include a grasp of ingredients,\nculinary techniques, and dietary preferences specific to a given culture. We\nintroduce a new task involving the translation and cultural adaptation of\nrecipes between Chinese and English-speaking cuisines. To support this\ninvestigation, we present CulturalRecipes, a unique dataset comprised of\nautomatically paired recipes written in Mandarin Chinese and English. This\ndataset is further enriched with a human-written and curated test set. In this\nintricate task of cross-cultural recipe adaptation, we evaluate the performance\nof various methods, including GPT-4 and other LLMs, traditional machine\ntranslation, and information retrieval techniques. Our comprehensive analysis\nincludes both automatic and human evaluation metrics. While GPT-4 exhibits\nimpressive abilities in adapting Chinese recipes into English, it still lags\nbehind human expertise when translating English recipes into Chinese. This\nunderscores the multifaceted nature of cultural adaptations. We anticipate that\nthese insights will significantly contribute to future research on\nculturally-aware language models and their practical application in culturally\ndiverse contexts.", "published": "2023-10-26 12:39:20", "link": "http://arxiv.org/abs/2310.17353v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in\n  Real-World User-AI Conversation", "abstract": "Despite remarkable advances that large language models have achieved in\nchatbots, maintaining a non-toxic user-AI interactive environment has become\nincreasingly critical nowadays. However, previous efforts in toxicity detection\nhave been mostly based on benchmarks derived from social media content, leaving\nthe unique challenges inherent to real-world user-AI interactions\ninsufficiently explored. In this work, we introduce ToxicChat, a novel\nbenchmark based on real user queries from an open-source chatbot. This\nbenchmark contains the rich, nuanced phenomena that can be tricky for current\ntoxicity detection models to identify, revealing a significant domain\ndifference compared to social media content. Our systematic evaluation of\nmodels trained on existing toxicity datasets has shown their shortcomings when\napplied to this unique domain of ToxicChat. Our work illuminates the\npotentially overlooked challenges of toxicity detection in real-world user-AI\nconversations. In the future, ToxicChat can be a valuable resource to drive\nfurther advancements toward building a safe and healthy environment for user-AI\ninteractions.", "published": "2023-10-26 13:35:41", "link": "http://arxiv.org/abs/2310.17389v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech\n  Systems for the MADASR 2023 Challenge", "abstract": "This paper describes Tallinn University of Technology (TalTech) systems\ndeveloped for the ASRU MADASR 2023 Challenge. The challenge focuses on\nautomatic speech recognition of dialect-rich Indian languages with limited\ntraining audio and text data. TalTech participated in two tracks of the\nchallenge: Track 1 that allowed using only the provided training data and Track\n3 which allowed using additional audio data. In both tracks, we relied on\nwav2vec2.0 models. Our methodology diverges from the traditional procedure of\nfinetuning pretrained wav2vec2.0 models in two key points: firstly, through the\nimplementation of the aligned data augmentation technique to enhance the\nlinguistic diversity of the training data, and secondly, via the application of\ndeep prefix tuning for dialect adaptation of wav2vec2.0 models. In both tracks,\nour approach yielded significant improvements over the provided baselines,\nachieving the lowest word error rates across all participating teams.", "published": "2023-10-26 14:57:08", "link": "http://arxiv.org/abs/2310.17448v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LightLM: A Lightweight Deep and Narrow Language Model for Generative\n  Recommendation", "abstract": "This paper presents LightLM, a lightweight Transformer-based language model\nfor generative recommendation. While Transformer-based generative modeling has\ngained importance in various AI sub-fields such as NLP and vision, generative\nrecommendation is still in its infancy due to its unique demand on personalized\ngenerative modeling. Existing works on generative recommendation often use\nNLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are\nheavy-weight and are not specifically designed for recommendation tasks.\nLightLM tackles the issue by introducing a light-weight deep and narrow\nTransformer architecture, which is specifically tailored for direct generation\nof recommendation items. This structure is especially apt for straightforward\ngenerative recommendation and stems from the observation that language model\ndoes not have to be too wide for this task, as the input predominantly consists\nof short tokens that are well-suited for the model's capacity. We also show\nthat our devised user and item ID indexing methods, i.e., Spectral\nCollaborative Indexing (SCI) and Graph Collaborative Indexing (GCI), enables\nthe deep and narrow Transformer architecture to outperform large-scale language\nmodels for recommendation. Besides, to address the hallucination problem of\ngenerating items as output, we propose the constrained generation process for\ngenerative recommenders. Experiments on real-world datasets show that LightLM\noutperforms various competitive baselines in terms of both recommendation\naccuracy and efficiency. The code can be found at\nhttps://github.com/dongyuanjushi/LightLM.", "published": "2023-10-26 15:44:57", "link": "http://arxiv.org/abs/2310.17488v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Improving Zero-shot Reader by Reducing Distractions from Irrelevant\n  Documents in Open-Domain Question Answering", "abstract": "Large language models (LLMs) enable zero-shot approaches in open-domain\nquestion answering (ODQA), yet with limited advancements as the reader is\ncompared to the retriever. This study aims at the feasibility of a zero-shot\nreader that addresses the challenges of computational cost and the need for\nlabeled data. We find that LLMs are distracted due to irrelevant documents in\nthe retrieved set and the overconfidence of the generated answers when they are\nexploited as zero-shot readers. To tackle these problems, we mitigate the\nimpact of such documents via Distraction-aware Answer Selection (DAS) with a\nnegation-based instruction and score adjustment for proper answer selection.\nExperimental results show that our approach successfully handles distraction\nacross diverse scenarios, enhancing the performance of zero-shot readers.\nFurthermore, unlike supervised readers struggling with unseen data, zero-shot\nreaders demonstrate outstanding transferability without any training.", "published": "2023-10-26 15:45:12", "link": "http://arxiv.org/abs/2310.17490v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "1D-Touch: NLP-Assisted Coarse Text Selection via a Semi-Direct Gesture", "abstract": "Existing text selection techniques on touchscreen focus on improving the\ncontrol for moving the carets. Coarse-grained text selection on word and phrase\nlevels has not received much support beyond word-snapping and entity\nrecognition. We introduce 1D-Touch, a novel text selection method that\ncomplements the carets-based sub-word selection by facilitating the selection\nof semantic units of words and above. This method employs a simple vertical\nslide gesture to expand and contract a selection area from a word. The\nexpansion can be by words or by semantic chunks ranging from sub-phrases to\nsentences. This technique shifts the concept of text selection, from defining a\nrange by locating the first and last words, towards a dynamic process of\nexpanding and contracting a textual semantic entity. To understand the effects\nof our approach, we prototyped and tested two variants: WordTouch, which offers\na straightforward word-by-word expansion, and ChunkTouch, which leverages NLP\nto chunk text into syntactic units, allowing the selection to grow by\nsemantically meaningful units in response to the sliding gesture. Our\nevaluation, focused on the coarse-grained selection tasks handled by 1D-Touch,\nshows a 20% improvement over the default word-snapping selection method on\nAndroid.", "published": "2023-10-26 17:01:22", "link": "http://arxiv.org/abs/2310.17576v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven\n  Perturbed Gradient Descent", "abstract": "Fine-tuning pretrained language models (PLMs) for downstream tasks is a\nlarge-scale optimization problem, in which the choice of the training algorithm\ncritically determines how well the trained model can generalize to unseen test\ndata, especially in the context of few-shot learning. To achieve good\ngeneralization performance and avoid overfitting, techniques such as data\naugmentation and pruning are often applied. However, adding these\nregularizations necessitates heavy tuning of the hyperparameters of\noptimization algorithms, such as the popular Adam optimizer. In this paper, we\npropose a two-stage fine-tuning method, PAC-tuning, to address this\noptimization challenge. First, based on PAC-Bayes training, PAC-tuning directly\nminimizes the PAC-Bayes generalization bound to learn proper parameter\ndistribution. Second, PAC-tuning modifies the gradient by injecting noise with\nthe variance learned in the first stage into the model parameters during\ntraining, resulting in a variant of perturbed gradient descent (PGD). In the\npast, the few-shot scenario posed difficulties for PAC-Bayes training because\nthe PAC-Bayes bound, when applied to large models with limited training data,\nmight not be stringent. Our experimental results across 5 GLUE benchmark tasks\ndemonstrate that PAC-tuning successfully handles the challenges of fine-tuning\ntasks and outperforms strong baseline methods by a visible margin, further\nconfirming the potential to apply PAC training for any other settings where the\nAdam optimizer is currently used for training.", "published": "2023-10-26 17:09:13", "link": "http://arxiv.org/abs/2310.17588v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An Open Source Data Contamination Report for Large Language Models", "abstract": "Data contamination in model evaluation has become increasingly prevalent with\nthe growing popularity of large language models. It allows models to \"cheat\"\nvia memorisation instead of displaying true capabilities. Therefore,\ncontamination analysis has become an crucial part of reliable model evaluation\nto validate results. However, existing contamination analysis is usually\nconducted internally by large language model developers and often lacks\ntransparency and completeness. This paper presents an extensive data\ncontamination report for over 15 popular large language models across six\npopular multiple-choice QA benchmarks. We also introduce an open-source\npipeline that enables the community to perform contamination analysis on\ncustomised data and models. Our experiments reveal varying contamination levels\nranging from 1\\% to 45\\% across benchmarks, with the contamination degree\nincreasing rapidly over time. Performance analysis of large language models\nindicates that data contamination does not necessarily lead to increased model\nmetrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed\non contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is\nnoted on contaminated MMLU. We also find larger models seem able to gain more\nadvantages than smaller models on contaminated test sets.", "published": "2023-10-26 17:11:42", "link": "http://arxiv.org/abs/2310.17589v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using State-of-the-Art Speech Models to Evaluate Oral Reading Fluency in\n  Ghana", "abstract": "This paper reports on a set of three recent experiments utilizing large-scale\nspeech models to evaluate the oral reading fluency (ORF) of students in Ghana.\nWhile ORF is a well-established measure of foundational literacy, assessing it\ntypically requires one-on-one sessions between a student and a trained\nevaluator, a process that is time-consuming and costly. Automating the\nevaluation of ORF could support better literacy instruction, particularly in\neducation contexts where formative assessment is uncommon due to large class\nsizes and limited resources. To our knowledge, this research is among the first\nto examine the use of the most recent versions of large-scale speech models\n(Whisper V2 wav2vec2.0) for ORF assessment in the Global South.\n  We find that Whisper V2 produces transcriptions of Ghanaian students reading\naloud with a Word Error Rate of 13.5. This is close to the model's average WER\non adult speech (12.8) and would have been considered state-of-the-art for\nchildren's speech transcription only a few years ago. We also find that when\nthese transcriptions are used to produce fully automated ORF scores, they\nclosely align with scores generated by expert human graders, with a correlation\ncoefficient of 0.96. Importantly, these results were achieved on a\nrepresentative dataset (i.e., students with regional accents, recordings taken\nin actual classrooms), using a free and publicly available speech model out of\nthe box (i.e., no fine-tuning). This suggests that using large-scale speech\nmodels to assess ORF may be feasible to implement and scale in lower-resource,\nlinguistically diverse educational contexts.", "published": "2023-10-26 17:30:13", "link": "http://arxiv.org/abs/2310.17606v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset", "abstract": "As an important component of intelligent legal systems, legal case retrieval\nplays a critical role in ensuring judicial justice and fairness. However, the\ndevelopment of legal case retrieval technologies in the Chinese legal system is\nrestricted by three problems in existing datasets: limited data size, narrow\ndefinitions of legal relevance, and naive candidate pooling strategies used in\ndata sampling. To alleviate these issues, we introduce LeCaRDv2, a large-scale\nLegal Case Retrieval Dataset (version 2). It consists of 800 queries and 55,192\ncandidates extracted from 4.3 million criminal case documents. To the best of\nour knowledge, LeCaRDv2 is one of the largest Chinese legal case retrieval\ndatasets, providing extensive coverage of criminal charges. Additionally, we\nenrich the existing relevance criteria by considering three key aspects:\ncharacterization, penalty, procedure. This comprehensive criteria enriches the\ndataset and may provides a more holistic perspective. Furthermore, we propose a\ntwo-level candidate set pooling strategy that effectively identify potential\ncandidates for each query case. It's important to note that all cases in the\ndataset have been annotated by multiple legal experts specializing in criminal\nlaw. Their expertise ensures the accuracy and reliability of the annotations.\nWe evaluate several state-of-the-art retrieval models at LeCaRDv2,\ndemonstrating that there is still significant room for improvement in legal\ncase retrieval. The details of LeCaRDv2 can be found at the anonymous website\nhttps://github.com/anonymous1113243/LeCaRDv2.", "published": "2023-10-26 17:32:55", "link": "http://arxiv.org/abs/2310.17609v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Proving Test Set Contamination in Black Box Language Models", "abstract": "Large language models are trained on vast amounts of internet data, prompting\nconcerns and speculation that they have memorized public benchmarks. Going from\nspeculation to proof of contamination is challenging, as the pretraining data\nused by proprietary models are often not publicly accessible. We show that it\nis possible to provide provable guarantees of test set contamination in\nlanguage models without access to pretraining data or model weights. Our\napproach leverages the fact that when there is no data contamination, all\norderings of an exchangeable benchmark should be equally likely. In contrast,\nthe tendency for language models to memorize example order means that a\ncontaminated language model will find certain canonical orderings to be much\nmore likely than others. Our test flags potential contamination whenever the\nlikelihood of a canonically ordered benchmark dataset is significantly higher\nthan the likelihood after shuffling the examples. We demonstrate that our\nprocedure is sensitive enough to reliably prove test set contamination in\nchallenging situations, including models as small as 1.4 billion parameters, on\nsmall test sets of only 1000 examples, and datasets that appear only a few\ntimes in the pretraining corpus. Using our test, we audit five popular publicly\naccessible language models for test set contamination and find little evidence\nfor pervasive contamination.", "published": "2023-10-26 17:43:13", "link": "http://arxiv.org/abs/2310.17623v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges", "abstract": "Evaluating Large Language Models (LLMs) in open-ended scenarios is\nchallenging because existing benchmarks and metrics can not measure them\ncomprehensively. To address this problem, we propose to fine-tune LLMs as\nscalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in\nopen-ended benchmarks. We first propose a comprehensive, large-scale,\nhigh-quality dataset containing task seeds, LLMs-generated answers, and\nGPT-4-generated judgments for fine-tuning high-performance judges, as well as a\nnew benchmark for evaluating the judges. We train JudgeLM at different scales\nfrom 7B, 13B, to 33B parameters, and conduct a systematic analysis of its\ncapabilities and behaviors. We then analyze the key biases in fine-tuning LLM\nas a judge and consider them as position bias, knowledge bias, and format bias.\nTo address these issues, JudgeLM introduces a bag of techniques including swap\naugmentation, reference support, and reference drop, which clearly enhance the\njudge's performance. JudgeLM obtains the state-of-the-art judge performance on\nboth the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM\nis efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8\nA100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an\nagreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM\nalso demonstrates extended capabilities in being judges of the single answer,\nmultimodal models, multiple answers, multi-turn chat, etc. Code is available at\nhttps://github.com/baaivision/JudgeLM.", "published": "2023-10-26 17:48:58", "link": "http://arxiv.org/abs/2310.17631v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for\n  Relation Extraction from Financial Documents", "abstract": "Relation extraction (RE) has achieved remarkable progress with the help of\npre-trained language models. However, existing RE models are usually incapable\nof handling two situations: implicit expressions and long-tail relation\nclasses, caused by language complexity and data sparsity. Further, these\napproaches and models are largely inaccessible to users who don't have direct\naccess to large language models (LLMs) and/or infrastructure for supervised\ntraining or fine-tuning. Rule-based systems also struggle with implicit\nexpressions. Apart from this, Real world financial documents such as various\n10-X reports (including 10-K, 10-Q, etc.) of publicly traded companies pose\nanother challenge to rule-based systems in terms of longer and complex\nsentences. In this paper, we introduce a simple approach that consults training\nrelations at test time through a nearest-neighbor search over dense vectors of\nlexico-syntactic patterns and provides a simple yet effective means to tackle\nthe above issues. We evaluate our approach on REFinD and show that our method\nachieves state-of-the-art performance. We further show that it can provide a\ngood start for human in the loop setup when a small number of annotations are\navailable and it is also beneficial when domain experts can provide high\nquality patterns.", "published": "2023-10-26 18:19:56", "link": "http://arxiv.org/abs/2310.17714v1", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Outlier Dimensions Encode Task-Specific Knowledge", "abstract": "Representations from large language models (LLMs) are known to be dominated\nby a small subset of dimensions with exceedingly high variance. Previous works\nhave argued that although ablating these outlier dimensions in LLM\nrepresentations hurts downstream performance, outlier dimensions are\ndetrimental to the representational quality of embeddings. In this study, we\ninvestigate how fine-tuning impacts outlier dimensions and show that 1) outlier\ndimensions that occur in pre-training persist in fine-tuned models and 2) a\nsingle outlier dimension can complete downstream tasks with a minimal error\nrate. Our results suggest that outlier dimensions can encode crucial\ntask-specific knowledge and that the value of a representation in a single\noutlier dimension drives downstream model decisions.", "published": "2023-10-26 18:22:13", "link": "http://arxiv.org/abs/2310.17715v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training\n  Quantization Framework for W8A8 Transformers", "abstract": "Quantization techniques are pivotal in reducing the memory and computational\ndemands of deep neural network inference. Existing solutions, such as\nZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook\ncrucial memory-bounded operators and the complexities of per-token\nquantization. Addressing these gaps, we present a novel, fully\nhardware-enhanced robust optimized post-training W8A8 quantization framework,\nZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and\ncompute-intensive operators, aiming for optimal hardware performance.\nAdditionally, it offers flexibility by allowing specific INT8 modules to switch\nto FP16/BF16 mode, enhancing accuracy.", "published": "2023-10-26 18:34:41", "link": "http://arxiv.org/abs/2310.17723v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Salespeople vs SalesBot: Exploring the Role of Educational Value in\n  Conversational Recommender Systems", "abstract": "Making big purchases requires consumers to research or consult a salesperson\nto gain domain expertise. However, existing conversational recommender systems\n(CRS) often overlook users' lack of background knowledge, focusing solely on\ngathering preferences. In this work, we define a new problem space for\nconversational agents that aim to provide both product recommendations and\neducational value through mixed-type mixed-initiative dialog. We introduce\nSalesOps, a framework that facilitates the simulation and evaluation of such\nsystems by leveraging recent advancements in large language models (LLMs). We\nbuild SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate\neither side of the framework. A comprehensive human study compares SalesBot\nagainst professional salespeople, revealing that although SalesBot approaches\nprofessional performance in terms of fluency and informativeness, it lags\nbehind in recommendation quality. We emphasize the distinct limitations both\nface in providing truthful information, highlighting the challenges of ensuring\nfaithfulness in the CRS context. We release our code and make all data\navailable.", "published": "2023-10-26 19:44:06", "link": "http://arxiv.org/abs/2310.17749v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Social Contract AI: Aligning AI Assistants with Implicit Group Norms", "abstract": "We explore the idea of aligning an AI assistant by inverting a model of\nusers' (unknown) preferences from observed interactions. To validate our\nproposal, we run proof-of-concept simulations in the economic ultimatum game,\nformalizing user preferences as policies that guide the actions of simulated\nplayers. We find that the AI assistant accurately aligns its behavior to match\nstandard policies from the economic literature (e.g., selfish, altruistic).\nHowever, the assistant's learned policies lack robustness and exhibit limited\ngeneralization in an out-of-distribution setting when confronted with a\ncurrency (e.g., grams of medicine) that was not included in the assistant's\ntraining distribution. Additionally, we find that when there is inconsistency\nin the relationship between language use and an unknown policy (e.g., an\naltruistic policy combined with rude language), the assistant's learning of the\npolicy is slowed. Overall, our preliminary results suggest that developing\nsimulation frameworks in which AI assistants need to infer preferences from\ndiverse users can provide a valuable approach for studying practical alignment\nquestions.", "published": "2023-10-26 20:27:03", "link": "http://arxiv.org/abs/2310.17769v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation of large language models using an Indian language LGBTI+\n  lexicon", "abstract": "Large language models (LLMs) are typically evaluated on the basis of\ntask-based benchmarks such as MMLU. Such benchmarks do not examine responsible\nbehaviour of LLMs in specific contexts. This is particularly true in the LGBTI+\ncontext where social stereotypes may result in variation in LGBTI+ terminology.\nTherefore, domain-specific lexicons or dictionaries may be useful as a\nrepresentative list of words against which the LLM's behaviour needs to be\nevaluated. This paper presents a methodology for evaluation of LLMs using an\nLGBTI+ lexicon in Indian languages. The methodology consists of four steps:\nformulating NLP tasks relevant to the expected behaviour, creating prompts that\ntest LLMs, using the LLMs to obtain the output and, finally, manually\nevaluating the results. Our qualitative analysis shows that the three LLMs we\nexperiment on are unable to detect underlying hateful content. Similarly, we\nobserve limitations in using machine translation as means to evaluate natural\nlanguage understanding in languages other than English. The methodology\npresented in this paper can be useful for LGBTI+ lexicons in other languages as\nwell as other domain-specific lexicons. The work done in this paper opens\navenues for responsible behaviour of LLMs, as demonstrated in the context of\nprevalent social perception of the LGBTI+ community.", "published": "2023-10-26 21:32:24", "link": "http://arxiv.org/abs/2310.17787v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Utilizing Language Models for Energy Load Forecasting", "abstract": "Energy load forecasting plays a crucial role in optimizing resource\nallocation and managing energy consumption in buildings and cities. In this\npaper, we propose a novel approach that leverages language models for energy\nload forecasting. We employ prompting techniques to convert energy consumption\ndata into descriptive sentences, enabling fine-tuning of language models. By\nadopting an autoregressive generating approach, our proposed method enables\npredictions of various horizons of future energy load consumption. Through\nextensive experiments on real-world datasets, we demonstrate the effectiveness\nand accuracy of our proposed method. Our results indicate that utilizing\nlanguage models for energy load forecasting holds promise for enhancing energy\nefficiency and facilitating intelligent decision-making in energy systems.", "published": "2023-10-26 21:36:06", "link": "http://arxiv.org/abs/2310.17788v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "\"You Are An Expert Linguistic Annotator\": Limits of LLMs as Analyzers of\n  Abstract Meaning Representation", "abstract": "Large language models (LLMs) show amazing proficiency and fluency in the use\nof language. Does this mean that they have also acquired insightful linguistic\nknowledge about the language, to an extent that they can serve as an \"expert\nlinguistic annotator\"? In this paper, we examine the successes and limitations\nof the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning\nstructure, focusing on the Abstract Meaning Representation (AMR; Banarescu et\nal. 2013) parsing formalism, which provides rich graphical representations of\nsentence meaning structure while abstracting away from surface forms. We\ncompare models' analysis of this semantic structure across two settings: 1)\ndirect production of AMR parses based on zero- and few-shot prompts, and 2)\nindirect partial reconstruction of AMR via metalinguistic natural language\nqueries (e.g., \"Identify the primary event of this sentence, and the predicate\ncorresponding to that event.\"). Across these settings, we find that models can\nreliably reproduce the basic format of AMR, and can often capture core event,\nargument, and modifier structure -- however, model outputs are prone to\nfrequent and major errors, and holistic analysis of parse acceptability shows\nthat even with few-shot demonstrations, models have virtually 0% success in\nproducing fully accurate parses. Eliciting natural language responses produces\nsimilar patterns of errors. Overall, our findings indicate that these models\nout-of-the-box can capture aspects of semantic structure, but there remain key\nlimitations in their ability to support fully accurate semantic analyses or\nparses.", "published": "2023-10-26 21:47:59", "link": "http://arxiv.org/abs/2310.17793v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Style-Aware Radiology Report Generation with RadGraph and Few-Shot\n  Prompting", "abstract": "Automatically generated reports from medical images promise to improve the\nworkflow of radiologists. Existing methods consider an image-to-report modeling\ntask by directly generating a fully-fledged report from an image. However, this\nconflates the content of the report (e.g., findings and their attributes) with\nits style (e.g., format and choice of words), which can lead to clinically\ninaccurate reports. To address this, we propose a two-step approach for\nradiology report generation. First, we extract the content from an image; then,\nwe verbalize the extracted content into a report that matches the style of a\nspecific radiologist. For this, we leverage RadGraph -- a graph representation\nof reports -- together with large language models (LLMs). In our quantitative\nevaluations, we find that our approach leads to beneficial performance. Our\nhuman evaluation with clinical raters highlights that the AI-generated reports\nare indistinguishably tailored to the style of individual radiologist despite\nleveraging only a few examples as context.", "published": "2023-10-26 23:06:38", "link": "http://arxiv.org/abs/2310.17811v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "In-Context Ability Transfer for Question Decomposition in Complex QA", "abstract": "Answering complex questions is a challenging task that requires question\ndecomposition and multistep reasoning for arriving at the solution. While\nexisting supervised and unsupervised approaches are specialized to a certain\ntask and involve training, recently proposed prompt-based approaches offer\ngeneralizable solutions to tackle a wide variety of complex question-answering\n(QA) tasks. However, existing prompt-based approaches that are effective for\ncomplex QA tasks involve expensive hand annotations from experts in the form of\nrationales and are not generalizable to newer complex QA scenarios and tasks.\nWe propose, icat (In-Context Ability Transfer) which induces reasoning\ncapabilities in LLMs without any LLM fine-tuning or manual annotation of\nin-context samples. We transfer the ability to decompose complex questions to\nsimpler questions or generate step-by-step rationales to LLMs, by careful\nselection from available data sources of related tasks. We also propose an\nautomated uncertainty-aware exemplar selection approach for selecting examples\nfrom transfer data sources. Finally, we conduct large-scale experiments on a\nvariety of complex QA tasks involving numerical reasoning, compositional\ncomplex QA, and heterogeneous complex QA which require decomposed reasoning. We\nshow that ICAT convincingly outperforms existing prompt-based solutions without\ninvolving any model training, showcasing the benefits of re-using existing\nabilities.", "published": "2023-10-26 11:11:07", "link": "http://arxiv.org/abs/2310.18371v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can LLMs Grade Short-Answer Reading Comprehension Questions : An\n  Empirical Study with a Novel Dataset", "abstract": "Open-ended questions, which require students to produce multi-word,\nnontrivial responses, are a popular tool for formative assessment as they\nprovide more specific insights into what students do and don't know. However,\ngrading open-ended questions can be time-consuming leading teachers to resort\nto simpler question formats or conduct fewer formative assessments. While there\nhas been a longstanding interest in automating of short-answer grading (ASAG),\nbut previous approaches have been technically complex, limiting their use in\nformative assessment contexts. The newest generation of Large Language Models\n(LLMs) potentially makes grading short answer questions more feasible. This\npaper investigates the potential for the newest version of LLMs to be used in\nASAG, specifically in the grading of short answer questions for formative\nassessments, in two ways. First, it introduces a novel dataset of short answer\nreading comprehension questions, drawn from a set of reading assessments\nconducted with over 150 students in Ghana. This dataset allows for the\nevaluation of LLMs in a new context, as they are predominantly designed and\ntrained on data from high-income North American countries. Second, the paper\nempirically evaluates how well various configurations of generative LLMs grade\nstudent short answer responses compared to expert human raters. The findings\nshow that GPT-4, with minimal prompt engineering, performed extremely well on\ngrading the novel dataset (QWK 0.92, F1 0.89), reaching near parity with expert\nhuman raters. To our knowledge this work is the first to empirically evaluate\nthe performance of generative LLMs on short answer reading comprehension\nquestions using real student data, with low technical hurdles to attaining this\nperformance. These findings suggest that generative LLMs could be used to grade\nformative literacy assessment tasks.", "published": "2023-10-26 17:05:40", "link": "http://arxiv.org/abs/2310.18373v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformers Learn to Achieve Second-Order Convergence Rates for\n  In-Context Linear Regression", "abstract": "Transformers excel at in-context learning (ICL) -- learning from\ndemonstrations without parameter updates -- but how they do so remains a\nmystery. Recent work suggests that Transformers may internally run Gradient\nDescent (GD), a first-order optimization method, to perform ICL. In this paper,\nwe instead demonstrate that Transformers learn to approximate second-order\noptimization methods for ICL. For in-context linear regression, Transformers\nshare a similar convergence rate as Iterative Newton's Method, both\nexponentially faster than GD. Empirically, predictions from successive\nTransformer layers closely match different iterations of Newton's Method\nlinearly, with each middle layer roughly computing 3 iterations; thus,\nTransformers and Newton's method converge at roughly the same rate. In\ncontrast, Gradient Descent converges exponentially more slowly. We also show\nthat Transformers can learn in-context on ill-conditioned data, a setting where\nGradient Descent struggles but Iterative Newton succeeds. Finally, to\ncorroborate our empirical findings, we prove that Transformers can implement\n$k$ iterations of Newton's method with $k + \\mathcal{O}(1)$ layers.", "published": "2023-10-26 01:08:47", "link": "http://arxiv.org/abs/2310.17086v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Topic Segmentation of Semi-Structured and Unstructured Conversational\n  Datasets using Language Models", "abstract": "Breaking down a document or a conversation into multiple contiguous segments\nbased on its semantic structure is an important and challenging problem in NLP,\nwhich can assist many downstream tasks. However, current works on topic\nsegmentation often focus on segmentation of structured texts. In this paper, we\ncomprehensively analyze the generalization capabilities of state-of-the-art\ntopic segmentation models on unstructured texts. We find that: (a) Current\nstrategies of pre-training on a large corpus of structured text such as\nWiki-727K do not help in transferability to unstructured conversational data.\n(b) Training from scratch with only a relatively small-sized dataset of the\ntarget unstructured domain improves the segmentation results by a significant\nmargin. We stress-test our proposed Topic Segmentation approach by\nexperimenting with multiple loss functions, in order to mitigate effects of\nimbalance in unstructured conversational datasets. Our empirical evaluation\nindicates that Focal Loss function is a robust alternative to Cross-Entropy and\nre-weighted Cross-Entropy loss function when segmenting unstructured and\nsemi-structured chats.", "published": "2023-10-26 03:37:51", "link": "http://arxiv.org/abs/2310.17120v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How do Language Models Bind Entities in Context?", "abstract": "To correctly use in-context information, language models (LMs) must bind\nentities to their attributes. For example, given a context describing a \"green\nsquare\" and a \"blue circle\", LMs must bind the shapes to their respective\ncolors. We analyze LM representations and identify the binding ID mechanism: a\ngeneral mechanism for solving the binding problem, which we observe in every\nsufficiently large model from the Pythia and LLaMA families. Using causal\ninterventions, we show that LMs' internal activations represent binding\ninformation by attaching binding ID vectors to corresponding entities and\nattributes. We further show that binding ID vectors form a continuous subspace,\nin which distances between binding ID vectors reflect their discernability.\nOverall, our results uncover interpretable strategies in LMs for representing\nsymbolic knowledge in-context, providing a step towards understanding general\nin-context reasoning in large-scale LMs.", "published": "2023-10-26 07:10:31", "link": "http://arxiv.org/abs/2310.17191v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Beyond MLE: Convex Learning for Text Generation", "abstract": "Maximum likelihood estimation (MLE) is a statistical method used to estimate\nthe parameters of a probability distribution that best explain the observed\ndata. In the context of text generation, MLE is often used to train generative\nlanguage models, which can then be used to generate new text. However, we argue\nthat MLE is not always necessary and optimal, especially for closed-ended text\ngeneration tasks like machine translation. In these tasks, the goal of model is\nto generate the most appropriate response, which does not necessarily require\nit to estimate the entire data distribution with MLE. To this end, we propose a\nnovel class of training objectives based on convex functions, which enables\ntext generation models to focus on highly probable outputs without having to\nestimate the entire data distribution. We investigate the theoretical\nproperties of the optimal predicted distribution when applying convex functions\nto the loss, demonstrating that convex functions can sharpen the optimal\ndistribution, thereby enabling the model to better capture outputs with high\nprobabilities. Experiments on various text generation tasks and models show the\neffectiveness of our approach. It enables autoregressive models to bridge the\ngap between greedy and beam search, and facilitates the learning of\nnon-autoregressive models with a maximum improvement of 9+ BLEU points.\nMoreover, our approach also exhibits significant impact on large language\nmodels (LLMs), substantially enhancing their generative capability on various\ntasks. Source code is available at\n\\url{https://github.com/ictnlp/Convex-Learning}.", "published": "2023-10-26 08:08:43", "link": "http://arxiv.org/abs/2310.17217v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TST$^\\mathrm{R}$: Target Similarity Tuning Meets the Real World", "abstract": "Target similarity tuning (TST) is a method of selecting relevant examples in\nnatural language (NL) to code generation through large language models (LLMs)\nto improve performance. Its goal is to adapt a sentence embedding model to have\nthe similarity between two NL inputs match the similarity between their\nassociated code outputs. In this paper, we propose different methods to apply\nand improve TST in the real world. First, we replace the sentence transformer\nwith embeddings from a larger model, which reduces sensitivity to the language\ndistribution and thus provides more flexibility in synthetic generation of\nexamples, and we train a tiny model that transforms these embeddings to a space\nwhere embedding similarity matches code similarity, which allows the model to\nremain a black box and only requires a few matrix multiplications at inference\ntime. Second, we show how to efficiently select a smaller number of training\nexamples to train the TST model. Third, we introduce a ranking-based evaluation\nfor TST that does not require end-to-end code generation experiments, which can\nbe expensive to perform.", "published": "2023-10-26 08:27:36", "link": "http://arxiv.org/abs/2310.17228v2", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Joint Entity and Relation Extraction with Span Pruning and Hypergraph\n  Neural Networks", "abstract": "Entity and Relation Extraction (ERE) is an important task in information\nextraction. Recent marker-based pipeline models achieve state-of-the-art\nperformance, but still suffer from the error propagation issue. Also, most of\ncurrent ERE models do not take into account higher-order interactions between\nmultiple entities and relations, while higher-order modeling could be\nbeneficial.In this work, we propose HyperGraph neural network for ERE\n($\\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based\npipleline model). To alleviate error propagation,we use a high-recall pruner\nmechanism to transfer the burden of entity identification and labeling from the\nNER module to the joint module of our model. For higher-order modeling, we\nbuild a hypergraph, where nodes are entities (provided by the span pruner) and\nrelations thereof, and hyperedges encode interactions between two different\nrelations or between a relation and its associated subject and object entities.\nWe then run a hypergraph neural network for higher-order inference by applying\nmessage passing over the built hypergraph. Experiments on three widely used\nbenchmarks (\\acef{}, \\ace{} and \\scierc{}) for ERE task show significant\nimprovements over the previous state-of-the-art PL-marker.", "published": "2023-10-26 08:36:39", "link": "http://arxiv.org/abs/2310.17238v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparing Photorealistic and Animated Embodied Conversational Agents in\n  Serious Games: An Empirical Study on User Experience", "abstract": "Embodied conversational agents (ECAs) are paradigms of conversational user\ninterfaces in the form of embodied characters. While ECAs offer various\nmanipulable features, this paper focuses on a study conducted to explore two\ndistinct levels of presentation realism. The two agent versions are\nphotorealistic and animated. The study aims to provide insights and design\nsuggestions for speech-enabled ECAs within serious game environments. A\nwithin-subjects, two-by-two factorial design was employed for this research\nwith a cohort of 36 participants balanced for gender. The results showed that\nboth the photorealistic and the animated versions were perceived as highly\nusable, with overall mean scores of 5.76 and 5.71, respectively. However, 69.4\nper cent of the participants stated they preferred the photorealistic version,\n25 per cent stated they preferred the animated version and 5.6 per cent had no\nstated preference. The photorealistic agents were perceived as more realistic\nand human-like, while the animated characters made the task feel more like a\ngame. Even though the agents' realism had no significant effect on usability,\nit positively influenced participants' perceptions of the agent. This research\naims to lay the groundwork for future studies on ECA realism's impact in\nserious games across diverse contexts.", "published": "2023-10-26 10:45:26", "link": "http://arxiv.org/abs/2310.17300v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "I.2.1; H.5.2; K.3.1"], "primary_category": "cs.HC"}
{"title": "FormaT5: Abstention and Examples for Conditional Table Formatting with\n  Natural Language", "abstract": "Formatting is an important property in tables for visualization,\npresentation, and analysis. Spreadsheet software allows users to automatically\nformat their tables by writing data-dependent conditional formatting (CF)\nrules. Writing such rules is often challenging for users as it requires them to\nunderstand and implement the underlying logic. We present FormaT5, a\ntransformer-based model that can generate a CF rule given the target table and\na natural language description of the desired formatting logic. We find that\nuser descriptions for these tasks are often under-specified or ambiguous,\nmaking it harder for code generation systems to accurately learn the desired\nrule in a single step. To tackle this problem of under-specification and\nminimise argument errors, FormaT5 learns to predict placeholders though an\nabstention objective. These placeholders can then be filled by a second model\nor, when examples of rows that should be formatted are available, by a\nprogramming-by-example system. To evaluate FormaT5 on diverse and real\nscenarios, we create an extensive benchmark of 1053 CF tasks, containing\nreal-world descriptions collected from four different sources. We release our\nbenchmarks to encourage research in this area. Abstention and filling allow\nFormaT5 to outperform 8 different neural approaches on our benchmarks, both\nwith and without examples. Our results illustrate the value of building\ndomain-specific learning systems.", "published": "2023-10-26 11:05:15", "link": "http://arxiv.org/abs/2310.17306v3", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.PL"], "primary_category": "cs.AI"}
{"title": "Dialogue-based generation of self-driving simulation scenarios using\n  Large Language Models", "abstract": "Simulation is an invaluable tool for developing and evaluating controllers\nfor self-driving cars. Current simulation frameworks are driven by\nhighly-specialist domain specific languages, and so a natural language\ninterface would greatly enhance usability. But there is often a gap, consisting\nof tacit assumptions the user is making, between a concise English utterance\nand the executable code that captures the user's intent. In this paper we\ndescribe a system that addresses this issue by supporting an extended\nmultimodal interaction: the user can follow up prior instructions with\nrefinements or revisions, in reaction to the simulations that have been\ngenerated from their utterances so far. We use Large Language Models (LLMs) to\nmap the user's English utterances in this interaction into domain-specific\ncode, and so we explore the extent to which LLMs capture the context\nsensitivity that's necessary for computing the speaker's intended message in\ndiscourse.", "published": "2023-10-26 13:07:01", "link": "http://arxiv.org/abs/2310.17372v1", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Tackling the Matrix Multiplication Micro-kernel Generation with Exo", "abstract": "The optimization of the matrix multiplication (or GEMM) has been a need\nduring the last decades. This operation is considered the flagship of current\nlinear algebra libraries such as BLIS, OpenBLAS, or Intel OneAPI because of its\nwidespread use in a large variety of scientific applications. The GEMM is\nusually implemented following the GotoBLAS philosophy, which tiles the GEMM\noperands and uses a series of nested loops for performance improvement. These\napproaches extract the maximum computational power of the architectures through\nsmall pieces of hardware-oriented, high-performance code called micro-kernel.\nHowever, this approach forces developers to generate, with a non-negligible\neffort, a dedicated micro-kernel for each new hardware.\n  In this work, we present a step-by-step procedure for generating\nmicro-kernels with the Exo compiler that performs close to (or even better\nthan) manually developed microkernels written with intrinsic functions or\nassembly language. Our solution also improves the portability of the generated\ncode, since a hardware target is fully specified by a concise library-based\ndescription of its instructions.", "published": "2023-10-26 14:09:57", "link": "http://arxiv.org/abs/2310.17408v2", "categories": ["cs.MS", "cs.CL", "cs.PF"], "primary_category": "cs.MS"}
{"title": "PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word\n  Tokenization on Downstream Applications", "abstract": "Large protein language models are adept at capturing the underlying\nevolutionary information in primary structures, offering significant practical\nvalue for protein engineering. Compared to natural language models, protein\namino acid sequences have a smaller data volume and a limited combinatorial\nspace. Choosing an appropriate vocabulary size to optimize the pre-trained\nmodel is a pivotal issue. Moreover, despite the wealth of benchmarks and\nstudies in the natural language community, there remains a lack of a\ncomprehensive benchmark for systematically evaluating protein language model\nquality. Given these challenges, PETA trained language models with 14 different\nvocabulary sizes under three tokenization methods. It conducted thousands of\ntests on 33 diverse downstream datasets to assess the models' transfer learning\ncapabilities, incorporating two classification heads and three random seeds to\nmitigate potential biases. Extensive experiments indicate that vocabulary sizes\nbetween 50 and 200 optimize the model, whereas sizes exceeding 800\ndetrimentally affect the model's representational performance. Our code, model\nweights and datasets are available at\nhttps://github.com/ginnm/ProteinPretraining.", "published": "2023-10-26 14:20:44", "link": "http://arxiv.org/abs/2310.17415v1", "categories": ["cs.CL", "cs.AI", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "The IMS Toucan System for the Blizzard Challenge 2023", "abstract": "For our contribution to the Blizzard Challenge 2023, we improved on the\nsystem we submitted to the Blizzard Challenge 2021. Our approach entails a\nrule-based text-to-phoneme processing system that includes rule-based\ndisambiguation of homographs in the French language. It then transforms the\nphonemes to spectrograms as intermediate representations using a fast and\nefficient non-autoregressive synthesis architecture based on Conformer and\nGlow. A GAN based neural vocoder that combines recent state-of-the-art\napproaches converts the spectrogram to the final wave. We carefully designed\nthe data processing, training, and inference procedures for the challenge data.\nOur system identifier is G. Open source code and demo are available.", "published": "2023-10-26 15:53:29", "link": "http://arxiv.org/abs/2310.17499v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CompeteAI: Understanding the Competition Dynamics in Large Language\n  Model-based Agents", "abstract": "Large language models (LLMs) have been widely used as agents to complete\ndifferent tasks, such as personal assistance or event planning. While most of\nthe work has focused on cooperation and collaboration between agents, little\nwork explores competition, another important mechanism that promotes the\ndevelopment of society and economy. In this paper, we seek to examine the\ncompetition dynamics in LLM-based agents. We first propose a general framework\nfor studying the competition between agents. Then, we implement a practical\ncompetitive environment using GPT-4 to simulate a virtual town with two types\nof agents, restaurant agents and customer agents. Specifically, the restaurant\nagents compete with each other to attract more customers, where competition\nencourages them to transform, such as cultivating new operating strategies.\nSimulation experiments reveal several interesting findings at the micro and\nmacro levels, which align well with existing market and sociological theories.\nWe hope that the framework and environment can be a promising testbed to study\ncompetition that fosters understanding of society. Code is available at:\nhttps://github.com/microsoft/competeai.", "published": "2023-10-26 16:06:20", "link": "http://arxiv.org/abs/2310.17512v2", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MA"], "primary_category": "cs.AI"}
{"title": "The Expressive Power of Low-Rank Adaptation", "abstract": "Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that\nleverages low-rank adaptation of weight matrices, has emerged as a prevalent\ntechnique for fine-tuning pre-trained models such as large language models and\ndiffusion models. Despite its huge success in practice, the theoretical\nunderpinnings of LoRA have largely remained unexplored. This paper takes the\nfirst step to bridge this gap by theoretically analyzing the expressive power\nof LoRA. We prove that, for fully connected neural networks, LoRA can adapt any\nmodel $f$ to accurately represent any smaller target model $\\overline{f}$ if\nLoRA-rank $\\geq(\\text{width of }f) \\times \\frac{\\text{depth of\n}\\overline{f}}{\\text{depth of }f}$. We also quantify the approximation error\nwhen LoRA-rank is lower than the threshold. For Transformer networks, we show\nany model can be adapted to a target model of the same size with\nrank-$(\\frac{\\text{embedding size}}{2})$ LoRA adapters.", "published": "2023-10-26 16:08:33", "link": "http://arxiv.org/abs/2310.17513v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Can large language models replace humans in the systematic review\n  process? Evaluating GPT-4's efficacy in screening and extracting data from\n  peer-reviewed and grey literature in multiple languages", "abstract": "Systematic reviews are vital for guiding practice, research, and policy, yet\nthey are often slow and labour-intensive. Large language models (LLMs) could\noffer a way to speed up and automate systematic reviews, but their performance\nin such tasks has not been comprehensively evaluated against humans, and no\nstudy has tested GPT-4, the biggest LLM so far. This pre-registered study\nevaluates GPT-4's capability in title/abstract screening, full-text review, and\ndata extraction across various literature types and languages using a\n'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human\nperformance in most tasks, results were skewed by chance agreement and dataset\nimbalance. After adjusting for these, there was a moderate level of performance\nfor data extraction, and - barring studies that used highly reliable prompts -\nscreening performance levelled at none to moderate for different stages and\nlanguages. When screening full-text literature using highly reliable prompts,\nGPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key\nstudies using highly reliable prompts improved its performance even more. Our\nfindings indicate that, currently, substantial caution should be used if LLMs\nare being used to conduct systematic reviews, but suggest that, for certain\nsystematic review tasks delivered under reliable prompts, LLMs can rival human\nperformance.", "published": "2023-10-26 16:18:30", "link": "http://arxiv.org/abs/2310.17526v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Bias and Fairness in Gender-Neutral Pretrained\n  Vision-and-Language Models", "abstract": "Pretrained machine learning models are known to perpetuate and even amplify\nexisting biases in data, which can result in unfair outcomes that ultimately\nimpact user experience. Therefore, it is crucial to understand the mechanisms\nbehind those prejudicial biases to ensure that model performance does not\nresult in discriminatory behaviour toward certain groups or populations. In\nthis work, we define gender bias as our case study. We quantify bias\namplification in pretraining and after fine-tuning on three families of\nvision-and-language models. We investigate the connection, if any, between the\ntwo learning stages, and evaluate how bias amplification reflects on model\nperformance. Overall, we find that bias amplification in pretraining and after\nfine-tuning are independent. We then examine the effect of continued\npretraining on gender-neutral data, finding that this reduces group\ndisparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without\nsignificantly compromising task performance.", "published": "2023-10-26 16:19:19", "link": "http://arxiv.org/abs/2310.17530v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Unpacking the Ethical Value Alignment in Big Models", "abstract": "Big models have greatly advanced AI's ability to understand, generate, and\nmanipulate information and content, enabling numerous applications. However, as\nthese models become increasingly integrated into everyday life, their inherent\nethical values and potential biases pose unforeseen risks to society. This\npaper provides an overview of the risks and challenges associated with big\nmodels, surveys existing AI ethics guidelines, and examines the ethical\nimplications arising from the limitations of these models. Taking a normative\nethics perspective, we propose a reassessment of recent normative guidelines,\nhighlighting the importance of collaborative efforts in academia to establish a\nunified and universal AI ethics framework. Furthermore, we investigate the\nmoral inclinations of current mainstream LLMs using the Moral Foundation\ntheory, analyze existing alignment algorithms, and outline the unique\nchallenges encountered in aligning ethical values within them. To address these\nchallenges, we introduce a novel conceptual paradigm for aligning the ethical\nvalues of big models and discuss promising research directions for alignment\ncriteria, evaluation, and method, representing an initial step towards the\ninterdisciplinary construction of the ethically aligned AI\n  This paper is a modified English version of our Chinese paper\nhttps://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended\nto help non-Chinese native speakers better understand our work.", "published": "2023-10-26 16:45:40", "link": "http://arxiv.org/abs/2310.17551v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Towards Matching Phones and Speech Representations", "abstract": "Learning phone types from phone instances has been a long-standing problem,\nwhile still being open. In this work, we revisit this problem in the context of\nself-supervised learning, and pose it as the problem of matching cluster\ncentroids to phone embeddings. We study two key properties that enable\nmatching, namely, whether cluster centroids of self-supervised representations\nreduce the variability of phone instances and respect the relationship among\nphones. We then use the matching result to produce pseudo-labels and introduce\na new loss function for improving self-supervised representations. Our\nexperiments show that the matching result captures the relationship among\nphones. Training the new loss function jointly with the regular self-supervised\nlosses, such as APC and CPC, significantly improves the downstream phone\nclassification.", "published": "2023-10-26 16:47:52", "link": "http://arxiv.org/abs/2310.17558v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models", "abstract": "With LLMs shifting their role from statistical modeling of language to\nserving as general-purpose AI agents, how should LLM evaluations change?\nArguably, a key ability of an AI agent is to flexibly combine, as needed, the\nbasic skills it has learned. The capability to combine skills plays an\nimportant role in (human) pedagogy and also in a paper on emergence phenomena\n(Arora & Goyal, 2023).\n  This work introduces Skill-Mix, a new evaluation to measure ability to\ncombine skills. Using a list of $N$ skills the evaluator repeatedly picks\nrandom subsets of $k$ skills and asks the LLM to produce text combining that\nsubset of skills. Since the number of subsets grows like $N^k$, for even modest\n$k$ this evaluation will, with high probability, require the LLM to produce\ntext significantly different from any text in the training set. The paper\ndevelops a methodology for (a) designing and administering such an evaluation,\nand (b) automatic grading (plus spot-checking by humans) of the results using\nGPT-4 as well as the open LLaMA-2 70B model.\n  Administering a version of to popular chatbots gave results that, while\ngenerally in line with prior expectations, contained surprises. Sizeable\ndifferences exist among model capabilities that are not captured by their\nranking on popular LLM leaderboards (\"cramming for the leaderboard\").\nFurthermore, simple probability calculations indicate that GPT-4's reasonable\nperformance on $k=5$ is suggestive of going beyond \"stochastic parrot\" behavior\n(Bender et al., 2021), i.e., it combines skills in ways that it had not seen\nduring training.\n  We sketch how the methodology can lead to a Skill-Mix based eco-system of\nopen evaluations for AI capabilities of future models.", "published": "2023-10-26 16:55:05", "link": "http://arxiv.org/abs/2310.17567v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Navigating to Success in Multi-Modal Human-Robot Collaboration: Analysis\n  and Corpus Release", "abstract": "Human-guided robotic exploration is a useful approach to gathering\ninformation at remote locations, especially those that might be too risky,\ninhospitable, or inaccessible for humans. Maintaining common ground between the\nremotely-located partners is a challenge, one that can be facilitated by\nmulti-modal communication. In this paper, we explore how participants utilized\nmultiple modalities to investigate a remote location with the help of a robotic\npartner. Participants issued spoken natural language instructions and received\nfrom the robot: text-based feedback, continuous 2D LIDAR mapping, and\nupon-request static photographs. We noticed that different strategies were\nadopted in terms of use of the modalities, and hypothesize that these\ndifferences may be correlated with success at several exploration sub-tasks. We\nfound that requesting photos may have improved the identification and counting\nof some key entities (doorways in particular) and that this strategy did not\nhinder the amount of overall area exploration. Future work with larger samples\nmay reveal the effects of more nuanced photo and dialogue strategies, which can\ninform the training of robotic agents. Additionally, we announce the release of\nour unique multi-modal corpus of human-robot communication in an exploration\ncontext: SCOUT, the Situated Corpus on Understanding Transactions.", "published": "2023-10-26 16:56:01", "link": "http://arxiv.org/abs/2310.17568v1", "categories": ["cs.HC", "cs.CL", "cs.RO"], "primary_category": "cs.HC"}
{"title": "Uncovering Meanings of Embeddings via Partial Orthogonality", "abstract": "Machine learning tools often rely on embedding text as vectors of real\nnumbers. In this paper, we study how the semantic structure of language is\nencoded in the algebraic structure of such embeddings. Specifically, we look at\na notion of ``semantic independence'' capturing the idea that, e.g.,\n``eggplant'' and ``tomato'' are independent given ``vegetable''. Although such\nexamples are intuitive, it is difficult to formalize such a notion of semantic\nindependence. The key observation here is that any sensible formalization\nshould obey a set of so-called independence axioms, and thus any algebraic\nencoding of this structure should also obey these axioms. This leads us\nnaturally to use partial orthogonality as the relevant algebraic structure. We\ndevelop theory and methods that allow us to demonstrate that partial\northogonality does indeed capture semantic independence. Complementary to this,\nwe also introduce the concept of independence preserving embeddings where\nembeddings preserve the conditional independence structures of a distribution,\nand we prove the existence of such embeddings and approximations to them.", "published": "2023-10-26 17:34:32", "link": "http://arxiv.org/abs/2310.17611v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "In-Context Learning Dynamics with Random Binary Sequences", "abstract": "Large language models (LLMs) trained on huge corpora of text datasets\ndemonstrate intriguing capabilities, achieving state-of-the-art performance on\ntasks they were not explicitly trained for. The precise nature of LLM\ncapabilities is often mysterious, and different prompts can elicit different\ncapabilities through in-context learning. We propose a framework that enables\nus to analyze in-context learning dynamics to understand latent concepts\nunderlying LLMs' behavioral patterns. This provides a more nuanced\nunderstanding than success-or-failure evaluation benchmarks, but does not\nrequire observing internal activations as a mechanistic interpretation of\ncircuits would. Inspired by the cognitive science of human randomness\nperception, we use random binary sequences as context and study dynamics of\nin-context learning by manipulating properties of context data, such as\nsequence length. In the latest GPT-3.5+ models, we find emergent abilities to\ngenerate seemingly random numbers and learn basic formal languages, with\nstriking in-context learning dynamics where model outputs transition sharply\nfrom seemingly random behaviors to deterministic repetition.", "published": "2023-10-26 17:54:52", "link": "http://arxiv.org/abs/2310.17639v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free\n  Deep Learning Studies: A Case Study on NLP", "abstract": "Reproducibility in scientific work has been becoming increasingly important\nin research communities such as machine learning, natural language processing,\nand computer vision communities due to the rapid development of the research\ndomains supported by recent advances in deep learning. In this work, we present\na significantly upgraded version of torchdistill, a modular-driven coding-free\ndeep learning framework significantly upgraded from the initial release, which\nsupports only image classification and object detection tasks for reproducible\nknowledge distillation experiments. To demonstrate that the upgraded framework\ncan support more tasks with third-party libraries, we reproduce the GLUE\nbenchmark results of BERT models using a script based on the upgraded\ntorchdistill, harmonizing with various Hugging Face libraries. All the 27\nfine-tuned BERT models and configurations to reproduce the results are\npublished at Hugging Face, and the model weights have already been widely used\nin research communities. We also reimplement popular small-sized models and new\nknowledge distillation methods and perform additional experiments for computer\nvision tasks.", "published": "2023-10-26 17:57:15", "link": "http://arxiv.org/abs/2310.17644v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CodeFusion: A Pre-trained Diffusion Model for Code Generation", "abstract": "Imagine a developer who can only change their last line of code, how often\nwould they have to start writing a function from scratch before it is correct?\nAuto-regressive models for code generation from natural language have a similar\nlimitation: they do not easily allow reconsidering earlier tokens generated. We\nintroduce CodeFusion, a pre-trained diffusion code generation model that\naddresses this limitation by iteratively denoising a complete program\nconditioned on the encoded natural language. We evaluate CodeFusion on the task\nof natural language to code generation for Bash, Python, and Microsoft Excel\nconditional formatting (CF) rules. Experiments show that CodeFusion (75M\nparameters) performs on par with state-of-the-art auto-regressive systems\n(350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and\ntop-5 accuracy due to its better balance in diversity versus quality.", "published": "2023-10-26 11:06:15", "link": "http://arxiv.org/abs/2310.17680v3", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Managing extreme AI risks amid rapid progress", "abstract": "Artificial Intelligence (AI) is progressing rapidly, and companies are\nshifting their focus to developing generalist AI systems that can autonomously\nact and pursue goals. Increases in capabilities and autonomy may soon massively\namplify AI's impact, with risks that include large-scale social harms,\nmalicious uses, and an irreversible loss of human control over autonomous AI\nsystems. Although researchers have warned of extreme risks from AI, there is a\nlack of consensus about how exactly such risks arise, and how to manage them.\nSociety's response, despite promising first steps, is incommensurate with the\npossibility of rapid, transformative progress that is expected by many experts.\nAI safety research is lagging. Present governance initiatives lack the\nmechanisms and institutions to prevent misuse and recklessness, and barely\naddress autonomous systems. In this short consensus paper, we describe extreme\nrisks from upcoming, advanced AI systems. Drawing on lessons learned from other\nsafety-critical technologies, we then outline a comprehensive plan combining\ntechnical research and development with proactive, adaptive governance\nmechanisms for a more commensurate preparation.", "published": "2023-10-26 17:59:06", "link": "http://arxiv.org/abs/2310.17688v3", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "From Transcripts to Insights: Uncovering Corporate Risks Using\n  Generative AI", "abstract": "We explore the value of generative AI tools, such as ChatGPT, in helping\ninvestors uncover dimensions of corporate risk. We develop and validate\nfirm-level measures of risk exposure to political, climate, and AI-related\nrisks. Using the GPT 3.5 model to generate risk summaries and assessments from\nthe context provided by earnings call transcripts, we show that GPT-based\nmeasures possess significant information content and outperform the existing\nrisk measures in predicting (abnormal) firm-level volatility and firms' choices\nsuch as investment and innovation. Importantly, information in risk assessments\ndominates that in risk summaries, establishing the value of general AI\nknowledge. We also find that generative AI is effective at detecting emerging\nrisks, such as AI risk, which has soared in recent quarters. Our measures\nperform well both within and outside the GPT's training window and are priced\nin equity markets. Taken together, an AI-based approach to risk measurement\nprovides useful insights to users of corporate disclosures at a low cost.", "published": "2023-10-26 18:30:37", "link": "http://arxiv.org/abs/2310.17721v2", "categories": ["econ.GN", "cs.AI", "cs.CL", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "Large Language Models as Generalizable Policies for Embodied Tasks", "abstract": "We show that large language models (LLMs) can be adapted to be generalizable\npolicies for embodied visual tasks. Our approach, called Large LAnguage model\nReinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take\nas input text instructions and visual egocentric observations and output\nactions directly in the environment. Using reinforcement learning, we train\nLLaRP to see and act solely through environmental interactions. We show that\nLLaRP is robust to complex paraphrasings of task instructions and can\ngeneralize to new tasks that require novel optimal behavior. In particular, on\n1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other\ncommon learned baselines or zero-shot applications of LLMs. Finally, to aid the\ncommunity in studying language conditioned, massively multi-task, embodied AI\nproblems we release a novel benchmark, Language Rearrangement, consisting of\n150,000 training and 1,000 testing tasks for language-conditioned\nrearrangement. Video examples of LLaRP in unseen Language Rearrangement\ninstructions are at https://llm-rl.github.io.", "published": "2023-10-26 18:32:05", "link": "http://arxiv.org/abs/2310.17722v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GROOViST: A Metric for Grounding Objects in Visual Storytelling", "abstract": "A proper evaluation of stories generated for a sequence of images -- the task\ncommonly referred to as visual storytelling -- must consider multiple aspects,\nsuch as coherence, grammatical correctness, and visual grounding. In this work,\nwe focus on evaluating the degree of grounding, that is, the extent to which a\nstory is about the entities shown in the images. We analyze current metrics,\nboth designed for this purpose and for general vision-text alignment. Given\ntheir observed shortcomings, we propose a novel evaluation tool, GROOViST, that\naccounts for cross-modal dependencies, temporal misalignments (the fact that\nthe order in which entities appear in the story and the image sequence may not\nmatch), and human intuitions on visual grounding. An additional advantage of\nGROOViST is its modular design, where the contribution of each component can be\nassessed and interpreted individually.", "published": "2023-10-26 20:27:16", "link": "http://arxiv.org/abs/2310.17770v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Privacy-preserving Representation Learning for Speech Understanding", "abstract": "Existing privacy-preserving speech representation learning methods target a\nsingle application domain. In this paper, we present a novel framework to\nanonymize utterance-level speech embeddings generated by pre-trained encoders\nand show its effectiveness for a range of speech classification tasks.\nSpecifically, given the representations from a pre-trained encoder, we train a\nTransformer to estimate the representations for the same utterances spoken by\nother speakers. During inference, the extracted representations can be\nconverted into different identities to preserve privacy. We compare the results\nwith the voice anonymization baselines from the VoicePrivacy 2022 challenge. We\nevaluate our framework on speaker identification for privacy and emotion\nrecognition, depression classification, and intent classification for utility.\nOur method outperforms the baselines on privacy and utility in paralinguistic\ntasks and achieves comparable performance for intent classification.", "published": "2023-10-26 07:20:23", "link": "http://arxiv.org/abs/2310.17194v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Boosting Multi-Speaker Expressive Speech Synthesis with Semi-supervised\n  Contrastive Learning", "abstract": "This paper aims to build a multi-speaker expressive TTS system, synthesizing\na target speaker's speech with multiple styles and emotions. To this end, we\npropose a novel contrastive learning-based TTS approach to transfer style and\nemotion across speakers. Specifically, contrastive learning from different\nlevels, i.e. utterance and category level, is leveraged to extract the\ndisentangled style, emotion, and speaker representations from speech for style\nand emotion transfer. Furthermore, a semi-supervised training strategy is\nintroduced to improve the data utilization efficiency by involving multi-domain\ndata, including style-labeled data, emotion-labeled data, and abundant\nunlabeled data. To achieve expressive speech with diverse styles and emotions\nfor a target speaker, the learned disentangled representations are integrated\ninto an improved VITS model. Experiments on multi-domain data demonstrate the\neffectiveness of the proposed method.", "published": "2023-10-26 01:58:38", "link": "http://arxiv.org/abs/2310.17101v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-time Neonatal Chest Sound Separation using Deep Learning", "abstract": "Auscultation for neonates is a simple and non-invasive method of providing\ndiagnosis for cardiovascular and respiratory disease. Such diagnosis often\nrequires high-quality heart and lung sounds to be captured during auscultation.\nHowever, in most cases, obtaining such high-quality sounds is non-trivial due\nto the chest sounds containing a mixture of heart, lung, and noise sounds. As\nsuch, additional preprocessing is needed to separate the chest sounds into\nheart and lung sounds. This paper proposes a novel deep-learning approach to\nseparate such chest sounds into heart and lung sounds. Inspired by the\nConv-TasNet model, the proposed model has an encoder, decoder, and mask\ngenerator. The encoder consists of a 1D convolution model and the decoder\nconsists of a transposed 1D convolution. The mask generator is constructed\nusing stacked 1D convolutions and transformers. The proposed model outperforms\nprevious methods in terms of objective distortion measures by 2.01 dB to 5.06\ndB in the artificial dataset, as well as computation time, with at least a\n17-time improvement. Therefore, our proposed model could be a suitable\npreprocessing step for any phonocardiogram-based health monitoring system.", "published": "2023-10-26 03:05:40", "link": "http://arxiv.org/abs/2310.17116v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Single channel speech enhancement by colored spectrograms", "abstract": "Speech enhancement concerns the processes required to remove unwanted\nbackground sounds from the target speech to improve its quality and\nintelligibility. In this paper, a novel approach for single-channel speech\nenhancement is presented, using colored spectrograms. We propose the use of a\ndeep neural network (DNN) architecture adapted from the pix2pix generative\nadversarial network (GAN) and train it over colored spectrograms of speech to\ndenoise them. After denoising, the colors of spectrograms are translated to\nmagnitudes of short-time Fourier transform (STFT) using a shallow regression\nneural network. These estimated STFT magnitudes are later combined with the\nnoisy phases to obtain an enhanced speech. The results show an improvement of\nalmost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1%\nin the short-term objective intelligibility (STOI) over the unprocessed noisy\ndata. The gain in quality and intelligibility over the unprocessed signal is\nalmost equal to the gain achieved by the baseline methods used for comparison\nwith the proposed model, but at a much reduced computational cost. The proposed\nsolution offers a comparative PESQ score at almost 10 times reduced\ncomputational cost than a similar baseline model that has generated the highest\nPESQ score trained on grayscaled spectrograms, while it provides only a 1%\ndeficit in STOI at 28 times reduced computational cost when compared to another\nbaseline system based on convolutional neural network-GAN (CNN-GAN) that\nproduces the most intelligible speech.", "published": "2023-10-26 04:29:27", "link": "http://arxiv.org/abs/2310.17142v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Content-based Controls For Music Large Language Modeling", "abstract": "Recent years have witnessed a rapid growth of large-scale language models in\nthe domain of music audio. Such models enable end-to-end generation of\nhigher-quality music, and some allow conditioned generation using text\ndescriptions. However, the control power of text controls on music is\nintrinsically limited, as they can only describe music indirectly through\nmeta-data (such as singers and instruments) or high-level representations (such\nas genre and emotion). We aim to further equip the models with direct and\ncontent-based controls on innate music languages such as pitch, chords and drum\ntrack. To this end, we contribute Coco-Mulla, a content-based control method\nfor music large language modeling. It uses a parameter-efficient fine-tuning\n(PEFT) method tailored for Transformer-based audio models. Experiments show\nthat our approach achieved high-quality music generation with low-resource\nsemi-supervised learning, tuning with less than 4% parameters compared to the\noriginal model and training on a small dataset with fewer than 300 songs.\nMoreover, our approach enables effective content-based controls, and we\nillustrate the control power via chords and rhythms, two of the most salient\nfeatures of music audio. Furthermore, we show that by combining content-based\ncontrols and text descriptions, our system achieves flexible music variation\ngeneration and arrangement. Our source codes and demos are available online.", "published": "2023-10-26 05:24:38", "link": "http://arxiv.org/abs/2310.17162v3", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Controllable Generation of Artificial Speaker Embeddings through\n  Discovery of Principal Directions", "abstract": "Customizing voice and speaking style in a speech synthesis system with\nintuitive and fine-grained controls is challenging, given that little data with\nappropriate labels is available. Furthermore, editing an existing human's voice\nalso comes with ethical concerns. In this paper, we propose a method to\ngenerate artificial speaker embeddings that cannot be linked to a real human\nwhile offering intuitive and fine-grained control over the voice and speaking\nstyle of the embeddings, without requiring any labels for speaker or style. The\nartificial and controllable embeddings can be fed to a speech synthesis system,\nconditioned on embeddings of real humans during training, without sacrificing\nprivacy during inference.", "published": "2023-10-26 15:54:12", "link": "http://arxiv.org/abs/2310.17502v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BERT-PIN: A BERT-based Framework for Recovering Missing Data Segments in\n  Time-series Load Profiles", "abstract": "Inspired by the success of the Transformer model in natural language\nprocessing and computer vision, this paper introduces BERT-PIN, a Bidirectional\nEncoder Representations from Transformers (BERT) powered Profile Inpainting\nNetwork. BERT-PIN recovers multiple missing data segments (MDSs) using load and\ntemperature time-series profiles as inputs. To adopt a standard Transformer\nmodel structure for profile inpainting, we segment the load and temperature\nprofiles into line segments, treating each segment as a word and the entire\nprofile as a sentence. We incorporate a top candidates selection process in\nBERT-PIN, enabling it to produce a sequence of probability distributions, based\non which users can generate multiple plausible imputed data sets, each\nreflecting different confidence levels. We develop and evaluate BERT-PIN using\nreal-world dataset for two applications: multiple MDSs recovery and demand\nresponse baseline estimation. Simulation results show that BERT-PIN outperforms\nthe existing methods in accuracy while is capable of restoring multiple MDSs\nwithin a longer window. BERT-PIN, served as a pre-trained model, can be\nfine-tuned for conducting many downstream tasks, such as classification and\nsuper resolution.", "published": "2023-10-26 19:30:31", "link": "http://arxiv.org/abs/2310.17742v1", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
