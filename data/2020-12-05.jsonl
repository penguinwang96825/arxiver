{"title": "Data-Efficient Methods for Dialogue Systems", "abstract": "Conversational User Interface (CUI) has become ubiquitous in everyday life,\nin consumer-focused products like Siri and Alexa or business-oriented\nsolutions. Deep learning underlies many recent breakthroughs in dialogue\nsystems but requires very large amounts of training data, often annotated by\nexperts. Trained with smaller data, these methods end up severely lacking\nrobustness (e.g. to disfluencies and out-of-domain input), and often just have\ntoo little generalisation power. In this thesis, we address the above issues by\nintroducing a series of methods for training robust dialogue systems from\nminimal data. Firstly, we study two orthogonal approaches to dialogue:\nlinguistically informed and machine learning-based - from the data efficiency\nperspective. We outline the steps to obtain data-efficient solutions with\neither approach. We then introduce two data-efficient models for dialogue\nresponse generation: the Dialogue Knowledge Transfer Network based on latent\nvariable dialogue representations, and the hybrid Generative-Retrieval\nTransformer model (ranked first at the DSTC 8 Fast Domain Adaptation task).\nNext, we address the problem of robustness given minimal data. As such, propose\na multitask LSTM-based model for domain-general disfluency detection. For the\nproblem of out-of-domain input, we present Turn Dropout, a data augmentation\ntechnique for anomaly detection only using in-domain data, and introduce\nautoencoder-augmented models for efficient training with Turn Dropout. Finally,\nwe focus on social dialogue and introduce a neural model for response ranking\nin social conversation used in Alana, the 3rd place winner in the Amazon Alexa\nPrize 2017 and 2018. We employ a novel technique of predicting the dialogue\nlength as the main ranking objective and show that this approach improves upon\nthe ratings-based counterpart in terms of data efficiency while matching it in\nperformance.", "published": "2020-12-05 02:51:09", "link": "http://arxiv.org/abs/2012.02929v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "On-Device Tag Generation for Unstructured Text", "abstract": "With the overwhelming transition to smart phones, storing important\ninformation in the form of unstructured text has become habitual to users of\nmobile devices. From grocery lists to drafts of emails and important speeches,\nusers store a lot of data in the form of unstructured text (for eg: in the\nNotes application) on their devices, leading to cluttering of data. This not\nonly prevents users from efficient navigation in the applications but also\nprecludes them from perceiving the relations that could be present across data\nin those applications. This paper proposes a novel pipeline to generate a set\nof tags using world knowledge based on the keywords and concepts present in\nunstructured textual data. These tags can then be used to summarize, categorize\nor search for the desired information thus enhancing user experience by\nallowing them to have a holistic outlook of the kind of information stored in\nthe form of unstructured text. In the proposed system, we use an on-device\n(mobile phone) efficient CNN model with pruned ConceptNet resource to achieve\nour goal. The architecture also presents a novel ranking algorithm to extract\nthe top n tags from any given text.", "published": "2020-12-05 09:18:43", "link": "http://arxiv.org/abs/2012.02983v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Codeswitched Sentence Creation using Dependency Parsing", "abstract": "Codeswitching has become one of the most common occurrences across\nmultilingual speakers of the world, especially in countries like India which\nencompasses around 23 official languages with the number of bilingual speakers\nbeing around 300 million. The scarcity of Codeswitched data becomes a\nbottleneck in the exploration of this domain with respect to various Natural\nLanguage Processing (NLP) tasks. We thus present a novel algorithm which\nharnesses the syntactic structure of English grammar to develop grammatically\nsensible Codeswitched versions of English-Hindi, English-Marathi and\nEnglish-Kannada data. Apart from maintaining the grammatical sanity to a great\nextent, our methodology also guarantees abundant generation of data from a\nminuscule snapshot of given data. We use multiple datasets to showcase the\ncapabilities of our algorithm while at the same time we assess the quality of\ngenerated Codeswitched data using some qualitative metrics along with providing\nbaseline results for couple of NLP tasks.", "published": "2020-12-05 10:00:06", "link": "http://arxiv.org/abs/2012.02990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling and Utilizing User's Internal State in Movie Recommendation\n  Dialogue", "abstract": "Intelligent dialogue systems are expected as a new interface between humans\nand machines. Such an intelligent dialogue system should estimate the user's\ninternal state (UIS) in dialogues and change its response appropriately\naccording to the estimation result. In this paper, we model the UIS in\ndialogues, taking movie recommendation dialogues as examples, and construct a\ndialogue system that changes its response based on the UIS. Based on the\ndialogue data analysis, we model the UIS as three elements: knowledge,\ninterest, and engagement. We train the UIS estimators on a dialogue corpus with\nthe modeled UIS's annotations. The estimators achieved high estimation\naccuracy. We also design response change rules that change the system's\nresponses according to each UIS. We confirmed that response changes using the\nresult of the UIS estimators improved the system utterances' naturalness in\nboth dialogue-wise evaluation and utterance-wise evaluation.", "published": "2020-12-05 20:50:53", "link": "http://arxiv.org/abs/2012.03118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Sentiment Classification with In-Domain Contrastive\n  Learning", "abstract": "Contrastive learning (CL) has been successful as a powerful representation\nlearning method. In this paper, we propose a contrastive learning framework for\ncross-domain sentiment classification. We aim to induce domain invariant\noptimal classifiers rather than distribution matching. To this end, we\nintroduce in-domain contrastive learning and entropy minimization. Also, we\nfind through ablation studies that these two techniques behaviour differently\nin case of large label distribution shift and conclude that the best practice\nis to choose one of them adaptively according to label distribution shift. The\nnew state-of-the-art results our model achieves on standard benchmarks show the\nefficacy of the proposed method.", "published": "2020-12-05 03:48:32", "link": "http://arxiv.org/abs/2012.02943v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Boost: Text Data Augmentation Through Reinforcement Learning Guided\n  Conditional Generation", "abstract": "Data augmentation is proven to be effective in many NLU tasks, especially for\nthose suffering from data scarcity. In this paper, we present a powerful and\neasy to deploy text augmentation framework, Data Boost, which augments data\nthrough reinforcement learning guided conditional generation. We evaluate Data\nBoost on three diverse text classification tasks under five different\nclassifier architectures. The result shows that Data Boost can boost the\nperformance of classifiers especially in low-resource data scenarios. For\ninstance, Data Boost improves F1 for the three tasks by 8.7% on average when\ngiven only 10% of the whole data for training. We also compare Data Boost with\nsix prior text augmentation methods. Through human evaluations (N=178), we\nconfirm that Data Boost augmentation has comparable quality as the original\ndata with respect to readability and class consistency.", "published": "2020-12-05 05:21:57", "link": "http://arxiv.org/abs/2012.02952v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhanced Offensive Language Detection Through Data Augmentation", "abstract": "Detecting offensive language on social media is an important task. The\nICWSM-2020 Data Challenge Task 2 is aimed at identifying offensive content\nusing a crowd-sourced dataset containing 100k labelled tweets. The dataset,\nhowever, suffers from class imbalance, where certain labels are extremely rare\ncompared with other classes (e.g, the hateful class is only 5% of the data). In\nthis work, we present Dager (Data Augmenter), a generation-based data\naugmentation method, that improves the performance of classification on\nimbalanced and low-resource data such as the offensive language dataset. Dager\nextracts the lexical features of a given class, and uses these features to\nguide the generation of a conditional generator built on GPT-2. The generated\ntext can then be added to the training set as augmentation data. We show that\napplying Dager can increase the F1 score of the data challenge by 11% when we\nuse 1% of the whole dataset for training (using BERT for classification);\nmoreover, the generated data also preserves the original labels very well. We\ntest Dager on four different classifiers (BERT, CNN, Bi-LSTM with attention,\nand Transformer), observing universal improvement on the detection, indicating\nour method is effective and classifier-agnostic.", "published": "2020-12-05 05:45:16", "link": "http://arxiv.org/abs/2012.02954v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Order-Free Tag Relations for Context-Aware Recommendation", "abstract": "Tag recommendation relies on either a ranking function for top-$k$ tags or an\nautoregressive generation method. However, the previous methods neglect one of\ntwo seemingly conflicting yet desirable characteristics of a tag set:\norderlessness and inter-dependency. While the ranking approach fails to address\nthe inter-dependency among tags when they are ranked, the autoregressive\napproach fails to take orderlessness into account because it is designed to\nutilize sequential relations among tokens. We propose a sequence-oblivious\ngeneration method for tag recommendation, in which the next tag to be generated\nis independent of the order of the generated tags and the order of the ground\ntruth tags occurring in training data. Empirical results on two different\ndomains, Instagram and Stack Overflow, show that our method is significantly\nsuperior to the previous approaches.", "published": "2020-12-05 06:10:56", "link": "http://arxiv.org/abs/2012.02957v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Reciprocal Supervised Learning Improves Neural Machine Translation", "abstract": "Despite the recent success on image classification, self-training has only\nachieved limited gains on structured prediction tasks such as neural machine\ntranslation (NMT). This is mainly due to the compositionality of the target\nspace, where the far-away prediction hypotheses lead to the notorious\nreinforced mistake problem. In this paper, we revisit the utilization of\nmultiple diverse models and present a simple yet effective approach named\nReciprocal-Supervised Learning (RSL). RSL first exploits individual models to\ngenerate pseudo parallel data, and then cooperatively trains each model on the\ncombined synthetic corpus. RSL leverages the fact that different parameterized\nmodels have different inductive biases, and better predictions can be made by\njointly exploiting the agreement among each other. Unlike the previous\nknowledge distillation methods built upon a much stronger teacher, RSL is\ncapable of boosting the accuracy of one model by introducing other comparable\nor even weaker models. RSL can also be viewed as a more efficient alternative\nto ensemble. Extensive experiments demonstrate the superior performance of RSL\non several benchmarks with significant margins.", "published": "2020-12-05 08:23:13", "link": "http://arxiv.org/abs/2012.02975v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pre-training Protein Language Models with Label-Agnostic Binding Pairs\n  Enhances Performance in Downstream Tasks", "abstract": "Less than 1% of protein sequences are structurally and functionally\nannotated. Natural Language Processing (NLP) community has recently embraced\nself-supervised learning as a powerful approach to learn representations from\nunlabeled text, in large part due to the attention-based context-aware\nTransformer models. In this work we present a modification to the RoBERTa model\nby inputting during pre-training a mixture of binding and non-binding protein\nsequences (from STRING database). However, the sequence pairs have no label to\nindicate their binding status, as the model relies solely on Masked Language\nModeling (MLM) objective during pre-training. After fine-tuning, such approach\nsurpasses models trained on single protein sequences for protein-protein\nbinding prediction, TCR-epitope binding prediction, cellular-localization and\nremote homology classification tasks. We suggest that the Transformer's\nattention mechanism contributes to protein binding site discovery. Furthermore,\nwe compress protein sequences by 64% with the Byte Pair Encoding (BPE)\nvocabulary consisting of 10K subwords, each around 3-4 amino acids long.\nFinally, to expand the model input space to even larger proteins and\nmulti-protein assemblies, we pre-train Longformer models that support 2,048\ntokens. Further work in token-level classification for secondary structure\nprediction is needed. Code available at:\nhttps://github.com/PaccMann/paccmann_proteomics", "published": "2020-12-05 17:37:41", "link": "http://arxiv.org/abs/2012.03084v1", "categories": ["q-bio.BM", "cs.CL"], "primary_category": "q-bio.BM"}
{"title": "Over a Decade of Social Opinion Mining: A Systematic Review", "abstract": "Social media popularity and importance is on the increase due to people using\nit for various types of social interaction across multiple channels. This\nsystematic review focuses on the evolving research area of Social Opinion\nMining, tasked with the identification of multiple opinion dimensions, such as\nsubjectivity, sentiment polarity, emotion, affect, sarcasm and irony, from\nuser-generated content represented across multiple social media platforms and\nin various media formats, like text, image, video and audio. Through Social\nOpinion Mining, natural language can be understood in terms of the different\nopinion dimensions, as expressed by humans. This contributes towards the\nevolution of Artificial Intelligence which in turn helps the advancement of\nseveral real-world use cases, such as customer service and decision making. A\nthorough systematic review was carried out on Social Opinion Mining research\nwhich totals 485 published studies and spans a period of twelve years between\n2007 and 2018. The in-depth analysis focuses on the social media platforms,\ntechniques, social datasets, language, modality, tools and technologies, and\nother aspects derived. Social Opinion Mining can be utilised in many\napplication areas, ranging from marketing, advertising and sales for\nproduct/service management, and in multiple domains and industries, such as\npolitics, technology, finance, healthcare, sports and government. The latest\ndevelopments in Social Opinion Mining beyond 2018 are also presented together\nwith future research directions, with the aim of leaving a wider academic and\nsocietal impact in several real-world applications.", "published": "2020-12-05 17:59:59", "link": "http://arxiv.org/abs/2012.03091v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does Yoga Make You Happy? Analyzing Twitter User Happiness using Textual\n  and Temporal Information", "abstract": "Although yoga is a multi-component practice to hone the body and mind and be\nknown to reduce anxiety and depression, there is still a gap in understanding\npeople's emotional state related to yoga in social media. In this study, we\ninvestigate the causal relationship between practicing yoga and being happy by\nincorporating textual and temporal information of users using Granger\ncausality. To find out causal features from the text, we measure two variables\n(i) Yoga activity level based on content analysis and (ii) Happiness level\nbased on emotional state. To understand users' yoga activity, we propose a\njoint embedding model based on the fusion of neural networks with attention\nmechanism by leveraging users' social and textual information. For measuring\nthe emotional state of yoga users (target domain), we suggest a transfer\nlearning approach to transfer knowledge from an attention-based neural network\nmodel trained on a source domain. Our experiment on Twitter dataset\ndemonstrates that there are 1447 users where \"yoga Granger-causes happiness\".", "published": "2020-12-05 03:30:49", "link": "http://arxiv.org/abs/2012.02939v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
