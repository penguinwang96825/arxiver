{"title": "Topic Sensitive Neural Headline Generation", "abstract": "Neural models have recently been used in text summarization including\nheadline generation. The model can be trained using a set of document-headline\npairs. However, the model does not explicitly consider topical similarities and\ndifferences of documents. We suggest to categorizing documents into various\ntopics so that documents within the same topic are similar in content and share\nsimilar summarization patterns. Taking advantage of topic information of\ndocuments, we propose topic sensitive neural headline generation model. Our\nmodel can generate more accurate summaries guided by document topics. We test\nour model on LCSTS dataset, and experiments show that our method outperforms\nother baselines on each topic and achieves the state-of-art performance.", "published": "2016-08-20 03:43:29", "link": "http://arxiv.org/abs/1608.05777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using the Output Embedding to Improve Language Models", "abstract": "We study the topmost weight matrix of neural network language models. We show\nthat this matrix constitutes a valid word embedding. When training language\nmodels, we recommend tying the input embedding and this output embedding. We\nanalyze the resulting update rules and show that the tied embedding evolves in\na more similar way to the output embedding than to the input embedding in the\nuntied model. We also offer a new method of regularizing the output embedding.\nOur methods lead to a significant reduction in perplexity, as we are able to\nshow on a variety of neural network language models. Finally, we show that\nweight tying can reduce the size of neural translation models to less than half\nof their original size without harming their performance.", "published": "2016-08-20 18:32:05", "link": "http://arxiv.org/abs/1608.05859v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "phi-LSTM: A Phrase-based Hierarchical LSTM Model for Image Captioning", "abstract": "A picture is worth a thousand words. Not until recently, however, we noticed\nsome success stories in understanding of visual scenes: a model that is able to\ndetect/name objects, describe their attributes, and recognize their\nrelationships/interactions. In this paper, we propose a phrase-based\nhierarchical Long Short-Term Memory (phi-LSTM) model to generate image\ndescription. The proposed model encodes sentence as a sequence of combination\nof phrases and words, instead of a sequence of words alone as in those\nconventional solutions. The two levels of this model are dedicated to i) learn\nto generate image relevant noun phrases, and ii) produce appropriate image\ndescription from the phrases and other words in the corpus. Adopting a\nconvolutional neural network to learn image features and the LSTM to learn the\nword sequence in a sentence, the proposed model has shown better or competitive\nresults in comparison to the state-of-the-art models on Flickr8k and Flickr30k\ndatasets.", "published": "2016-08-20 12:12:09", "link": "http://arxiv.org/abs/1608.05813v5", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Learning Word Embeddings from Intrinsic and Extrinsic Views", "abstract": "While word embeddings are currently predominant for natural language\nprocessing, most of existing models learn them solely from their contexts.\nHowever, these context-based word embeddings are limited since not all words'\nmeaning can be learned based on only context. Moreover, it is also difficult to\nlearn the representation of the rare words due to data sparsity problem. In\nthis work, we address these issues by learning the representations of words by\nintegrating their intrinsic (descriptive) and extrinsic (contextual)\ninformation. To prove the effectiveness of our model, we evaluate it on four\ntasks, including word similarity, reverse dictionaries,Wiki link prediction,\nand document classification. Experiment results show that our model is powerful\nin both word and document modeling.", "published": "2016-08-20 17:34:38", "link": "http://arxiv.org/abs/1608.05852v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
