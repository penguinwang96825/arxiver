{"title": "Word Segmentation for Asian Languages: Chinese, Korean, and Japanese", "abstract": "We provide a detailed overview of various approaches to word segmentation of\nAsian Languages, specifically Chinese, Korean, and Japanese languages. For each\nlanguage, approaches to deal with word segmentation differs. We also include\nour analysis about certain advantages and disadvantages to each method. In\naddition, there is room for future work in this field.", "published": "2024-07-28 05:06:58", "link": "http://arxiv.org/abs/2407.19400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impact of Decoding Methods on Human Alignment of Conversational LLMs", "abstract": "To be included into chatbot systems, Large language models (LLMs) must be\naligned with human conversational conventions. However, being trained mainly on\nweb-scraped data gives existing LLMs a voice closer to informational text than\nactual human speech. In this paper, we examine the effect of decoding methods\non the alignment between LLM-generated and human conversations, including Beam\nSearch, Top K Sampling, and Nucleus Sampling. We present new measures of\nalignment in substance, style, and psychometric orientation, and experiment\nwith two conversation datasets. Our results provide subtle insights: better\nalignment is attributed to fewer beams in Beam Search and lower values of P in\nNucleus Sampling. We also find that task-oriented and open-ended datasets\nperform differently in terms of alignment, indicating the significance of\ntaking into account the context of the interaction.", "published": "2024-07-28 16:31:09", "link": "http://arxiv.org/abs/2407.19526v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Sentence Embeddings for Portuguese with the Serafim PT* encoders\n  family", "abstract": "Sentence encoder encode the semantics of their input, enabling key downstream\napplications such as classification, clustering, or retrieval. In this paper,\nwe present Serafim PT*, a family of open-source sentence encoders for\nPortuguese with various sizes, suited to different hardware/compute budgets.\nEach model exhibits state-of-the-art performance and is made openly available\nunder a permissive license, allowing its use for both commercial and research\npurposes. Besides the sentence encoders, this paper contributes a systematic\nstudy and lessons learned concerning the selection criteria of learning\nobjectives and parameters that support top-performing encoders.", "published": "2024-07-28 16:34:25", "link": "http://arxiv.org/abs/2407.19527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Motamot: A Dataset for Revealing the Supremacy of Large Language Models\n  over Transformer Models in Bengali Political Sentiment Analysis", "abstract": "Sentiment analysis is the process of identifying and categorizing people's\nemotions or opinions regarding various topics. Analyzing political sentiment is\ncritical for understanding the complexities of public opinion processes,\nespecially during election seasons. It gives significant information on voter\npreferences, attitudes, and current trends. In this study, we investigate\npolitical sentiment analysis during Bangladeshi elections, specifically\nexamining how effectively Pre-trained Language Models (PLMs) and Large Language\nModels (LLMs) capture complex sentiment characteristics. Our study centers on\nthe creation of the \"Motamot\" dataset, comprising 7,058 instances annotated\nwith positive and negative sentiments, sourced from diverse online newspaper\nportals, forming a comprehensive resource for political sentiment analysis. We\nmeticulously evaluate the performance of various PLMs including BanglaBERT,\nBangla BERT Base, XLM-RoBERTa, mBERT, and sahajBERT, alongside LLMs such as\nGemini 1.5 Pro and GPT 3.5 Turbo. Moreover, we explore zero-shot and few-shot\nlearning strategies to enhance our understanding of political sentiment\nanalysis methodologies. Our findings underscore BanglaBERT's commendable\naccuracy of 88.10% among PLMs. However, the exploration into LLMs reveals even\nmore promising results. Through the adept application of Few-Shot learning\ntechniques, Gemini 1.5 Pro achieves an impressive accuracy of 96.33%,\nsurpassing the remarkable performance of GPT 3.5 Turbo, which stands at 94%.\nThis underscores Gemini 1.5 Pro's status as the superior performer in this\ncomparison.", "published": "2024-07-28 16:34:53", "link": "http://arxiv.org/abs/2407.19528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal\n  Domain", "abstract": "In this paper, we introduce SaulLM-54B and SaulLM-141B, two large language\nmodels (LLMs) tailored for the legal sector. These models, which feature\narchitectures of 54 billion and 141 billion parameters, respectively, are based\non the Mixtral architecture. The development of SaulLM-54B and SaulLM-141B is\nguided by large-scale domain adaptation, divided into three strategies: (1) the\nexploitation of continued pretraining involving a base corpus that includes\nover 540 billion of legal tokens, (2) the implementation of a specialized legal\ninstruction-following protocol, and (3) the alignment of model outputs with\nhuman preferences in legal interpretations. The integration of synthetically\ngenerated data in the second and third steps enhances the models' capabilities\nin interpreting and processing legal texts, effectively reaching\nstate-of-the-art performance and outperforming previous open-source models on\nLegalBench-Instruct. This work explores the trade-offs involved in\ndomain-specific adaptation at this scale, offering insights that may inform\nfuture studies on domain adaptation using strong decoder models. Building upon\nSaulLM-7B, this study refines the approach to produce an LLM better equipped\nfor legal tasks. We are releasing base, instruct, and aligned versions on top\nof SaulLM-54B and SaulLM-141B under the MIT License to facilitate reuse and\ncollaborative research.", "published": "2024-07-28 20:50:53", "link": "http://arxiv.org/abs/2407.19584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Genre and Success Classification through Song Lyrics using\n  DistilBERT: A Fun NLP Venture", "abstract": "This paper presents a natural language processing (NLP) approach to the\nproblem of thoroughly comprehending song lyrics, with particular attention on\ngenre classification, view-based success prediction, and approximate release\nyear. Our tests provide promising results with 65\\% accuracy in genre\nclassification and 79\\% accuracy in success prediction, leveraging a DistilBERT\nmodel for genre classification and BERT embeddings for release year prediction.\nSupport Vector Machines outperformed other models in predicting the release\nyear, achieving the lowest root mean squared error (RMSE) of 14.18. Our study\noffers insights that have the potential to revolutionize our relationship with\nmusic by addressing the shortcomings of current approaches in properly\nunderstanding the emotional intricacies of song lyrics.", "published": "2024-07-28 13:35:03", "link": "http://arxiv.org/abs/2407.21068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLAVADI: What Matters For Multimodal Large Language Models Distillation", "abstract": "The recent surge in Multimodal Large Language Models (MLLMs) has showcased\ntheir remarkable potential for achieving generalized intelligence by\nintegrating visual understanding into Large Language Models.Nevertheless, the\nsheer model size of MLLMs leads to substantial memory and computational demands\nthat hinder their widespread deployment. In this work, we do not propose a new\nefficient model structure or train small-scale MLLMs from scratch. Instead, we\nfocus on what matters for training small-scale MLLMs through knowledge\ndistillation, which is the first step from the multimodal distillation\nperspective. Our extensive studies involve training strategies, model choices,\nand distillation algorithms in the knowledge distillation process. These\nresults show that joint alignment for both tokens and logit alignment plays\ncritical roles in teacher-student frameworks. In addition, we draw a series of\nintriguing observations from this study. By evaluating different benchmarks and\nproper strategy, even a 2.7B small-scale model can perform on par with larger\nmodels with 7B or 13B parameters. Our code and models will be publicly\navailable for further research.", "published": "2024-07-28 06:10:47", "link": "http://arxiv.org/abs/2407.19409v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Visual Riddles: a Commonsense and World Knowledge Challenge for Large\n  Vision and Language Models", "abstract": "Imagine observing someone scratching their arm; to understand why, additional\ncontext would be necessary. However, spotting a mosquito nearby would\nimmediately offer a likely explanation for the person's discomfort, thereby\nalleviating the need for further information. This example illustrates how\nsubtle visual cues can challenge our cognitive skills and demonstrates the\ncomplexity of interpreting visual scenarios. To study these skills, we present\nVisual Riddles, a benchmark aimed to test vision and language models on visual\nriddles requiring commonsense and world knowledge. The benchmark comprises 400\nvisual riddles, each featuring a unique image created by a variety of\ntext-to-image models, question, ground-truth answer, textual hint, and\nattribution. Human evaluation reveals that existing models lag significantly\nbehind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading\nwith 40% accuracy. Our benchmark comes with automatic evaluation tasks to make\nassessment scalable. These findings underscore the potential of Visual Riddles\nas a valuable resource for enhancing vision and language models' capabilities\nin interpreting complex visual scenarios.", "published": "2024-07-28 11:56:03", "link": "http://arxiv.org/abs/2407.19474v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?", "abstract": "Large Language Models (LLMs) have demonstrated proficiency in a wide array of\nnatural language processing tasks. However, its effectiveness over\ndiscourse-level event relation extraction (ERE) tasks remains unexplored. In\nthis paper, we assess the effectiveness of LLMs in addressing discourse-level\nERE tasks characterized by lengthy documents and intricate relations\nencompassing coreference, temporal, causal, and subevent types. Evaluation is\nconducted using an commercial model, GPT-3.5, and an open-source model,\nLLaMA-2. Our study reveals a notable underperformance of LLMs compared to the\nbaseline established through supervised learning. Although Supervised\nFine-Tuning (SFT) can improve LLMs performance, it does not scale well compared\nto the smaller supervised baseline model. Our quantitative and qualitative\nanalysis shows that LLMs have several weaknesses when applied for extracting\nevent relations, including a tendency to fabricate event mentions, and failures\nto capture transitivity rules among relations, detect long distance relations,\nor comprehend contexts with dense event mentions. Code available at:\nhttps://github.com/WeiKangda/LLM-ERE.git.", "published": "2024-07-28 19:27:06", "link": "http://arxiv.org/abs/2407.19568v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meta-Rewarding Language Models: Self-Improving Alignment with\n  LLM-as-a-Meta-Judge", "abstract": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many\ndomains. While improving these models traditionally relies on costly human\ndata, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs\ncan improve by judging their own responses instead of relying on human\nlabelers. However, existing methods have primarily focused on improving model\nresponses rather than judgment capabilities, resulting in rapid saturation\nduring iterative training. To address this issue, we introduce a novel\nMeta-Rewarding step to the self-improvement process, where the model judges its\nown judgements and uses that feedback to refine its judgment skills.\nSurprisingly, this unsupervised approach improves the model's ability to judge\n{\\em and} follow instructions, as demonstrated by a win rate improvement of\nLlama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on\nArena-Hard. These results strongly suggest the potential for self-improving\nmodels without human supervision.", "published": "2024-07-28 21:58:28", "link": "http://arxiv.org/abs/2407.19594v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "You shall know a piece by the company it keeps. Chess plays as a data\n  for word2vec models", "abstract": "In this paper, I apply linguistic methods of analysis to non-linguistic data,\nchess plays, metaphorically equating one with the other and seeking analogies.\nChess game notations are also a kind of text, and one can consider the records\nof moves or positions of pieces as words and statements in a certain language.\nIn this article I show how word embeddings (word2vec) can work on chess game\ntexts instead of natural language texts. I don't see how this representation of\nchess data can be used productively. It's unlikely that these vector models\nwill help engines or people choose the best move. But in a purely academic\nsense, it's clear that such methods of information representation capture\nsomething important about the very nature of the game, which doesn't\nnecessarily lead to a win.", "published": "2024-07-28 22:12:36", "link": "http://arxiv.org/abs/2407.19600v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Occam's Razor and Bender and Koller's Octopus", "abstract": "We discuss the teaching of the discussion surrounding Bender and Koller's\nprominent ACL 2020 paper, \"Climbing toward NLU: on meaning form, and\nunderstanding in the age of data\" \\cite{bender2020climbing}. We present what we\nunderstand to be the main contentions of the paper, and then recommend that the\nstudents engage with the natural counter-arguments to the claims in the paper.\nWe attach teaching materials that we use to facilitate teaching this topic to\nundergraduate students.", "published": "2024-07-28 18:33:58", "link": "http://arxiv.org/abs/2407.21070v2", "categories": ["cs.CL", "physics.hist-ph"], "primary_category": "cs.CL"}
{"title": "Mini-batch Coresets for Memory-efficient Language Model Training on Data\n  Mixtures", "abstract": "Training with larger mini-batches improves the convergence rate and can yield\nsuperior performance. However, training with large mini-batches becomes\nprohibitive for Large Language Models (LLMs), due to the large GPU memory\nrequirement. To address this problem, an effective approach is finding small\nmini-batch coresets that closely match the gradient of larger mini-batches.\nHowever, this approach becomes infeasible and ineffective for LLMs, due to the\nhighly imbalanced mixture of sources in language data, use of the Adam\noptimizer, and the very large gradient dimensionality of LLMs. In this work, we\naddress the above challenges by proposing Coresets for Training LLMs (CoLM).\nFirst, we show that mini-batch coresets found by gradient matching do not\ncontain representative examples of the small sources w.h.p., and thus including\nall examples of the small sources in the mini-batch coresets is crucial for\noptimal performance. Second, we normalize the gradients by their historical\nexponential to find mini-batch coresets for training with Adam. Finally, we\nleverage zeroth-order methods to find smooth gradient of the last V-projection\nmatrix and sparsify it to keep the dimensions with the largest normalized\ngradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, Zephyr, and\nLlama-3 models with LoRA on MathInstruct and SuperGLUE benchmark. Remarkably,\nCoLM reduces the memory requirement of fine-tuning by 2x and even outperforms\ntraining with 4x larger mini-batches. Moreover, CoLM seamlessly integrates with\nexisting memory-efficient training methods like LoRA, further reducing the\nmemory requirements of training LLMs.", "published": "2024-07-28 20:39:16", "link": "http://arxiv.org/abs/2407.19580v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards a Universal Method for Meaningful Signal Detection", "abstract": "It is known that human speech and certain animal vocalizations can convey\nmeaningful content because we can decipher the content that a given utterance\ndoes convey. This paper explores an alternative approach to determining whether\na signal is meaningful, one that analyzes only the signal itself and is\nindependent of what the conveyed meaning might be. We devise a method that\ntakes a waveform as input and outputs a score indicating its degree of\n`meaningfulness`. We cluster contiguous portions of the input to minimize the\ntotal description length, and then take the length of the code of the assigned\ncluster labels as meaningfulness score. We evaluate our method empirically,\nagainst several baselines, and show that it is the only one to give a high\nscore to human speech in various languages and with various speakers, a\nmoderate score to animal vocalizations from birds and orcas, and a low score to\nambient noise from various sources.", "published": "2024-07-28 15:45:08", "link": "http://arxiv.org/abs/2408.00016v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ASI-Seg: Audio-Driven Surgical Instrument Segmentation with Surgeon\n  Intention Understanding", "abstract": "Surgical instrument segmentation is crucial in surgical scene understanding,\nthereby facilitating surgical safety. Existing algorithms directly detected all\ninstruments of pre-defined categories in the input image, lacking the\ncapability to segment specific instruments according to the surgeon's\nintention. During different stages of surgery, surgeons exhibit varying\npreferences and focus toward different surgical instruments. Therefore, an\ninstrument segmentation algorithm that adheres to the surgeon's intention can\nminimize distractions from irrelevant instruments and assist surgeons to a\ngreat extent. The recent Segment Anything Model (SAM) reveals the capability to\nsegment objects following prompts, but the manual annotations for prompts are\nimpractical during the surgery. To address these limitations in operating\nrooms, we propose an audio-driven surgical instrument segmentation framework,\nnamed ASI-Seg, to accurately segment the required surgical instruments by\nparsing the audio commands of surgeons. Specifically, we propose an\nintention-oriented multimodal fusion to interpret the segmentation intention\nfrom audio commands and retrieve relevant instrument details to facilitate\nsegmentation. Moreover, to guide our ASI-Seg segment of the required surgical\ninstruments, we devise a contrastive learning prompt encoder to effectively\ndistinguish the required instruments from the irrelevant ones. Therefore, our\nASI-Seg promotes the workflow in the operating rooms, thereby providing\ntargeted support and reducing the cognitive load on surgeons. Extensive\nexperiments are performed to validate the ASI-Seg framework, which reveals\nremarkable advantages over classical state-of-the-art and medical SAMs in both\nsemantic segmentation and intention-oriented segmentation. The source code is\navailable at https://github.com/Zonmgin-Zhang/ASI-Seg.", "published": "2024-07-28 09:25:59", "link": "http://arxiv.org/abs/2407.19435v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.RO"], "primary_category": "cs.CV"}
{"title": "ELP-Adapters: Parameter Efficient Adapter Tuning for Various Speech\n  Processing Tasks", "abstract": "Self-supervised learning has emerged as a key approach for learning generic\nrepresentations from speech data. Despite promising results in downstream tasks\nsuch as speech recognition, speaker verification, and emotion recognition, a\nsignificant number of parameters is required, which makes fine-tuning for each\ntask memory-inefficient. To address this limitation, we introduce ELP-adapter\ntuning, a novel method for parameter-efficient fine-tuning using three types of\nadapter, namely encoder adapters (E-adapters), layer adapters (L-adapters), and\na prompt adapter (P-adapter). The E-adapters are integrated into\ntransformer-based encoder layers and help to learn fine-grained speech\nrepresentations that are effective for speech recognition. The L-adapters\ncreate paths from each encoder layer to the downstream head and help to extract\nnon-linguistic features from lower encoder layers that are effective for\nspeaker verification and emotion recognition. The P-adapter appends pseudo\nfeatures to CNN features to further improve effectiveness and efficiency. With\nthese adapters, models can be quickly adapted to various speech processing\ntasks. Our evaluation across four downstream tasks using five backbone models\ndemonstrated the effectiveness of the proposed method. With the WavLM backbone,\nits performance was comparable to or better than that of full fine-tuning on\nall tasks while requiring 90% fewer learnable parameters.", "published": "2024-07-28 05:26:03", "link": "http://arxiv.org/abs/2407.21066v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ctPuLSE: Close-Talk, and Pseudo-Label Based Far-Field, Speech\n  Enhancement", "abstract": "The current dominant approach for neural speech enhancement is via\npurely-supervised deep learning on simulated pairs of far-field\nnoisy-reverberant speech (i.e., mixtures) and clean speech. The trained models,\nhowever, often exhibit limited generalizability to real-recorded mixtures. To\ndeal with this, this paper investigates training enhancement models directly on\nreal mixtures. However, a major difficulty challenging this approach is that,\nsince the clean speech of real mixtures is unavailable, there lacks a good\nsupervision for real mixtures. In this context, assuming that a training set\nconsisting of real-recorded pairs of close-talk and far-field mixtures is\navailable, we propose to address this difficulty via close-talk speech\nenhancement, where an enhancement model is first trained on simulated mixtures\nto enhance real-recorded close-talk mixtures and the estimated close-talk\nspeech can then be utilized as a supervision (i.e., pseudo-label) for training\nfar-field speech enhancement models directly on the paired real-recorded\nfar-field mixtures. We name the proposed system $\\textit{ctPuLSE}$. Evaluation\nresults on the CHiME-4 dataset show that ctPuLSE can derive high-quality\npseudo-labels and yield far-field speech enhancement models with strong\ngeneralizability to real data.", "published": "2024-07-28 12:41:25", "link": "http://arxiv.org/abs/2407.19485v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
