{"title": "Speaker Role Contextual Modeling for Language Understanding and Dialogue\n  Policy Learning", "abstract": "Language understanding (LU) and dialogue policy learning are two essential\ncomponents in conversational systems. Human-human dialogues are not\nwell-controlled and often random and unpredictable due to their own goals and\nspeaking habits. This paper proposes a role-based contextual model to consider\ndifferent speaker roles independently based on the various speaking patterns in\nthe multi-turn dialogues. The experiments on the benchmark dataset show that\nthe proposed role-based model successfully learns role-specific behavioral\npatterns for contextual encoding and then significantly improves language\nunderstanding and dialogue policy learning tasks.", "published": "2017-09-30 08:46:03", "link": "http://arxiv.org/abs/1710.00164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken\n  Language Understanding", "abstract": "Spoken language understanding (SLU) is an essential component in\nconversational systems. Most SLU component treats each utterance independently,\nand then the following components aggregate the multi-turn information in the\nseparate phases. In order to avoid error propagation and effectively utilize\ncontexts, prior work leveraged history for contextual SLU. However, the\nprevious model only paid attention to the content in history utterances without\nconsidering their temporal information and speaker roles. In the dialogues, the\nmost recent utterances should be more important than the least recent ones.\nFurthermore, users usually pay attention to 1) self history for reasoning and\n2) others' utterances for listening, the speaker of the utterances may provides\ninformative cues to help understanding. Therefore, this paper proposes an\nattention-based network that additionally leverages temporal information and\nspeaker role for better SLU, where the attention to contexts and speaker roles\ncan be automatically learned in an end-to-end manner. The experiments on the\nbenchmark Dialogue State Tracking Challenge 4 (DSTC4) dataset show that the\ntime-aware dynamic role attention networks significantly improve the\nunderstanding performance.", "published": "2017-09-30 08:50:16", "link": "http://arxiv.org/abs/1710.00165v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bag-of-Vector Embeddings of Dependency Graphs for Semantic Induction", "abstract": "Vector-space models, from word embeddings to neural network parsers, have\nmany advantages for NLP. But how to generalise from fixed-length word vectors\nto a vector space for arbitrary linguistic structures is still unclear. In this\npaper we propose bag-of-vector embeddings of arbitrary linguistic graphs. A\nbag-of-vector space is the minimal nonparametric extension of a vector space,\nallowing the representation to grow with the size of the graph, but not tying\nthe representation to any specific tree or graph structure. We propose\nefficient training and inference algorithms based on tensor factorisation for\nembedding arbitrary graphs in a bag-of-vector space. We demonstrate the\nusefulness of this representation by training bag-of-vector embeddings of\ndependency graphs and evaluating them on unsupervised semantic induction for\nthe Semantic Textual Similarity and Natural Language Inference tasks.", "published": "2017-09-30 14:21:12", "link": "http://arxiv.org/abs/1710.00205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
