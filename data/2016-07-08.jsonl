{"title": "Consensus Attention-based Neural Networks for Chinese Reading\n  Comprehension", "abstract": "Reading comprehension has embraced a booming in recent NLP research. Several\ninstitutes have released the Cloze-style reading comprehension data, and these\nhave greatly accelerated the research of machine comprehension. In this work,\nwe firstly present Chinese reading comprehension datasets, which consist of\nPeople Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we\npropose a consensus attention-based neural network architecture to tackle the\nCloze-style reading comprehension problem, which aims to induce a consensus\nattention over every words in the query. Experimental results show that the\nproposed neural network significantly outperforms the state-of-the-art\nbaselines in several public datasets. Furthermore, we setup a baseline for\nChinese reading comprehension task, and hopefully this would speed up the\nprocess for future research.", "published": "2016-07-08 06:46:48", "link": "http://arxiv.org/abs/1607.02250v3", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Collaborative Training of Tensors for Compositional Distributional\n  Semantics", "abstract": "Type-based compositional distributional semantic models present an\ninteresting line of research into functional representations of linguistic\nmeaning. One of the drawbacks of such models, however, is the lack of training\ndata required to train each word-type combination. In this paper we address\nthis by introducing training methods that share parameters between similar\nwords. We show that these methods enable zero-shot learning for words that have\nno training data at all, as well as enabling construction of high-quality\ntensors from very few training examples per word.", "published": "2016-07-08 11:01:56", "link": "http://arxiv.org/abs/1607.02310v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lexical Based Semantic Orientation of Online Customer Reviews and Blogs", "abstract": "Rapid increase in internet users along with growing power of online review\nsites and social media has given birth to sentiment analysis or opinion mining,\nwhich aims at determining what other people think and comment. Sentiments or\nOpinions contain public generated content about products, services, policies\nand politics. People are usually interested to seek positive and negative\nopinions containing likes and dislikes, shared by users for features of\nparticular product or service. This paper proposed sentence-level lexical based\ndomain independent sentiment classification method for different types of data\nsuch as reviews and blogs. The proposed method is based on general lexicons\ni.e. WordNet, SentiWordNet and user defined lexical dictionaries for semantic\norientation. The relations and glosses of these dictionaries provide solution\nto the domain portability problem. The method performs better than word and\ntext level corpus based machine learning methods for semantic orientation. The\nresults show the proposed method performs better as it shows precision of 87%\nand83% at document and sentence levels respectively for online comments.", "published": "2016-07-08 13:20:35", "link": "http://arxiv.org/abs/1607.02355v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Actionable and Political Text Classification using Word Embeddings and\n  LSTM", "abstract": "In this work, we apply word embeddings and neural networks with Long\nShort-Term Memory (LSTM) to text classification problems, where the\nclassification criteria are decided by the context of the application. We\nexamine two applications in particular. The first is that of Actionability,\nwhere we build models to classify social media messages from customers of\nservice providers as Actionable or Non-Actionable. We build models for over 30\ndifferent languages for actionability, and most of the models achieve accuracy\naround 85%, with some reaching over 90% accuracy. We also show that using LSTM\nneural networks with word embeddings vastly outperform traditional techniques.\nSecond, we explore classification of messages with respect to political\nleaning, where social media messages are classified as Democratic or\nRepublican. The model is able to classify messages with a high accuracy of\n87.57%. As part of our experiments, we vary different hyperparameters of the\nneural networks, and report the effect of such variation on the accuracy. These\nactionability models have been deployed to production and help company agents\nprovide customer support by prioritizing which messages to respond to. The\nmodel for political leaning has been opened and made available for wider use.", "published": "2016-07-08 19:53:56", "link": "http://arxiv.org/abs/1607.02501v2", "categories": ["cs.CL", "cs.IR", "68T50, 92B20"], "primary_category": "cs.CL"}
{"title": "Document Clustering Games in Static and Dynamic Scenarios", "abstract": "In this work we propose a game theoretic model for document clustering. Each\ndocument to be clustered is represented as a player and each cluster as a\nstrategy. The players receive a reward interacting with other players that they\ntry to maximize choosing their best strategies. The geometry of the data is\nmodeled with a weighted graph that encodes the pairwise similarity among\ndocuments, so that similar players are constrained to choose similar\nstrategies, updating their strategy preferences at each iteration of the games.\nWe used different approaches to find the prototypical elements of the clusters\nand with this information we divided the players into two disjoint sets, one\ncollecting players with a definite strategy and the other one collecting\nplayers that try to learn from others the correct strategy to play. The latter\nset of players can be considered as new data points that have to be clustered\naccording to previous information. This representation is useful in scenarios\nin which the data are streamed continuously. The evaluation of the system was\nconducted on 13 document datasets using different settings. It shows that the\nproposed method performs well compared to different document clustering\nalgorithms.", "published": "2016-07-08 16:17:12", "link": "http://arxiv.org/abs/1607.02436v1", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI"}
{"title": "Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior\n  Knowledge", "abstract": "We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural\nNetworks that replaces the softmax output layer by a log-linear output layer,\nof which the softmax is a special case. This conceptually simple move has two\nmain advantages. First, it allows the learner to combat training data sparsity\nby allowing it to model words (or more generally, output symbols) as complex\ncombinations of attributes without requiring that each combination is directly\nobserved in the training data (as the softmax does). Second, it permits the\ninclusion of flexible prior knowledge in the form of a priori specified modular\nfeatures, where the neural network component learns to dynamically control the\nweights of a log-linear distribution exploiting these features.\n  We conduct experiments in the domain of language modelling of French, that\nexploit morphological prior knowledge and show an important decrease in\nperplexity relative to a baseline RNN.\n  We provide other motivating iillustrations, and finally argue that the\nlog-linear and the neural-network components contribute complementary strengths\nto the LL-RNN: the LL aspect allows the model to incorporate rich prior\nknowledge, while the NN aspect, according to the \"representation learning\"\nparadigm, allows the model to discover novel combination of characteristics.", "published": "2016-07-08 17:35:51", "link": "http://arxiv.org/abs/1607.02467v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.AI"}
