{"title": "A Monolingual Approach to Contextualized Word Embeddings for\n  Mid-Resource Languages", "abstract": "We use the multilingual OSCAR corpus, extracted from Common Crawl via\nlanguage classification, filtering and cleaning, to train monolingual\ncontextualized word embeddings (ELMo) for five mid-resource languages. We then\ncompare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for\nthese languages on the part-of-speech tagging and parsing tasks. We show that,\ndespite the noise in the Common-Crawl-based OSCAR data, embeddings trained on\nOSCAR perform much better than monolingual embeddings trained on Wikipedia.\nThey actually equal or improve the current state of the art in tagging and\nparsing for all five languages. In particular, they also improve over\nmultilingual Wikipedia-based contextual embeddings (multilingual BERT), which\nalmost always constitutes the previous state of the art, thereby showing that\nthe benefit of a larger, more diverse corpus surpasses the cross-lingual\nbenefit of multilingual embedding architectures.", "published": "2020-06-11 05:25:18", "link": "http://arxiv.org/abs/2006.06202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discrete Latent Variable Representations for Low-Resource Text\n  Classification", "abstract": "While much work on deep latent variable models of text uses continuous latent\nvariables, discrete latent variables are interesting because they are more\ninterpretable and typically more space efficient. We consider several\napproaches to learning discrete latent variable models for text in the case\nwhere exact marginalization over these variables is intractable. We compare the\nperformance of the learned representations as features for low-resource\ndocument and sentence classification. Our best models outperform the previous\nbest reported results with continuous representations in these low-resource\nsettings, while learning significantly more compressed representations.\nInterestingly, we find that an amortized variant of Hard EM performs\nparticularly well in the lowest-resource regimes.", "published": "2020-06-11 06:55:13", "link": "http://arxiv.org/abs/2006.06226v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Data for Sarcasm Detection with Unlabeled Conversation\n  Context", "abstract": "We present a novel data augmentation technique, CRA (Contextual Response\nAugmentation), which utilizes conversational context to generate meaningful\nsamples for training. We also mitigate the issues regarding unbalanced context\nlengths by changing the input-output format of the model such that it can deal\nwith varying context lengths effectively. Specifically, our proposed model,\ntrained with the proposed data augmentation technique, participated in the\nsarcasm detection task of FigLang2020, have won and achieves the best\nperformance in both Reddit and Twitter datasets.", "published": "2020-06-11 09:00:11", "link": "http://arxiv.org/abs/2006.06259v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine\n  Translation Evaluation Metrics", "abstract": "Automatic metrics are fundamental for the development and evaluation of\nmachine translation systems. Judging whether, and to what extent, automatic\nmetrics concur with the gold standard of human evaluation is not a\nstraightforward problem. We show that current methods for judging metrics are\nhighly sensitive to the translations used for assessment, particularly the\npresence of outliers, which often leads to falsely confident conclusions about\na metric's efficacy. Finally, we turn to pairwise system ranking, developing a\nmethod for thresholding performance improvement under an automatic metric\nagainst human judgements, which allows quantification of type I versus type II\nerrors incurred, i.e., insignificant human differences in system quality that\nare accepted, and significant human differences that are rejected. Together,\nthese findings suggest improvements to the protocols for metric evaluation and\nsystem performance evaluation in machine translation.", "published": "2020-06-11 09:12:53", "link": "http://arxiv.org/abs/2006.06264v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Provenance for Linguistic Corpora Through Nanopublications", "abstract": "Research in Computational Linguistics is dependent on text corpora for\ntraining and testing new tools and methodologies. While there exists a plethora\nof annotated linguistic information, these corpora are often not interoperable\nwithout significant manual work. Moreover, these annotations might have evolved\ninto different versions, making it challenging for researchers to know the\ndata's provenance. This paper addresses this issue with a case study on event\nannotated corpora and by creating a new, more interoperable representation of\nthis data in the form of nanopublications. We demonstrate how linguistic\nannotations from separate corpora can be reliably linked from the start, and\nthereby be accessed and queried as if they were a single dataset. We describe\nhow such nanopublications can be created and demonstrate how SPARQL queries can\nbe performed to extract interesting content from the new representations. The\nqueries show that information of multiple corpora can be retrieved more easily\nand effectively because the information of different corpora is represented in\na uniform data format.", "published": "2020-06-11 11:30:30", "link": "http://arxiv.org/abs/2006.06341v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot\n  Cross-Lingual NLP", "abstract": "Multi-lingual contextualized embeddings, such as multilingual-BERT (mBERT),\nhave shown success in a variety of zero-shot cross-lingual tasks. However,\nthese models are limited by having inconsistent contextualized representations\nof subwords across different languages. Existing work addresses this issue by\nbilingual projection and fine-tuning technique. We propose a data augmentation\nframework to generate multi-lingual code-switching data to fine-tune mBERT,\nwhich encourages model to align representations from source and multiple target\nlanguages once by mixing their context information. Compared with the existing\nwork, our method does not rely on bilingual sentences for training, and\nrequires only one training process for multiple target languages. Experimental\nresults on five tasks with 19 languages show that our method leads to\nsignificantly improved performances for all the tasks compared with mBERT.", "published": "2020-06-11 13:15:59", "link": "http://arxiv.org/abs/2006.06402v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-hop Reading Comprehension across Documents with Path-based Graph\n  Convolutional Network", "abstract": "Multi-hop reading comprehension across multiple documents attracts much\nattention recently. In this paper, we propose a novel approach to tackle this\nmulti-hop reading comprehension problem. Inspired by human reasoning\nprocessing, we construct a path-based reasoning graph from supporting\ndocuments. This graph can combine both the idea of the graph-based and\npath-based approaches, so it is better for multi-hop reasoning. Meanwhile, we\npropose Gated-RGCN to accumulate evidence on the path-based reasoning graph,\nwhich contains a new question-aware gating mechanism to regulate the usefulness\nof information propagating across documents and add question information during\nreasoning. We evaluate our approach on WikiHop dataset, and our approach\nachieves state-of-the-art accuracy against previously published approaches.\nEspecially, our ensemble model surpasses human performance by 4.2%.", "published": "2020-06-11 14:43:34", "link": "http://arxiv.org/abs/2006.06478v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emora STDM: A Versatile Framework for Innovative Dialogue System\n  Development", "abstract": "This demo paper presents Emora STDM (State Transition Dialogue Manager), a\ndialogue system development framework that provides novel workflows for rapid\nprototyping of chat-based dialogue managers as well as collaborative\ndevelopment of complex interactions. Our framework caters to a wide range of\nexpertise levels by supporting interoperability between two popular approaches,\nstate machine and information state, to dialogue management. Our Natural\nLanguage Expression package allows seamless integration of pattern matching,\ncustom NLP modules, and database querying, that makes the workflows much more\nefficient. As a user study, we adopt this framework to an interdisciplinary\nundergraduate course where students with both technical and non-technical\nbackgrounds are able to develop creative dialogue managers in a short period of\ntime.", "published": "2020-06-11 01:31:17", "link": "http://arxiv.org/abs/2006.06143v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transparency in Language Generation: Levels of Automation", "abstract": "Language models and conversational systems are growing increasingly advanced,\ncreating outputs that may be mistaken for humans. Consumers may thus be misled\nby advertising, media reports, or vagueness regarding the role of automation in\nthe production of language. We propose a taxonomy of language automation, based\non the SAE levels of driving automation, to establish a shared set of terms for\ndescribing automated language. It is our hope that the proposed taxonomy can\nincrease transparency in this rapidly advancing field.", "published": "2020-06-11 10:01:59", "link": "http://arxiv.org/abs/2006.06295v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "See what I'm saying? Comparing Intelligent Personal Assistant use for\n  Native and Non-Native Language Speakers", "abstract": "Limited linguistic coverage for Intelligent Personal Assistants (IPAs) means\nthat many interact in a non-native language. Yet we know little about how IPAs\ncurrently support or hinder these users. Through native (L1) and non-native\n(L2) English speakers interacting with Google Assistant on a smartphone and\nsmart speaker, we aim to understand this more deeply. Interviews revealed that\nL2 speakers prioritised utterance planning around perceived linguistic\nlimitations, as opposed to L1 speakers prioritising succinctness because of\nsystem limitations. L2 speakers see IPAs as insensitive to linguistic needs\nresulting in failed interaction. L2 speakers clearly preferred using\nsmartphones, as visual feedback supported diagnoses of communication breakdowns\nwhilst allowing time to process query results. Conversely, L1 speakers\npreferred smart speakers, with audio feedback being seen as sufficient. We\ndiscuss the need to tailor the IPA experience for L2 users, emphasising visual\nfeedback whilst reducing the burden of language production.", "published": "2020-06-11 11:03:49", "link": "http://arxiv.org/abs/2006.06328v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Mental Workload and Language Production in Non-Native Speaker IPA\n  Interaction", "abstract": "Through proliferation on smartphones and smart speakers, intelligent personal\nassistants (IPAs) have made speech a common interaction modality. Yet, due to\nlinguistic coverage and varying levels of functionality, many speakers engage\nwith IPAs using a non-native language. This may impact the mental workload and\npattern of language production displayed by non-native speakers. We present a\nmixed-design experiment, wherein native (L1) and non-native (L2) English\nspeakers completed tasks with IPAs through smartphones and smart speakers. We\nfound significantly higher mental workload for L2 speakers during IPA\ninteractions. Contrary to our hypotheses, we found no significant differences\nbetween L1 and L2 speakers in terms of number of turns, lexical complexity,\ndiversity, or lexical adaptation when encountering errors. These findings are\ndiscussed in relation to language production and processing load increases for\nL2 speakers in IPA interaction.", "published": "2020-06-11 11:06:42", "link": "http://arxiv.org/abs/2006.06331v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "A Probabilistic Model with Commonsense Constraints for Pattern-based\n  Temporal Fact Extraction", "abstract": "Textual patterns (e.g., Country's president Person) are specified and/or\ngenerated for extracting factual information from unstructured data.\nPattern-based information extraction methods have been recognized for their\nefficiency and transferability. However, not every pattern is reliable: A major\nchallenge is to derive the most complete and accurate facts from diverse and\nsometimes conflicting extractions. In this work, we propose a probabilistic\ngraphical model which formulates fact extraction in a generative process. It\nautomatically infers true facts and pattern reliability without any\nsupervision. It has two novel designs specially for temporal facts: (1) it\nmodels pattern reliability on two types of time signals, including temporal tag\nin text and text generation time; (2) it models commonsense constraints as\nobservable variables. Experimental results demonstrate that our model\nsignificantly outperforms existing methods on extracting true temporal facts\nfrom news data.", "published": "2020-06-11 13:48:04", "link": "http://arxiv.org/abs/2006.06436v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Learning advanced mathematical computations from examples", "abstract": "Using transformers over large generated datasets, we train models to learn\nmathematical properties of differential systems, such as local stability,\nbehavior at infinity and controllability. We achieve near perfect prediction of\nqualitative characteristics, and good approximations of numerical features of\nthe system. This demonstrates that neural networks can learn to perform complex\ncomputations, grounded in advanced theory, from examples, without built-in\nmathematical knowledge.", "published": "2020-06-11 14:18:35", "link": "http://arxiv.org/abs/2006.06462v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring Weaknesses of VQA Models through Attribution Driven Insights", "abstract": "Deep Neural Networks have been successfully used for the task of Visual\nQuestion Answering for the past few years owing to the availability of relevant\nlarge scale datasets. However these datasets are created in artificial settings\nand rarely reflect the real world scenario. Recent research effectively applies\nthese VQA models for answering visual questions for the blind. Despite\nachieving high accuracy these models appear to be susceptible to variation in\ninput questions.We analyze popular VQA models through the lens of attribution\n(input's influence on predictions) to gain valuable insights. Further, We use\nthese insights to craft adversarial attacks which inflict significant damage to\nthese systems with negligible change in meaning of the input questions. We\nbelieve this will enhance development of systems more robust to the possible\nvariations in inputs when deployed to assist the visually impaired.", "published": "2020-06-11 17:30:07", "link": "http://arxiv.org/abs/2006.06637v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "VirTex: Learning Visual Representations from Textual Annotations", "abstract": "The de-facto approach to many vision tasks is to start from pretrained visual\nrepresentations, typically learned via supervised training on ImageNet. Recent\nmethods have explored unsupervised pretraining to scale to vast quantities of\nunlabeled images. In contrast, we aim to learn high-quality visual\nrepresentations from fewer images. To this end, we revisit supervised\npretraining, and seek data-efficient alternatives to classification-based\npretraining. We propose VirTex -- a pretraining approach using semantically\ndense captions to learn visual representations. We train convolutional networks\nfrom scratch on COCO Captions, and transfer them to downstream recognition\ntasks including image classification, object detection, and instance\nsegmentation. On all tasks, VirTex yields features that match or exceed those\nlearned on ImageNet -- supervised or unsupervised -- despite using up to ten\ntimes fewer images.", "published": "2020-06-11 17:58:48", "link": "http://arxiv.org/abs/2006.06666v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "High-Precision Extraction of Emerging Concepts from Scientific\n  Literature", "abstract": "Identification of new concepts in scientific literature can help power\nfaceted search, scientific trend analysis, knowledge-base construction, and\nmore, but current methods are lacking. Manual identification cannot keep up\nwith the torrent of new publications, while the precision of existing automatic\ntechniques is too low for many applications. We present an unsupervised concept\nextraction method for scientific literature that achieves much higher precision\nthan previous work. Our approach relies on a simple but novel intuition: each\nscientific concept is likely to be introduced or popularized by a single paper\nthat is disproportionately cited by subsequent papers mentioning the concept.\nFrom a corpus of computer science papers on arXiv, we find that our method\nachieves a Precision@1000 of 99%, compared to 86% for prior work, and a\nsubstantially better precision-yield trade-off across the top 15,000\nextractions. To stimulate research in this area, we release our code and data\n(https://github.com/allenai/ForeCite).", "published": "2020-06-11 23:48:27", "link": "http://arxiv.org/abs/2006.06877v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Large-Scale Adversarial Training for Vision-and-Language Representation\n  Learning", "abstract": "We present VILLA, the first known effort on large-scale adversarial training\nfor vision-and-language (V+L) representation learning. VILLA consists of two\ntraining stages: (i) task-agnostic adversarial pre-training; followed by (ii)\ntask-specific adversarial finetuning. Instead of adding adversarial\nperturbations on image pixels and textual tokens, we propose to perform\nadversarial training in the embedding space of each modality. To enable\nlarge-scale training, we adopt the \"free\" adversarial training strategy, and\ncombine it with KL-divergence-based regularization to promote higher invariance\nin the embedding space. We apply VILLA to current best-performing V+L models,\nand achieve new state of the art on a wide range of tasks, including Visual\nQuestion Answering, Visual Commonsense Reasoning, Image-Text Retrieval,\nReferring Expression Comprehension, Visual Entailment, and NLVR2.", "published": "2020-06-11 05:14:35", "link": "http://arxiv.org/abs/2006.06195v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Performance in the Courtroom: Automated Processing and Visualization of\n  Appeal Court Decisions in France", "abstract": "Artificial Intelligence techniques are already popular and important in the\nlegal domain. We extract legal indicators from judicial judgment to decrease\nthe asymmetry of information of the legal system and the access-to-justice gap.\nWe use NLP methods to extract interesting entities/data from judgments to\nconstruct networks of lawyers and judgments. We propose metrics to rank lawyers\nbased on their experience, wins/loss ratio and their importance in the network\nof lawyers. We also perform community detection in the network of judgments and\npropose metrics to represent the difficulty of cases capitalising on\ncommunities features.", "published": "2020-06-11 08:22:59", "link": "http://arxiv.org/abs/2006.06251v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "XiaoiceSing: A High-Quality and Integrated Singing Voice Synthesis\n  System", "abstract": "This paper presents XiaoiceSing, a high-quality singing voice synthesis\nsystem which employs an integrated network for spectrum, F0 and duration\nmodeling. We follow the main architecture of FastSpeech while proposing some\nsinging-specific design: 1) Besides phoneme ID and position encoding, features\nfrom musical score (e.g.note pitch and length) are also added. 2) To attenuate\noff-key issues, we add a residual connection in F0 prediction. 3) In addition\nto the duration loss of each phoneme, the duration of all the phonemes in a\nmusical note is accumulated to calculate the syllable duration loss for rhythm\nenhancement. Experiment results show that XiaoiceSing outperforms the baseline\nsystem of convolutional neural networks by 1.44 MOS on sound quality, 1.18 on\npronunciation accuracy and 1.38 on naturalness respectively. In two A/B tests,\nthe proposed F0 and duration modeling methods achieve 97.3% and 84.3%\npreference rate over baseline respectively, which demonstrates the overwhelming\nadvantages of XiaoiceSing.", "published": "2020-06-11 09:09:59", "link": "http://arxiv.org/abs/2006.06261v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Extracting and categorising the reactions to COVID-19 by the South\n  African public -- A social media study", "abstract": "Social Media can be used to extract discussion topics during a disaster. With\nthe COVID-19 pandemic impact on South Africa, we need to understand how the law\nand regulation promulgated by the government in response to the pandemic\ncontrasts with discussion topics social media users have been engaging in. In\nthis work, we expand on traditional media analysis by using Social Media\ndiscussions driven by or directed to South African government officials. We\nfind topics that are similar as well as different in some cases. The findings\ncan inform further study into social media during disaster settings in South\nAfrica and beyond. This paper sets a framework for future analysis in\nunderstanding the opinions of the public during a pandemic and how these\nopinions can be distilled [in a semi-automated approach] to inform government\ncommunication in the future.", "published": "2020-06-11 11:19:43", "link": "http://arxiv.org/abs/2006.06336v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason\n  Over Implicit Knowledge", "abstract": "To what extent can a neural network systematically reason over symbolic\nfacts? Evidence suggests that large pre-trained language models (LMs) acquire\nsome reasoning capacity, but this ability is difficult to control. Recently, it\nhas been shown that Transformer-based models succeed in consistent reasoning\nover explicit symbolic facts, under a \"closed-world\" assumption. However, in an\nopen-domain setup, it is desirable to tap into the vast reservoir of implicit\nknowledge already encoded in the parameters of pre-trained LMs. In this work,\nwe provide a first demonstration that LMs can be trained to reliably perform\nsystematic reasoning combining both implicit, pre-trained knowledge and\nexplicit natural language statements. To do this, we describe a procedure for\nautomatically generating datasets that teach a model new reasoning skills, and\ndemonstrate that models learn to effectively perform inference which involves\nimplicit taxonomic and world knowledge, chaining and counting. Finally, we show\nthat \"teaching\" models to reason generalizes beyond the training distribution:\nthey successfully compose the usage of multiple reasoning skills in single\nexamples. Our work paves a path towards open-domain systems that constantly\nimprove by interacting with users who can instantly correct a model by adding\nsimple natural language statements.", "published": "2020-06-11 17:02:20", "link": "http://arxiv.org/abs/2006.06609v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Disentangled Non-Local Neural Networks", "abstract": "The non-local block is a popular module for strengthening the context\nmodeling ability of a regular convolutional neural network. This paper first\nstudies the non-local block in depth, where we find that its attention\ncomputation can be split into two terms, a whitened pairwise term accounting\nfor the relationship between two pixels and a unary term representing the\nsaliency of every pixel. We also observe that the two terms trained alone tend\nto model different visual clues, e.g. the whitened pairwise term learns\nwithin-region relationships while the unary term learns salient boundaries.\nHowever, the two terms are tightly coupled in the non-local block, which\nhinders the learning of each. Based on these findings, we present the\ndisentangled non-local block, where the two terms are decoupled to facilitate\nlearning for both terms. We demonstrate the effectiveness of the decoupled\ndesign on various tasks, such as semantic segmentation on Cityscapes, ADE20K\nand PASCAL Context, object detection on COCO, and action recognition on\nKinetics.", "published": "2020-06-11 17:59:22", "link": "http://arxiv.org/abs/2006.06668v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Modelling Hierarchical Structure between Dialogue Policy and Natural\n  Language Generator with Option Framework for Task-oriented Dialogue System", "abstract": "Designing task-oriented dialogue systems is a challenging research topic,\nsince it needs not only to generate utterances fulfilling user requests but\nalso to guarantee the comprehensibility. Many previous works trained end-to-end\n(E2E) models with supervised learning (SL), however, the bias in annotated\nsystem utterances remains as a bottleneck. Reinforcement learning (RL) deals\nwith the problem through using non-differentiable evaluation metrics (e.g., the\nsuccess rate) as rewards. Nonetheless, existing works with RL showed that the\ncomprehensibility of generated system utterances could be corrupted when\nimproving the performance on fulfilling user requests. In our work, we (1)\npropose modelling the hierarchical structure between dialogue policy and\nnatural language generator (NLG) with the option framework, called HDNO, where\nthe latent dialogue act is applied to avoid designing specific dialogue act\nrepresentations; (2) train HDNO via hierarchical reinforcement learning (HRL),\nas well as suggest the asynchronous updates between dialogue policy and NLG\nduring training to theoretically guarantee their convergence to a local\nmaximizer; and (3) propose using a discriminator modelled with language models\nas an additional reward to further improve the comprehensibility. We test HDNO\non MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in\ncomparison with word-level E2E model trained with RL, LaRL and HDSA, showing\nimprovements on the performance evaluated by automatic evaluation metrics and\nhuman evaluation. Finally, we demonstrate the semantic meanings of latent\ndialogue acts to show the explanability for HDNO.", "published": "2020-06-11 20:55:28", "link": "http://arxiv.org/abs/2006.06814v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FastPitch: Parallel Text-to-speech with Pitch Prediction", "abstract": "We present FastPitch, a fully-parallel text-to-speech model based on\nFastSpeech, conditioned on fundamental frequency contours. The model predicts\npitch contours during inference. By altering these predictions, the generated\nspeech can be more expressive, better match the semantic of the utterance, and\nin the end more engaging to the listener. Uniformly increasing or decreasing\npitch with FastPitch generates speech that resembles the voluntary modulation\nof voice. Conditioning on frequency contours improves the overall quality of\nsynthesized speech, making it comparable to state-of-the-art. It does not\nintroduce an overhead, and FastPitch retains the favorable, fully-parallel\nTransformer architecture, with over 900x real-time factor for mel-spectrogram\nsynthesis of a typical utterance.", "published": "2020-06-11 23:23:58", "link": "http://arxiv.org/abs/2006.06873v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ETHOS: an Online Hate Speech Detection Dataset", "abstract": "Online hate speech is a recent problem in our society that is rising at a\nsteady pace by leveraging the vulnerabilities of the corresponding regimes that\ncharacterise most social media platforms. This phenomenon is primarily fostered\nby offensive comments, either during user interaction or in the form of a\nposted multimedia context. Nowadays, giant corporations own platforms where\nmillions of users log in every day, and protection from exposure to similar\nphenomena appears to be necessary in order to comply with the corresponding\nlegislation and maintain a high level of service quality. A robust and reliable\nsystem for detecting and preventing the uploading of relevant content will have\na significant impact on our digitally interconnected society. Several aspects\nof our daily lives are undeniably linked to our social profiles, making us\nvulnerable to abusive behaviours. As a result, the lack of accurate hate speech\ndetection mechanisms would severely degrade the overall user experience,\nalthough its erroneous operation would pose many ethical concerns. In this\npaper, we present 'ETHOS', a textual dataset with two variants: binary and\nmulti-label, based on YouTube and Reddit comments validated using the\nFigure-Eight crowdsourcing platform. Furthermore, we present the annotation\nprotocol used to create this dataset: an active sampling procedure for\nbalancing our data in relation to the various aspects defined. Our key\nassumption is that, even gaining a small amount of labelled data from such a\ntime-consuming process, we can guarantee hate speech occurrences in the\nexamined material.", "published": "2020-06-11 08:59:57", "link": "http://arxiv.org/abs/2006.08328v2", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.6; I.2.7; I.5.4; H.2.4"], "primary_category": "cs.CL"}
{"title": "An Objective Measure of Quality for Time-Scale Modification of Audio", "abstract": "Objective evaluation of audio processed with Time-Scale Modification (TSM)\nremains an open problem. Recently, a dataset of time-scaled audio with\nsubjective quality labels was published and used to create an initial objective\nmeasure of quality. In this paper, an improved objective measure of quality for\ntime-scaled audio is proposed. The measure uses hand-crafted features and a\nfully connected network to predict subjective mean opinion scores. Basic and\nAdvanced Perceptual Evaluation of Audio Quality features are used in addition\nto nine features specific to TSM artefacts. Six methods of alignment are\nexplored, with interpolation of the reference magnitude spectrum to the length\nof the test magnitude spectrum giving the best performance. The proposed\nmeasure achieves a mean Root Mean Squared Error of 0.487 and a mean Pearson\ncorrelation of 0.865, equivalent to 98th and 82nd percentiles of subjective\nsessions respectively. The proposed measure is used to evaluate time-scale\nmodification algorithms, finding that Elastique gives the highest objective\nquality for Solo instrument and voice signals, while the Identity Phase-Locking\nPhase Vocoder gives the highest objective quality for music signals and the\nbest overall quality. The objective measure is available at\nhttps://www.github.com/zygurt/TSM.", "published": "2020-06-11 02:11:44", "link": "http://arxiv.org/abs/2006.06153v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dance Revolution: Long-Term Dance Generation with Music via Curriculum\n  Learning", "abstract": "Dancing to music is one of human's innate abilities since ancient times. In\nmachine learning research, however, synthesizing dance movements from music is\na challenging problem. Recently, researchers synthesize human motion sequences\nthrough autoregressive models like recurrent neural network (RNN). Such an\napproach often generates short sequences due to an accumulation of prediction\nerrors that are fed back into the neural network. This problem becomes even\nmore severe in the long motion sequence generation. Besides, the consistency\nbetween dance and music in terms of style, rhythm and beat is yet to be taken\ninto account during modeling. In this paper, we formalize the music-conditioned\ndance generation as a sequence-to-sequence learning problem and devise a novel\nseq2seq architecture to efficiently process long sequences of music features\nand capture the fine-grained correspondence between music and dance.\nFurthermore, we propose a novel curriculum learning strategy to alleviate error\naccumulation of autoregressive models in long motion sequence generation, which\ngently changes the training process from a fully guided teacher-forcing scheme\nusing the previous ground-truth movements, towards a less guided autoregressive\nscheme mostly using the generated movements instead. Extensive experiments show\nthat our approach significantly outperforms the existing state-of-the-arts on\nautomatic metrics and human evaluation. We also make a demo video to\ndemonstrate the superior performance of our proposed approach at\nhttps://www.youtube.com/watch?v=lmE20MEheZ8.", "published": "2020-06-11 00:08:25", "link": "http://arxiv.org/abs/2006.06119v8", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Telling Left from Right: Learning Spatial Correspondence of Sight and\n  Sound", "abstract": "Self-supervised audio-visual learning aims to capture useful representations\nof video by leveraging correspondences between visual and audio inputs.\nExisting approaches have focused primarily on matching semantic information\nbetween the sensory streams. We propose a novel self-supervised task to\nleverage an orthogonal principle: matching spatial information in the audio\nstream to the positions of sound sources in the visual stream. Our approach is\nsimple yet effective. We train a model to determine whether the left and right\naudio channels have been flipped, forcing it to reason about spatial\nlocalization across the visual and audio streams. To train and evaluate our\nmethod, we introduce a large-scale video dataset, YouTube-ASMR-300K, with\nspatial audio comprising over 900 hours of footage. We demonstrate that\nunderstanding spatial correspondence enables models to perform better on three\naudio-visual tasks, achieving quantitative gains over supervised and\nself-supervised baselines that do not leverage spatial audio cues. We also show\nhow to extend our self-supervised approach to 360 degree videos with ambisonic\naudio.", "published": "2020-06-11 04:00:24", "link": "http://arxiv.org/abs/2006.06175v2", "categories": ["cs.CV", "cs.SD", "eess.AS", "68T45", "I.4.0"], "primary_category": "cs.CV"}
{"title": "Investigating Robustness of Adversarial Samples Detection for Automatic\n  Speaker Verification", "abstract": "Recently adversarial attacks on automatic speaker verification (ASV) systems\nattracted widespread attention as they pose severe threats to ASV systems.\nHowever, methods to defend against such attacks are limited. Existing\napproaches mainly focus on retraining ASV systems with adversarial data\naugmentation. Also, countermeasure robustness against different attack settings\nare insufficiently investigated. Orthogonal to prior approaches, this work\nproposes to defend ASV systems against adversarial attacks with a separate\ndetection network, rather than augmenting adversarial data into ASV training. A\nVGG-like binary classification detector is introduced and demonstrated to be\neffective on detecting adversarial samples. To investigate detector robustness\nin a realistic defense scenario where unseen attack settings may exist, we\nanalyze various kinds of unseen attack settings' impact and observe that the\ndetector is robust (6.27\\% EER_{det} degradation in the worst case) against\nunseen substitute ASV systems, but it has weak robustness (50.37\\% EER_{det}\ndegradation in the worst case) against unseen perturbation methods. The weak\nrobustness against unseen perturbation methods shows a direction for developing\nstronger countermeasures.", "published": "2020-06-11 04:31:56", "link": "http://arxiv.org/abs/2006.06186v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Perceiving Music Quality with GANs", "abstract": "Several methods have been developed to assess the perceptual quality of audio\nunder transforms like lossy compression. However, they require paired reference\nsignals of the unaltered content, limiting their use in applications where\nreferences are unavailable. This has hindered progress in audio generation and\nstyle transfer, where a no-reference quality assessment method would allow more\nreproducible comparisons across methods. We propose training a GAN on a large\nmusic library, and using its discriminator as a no-reference quality assessment\nmeasure of the perceived quality of music. This method is unsupervised, needs\nno access to degraded material and can be tuned for various domains of music.\nIn a listening test with 448 human subjects, where participants rated\nprofessionally produced music tracks degraded with different levels and types\nof signal degradations such as waveshaping distortion and low-pass filtering,\nwe establish a dataset of human rated material. By using the human rated\ndataset we show that the discriminator score correlates significantly with the\nsubjective ratings, suggesting that the proposed method can be used to create a\nno-reference musical audio quality assessment measure.", "published": "2020-06-11 09:45:54", "link": "http://arxiv.org/abs/2006.06287v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Anti-Transfer Learning for Task Invariance in Convolutional Neural\n  Networks for Speech Processing", "abstract": "We introduce the novel concept of anti-transfer learning for speech\nprocessing with convolutional neural networks. While transfer learning assumes\nthat the learning process for a target task will benefit from re-using\nrepresentations learned for another task, anti-transfer avoids the learning of\nrepresentations that have been learned for an orthogonal task, i.e., one that\nis not relevant and potentially misleading for the target task, such as speaker\nidentity for speech recognition or speech content for emotion recognition. In\nanti-transfer learning, we penalize similarity between activations of a network\nbeing trained and another one previously trained on an orthogonal task, which\nyields more suitable representations. This leads to better generalization and\nprovides a degree of control over correlations that are spurious or\nundesirable, e.g. to avoid social bias. We have implemented anti-transfer for\nconvolutional neural networks in different configurations with several\nsimilarity metrics and aggregation functions, which we evaluate and analyze\nwith several speech and audio tasks and settings, using six datasets. We show\nthat anti-transfer actually leads to the intended invariance to the orthogonal\ntask and to more appropriate features for the target task at hand.\nAnti-transfer learning consistently improves classification accuracy in all\ntest cases. While anti-transfer creates computation and memory cost at training\ntime, there is relatively little computation cost when using pre-trained models\nfor orthogonal tasks. Anti-transfer is widely applicable and particularly\nuseful where a specific invariance is desirable or where trained models are\navailable and labeled data for orthogonal tasks are difficult to obtain.", "published": "2020-06-11 15:03:29", "link": "http://arxiv.org/abs/2006.06494v2", "categories": ["cs.LG", "cs.NE", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Let's Face It: Probabilistic Multi-modal Interlocutor-aware Generation\n  of Facial Gestures in Dyadic Settings", "abstract": "To enable more natural face-to-face interactions, conversational agents need\nto adapt their behavior to their interlocutors. One key aspect of this is\ngeneration of appropriate non-verbal behavior for the agent, for example facial\ngestures, here defined as facial expressions and head movements. Most existing\ngesture-generating systems do not utilize multi-modal cues from the\ninterlocutor when synthesizing non-verbal behavior. Those that do, typically\nuse deterministic methods that risk producing repetitive and non-vivid motions.\nIn this paper, we introduce a probabilistic method to synthesize\ninterlocutor-aware facial gestures - represented by highly expressive FLAME\nparameters - in dyadic conversations. Our contributions are: a) a method for\nfeature extraction from multi-party video and speech recordings, resulting in a\nrepresentation that allows for independent control and manipulation of\nexpression and speech articulation in a 3D avatar; b) an extension to MoGlow, a\nrecent motion-synthesis method based on normalizing flows, to also take\nmulti-modal signals from the interlocutor as input and subsequently output\ninterlocutor-aware facial gestures; and c) a subjective evaluation assessing\nthe use and relative importance of the input modalities. The results show that\nthe model successfully leverages the input from the interlocutor to generate\nmore appropriate behavior. Videos, data, and code available at:\nhttps://jonepatr.github.io/lets_face_it.", "published": "2020-06-11 14:11:51", "link": "http://arxiv.org/abs/2006.09888v2", "categories": ["cs.CV", "cs.HC", "cs.LG", "cs.SD", "eess.AS", "eess.IV", "stat.ML"], "primary_category": "cs.CV"}
