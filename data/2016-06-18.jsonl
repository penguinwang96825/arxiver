{"title": "Egyptian Arabic to English Statistical Machine Translation System for\n  NIST OpenMT'2015", "abstract": "The paper describes the Egyptian Arabic-to-English statistical machine\ntranslation (SMT) system that the QCRI-Columbia-NYUAD (QCN) group submitted to\nthe NIST OpenMT'2015 competition. The competition focused on informal dialectal\nArabic, as used in SMS, chat, and speech. Thus, our efforts focused on\nprocessing and standardizing Arabic, e.g., using tools such as 3arrib and\nMADAMIRA. We further trained a phrase-based SMT system using state-of-the-art\nfeatures and components such as operation sequence model, class-based language\nmodel, sparse features, neural network joint model, genre-based\nhierarchically-interpolated language model, unsupervised transliteration\nmining, phrase-table merging, and hypothesis combination. Our system ranked\nsecond on all three genres.", "published": "2016-06-18 14:34:07", "link": "http://arxiv.org/abs/1606.05759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalizing to Unseen Entities and Entity Pairs with Row-less Universal\n  Schema", "abstract": "Universal schema predicts the types of entities and relations in a knowledge\nbase (KB) by jointly embedding the union of all available schema types---not\nonly types from multiple structured databases (such as Freebase or Wikipedia\ninfoboxes), but also types expressed as textual patterns from raw text. This\nprediction is typically modeled as a matrix completion problem, with one type\nper column, and either one or two entities per row (in the case of entity types\nor binary relation types, respectively). Factorizing this sparsely observed\nmatrix yields a learned vector embedding for each row and each column. In this\npaper we explore the problem of making predictions for entities or entity-pairs\nunseen at training time (and hence without a pre-learned row embedding). We\npropose an approach having no per-row parameters at all; rather we produce a\nrow vector on the fly using a learned aggregation function of the vectors of\nthe observed columns for that row. We experiment with various aggregation\nfunctions, including neural network attention models. Our approach can be\nunderstood as a natural language database, in that questions about KB entities\nare answered by attending to textual or database evidence. In experiments\npredicting both relations and entity types, we demonstrate that despite having\nan order of magnitude fewer parameters than traditional universal schema, we\ncan match the accuracy of the traditional model, and more importantly, we can\nnow make predictions about unseen rows with nearly the same accuracy as rows\navailable at training time.", "published": "2016-06-18 20:38:42", "link": "http://arxiv.org/abs/1606.05804v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
