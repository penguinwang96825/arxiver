{"title": "Feature-based Decipherment for Large Vocabulary Machine Translation", "abstract": "Orthographic similarities across languages provide a strong signal for\nprobabilistic decipherment, especially for closely related language pairs. The\nexisting decipherment models, however, are not well-suited for exploiting these\northographic similarities. We propose a log-linear model with latent variables\nthat incorporates orthographic similarity features. Maximum likelihood training\nis computationally expensive for the proposed log-linear model. To address this\nchallenge, we perform approximate inference via MCMC sampling and contrastive\ndivergence. Our results show that the proposed log-linear model with\ncontrastive divergence scales to large vocabularies and outperforms the\nexisting generative decipherment models by exploiting the orthographic\nfeatures.", "published": "2015-08-10 07:02:49", "link": "http://arxiv.org/abs/1508.02142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improve the Evaluation of Fluency Using Entropy for Machine Translation\n  Evaluation Metrics", "abstract": "The widely-used automatic evaluation metrics cannot adequately reflect the\nfluency of the translations. The n-gram-based metrics, like BLEU, limit the\nmaximum length of matched fragments to n and cannot catch the matched fragments\nlonger than n, so they can only reflect the fluency indirectly. METEOR, which\nis not limited by n-gram, uses the number of matched chunks but it does not\nconsider the length of each chunk. In this paper, we propose an entropy-based\nmethod, which can sufficiently reflect the fluency of translations through the\ndistribution of matched words. This method can easily combine with the\nwidely-used automatic evaluation metrics to improve the evaluation of fluency.\nExperiments show that the correlations of BLEU and METEOR are improved on\nsentence level after combining with the entropy-based method on WMT 2010 and\nWMT 2012.", "published": "2015-08-10 12:46:52", "link": "http://arxiv.org/abs/1508.02225v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Phrase-based Machine Translation to Normalise Medical Terms in\n  Social Media Messages", "abstract": "Previous studies have shown that health reports in social media, such as\nDailyStrength and Twitter, have potential for monitoring health conditions\n(e.g. adverse drug reactions, infectious diseases) in particular communities.\nHowever, in order for a machine to understand and make inferences on these\nhealth conditions, the ability to recognise when laymen's terms refer to a\nparticular medical concept (i.e.\\ text normalisation) is required. To achieve\nthis, we propose to adapt an existing phrase-based machine translation (MT)\ntechnique and a vector representation of words to map between a social media\nphrase and a medical concept. We evaluate our proposed approach using a\ncollection of phrases from tweets related to adverse drug reactions. Our\nexperimental results show that the combination of a phrase-based MT technique\nand the similarity between word vector representations outperforms the\nbaselines that apply only either of them by up to 55%.", "published": "2015-08-10 15:29:22", "link": "http://arxiv.org/abs/1508.02285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Word Significance using Distributed Representations of Words", "abstract": "Distributed representations of words as real-valued vectors in a relatively\nlow-dimensional space aim at extracting syntactic and semantic features from\nlarge text corpora. A recently introduced neural network, named word2vec\n(Mikolov et al., 2013a; Mikolov et al., 2013b), was shown to encode semantic\ninformation in the direction of the word vectors. In this brief report, it is\nproposed to use the length of the vectors, together with the term frequency, as\nmeasure of word significance in a corpus. Experimental evidence using a\ndomain-specific corpus of abstracts is presented to support this proposal. A\nuseful visualization technique for text corpora emerges, where words are mapped\nonto a two-dimensional plane and automatically ranked by significance.", "published": "2015-08-10 15:52:49", "link": "http://arxiv.org/abs/1508.02297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Removing Biases from Trainable MT Metrics by Using Self-Training", "abstract": "Most trainable machine translation (MT) metrics train their weights on human\njudgments of state-of-the-art MT systems outputs. This makes trainable metrics\nbiases in many ways. One of them is preferring longer translations. These\nbiased metrics when used for tuning are evaluating different types of\ntranslations -- n-best lists of translations with very diverse quality. Systems\ntuned with these metrics tend to produce overly long translations that are\npreferred by the metric but not by humans. This is usually solved by manually\ntweaking metric's weights to equally value recall and precision. Our solution\nis more general: (1) it does not address only the recall bias but also all\nother biases that might be present in the data and (2) it does not require any\nknowledge of the types of features used which is useful in cases when manual\ntuning of metric's weights is not possible. This is accomplished by\nself-training on unlabeled n-best lists by using metric that was initially\ntrained on standard human judgments. One way of looking at this is as domain\nadaptation from the domain of state-of-the-art MT translations to diverse\nn-best list translations.", "published": "2015-08-10 22:24:36", "link": "http://arxiv.org/abs/1508.02445v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Structural Kernels for Natural Language Processing", "abstract": "Structural kernels are a flexible learning paradigm that has been widely used\nin Natural Language Processing. However, the problem of model selection in\nkernel-based methods is usually overlooked. Previous approaches mostly rely on\nsetting default values for kernel hyperparameters or using grid search, which\nis slow and coarse-grained. In contrast, Bayesian methods allow efficient model\nselection by maximizing the evidence on the training data through\ngradient-based methods. In this paper we show how to perform this in the\ncontext of structural kernels by using Gaussian Processes. Experimental results\non tree kernels show that this procedure results in better prediction\nperformance compared to hyperparameter optimization via grid search. The\nframework proposed in this paper can be adapted to other structures besides\ntrees, e.g., strings and graphs, thereby extending the utility of kernel-based\nmethods.", "published": "2015-08-10 05:57:14", "link": "http://arxiv.org/abs/1508.02131v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Approximation-Aware Dependency Parsing by Belief Propagation", "abstract": "We show how to train the fast dependency parser of Smith and Eisner (2008)\nfor improved accuracy. This parser can consider higher-order interactions among\nedges while retaining O(n^3) runtime. It outputs the parse with maximum\nexpected recall -- but for speed, this expectation is taken under a posterior\ndistribution that is constructed only approximately, using loopy belief\npropagation through structured factors. We show how to adjust the model\nparameters to compensate for the errors introduced by this approximation, by\nfollowing the gradient of the actual loss on training data. We find this\ngradient by back-propagation. That is, we treat the entire parser\n(approximations and all) as a differentiable circuit, as Stoyanov et al. (2011)\nand Domke (2010) did for loopy CRFs. The resulting trained parser obtains\nhigher accuracy with fewer iterations of belief propagation than one trained by\nconditional log-likelihood.", "published": "2015-08-10 19:48:33", "link": "http://arxiv.org/abs/1508.02375v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models\n  of Meaning", "abstract": "Deep compositional models of meaning acting on distributional representations\nof words in order to produce vectors of larger text constituents are evolving\nto a popular area of NLP research. We detail a compositional distributional\nframework based on a rich form of word embeddings that aims at facilitating the\ninteractions between words in the context of a sentence. Embeddings and\ncomposition layers are jointly learned against a generic objective that\nenhances the vectors with syntactic information from the surrounding context.\nFurthermore, each word is associated with a number of senses, the most\nplausible of which is selected dynamically during the composition process. We\nevaluate the produced vectors qualitatively and quantitatively with positive\nresults. At the sentence level, the effectiveness of the framework is\ndemonstrated on the MSRPar task, for which we report results within the\nstate-of-the-art range.", "published": "2015-08-10 19:04:18", "link": "http://arxiv.org/abs/1508.02354v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
