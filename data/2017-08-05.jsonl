{"title": "Automatic Question-Answering Using A Deep Similarity Neural Network", "abstract": "Automatic question-answering is a classical problem in natural language\nprocessing, which aims at designing systems that can automatically answer a\nquestion, in the same way as human does. In this work, we propose a deep\nlearning based model for automatic question-answering. First the questions and\nanswers are embedded using neural probabilistic modeling. Then a deep\nsimilarity neural network is trained to find the similarity score of a pair of\nanswer and question. Then for each question, the best answer is found as the\none with the highest similarity score. We first train this model on a\nlarge-scale public question-answering database, and then fine-tune it to\ntransfer to the customer-care chat data. We have also tested our framework on a\npublic question-answering database and achieved very good performance.", "published": "2017-08-05 05:50:44", "link": "http://arxiv.org/abs/1708.01713v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Referenceless Quality Estimation for Natural Language Generation", "abstract": "Traditional automatic evaluation measures for natural language generation\n(NLG) use costly human-authored references to estimate the quality of a system\noutput. In this paper, we propose a referenceless quality estimation (QE)\napproach based on recurrent neural networks, which predicts a quality score for\na NLG system output by comparing it to the source meaning representation only.\nOur method outperforms traditional metrics and a constant baseline in most\nrespects; we also show that synthetic data helps to increase correlation\nresults by 21% compared to the base system. Our results are comparable to\nresults obtained in similar QE tasks despite the more challenging setting.", "published": "2017-08-05 12:24:04", "link": "http://arxiv.org/abs/1708.01759v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Syllable-based Technique for Word Embeddings of Korean Words", "abstract": "Word embedding has become a fundamental component to many NLP tasks such as\nnamed entity recognition and machine translation. However, popular models that\nlearn such embeddings are unaware of the morphology of words, so it is not\ndirectly applicable to highly agglutinative languages such as Korean. We\npropose a syllable-based learning model for Korean using a convolutional neural\nnetwork, in which word representation is composed of trained syllable vectors.\nOur model successfully produces morphologically meaningful representation of\nKorean words compared to the original Skip-gram embeddings. The results also\nshow that it is quite robust to the Out-of-Vocabulary problem.", "published": "2017-08-05 13:21:01", "link": "http://arxiv.org/abs/1708.01766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extractive Multi Document Summarization using Dynamical Measurements of\n  Complex Networks", "abstract": "Due to the large amount of textual information available on Internet, it is\nof paramount relevance to use techniques that find relevant and concise\ncontent. A typical task devoted to the identification of informative sentences\nin documents is the so called extractive document summarization task. In this\npaper, we use complex network concepts to devise an extractive Multi Document\nSummarization (MDS) method, which extracts the most central sentences from\nseveral textual sources. In the proposed model, texts are represented as\nnetworks, where nodes represent sentences and the edges are established based\non the number of shared words. Differently from previous works, the\nidentification of relevant terms is guided by the characterization of nodes via\ndynamical measurements of complex networks, including symmetry, accessibility\nand absorption time. The evaluation of the proposed system revealed that\nexcellent results were obtained with particular dynamical measurements,\nincluding those based on the exploration of networks via random walks.", "published": "2017-08-05 13:32:58", "link": "http://arxiv.org/abs/1708.01769v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Word Predictions", "abstract": "In the encoder-decoder architecture for neural machine translation (NMT), the\nhidden states of the recurrent structures in the encoder and decoder carry the\ncrucial information about the sentence.These vectors are generated by\nparameters which are updated by back-propagation of translation errors through\ntime. We argue that propagating errors through the end-to-end recurrent\nstructures are not a direct way of control the hidden vectors. In this paper,\nwe propose to use word predictions as a mechanism for direct supervision. More\nspecifically, we require these vectors to be able to predict the vocabulary in\ntarget sentence. Our simple mechanism ensures better representations in the\nencoder and decoder without using any extra data or annotation. It is also\nhelpful in reducing the target side vocabulary and improving the decoding\nefficiency. Experiments on Chinese-English and German-English machine\ntranslation tasks show BLEU improvements by 4.53 and 1.3, respectively", "published": "2017-08-05 13:38:10", "link": "http://arxiv.org/abs/1708.01771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparison of Neural Models for Word Ordering", "abstract": "We compare several language models for the word-ordering task and propose a\nnew bag-to-sequence neural model based on attention-based sequence-to-sequence\nmodels. We evaluate the model on a large German WMT data set where it\nsignificantly outperforms existing models. We also describe a novel search\nstrategy for LM-based word ordering and report results on the English Penn\nTreebank. Our best model setup outperforms prior work both in terms of speed\nand quality.", "published": "2017-08-05 19:06:00", "link": "http://arxiv.org/abs/1708.01809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Automatic Terminology Ontology Learning Based on Topic Modeling", "abstract": "Ontologies provide features like a common vocabulary, reusability,\nmachine-readable content, and also allows for semantic search, facilitate agent\ninteraction and ordering & structuring of knowledge for the Semantic Web (Web\n3.0) application. However, the challenge in ontology engineering is automatic\nlearning, i.e., the there is still a lack of fully automatic approach from a\ntext corpus or dataset of various topics to form ontology using machine\nlearning techniques. In this paper, two topic modeling algorithms are explored,\nnamely LSI & SVD and Mr.LDA for learning topic ontology. The objective is to\ndetermine the statistical relationship between document and terms to build a\ntopic ontology and ontology graph with minimum human intervention. Experimental\nanalysis on building a topic ontology and semantic retrieving corresponding\ntopic ontology for the user's query demonstrating the effectiveness of the\nproposed approach.", "published": "2017-08-05 08:30:48", "link": "http://arxiv.org/abs/1709.01991v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "e-QRAQ: A Multi-turn Reasoning Dataset and Simulator with Explanations", "abstract": "In this paper we present a new dataset and user simulator e-QRAQ (explainable\nQuery, Reason, and Answer Question) which tests an Agent's ability to read an\nambiguous text; ask questions until it can answer a challenge question; and\nexplain the reasoning behind its questions and answer. The User simulator\nprovides the Agent with a short, ambiguous story and a challenge question about\nthe story. The story is ambiguous because some of the entities have been\nreplaced by variables. At each turn the Agent may ask for the value of a\nvariable or try to answer the challenge question. In response the User\nsimulator provides a natural language explanation of why the Agent's query or\nanswer was useful in narrowing down the set of possible answers, or not. To\ndemonstrate one potential application of the e-QRAQ dataset, we train a new\nneural architecture based on End-to-End Memory Networks to successfully\ngenerate both predictions and partial explanations of its current understanding\nof the problem. We observe a strong correlation between the quality of the\nprediction and explanation.", "published": "2017-08-05 15:06:56", "link": "http://arxiv.org/abs/1708.01776v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
