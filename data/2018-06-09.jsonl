{"title": "Word Familiarity and Frequency", "abstract": "Word frequency is assumed to correlate with word familiarity, but the\nstrength of this correlation has not been thoroughly investigated. In this\npaper, we report on our analysis of the correlation between a word familiarity\nrating list obtained through a psycholinguistic experiment and the\nlog-frequency obtained from various corpora of different kinds and sizes (up to\nthe terabyte scale) for English and Japanese. Major findings are threefold:\nFirst, for a given corpus, familiarity is necessary for a word to achieve high\nfrequency, but familiar words are not necessarily frequent. Second, correlation\nincreases with the corpus data size. Third, a corpus of spoken language\ncorrelates better than one of written language. These findings suggest that\ncognitive familiarity ratings are correlated to frequency, but more highly to\nthat of spoken rather than written language.", "published": "2018-06-09 07:40:38", "link": "http://arxiv.org/abs/1806.03431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Lexical Features for Improved Neural Network Named-Entity\n  Recognition", "abstract": "Neural network approaches to Named-Entity Recognition reduce the need for\ncarefully hand-crafted features. While some features do remain in\nstate-of-the-art systems, lexical features have been mostly discarded, with the\nexception of gazetteers. In this work, we show that this is unfair: lexical\nfeatures are actually quite useful. We propose to embed words and entity types\ninto a low-dimensional vector space we train from annotated data produced by\ndistant supervision thanks to Wikipedia. From this, we compute - offline - a\nfeature vector representing each word. When used with a vanilla recurrent\nneural network model, this representation yields substantial improvements. We\nestablish a new state-of-the-art F1 score of 87.95 on ONTONOTES 5.0, while\nmatching state-of-the-art performance with a F1 score of 91.73 on the\nover-studied CONLL-2003 dataset.", "published": "2018-06-09 15:40:09", "link": "http://arxiv.org/abs/1806.03489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diachronic word embeddings and semantic shifts: a survey", "abstract": "Recent years have witnessed a surge of publications aimed at tracing temporal\nchanges in lexical semantics using distributional methods, particularly\nprediction-based word embedding models. However, this vein of research lacks\nthe cohesion, common terminology and shared practices of more established areas\nof natural language processing. In this paper, we survey the current state of\nacademic research related to diachronic word embeddings and semantic shifts\ndetection. We start with discussing the notion of semantic shifts, and then\ncontinue with an overview of the existing methods for tracing such time-related\nshifts with word embedding models. We propose several axes along which these\nmethods can be compared, and outline the main challenges before this emerging\nsubfield of NLP, as well as prospects and possible applications.", "published": "2018-06-09 20:23:27", "link": "http://arxiv.org/abs/1806.03537v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Second Language Acquisition Modeling: An Ensemble Approach", "abstract": "Accurate prediction of students knowledge is a fundamental building block of\npersonalized learning systems. Here, we propose a novel ensemble model to\npredict student knowledge gaps. Applying our approach to student trace data\nfrom the online educational platform Duolingo we achieved highest score on both\nevaluation metrics for all three datasets in the 2018 Shared Task on Second\nLanguage Acquisition Modeling. We describe our model and discuss relevance of\nthe task compared to how it would be setup in a production environment for\npersonalized education.", "published": "2018-06-09 18:09:37", "link": "http://arxiv.org/abs/1806.04525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Search in Long Documents Using Document Structure", "abstract": "Reading comprehension models are based on recurrent neural networks that\nsequentially process the document tokens. As interest turns to answering more\ncomplex questions over longer documents, sequential reading of large portions\nof text becomes a substantial bottleneck. Inspired by how humans use document\nstructure, we propose a novel framework for reading comprehension. We represent\ndocuments as trees, and model an agent that learns to interleave quick\nnavigation through the document tree with more expensive answer extraction. To\nencourage exploration of the document tree, we propose a new algorithm, based\non Deep Q-Network (DQN), which strategically samples tree nodes at training\ntime. Empirically we find our algorithm improves question answering performance\ncompared to DQN and a strong information-retrieval (IR) baseline, and that\nensembling our model with the IR baseline results in further gains in\nperformance.", "published": "2018-06-09 18:55:00", "link": "http://arxiv.org/abs/1806.03529v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data\n  for Future Prediction", "abstract": "Future predictions on sequence data (e.g., videos or audios) require the\nalgorithms to capture non-Markovian and compositional properties of high-level\nsemantics. Context-free grammars are natural choices to capture such\nproperties, but traditional grammar parsers (e.g., Earley parser) only take\nsymbolic sentences as inputs. In this paper, we generalize the Earley parser to\nparse sequence data which is neither segmented nor labeled. This generalized\nEarley parser integrates a grammar parser with a classifier to find the optimal\nsegmentation and labels, and makes top-down future predictions. Experiments\nshow that our method significantly outperforms other approaches for future\nhuman activity prediction.", "published": "2018-06-09 16:07:02", "link": "http://arxiv.org/abs/1806.03497v1", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Angular Softmax Loss for End-to-end Speaker Verification", "abstract": "End-to-end speaker verification systems have received increasing interests.\nThe traditional i-vector approach trains a generative model (basically a\nfactor-analysis model) to extract i-vectors as speaker embeddings. In contrast,\nthe end-to-end approach directly trains a discriminative model (often a neural\nnetwork) to learn discriminative speaker embeddings; a crucial component is the\ntraining criterion. In this paper, we use angular softmax (A-softmax), which is\noriginally proposed for face verification, as the loss function for feature\nlearning in end-to-end speaker verification. By introducing margins between\nclasses into softmax loss, A-softmax can learn more discriminative features\nthan softmax loss and triplet loss, and at the same time, is easy and stable\nfor usage. We make two contributions in this work. 1) We introduce A-softmax\nloss into end-to-end speaker verification and achieve significant EER\nreductions. 2) We find that the combination of using A-softmax in training the\nfront-end and using PLDA in the back-end scoring further boosts the performance\nof end-to-end systems under short utterance condition (short in both enrollment\nand test). Experiments are conducted on part of $Fisher$ dataset and\ndemonstrate the improvements of using A-softmax.", "published": "2018-06-09 11:48:03", "link": "http://arxiv.org/abs/1806.03464v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
