{"title": "Sifting Robotic from Organic Text: A Natural Language Approach for\n  Detecting Automation on Twitter", "abstract": "Twitter, a popular social media outlet, has evolved into a vast source of\nlinguistic data, rich with opinion, sentiment, and discussion. Due to the\nincreasing popularity of Twitter, its perceived potential for exerting social\ninfluence has led to the rise of a diverse community of automatons, commonly\nreferred to as bots. These inorganic and semi-organic Twitter entities can\nrange from the benevolent (e.g., weather-update bots, help-wanted-alert bots)\nto the malevolent (e.g., spamming messages, advertisements, or radical\nopinions). Existing detection algorithms typically leverage meta-data (time\nbetween tweets, number of followers, etc.) to identify robotic accounts. Here,\nwe present a powerful classification scheme that exclusively uses the natural\nlanguage text from organic users to provide a criterion for identifying\naccounts posting automated messages. Since the classifier operates on text\nalone, it is flexible and may be applied to any textual data beyond the\nTwitter-sphere.", "published": "2015-05-17 01:22:00", "link": "http://arxiv.org/abs/1505.04342v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CCG Parsing and Multiword Expressions", "abstract": "This thesis presents a study about the integration of information about\nMultiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar\n(CCG). We build on previous work which has shown the benefit of adding\ninformation about MWEs to syntactic parsing by implementing a similar pipeline\nwith CCG parsing. More specifically, we collapse MWEs to one token in training\nand test data in CCGbank, a corpus which contains sentences annotated with CCG\nderivations. Our collapsing algorithm however can only deal with MWEs when they\nform a constituent in the data which is one of the limitations of our approach.\n  We study the effect of collapsing training and test data. A parsing effect\ncan be obtained if collapsed data help the parser in its decisions and a\ntraining effect can be obtained if training on the collapsed data improves\nresults. We also collapse the gold standard and show that our model\nsignificantly outperforms the baseline model on our gold standard, which\nindicates that there is a training effect. We show that the baseline model\nperforms significantly better on our gold standard when the data are collapsed\nbefore parsing than when the data are collapsed after parsing which indicates\nthat there is a parsing effect. We show that these results can lead to improved\nperformance on the non-collapsed standard benchmark although we fail to show\nthat it does so significantly. We conclude that despite the limited settings,\nthere are noticeable improvements from using MWEs in parsing. We discuss ways\nin which the incorporation of MWEs into parsing can be improved and hypothesize\nthat this will lead to more substantial results.\n  We finally show that turning the MWE recognition part of the pipeline into an\nexperimental part is a useful thing to do as we obtain different results with\ndifferent recognizers.", "published": "2015-05-17 17:26:36", "link": "http://arxiv.org/abs/1505.04420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
