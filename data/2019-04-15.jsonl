{"title": "Pun Generation with Surprise", "abstract": "We tackle the problem of generating a pun sentence given a pair of homophones\n(e.g., \"died\" and \"dyed\"). Supervised text generation is inappropriate due to\nthe lack of a large corpus of puns, and even if such a corpus existed, mimicry\nis at odds with generating novel content. In this paper, we propose an\nunsupervised approach to pun generation using a corpus of unhumorous text and\nwhat we call the local-global surprisal principle: we posit that in a pun\nsentence, there is a strong association between the pun word (e.g., \"dyed\") and\nthe distant context, as well as a strong association between the alternative\nword (e.g., \"died\") and the immediate context. This contrast creates surprise\nand thus humor. We instantiate this principle for pun generation in two ways:\n(i) as a measure based on the ratio of probabilities under a language model,\nand (ii) a retrieve-and-edit approach based on words suggested by a skip-gram\nmodel. Human evaluation shows that our retrieve-and-edit approach generates\npuns successfully 31% of the time, tripling the success rate of a neural\ngeneration baseline.", "published": "2019-04-15 03:40:16", "link": "http://arxiv.org/abs/1904.06828v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Human Text Comprehension through Semi-Markov CRF-based Neural\n  Section Title Generation", "abstract": "Titles of short sections within long documents support readers by guiding\ntheir focus towards relevant passages and by providing anchor-points that help\nto understand the progression of the document. The positive effects of section\ntitles are even more pronounced when measured on readers with less developed\nreading abilities, for example in communities with limited labeled text\nresources.\n  We, therefore, aim to develop techniques to generate section titles in\nlow-resource environments. In particular, we present an extractive pipeline for\nsection title generation by first selecting the most salient sentence and then\napplying deletion-based compression. Our compression approach is based on a\nSemi-Markov Conditional Random Field that leverages unsupervised\nword-representations such as ELMo or BERT, eliminating the need for a complex\nencoder-decoder architecture. The results show that this approach leads to\ncompetitive performance with sequence-to-sequence models with high resources,\nwhile strongly outperforming it with low resources. In a human-subject study\nacross subjects with varying reading abilities, we find that our section titles\nimprove the speed of completing comprehension tasks while retaining similar\naccuracy.", "published": "2019-04-15 15:51:15", "link": "http://arxiv.org/abs/1904.07142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-Passing Models for Robust and Data-Efficient End-to-End Speech\n  Translation", "abstract": "Speech translation has traditionally been approached through cascaded models\nconsisting of a speech recognizer trained on a corpus of transcribed speech,\nand a machine translation system trained on parallel texts. Several recent\nworks have shown the feasibility of collapsing the cascade into a single,\ndirect model that can be trained in an end-to-end fashion on a corpus of\ntranslated speech. However, experiments are inconclusive on whether the cascade\nor the direct model is stronger, and have only been conducted under the\nunrealistic assumption that both are trained on equal amounts of data, ignoring\nother available speech recognition and machine translation corpora.\n  In this paper, we demonstrate that direct speech translation models require\nmore data to perform well than cascaded models, and while they allow including\nauxiliary data through multi-task training, they are poor at exploiting such\ndata, putting them at a severe disadvantage. As a remedy, we propose the use of\nend-to-end trainable models with two attention mechanisms, the first\nestablishing source speech to source text alignments, the second modeling\nsource to target text alignment. We show that such models naturally decompose\ninto multi-task-trainable recognition and translation tasks and propose an\nattention-passing technique that alleviates error propagation issues in a\nprevious formulation of a model with two attention stages. Our proposed model\noutperforms all examined baselines and is able to exploit auxiliary training\ndata much more effectively than direct attentional models.", "published": "2019-04-15 17:33:38", "link": "http://arxiv.org/abs/1904.07209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Head Multi-Layer Attention to Deep Language Representations for\n  Grammatical Error Detection", "abstract": "It is known that a deep neural network model pre-trained with large-scale\ndata greatly improves the accuracy of various tasks, especially when there are\nresource constraints. However, the information needed to solve a given task can\nvary, and simply using the output of the final layer is not necessarily\nsufficient. Moreover, to our knowledge, exploiting large language\nrepresentation models to detect grammatical errors has not yet been studied. In\nthis work, we investigate the effect of utilizing information not only from the\nfinal layer but also from intermediate layers of a pre-trained language\nrepresentation model to detect grammatical errors. We propose a multi-head\nmulti-layer attention model that determines the appropriate layers in\nBidirectional Encoder Representation from Transformers (BERT). The proposed\nmethod achieved the best scores on three datasets for grammatical error\ndetection tasks, outperforming the current state-of-the-art method by 6.0\npoints on FCE, 8.2 points on CoNLL14, and 12.2 points on JFLEG in terms of\nF_0.5. We also demonstrate that by using multi-head multi-layer attention, our\nmodel can exploit a broader range of information for each token in a sentence\nthan a model that uses only the final layer's information.", "published": "2019-04-15 21:36:21", "link": "http://arxiv.org/abs/1904.07334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A framework for streamlined statistical prediction using topic models", "abstract": "In the Humanities and Social Sciences, there is increasing interest in\napproaches to information extraction, prediction, intelligent linkage, and\ndimension reduction applicable to large text corpora. With approaches in these\nfields being grounded in traditional statistical techniques, the need arises\nfor frameworks whereby advanced NLP techniques such as topic modelling may be\nincorporated within classical methodologies. This paper provides a classical,\nsupervised, statistical learning framework for prediction from text, using\ntopic models as a data reduction method and the topics themselves as\npredictors, alongside typical statistical tools for predictive modelling. We\napply this framework in a Social Sciences context (applied animal behaviour) as\nwell as a Humanities context (narrative analysis) as examples of this\nframework. The results show that topic regression models perform comparably to\ntheir much less efficient equivalents that use individual words as predictors.", "published": "2019-04-15 10:06:47", "link": "http://arxiv.org/abs/1904.06941v1", "categories": ["stat.AP", "cs.CL"], "primary_category": "stat.AP"}
{"title": "CEDR: Contextualized Embeddings for Document Ranking", "abstract": "Although considerable attention has been given to neural ranking\narchitectures recently, far less attention has been paid to the term\nrepresentations that are used as input to these models. In this work, we\ninvestigate how two pretrained contextualized language models (ELMo and BERT)\ncan be utilized for ad-hoc document ranking. Through experiments on TREC\nbenchmarks, we find that several existing neural ranking architectures can\nbenefit from the additional context provided by contextualized language models.\nFurthermore, we propose a joint approach that incorporates BERT's\nclassification vector into existing neural models and show that it outperforms\nstate-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR\n(Contextualized Embeddings for Document Ranking). We also address practical\nchallenges in using these models for ranking, including the maximum input\nlength imposed by BERT and runtime performance impacts of contextualized\nlanguage models.", "published": "2019-04-15 14:55:59", "link": "http://arxiv.org/abs/1904.07094v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Latent Code and Text-based Generative Adversarial Networks for Soft-text\n  Generation", "abstract": "Text generation with generative adversarial networks (GANs) can be divided\ninto the text-based and code-based categories according to the type of signals\nused for discrimination. In this work, we introduce a novel text-based approach\ncalled Soft-GAN to effectively exploit GAN setup for text generation. We\ndemonstrate how autoencoders (AEs) can be used for providing a continuous\nrepresentation of sentences, which we will refer to as soft-text. This soft\nrepresentation will be used in GAN discrimination to synthesize similar\nsoft-texts. We also propose hybrid latent code and text-based GAN (LATEXT-GAN)\napproaches with one or more discriminators, in which a combination of the\nlatent code and the soft-text is used for GAN discriminations. We perform a\nnumber of subjective and objective experiments on two well-known datasets (SNLI\nand Image COCO) to validate our techniques. We discuss the results using\nseveral evaluation metrics and show that the proposed techniques outperform the\ntraditional GAN-based text-generation methods.", "published": "2019-04-15 19:07:49", "link": "http://arxiv.org/abs/1904.07293v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language Semantics With Pictures: Some Language & Vision\n  Datasets and Potential Uses for Computational Semantics", "abstract": "Propelling, and propelled by, the \"deep learning revolution\", recent years\nhave seen the introduction of ever larger corpora of images annotated with\nnatural language expressions. We survey some of these corpora, taking a\nperspective that reverses the usual directionality, as it were, by viewing the\nimages as semantic annotation of the natural language expressions. We discuss\ndatasets that can be derived from the corpora, and tasks of potential interest\nfor computational semanticists that can be defined on those. In this, we make\nuse of relations provided by the corpora (namely, the link between expression\nand image, and that between two expressions linked to the same image) and\nrelations that we can add (similarity relations between expressions, or between\nimages). Specifically, we show that in this way we can create data that can be\nused to learn and evaluate lexical and compositional grounded semantics, and we\nshow that the \"linked to same image\" relation tracks a semantic implication\nrelation that is recognisable to annotators even in the absence of the linking\nimage as evidence. Finally, as an example of possible benefits of this\napproach, we show that an exemplar-model-based approach to implication beats a\n(simple) distributional space-based one on some derived datasets, while lending\nitself to explainability.", "published": "2019-04-15 20:15:46", "link": "http://arxiv.org/abs/1904.07318v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Learning Twitter User Sentiments on Climate Change with Limited Labeled\n  Data", "abstract": "While it is well-documented that climate change accepters and deniers have\nbecome increasingly polarized in the United States over time, there has been no\nlarge-scale examination of whether these individuals are prone to changing\ntheir opinions as a result of natural external occurrences. On the\nsub-population of Twitter users, we examine whether climate change sentiment\nchanges in response to five separate natural disasters occurring in the U.S. in\n2018. We begin by showing that relevant tweets can be classified with over 75%\naccuracy as either accepting or denying climate change when using our\nmethodology to compensate for limited labeled data; results are robust across\nseveral machine learning models and yield geographic-level results in line with\nprior research. We then apply RNNs to conduct a cohort-level analysis showing\nthat the 2018 hurricanes yielded a statistically significant increase in\naverage tweet sentiment affirming climate change. However, this effect does not\nhold for the 2018 blizzard and wildfires studied, implying that Twitter users'\nopinions on climate change are fairly ingrained on this subset of natural\ndisasters.", "published": "2019-04-15 21:51:21", "link": "http://arxiv.org/abs/1904.07342v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterization of citizens using word2vec and latent topic analysis in\n  a large set of tweets", "abstract": "With the increasing use of the Internet and mobile devices, social networks\nare becoming the most used media to communicate citizens' ideas and thoughts.\nThis information is very useful to identify communities with common ideas based\non what they publish in the network. This paper presents a method to\nautomatically detect city communities based on machine learning techniques\napplied to a set of tweets from Bogot\\'a's citizens. An analysis was performed\nin a collection of 2,634,176 tweets gathered from Twitter in a period of six\nmonths. Results show that the proposed method is an interesting tool to\ncharacterize a city population based on a machine learning methods and text\nanalytics.", "published": "2019-04-15 13:25:38", "link": "http://arxiv.org/abs/1904.08926v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "An Empirical Investigation of Global and Local Normalization for\n  Recurrent Neural Sequence Models Using a Continuous Relaxation to Beam Search", "abstract": "Globally normalized neural sequence models are considered superior to their\nlocally normalized equivalents because they may ameliorate the effects of label\nbias. However, when considering high-capacity neural parametrizations that\ncondition on the whole input sequence, both model classes are theoretically\nequivalent in terms of the distributions they are capable of representing.\nThus, the practical advantage of global normalization in the context of modern\nneural methods remains unclear. In this paper, we attempt to shed light on this\nproblem through an empirical study. We extend an approach for search-aware\ntraining via a continuous relaxation of beam search (Goyal et al., 2017b) in\norder to enable training of globally normalized recurrent sequence models\nthrough simple backpropagation. We then use this technique to conduct an\nempirical study of the interaction between global normalization, high-capacity\nencoders, and search-aware optimization. We observe that in the context of\ninexact search, globally normalized neural models are still more effective than\ntheir locally normalized counterparts. Further, since our training approach is\nsensitive to warm-starting with pre-trained models, we also propose a novel\ninitialization strategy based on self-normalization for pre-training globally\nnormalized models. We perform analysis of our approach on two tasks: CCG\nsupertagging and Machine Translation, and demonstrate the importance of global\nnormalization under different conditions while using search-aware training.", "published": "2019-04-15 04:17:13", "link": "http://arxiv.org/abs/1904.06834v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Self-critical n-step Training for Image Captioning", "abstract": "Existing methods for image captioning are usually trained by cross entropy\nloss, which leads to exposure bias and the inconsistency between the optimizing\nfunction and evaluation metrics. Recently it has been shown that these two\nissues can be addressed by incorporating techniques from reinforcement\nlearning, where one of the popular techniques is the advantage actor-critic\nalgorithm that calculates per-token advantage by estimating state value with a\nparametrized estimator at the cost of introducing estimation bias. In this\npaper, we estimate state value without using a parametrized value estimator.\nWith the properties of image captioning, namely, the deterministic state\ntransition function and the sparse reward, state value is equivalent to its\npreceding state-action value, and we reformulate advantage function by simply\nreplacing the former with the latter. Moreover, the reformulated advantage is\nextended to n-step, which can generally increase the absolute value of the mean\nof reformulated advantage while lowering variance. Then two kinds of rollout\nare adopted to estimate state-action value, which we call self-critical n-step\ntraining. Empirically we find that our method can obtain better performance\ncompared to the state-of-the-art methods that use the sequence level advantage\nand parametrized estimator respectively on the widely used MSCOCO benchmark.", "published": "2019-04-15 05:47:23", "link": "http://arxiv.org/abs/1904.06861v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Semantic query-by-example speech search using visual grounding", "abstract": "A number of recent studies have started to investigate how speech systems can\nbe trained on untranscribed speech by leveraging accompanying images at\ntraining time. Examples of tasks include keyword prediction and within- and\nacross-mode retrieval. Here we consider how such models can be used for\nquery-by-example (QbE) search, the task of retrieving utterances relevant to a\ngiven spoken query. We are particularly interested in semantic QbE, where the\ntask is not only to retrieve utterances containing exact instances of the\nquery, but also utterances whose meaning is relevant to the query. We follow a\nsegmental QbE approach where variable-duration speech segments (queries, search\nutterances) are mapped to fixed-dimensional embedding vectors. We show that a\nQbE system using an embedding function trained on visually grounded speech data\noutperforms a purely acoustic QbE system in terms of both exact and semantic\nretrieval performance.", "published": "2019-04-15 14:35:40", "link": "http://arxiv.org/abs/1904.07078v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Tracing Forum Posts to MOOC Content using Topic Analysis", "abstract": "Massive Open Online Courses are educational programs that are open and\naccessible to a large number of people through the internet. To facilitate\nlearning, MOOC discussion forums exist where students and instructors\ncommunicate questions, answers, and thoughts related to the course.\n  The primary objective of this paper is to investigate tracing discussion\nforum posts back to course lecture videos and readings using topic analysis. We\nutilize both unsupervised and supervised variants of Latent Dirichlet\nAllocation (LDA) to extract topics from course material and classify forum\nposts. We validate our approach on posts bootstrapped from five Coursera\ncourses and determine that topic models can be used to map student discussion\nposts back to the underlying course lecture or reading. Labeled LDA outperforms\nunsupervised Hierarchical Dirichlet Process LDA and base LDA for our\ntraceability task. This research is useful as it provides an automated approach\nfor clustering student discussions by course material, enabling instructors to\nquickly evaluate student misunderstanding of content and clarify materials\naccordingly.", "published": "2019-04-15 19:49:06", "link": "http://arxiv.org/abs/1904.07307v1", "categories": ["cs.IR", "cs.CL", "cs.CY"], "primary_category": "cs.IR"}
{"title": "Something's Brewing! Early Prediction of Controversy-causing Posts from\n  Discussion Features", "abstract": "Controversial posts are those that split the preferences of a community,\nreceiving both significant positive and significant negative feedback. Our\ninclusion of the word \"community\" here is deliberate: what is controversial to\nsome audiences may not be so to others. Using data from several different\ncommunities on reddit.com, we predict the ultimate controversiality of posts,\nleveraging features drawn from both the textual content and the tree structure\nof the early comments that initiate the discussion. We find that even when only\na handful of comments are available, e.g., the first 5 comments made within 15\nminutes of the original post, discussion features often add predictive capacity\nto strong content-and-rate only baselines. Additional experiments on domain\ntransfer suggest that conversation-structure features often generalize to other\ncommunities better than conversation-content features do.", "published": "2019-04-15 23:56:25", "link": "http://arxiv.org/abs/1904.07372v1", "categories": ["cs.CL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Proximal binaural sound can induce subjective frisson", "abstract": "Auditory frisson is the experience of feeling of cold or shivering related to\nsound in the absence of a physical cold stimulus. Multiple examples of\nfrisson-inducing sounds have been reported, but the mechanism of auditory\nfrisson remains elusive. Typical frisson-inducing sounds may contain a looming\neffect, in which a sound appears to approach the listener's peripersonal space.\nPrevious studies on sound in peripersonal space have provided objective\nmeasurements of sound-inducing effects, but few have investigated the\nsubjective experience of frisson-inducing sounds. Here we explored whether it\nis possible to produce subjective feelings of frisson by moving a noise sound\n(white noise, rolling beads noise, or frictional noise produced by rubbing a\nplastic bag) stimulus around a listener's head. Our results demonstrated that\nsound-induced frisson can be experienced stronger when auditory stimuli are\nrotated around the head (binaural moving sounds) than the one without the\nrotation (monaural static sounds), regardless of the source of the noise sound.\nPearson's correlation analysis showed that several acoustic features of\nauditory stimuli, such as variance of interaural level difference (ILD),\nloudness, and sharpness, were correlated with the magnitude of subjective\nfrisson. We had also observed that the subjective feelings of frisson by moving\na musical sound had increased comparing with a static musical sound.", "published": "2019-04-15 05:19:05", "link": "http://arxiv.org/abs/1904.06851v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Singing voice synthesis based on convolutional neural networks", "abstract": "The present paper describes a singing voice synthesis based on convolutional\nneural networks (CNNs). Singing voice synthesis systems based on deep neural\nnetworks (DNNs) are currently being proposed and are improving the naturalness\nof synthesized singing voices. In these systems, the relationship between\nmusical score feature sequences and acoustic feature sequences extracted from\nsinging voices is modeled by DNNs. Then, an acoustic feature sequence of an\narbitrary musical score is output in units of frames by the trained DNNs, and a\nnatural trajectory of a singing voice is obtained by using a parameter\ngeneration algorithm. As singing voices contain rich expression, a powerful\ntechnique to model them accurately is required. In the proposed technique,\nlong-term dependencies of singing voices are modeled by CNNs. An acoustic\nfeature sequence is generated in units of segments that consist of long-term\nframes, and a natural trajectory is obtained without the parameter generation\nalgorithm. Experimental results in a subjective listening test show that the\nproposed architecture can synthesize natural sounding singing voices.", "published": "2019-04-15 06:23:44", "link": "http://arxiv.org/abs/1904.06868v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Are Nearby Neighbors Relatives?: Testing Deep Music Embeddings", "abstract": "Deep neural networks have frequently been used to directly learn\nrepresentations useful for a given task from raw input data. In terms of\noverall performance metrics, machine learning solutions employing deep\nrepresentations frequently have been reported to greatly outperform those using\nhand-crafted feature representations. At the same time, they may pick up on\naspects that are predominant in the data, yet not actually meaningful or\ninterpretable. In this paper, we therefore propose a systematic way to test the\ntrustworthiness of deep music representations, considering musical semantics.\nThe underlying assumption is that in case a deep representation is to be\ntrusted, distance consistency between known related points should be maintained\nboth in the input audio space and corresponding latent deep space. We generate\nknown related points through semantically meaningful transformations, both\nconsidering imperceptible and graver transformations. Then, we examine within-\nand between-space distance consistencies, both considering audio space and\nlatent embedded space, the latter either being a result of a conventional\nfeature extractor or a deep encoder. We illustrate how our method, as a\ncomplement to task-specific performance, provides interpretable insight into\nwhat a network may have captured from training data signals.", "published": "2019-04-15 16:08:41", "link": "http://arxiv.org/abs/1904.07154v3", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RHR-Net: A Residual Hourglass Recurrent Neural Network for Speech\n  Enhancement", "abstract": "Most current speech enhancement models use spectrogram features that require\nan expensive transformation and result in phase information loss. Previous work\nhas overcome these issues by using convolutional networks to learn long-range\ntemporal correlations across high-resolution waveforms. These models, however,\nare limited by memory-intensive dilated convolution and aliasing artifacts from\nupsampling. We introduce an end-to-end fully-recurrent hourglass-shaped neural\nnetwork architecture with residual connections for waveform-based\nsingle-channel speech enhancement. Our model can efficiently capture long-range\ntemporal dependencies by reducing the features resolution without information\nloss. Experimental results show that our model outperforms state-of-the-art\napproaches in six evaluation metrics.", "published": "2019-04-15 19:17:28", "link": "http://arxiv.org/abs/1904.07294v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
