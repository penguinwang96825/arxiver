{"title": "Sentiment Frames for Attitude Extraction in Russian", "abstract": "Texts can convey several types of inter-related information concerning\nopinions and attitudes. Such information includes the author's attitude towards\nmentioned entities, attitudes of the entities towards each other, positive and\nnegative effects on the entities in the described situations. In this paper, we\ndescribed the lexicon RuSentiFrames for Russian, where predicate words and\nexpressions are collected and linked to so-called sentiment frames conveying\nseveral types of presupposed information on attitudes and effects. We applied\nthe created frames in the task of extracting attitudes from a large news\ncollection.", "published": "2020-06-19 06:07:48", "link": "http://arxiv.org/abs/2006.10973v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Syntactic-Semantic Parsing Based on Constituent and\n  Dependency Structures", "abstract": "Syntactic and semantic parsing has been investigated for decades, which is\none primary topic in the natural language processing community. This article\naims for a brief survey on this topic. The parsing community includes many\ntasks, which are difficult to be covered fully. Here we focus on two of the\nmost popular formalizations of parsing: constituent parsing and dependency\nparsing. Constituent parsing is majorly targeted to syntactic analysis, and\ndependency parsing can handle both syntactic and semantic analysis. This\narticle briefly reviews the representative models of constituent parsing and\ndependency parsing, and also dependency graph parsing with rich semantics.\nBesides, we also review the closely-related topics such as cross-domain,\ncross-lingual and joint parsing models, parser application as well as corpus\ndevelopment of parsing in the article.", "published": "2020-06-19 10:21:17", "link": "http://arxiv.org/abs/2006.11056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dataset for Automatic Summarization of Russian News", "abstract": "Automatic text summarization has been studied in a variety of domains and\nlanguages. However, this does not hold for the Russian language. To overcome\nthis issue, we present Gazeta, the first dataset for summarization of Russian\nnews. We describe the properties of this dataset and benchmark several\nextractive and abstractive models. We demonstrate that the dataset is a valid\ntask for methods of text summarization for Russian. Additionally, we prove the\npretrained mBART model to be useful for Russian text summarization.", "published": "2020-06-19 10:44:06", "link": "http://arxiv.org/abs/2006.11063v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mechanisms for Handling Nested Dependencies in Neural-Network Language\n  Models and Humans", "abstract": "Recursive processing in sentence comprehension is considered a hallmark of\nhuman linguistic abilities. However, its underlying neural mechanisms remain\nlargely unknown. We studied whether a modern artificial neural network trained\nwith \"deep learning\" methods mimics a central aspect of human sentence\nprocessing, namely the storing of grammatical number and gender information in\nworking memory and its use in long-distance agreement (e.g., capturing the\ncorrect number agreement between subject and verb when they are separated by\nother phrases). Although the network, a recurrent architecture with Long\nShort-Term Memory units, was solely trained to predict the next word in a large\ncorpus, analysis showed the emergence of a very sparse set of specialized units\nthat successfully handled local and long-distance syntactic agreement for\ngrammatical number. However, the simulations also showed that this mechanism\ndoes not support full recursion and fails with some long-range embedded\ndependencies. We tested the model's predictions in a behavioral experiment\nwhere humans detected violations in number agreement in sentences with\nsystematic variations in the singular/plural status of multiple nouns, with or\nwithout embedding. Human and model error patterns were remarkably similar,\nshowing that the model echoes various effects observed in human data. However,\na key difference was that, with embedded long-range dependencies, humans\nremained above chance level, while the model's systematic errors brought it\nbelow chance. Overall, our study shows that exploring the ways in which modern\nartificial neural networks process sentences leads to precise and testable\nhypotheses about human linguistic performance.", "published": "2020-06-19 12:00:05", "link": "http://arxiv.org/abs/2006.11098v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "New Vietnamese Corpus for Machine Reading Comprehension of Health News\n  Articles", "abstract": "Large-scale and high-quality corpora are necessary for evaluating machine\nreading comprehension models on a low-resource language like Vietnamese.\nBesides, machine reading comprehension (MRC) for the health domain offers great\npotential for practical applications; however, there is still very little MRC\nresearch in this domain. This paper presents ViNewsQA as a new corpus for the\nVietnamese language to evaluate healthcare reading comprehension models. The\ncorpus comprises 22,057 human-generated question-answer pairs. Crowd-workers\ncreate the questions and their answers based on a collection of over 4,416\nonline Vietnamese healthcare news articles, where the answers comprise spans\nextracted from the corresponding articles. In particular, we develop a process\nof creating a corpus for the Vietnamese machine reading comprehension.\nComprehensive evaluations demonstrate that our corpus requires abilities beyond\nsimple reasoning, such as word matching and demanding difficult reasoning based\non single-or-multiple-sentence information. We conduct experiments using\ndifferent types of machine reading comprehension methods to achieve the first\nbaseline performances, compared with further models' performances. We also\nmeasure human performance on the corpus and compared it with several powerful\nneural network-based and transfer learning-based models. Our experiments show\nthat the best machine model is ALBERT, which achieves an exact match score of\n65.26% and an F1-score of 84.89% on our corpus. The significant differences\nbetween humans and the best-performance model (14.53% of EM and 10.90% of\nF1-score) on the test set of our corpus indicate that improvements in ViNewsQA\ncould be explored in the future study. Our corpus is publicly available on our\nwebsite for the research purpose to encourage the research community to make\nthese improvements.", "published": "2020-06-19 13:49:26", "link": "http://arxiv.org/abs/2006.11138v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graphs with Multiple Sources per Vertex", "abstract": "Several attempts have been made at constructing Abstract Meaning\nRepresentations (AMRs) compositionally, and recently the idea of using s-graphs\nwith the HR-algebra (Koller, 2015) has been simplified to reduce the number of\noptions when parsing (Groschwitz et al., 2017). This apply-modify algebra\n(AM-algebra) is a linguistically plausible graph algebra with two classes of\noperations, both of rank two: the apply operation is used to combine a\npredicate with its argument; the modify operation is used to modify a\npredicate. While the AM-algebra correctly handles relative clauses and complex\ncases of coordination, it cannot parse reflexive sentences like: \"The raven\nwashes herself.\" To facilitate processing of such reflexive sentences, this\npaper proposes to change the definition of s-graphs underlying the AM-algebra\nto allow vertices with multiple sources, and additionally proposes an adaption\nto the type system of the algebra to correctly handle such vertices.", "published": "2020-06-19 14:43:12", "link": "http://arxiv.org/abs/2006.11159v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Chatbot: A Conversational Agent employed with Named Entity Recognition\n  Model using Artificial Neural Network", "abstract": "Chatbot is a technology that is used to mimic human behavior using natural\nlanguage. There are different types of Chatbot that can be used as\nconversational agent in various business domains in order to increase the\ncustomer service and satisfaction. For any business domain, it requires a\nknowledge base to be built for that domain and design an information retrieval\nbased system that can respond the user with a piece of documentation or\ngenerated sentences. The core component of a Chatbot is Natural Language\nUnderstanding (NLU) which has been impressively improved by deep learning\nmethods. But we often lack such properly built NLU modules and requires more\ntime to build it from scratch for high quality conversations. This may\nencourage fresh learners to build a Chatbot from scratch with simple\narchitecture and using small dataset, although it may have reduced\nfunctionality, rather than building high quality data driven methods. This\nresearch focuses on Named Entity Recognition (NER) and Intent Classification\nmodels which can be integrated into NLU service of a Chatbot. Named entities\nwill be inserted manually in the knowledge base and automatically detected in a\ngiven sentence. The NER model in the proposed architecture is based on\nartificial neural network which is trained on manually created entities and\nevaluated using CoNLL-2003 dataset.", "published": "2020-06-19 14:47:21", "link": "http://arxiv.org/abs/2007.04248v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Neural Topic Modeling with Continual Lifelong Learning", "abstract": "Lifelong learning has recently attracted attention in building machine\nlearning systems that continually accumulate and transfer knowledge to help\nfuture learning. Unsupervised topic modeling has been popularly used to\ndiscover topics from document collections. However, the application of topic\nmodeling is challenging due to data sparsity, e.g., in a small collection of\n(short) documents and thus, generate incoherent topics and sub-optimal document\nrepresentations. To address the problem, we propose a lifelong learning\nframework for neural topic modeling that can continuously process streams of\ndocument collections, accumulate topics and guide future topic modeling tasks\nby knowledge transfer from several sources to better deal with the sparse data.\nIn the lifelong process, we particularly investigate jointly: (1) sharing\ngenerative homologies (latent topics) over lifetime to transfer prior\nknowledge, and (2) minimizing catastrophic forgetting to retain the past\nlearning via novel selective data augmentation, co-training and topic\nregularization approaches. Given a stream of document collections, we apply the\nproposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three\nsparse document collections as future tasks and demonstrate improved\nperformance quantified by perplexity, topic coherence and information retrieval\ntask.", "published": "2020-06-19 00:43:23", "link": "http://arxiv.org/abs/2006.10909v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Joint Speaker Counting, Speech Recognition, and Speaker Identification\n  for Overlapped Speech of Any Number of Speakers", "abstract": "We propose an end-to-end speaker-attributed automatic speech recognition\nmodel that unifies speaker counting, speech recognition, and speaker\nidentification on monaural overlapped speech. Our model is built on serialized\noutput training (SOT) with attention-based encoder-decoder, a recently proposed\nmethod for recognizing overlapped speech comprising an arbitrary number of\nspeakers. We extend SOT by introducing a speaker inventory as an auxiliary\ninput to produce speaker labels as well as multi-speaker transcriptions. All\nmodel parameters are optimized by speaker-attributed maximum mutual information\ncriterion, which represents a joint probability for overlapped speech\nrecognition and speaker identification. Experiments on LibriSpeech corpus show\nthat our proposed method achieves significantly better speaker-attributed word\nerror rate than the baseline that separately performs overlapped speech\nrecognition and speaker identification.", "published": "2020-06-19 02:05:18", "link": "http://arxiv.org/abs/2006.10930v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Qualitative Evaluation of Language Models on Automatic\n  Question-Answering for COVID-19", "abstract": "COVID-19 has resulted in an ongoing pandemic and as of 12 June 2020, has\ncaused more than 7.4 million cases and over 418,000 deaths. The highly dynamic\nand rapidly evolving situation with COVID-19 has made it difficult to access\naccurate, on-demand information regarding the disease. Online communities,\nforums, and social media provide potential venues to search for relevant\nquestions and answers, or post questions and seek answers from other members.\nHowever, due to the nature of such sites, there are always a limited number of\nrelevant questions and responses to search from, and posted questions are\nrarely answered immediately. With the advancements in the field of natural\nlanguage processing, particularly in the domain of language models, it has\nbecome possible to design chatbots that can automatically answer consumer\nquestions. However, such models are rarely applied and evaluated in the\nhealthcare domain, to meet the information needs with accurate and up-to-date\nhealthcare data. In this paper, we propose to apply a language model for\nautomatically answering questions related to COVID-19 and qualitatively\nevaluate the generated responses. We utilized the GPT-2 language model and\napplied transfer learning to retrain it on the COVID-19 Open Research Dataset\n(CORD-19) corpus. In order to improve the quality of the generated responses,\nwe applied 4 different approaches, namely tf-idf, BERT, BioBERT, and USE to\nfilter and retain relevant sentences in the responses. In the performance\nevaluation step, we asked two medical experts to rate the responses. We found\nthat BERT and BioBERT, on average, outperform both tf-idf and USE in\nrelevance-based sentence filtering tasks. Additionally, based on the chatbot,\nwe created a user-friendly interactive web application to be hosted online.", "published": "2020-06-19 05:13:57", "link": "http://arxiv.org/abs/2006.10964v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SqueezeBERT: What can computer vision teach NLP about efficient neural\n  networks?", "abstract": "Humans read and write hundreds of billions of messages every day. Further,\ndue to the availability of large datasets, large computing systems, and better\nneural network models, natural language processing (NLP) technology has made\nsignificant strides in understanding, proofreading, and organizing these\nmessages. Thus, there is a significant opportunity to deploy NLP in myriad\napplications to help web users, social networks, and businesses. In particular,\nwe consider smartphones and other mobile devices as crucial platforms for\ndeploying NLP models at scale. However, today's highly-accurate NLP neural\nnetwork models such as BERT and RoBERTa are extremely computationally\nexpensive, with BERT-base taking 1.7 seconds to classify a text snippet on a\nPixel 3 smartphone. In this work, we observe that methods such as grouped\nconvolutions have yielded significant speedups for computer vision networks,\nbut many of these techniques have not been adopted by NLP neural network\ndesigners. We demonstrate how to replace several operations in self-attention\nlayers with grouped convolutions, and we use this technique in a novel network\narchitecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the\nPixel 3 while achieving competitive accuracy on the GLUE test set. The\nSqueezeBERT code will be released.", "published": "2020-06-19 18:40:29", "link": "http://arxiv.org/abs/2006.11316v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Waveform-based Voice Activity Detection Exploiting Fully Convolutional\n  networks with Multi-Branched Encoders", "abstract": "In this study, we propose an encoder-decoder structured system with fully\nconvolutional networks to implement voice activity detection (VAD) directly on\nthe time-domain waveform. The proposed system processes the input waveform to\nidentify its segments to be either speech or non-speech. This novel\nwaveform-based VAD algorithm, with a short-hand notation \"WVAD\", has two main\nparticularities. First, as compared to most conventional VAD systems that use\nspectral features, raw-waveforms employed in WVAD contain more comprehensive\ninformation and thus are supposed to facilitate more accurate speech/non-speech\npredictions. Second, based on the multi-branched architecture, WVAD can be\nextended by using an ensemble of encoders, referred to as WEVAD, that\nincorporate multiple attribute information in utterances, and thus can yield\nbetter VAD performance for specified acoustic conditions. We evaluated the\npresented WVAD and WEVAD for the VAD task in two datasets: First, the\nexperiments conducted on AURORA2 reveal that WVAD outperforms many\nstate-of-the-art VAD algorithms. Next, the TMHINT task confirms that through\ncombining multiple attributes in utterances, WEVAD behaves even better than\nWVAD.", "published": "2020-06-19 13:50:06", "link": "http://arxiv.org/abs/2006.11139v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Clarity: Machine Learning Challenges to Revolutionise Hearing Device\n  Processing", "abstract": "In the Clarity project, we will run a series of machine learning challenges\nto revolutionise speech processing for hearing devices. Over five years, there\nwill be three paired challenges. Each pair will consist of a competition\nfocussed on hearing-device processing (\"enhancement\") and another focussed on\nspeech perception modelling (\"prediction\"). The enhancement challenges will\ndeliver new and improved approaches for hearing device signal processing for\nspeech. The parallel prediction challenges will develop and improve methods for\npredicting speech intelligibility and quality for hearing impaired listeners.\nThis Engineering and Physical Sciences Research Council (EPSRC) funded project\ninvolves researchers from the Universities of Sheffield, Salford, Nottingham\nand Cardiff in conjunction with the Hearing Industry Research Consortium,\nAction on Hearing Loss, Amazon, and Honda. To register interest in the\nchallenges, go to www.claritychallenge.org.", "published": "2020-06-19 13:51:21", "link": "http://arxiv.org/abs/2006.11140v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Boosting Active Learning for Speech Recognition with Noisy\n  Pseudo-labeled Samples", "abstract": "The cost of annotating transcriptions for large speech corpora becomes a\nbottleneck to maximally enjoy the potential capacity of deep neural\nnetwork-based automatic speech recognition models. In this paper, we present a\nnew training pipeline boosting the conventional active learning approach\ntargeting label-efficient learning to resolve the mentioned problem. Existing\nactive learning methods only focus on selecting a set of informative samples\nunder a labeling budget. One step further, we suggest that the training\nefficiency can be further improved by utilizing the unlabeled samples,\nexceeding the labeling budget, by introducing sophisticatedly configured\nunsupervised loss complementing supervised loss effectively. We propose new\nunsupervised loss based on consistency regularization, and we configure\nappropriate augmentation techniques for utterances to adopt consistency\nregularization in the automatic speech recognition task. From the qualitative\nand quantitative experiments on the real-world dataset and under real-usage\nscenarios, we show that the proposed training pipeline can boost the efficacy\nof active learning approaches, thus successfully reducing a sustainable amount\nof human labeling cost.", "published": "2020-06-19 08:54:46", "link": "http://arxiv.org/abs/2006.11021v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Towards Reliable Real-time Opera Tracking: Combining Alignment with\n  Audio Event Detectors to Increase Robustness", "abstract": "Recent advances in real-time music score following have made it possible for\nmachines to automatically track highly complex polyphonic music, including full\norchestra performances. In this paper, we attempt to take this to an even\nhigher level, namely, live tracking of full operas. We first apply a\nstate-of-the-art audio alignment method based on online Dynamic Time-Warping\n(OLTW) to full-length recordings of a Mozart opera and, analyzing the tracker's\nmost severe errors, identify three common sources of problems specific to the\nopera scenario. To address these, we propose a combination of a DTW-based music\ntracker with specialized audio event detectors (for applause, silence/noise,\nand speech) that condition the DTW algorithm in a top-down fashion, and show,\nstep by step, how these detectors add robustness to the score follower.\nHowever, there remain a number of open problems which we identify as targets\nfor ongoing and future research.", "published": "2020-06-19 09:31:07", "link": "http://arxiv.org/abs/2006.11033v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
