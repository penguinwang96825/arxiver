{"title": "Attention Head Masking for Inference Time Content Selection in\n  Abstractive Summarization", "abstract": "How can we effectively inform content selection in Transformer-based\nabstractive summarization models? In this work, we present a\nsimple-yet-effective attention head masking technique, which is applied on\nencoder-decoder attentions to pinpoint salient content at inference time. Using\nattention head masking, we are able to reveal the relation between\nencoder-decoder attentions and content selection behaviors of summarization\nmodels. We then demonstrate its effectiveness on three document summarization\ndatasets based on both in-domain and cross-domain settings. Importantly, our\nmodels outperform prior state-of-the-art models on CNN/Daily Mail and New York\nTimes datasets. Moreover, our inference-time masking technique is also\ndata-efficient, requiring only 20% of the training samples to outperform BART\nfine-tuned on the full CNN/DailyMail dataset.", "published": "2021-04-06 00:49:25", "link": "http://arxiv.org/abs/2104.02205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HBert + BiasCorp -- Fighting Racism on the Web", "abstract": "Subtle and overt racism is still present both in physical and online\ncommunities today and has impacted many lives in different segments of the\nsociety. In this short piece of work, we present how we're tackling this\nsocietal issue with Natural Language Processing. We are releasing BiasCorp, a\ndataset containing 139,090 comments and news segment from three specific\nsources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually\nannotated) is ready for publication. We are currently in the final phase of\nmanually labeling the remaining dataset using Amazon Mechanical Turk. BERT has\nbeen used widely in several downstream tasks. In this work, we present hBERT,\nwhere we modify certain layers of the pretrained BERT model with the new\nHopfield Layer. hBert generalizes well across different distributions with the\nadded advantage of a reduced model complexity. We are also releasing a\nJavaScript library and a Chrome Extension Application, to help developers make\nuse of our trained model in web applications (say chat application) and for\nusers to identify and report racially biased contents on the web respectively.", "published": "2021-04-06 02:17:20", "link": "http://arxiv.org/abs/2104.02242v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-autoregressive Mandarin-English Code-switching Speech Recognition", "abstract": "Mandarin-English code-switching (CS) is frequently used among East and\nSoutheast Asian people. However, the intra-sentence language switching of the\ntwo very different languages makes recognizing CS speech challenging.\nMeanwhile, the recent successful non-autoregressive (NAR) ASR models remove the\nneed for left-to-right beam decoding in autoregressive (AR) models and achieved\noutstanding performance and fast inference speed, but it has not been applied\nto Mandarin-English CS speech recognition. This paper takes advantage of the\nMask-CTC NAR ASR framework to tackle the CS speech recognition issue. We\nfurther propose to change the Mandarin output target of the encoder to Pinyin\nfor faster encoder training and introduce the Pinyin-to-Mandarin decoder to\nlearn contextualized information. Moreover, we use word embedding label\nsmoothing to regularize the decoder with contextualized information and\nprojection matrix regularization to bridge that gap between the encoder and\ndecoder. We evaluate these methods on the SEAME corpus and achieved exciting\nresults.", "published": "2021-04-06 03:01:09", "link": "http://arxiv.org/abs/2104.02258v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ODE Transformer: An Ordinary Differential Equation-Inspired Model for\n  Neural Machine Translation", "abstract": "It has been found that residual networks are an Euler discretization of\nsolutions to Ordinary Differential Equations (ODEs). In this paper, we explore\na deeper relationship between Transformer and numerical methods of ODEs. We\nshow that a residual block of layers in Transformer can be described as a\nhigher-order solution to ODEs. This leads us to design a new architecture (call\nit ODE Transformer) analogous to the Runge-Kutta method that is well motivated\nin ODEs. As a natural extension to Transformer, ODE Transformer is easy to\nimplement and parameter efficient. Our experiments on three WMT tasks\ndemonstrate the genericity of this model, and large improvements in performance\nover several strong baselines. It achieves 30.76 and 44.11 BLEU scores on the\nWMT'14 En-De and En-Fr test data. This sets a new state-of-the-art on the\nWMT'14 En-Fr task.", "published": "2021-04-06 06:13:02", "link": "http://arxiv.org/abs/2104.02308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SERRANT: a syntactic classifier for English Grammatical Error Types", "abstract": "SERRANT is a system and code for automatic classification of English\ngrammatical errors that combines SErCl and ERRANT. SERRANT uses ERRANT's\nannotations when they are informative and those provided by SErCl otherwise.", "published": "2021-04-06 06:26:58", "link": "http://arxiv.org/abs/2104.02310v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI4D -- African Language Program", "abstract": "Advances in speech and language technologies enable tools such as\nvoice-search, text-to-speech, speech recognition and machine translation. These\nare however only available for high resource languages like English, French or\nChinese. Without foundational digital resources for African languages, which\nare considered low-resource in the digital context, these advanced tools remain\nout of reach. This work details the AI4D - African Language Program, a 3-part\nproject that 1) incentivised the crowd-sourcing, collection and curation of\nlanguage datasets through an online quantitative and qualitative challenge, 2)\nsupported research fellows for a period of 3-4 months to create datasets\nannotated for NLP tasks, and 3) hosted competitive Machine Learning challenges\non the basis of these datasets. Key outcomes of the work so far include 1) the\ncreation of 9+ open source, African language datasets annotated for a variety\nof ML tasks, and 2) the creation of baseline models for these datasets through\nhosting of competitive ML challenges.", "published": "2021-04-06 13:51:16", "link": "http://arxiv.org/abs/2104.02516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EasyCall corpus: a dysarthric speech dataset", "abstract": "This paper introduces a new dysarthric speech command dataset in Italian,\ncalled EasyCall corpus. The dataset consists of 21386 audio recordings from 24\nhealthy and 31 dysarthric speakers, whose individual degree of speech\nimpairment was assessed by neurologists through the Therapy Outcome Measure.\nThe corpus aims at providing a resource for the development of ASR-based\nassistive technologies for patients with dysarthria. In particular, it may be\nexploited to develop a voice-controlled contact application for commercial\nsmartphones, aiming at improving dysarthric patients' ability to communicate\nwith their family and caregivers. Before recording the dataset, participants\nwere administered a survey to evaluate which commands are more likely to be\nemployed by dysarthric individuals in a voice-controlled contact application.\nIn addition, the dataset includes a list of non-commands (i.e., words\nnear/inside commands or phonetically close to commands) that can be leveraged\nto build a more robust command recognition system. At present commercial ASR\nsystems perform poorly on the EasyCall Corpus as we report in this paper. This\nresult corroborates the need for dysarthric speech corpora for developing\neffective assistive technologies. To the best of our knowledge, this database\nrepresents the richest corpus of dysarthric speech to date.", "published": "2021-04-06 14:32:47", "link": "http://arxiv.org/abs/2104.02542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized Entity Resolution with Dynamic Heterogeneous Knowledge\n  Graph Representations", "abstract": "The growing popularity of Virtual Assistants poses new challenges for Entity\nResolution, the task of linking mentions in text to their referent entities in\na knowledge base. Specifically, in the shopping domain, customers tend to use\nimplicit utterances (e.g., \"organic milk\") rather than explicit names, leading\nto a large number of candidate products. Meanwhile, for the same query,\ndifferent customers may expect different results. For example, with \"add milk\nto my cart\", a customer may refer to a certain organic product, while some\ncustomers may want to re-order products they regularly purchase. To address\nthese issues, we propose a new framework that leverages personalized features\nto improve the accuracy of product ranking. We first build a cross-source\nheterogeneous knowledge graph from customer purchase history and product\nknowledge graph to jointly learn customer and product embeddings. After that,\nwe incorporate product, customer, and history representations into a neural\nreranking model to predict which candidate is most likely to be purchased for a\nspecific customer. Experiments show that our model substantially improves the\naccuracy of the top ranked candidates by 24.6% compared to the state-of-the-art\nproduct search model.", "published": "2021-04-06 16:58:27", "link": "http://arxiv.org/abs/2104.02667v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text-guided Legal Knowledge Graph Reasoning", "abstract": "Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zxlzr/LegalPP} for reproducibility.", "published": "2021-04-06 04:42:56", "link": "http://arxiv.org/abs/2104.02284v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Student-Teacher Architecture for Dialog Domain Adaptation under the\n  Meta-Learning Setting", "abstract": "Numerous new dialog domains are being created every day while collecting data\nfor these domains is extremely costly since it involves human interactions.\nTherefore, it is essential to develop algorithms that can adapt to different\ndomains efficiently when building data-driven dialog models. The most recent\nresearches on domain adaption focus on giving the model a better\ninitialization, rather than optimizing the adaptation process. We propose an\nefficient domain adaptive task-oriented dialog system model, which incorporates\na meta-teacher model to emphasize the different impacts between generated\ntokens with respect to the context. We first train our base dialog model and\nmeta-teacher model adversarially in a meta-learning setting on rich-resource\ndomains. The meta-teacher learns to quantify the importance of tokens under\ndifferent contexts across different domains. During adaptation, the\nmeta-teacher guides the dialog model to focus on important tokens in order to\nachieve better adaptation efficiency. We evaluate our model on two multi-domain\ndatasets, MultiWOZ and Google Schema-Guided Dialogue, and achieve\nstate-of-the-art performance.", "published": "2021-04-06 17:31:28", "link": "http://arxiv.org/abs/2104.02689v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with\n  Common Sense and World Knowledge", "abstract": "Cant is important for understanding advertising, comedies and dog-whistle\npolitics. However, computational research on cant is hindered by a lack of\navailable datasets. In this paper, we propose a large and diverse Chinese\ndataset for creating and understanding cant from a computational linguistics\nperspective. We formulate a task for cant understanding and provide both\nquantitative and qualitative analysis for tested word embedding similarity and\npretrained language models. Experiments suggest that such a task requires deep\nlanguage understanding, common sense, and world knowledge and thus can be a\ngood testbed for pretrained language models and help models perform better on\nother tasks. The code is available at https://github.com/JetRunner/dogwhistle.\nThe data and leaderboard are available at\nhttps://competitions.codalab.org/competitions/30451.", "published": "2021-04-06 17:55:43", "link": "http://arxiv.org/abs/2104.02704v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word\n  Representations", "abstract": "Word vector embeddings have been shown to contain and amplify biases in data\nthey are extracted from. Consequently, many techniques have been proposed to\nidentify, mitigate, and attenuate these biases in word representations. In this\npaper, we utilize interactive visualization to increase the interpretability\nand accessibility of a collection of state-of-the-art debiasing techniques. To\naid this, we present Visualization of Embedding Representations for deBiasing\nsystem (\"VERB\"), an open-source web-based visualization tool that helps the\nusers gain a technical understanding and visual intuition of the inner workings\nof debiasing techniques, with a focus on their geometric properties. In\nparticular, VERB offers easy-to-follow use cases in exploring the effects of\nthese debiasing techniques on the geometry of high-dimensional word vectors. To\nhelp understand how various debiasing techniques change the underlying\ngeometry, VERB decomposes each technique into interpretable sequences of\nprimitive transformations and highlights their effect on the word vectors using\ndimensionality reduction and interactive visual exploration. VERB is designed\nto target natural language processing (NLP) practitioners who are designing\ndecision-making systems on top of word embeddings, and also researchers working\nwith fairness and ethics of machine learning systems in NLP. It can also serve\nas a visual medium for education, which helps an NLP novice to understand and\nmitigate biases in word embeddings.", "published": "2021-04-06 21:29:16", "link": "http://arxiv.org/abs/2104.02797v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Dissecting User-Perceived Latency of On-Device E2E Speech Recognition", "abstract": "As speech-enabled devices such as smartphones and smart speakers become\nincreasingly ubiquitous, there is growing interest in building automatic speech\nrecognition (ASR) systems that can run directly on-device; end-to-end (E2E)\nspeech recognition models such as recurrent neural network transducers and\ntheir variants have recently emerged as prime candidates for this task. Apart\nfrom being accurate and compact, such systems need to decode speech with low\nuser-perceived latency (UPL), producing words as soon as they are spoken. This\nwork examines the impact of various techniques - model architectures, training\ncriteria, decoding hyperparameters, and endpointer parameters - on UPL. Our\nanalyses suggest that measures of model size (parameters, input chunk sizes),\nor measures of computation (e.g., FLOPS, RTF) that reflect the model's ability\nto process input frames are not always strongly correlated with observed UPL.\nThus, conventional algorithmic latency measurements might be inadequate in\naccurately capturing latency observed when models are deployed on embedded\ndevices. Instead, we find that factors affecting token emission latency, and\nendpointing behavior have a larger impact on UPL. We achieve the best trade-off\nbetween latency and word error rate when performing ASR jointly with\nendpointing, while utilizing the recently proposed alignment regularization\nmechanism.", "published": "2021-04-06 00:55:11", "link": "http://arxiv.org/abs/2104.02207v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Flexi-Transducer: Optimizing Latency, Accuracy and Compute\n  forMulti-Domain On-Device Scenarios", "abstract": "Often, the storage and computational constraints of embeddeddevices demand\nthat a single on-device ASR model serve multiple use-cases / domains. In this\npaper, we propose aFlexibleTransducer(FlexiT) for on-device automatic speech\nrecognition to flexibly deal with multiple use-cases / domains with different\naccuracy and latency requirements. Specifically, using a single compact model,\nFlexiT provides a fast response for voice commands, and accurate transcription\nbut with more latency for dictation. In order to achieve flexible and better\naccuracy and latency trade-offs, the following techniques are used. Firstly, we\npropose using domain-specific altering of segment size for Emformer encoder\nthat enables FlexiT to achieve flexible de-coding. Secondly, we use Alignment\nRestricted RNNT loss to achieve flexible fine-grained control on token emission\nlatency for different domains. Finally, we add a domain indicator vector as an\nadditional input to the FlexiT model. Using the combination of techniques, we\nshow that a single model can be used to improve WERs and real time factor for\ndictation scenarios while maintaining optimal latency for voice commands\nuse-cases", "published": "2021-04-06 01:50:19", "link": "http://arxiv.org/abs/2104.02232v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "OodGAN: Generative Adversarial Network for Out-of-Domain Data Generation", "abstract": "Detecting an Out-of-Domain (OOD) utterance is crucial for a robust dialog\nsystem. Most dialog systems are trained on a pool of annotated OOD data to\nachieve this goal. However, collecting the annotated OOD data for a given\ndomain is an expensive process. To mitigate this issue, previous works have\nproposed generative adversarial networks (GAN) based models to generate OOD\ndata for a given domain automatically. However, these proposed models do not\nwork directly with the text. They work with the text's latent space instead,\nenforcing these models to include components responsible for encoding text into\nlatent space and decoding it back, such as auto-encoder. These components\nincrease the model complexity, making it difficult to train. We propose OodGAN,\na sequential generative adversarial network (SeqGAN) based model for OOD data\ngeneration. Our proposed model works directly on the text and hence eliminates\nthe need to include an auto-encoder. OOD data generated using OodGAN model\noutperforms state-of-the-art in OOD detection metrics for ROSTD (67% relative\nimprovement in FPR 0.95) and OSQ datasets (28% relative improvement in FPR\n0.95) (Zheng et al., 2020).", "published": "2021-04-06 13:08:39", "link": "http://arxiv.org/abs/2104.02484v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Topic-Metadata Relationships with the STM: A Bayesian Approach", "abstract": "Topic models such as the Structural Topic Model (STM) estimate latent topical\nclusters within text. An important step in many topic modeling applications is\nto explore relationships between the discovered topical structure and metadata\nassociated with the text documents. Methods used to estimate such relationships\nmust take into account that the topical structure is not directly observed, but\ninstead being estimated itself. The authors of the STM, for instance, perform\nrepeated OLS regressions of sampled topic proportions on metadata covariates by\nusing a Monte Carlo sampling technique known as the method of composition. In\nthis paper, we propose two improvements: first, we replace OLS with more\nappropriate Beta regression. Second, we suggest a fully Bayesian approach\ninstead of the current blending of frequentist and Bayesian methods. We\ndemonstrate our improved methodology by exploring relationships between Twitter\nposts by German members of parliament (MPs) and different metadata covariates.", "published": "2021-04-06 13:28:04", "link": "http://arxiv.org/abs/2104.02496v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "LT-LM: a novel non-autoregressive language model for single-shot lattice\n  rescoring", "abstract": "Neural network-based language models are commonly used in rescoring\napproaches to improve the quality of modern automatic speech recognition (ASR)\nsystems. Most of the existing methods are computationally expensive since they\nuse autoregressive language models. We propose a novel rescoring approach,\nwhich processes the entire lattice in a single call to the model. The key\nfeature of our rescoring policy is a novel non-autoregressive Lattice\nTransformer Language Model (LT-LM). This model takes the whole lattice as an\ninput and predicts a new language score for each arc. Additionally, we propose\nthe artificial lattices generation approach to incorporate a large amount of\ntext data in the LT-LM training process. Our single-shot rescoring performs\norders of magnitude faster than other rescoring methods in our experiments. It\nis more than 300 times faster than pruned RNNLM lattice rescoring and N-best\nrescoring while slightly inferior in terms of WER.", "published": "2021-04-06 14:06:07", "link": "http://arxiv.org/abs/2104.02526v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Optimal Transport-based Adaptation in Dysarthric Speech Tasks", "abstract": "In many real-world applications, the mismatch between distributions of\ntraining data (source) and test data (target) significantly degrades the\nperformance of machine learning algorithms. In speech data, causes of this\nmismatch include different acoustic environments or speaker characteristics. In\nthis paper, we address this issue in the challenging context of dysarthric\nspeech, by multi-source domain/speaker adaptation (MSDA/MSSA). Specifically, we\npropose the use of an optimal-transport based approach, called MSDA via\nWeighted Joint Optimal Transport (MSDA-WDJOT). We confront the mismatch problem\nin dysarthria detection for which the proposed approach outperforms both the\nBaseline and the state-of-the-art MSDA models, improving the detection accuracy\nof 0.9% over the best competitor method. We then employ MSDA-WJDOT for\ndysarthric speaker adaptation in command speech recognition. This provides a\nCommand Error Rate relative reduction of 16% and 7% over the baseline and the\nbest competitor model, respectively. Interestingly, MSDA-WJDOT provides a\nsimilarity score between the source and the target, i.e. between speakers in\nthis case. We leverage this similarity measure to define a Dysarthric and\nHealthy score of the target speaker and diagnose the dysarthria with an\naccuracy of 95%.", "published": "2021-04-06 14:26:34", "link": "http://arxiv.org/abs/2104.02535v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Relaxing the Conditional Independence Assumption of CTC-based ASR by\n  Conditioning on Intermediate Predictions", "abstract": "This paper proposes a method to relax the conditional independence assumption\nof connectionist temporal classification (CTC)-based automatic speech\nrecognition (ASR) models. We train a CTC-based ASR model with auxiliary CTC\nlosses in intermediate layers in addition to the original CTC loss in the last\nlayer. During both training and inference, each generated prediction in the\nintermediate layers is summed to the input of the next layer to condition the\nprediction of the last layer on those intermediate predictions. Our method is\neasy to implement and retains the merits of CTC-based ASR: a simple model\narchitecture and fast decoding speed. We conduct experiments on three different\nASR corpora. Our proposed method improves a standard CTC model significantly\n(e.g., more than 20 % relative word error rate reduction on the WSJ corpus)\nwith a little computational overhead. Moreover, for the TEDLIUM2 corpus and the\nAISHELL-1 corpus, it achieves a comparable performance to a strong\nautoregressive model with beam search, but the decoding speed is at least 30\ntimes faster.", "published": "2021-04-06 18:00:03", "link": "http://arxiv.org/abs/2104.02724v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient transfer learning for NLP with ELECTRA", "abstract": "Clark et al. [2020] claims that the ELECTRA approach is highly efficient in\nNLP performances relative to computation budget. As such, this reproducibility\nstudy focus on this claim, summarized by the following question: Can we use\nELECTRA to achieve close to SOTA performances for NLP in low-resource settings,\nin term of compute cost?", "published": "2021-04-06 19:34:36", "link": "http://arxiv.org/abs/2104.02756v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CodeTrans: Towards Cracking the Language of Silicon's Code Through\n  Self-Supervised Deep Learning and High Performance Computing", "abstract": "Currently, a growing number of mature natural language processing\napplications make people's life more convenient. Such applications are built by\nsource code - the language in software engineering. However, the applications\nfor understanding source code language to ease the software engineering process\nare under-researched. Simultaneously, the transformer model, especially its\ncombination with transfer learning, has been proven to be a powerful technique\nfor natural language processing tasks. These breakthroughs point out a\npromising direction for process source code and crack software engineering\ntasks. This paper describes CodeTrans - an encoder-decoder transformer model\nfor tasks in the software engineering domain, that explores the effectiveness\nof encoder-decoder transformer models for six software engineering tasks,\nincluding thirteen sub-tasks. Moreover, we have investigated the effect of\ndifferent training strategies, including single-task learning, transfer\nlearning, multi-task learning, and multi-task learning with fine-tuning.\nCodeTrans outperforms the state-of-the-art models on all the tasks. To expedite\nfuture works in the software engineering domain, we have published our\npre-trained models of CodeTrans. https://github.com/agemagician/CodeTrans", "published": "2021-04-06 11:57:12", "link": "http://arxiv.org/abs/2104.02443v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Detecting English Speech in the Air Traffic Control Voice Communication", "abstract": "We launched a community platform for collecting the ATC speech world-wide in\nthe ATCO2 project. Filtering out unseen non-English speech is one of the main\ncomponents in the data processing pipeline. The proposed English Language\nDetection (ELD) system is based on the embeddings from Bayesian subspace\nmultinomial model. It is trained on the word confusion network from an ASR\nsystem. It is robust, easy to train, and light weighted. We achieved 0.0439\nequal-error-rate (EER), a 50% relative reduction as compared to the\nstate-of-the-art acoustic ELD system based on x-vectors, in the in-domain\nscenario. Further, we achieved an EER of 0.1352, a 33% relative reduction as\ncompared to the acoustic ELD, in the unseen language (out-of-domain) condition.\nWe plan to publish the evaluation dataset from the ATCO2 project.", "published": "2021-04-06 07:29:09", "link": "http://arxiv.org/abs/2104.02332v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "LEAP Submission for the Third DIHARD Diarization Challenge", "abstract": "The LEAP submission for DIHARD-III challenge is described in this paper. The\nproposed system is composed of a speech bandwidth classifier, and diarization\nsystems fine-tuned for narrowband and wideband speech separately. We use an\nend-to-end speaker diarization system for the narrowband conversational\ntelephone speech recordings. For the wideband multi-speaker recordings, we use\na neural embedding based clustering approach, similar to the baseline system.\nThe embeddings are extracted from a time-delay neural network (called\nx-vectors) followed by the graph based path integral clustering (PIC) approach.\nThe LEAP system showed 24% and 18% relative improvements for Track-1 and\nTrack-2 respectively over the baseline system provided by the organizers. This\npaper describes the challenge submission, the post-evaluation analysis and\nimprovements observed on the DIHARD-III dataset.", "published": "2021-04-06 08:36:08", "link": "http://arxiv.org/abs/2104.02359v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Integrating Frequency Translational Invariance in TDNNs and Frequency\n  Positional Information in 2D ResNets to Enhance Speaker Verification", "abstract": "This paper describes the IDLab submission for the text-independent task of\nthe Short-duration Speaker Verification Challenge 2021 (SdSVC-21). This speaker\nverification competition focuses on short duration test recordings and\ncross-lingual trials, along with the constraint of limited availability of\nin-domain DeepMine Farsi training data. Currently, both Time Delay Neural\nNetworks (TDNNs) and ResNets achieve state-of-the-art results in speaker\nverification. These architectures are structurally very different and the\nconstruction of hybrid networks looks a promising way forward. We introduce a\n2D convolutional stem in a strong ECAPA-TDNN baseline to transfer some of the\nstrong characteristics of a ResNet based model to this hybrid CNN-TDNN\narchitecture. Similarly, we incorporate absolute frequency positional encodings\nin an SE-ResNet34 architecture. These learnable feature map biases along the\nfrequency axis offer this architecture a straightforward way to exploit\nfrequency positional information. We also propose a frequency-wise variant of\nSqueeze-Excitation (SE) which better preserves frequency-specific information\nwhen rescaling the feature maps. Both modified architectures significantly\noutperform their corresponding baseline on the SdSVC-21 evaluation data and the\noriginal VoxCeleb1 test set. A four system fusion containing the two improved\narchitectures achieved a third place in the final SdSVC-21 Task 2 ranking.", "published": "2021-04-06 08:55:44", "link": "http://arxiv.org/abs/2104.02370v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards Consistent Hybrid HMM Acoustic Modeling", "abstract": "High-performance hybrid automatic speech recognition (ASR) systems are often\ntrained with clustered triphone outputs, and thus require a complex training\npipeline to generate the clustering. The same complex pipeline is often\nutilized in order to generate an alignment for use in frame-wise cross-entropy\ntraining. In this work, we propose a flat-start factored hybrid model trained\nby modeling the full set of triphone states explicitly without relying on\nclustering methods. This greatly simplifies the training of new models.\nFurthermore, we study the effect of different alignments used for Viterbi\ntraining. Our proposed models achieve competitive performance on the\nSwitchboard task compared to systems using clustered triphones and other\nflat-start models in the literature.", "published": "2021-04-06 09:24:01", "link": "http://arxiv.org/abs/2104.02387v3", "categories": ["cs.SD", "eess.AS", "68T10", "I.2.7"], "primary_category": "cs.SD"}
{"title": "ProsoBeast Prosody Annotation Tool", "abstract": "The labelling of speech corpora is a laborious and time-consuming process.\nThe ProsoBeast Annotation Tool seeks to ease and accelerate this process by\nproviding an interactive 2D representation of the prosodic landscape of the\ndata, in which contours are distributed based on their similarity. This\ninteractive map allows the user to inspect and label the utterances. The tool\nintegrates several state-of-the-art methods for dimensionality reduction and\nfeature embedding, including variational autoencoders. The user can use these\nto find a good representation for their data. In addition, as most of these\nmethods are stochastic, each can be used to generate an unlimited number of\ndifferent prosodic maps. The web app then allows the user to seamlessly switch\nbetween these alternative representations in the annotation process.\nExperiments with a sample prosodically rich dataset have shown that the tool\nmanages to find good representations of varied data and is helpful both for\nannotation and label correction. The tool is released as free software for use\nby the community.", "published": "2021-04-06 10:04:48", "link": "http://arxiv.org/abs/2104.02397v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "An Initial Investigation for Detecting Partially Spoofed Audio", "abstract": "All existing databases of spoofed speech contain attack data that is spoofed\nin its entirety. In practice, it is entirely plausible that successful attacks\ncan be mounted with utterances that are only partially spoofed. By definition,\npartially-spoofed utterances contain a mix of both spoofed and bona fide\nsegments, which will likely degrade the performance of countermeasures trained\nwith entirely spoofed utterances. This hypothesis raises the obvious question:\n'Can we detect partially-spoofed audio?' This paper introduces a new database\nof partially-spoofed data, named PartialSpoof, to help address this question.\nThis new database enables us to investigate and compare the performance of\ncountermeasures on both utterance- and segmental- level labels. Experimental\nresults using the utterance-level labels reveal that the reliability of\ncountermeasures trained to detect fully-spoofed data is found to degrade\nsubstantially when tested with partially-spoofed data, whereas training on\npartially-spoofed data performs reliably in the case of both fully- and\npartially-spoofed utterances. Additional experiments using segmental-level\nlabels show that spotting injected spoofed segments included in an utterance is\na much more challenging task even if the latest countermeasure models are used.", "published": "2021-04-06 13:52:31", "link": "http://arxiv.org/abs/2104.02518v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker embeddings by modeling channel-wise correlations", "abstract": "Speaker embeddings extracted with deep 2D convolutional neural networks are\ntypically modeled as projections of first and second order statistics of\nchannel-frequency pairs onto a linear layer, using either average or attentive\npooling along the time axis. In this paper we examine an alternative pooling\nmethod, where pairwise correlations between channels for given frequencies are\nused as statistics. The method is inspired by style-transfer methods in\ncomputer vision, where the style of an image, modeled by the matrix of\nchannel-wise correlations, is transferred to another image, in order to produce\na new image having the style of the first and the content of the second. By\ndrawing analogies between image style and speaker characteristics, and between\nimage content and phonetic sequence, we explore the use of such channel-wise\ncorrelations features to train a ResNet architecture in an end-to-end fashion.\nOur experiments on VoxCeleb demonstrate the effectiveness of the proposed\npooling method in speaker recognition.", "published": "2021-04-06 15:10:14", "link": "http://arxiv.org/abs/2104.02571v2", "categories": ["eess.AS", "cs.CV"], "primary_category": "eess.AS"}
{"title": "Extremely Low Footprint End-to-End ASR System for Smart Device", "abstract": "Recently, end-to-end (E2E) speech recognition has become popular, since it\ncan integrate the acoustic, pronunciation and language models into a single\nneural network, which outperforms conventional models. Among E2E approaches,\nattention-based models, e.g. Transformer, have emerged as being superior. Such\nmodels have opened the door to deployment of ASR on smart devices, however they\nstill suffer from requiring a large number of model parameters. We propose an\nextremely low footprint E2E ASR system for smart devices, to achieve the goal\nof satisfying resource constraints without sacrificing recognition accuracy. We\ndesign cross-layer weight sharing to improve parameter efficiency and further\nexploit model compression methods including sparsification and quantization, to\nreduce memory storage and boost decoding efficiency. We evaluate our approaches\non the public AISHELL-1 and AISHELL-2 benchmarks. On the AISHELL-2 task, the\nproposed method achieves more than 10x compression (model size reduces from 248\nto 24MB), at the cost of only minor performance loss (CER reduces from 6.49% to\n6.92%).", "published": "2021-04-06 12:44:12", "link": "http://arxiv.org/abs/2104.05784v5", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Binary Neural Network for Speaker Verification", "abstract": "Although deep neural networks are successful for many tasks in the speech\ndomain, the high computational and memory costs of deep neural networks make it\ndifficult to directly deploy highperformance Neural Network systems on\nlow-resource embedded devices. There are several mechanisms to reduce the size\nof the neural networks i.e. parameter pruning, parameter quantization, etc.\nThis paper focuses on how to apply binary neural networks to the task of\nspeaker verification. The proposed binarization of training parameters can\nlargely maintain the performance while significantly reducing storage space\nrequirements and computational costs. Experiment results show that, after\nbinarizing the Convolutional Neural Network, the ResNet34-based network\nachieves an EER of around 5% on the Voxceleb1 testing dataset and even\noutperforms the traditional real number network on the text-dependent dataset:\nXiaole while having a 32x memory saving.", "published": "2021-04-06 06:04:57", "link": "http://arxiv.org/abs/2104.02306v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MuSLCAT: Multi-Scale Multi-Level Convolutional Attention Transformer for\n  Discriminative Music Modeling on Raw Waveforms", "abstract": "In this work, we aim to improve the expressive capacity of waveform-based\ndiscriminative music networks by modeling both sequential (temporal) and\nhierarchical information in an efficient end-to-end architecture. We present\nMuSLCAT, or Multi-scale and Multi-level Convolutional Attention Transformer, a\nnovel architecture for learning robust representations of complex music tags\ndirectly from raw waveform recordings. We also introduce a lightweight variant\nof MuSLCAT called MuSLCAN, short for Multi-scale and Multi-level Convolutional\nAttention Network. Both MuSLCAT and MuSLCAN model features from multiple scales\nand levels by integrating a frontend-backend architecture. The frontend targets\ndifferent frequency ranges while modeling long-range dependencies and\nmulti-level interactions by using two convolutional attention networks with\nattention-augmented convolution (AAC) blocks. The backend dynamically\nrecalibrates multi-scale and level features extracted from the frontend by\nincorporating self-attention. The difference between MuSLCAT and MuSLCAN is\ntheir backend components. MuSLCAT's backend is a modified version of BERT.\nWhile MuSLCAN's is a simple AAC block. We validate the proposed MuSLCAT and\nMuSLCAN architectures by comparing them to state-of-the-art networks on four\nbenchmark datasets for music tagging and genre recognition. Our experiments\nshow that MuSLCAT and MuSLCAN consistently yield competitive results when\ncompared to state-of-the-art waveform-based models yet require considerably\nfewer parameters.", "published": "2021-04-06 06:17:22", "link": "http://arxiv.org/abs/2104.02309v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling", "abstract": "In this work, we introduce NU-Wave, the first neural audio upsampling model\nto produce waveforms of sampling rate 48kHz from coarse 16kHz or 24kHz inputs,\nwhile prior works could generate only up to 16kHz. NU-Wave is the first\ndiffusion probabilistic model for audio super-resolution which is engineered\nbased on neural vocoders. NU-Wave generates high-quality audio that achieves\nhigh performance in terms of signal-to-noise ratio (SNR), log-spectral distance\n(LSD), and accuracy of the ABX test. In all cases, NU-Wave outperforms the\nbaseline models despite the substantially smaller model capacity (3.0M\nparameters) than baselines (5.4-21%). The audio samples of our model are\navailable at https://mindslab-ai.github.io/nuwave, and the code will be made\navailable soon.", "published": "2021-04-06 06:52:53", "link": "http://arxiv.org/abs/2104.02321v2", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Using Voice and Biofeedback to Predict User Engagement during Product\n  Feedback Interviews", "abstract": "Capturing users' engagement is crucial for gathering feedback about the\nfeatures of a software product. In a market-driven context, current approaches\nto collect and analyze users' feedback are based on techniques leveraging\ninformation extracted from product reviews and social media. These approaches\nare hardly applicable in bespoke software development, or in contexts in which\none needs to gather information from specific users. In such cases, companies\nneed to resort to face-to-face interviews to get feedback on their products. In\nthis paper, we propose to utilize biometric data, in terms of physiological and\nvoice features, to complement interviews with information about the engagement\nof the user on the discussed product-relevant topics. We evaluate our approach\nby interviewing users while gathering their physiological data (i.e.,\nbiofeedback) using an Empatica E4 wristband, and capturing their voice through\nthe default audio-recorder of a common laptop. Our results show that we can\npredict users' engagement by training supervised machine learning algorithms on\nbiometric data (F1=0.72), and that voice features alone are sufficiently\neffective (F1=0.71). Our work contributes with one the first studies in\nrequirements engineering in which biometrics are used to identify emotions.\nThis is also the first study in software engineering that considers voice\nanalysis. The usage of voice features could be particularly helpful for\nemotion-aware requirements elicitation in remote communication, either\nperformed by human analysts or voice-based chatbots, and can also be exploited\nto support the analysis of meetings in software engineering research.", "published": "2021-04-06 10:34:36", "link": "http://arxiv.org/abs/2104.02410v5", "categories": ["cs.SE", "cs.LG", "cs.SD", "eess.AS", "68N30", "D.2.1; D.2.2"], "primary_category": "cs.SE"}
{"title": "Speaker Diarization using Two-pass Leave-One-Out Gaussian PLDA\n  Clustering of DNN Embeddings", "abstract": "Many modern systems for speaker diarization, such as the recently-developed\nVBx approach, rely on clustering of DNN speaker embeddings followed by\nresegmentation. Two problems with this approach are that the DNN is not\ndirectly optimized for this task, and the parameters need significant retuning\nfor different applications. We have recently presented progress in this\ndirection with a Leave-One-Out Gaussian PLDA (LGP) clustering algorithm and an\napproach to training the DNN such that embeddings directly optimize performance\nof this scoring method. This paper presents a new two-pass version of this\nsystem, where the second pass uses finer time resolution to significantly\nimprove overall performance. For the Callhome corpus, we achieve the first\npublished error rate below 4% without any task-dependent parameter tuning. We\nalso show significant progress towards a robust single solution for multiple\ndiarization tasks.", "published": "2021-04-06 12:52:55", "link": "http://arxiv.org/abs/2104.02469v3", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Comparing CTC and LFMMI for out-of-domain adaptation of wav2vec 2.0\n  acoustic model", "abstract": "In this work, we investigate if the wav2vec 2.0 self-supervised pretraining\nhelps mitigate the overfitting issues with connectionist temporal\nclassification (CTC) training to reduce its performance gap with flat-start\nlattice-free MMI (E2E-LFMMI) for automatic speech recognition with limited\ntraining data. Towards that objective, we use the pretrained wav2vec 2.0 BASE\nmodel and fine-tune it on three different datasets including out-of-domain\n(Switchboard) and cross-lingual (Babel) scenarios. Our results show that for\nsupervised adaptation of the wav2vec 2.0 model, both E2E-LFMMI and CTC achieve\nsimilar results; significantly outperforming the baselines trained only with\nsupervised data. Fine-tuning the wav2vec 2.0 model with E2E-LFMMI and CTC we\nobtain the following relative WER improvements over the supervised baseline\ntrained with E2E-LFMMI. We get relative improvements of 40% and 44% on the\nclean-set and 64% and 58% on the test set of Librispeech (100h) respectively.\nOn Switchboard (300h) we obtain relative improvements of 33% and 35%\nrespectively. Finally, for Babel languages, we obtain relative improvements of\n26% and 23% on Swahili (38h) and 18% and 17% on Tagalog (84h) respectively.", "published": "2021-04-06 14:56:04", "link": "http://arxiv.org/abs/2104.02558v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Localizing Visual Sounds the Hard Way", "abstract": "The objective of this work is to localize sound sources that are visible in a\nvideo without using manual annotations. Our key technical contribution is to\nshow that, by training the network to explicitly discriminate challenging image\nfragments, even for images that do contain the object emitting the sound, we\ncan significantly boost the localization performance. We do so elegantly by\nintroducing a mechanism to mine hard samples and add them to a contrastive\nlearning formulation automatically. We show that our algorithm achieves\nstate-of-the-art performance on the popular Flickr SoundNet dataset.\nFurthermore, we introduce the VGG-Sound Source (VGG-SS) benchmark, a new set of\nannotations for the recently-introduced VGG-Sound dataset, where the sound\nsources visible in each video clip are explicitly marked with bounding box\nannotations. This dataset is 20 times larger than analogous existing ones,\ncontains 5K videos spanning over 200 categories, and, differently from Flickr\nSoundNet, is video-based. On VGG-SS, we also show that our algorithm achieves\nstate-of-the-art performance against several baselines.", "published": "2021-04-06 17:38:18", "link": "http://arxiv.org/abs/2104.02691v1", "categories": ["cs.CV", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Exploring Targeted Universal Adversarial Perturbations to End-to-end ASR\n  Models", "abstract": "Although end-to-end automatic speech recognition (e2e ASR) models are widely\ndeployed in many applications, there have been very few studies to understand\nmodels' robustness against adversarial perturbations. In this paper, we explore\nwhether a targeted universal perturbation vector exists for e2e ASR models. Our\ngoal is to find perturbations that can mislead the models to predict the given\ntargeted transcript such as \"thank you\" or empty string on any input utterance.\nWe study two different attacks, namely additive and prepending perturbations,\nand their performances on the state-of-the-art LAS, CTC and RNN-T models. We\nfind that LAS is the most vulnerable to perturbations among the three models.\nRNN-T is more robust against additive perturbations, especially on long\nutterances. And CTC is robust against both additive and prepending\nperturbations. To attack RNN-T, we find prepending perturbation is more\neffective than the additive perturbation, and can mislead the models to predict\nthe same short target on utterances of arbitrary length.", "published": "2021-04-06 19:39:05", "link": "http://arxiv.org/abs/2104.02757v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Rank Microphones for Distant Speech Recognition", "abstract": "Fully exploiting ad-hoc microphone networks for distant speech recognition is\nstill an open issue. Empirical evidence shows that being able to select the\nbest microphone leads to significant improvements in recognition without any\nadditional effort on front-end processing. Current channel selection techniques\neither rely on signal, decoder or posterior-based features. Signal-based\nfeatures are inexpensive to compute but do not always correlate with\nrecognition performance. Instead decoder and posterior-based features exhibit\nbetter correlation but require substantial computational resources. In this\nwork, we tackle the channel selection problem by proposing MicRank, a learning\nto rank framework where a neural network is trained to rank the available\nchannels using directly the recognition performance on the training set. The\nproposed approach is agnostic with respect to the array geometry and type of\nrecognition back-end. We investigate different learning to rank strategies\nusing a synthetic dataset developed on purpose and the CHiME-6 data. Results\nshow that the proposed approach is able to considerably improve over previous\nselection techniques, reaching comparable and in some instances better\nperformance than oracle signal-based measures.", "published": "2021-04-06 22:39:30", "link": "http://arxiv.org/abs/2104.02819v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
