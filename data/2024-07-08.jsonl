{"title": "LLMBox: A Comprehensive Library for Large Language Models", "abstract": "To facilitate the research on large language models (LLMs), this paper\npresents a comprehensive and unified library, LLMBox, to ease the development,\nuse, and evaluation of LLMs. This library is featured with three main merits:\n(1) a unified data interface that supports the flexible implementation of\nvarious training strategies, (2) a comprehensive evaluation that covers\nextensive tasks, datasets, and models, and (3) more practical consideration,\nespecially on user-friendliness and efficiency. With our library, users can\neasily reproduce existing methods, train new models, and conduct comprehensive\nperformance comparisons. To rigorously test LLMBox, we conduct extensive\nexperiments in a diverse coverage of evaluation settings, and experimental\nresults demonstrate the effectiveness and efficiency of our library in\nsupporting various implementations related to LLMs. The detailed introduction\nand usage guidance can be found at https://github.com/RUCAIBox/LLMBox.", "published": "2024-07-08 02:39:33", "link": "http://arxiv.org/abs/2407.05563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-world Multi-label Text Classification with Extremely Weak\n  Supervision", "abstract": "We study open-world multi-label text classification under extremely weak\nsupervision (XWS), where the user only provides a brief description for\nclassification objectives without any labels or ground-truth label space.\nSimilar single-label XWS settings have been explored recently, however, these\nmethods cannot be easily adapted for multi-label. We observe that (1) most\ndocuments have a dominant class covering the majority of content and (2)\nlong-tail labels would appear in some documents as a dominant class. Therefore,\nwe first utilize the user description to prompt a large language model (LLM)\nfor dominant keyphrases of a subset of raw documents, and then construct a\n(initial) label space via clustering. We further apply a zero-shot multi-label\nclassifier to locate the documents with small top predicted scores, so we can\nrevisit their dominant keyphrases for more long-tail labels. We iterate this\nprocess to discover a comprehensive label space and construct a multi-label\nclassifier as a novel method, X-MLClass. X-MLClass exhibits a remarkable\nincrease in ground-truth label space coverage on various datasets, for example,\na 40% improvement on the AAPD dataset over topic modeling and keyword\nextraction methods. Moreover, X-MLClass achieves the best end-to-end\nmulti-label classification accuracy.", "published": "2024-07-08 04:52:49", "link": "http://arxiv.org/abs/2407.05609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieved In-Context Principles from Previous Mistakes", "abstract": "In-context learning (ICL) has been instrumental in adapting Large Language\nModels (LLMs) to downstream tasks using correct input-output examples. Recent\nadvances have attempted to improve model performance through principles derived\nfrom mistakes, yet these approaches suffer from lack of customization and\ninadequate error coverage. To address these limitations, we propose Retrieved\nIn-Context Principles (RICP), a novel teacher-student framework. In RICP, the\nteacher model analyzes mistakes from the student model to generate reasons and\ninsights for preventing similar mistakes. These mistakes are clustered based on\ntheir underlying reasons for developing task-level principles, enhancing the\nerror coverage of principles. During inference, the most relevant mistakes for\neach question are retrieved to create question-level principles, improving the\ncustomization of the provided guidance. RICP is orthogonal to existing\nprompting methods and does not require intervention from the teacher model\nduring inference. Experimental results across seven reasoning benchmarks reveal\nthat RICP effectively enhances performance when applied to various prompting\nstrategies.", "published": "2024-07-08 07:32:26", "link": "http://arxiv.org/abs/2407.05682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Factuality and Diversity Reconciled Decoding Method for\n  Knowledge-Grounded Dialogue Generation", "abstract": "Grounding external knowledge can enhance the factuality of responses in\ndialogue generation. However, excessive emphasis on it might result in the lack\nof engaging and diverse expressions. Through the introduction of randomness in\nsampling, current approaches can increase the diversity. Nevertheless, such\nsampling method could undermine the factuality in dialogue generation. In this\nstudy, to discover a solution for advancing creativity without relying on\nquestionable randomness and to subtly reconcile the factuality and diversity\nwithin the source-grounded paradigm, a novel method named DoGe is proposed.\nDoGe can dynamically alternate between the utilization of internal parameter\nknowledge and external source knowledge based on the model's factual\nconfidence. Extensive experiments on three widely-used datasets show that DoGe\ncan not only enhance response diversity but also maintain factuality, and it\nsignificantly surpasses other various decoding strategy baselines.", "published": "2024-07-08 08:23:11", "link": "http://arxiv.org/abs/2407.05718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation", "abstract": "Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.\nSpecifically, we construct multi-turn dialogues through a three-step pipeline\ncomprising multi-turn QA generation, evidence judgment, and dialogue\nrefinement. We augment this process with real-world psychological case\nbackgrounds extracted from online platforms, enhancing the relevance and\napplicability of the generated data. Additionally, to compare the performance\nof PsycoLLM with other LLMs, we develop a comprehensive psychological benchmark\nbased on authoritative psychological counseling examinations in China, which\nincludes assessments of professional ethics, theoretical proficiency, and case\nanalysis. The experimental results on the benchmark illustrate the\neffectiveness of PsycoLLM, which demonstrates superior performance compared to\nother LLMs.", "published": "2024-07-08 08:25:56", "link": "http://arxiv.org/abs/2407.05721v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Study of Symmetrical Reasoning in Conversational Chatbots", "abstract": "This work explores the capability of conversational chatbots powered by large\nlanguage models (LLMs), to understand and characterize predicate symmetry, a\ncognitive linguistic function traditionally believed to be an inherent human\ntrait. Leveraging in-context learning (ICL), a paradigm shift enabling chatbots\nto learn new tasks from prompts without re-training, we assess the symmetrical\nreasoning of five chatbots: ChatGPT 4, Huggingface chat AI, Microsoft's Copilot\nAI, LLaMA through Perplexity, and Gemini Advanced. Using the Symmetry Inference\nSentence (SIS) dataset by Tanchip et al. (2020), we compare chatbot responses\nagainst human evaluations to gauge their understanding of predicate symmetry.\nExperiment results reveal varied performance among chatbots, with some\napproaching human-like reasoning capabilities. Gemini, for example, reaches a\ncorrelation of 0.85 with human scores, while providing a sounding justification\nfor each symmetry evaluation. This study underscores the potential and\nlimitations of LLMs in mirroring complex cognitive processes as symmetrical\nreasoning.", "published": "2024-07-08 08:38:43", "link": "http://arxiv.org/abs/2407.05734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Multilingual Large Language Models Mitigate Stereotype Bias?", "abstract": "While preliminary findings indicate that multilingual LLMs exhibit reduced\nbias compared to monolingual ones, a comprehensive understanding of the effect\nof multilingual training on bias mitigation, is lacking. This study addresses\nthis gap by systematically training six LLMs of identical size (2.6B\nparameters) and architecture: five monolingual models (English, German, French,\nItalian, and Spanish) and one multilingual model trained on an equal\ndistribution of data across these languages, all using publicly available data.\nTo ensure robust evaluation, standard bias benchmarks were automatically\ntranslated into the five target languages and verified for both translation\nquality and bias preservation by human annotators. Our results consistently\ndemonstrate that multilingual training effectively mitigates bias. Moreover, we\nobserve that multilingual models achieve not only lower bias but also superior\nprediction accuracy when compared to monolingual models with the same amount of\ntraining data, model architecture, and size.", "published": "2024-07-08 08:46:50", "link": "http://arxiv.org/abs/2407.05740v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Understand Layout", "abstract": "Large language models (LLMs) demonstrate extraordinary abilities in a wide\nrange of natural language processing (NLP) tasks. In this paper, we show that,\nbeyond text understanding capability, LLMs are capable of processing text\nlayouts that are denoted by spatial markers. They are able to answer questions\nthat require explicit spatial perceiving and reasoning, while a drastic\nperformance drop is observed when the spatial markers from the original data\nare excluded. We perform a series of experiments with the GPT-3.5, Baichuan2,\nLlama2 and ChatGLM3 models on various types of layout-sensitive datasets for\nfurther analysis. The experimental results reveal that the layout understanding\nability of LLMs is mainly introduced by the coding data for pretraining, which\nis further enhanced at the instruction-tuning stage. In addition, layout\nunderstanding can be enhanced by integrating low-cost, auto-generated data\napproached by a novel text game. Finally, we show that layout understanding\nability is beneficial for building efficient visual question-answering (VQA)\nsystems.", "published": "2024-07-08 09:03:12", "link": "http://arxiv.org/abs/2407.05750v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perceptions to Beliefs: Exploring Precursory Inferences for Theory of\n  Mind in Large Language Models", "abstract": "While humans naturally develop theory of mind (ToM), the capability to\nunderstand other people's mental states and beliefs, state-of-the-art large\nlanguage models (LLMs) underperform on simple ToM benchmarks. We posit that we\ncan extend our understanding of LLMs' ToM abilities by evaluating key human ToM\nprecursors$-$perception inference and perception-to-belief inference$-$in LLMs.\nWe introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate these\nprecursory inferences for ToM in LLMs by annotating characters' perceptions on\nToMi and FANToM, respectively. Our evaluation of eight state-of-the-art LLMs\nreveals that the models generally perform well in perception inference while\nexhibiting limited capability in perception-to-belief inference (e.g., lack of\ninhibitory control). Based on these results, we present PercepToM, a novel ToM\nmethod leveraging LLMs' strong perception inference capability while\nsupplementing their limited perception-to-belief inference. Experimental\nresults demonstrate that PercepToM significantly enhances LLM's performance,\nespecially in false belief scenarios.", "published": "2024-07-08 14:58:29", "link": "http://arxiv.org/abs/2407.06004v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PAS: Data-Efficient Plug-and-Play Prompt Augmentation System", "abstract": "In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.", "published": "2024-07-08 15:25:33", "link": "http://arxiv.org/abs/2407.06027v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MST5 -- Multilingual Question Answering over Knowledge Graphs", "abstract": "Knowledge Graph Question Answering (KGQA) simplifies querying vast amounts of\nknowledge stored in a graph-based model using natural language. However, the\nresearch has largely concentrated on English, putting non-English speakers at a\ndisadvantage. Meanwhile, existing multilingual KGQA systems face challenges in\nachieving performance comparable to English systems, highlighting the\ndifficulty of generating SPARQL queries from diverse languages. In this\nresearch, we propose a simplified approach to enhance multilingual KGQA systems\nby incorporating linguistic context and entity information directly into the\nprocessing pipeline of a language model. Unlike existing methods that rely on\nseparate encoders for integrating auxiliary information, our strategy leverages\na single, pretrained multilingual transformer-based language model to manage\nboth the primary input and the auxiliary data. Our methodology significantly\nimproves the language model's ability to accurately convert a natural language\nquery into a relevant SPARQL query. It demonstrates promising results on the\nmost recent QALD datasets, namely QALD-9-Plus and QALD-10. Furthermore, we\nintroduce and evaluate our approach on Chinese and Japanese, thereby expanding\nthe language diversity of the existing datasets.", "published": "2024-07-08 15:37:51", "link": "http://arxiv.org/abs/2407.06041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in\n  the Era of Large Language Models", "abstract": "The remarkable success of Large Language Models (LLMs) has ushered natural\nlanguage processing (NLP) research into a new era. Despite their diverse\ncapabilities, LLMs trained on different corpora exhibit varying strengths and\nweaknesses, leading to challenges in maximizing their overall efficiency and\nversatility. To address these challenges, recent studies have explored\ncollaborative strategies for LLMs. This paper provides a comprehensive overview\nof this emerging research area, highlighting the motivation behind such\ncollaborations. Specifically, we categorize collaborative strategies into three\nprimary approaches: Merging, Ensemble, and Cooperation. Merging involves\nintegrating multiple LLMs in the parameter space. Ensemble combines the outputs\nof various LLMs. Cooperation} leverages different LLMs to allow full play to\ntheir diverse capabilities for specific tasks. We provide in-depth\nintroductions to these methods from different perspectives and discuss their\npotential applications. Additionally, we outline future research directions,\nhoping this work will catalyze further studies on LLM collaborations and paving\nthe way for advanced NLP applications.", "published": "2024-07-08 16:29:08", "link": "http://arxiv.org/abs/2407.06089v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Language Model Rationality with Bi-Directional Deliberation\n  Reasoning", "abstract": "This paper introduces BI-Directional DEliberation Reasoning (BIDDER), a novel\nreasoning approach to enhance the decision rationality of language models.\nTraditional reasoning methods typically rely on historical information and\nemploy uni-directional (left-to-right) reasoning strategy. This lack of\nbi-directional deliberation reasoning results in limited awareness of potential\nfuture outcomes and insufficient integration of historical context, leading to\nsuboptimal decisions. BIDDER addresses this gap by incorporating principles of\nrational decision-making, specifically managing uncertainty and predicting\nexpected utility. Our approach involves three key processes: Inferring hidden\nstates to represent uncertain information in the decision-making process from\nhistorical data; Using these hidden states to predict future potential states\nand potential outcomes; Integrating historical information (past contexts) and\nlong-term outcomes (future contexts) to inform reasoning. By leveraging\nbi-directional reasoning, BIDDER ensures thorough exploration of both past and\nfuture contexts, leading to more informed and rational decisions. We tested\nBIDDER's effectiveness in two well-defined scenarios: Poker (Limit Texas\nHold'em) and Negotiation. Our experiments demonstrate that BIDDER significantly\nimproves the decision-making capabilities of LLMs and LLM agents.", "published": "2024-07-08 16:48:48", "link": "http://arxiv.org/abs/2407.06112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personality Analysis for Social Media Users using Arabic language and\n  its Effect on Sentiment Analysis", "abstract": "Social media is heading towards more and more personalization, where\nindividuals reveal their beliefs, interests, habits, and activities, simply\noffering glimpses into their personality traits. This study, explores the\ncorrelation between the use of Arabic language on twitter, personality traits\nand its impact on sentiment analysis. We indicated the personality traits of\nusers based on the information extracted from their profile activities, and the\ncontent of their tweets. Our analysis incorporated linguistic features, profile\nstatistics (including gender, age, bio, etc.), as well as additional features\nlike emoticons. To obtain personality data, we crawled the timelines and\nprofiles of users who took the 16personalities test in Arabic on\n16personalities.com. Our dataset, \"AraPers\", comprised 3,250 users who shared\ntheir personality results on twitter. We implemented various machine learning\ntechniques, to reveal personality traits and developed a dedicated model for\nthis purpose, achieving a 74.86% accuracy rate with BERT, analysis of this\ndataset proved that linguistic features, profile features and derived model can\nbe used to differentiate between different personality traits. Furthermore, our\nfindings demonstrated that personality affect sentiment in social media. This\nresearch contributes to the ongoing efforts in developing robust understanding\nof the relation between human behaviour on social media and personality\nfeatures for real-world applications, such as political discourse analysis, and\npublic opinion tracking.", "published": "2024-07-08 18:27:54", "link": "http://arxiv.org/abs/2407.06314v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When in Doubt, Cascade: Towards Building Efficient and Capable\n  Guardrails", "abstract": "Large language models (LLMs) have convincing performance in a variety of\ndownstream tasks. However, these systems are prone to generating undesirable\noutputs such as harmful and biased text. In order to remedy such generations,\nthe development of guardrail (or detector) models has gained traction.\nMotivated by findings from developing a detector for social bias, we adopt the\nnotion of a use-mention distinction - which we identified as the primary source\nof under-performance in the preliminary versions of our social bias detector.\nArmed with this information, we describe a fully extensible and reproducible\nsynthetic data generation pipeline which leverages taxonomy-driven instructions\nto create targeted and labeled data. Using this pipeline, we generate over 300K\nunique contrastive samples and provide extensive experiments to systematically\nevaluate performance on a suite of open source datasets. We show that our\nmethod achieves competitive performance with a fraction of the cost in compute\nand offers insight into iteratively developing efficient and capable guardrail\nmodels.\n  Warning: This paper contains examples of text which are toxic, biased, and\npotentially harmful.", "published": "2024-07-08 18:39:06", "link": "http://arxiv.org/abs/2407.06323v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CharSS: Character-Level Transformer Model for Sanskrit Word Segmentation", "abstract": "Subword tokens in Indian languages inherently carry meaning, and isolating\nthem can enhance NLP tasks, making sub-word segmentation a crucial process.\nSegmenting Sanskrit and other Indian languages into subtokens is not\nstraightforward, as it may include sandhi, which may lead to changes in the\nword boundaries. We propose a new approach of utilizing a Character-level\nTransformer model for Sanskrit Word Segmentation (CharSS). We perform\nexperiments on three benchmark datasets to compare the performance of our\nmethod against existing methods. On the UoH+SandhiKosh dataset, our method\noutperforms the current state-of-the-art system by an absolute gain of 6.72\npoints in split prediction accuracy. On the hackathon dataset, our method\nachieves a gain of 2.27 points over the current SOTA system in terms of perfect\nmatch metric. We also propose a use-case of Sanskrit-based segments for a\nlinguistically informed translation of technical terms to lexically similar\nlow-resource Indian languages. In two separate experimental settings for this\ntask, we achieve an average improvement of 8.46 and 6.79 chrF++ scores,\nrespectively.", "published": "2024-07-08 18:50:13", "link": "http://arxiv.org/abs/2407.06331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data, Data Everywhere: A Guide for Pretraining Dataset Construction", "abstract": "The impressive capabilities of recent language models can be largely\nattributed to the multi-trillion token pretraining datasets that they are\ntrained on. However, model developers fail to disclose their construction\nmethodology which has lead to a lack of open information on how to develop\neffective pretraining sets. To address this issue, we perform the first\nsystematic study across the entire pipeline of pretraining set construction.\nFirst, we run ablations on existing techniques for pretraining set development\nto identify which methods translate to the largest gains in model accuracy on\ndownstream evaluations. Then, we categorize the most widely used data source,\nweb crawl snapshots, across the attributes of toxicity, quality, type of\nspeech, and domain. Finally, we show how such attribute information can be used\nto further refine and improve the quality of a pretraining set. These findings\nconstitute an actionable set of steps that practitioners can use to develop\nhigh quality pretraining sets.", "published": "2024-07-08 20:47:58", "link": "http://arxiv.org/abs/2407.06380v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Gendered Stereotypes in Emotional Attributes for\n  Bangla in Multilingual Large Language Models", "abstract": "The influence of Large Language Models (LLMs) is rapidly growing, automating\nmore jobs over time. Assessing the fairness of LLMs is crucial due to their\nexpanding impact. Studies reveal the reflection of societal norms and biases in\nLLMs, which creates a risk of propagating societal stereotypes in downstream\ntasks. Many studies on bias in LLMs focus on gender bias in various NLP\napplications. However, there's a gap in research on bias in emotional\nattributes, despite the close societal link between emotion and gender. This\ngap is even larger for low-resource languages like Bangla. Historically, women\nare associated with emotions like empathy, fear, and guilt, while men are\nlinked to anger, bravado, and authority. This pattern reflects societal norms\nin Bangla-speaking regions. We offer the first thorough investigation of\ngendered emotion attribution in Bangla for both closed and open source LLMs in\nthis work. Our aim is to elucidate the intricate societal relationship between\ngender and emotion specifically within the context of Bangla. We have been\nsuccessful in showing the existence of gender bias in the context of emotions\nin Bangla through analytical methods and also show how emotion attribution\nchanges on the basis of gendered role selection in LLMs. All of our resources\nincluding code and data are made publicly available to support future research\non Bangla NLP.\n  Warning: This paper contains explicit stereotypical statements that many may\nfind offensive.", "published": "2024-07-08 22:22:15", "link": "http://arxiv.org/abs/2407.06432v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ISPO: An Integrated Ontology of Symptom Phenotypes for Semantic\n  Integration of Traditional Chinese Medical Data", "abstract": "Symptom phenotypes are one of the key types of manifestations for diagnosis\nand treatment of various disease conditions. However, the diversity of symptom\nterminologies is one of the major obstacles hindering the analysis and\nknowledge sharing of various types of symptom-related medical data particularly\nin the fields of Traditional Chinese Medicine (TCM). Objective: This study\naimed to construct an Integrated Ontology of symptom phenotypes (ISPO) to\nsupport the data mining of Chinese EMRs and real-world study in TCM field.\nMethods: To construct an integrated ontology of symptom phenotypes (ISPO), we\nmanually annotated classical TCM textbooks and large-scale Chinese electronic\nmedical records (EMRs) to collect symptom terms with support from a medical\ntext annotation system. Furthermore, to facilitate the semantic\ninteroperability between different terminologies, we incorporated public\navailable biomedical vocabularies by manual mapping between Chinese terms and\nEnglish terms with cross-references to source vocabularies. In addition, we\nevaluated the ISPO using independent clinical EMRs to provide a high-usable\nmedical ontology for clinical data analysis. Results: By integrating 78,696\ninpatient cases of EMRs, 5 biomedical vocabularies, 21 TCM books and\ndictionaries, ISPO provides 3,147 concepts, 23,475 terms, and 55,552 definition\nor contextual texts. Adhering to the taxonomical structure of the related\nanatomical systems of symptom phenotypes, ISPO provides 12 top-level categories\nand 79 middle-level sub-categories. The validation of data analysis showed the\nISPO has a coverage rate of 95.35%, 98.53% and 92.66% for symptom terms with\noccurrence rates of 0.5% in additional three independent curated clinical\ndatasets, which can demonstrate the significant value of ISPO in mapping\nclinical terms to ontologies.", "published": "2024-07-08 15:23:50", "link": "http://arxiv.org/abs/2407.12851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Debunking of Climate Misinformation", "abstract": "Misinformation about climate change causes numerous negative impacts,\nnecessitating corrective responses. Psychological research has offered various\nstrategies for reducing the influence of climate misinformation, such as the\nfact-myth-fallacy-fact-structure. However, practically implementing corrective\ninterventions at scale represents a challenge. Automatic detection and\ncorrection of misinformation offers a solution to the misinformation problem.\nThis study documents the development of large language models that accept as\ninput a climate myth and produce a debunking that adheres to the\nfact-myth-fallacy-fact (``truth sandwich'') structure, by incorporating\ncontrarian claim classification and fallacy detection into an LLM prompting\nframework. We combine open (Mixtral, Palm2) and proprietary (GPT-4) LLMs with\nprompting strategies of varying complexity. Experiments reveal promising\nperformance of GPT-4 and Mixtral if combined with structured prompts. We\nidentify specific challenges of debunking generation and human evaluation, and\nmap out avenues for future work. We release a dataset of high-quality\ntruth-sandwich debunkings, source code and a demo of the debunking system.", "published": "2024-07-08 04:21:58", "link": "http://arxiv.org/abs/2407.05599v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Multi-label Learning with Random Circular Vectors", "abstract": "The extreme multi-label classification~(XMC) task involves learning a\nclassifier that can predict from a large label set the most relevant subset of\nlabels for a data instance. While deep neural networks~(DNNs) have demonstrated\nremarkable success in XMC problems, the task is still challenging because it\nmust deal with a large number of output labels, which make the DNN training\ncomputationally expensive. This paper addresses the issue by exploring the use\nof random circular vectors, where each vector component is represented as a\ncomplex amplitude. In our framework, we can develop an output layer and loss\nfunction of DNNs for XMC by representing the final output layer as a fully\nconnected layer that directly predicts a low-dimensional circular vector\nencoding a set of labels for a data instance. We conducted experiments on\nsynthetic datasets to verify that circular vectors have better label encoding\ncapacity and retrieval ability than normal real-valued vectors. Then, we\nconducted experiments on actual XMC datasets and found that these appealing\nproperties of circular vectors contribute to significant improvements in task\nperformance compared with a previous model using random real-valued vectors,\nwhile reducing the size of the output layers by up to 99%.", "published": "2024-07-08 06:29:46", "link": "http://arxiv.org/abs/2407.05656v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Pruning Large Language Models to Intra-module Low-rank Architecture with\n  Transitional Activations", "abstract": "Structured pruning fundamentally reduces computational and memory overheads\nof large language models (LLMs) and offers a feasible solution for end-side LLM\ndeployment. Structurally pruned models remain dense and high-precision, highly\ncompatible with further tuning and compression. However, as the coarse-grained\nstructured pruning poses large damage to the highly interconnected model,\nachieving a high compression ratio for scaled-up LLMs remains a challenge. In\nthis paper, we introduce a task-agnostic structured pruning approach coupled\nwith a compact Transformer architecture design. The proposed approach, named\nTransAct, reduces transitional activations inside multi-head attention (MHA)\nand multi-layer perceptron (MLP) modules, while preserving the inter-module\nactivations that are sensitive to perturbations. Hence, the LLM is pruned into\nan intra-module low-rank architecture, significantly reducing weights, KV Cache\nand attention computation. TransAct is implemented on the LLaMA model and\nevaluated on downstream benchmarks. Results verify the optimality of our\napproach at high compression with respect to both efficiency and performance.\nFurther, ablation studies reveal the strength of activation-guided iterative\npruning and provide experimental analysis on the redundancy of MHA and MLP\nmodules.", "published": "2024-07-08 07:45:38", "link": "http://arxiv.org/abs/2407.05690v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative\n  Judgment Approach Based on Rater Cognition", "abstract": "Large Language Models (LLMs) have shown promise in Automated Essay Scoring\n(AES), but their zero-shot and few-shot performance often falls short compared\nto state-of-the-art models and human raters. However, fine-tuning LLMs for each\nspecific task is impractical due to the variety of essay prompts and rubrics\nused in real-world educational contexts. This study proposes a novel approach\ncombining LLMs and Comparative Judgment (CJ) for AES, using zero-shot prompting\nto choose between two essays. We demonstrate that a CJ method surpasses\ntraditional rubric-based scoring in essay scoring using LLMs.", "published": "2024-07-08 08:37:00", "link": "http://arxiv.org/abs/2407.05733v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "When is the consistent prediction likely to be a correct prediction?", "abstract": "Self-consistency (Wang et al., 2023) suggests that the most consistent answer\nobtained through large language models (LLMs) is more likely to be correct. In\nthis paper, we challenge this argument and propose a nuanced correction. Our\nobservations indicate that consistent answers derived through more computation\ni.e. longer reasoning texts, rather than simply the most consistent answer\nacross all outputs, are more likely to be correct. This is predominantly\nbecause we demonstrate that LLMs can autonomously produce chain-of-thought\n(CoT) style reasoning with no custom prompts merely while generating longer\nresponses, which lead to consistent predictions that are more accurate. In the\nzero-shot setting, by sampling Mixtral-8x7B model multiple times and\nconsidering longer responses, we achieve 86% of its self-consistency\nperformance obtained through zero-shot CoT prompting on the GSM8K and\nMultiArith datasets. Finally, we demonstrate that the probability of LLMs\ngenerating a longer response is quite low, highlighting the need for decoding\nstrategies conditioned on output length.", "published": "2024-07-08 09:37:27", "link": "http://arxiv.org/abs/2407.05778v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Judicial Entity Extraction: A Comparative\n  Study", "abstract": "Domain-specific Entity Recognition holds significant importance in legal\ncontexts, serving as a fundamental task that supports various applications such\nas question-answering systems, text summarization, machine translation,\nsentiment analysis, and information retrieval specifically within case law\ndocuments. Recent advancements have highlighted the efficacy of Large Language\nModels in natural language processing tasks, demonstrating their capability to\naccurately detect and classify domain-specific facts (entities) from\nspecialized texts like clinical and financial documents. This research\ninvestigates the application of Large Language Models in identifying\ndomain-specific entities (e.g., courts, petitioner, judge, lawyer, respondents,\nFIR nos.) within case law documents, with a specific focus on their aptitude\nfor handling domain-specific language complexity and contextual variations. The\nstudy evaluates the performance of state-of-the-art Large Language Model\narchitectures, including Large Language Model Meta AI 3, Mistral, and Gemma, in\nthe context of extracting judicial facts tailored to Indian judicial texts.\nMistral and Gemma emerged as the top-performing models, showcasing balanced\nprecision and recall crucial for accurate entity identification. These findings\nconfirm the value of Large Language Models in judicial documents and\ndemonstrate how they can facilitate and quicken scientific research by\nproducing precise, organised data outputs that are appropriate for in-depth\nexamination.", "published": "2024-07-08 09:49:03", "link": "http://arxiv.org/abs/2407.05786v1", "categories": ["cs.CL", "cs.AI", "I.2.1"], "primary_category": "cs.CL"}
{"title": "An Empirical Comparison of Vocabulary Expansion and Initialization\n  Approaches for Language Models", "abstract": "Language Models (LMs) excel in natural language processing tasks for English\nbut show reduced performance in most other languages. This problem is commonly\ntackled by continually pre-training and fine-tuning these models for said\nlanguages. A significant issue in this process is the limited vocabulary\ncoverage in the original model's tokenizer, leading to inadequate\nrepresentation of new languages and necessitating an expansion of the\ntokenizer. The initialization of the embeddings corresponding to new vocabulary\nitems presents a further challenge. Current strategies require cross-lingual\nembeddings and lack a solid theoretical foundation as well as comparisons with\nstrong baselines. In this paper, we first establish theoretically that\ninitializing within the convex hull of existing embeddings is a good\ninitialization, followed by a novel but simple approach, Constrained Word2Vec\n(CW2V), which does not require cross-lingual embeddings. Our study evaluates\ndifferent initialization methods for expanding RoBERTa and LLaMA 2 across four\nlanguages and five tasks. The results show that CW2V performs equally well or\neven better than more advanced techniques. Additionally, simpler approaches\nlike multivariate initialization perform on par with these advanced methods\nindicating that efficient large-scale multilingual continued pretraining can be\nachieved even with simpler initialization methods. We release our code publicly\n(https://github.com/AI4Bharat/VocabAdaptation_LLM/tree/CW2V).", "published": "2024-07-08 11:38:49", "link": "http://arxiv.org/abs/2407.05841v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge\n  Graph-based False Premise Questions", "abstract": "Recent studies have demonstrated that large language models (LLMs) are\nsusceptible to being misled by false premise questions (FPQs), leading to\nerrors in factual knowledge, know as factuality hallucination. Existing\nbenchmarks that assess this vulnerability primarily rely on manual\nconstruction, resulting in limited scale and lack of scalability. In this work,\nwe introduce an automated, scalable pipeline to create FPQs based on knowledge\ngraphs (KGs). The first step is modifying true triplets extracted from KGs to\ncreate false premises. Subsequently, utilizing the state-of-the-art\ncapabilities of GPTs, we generate semantically rich FPQs. Based on the proposed\nmethod, we present a comprehensive benchmark, the Knowledge Graph-based False\nPremise Questions (KG-FPQ), which contains approximately 178k FPQs across three\nknowledge domains, at six levels of confusability, and in two task formats.\nUsing KG-FPQ, we conduct extensive evaluations on several representative LLMs\nand provide valuable insights. The KG-FPQ dataset and code are available\nat~https://github.com/yanxuzhu/KG-FPQ.", "published": "2024-07-08 12:31:03", "link": "http://arxiv.org/abs/2407.05868v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Affordances-Oriented Planning using Foundation Models for Continuous\n  Vision-Language Navigation", "abstract": "LLM-based agents have demonstrated impressive zero-shot performance in\nvision-language navigation (VLN) task. However, existing LLM-based methods\noften focus only on solving high-level task planning by selecting nodes in\npredefined navigation graphs for movements, overlooking low-level control in\nnavigation scenarios. To bridge this gap, we propose AO-Planner, a novel\nAffordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates\nvarious foundation models to achieve affordances-oriented low-level motion\nplanning and high-level decision-making, both performed in a zero-shot setting.\nSpecifically, we employ a Visual Affordances Prompting (VAP) approach, where\nthe visible ground is segmented by SAM to provide navigational affordances,\nbased on which the LLM selects potential candidate waypoints and plans\nlow-level paths towards selected waypoints. We further propose a high-level\nPathAgent which marks planned paths into the image input and reasons the most\nprobable path by comprehending all environmental information. Finally, we\nconvert the selected path into 3D coordinates using camera intrinsic parameters\nand depth information, avoiding challenging 3D predictions for LLMs.\nExperiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner\nachieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our\nmethod can also serve as a data annotator to obtain pseudo-labels, distilling\nits waypoint prediction ability into a learning-based predictor. This new\npredictor does not require any waypoint data from the simulator and achieves\n47% SR competing with supervised methods. We establish an effective connection\nbetween LLM and 3D world, presenting novel prospects for employing foundation\nmodels in low-level motion control.", "published": "2024-07-08 12:52:46", "link": "http://arxiv.org/abs/2407.05890v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Towards Optimizing and Evaluating a Retrieval Augmented QA Chatbot using\n  LLMs with Human in the Loop", "abstract": "Large Language Models have found application in various mundane and\nrepetitive tasks including Human Resource (HR) support. We worked with the\ndomain experts of SAP SE to develop an HR support chatbot as an efficient and\neffective tool for addressing employee inquiries. We inserted a\nhuman-in-the-loop in various parts of the development cycles such as dataset\ncollection, prompt optimization, and evaluation of generated output. By\nenhancing the LLM-driven chatbot's response quality and exploring alternative\nretrieval methods, we have created an efficient, scalable, and flexible tool\nfor HR professionals to address employee inquiries effectively. Our experiments\nand evaluation conclude that GPT-4 outperforms other models and can overcome\ninconsistencies in data through internal reasoning capabilities. Additionally,\nthrough expert analysis, we infer that reference-free evaluation metrics such\nas G-Eval and Prometheus demonstrate reliability closely aligned with that of\nhuman evaluation.", "published": "2024-07-08 13:32:14", "link": "http://arxiv.org/abs/2407.05925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation\n  Capabilities Beyond 100 Languages", "abstract": "Large Language Models (LLMs) demonstrate remarkable translation capabilities\nin high-resource language tasks, yet their performance in low-resource\nlanguages is hindered by insufficient multilingual data during pre-training. To\naddress this, we conduct extensive multilingual continual pre-training on the\nLLaMA series models, enabling translation support across more than 100\nlanguages. Through a comprehensive analysis of training strategies, such as\nvocabulary expansion and data augmentation, we develop LLaMAX. Remarkably,\nwithout sacrificing its generalization ability, LLaMAX achieves significantly\nhigher translation performance compared to existing open-source LLMs (by more\nthan 10 spBLEU points) and performs on-par with specialized translation model\n(M2M-100-12B) on the Flores-101 benchmark. Extensive experiments indicate that\nLLaMAX can serve as a robust multilingual foundation model. The code\n\\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} and the models\n\\footnote{\\url{https://huggingface.co/LLaMAX/.}} are publicly available.", "published": "2024-07-08 14:18:28", "link": "http://arxiv.org/abs/2407.05975v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Igea: a Decoder-Only Language Model for Biomedical Text Generation in\n  Italian", "abstract": "The development of domain-specific language models has significantly advanced\nnatural language processing applications in various specialized fields,\nparticularly in biomedicine. However, the focus has largely been on\nEnglish-language models, leaving a gap for less-resourced languages such as\nItalian. This paper introduces Igea, the first decoder-only language model\ndesigned explicitly for biomedical text generation in Italian. Built on the\nMinerva model and continually pretrained on a diverse corpus of Italian medical\ntexts, Igea is available in three model sizes: 350 million, 1 billion, and 3\nbillion parameters. The models aim to balance computational efficiency and\nperformance, addressing the challenges of managing the peculiarities of medical\nterminology in Italian. We evaluate Igea using a mix of in-domain biomedical\ncorpora and general-purpose benchmarks, highlighting its efficacy and retention\nof general knowledge even after the domain-specific training. This paper\ndiscusses the model's development and evaluation, providing a foundation for\nfuture advancements in Italian biomedical NLP.", "published": "2024-07-08 15:04:21", "link": "http://arxiv.org/abs/2407.06011v1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Distilling System 2 into System 1", "abstract": "Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well.", "published": "2024-07-08 15:17:46", "link": "http://arxiv.org/abs/2407.06023v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vision-Braille: An End-to-End Tool for Chinese Braille Image-to-Text\n  Translation", "abstract": "Visually impaired people are a large group who can only use braille for\nreading and writing. However, the lack of special educational resources is the\nbottleneck for educating them. Educational equity is a reflection of the level\nof social civilization, cultural equality, and individual dignity. Facilitating\nand improving lifelong learning channels for the visually impaired is of great\nsignificance. Their written braille homework or exam papers cannot be\nunderstood by sighted teachers, because of the lack of a highly accurate\nbraille translation system, especially in Chinese which has tone marks. braille\nwriters often omit tone marks to save space, leading to confusion when braille\nwith the same consonants and vowels is translated into Chinese. Previous\nalgorithms were insufficient in extracting contextual information, resulting in\nlow accuracy of braille translations into Chinese. This project informatively\nfine-tuned the mT5 model with an Encoder-decoder architecture for braille to\nChinese character conversion. This research created a training set of braille\nand corresponding Chinese text from the Leipzig Corpora. This project\nsignificantly reduced the confusion in braille, achieving $62.4$ and $62.3$\nBLEU scores in the validation and test sets, with a curriculum learning\nfine-tuning method. By incorporating the braille recognition algorithm, this\nproject is the first publicly available braille translation system and can\nbenefit lots of visually impaired students and families who are preparing for\nthe Chinese College Test and help to propel their college dreams in the future.\nThere is a demo on our homepage\\footnote{\\url{https://vision-braille.com/}}.", "published": "2024-07-08 15:51:37", "link": "http://arxiv.org/abs/2407.06048v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "From Loops to Oops: Fallback Behaviors of Language Models Under\n  Uncertainty", "abstract": "Large language models (LLMs) often exhibit undesirable behaviors, such as\nhallucinations and sequence repetitions. We propose to view these behaviors as\nfallbacks that models exhibit under epistemic uncertainty, and investigate the\nconnection between them. We categorize fallback behaviors - sequence\nrepetitions, degenerate text, and hallucinations - and extensively analyze them\nin models from the same family that differ by the amount of pretraining tokens,\nparameter count, or the inclusion of instruction-following training. Our\nexperiments reveal a clear and consistent ordering of fallback behaviors,\nacross all these axes: the more advanced an LLM is (i.e., trained on more\ntokens, has more parameters, or instruction-tuned), its fallback behavior\nshifts from sequence repetitions, to degenerate text, and then to\nhallucinations. Moreover, the same ordering is observed during the generation\nof a single sequence, even for the best-performing models; as uncertainty\nincreases, models shift from generating hallucinations to producing degenerate\ntext and finally sequence repetitions. Lastly, we demonstrate that while common\ndecoding techniques, such as random sampling, alleviate unwanted behaviors like\nsequence repetitions, they increase harder-to-detect hallucinations.", "published": "2024-07-08 16:13:42", "link": "http://arxiv.org/abs/2407.06071v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Epistemological Bias As a Means for the Automated Detection of\n  Injustices in Text", "abstract": "Injustice occurs when someone experiences unfair treatment or their rights\nare violated and is often due to the presence of implicit biases and prejudice\nsuch as stereotypes. The automated identification of injustice in text has\nreceived little attention, due in part to the fact that underlying implicit\nbiases or stereotypes are rarely explicitly stated and that instances often\noccur unconsciously due to the pervasive nature of prejudice in society. Here,\nwe describe a novel framework that combines the use of a fine-tuned BERT-based\nbias detection model, two stereotype detection models, and a lexicon-based\napproach to show that epistemological biases (i.e., words, which presupposes,\nentails, asserts, hedges, or boosts text to erode or assert a person's capacity\nas a knower) can assist with the automatic detection of injustice in text. The\nnews media has many instances of injustice (i.e. discriminatory narratives),\nthus it is our use case here. We conduct and discuss an empirical qualitative\nresearch study which shows how the framework can be applied to detect\ninjustices, even at higher volumes of data.", "published": "2024-07-08 16:38:31", "link": "http://arxiv.org/abs/2407.06098v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What's Wrong with Your Code Generated by Large Language Models? An\n  Extensive Study", "abstract": "The increasing development of large language models (LLMs) in code generation\nhas drawn significant attention among researchers. To enhance LLM-based code\ngeneration ability, current efforts are predominantly directed towards\ncollecting high-quality datasets and leveraging diverse training technologies.\nHowever, there is a notable lack of comprehensive studies examining the\nlimitations and boundaries of these existing methods. To bridge this gap, we\nconducted an extensive empirical study evaluating the performance of three\nleading closed-source LLMs and four popular open-source LLMs on three commonly\nused benchmarks. Our investigation, which evaluated the length, cyclomatic\ncomplexity and API number of the generated code, revealed that these LLMs face\nchallenges in generating successful code for more complex problems, and tend to\nproduce code that is shorter yet more complicated as compared to canonical\nsolutions. Additionally, we developed a taxonomy of bugs for incorrect codes\nthat includes three categories and 12 sub-categories, and analyze the root\ncause for common bug types. Furthermore, to better understand the performance\nof LLMs in real-world projects, we manually created a real-world benchmark\ncomprising 140 code generation tasks. Our analysis highlights distinct\ndifferences in bug distributions between actual scenarios and existing\nbenchmarks. Finally, we propose a novel training-free iterative method that\nintroduces self-critique, enabling LLMs to critique and correct their generated\ncode based on bug types and compiler feedback. Experimental results demonstrate\nthat our approach can significantly mitigate bugs and increase the passing rate\nby 29.2% after two iterations, indicating substantial potential for LLMs to\nhandle more complex problems.", "published": "2024-07-08 17:27:17", "link": "http://arxiv.org/abs/2407.06153v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "On Speeding Up Language Model Evaluation", "abstract": "Developing prompt-based methods with Large Language Models (LLMs) requires\nmaking numerous decisions, which give rise to a combinatorial search problem\nover hyper-parameters. This exhaustive evaluation can be time-consuming and\ncostly. In this paper, we propose an $\\textit{adaptive}$ approach to explore\nthis space. We are exploiting the fact that often only few samples are needed\nto identify clearly superior or inferior settings, and that many evaluation\ntests are highly correlated. We lean on multi-armed bandits to sequentially\nidentify the next (method, validation sample)-pair to evaluate and utilize\nlow-rank matrix factorization to fill in missing evaluations. We carefully\nassess the efficacy of our approach on several competitive benchmark problems\nand show that it can identify the top-performing method using only 5-15% of the\ntypical resources -- resulting in 85-95% LLM cost savings. Our code is\navailable at https://github.com/kilian-group/banditeval.", "published": "2024-07-08 17:48:42", "link": "http://arxiv.org/abs/2407.06172v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates", "abstract": "Large language models (LLMs) are increasingly being used to synthesize and\nreason about source code. However, the static nature of these models' knowledge\ndoes not reflect the fact that libraries and API functions they invoke are\ncontinuously evolving, with functionality being added or changing. While\nnumerous benchmarks evaluate how LLMs can generate code, no prior work has\nstudied how an LLMs' knowledge about code API functions can be updated. To fill\nthis gap, we present CodeUpdateArena, a benchmark for knowledge editing in the\ncode domain. An instance in our benchmark consists of a synthetic API function\nupdate paired with a program synthesis example that uses the updated\nfunctionality; our goal is to update an LLM to be able to solve this program\nsynthesis example without providing documentation of the update at inference\ntime. Compared to knowledge editing for facts encoded in text, success here is\nmore challenging: a code LLM must correctly reason about the semantics of the\nmodified function rather than just reproduce its syntax. Our dataset is\nconstructed by first prompting GPT-4 to generate atomic and executable function\nupdates. Then, for each update, we generate program synthesis examples whose\ncode solutions are prone to use the update. Our benchmark covers updates of\nvarious types to 54 functions from seven diverse Python packages, with a total\nof 670 program synthesis examples. Our experiments show that prepending\ndocumentation of the update to open-source code LLMs (i.e., DeepSeek,\nCodeLlama) does not allow them to incorporate changes for problem solving, and\nexisting knowledge editing techniques also have substantial room for\nimprovement. We hope our benchmark will inspire new methods for knowledge\nupdating in code LLMs.", "published": "2024-07-08 17:55:04", "link": "http://arxiv.org/abs/2407.06249v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Large Language Model Recall Uncertainty is Modulated by the Fan Effect", "abstract": "This paper evaluates whether large language models (LLMs) exhibit cognitive\nfan effects, similar to those discovered by Anderson in humans, after being\npre-trained on human textual data. We conduct two sets of in-context recall\nexperiments designed to elicit fan effects. Consistent with human results, we\nfind that LLM recall uncertainty, measured via token probability, is influenced\nby the fan effect. Our results show that removing uncertainty disrupts the\nobserved effect. The experiments suggest the fan effect is consistent whether\nthe fan value is induced in-context or in the pre-training data. Finally, these\nfindings provide in-silico evidence that fan effects and typicality are\nexpressions of the same phenomena.", "published": "2024-07-08 19:40:50", "link": "http://arxiv.org/abs/2407.06349v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Capability of ChatGPT to Reproduce Human Labels for Social\n  Computing Tasks (Extended Version)", "abstract": "Harnessing the potential of large language models (LLMs) like ChatGPT can\nhelp address social challenges through inclusive, ethical, and sustainable\nmeans. In this paper, we investigate the extent to which ChatGPT can annotate\ndata for social computing tasks, aiming to reduce the complexity and cost of\nundertaking web research. To evaluate ChatGPT's potential, we re-annotate seven\ndatasets using ChatGPT, covering topics related to pressing social issues like\nCOVID-19 misinformation, social bot deception, cyberbully, clickbait news, and\nthe Russo-Ukrainian War. Our findings demonstrate that ChatGPT exhibits promise\nin handling these data annotation tasks, albeit with some challenges. Across\nthe seven datasets, ChatGPT achieves an average annotation F1-score of 72.00%.\nIts performance excels in clickbait news annotation, correctly labeling 89.66%\nof the data. However, we also observe significant variations in performance\nacross individual labels. Our study reveals predictable patterns in ChatGPT's\nannotation performance. Thus, we propose GPT-Rater, a tool to predict if\nChatGPT can correctly label data for a given annotation task. Researchers can\nuse this to identify where ChatGPT might be suitable for their annotation\nrequirements. We show that GPT-Rater effectively predicts ChatGPT's\nperformance. It performs best on a clickbait headlines dataset by achieving an\naverage F1-score of 95.00%. We believe that this research opens new avenues for\nanalysis and can reduce barriers to engaging in social computing research.", "published": "2024-07-08 22:04:30", "link": "http://arxiv.org/abs/2407.06422v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "abstract": "Language models (LMs) are trained on vast amounts of text data, which may\ninclude private and copyrighted content. Data owners may request the removal of\ntheir data from a trained model due to privacy or copyright concerns. However,\nexactly unlearning only these datapoints (i.e., retraining with the data\nremoved) is intractable in modern-day models. This has led to the development\nof many approximate unlearning algorithms. The evaluation of the efficacy of\nthese algorithms has traditionally been narrow in scope, failing to precisely\nquantify the success and practicality of the algorithm from the perspectives of\nboth the model deployers and the data owners. We address this issue by\nproposing MUSE, a comprehensive machine unlearning evaluation benchmark that\nenumerates six diverse desirable properties for unlearned models: (1) no\nverbatim memorization, (2) no knowledge memorization, (3) no privacy leakage,\n(4) utility preservation on data not intended for removal, (5) scalability with\nrespect to the size of removal requests, and (6) sustainability over sequential\nunlearning requests. Using these criteria, we benchmark how effectively eight\npopular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter\nbooks and news articles. Our results demonstrate that most algorithms can\nprevent verbatim memorization and knowledge memorization to varying degrees,\nbut only one algorithm does not lead to severe privacy leakage. Furthermore,\nexisting algorithms fail to meet deployer's expectations because they often\ndegrade general model utility and also cannot sustainably accommodate\nsuccessive unlearning requests or large-scale content removal. Our findings\nidentify key issues with the practicality of existing unlearning algorithms on\nlanguage models, and we release our benchmark to facilitate further\nevaluations: muse-bench.github.io", "published": "2024-07-08 23:47:29", "link": "http://arxiv.org/abs/2407.06460v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Historical Ink: Semantic Shift Detection for 19th Century Spanish", "abstract": "This paper explores the evolution of word meanings in 19th-century Spanish\ntexts, with an emphasis on Latin American Spanish, using computational\nlinguistics techniques. It addresses the Semantic Shift Detection (SSD) task,\nwhich is crucial for understanding linguistic evolution, particularly in\nhistorical contexts. The study focuses on analyzing a set of Spanish target\nwords. To achieve this, a 19th-century Spanish corpus is constructed, and a\ncustomizable pipeline for SSD tasks is developed. This pipeline helps find the\nsenses of a word and measure their semantic change between two corpora using\nfine-tuned BERT-like models with old Spanish texts for both Latin American and\ngeneral Spanish cases. The results provide valuable insights into the cultural\nand societal shifts reflected in language changes over time.", "published": "2024-07-08 16:49:34", "link": "http://arxiv.org/abs/2407.12852v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "On the Power of Convolution Augmented Transformer", "abstract": "The transformer architecture has catalyzed revolutionary advances in language\nmodeling. However, recent architectural recipes, such as state-space models,\nhave bridged the performance gap. Motivated by this, we examine the benefits of\nConvolution-Augmented Transformer (CAT) for recall, copying, and length\ngeneralization tasks. CAT incorporates convolutional filters in the K/Q/V\nembeddings of an attention layer. Through CAT, we show that the locality of the\nconvolution synergizes with the global view of the attention. Unlike comparable\narchitectures, such as Mamba or transformer, CAT can provably solve the\nassociative recall (AR) and copying tasks using a single layer while also\nenjoying guaranteed length generalization. We also establish computational\ntradeoffs between convolution and attention by characterizing how convolution\ncan mitigate the need for full attention by summarizing the context window and\ncreating salient summary tokens to attend. Evaluations on real datasets\ncorroborate our findings and demonstrate that CAT and its variations indeed\nenhance the language modeling performance.", "published": "2024-07-08 04:08:35", "link": "http://arxiv.org/abs/2407.05591v1", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "A Benchmark for Multi-speaker Anonymization", "abstract": "Privacy-preserving voice protection approaches primarily suppress\nprivacy-related information derived from paralinguistic attributes while\npreserving the linguistic content. Existing solutions focus particularly on\nsingle-speaker scenarios. However, they lack practicality for real-world\napplications, i.e., multi-speaker scenarios. In this paper, we present an\ninitial attempt to provide a multi-speaker anonymization benchmark by defining\nthe task and evaluation protocol, proposing benchmarking solutions, and\ndiscussing the privacy leakage of overlapping conversations. The proposed\nbenchmark solutions are based on a cascaded system that integrates\nspectral-clustering-based speaker diarization and disentanglement-based speaker\nanonymization using a selection-based anonymizer. To improve utility, the\nbenchmark solutions are further enhanced by two conversation-level speaker\nvector anonymization methods. The first method minimizes the differential\nsimilarity across speaker pairs in the original and anonymized conversations,\nwhich maintains original speaker relationships in the anonymized version. The\nother minimizes the aggregated similarity across anonymized speakers, which\nachieves better differentiation between speakers.Experiments conducted on both\nnon-overlap simulated and real-world datasets demonstrate the effectiveness of\nthe multi-speaker anonymization system with the proposed speaker anonymizers.\nAdditionally, we analyzed overlapping speech regarding privacy leakage and\nprovided potential solutions", "published": "2024-07-08 04:48:43", "link": "http://arxiv.org/abs/2407.05608v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Coding Reliable LLM-based Integrated Task and Knowledge Agents with\n  GenieWorksheets", "abstract": "Large Language Models (LLMs) present an opportunity to create automated\nassistants that can help users navigate complex tasks. However, existing\napproaches have limitations in handling conditional logic, integrating\nknowledge sources, and consistently following instructions. Researchers and\nindustry professionals often employ ad hoc pipelines to construct\nconversational agents. These pipelines aim to maintain context, address failure\ncases, and minimize hallucinations, yet frequently fail to achieve these\nobjectives. To this end, we present Genie - a programmable framework for\ncreating task-oriented conversational agents that are designed to handle\ncomplex user interactions and knowledge queries. Unlike LLMs, Genie provides\nreliable grounded responses, with controllable agent policies through its\nexpressive specification, Genie Worksheet. In contrast to dialog trees, it is\nresilient to diverse user queries, helpful with knowledge sources, and offers\nease of programming policies through its declarative paradigm. The agents built\nusing Genie outperforms the state-of-the-art method on complex logic domains in\nSTARV2 dataset by up to 20.5%. Additionally, through a real-user study\ninvolving 62 participants, we show that Genie beats the GPT-4 with function\ncalling baseline by 21.1%, 20.1%, and 61% on execution accuracy, dialogue act\naccuracy, and goal completion rate, respectively, on three diverse real-world\ndomains", "published": "2024-07-08 07:17:40", "link": "http://arxiv.org/abs/2407.05674v2", "categories": ["cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.AI"}
{"title": "Sub-SA: Strengthen In-context Learning via Submodular Selective\n  Annotation", "abstract": "In-context learning (ICL) leverages in-context examples as prompts for the\npredictions of Large Language Models (LLMs). These prompts play a crucial role\nin achieving strong performance. However, the selection of suitable prompts\nfrom a large pool of labeled examples often entails significant annotation\ncosts. To address this challenge, we propose Sub-SA (Submodular Selective\nAnnotation), a submodule-based selective annotation method. The aim of Sub-SA\nis to reduce annotation costs while improving the quality of in-context\nexamples and minimizing the time consumption of the selection process. In\nSub-SA, we design a submodular function that facilitates effective subset\nselection for annotation and demonstrates the characteristics of monotonically\nand submodularity from the theoretical perspective. Specifically, we propose\nRPR (Reward and Penalty Regularization) to better balance the diversity and\nrepresentativeness of the unlabeled dataset attributed to a reward term and a\npenalty term, respectively. Consequently, the selection for annotations can be\neffectively addressed with a simple yet effective greedy search algorithm based\non the submodular function. Finally, we apply the similarity prompt retrieval\nto get the examples for ICL.", "published": "2024-07-08 07:47:30", "link": "http://arxiv.org/abs/2407.05693v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Limitations of Compute Thresholds as a Governance Strategy", "abstract": "At face value, this essay is about understanding a fairly esoteric governance\ntool called compute thresholds. However, in order to grapple with whether these\nthresholds will achieve anything, we must first understand how they came to be.\nTo do so, we need to engage with a decades-old debate at the heart of computer\nscience progress, namely, is bigger always better? Does a certain inflection\npoint of compute result in changes to the risk profile of a model? Hence, this\nessay may be of interest not only to policymakers and the wider public but also\nto computer scientists interested in understanding the role of compute in\nunlocking breakthroughs. This discussion is timely given the wide adoption of\ncompute thresholds in both the White House Executive Orders on AI Safety (EO)\nand the EU AI Act to identify more risky systems. A key conclusion of this\nessay is that compute thresholds, as currently implemented, are shortsighted\nand likely to fail to mitigate risk. The relationship between compute and risk\nis highly uncertain and rapidly changing. Relying upon compute thresholds\noverestimates our ability to predict what abilities emerge at different scales.\nThis essay ends with recommendations for a better way forward.", "published": "2024-07-08 07:53:06", "link": "http://arxiv.org/abs/2407.05694v2", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.LG"], "primary_category": "cs.AI"}
{"title": "InverseCoder: Self-improving Instruction-Tuned Code LLMs with\n  Inverse-Instruct", "abstract": "Recent advancements in open-source code large language models (LLMs) have\nbeen driven by fine-tuning on the data generated from powerful closed-source\nLLMs, which are expensive to obtain. This paper explores whether it is possible\nto use a fine-tuned open-source model to generate additional data to augment\nits instruction-tuning dataset. We make two observations: (1) A code snippet\ncan serve as the response to different instructions. (2) Instruction-tuned code\nLLMs perform better at translating code into instructions than the reverse.\nBased on these observations, we propose Inverse-Instruct, a data augmentation\ntechnique that uses a fine-tuned LLM to generate additional instructions of\ncode responses from its own training dataset. The additional\ninstruction-response pairs are added to the original dataset, and a stronger\ncode LLM can be obtained by fine-tuning on the augmented dataset. We\nempirically validate Inverse-Instruct on a range of open-source code models\n(e.g. CodeLlama-Python and DeepSeek-Coder) and benchmarks (e.g., HumanEval(+),\nMBPP(+), DS-1000 and MultiPL-E), showing it consistently improves the base\nmodels.", "published": "2024-07-08 08:00:05", "link": "http://arxiv.org/abs/2407.05700v2", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Generation and De-Identification of Indian Clinical Discharge Summaries\n  using LLMs", "abstract": "The consequences of a healthcare data breach can be devastating for the\npatients, providers, and payers. The average financial impact of a data breach\nin recent months has been estimated to be close to USD 10 million. This is\nespecially significant for healthcare organizations in India that are managing\nrapid digitization while still establishing data governance procedures that\nalign with the letter and spirit of the law. Computer-based systems for\nde-identification of personal information are vulnerable to data drift, often\nrendering them ineffective in cross-institution settings. Therefore, a rigorous\nassessment of existing de-identification against local health datasets is\nimperative to support the safe adoption of digital health initiatives in India.\nUsing a small set of de-identified patient discharge summaries provided by an\nIndian healthcare institution, in this paper, we report the nominal performance\nof de-identification algorithms (based on language models) trained on publicly\navailable non-Indian datasets, pointing towards a lack of cross-institutional\ngeneralization. Similarly, experimentation with off-the-shelf de-identification\nsystems reveals potential risks associated with the approach. To overcome data\nscarcity, we explore generating synthetic clinical reports (using publicly\navailable and Indian summaries) by performing in-context learning over Large\nLanguage Models (LLMs). Our experiments demonstrate the use of generated\nreports as an effective strategy for creating high-performing de-identification\nsystems with good generalization capabilities.", "published": "2024-07-08 12:47:03", "link": "http://arxiv.org/abs/2407.05887v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variational Best-of-N Alignment", "abstract": "Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on controlled\ngeneration and summarization tasks show that BoN is the most effective\nalignment method, and our variational approximation to BoN achieves the closest\nperformance to BoN and surpasses models fine-tuned using the standard\nKL-constrained RL objective. In the controlled generation task, vBoN appears\nmore frequently on the Pareto frontier of reward and KL divergence compared to\nother alignment methods. In the summarization task, vBoN achieves high reward\nvalues across various sampling temperatures.", "published": "2024-07-08 15:59:44", "link": "http://arxiv.org/abs/2407.06057v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ANOLE: An Open, Autoregressive, Native Large Multimodal Models for\n  Interleaved Image-Text Generation", "abstract": "Previous open-source large multimodal models (LMMs) have faced several\nlimitations: (1) they often lack native integration, requiring adapters to\nalign visual representations with pre-trained large language models (LLMs); (2)\nmany are restricted to single-modal generation; (3) while some support\nmultimodal generation, they rely on separate diffusion models for visual\nmodeling and generation. To mitigate these limitations, we present Anole, an\nopen, autoregressive, native large multimodal model for interleaved image-text\ngeneration. We build Anole from Meta AI's Chameleon, adopting an innovative\nfine-tuning strategy that is both data-efficient and parameter-efficient. Anole\ndemonstrates high-quality, coherent multimodal generation capabilities. We have\nopen-sourced our model, training framework, and instruction tuning data.", "published": "2024-07-08 17:08:02", "link": "http://arxiv.org/abs/2407.06135v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling\n  Tasks", "abstract": "We present and evaluate a method called grammar masking, which is used to\nguide large language models (LLMs) toward producing syntactically correct\nmodels for a given context-free grammar. Prompt engineering methods such as\nfew-shot learning or priming can be used to improve the chances of an LLM\nproducing correct syntax, but the more complex the grammar, the more\ntime-consuming and less promising these methods become. Previous work is\nfocused primarily on the usage of either language model training or prompt\nengineering. In this work, a method is presented that restricts the output to a\ngiven grammar using constrained decoding to ensure the output adheres to a\nvalid syntax. We use several DSLs built with MontiCore and task multiple LLMs\nto produce models with and without constrained decoding. A corresponding parser\nis used to confirm the syntactic correctness of each model. We show that\ngrammar masking can dramatically improve the modeling capabilities of several\nLLMs, reducing the need for well-refined prompting while increasing the chance\nof producing correct models.", "published": "2024-07-08 17:19:59", "link": "http://arxiv.org/abs/2407.06146v2", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Vision-Language Models under Cultural and Inclusive Considerations", "abstract": "Large vision-language models (VLMs) can assist visually impaired people by\ndescribing images from their daily lives. Current evaluation datasets may not\nreflect diverse cultural user backgrounds or the situational context of this\nuse case. To address this problem, we create a survey to determine caption\npreferences and propose a culture-centric evaluation benchmark by filtering\nVizWiz, an existing dataset with images taken by people who are blind. We then\nevaluate several VLMs, investigating their reliability as visual assistants in\na culturally diverse setting. While our results for state-of-the-art models are\npromising, we identify challenges such as hallucination and misalignment of\nautomatic evaluation metrics with human judgment. We make our survey, data,\ncode, and model outputs publicly available.", "published": "2024-07-08 17:50:00", "link": "http://arxiv.org/abs/2407.06177v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CV"}
{"title": "Multi-Object Hallucination in Vision-Language Models", "abstract": "Large vision language models (LVLMs) often suffer from object hallucination,\nproducing objects not present in the given images. While current benchmarks for\nobject hallucination primarily concentrate on the presence of a single object\nclass rather than individual entities, this work systematically investigates\nmulti-object hallucination, examining how models misperceive (e.g., invent\nnonexistent objects or become distracted) when tasked with focusing on multiple\nobjects simultaneously. We introduce Recognition-based Object Probing\nEvaluation (ROPE), an automated evaluation protocol that considers the\ndistribution of object classes within a single image during testing and uses\nvisual referring prompts to eliminate ambiguity. With comprehensive empirical\nstudies and analysis of potential factors leading to multi-object\nhallucination, we found that (1). LVLMs suffer more hallucinations when\nfocusing on multiple objects compared to a single object. (2). The tested\nobject class distribution affects hallucination behaviors, indicating that\nLVLMs may follow shortcuts and spurious correlations. (3). Hallucinatory\nbehaviors are influenced by data-specific factors, salience and frequency, and\nmodel intrinsic behaviors. We hope to enable LVLMs to recognize and reason\nabout multiple objects that often occur in realistic visual scenes, provide\ninsights, and quantify our progress towards mitigating the issues.", "published": "2024-07-08 17:59:57", "link": "http://arxiv.org/abs/2407.06192v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ORAN-Bench-13K: An Open Source Benchmark for Assessing LLMs in Open\n  Radio Access Networks", "abstract": "Large Language Models (LLMs) can revolutionize how we deploy and operate Open\nRadio Access Networks (O-RAN) by enhancing network analytics, anomaly\ndetection, and code generation and significantly increasing the efficiency and\nreliability of a plethora of O-RAN tasks. In this paper, we present\nORAN-Bench-13K, the first comprehensive benchmark designed to evaluate the\nperformance of Large Language Models (LLMs) within the context of O-RAN. Our\nbenchmark consists of 13,952 meticulously curated multiple-choice questions\ngenerated from 116 O-RAN specification documents. We leverage a novel\nthree-stage LLM framework, and the questions are categorized into three\ndistinct difficulties to cover a wide spectrum of ORAN-related knowledge. We\nthoroughly evaluate the performance of several state-of-the-art LLMs, including\nGemini, Chat-GPT, and Mistral. Additionally, we propose ORANSight, a\nRetrieval-Augmented Generation (RAG)-based pipeline that demonstrates superior\nperformance on ORAN-Bench-13K compared to other tested closed-source models.\nOur findings indicate that current popular LLM models are not proficient in\nO-RAN, highlighting the need for specialized models. We observed a noticeable\nperformance improvement when incorporating the RAG-based ORANSight pipeline,\nwith a Macro Accuracy of 0.784 and a Weighted Accuracy of 0.776, which was on\naverage 21.55% and 22.59% better than the other tested LLMs.", "published": "2024-07-08 13:07:50", "link": "http://arxiv.org/abs/2407.06245v2", "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.NI"}
{"title": "Hybrid X-Linker: Automated Data Generation and Extreme Multi-label\n  Ranking for Biomedical Entity Linking", "abstract": "State-of-the-art deep learning entity linking methods rely on extensive\nhuman-labelled data, which is costly to acquire. Current datasets are limited\nin size, leading to inadequate coverage of biomedical concepts and diminished\nperformance when applied to new data. In this work, we propose to automatically\ngenerate data to create large-scale training datasets, which allows the\nexploration of approaches originally developed for the task of extreme\nmulti-label ranking in the biomedical entity linking task. We propose the\nhybrid X-Linker pipeline that includes different modules to link disease and\nchemical entity mentions to concepts in the MEDIC and the CTD-Chemical\nvocabularies, respectively. X-Linker was evaluated on several biomedical\ndatasets: BC5CDR-Disease, BioRED-Disease, NCBI-Disease, BC5CDR-Chemical,\nBioRED-Chemical, and NLM-Chem, achieving top-1 accuracies of 0.8307, 0.7969,\n0.8271, 0.9511, 0.9248, and 0.7895, respectively. X-Linker demonstrated\nsuperior performance in three datasets: BC5CDR-Disease, NCBI-Disease, and\nBioRED-Chemical. In contrast, SapBERT outperformed X-Linker in the remaining\nthree datasets. Both models rely only on the mention string for their\noperations. The source code of X-Linker and its associated data are publicly\navailable for performing biomedical entity linking without requiring\npre-labelled entities with identifiers from specific knowledge organization\nsystems.", "published": "2024-07-08 18:04:22", "link": "http://arxiv.org/abs/2407.06292v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "VIMI: Grounding Video Generation through Multi-modal Instruction", "abstract": "Existing text-to-video diffusion models rely solely on text-only encoders for\ntheir pretraining. This limitation stems from the absence of large-scale\nmultimodal prompt video datasets, resulting in a lack of visual grounding and\nrestricting their versatility and application in multimodal integration. To\naddress this, we construct a large-scale multimodal prompt dataset by employing\nretrieval methods to pair in-context examples with the given text prompts and\nthen utilize a two-stage training strategy to enable diverse video generation\ntasks within the same model. In the first stage, we propose a multimodal\nconditional video generation framework for pretraining on these augmented\ndatasets, establishing a foundational model for grounded video generation.\nSecondly, we finetune the model from the first stage on three video generation\ntasks, incorporating multi-modal instructions. This process further refines the\nmodel's ability to handle diverse inputs and tasks, ensuring seamless\nintegration of multi-modal information. After this two-stage train-ing process,\nVIMI demonstrates multimodal understanding capabilities, producing contextually\nrich and personalized videos grounded in the provided inputs, as shown in\nFigure 1. Compared to previous visual grounded video generation methods, VIMI\ncan synthesize consistent and temporally coherent videos with large motion\nwhile retaining the semantic control. Lastly, VIMI also achieves\nstate-of-the-art text-to-video generation results on UCF101 benchmark.", "published": "2024-07-08 18:12:49", "link": "http://arxiv.org/abs/2407.06304v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "B'MOJO: Hybrid State Space Realizations of Foundation Models with\n  Eidetic and Fading Memory", "abstract": "We describe a family of architectures to support transductive inference by\nallowing memory to grow to a finite but a-priori unknown bound while making\nefficient use of finite resources for inference. Current architectures use such\nresources to represent data either eidetically over a finite span (\"context\" in\nTransformers), or fading over an infinite span (in State Space Models, or\nSSMs). Recent hybrid architectures have combined eidetic and fading memory, but\nwith limitations that do not allow the designer or the learning process to\nseamlessly modulate the two, nor to extend the eidetic memory span. We leverage\nideas from Stochastic Realization Theory to develop a class of models called\nB'MOJO to seamlessly combine eidetic and fading memory within an elementary\ncomposable module. The overall architecture can be used to implement models\nthat can access short-term eidetic memory \"in-context,\" permanent structural\nmemory \"in-weights,\" fading memory \"in-state,\" and long-term eidetic memory\n\"in-storage\" by natively incorporating retrieval from an asynchronously updated\nmemory. We show that Transformers, existing SSMs such as Mamba, and hybrid\narchitectures such as Jamba are special cases of B'MOJO and describe a basic\nimplementation, to be open sourced, that can be stacked and scaled efficiently\nin hardware. We test B'MOJO on transductive inference tasks, such as\nassociative recall, where it outperforms existing SSMs and Hybrid models; as a\nbaseline, we test ordinary language modeling where B'MOJO achieves perplexity\ncomparable to similarly-sized Transformers and SSMs up to 1.4B parameters,\nwhile being up to 10% faster to train. Finally, we show that B'MOJO's ability\nto modulate eidetic and fading memory results in better inference on longer\nsequences tested up to 32K tokens, four-fold the length of the longest\nsequences seen during training.", "published": "2024-07-08 18:41:01", "link": "http://arxiv.org/abs/2407.06324v1", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "If You Don't Understand It, Don't Use It: Eliminating Trojans with\n  Filters Between Layers", "abstract": "Large language models (LLMs) sometimes exhibit dangerous unintended\nbehaviors. Finding and fixing these is challenging because the attack surface\nis massive -- it is not tractable to exhaustively search for all possible\ninputs that may elicit such behavior. One specific and particularly challenging\ncase is that if data-poisoning-injected trojans, since there is no way to know\nwhat they are to search for them. To our knowledge, there is no generally\napplicable method to unlearn unknown trojans injected during pre-training. This\nwork seeks to provide a general purpose recipe (filters) and a specific\nimplementation (LoRA) filters that work in practice on small to medium sized\nmodels. The focus is primarily empirical, though some perplexing behavior opens\nthe door to the fundamental question of how LLMs store and process information.\nNot unexpectedly, we find that our filters work best on the residual stream and\nthe latest layers.", "published": "2024-07-08 21:40:23", "link": "http://arxiv.org/abs/2407.06411v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "DebUnc: Improving Large Language Model Agent Communication With\n  Uncertainty Metrics", "abstract": "Multi-agent debates have been introduced to improve the accuracy of Large\nLanguage Models (LLMs) by having multiple agents discuss solutions to a problem\nover several rounds of debate. However, models often generate incorrect yet\nconfident-sounding responses, which can mislead others. This issue arises\npartly because agents do not consider how confident their peers are. To address\nthis, we propose DebUnc, a debate framework that uses uncertainty metrics to\nassess agent confidence. Confidence is then conveyed through a modified\nattention mechanism that adjusts token weights, or through textual prompts.\nEvaluations across benchmarks show that attention-based methods are\nparticularly effective and that performance continues to improve as uncertainty\nestimation becomes more reliable. The code is available at\nhttps://github.com/lukeyoffe/debunc.", "published": "2024-07-08 22:15:01", "link": "http://arxiv.org/abs/2407.06426v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "SOLO: A Single Transformer for Scalable Vision-Language Modeling", "abstract": "We present SOLO, a single transformer for Scalable visiOn-Language mOdeling.\nCurrent large vision-language models (LVLMs) such as LLaVA mostly employ\nheterogeneous architectures that connect pre-trained visual encoders with large\nlanguage models (LLMs) to facilitate visual recognition and complex reasoning.\nAlthough achieving remarkable performance with relatively lightweight training,\nwe identify four primary scalability limitations: (1) The visual capacity is\nconstrained by pre-trained visual encoders, which are typically an order of\nmagnitude smaller than LLMs. (2) The heterogeneous architecture complicates the\nuse of established hardware and software infrastructure. (3) Study of scaling\nlaws on such architecture must consider three separate components - visual\nencoder, connector, and LLMs, which complicates the analysis. (4) The use of\nexisting visual encoders typically requires following a pre-defined\nspecification of image inputs pre-processing, for example, by reshaping inputs\nto fixed-resolution square images, which presents difficulties in processing\nand training on high-resolution images or those with unusual aspect ratio. A\nunified single Transformer architecture, like SOLO, effectively addresses these\nscalability concerns in LVLMs; however, its limited adoption in the modern\ncontext likely stems from the absence of reliable training recipes that balance\nboth modalities and ensure stable training for billion-scale models. In this\npaper, we introduce the first open-source training recipe for developing SOLO,\nan open-source 7B LVLM using moderate academic resources. The training recipe\ninvolves initializing from LLMs, sequential pre-training on ImageNet and\nweb-scale data, and instruction fine-tuning on our curated high-quality\ndatasets. On extensive evaluation, SOLO demonstrates performance comparable to\nLLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning.", "published": "2024-07-08 22:40:15", "link": "http://arxiv.org/abs/2407.06438v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Survey on LoRA of Large Language Models", "abstract": "Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github\npage~\\footnote{\\href{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}}\nfor readers to check the updates and initiate discussions on this survey paper.", "published": "2024-07-08 12:32:10", "link": "http://arxiv.org/abs/2407.11046v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Limits to Predicting Online Speech Using Large Language Models", "abstract": "We study the predictability of online speech on social media, and whether\npredictability improves with information outside a user's own posts. Recent\ntheoretical results suggest that posts from a user's social circle are as\npredictive of the user's future posts as that of the user's past posts.\nMotivated by the success of large language models, we empirically test this\nhypothesis. We define predictability as a measure of the model's uncertainty,\ni.e., its negative log-likelihood on future tokens given context. As the basis\nof our study, we collect 10M tweets for ``tweet-tuning'' base models and a\nfurther 6.25M posts from more than five thousand X (previously Twitter) users\nand their peers. Across four large language models ranging in size from 1.5\nbillion to 70 billion parameters, we find that predicting a user's posts from\ntheir peers' posts performs poorly. Moreover, the value of the user's own posts\nfor prediction is consistently higher than that of their peers'. We extend our\ninvestigation with a detailed analysis on what's learned in-context and the\nrobustness of our findings. From context, base models learn to correctly\npredict @-mentions and hashtags. Moreover, our results replicate if instead of\nprompting the model with additional context, we finetune on it. Across the\nboard, we find that predicting the posts of individual users remains hard.", "published": "2024-07-08 09:50:49", "link": "http://arxiv.org/abs/2407.12850v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BoRA: Bayesian Hierarchical Low-Rank Adaption for Multi-Task Large\n  Language Models", "abstract": "This paper introduces Bayesian Hierarchical Low-Rank Adaption (BoRA), a novel\nmethod for finetuning multi-task Large Language Models (LLMs). Current\nfinetuning approaches, such as Low-Rank Adaption (LoRA), perform exeptionally\nwell in reducing training parameters and memory usage but face limitations when\napplied to multiple similar tasks. Practitioners usually have to choose between\ntraining separate models for each task or a single model for all tasks, both of\nwhich come with trade-offs in specialization and data utilization. BoRA\naddresses these trade-offs by leveraging a Bayesian hierarchical model that\nallows tasks to share information through global hierarchical priors. This\nenables tasks with limited data to benefit from the overall structure derived\nfrom related tasks while allowing tasks with more data to specialize. Our\nexperimental results show that BoRA outperforms both individual and unified\nmodel approaches, achieving lower perplexity and better generalization across\ntasks. This method provides a scalable and efficient solution for multi-task\nLLM finetuning, with significant practical implications for diverse\napplications.", "published": "2024-07-08 06:38:50", "link": "http://arxiv.org/abs/2407.15857v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Analyzing Speech Unit Selection for Textless Speech-to-Speech\n  Translation", "abstract": "Recent advancements in textless speech-to-speech translation systems have\nbeen driven by the adoption of self-supervised learning techniques. Although\nmost state-of-the-art systems adopt a similar architecture to transform source\nlanguage speech into sequences of discrete representations in the target\nlanguage, the criteria for selecting these target speech units remains an open\nquestion. This work explores the selection process through a study of\ndownstream tasks such as automatic speech recognition, speech synthesis,\nspeaker recognition, and emotion recognition. Interestingly, our findings\nreveal a discrepancy in the optimization of discrete speech units: units that\nperform well in resynthesis performance do not necessarily correlate with those\nthat enhance translation efficacy. This discrepancy underscores the nuanced\ncomplexity of target feature selection and its impact on the overall\nperformance of speech-to-speech translation systems.", "published": "2024-07-08 08:53:26", "link": "http://arxiv.org/abs/2407.18332v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models", "abstract": "The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset including real-world prompts, LLM-generated prompts and jailbreak\nattack-based prompts. Based on our evaluation results, we draw several\nimportant findings, including: 1) no single model excels in all aspects, with\ndifferent models showing various strengths; 2) the correlation between GPT-4\nassessments and manual reviews is generally high; 3) there is a trade-off\nbetween the usability and safety of text-to-video generative models. This\nindicates that as the field of video generation rapidly advances, safety risks\nare set to surge, highlighting the urgency of prioritizing video safety. We\nhope that T2VSafetyBench can provide insights for better understanding the\nsafety of video generation in the era of generative AI.", "published": "2024-07-08 14:04:58", "link": "http://arxiv.org/abs/2407.05965v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "New Directions in Text Classification Research: Maximizing The\n  Performance of Sentiment Classification from Limited Data", "abstract": "The stakeholders' needs in sentiment analysis for various issues, whether\npositive or negative, are speed and accuracy. One new challenge in sentiment\nanalysis tasks is the limited training data, which often leads to suboptimal\nmachine learning models and poor performance on test data. This paper discusses\nthe problem of text classification based on limited training data (300 to 600\nsamples) into three classes: positive, negative, and neutral. A benchmark\ndataset is provided for training and testing data on the issue of Kaesang\nPangarep's appointment as Chairman of PSI. External data for aggregation and\naugmentation purposes are provided, consisting of two datasets: the topic of\nCovid Vaccination sentiment and an open topic. The official score used is the\nF1-score, which balances precision and recall among the three classes,\npositive, negative, and neutral. A baseline score is provided as a reference\nfor researchers for unoptimized classification methods. The optimized score is\nprovided as a reference for the target score to be achieved by any proposed\nmethod. Both scoring (baseline and optimized) use the SVM method, which is\nwidely reported as the state-of-the-art in conventional machine learning\nmethods. The F1-scores achieved by the baseline and optimized methods are\n40.83% and 51.28%, respectively.", "published": "2024-07-08 05:42:29", "link": "http://arxiv.org/abs/2407.05627v1", "categories": ["cs.CL", "cs.IR", "cs.IT", "cs.LG", "cs.SI", "math.IT"], "primary_category": "cs.CL"}
{"title": "XANE Background Acoustic Embeddings: Ablation and Clustering Analysis", "abstract": "We explore the recently proposed explainable acoustic neural embedding~(XANE)\nsystem that models the background acoustics of a speech signal in a\nnon-intrusive manner. The XANE embeddings are used to estimate specific\nparameters related to the background acoustic properties of the signal which\nallows the embeddings to be explainable in terms of those parameters. We\nperform ablation studies on the XANE system and show that estimating all\nacoustic parameters jointly has an overall positive effect. Furthermore, we\nillustrate the value of XANE embeddings by performing clustering experiments on\nunseen test data and show that the proposed embeddings achieve a mean F1 score\nof 92\\% for three different tasks, outperforming significantly the WavLM based\nsignal embeddings and are complimentary to speaker embeddings.", "published": "2024-07-08 19:23:47", "link": "http://arxiv.org/abs/2407.06342v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Two-Path GMM-ResNet and GMM-SENet for ASV Spoofing Detection", "abstract": "The automatic speaker verification system is sometimes vulnerable to various\nspoofing attacks. The 2-class Gaussian Mixture Model classifier for genuine and\nspoofed speech is usually used as the baseline for spoofing detection. However,\nthe GMM classifier does not separately consider the scores of feature frames on\neach Gaussian component. In addition, the GMM accumulates the scores on all\nframes independently, and does not consider their correlations. We propose the\ntwo-path GMM-ResNet and GMM-SENet models for spoofing detection, whose input is\nthe Gaussian probability features based on two GMMs trained on genuine and\nspoofed speech respectively. The models consider not only the score\ndistribution on GMM components, but also the relationship between adjacent\nframes. A two-step training scheme is applied to improve the system robustness.\nExperiments on the ASVspoof 2019 show that the LFCC+GMM-ResNet system can\nrelatively reduce min-tDCF and EER by 76.1% and 76.3% on logical access\nscenario compared with the GMM, and the LFCC+GMM-SENet system by 94.4% and\n95.4% on physical access scenario. After score fusion, the systems give the\nsecond-best results on both scenarios.", "published": "2024-07-08 04:42:36", "link": "http://arxiv.org/abs/2407.05605v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automating Urban Soundscape Enhancements with AI: In-situ Assessment of\n  Quality and Restorativeness in Traffic-Exposed Residential Areas", "abstract": "Formalized in ISO 12913, the \"soundscape\" approach is a paradigmatic shift\ntowards perception-based urban sound management, aiming to alleviate the\nsubstantial socioeconomic costs of noise pollution to advance the United\nNations Sustainable Development Goals. Focusing on traffic-exposed outdoor\nresidential sites, we implemented an automatic masker selection system (AMSS)\nutilizing natural sounds to mask (or augment) traffic soundscapes. We employed\na pre-trained AI model to automatically select the optimal masker and adjust\nits playback level, adapting to changes over time in the ambient environment to\nmaximize \"Pleasantness\", a perceptual dimension of soundscape quality in ISO\n12913. Our validation study involving ($N=68$) residents revealed a significant\n14.6 % enhancement in \"Pleasantness\" after intervention, correlating with\nincreased restorativeness and positive affect. Perceptual enhancements at the\ntraffic-exposed site matched those at a quieter control site with 6 dB(A) lower\n$L_\\text{A,eq}$ and road traffic noise dominance, affirming the efficacy of\nAMSS as a soundscape intervention, while streamlining the labour-intensive\nassessment of \"Pleasantness\" with probabilistic AI prediction.", "published": "2024-07-08 08:48:43", "link": "http://arxiv.org/abs/2407.05744v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transfer Learning with Pseudo Multi-Label Birdcall Classification for\n  DS@GT BirdCLEF 2024", "abstract": "We present working notes for the DS@GT team on transfer learning with pseudo\nmulti-label birdcall classification for the BirdCLEF 2024 competition, focused\non identifying Indian bird species in recorded soundscapes. Our approach\nutilizes production-grade models such as the Google Bird Vocalization\nClassifier, BirdNET, and EnCodec to address representation and labeling\nchallenges in the competition. We explore the distributional shift between this\nyear's edition of unlabeled soundscapes representative of the hidden test set\nand propose a pseudo multi-label classification strategy to leverage the\nunlabeled data. Our highest post-competition public leaderboard score is 0.63\nusing BirdNET embeddings with Bird Vocalization pseudo-labels. Our code is\navailable at https://github.com/dsgt-kaggle-clef/birdclef-2024", "published": "2024-07-08 18:04:19", "link": "http://arxiv.org/abs/2407.06291v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Read, Watch and Scream! Sound Generation from Text and Video", "abstract": "Despite the impressive progress of multimodal generative models,\nvideo-to-audio generation still suffers from limited performance and limits the\nflexibility to prioritize sound synthesis for specific objects within the\nscene. Conversely, text-to-audio generation methods generate high-quality audio\nbut pose challenges in ensuring comprehensive scene depiction and time-varying\ncontrol. To tackle these challenges, we propose a novel video-and-text-to-audio\ngeneration method, called \\ours, where video serves as a conditional control\nfor a text-to-audio generation model. Especially, our method estimates the\nstructural information of sound (namely, energy) from the video while receiving\nkey content cues from a user prompt. We employ a well-performing text-to-audio\nmodel to consolidate the video control, which is much more efficient for\ntraining multimodal diffusion models with massive triplet-paired\n(audio-video-text) data. In addition, by separating the generative components\nof audio, it becomes a more flexible system that allows users to freely adjust\nthe energy, surrounding environment, and primary sound source according to\ntheir preferences. Experimental results demonstrate that our method shows\nsuperiority in terms of quality, controllability, and training efficiency. Code\nand demo are available at https://naver-ai.github.io/rewas.", "published": "2024-07-08 01:59:17", "link": "http://arxiv.org/abs/2407.05551v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "MSP-Podcast SER Challenge 2024: L'antenne du Ventoux Multimodal\n  Self-Supervised Learning for Speech Emotion Recognition", "abstract": "In this work, we detail our submission to the 2024 edition of the MSP-Podcast\nSpeech Emotion Recognition (SER) Challenge. This challenge is divided into two\ndistinct tasks: Categorical Emotion Recognition and Emotional Attribute\nPrediction. We concentrated our efforts on Task 1, which involves the\ncategorical classification of eight emotional states using data from the\nMSP-Podcast dataset. Our approach employs an ensemble of models, each trained\nindependently and then fused at the score level using a Support Vector Machine\n(SVM) classifier. The models were trained using various strategies, including\nSelf-Supervised Learning (SSL) fine-tuning across different modalities: speech\nalone, text alone, and a combined speech and text approach. This joint training\nmethodology aims to enhance the system's ability to accurately classify\nemotional states. This joint training methodology aims to enhance the system's\nability to accurately classify emotional states. Thus, the system obtained\nF1-macro of 0.35\\% on development set.", "published": "2024-07-08 08:52:06", "link": "http://arxiv.org/abs/2407.05746v1", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Dirichlet process mixture model based on topologically augmented signal\n  representation for clustering infant vocalizations", "abstract": "Based on audio recordings made once a month during the first 12 months of a\nchild's life, we propose a new method for clustering this set of vocalizations.\nWe use a topologically augmented representation of the vocalizations, employing\ntwo persistence diagrams for each vocalization: one computed on the surface of\nits spectrogram and one on the Takens' embeddings of the vocalization. A\nsynthetic persistent variable is derived for each diagram and added to the\nMFCCs (Mel-frequency cepstral coefficients). Using this representation, we fit\na non-parametric Bayesian mixture model with a Dirichlet process prior to model\nthe number of components. This procedure leads to a novel data-driven\ncategorization of vocal productions. Our findings reveal the presence of 8\nclusters of vocalizations, allowing us to compare their temporal distribution\nand acoustic profiles in the first 12 months of life.", "published": "2024-07-08 09:12:52", "link": "http://arxiv.org/abs/2407.05760v1", "categories": ["stat.AP", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "stat.AP"}
{"title": "Cervical Auscultation Machine Learning for Dysphagia Assessment", "abstract": "This study evaluates the use of machine learning, specifically the Random\nForest Classifier, to differentiate normal and pathological swallowing sounds.\nEmploying a commercially available wearable stethoscope, we recorded swallows\nfrom both healthy adults and patients with dysphagia. The analysis revealed\nstatistically significant differences in acoustic features, such as spectral\ncrest, and zero-crossing rate between normal and pathological swallows, while\nno discriminating differences were demonstrated between different fluidand diet\nconsistencies. The system demonstrated fair sensitivity (mean plus or minus SD:\n74% plus or minus 8%) and specificity (89% plus or minus 6%) for dysphagic\nswallows. The model attained an overall accuracy of 83% plus or minus 3%, and\nF1 score of 78% plus or minus 5%. These results demonstrate that machine\nlearning can be a valuable tool in non-invasive dysphagia assessment, although\nchallenges such as sampling rate limitations and variability in sensitivity and\nspecificity in discriminating between normal and pathological sounds are noted.\nThe study underscores the need for further research to optimize these\ntechniques for clinical use.", "published": "2024-07-08 12:31:49", "link": "http://arxiv.org/abs/2407.05870v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sequential Contrastive Audio-Visual Learning", "abstract": "Contrastive learning has emerged as a powerful technique in audio-visual\nrepresentation learning, leveraging the natural co-occurrence of audio and\nvisual modalities in webscale video datasets. However, conventional contrastive\naudio-visual learning (CAV) methodologies often rely on aggregated\nrepresentations derived through temporal aggregation, neglecting the intrinsic\nsequential nature of the data. This oversight raises concerns regarding the\nability of standard approaches to capture and utilize fine-grained information\nwithin sequences. In response to this limitation, we propose sequential\ncontrastive audiovisual learning (SCAV), which contrasts examples based on\ntheir non-aggregated representation space using multidimensional sequential\ndistances. Audio-visual retrieval experiments with the VGGSound and Music\ndatasets demonstrate the effectiveness of SCAV, with up to 3.5x relative\nimprovements in recall against traditional aggregation-based contrastive\nlearning and other previously proposed methods, which utilize more parameters\nand data. We also show that models trained with SCAV exhibit a significant\ndegree of flexibility regarding the metric employed for retrieval, allowing us\nto use a hybrid retrieval approach that is both effective and efficient.", "published": "2024-07-08 09:45:20", "link": "http://arxiv.org/abs/2407.05782v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MERGE -- A Bimodal Dataset for Static Music Emotion Recognition", "abstract": "The Music Emotion Recognition (MER) field has seen steady developments in\nrecent years, with contributions from feature engineering, machine learning,\nand deep learning. The landscape has also shifted from audio-centric systems to\nbimodal ensembles that combine audio and lyrics. However, a severe lack of\npublic and sizeable bimodal databases has hampered the development and\nimprovement of bimodal audio-lyrics systems. This article proposes three new\naudio, lyrics, and bimodal MER research datasets, collectively called MERGE,\ncreated using a semi-automatic approach. To comprehensively assess the proposed\ndatasets and establish a baseline for benchmarking, we conducted several\nexperiments for each modality, using feature engineering, machine learning, and\ndeep learning methodologies. In addition, we propose and validate fixed\ntrain-validate-test splits. The obtained results confirm the viability of the\nproposed datasets, achieving the best overall result of 79.21% F1-score for\nbimodal classification using a deep neural network.", "published": "2024-07-08 16:01:04", "link": "http://arxiv.org/abs/2407.06060v2", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Homogeneous Speaker Features for On-the-Fly Dysarthric and Elderly\n  Speaker Adaptation", "abstract": "The application of data-intensive automatic speech recognition (ASR)\ntechnologies to dysarthric and elderly adult speech is confronted by their\nmismatch against healthy and nonaged voices, data scarcity and large\nspeaker-level variability. To this end, this paper proposes two novel\ndata-efficient methods to learn homogeneous dysarthric and elderly\nspeaker-level features for rapid, on-the-fly test-time adaptation of DNN/TDNN\nand Conformer ASR models. These include: 1) speaker-level variance-regularized\nspectral basis embedding (VR-SBE) features that exploit a special\nregularization term to enforce homogeneity of speaker features in adaptation;\nand 2) feature-based learning hidden unit contributions (f-LHUC) transforms\nthat are conditioned on VR-SBE features. Experiments are conducted on four\ntasks across two languages: the English UASpeech and TORGO dysarthric speech\ndatasets, the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly\nspeech corpora. The proposed on-the-fly speaker adaptation techniques\nconsistently outperform baseline iVector and xVector adaptation by\nstatistically significant word or character error rate reductions up to 5.32%\nabsolute (18.57% relative) and batch-mode LHUC speaker adaptation by 2.24%\nabsolute (9.20% relative), while operating with real-time factors speeding up\nto 33.6 times against xVectors during adaptation. The efficacy of the proposed\nadaptation techniques is demonstrated in a comparison against current ASR\ntechnologies including SSL pre-trained systems on UASpeech, where our best\nsystem produces a state-of-the-art WER of 23.33%. Analyses show VR-SBE features\nand f-LHUC transforms are insensitive to speaker-level data quantity in\ntesttime adaptation. T-SNE visualization reveals they have stronger\nspeaker-level homogeneity than baseline iVectors, xVectors and batch-mode LHUC\ntransforms.", "published": "2024-07-08 18:20:24", "link": "http://arxiv.org/abs/2407.06310v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
