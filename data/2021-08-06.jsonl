{"title": "LadRa-Net: Locally-Aware Dynamic Re-read Attention Net for Sentence\n  Semantic Matching", "abstract": "Sentence semantic matching requires an agent to determine the semantic\nrelation between two sentences, which is widely used in various natural\nlanguage tasks, such as Natural Language Inference (NLI), Paraphrase\nIdentification (PI), and so on. Much recent progress has been made in this\narea, especially attention-based methods and pre-trained language model based\nmethods. However, most of these methods focus on all the important parts in\nsentences in a static way and only emphasize how important the words are to the\nquery, inhibiting the ability of attention mechanism. In order to overcome this\nproblem and boost the performance of attention mechanism, we propose a novel\ndynamic re-read attention, which can pay close attention to one small region of\nsentences at each step and re-read the important parts for better sentence\nrepresentations. Based on this attention variation, we develop a novel Dynamic\nRe-read Network (DRr-Net) for sentence semantic matching. Moreover, selecting\none small region in dynamic re-read attention seems insufficient for sentence\nsemantics, and employing pre-trained language models as input encoders will\nintroduce incomplete and fragile representation problems. To this end, we\nextend DRrNet to Locally-Aware Dynamic Re-read Attention Net (LadRa-Net), in\nwhich local structure of sentences is employed to alleviate the shortcoming of\nByte-Pair Encoding (BPE) in pre-trained language models and boost the\nperformance of dynamic reread attention. Extensive experiments on two popular\nsentence semantic matching tasks demonstrate that DRr-Net can significantly\nimprove the performance of sentence semantic matching. Meanwhile, LadRa-Net is\nable to achieve better performance by considering the local structures of\nsentences. In addition, it is exceedingly interesting that some discoveries in\nour experiments are consistent with some findings of psychological research.", "published": "2021-08-06 02:07:04", "link": "http://arxiv.org/abs/2108.02915v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Semantic Regression for Text Generation", "abstract": "Recall the classical text generation works, the generation framework can be\nbriefly divided into two phases: \\textbf{idea reasoning} and \\textbf{surface\nrealization}. The target of idea reasoning is to figure out the main idea which\nwill be presented in the following talking/writing periods. Surface realization\naims to arrange the most appropriate sentence to depict and convey the\ninformation distilled from the main idea. However, the current popular\ntoken-by-token text generation methods ignore this crucial process and suffer\nfrom many serious issues, such as idea/topic drift. To tackle the problems and\nrealize this two-phase paradigm, we propose a new framework named Sentence\nSemantic Regression (\\textbf{SSR}) based on sentence-level language modeling.\nFor idea reasoning, two architectures \\textbf{SSR-AR} and \\textbf{SSR-NonAR}\nare designed to conduct sentence semantic regression autoregressively (like\nGPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a\nmixed-granularity sentence decoder is designed to generate text with better\nconsistency by jointly incorporating the predicted sentence-level main idea as\nwell as the preceding contextual token-level information. We conduct\nexperiments on four tasks of story ending prediction, story ending generation,\ndialogue generation, and sentence infilling. The results show that SSR can\nobtain better performance in terms of automatic metrics and human evaluation.", "published": "2021-08-06 07:35:59", "link": "http://arxiv.org/abs/2108.02984v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SWSR: A Chinese Dataset and Lexicon for Online Sexism Detection", "abstract": "Online sexism has become an increasing concern in social media platforms as\nit has affected the healthy development of the Internet and can have negative\neffects in society. While research in the sexism detection domain is growing,\nmost of this research focuses on English as the language and on Twitter as the\nplatform. Our objective here is to broaden the scope of this research by\nconsidering the Chinese language on Sina Weibo. We propose the first Chinese\nsexism dataset -- Sina Weibo Sexism Review (SWSR) dataset --, as well as a\nlarge Chinese lexicon SexHateLex made of abusive and gender-related terms. We\nintroduce our data collection and annotation process, and provide an\nexploratory analysis of the dataset characteristics to validate its quality and\nto show how sexism is manifested in Chinese. The SWSR dataset provides labels\nat different levels of granularity including (i) sexism or non-sexism, (ii)\nsexism category and (iii) target type, which can be exploited, among others,\nfor building computational methods to identify and investigate finer-grained\ngender-related abusive language. We conduct experiments for the three sexism\nclassification tasks making use of state-of-the-art machine learning models.\nOur results show competitive performance, providing a benchmark for sexism\ndetection in the Chinese language, as well as an error analysis highlighting\nopen challenges needing more research in Chinese NLP. The SWSR dataset and\nSexHateLex lexicon are publicly available.", "published": "2021-08-06 12:06:40", "link": "http://arxiv.org/abs/2108.03070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Capsule Network for Hate Speech Detection in Social Media", "abstract": "Most hate speech detection research focuses on a single language, generally\nEnglish, which limits their generalisability to other languages. In this paper\nwe investigate the cross-lingual hate speech detection task, tackling the\nproblem by adapting the hate speech resources from one language to another. We\npropose a cross-lingual capsule network learning model coupled with extra\ndomain-specific lexical semantics for hate speech (CCNL-Ex). Our model achieves\nstate-of-the-art performance on benchmark datasets from AMI@Evalita2018 and\nAMI@Ibereval2018 involving three languages: English, Spanish and Italian,\noutperforming state-of-the-art baselines on all six language pairs.", "published": "2021-08-06 12:53:41", "link": "http://arxiv.org/abs/2108.03089v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facebook AI WMT21 News Translation Task Submission", "abstract": "We describe Facebook's multilingual model submission to the WMT2021 shared\ntask on news translation. We participate in 14 language directions: English to\nand from Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese. To\ndevelop systems covering all these directions, we focus on multilingual models.\nWe utilize data from all available sources --- WMT, large-scale data mining,\nand in-domain backtranslation --- to create high quality bilingual and\nmultilingual baselines. Subsequently, we investigate strategies for scaling\nmultilingual model size, such that one system has sufficient capacity for high\nquality representations of all eight languages. Our final submission is an\nensemble of dense and sparse Mixture-of-Expert multilingual translation models,\nfollowed by finetuning on in-domain news data and noisy channel reranking.\nCompared to previous year's winning submissions, our multilingual system\nimproved the translation quality on all language directions, with an average\nimprovement of 2.0 BLEU. In the WMT2021 task, our system ranks first in 10\ndirections based on automatic evaluation.", "published": "2021-08-06 18:26:38", "link": "http://arxiv.org/abs/2108.03265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Zero-shot Language Modeling", "abstract": "Can we construct a neural model that is inductively biased towards learning\nhuman languages? Motivated by this question, we aim at constructing an\ninformative prior over neural weights, in order to adapt quickly to held-out\nlanguages in the task of character-level language modeling. We infer this\ndistribution from a sample of typologically diverse training languages via\nLaplace approximation. The use of such a prior outperforms baseline models with\nan uninformative prior (so-called \"fine-tuning\") in both zero-shot and few-shot\nsettings. This shows that the prior is imbued with universal phonological\nknowledge. Moreover, we harness additional language-specific side information\nas distant supervision for held-out languages. Specifically, we condition\nlanguage models on features from typological databases, by concatenating them\nto hidden states or generating weights with hyper-networks. These features\nappear beneficial in the few-shot setting, but not in the zero-shot setting.\nSince the paucity of digital texts affects the majority of the world's\nlanguages, we hope that these findings will help broaden the scope of\napplications for language technology.", "published": "2021-08-06 23:49:18", "link": "http://arxiv.org/abs/2108.03334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR\n  documents", "abstract": "Document digitization is essential for the digital transformation of our\nsocieties, yet a crucial step in the process, Optical Character Recognition\n(OCR), is still not perfect. Even commercial OCR systems can produce\nquestionable output depending on the fidelity of the scanned documents. In this\npaper, we demonstrate an effective framework for mitigating OCR errors for any\ndownstream NLP task, using Named Entity Recognition (NER) as an example. We\nfirst address the data scarcity problem for model training by constructing a\ndocument synthesis pipeline, generating realistic but degraded data with NER\nlabels. We measure the NER accuracy drop at various degradation levels and show\nthat a text restoration model, trained on the degraded data, significantly\ncloses the NER accuracy gaps caused by OCR errors, including on an\nout-of-domain dataset. For the benefit of the community, we have made the\ndocument synthesis pipeline available as an open-source project.", "published": "2021-08-06 00:32:54", "link": "http://arxiv.org/abs/2108.02899v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "StrucTexT: Structured Text Understanding with Multi-Modal Transformers", "abstract": "Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.", "published": "2021-08-06 02:57:07", "link": "http://arxiv.org/abs/2108.02923v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deriving Disinformation Insights from Geolocalized Twitter Callouts", "abstract": "This paper demonstrates a two-stage method for deriving insights from social\nmedia data relating to disinformation by applying a combination of geospatial\nclassification and embedding-based language modelling across multiple\nlanguages. In particular, the analysis in centered on Twitter and\ndisinformation for three European languages: English, French and Spanish.\nFirstly, Twitter data is classified into European and non-European sets using\nBERT. Secondly, Word2vec is applied to the classified texts resulting in\nEurocentric, non-Eurocentric and global representations of the data for the\nthree target languages. This comparative analysis demonstrates not only the\nefficacy of the classification method but also highlights geographic, temporal\nand linguistic differences in the disinformation-related media. Thus, the\ncontributions of the work are threefold: (i) a novel language-independent\ntransformer-based geolocation method; (ii) an analytical approach that exploits\nlexical specificity and word embeddings to interrogate user-generated content;\nand (iii) a dataset of 36 million disinformation related tweets in English,\nFrench and Spanish.", "published": "2021-08-06 11:39:05", "link": "http://arxiv.org/abs/2108.03067v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Offensive Language and Hate Speech Detection with Deep Learning and\n  Transfer Learning", "abstract": "Toxic online speech has become a crucial problem nowadays due to an\nexponential increase in the use of internet by people from different cultures\nand educational backgrounds. Differentiating if a text message belongs to hate\nspeech and offensive language is a key challenge in automatic detection of\ntoxic text content. In this paper, we propose an approach to automatically\nclassify tweets into three classes: Hate, offensive and Neither. Using public\ntweet data set, we first perform experiments to build BI-LSTM models from empty\nembedding and then we also try the same neural network architecture with\npre-trained Glove embedding. Next, we introduce a transfer learning approach\nfor hate speech detection using an existing pre-trained language model BERT\n(Bidirectional Encoder Representations from Transformers), DistilBert\n(Distilled version of BERT) and GPT-2 (Generative Pre-Training). We perform\nhyper parameters tuning analysis of our best model (BI-LSTM) considering\ndifferent neural network architectures, learn-ratings and normalization methods\netc. After tuning the model and with the best combination of parameters, we\nachieve over 92 percent accuracy upon evaluating it on test data. We also\ncreate a class module which contains main functionality including text\nclassification, sentiment checking and text data augmentation. This model could\nserve as an intermediate module between user and Twitter.", "published": "2021-08-06 20:59:47", "link": "http://arxiv.org/abs/2108.03305v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is it Fake? News Disinformation Detection on South African News Websites", "abstract": "Disinformation through fake news is an ongoing problem in our society and has\nbecome easily spread through social media. The most cost and time effective way\nto filter these large amounts of data is to use a combination of human and\ntechnical interventions to identify it. From a technical perspective, Natural\nLanguage Processing (NLP) is widely used in detecting fake news. Social media\ncompanies use NLP techniques to identify the fake news and warn their users,\nbut fake news may still slip through undetected. It is especially a problem in\nmore localised contexts (outside the United States of America). How do we\nadjust fake news detection systems to work better for local contexts such as in\nSouth Africa. In this work we investigate fake news detection on South African\nwebsites. We curate a dataset of South African fake news and then train\ndetection models. We contrast this with using widely available fake news\ndatasets (from mostly USA website). We also explore making the datasets more\ndiverse by combining them and observe the differences in behaviour in writing\nbetween nations' fake news using interpretable machine learning.", "published": "2021-08-06 04:54:03", "link": "http://arxiv.org/abs/2108.02941v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tell me a story about yourself: The words of shopping experience and\n  self-satisfaction", "abstract": "In this paper we investigate the verbal expression of shopping experience\nobtained by a sample of customers asked to freely verbalize how they felt when\nentering a store. Using novel tools of Text Mining and Social Network Analysis,\nwe analyzed the interviews to understand the connection between the emotions\naroused during the shopping experience, satisfaction and the way participants\nlink these concepts to self-satisfaction and self-identity. The results show a\nprominent role of emotions in the discourse about the shopping experience\nbefore purchasing and an inward-looking connection to the self. Our results\nalso suggest that modern retail environment should enhance the hedonic shopping\nexperience in terms of fun, fantasy, moods, and emotions.", "published": "2021-08-06 09:22:59", "link": "http://arxiv.org/abs/2108.03016v1", "categories": ["cs.CL", "cs.SI", "physics.soc-ph", "I.2.7; J.4; H.4.0"], "primary_category": "cs.CL"}
{"title": "Deep Residual Echo Suppression and Noise Reduction: A Multi-Input FCRN\n  Approach in a Hybrid Speech Enhancement System", "abstract": "Deep neural network (DNN)-based approaches to acoustic echo cancellation\n(AEC) and hybrid speech enhancement systems have gained increasing attention\nrecently, introducing significant performance improvements to this research\nfield. Using the fully convolutional recurrent network (FCRN) architecture that\nis among state of the art topologies for noise reduction, we present a novel\ndeep residual echo suppression and noise reduction with up to four input\nsignals as part of a hybrid speech enhancement system with a linear frequency\ndomain adaptive Kalman filter AEC. In an extensive ablation study, we reveal\ntrade-offs with regard to echo suppression, noise reduction, and near-end\nspeech quality, and provide surprising insights to the choice of the FCRN\ninputs: In contrast to often seen input combinations for this task, we propose\nnot to use the loudspeaker reference signal, but the enhanced signal after AEC,\nthe microphone signal, and the echo estimate, yielding improvements over\nprevious approaches by more than 0.2 PESQ points.", "published": "2021-08-06 11:00:54", "link": "http://arxiv.org/abs/2108.03051v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Complex-valued Spatial Autoencoders for Multichannel Speech Enhancement", "abstract": "In this contribution, we present a novel online approach to multichannel\nspeech enhancement. The proposed method estimates the enhanced signal through a\nfilter-and-sum framework. More specifically, complex-valued masks are estimated\nby a deep complex-valued neural network, termed the complex-valued spatial\nautoencoder. The proposed network is capable of exploiting as well as\nmanipulating both the phase and the amplitude of the microphone signals. As\nshown by the experimental results, the proposed approach is able to exploit\nboth spatial and spectral characteristics of the desired source signal\nresulting in a physically plausible spatial selectivity and superior speech\nquality compared to other baseline methods.", "published": "2021-08-06 14:03:20", "link": "http://arxiv.org/abs/2108.03130v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An Empirical Study on End-to-End Singing Voice Synthesis with\n  Encoder-Decoder Architectures", "abstract": "With the rapid development of neural network architectures and speech\nprocessing models, singing voice synthesis with neural networks is becoming the\ncutting-edge technique of digital music production. In this work, in order to\nexplore how to improve the quality and efficiency of singing voice synthesis,\nin this work, we use encoder-decoder neural models and a number of vocoders to\nachieve singing voice synthesis. We conduct experiments to demonstrate that the\nmodels can be trained using voice data with pitch information, lyrics and beat\ninformation, and the trained models can produce smooth, clear and natural\nsinging voice that is close to real human voice. As the models work in the\nend-to-end manner, they allow users who are not domain experts to directly\nproduce singing voice by arranging pitches, lyrics and beats.", "published": "2021-08-06 08:51:16", "link": "http://arxiv.org/abs/2108.03008v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Right to Talk: An Audio-Visual Transformer Approach", "abstract": "Turn-taking has played an essential role in structuring the regulation of a\nconversation. The task of identifying the main speaker (who is properly taking\nhis/her turn of speaking) and the interrupters (who are interrupting or\nreacting to the main speaker's utterances) remains a challenging task. Although\nsome prior methods have partially addressed this task, there still remain some\nlimitations. Firstly, a direct association of Audio and Visual features may\nlimit the correlations to be extracted due to different modalities. Secondly,\nthe relationship across temporal segments helping to maintain the consistency\nof localization, separation, and conversation contexts is not effectively\nexploited. Finally, the interactions between speakers that usually contain the\ntracking and anticipatory decisions about the transition to a new speaker are\nusually ignored. Therefore, this work introduces a new Audio-Visual Transformer\napproach to the problem of localization and highlighting the main speaker in\nboth audio and visual channels of a multi-speaker conversation video in the\nwild. The proposed method exploits different types of correlations presented in\nboth visual and audio signals. The temporal audio-visual relationships across\nspatial-temporal space are anticipated and optimized via the self-attention\nmechanism in a Transformerstructure. Moreover, a newly collected dataset is\nintroduced for the main speaker detection. To the best of our knowledge, it is\none of the first studies that is able to automatically localize and highlight\nthe main speaker in both visual and audio channels in multi-speaker\nconversation videos.", "published": "2021-08-06 18:04:24", "link": "http://arxiv.org/abs/2108.03256v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
