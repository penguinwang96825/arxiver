{"title": "Few-shot Subgoal Planning with Language Models", "abstract": "Pre-trained large language models have shown successful progress in many\nlanguage understanding benchmarks. This work explores the capability of these\nmodels to predict actionable plans in real-world environments. Given a text\ninstruction, we show that language priors encoded in pre-trained language\nmodels allow us to infer fine-grained subgoal sequences. In contrast to recent\nmethods which make strong assumptions about subgoal supervision, our\nexperiments show that language models can infer detailed subgoal sequences from\nfew training sequences without any fine-tuning. We further propose a simple\nstrategy to re-rank language model predictions based on interaction and\nfeedback from the environment. Combined with pre-trained navigation and visual\nreasoning components, our approach demonstrates competitive performance on\nsubgoal prediction and task completion in the ALFRED benchmark compared to\nprior methods that assume more subgoal supervision.", "published": "2022-05-28 01:03:30", "link": "http://arxiv.org/abs/2205.14288v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Reference Is Not Enough: Diverse Distillation with Reference\n  Selection for Non-Autoregressive Translation", "abstract": "Non-autoregressive neural machine translation (NAT) suffers from the\nmulti-modality problem: the source sentence may have multiple correct\ntranslations, but the loss function is calculated only according to the\nreference sentence. Sequence-level knowledge distillation makes the target more\ndeterministic by replacing the target with the output from an autoregressive\nmodel. However, the multi-modality problem in the distilled dataset is still\nnonnegligible. Furthermore, learning from a specific teacher limits the upper\nbound of the model capability, restricting the potential of NAT models. In this\npaper, we argue that one reference is not enough and propose diverse\ndistillation with reference selection (DDRS) for NAT. Specifically, we first\npropose a method called SeedDiv for diverse machine translation, which enables\nus to generate a dataset containing multiple high-quality reference\ntranslations for each source sentence. During the training, we compare the NAT\noutput with all references and select the one that best fits the NAT output to\ntrain the model. Experiments on widely-used machine translation benchmarks\ndemonstrate the effectiveness of DDRS, which achieves 29.82 BLEU with only one\ndecoding pass on WMT14 En-De, improving the state-of-the-art performance for\nNAT by over 1 BLEU. Source code: https://github.com/ictnlp/DDRS-NAT", "published": "2022-05-28 04:59:33", "link": "http://arxiv.org/abs/2205.14333v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Relation-Specific Attentions over Entity Mentions for Enhanced\n  Document-Level Relation Extraction", "abstract": "Compared with traditional sentence-level relation extraction, document-level\nrelation extraction is a more challenging task where an entity in a document\nmay be mentioned multiple times and associated with multiple relations.\nHowever, most methods of document-level relation extraction do not distinguish\nbetween mention-level features and entity-level features, and just apply simple\npooling operation for aggregating mention-level features into entity-level\nfeatures. As a result, the distinct semantics between the different mentions of\nan entity are overlooked. To address this problem, we propose RSMAN in this\npaper which performs selective attentions over different entity mentions with\nrespect to candidate relations. In this manner, the flexible and\nrelation-specific representations of entities are obtained which indeed benefit\nrelation classification. Our extensive experiments upon two benchmark datasets\nshow that our RSMAN can bring significant improvements for some backbone models\nto achieve state-of-the-art performance, especially when an entity have\nmultiple mentions in the document.", "published": "2022-05-28 10:40:31", "link": "http://arxiv.org/abs/2205.14393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset", "abstract": "As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.", "published": "2022-05-28 15:39:09", "link": "http://arxiv.org/abs/2205.14462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Approximate Conditional Coverage & Calibration via Neural Model\n  Approximations", "abstract": "A typical desideratum for quantifying the uncertainty from a classification\nmodel as a prediction set is class-conditional singleton set calibration. That\nis, such sets should map to the output of well-calibrated selective\nclassifiers, matching the observed frequencies of similar instances. Recent\nworks proposing adaptive and localized conformal p-values for deep networks do\nnot guarantee this behavior, nor do they achieve it empirically. Instead, we\nuse the strong signals for prediction reliability from KNN-based approximations\nof Transformer networks to construct data-driven partitions for Mondrian\nConformal Predictors, which are treated as weak selective classifiers that are\nthen calibrated via a new Inductive Venn Predictor, the Venn-ADMIT Predictor.\nThe resulting selective classifiers are well-calibrated, in a conservative but\npractically useful sense for a given threshold. They are inherently robust to\nchanges in the proportions of the data partitions, and straightforward\nconservative heuristics provide additional robustness to covariate shifts. We\ncompare and contrast to the quantities produced by recent Conformal Predictors\non several representative and challenging natural language processing\nclassification tasks, including class-imbalanced and distribution-shifted\nsettings.", "published": "2022-05-28 02:59:05", "link": "http://arxiv.org/abs/2205.14310v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning Non-Autoregressive Models from Search for Unsupervised Sentence\n  Summarization", "abstract": "Text summarization aims to generate a short summary for an input text. In\nthis work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS)\napproach, which does not require parallel data for training. Our NAUS first\nperforms edit-based search towards a heuristically defined score, and generates\na summary as pseudo-groundtruth. Then, we train an encoder-only\nnon-autoregressive Transformer based on the search result. We also propose a\ndynamic programming approach for length-control decoding, which is important\nfor the summarization task. Experiments on two datasets show that NAUS achieves\nstate-of-the-art performance for unsupervised summarization, yet largely\nimproving inference efficiency. Further, our algorithm is able to perform\nexplicit length-transfer summary generation.", "published": "2022-05-28 21:09:23", "link": "http://arxiv.org/abs/2205.14521v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Character-Level Length-Control Algorithm for Non-Autoregressive\n  Sentence Summarization", "abstract": "Sentence summarization aims at compressing a long sentence into a short one\nthat keeps the main gist, and has extensive real-world applications such as\nheadline generation. In previous work, researchers have developed various\napproaches to improve the ROUGE score, which is the main evaluation metric for\nsummarization, whereas controlling the summary length has not drawn much\nattention. In our work, we address a new problem of explicit character-level\nlength control for summarization, and propose a dynamic programming algorithm\nbased on the Connectionist Temporal Classification (CTC) model. Results show\nthat our approach not only achieves higher ROUGE scores but also yields more\ncomplete sentences.", "published": "2022-05-28 21:09:53", "link": "http://arxiv.org/abs/2205.14522v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Activation Network For Low Resource Multilingual Speech\n  Recognition", "abstract": "Low resource automatic speech recognition (ASR) is a useful but thorny task,\nsince deep learning ASR models usually need huge amounts of training data. The\nexisting models mostly established a bottleneck (BN) layer by pre-training on a\nlarge source language, and transferring to the low resource target language. In\nthis work, we introduced an adaptive activation network to the upper layers of\nASR model, and applied different activation functions to different languages.\nWe also proposed two approaches to train the model: (1) cross-lingual learning,\nreplacing the activation function from source language to target language, (2)\nmultilingual learning, jointly training the Connectionist Temporal\nClassification (CTC) loss of each language and the relevance of different\nlanguages. Our experiments on IARPA Babel datasets demonstrated that our\napproaches outperform the from-scratch training and traditional bottleneck\nfeature based methods. In addition, combining the cross-lingual learning and\nmultilingual learning together could further improve the performance of\nmultilingual speech recognition.", "published": "2022-05-28 04:02:59", "link": "http://arxiv.org/abs/2205.14326v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speech Augmentation Based Unsupervised Learning for Keyword Spotting", "abstract": "In this paper, we investigated a speech augmentation based unsupervised\nlearning approach for keyword spotting (KWS) task. KWS is a useful speech\napplication, yet also heavily depends on the labeled data. We designed a\nCNN-Attention architecture to conduct the KWS task. CNN layers focus on the\nlocal acoustic features, and attention layers model the long-time dependency.\nTo improve the robustness of KWS model, we also proposed an unsupervised\nlearning method. The unsupervised loss is based on the similarity between the\noriginal and augmented speech features, as well as the audio reconstructing\ninformation. Two speech augmentation methods are explored in the unsupervised\nlearning: speed and intensity. The experiments on Google Speech Commands V2\nDataset demonstrated that our CNN-Attention model has competitive results.\nMoreover, the augmentation based unsupervised learning could further improve\nthe classification accuracy of KWS task. In our experiments, with augmentation\nbased unsupervised learning, our KWS model achieves better performance than\nother unsupervised methods, such as CPC, APC, and MPC.", "published": "2022-05-28 04:11:31", "link": "http://arxiv.org/abs/2205.14329v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Teaching Models to Express Their Uncertainty in Words", "abstract": "We show that a GPT-3 model can learn to express uncertainty about its own\nanswers in natural language -- without use of model logits. When given a\nquestion, the model generates both an answer and a level of confidence (e.g.\n\"90% confidence\" or \"high confidence\"). These levels map to probabilities that\nare well calibrated. The model also remains moderately calibrated under\ndistribution shift, and is sensitive to uncertainty in its own answers, rather\nthan imitating human examples. To our knowledge, this is the first time a model\nhas been shown to express calibrated uncertainty about its own answers in\nnatural language. For testing calibration, we introduce the CalibratedMath\nsuite of tasks. We compare the calibration of uncertainty expressed in words\n(\"verbalized probability\") to uncertainty extracted from model logits. Both\nkinds of uncertainty are capable of generalizing calibration under distribution\nshift. We also provide evidence that GPT-3's ability to generalize calibration\ndepends on pre-trained latent representations that correlate with epistemic\nuncertainty over its answers.", "published": "2022-05-28 05:02:31", "link": "http://arxiv.org/abs/2205.14334v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient and Student-Friendly Knowledge Distillation", "abstract": "Knowledge distillation (KD) has been extensively employed to transfer the\nknowledge from a large teacher model to the smaller students, where the\nparameters of the teacher are fixed (or partially) during training. Recent\nstudies show that this mode may cause difficulties in knowledge transfer due to\nthe mismatched model capacities. To alleviate the mismatch problem,\nteacher-student joint training methods, e.g., online distillation, have been\nproposed, but it always requires expensive computational cost. In this paper,\nwe present a parameter-efficient and student-friendly knowledge distillation\nmethod, namely PESF-KD, to achieve efficient and sufficient knowledge transfer\nby updating relatively few partial parameters. Technically, we first\nmathematically formulate the mismatch as the sharpness gap between their\npredictive distributions, where we show such a gap can be narrowed with the\nappropriate smoothness of the soft label. Then, we introduce an adapter module\nfor the teacher and only update the adapter to obtain soft labels with\nappropriate smoothness. Experiments on a variety of benchmarks show that\nPESF-KD can significantly reduce the training cost while obtaining competitive\nresults compared to advanced online distillation methods. Code will be released\nupon acceptance.", "published": "2022-05-28 16:11:49", "link": "http://arxiv.org/abs/2205.15308v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Deep Representation Decomposition for Rate-Invariant Speaker\n  Verification", "abstract": "While promising performance for speaker verification has been achieved by\ndeep speaker embeddings, the advantage would reduce in the case of\nspeaking-style variability. Speaking rate mismatch is often observed in\npractical speaker verification systems, which may actually degrade the system\nperformance. To reduce intra-class discrepancy caused by speaking rate, we\npropose a deep representation decomposition approach with adversarial learning\nto learn speaking rate-invariant speaker embeddings. Specifically, adopting an\nattention block, we decompose the original embedding into an identity-related\ncomponent and a rate-related component through multi-task training.\nAdditionally, to reduce the latent relationship between the two decomposed\ncomponents, we further propose a cosine mapping block to train the parameters\nadversarially to minimize the cosine similarity between the two decomposed\ncomponents. As a result, identity-related features become robust to speaking\nrate and then are used for verification. Experiments are conducted on VoxCeleb1\ndata and HI-MIA data to demonstrate the effectiveness of our proposed approach.", "published": "2022-05-28 01:27:06", "link": "http://arxiv.org/abs/2205.14294v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Is Lip Region-of-Interest Sufficient for Lipreading?", "abstract": "Lip region-of-interest (ROI) is conventionally used for visual input in the\nlipreading task. Few works have adopted the entire face as visual input because\nlip-excluded parts of the face are usually considered to be redundant and\nirrelevant to visual speech recognition. However, faces contain much more\ndetailed information than lips, such as speakers' head pose, emotion, identity\netc. We argue that such information might benefit visual speech recognition if\na powerful feature extractor employing the entire face is trained. In this\nwork, we propose to adopt the entire face for lipreading with self-supervised\nlearning. AV-HuBERT, an audio-visual multi-modal self-supervised learning\nframework, was adopted in our experiments. Our experimental results showed that\nadopting the entire face achieved 16% relative word error rate (WER) reduction\non the lipreading task, compared with the baseline method using lip as visual\ninput. Without self-supervised pretraining, the model with face input achieved\na higher WER than that using lip input in the case of limited training data (30\nhours), while a slightly lower WER when using large amount of training data\n(433 hours).", "published": "2022-05-28 01:34:24", "link": "http://arxiv.org/abs/2205.14295v2", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Feature Pyramid Attention based Residual Neural Network for\n  Environmental Sound Classification", "abstract": "Environmental sound classification (ESC) is a challenging problem due to the\nunstructured spatial-temporal relations that exist in the sound signals.\nRecently, many studies have focused on abstracting features from convolutional\nneural networks while the learning of semantically relevant frames of sound\nsignals has been overlooked. To this end, we present an end-to-end framework,\nnamely feature pyramid attention network (FPAM), focusing on abstracting the\nsemantically relevant features for ESC. We first extract the feature maps of\nthe preprocessed spectrogram of the sound waveform by a backbone network. Then,\nto build multi-scale hierarchical features of sound spectrograms, we construct\na feature pyramid representation of the sound spectrograms by aggregating the\nfeature maps from multi-scale layers, where the temporal frames and spatial\nlocations of semantically relevant frames are localized by FPAM. Specifically,\nthe multiple features are first processed by a dimension alignment module.\nAfterward, the pyramid spatial attention module (PSA) is attached to localize\nthe important frequency regions spatially with a spatial attention module\n(SAM). Last, the processed feature maps are refined by a pyramid channel\nattention (PCA) to localize the important temporal frames. To justify the\neffectiveness of the proposed FPAM, visualization of attention maps on the\nspectrograms has been presented. The visualization results show that FPAM can\nfocus more on the semantic relevant regions while neglecting the noises. The\neffectiveness of the proposed methods is validated on two widely used ESC\ndatasets: the ESC-50 and ESC-10 datasets. The experimental results show that\nthe FPAM yields comparable performance to state-of-the-art methods. A\nsubstantial performance increase has been achieved by FPAM compared with the\nbaseline methods.", "published": "2022-05-28 12:06:11", "link": "http://arxiv.org/abs/2205.14411v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SuperVoice: Text-Independent Speaker Verification Using Ultrasound\n  Energy in Human Speech", "abstract": "Voice-activated systems are integrated into a variety of desktop, mobile, and\nInternet-of-Things (IoT) devices. However, voice spoofing attacks, such as\nimpersonation and replay attacks, in which malicious attackers synthesize the\nvoice of a victim or simply replay it, have brought growing security concerns.\nExisting speaker verification techniques distinguish individual speakers via\nthe spectrographic features extracted from an audible frequency range of voice\ncommands. However, they often have high error rates and/or long delays. In this\npaper, we explore a new direction of human voice research by scrutinizing the\nunique characteristics of human speech at the ultrasound frequency band. Our\nresearch indicates that the high-frequency ultrasound components (e.g. speech\nfricatives) from 20 to 48 kHz can significantly enhance the security and\naccuracy of speaker verification. We propose a speaker verification system,\nSUPERVOICE that uses a two-stream DNN architecture with a feature fusion\nmechanism to generate distinctive speaker models. To test the system, we create\na speech dataset with 12 hours of audio (8,950 voice samples) from 127\nparticipants. In addition, we create a second spoofed voice dataset to evaluate\nits security. In order to balance between controlled recordings and real-world\napplications, the audio recordings are collected from two quiet rooms by 8\ndifferent recording devices, including 7 smartphones and an ultrasound\nmicrophone. Our evaluation shows that SUPERVOICE achieves 0.58% equal error\nrate in the speaker verification task, it only takes 120 ms for testing an\nincoming utterance, outperforming all existing speaker verification systems.\nMoreover, within 91 ms processing time, SUPERVOICE achieves 0% equal error rate\nin detecting replay attacks launched by 5 different loudspeakers.", "published": "2022-05-28 18:00:50", "link": "http://arxiv.org/abs/2205.14496v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
