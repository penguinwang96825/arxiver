{"title": "Natural Language Inference from Multiple Premises", "abstract": "We define a novel textual entailment task that requires inference over\nmultiple premise sentences. We present a new dataset for this task that\nminimizes trivial lexical inferences, emphasizes knowledge of everyday events,\nand presents a more challenging setting for textual entailment. We evaluate\nseveral strong neural baselines and analyze how the multiple premise task\ndiffers from standard textual entailment.", "published": "2017-10-09 03:07:54", "link": "http://arxiv.org/abs/1710.02925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Page Stream Segmentation with Convolutional Neural Nets Combining\n  Textual and Visual Features", "abstract": "In recent years, (retro-)digitizing paper-based files became a major\nundertaking for private and public archives as well as an important task in\nelectronic mailroom applications. As a first step, the workflow involves\nscanning and Optical Character Recognition (OCR) of documents. Preservation of\ndocument contexts of single page scans is a major requirement in this context.\nTo facilitate workflows involving very large amounts of paper scans, page\nstream segmentation (PSS) is the task to automatically separate a stream of\nscanned images into multi-page documents. In a digitization project together\nwith a German federal archive, we developed a novel approach based on\nconvolutional neural networks (CNN) combining image and text features to\nachieve optimal document separation results. Evaluation shows that our PSS\narchitecture achieves an accuracy up to 93 % which can be regarded as a new\nstate-of-the-art for this task.", "published": "2017-10-09 09:29:55", "link": "http://arxiv.org/abs/1710.03006v3", "categories": ["cs.CL", "I.5.4"], "primary_category": "cs.CL"}
{"title": "Deep Learning Paradigm with Transformed Monolingual Word Embeddings for\n  Multilingual Sentiment Analysis", "abstract": "The surge of social media use brings huge demand of multilingual sentiment\nanalysis (MSA) for unveiling cultural difference. So far, traditional methods\nresorted to machine translation---translating texts in other languages to\nEnglish, and then adopt the methods once worked in English. However, this\nparadigm is conditioned by the quality of machine translation. In this paper,\nwe propose a new deep learning paradigm to assimilate the differences between\nlanguages for MSA. We first pre-train monolingual word embeddings separately,\nthen map word embeddings in different spaces into a shared embedding space, and\nthen finally train a parameter-sharing deep neural network for MSA. The\nexperimental results show that our paradigm is effective. Especially, our CNN\nmodel outperforms a state-of-the-art baseline by around 2.1% in terms of\nclassification accuracy.", "published": "2017-10-09 17:30:12", "link": "http://arxiv.org/abs/1710.03203v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What does Attention in Neural Machine Translation Pay Attention to?", "abstract": "Attention in neural machine translation provides the possibility to encode\nrelevant parts of the source sentence at each translation step. As a result,\nattention is considered to be an alignment model as well. However, there is no\nwork that specifically studies attention and provides analysis of what is being\nlearned by attention models. Thus, the question still remains that how\nattention is similar or different from the traditional alignment. In this\npaper, we provide detailed analysis of attention and compare it to traditional\nalignment. We answer the question of whether attention is only capable of\nmodelling translational equivalent or it captures more information. We show\nthat attention is different from alignment in some cases and is capturing\nuseful information other than alignments.", "published": "2017-10-09 23:21:34", "link": "http://arxiv.org/abs/1710.03348v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LD-SDS: Towards an Expressive Spoken Dialogue System based on\n  Linked-Data", "abstract": "In this work we discuss the related challenges and describe an approach\ntowards the fusion of state-of-the-art technologies from the Spoken Dialogue\nSystems (SDS) and the Semantic Web and Information Retrieval domains. We\nenvision a dialogue system named LD-SDS that will support advanced, expressive,\nand engaging user requests, over multiple, complex, rich, and open-domain data\nsources that will leverage the wealth of the available Linked Data.\nSpecifically, we focus on: a) improving the identification, disambiguation and\nlinking of entities occurring in data sources and user input; b) offering\nadvanced query services for exploiting the semantics of the data, with\nreasoning and exploratory capabilities; and c) expanding the typical\ninformation seeking dialogue model (slot filling) to better reflect real-world\nconversational search scenarios.", "published": "2017-10-09 07:36:18", "link": "http://arxiv.org/abs/1710.02973v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Multitask training with unlabeled data for end-to-end sign language\n  fingerspelling recognition", "abstract": "We address the problem of automatic American Sign Language fingerspelling\nrecognition from video. Prior work has largely relied on frame-level labels,\nhand-crafted features, or other constraints, and has been hampered by the\nscarcity of data for this task. We introduce a model for fingerspelling\nrecognition that addresses these issues. The model consists of an\nauto-encoder-based feature extractor and an attention-based neural\nencoder-decoder, which are trained jointly. The model receives a sequence of\nimage frames and outputs the fingerspelled word, without relying on any\nframe-level training labels or hand-crafted features. In addition, the\nauto-encoder subcomponent makes it possible to leverage unlabeled data to\nimprove the feature learning. The model achieves 11.6% and 4.4% absolute letter\naccuracy improvement respectively in signer-independent and signer-adapted\nfingerspelling recognition over previous approaches that required frame-level\ntraining labels.", "published": "2017-10-09 18:21:57", "link": "http://arxiv.org/abs/1710.03255v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A report on sound event detection with different binaural features", "abstract": "In this paper, we compare the performance of using binaural audio features in\nplace of single-channel features for sound event detection. Three different\nbinaural features are studied and evaluated on the publicly available TUT Sound\nEvents 2017 dataset of length 70 minutes. Sound event detection is performed\nseparately with single-channel and binaural features using stacked\nconvolutional and recurrent neural network and the evaluation is reported using\nstandard metrics of error rate and F-score. The studied binaural features are\nseen to consistently perform equal to or better than the single-channel\nfeatures with respect to error rate metric.", "published": "2017-10-09 09:12:01", "link": "http://arxiv.org/abs/1710.02997v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound event detection using weakly labeled dataset with stacked\n  convolutional and recurrent neural network", "abstract": "This paper proposes a neural network architecture and training scheme to\nlearn the start and end time of sound events (strong labels) in an audio\nrecording given just the list of sound events existing in the audio without\ntime information (weak labels). We achieve this by using a stacked\nconvolutional and recurrent neural network with two prediction layers in\nsequence one for the strong followed by the weak label. The network is trained\nusing frame-wise log mel-band energy as the input audio feature, and weak\nlabels provided in the dataset as labels for the weak label prediction layer.\nStrong labels are generated by replicating the weak labels as many number of\ntimes as the frames in the input audio feature, and used for strong label layer\nduring training. We propose to control what the network learns from the weak\nand strong labels by different weighting for the loss computed in the two\nprediction layers. The proposed method is evaluated on a publicly available\ndataset of 155 hours with 17 sound event classes. The method achieves the best\nerror rate of 0.84 for strong labels and F-score of 43.3% for weak labels on\nthe unseen test split.", "published": "2017-10-09 09:14:24", "link": "http://arxiv.org/abs/1710.02998v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
