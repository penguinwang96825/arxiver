{"title": "Causal ATE Mitigates Unintended Bias in Controlled Text Generation", "abstract": "We study attribute control in language models through the method of Causal\nAverage Treatment Effect (Causal ATE). Existing methods for the attribute\ncontrol task in Language Models (LMs) check for the co-occurrence of words in a\nsentence with the attribute of interest, and control for them. However,\nspurious correlation of the words with the attribute in the training dataset,\ncan cause models to hallucinate the presence of the attribute when presented\nwith the spurious correlate during inference. We show that the simple\nperturbation-based method of Causal ATE removes this unintended effect.\nSpecifically, we ground it in the problem of toxicity mitigation, where a\nsignificant challenge lies in the inadvertent bias that often emerges towards\nprotected groups post detoxification. We show that this unintended bias can be\nsolved by the use of the Causal ATE metric and rigorously prove our claim. We\nprovide experimental validations for our claims and release our code\n(anonymously) here:\nhttps://github.com/causalate-mitigates-bias/causal-ate-mitigates-bias.", "published": "2023-11-19 05:00:39", "link": "http://arxiv.org/abs/2311.11229v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Large Language Models in Mental Health Applications", "abstract": "Large Language Models (LLMs) have become valuable assets in mental health,\nshowing promise in both classification tasks and counseling applications. This\npaper offers a perspective on using LLMs in mental health applications. It\ndiscusses the instability of generative models for prediction and the potential\nfor generating hallucinatory outputs, underscoring the need for ongoing audits\nand evaluations to maintain their reliability and dependability. The paper also\ndistinguishes between the often interchangeable terms ``explainability'' and\n``interpretability'', advocating for developing inherently interpretable\nmethods instead of relying on potentially hallucinated self-explanations\ngenerated by LLMs. Despite the advancements in LLMs, human counselors'\nempathetic understanding, nuanced interpretation, and contextual awareness\nremain irreplaceable in the sensitive and complex realm of mental health\ncounseling. The use of LLMs should be approached with a judicious and\nconsiderate mindset, viewing them as tools that complement human expertise\nrather than seeking to replace it.", "published": "2023-11-19 08:40:01", "link": "http://arxiv.org/abs/2311.11267v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cross-Attention Augmented Model for Event-Triggered Context-Aware\n  Story Generation", "abstract": "Despite recent advancements, existing story generation systems continue to\nencounter difficulties in effectively incorporating contextual and event\nfeatures, which greatly influence the quality of generated narratives. To\ntackle these challenges, we introduce a novel neural generation model, EtriCA,\nthat enhances the relevance and coherence of generated stories by employing a\ncross-attention mechanism to map context features onto event sequences through\nresidual mapping. This feature capturing mechanism enables our model to exploit\nlogical relationships between events more effectively during the story\ngeneration process. To further enhance our proposed model, we employ a\npost-training framework for knowledge enhancement (KeEtriCA) on a large-scale\nbook corpus. This allows EtriCA to adapt to a wider range of data samples. This\nresults in approximately 5\\% improvement in automatic metrics and over 10\\%\nimprovement in human evaluation. We conduct extensive experiments, including\ncomparisons with state-of-the-art (SOTA) baseline models, to evaluate the\nperformance of our framework on story generation. The experimental results,\nencompassing both automated metrics and human assessments, demonstrate the\nsuperiority of our model over existing state-of-the-art baselines. These\nresults underscore the effectiveness of our model in leveraging context and\nevent features to improve the quality of generated narratives.", "published": "2023-11-19 08:54:47", "link": "http://arxiv.org/abs/2311.11271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies", "abstract": "Various NLP tasks require a complex hierarchical structure over nodes, where\neach node is a cluster of items. Examples include generating entailment graphs,\nhierarchical cross-document coreference resolution, annotating event and\nsubevent relations, etc. To enable efficient annotation of such hierarchical\nstructures, we release CHAMP, an open source tool allowing to incrementally\nconstruct both clusters and hierarchy simultaneously over any type of texts.\nThis incremental approach significantly reduces annotation time compared to the\ncommon pairwise annotation approach and also guarantees maintaining\ntransitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a\nconsolidation mode, where an adjudicator can easily compare multiple cluster\nhierarchy annotations and resolve disagreements.", "published": "2023-11-19 11:22:00", "link": "http://arxiv.org/abs/2311.11301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for\n  Improving ASR Robustness in Spoken Language Understanding", "abstract": "Spoken language understanding (SLU) is a fundamental task in the\ntask-oriented dialogue systems. However, the inevitable errors from automatic\nspeech recognition (ASR) usually impair the understanding performance and lead\nto error propagation. Although there are some attempts to address this problem\nthrough contrastive learning, they (1) treat clean manual transcripts and ASR\ntranscripts equally without discrimination in fine-tuning; (2) neglect the fact\nthat the semantically similar pairs are still pushed away when applying\ncontrastive learning; (3) suffer from the problem of Kullback-Leibler (KL)\nvanishing. In this paper, we propose Mutual Learning and Large-Margin\nContrastive Learning (ML-LMCL), a novel framework for improving ASR robustness\nin SLU. Specifically, in fine-tuning, we apply mutual learning and train two\nSLU models on the manual transcripts and the ASR transcripts, respectively,\naiming to iteratively share knowledge between these two models. We also\nintroduce a distance polarization regularizer to avoid pushing away the\nintra-cluster pairs as much as possible. Moreover, we use a cyclical annealing\nschedule to mitigate KL vanishing issue. Experiments on three datasets show\nthat ML-LMCL outperforms existing models and achieves new state-of-the-art\nperformance.", "published": "2023-11-19 16:53:35", "link": "http://arxiv.org/abs/2311.11375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using\n  Clustering and Information Theory Techniques", "abstract": "With the development of generative models like GPT-3, it is increasingly more\nchallenging to differentiate generated texts from human-written ones. There is\na large number of studies that have demonstrated good results in bot\nidentification. However, the majority of such works depend on supervised\nlearning methods that require labelled data and/or prior knowledge about the\nbot-model architecture. In this work, we propose a bot identification algorithm\nthat is based on unsupervised learning techniques and does not depend on a\nlarge amount of labelled data. By combining findings in semantic analysis by\nclustering (crisp and fuzzy) and information techniques, we construct a robust\nmodel that detects a generated text for different types of bot. We find that\nthe generated texts tend to be more chaotic while literary works are more\ncomplex. We also demonstrate that the clustering of human texts results in\nfuzzier clusters in comparison to the more compact and well-separated clusters\nof bot-generated texts.", "published": "2023-11-19 22:29:15", "link": "http://arxiv.org/abs/2311.11441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPLAIN: Augmenting Cybersecurity Warnings with Reasons and Data", "abstract": "Effective cyber threat recognition and prevention demand comprehensible\nforecasting systems, as prior approaches commonly offer limited and,\nultimately, unconvincing information. We introduce Simplified Plaintext\nLanguage (SPLAIN), a natural language generator that converts warning data into\nuser-friendly cyber threat explanations. SPLAIN is designed to generate clear,\nactionable outputs, incorporating hierarchically organized explanatory details\nabout input data and system functionality. Given the inputs of individual\nsensor-induced forecasting signals and an overall warning from a fusion module,\nSPLAIN queries each signal for information on contributing sensors and data\nsignals. This collected data is processed into a coherent English explanation,\nencompassing forecasting, sensing, and data elements for user review. SPLAIN's\ntemplate-based approach ensures consistent warning structure and vocabulary.\nSPLAIN's hierarchical output structure allows each threat and its components to\nbe expanded to reveal underlying explanations on demand. Our conclusions\nemphasize the need for designers to specify the \"how\" and \"why\" behind cyber\nwarnings, advocate for simple structured templates in generating consistent\nexplanations, and recognize that direct causal links in Machine Learning\napproaches may not always be identifiable, requiring some explanations to focus\non general methodologies, such as model and training data.", "published": "2023-11-19 03:43:42", "link": "http://arxiv.org/abs/2311.11215v1", "categories": ["cs.CL", "cs.AI", "I.2"], "primary_category": "cs.CL"}
{"title": "Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis\n  of COVID-19 Vaccines in India", "abstract": "In March 2020, the World Health Organisation declared COVID-19 a global\npandemic as it spread to nearly every country. By mid-2021, India had\nintroduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure\nsuccessful vaccination in a densely populated country like India, understanding\npublic sentiment was crucial. Social media, particularly Reddit with over 430\nmillion users, played a vital role in disseminating information. This study\nemploys data mining techniques to analyze Reddit data and gauge Indian\nsentiments towards COVID-19 vaccines. Using Python's Text Blob library,\ncomments are annotated to assess general sentiments. Results show that most\nReddit users in India expressed neutrality about vaccination, posing a\nchallenge for the Indian government's efforts to vaccinate a significant\nportion of the population.", "published": "2023-11-19 22:14:48", "link": "http://arxiv.org/abs/2311.11435v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM aided semi-supervision for Extractive Dialog Summarization", "abstract": "Generating high-quality summaries for chat dialogs often requires large\nlabeled datasets. We propose a method to efficiently use unlabeled data for\nextractive summarization of customer-agent dialogs. In our method, we frame\nsummarization as a question-answering problem and use state-of-the-art large\nlanguage models (LLMs) to generate pseudo-labels for a dialog. We then use\nthese pseudo-labels to fine-tune a chat summarization model, effectively\ntransferring knowledge from the large LLM into a smaller specialized model. We\ndemonstrate our method on the \\tweetsumm dataset, and show that using 10% of\nthe original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L,\nwhereas the current state-of-the-art trained on the entire training data set\nobtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case\n(i.e., ROUGE-L) we still effectively retain 94.7% of the performance while\nusing only 10% of the data.", "published": "2023-11-19 23:59:22", "link": "http://arxiv.org/abs/2311.11462v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unmasking and Improving Data Credibility: A Study with Datasets for\n  Training Harmless Language Models", "abstract": "Language models have shown promise in various tasks but can be affected by\nundesired data during training, fine-tuning, or alignment. For example, if some\nunsafe conversations are wrongly annotated as safe ones, the model fine-tuned\non these samples may be harmful. Therefore, the correctness of annotations,\ni.e., the credibility of the dataset, is important. This study focuses on the\ncredibility of real-world datasets, including the popular benchmarks Jigsaw\nCivil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that\ncan be used for training a harmless language model. Given the cost and\ndifficulty of cleaning these datasets by humans, we introduce a systematic\nframework for evaluating the credibility of datasets, identifying label errors,\nand evaluating the influence of noisy labels in the curated language data,\nspecifically focusing on unsafe comments and conversation classification. With\nthe framework, we find and fix an average of 6.16% label errors in 11 datasets\nconstructed from the above benchmarks. The data credibility and downstream\nlearning performance can be remarkably improved by directly fixing label\nerrors, indicating the significance of cleaning existing real-world datasets.\nWe provide an open-source tool, Docta, for data cleaning at\nhttps://github.com/Docta-ai/docta.", "published": "2023-11-19 02:34:12", "link": "http://arxiv.org/abs/2311.11202v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Towards Real-World Writing Assistance: A Chinese Character Checking\n  Benchmark with Faked and Misspelled Characters", "abstract": "Writing assistance is an application closely related to human life and is\nalso a fundamental Natural Language Processing (NLP) research field. Its aim is\nto improve the correctness and quality of input texts, with character checking\nbeing crucial in detecting and correcting wrong characters. From the\nperspective of the real world where handwriting occupies the vast majority,\ncharacters that humans get wrong include faked characters (i.e., untrue\ncharacters created due to writing errors) and misspelled characters (i.e., true\ncharacters used incorrectly due to spelling errors). However, existing datasets\nand related studies only focus on misspelled characters mainly caused by\nphonological or visual confusion, thereby ignoring faked characters which are\nmore common and difficult. To break through this dilemma, we present\nVisual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with\nfaked and misspelled Chinese characters. To the best of our knowledge,\nVisual-C$^3$ is the first real-world visual and the largest human-crafted\ndataset for the Chinese character checking scenario. Additionally, we also\npropose and evaluate novel baseline methods on Visual-C$^3$. Extensive\nempirical results and analyses show that Visual-C$^3$ is high-quality yet\nchallenging. The Visual-C$^3$ dataset and the baseline methods will be publicly\navailable to facilitate further research in the community.", "published": "2023-11-19 08:41:43", "link": "http://arxiv.org/abs/2311.11268v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Portuguese FAQ for Financial Services", "abstract": "Scarcity of domain-specific data in the Portuguese financial domain has\ndisfavored the development of Natural Language Processing (NLP) applications.\nTo address this limitation, the present study advocates for the utilization of\nsynthetic data generated through data augmentation techniques. The\ninvestigation focuses on the augmentation of a dataset sourced from the Central\nBank of Brazil FAQ, employing techniques that vary in semantic similarity.\nSupervised and unsupervised tasks are conducted to evaluate the impact of\naugmented data on both low and high semantic similarity scenarios.\nAdditionally, the resultant dataset will be publicly disseminated on the\nHugging Face Datasets platform, thereby enhancing accessibility and fostering\nbroader engagement within the NLP research community.", "published": "2023-11-19 14:07:57", "link": "http://arxiv.org/abs/2311.11331v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Question Answering over Financial Documents using Large\n  Language Models", "abstract": "We introduce a large language model (LLM) based approach to answer complex\nquestions requiring multi-hop numerical reasoning over financial reports. While\nLLMs have exhibited remarkable performance on various natural language and\nreasoning tasks, complex reasoning problems often rely on few-shot prompts that\nrequire carefully crafted examples. In contrast, our approach uses novel\nzero-shot prompts that guide the LLM to encode the required reasoning into a\nPython program or a domain specific language. The generated program is then\nexecuted by a program interpreter, thus mitigating the limitations of LLM in\nperforming accurate arithmetic calculations.\n  We evaluate the proposed approach on three financial datasets using some of\nthe recently developed generative pretrained transformer (GPT) models and\nperform comparisons with various zero-shot baselines. The experimental results\ndemonstrate that our approach significantly improves the accuracy for all the\nLLMs over their respective baselines. We provide a detailed analysis of the\nresults, generating insights to support our findings. The success of our\napproach demonstrates the enormous potential to extract complex domain specific\nnumerical reasoning by designing zero-shot prompts to effectively exploit the\nknowledge embedded in LLMs.", "published": "2023-11-19 16:23:34", "link": "http://arxiv.org/abs/2311.14722v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Security Risk Taxonomy for Prompt-Based Interaction With Large\n  Language Models", "abstract": "As large language models (LLMs) permeate more and more applications, an\nassessment of their associated security risks becomes increasingly necessary.\nThe potential for exploitation by malicious actors, ranging from disinformation\nto data breaches and reputation damage, is substantial. This paper addresses a\ngap in current research by specifically focusing on security risks posed by\nLLMs within the prompt-based interaction scheme, which extends beyond the\nwidely covered ethical and societal implications. Our work proposes a taxonomy\nof security risks along the user-model communication pipeline and categorizes\nthe attacks by target and attack type alongside the commonly used\nconfidentiality, integrity, and availability (CIA) triad. The taxonomy is\nreinforced with specific attack examples to showcase the real-world impact of\nthese risks. Through this taxonomy, we aim to inform the development of robust\nand secure LLM applications, enhancing their safety and trustworthiness.", "published": "2023-11-19 20:22:05", "link": "http://arxiv.org/abs/2311.11415v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Label-Synchronous Neural Transducer for Adaptable Online E2E Speech\n  Recognition", "abstract": "Although end-to-end (E2E) automatic speech recognition (ASR) has shown\nstate-of-the-art recognition accuracy, it tends to be implicitly biased towards\nthe training data distribution which can degrade generalisation. This paper\nproposes a label-synchronous neural transducer (LS-Transducer), which provides\na natural approach to domain adaptation based on text-only data. The\nLS-Transducer extracts a label-level encoder representation before combining it\nwith the prediction network output. Since blank tokens are no longer needed,\nthe prediction network performs as a standard language model, which can be\neasily adapted using text-only data. An Auto-regressive Integrate-and-Fire\n(AIF) mechanism is proposed to generate the label-level encoder representation\nwhile retaining low latency operation that can be used for streaming. In\naddition, a streaming joint decoding method is designed to improve ASR accuracy\nwhile retaining synchronisation with AIF. Experiments show that compared to\nstandard neural transducers, the proposed LS-Transducer gave a 12.9% relative\nWER reduction (WERR) for intra-domain LibriSpeech data, as well as 21.4% and\n24.6% relative WERRs on cross-domain TED-LIUM 2 and AESRC2020 data with an\nadapted prediction network.", "published": "2023-11-19 15:31:42", "link": "http://arxiv.org/abs/2311.11353v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "M$^{2}$UGen: Multi-modal Music Understanding and Generation with the\n  Power of Large Language Models", "abstract": "The current landscape of research leveraging large language models (LLMs) is\nexperiencing a surge. Many works harness the powerful reasoning capabilities of\nthese models to comprehend various modalities, such as text, speech, images,\nvideos, etc. They also utilize LLMs to understand human intention and generate\ndesired outputs like images, videos, and music. However, research that combines\nboth understanding and generation using LLMs is still limited and in its\nnascent stage. To address this gap, we introduce a Multi-modal Music\nUnderstanding and Generation (M$^{2}$UGen) framework that integrates LLM's\nabilities to comprehend and generate music for different modalities. The\nM$^{2}$UGen framework is purpose-built to unlock creative potential from\ndiverse sources of inspiration, encompassing music, image, and video through\nthe use of pretrained MERT, ViT, and ViViT models, respectively. To enable\nmusic generation, we explore the use of AudioLDM 2 and MusicGen. Bridging\nmulti-modal understanding and music generation is accomplished through the\nintegration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA\nmodel to generate extensive datasets that support text/image/video-to-music\ngeneration, facilitating the training of our M$^{2}$UGen framework. We conduct\na thorough evaluation of our proposed framework. The experimental results\ndemonstrate that our model achieves or surpasses the performance of the current\nstate-of-the-art models.", "published": "2023-11-19 06:50:52", "link": "http://arxiv.org/abs/2311.11255v5", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Encoding Performance Data in MEI with the Automatic Music Performance\n  Analysis and Comparison Toolkit (AMPACT)", "abstract": "This paper presents a new method of encoding performance data in MEI using\nthe recently added \\texttt{<extData>} element. Performance data was extracted\nusing the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT)\nand encoded as a JSON object within an \\texttt{<extData>} element linked to a\nspecific musical note. A set of pop music vocals has was encoded to demonstrate\nboth the range of descriptors that can be encoded in <extData> and how AMPACT\ncan be used for extracting performance data in the absence of a fully specified\nmusical score.", "published": "2023-11-19 16:18:58", "link": "http://arxiv.org/abs/2311.11363v1", "categories": ["cs.SD", "cs.DL", "eess.AS"], "primary_category": "cs.SD"}
