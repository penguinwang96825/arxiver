{"title": "CNewSum: A Large-scale Chinese News Summarization Dataset with\n  Human-annotated Adequacy and Deducibility Level", "abstract": "Automatic text summarization aims to produce a brief but crucial summary for\nthe input documents. Both extractive and abstractive methods have witnessed\ngreat success in English datasets in recent years. However, there has been a\nminimal exploration of text summarization in Chinese, limited by the lack of\nlarge-scale datasets. In this paper, we present a large-scale Chinese news\nsummarization dataset CNewSum, which consists of 304,307 documents and\nhuman-written summaries for the news feed. It has long documents with\nhigh-abstractive summaries, which can encourage document-level understanding\nand generation for current summarization models. An additional distinguishing\nfeature of CNewSum is that its test set contains adequacy and deducibility\nannotations for the summaries. The adequacy level measures the degree of\nsummary information covered by the document, and the deducibility indicates the\nreasoning ability the model needs to generate the summary. These annotations\ncan help researchers analyze and target their model performance bottleneck. We\nexamine recent methods on CNewSum and release our dataset to provide a solid\ntestbed for automatic Chinese summarization research.", "published": "2021-10-21 03:37:46", "link": "http://arxiv.org/abs/2110.10874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Non-autoregressive Generation with Mixup Training", "abstract": "While pre-trained language models have achieved great success on various\nnatural language understanding tasks, how to effectively leverage them into\nnon-autoregressive generation tasks remains a challenge. To solve this problem,\nwe present a non-autoregressive generation model based on pre-trained\ntransformer models. To bridge the gap between autoregressive and\nnon-autoregressive models, we propose a simple and effective iterative training\nmethod called MIx Source and pseudo Target (MIST). Unlike other iterative\ndecoding methods, which sacrifice the inference speed to achieve better\nperformance based on multiple decoding iterations, MIST works in the training\nstage and has no effect on inference time. Our experiments on three generation\nbenchmarks including question generation, summarization and paraphrase\ngeneration, show that the proposed framework achieves the new state-of-the-art\nresults for fully non-autoregressive models. We also demonstrate that our\nmethod can be used to a variety of pre-trained models. For instance, MIST based\non the small pre-trained model also obtains comparable performance with seq2seq\nmodels.", "published": "2021-10-21 13:04:21", "link": "http://arxiv.org/abs/2110.11115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Asynchronous Decentralized Distributed Training of Acoustic Models", "abstract": "Large-scale distributed training of deep acoustic models plays an important\nrole in today's high-performance automatic speech recognition (ASR). In this\npaper we investigate a variety of asynchronous decentralized distributed\ntraining strategies based on data parallel stochastic gradient descent (SGD) to\nshow their superior performance over the commonly-used synchronous distributed\ntraining via allreduce, especially when dealing with large batch sizes.\nSpecifically, we study three variants of asynchronous decentralized parallel\nSGD (ADPSGD), namely, fixed and randomized communication patterns on a ring as\nwell as a delay-by-one scheme. We introduce a mathematical model of ADPSGD,\ngive its theoretical convergence rate, and compare the empirical convergence\nbehavior and straggler resilience properties of the three variants. Experiments\nare carried out on an IBM supercomputer for training deep long short-term\nmemory (LSTM) acoustic models on the 2000-hour Switchboard dataset. Recognition\nand speedup performance of the proposed strategies are evaluated under various\ntraining configurations. We show that ADPSGD with fixed and randomized\ncommunication patterns cope well with slow learners. When learners are equally\nfast, ADPSGD with the delay-by-one strategy has the fastest convergence with\nlarge batches. In particular, using the delay-by-one strategy, we can train the\nacoustic model in less than 2 hours using 128 V100 GPUs with competitive word\nerror rates.", "published": "2021-10-21 15:14:58", "link": "http://arxiv.org/abs/2110.11199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Principled Representation Learning for Entity Alignment", "abstract": "Embedding-based entity alignment (EEA) has recently received great attention.\nDespite significant performance improvement, few efforts have been paid to\nfacilitate understanding of EEA methods. Most existing studies rest on the\nassumption that a small number of pre-aligned entities can serve as anchors\nconnecting the embedding spaces of two KGs. Nevertheless, no one investigates\nthe rationality of such an assumption. To fill the research gap, we define a\ntypical paradigm abstracted from existing EEA methods and analyze how the\nembedding discrepancy between two potentially aligned entities is implicitly\nbounded by a predefined margin in the scoring function. Further, we find that\nsuch a bound cannot guarantee to be tight enough for alignment learning. We\nmitigate this problem by proposing a new approach, named NeoEA, to explicitly\nlearn KG-invariant and principled entity embeddings. In this sense, an EEA\nmodel not only pursues the closeness of aligned entities based on geometric\ndistance, but also aligns the neural ontologies of two KGs by eliminating the\ndiscrepancy in embedding distribution and underlying ontology knowledge. Our\nexperiments demonstrate consistent and significant improvement in performance\nagainst the best-performing EEA methods.", "published": "2021-10-21 03:21:58", "link": "http://arxiv.org/abs/2110.10871v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Performance in Open-Domain Dialogue with PARADISE", "abstract": "There has recently been an explosion of work on spoken dialogue systems,\nalong with an increased interest in open-domain systems that engage in casual\nconversations on popular topics such as movies, books and music. These systems\naim to socially engage, entertain, and even empathize with their users. Since\nthe achievement of such social goals is hard to measure, recent research has\nused dialogue length or human ratings as evaluation metrics, and developed\nmethods for automatically calculating novel metrics, such as coherence,\nconsistency, relevance and engagement. Here we develop a PARADISE model for\npredicting the performance of Athena, a dialogue system that has participated\nin thousands of conversations with real users, while competing as a finalist in\nthe Alexa Prize. We use both user ratings and dialogue length as metrics for\ndialogue quality, and experiment with predicting these metrics using automatic\nfeatures that are both system dependent and independent. Our goal is to learn a\ngeneral objective function that can be used to optimize the dialogue choices of\nany Alexa Prize system in real time and evaluate its performance. Our best\nmodel for predicting user ratings gets an R$^2$ of .136 with a DistilBert\nmodel, and the best model for predicting length with system independent\nfeatures gets an R$^2$ of .865, suggesting that conversation length may be a\nmore reliable measure for automatic training of dialogue systems.", "published": "2021-10-21 14:17:59", "link": "http://arxiv.org/abs/2110.11164v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topic-Guided Abstractive Multi-Document Summarization", "abstract": "A critical point of multi-document summarization (MDS) is to learn the\nrelations among various documents. In this paper, we propose a novel\nabstractive MDS model, in which we represent multiple documents as a\nheterogeneous graph, taking semantic nodes of different granularities into\naccount, and then apply a graph-to-sequence framework to generate summaries.\nMoreover, we employ a neural topic model to jointly discover latent topics that\ncan act as cross-document semantic units to bridge different documents and\nprovide global information to guide the summary generation. Since topic\nextraction can be viewed as a special type of summarization that \"summarizes\"\ntexts into a more abstract format, i.e., a topic distribution, we adopt a\nmulti-task learning strategy to jointly train the topic and summarization\nmodule, allowing the promotion of each other. Experimental results on the\nMulti-News dataset demonstrate that our model outperforms previous\nstate-of-the-art MDS models on both Rouge metrics and human evaluation,\nmeanwhile learns high-quality topics.", "published": "2021-10-21 15:32:30", "link": "http://arxiv.org/abs/2110.11207v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Anti-Vaccine Users on Twitter", "abstract": "Vaccine hesitancy, which has recently been driven by online narratives,\nsignificantly degrades the efficacy of vaccination strategies, such as those\nfor COVID-19. Despite broad agreement in the medical community about the safety\nand efficacy of available vaccines, a large number of social media users\ncontinue to be inundated with false information about vaccines and are\nindecisive or unwilling to be vaccinated. The goal of this study is to better\nunderstand anti-vaccine sentiment by developing a system capable of\nautomatically identifying the users responsible for spreading anti-vaccine\nnarratives. We introduce a publicly available Python package capable of\nanalyzing Twitter profiles to assess how likely that profile is to share\nanti-vaccine sentiment in the future. The software package is built using text\nembedding methods, neural networks, and automated dataset generation and is\ntrained on several million tweets. We find this model can accurately detect\nanti-vaccine users up to a year before they tweet anti-vaccine hashtags or\nkeywords. We also show examples of how text analysis helps us understand\nanti-vaccine discussions by detecting moral and emotional differences between\nanti-vaccine spreaders on Twitter and regular users. Our results will help\nresearchers and policy-makers understand how users become anti-vaccine and what\nthey discuss on Twitter. Policy-makers can utilize this information for better\ntargeted campaigns that debunk harmful anti-vaccination myths.", "published": "2021-10-21 17:59:25", "link": "http://arxiv.org/abs/2110.11333v3", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "SYNERGY: Building Task Bots at Scale Using Symbolic Knowledge and\n  Machine Teaching", "abstract": "In this paper we explore the use of symbolic knowledge and machine teaching\nto reduce human data labeling efforts in building neural task bots. We propose\nSYNERGY, a hybrid learning framework where a task bot is developed in two\nsteps: (i) Symbolic knowledge to neural networks: Large amounts of simulated\ndialog sessions are generated based on task-specific symbolic knowledge which\nis represented as a task schema consisting of dialog flows and task-oriented\ndatabases. Then a pre-trained neural dialog model, SOLOIST, is fine-tuned on\nthe simulated dialogs to build a bot for the task. (ii) Neural learning: The\nfine-tuned neural dialog model is continually refined with a handful of real\ntask-specific dialogs via machine teaching, where training samples are\ngenerated by human teachers interacting with the task bot. We validate SYNERGY\non four dialog tasks. Experimental results show that SYNERGY maps task-specific\nknowledge into neural dialog models achieving greater diversity and coverage of\ndialog flows, and continually improves model performance with machine teaching,\nthus demonstrating strong synergistic effects of symbolic knowledge and machine\nteaching.", "published": "2021-10-21 23:13:04", "link": "http://arxiv.org/abs/2110.11514v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Integrating Visuospatial, Linguistic and Commonsense Structure into\n  Story Visualization", "abstract": "While much research has been done in text-to-image synthesis, little work has\nbeen done to explore the usage of linguistic structure of the input text. Such\ninformation is even more important for story visualization since its inputs\nhave an explicit narrative structure that needs to be translated into an image\nsequence (or visual story). Prior work in this domain has shown that there is\nample room for improvement in the generated image sequence in terms of visual\nquality, consistency and relevance. In this paper, we first explore the use of\nconstituency parse trees using a Transformer-based recurrent architecture for\nencoding structured input. Second, we augment the structured input with\ncommonsense information and study the impact of this external knowledge on the\ngeneration of visual story. Third, we also incorporate visual structure via\nbounding boxes and dense captioning to provide feedback about the\ncharacters/objects in generated images within a dual learning setup. We show\nthat off-the-shelf dense-captioning models trained on Visual Genome can improve\nthe spatial structure of images from a different target domain without needing\nfine-tuning. We train the model end-to-end using intra-story contrastive loss\n(between words and image sub-regions) and show significant improvements in\nseveral metrics (and human evaluation) for multiple datasets. Finally, we\nprovide an analysis of the linguistic and visuo-spatial information. Code and\ndata: https://github.com/adymaharana/VLCStoryGan.", "published": "2021-10-21 00:16:02", "link": "http://arxiv.org/abs/2110.10834v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Single-Modal Entropy based Active Learning for Visual Question Answering", "abstract": "Constructing a large-scale labeled dataset in the real world, especially for\nhigh-level tasks (eg, Visual Question Answering), can be expensive and\ntime-consuming. In addition, with the ever-growing amounts of data and\narchitecture complexity, Active Learning has become an important aspect of\ncomputer vision research. In this work, we address Active Learning in the\nmulti-modal setting of Visual Question Answering (VQA). In light of the\nmulti-modal inputs, image and question, we propose a novel method for effective\nsample acquisition through the use of ad hoc single-modal branches for each\ninput to leverage its information. Our mutual information based sample\nacquisition strategy Single-Modal Entropic Measure (SMEM) in addition to our\nself-distillation technique enables the sample acquisitor to exploit all\npresent modalities and find the most informative samples. Our novel idea is\nsimple to implement, cost-efficient, and readily adaptable to other multi-modal\ntasks. We confirm our findings on various VQA datasets through state-of-the-art\nperformance by comparing to existing Active Learning baselines.", "published": "2021-10-21 05:38:45", "link": "http://arxiv.org/abs/2110.10906v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Neuro-Symbolic Reinforcement Learning with First-Order Logic", "abstract": "Deep reinforcement learning (RL) methods often require many trials before\nconvergence, and no direct interpretability of trained policies is provided. In\norder to achieve fast convergence and interpretability for the policy in RL, we\npropose a novel RL method for text-based games with a recent neuro-symbolic\nframework called Logical Neural Network, which can learn symbolic and\ninterpretable rules in their differentiable network. The method is first to\nextract first-order logical facts from text observation and external word\nmeaning network (ConceptNet), then train a policy in the network with directly\ninterpretable logical operators. Our experimental results show RL training with\nthe proposed method converges significantly faster than other state-of-the-art\nneuro-symbolic methods in a TextWorld benchmark.", "published": "2021-10-21 08:21:49", "link": "http://arxiv.org/abs/2110.10963v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "LOA: Logical Optimal Actions for Text-based Interaction Games", "abstract": "We present Logical Optimal Actions (LOA), an action decision architecture of\nreinforcement learning applications with a neuro-symbolic framework which is a\ncombination of neural network and symbolic knowledge acquisition approach for\nnatural language interaction games. The demonstration for LOA experiments\nconsists of a web-based interactive platform for text-based games and\nvisualization for acquired knowledge for improving interpretability for trained\nrules. This demonstration also provides a comparison module with other\nneuro-symbolic approaches as well as non-symbolic state-of-the-art agent models\non the same text-based games. Our LOA also provides open-sourced implementation\nin Python for the reinforcement learning environment to facilitate an\nexperiment for studying neuro-symbolic agents. Code: https://github.com/ibm/loa", "published": "2021-10-21 08:36:11", "link": "http://arxiv.org/abs/2110.10973v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Robustness through Data Augmentation Loss Consistency", "abstract": "While deep learning through empirical risk minimization (ERM) has succeeded\nat achieving human-level performance at a variety of complex tasks, ERM is not\nrobust to distribution shifts or adversarial attacks. Synthetic data\naugmentation followed by empirical risk minimization (DA-ERM) is a simple and\nwidely used solution to improve robustness in ERM. In addition, consistency\nregularization can be applied to further improve the robustness of the model by\nforcing the representation of the original sample and the augmented one to be\nsimilar. However, existing consistency regularization methods are not\napplicable to covariant data augmentation, where the label in the augmented\nsample is dependent on the augmentation function. For example, dialog state\ncovaries with named entity when we augment data with a new named entity. In\nthis paper, we propose data augmented loss invariant regularization (DAIR), a\nsimple form of consistency regularization that is applied directly at the loss\nlevel rather than intermediate features, making it widely applicable to both\ninvariant and covariant data augmentation regardless of network architecture,\nproblem setup, and task. We apply DAIR to real-world learning problems\ninvolving covariant data augmentation: robust neural task-oriented dialog state\ntracking and robust visual question answering. We also apply DAIR to tasks\ninvolving invariant data augmentation: robust regression, robust classification\nagainst adversarial attacks, and robust ImageNet classification under\ndistribution shift. Our experiments show that DAIR consistently outperforms ERM\nand DA-ERM with little marginal computational cost and sets new\nstate-of-the-art results in several benchmarks involving covariant data\naugmentation. Our code of all experiments is available at:\nhttps://github.com/optimization-for-data-driven-science/DAIR.git", "published": "2021-10-21 15:30:40", "link": "http://arxiv.org/abs/2110.11205v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Fast Model Editing at Scale", "abstract": "While large pre-trained models have enabled impressive results on a variety\nof downstream tasks, the largest existing models still make errors, and even\naccurate predictions may become outdated over time. Because detecting all such\nfailures at training time is impossible, enabling both developers and end users\nof such models to correct inaccurate outputs while leaving the model otherwise\nintact is desirable. However, the distributed, black-box nature of the\nrepresentations learned by large neural networks makes producing such targeted\nedits difficult. If presented with only a single problematic input and new\ndesired output, fine-tuning approaches tend to overfit; other editing\nalgorithms are either computationally infeasible or simply ineffective when\napplied to very large models. To enable easy post-hoc editing at scale, we\npropose Model Editor Networks using Gradient Decomposition (MEND), a collection\nof small auxiliary editing networks that use a single desired input-output pair\nto make fast, local edits to a pre-trained model's behavior. MEND learns to\ntransform the gradient obtained by standard fine-tuning, using a low-rank\ndecomposition of the gradient to make the parameterization of this\ntransformation tractable. MEND can be trained on a single GPU in less than a\nday even for 10 billion+ parameter models; once trained MEND enables rapid\napplication of new edits to the pre-trained model. Our experiments with T5,\nGPT, BERT, and BART models show that MEND is the only approach to model editing\nthat effectively edits the behavior of models with more than 10 billion\nparameters. Code and data available at\nhttps://sites.google.com/view/mend-editing.", "published": "2021-10-21 17:41:56", "link": "http://arxiv.org/abs/2110.11309v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Objective Measures of Perceptual Audio Quality Reviewed: An Evaluation\n  of Their Application Domain Dependence", "abstract": "Over the past few decades, computational methods have been developed to\nestimate perceptual audio quality. These methods, also referred to as objective\nquality measures, are usually developed and intended for a specific application\ndomain. Because of their convenience, they are often used outside their\noriginal intended domain, even if it is unclear whether they provide reliable\nquality estimates in this case. This work studies the correlation of well-known\nstate-of-the-art objective measures with human perceptual scores in two\ndifferent domains: audio coding and source separation. The following objective\nmeasures are considered: fwSNRseg, dLLR, PESQ, PEAQ, POLQA, PEMO-Q,\nViSQOLAudio, (SI-)BSSEval, PEASS, LKR-PI, 2f-model, and HAAQI. Additionally, a\nnovel measure (SI-SA2f) is presented, based on the 2f-model and a BSSEval-based\nsignal decomposition. We use perceptual scores from 7 listening tests about\naudio coding and 7 listening tests about source separation as ground-truth data\nfor the correlation analysis. The results show that one method (2f-model)\nperforms significantly better than the others on both domains and indicate that\nthe dataset for training the method and a robust underlying auditory model are\ncrucial factors towards a universal, domain-independent objective measure.", "published": "2021-10-21 19:15:36", "link": "http://arxiv.org/abs/2110.11438v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Coding Approach Towards Non-Linear Spectro-Temporal Receptive\n  Fields", "abstract": "Linear Non-Linear(LN) models are widely used to characterize the receptive\nfields of early-stage auditory processing. We apply the principle of efficient\ncoding to the LN model of Spectro-Temporal Receptive Fields(STRFs) of the\nneurons in primary auditory cortex. The Efficient Coding Principle has been\npreviously used to understand early visual receptive fields and linear STRFs in\nauditory processing. Efficient coding is realized by jointly optimizing the\nmutual information between stimuli and neural responses subjected to the\nmetabolic cost of firing spikes. We compare the predictions of the efficient\ncoding principle with the physiological observations, which match qualitatively\nunder realistic conditions of noise in stimuli and the spike generation\nprocess.", "published": "2021-10-21 10:55:37", "link": "http://arxiv.org/abs/2110.12903v1", "categories": ["q-bio.NC", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Cortical representations of Auditory Perception using Graph Independent\n  Component on EEG", "abstract": "Recent studies indicate that the neurons involved in a cognitive task aren't\nlocally limited but span out to multiple human brain regions. We obtain network\ncomponents and their locations for the task of listening to music. The recorded\nEEG data is modeled as a graph, and it is assumed that the overall activity is\na contribution of several independent subnetworks. To identify these intrinsic\ncognitive subnetworks corresponding to music perception, we propose to\ndecompose the whole brain graph-network into multiple subnetworks. We perform\nthis decomposition to a group of brain networks by performing Graph-Independent\nComponent Analysis. Graph-ICA is a variant of ICA that decomposes the measured\ngraph into independent source graphs. Having obtained independent subnetworks,\nwe calculate the electrode positions by computing the local maxima of these\nsubnetwork matrices. We observe that the computed electrodes' location\ncorresponds to the temporal lobes and the Broca's area, which are indeed\ninvolved in the task of auditory processing and perception. The computed\nelectrodes also span the brain's frontal lobe, which is involved in attention\nand generating a stimulus-evoked response. The weight of the subnetwork that\ncorresponds to the aforementioned brain regions increases with the increase in\nthe music recording's tempo. The results suggest that whole-brain networks can\nbe decomposed into independent subnetworks and analyze cognitive responses to\nmusic stimulus.", "published": "2021-10-21 11:04:53", "link": "http://arxiv.org/abs/2110.12904v1", "categories": ["q-bio.NC", "eess.AS"], "primary_category": "q-bio.NC"}
{"title": "Optimizing Multi-Taper Features for Deep Speaker Verification", "abstract": "Multi-taper estimators provide low-variance power spectrum estimates that can\nbe used in place of the windowed discrete Fourier transform (DFT) to extract\nspeech features such as mel-frequency cepstral coefficients (MFCCs). Even if\npast work has reported promising automatic speaker verification (ASV) results\nwith Gaussian mixture model-based classifiers, the performance of multi-taper\nMFCCs with deep ASV systems remains an open question. Instead of a static-taper\ndesign, we propose to optimize the multi-taper estimator jointly with a deep\nneural network trained for ASV tasks. With a maximum improvement on the SITW\ncorpus of 25.8% in terms of equal error rate over the static-taper, our method\nhelps preserve a balanced level of leakage and variance, providing more\nrobustness.", "published": "2021-10-21 08:56:11", "link": "http://arxiv.org/abs/2110.10983v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RCT: Random Consistency Training for Semi-supervised Sound Event\n  Detection", "abstract": "Sound event detection (SED), as a core module of acoustic environmental\nanalysis, suffers from the problem of data deficiency. The integration of\nsemi-supervised learning (SSL) largely mitigates such problem while bringing no\nextra annotation budget. This paper researches on several core modules of SSL,\nand introduces a random consistency training (RCT) strategy. First, a\nself-consistency loss is proposed to fuse with the teacher-student model to\nstabilize the training. Second, a hard mixup data augmentation is proposed to\naccount for the additive property of sounds. Third, a random augmentation\nscheme is applied to flexibly combine different types of data augmentations.\nExperiments show that the proposed strategy outperform other widely-used\nstrategies.", "published": "2021-10-21 13:50:35", "link": "http://arxiv.org/abs/2110.11144v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Synt++: Utilizing Imperfect Synthetic Data to Improve Speech Recognition", "abstract": "With recent advances in speech synthesis, synthetic data is becoming a viable\nalternative to real data for training speech recognition models. However,\nmachine learning with synthetic data is not trivial due to the gap between the\nsynthetic and the real data distributions. Synthetic datasets may contain\nartifacts that do not exist in real data such as structured noise, content\nerrors, or unrealistic speaking styles. Moreover, the synthesis process may\nintroduce a bias due to uneven sampling of the data manifold. We propose two\nnovel techniques during training to mitigate the problems due to the\ndistribution gap: (i) a rejection sampling algorithm and (ii) using separate\nbatch normalization statistics for the real and the synthetic samples. We show\nthat these methods significantly improve the training of speech recognition\nmodels using synthetic data. We evaluate the proposed approach on keyword\ndetection and Automatic Speech Recognition (ASR) tasks, and observe up to 18%\nand 13% relative error reduction, respectively, compared to naively using the\nsynthetic data.", "published": "2021-10-21 21:11:42", "link": "http://arxiv.org/abs/2110.11479v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Wav2CLIP: Learning Robust Audio Representations From CLIP", "abstract": "We propose Wav2CLIP, a robust audio representation learning method by\ndistilling from Contrastive Language-Image Pre-training (CLIP). We\nsystematically evaluate Wav2CLIP on a variety of audio tasks including\nclassification, retrieval, and generation, and show that Wav2CLIP can\noutperform several publicly available pre-trained audio representation\nalgorithms. Wav2CLIP projects audio into a shared embedding space with images\nand text, which enables multimodal applications such as zero-shot\nclassification, and cross-modal retrieval. Furthermore, Wav2CLIP needs just\n~10% of the data to achieve competitive performance on downstream tasks\ncompared with fully supervised models, and is more efficient to pre-train than\ncompeting methods as it does not require learning a visual model in concert\nwith an auditory model. Finally, we demonstrate image generation from Wav2CLIP\nas qualitative assessment of the shared embedding space. Our code and model\nweights are open sourced and made available for further applications.", "published": "2021-10-21 22:00:13", "link": "http://arxiv.org/abs/2110.11499v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
