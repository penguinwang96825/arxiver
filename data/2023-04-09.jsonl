{"title": "Continual Graph Convolutional Network for Text Classification", "abstract": "Graph convolutional network (GCN) has been successfully applied to capture\nglobal non-consecutive and long-distance semantic information for text\nclassification. However, while GCN-based methods have shown promising results\nin offline evaluations, they commonly follow a seen-token-seen-document\nparadigm by constructing a fixed document-token graph and cannot make\ninferences on new documents. It is a challenge to deploy them in online systems\nto infer steaming text data. In this work, we present a continual GCN model\n(ContGCN) to generalize inferences from observed documents to unobserved\ndocuments. Concretely, we propose a new all-token-any-document paradigm to\ndynamically update the document-token graph in every batch during both the\ntraining and testing phases of an online system. Moreover, we design an\noccurrence memory module and a self-supervised contrastive learning objective\nto update ContGCN in a label-free manner. A 3-month A/B test on Huawei public\nopinion analysis system shows ContGCN achieves 8.86% performance gain compared\nwith state-of-the-art methods. Offline experiments on five public datasets also\nshow ContGCN can improve inference quality. The source code will be released at\nhttps://github.com/Jyonn/ContGCN.", "published": "2023-04-09 03:59:48", "link": "http://arxiv.org/abs/2304.04152v1", "categories": ["cs.CL", "I.7.0"], "primary_category": "cs.CL"}
{"title": "Similarity-Aware Multimodal Prompt Learning for Fake News Detection", "abstract": "The standard paradigm for fake news detection mainly utilizes text\ninformation to model the truthfulness of news. However, the discourse of online\nfake news is typically subtle and it requires expert knowledge to use textual\ninformation to debunk fake news. Recently, studies focusing on multimodal fake\nnews detection have outperformed text-only methods. Recent approaches utilizing\nthe pre-trained model to extract unimodal features, or fine-tuning the\npre-trained model directly, have become a new paradigm for detecting fake news.\nAgain, this paradigm either requires a large number of training instances, or\nupdates the entire set of pre-trained model parameters, making real-world fake\nnews detection impractical. Furthermore, traditional multimodal methods fuse\nthe cross-modal features directly without considering that the uncorrelated\nsemantic representation might inject noise into the multimodal features. This\npaper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE)\nframework. First, we incorporate prompt learning into multimodal fake news\ndetection. Prompt learning, which only tunes prompts with a frozen language\nmodel, can reduce memory usage significantly and achieve comparable\nperformances, compared with fine-tuning. We analyse three prompt templates with\na soft verbalizer to detect fake news. In addition, we introduce the\nsimilarity-aware fusing method to adaptively fuse the intensity of multimodal\nrepresentation and mitigate the noise injection via uncorrelated cross-modal\nfeatures. For evaluation, SAMPLE surpasses the F1 and the accuracies of\nprevious works on two benchmark multimodal datasets, demonstrating the\neffectiveness of the proposed method in detecting fake news. In addition,\nSAMPLE also is superior to other approaches regardless of few-shot and\ndata-rich settings.", "published": "2023-04-09 08:10:05", "link": "http://arxiv.org/abs/2304.04187v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extractive Summarization via ChatGPT for Faithful Summary Generation", "abstract": "Extractive summarization is a crucial task in natural language processing\nthat aims to condense long documents into shorter versions by directly\nextracting sentences. The recent introduction of large language models has\nattracted significant interest in the NLP community due to its remarkable\nperformance on a wide range of downstream tasks. This paper first presents a\nthorough evaluation of ChatGPT's performance on extractive summarization and\ncompares it with traditional fine-tuning methods on various benchmark datasets.\nOur experimental analysis reveals that ChatGPT exhibits inferior extractive\nsummarization performance in terms of ROUGE scores compared to existing\nsupervised systems, while achieving higher performance based on LLM-based\nevaluation metrics. In addition, we explore the effectiveness of in-context\nlearning and chain-of-thought reasoning for enhancing its performance.\nFurthermore, we find that applying an extract-then-generate pipeline with\nChatGPT yields significant performance improvements over abstractive baselines\nin terms of summary faithfulness. These observations highlight potential\ndirections for enhancing ChatGPT's capabilities in faithful summarization using\ntwo-stage approaches.", "published": "2023-04-09 08:26:04", "link": "http://arxiv.org/abs/2304.04193v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding", "abstract": "Zero-shot dialogue understanding aims to enable dialogue to track the user's\nneeds without any training data, which has gained increasing attention. In this\nwork, we investigate the understanding ability of ChatGPT for zero-shot\ndialogue understanding tasks including spoken language understanding (SLU) and\ndialogue state tracking (DST). Experimental results on four popular benchmarks\nreveal the great potential of ChatGPT for zero-shot dialogue understanding. In\naddition, extensive analysis shows that ChatGPT benefits from the multi-turn\ninteractive prompt in the DST task but struggles to perform slot filling for\nSLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue\nunderstanding tasks, hoping to provide some insights for future research on\nbuilding zero-shot dialogue understanding systems with Large Language Models\n(LLMs).", "published": "2023-04-09 15:28:36", "link": "http://arxiv.org/abs/2304.04256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RISC: Generating Realistic Synthetic Bilingual Insurance Contract", "abstract": "This paper presents RISC, an open-source Python package data generator\n(https://github.com/GRAAL-Research/risc). RISC generates look-alike automobile\ninsurance contracts based on the Quebec regulatory insurance form in French and\nEnglish. Insurance contracts are 90 to 100 pages long and use complex legal and\ninsurance-specific vocabulary for a layperson. Hence, they are a much more\ncomplex class of documents than those in traditional NLP corpora. Therefore, we\nintroduce RISCBAC, a Realistic Insurance Synthetic Bilingual Automobile\nContract dataset based on the mandatory Quebec car insurance contract. The\ndataset comprises 10,000 French and English unannotated insurance contracts.\nRISCBAC enables NLP research for unsupervised automatic summarisation, question\nanswering, text simplification, machine translation and more. Moreover, it can\nbe further automatically annotated as a dataset for supervised tasks such as\nNER", "published": "2023-04-09 10:42:18", "link": "http://arxiv.org/abs/2304.04212v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for\n  Medical domain", "abstract": "This paper introduces FrenchMedMCQA, the first publicly available\nMultiple-Choice Question Answering (MCQA) dataset in French for medical domain.\nIt is composed of 3,105 questions taken from real exams of the French medical\nspecialization diploma in pharmacy, mixing single and multiple answers. Each\ninstance of the dataset contains an identifier, a question, five possible\nanswers and their manual correction(s). We also propose first baseline models\nto automatically process this MCQA task in order to report on the current\nperformances and to highlight the difficulty of the task. A detailed analysis\nof the results showed that it is necessary to have representations adapted to\nthe medical domain or to the MCQA task: in our case, English specialized models\nyielded better results than generic French ones, even though FrenchMedMCQA is\nin French. Corpus, models and tools are available online.", "published": "2023-04-09 16:57:40", "link": "http://arxiv.org/abs/2304.04280v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Ready for Healthcare? A Comparative Study on\n  Clinical Language Understanding", "abstract": "Large language models (LLMs) have made significant progress in various\ndomains, including healthcare. However, the specialized nature of clinical\nlanguage understanding tasks presents unique challenges and limitations that\nwarrant further investigation. In this study, we conduct a comprehensive\nevaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within\nthe realm of clinical language understanding tasks. These tasks span a diverse\nrange, including named entity recognition, relation extraction, natural\nlanguage inference, semantic textual similarity, document classification, and\nquestion-answering. We also introduce a novel prompting strategy,\nself-questioning prompting (SQP), tailored to enhance LLMs' performance by\neliciting informative questions and answers pertinent to the clinical scenarios\nat hand. Our evaluation underscores the significance of task-specific learning\nstrategies and prompting techniques for improving LLMs' effectiveness in\nhealthcare-related tasks. Additionally, our in-depth error analysis on the\nchallenging relation extraction task offers valuable insights into error\ndistribution and potential avenues for improvement using SQP. Our study sheds\nlight on the practical implications of employing LLMs in the specialized domain\nof healthcare, serving as a foundation for future research and the development\nof potential applications in healthcare settings.", "published": "2023-04-09 16:31:47", "link": "http://arxiv.org/abs/2304.05368v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability\n  Analysis against Human Performance", "abstract": "ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that\nare slated to promise different applications in diverse areas. In education,\nthese AI technologies have been tested for applications in assessment and\nteaching. In assessment, AI has long been used in automated essay scoring and\nautomated item generation. One psychometric property that these tools must have\nto assist or replace humans in assessment is high reliability in terms of\nagreement between AI scores and human raters. In this paper, we measure the\nreliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and\ntrained humans in perceiving and rating the complexity of writing prompts.\nIntraclass correlation (ICC) as a performance metric showed that the\ninter-reliability of both the OpenAI ChatGPT and the Google Bard were low\nagainst the gold standard of human ratings.", "published": "2023-04-09 04:53:15", "link": "http://arxiv.org/abs/2304.05372v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An investigation of phrase break prediction in an End-to-End TTS system", "abstract": "Purpose: This work explores the use of external phrase break prediction\nmodels to enhance listener comprehension in End-to-End Text-to-Speech (TTS)\nsystems.\n  Methods: The effectiveness of these models is evaluated based on listener\npreferences in subjective tests. Two approaches are explored: (1) a\nbidirectional LSTM model with task-specific embeddings trained from scratch,\nand (2) a pre-trained BERT model fine-tuned on phrase break prediction. Both\nmodels are trained on a multi-speaker English corpus to predict phrase break\nlocations in text. The End-to-End TTS system used comprises a Tacotron2 model\nwith Dynamic Convolutional Attention for mel spectrogram prediction and a\nWaveRNN vocoder for waveform generation.\n  Results: The listening tests show a clear preference for text synthesized\nwith predicted phrase breaks over text synthesized without them.\n  Conclusion: These results confirm the value of incorporating external\nphrasing models within End-to-End TTS to enhance listener comprehension.", "published": "2023-04-09 04:26:58", "link": "http://arxiv.org/abs/2304.04157v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual\n  and Multilingual Approaches for Detecting Online News Genre, Framing and\n  Persuasion Techniques", "abstract": "This paper describes the participation of team QUST in the SemEval2023 task\n3. The monolingual models are first evaluated with the under-sampling of the\nmajority classes in the early stage of the task. Then, the pre-trained\nmultilingual model is fine-tuned with a combination of the class weights and\nthe sample weights. Two different fine-tuning strategies, the task-agnostic and\nthe task-dependent, are further investigated. All experiments are conducted\nunder the 10-fold cross-validation, the multilingual approaches are superior to\nthe monolingual ones. The submitted system achieves the second best in Italian\nand Spanish (zero-shot) in subtask-1.", "published": "2023-04-09 08:14:01", "link": "http://arxiv.org/abs/2304.04190v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Editable User Profiles for Controllable Text Recommendation", "abstract": "Methods for making high-quality recommendations often rely on learning latent\nrepresentations from interaction data. These methods, while performant, do not\nprovide ready mechanisms for users to control the recommendation they receive.\nOur work tackles this problem by proposing LACE, a novel concept value\nbottleneck model for controllable text recommendations. LACE represents each\nuser with a succinct set of human-readable concepts through retrieval given\nuser-interacted documents and learns personalized representations of the\nconcepts based on user documents. This concept based user profile is then\nleveraged to make recommendations. The design of our model affords control over\nthe recommendations through a number of intuitive interactions with a\ntransparent user profile. We first establish the quality of recommendations\nobtained from LACE in an offline evaluation on three recommendation tasks\nspanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we\nvalidate the controllability of LACE under simulated user interactions.\nFinally, we implement LACE in an interactive controllable recommender system\nand conduct a user study to demonstrate that users are able to improve the\nquality of recommendations they receive through interactions with an editable\nuser profile.", "published": "2023-04-09 14:52:18", "link": "http://arxiv.org/abs/2304.04250v3", "categories": ["cs.IR", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.IR"}
{"title": "ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous\n  States in Realistic 3D Scenes", "abstract": "Understanding the continuous states of objects is essential for task learning\nand planning in the real world. However, most existing task learning benchmarks\nassume discrete (e.g., binary) object goal states, which poses challenges for\nthe learning of complex tasks and transferring learned policy from simulated\nenvironments to the real world. Furthermore, state discretization limits a\nrobot's ability to follow human instructions based on the grounding of actions\nand states. To tackle these challenges, we present ARNOLD, a benchmark that\nevaluates language-grounded task learning with continuous states in realistic\n3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve\nunderstanding object states and learning policies for continuous goals. To\npromote language-instructed learning, we provide expert demonstrations with\ntemplate-generated language descriptions. We assess task performance by\nutilizing the latest language-conditioned policy learning models. Our results\nindicate that current models for language-conditioned manipulations continue to\nexperience significant challenges in novel goal-state generalizations, scene\ngeneralizations, and object generalizations. These findings highlight the need\nto develop new algorithms that address this gap and underscore the potential\nfor further research in this area. Project website:\nhttps://arnold-benchmark.github.io.", "published": "2023-04-09 21:42:57", "link": "http://arxiv.org/abs/2304.04321v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Hi Sheldon! Creating Deep Personalized Characters from TV Shows", "abstract": "Imagine an interesting multimodal interactive scenario that you can see,\nhear, and chat with an AI-generated digital character, who is capable of\nbehaving like Sheldon from The Big Bang Theory, as a DEEP copy from appearance\nto personality. Towards this fantastic multimodal chatting scenario, we propose\na novel task, named Deep Personalized Character Creation (DPCC): creating\nmultimodal chat personalized characters from multimodal data such as TV shows.\nSpecifically, given a single- or multi-modality input (text, audio, video), the\ngoal of DPCC is to generate a multi-modality (text, audio, video) response,\nwhich should be well-matched the personality of a specific character such as\nSheldon, and of high quality as well. To support this novel task, we further\ncollect a character centric multimodal dialogue dataset, named Deep\nPersonalized Character Dataset (DPCD), from TV shows. DPCD contains\ncharacter-specific multimodal dialogue data of ~10k utterances and ~6 hours of\naudio/video per character, which is around 10 times larger compared to existing\nrelated datasets.On DPCD, we present a baseline method for the DPCC task and\ncreate 5 Deep personalized digital Characters (DeepCharacters) from Big Bang TV\nShows. We conduct both subjective and objective experiments to evaluate the\nmultimodal response from DeepCharacters in terms of characterization and\nquality. The results demonstrates that, on our collected DPCD dataset, the\nproposed baseline can create personalized digital characters for generating\nmultimodal response.Our collected DPCD dataset, the code of data collection and\nour baseline will be published soon.", "published": "2023-04-09 00:39:43", "link": "http://arxiv.org/abs/2304.11093v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
