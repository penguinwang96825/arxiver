{"title": "MOT: A Mixture of Actors Reinforcement Learning Method by Optimal Transport for Algorithmic Trading", "abstract": "Algorithmic trading refers to executing buy and sell orders for specific\nassets based on automatically identified trading opportunities. Strategies\nbased on reinforcement learning (RL) have demonstrated remarkable capabilities\nin addressing algorithmic trading problems. However, the trading patterns\ndiffer among market conditions due to shifted distribution data. Ignoring\nmultiple patterns in the data will undermine the performance of RL. In this\npaper, we propose MOT,which designs multiple actors with disentangled\nrepresentation learning to model the different patterns of the market.\nFurthermore, we incorporate the Optimal Transport (OT) algorithm to allocate\nsamples to the appropriate actor by introducing a regularization loss term.\nAdditionally, we propose Pretrain Module to facilitate imitation learning by\naligning the outputs of actors with expert strategy and better balance the\nexploration and exploitation of RL. Experimental results on real futures market\ndata demonstrate that MOT exhibits excellent profit capabilities while\nbalancing risks. Ablation studies validate the effectiveness of the components\nof MOT.", "published": "2024-06-03 01:42:52", "link": "http://arxiv.org/abs/2407.01577v1", "categories": ["q-fin.TR", "cs.AI", "cs.LG"], "primary_category": "q-fin.TR"}
{"title": "A Survey of Useful LLM Evaluation", "abstract": "LLMs have gotten attention across various research domains due to their\nexceptional performance on a wide range of complex tasks. Therefore, refined\nmethods to evaluate the capabilities of LLMs are needed to determine the tasks\nand responsibility they should undertake. Our study mainly discussed how LLMs,\nas useful tools, should be effectively assessed. We proposed the two-stage\nframework: from ``core ability'' to ``agent'', clearly explaining how LLMs can\nbe applied based on their specific capabilities, along with the evaluation\nmethods in each stage. Core ability refers to the capabilities that LLMs need\nin order to generate high-quality natural language texts. After confirming LLMs\npossess core ability, they can solve real-world and complex tasks as agent. In\nthe \"core ability\" stage, we discussed the reasoning ability, societal impact,\nand domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied\naction, planning, and tool learning of LLMs agent applications. Finally, we\nexamined the challenges currently confronting the evaluation methods for LLMs,\nas well as the directions for future development.", "published": "2024-06-03 02:20:03", "link": "http://arxiv.org/abs/2406.00936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using RL to Identify Divisive Perspectives Improves LLMs Abilities to\n  Identify Communities on Social Media", "abstract": "The large scale usage of social media, combined with its significant impact,\nhas made it increasingly important to understand it. In particular, identifying\nuser communities, can be helpful for many downstream tasks. However,\nparticularly when models are trained on past data and tested on future, doing\nthis is difficult.\n  In this paper, we hypothesize to take advantage of Large Language Models\n(LLMs), to better identify user communities. Due to the fact that many LLMs,\nsuch as ChatGPT, are fixed and must be treated as black-boxes, we propose an\napproach to better prompt them, by training a smaller LLM to do this. We devise\nstrategies to train this smaller model, showing how it can improve the larger\nLLMs ability to detect communities. Experimental results show improvements on\nReddit and Twitter data, on the tasks of community detection, bot detection,\nand news media profiling.", "published": "2024-06-03 03:45:31", "link": "http://arxiv.org/abs/2406.00969v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Drug-Gene Relations via Analogy Tasks with Word Embeddings", "abstract": "Natural language processing (NLP) is utilized in a wide range of fields,\nwhere words in text are typically transformed into feature vectors called\nembeddings. BioConceptVec is a specific example of embeddings tailored for\nbiology, trained on approximately 30 million PubMed abstracts using models such\nas skip-gram. Generally, word embeddings are known to solve analogy tasks\nthrough simple vector arithmetic. For instance, $\\mathrm{\\textit{king}} -\n\\mathrm{\\textit{man}} + \\mathrm{\\textit{woman}}$ predicts\n$\\mathrm{\\textit{queen}}$. In this study, we demonstrate that BioConceptVec\nembeddings, along with our own embeddings trained on PubMed abstracts, contain\ninformation about drug-gene relations and can predict target genes from a given\ndrug through analogy computations. We also show that categorizing drugs and\ngenes using biological pathways improves performance. Furthermore, we\nillustrate that vectors derived from known relations in the past can predict\nunknown future relations in datasets divided by year. Despite the simplicity of\nimplementing analogy tasks as vector additions, our approach demonstrated\nperformance comparable to that of large language models such as GPT-4 in\npredicting drug-gene relations.", "published": "2024-06-03 04:36:38", "link": "http://arxiv.org/abs/2406.00984v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Qualitative and Computational Approaches for Literary Analysis\n  of Finnish Novels", "abstract": "What can we learn from the classics of Finnish literature by using\ncomputational emotion analysis? This article tries to answer this question by\nexamining how computational methods of sentiment analysis can be used in the\nstudy of literary works in conjunction with a qualitative or more 'traditional'\napproach to literature and affect. We present and develop a simple but robust\ncomputational approach of affect analysis that uses a carefully curated emotion\nlexicon adapted to Finnish turn-of-the-century literary texts combined with\nword embeddings to map out the semantic emotional spaces of seminal works of\nFinnish literature. We focus our qualitative analysis on selected case studies:\nfour works by Juhani Aho, Minna Canth, Maria Jotuni, and F. E. Sillanp\\\"a\\\"a,\nbut provide emotion arcs for a total of 975 Finnish novels.\n  We argue that a computational analysis of a text's lexicon can be valuable in\nevaluating the large distribution of the emotional valence in a text and\nprovide guidelines to help other researchers replicate our findings. We show\nthat computational approaches have a place in traditional studies on affect in\nliterature as a support tool for close-reading-based analyses, but also\nallowing for large-scale comparison between, for example, genres or national\ncanons.", "published": "2024-06-03 06:07:44", "link": "http://arxiv.org/abs/2406.01021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strengthened Symbol Binding Makes Large Language Models Reliable\n  Multiple-Choice Selectors", "abstract": "Multiple-Choice Questions (MCQs) constitute a critical area of research in\nthe study of Large Language Models (LLMs). Previous works have investigated the\nselection bias problem in MCQs within few-shot scenarios, in which the LLM's\nperformance may be influenced by the presentation of answer choices, leaving\nthe selection bias during Supervised Fine-Tuning (SFT) unexplored. In this\npaper, we reveal that selection bias persists in the SFT phase , primarily due\nto the LLM's inadequate Multiple Choice Symbol Binding (MCSB) ability. This\nlimitation implies that the model struggles to associate the answer options\nwith their corresponding symbols (e.g., A/B/C/D) effectively. To enhance the\nmodel's MCSB capability, we first incorporate option contents into the loss\nfunction and subsequently adjust the weights of the option symbols and\ncontents, guiding the model to understand the option content of the current\nsymbol. Based on this, we introduce an efficient SFT algorithm for MCQs, termed\nPoint-wise Intelligent Feedback (PIF). PIF constructs negative instances by\nrandomly combining the incorrect option contents with all candidate symbols,\nand proposes a point-wise loss to provide feedback on these negative samples\ninto LLMs. Our experimental results demonstrate that PIF significantly reduces\nthe model's selection bias by improving its MCSB capability. Remarkably, PIF\nexhibits a substantial enhancement in the accuracy for MCQs.", "published": "2024-06-03 06:20:12", "link": "http://arxiv.org/abs/2406.01026v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MACT: Model-Agnostic Cross-Lingual Training for Discourse Representation\n  Structure Parsing", "abstract": "Discourse Representation Structure (DRS) is an innovative semantic\nrepresentation designed to capture the meaning of texts with arbitrary lengths\nacross languages. The semantic representation parsing is essential for\nachieving natural language understanding through logical forms. Nevertheless,\nthe performance of DRS parsing models remains constrained when trained\nexclusively on monolingual data. To tackle this issue, we introduce a\ncross-lingual training strategy. The proposed method is model-agnostic yet\nhighly effective. It leverages cross-lingual training data and fully exploits\nthe alignments between languages encoded in pre-trained language models. The\nexperiments conducted on the standard benchmarks demonstrate that models\ntrained using the cross-lingual training method exhibit significant\nimprovements in DRS clause and graph parsing in English, German, Italian and\nDutch. Comparing our final models to previous works, we achieve\nstate-of-the-art results in the standard benchmarks. Furthermore, the detailed\nanalysis provides deep insights into the performance of the parsers, offering\ninspiration for future research in DRS parsing. We keep updating new results on\nbenchmarks to the appendix.", "published": "2024-06-03 07:02:57", "link": "http://arxiv.org/abs/2406.01052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding ChatGPT to Generate Salient Domain Summaries", "abstract": "ChatGPT is instruct-tuned to generate general and human-expected content to\nalign with human preference through Reinforcement Learning from Human Feedback\n(RLHF), meanwhile resulting in generated responses not salient enough.\nTherefore, in this case, ChatGPT may fail to satisfy domain requirements in\nzero-shot settings, leading to poor ROUGE scores. Inspired by the In-Context\nLearning (ICL) and retelling ability of ChatGPT, this paper proposes PADS, a\n\\textbf{P}ipeline for \\textbf{A}ssisting ChatGPT in \\textbf{D}omain\n\\textbf{S}ummarization. PADS consists of a retriever to retrieve similar\nexamples from corpora and a rank model to rerank the multiple candidate\nsummaries generated by ChatGPT. Specifically, given an inference document, we\nfirst retrieve an in-context demonstration via the retriever. Then, we require\nChatGPT to generate $k$ candidate summaries for the inference document at a\ntime under the guidance of the retrieved demonstration. Finally, the rank model\nindependently scores the $k$ candidate summaries according to their quality and\nselects the optimal one. We extensively explore dense and sparse retrieval\nmethods to select effective demonstrations for reference and efficiently train\nthe rank model to reflect the quality of candidate summaries for each given\nsummarized document. Additionally, PADS contains merely 400M trainable\nparameters originating from the rank model and we merely collect 2.5k data to\ntrain it. We evaluate PADS on five datasets from different domains, and the\nresult indicates that each module in PADS is committed to effectively guiding\nChatGPT to generate salient summaries fitting different domain requirements.\nSpecifically, in the popular summarization dataset Gigaword, PADS achieves over\n+8 gain on ROUGE-L, compared with the naive ChatGPT in the zero-shot setting.\n\\footnote{Our code are available at \\url{https://github.com/jungao1106/PADS}}", "published": "2024-06-03 07:42:45", "link": "http://arxiv.org/abs/2406.01070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Reasoning: A GNN-LLM Collaborative Framework for Knowledge Graph\n  Question Answering", "abstract": "Large Language Models (LLMs) excel at intuitive, implicit reasoning. Guiding\nLLMs to construct thought chains can enhance their deliberate reasoning\nabilities, but also faces challenges such as hallucination. Knowledge Graphs\n(KGs) can provide explicit structured knowledge for LLMs to alleviate these\nissues. However, existing KG-enhanced methods often overlook explicit graph\nlearning, making it challenging to efficiently provide precise reasoning chains\nfor LLMs. Following dual-process theory, we propose Dual-Reasoning (DualR), a\nnovel framework that integrates an external system based on Graph Neural\nNetwork (GNN) for explicit reasoning on KGs, complementing the implicit\nreasoning of LLMs through externalized reasoning chains. DualR designs an\nLLM-empowered GNN module for explicit learning on KGs, efficiently extracting\nhigh-quality reasoning chains. These reasoning chains are then refined to a\nknowledge-enhanced multiple-choice prompt, guiding a frozen LLM to reason\nthoughtfully for final answer determination. Extensive experiments on three\nbenchmark KGQA datasets demonstrate that DualR achieves state-of-the-art\nperformance while maintaining high efficiency and interpretability.", "published": "2024-06-03 09:38:28", "link": "http://arxiv.org/abs/2406.01145v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and\n  Personalization", "abstract": "The concept of persona, originally adopted in dialogue literature, has\nre-surged as a promising framework for tailoring large language models (LLMs)\nto specific context (e.g., personalized search, LLM-as-a-judge). However, the\ngrowing research on leveraging persona in LLMs is relatively disorganized and\nlacks a systematic taxonomy. To close the gap, we present a comprehensive\nsurvey to categorize the current state of the field. We identify two lines of\nresearch, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and\n(2) LLM Personalization, where LLMs take care of user personas. Additionally,\nwe introduce existing methods for LLM personality evaluation. To the best of\nour knowledge, we present the first survey for role-playing and personalization\nin LLMs under the unified view of persona. We continuously maintain a paper\ncollection to foster future endeavors:\nhttps://github.com/MiuLab/PersonaLLM-Survey", "published": "2024-06-03 10:08:23", "link": "http://arxiv.org/abs/2406.01171v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demonstration Augmentation for Zero-shot In-context Learning", "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known\nas In-context Learning (ICL), which enables them to acquire knowledge from\ntextual demonstrations without the need for parameter updates. However, many\nstudies have highlighted that the model's performance is sensitive to the\nchoice of demonstrations, presenting a significant challenge for practical\napplications where we lack prior knowledge of user queries. Consequently, we\nneed to construct an extensive demonstration pool and incorporate external\ndatabases to assist the model, leading to considerable time and financial\ncosts. In light of this, some recent research has shifted focus towards\nzero-shot ICL, aiming to reduce the model's reliance on external information by\nleveraging their inherent generative capabilities. Despite the effectiveness of\nthese approaches, the content generated by the model may be unreliable, and the\ngeneration process is time-consuming. To address these issues, we propose\nDemonstration Augmentation for In-context Learning (DAIL), which employs the\nmodel's previously predicted historical samples as demonstrations for\nsubsequent ones. DAIL brings no additional inference cost and does not rely on\nthe model's generative capabilities. Our experiments reveal that DAIL can\nsignificantly improve the model's performance over direct zero-shot inference\nand can even outperform few-shot ICL without any external information.", "published": "2024-06-03 11:46:42", "link": "http://arxiv.org/abs/2406.01224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EffiQA: Efficient Question-Answering with Strategic Multi-Model\n  Collaboration on Knowledge Graphs", "abstract": "While large language models (LLMs) have shown remarkable capabilities in\nnatural language processing, they struggle with complex, multi-step reasoning\ntasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs\nand KGs either underutilize the reasoning abilities of LLMs or suffer from\nprohibitive computational costs due to tight coupling. To address these\nlimitations, we propose a novel collaborative framework named EffiQA that can\nstrike a balance between performance and efficiency via an iterative paradigm.\nEffiQA consists of three stages: global planning, efficient KG exploration, and\nself-reflection. Specifically, EffiQA leverages the commonsense capability of\nLLMs to explore potential reasoning pathways through global planning. Then, it\noffloads semantic pruning to a small plug-in model for efficient KG\nexploration. Finally, the exploration results are fed to LLMs for\nself-reflection to further improve the global planning and efficient KG\nexploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's\neffectiveness, achieving an optimal balance between reasoning accuracy and\ncomputational costs. We hope the proposed new framework will pave the way for\nefficient, knowledge-intensive querying by redefining the integration of LLMs\nand KGs, fostering future research on knowledge-based question answering.", "published": "2024-06-03 11:56:07", "link": "http://arxiv.org/abs/2406.01238v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EduNLP: Towards a Unified and Modularized Library for Educational\n  Resources", "abstract": "Educational resource understanding is vital to online learning platforms,\nwhich have demonstrated growing applications recently. However, researchers and\ndevelopers always struggle with using existing general natural language\ntoolkits or domain-specific models. The issue raises a need to develop an\neffective and easy-to-use one that benefits AI education-related research and\napplications. To bridge this gap, we present a unified, modularized, and\nextensive library, EduNLP, focusing on educational resource understanding. In\nthe library, we decouple the whole workflow to four key modules with consistent\ninterfaces including data configuration, processing, model implementation, and\nmodel evaluation. We also provide a configurable pipeline to unify the data\nusage and model usage in standard ways, where users can customize their own\nneeds. For the current version, we primarily provide 10 typical models from\nfour categories, and 5 common downstream-evaluation tasks in the education\ndomain on 8 subjects for users' usage. The project is released at:\nhttps://github.com/bigdata-ustc/EduNLP.", "published": "2024-06-03 12:45:40", "link": "http://arxiv.org/abs/2406.01276v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of\n  Self-Correction of LLMs", "abstract": "Self-correction is an approach to improving responses from large language\nmodels (LLMs) by refining the responses using LLMs during inference. Prior work\nhas proposed various self-correction frameworks using different sources of\nfeedback, including self-evaluation and external feedback. However, there is\nstill no consensus on the question of when LLMs can correct their own mistakes,\nas recent studies also report negative results. In this work, we critically\nsurvey broad papers and discuss the conditions required for successful\nself-correction. We first find that prior studies often do not define their\nresearch questions in detail and involve impractical frameworks or unfair\nevaluations that over-evaluate self-correction. To tackle these issues, we\ncategorize research questions in self-correction research and provide a\nchecklist for designing appropriate experiments. Our critical survey based on\nthe newly categorized research questions shows that (1) no prior work\ndemonstrates successful self-correction with feedback from prompted LLMs,\nexcept for studies in tasks that are exceptionally suited for self-correction,\n(2) self-correction works well in tasks that can use reliable external\nfeedback, and (3) large-scale fine-tuning enables self-correction.", "published": "2024-06-03 13:05:46", "link": "http://arxiv.org/abs/2406.01297v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Distractor Generation via Large Language Model Distilling\n  and Counterfactual Contrastive Decoding", "abstract": "Within the context of reading comprehension, the task of Distractor\nGeneration (DG) aims to generate several incorrect options to confuse readers.\nTraditional supervised methods for DG rely heavily on expensive human-annotated\ndistractor labels. In this paper, we propose an unsupervised DG framework,\nleveraging Large Language Models (LLMs) as cost-effective annotators to enhance\nthe DG capability of smaller student models. Specially, to perform knowledge\ndistilling, we propose a dual task training strategy that integrates pseudo\ndistractors from LLMs and the original answer in-formation as the objective\ntargets with a two-stage training process. Moreover, we devise a counterfactual\ncontrastive decoding mechanism for increasing the distracting capability of the\nDG model. Experiments show that our unsupervised generation method with\nBart-base greatly surpasses GPT-3.5-turbo performance with only 200 times fewer\nmodel parameters. Our proposed unsupervised DG method offers a cost-effective\nframework for practical reading comprehension applications, without the need of\nlaborious distractor annotation and costly large-size models", "published": "2024-06-03 13:20:05", "link": "http://arxiv.org/abs/2406.01306v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FactGenius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to\n  Improve Fact Verification with Knowledge Graphs", "abstract": "Fact-checking is a crucial natural language processing (NLP) task that\nverifies the truthfulness of claims by considering reliable evidence.\nTraditional methods are often limited by labour-intensive data curation and\nrule-based approaches. In this paper, we present FactGenius, a novel method\nthat enhances fact-checking by combining zero-shot prompting of large language\nmodels (LLMs) with fuzzy text matching on knowledge graphs (KGs). Leveraging\nDBpedia, a structured linked data dataset derived from Wikipedia, FactGenius\nrefines LLM-generated connections using similarity measures to ensure accuracy.\nThe evaluation of FactGenius on the FactKG, a benchmark dataset for fact\nverification, demonstrates that it significantly outperforms existing\nbaselines, particularly when fine-tuning RoBERTa as a classifier. The two-stage\napproach of filtering and validating connections proves crucial, achieving\nsuperior performance across various reasoning types and establishing FactGenius\nas a promising tool for robust fact-checking. The code and materials are\navailable at https://github.com/SushantGautam/FactGenius.", "published": "2024-06-03 13:24:37", "link": "http://arxiv.org/abs/2406.01311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Analysis, Description, and Typological Exploration with\n  Categorial Grammar (TheBench Guide)", "abstract": "TheBench is a tool to study monadic structures in natural language. It is for\nwriting monadic grammars to explore analyses, compare diverse languages through\ntheir categories, and to train models of grammar from form-meaning pairs where\nsyntax is latent variable.\n  Monadic structures are binary combinations of elements that employ semantics\nof composition only. TheBench is essentially old-school categorial grammar to\nsyntacticize the idea, with the implication that although syntax is autonomous\n(recall \\emph{colorless green ideas sleep furiously}), the treasure is in the\nbaggage it carries at every step, viz. semantics, more narrowly,\npredicate-argument structures indicating choice of categorial reference and its\nconsequent placeholders for decision in such structures.\n  There is some new thought in old school. Unlike traditional categorial\ngrammars, application is turned into composition in monadic analysis. Moreover,\nevery correspondence requires specifying two command relations, one on\nsyntactic command and the other on semantic command. A monadic grammar of\nTheBench contains only synthetic elements (called `objects' in category theory\nof mathematics) that are shaped by this analytic invariant, viz. composition.\nBoth ingredients (command relations) of any analytic step must therefore be\nfunctions (`arrows' in category theory). TheBench is one implementation of the\nidea for iterative development of such functions along with grammar of\nsynthetic elements.", "published": "2024-06-03 14:39:06", "link": "http://arxiv.org/abs/2406.01372v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large\n  Language Models", "abstract": "Continual Pre-Training (CPT) on Large Language Models (LLMs) has been widely\nused to expand the model's fundamental understanding of specific downstream\ndomains (e.g., math and code). For the CPT on domain-specific LLMs, one\nimportant question is how to choose the optimal mixture ratio between the\ngeneral-corpus (e.g., Dolma, Slim-pajama) and the downstream domain-corpus.\nExisting methods usually adopt laborious human efforts by grid-searching on a\nset of mixture ratios, which require high GPU training consumption costs.\nBesides, we cannot guarantee the selected ratio is optimal for the specific\ndomain. To address the limitations of existing methods, inspired by the Scaling\nLaw for performance prediction, we propose to investigate the Scaling Law of\nthe Domain-specific Continual Pre-Training (D-CPT Law) to decide the optimal\nmixture ratio with acceptable training costs for LLMs of different sizes.\nSpecifically, by fitting the D-CPT Law, we can easily predict the general and\ndownstream performance of arbitrary mixture ratios, model sizes, and dataset\nsizes using small-scale training costs on limited experiments. Moreover, we\nalso extend our standard D-CPT Law on cross-domain settings and propose the\nCross-Domain D-CPT Law to predict the D-CPT law of target domains, where very\nsmall training costs (about 1% of the normal training costs) are needed for the\ntarget domains. Comprehensive experimental results on six downstream domains\ndemonstrate the effectiveness and generalizability of our proposed D-CPT Law\nand Cross-Domain D-CPT Law.", "published": "2024-06-03 14:40:31", "link": "http://arxiv.org/abs/2406.01375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparsity-Accelerated Training for Large Language Models", "abstract": "Large language models (LLMs) have demonstrated proficiency across various\nnatural language processing (NLP) tasks but often require additional training,\nsuch as continual pre-training and supervised fine-tuning. However, the costs\nassociated with this, primarily due to their large parameter count, remain\nhigh. This paper proposes leveraging \\emph{sparsity} in pre-trained LLMs to\nexpedite this training process. By observing sparsity in activated neurons\nduring forward iterations, we identify the potential for computational\nspeed-ups by excluding inactive neurons. We address associated challenges by\nextending existing neuron importance evaluation metrics and introducing a\nladder omission rate scheduler. Our experiments on Llama-2 demonstrate that\nSparsity-Accelerated Training (SAT) achieves comparable or superior performance\nto standard training while significantly accelerating the process.\nSpecifically, SAT achieves a $45\\%$ throughput improvement in continual\npre-training and saves $38\\%$ training time in supervised fine-tuning in\npractice. It offers a simple, hardware-agnostic, and easily deployable\nframework for additional LLM training. Our code is available at\nhttps://github.com/OpenDFM/SAT.", "published": "2024-06-03 14:56:09", "link": "http://arxiv.org/abs/2406.01392v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of\n  Knowledge Editing in Large Language Models", "abstract": "Knowledge editing is a rising technique for efficiently updating factual\nknowledge in large language models (LLMs) with minimal alteration of\nparameters. However, recent studies have identified side effects, such as\nknowledge distortion and the deterioration of general abilities, that have\nemerged after editing. Despite these findings, evaluating the pitfalls of\nknowledge editing often relies on inconsistent metrics and benchmarks, lacking\na uniform standard. In response, this survey presents a comprehensive study of\nthese side effects, providing a unified perspective on the challenges of\nknowledge editing in LLMs by conducting experiments with consistent metrics and\nbenchmarks. Additionally, we review related works and outline potential\nresearch directions to address these limitations. Our survey highlights the\nlimitations of current knowledge editing methods, emphasizing the need for a\ndeeper understanding of the inner knowledge structures of LLMs and improved\nknowledge editing methods. To foster future research, we have released the\ncomplementary materials publicly in https://github.com/MiuLab/EditLLM-Survey.", "published": "2024-06-03 15:28:21", "link": "http://arxiv.org/abs/2406.01436v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LexMatcher: Dictionary-centric Data Collection for LLM-based Machine\n  Translation", "abstract": "The fine-tuning of open-source large language models (LLMs) for machine\ntranslation has recently received considerable attention, marking a shift\ntowards data-centric research from traditional neural machine translation.\nHowever, the area of data collection for instruction fine-tuning in machine\ntranslation remains relatively underexplored. In this paper, we present\nLexMatcher, a simple yet effective method for data curation, the design of\nwhich is driven by the coverage of senses found in bilingual dictionaries. The\nconstruction process comprises data retrieval from an existing corpus and data\naugmentation that supplements the infrequent senses of polysemous words.\nUtilizing LLaMA2 as our base model, our approach outperforms the established\nbaselines on the WMT2022 test sets and also exhibits remarkable performance in\ntasks related to word sense disambiguation and specialized terminology\ntranslation. These results underscore the effectiveness of LexMatcher in\nenhancing LLM-based machine translation. The code, data, and models are\navailable at https://github.com/ARIES-LM/Lexmatcher-MT.git.", "published": "2024-06-03 15:30:36", "link": "http://arxiv.org/abs/2406.01441v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "abstract": "Finetuning language agents with reasoning-action trajectories is effective,\nbut obtaining these trajectories from human annotations or stronger models is\ncostly and sometimes impractical. In this paper, we investigate the use of\nself-training in language agents, which can generate supervision from the agent\nitself, offering a promising alternative without relying on human or stronger\nmodel demonstrations. Self-training, however, requires high-quality\nmodel-generated samples, which are hard to obtain for challenging language\nagent tasks. To address this, we present Reflection-Reinforced Self-Training\n(Re-ReST), which uses a \\textit{reflector} to refine low-quality generated\nsamples during self-training. The reflector takes the agent's output and\nfeedback from an external environment (e.g., unit test results in code\ngeneration) to produce improved samples. This technique enhances the quality of\ninferior samples and efficiently enriches the self-training dataset with\nhigher-quality samples. We conduct extensive experiments on open-source\nlanguage agents across tasks, including multi-hop question answering,\nsequential decision-making, code generation, visual question answering, and\ntext-to-image generation. The results demonstrate the effectiveness of\nself-training and Re-ReST in language agent tasks, with self-training improving\nbaselines by 7.6\\% on HotpotQA and 28.4\\% on AlfWorld, and Re-ReST further\nboosting performance by 2.0\\% and 14.1\\%, respectively. Our studies also\nconfirm the efficiency of using a reflector to generate high-quality samples\nfor self-training. Moreover, we demonstrate a method to employ reflection\nduring inference without ground-truth feedback, addressing the limitation of\nprevious reflection work. Our code is released at\nhttps://github.com/PlusLabNLP/Re-ReST.", "published": "2024-06-03 16:21:38", "link": "http://arxiv.org/abs/2406.01495v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAD: Multi-Alignment MEG-to-Text Decoding", "abstract": "Deciphering language from brain activity is a crucial task in brain-computer\ninterface (BCI) research. Non-invasive cerebral signaling techniques including\nelectroencephalography (EEG) and magnetoencephalography (MEG) are becoming\nincreasingly popular due to their safety and practicality, avoiding invasive\nelectrode implantation. However, current works under-investigated three points:\n1) a predominant focus on EEG with limited exploration of MEG, which provides\nsuperior signal quality; 2) poor performance on unseen text, indicating the\nneed for models that can better generalize to diverse linguistic contexts; 3)\ninsufficient integration of information from other modalities, which could\npotentially constrain our capacity to comprehensively understand the intricate\ndynamics of brain activity.\n  This study presents a novel approach for translating MEG signals into text\nusing a speech-decoding framework with multiple alignments. Our method is the\nfirst to introduce an end-to-end multi-alignment framework for totally unseen\ntext generation directly from MEG signals. We achieve an impressive BLEU-1\nscore on the $\\textit{GWilliams}$ dataset, significantly outperforming the\nbaseline from 5.49 to 10.44 on the BLEU-1 metric. This improvement demonstrates\nthe advancement of our model towards real-world applications and underscores\nits potential in advancing BCI research. Code is available at\n$\\href{https://github.com/NeuSpeech/MAD-MEG2text}{https://github.com/NeuSpeech/MAD-MEG2text}$.", "published": "2024-06-03 16:43:10", "link": "http://arxiv.org/abs/2406.01512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoFiT: Localized Fine-tuning on LLM Representations", "abstract": "Recent work in interpretability shows that large language models (LLMs) can\nbe adapted for new tasks in a learning-free way: it is possible to intervene on\nLLM representations to elicit desired behaviors for alignment. For instance,\nadding certain bias vectors to the outputs of certain attention heads is\nreported to boost the truthfulness of models. In this work, we show that\nlocalized fine-tuning serves as an effective alternative to such representation\nintervention methods. We introduce a framework called Localized Fine-Tuning on\nLLM Representations (LoFiT), which identifies a subset of attention heads that\nare most important for learning a specific task, then trains offset vectors to\nadd to the model's hidden representations at those selected heads. LoFiT\nlocalizes to a sparse set of heads (3%-10%) and learns the offset vectors from\nlimited training data, comparable to the settings used for representation\nintervention. For truthfulness and reasoning tasks, we find that LoFiT's\nintervention vectors are more effective for LLM adaptation than vectors from\nrepresentation intervention methods such as Inference-time Intervention. We\nalso find that the localization step is important: selecting a task-specific\nset of attention heads can lead to higher performance than intervening on heads\nselected for a different task. Finally, across 7 tasks we study, LoFiT achieves\ncomparable performance to other parameter-efficient fine-tuning methods such as\nLoRA, despite modifying 20x-200x fewer parameters than these methods.", "published": "2024-06-03 17:45:41", "link": "http://arxiv.org/abs/2406.01563v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language\n  Understanding Benchmark", "abstract": "In the age of large-scale language models, benchmarks like the Massive\nMultitask Language Understanding (MMLU) have been pivotal in pushing the\nboundaries of what AI can achieve in language comprehension and reasoning\nacross diverse domains. However, as models continue to improve, their\nperformance on these benchmarks has begun to plateau, making it increasingly\ndifficult to discern differences in model capabilities. This paper introduces\nMMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven\nMMLU benchmark by integrating more challenging, reasoning-focused questions and\nexpanding the choice set from four to ten options. Additionally, MMLU-Pro\neliminates the trivial and noisy questions in MMLU. Our experimental results\nshow that MMLU-Pro not only raises the challenge, causing a significant drop in\naccuracy by 16% to 33% compared to MMLU but also demonstrates greater stability\nunder varying prompts. With 24 different prompt styles tested, the sensitivity\nof model scores to prompt variations decreased from 4-5% in MMLU to just 2% in\nMMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)\nreasoning achieved better performance on MMLU-Pro compared to direct answering,\nwhich is in stark contrast to the findings on the original MMLU, indicating\nthat MMLU-Pro includes more complex reasoning questions. Our assessments\nconfirm that MMLU-Pro is a more discriminative benchmark to better track\nprogress in the field.", "published": "2024-06-03 17:53:00", "link": "http://arxiv.org/abs/2406.01574v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs", "abstract": "Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address Normal\nOutliers, which are activations across all tokens with relatively large\nmagnitudes. However, these methods struggle with smoothing Massive Outliers\nthat display significantly larger values, which leads to significant\nperformance degradation in low-bit quantization. In this paper, we introduce\nDuQuant, a novel approach that utilizes rotation and permutation\ntransformations to more effectively mitigate both massive and normal outliers.\nFirst, DuQuant starts by constructing the rotation matrix, using specific\noutlier dimensions as prior knowledge, to redistribute outliers to adjacent\nchannels by block-wise rotation. Second, We further employ a zigzag permutation\nto balance the distribution of outliers across blocks, thereby reducing\nblock-wise variance. A subsequent rotation further smooths the activation\nlandscape, enhancing model performance. DuQuant simplifies the quantization\nprocess and excels in managing outliers, outperforming the state-of-the-art\nbaselines across various sizes and types of LLMs on multiple tasks, even with\n4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant.", "published": "2024-06-03 18:27:44", "link": "http://arxiv.org/abs/2406.01721v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Harnessing Large Language Models for Comprehension of\n  Conversational Grounding", "abstract": "Conversational grounding is a collaborative mechanism for establishing mutual\nknowledge among participants engaged in a dialogue. This experimental study\nanalyzes information-seeking conversations to investigate the capabilities of\nlarge language models in classifying dialogue turns related to explicit or\nimplicit grounding and predicting grounded knowledge elements. Our experimental\nresults reveal challenges encountered by large language models in the two tasks\nand discuss ongoing research efforts to enhance large language model-based\nconversational grounding comprehension through pipeline architectures and\nknowledge bases. These initiatives aim to develop more effective dialogue\nsystems that are better equipped to handle the intricacies of grounded\nknowledge in conversations.", "published": "2024-06-03 19:34:39", "link": "http://arxiv.org/abs/2406.01749v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with\n  Cross-Lingual Feedback", "abstract": "To democratize large language models (LLMs) to most natural languages, it is\nimperative to make these models capable of understanding and generating texts\nin many languages, in particular low-resource ones. While recent multilingual\nLLMs demonstrate remarkable performance in such capabilities, these LLMs still\nsupport a limited number of human languages due to the lack of training data\nfor low-resource languages. Moreover, these LLMs are not yet aligned with human\npreference for downstream tasks, which is crucial for the success of LLMs in\nEnglish. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively\nxLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100\nlanguages. To do so, we construct two datasets: a multilingual instruction\ndataset including 100 languages, which represents the largest language coverage\nto date, and a cross-lingual human feedback dataset encompassing 30 languages.\nWe perform multilingual instruction tuning on the constructed instruction data\nand further align the LLMs with human feedback using the DPO algorithm on our\ncross-lingual human feedback dataset. We evaluate the multilingual\nunderstanding and generating capabilities of xLLMs-100 on five multilingual\nbenchmarks. Experimental results show that xLLMs-100 consistently outperforms\nits peers across the benchmarks by considerable margins, defining a new\nstate-of-the-art multilingual LLM that supports 100 languages.", "published": "2024-06-03 20:25:12", "link": "http://arxiv.org/abs/2406.01771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models", "abstract": "The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling unprecedented capabilities in understanding and\ngenerating human-like text. However, the computational cost and convergence\ntimes associated with fine-tuning these models remain significant challenges.\nLow-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these\nissues by introducing efficient fine-tuning techniques with a reduced number of\ntrainable parameters. In this paper, we present OLoRA, an enhancement to the\nLoRA method that leverages orthonormal matrix initialization through QR\ndecomposition. OLoRA significantly accelerates the convergence of LLM training\nwhile preserving the efficiency benefits of LoRA, such as the number of\ntrainable parameters and GPU memory footprint. Our empirical evaluations\ndemonstrate that OLoRA not only converges faster but also exhibits improved\nperformance compared to standard LoRA across a variety of language modeling\ntasks. This advancement opens new avenues for more efficient and accessible\nfine-tuning of LLMs, potentially enabling broader adoption and innovation in\nnatural language applications.", "published": "2024-06-03 20:37:27", "link": "http://arxiv.org/abs/2406.01775v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Review of Computational Epigraphy", "abstract": "Computational Epigraphy refers to the process of extracting text from stone\ninscription, transliteration, interpretation, and attribution with the aid of\ncomputational methods. Traditional epigraphy methods are time consuming, and\ntend to damage the stone inscriptions while extracting text. Additionally,\ninterpretation and attribution are subjective and can vary between different\nepigraphers. However, using modern computation methods can not only be used to\nextract text, but also interpret and attribute the text in a robust way. We\nsurvey and document the existing computational methods that aid in the\nabove-mentioned tasks in epigraphy.", "published": "2024-06-03 15:55:24", "link": "http://arxiv.org/abs/2406.06570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive\n  Clinical Reasoning", "abstract": "Users typically engage with LLMs interactively, yet most existing benchmarks\nevaluate them in a static, single-turn format, posing reliability concerns in\ninteractive scenarios. We identify a key obstacle towards reliability: LLMs are\ntrained to answer any question, even with incomplete context or insufficient\nknowledge. In this paper, we propose to change the static paradigm to an\ninteractive one, develop systems that proactively ask questions to gather more\ninformation and respond reliably, and introduce an benchmark - MediQ - to\nevaluate question-asking ability in LLMs. MediQ simulates clinical interactions\nconsisting of a Patient System and an adaptive Expert System; with potentially\nincomplete initial information, the Expert refrains from making diagnostic\ndecisions when unconfident, and instead elicits missing details via follow-up\nquestions. We provide a pipeline to convert single-turn medical benchmarks into\nan interactive format. Our results show that directly prompting\nstate-of-the-art LLMs to ask questions degrades performance, indicating that\nadapting LLMs to proactive information-seeking settings is nontrivial. We\nexperiment with abstention strategies to better estimate model confidence and\ndecide when to ask questions, improving diagnostic accuracy by 22.3%; however,\nperformance still lags compared to an (unrealistic in practice) upper bound\nwith complete information upfront. Further analyses show improved interactive\nperformance with filtering irrelevant contexts and reformatting conversations.\nOverall, we introduce a novel problem towards LLM reliability, an interactive\nMediQ benchmark and a novel question-asking system, and highlight directions to\nextend LLMs' information-seeking abilities in critical domains.", "published": "2024-06-03 01:32:52", "link": "http://arxiv.org/abs/2406.00922v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Annotation Guidelines-Based Knowledge Augmentation: Towards Enhancing\n  Large Language Models for Educational Text Classification", "abstract": "Various machine learning approaches have gained significant popularity for\nthe automated classification of educational text to identify indicators of\nlearning engagement -- i.e. learning engagement classification (LEC). LEC can\noffer comprehensive insights into human learning processes, attracting\nsignificant interest from diverse research communities, including Natural\nLanguage Processing (NLP), Learning Analytics, and Educational Data Mining.\nRecently, Large Language Models (LLMs), such as ChatGPT, have demonstrated\nremarkable performance in various NLP tasks. However, their comprehensive\nevaluation and improvement approaches in LEC tasks have not been thoroughly\ninvestigated. In this study, we propose the Annotation Guidelines-based\nKnowledge Augmentation (AGKA) approach to improve LLMs. AGKA employs GPT 4.0 to\nretrieve label definition knowledge from annotation guidelines, and then\napplies the random under-sampler to select a few typical examples.\nSubsequently, we conduct a systematic evaluation benchmark of LEC, which\nincludes six LEC datasets covering behavior classification (question and\nurgency level), emotion classification (binary and epistemic emotion), and\ncognition classification (opinion and cognitive presence). The study results\ndemonstrate that AGKA can enhance non-fine-tuned LLMs, particularly GPT 4.0 and\nLlama 3 70B. GPT 4.0 with AGKA few-shot outperforms full-shot fine-tuned models\nsuch as BERT and RoBERTa on simple binary classification datasets. However, GPT\n4.0 lags in multi-class tasks that require a deep understanding of complex\nsemantic information. Notably, Llama 3 70B with AGKA is a promising combination\nbased on open-source LLM, because its performance is on par with closed-source\nGPT 4.0 with AGKA. In addition, LLMs struggle to distinguish between labels\nwith similar names in multi-class classification.", "published": "2024-06-03 03:09:01", "link": "http://arxiv.org/abs/2406.00954v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Luna: An Evaluation Foundation Model to Catch Language Model\n  Hallucinations with High Accuracy and Low Cost", "abstract": "Retriever Augmented Generation (RAG) systems have become pivotal in enhancing\nthe capabilities of language models by incorporating external knowledge\nretrieval mechanisms. However, a significant challenge in deploying these\nsystems in industry applications is the detection and mitigation of\nhallucinations: instances where the model generates information that is not\ngrounded in the retrieved context. Addressing this issue is crucial for\nensuring the reliability and accuracy of responses generated by large language\nmodels (LLMs) in diverse industry settings. Current hallucination detection\ntechniques fail to deliver accuracy, low latency, and low cost simultaneously.\nWe introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination\ndetection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and\ncommercial evaluation frameworks on the hallucination detection task, with 97%\nand 91% reduction in cost and latency, respectively. Luna is lightweight and\ngeneralizes across multiple industry verticals and out-of-domain data, making\nit an ideal candidate for industry LLM applications.", "published": "2024-06-03 04:14:21", "link": "http://arxiv.org/abs/2406.00975v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Selectively Answering Visual Questions", "abstract": "Recently, large multi-modal models (LMMs) have emerged with the capacity to\nperform vision tasks such as captioning and visual question answering (VQA)\nwith unprecedented accuracy. Applications such as helping the blind or visually\nimpaired have a critical need for precise answers. It is specially important\nfor models to be well calibrated and be able to quantify their uncertainty in\norder to selectively decide when to answer and when to abstain or ask for\nclarifications. We perform the first in-depth analysis of calibration methods\nand metrics for VQA with in-context learning LMMs. Studying VQA on two\nanswerability benchmarks, we show that the likelihood score of visually\ngrounded models is better calibrated than in their text-only counterparts for\nin-context learning, where sampling based methods are generally superior, but\nno clear winner arises. We propose Avg BLEU, a calibration score combining the\nbenefits of both sampling and likelihood methods across modalities.", "published": "2024-06-03 04:28:10", "link": "http://arxiv.org/abs/2406.00980v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Take its Essence, Discard its Dross! Debiasing for Toxic Language\n  Detection via Counterfactual Causal Effect", "abstract": "Current methods of toxic language detection (TLD) typically rely on specific\ntokens to conduct decisions, which makes them suffer from lexical bias, leading\nto inferior performance and generalization. Lexical bias has both \"useful\" and\n\"misleading\" impacts on understanding toxicity. Unfortunately, instead of\ndistinguishing between these impacts, current debiasing methods typically\neliminate them indiscriminately, resulting in a degradation in the detection\naccuracy of the model. To this end, we propose a Counterfactual Causal\nDebiasing Framework (CCDF) to mitigate lexical bias in TLD. It preserves the\n\"useful impact\" of lexical bias and eliminates the \"misleading impact\".\nSpecifically, we first represent the total effect of the original sentence and\nbiased tokens on decisions from a causal view. We then conduct counterfactual\ninference to exclude the direct causal effect of lexical bias from the total\neffect. Empirical evaluations demonstrate that the debiased TLD model\nincorporating CCDF achieves state-of-the-art performance in both accuracy and\nfairness compared to competitive baselines applied on several vanilla models.\nThe generalization capability of our model outperforms current debiased models\nfor out-of-distribution data.", "published": "2024-06-03 04:34:30", "link": "http://arxiv.org/abs/2406.00983v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scalable Ensembling For Mitigating Reward Overoptimisation", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has enabled significant\nadvancements within language modeling for powerful, instruction-following\nmodels. However, the alignment of these models remains a pressing challenge as\nthe policy tends to overfit the learned ``proxy\" reward model past an\ninflection point of utility as measured by a ``gold\" reward model that is more\nperformant -- a phenomenon known as overoptimisation. Prior work has mitigated\nthis issue by computing a pessimistic statistic over an ensemble of reward\nmodels, which is common in Offline Reinforcement Learning but incredibly costly\nfor language models with high memory requirements, making such approaches\ninfeasible for sufficiently large models. To this end, we propose using a\nshared encoder but separate linear heads. We find this leads to similar\nperformance as the full ensemble while allowing tremendous savings in memory\nand time required for training for models of similar size.", "published": "2024-06-03 05:46:53", "link": "http://arxiv.org/abs/2406.01013v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective\n  Navigation via Multi-Agent Collaboration", "abstract": "Mobile device operation tasks are increasingly becoming a popular multi-modal\nAI application scenario. Current Multi-modal Large Language Models (MLLMs),\nconstrained by their training data, lack the capability to function effectively\nas operation assistants. Instead, MLLM-based agents, which enhance capabilities\nthrough tool invocation, are gradually being applied to this scenario. However,\nthe two major navigation challenges in mobile device operation tasks, task\nprogress navigation and focus content navigation, are significantly complicated\nunder the single-agent architecture of existing work. This is due to the overly\nlong token sequences and the interleaved text-image data format, which limit\nperformance. To address these navigation challenges effectively, we propose\nMobile-Agent-v2, a multi-agent architecture for mobile device operation\nassistance. The architecture comprises three agents: planning agent, decision\nagent, and reflection agent. The planning agent generates task progress, making\nthe navigation of history operations more efficient. To retain focus content,\nwe design a memory unit that updates with task progress. Additionally, to\ncorrect erroneous operations, the reflection agent observes the outcomes of\neach operation and handles any mistakes accordingly. Experimental results\nindicate that Mobile-Agent-v2 achieves over a 30% improvement in task\ncompletion compared to the single-agent architecture of Mobile-Agent. The code\nis open-sourced at https://github.com/X-PLUG/MobileAgent.", "published": "2024-06-03 05:50:00", "link": "http://arxiv.org/abs/2406.01014v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs", "abstract": "Large Language Models (LLMs) demonstrate significant capabilities in\nprocessing natural language data, promising efficient knowledge extraction from\ndiverse textual sources to enhance situational awareness and support\ndecision-making. However, concerns arise due to their susceptibility to\nhallucination, resulting in contextually inaccurate content. This work focuses\non harnessing LLMs for automated Event Extraction, introducing a new method to\naddress hallucination by decomposing the task into Event Detection and Event\nArgument Extraction. Moreover, the proposed method integrates dynamic\nschema-aware augmented retrieval examples into prompts tailored for each\nspecific inquiry, thereby extending and adapting advanced prompting techniques\nsuch as Retrieval-Augmented Generation. Evaluation findings on prominent event\nextraction benchmarks and results from a synthesized benchmark illustrate the\nmethod's superior performance compared to baseline approaches.", "published": "2024-06-03 06:55:10", "link": "http://arxiv.org/abs/2406.01045v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synergizing Unsupervised and Supervised Learning: A Hybrid Approach for\n  Accurate Natural Language Task Modeling", "abstract": "While supervised learning models have shown remarkable performance in various\nnatural language processing (NLP) tasks, their success heavily relies on the\navailability of large-scale labeled datasets, which can be costly and\ntime-consuming to obtain. Conversely, unsupervised learning techniques can\nleverage abundant unlabeled text data to learn rich representations, but they\ndo not directly optimize for specific NLP tasks. This paper presents a novel\nhybrid approach that synergizes unsupervised and supervised learning to improve\nthe accuracy of NLP task modeling. While supervised models excel at specific\ntasks, they rely on large labeled datasets. Unsupervised techniques can learn\nrich representations from abundant unlabeled text but don't directly optimize\nfor tasks. Our methodology integrates an unsupervised module that learns\nrepresentations from unlabeled corpora (e.g., language models, word embeddings)\nand a supervised module that leverages these representations to enhance\ntask-specific models. We evaluate our approach on text classification and named\nentity recognition (NER), demonstrating consistent performance gains over\nsupervised baselines. For text classification, contextual word embeddings from\na language model pretrain a recurrent or transformer-based classifier. For NER,\nword embeddings initialize a BiLSTM sequence labeler. By synergizing\ntechniques, our hybrid approach achieves SOTA results on benchmark datasets,\npaving the way for more data-efficient and robust NLP systems.", "published": "2024-06-03 08:31:35", "link": "http://arxiv.org/abs/2406.01096v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Latent Logic Tree Extraction for Event Sequence Explanation from LLMs", "abstract": "Modern high-stakes systems, such as healthcare or robotics, often generate\nvast streaming event sequences. Our goal is to design an efficient,\nplug-and-play tool to elicit logic tree-based explanations from Large Language\nModels (LLMs) to provide customized insights into each observed event sequence.\nBuilt on the temporal point process model for events, our method employs the\nlikelihood function as a score to evaluate generated logic trees. We propose an\namortized Expectation-Maximization (EM) learning framework and treat the logic\ntree as latent variables. In the E-step, we evaluate the posterior distribution\nover the latent logic trees using an LLM prior and the likelihood of the\nobserved event sequences. LLM provides a high-quality prior for the latent\nlogic trees, however, since the posterior is built over a discrete\ncombinatorial space, we cannot get the closed-form solution. We propose to\ngenerate logic tree samples from the posterior using a learnable GFlowNet,\nwhich is a diversity-seeking generator for structured discrete variables. The\nM-step employs the generated logic rules to approximate marginalization over\nthe posterior, facilitating the learning of model parameters and refining the\ntunable LLM prior parameters. In the online setting, our locally built,\nlightweight model will iteratively extract the most relevant rules from LLMs\nfor each sequence using only a few iterations. Empirical demonstrations\nshowcase the promising performance and adaptability of our framework.", "published": "2024-06-03 09:10:42", "link": "http://arxiv.org/abs/2406.01124v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TCMBench: A Comprehensive Benchmark for Evaluating Large Language Models\n  in Traditional Chinese Medicine", "abstract": "Large language models (LLMs) have performed remarkably well in various\nnatural language processing tasks by benchmarking, including in the Western\nmedical domain. However, the professional evaluation benchmarks for LLMs have\nyet to be covered in the traditional Chinese medicine(TCM) domain, which has a\nprofound history and vast influence. To address this research gap, we introduce\nTCM-Bench, an comprehensive benchmark for evaluating LLM performance in TCM. It\ncomprises the TCM-ED dataset, consisting of 5,473 questions sourced from the\nTCM Licensing Exam (TCMLE), including 1,300 questions with authoritative\nanalysis. It covers the core components of TCMLE, including TCM basis and\nclinical practice. To evaluate LLMs beyond accuracy of question answering, we\npropose TCMScore, a metric tailored for evaluating the quality of answers\ngenerated by LLMs for TCM related questions. It comprehensively considers the\nconsistency of TCM semantics and knowledge. After conducting comprehensive\nexperimental analyses from diverse perspectives, we can obtain the following\nfindings: (1) The unsatisfactory performance of LLMs on this benchmark\nunderscores their significant room for improvement in TCM. (2) Introducing\ndomain knowledge can enhance LLMs' performance. However, for in-domain models\nlike ZhongJing-TCM, the quality of generated analysis text has decreased, and\nwe hypothesize that their fine-tuning process affects the basic LLM\ncapabilities. (3) Traditional metrics for text generation quality like Rouge\nand BertScore are susceptible to text length and surface semantic ambiguity,\nwhile domain-specific metrics such as TCMScore can further supplement and\nexplain their evaluation results. These findings highlight the capabilities and\nlimitations of LLMs in the TCM and aim to provide a more profound assistance to\nmedical research.", "published": "2024-06-03 09:11:13", "link": "http://arxiv.org/abs/2406.01126v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are AI-Generated Text Detectors Robust to Adversarial Perturbations?", "abstract": "The widespread use of large language models (LLMs) has sparked concerns about\nthe potential misuse of AI-generated text, as these models can produce content\nthat closely resembles human-generated text. Current detectors for AI-generated\ntext (AIGT) lack robustness against adversarial perturbations, with even minor\nchanges in characters or words causing a reversal in distinguishing between\nhuman-created and AI-generated text. This paper investigates the robustness of\nexisting AIGT detection methods and introduces a novel detector, the Siamese\nCalibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction\nnetwork to add and remove noise from text, extracting a semantic representation\nthat is robust to local perturbations. We also propose a siamese calibration\ntechnique to train the model to make equally confidence predictions under\ndifferent noise, which improves the model's robustness against adversarial\nperturbations. Experiments on four publicly available datasets show that the\nSCRN outperforms all baseline methods, achieving 6.5\\%-18.25\\% absolute\naccuracy improvement over the best baseline method under adversarial attacks.\nMoreover, it exhibits superior generalizability in cross-domain, cross-genre,\nand mixed-source scenarios. The code is available at\n\\url{https://github.com/CarlanLark/Robust-AIGC-Detector}.", "published": "2024-06-03 10:21:48", "link": "http://arxiv.org/abs/2406.01179v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of Generative Information Retrieval", "abstract": "Generative Retrieval (GR) is an emerging paradigm in information retrieval\nthat leverages generative models to directly map queries to relevant document\nidentifiers (DocIDs) without the need for traditional query processing or\ndocument reranking. This survey provides a comprehensive overview of GR,\nhighlighting key developments, indexing and retrieval strategies, and\nchallenges. We discuss various document identifier strategies, including\nnumerical and string-based identifiers, and explore different document\nrepresentation methods. Our primary contribution lies in outlining future\nresearch directions that could profoundly impact the field: improving the\nquality of query generation, exploring learnable document identifiers,\nenhancing scalability, and integrating GR with multi-task learning frameworks.\nBy examining state-of-the-art GR techniques and their applications, this survey\naims to provide a foundational understanding of GR and inspire further\ninnovations in this transformative approach to information retrieval. We also\nmake the complementary materials such as paper collection publicly available at\nhttps://github.com/MiuLab/GenIR-Survey/", "published": "2024-06-03 10:59:33", "link": "http://arxiv.org/abs/2406.01197v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Automatic Essay Multi-dimensional Scoring with Fine-tuning and Multiple\n  Regression", "abstract": "Automated essay scoring (AES) involves predicting a score that reflects the\nwriting quality of an essay. Most existing AES systems produce only a single\noverall score. However, users and L2 learners expect scores across different\ndimensions (e.g., vocabulary, grammar, coherence) for English essays in\nreal-world applications. To address this need, we have developed two models\nthat automatically score English essays across multiple dimensions by employing\nfine-tuning and other strategies on two large datasets. The results demonstrate\nthat our systems achieve impressive performance in evaluation using three\ncriteria: precision, F1 score, and Quadratic Weighted Kappa. Furthermore, our\nsystem outperforms existing methods in overall scoring.", "published": "2024-06-03 10:59:50", "link": "http://arxiv.org/abs/2406.01198v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Pseudo Labels with Global-Local Denoising Framework for\n  Cross-lingual Named Entity Recognition", "abstract": "Cross-lingual named entity recognition (NER) aims to train an NER model for\nthe target language leveraging only labeled source language data and unlabeled\ntarget language data. Prior approaches either perform label projection on\ntranslated source language data or employ a source model to assign pseudo\nlabels for target language data and train a target model on these\npseudo-labeled data to generalize to the target language. However, these\nautomatic labeling procedures inevitably introduce noisy labels, thus leading\nto a performance drop. In this paper, we propose a Global-Local Denoising\nframework (GLoDe) for cross-lingual NER. Specifically, GLoDe introduces a\nprogressive denoising strategy to rectify incorrect pseudo labels by leveraging\nboth global and local distribution information in the semantic space. The\nrefined pseudo-labeled target language data significantly improves the model's\ngeneralization ability. Moreover, previous methods only consider improving the\nmodel with language-agnostic features, however, we argue that target\nlanguage-specific features are also important and should never be ignored. To\nthis end, we employ a simple auxiliary task to achieve this goal. Experimental\nresults on two benchmark datasets with six target languages demonstrate that\nour proposed GLoDe significantly outperforms current state-of-the-art methods.", "published": "2024-06-03 11:29:19", "link": "http://arxiv.org/abs/2406.01213v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-word Term Embeddings Improve Lexical Product Retrieval", "abstract": "Product search is uniquely different from search for documents, Internet\nresources or vacancies, therefore it requires the development of specialized\nsearch systems. The present work describes the H1 embdedding model, designed\nfor an offline term indexing of product descriptions at e-commerce platforms.\nThe model is compared to other state-of-the-art (SoTA) embedding models within\na framework of hybrid product search system that incorporates the advantages of\nlexical methods for product retrieval and semantic embedding-based methods. We\npropose an approach to building semantically rich term vocabularies for search\nindexes. Compared to other production semantic models, H1 paired with the\nproposed approach stands out due to its ability to process multi-word product\nterms as one token. As an example, for search queries \"new balance shoes\",\n\"gloria jeans kids wear\" brand entity will be represented as one token - \"new\nbalance\", \"gloria jeans\". This results in an increased precision of the system\nwithout affecting the recall. The hybrid search system with proposed model\nscores mAP@12 = 56.1% and R@1k = 86.6% on the WANDS public dataset, beating\nother SoTA analogues.", "published": "2024-06-03 11:52:52", "link": "http://arxiv.org/abs/2406.01233v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Focus on the Core: Efficient Attention via Pruned Token Compression for\n  Document Classification", "abstract": "Transformer-based models have achieved dominant performance in numerous NLP\ntasks. Despite their remarkable successes, pre-trained transformers such as\nBERT suffer from a computationally expensive self-attention mechanism that\ninteracts with all tokens, including the ones unfavorable to classification\nperformance. To overcome these challenges, we propose integrating two\nstrategies: token pruning and token combining. Token pruning eliminates less\nimportant tokens in the attention mechanism's key and value as they pass\nthrough the layers. Additionally, we adopt fuzzy logic to handle uncertainty\nand alleviate potential mispruning risks arising from an imbalanced\ndistribution of each token's importance. Token combining, on the other hand,\ncondenses input sequences into smaller sizes in order to further compress the\nmodel. By integrating these two approaches, we not only improve the model's\nperformance but also reduce its computational demands. Experiments with various\ndatasets demonstrate superior performance compared to baseline models,\nespecially with the best improvement over the existing BERT model, achieving\n+5%p in accuracy and +5.6%p in F1 score. Additionally, memory cost is reduced\nto 0.61x, and a speedup of 1.64x is achieved.", "published": "2024-06-03 12:51:52", "link": "http://arxiv.org/abs/2406.01283v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Probing Language Models for Pre-training Data Detection", "abstract": "Large Language Models (LLMs) have shown their impressive capabilities, while\nalso raising concerns about the data contamination problems due to privacy\nissues and leakage of benchmark datasets in the pre-training phase. Therefore,\nit is vital to detect the contamination by checking whether an LLM has been\npre-trained on the target texts. Recent studies focus on the generated texts\nand compute perplexities, which are superficial features and not reliable. In\nthis study, we propose to utilize the probing technique for pre-training data\ndetection by examining the model's internal activations. Our method is simple\nand effective and leads to more trustworthy pre-training data detection.\nAdditionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv\nabstracts from Computer Science and Mathematics categories. Our experiments\ndemonstrate that our method outperforms all baselines, and achieves\nstate-of-the-art performance on both WikiMIA and ArxivMIA, with additional\nexperiments confirming its efficacy (Our code and dataset are available at\nhttps://github.com/zhliu0106/probing-lm-data).", "published": "2024-06-03 13:58:04", "link": "http://arxiv.org/abs/2406.01333v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code\n  Completion Abilities of Code Large Language Models", "abstract": "Code completion models have made significant progress in recent years.\nRecently, repository-level code completion has drawn more attention in modern\nsoftware development, and several baseline methods and benchmarks have been\nproposed. However, existing repository-level code completion methods often fall\nshort of fully using the extensive context of a project repository, such as the\nintricacies of relevant files and class hierarchies. Besides, the existing\nbenchmarks usually focus on limited code completion scenarios, which cannot\nreflect the repository-level code completion abilities well of existing\nmethods. To address these limitations, we propose the R2C2-Coder to enhance and\nbenchmark the real-world repository-level code completion abilities of code\nLarge Language Models, where the R2C2-Coder includes a code prompt construction\nmethod R2C2-Enhance and a well-designed benchmark R2C2-Bench. Specifically,\nfirst, in R2C2-Enhance, we first construct the candidate retrieval pool and\nthen assemble the completion prompt by retrieving from the retrieval pool for\neach completion cursor position. Second, based on R2C2 -Enhance, we can\nconstruct a more challenging and diverse R2C2-Bench with training, validation\nand test splits, where a context perturbation strategy is proposed to simulate\nthe real-world repository-level code completion well. Extensive results on\nmultiple benchmarks demonstrate the effectiveness of our R2C2-Coder.", "published": "2024-06-03 14:24:29", "link": "http://arxiv.org/abs/2406.01359v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Privacy in LLM-based Recommendation: Recent Advances and Future\n  Directions", "abstract": "Nowadays, large language models (LLMs) have been integrated with conventional\nrecommendation models to improve recommendation performance. However, while\nmost of the existing works have focused on improving the model performance, the\nprivacy issue has only received comparatively less attention. In this paper, we\nreview recent advancements in privacy within LLM-based recommendation,\ncategorizing them into privacy attacks and protection mechanisms. Additionally,\nwe highlight several challenges and propose future directions for the community\nto address these critical problems.", "published": "2024-06-03 14:31:47", "link": "http://arxiv.org/abs/2406.01363v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Perform the Way People Expect? Measuring the\n  Human Generalization Function", "abstract": "What makes large language models (LLMs) impressive is also what makes them\nhard to evaluate: their diversity of uses. To evaluate these models, we must\nunderstand the purposes they will be used for. We consider a setting where\nthese deployment decisions are made by people, and in particular, people's\nbeliefs about where an LLM will perform well. We model such beliefs as the\nconsequence of a human generalization function: having seen what an LLM gets\nright or wrong, people generalize to where else it might succeed. We collect a\ndataset of 19K examples of how humans make generalizations across 79 tasks from\nthe MMLU and BIG-Bench benchmarks. We show that the human generalization\nfunction can be predicted using NLP methods: people have consistent structured\nways to generalize. We then evaluate LLM alignment with the human\ngeneralization function. Our results show that -- especially for cases where\nthe cost of mistakes is high -- more capable models (e.g. GPT-4) can do worse\non the instances people choose to use them for, exactly because they are not\naligned with the human generalization function.", "published": "2024-06-03 14:45:21", "link": "http://arxiv.org/abs/2406.01382v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Alibaba LingmaAgent: Improving Automated Issue Resolution via\n  Comprehensive Repository Exploration", "abstract": "This paper presents Alibaba LingmaAgent, a novel Automated Software\nEngineering method designed to comprehensively understand and utilize whole\nsoftware repositories for issue resolution. Deployed in TONGYI Lingma, an\nIDE-based coding assistant developed by Alibaba Cloud, LingmaAgent addresses\nthe limitations of existing LLM-based agents that primarily focus on local code\ninformation. Our approach introduces a top-down method to condense critical\nrepository information into a knowledge graph, reducing complexity, and employs\na Monte Carlo tree search based strategy enabling agents to explore and\nunderstand entire repositories. We guide agents to summarize, analyze, and plan\nusing repository-level knowledge, allowing them to dynamically acquire\ninformation and generate patches for real-world GitHub issues. In extensive\nexperiments, LingmaAgent demonstrated significant improvements, achieving an\n18.5\\% relative improvement on the SWE-bench Lite benchmark compared to\nSWE-agent. In production deployment and evaluation at Alibaba Cloud,\nLingmaAgent automatically resolved 16.9\\% of in-house issues faced by\ndevelopment engineers, and solved 43.3\\% of problems after manual intervention.\nAdditionally, we have open-sourced a Python prototype of LingmaAgent for\nreference by other industrial developers\nhttps://github.com/RepoUnderstander/RepoUnderstander. In fact, LingmaAgent has\nbeen used as a developed reference by many subsequently agents.", "published": "2024-06-03 15:20:06", "link": "http://arxiv.org/abs/2406.01422v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Superhuman performance in urology board questions by an explainable\n  large language model enabled for context integration of the European\n  Association of Urology guidelines: the UroBot study", "abstract": "Large Language Models (LLMs) are revolutionizing medical Question-Answering\n(medQA) through extensive use of medical literature. However, their performance\nis often hampered by outdated training data and a lack of explainability, which\nlimits clinical applicability. This study aimed to create and assess UroBot, a\nurology-specialized chatbot, by comparing it with state-of-the-art models and\nthe performance of urologists on urological board questions, ensuring full\nclinician-verifiability. UroBot was developed using OpenAI's GPT-3.5, GPT-4,\nand GPT-4o models, employing retrieval-augmented generation (RAG) and the\nlatest 2023 guidelines from the European Association of Urology (EAU). The\nevaluation included ten runs of 200 European Board of Urology (EBU) In-Service\nAssessment (ISA) questions, with performance assessed by the mean Rate of\nCorrect Answers (RoCA). UroBot-4o achieved an average RoCA of 88.4%, surpassing\nGPT-4o by 10.8%, with a score of 77.6%. It was also clinician-verifiable and\nexhibited the highest run agreement as indicated by Fleiss' Kappa (k = 0.979).\nBy comparison, the average performance of urologists on board questions, as\nreported in the literature, is 68.7%. UroBot's clinician-verifiable nature and\nsuperior accuracy compared to both existing models and urologists on board\nquestions highlight its potential for clinical integration. The study also\nprovides the necessary code and instructions for further development of UroBot.", "published": "2024-06-03 15:26:06", "link": "http://arxiv.org/abs/2406.01428v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "What Are Large Language Models Mapping to in the Brain? A Case Against\n  Over-Reliance on Brain Scores", "abstract": "Given the remarkable capabilities of large language models (LLMs), there has\nbeen a growing interest in evaluating their similarity to the human brain. One\napproach towards quantifying this similarity is by measuring how well a model\npredicts neural signals, also called \"brain score\". Internal representations\nfrom LLMs achieve state-of-the-art brain scores, leading to speculation that\nthey share computational principles with human language processing. This\ninference is only valid if the subset of neural activity predicted by LLMs\nreflects core elements of language processing. Here, we question this\nassumption by analyzing three neural datasets used in an impactful study on\nLLM-to-brain mappings, with a particular focus on an fMRI dataset where\nparticipants read short passages. We first find that when using shuffled\ntrain-test splits, as done in previous studies with these datasets, a trivial\nfeature that encodes temporal autocorrelation not only outperforms LLMs but\nalso accounts for the majority of neural variance that LLMs explain. We\ntherefore use contiguous splits moving forward. Second, we explain the\nsurprisingly high brain scores of untrained LLMs by showing they do not account\nfor additional neural variance beyond two simple features: sentence length and\nsentence position. This undermines evidence used to claim that the transformer\narchitecture biases computations to be more brain-like. Third, we find that\nbrain scores of trained LLMs on this dataset can largely be explained by\nsentence length, position, and pronoun-dereferenced static word embeddings; a\nsmall, additional amount is explained by sense-specific embeddings and\ncontextual representations of sentence structure. We conclude that\nover-reliance on brain scores can lead to over-interpretations of similarity\nbetween LLMs and brains, and emphasize the importance of deconstructing what\nLLMs are mapping to in neural signals.", "published": "2024-06-03 17:13:27", "link": "http://arxiv.org/abs/2406.01538v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Information Bottleneck Perspective for Effective Noise Filtering on\n  Retrieval-Augmented Generation", "abstract": "Retrieval-augmented generation integrates the capabilities of large language\nmodels with relevant information retrieved from an extensive corpus, yet\nencounters challenges when confronted with real-world noisy data. One recent\nsolution is to train a filter module to find relevant content but only achieve\nsuboptimal noise compression. In this paper, we propose to introduce the\ninformation bottleneck theory into retrieval-augmented generation. Our approach\ninvolves the filtration of noise by simultaneously maximizing the mutual\ninformation between compression and ground output, while minimizing the mutual\ninformation between compression and retrieved passage. In addition, we derive\nthe formula of information bottleneck to facilitate its application in novel\ncomprehensive evaluations, the selection of supervised fine-tuning data, and\nthe construction of reinforcement learning rewards. Experimental results\ndemonstrate that our approach achieves significant improvements across various\nquestion answering datasets, not only in terms of the correctness of answer\ngeneration but also in the conciseness with $2.5\\%$ compression rate.", "published": "2024-06-03 17:31:06", "link": "http://arxiv.org/abs/2406.01549v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contextualized Sequence Likelihood: Enhanced Confidence Scores for\n  Natural Language Generation", "abstract": "The advent of large language models (LLMs) has dramatically advanced the\nstate-of-the-art in numerous natural language generation tasks. For LLMs to be\napplied reliably, it is essential to have an accurate measure of their\nconfidence. Currently, the most commonly used confidence score function is the\nlikelihood of the generated sequence, which, however, conflates semantic and\nsyntactic components. For instance, in question-answering (QA) tasks, an\nawkward phrasing of the correct answer might result in a lower probability\nprediction. Additionally, different tokens should be weighted differently\ndepending on the context. In this work, we propose enhancing the predicted\nsequence probability by assigning different weights to various tokens using\nattention values elicited from the base LLM. By employing a validation set, we\ncan identify the relevant attention heads, thereby significantly improving the\nreliability of the vanilla sequence probability confidence measure. We refer to\nthis new score as the Contextualized Sequence Likelihood (CSL). CSL is easy to\nimplement, fast to compute, and offers considerable potential for further\nimprovement with task-specific prompts. Across several QA datasets and a\ndiverse array of LLMs, CSL has demonstrated significantly higher reliability\nthan state-of-the-art baselines in predicting generation quality, as measured\nby the AUROC or AUARC.", "published": "2024-06-03 21:55:07", "link": "http://arxiv.org/abs/2406.01806v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Open Multilingual System for Scoring Readability of Wikipedia", "abstract": "With over 60M articles, Wikipedia has become the largest platform for open\nand freely accessible knowledge. While it has more than 15B monthly visits, its\ncontent is believed to be inaccessible to many readers due to the lack of\nreadability of its text. However, previous investigations of the readability of\nWikipedia have been restricted to English only, and there are currently no\nsystems supporting the automatic readability assessment of the 300+ languages\nin Wikipedia. To bridge this gap, we develop a multilingual model to score the\nreadability of Wikipedia articles. To train and evaluate this model, we create\na novel multilingual dataset spanning 14 languages, by matching articles from\nWikipedia to simplified Wikipedia and online children encyclopedias. We show\nthat our model performs well in a zero-shot scenario, yielding a ranking\naccuracy of more than 80% across 14 languages and improving upon previous\nbenchmarks. These results demonstrate the applicability of the model at scale\nfor languages in which there is no ground-truth data available for model\nfine-tuning. Furthermore, we provide the first overview on the state of\nreadability in Wikipedia beyond English.", "published": "2024-06-03 23:07:18", "link": "http://arxiv.org/abs/2406.01835v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Achieving Sparse Activation in Small Language Models", "abstract": "Sparse activation, which selectively activates only an input-dependent set of\nneurons in inference, is a useful technique to reduce the computing cost of\nLarge Language Models (LLMs) without retraining or adaptation efforts. However,\nwhether it can be applied to the recently emerging Small Language Models (SLMs)\nremains questionable, because SLMs are generally less over-parameterized than\nLLMs. In this paper, we aim to achieve sparse activation in SLMs. We first show\nthat the existing sparse activation schemes in LLMs that build on neurons'\noutput magnitudes cannot be applied to SLMs, and activating neurons based on\ntheir attribution scores is a better alternative. Further, we demonstrated and\nquantified the large errors of existing attribution metrics when being used for\nsparse activation, due to the interdependency among attribution scores of\nneurons across different layers. Based on these observations, we proposed a new\nattribution metric that can provably correct such errors and achieve precise\nsparse activation. Experiments over multiple popular SLMs and datasets show\nthat our approach can achieve 80% sparsification ratio with <5% model accuracy\nloss, comparable to the sparse activation achieved in LLMs. The source code is\navailable at: https://github.com/pittisl/Sparse-Activation.", "published": "2024-06-03 03:21:49", "link": "http://arxiv.org/abs/2406.06562v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts\n  Language Models", "abstract": "In this technical report, we introduce the training methodologies implemented\nin the development of Skywork-MoE, a high-performance mixture-of-experts (MoE)\nlarge language model (LLM) with 146 billion parameters and 16 experts. It is\ninitialized from the pre-existing dense checkpoints of our Skywork-13B model.\nWe explore the comparative effectiveness of upcycling versus training from\nscratch initializations. Our findings suggest that the choice between these two\napproaches should consider both the performance of the existing dense\ncheckpoints and the MoE training budget. We highlight two innovative\ntechniques: gating logit normalization, which improves expert diversification,\nand adaptive auxiliary loss coefficients, allowing for layer-specific\nadjustment of auxiliary loss coefficients. Our experimental results validate\nthe effectiveness of these methods. Leveraging these techniques and insights,\nwe trained our upcycled Skywork-MoE on a condensed subset of our SkyPile\ncorpus. The evaluation results demonstrate that our model delivers strong\nperformance across a wide range of benchmarks.", "published": "2024-06-03 03:58:41", "link": "http://arxiv.org/abs/2406.06563v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM", "abstract": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm.", "published": "2024-06-03 16:43:04", "link": "http://arxiv.org/abs/2406.06571v5", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MedFuzz: Exploring the Robustness of Large Language Models in Medical\n  Question Answering", "abstract": "Large language models (LLM) have achieved impressive performance on medical\nquestion-answering benchmarks. However, high benchmark accuracy does not imply\nthat the performance generalizes to real-world clinical settings. Medical\nquestion-answering benchmarks rely on assumptions consistent with quantifying\nLLM performance but that may not hold in the open world of the clinic. Yet LLMs\nlearn broad knowledge that can help the LLM generalize to practical conditions\nregardless of unrealistic assumptions in celebrated benchmarks. We seek to\nquantify how well LLM medical question-answering benchmark performance\ngeneralizes when benchmark assumptions are violated. Specifically, we present\nan adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz\nattempts to modify benchmark questions in ways aimed at confounding the LLM. We\ndemonstrate the approach by targeting strong assumptions about patient\ncharacteristics presented in the MedQA benchmark. Successful \"attacks\" modify a\nbenchmark item in ways that would be unlikely to fool a medical expert but\nnonetheless \"trick\" the LLM into changing from a correct to an incorrect\nanswer. Further, we present a permutation test technique that can ensure a\nsuccessful attack is statistically significant. We show how to use performance\non a \"MedFuzzed\" benchmark, as well as individual successful attacks. The\nmethods show promise at providing insights into the ability of an LLM to\noperate robustly in more realistic settings.", "published": "2024-06-03 18:15:56", "link": "http://arxiv.org/abs/2406.06573v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Transparency: Exploring LLM Trainings Datasets through Visual\n  Topic Modeling and Semantic Frame", "abstract": "LLMs are now responsible for making many decisions on behalf of humans: from\nanswering questions to classifying things, they have become an important part\nof everyday life. While computation and model architecture have been rapidly\nexpanding in recent years, the efforts towards curating training datasets are\nstill in their beginnings. This underappreciation of training datasets has led\nLLMs to create biased and low-quality content. In order to solve that issue, we\npresent Bunka, a software that leverages AI and Cognitive Science to improve\nthe refinement of textual datasets. We show how Topic Modeling coupled with\n2-dimensional Cartography can increase the transparency of datasets. We then\nshow how the same Topic Modeling techniques can be applied to Preferences\ndatasets to accelerate the fine-tuning process and increase the capacities of\nthe model on different benchmarks. Lastly, we show how using Frame Analysis can\ngive insights into existing biases in the training corpus. Overall, we argue\nthat we need better tools to explore and increase the quality and transparency\nof LLMs training datasets.", "published": "2024-06-03 18:44:13", "link": "http://arxiv.org/abs/2406.06574v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and\n  Abbreviation De-hallucination", "abstract": "Electronic design engineers are challenged to find relevant information\nefficiently for a myriad of tasks within design construction, verification and\ntechnology development. Large language models (LLM) have the potential to help\nimprove productivity by serving as conversational agents that effectively\nfunction as subject-matter experts. In this paper we demonstrate Ask-EDA, a\nchat agent designed to serve as a 24x7 expert available to provide guidance to\ndesign engineers. Ask-EDA leverages LLM, hybrid retrieval augmented generation\n(RAG) and abbreviation de-hallucination (ADH) techniques to deliver more\nrelevant and accurate responses. We curated three evaluation datasets, namely\nq2a-100, cmds-100 and abbr-100. Each dataset is tailored to assess a distinct\naspect: general design question answering, design command handling and\nabbreviation resolution. We demonstrated that hybrid RAG offers over a 40%\nimprovement in Recall on the q2a-100 dataset and over a 60% improvement on the\ncmds-100 dataset compared to not using RAG, while ADH yields over a 70%\nenhancement in Recall on the abbr-100 dataset. The evaluation results show that\nAsk-EDA can effectively respond to design-related inquiries.", "published": "2024-06-03 19:40:28", "link": "http://arxiv.org/abs/2406.06575v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Life Cycle of Large Language Models: A Review of Biases in Education", "abstract": "Large Language Models (LLMs) are increasingly adopted in educational contexts\nto provide personalized support to students and teachers. The unprecedented\ncapacity of LLM-based applications to understand and generate natural language\ncan potentially improve instructional effectiveness and learning outcomes, but\nthe integration of LLMs in education technology has renewed concerns over\nalgorithmic bias which may exacerbate educational inequities. In this review,\nbuilding on prior work on mapping the traditional machine learning life cycle,\nwe provide a holistic map of the LLM life cycle from the initial development of\nLLMs to customizing pre-trained models for various applications in educational\nsettings. We explain each step in the LLM life cycle and identify potential\nsources of bias that may arise in the context of education. We discuss why\ncurrent measures of bias from traditional machine learning fail to transfer to\nLLM-generated content in education, such as tutoring conversations because the\ntext is high-dimensional, there can be multiple correct responses, and\ntailoring responses may be pedagogically desirable rather than unfair. This\nreview aims to clarify the complex nature of bias in LLM applications and\nprovide practical guidance for their evaluation to promote educational equity.", "published": "2024-06-03 18:00:28", "link": "http://arxiv.org/abs/2407.11203v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A Theory for Token-Level Harmonization in Retrieval-Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\nlarge language models (LLMs). Studies show that while RAG provides valuable\nexternal information (benefit), it may also mislead LLMs (detriment) with noisy\nor incorrect retrieved texts. Although many existing methods attempt to\npreserve benefit and avoid detriment, they lack a theoretical explanation for\nRAG. The benefit and detriment in the next token prediction of RAG remain a\nblack box that cannot be quantified or compared in an explainable manner, so\nexisting methods are data-driven, need additional utility evaluators or\npost-hoc. This paper takes the first step towards providing a theory to explain\nand trade off the benefit and detriment in RAG. First, we model RAG as the\nfusion between distribution of LLMs knowledge and distribution of retrieved\ntexts. Then, we formalize the trade-off between the value of external knowledge\n(benefit) and its potential risk of misleading LLMs (detriment) in next token\nprediction of RAG by distribution difference in this fusion. Finally, we prove\nthat the actual effect of RAG on the token, which is the comparison between\nbenefit and detriment, can be predicted without any training or accessing the\nutility of retrieval. Based on our theory, we propose a practical novel method,\nTok-RAG, which achieves collaborative generation between the pure LLM and RAG\nat token level to preserve benefit and avoid detriment. Experiments in\nreal-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\neffectiveness of our method and support our theoretical findings.", "published": "2024-06-03 02:56:14", "link": "http://arxiv.org/abs/2406.00944v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical\n  Transformer", "abstract": "While recent advancements in speech language models have achieved significant\nprogress, they face remarkable challenges in modeling the long acoustic\nsequences of neural audio codecs. In this paper, we introduce\n\\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer\n(GPST), a hierarchical transformer designed for efficient speech language\nmodeling. GPST quantizes audio waveforms into two distinct types of discrete\nspeech representations and integrates them within a hierarchical transformer\narchitecture, allowing for a unified one-stage generation process and enhancing\nHi-Res audio generation capabilities. By training on large corpora of speeches\nin an end-to-end unsupervised manner, GPST can generate syntactically\nconsistent speech with diverse speaker identities. Given a brief 3-second\nprompt, GPST can produce natural and coherent personalized speech,\ndemonstrating in-context learning abilities. Moreover, our approach can be\neasily extended to spoken cross-lingual speech generation by incorporating\nmulti-lingual semantic tokens and universal acoustic tokens. Experimental\nresults indicate that GPST significantly outperforms the existing speech\nlanguage models in terms of word error rate, speech quality, and speaker\nsimilarity. The code is available at \\url{https://github.com/youngsheen/GPST}.", "published": "2024-06-03 04:16:30", "link": "http://arxiv.org/abs/2406.00976v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Seeing the Forest through the Trees: Data Leakage from Partial\n  Transformer Gradients", "abstract": "Recent studies have shown that distributed machine learning is vulnerable to\ngradient inversion attacks, where private training data can be reconstructed by\nanalyzing the gradients of the models shared in training. Previous attacks\nestablished that such reconstructions are possible using gradients from all\nparameters in the entire models. However, we hypothesize that most of the\ninvolved modules, or even their sub-modules, are at risk of training data\nleakage, and we validate such vulnerabilities in various intermediate layers of\nlanguage models. Our extensive experiments reveal that gradients from a single\nTransformer layer, or even a single linear component with 0.54% parameters, are\nsusceptible to training data leakage. Additionally, we show that applying\ndifferential privacy on gradients during training offers limited protection\nagainst the novel vulnerability of data disclosure.", "published": "2024-06-03 05:15:04", "link": "http://arxiv.org/abs/2406.00999v2", "categories": ["cs.LG", "cs.CL", "cs.CR", "I.2.7; I.2.11"], "primary_category": "cs.LG"}
{"title": "SemCoder: Training Code Language Models with Comprehensive Semantics\n  Reasoning", "abstract": "Code Large Language Models (Code LLMs) have excelled at tasks like code\ncompletion but often miss deeper semantics such as execution effects and\ndynamic states. This paper aims to bridge the gap between Code LLMs' reliance\non static text data and the need for semantic understanding for complex tasks\nlike debugging and program repair. We introduce a novel strategy, monologue\nreasoning, to train Code LLMs to reason comprehensive semantics, encompassing\nhigh-level functional descriptions, local execution effects of individual\nstatements, and overall input/output behavior, thereby linking static code text\nwith dynamic execution states. We begin by collecting PyX, a clean Python\ncorpus of fully executable code samples with functional descriptions and test\ncases. We propose training Code LLMs not only to write code but also to\nunderstand code semantics by reasoning about key properties, constraints, and\nexecution behaviors using natural language, mimicking human verbal debugging,\ni.e., rubber-duck debugging. This approach led to the development of SemCoder,\na Code LLM with only 6.7B parameters, which shows competitive performance with\nGPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder\nachieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I\n(GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also\nstudy the effectiveness of SemCoder's monologue-style execution reasoning\ncompared to concrete scratchpad reasoning, showing that our approach integrates\nsemantics from multiple dimensions more smoothly. Finally, we demonstrate the\npotential of applying learned semantics to improve Code LLMs' debugging and\nself-refining capabilities. Our data, code, and models are available at:\nhttps://github.com/ARiSE-Lab/SemCoder.", "published": "2024-06-03 05:36:57", "link": "http://arxiv.org/abs/2406.01006v2", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Towards Scalable Automated Alignment of LLMs: A Survey", "abstract": "Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.", "published": "2024-06-03 12:10:26", "link": "http://arxiv.org/abs/2406.01252v3", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models\n  and Their Defenses", "abstract": "Recently, Anil et al. (2024) show that many-shot (up to hundreds of)\ndemonstrations can jailbreak state-of-the-art LLMs by exploiting their\nlong-context capability. Nevertheless, is it possible to use few-shot\ndemonstrations to efficiently jailbreak LLMs within limited context sizes?\nWhile the vanilla few-shot jailbreaking may be inefficient, we propose improved\ntechniques such as injecting special system tokens like [/INST] and employing\ndemo-level random search from a collected demo pool. These simple techniques\nresult in surprisingly effective jailbreaking against aligned LLMs (even with\nadvanced defenses). For examples, our method achieves >80% (mostly >95%) ASRs\non Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are\nenhanced by strong defenses such as perplexity detection and/or SmoothLLM,\nwhich is challenging for suffix-based jailbreaking. In addition, we conduct\ncomprehensive and elaborate (e.g., making sure to use correct system prompts)\nevaluations against other aligned LLMs and advanced defenses, where our method\nconsistently achieves nearly 100% ASRs. Our code is available at\nhttps://github.com/sail-sg/I-FSJ.", "published": "2024-06-03 12:59:17", "link": "http://arxiv.org/abs/2406.01288v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CodeR: Issue Resolving with Multi-Agent and Task Graphs", "abstract": "GitHub issue resolving recently has attracted significant attention from\nacademia and industry. SWE-bench is proposed to measure the performance in\nresolving issues. In this paper, we propose CodeR, which adopts a multi-agent\nframework and pre-defined task graphs to Repair & Resolve reported bugs and add\nnew features within code Repository. On SWE-bench lite, CodeR is able to solve\n28.33% of issues, when submitting only once for each issue. We examine the\nperformance impact of each design of CodeR and offer insights to advance this\nresearch direction.", "published": "2024-06-03 13:13:35", "link": "http://arxiv.org/abs/2406.01304v3", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of\n  LLM Safeguards", "abstract": "Input-output safeguards are used to detect anomalies in the traces produced\nby Large Language Models (LLMs) systems. These detectors are at the core of\ndiverse safety-critical applications such as real-time monitoring, offline\nevaluation of traces, and content moderation. However, there is no widely\nrecognized methodology to evaluate them. To fill this gap, we introduce the\nBenchmarks for the Evaluation of LLM Safeguards (BELLS), a structured\ncollection of tests, organized into three categories: (1) established failure\ntests, based on already-existing benchmarks for well-defined failure modes,\naiming to compare the performance of current input-output safeguards; (2)\nemerging failure tests, to measure generalization to never-seen-before failure\nmodes and encourage the development of more general safeguards; (3) next-gen\narchitecture tests, for more complex scaffolding (such as LLM-agents and\nmulti-agent systems), aiming to foster the development of safeguards that could\nadapt to future applications for which no safeguard currently exists.\nFurthermore, we implement and share the first next-gen architecture test, using\nthe MACHIAVELLI environment, along with an interactive visualization of the\ndataset.", "published": "2024-06-03 14:32:30", "link": "http://arxiv.org/abs/2406.01364v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Universal In-Context Approximation By Prompting Fully Recurrent Models", "abstract": "Zero-shot and in-context learning enable solving tasks without model\nfine-tuning, making them essential for developing generative model solutions.\nTherefore, it is crucial to understand whether a pretrained model can be\nprompted to approximate any function, i.e., whether it is a universal\nin-context approximator. While it was recently shown that transformer models do\npossess this property, these results rely on their attention mechanism. Hence,\nthese findings do not apply to fully recurrent architectures like RNNs, LSTMs,\nand the increasingly popular SSMs. We demonstrate that RNNs, LSTMs, GRUs,\nLinear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can\nalso serve as universal in-context approximators. To streamline our argument,\nwe introduce a programming language called LSRL that compiles to these fully\nrecurrent architectures. LSRL may be of independent interest for further\nstudies of fully recurrent models, such as constructing interpretability\nbenchmarks. We also study the role of multiplicative gating and observe that\narchitectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin) can\nimplement certain operations more stably, making them more viable candidates\nfor practical in-context universal approximation.", "published": "2024-06-03 15:25:13", "link": "http://arxiv.org/abs/2406.01424v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enabling ASR for Low-Resource Languages: A Comprehensive Dataset\n  Creation Approach", "abstract": "In recent years, automatic speech recognition (ASR) systems have\nsignificantly improved, especially in languages with a vast amount of\ntranscribed speech data. However, ASR systems tend to perform poorly for\nlow-resource languages with fewer resources, such as minority and regional\nlanguages. This study introduces a novel pipeline designed to generate ASR\ntraining datasets from audiobooks, which typically feature a single transcript\nassociated with hours-long audios. The common structure of these audiobooks\nposes a unique challenge due to the extensive length of audio segments, whereas\noptimal ASR training requires segments ranging from 4 to 15 seconds. To address\nthis, we propose a method for effectively aligning audio with its corresponding\ntext and segmenting it into lengths suitable for ASR training. Our approach\nsimplifies data preparation for ASR systems in low-resource languages and\ndemonstrates its application through a case study involving the Armenian\nlanguage. Our method, which is \"portable\" to many low-resource languages, not\nonly mitigates the issue of data scarcity but also enhances the performance of\nASR models for underrepresented languages.", "published": "2024-06-03 15:38:40", "link": "http://arxiv.org/abs/2406.01446v1", "categories": ["cs.CL", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Differentially Private Tabular Data Synthesis using Large Language\n  Models", "abstract": "Synthetic tabular data generation with differential privacy is a crucial\nproblem to enable data sharing with formal privacy. Despite a rich history of\nmethodological research and development, developing differentially private\ntabular data generators that can provide realistic synthetic datasets remains\nchallenging. This paper introduces DP-LLMTGen -- a novel framework for\ndifferentially private tabular data synthesis that leverages pretrained large\nlanguage models (LLMs). DP-LLMTGen models sensitive datasets using a two-stage\nfine-tuning procedure with a novel loss function specifically designed for\ntabular data. Subsequently, it generates synthetic data through sampling the\nfine-tuned LLMs. Our empirical evaluation demonstrates that DP-LLMTGen\noutperforms a variety of existing mechanisms across multiple datasets and\nprivacy settings. Additionally, we conduct an ablation study and several\nexperimental analyses to deepen our understanding of LLMs in addressing this\nimportant problem. Finally, we highlight the controllable generation ability of\nDP-LLMTGen through a fairness-constrained generation setting.", "published": "2024-06-03 15:43:57", "link": "http://arxiv.org/abs/2406.01457v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "The Importance of Online Data: Understanding Preference Fine-tuning via\n  Coverage", "abstract": "Learning from human preference data has emerged as the dominant paradigm for\nfine-tuning large language models (LLMs). The two most common families of\ntechniques -- online reinforcement learning (RL) such as Proximal Policy\nOptimization (PPO) and offline contrastive methods such as Direct Preference\nOptimization (DPO) -- were positioned as equivalent in prior work due to the\nfact that both have to start from the same offline preference dataset. To\nfurther expand our theoretical understanding of the similarities and\ndifferences between online and offline techniques for preference fine-tuning,\nwe conduct a rigorous analysis through the lens of dataset coverage, a concept\nthat captures how the training data covers the test distribution and is widely\nused in RL. We prove that a global coverage condition is both necessary and\nsufficient for offline contrastive methods to converge to the optimal policy,\nbut a weaker partial coverage condition suffices for online RL methods. This\nseparation provides one explanation of why online RL methods can perform better\nthan offline methods, especially when the offline preference data is not\ndiverse enough. Finally, motivated by our preceding theoretical observations,\nwe derive a hybrid preference optimization (HyPO) algorithm that uses offline\ndata for contrastive-based preference optimization and online data for KL\nregularization. Theoretically and empirically, we demonstrate that HyPO is more\nperformant than its pure offline counterpart DPO, while still preserving its\ncomputation and memory efficiency.", "published": "2024-06-03 15:51:04", "link": "http://arxiv.org/abs/2406.01462v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Understanding Token Probability Encoding in Output Embeddings", "abstract": "In this paper, we investigate the output token probability information in the\noutput embedding of language models. We find an approximate common log-linear\nencoding of output token probabilities within the output embedding vectors and\nempirically demonstrate that it is accurate and sparse. As a causality\nexamination, we steer the encoding in output embedding to modify the output\nprobability distribution accurately. Moreover, the sparsity we find in output\nprobability encoding suggests that a large number of dimensions in the output\nembedding do not contribute to causal language modeling. Therefore, we attempt\nto delete the output-unrelated dimensions and find more than 30% of the\ndimensions can be deleted without significant movement in output distribution\nand sequence generation. Additionally, in the pre-training dynamics of language\nmodels, we find that the output embeddings capture the corpus token frequency\ninformation in early steps, even before an obvious convergence of parameters\nstarts.", "published": "2024-06-03 15:57:29", "link": "http://arxiv.org/abs/2406.01468v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Geometry of Categorical and Hierarchical Concepts in Large Language\n  Models", "abstract": "The linear representation hypothesis is the informal idea that semantic\nconcepts are encoded as linear directions in the representation spaces of large\nlanguage models (LLMs). Previous work has shown how to make this notion precise\nfor representing binary concepts that have natural contrasts (e.g., {male,\nfemale}) as directions in representation space. However, many natural concepts\ndo not have natural contrasts (e.g., whether the output is about an animal). In\nthis work, we show how to extend the formalization of the linear representation\nhypothesis to represent features (e.g., is_animal) as vectors. This allows us\nto immediately formalize the representation of categorical concepts as\npolytopes in the representation space. Further, we use the formalization to\nprove a relationship between the hierarchical structure of concepts and the\ngeometry of their representations. We validate these theoretical results on the\nGemma and LLaMA-3 large language models, estimating representations for 900+\nhierarchically related concepts using data from WordNet.", "published": "2024-06-03 16:34:01", "link": "http://arxiv.org/abs/2406.01506v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Decoupled Alignment for Robust Plug-and-Play Adaptation", "abstract": "We introduce a low-resource safety enhancement method for aligning large\nlanguage models (LLMs) without the need for supervised fine-tuning (SFT) or\nreinforcement learning from human feedback (RLHF). Our main idea is to exploit\nknowledge distillation to extract the alignment information from existing\nwell-aligned LLMs and integrate it into unaligned LLMs in a plug-and-play\nfashion. Methodology, we employ delta debugging to identify the critical\ncomponents of knowledge necessary for effective distillation. On the harmful\nquestion dataset, our method significantly enhances the average defense success\nrate by approximately 14.41%, reaching as high as 51.39%, in 17 unaligned\npre-trained LLMs, without compromising performance.", "published": "2024-06-03 16:46:18", "link": "http://arxiv.org/abs/2406.01514v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Helix: Serving Large Language Models over Heterogeneous GPUs and Network\n  via Max-Flow", "abstract": "This paper introduces Helix, a distributed system for high-throughput,\nlow-latency large language model (LLM) serving in heterogeneous GPU clusters.\nThe key idea behind Helix is to formulate inference computation of LLMs over\nheterogeneous GPUs and network connections as a max-flow problem on directed,\nweighted graphs, whose nodes represent GPU instances and edges capture both GPU\nand network heterogeneity through their capacities. Helix then uses a mixed\ninteger linear programming (MILP) algorithm to discover highly optimized\nstrategies to serve LLMs on heterogeneous GPUs. This approach allows Helix to\njointly optimize model placement and request scheduling, two highly entangled\ntasks in heterogeneous LLM serving. Our evaluation on several heterogeneous\nclusters ranging from 24 to 42 GPU nodes shows that Helix improves serving\nthroughput by up to 3.3x and reduces prompting and decoding latency by up to\n66% and 24%, respectively, compared to existing approaches. Helix is available\nat https://github.com/Thesys-lab/Helix-ASPLOS25.", "published": "2024-06-03 17:47:53", "link": "http://arxiv.org/abs/2406.01566v2", "categories": ["cs.DC", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via\n  Cross-Modality Alignment", "abstract": "Multivariate time series forecasting (MTSF) aims to learn temporal dynamics\namong variables to forecast future time series. Existing statistical and deep\nlearning-based methods suffer from limited learnable parameters and small-scale\ntraining data. Recently, large language models (LLMs) combining time series\nwith textual prompts have achieved promising performance in MTSF. However, we\ndiscovered that current LLM-based solutions fall short in learning disentangled\nembeddings. We introduce TimeCMA, an intuitive yet effective framework for MTSF\nvia cross-modality alignment. Specifically, we present a dual-modality encoding\nwith two branches: the time series encoding branch extracts disentangled yet\nweak time series embeddings, and the LLM-empowered encoding branch wraps the\nsame time series with text as prompts to obtain entangled yet robust prompt\nembeddings. As a result, such a cross-modality alignment retrieves both\ndisentangled and robust time series embeddings, \"the best of two worlds\", from\nthe prompt embeddings based on time series and prompt modality similarities. As\nanother key design, to reduce the computational costs from time series with\ntheir length textual prompts, we design an effective prompt to encourage the\nmost essential temporal information to be encapsulated in the last token: only\nthe last token is passed to downstream prediction. We further store the last\ntoken embeddings to accelerate inference speed. Extensive experiments on eight\nreal datasets demonstrate that TimeCMA outperforms state-of-the-arts.", "published": "2024-06-03 00:27:29", "link": "http://arxiv.org/abs/2406.01638v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AI-based Classification of Customer Support Tickets: State of the Art\n  and Implementation with AutoML", "abstract": "Automation of support ticket classification is crucial to improve customer\nsupport performance and shortening resolution time for customer inquiries. This\nresearch aims to test the applicability of automated machine learning (AutoML)\nas a technology to train a machine learning model (ML model) that can classify\nsupport tickets. The model evaluation conducted in this research shows that\nAutoML can be used to train ML models with good classification performance.\nMoreover, this paper fills a research gap by providing new insights into\ndeveloping AI solutions without a dedicated professional by utilizing AutoML,\nwhich makes this technology more accessible for companies without specialized\nAI departments and staff.", "published": "2024-06-03 21:13:02", "link": "http://arxiv.org/abs/2406.01789v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC", "I.2; I.2.7; K.6"], "primary_category": "cs.LG"}
{"title": "SwitchLoRA: Switched Low-Rank Adaptation Can Learn Full-Rank Information", "abstract": "In the training of large language models, parameter-efficient techniques such\nas LoRA optimize memory usage and reduce communication overhead and memory\nusage during the fine-tuning phase. However, applying such techniques directly\nduring the pre-training phase results in poor performance, primarily because\nthe premature implementation of low-rank training significantly reduces model\naccuracy. Existing methods like ReLoRA and GaLore have attempted to address\nthis challenge by updating the low-rank subspace. However, they still fall\nshort of achieving the accuracy of full-rank training. Specifically, ReLoRA\nrestricts the frequency of updates to preserve optimizer states consistency,\nhindering its ability to closely approximate full-rank training behavior.\nMeanwhile, GaLore relies on Singular Value Decomposition (SVD) to approximate\nthe full-rank space, which introduces accuracy loss during the approximation\nprocess. In this paper, we introduce SwitchLoRA, a parameter-efficient training\ntechnique that frequently and smoothly replaces the trainable parameters of\nLoRA adapters with alternative parameters. SwitchLoRA updates the low-rank\nsubspace incrementally, targeting only a few dimensions at a time to minimize\nthe impact on optimizer states. This allows a higher update frequency, thereby\nenhancing accuracy by enabling the updated parameters to more closely mimic\nfull-rank behavior during the pre-training phase. Our results demonstrate that\nSwitchLoRA actually surpasses full-rank training, reducing perplexity from\n15.23 to 15.01 on the LLaMA 1.3B model, while also cutting communication\noverhead by 54\\% and memory usage by 13\\%. Furthermore, after full fine-tuning\nthe SwitchLoRA pre-trained model and the full-rank pre-trained model on the\nGLUE benchmark, the SwitchLoRA pre-trained model showed an average accuracy\ngain of about 1\\% over the full-rank pre-trained model.", "published": "2024-06-03 05:40:34", "link": "http://arxiv.org/abs/2406.06564v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures", "abstract": "Evaluating large language models (LLMs) is challenging. Traditional\nground-truth-based benchmarks fail to capture the comprehensiveness and nuance\nof real-world queries, while LLM-as-judge benchmarks suffer from grading biases\nand limited query quantity. Both of them may also become contaminated over\ntime. User-facing evaluation, such as Chatbot Arena, provides reliable signals\nbut is costly and slow. In this work, we propose MixEval, a new paradigm for\nestablishing efficient, gold-standard LLM evaluation by strategically mixing\noff-the-shelf benchmarks. It bridges (1) comprehensive and well-distributed\nreal-world user queries and (2) efficient and fairly-graded ground-truth-based\nbenchmarks, by matching queries mined from the web with similar queries from\nexisting benchmarks. Based on MixEval, we further build MixEval-Hard, which\noffers more room for model improvement. Our benchmarks' advantages lie in (1) a\n0.96 model ranking correlation with Chatbot Arena arising from the highly\nimpartial query distribution and grading mechanism, (2) fast, cheap, and\nreproducible execution (6% of the time and cost of MMLU), and (3) dynamic\nevaluation enabled by the rapid and stable data update pipeline. We provide\nextensive meta-evaluation and analysis for our and existing LLM benchmarks to\ndeepen the community's understanding of LLM evaluation and guide future\nresearch directions.", "published": "2024-06-03 05:47:05", "link": "http://arxiv.org/abs/2406.06565v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language Interaction with a Household Electricity\n  Knowledge-based Digital Twin", "abstract": "Domain specific digital twins, representing a digital replica of various\nsegments of the smart grid, are foreseen as able to model, simulate, and\ncontrol the respective segments. At the same time, knowledge-based digital\ntwins, coupled with AI, may also empower humans to understand aspects of the\nsystem through natural language interaction in view of planning and policy\nmaking. This paper is the first to assess and report on the potential of\nRetrieval Augmented Generation (RAG) question answers related to household\nelectrical energy measurement aspects leveraging a knowledge-based energy\ndigital twin. Relying on the recently published electricity consumption\nknowledge graph that actually represents a knowledge-based digital twin, we\nstudy the capabilities of ChatGPT, Gemini and Llama in answering electricity\nrelated questions. Furthermore, we compare the answers with the ones generated\nthrough a RAG techniques that leverages an existing electricity knowledge-based\ndigital twin. Our findings illustrate that the RAG approach not only reduces\nthe incidence of incorrect information typically generated by LLMs but also\nsignificantly improves the quality of the output by grounding responses in\nverifiable data. This paper details our methodology, presents a comparative\nanalysis of responses with and without RAG, and discusses the implications of\nour findings for future applications of AI in specialized sectors like energy\ndata analysis.", "published": "2024-06-03 07:44:32", "link": "http://arxiv.org/abs/2406.06566v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion", "abstract": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.", "published": "2024-06-03 13:28:43", "link": "http://arxiv.org/abs/2406.06567v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enhancing Clinical Documentation with Synthetic Data: Leveraging\n  Generative Models for Improved Accuracy", "abstract": "Accurate and comprehensive clinical documentation is crucial for delivering\nhigh-quality healthcare, facilitating effective communication among providers,\nand ensuring compliance with regulatory requirements. However, manual\ntranscription and data entry processes can be time-consuming, error-prone, and\nsusceptible to inconsistencies, leading to incomplete or inaccurate medical\nrecords. This paper proposes a novel approach to augment clinical documentation\nby leveraging synthetic data generation techniques to generate realistic and\ndiverse clinical transcripts. We present a methodology that combines\nstate-of-the-art generative models, such as Generative Adversarial Networks\n(GANs) and Variational Autoencoders (VAEs), with real-world clinical transcript\nand other forms of clinical data to generate synthetic transcripts. These\nsynthetic transcripts can then be used to supplement existing documentation\nworkflows, providing additional training data for natural language processing\nmodels and enabling more accurate and efficient transcription processes.\nThrough extensive experiments on a large dataset of anonymized clinical\ntranscripts, we demonstrate the effectiveness of our approach in generating\nhigh-quality synthetic transcripts that closely resemble real-world data.\nQuantitative evaluation metrics, including perplexity scores and BLEU scores,\nas well as qualitative assessments by domain experts, validate the fidelity and\nutility of the generated synthetic transcripts. Our findings highlight\nsynthetic data generation's potential to address clinical documentation\nchallenges, improving patient care, reducing administrative burdens, and\nenhancing healthcare system efficiency.", "published": "2024-06-03 15:49:03", "link": "http://arxiv.org/abs/2406.06569v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph Neural Network Enhanced Retrieval for Question Answering of LLMs", "abstract": "Retrieval augmented generation has revolutionized large language model (LLM)\noutputs by providing factual supports. Nevertheless, it struggles to capture\nall the necessary knowledge for complex reasoning questions. Existing retrieval\nmethods typically divide reference documents into passages, treating them in\nisolation. These passages, however, are often interrelated, such as passages\nthat are contiguous or share the same keywords. Therefore, it is crucial to\nrecognize such relatedness for enhancing the retrieval process. In this paper,\nwe propose a novel retrieval method, called GNN-Ret, which leverages graph\nneural networks (GNNs) to enhance retrieval by exploiting the relatedness\nbetween passages. Specifically, we first construct a graph of passages by\nconnecting passages that are structure-related or keyword-related. A graph\nneural network (GNN) is then leveraged to exploit the relationships between\npassages and improve the retrieval of supporting passages. Furthermore, we\nextend our method to handle multi-hop reasoning questions using a recurrent\ngraph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates\nthe graphs of passages from previous steps, thereby enhancing the retrieval of\nsupporting passages. Extensive experiments on benchmark datasets demonstrate\nthat GNN-Ret achieves higher accuracy for question answering with a single\nquery of LLMs than strong baselines that require multiple queries, and RGNN-Ret\nfurther improves accuracy and achieves state-of-the-art performance, with up to\n10.4% accuracy improvement on the 2WikiMQA dataset.", "published": "2024-06-03 17:07:46", "link": "http://arxiv.org/abs/2406.06572v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Harmful Suicide Content Detection", "abstract": "Harmful suicide content on the Internet is a significant risk factor inducing\nsuicidal thoughts and behaviors among vulnerable populations. Despite global\nefforts, existing resources are insufficient, specifically in high-risk regions\nlike the Republic of Korea. Current research mainly focuses on understanding\nnegative effects of such content or suicide risk in individuals, rather than on\nautomatically detecting the harmfulness of content. To fill this gap, we\nintroduce a harmful suicide content detection task for classifying online\nsuicide content into five harmfulness levels. We develop a multi-modal\nbenchmark and a task description document in collaboration with medical\nprofessionals, and leverage large language models (LLMs) to explore efficient\nmethods for moderating such content. Our contributions include proposing a\nnovel detection task, a multi-modal Korean benchmark with expert annotations,\nand suggesting strategies using LLMs to detect illegal and harmful content.\nOwing to the potential harm involved, we publicize our implementations and\nbenchmark, incorporating an ethical verification process.", "published": "2024-06-03 03:43:44", "link": "http://arxiv.org/abs/2407.13942v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of\n  Large Language Models", "abstract": "Large Language Models (LLMs) are constrained by outdated information and a\ntendency to generate incorrect data, commonly referred to as \"hallucinations.\"\nRetrieval-Augmented Generation (RAG) addresses these limitations by combining\nthe strengths of retrieval-based methods and generative models. This approach\ninvolves retrieving relevant information from a large, up-to-date dataset and\nusing it to enhance the generation process, leading to more accurate and\ncontextually appropriate responses. Despite its benefits, RAG introduces a new\nattack surface for LLMs, particularly because RAG databases are often sourced\nfrom public data, such as the web. In this paper, we propose \\TrojRAG{} to\nidentify the vulnerabilities and attacks on retrieval parts (RAG database) and\ntheir indirect attacks on generative parts (LLMs). Specifically, we identify\nthat poisoning several customized content passages could achieve a retrieval\nbackdoor, where the retrieval works well for clean queries but always returns\ncustomized poisoned adversarial queries. Triggers and poisoned passages can be\nhighly customized to implement various attacks. For example, a trigger could be\na semantic group like \"The Republican Party, Donald Trump, etc.\" Adversarial\npassages can be tailored to different contents, not only linked to the triggers\nbut also used to indirectly attack generative LLMs without modifying them.\nThese attacks can include denial-of-service attacks on RAG and semantic\nsteering attacks on LLM generations conditioned by the triggers. Our\nexperiments demonstrate that by just poisoning 10 adversarial passages can\ninduce 98.2\\% success rate to retrieve the adversarial passages. Then, these\npassages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\%\nor increase the rate of negative responses from 0.22\\% to 72\\% for targeted\nqueries.", "published": "2024-06-03 02:25:33", "link": "http://arxiv.org/abs/2406.00083v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Guided Score identity Distillation for Data-Free One-Step Text-to-Image\n  Generation", "abstract": "Diffusion-based text-to-image generation models trained on extensive\ntext-image pairs have demonstrated the ability to produce photorealistic images\naligned with textual descriptions. However, a significant limitation of these\nmodels is their slow sample generation process, which requires iterative\nrefinement through the same network. To overcome this, we introduce a data-free\nguided distillation method that enables the efficient distillation of\npretrained Stable Diffusion models without access to the real training data,\noften restricted due to legal, privacy, or cost concerns. This method enhances\nScore identity Distillation (SiD) with Long and Short Classifier-Free Guidance\n(LSG), an innovative strategy that applies Classifier-Free Guidance (CFG) not\nonly to the evaluation of the pretrained diffusion model but also to the\ntraining and evaluation of the fake score network. We optimize a model-based\nexplicit score matching loss using a score-identity-based approximation\nalongside our proposed guidance strategies for practical computation. By\nexclusively training with synthetic images generated by its one-step generator,\nour data-free distillation method rapidly improves FID and CLIP scores,\nachieving state-of-the-art FID performance while maintaining a competitive CLIP\nscore. Notably, the one-step distillation of Stable Diffusion 1.5 achieves an\nFID of 8.15 on the COCO-2014 validation set, a record low value under the\ndata-free setting. Our code and checkpoints are available at\nhttps://github.com/mingyuanzhou/SiD-LSG.", "published": "2024-06-03 17:44:11", "link": "http://arxiv.org/abs/2406.01561v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Text-guided Controllable Mesh Refinement for Interactive 3D Modeling", "abstract": "We propose a novel technique for adding geometric details to an input coarse\n3D mesh guided by a text prompt. Our method is composed of three stages. First,\nwe generate a single-view RGB image conditioned on the input coarse geometry\nand the input text prompt. This single-view image generation step allows the\nuser to pre-visualize the result and offers stronger conditioning for\nsubsequent multi-view generation. Second, we use our novel multi-view normal\ngeneration architecture to jointly generate six different views of the normal\nimages. The joint view generation reduces inconsistencies and leads to sharper\ndetails. Third, we optimize our mesh with respect to all views and generate a\nfine, detailed geometry as output. The resulting method produces an output\nwithin seconds and offers explicit user control over the coarse structure,\npose, and desired details of the resulting 3D mesh.", "published": "2024-06-03 17:59:43", "link": "http://arxiv.org/abs/2406.01592v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Accent Conversion in Text-To-Speech Using Multi-Level VAE and\n  Adversarial Training", "abstract": "With rapid globalization, the need to build inclusive and representative\nspeech technology cannot be overstated. Accent is an important aspect of speech\nthat needs to be taken into consideration while building inclusive speech\nsynthesizers. Inclusive speech technology aims to erase any biases towards\nspecific groups, such as people of certain accent. We note that\nstate-of-the-art Text-to-Speech (TTS) systems may currently not be suitable for\nall people, regardless of their background, as they are designed to generate\nhigh-quality voices without focusing on accent. In this paper, we propose a TTS\nmodel that utilizes a Multi-Level Variational Autoencoder with adversarial\nlearning to address accented speech synthesis and conversion in TTS, with a\nvision for more inclusive systems in the future. We evaluate the performance\nthrough both objective metrics and subjective listening tests. The results show\nan improvement in accent conversion ability compared to the baseline.", "published": "2024-06-03 05:56:02", "link": "http://arxiv.org/abs/2406.01018v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ControlSpeech: Towards Simultaneous Zero-shot Speaker Cloning and\n  Zero-shot Language Style Control With Decoupled Codec", "abstract": "In this paper, we present ControlSpeech, a text-to-speech (TTS) system\ncapable of fully cloning the speaker's voice and enabling arbitrary control and\nadjustment of speaking style, merely based on a few seconds of audio prompt and\na simple textual style description prompt. Prior zero-shot TTS models and\ncontrollable TTS models either could only mimic the speaker's voice without\nfurther control and adjustment capabilities or were unrelated to\nspeaker-specific voice generation. Therefore, ControlSpeech focuses on a more\nchallenging new task-a TTS system with controllable timbre, content, and style\nat the same time. ControlSpeech takes speech prompts, content prompts, and\nstyle prompts as inputs and utilizes bidirectional attention and mask-based\nparallel decoding to capture corresponding codec representations in a discrete\ndecoupling codec space. Moreover, we discovered the issue of text style\ncontrollability in a many-to-many mapping fashion and proposed the Style\nMixture Semantic Density (SMSD) model to resolve this problem. SMSD module\nwhich is based on Gaussian mixture density networks, is designed to enhance the\nfine-grained partitioning and sampling capabilities of style semantic\ninformation and generate speech with more diverse styles. In terms of\nexperiments, we make available a controllable model toolkit called\nControlToolkit with a new style controllable dataset, some replicated baseline\nmodels and propose new metrics to evaluate both the control capability and the\nquality of generated audio in ControlSpeech. The relevant ablation studies\nvalidate the necessity of each component in ControlSpeech is necessary. We hope\nthat ControlSpeech can establish the next foundation paradigm of controllable\nspeech synthesis. The relevant code and demo are available at\nhttps://github.com/jishengpeng/ControlSpeech .", "published": "2024-06-03 11:15:16", "link": "http://arxiv.org/abs/2406.01205v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TinySV: Speaker Verification in TinyML with On-device Learning", "abstract": "TinyML is a novel area of machine learning that gained huge momentum in the\nlast few years thanks to the ability to execute machine learning algorithms on\ntiny devices (such as Internet-of-Things or embedded systems). Interestingly,\nresearch in this area focused on the efficient execution of the inference phase\nof TinyML models on tiny devices, while very few solutions for on-device\nlearning of TinyML models are available in the literature due to the relevant\noverhead introduced by the learning algorithms.\n  The aim of this paper is to introduce a new type of adaptive TinyML solution\nthat can be used in tasks, such as the presented \\textit{Tiny Speaker\nVerification} (TinySV), that require to be tackled with an on-device learning\nalgorithm. Achieving this goal required (i) reducing the memory and\ncomputational demand of TinyML learning algorithms, and (ii) designing a TinyML\nlearning algorithm operating with few and possibly unlabelled training data.\nThe proposed TinySV solution relies on a two-layer hierarchical TinyML solution\ncomprising Keyword Spotting and Adaptive Speaker Verification module. We\nevaluated the effectiveness and efficiency of the proposed TinySV solution on a\ndataset collected expressly for the task and tested the proposed solution on a\nreal-world IoT device (Infineon PSoC 62S2 Wi-Fi BT Pioneer Kit).", "published": "2024-06-03 17:27:40", "link": "http://arxiv.org/abs/2406.01655v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PPINtonus: Early Detection of Parkinson's Disease Using Deep-Learning\n  Tonal Analysis", "abstract": "PPINtonus is a system for the early detection of Parkinson's Disease (PD)\nutilizing deep-learning tonal analysis, providing a cost-effective and\naccessible alternative to traditional neurological examinations. Partnering\nwith the Parkinson's Voice Project (PVP), PPINtonus employs a semi-supervised\nconditional generative adversarial network to generate synthetic data points,\nenhancing the training dataset for a multi-layered deep neural network.\nCombined with PRAAT phonetics software, this network accurately assesses\nbiomedical voice measurement values from a simple 120-second vocal test\nperformed with a standard microphone in typical household noise conditions. The\nmodel's performance was validated using a confusion matrix, achieving an\nimpressive 92.5 \\% accuracy with a low false negative rate. PPINtonus\ndemonstrated a precision of 92.7 \\%, making it a reliable tool for early PD\ndetection. The non-intrusive and efficient methodology of PPINtonus can\nsignificantly benefit developing countries by enabling early diagnosis and\nimproving the quality of life for millions of PD patients through timely\nintervention and management.", "published": "2024-06-03 01:07:42", "link": "http://arxiv.org/abs/2406.02608v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "animal2vec and MeerKAT: A self-supervised transformer for rare-event raw\n  audio input and a large-scale reference dataset for bioacoustics", "abstract": "Bioacoustic research, vital for understanding animal behavior, conservation,\nand ecology, faces a monumental challenge: analyzing vast datasets where animal\nvocalizations are rare. While deep learning techniques are becoming standard,\nadapting them to bioacoustics remains difficult. We address this with\nanimal2vec, an interpretable large transformer model, and a self-supervised\ntraining scheme tailored for sparse and unbalanced bioacoustic data. It learns\nfrom unlabeled audio and then refines its understanding with labeled data.\nFurthermore, we introduce and publicly release MeerKAT: Meerkat Kalahari Audio\nTranscripts, a dataset of meerkat (Suricata suricatta) vocalizations with\nmillisecond-resolution annotations, the largest labeled dataset on non-human\nterrestrial mammals currently available. Our model outperforms existing methods\non MeerKAT and the publicly available NIPS4Bplus birdsong dataset. Moreover,\nanimal2vec performs well even with limited labeled data (few-shot learning).\nanimal2vec and MeerKAT provide a new reference point for bioacoustic research,\nenabling scientists to analyze large amounts of data even with scarce ground\ntruth information.", "published": "2024-06-03 12:11:01", "link": "http://arxiv.org/abs/2406.01253v2", "categories": ["cs.SD", "cs.AI", "eess.AS", "q-bio.QM", "stat.AP"], "primary_category": "cs.SD"}
{"title": "Sequence-to-Sequence Multi-Modal Speech In-Painting", "abstract": "Speech in-painting is the task of regenerating missing audio contents using\nreliable context information. Despite various recent studies in multi-modal\nperception of audio in-painting, there is still a need for an effective\ninfusion of visual and auditory information in speech in-painting. In this\npaper, we introduce a novel sequence-to-sequence model that leverages the\nvisual information to in-paint audio signals via an encoder-decoder\narchitecture. The encoder plays the role of a lip-reader for facial recordings\nand the decoder takes both encoder outputs as well as the distorted audio\nspectrograms to restore the original speech. Our model outperforms an\naudio-only speech in-painting model and has comparable results with a recent\nmulti-modal speech in-painter in terms of speech quality and intelligibility\nmetrics for distortions of 300 ms to 1500 ms duration, which proves the\neffectiveness of the introduced multi-modality in speech in-painting.", "published": "2024-06-03 13:42:10", "link": "http://arxiv.org/abs/2406.01321v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
