{"title": "Generation of lyrics lines conditioned on music audio clips", "abstract": "We present a system for generating novel lyrics lines conditioned on music\naudio. A bimodal neural network model learns to generate lines conditioned on\nany given short audio clip. The model consists of a spectrogram variational\nautoencoder (VAE) and a text VAE. Both automatic and human evaluations\ndemonstrate effectiveness of our model in generating lines that have an\nemotional impact matching a given audio clip. The system is intended to serve\nas a creativity tool for songwriters.", "published": "2020-09-30 01:22:58", "link": "http://arxiv.org/abs/2009.14375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development of Word Embeddings for Uzbek Language", "abstract": "In this paper, we share the process of developing word embeddings for the\nCyrillic variant of the Uzbek language. The result of our work is the first\npublicly available set of word vectors trained on the word2vec, GloVe, and\nfastText algorithms using a high-quality web crawl corpus developed in-house.\nThe developed word embeddings can be used in many natural language processing\ndownstream tasks.", "published": "2020-09-30 01:52:00", "link": "http://arxiv.org/abs/2009.14384v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple Word Embeddings for Increased Diversity of Representation", "abstract": "Most state-of-the-art models in natural language processing (NLP) are neural\nmodels built on top of large, pre-trained, contextual language models that\ngenerate representations of words in context and are fine-tuned for the task at\nhand. The improvements afforded by these \"contextual embeddings\" come with a\nhigh computational cost. In this work, we explore a simple technique that\nsubstantially and consistently improves performance over a strong baseline with\nnegligible increase in run time. We concatenate multiple pre-trained embeddings\nto strengthen our representation of words. We show that this concatenation\ntechnique works across many tasks, datasets, and model types. We analyze\naspects of pre-trained embedding similarity and vocabulary coverage and find\nthat the representational diversity between different pre-trained embeddings is\nthe driving force of why this technique works. We provide open source\nimplementations of our models in both TensorFlow and PyTorch.", "published": "2020-09-30 02:33:09", "link": "http://arxiv.org/abs/2009.14394v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Automatic Post-Editing Improve NMT?", "abstract": "Automatic post-editing (APE) aims to improve machine translations, thereby\nreducing human post-editing effort. APE has had notable success when used with\nstatistical machine translation (SMT) systems but has not been as successful\nover neural machine translation (NMT) systems. This has raised questions on the\nrelevance of APE task in the current scenario. However, the training of APE\nmodels has been heavily reliant on large-scale artificial corpora combined with\nonly limited human post-edited data. We hypothesize that APE models have been\nunderperforming in improving NMT translations due to the lack of adequate\nsupervision. To ascertain our hypothesis, we compile a larger corpus of human\npost-edits of English to German NMT. We empirically show that a state-of-art\nneural APE model trained on this corpus can significantly improve a strong\nin-domain NMT system, challenging the current understanding in the field. We\nfurther investigate the effects of varying training data sizes, using\nartificial training data, and domain specificity for the APE task. We release\nthis new corpus under CC BY-NC-SA 4.0 license at\nhttps://github.com/shamilcm/pedra.", "published": "2020-09-30 02:34:19", "link": "http://arxiv.org/abs/2009.14395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural RST-based Evaluation of Discourse Coherence", "abstract": "This paper evaluates the utility of Rhetorical Structure Theory (RST) trees\nand relations in discourse coherence evaluation. We show that incorporating\nsilver-standard RST features can increase accuracy when classifying coherence.\nWe demonstrate this through our tree-recursive neural model, namely\nRST-Recursive, which takes advantage of the text's RST features produced by a\nstate of the art RST parser. We evaluate our approach on the Grammarly Corpus\nfor Discourse Coherence (GCDC) and show that when ensembled with the current\nstate of the art, we can achieve the new state of the art accuracy on this\nbenchmark. Furthermore, when deployed alone, RST-Recursive achieves competitive\naccuracy while having 62% fewer parameters.", "published": "2020-09-30 06:00:37", "link": "http://arxiv.org/abs/2009.14463v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Hard Retrieval Decoder Attention for Transformers", "abstract": "The Transformer translation model is based on the multi-head attention\nmechanism, which can be parallelized easily. The multi-head attention network\nperforms the scaled dot-product attention function in parallel, empowering the\nmodel by jointly attending to information from different representation\nsubspaces at different positions. In this paper, we present an approach to\nlearning a hard retrieval attention where an attention head only attends to one\ntoken in the sentence rather than all tokens. The matrix multiplication between\nattention probabilities and the value sequence in the standard scaled\ndot-product attention can thus be replaced by a simple and efficient retrieval\noperation. We show that our hard retrieval attention mechanism is 1.43 times\nfaster in decoding, while preserving translation quality on a wide range of\nmachine translation tasks when used in the decoder self- and cross-attention\nnetworks.", "published": "2020-09-30 13:18:57", "link": "http://arxiv.org/abs/2009.14658v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Vietnamese Dataset for Evaluating Machine Reading Comprehension", "abstract": "Over 97 million people speak Vietnamese as their native language in the\nworld. However, there are few research studies on machine reading comprehension\n(MRC) for Vietnamese, the task of understanding a text and answering questions\nrelated to it. Due to the lack of benchmark datasets for Vietnamese, we present\nthe Vietnamese Question Answering Dataset (UIT-ViQuAD), a new dataset for the\nlow-resource language as Vietnamese to evaluate MRC models. This dataset\ncomprises over 23,000 human-generated question-answer pairs based on 5,109\npassages of 174 Vietnamese articles from Wikipedia. In particular, we propose a\nnew process of dataset creation for Vietnamese MRC. Our in-depth analyses\nillustrate that our dataset requires abilities beyond simple reasoning like\nword matching and demands single-sentence and multiple-sentence inferences.\nBesides, we conduct experiments on state-of-the-art MRC methods for English and\nChinese as the first experimental models on UIT-ViQuAD. We also estimate human\nperformance on the dataset and compare it to the experimental results of\npowerful machine learning models. As a result, the substantial differences\nbetween human performance and the best model performance on the dataset\nindicate that improvements can be made on UIT-ViQuAD in future research. Our\ndataset is freely available on our website to encourage the research community\nto overcome challenges in Vietnamese MRC.", "published": "2020-09-30 15:06:56", "link": "http://arxiv.org/abs/2009.14725v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging Information-Seeking Human Gaze and Machine Reading\n  Comprehension", "abstract": "In this work, we analyze how human gaze during reading comprehension is\nconditioned on the given reading comprehension question, and whether this\nsignal can be beneficial for machine reading comprehension. To this end, we\ncollect a new eye-tracking dataset with a large number of participants engaging\nin a multiple choice reading comprehension task. Our analysis of this data\nreveals increased fixation times over parts of the text that are most relevant\nfor answering the question. Motivated by this finding, we propose making\nautomated reading comprehension more human-like by mimicking human\ninformation-seeking reading behavior during reading comprehension. We\ndemonstrate that this approach leads to performance gains on multiple choice\nquestion answering in English for a state-of-the-art reading comprehension\nmodel.", "published": "2020-09-30 16:34:27", "link": "http://arxiv.org/abs/2009.14780v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT for Monolingual and Cross-Lingual Reverse Dictionary", "abstract": "Reverse dictionary is the task to find the proper target word given the word\ndescription. In this paper, we tried to incorporate BERT into this task.\nHowever, since BERT is based on the byte-pair-encoding (BPE) subword encoding,\nit is nontrivial to make BERT generate a word given the description. We propose\na simple but effective method to make BERT generate the target word for this\nspecific task. Besides, the cross-lingual reverse dictionary is the task to\nfind the proper target word described in another language. Previous models have\nto keep two different word embeddings and learn to align these embeddings.\nNevertheless, by using the Multilingual BERT (mBERT), we can efficiently\nconduct the cross-lingual reverse dictionary with one subword embedding, and\nthe alignment between languages is not necessary. More importantly, mBERT can\nachieve remarkable cross-lingual reverse dictionary performance even without\nthe parallel corpus, which means it can conduct the cross-lingual reverse\ndictionary with only corresponding monolingual data. Code is publicly available\nat https://github.com/yhcc/BertForRD.git.", "published": "2020-09-30 17:00:10", "link": "http://arxiv.org/abs/2009.14790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Tale of Two Linkings: Dynamically Gating between Schema Linking and\n  Structural Linking for Text-to-SQL Parsing", "abstract": "In Text-to-SQL semantic parsing, selecting the correct entities (tables and\ncolumns) for the generated SQL query is both crucial and challenging; the\nparser is required to connect the natural language (NL) question and the SQL\nquery to the structured knowledge in the database. We formulate two linking\nprocesses to address this challenge: schema linking which links explicit NL\nmentions to the database and structural linking which links the entities in the\noutput SQL with their structural relationships in the database schema.\nIntuitively, the effectiveness of these two linking processes changes based on\nthe entity being generated, thus we propose to dynamically choose between them\nusing a gating mechanism. Integrating the proposed method with two graph neural\nnetwork-based semantic parsers together with BERT representations demonstrates\nsubstantial gains in parsing accuracy on the challenging Spider dataset.\nAnalyses show that our proposed method helps to enhance the structure of the\nmodel output when generating complicated SQL queries and offers more\nexplainable predictions.", "published": "2020-09-30 17:32:27", "link": "http://arxiv.org/abs/2009.14809v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Romanization for Model Transfer Between Scripts in Neural Machine\n  Translation", "abstract": "Transfer learning is a popular strategy to improve the quality of\nlow-resource machine translation. For an optimal transfer of the embedding\nlayer, the child and parent model should share a substantial part of the\nvocabulary. This is not the case when transferring to languages with a\ndifferent script. We explore the benefit of romanization in this scenario. Our\nresults show that romanization entails information loss and is thus not always\nsuperior to simpler vocabulary transfer methods, but can improve the transfer\nbetween related languages with different scripts. We compare two romanization\ntools and find that they exhibit different degrees of information loss, which\naffects translation quality. Finally, we extend romanization to the target\nside, showing that this can be a successful strategy when coupled with a simple\nderomanization model.", "published": "2020-09-30 17:54:56", "link": "http://arxiv.org/abs/2009.14824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning from Mistakes: Combining Ontologies via Self-Training for\n  Dialogue Generation", "abstract": "Natural language generators (NLGs) for task-oriented dialogue typically take\na meaning representation (MR) as input. They are trained end-to-end with a\ncorpus of MR/utterance pairs, where the MRs cover a specific set of dialogue\nacts and domain attributes. Creation of such datasets is labor-intensive and\ntime-consuming. Therefore, dialogue systems for new domain ontologies would\nbenefit from using data for pre-existing ontologies. Here we explore, for the\nfirst time, whether it is possible to train an NLG for a new larger ontology\nusing existing training sets for the restaurant domain, where each set is based\non a different ontology. We create a new, larger combined ontology, and then\ntrain an NLG to produce utterances covering it. For example, if one dataset has\nattributes for family-friendly and rating information, and the other has\nattributes for decor and service, our aim is an NLG for the combined ontology\nthat can produce utterances that realize values for family-friendly, rating,\ndecor and service. Initial experiments with a baseline neural\nsequence-to-sequence model show that this task is surprisingly challenging. We\nthen develop a novel self-training method that identifies (errorful) model\noutputs, automatically constructs a corrected MR input to form a new (MR,\nutterance) training pair, and then repeatedly adds these new instances back\ninto the training data. We then test the resulting model on a new test set. The\nresult is a self-trained model whose performance is an absolute 75.4%\nimprovement over the baseline model. We also report a human qualitative\nevaluation of the final model showing that it achieves high naturalness,\nsemantic coherence and grammaticality", "published": "2020-09-30 23:54:38", "link": "http://arxiv.org/abs/2010.00150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ethically Collecting Multi-Modal Spontaneous Conversations with People\n  that have Cognitive Impairments", "abstract": "In order to make spoken dialogue systems (such as Amazon Alexa or Google\nAssistant) more accessible and naturally interactive for people with cognitive\nimpairments, appropriate data must be obtainable. Recordings of multi-modal\nspontaneous conversations with vulnerable user groups are scarce however and\nthis valuable data is challenging to collect. Researchers that call for this\ndata are commonly inexperienced in ethical and legal issues around working with\nvulnerable participants. Additionally, standard recording equipment is insecure\nand should not be used to capture sensitive data. We spent a year consulting\nexperts on how to ethically capture and share recordings of multi-modal\nspontaneous conversations with vulnerable user groups. In this paper we provide\nguidance, collated from these experts, on how to ethically collect such data\nand we present a new system - \"CUSCO\" - to capture, transport and exchange\nsensitive data securely. This framework is intended to be easily followed and\nimplemented to encourage further publications of similar corpora. Using this\nguide and secure recording system, researchers can review and refine their\nethical measures.", "published": "2020-09-30 00:57:33", "link": "http://arxiv.org/abs/2009.14361v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Towards Improved Model Design for Authorship Identification: A Survey on\n  Writing Style Understanding", "abstract": "Authorship identification tasks, which rely heavily on linguistic styles,\nhave always been an important part of Natural Language Understanding (NLU)\nresearch. While other tasks based on linguistic style understanding benefit\nfrom deep learning methods, these methods have not behaved as well as\ntraditional machine learning methods in many authorship-based tasks. With these\ntasks becoming more and more challenging, however, traditional machine learning\nmethods based on handcrafted feature sets are already approaching their\nperformance limits. Thus, in order to inspire future applications of deep\nlearning methods in authorship-based tasks in ways that benefit the extraction\nof stylistic features, we survey authorship-based tasks and other tasks related\nto writing style understanding. We first describe our survey results on the\ncurrent state of research in both sets of tasks and summarize existing\nachievements and problems in authorship-related tasks. We then describe\noutstanding methods in style-related tasks in general and analyze how they are\nused in combination in the top-performing models. We are optimistic about the\napplicability of these models to authorship-based tasks and hope our survey\nwill help advance research in this field.", "published": "2020-09-30 05:17:42", "link": "http://arxiv.org/abs/2009.14445v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LEBANONUPRISING: a thorough study of Lebanese tweets", "abstract": "Recent studies showed a huge interest in social networks sentiment analysis.\nTwitter, which is a microblogging service, can be a great source of information\non how the users feel about a certain topic, or what their opinion is regarding\na social, economic and even political matter. On October 17, Lebanon witnessed\nthe start of a revolution; the LebanonUprising hashtag became viral on Twitter.\nA dataset consisting of a 100,0000 tweets was collected between 18 and 21\nOctober. In this paper, we conducted a sentiment analysis study for the tweets\nin spoken Lebanese Arabic related to the LebanonUprising hashtag using\ndifferent machine learning algorithms. The dataset was manually annotated to\nmeasure the precision and recall metrics and to compare between the different\nalgorithms. Furthermore, the work completed in this paper provides two more\ncontributions. The first is related to building a Lebanese to Modern Standard\nArabic mapping dictionary that was used for the preprocessing of the tweets and\nthe second is an attempt to move from sentiment analysis to emotion detection\nusing emojis, and the two emotions we tried to predict were the \"sarcastic\" and\n\"funny\" emotions. We built a training set from the tweets collected in October\n2019 and then we used this set to predict sentiments and emotions of the tweets\nwe collected between May and August 2020. The analysis we conducted shows the\nvariation in sentiments, emotions and users between the two datasets. The\nresults we obtained seem satisfactory especially considering that there was no\nprevious or similar work done involving Lebanese Arabic tweets, to our\nknowledge.", "published": "2020-09-30 05:50:08", "link": "http://arxiv.org/abs/2009.14459v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TaxiNLI: Taking a Ride up the NLU Hill", "abstract": "Pre-trained Transformer-based neural architectures have consistently achieved\nstate-of-the-art performance in the Natural Language Inference (NLI) task.\nSince NLI examples encompass a variety of linguistic, logical, and reasoning\nphenomena, it remains unclear as to which specific concepts are learnt by the\ntrained systems and where they can achieve strong generalization. To\ninvestigate this question, we propose a taxonomic hierarchy of categories that\nare relevant for the NLI task. We introduce TAXINLI, a new dataset, that has\n10k examples from the MNLI dataset (Williams et al., 2018) with these taxonomic\nlabels. Through various experiments on TAXINLI, we observe that whereas for\ncertain taxonomic categories SOTA neural models have achieved near perfect\naccuracies - a large jump over the previous models - some categories still\nremain difficult. Our work adds to the growing body of literature that shows\nthe gaps in the current NLI systems and datasets through a systematic\npresentation and analysis of reasoning categories.", "published": "2020-09-30 08:45:25", "link": "http://arxiv.org/abs/2009.14505v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Cross-lingual Spoken Language Understanding with Regularized\n  Representation Alignment", "abstract": "Despite the promising results of current cross-lingual models for spoken\nlanguage understanding systems, they still suffer from imperfect cross-lingual\nrepresentation alignments between the source and target languages, which makes\nthe performance sub-optimal. To cope with this issue, we propose a\nregularization approach to further align word-level and sentence-level\nrepresentations across languages without any external resource. First, we\nregularize the representation of user utterances based on their corresponding\nlabels. Second, we regularize the latent variable model (Liu et al., 2019) by\nleveraging adversarial training to disentangle the latent variables.\nExperiments on the cross-lingual spoken language understanding task show that\nour model outperforms current state-of-the-art methods in both few-shot and\nzero-shot scenarios, and our model, trained on a few-shot setting with only 3\\%\nof the target language training data, achieves comparable performance to the\nsupervised training with all the training data.", "published": "2020-09-30 08:56:53", "link": "http://arxiv.org/abs/2009.14510v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Case-Based Abductive Natural Language Inference", "abstract": "Most of the contemporary approaches for multi-hop Natural Language Inference\n(NLI) construct explanations considering each test case in isolation. However,\nthis paradigm is known to suffer from semantic drift, a phenomenon that causes\nthe construction of spurious explanations leading to wrong conclusions. In\ncontrast, this paper proposes an abductive framework for multi-hop NLI\nexploring the retrieve-reuse-refine paradigm in Case-Based Reasoning (CBR).\nSpecifically, we present Case-Based Abductive Natural Language Inference\n(CB-ANLI), a model that addresses unseen inference problems by analogical\ntransfer of prior explanations from similar examples. We empirically evaluate\nthe abductive framework on commonsense and scientific question answering tasks,\ndemonstrating that CB-ANLI can be effectively integrated with sparse and dense\npre-trained encoders to improve multi-hop inference, or adopted as an evidence\nretriever for Transformers. Moreover, an empirical analysis of semantic drift\nreveals that the CBR paradigm boosts the quality of the most challenging\nexplanations, a feature that has a direct impact on robustness and accuracy in\ndownstream inference tasks.", "published": "2020-09-30 09:50:39", "link": "http://arxiv.org/abs/2009.14539v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Dilated Convolutional Attention Network for Medical Code Assignment from\n  Clinical Text", "abstract": "Medical code assignment, which predicts medical codes from clinical texts, is\na fundamental task of intelligent medical information systems. The emergence of\ndeep models in natural language processing has boosted the development of\nautomatic assignment methods. However, recent advanced neural architectures\nwith flat convolutions or multi-channel feature concatenation ignore the\nsequential causal constraint within a text sequence and may not learn\nmeaningful clinical text representations, especially for lengthy clinical notes\nwith long-term sequential dependency. This paper proposes a Dilated\nConvolutional Attention Network (DCAN), integrating dilated convolutions,\nresidual connections, and label attention, for medical code assignment. It\nadopts dilated convolutions to capture complex medical patterns with a\nreceptive field which increases exponentially with dilation size. Experiments\non a real-world clinical dataset empirically show that our model improves the\nstate of the art.", "published": "2020-09-30 11:55:58", "link": "http://arxiv.org/abs/2009.14578v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "RDSGAN: Rank-based Distant Supervision Relation Extraction with\n  Generative Adversarial Framework", "abstract": "Distant supervision has been widely used for relation extraction but suffers\nfrom noise labeling problem. Neural network models are proposed to denoise with\nattention mechanism but cannot eliminate noisy data due to its non-zero\nweights. Hard decision is proposed to remove wrongly-labeled instances from the\npositive set though causes loss of useful information contained in removed\ninstances. In this paper, we propose a novel generative neural framework named\nRDSGAN (Rank-based Distant Supervision GAN) which automatically generates valid\ninstances for distant supervision relation extraction. Our framework combines\nsoft attention and hard decision to learn the distribution of true positive\ninstances via adversarial training and selects valid instances conforming to\nthe distribution via rank-based distant supervision, which addresses the false\npositive problem. Experimental results show the superiority of our framework\nover strong baselines.", "published": "2020-09-30 14:59:09", "link": "http://arxiv.org/abs/2009.14722v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Point-of-Interest Type Inference from Social Media Text", "abstract": "Physical places help shape how we perceive the experiences we have there. For\nthe first time, we study the relationship between social media text and the\ntype of the place from where it was posted, whether a park, restaurant, or\nsomeplace else. To facilitate this, we introduce a novel data set of\n$\\sim$200,000 English tweets published from 2,761 different points-of-interest\nin the U.S., enriched with place type information. We train classifiers to\npredict the type of the location a tweet was sent from that reach a macro F1 of\n43.67 across eight classes and uncover the linguistic markers associated with\neach type of place. The ability to predict semantic place information from a\ntweet has applications in recommendation systems, personalization services and\ncultural geography.", "published": "2020-09-30 15:21:19", "link": "http://arxiv.org/abs/2009.14734v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "AbuseAnalyzer: Abuse Detection, Severity and Target Prediction for Gab\n  Posts", "abstract": "While extensive popularity of online social media platforms has made\ninformation dissemination faster, it has also resulted in widespread online\nabuse of different types like hate speech, offensive language, sexist and\nracist opinions, etc. Detection and curtailment of such abusive content is\ncritical for avoiding its psychological impact on victim communities, and\nthereby preventing hate crimes. Previous works have focused on classifying user\nposts into various forms of abusive behavior. But there has hardly been any\nfocus on estimating the severity of abuse and the target. In this paper, we\npresent a first of the kind dataset with 7601 posts from Gab which looks at\nonline abuse from the perspective of presence of abuse, severity and target of\nabusive behavior. We also propose a system to address these tasks, obtaining an\naccuracy of ~80% for abuse presence, ~82% for abuse target prediction, and ~65%\nfor abuse severity prediction.", "published": "2020-09-30 18:12:50", "link": "http://arxiv.org/abs/2010.00038v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multi-document Summarization with Maximal Marginal Relevance-guided\n  Reinforcement Learning", "abstract": "While neural sequence learning methods have made significant progress in\nsingle-document summarization (SDS), they produce unsatisfactory results on\nmulti-document summarization (MDS). We observe two major challenges when\nadapting SDS advances to MDS: (1) MDS involves larger search space and yet more\nlimited training data, setting obstacles for neural methods to learn adequate\nrepresentations; (2) MDS needs to resolve higher information redundancy among\nthe source documents, which SDS methods are less effective to handle. To close\nthe gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement\nLearning for MDS, which unifies advanced neural SDS methods and statistical\nmeasures used in classical MDS. RL-MMR casts MMR guidance on fewer promising\ncandidates, which restrains the search space and thus leads to better\nrepresentation learning. Additionally, the explicit redundancy measure in MMR\nhelps the neural representation of the summary to better capture redundancy.\nExtensive experiments demonstrate that RL-MMR achieves state-of-the-art\nperformance on benchmark MDS datasets. In particular, we show the benefits of\nincorporating MMR into end-to-end learning when adapting SDS to MDS in terms of\nboth learning effectiveness and efficiency.", "published": "2020-09-30 21:50:46", "link": "http://arxiv.org/abs/2010.00117v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interactive Re-Fitting as a Technique for Improving Word Embeddings", "abstract": "Word embeddings are a fixed, distributional representation of the context of\nwords in a corpus learned from word co-occurrences. While word embeddings have\nproven to have many practical uses in natural language processing tasks, they\nreflect the attributes of the corpus upon which they are trained. Recent work\nhas demonstrated that post-processing of word embeddings to apply information\nfound in lexical dictionaries can improve their quality. We build on this\npost-processing technique by making it interactive. Our approach makes it\npossible for humans to adjust portions of a word embedding space by moving sets\nof words closer to one another. One motivating use case for this capability is\nto enable users to identify and reduce the presence of bias in word embeddings.\nOur approach allows users to trigger selective post-processing as they interact\nwith and assess potential bias in word embeddings.", "published": "2020-09-30 21:54:22", "link": "http://arxiv.org/abs/2010.00121v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked\n  Language Models", "abstract": "Pretrained language models, especially masked language models (MLMs) have\nseen success across many NLP tasks. However, there is ample evidence that they\nuse the cultural biases that are undoubtedly present in the corpora they are\ntrained on, implicitly creating harm with biased representations. To measure\nsome forms of social bias in language models against protected demographic\ngroups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark\n(CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing\nwith nine types of bias, like race, religion, and age. In CrowS-Pairs a model\nis presented with two sentences: one that is more stereotyping and another that\nis less stereotyping. The data focuses on stereotypes about historically\ndisadvantaged groups and contrasts them with advantaged groups. We find that\nall three of the widely-used MLMs we evaluate substantially favor sentences\nthat express stereotypes in every category in CrowS-Pairs. As work on building\nless biased models advances, this dataset can be used as a benchmark to\nevaluate progress.", "published": "2020-09-30 22:38:40", "link": "http://arxiv.org/abs/2010.00133v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End-to-End Spoken Language Understanding Without Full Transcripts", "abstract": "An essential component of spoken language understanding (SLU) is slot\nfilling: representing the meaning of a spoken utterance using semantic entity\nlabels. In this paper, we develop end-to-end (E2E) spoken language\nunderstanding systems that directly convert speech input to semantic entities\nand investigate if these E2E SLU models can be trained solely on semantic\nentity annotations without word-for-word transcripts. Training such models is\nvery useful as they can drastically reduce the cost of data collection. We\ncreated two types of such speech-to-entities models, a CTC model and an\nattention-based encoder-decoder model, by adapting models trained originally\nfor speech recognition. Given that our experiments involve speech input, these\nsystems need to recognize both the entity label and words representing the\nentity value correctly. For our speech-to-entities experiments on the ATIS\ncorpus, both the CTC and attention models showed impressive ability to skip\nnon-entity words: there was little degradation when trained on just entities\nversus full transcripts. We also explored the scenario where the entities are\nin an order not necessarily related to spoken order in the utterance. With its\nability to do re-ordering, the attention model did remarkably well, achieving\nonly about 2% degradation in speech-to-bag-of-entities F1 score.", "published": "2020-09-30 01:54:13", "link": "http://arxiv.org/abs/2009.14386v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards a Multi-modal, Multi-task Learning based Pre-training Framework\n  for Document Representation Learning", "abstract": "Recent approaches in literature have exploited the multi-modal information in\ndocuments (text, layout, image) to serve specific downstream document tasks.\nHowever, they are limited by their - (i) inability to learn cross-modal\nrepresentations across text, layout and image dimensions for documents and (ii)\ninability to process multi-page documents. Pre-training techniques have been\nshown in Natural Language Processing (NLP) domain to learn generic textual\nrepresentations from large unlabelled datasets, applicable to various\ndownstream NLP tasks. In this paper, we propose a multi-task learning-based\nframework that utilizes a combination of self-supervised and supervised\npre-training tasks to learn a generic document representation applicable to\nvarious downstream document tasks. Specifically, we introduce Document Topic\nModelling and Document Shuffle Prediction as novel pre-training tasks to learn\nrich image representations along with the text and layout representations for\ndocuments. We utilize the Longformer network architecture as the backbone to\nencode the multi-modal information from multi-page documents in an end-to-end\nfashion. We showcase the applicability of our pre-training framework on a\nvariety of different real-world document tasks such as document classification,\ndocument information extraction, and document retrieval. We evaluate our\nframework on different standard document datasets and conduct exhaustive\nexperiments to compare performance against various ablations of our framework\nand state-of-the-art baselines.", "published": "2020-09-30 05:39:04", "link": "http://arxiv.org/abs/2009.14457v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Systematic Generalization in Neural Proof Generation with\n  Transformers", "abstract": "We are interested in understanding how well Transformer language models\n(TLMs) can perform reasoning tasks when trained on knowledge encoded in the\nform of natural language. We investigate their systematic generalization\nabilities on a logical reasoning task in natural language, which involves\nreasoning over relationships between entities grounded in first-order logical\nproofs. Specifically, we perform soft theorem-proving by leveraging TLMs to\ngenerate natural language proofs. We test the generated proofs for logical\nconsistency, along with the accuracy of the final inference. We observe\nlength-generalization issues when evaluated on longer-than-trained sequences.\nHowever, we observe TLMs improve their generalization performance after being\nexposed to longer, exhaustive proofs. In addition, we discover that TLMs are\nable to generalize better using backward-chaining proofs compared to their\nforward-chaining counterparts, while they find it easier to generate forward\nchaining proofs. We observe that models that are not trained to generate proofs\nare better at generalizing to problems based on longer proofs. This suggests\nthat Transformers have efficient internal reasoning strategies that are harder\nto interpret. These results highlight the systematic generalization behavior of\nTLMs in the context of logical reasoning, and we believe this work motivates\ndeeper inspection of their underlying reasoning strategies.", "published": "2020-09-30 16:54:37", "link": "http://arxiv.org/abs/2009.14786v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Rethinking Attention with Performers", "abstract": "We introduce Performers, Transformer architectures which can estimate regular\n(softmax) full-rank-attention Transformers with provable accuracy, but using\nonly linear (as opposed to quadratic) space and time complexity, without\nrelying on any priors such as sparsity or low-rankness. To approximate softmax\nattention-kernels, Performers use a novel Fast Attention Via positive\nOrthogonal Random features approach (FAVOR+), which may be of independent\ninterest for scalable kernel methods. FAVOR+ can be also used to efficiently\nmodel kernelizable attention mechanisms beyond softmax. This representational\npower is crucial to accurately compare softmax with other kernels for the first\ntime on large-scale tasks, beyond the reach of regular Transformers, and\ninvestigate optimal attention-kernels. Performers are linear architectures\nfully compatible with regular Transformers and with strong theoretical\nguarantees: unbiased or nearly-unbiased estimation of the attention matrix,\nuniform convergence and low estimation variance. We tested Performers on a rich\nset of tasks stretching from pixel-prediction through text models to protein\nsequence modeling. We demonstrate competitive results with other examined\nefficient sparse and dense attention methods, showcasing effectiveness of the\nnovel attention-learning paradigm leveraged by Performers.", "published": "2020-09-30 17:09:09", "link": "http://arxiv.org/abs/2009.14794v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Rethinking Evaluation Methodology for Audio-to-Score Alignment", "abstract": "This paper offers a precise, formal definition of an audio-to-score\nalignment. While the concept of an alignment is intuitively grasped, this\nprecision affords us new insight into the evaluation of audio-to-score\nalignment algorithms. Motivated by these insights, we introduce new evaluation\nmetrics for audio-to-score alignment. Using an alignment evaluation dataset\nderived from pairs of KernScores and MAESTRO performances, we study the\nbehavior of our new metrics and the standard metrics on several classical\nalignment algorithms.", "published": "2020-09-30 01:22:37", "link": "http://arxiv.org/abs/2009.14374v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transfer Learning from Speech Synthesis to Voice Conversion with\n  Non-Parallel Training Data", "abstract": "This paper presents a novel framework to build a voice conversion (VC) system\nby learning from a text-to-speech (TTS) synthesis system, that is called TTS-VC\ntransfer learning. We first develop a multi-speaker speech synthesis system\nwith sequence-to-sequence encoder-decoder architecture, where the encoder\nextracts robust linguistic representations of text, and the decoder,\nconditioned on target speaker embedding, takes the context vectors and the\nattention recurrent network cell output to generate target acoustic features.\nWe take advantage of the fact that TTS system maps input text to speaker\nindependent context vectors, and reuse such a mapping to supervise the training\nof latent representations of an encoder-decoder voice conversion system. In the\nvoice conversion system, the encoder takes speech instead of text as input,\nwhile the decoder is functionally similar to TTS decoder. As we condition the\ndecoder on speaker embedding, the system can be trained on non-parallel data\nfor any-to-any voice conversion. During voice conversion training, we present\nboth text and speech to speech synthesis and voice conversion networks\nrespectively. At run-time, the voice conversion network uses its own\nencoder-decoder architecture. Experiments show that the proposed approach\noutperforms two competitive voice conversion baselines consistently, namely\nphonetic posteriorgram and variational autoencoder methods, in terms of speech\nquality, naturalness, and speaker similarity.", "published": "2020-09-30 02:50:13", "link": "http://arxiv.org/abs/2009.14399v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Event-Independent Network for Polyphonic Sound Event Localization and\n  Detection", "abstract": "Polyphonic sound event localization and detection is not only detecting what\nsound events are happening but localizing corresponding sound sources. This\nseries of tasks was first introduced in DCASE 2019 Task 3. In 2020, the sound\nevent localization and detection task introduces additional challenges in\nmoving sound sources and overlapping-event cases, which include two events of\nthe same type with two different direction-of-arrival (DoA) angles. In this\npaper, a novel event-independent network for polyphonic sound event\nlocalization and detection is proposed. Unlike the two-stage method we proposed\nin DCASE 2019 Task 3, this new network is fully end-to-end. Inputs to the\nnetwork are first-order Ambisonics (FOA) time-domain signals, which are then\nfed into a 1-D convolutional layer to extract acoustic features. The network is\nthen split into two parallel branches. The first branch is for sound event\ndetection (SED), and the second branch is for DoA estimation. There are three\ntypes of predictions from the network, SED predictions, DoA predictions, and\nevent activity detection (EAD) predictions that are used to combine the SED and\nDoA features for on-set and off-set estimation. All of these predictions have\nthe format of two tracks indicating that there are at most two overlapping\nevents. Within each track, there could be at most one event happening. This\narchitecture introduces a problem of track permutation. To address this\nproblem, a frame-level permutation invariant training method is used.\nExperimental results show that the proposed method can detect polyphonic sound\nevents and their corresponding DoAs. Its performance on the Task 3 dataset is\ngreatly increased as compared with that of the baseline method.", "published": "2020-09-30 23:01:33", "link": "http://arxiv.org/abs/2010.00140v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Embedded Emotions -- A Data Driven Approach to Learn Transferable\n  Feature Representations from Raw Speech Input for Emotion Recognition", "abstract": "Traditional approaches to automatic emotion recognition are relying on the\napplication of handcrafted features. More recently however the advent of deep\nlearning enabled algorithms to learn meaningful representations of input data\nautomatically. In this paper, we investigate the applicability of transferring\nknowledge learned from large text and audio corpora to the task of automatic\nemotion recognition. To evaluate the practicability of our approach, we are\ntaking part in this year's Interspeech ComParE Elderly Emotion Sub-Challenge,\nwhere the goal is to classify spoken narratives of elderly people with respect\nto the emotion of the speaker. Our results show that the learned feature\nrepresentations can be effectively applied for classifying emotions from spoken\nlanguage. We found the performance of the features extracted from the audio\nsignal to be not as consistent as those that have been extracted from the\ntranscripts. While the acoustic features achieved best in class results on the\ndevelopment set, when compared to the baseline systems, their performance\ndropped considerably on the test set of the challenge. The features extracted\nfrom the text form, however, are showing promising results on both sets and are\noutperforming the official baseline by 5.7 percentage points unweighted average\nrecall.", "published": "2020-09-30 09:18:31", "link": "http://arxiv.org/abs/2009.14523v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transfer Learning from Monolingual ASR to Transcription-free\n  Cross-lingual Voice Conversion", "abstract": "Cross-lingual voice conversion (VC) is a task that aims to synthesize target\nvoices with the same content while source and target speakers speak in\ndifferent languages. Its challenge lies in the fact that the source and target\ndata are naturally non-parallel, and it is even difficult to bridge the gaps\nbetween languages with no transcriptions provided. In this paper, we focus on\nknowledge transfer from monolin-gual ASR to cross-lingual VC, in order to\naddress the con-tent mismatch problem. To achieve this, we first train a\nmonolingual acoustic model for the source language, use it to extract phonetic\nfeatures for all the speech in the VC dataset, and then train a Seq2Seq\nconversion model to pre-dict the mel-spectrograms. We successfully address\ncross-lingual VC without any transcription or language-specific knowledge for\nforeign speech. We experiment this on Voice Conversion Challenge 2020 datasets\nand show that our speaker-dependent conversion model outperforms the zero-shot\nbaseline, achieving MOS of 3.83 and 3.54 in speech quality and speaker\nsimilarity for cross-lingual conversion. When compared to Cascade ASR-TTS\nmethod, our proposed one significantly reduces the MOS drop be-tween intra- and\ncross-lingual conversion.", "published": "2020-09-30 13:44:35", "link": "http://arxiv.org/abs/2009.14668v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The MIDI Degradation Toolkit: Symbolic Music Augmentation and Correction", "abstract": "In this paper, we introduce the MIDI Degradation Toolkit (MDTK), containing\nfunctions which take as input a musical excerpt (a set of notes with pitch,\nonset time, and duration), and return a \"degraded\" version of that excerpt with\nsome error (or errors) introduced. Using the toolkit, we create the Altered and\nCorrupted MIDI Excerpts dataset version 1.0 (ACME v1.0), and propose four tasks\nof increasing difficulty to detect, classify, locate, and correct the\ndegradations. We hypothesize that models trained for these tasks can be useful\nin (for example) improving automatic music transcription performance if applied\nas a post-processing step. To that end, MDTK includes a script that measures\nthe distribution of different types of errors in a transcription, and creates a\ndegraded dataset with similar properties. MDTK's degradations can also be\napplied dynamically to a dataset during training (with or without the above\nscript), generating novel degraded excerpts each epoch. MDTK could also be used\nto test the robustness of any system designed to take MIDI (or similar) data as\ninput (e.g. systems designed for voice separation, metrical alignment, or chord\ndetection) to such transcription errors or otherwise noisy data. The toolkit\nand dataset are both publicly available online, and we encourage contribution\nand feedback from the community.", "published": "2020-09-30 19:03:35", "link": "http://arxiv.org/abs/2010.00059v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
