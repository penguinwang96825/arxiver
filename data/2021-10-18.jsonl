{"title": "ViraPart: A Text Refinement Framework for Automatic Speech Recognition\n  and Natural Language Processing Tasks in Persian", "abstract": "The Persian language is an inflectional subject-object-verb language. This\nfact makes Persian a more uncertain language. However, using techniques such as\nZero-Width Non-Joiner (ZWNJ) recognition, punctuation restoration, and Persian\nEzafe construction will lead us to a more understandable and precise language.\nIn most of the works in Persian, these techniques are addressed individually.\nDespite that, we believe that for text refinement in Persian, all of these\ntasks are necessary. In this work, we proposed a ViraPart framework that uses\nembedded ParsBERT in its core for text clarifications. First, used the BERT\nvariant for Persian following by a classifier layer for classification\nprocedures. Next, we combined models outputs to output cleartext. In the end,\nthe proposed model for ZWNJ recognition, punctuation restoration, and Persian\nEzafe construction performs the averaged F1 macro scores of 96.90%, 92.13%, and\n98.50%, respectively. Experimental results show that our proposed approach is\nvery effective in text refinement for the Persian language.", "published": "2021-10-18 08:20:40", "link": "http://arxiv.org/abs/2110.09086v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Natural Language Processing to Understand Reasons and Motivators\n  Behind Customer Calls in Financial Domain", "abstract": "In this era of abundant digital information, customer satisfaction has become\none of the prominent factors in the success of any business. Customers want a\none-click solution for almost everything. They tend to get unsatisfied if they\nhave to call about something which they could have done online. Moreover,\nincoming calls are a high-cost component for any business. Thus, it is\nessential to develop a framework capable of mining the reasons and motivators\nbehind customer calls. This paper proposes two models. Firstly, an\nattention-based stacked bidirectional Long Short Term Memory Network followed\nby Hierarchical Clustering for extracting these reasons from transcripts of\ninbound calls. Secondly, a set of ensemble models based on probabilities from\nSupport Vector Machines and Logistic Regression. It is capable of detecting\nfactors that led to these calls. Extensive evaluation proves the effectiveness\nof these models.", "published": "2021-10-18 08:30:50", "link": "http://arxiv.org/abs/2110.09094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of French Phonetic Idiosyncrasies for Accent Recognition", "abstract": "Speech recognition systems have made tremendous progress since the last few\ndecades. They have developed significantly in identifying the speech of the\nspeaker. However, there is a scope of improvement in speech recognition systems\nin identifying the nuances and accents of a speaker. It is known that any\nspecific natural language may possess at least one accent. Despite the\nidentical word phonemic composition, if it is pronounced in different accents,\nwe will have sound waves, which are different from each other. Differences in\npronunciation, in accent and intonation of speech in general, create one of the\nmost common problems of speech recognition. If there are a lot of accents in\nlanguage we should create the acoustic model for each separately. We carry out\na systematic analysis of the problem in the accurate classification of accents.\nWe use traditional machine learning techniques and convolutional neural\nnetworks, and show that the classical techniques are not sufficiently efficient\nto solve this problem. Using spectrograms of speech signals, we propose a\nmulti-class classification framework for accent recognition. In this paper, we\nfocus our attention on the French accent. We also identify its limitation by\nunderstanding the impact of French idiosyncrasies on its spectrograms.", "published": "2021-10-18 10:50:50", "link": "http://arxiv.org/abs/2110.09179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Arabic Parallel Gender Corpus 2.0: Extensions and Analyses", "abstract": "Gender bias in natural language processing (NLP) applications, particularly\nmachine translation, has been receiving increasing attention. Much of the\nresearch on this issue has focused on mitigating gender bias in English NLP\nmodels and systems. Addressing the problem in poorly resourced, and/or\nmorphologically rich languages has lagged behind, largely due to the lack of\ndatasets and resources. In this paper, we introduce a new corpus for gender\nidentification and rewriting in contexts involving one or two target users (I\nand/or You) -- first and second grammatical persons with independent\ngrammatical gender preferences. We focus on Arabic, a gender-marking\nmorphologically rich language. The corpus has multiple parallel components:\nfour combinations of 1st and 2nd person in feminine and masculine grammatical\ngenders, as well as English, and English to Arabic machine translation output.\nThis corpus expands on Habash et al. (2019)'s Arabic Parallel Gender Corpus\n(APGC v1.0) by adding second person targets as well as increasing the total\nnumber of sentences over 6.5 times, reaching over 590K words. Our new dataset\nwill aid the research and development of gender identification, controlled text\ngeneration, and post-editing rewrite systems that could be used to personalize\nNLP applications and provide users with the correct outputs based on their\ngrammatical gender preferences. We make the Arabic Parallel Gender Corpus (APGC\nv2.0) publicly available.", "published": "2021-10-18 12:06:17", "link": "http://arxiv.org/abs/2110.09216v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Data Bootstrapping Recipe for Low Resource Multilingual Relation\n  Classification", "abstract": "Relation classification (sometimes called 'extraction') requires trustworthy\ndatasets for fine-tuning large language models, as well as for evaluation. Data\ncollection is challenging for Indian languages, because they are syntactically\nand morphologically diverse, as well as different from resource-rich languages\nlike English. Despite recent interest in deep generative models for Indian\nlanguages, relation classification is still not well served by public data\nsets. In response, we present IndoRE, a dataset with 21K entity and relation\ntagged gold sentences in three Indian languages, plus English. We start with a\nmultilingual BERT (mBERT) based system that captures entity span positions and\ntype information and provides competitive monolingual relation classification.\nUsing this system, we explore and compare transfer mechanisms between\nlanguages. In particular, we study the accuracy efficiency tradeoff between\nexpensive gold instances vs. translated and aligned 'silver' instances. We\nrelease the dataset for future research.", "published": "2021-10-18 18:40:46", "link": "http://arxiv.org/abs/2110.09570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Domain Adaptation for NMT: Decoupling Language and Domain\n  Information with Adapters", "abstract": "Adapter layers are lightweight, learnable units inserted between transformer\nlayers. Recent work explores using such layers for neural machine translation\n(NMT), to adapt pre-trained models to new domains or language pairs, training\nonly a small set of parameters for each new setting (language pair or domain).\nIn this work we study the compositionality of language and domain adapters in\nthe context of Machine Translation. We aim to study, 1) parameter-efficient\nadaptation to multiple domains and languages simultaneously (full-resource\nscenario) and 2) cross-lingual transfer in domains where parallel data is\nunavailable for certain language pairs (partial-resource scenario). We find\nthat in the partial resource scenario a naive combination of domain-specific\nand language-specific adapters often results in `catastrophic forgetting' of\nthe missing languages. We study other ways to combine the adapters to alleviate\nthis issue and maximize cross-lingual transfer. With our best adapter\ncombinations, we obtain improvements of 3-4 BLEU on average for source\nlanguages that do not have in-domain data. For target languages without\nin-domain data, we achieve a similar improvement by combining adapters with\nback-translation. Supplementary material is available at\nhttps://tinyurl.com/r66stbxj", "published": "2021-10-18 18:55:23", "link": "http://arxiv.org/abs/2110.09574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Transfer Learning & Beyond: Transformer Language Models in\n  Information Systems Research", "abstract": "AI is widely thought to be poised to transform business, yet current\nperceptions of the scope of this transformation may be myopic. Recent progress\nin natural language processing involving transformer language models (TLMs)\noffers a potential avenue for AI-driven business and societal transformation\nthat is beyond the scope of what most currently foresee. We review this recent\nprogress as well as recent literature utilizing text mining in top IS journals\nto develop an outline for how future IS research can benefit from these new\ntechniques. Our review of existing IS literature reveals that suboptimal text\nmining techniques are prevalent and that the more advanced TLMs could be\napplied to enhance and increase IS research involving text data, and to enable\nnew IS research topics, thus creating more value for the research community.\nThis is possible because these techniques make it easier to develop very\npowerful custom systems and their performance is superior to existing methods\nfor a wide range of tasks and applications. Further, multilingual language\nmodels make possible higher quality text analytics for research in multiple\nlanguages. We also identify new avenues for IS research, like language user\ninterfaces, that may offer even greater potential for future IS research.", "published": "2021-10-18 02:01:39", "link": "http://arxiv.org/abs/2110.08975v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ensembling Graph Predictions for AMR Parsing", "abstract": "In many machine learning tasks, models are trained to predict structure data\nsuch as graphs. For example, in natural language processing, it is very common\nto parse texts into dependency trees or abstract meaning representation (AMR)\ngraphs. On the other hand, ensemble methods combine predictions from multiple\nmodels to create a new one that is more robust and accurate than individual\npredictions. In the literature, there are many ensembling techniques proposed\nfor classification or regression problems, however, ensemble graph prediction\nhas not been studied thoroughly. In this work, we formalize this problem as\nmining the largest graph that is the most supported by a collection of graph\npredictions. As the problem is NP-Hard, we propose an efficient heuristic\nalgorithm to approximate the optimal solution. To validate our approach, we\ncarried out experiments in AMR parsing problems. The experimental results\ndemonstrate that the proposed approach can combine the strength of\nstate-of-the-art AMR parsers to create new predictions that are more accurate\nthan any individual models in five standard benchmark datasets.", "published": "2021-10-18 09:35:39", "link": "http://arxiv.org/abs/2110.09131v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contextual Hate Speech Detection in Code Mixed Text using Transformer\n  Based Approaches", "abstract": "In the recent past, social media platforms have helped people in connecting\nand communicating to a wider audience. But this has also led to a drastic\nincrease in cyberbullying. It is essential to detect and curb hate speech to\nkeep the sanity of social media platforms. Also, code mixed text containing\nmore than one language is frequently used on these platforms. We, therefore,\npropose automated techniques for hate speech detection in code mixed text from\nscraped Twitter. We specifically focus on code mixed English-Hindi text and\ntransformer-based approaches. While regular approaches analyze the text\nindependently, we also make use of content text in the form of parent tweets.\nWe try to evaluate the performances of multilingual BERT and Indic-BERT in\nsingle-encoder and dual-encoder settings. The first approach is to concatenate\nthe target text and context text using a separator token and get a single\nrepresentation from the BERT model. The second approach encodes the two texts\nindependently using a dual BERT encoder and the corresponding representations\nare averaged. We show that the dual-encoder approach using independent\nrepresentations yields better performance. We also employ simple ensemble\nmethods to further improve the performance. Using these methods we report the\nbest F1 score of 73.07% on the HASOC 2021 ICHCL code mixed data set.", "published": "2021-10-18 14:05:36", "link": "http://arxiv.org/abs/2110.09338v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ceasing hate withMoH: Hate Speech Detection in Hindi-English\n  Code-Switched Language", "abstract": "Social media has become a bedrock for people to voice their opinions\nworldwide. Due to the greater sense of freedom with the anonymity feature, it\nis possible to disregard social etiquette online and attack others without\nfacing severe consequences, inevitably propagating hate speech. The current\nmeasures to sift the online content and offset the hatred spread do not go far\nenough. One factor contributing to this is the prevalence of regional languages\nin social media and the paucity of language flexible hate speech detectors. The\nproposed work focuses on analyzing hate speech in Hindi-English code-switched\nlanguage. Our method explores transformation techniques to capture precise text\nrepresentation. To contain the structure of data and yet use it with existing\nalgorithms, we developed MoH or Map Only Hindi, which means \"Love\" in Hindi.\nMoH pipeline consists of language identification, Roman to Devanagari Hindi\ntransliteration using a knowledge base of Roman Hindi words. Finally, it\nemploys the fine-tuned Multilingual Bert and MuRIL language models. We\nconducted several quantitative experiment studies on three datasets and\nevaluated performance using Precision, Recall, and F1 metrics. The first\nexperiment studies MoH mapped text's performance with classical machine\nlearning models and shows an average increase of 13% in F1 scores. The second\ncompares the proposed work's scores with those of the baseline models and\noffers a rise in performance by 6%. Finally, the third reaches the proposed MoH\ntechnique with various data simulations using the existing transliteration\nlibrary. Here, MoH outperforms the rest by 15%. Our results demonstrate a\nsignificant improvement in the state-of-the-art scores on all three datasets.", "published": "2021-10-18 15:24:32", "link": "http://arxiv.org/abs/2110.09393v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SentimentArcs: A Novel Method for Self-Supervised Sentiment Analysis of\n  Time Series Shows SOTA Transformers Can Struggle Finding Narrative Arcs", "abstract": "SOTA Transformer and DNN short text sentiment classifiers report over 97%\naccuracy on narrow domains like IMDB movie reviews. Real-world performance is\nsignificantly lower because traditional models overfit benchmarks and\ngeneralize poorly to different or more open domain texts. This paper introduces\nSentimentArcs, a new self-supervised time series sentiment analysis methodology\nthat addresses the two main limitations of traditional supervised sentiment\nanalysis: limited labeled training datasets and poor generalization. A large\nensemble of diverse models provides a synthetic ground truth for\nself-supervised learning. Novel metrics jointly optimize an exhaustive search\nacross every possible corpus:model combination. The joint optimization over\nboth the corpus and model solves the generalization problem. Simple\nvisualizations exploit the temporal structure in narratives so domain experts\ncan quickly spot trends, identify key features, and note anomalies over\nhundreds of arcs and millions of data points. To our knowledge, this is the\nfirst self-supervised method for time series sentiment analysis and the largest\nsurvey directly comparing real-world model performance on long-form narratives.", "published": "2021-10-18 16:45:31", "link": "http://arxiv.org/abs/2110.09454v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NormFormer: Improved Transformer Pretraining with Extra Normalization", "abstract": "During pretraining, the Pre-LayerNorm transformer suffers from a gradient\nmagnitude mismatch: gradients at early layers are much larger than at later\nlayers. These issues can be alleviated by our proposed NormFormer architecture,\nwhich adds three normalization operations to each layer: a Layer Norm after\nself attention, head-wise scaling of self-attention outputs, and a Layer Norm\nafter the first fully connected layer. The extra operations incur negligible\ncompute cost (+0.4% parameter increase), but improve pretraining perplexity and\ndownstream task performance for both causal and masked language models ranging\nfrom 125 Million to 2.7 Billion parameters. For example, adding NormFormer on\ntop of our strongest 1.3B parameter baseline can reach equal perplexity 24%\nfaster, or converge 0.27 perplexity better in the same compute budget. This\nmodel reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked\nlanguage modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on\naverage. Code to train NormFormer models is available in fairseq\nhttps://github.com/pytorch/fairseq/tree/main/examples/normformer .", "published": "2021-10-18 16:47:45", "link": "http://arxiv.org/abs/2110.09456v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Label-Descriptive Patterns and Their Application to Characterizing\n  Classification Errors", "abstract": "State-of-the-art deep learning methods achieve human-like performance on many\ntasks, but make errors nevertheless. Characterizing these errors in easily\ninterpretable terms gives insight into whether a classifier is prone to making\nsystematic errors, but also gives a way to act and improve the classifier. We\npropose to discover those feature-value combinations (i.e., patterns) that\nstrongly correlate with correct resp. erroneous predictions to obtain a global\nand interpretable description for arbitrary classifiers. We show this is an\ninstance of the more general label description problem, which we formulate in\nterms of the Minimum Description Length principle. To discover a good pattern\nset, we develop the efficient Premise algorithm. Through an extensive set of\nexperiments we show it performs very well in practice on both synthetic and\nreal-world data. Unlike existing solutions, it ably recovers ground truth\npatterns, even on highly imbalanced data over many features. Through two case\nstudies on Visual Question Answering and Named Entity Recognition, we confirm\nthat Premise gives clear and actionable insight into the systematic errors made\nby modern NLP classifiers.", "published": "2021-10-18 19:42:21", "link": "http://arxiv.org/abs/2110.09599v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BERMo: What can BERT learn from ELMo?", "abstract": "We propose BERMo, an architectural modification to BERT, which makes\npredictions based on a hierarchy of surface, syntactic and semantic language\nfeatures. We use linear combination scheme proposed in Embeddings from Language\nModels (ELMo) to combine the scaled internal representations from different\nnetwork depths. Our approach has two-fold benefits: (1) improved gradient flow\nfor the downstream task as every layer has a direct connection to the gradients\nof the loss function and (2) increased representative power as the model no\nlonger needs to copy the features learned in the shallower layer which are\nnecessary for the downstream task. Further, our model has a negligible\nparameter overhead as there is a single scalar parameter associated with each\nlayer in the network. Experiments on the probing task from SentEval dataset\nshow that our model performs up to $4.65\\%$ better in accuracy than the\nbaseline with an average improvement of $2.67\\%$ on the semantic tasks. When\nsubject to compression techniques, we find that our model enables stable\npruning for compressing small datasets like SST-2, where the BERT model\ncommonly diverges. We observe that our approach converges $1.67\\times$ and\n$1.15\\times$ faster than the baseline on MNLI and QQP tasks from GLUE dataset.\nMoreover, our results show that our approach can obtain better parameter\nefficiency for penalty based pruning approaches on QQP task.", "published": "2021-10-18 17:35:41", "link": "http://arxiv.org/abs/2110.15802v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ranking Facts for Explaining Answers to Elementary Science Questions", "abstract": "In multiple-choice exams, students select one answer from among typically\nfour choices and can explain why they made that particular choice. Students are\ngood at understanding natural language questions and based on their domain\nknowledge can easily infer the question's answer by 'connecting the dots'\nacross various pertinent facts.\n  Considering automated reasoning for elementary science question answering, we\naddress the novel task of generating explanations for answers from\nhuman-authored facts. For this, we examine the practically scalable framework\nof feature-rich support vector machines leveraging domain-targeted,\nhand-crafted features. Explanations are created from a human-annotated set of\nnearly 5,000 candidate facts in the WorldTree corpus. Our aim is to obtain\nbetter matches for valid facts of an explanation for the correct answer of a\nquestion over the available fact candidates. To this end, our features offer a\ncomprehensive linguistic and semantic unification paradigm. The machine\nlearning problem is the preference ordering of facts, for which we test\npointwise regression versus pairwise learning-to-rank.\n  Our contributions are: (1) a case study in which two preference ordering\napproaches are systematically compared; (2) it is a practically competent\napproach that can outperform some variants of BERT-based reranking models; and\n(3) the human-engineered features make it an interpretable machine learning\nmodel for the task.", "published": "2021-10-18 06:15:11", "link": "http://arxiv.org/abs/2110.09036v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SC"], "primary_category": "cs.CL"}
{"title": "LDNet: Unified Listener Dependent Modeling in MOS Prediction for\n  Synthetic Speech", "abstract": "An effective approach to automatically predict the subjective rating for\nsynthetic speech is to train on a listening test dataset with human-annotated\nscores. Although each speech sample in the dataset is rated by several\nlisteners, most previous works only used the mean score as the training target.\nIn this work, we present LDNet, a unified framework for mean opinion score\n(MOS) prediction that predicts the listener-wise perceived quality given the\ninput speech and the listener identity. We reflect recent advances in LD\nmodeling, including design choices of the model architecture, and propose two\ninference methods that provide more stable results and efficient computation.\nWe conduct systematic experiments on the voice conversion challenge (VCC) 2018\nbenchmark and a newly collected large-scale MOS dataset, providing an in-depth\nanalysis of the proposed framework. Results show that the mean listener\ninference method is a better way to utilize the mean scores, whose\neffectiveness is more obvious when having more ratings per sample.", "published": "2021-10-18 08:52:31", "link": "http://arxiv.org/abs/2110.09103v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BEAMetrics: A Benchmark for Language Generation Evaluation Evaluation", "abstract": "Natural language processing (NLP) systems are increasingly trained to\ngenerate open-ended text rather than classifying between responses. This makes\nresearch on evaluation metrics for generated language -- functions that score\nsystem output given the context and/or human reference responses -- of critical\nimportance. However, different metrics have different strengths and biases, and\nreflect human intuitions better on some tasks than others. There is currently\nno simple, unified way to compare, analyse or evaluate metrics across a\nrepresentative set of tasks. Here, we describe the Benchmark to Evaluate\nAutomatic Metrics (BEAMetrics), a resource to make research into new metrics\nitself easier to evaluate. BEAMetrics users can quickly compare existing and\nnew metrics with human judgements across a diverse set of tasks, quality\ndimensions (fluency vs. coherence vs. informativeness etc), and languages. As\ngeneration experts might predict, BEAMetrics reveals stark task-dependent\ndifferences between existing metrics, and consistently poor performance on\ntasks with complex answer spaces or high reliance on general knowledge. While\nthis analysis highlights a critical issue facing current research practice,\nBEAMetrics also contribute to its resolution by facilitating research into\nbetter metrics -- particularly those that can account for the complex\ninteraction between context and general knowledge inherent to many modern NLP\napplications. BEAMetrics is available under the MIT License:\nhttps://github.com/ThomasScialom/BEAMetrics", "published": "2021-10-18 10:03:19", "link": "http://arxiv.org/abs/2110.09147v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Sequence Training of Attention Models using Approximative\n  Recombination", "abstract": "Sequence discriminative training is a great tool to improve the performance\nof an automatic speech recognition system. It does, however, necessitate a sum\nover all possible word sequences, which is intractable to compute in practice.\nCurrent state-of-the-art systems with unlimited label context circumvent this\nproblem by limiting the summation to an n-best list of relevant competing\nhypotheses obtained from beam search.\n  This work proposes to perform (approximative) recombinations of hypotheses\nduring beam search, if they share a common local history. The error that is\nincurred by the approximation is analyzed and it is shown that using this\ntechnique the effective beam size can be increased by several orders of\nmagnitude without significantly increasing the computational requirements.\nLastly, it is shown that this technique can be used to effectively perform\nsequence discriminative training for attention-based encoder-decoder acoustic\nmodels on the LibriSpeech task.", "published": "2021-10-18 12:47:53", "link": "http://arxiv.org/abs/2110.09245v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Intent Classification Using Pre-trained Language Agnostic Embeddings For\n  Low Resource Languages", "abstract": "Building Spoken Language Understanding (SLU) systems that do not rely on\nlanguage specific Automatic Speech Recognition (ASR) is an important yet less\nexplored problem in language processing. In this paper, we present a\ncomparative study aimed at employing a pre-trained acoustic model to perform\nSLU in low resource scenarios. Specifically, we use three different embeddings\nextracted using Allosaurus, a pre-trained universal phone decoder: (1) Phone\n(2) Panphone, and (3) Allo embeddings. These embeddings are then used in\nidentifying the spoken intent. We perform experiments across three different\nlanguages: English, Sinhala, and Tamil each with different data sizes to\nsimulate high, medium, and low resource scenarios. Our system improves on the\nstate-of-the-art (SOTA) intent classification accuracy by approximately 2.11%\nfor Sinhala and 7.00% for Tamil and achieves competitive results on English.\nFurthermore, we present a quantitative analysis of how the performance scales\nwith the number of training examples used per intent.", "published": "2021-10-18 13:06:59", "link": "http://arxiv.org/abs/2110.09264v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Automatic Learning of Subword Dependent Model Scales", "abstract": "To improve the performance of state-of-the-art automatic speech recognition\nsystems it is common practice to include external knowledge sources such as\nlanguage models or prior corrections. This is usually done via log-linear model\ncombination using separate scaling parameters for each model. Typically these\nparameters are manually optimized on some held-out data.\n  In this work we propose to optimize these scaling parameters via automatic\ndifferentiation and stochastic gradient decent similar to the neural network\nmodel parameters. We show on the LibriSpeech (LBS) and Switchboard (SWB)\ncorpora that the model scales for a combination of attentionbased\nencoder-decoder acoustic model and language model can be learned as effectively\nas with manual tuning. We further extend this approach to subword dependent\nmodel scales which could not be tuned manually which leads to 7% improvement on\nLBS and 3% on SWB. We also show that joint training of scales and model\nparameters is possible and gives additional 6% improvement on LBS.", "published": "2021-10-18 13:48:28", "link": "http://arxiv.org/abs/2110.09324v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Measuring Cognitive Status from Speech in a Smart Home Environment", "abstract": "The population is aging, and becoming more tech-savvy. The United Nations\npredicts that by 2050, one in six people in the world will be over age 65 (up\nfrom one in 11 in 2019), and this increases to one in four in Europe and\nNorthern America. Meanwhile, the proportion of American adults over 65 who own\na smartphone has risen 24 percentage points from 2013-2017, and the majority\nhave Internet in their homes. Smart devices and smart home technology have\nprofound potential to transform how people age, their ability to live\nindependently in later years, and their interactions with their circle of care.\nCognitive health is a key component to independence and well-being in old age,\nand smart homes present many opportunities to measure cognitive status in a\ncontinuous, unobtrusive manner. In this article, we focus on speech as a\nmeasurement instrument for cognitive health. Existing methods of cognitive\nassessment suffer from a number of limitations that could be addressed through\nsmart home speech sensing technologies. We begin with a brief tutorial on\nmeasuring cognitive status from speech, including some pointers to useful\nopen-source software toolboxes for the interested reader. We then present an\noverview of the preliminary results from pilot studies on active and passive\nsmart home speech sensing for the measurement of cognitive health, and conclude\nwith some recommendations and challenge statements for the next wave of work in\nthis area, to help overcome both technical and ethical barriers to success.", "published": "2021-10-18 15:50:05", "link": "http://arxiv.org/abs/2110.09421v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Don't Judge Me by My Face : An Indirect Adversarial Approach to Remove\n  Sensitive Information From Multimodal Neural Representation in Asynchronous\n  Job Video Interviews", "abstract": "se of machine learning for automatic analysis of job interview videos has\nrecently seen increased interest. Despite claims of fair output regarding\nsensitive information such as gender or ethnicity of the candidates, the\ncurrent approaches rarely provide proof of unbiased decision-making, or that\nsensitive information is not used. Recently, adversarial methods have been\nproved to effectively remove sensitive information from the latent\nrepresentation of neural networks. However, these methods rely on the use of\nexplicitly labeled protected variables (e.g. gender), which cannot be collected\nin the context of recruiting in some countries (e.g. France). In this article,\nwe propose a new adversarial approach to remove sensitive information from the\nlatent representation of neural networks without the need to collect any\nsensitive variable. Using only a few frames of the interview, we train our\nmodel to not be able to find the face of the candidate related to the job\ninterview in the inner layers of the model. This, in turn, allows us to remove\nrelevant private information from these layers. Comparing our approach to a\nstandard baseline on a public dataset with gender and ethnicity annotations, we\nshow that it effectively removes sensitive information from the main network.\nMoreover, to the best of our knowledge, this is the first application of\nadversarial techniques for obtaining a multimodal fair representation in the\ncontext of video job interviews. In summary, our contributions aim at improving\nfairness of the upcoming automatic systems processing videos of job interviews\nfor equality in job selection.", "published": "2021-10-18 15:53:15", "link": "http://arxiv.org/abs/2110.09424v1", "categories": ["cs.CV", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Protecting Anonymous Speech: A Generative Adversarial Network\n  Methodology for Removing Stylistic Indicators in Text", "abstract": "With Internet users constantly leaving a trail of text, whether through\nblogs, emails, or social media posts, the ability to write and protest\nanonymously is being eroded because artificial intelligence, when given a\nsample of previous work, can match text with its author out of hundreds of\npossible candidates. Existing approaches to authorship anonymization, also\nknown as authorship obfuscation, often focus on protecting binary demographic\nattributes rather than identity as a whole. Even those that do focus on\nobfuscating identity require manual feedback, lose the coherence of the\noriginal sentence, or only perform well given a limited subset of authors. In\nthis paper, we develop a new approach to authorship anonymization by\nconstructing a generative adversarial network that protects identity and\noptimizes for three different losses corresponding to anonymity, fluency, and\ncontent preservation. Our fully automatic method achieves comparable results to\nother methods in terms of content preservation and fluency, but greatly\noutperforms baselines in regards to anonymization. Moreover, our approach is\nable to generalize well to an open-set context and anonymize sentences from\nauthors it has not encountered before.", "published": "2021-10-18 17:45:56", "link": "http://arxiv.org/abs/2110.09495v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Monotonic Simultaneous Translation with Chunk-wise Reordering and\n  Refinement", "abstract": "Recent work in simultaneous machine translation is often trained with\nconventional full sentence translation corpora, leading to either excessive\nlatency or necessity to anticipate as-yet-unarrived words, when dealing with a\nlanguage pair whose word orders significantly differ. This is unlike human\nsimultaneous interpreters who produce largely monotonic translations at the\nexpense of the grammaticality of a sentence being translated. In this paper, we\nthus propose an algorithm to reorder and refine the target side of a full\nsentence translation corpus, so that the words/phrases between the source and\ntarget sentences are aligned largely monotonically, using word alignment and\nnon-autoregressive neural machine translation. We then train a widely used\nwait-k simultaneous translation model on this reordered-and-refined corpus. The\nproposed approach improves BLEU scores and resulting translations exhibit\nenhanced monotonicity with source sentences.", "published": "2021-10-18 22:51:21", "link": "http://arxiv.org/abs/2110.09646v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Systematic Review on the Detection of Fake News Articles", "abstract": "It has been argued that fake news and the spread of false information pose a\nthreat to societies throughout the world, from influencing the results of\nelections to hindering the efforts to manage the COVID-19 pandemic. To combat\nthis threat, a number of Natural Language Processing (NLP) approaches have been\ndeveloped. These leverage a number of datasets, feature extraction/selection\ntechniques and machine learning (ML) algorithms to detect fake news before it\nspreads. While these methods are well-documented, there is less evidence\nregarding their efficacy in this domain. By systematically reviewing the\nliterature, this paper aims to delineate the approaches for fake news detection\nthat are most performant, identify limitations with existing approaches, and\nsuggest ways these can be mitigated. The analysis of the results indicates that\nEnsemble Methods using a combination of news content and socially-based\nfeatures are currently the most effective. Finally, it is proposed that future\nresearch should focus on developing approaches that address generalisability\nissues (which, in part, arise from limitations with current datasets),\nexplainability and bias.", "published": "2021-10-18 21:29:11", "link": "http://arxiv.org/abs/2110.11240v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SCENIC: A JAX Library for Computer Vision Research and Beyond", "abstract": "Scenic is an open-source JAX library with a focus on Transformer-based models\nfor computer vision research and beyond. The goal of this toolkit is to\nfacilitate rapid experimentation, prototyping, and research of new vision\narchitectures and models. Scenic supports a diverse range of vision tasks\n(e.g., classification, segmentation, detection)and facilitates working on\nmulti-modal problems, along with GPU/TPU support for multi-host, multi-device\nlarge-scale training. Scenic also offers optimized implementations of\nstate-of-the-art research models spanning a wide range of modalities. Scenic\nhas been successfully used for numerous projects and published papers and\ncontinues serving as the library of choice for quick prototyping and\npublication of new research ideas.", "published": "2021-10-18 08:41:17", "link": "http://arxiv.org/abs/2110.11403v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Embracing advanced AI/ML to help investors achieve success: Vanguard\n  Reinforcement Learning for Financial Goal Planning", "abstract": "In the world of advice and financial planning, there is seldom one right\nanswer. While traditional algorithms have been successful in solving linear\nproblems, its success often depends on choosing the right features from a\ndataset, which can be a challenge for nuanced financial planning scenarios.\nReinforcement learning is a machine learning approach that can be employed with\ncomplex data sets where picking the right features can be nearly impossible. In\nthis paper, we will explore the use of machine learning for financial\nforecasting, predicting economic indicators, and creating a savings strategy.\nVanguard ML algorithm for goals-based financial planning is based on deep\nreinforcement learning that identifies optimal savings rates across multiple\ngoals and sources of income to help clients achieve financial success. Vanguard\nlearning algorithms are trained to identify market indicators and behaviors too\ncomplex to capture with formulas and rules, instead, it works to model the\nfinancial success trajectory of investors and their investment outcomes as a\nMarkov decision process. We believe that reinforcement learning can be used to\ncreate value for advisors and end-investors, creating efficiency, more\npersonalized plans, and data to enable customized solutions.", "published": "2021-10-18 18:46:20", "link": "http://arxiv.org/abs/2110.12003v1", "categories": ["q-fin.ST", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Speech Representation Learning Through Self-supervised Pretraining And\n  Multi-task Finetuning", "abstract": "Speech representation learning plays a vital role in speech processing. Among\nthem, self-supervised learning (SSL) has become an important research\ndirection. It has been shown that an SSL pretraining model can achieve\nexcellent performance in various downstream tasks of speech processing. On the\nother hand, supervised multi-task learning (MTL) is another representation\nlearning paradigm, which has been proven effective in computer vision (CV) and\nnatural language processing (NLP). However, there is no systematic research on\nthe general representation learning model trained by supervised MTL in speech\nprocessing. In this paper, we show that MTL finetuning can further improve SSL\npretraining. We analyze the generalizability of supervised MTL finetuning to\nexamine if the speech representation learned by MTL finetuning can generalize\nto unseen new tasks.", "published": "2021-10-18 07:16:04", "link": "http://arxiv.org/abs/2110.09930v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Supervised Metric Learning for Music Structure Features", "abstract": "Music structure analysis (MSA) methods traditionally search for musically\nmeaningful patterns in audio: homogeneity, repetition, novelty, and\nsegment-length regularity. Hand-crafted audio features such as MFCCs or\nchromagrams are often used to elicit these patterns. However, with more\nannotations of section labels (e.g., verse, chorus, and bridge) becoming\navailable, one can use supervised feature learning to make these patterns even\nclearer and improve MSA performance. To this end, we take a supervised metric\nlearning approach: we train a deep neural network to output embeddings that are\nnear each other for two spectrogram inputs if both have the same section type\n(according to an annotation), and otherwise far apart. We propose a batch\nsampling scheme to ensure the labels in a training pair are interpreted\nmeaningfully. The trained model extracts features that can be used in existing\nMSA algorithms. In evaluations with three datasets (HarmonixSet, SALAMI, and\nRWC), we demonstrate that using the proposed features can improve a traditional\nMSA algorithm significantly in both intra- and cross-dataset scenarios.", "published": "2021-10-18 03:38:08", "link": "http://arxiv.org/abs/2110.09000v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Similarity-and-Independence-Aware Beamformer with Iterative Casting and\n  Boost Start for Target Source Extraction Using Reference", "abstract": "Target source extraction is significant for improving human speech\nintelligibility and the speech recognition performance of computers. This study\ndescribes a method for target source extraction, called the\nsimilarity-and-independence-aware beamformer (SIBF). The SIBF extracts the\ntarget source using a rough magnitude spectrogram as the reference signal. The\nadvantage of the SIBF is that it can obtain a more accurate signal than the\nspectrogram generated by target-enhancing methods such as speech enhancement\nbased on deep neural networks. For the extraction, we extend the framework of\ndeflationary independent component analysis (ICA) by considering the\nsimilarities between the reference and extracted target sources, in addition to\nthe mutual independence of all the potential sources. To solve the extraction\nproblem by maximum-likelihood estimation, we introduce three source models that\ncan reflect the similarities. The major contributions of this study are as\nfollows. First, the extraction performance is improved using two methods,\nnamely boost start for faster convergence and iterative casting for generating\na more accurate reference. The effectiveness of these methods is verified\nthrough experiments using the CHiME3 dataset. Second, a concept of a fixed\npoint pertaining to accuracy is developed. This concept facilitates\nunderstanding the relationship between the reference and SIBF output in terms\nof accuracy. Third, a unified formulation of the SIBF and mask-based beamformer\nis realized to apply the expertise of conventional BFs to the SIBF. The\nfindings of this study can also improve the performance of the SIBF and promote\nresearch on ICA and conventional beamformers.\n  Index Terms: beamformer, independent component analysis, source separation,\nspeech enhancement, target source extraction", "published": "2021-10-18 05:23:47", "link": "http://arxiv.org/abs/2110.09019v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "KaraTuner: Towards end to end natural pitch correction for singing voice\n  in karaoke", "abstract": "An automatic pitch correction system typically includes several stages, such\nas pitch extraction, deviation estimation, pitch shift processing, and\ncross-fade smoothing. However, designing these components with strategies often\nrequires domain expertise and they are likely to fail on corner cases. In this\npaper, we present KaraTuner, an end-to-end neural architecture that predicts\npitch curve and resynthesizes the singing voice directly from the tuned pitch\nand vocal spectrum extracted from the original recordings. Several vital\ntechnical points have been introduced in KaraTuner to ensure pitch accuracy,\npitch naturalness, timbre consistency, and sound quality. A feed-forward\nTransformer is employed in the pitch predictor to capture longterm dependencies\nin the vocal spectrum and musical note. We also develop a pitch-controllable\nvocoder based on a novel source-filter block and the Fre-GAN architecture.\nKaraTuner obtains a higher preference than the rule-based pitch correction\napproach through A/B tests, and perceptual experiments show that the proposed\nvocoder achieves significant advantages in timbre consistency and sound quality\ncompared with the parametric WORLD vocoder, phase vocoder and CLPC vocoder.", "published": "2021-10-18 09:15:22", "link": "http://arxiv.org/abs/2110.09121v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tackling the Score Shift in Cross-Lingual Speaker Verification by\n  Exploiting Language Information", "abstract": "This paper contains a post-challenge performance analysis on cross-lingual\nspeaker verification of the IDLab submission to the VoxCeleb Speaker\nRecognition Challenge 2021 (VoxSRC-21). We show that current speaker embedding\nextractors consistently underestimate speaker similarity in within-speaker\ncross-lingual trials. Consequently, the typical training and scoring protocols\ndo not put enough emphasis on the compensation of intra-speaker language\nvariability. We propose two techniques to increase cross-lingual speaker\nverification robustness. First, we enhance our previously proposed Large-Margin\nFine-Tuning (LM-FT) training stage with a mini-batch sampling strategy which\nincreases the amount of intra-speaker cross-lingual samples within the\nmini-batch. Second, we incorporate language information in the logistic\nregression calibration stage. We integrate quality metrics based on soft and\nhard decisions of a VoxLingua107 language identification model. The proposed\ntechniques result in a 11.7% relative improvement over the baseline model on\nthe VoxSRC-21 test set and contributed to our third place finish in the\ncorresponding challenge.", "published": "2021-10-18 10:12:51", "link": "http://arxiv.org/abs/2110.09150v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Models for Query by Vocal Percussion: A Comparative Study", "abstract": "The imitation of percussive sounds via the human voice is a natural and\neffective tool for communicating rhythmic ideas on the fly. Thus, the automatic\nretrieval of drum sounds using vocal percussion can help artists prototype drum\npatterns in a comfortable and quick way, smoothing the creative workflow as a\nresult. Here we explore different strategies to perform this type of query,\nmaking use of both traditional machine learning algorithms and recent deep\nlearning techniques. The main hyperparameters from the models involved are\ncarefully selected by feeding performance metrics to a grid search algorithm.\nWe also look into several audio data augmentation techniques, which can\npotentially regularise deep learning models and improve generalisation. We\ncompare the final performances in terms of effectiveness (classification\naccuracy), efficiency (computational speed), stability (performance\nconsistency), and interpretability (decision patterns), and discuss the\nrelevance of these results when it comes to the design of successful\nquery-by-vocal-percussion systems.", "published": "2021-10-18 12:27:58", "link": "http://arxiv.org/abs/2110.09223v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Who calls the shots? Rethinking Few-Shot Learning for Audio", "abstract": "Few-shot learning aims to train models that can recognize novel classes given\njust a handful of labeled examples, known as the support set. While the field\nhas seen notable advances in recent years, they have often focused on\nmulti-class image classification. Audio, in contrast, is often multi-label due\nto overlapping sounds, resulting in unique properties such as polyphony and\nsignal-to-noise ratios (SNR). This leads to unanswered questions concerning the\nimpact such audio properties may have on few-shot learning system design,\nperformance, and human-computer interaction, as it is typically up to the user\nto collect and provide inference-time support set examples. We address these\nquestions through a series of experiments designed to elucidate the answers to\nthese questions. We introduce two novel datasets, FSD-MIX-CLIPS and\nFSD-MIX-SED, whose programmatic generation allows us to explore these questions\nsystematically. Our experiments lead to audio-specific insights on few-shot\nlearning, some of which are at odds with recent findings in the image domain:\nthere is no best one-size-fits-all model, method, and support set selection\ncriterion. Rather, it depends on the expected application scenario. Our code\nand data are available at https://github.com/wangyu/rethink-audio-fsl.", "published": "2021-10-18 19:47:15", "link": "http://arxiv.org/abs/2110.09600v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VRM-Phase I VKW system description of long-short video customizable\n  keyword wakeup challenge", "abstract": "Keyword wakeup technology has always been a research hotspot in speech\nprocessing, but many related works were done on different datasets. We\norganized a Chinese long-short video keyword wakeup challenge (Video Keyword\nWakeup Challenge, VKW) for testing the ability of each participating team to\nbuild a keyword wakeup system under the public dataset. All submitted systems\nnot only need to support the setting of multiple different keywords, but also\nneed to support the wakeup of any costumed keyword.This paper mainly describes\nthe basic situation of the VKW challenge and the experimental results of some\nparticipating teams.", "published": "2021-10-18 08:42:31", "link": "http://arxiv.org/abs/2110.15316v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real Additive Margin Softmax for Speaker Verification", "abstract": "The additive margin softmax (AM-Softmax) loss has delivered remarkable\nperformance in speaker verification. A supposed behavior of AM-Softmax is that\nit can shrink within-class variation by putting emphasis on target logits,\nwhich in turn improves margin between target and non-target classes. In this\npaper, we conduct a careful analysis on the behavior of AM-Softmax loss, and\nshow that this loss does not implement real max-margin training. Based on this\nobservation, we present a Real AM-Softmax loss which involves a true margin\nfunction in the softmax training. Experiments conducted on VoxCeleb1, SITW and\nCNCeleb demonstrated that the corrected AM-Softmax loss consistently\noutperforms the original one. The code has been released at\nhttps://gitlab.com/csltstu/sunine.", "published": "2021-10-18 09:11:14", "link": "http://arxiv.org/abs/2110.09116v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpecTNT: a Time-Frequency Transformer for Music Audio", "abstract": "Transformers have drawn attention in the MIR field for their remarkable\nperformance shown in natural language processing and computer vision. However,\nprior works in the audio processing domain mostly use Transformer as a temporal\nfeature aggregator that acts similar to RNNs. In this paper, we propose\nSpecTNT, a Transformer-based architecture to model both spectral and temporal\nsequences of an input time-frequency representation. Specifically, we introduce\na novel variant of the Transformer-in-Transformer (TNT) architecture. In each\nSpecTNT block, a spectral Transformer extracts frequency-related features into\nthe frequency class token (FCT) for each frame. Later, the FCTs are linearly\nprojected and added to the temporal embeddings (TEs), which aggregate useful\ninformation from the FCTs. Then, a temporal Transformer processes the TEs to\nexchange information across the time axis. By stacking the SpecTNT blocks, we\nbuild the SpecTNT model to learn the representation for music signals. In\nexperiments, SpecTNT demonstrates state-of-the-art performance in music tagging\nand vocal melody extraction, and shows competitive performance for chord\nrecognition. The effectiveness of SpecTNT and other design choices are further\nexamined through ablation studies.", "published": "2021-10-18 09:30:58", "link": "http://arxiv.org/abs/2110.09127v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EIHW-MTG: Second DiCOVA Challenge System Report", "abstract": "This work presents an outer product-based approach to fuse the embedded\nrepresentations generated from the spectrograms of cough, breath, and speech\nsamples for the automatic detection of COVID-19. To extract deep learnt\nrepresentations from the spectrograms, we compare the performance of a CNN\ntrained from scratch and a ResNet18 architecture fine-tuned for the task at\nhand. Furthermore, we investigate whether the patients' sex and the use of\ncontextual attention mechanisms is beneficial. Our experiments use the dataset\nreleased as part of the Second Diagnosing COVID-19 using Acoustics (DiCOVA)\nChallenge. The results suggest the suitability of fusing breath and speech\ninformation to detect COVID-19. An Area Under the Curve (AUC) of 84.06% is\nobtained on the test partition when using a CNN trained from scratch with\ncontextual attention mechanisms. When using the ResNet18 architecture for\nfeature extraction, the baseline model scores the highest performance with an\nAUC of 84.26%.", "published": "2021-10-18 12:39:00", "link": "http://arxiv.org/abs/2110.09239v1", "categories": ["cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "FMFCC-A: A Challenging Mandarin Dataset for Synthetic Speech Detection", "abstract": "As increasing development of text-to-speech (TTS) and voice conversion (VC)\ntechnologies, the detection of synthetic speech has been suffered dramatically.\nIn order to promote the development of synthetic speech detection model against\nMandarin TTS and VC technologies, we have constructed a challenging Mandarin\ndataset and organized the accompanying audio track of the first fake media\nforensic challenge of China Society of Image and Graphics (FMFCC-A). The\nFMFCC-A dataset is by far the largest publicly-available Mandarin dataset for\nsynthetic speech detection, which contains 40,000 synthesized Mandarin\nutterances that generated by 11 Mandarin TTS systems and two Mandarin VC\nsystems, and 10,000 genuine Mandarin utterances collected from 58 speakers. The\nFMFCC-A dataset is divided into the training, development and evaluation sets,\nwhich are used for the research of detection of synthesized Mandarin speech\nunder various previously unknown speech synthesis systems or audio\npost-processing operations. In addition to describing the construction of the\nFMFCC-A dataset, we provide a detailed analysis of two baseline methods and the\ntop-performing submissions from the FMFCC-A, which illustrates the usefulness\nand challenge of FMFCC-A dataset. We hope that the FMFCC-A dataset can fill the\ngap of lack of Mandarin datasets for synthetic speech detection.", "published": "2021-10-18 16:22:29", "link": "http://arxiv.org/abs/2110.09441v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Domain Adaptation with Paired Examples for Acoustic Scene\n  Classification on Different Recording Devices", "abstract": "In classification tasks, the classification accuracy diminishes when the data\nis gathered in different domains. To address this problem, in this paper, we\ninvestigate several adversarial models for domain adaptation (DA) and their\neffect on the acoustic scene classification task. The studied models include\nseveral types of generative adversarial networks (GAN), with different loss\nfunctions, and the so-called cycle GAN which consists of two interconnected GAN\nmodels. The experiments are performed on the DCASE20 challenge task 1A dataset,\nin which we can leverage the paired examples of data recorded using different\ndevices, i.e., the source and target domain recordings. The results of\nperformed experiments indicate that the best performing domain adaptation can\nbe obtained using the cycle GAN, which achieves as much as 66% relative\nimprovement in accuracy for the target domain device, while only 6\\% relative\ndecrease in accuracy on the source domain. In addition, by utilizing the paired\ndata examples, we are able to improve the overall accuracy over the model\ntrained using larger unpaired data set, while decreasing the computational cost\nof the model training.", "published": "2021-10-18 19:34:12", "link": "http://arxiv.org/abs/2110.09598v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Synthesis of Footsteps Sound Effects with Generative Adversarial\n  Networks", "abstract": "Footsteps are among the most ubiquitous sound effects in multimedia\napplications. There is substantial research into understanding the acoustic\nfeatures and developing synthesis models for footstep sound effects. In this\npaper, we present a first attempt at adopting neural synthesis for this task.\nWe implemented two GAN-based architectures and compared the results with real\nrecordings as well as six traditional sound synthesis methods. Our\narchitectures reached realism scores as high as recorded samples, showing\nencouraging results for the task at hand.", "published": "2021-10-18 20:04:46", "link": "http://arxiv.org/abs/2110.09605v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Personalized Speech Enhancement: New Models and Comprehensive Evaluation", "abstract": "Personalized speech enhancement (PSE) models utilize additional cues, such as\nspeaker embeddings like d-vectors, to remove background noise and interfering\nspeech in real-time and thus improve the speech quality of online video\nconferencing systems for various acoustic scenarios. In this work, we propose\ntwo neural networks for PSE that achieve superior performance to the previously\nproposed VoiceFilter. In addition, we create test sets that capture a variety\nof scenarios that users can encounter during video conferencing. Furthermore,\nwe propose a new metric to measure the target speaker over-suppression (TSOS)\nproblem, which was not sufficiently investigated before despite its critical\nimportance in deployment. Besides, we propose multi-task training with a speech\nrecognition back-end. Our results show that the proposed models can yield\nbetter speech recognition accuracy, speech intelligibility, and perceptual\nquality than the baseline models, and the multi-task training can alleviate the\nTSOS issue in addition to improving the speech recognition accuracy.", "published": "2021-10-18 21:21:23", "link": "http://arxiv.org/abs/2110.09625v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CycleFlow: Purify Information Factors by Cycle Loss", "abstract": "SpeechFlow is a powerful factorization model based on information bottleneck\n(IB), and its effectiveness has been reported by several studies. A potential\nproblem of SpeechFlow, however, is that if the IB channels are not well\ndesigned, the resultant factors cannot be well disentangled. In this study, we\npropose a CycleFlow model that combines random factor substitution and cycle\nloss to solve this problem. Experiments on voice conversion tasks demonstrate\nthat this simple technique can effectively reduce mutual information among\nindividual factors, and produce clearly better conversion than the IB-based\nSpeechFlow. CycleFlow can also be used as a powerful tool for speech editing.\nWe demonstrate this usage by an emotion perception experiment.", "published": "2021-10-18 13:19:08", "link": "http://arxiv.org/abs/2110.09928v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
