{"title": "IDS at SemEval-2020 Task 10: Does Pre-trained Language Model Know What\n  to Emphasize?", "abstract": "We propose a novel method that enables us to determine words that deserve to\nbe emphasized from written text in visual media, relying only on the\ninformation from the self-attention distributions of pre-trained language\nmodels (PLMs). With extensive experiments and analyses, we show that 1) our\nzero-shot approach is superior to a reasonable baseline that adopts TF-IDF and\nthat 2) there exist several attention heads in PLMs specialized for emphasis\nselection, confirming that PLMs are capable of recognizing important words in\nsentences.", "published": "2020-07-24 07:28:47", "link": "http://arxiv.org/abs/2007.12390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MULTISEM at SemEval-2020 Task 3: Fine-tuning BERT for Lexical Meaning", "abstract": "We present the MULTISEM systems submitted to SemEval 2020 Task 3: Graded Word\nSimilarity in Context (GWSC). We experiment with injecting semantic knowledge\ninto pre-trained BERT models through fine-tuning on lexical semantic tasks\nrelated to GWSC. We use existing semantically annotated datasets and propose to\napproximate similarity through automatically generated lexical substitutes in\ncontext. We participate in both GWSC subtasks and address two languages,\nEnglish and Finnish. Our best English models occupy the third and fourth\npositions in the ranking for the two subtasks. Performance is lower for the\nFinnish models which are mid-ranked in the respective subtasks, highlighting\nthe important role of data availability for fine-tuning.", "published": "2020-07-24 09:50:26", "link": "http://arxiv.org/abs/2007.12432v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FiSSA at SemEval-2020 Task 9: Fine-tuned For Feelings", "abstract": "In this paper, we present our approach for sentiment classification on\nSpanish-English code-mixed social media data in the SemEval-2020 Task 9. We\ninvestigate performance of various pre-trained Transformer models by using\ndifferent fine-tuning strategies. We explore both monolingual and multilingual\nmodels with the standard fine-tuning method. Additionally, we propose a custom\nmodel that we fine-tune in two steps: once with a language modeling objective,\nand once with a task-specific objective. Although two-step fine-tuning improves\nsentiment classification performance over the base model, the large\nmultilingual XLM-RoBERTa model achieves best weighted F1-score with 0.537 on\ndevelopment data and 0.739 on test data. With this score, our team jupitter\nplaced tenth overall in the competition.", "published": "2020-07-24 14:48:27", "link": "http://arxiv.org/abs/2007.12544v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JUNLP@SemEval-2020 Task 9:Sentiment Analysis of Hindi-English code mixed\n  data using Grid Search Cross Validation", "abstract": "Code-mixing is a phenomenon which arises mainly in multilingual societies.\nMultilingual people, who are well versed in their native languages and also\nEnglish speakers, tend to code-mix using English-based phonetic typing and the\ninsertion of anglicisms in their main language. This linguistic phenomenon\nposes a great challenge to conventional NLP domains such as Sentiment Analysis,\nMachine Translation, and Text Summarization, to name a few. In this work, we\nfocus on working out a plausible solution to the domain of Code-Mixed Sentiment\nAnalysis. This work was done as participation in the SemEval-2020 Sentimix\nTask, where we focused on the sentiment analysis of English-Hindi code-mixed\nsentences. our username for the submission was \"sainik.mahata\" and team name\nwas \"JUNLP\". We used feature extraction algorithms in conjunction with\ntraditional machine learning algorithms such as SVR and Grid Search in an\nattempt to solve the task. Our approach garnered an f1-score of 66.2\\% when\ntested using metrics prepared by the organizers of the task.", "published": "2020-07-24 15:06:48", "link": "http://arxiv.org/abs/2007.12561v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named entity recognition in chemical patents using ensemble of\n  contextual language models", "abstract": "Chemical patent documents describe a broad range of applications holding key\nreaction and compound information, such as chemical structure, reaction\nformulas, and molecular properties. These informational entities should be\nfirst identified in text passages to be utilized in downstream tasks. Text\nmining provides means to extract relevant information from chemical patents\nthrough information extraction techniques. As part of the Information\nExtraction task of the Cheminformatics Elsevier Melbourne University challenge,\nin this work we study the effectiveness of contextualized language models to\nextract reaction information in chemical patents. We assess transformer\narchitectures trained on a generic and specialised corpora to propose a new\nensemble model. Our best model, based on a majority ensemble approach, achieves\nan exact F1-score of 92.30% and a relaxed F1-score of 96.24%. The results show\nthat ensemble of contextualized language models can provide an effective method\nto extract information from chemical patents.", "published": "2020-07-24 15:23:45", "link": "http://arxiv.org/abs/2007.12569v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SummEval: Re-evaluating Summarization Evaluation", "abstract": "The scarcity of comprehensive up-to-date studies on evaluation metrics for\ntext summarization and the lack of consensus regarding evaluation protocols\ncontinue to inhibit progress. We address the existing shortcomings of\nsummarization evaluation methods along five dimensions: 1) we re-evaluate 14\nautomatic evaluation metrics in a comprehensive and consistent fashion using\nneural summarization model outputs along with expert and crowd-sourced human\nannotations, 2) we consistently benchmark 23 recent summarization models using\nthe aforementioned automatic evaluation metrics, 3) we assemble the largest\ncollection of summaries generated by models trained on the CNN/DailyMail news\ndataset and share it in a unified format, 4) we implement and share a toolkit\nthat provides an extensible and unified API for evaluating summarization models\nacross a broad range of automatic metrics, 5) we assemble and share the largest\nand most diverse, in terms of model types, collection of human judgments of\nmodel-generated summaries on the CNN/Daily Mail dataset annotated by both\nexpert judges and crowd-source workers. We hope that this work will help\npromote a more complete evaluation protocol for text summarization as well as\nadvance research in developing evaluation metrics that better correlate with\nhuman judgments.", "published": "2020-07-24 16:25:19", "link": "http://arxiv.org/abs/2007.12626v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IUST at SemEval-2020 Task 9: Sentiment Analysis for Code-Mixed Social\n  Media Text using Deep Neural Networks and Linear Baselines", "abstract": "Sentiment Analysis is a well-studied field of Natural Language Processing.\nHowever, the rapid growth of social media and noisy content within them poses\nsignificant challenges in addressing this problem with well-established methods\nand tools. One of these challenges is code-mixing, which means using different\nlanguages to convey thoughts in social media texts. Our group, with the name of\nIUST(username: TAHA), participated at the SemEval-2020 shared task 9 on\nSentiment Analysis for Code-Mixed Social Media Text, and we have attempted to\ndevelop a system to predict the sentiment of a given code-mixed tweet. We used\ndifferent preprocessing techniques and proposed to use different methods that\nvary from NBSVM to more complicated deep neural network models. Our best\nperforming method obtains an F1 score of 0.751 for the Spanish-English sub-task\nand 0.706 over the Hindi-English sub-task.", "published": "2020-07-24 18:48:37", "link": "http://arxiv.org/abs/2007.12733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistent Transcription and Translation of Speech", "abstract": "The conventional paradigm in speech translation starts with a speech\nrecognition step to generate transcripts, followed by a translation step with\nthe automatic transcripts as input. To address various shortcomings of this\nparadigm, recent work explores end-to-end trainable direct models that\ntranslate without transcribing. However, transcripts can be an indispensable\noutput in practical applications, which often display transcripts alongside the\ntranslations to users.\n  We make this common requirement explicit and explore the task of jointly\ntranscribing and translating speech. While high accuracy of transcript and\ntranslation are crucial, even highly accurate systems can suffer from\ninconsistencies between both outputs that degrade the user experience. We\nintroduce a methodology to evaluate consistency and compare several modeling\napproaches, including the traditional cascaded approach and end-to-end models.\nWe find that direct models are poorly suited to the joint\ntranscription/translation task, but that end-to-end models that feature a\ncoupled inference procedure are able to achieve strong consistency. We further\nintroduce simple techniques for directly optimizing for consistency, and\nanalyze the resulting trade-offs between consistency, transcription accuracy,\nand translation accuracy.", "published": "2020-07-24 19:17:26", "link": "http://arxiv.org/abs/2007.12741v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IR-BERT: Leveraging BERT for Semantic Search in Background Linking for\n  News Articles", "abstract": "This work describes our two approaches for the background linking task of\nTREC 2020 News Track. The main objective of this task is to recommend a list of\nrelevant articles that the reader should refer to in order to understand the\ncontext and gain background information of the query article. Our first\napproach focuses on building an effective search query by combining weighted\nkeywords extracted from the query document and uses BM25 for retrieval. The\nsecond approach leverages the capability of SBERT (Nils Reimers et al.) to\nlearn contextual representations of the query in order to perform semantic\nsearch over the corpus. We empirically show that employing a language model\nbenefits our approach in understanding the context as well as the background of\nthe query article. The proposed approaches are evaluated on the TREC 2018\nWashington Post dataset and our best model outperforms the TREC median as well\nas the highest scoring model of 2018 in terms of the nDCG@5 metric. We further\npropose a diversity measure to evaluate the effectiveness of the various\napproaches in retrieving a diverse set of documents. This would potentially\nmotivate researchers to work on introducing diversity in their recommended\nlist. We have open sourced our implementation on Github and plan to submit our\nruns for the background linking task in TREC 2020.", "published": "2020-07-24 16:02:14", "link": "http://arxiv.org/abs/2007.12603v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Personalised Visual Art Recommendation by Learning Latent Semantic\n  Representations", "abstract": "In Recommender systems, data representation techniques play a great role as\nthey have the power to entangle, hide and reveal explanatory factors embedded\nwithin datasets. Hence, they influence the quality of recommendations.\nSpecifically, in Visual Art (VA) recommendations the complexity of the concepts\nembodied within paintings, makes the task of capturing semantics by machines\nfar from trivial. In VA recommendation, prominent works commonly use manually\ncurated metadata to drive recommendations. Recent works in this domain aim at\nleveraging visual features extracted using Deep Neural Networks (DNN). However,\nsuch data representation approaches are resource demanding and do not have a\ndirect interpretation, hindering user acceptance. To address these limitations,\nwe introduce an approach for Personalised Recommendation of Visual arts based\non learning latent semantic representation of paintings. Specifically, we\ntrained a Latent Dirichlet Allocation (LDA) model on textual descriptions of\npaintings. Our LDA model manages to successfully uncover non-obvious semantic\nrelationships between paintings whilst being able to offer explainable\nrecommendations. Experimental evaluations demonstrate that our method tends to\nperform better than exploiting visual features extracted using pre-trained Deep\nNeural Networks.", "published": "2020-07-24 14:50:10", "link": "http://arxiv.org/abs/2008.02687v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Survey on Graph Neural Networks for Knowledge Graph Completion", "abstract": "Knowledge Graphs are increasingly becoming popular for a variety of\ndownstream tasks like Question Answering and Information Retrieval. However,\nthe Knowledge Graphs are often incomplete, thus leading to poor performance. As\na result, there has been a lot of interest in the task of Knowledge Base\nCompletion. More recently, Graph Neural Networks have been used to capture\nstructural information inherently stored in these Knowledge Graphs and have\nbeen shown to achieve SOTA performance across a variety of datasets. In this\nsurvey, we understand the various strengths and weaknesses of the proposed\nmethodology and try to find new exciting research problems in this area that\nrequire further investigation.", "published": "2020-07-24 06:46:46", "link": "http://arxiv.org/abs/2007.12374v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COVID-19 Knowledge Graph: Accelerating Information Retrieval and\n  Discovery for Scientific Literature", "abstract": "The coronavirus disease (COVID-19) has claimed the lives of over 350,000\npeople and infected more than 6 million people worldwide. Several search\nengines have surfaced to provide researchers with additional tools to find and\nretrieve information from the rapidly growing corpora on COVID-19. These\nengines lack extraction and visualization tools necessary to retrieve and\ninterpret complex relations inherent to scientific literature. Moreover,\nbecause these engines mainly rely upon semantic information, their ability to\ncapture complex global relationships across documents is limited, which reduces\nthe quality of similarity-based article recommendations for users. In this\nwork, we present the COVID-19 Knowledge Graph (CKG), a heterogeneous graph for\nextracting and visualizing complex relationships between COVID-19 scientific\narticles. The CKG combines semantic information with document topological\ninformation for the application of similar document retrieval. The CKG is\nconstructed using the latent schema of the data, and then enriched with\nbiomedical entity information extracted from the unstructured text of articles\nusing scalable AWS technologies to form relations in the graph. Finally, we\npropose a document similarity engine that leverages low-dimensional graph\nembeddings from the CKG with semantic embeddings for similar article retrieval.\nAnalysis demonstrates the quality of relationships in the CKG and shows that it\ncan be used to uncover meaningful information in COVID-19 scientific articles.\nThe CKG helps power www.cord19.aws and is publicly available.", "published": "2020-07-24 18:29:43", "link": "http://arxiv.org/abs/2007.12731v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Dialog without Dialog Data: Learning Visual Dialog Agents from VQA Data", "abstract": "Can we develop visually grounded dialog agents that can efficiently adapt to\nnew tasks without forgetting how to talk to people? Such agents could leverage\na larger variety of existing data to generalize to new tasks, minimizing\nexpensive data collection and annotation. In this work, we study a setting we\ncall \"Dialog without Dialog\", which requires agents to develop visually\ngrounded dialog models that can adapt to new tasks without language level\nsupervision. By factorizing intention and language, our model minimizes\nlinguistic drift after fine-tuning for new tasks. We present qualitative\nresults, automated metrics, and human studies that all show our model can adapt\nto new tasks and maintain language quality. Baselines either fail to perform\nwell at new tasks or experience language drift, becoming unintelligible to\nhumans. Code has been made available at\nhttps://github.com/mcogswell/dialog_without_dialog", "published": "2020-07-24 19:35:57", "link": "http://arxiv.org/abs/2007.12750v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "BabyAI 1.1", "abstract": "The BabyAI platform is designed to measure the sample efficiency of training\nan agent to follow grounded-language instructions. BabyAI 1.0 presents baseline\nresults of an agent trained by deep imitation or reinforcement learning. BabyAI\n1.1 improves the agent's architecture in three minor ways. This increases\nreinforcement learning sample efficiency by up to 3 times and improves\nimitation learning performance on the hardest level from 77 % to 90.4 %. We\nhope that these improvements increase the computational efficiency of BabyAI\nexperiments and help users design better agents.", "published": "2020-07-24 21:19:49", "link": "http://arxiv.org/abs/2007.12770v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Dereverberation using joint estimation of dry speech signal and acoustic\n  system", "abstract": "The purpose of speech dereverberation is to remove quality-degrading effects\nof a time-invariant impulse response filter from the signal. In this report, we\ndescribe an approach to speech dereverberation that involves joint estimation\nof the dry speech signal and of the room impulse response. We explore deep\nlearning models that apply to each task separately, and how these can be\ncombined in a joint model with shared parameters.", "published": "2020-07-24 15:33:08", "link": "http://arxiv.org/abs/2007.12581v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
