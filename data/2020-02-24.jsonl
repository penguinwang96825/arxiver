{"title": "GRET: Global Representation Enhanced Transformer", "abstract": "Transformer, based on the encoder-decoder framework, has achieved\nstate-of-the-art performance on several natural language generation tasks. The\nencoder maps the words in the input sentence into a sequence of hidden states,\nwhich are then fed into the decoder to generate the output sentence. These\nhidden states usually correspond to the input words and focus on capturing\nlocal information. However, the global (sentence level) information is seldom\nexplored, leaving room for the improvement of generation quality. In this\npaper, we propose a novel global representation enhanced Transformer (GRET) to\nexplicitly model global representation in the Transformer network.\nSpecifically, in the proposed model, an external state is generated for the\nglobal representation from the encoder. The global representation is then fused\ninto the decoder during the decoding process to improve generation quality. We\nconduct experiments in two text generation tasks: machine translation and text\nsummarization. Experimental results on four WMT machine translation tasks and\nLCSTS text summarization task demonstrate the effectiveness of the proposed\napproach on natural language generation.", "published": "2020-02-24 07:37:17", "link": "http://arxiv.org/abs/2002.10101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Select Bi-Aspect Information for Document-Scale Text Content\n  Manipulation", "abstract": "In this paper, we focus on a new practical task, document-scale text content\nmanipulation, which is the opposite of text style transfer and aims to preserve\ntext styles while altering the content. In detail, the input is a set of\nstructured records and a reference text for describing another recordset. The\noutput is a summary that accurately describes the partial content in the source\nrecordset with the same writing style of the reference. The task is\nunsupervised due to lack of parallel data, and is challenging to select\nsuitable records and style words from bi-aspect inputs respectively and\ngenerate a high-fidelity long document. To tackle those problems, we first\nbuild a dataset based on a basketball game report corpus as our testbed, and\npresent an unsupervised neural model with interactive attention mechanism,\nwhich is used for learning the semantic relationship between records and\nreference texts to achieve better content transfer and better style\npreservation. In addition, we also explore the effectiveness of the\nback-translation in our task for constructing some pseudo-training pairs.\nEmpirical results show superiority of our approaches over competitive methods,\nand the models also yield a new state-of-the-art result on a sentence-level\ndataset.", "published": "2020-02-24 12:52:10", "link": "http://arxiv.org/abs/2002.10210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine\n  Translation", "abstract": "Transformer-based models have brought a radical change to neural machine\ntranslation. A key feature of the Transformer architecture is the so-called\nmulti-head attention mechanism, which allows the model to focus simultaneously\non different parts of the input. However, recent works have shown that most\nattention heads learn simple, and often redundant, positional patterns. In this\npaper, we propose to replace all but one attention head of each encoder layer\nwith simple fixed -- non-learnable -- attentive patterns that are solely based\non position and do not require any external knowledge. Our experiments with\ndifferent data sizes and multiple language pairs show that fixing the attention\nheads on the encoder side of the Transformer at training time does not impact\nthe translation quality and even increases BLEU scores by up to 3 points in\nlow-resource scenarios.", "published": "2020-02-24 13:53:06", "link": "http://arxiv.org/abs/2002.10260v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Knowledge-Grounded Dialogue Generation", "abstract": "Responding with knowledge has been recognized as an important capability for\nan intelligent conversational agent. Yet knowledge-grounded dialogues, as\ntraining data for learning such a response generation model, are difficult to\nobtain. Motivated by the challenge in practice, we consider knowledge-grounded\ndialogue generation under a natural assumption that only limited training\nexamples are available. In such a low-resource setting, we devise a\ndisentangled response decoder in order to isolate parameters that depend on\nknowledge-grounded dialogues from the entire generation model. By this means,\nthe major part of the model can be learned from a large number of ungrounded\ndialogues and unstructured documents, while the remaining small parameters can\nbe well fitted using the limited training examples. Evaluation results on two\nbenchmarks indicate that with only 1/8 training data, our model can achieve the\nstate-of-the-art performance and generalize well on out-of-domain knowledge.", "published": "2020-02-24 16:20:32", "link": "http://arxiv.org/abs/2002.10348v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Twitter Corpus and Baselines for Evaluating Demographic\n  Bias in Hate Speech Recognition", "abstract": "Existing research on fairness evaluation of document classification models\nmainly uses synthetic monolingual data without ground truth for author\ndemographic attributes. In this work, we assemble and publish a multilingual\nTwitter corpus for the task of hate speech detection with inferred four author\ndemographic factors: age, country, gender and race/ethnicity. The corpus covers\nfive languages: English, Italian, Polish, Portuguese and Spanish. We evaluate\nthe inferred demographic labels with a crowdsourcing platform, Figure Eight. To\nexamine factors that can cause biases, we take an empirical analysis of\ndemographic predictability on the English corpus. We measure the performance of\nfour popular document classifiers and evaluate the fairness and bias of the\nbaseline classifiers on the author-level demographic attributes.", "published": "2020-02-24 16:45:59", "link": "http://arxiv.org/abs/2002.10361v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resources for Turkish Dependency Parsing: Introducing the BOUN Treebank\n  and the BoAT Annotation Tool", "abstract": "In this paper, we introduce the resources that we developed for Turkish\ndependency parsing, which include a novel manually annotated treebank (BOUN\nTreebank), along with the guidelines we adopted, and a new annotation tool\n(BoAT). The manual annotation process we employed was shaped and implemented by\na team of four linguists and five Natural Language Processing (NLP)\nspecialists. Decisions regarding the annotation of the BOUN Treebank were made\nin line with the Universal Dependencies (UD) framework as well as our recent\nefforts for unifying the Turkish UD treebanks through manual re-annotation. To\nthe best of our knowledge, BOUN Treebank is the largest Turkish treebank. It\ncontains a total of 9,761 sentences from various topics including biographical\ntexts, national newspapers, instructional texts, popular culture articles, and\nessays. In addition, we report the parsing results of a state-of-the-art\ndependency parser obtained over the BOUN Treebank as well as two other\ntreebanks in Turkish. Our results demonstrate that the unification of the\nTurkish annotation scheme and the introduction of a more comprehensive treebank\nlead to improved performance with regard to dependency parsing.", "published": "2020-02-24 17:59:11", "link": "http://arxiv.org/abs/2002.10416v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parsing Early Modern English for Linguistic Search", "abstract": "We investigate the question of whether advances in NLP over the last few\nyears make it possible to vastly increase the size of data usable for research\nin historical syntax. This brings together many of the usual tools in NLP -\nword embeddings, tagging, and parsing - in the service of linguistic queries\nover automatically annotated corpora. We train a part-of-speech (POS) tagger\nand parser on a corpus of historical English, using ELMo embeddings trained\nover a billion words of similar text. The evaluation is based on the standard\nmetrics, as well as on the accuracy of the query searches using the parsed\ndata.", "published": "2020-02-24 21:04:51", "link": "http://arxiv.org/abs/2002.10546v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Distributional Thesaurus Embedding for Co-hyponymy Detection", "abstract": "Discriminating lexical relations among distributionally similar words has\nalways been a challenge for natural language processing (NLP) community. In\nthis paper, we investigate whether the network embedding of distributional\nthesaurus can be effectively utilized to detect co-hyponymy relations. By\nextensive experiments over three benchmark datasets, we show that the vector\nrepresentation obtained by applying node2vec on distributional thesaurus\noutperforms the state-of-the-art models for binary classification of\nco-hyponymy vs. hypernymy, as well as co-hyponymy vs. meronymy, by huge\nmargins.", "published": "2020-02-24 20:11:35", "link": "http://arxiv.org/abs/2002.11506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hybrid Approach to Dependency Parsing: Combining Rules and Morphology\n  with Deep Learning", "abstract": "Fully data-driven, deep learning-based models are usually designed as\nlanguage-independent and have been shown to be successful for many natural\nlanguage processing tasks. However, when the studied language is low-resourced\nand the amount of training data is insufficient, these models can benefit from\nthe integration of natural language grammar-based information. We propose two\napproaches to dependency parsing especially for languages with restricted\namount of training data. Our first approach combines a state-of-the-art deep\nlearning-based parser with a rule-based approach and the second one\nincorporates morphological information into the parser. In the rule-based\napproach, the parsing decisions made by the rules are encoded and concatenated\nwith the vector representations of the input words as additional information to\nthe deep network. The morphology-based approach proposes different methods to\ninclude the morphological structure of words into the parser network.\nExperiments are conducted on the IMST-UD Treebank and the results suggest that\nintegration of explicit knowledge about the target language to a neural parser\nthrough a rule-based parsing system and morphological analysis leads to more\naccurate annotations and hence, increases the parsing performance in terms of\nattachment scores. The proposed methods are developed for Turkish, but can be\nadapted to other languages as well.", "published": "2020-02-24 08:34:33", "link": "http://arxiv.org/abs/2002.10116v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "KBSET -- Knowledge-Based Support for Scholarly Editing and Text\n  Processing with Declarative LaTeX Markup and a Core Written in SWI-Prolog", "abstract": "KBSET is an environment that provides support for scholarly editing in two\nflavors: First, as a practical tool KBSET/Letters that accompanies the\ndevelopment of editions of correspondences (in particular from the 18th and\n19th century), completely from source documents to PDF and HTML presentations.\nSecond, as a prototypical tool KBSET/NER for experimentally investigating novel\nforms of working on editions that are centered around automated named entity\nrecognition. KBSET can process declarative application-specific markup that is\nexpressed in LaTeX notation and incorporate large external fact bases that are\ntypically provided in RDF. KBSET includes specially developed LaTeX styles and\na core system that is written in SWI-Prolog, which is used there in many roles,\nutilizing that it realizes the potential of Prolog as a unifying language.", "published": "2020-02-24 15:57:41", "link": "http://arxiv.org/abs/2002.10329v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation", "abstract": "Fine-tuning pre-trained language models like BERT has become an effective way\nin NLP and yields state-of-the-art results on many downstream tasks. Recent\nstudies on adapting BERT to new tasks mainly focus on modifying the model\nstructure, re-designing the pre-train tasks, and leveraging external data and\nknowledge. The fine-tuning strategy itself has yet to be fully explored. In\nthis paper, we improve the fine-tuning of BERT with two effective mechanisms:\nself-ensemble and self-distillation. The experiments on text classification and\nnatural language inference tasks show our proposed methods can significantly\nimprove the adaption of BERT without any external data or knowledge.", "published": "2020-02-24 16:17:12", "link": "http://arxiv.org/abs/2002.10345v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discriminative Adversarial Search for Abstractive Summarization", "abstract": "We introduce a novel approach for sequence decoding, Discriminative\nAdversarial Search (DAS), which has the desirable properties of alleviating the\neffects of exposure bias without requiring external metrics. Inspired by\nGenerative Adversarial Networks (GANs), wherein a discriminator is used to\nimprove the generator, our method differs from GANs in that the generator\nparameters are not updated at training time and the discriminator is only used\nto drive sequence generation at inference time.\n  We investigate the effectiveness of the proposed approach on the task of\nAbstractive Summarization: the results obtained show that a naive application\nof DAS improves over the state-of-the-art methods, with further gains obtained\nvia discriminator retraining. Moreover, we show how DAS can be effective for\ncross-domain adaptation. Finally, all results reported are obtained without\nadditional rule-based filtering strategies, commonly used by the best\nperforming systems available: this indicates that DAS can effectively be\ndeployed without relying on post-hoc modifications of the generated outputs.", "published": "2020-02-24 17:07:32", "link": "http://arxiv.org/abs/2002.10375v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automating Discovery of Dominance in Synchronous Computer-Mediated\n  Communication", "abstract": "With the advent of electronic interaction, dominance (or the assertion of\ncontrol over others) has acquired new dimensions. This study investigates the\ndynamics and characteristics of dominance in virtual interaction by analyzing\nelectronic chat transcripts of groups solving a hidden profile task. We\ninvestigate computer-mediated communication behavior patterns that demonstrate\ndominance and identify a number of relevant variables. These indicators are\ncalculated with automatic and manual coding of text transcripts. A comparison\nof both sets of variables indicates that automatic text analysis methods yield\nsimilar conclusions than manual coding. These findings are encouraging to\nadvance research in text analysis methods in general, and in the study of\nvirtual team dominance in particular.", "published": "2020-02-24 23:07:38", "link": "http://arxiv.org/abs/2002.10582v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Emosaic: Visualizing Affective Content of Text at Varying Granularity", "abstract": "This paper presents Emosaic, a tool for visualizing the emotional tone of\ntext documents, considering multiple dimensions of emotion and varying levels\nof semantic granularity. Emosaic is grounded in psychological research on the\nrelationship between language, affect, and color perception. We capitalize on\nan established three-dimensional model of human emotion: valence (good, nice\nvs. bad, awful), arousal (calm, passive vs. exciting, active) and dominance\n(weak, controlled vs. strong, in control). Previously, multi-dimensional models\nof emotion have been used rarely in visualizations of textual data, due to the\nperceptual challenges involved. Furthermore, until recently most text\nvisualizations remained at a high level, precluding closer engagement with the\ndeep semantic content of the text. Informed by empirical studies, we introduce\na color mapping that translates any point in three-dimensional affective space\ninto a unique color. Emosaic uses affective dictionaries of words annotated\nwith the three emotional parameters of the valence-arousal-dominance model to\nextract emotional meanings from texts and then assigns to them corresponding\ncolor parameters of the hue-saturation-brightness color space. This approach of\nmapping emotion to color is aimed at helping readers to more easily grasp the\nemotional tone of the text. Several features of Emosaic allow readers to\ninteractively explore the affective content of the text in more detail; e.g.,\nin aggregated form as histograms, in sequential form following the order of\ntext, and in detail embedded into the text display itself. Interaction\ntechniques have been included to allow for filtering and navigating of text and\nvisualizations.", "published": "2020-02-24 07:25:01", "link": "http://arxiv.org/abs/2002.10096v1", "categories": ["cs.HC", "cs.CL", "cs.CY", "H.5.2; J.5"], "primary_category": "cs.HC"}
{"title": "Predicting Subjective Features of Questions of QA Websites using BERT", "abstract": "Community Question-Answering websites, such as StackOverflow and Quora,\nexpect users to follow specific guidelines in order to maintain content\nquality. These systems mainly rely on community reports for assessing contents,\nwhich has serious problems such as the slow handling of violations, the loss of\nnormal and experienced users' time, the low quality of some reports, and\ndiscouraging feedback to new users. Therefore, with the overall goal of\nproviding solutions for automating moderation actions in Q&A websites, we aim\nto provide a model to predict 20 quality or subjective aspects of questions in\nQA websites. To this end, we used data gathered by the CrowdSource team at\nGoogle Research in 2019 and a fine-tuned pre-trained BERT model on our problem.\nBased on the evaluation by Mean-Squared-Error (MSE), the model achieved a value\nof 0.046 after 2 epochs of training, which did not improve substantially in the\nnext ones. Results confirm that by simple fine-tuning, we can achieve accurate\nmodels in little time and on less amount of data.", "published": "2020-02-24 07:56:02", "link": "http://arxiv.org/abs/2002.10107v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.7; I.2"], "primary_category": "cs.CL"}
{"title": "FONDUE: A Framework for Node Disambiguation Using Network Embeddings", "abstract": "Real-world data often presents itself in the form of a network. Examples\ninclude social networks, citation networks, biological networks, and knowledge\ngraphs. In their simplest form, networks represent real-life entities (e.g.\npeople, papers, proteins, concepts) as nodes, and describe them in terms of\ntheir relations with other entities by means of edges between these nodes. This\ncan be valuable for a range of purposes from the study of information diffusion\nto bibliographic analysis, bioinformatics research, and question-answering.\n  The quality of networks is often problematic though, affecting downstream\ntasks. This paper focuses on the common problem where a node in the network in\nfact corresponds to multiple real-life entities. In particular, we introduce\nFONDUE, an algorithm based on network embedding for node disambiguation. Given\na network, FONDUE identifies nodes that correspond to multiple entities, for\nsubsequent splitting. Extensive experiments on twelve benchmark datasets\ndemonstrate that FONDUE is substantially and uniformly more accurate for\nambiguous node identification compared to the existing state-of-the-art, at a\ncomparable computational cost, while less optimal for determining the best way\nto split ambiguous nodes.", "published": "2020-02-24 09:34:18", "link": "http://arxiv.org/abs/2002.10127v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Leveraging Code Generation to Improve Code Retrieval and Summarization\n  via Dual Learning", "abstract": "Code summarization generates brief natural language description given a\nsource code snippet, while code retrieval fetches relevant source code given a\nnatural language query. Since both tasks aim to model the association between\nnatural language and programming language, recent studies have combined these\ntwo tasks to improve their performance. However, researchers have yet been able\nto effectively leverage the intrinsic connection between the two tasks as they\ntrain these tasks in a separate or pipeline manner, which means their\nperformance can not be well balanced. In this paper, we propose a novel\nend-to-end model for the two tasks by introducing an additional code generation\ntask. More specifically, we explicitly exploit the probabilistic correlation\nbetween code summarization and code generation with dual learning, and utilize\nthe two encoders for code summarization and code generation to train the code\nretrieval task via multi-task learning. We have carried out extensive\nexperiments on an existing dataset of SQL and Python, and results show that our\nmodel can significantly improve the results of the code retrieval task over\nthe-state-of-art models, as well as achieve competitive performance in terms of\nBLEU score for the code summarization task.", "published": "2020-02-24 12:26:11", "link": "http://arxiv.org/abs/2002.10198v2", "categories": ["cs.IR", "cs.CL", "cs.SE"], "primary_category": "cs.IR"}
{"title": "Semi-Supervised Speech Recognition via Local Prior Matching", "abstract": "For sequence transduction tasks like speech recognition, a strong structured\nprior model encodes rich information about the target space, implicitly ruling\nout invalid sequences by assigning them low probability. In this work, we\npropose local prior matching (LPM), a semi-supervised objective that distills\nknowledge from a strong prior (e.g. a language model) to provide learning\nsignal to a discriminative model trained on unlabeled speech. We demonstrate\nthat LPM is theoretically well-motivated, simple to implement, and superior to\nexisting knowledge distillation techniques under comparable settings. Starting\nfrom a baseline trained on 100 hours of labeled speech, with an additional 360\nhours of unlabeled data, LPM recovers 54% and 73% of the word error rate on\nclean and noisy test sets relative to a fully supervised model on the same\ndata.", "published": "2020-02-24 16:07:11", "link": "http://arxiv.org/abs/2002.10336v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "End-to-End Neural Diarization: Reformulating Speaker Diarization as\n  Simple Multi-label Classification", "abstract": "The most common approach to speaker diarization is clustering of speaker\nembeddings. However, the clustering-based approach has a number of problems;\ni.e., (i) it is not optimized to minimize diarization errors directly, (ii) it\ncannot handle speaker overlaps correctly, and (iii) it has trouble adapting\ntheir speaker embedding models to real audio recordings with speaker overlaps.\nTo solve these problems, we propose the End-to-End Neural Diarization (EEND),\nin which a neural network directly outputs speaker diarization results given a\nmulti-speaker recording. To realize such an end-to-end model, we formulate the\nspeaker diarization problem as a multi-label classification problem and\nintroduce a permutation-free objective function to directly minimize\ndiarization errors. Besides its end-to-end simplicity, the EEND method can\nexplicitly handle speaker overlaps during training and inference. Just by\nfeeding multi-speaker recordings with corresponding speaker segment labels, our\nmodel can be easily adapted to real conversations. We evaluated our method on\nsimulated speech mixtures and real conversation datasets. The results showed\nthat the EEND method outperformed the state-of-the-art x-vector\nclustering-based method, while it correctly handled speaker overlaps. We\nexplored the neural network architecture for the EEND method, and found that\nthe self-attention-based neural network was the key to achieving excellent\nperformance. In contrast to conditioning the network only on its previous and\nnext hidden states, as is done using bidirectional long short-term memory\n(BLSTM), self-attention is directly conditioned on all the frames. By\nvisualizing the attention weights, we show that self-attention captures global\nspeaker characteristics in addition to local speech activity dynamics, making\nit especially suitable for dealing with the speaker diarization problem.", "published": "2020-02-24 14:53:32", "link": "http://arxiv.org/abs/2003.02966v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Distributed Training of Deep Neural Network Acoustic Models for\n  Automatic Speech Recognition", "abstract": "The past decade has witnessed great progress in Automatic Speech Recognition\n(ASR) due to advances in deep learning. The improvements in performance can be\nattributed to both improved models and large-scale training data. Key to\ntraining such models is the employment of efficient distributed learning\ntechniques. In this article, we provide an overview of distributed training\ntechniques for deep neural network acoustic models for ASR. Starting with the\nfundamentals of data parallel stochastic gradient descent (SGD) and ASR\nacoustic modeling, we will investigate various distributed training strategies\nand their realizations in high performance computing (HPC) environments with an\nemphasis on striking the balance between communication and computation.\nExperiments are carried out on a popular public benchmark to study the\nconvergence, speedup and recognition performance of the investigated\nstrategies.", "published": "2020-02-24 19:31:50", "link": "http://arxiv.org/abs/2002.10502v1", "categories": ["cs.DC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.DC"}
