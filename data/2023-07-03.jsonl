{"title": "CollabKG: A Learnable Human-Machine-Cooperative Information Extraction\n  Toolkit for (Event) Knowledge Graph Construction", "abstract": "In order to construct or extend entity-centric and event-centric knowledge\ngraphs (KG and EKG), the information extraction (IE) annotation toolkit is\nessential. However, existing IE toolkits have several non-trivial problems,\nsuch as not supporting multi-tasks, not supporting automatic updates. In this\nwork, we present CollabKG, a learnable human-machine-cooperative IE toolkit for\nKG and EKG construction. Specifically, for the multi-task issue, CollabKG\nunifies different IE subtasks, including named entity recognition (NER),\nentity-relation triple extraction (RE), and event extraction (EE), and supports\nboth KG and EKG. Then, combining advanced prompting-based IE technology, the\nhuman-machine-cooperation mechanism with LLMs as the assistant machine is\npresented which can provide a lower cost as well as a higher performance.\nLastly, owing to the two-way interaction between the human and machine,\nCollabKG with learning ability allows self-renewal. Besides, CollabKG has\nseveral appealing features (e.g., customization, training-free, propagation,\netc.) that make the system powerful, easy-to-use, and high-productivity. We\nholistically compare our toolkit with other existing tools on these features.\nHuman evaluation quantitatively illustrates that CollabKG significantly\nimproves annotation quality, efficiency, and stability simultaneously.", "published": "2023-07-03 06:18:13", "link": "http://arxiv.org/abs/2307.00769v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VOLTA: Improving Generative Diversity by Variational Mutual Information\n  Maximizing Autoencoder", "abstract": "The natural language generation domain has witnessed great success thanks to\nTransformer models. Although they have achieved state-of-the-art generative\nquality, they often neglect generative diversity. Prior attempts to tackle this\nissue suffer from either low model capacity or over-complicated architectures.\nSome recent methods employ the VAE framework to enhance diversity, but their\nlatent variables fully depend on the input context, restricting exploration of\nthe latent space. In this paper, we introduce VOLTA, a framework that elevates\ngenerative diversity by bridging Transformer with VAE via a more effective\ncross-attention-based connection, departing from conventional embedding\nconcatenation or summation. Additionally, we propose integrating InfoGAN-style\nlatent codes to enable input-independent variability, further diversifying the\ngeneration. Moreover, our framework accommodates discrete inputs alongside its\nexisting support for continuous inputs. We perform comprehensive experiments\nwith two types of Transformers on six datasets from three different NLG tasks\nto show that our approach can significantly improve generative diversity while\nmaintaining generative quality.", "published": "2023-07-03 08:45:42", "link": "http://arxiv.org/abs/2307.00852v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Suicide Prevention from Bipolar Disorder with Temporal\n  Symptom-Aware Multitask Learning", "abstract": "Bipolar disorder (BD) is closely associated with an increased risk of\nsuicide. However, while the prior work has revealed valuable insight into\nunderstanding the behavior of BD patients on social media, little attention has\nbeen paid to developing a model that can predict the future suicidality of a BD\npatient. Therefore, this study proposes a multi-task learning model for\npredicting the future suicidality of BD patients by jointly learning current\nsymptoms. We build a novel BD dataset clinically validated by psychiatrists,\nincluding 14 years of posts on bipolar-related subreddits written by 818 BD\npatients, along with the annotations of future suicidality and BD symptoms. We\nalso suggest a temporal symptom-aware attention mechanism to determine which\nsymptoms are the most influential for predicting future suicidality over time\nthrough a sequence of BD posts. Our experiments demonstrate that the proposed\nmodel outperforms the state-of-the-art models in both BD symptom identification\nand future suicidality prediction tasks. In addition, the proposed temporal\nsymptom-aware attention provides interpretable attention weights, helping\nclinicians to apprehend BD patients more comprehensively and to provide timely\nintervention by tracking mental state progression.", "published": "2023-07-03 13:18:55", "link": "http://arxiv.org/abs/2307.00995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating Post-OCR Denoising Complexity on Numerical Texts", "abstract": "Post-OCR processing has significantly improved over the past few years.\nHowever, these have been primarily beneficial for texts consisting of natural,\nalphabetical words, as opposed to documents of numerical nature such as\ninvoices, payslips, medical certificates, etc. To evaluate the OCR\npost-processing difficulty of these datasets, we propose a method to estimate\nthe denoising complexity of a text and evaluate it on several datasets of\nvarying nature, and show that texts of numerical nature have a significant\ndisadvantage. We evaluate the estimated complexity ranking with respect to the\nerror rates of modern-day denoising approaches to show the validity of our\nestimator.", "published": "2023-07-03 13:49:14", "link": "http://arxiv.org/abs/2307.01020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Multiple-Choice Reading and Listening Comprehension Tests", "abstract": "Multiple-choice reading and listening comprehension tests are an important\npart of language assessment. Content creators for standard educational tests\nneed to carefully curate questions that assess the comprehension abilities of\ncandidates taking the tests. However, recent work has shown that a large number\nof questions in general multiple-choice reading comprehension datasets can be\nanswered without comprehension, by leveraging world knowledge instead. This\nwork investigates how much of a contextual passage needs to be read in\nmultiple-choice reading based on conversation transcriptions and listening\ncomprehension tests to be able to work out the correct answer. We find that\nautomated reading comprehension systems can perform significantly better than\nrandom with partial or even no access to the context passage. These findings\noffer an approach for content creators to automatically capture the trade-off\nbetween comprehension and world knowledge required for their proposed\nquestions.", "published": "2023-07-03 14:55:02", "link": "http://arxiv.org/abs/2307.01076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Evolution of Substance Use Coverage in the Philadelphia Inquirer", "abstract": "The media's representation of illicit substance use can lead to harmful\nstereotypes and stigmatization for individuals struggling with addiction,\nultimately influencing public perception, policy, and public health outcomes.\nTo explore how the discourse and coverage of illicit drug use changed over\ntime, this study analyzes 157,476 articles published in the Philadelphia\nInquirer over a decade. Specifically, the study focuses on articles that\nmentioned at least one commonly abused substance, resulting in a sample of\n3,903 articles. Our analysis shows that cannabis and narcotics are the most\nfrequently discussed classes of drugs. Hallucinogenic drugs are portrayed more\npositively than other categories, whereas narcotics are portrayed the most\nnegatively. Our research aims to highlight the need for accurate and inclusive\nportrayals of substance use and addiction in the media.", "published": "2023-07-03 19:09:04", "link": "http://arxiv.org/abs/2307.01299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Cross-Lingual Transfer Learning in Spoken Named Entity\n  Recognition Systems", "abstract": "Recent Named Entity Recognition (NER) advancements have significantly\nenhanced text classification capabilities. This paper focuses on spoken NER,\naimed explicitly at spoken document retrieval, an area not widely studied due\nto the lack of comprehensive datasets for spoken contexts. Additionally, the\npotential for cross-lingual transfer learning in low-resource situations\ndeserves further investigation. In our study, we applied transfer learning\ntechniques across Dutch, English, and German using both pipeline and End-to-End\n(E2E) approaches. We employed Wav2Vec2 XLS-R models on custom pseudo-annotated\ndatasets to evaluate the adaptability of cross-lingual systems. Our exploration\nof different architectural configurations assessed the robustness of these\nsystems in spoken NER. Results showed that the E2E model was superior to the\npipeline model, particularly with limited annotation resources. Furthermore,\ntransfer learning from German to Dutch improved performance by 7% over the\nstandalone Dutch E2E system and 4% over the Dutch pipeline model. Our findings\nhighlight the effectiveness of cross-lingual transfer in spoken NER and\nemphasize the need for additional data collection to improve these systems.", "published": "2023-07-03 19:30:24", "link": "http://arxiv.org/abs/2307.01310v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Language Models are not Multicultural: A Case Study in\n  Emotion", "abstract": "Emotions are experienced and expressed differently across the world. In order\nto use Large Language Models (LMs) for multilingual tasks that require\nemotional sensitivity, LMs must reflect this cultural variation in emotion. In\nthis study, we investigate whether the widely-used multilingual LMs in 2023\nreflect differences in emotional expressions across cultures and languages. We\nfind that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,\nand generative LMs (e.g., ChatGPT) reflect Western norms, even when responding\nto prompts in other languages. Our results show that multilingual LMs do not\nsuccessfully learn the culturally appropriate nuances of emotion and we\nhighlight possible research directions towards correcting this.", "published": "2023-07-03 21:54:28", "link": "http://arxiv.org/abs/2307.01370v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALBERTI, a Multilingual Domain Specific Language Model for Poetry\n  Analysis", "abstract": "The computational analysis of poetry is limited by the scarcity of tools to\nautomatically analyze and scan poems. In a multilingual settings, the problem\nis exacerbated as scansion and rhyme systems only exist for individual\nlanguages, making comparative studies very challenging and time consuming. In\nthis work, we present \\textsc{Alberti}, the first multilingual pre-trained\nlarge language model for poetry. Through domain-specific pre-training (DSP), we\nfurther trained multilingual BERT on a corpus of over 12 million verses from 12\nlanguages. We evaluated its performance on two structural poetry tasks: Spanish\nstanza type classification, and metrical pattern prediction for Spanish,\nEnglish and German. In both cases, \\textsc{Alberti} outperforms multilingual\nBERT and other transformers-based models of similar sizes, and even achieves\nstate-of-the-art results for German when compared to rule-based systems,\ndemonstrating the feasibility and effectiveness of DSP in the poetry domain.", "published": "2023-07-03 22:50:53", "link": "http://arxiv.org/abs/2307.01387v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning Improves Performance In Deep Argument Mining Models", "abstract": "The successful analysis of argumentative techniques from user-generated text\nis central to many downstream tasks such as political and market analysis.\nRecent argument mining tools use state-of-the-art deep learning methods to\nextract and annotate argumentative techniques from various online text corpora,\nhowever each task is treated as separate and different bespoke models are\nfine-tuned for each dataset. We show that different argument mining tasks share\ncommon semantic and logical structure by implementing a multi-task approach to\nargument mining that achieves better performance than state-of-the-art methods\nfor the same problems. Our model builds a shared representation of the input\ntext that is common to all tasks and exploits similarities between tasks in\norder to further boost performance via parameter-sharing. Our results are\nimportant for argument mining as they show that different tasks share\nsubstantial similarities and suggest a holistic approach to the extraction of\nargumentative techniques from text.", "published": "2023-07-03 23:42:29", "link": "http://arxiv.org/abs/2307.01401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Shutdown Avoidance of Language Models in Textual Scenarios", "abstract": "Recently, there has been an increase in interest in evaluating large language\nmodels for emergent and dangerous capabilities. Importantly, agents could\nreason that in some scenarios their goal is better achieved if they are not\nturned off, which can lead to undesirable behaviors. In this paper, we\ninvestigate the potential of using toy textual scenarios to evaluate\ninstrumental reasoning and shutdown avoidance in language models such as GPT-4\nand Claude. Furthermore, we explore whether shutdown avoidance is merely a\nresult of simple pattern matching between the dataset and the prompt or if it\nis a consistent behaviour across different environments and variations.\n  We evaluated behaviours manually and also experimented with using language\nmodels for automatic evaluations, and these evaluations demonstrate that simple\npattern matching is likely not the sole contributing factor for shutdown\navoidance. This study provides insights into the behaviour of language models\nin shutdown avoidance scenarios and inspires further research on the use of\ntextual scenarios for evaluations.", "published": "2023-07-03 07:05:59", "link": "http://arxiv.org/abs/2307.00787v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UniFine: A Unified and Fine-grained Approach for Zero-shot\n  Vision-Language Understanding", "abstract": "Vision-language tasks, such as VQA, SNLI-VE, and VCR are challenging because\nthey require the model's reasoning ability to understand the semantics of the\nvisual world and natural language. Supervised methods working for\nvision-language tasks have been well-studied. However, solving these tasks in a\nzero-shot setting is less explored. Since Contrastive Language-Image\nPre-training (CLIP) has shown remarkable zero-shot performance on image-text\nmatching, previous works utilized its strong zero-shot ability by converting\nvision-language tasks into an image-text matching problem, and they mainly\nconsider global-level matching (e.g., the whole image or sentence). However, we\nfind visual and textual fine-grained information, e.g., keywords in the\nsentence and objects in the image, can be fairly informative for semantics\nunderstanding. Inspired by this, we propose a unified framework to take\nadvantage of the fine-grained information for zero-shot vision-language\nlearning, covering multiple tasks such as VQA, SNLI-VE, and VCR. Our\nexperiments show that our framework outperforms former zero-shot methods on VQA\nand achieves substantial improvement on SNLI-VE and VCR. Furthermore, our\nablation studies confirm the effectiveness and generalizability of our proposed\nmethod.", "published": "2023-07-03 09:03:12", "link": "http://arxiv.org/abs/2307.00862v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mining Clues from Incomplete Utterance: A Query-enhanced Network for\n  Incomplete Utterance Rewriting", "abstract": "Incomplete utterance rewriting has recently raised wide attention. However,\nprevious works do not consider the semantic structural information between\nincomplete utterance and rewritten utterance or model the semantic structure\nimplicitly and insufficiently. To address this problem, we propose a\nQUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly\nbrings guided semantic structural knowledge between the incomplete utterance\nand the rewritten utterance making model perceive where to refer back to or\nrecover omitted tokens. Then, we adopt a fast and effective edit operation\nscoring network to model the relation between two tokens. Benefiting from\nproposed query template and the well-designed edit operation scoring network,\nQUEEN achieves state-of-the-art performance on several public datasets.", "published": "2023-07-03 09:08:06", "link": "http://arxiv.org/abs/2307.00866v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Node-weighted Graph Convolutional Network for Depression Detection in\n  Transcribed Clinical Interviews", "abstract": "We propose a simple approach for weighting self-connecting edges in a Graph\nConvolutional Network (GCN) and show its impact on depression detection from\ntranscribed clinical interviews. To this end, we use a GCN for modeling\nnon-consecutive and long-distance semantics to classify the transcriptions into\ndepressed or control subjects. The proposed method aims to mitigate the\nlimiting assumptions of locality and the equal importance of self-connections\nvs. edges to neighboring nodes in GCNs, while preserving attractive features\nsuch as low computational cost, data agnostic, and interpretability\ncapabilities. We perform an exhaustive evaluation in two benchmark datasets.\nResults show that our approach consistently outperforms the vanilla GCN model\nas well as previously reported results, achieving an F1=0.84 on both datasets.\nFinally, a qualitative analysis illustrates the interpretability capabilities\nof the proposed approach and its alignment with previous findings in\npsychology.", "published": "2023-07-03 10:44:07", "link": "http://arxiv.org/abs/2307.00920v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Design of Semantic Similarity Ensembles Using Grammatical\n  Evolution", "abstract": "Semantic similarity measures are widely used in natural language processing\nto catalyze various computer-related tasks. However, no single semantic\nsimilarity measure is the most appropriate for all tasks, and researchers often\nuse ensemble strategies to ensure performance. This research work proposes a\nmethod for automatically designing semantic similarity ensembles. In fact, our\nproposed method uses grammatical evolution, for the first time, to\nautomatically select and aggregate measures from a pool of candidates to create\nan ensemble that maximizes correlation to human judgment. The method is\nevaluated on several benchmark datasets and compared to state-of-the-art\nensembles, showing that it can significantly improve similarity assessment\naccuracy and outperform existing methods in some cases. As a result, our\nresearch demonstrates the potential of using grammatical evolution to\nautomatically compare text and prove the benefits of using ensembles for\nsemantic similarity tasks. The source code that illustrates our approach can be\ndownloaded from https://github.com/jorge-martinez-gil/sesige.", "published": "2023-07-03 10:53:05", "link": "http://arxiv.org/abs/2307.00925v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Challenges in Domain-Specific Abstractive Summarization and How to\n  Overcome them", "abstract": "Large Language Models work quite well with general-purpose data and many\ntasks in Natural Language Processing. However, they show several limitations\nwhen used for a task such as domain-specific abstractive text summarization.\nThis paper identifies three of those limitations as research problems in the\ncontext of abstractive text summarization: 1) Quadratic complexity of\ntransformer-based models with respect to the input text length; 2) Model\nHallucination, which is a model's ability to generate factually incorrect text;\nand 3) Domain Shift, which happens when the distribution of the model's\ntraining and test corpus is not the same. Along with a discussion of the open\nresearch questions, this paper also provides an assessment of existing\nstate-of-the-art techniques relevant to domain-specific text summarization to\naddress the research gaps.", "published": "2023-07-03 12:26:44", "link": "http://arxiv.org/abs/2307.00963v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Instruction Tuning with Polite Flamingo", "abstract": "Recent research has demonstrated that the multi-task fine-tuning of\nmulti-modal Large Language Models (LLMs) using an assortment of annotated\ndownstream vision-language datasets significantly enhances their performance.\nYet, during this process, a side effect, which we termed as the \"multi-modal\nalignment tax\", surfaces. This side effect negatively impacts the model's\nability to format responses appropriately -- for instance, its \"politeness\" --\ndue to the overly succinct and unformatted nature of raw annotations, resulting\nin reduced human preference. In this paper, we introduce Polite Flamingo, a\nmulti-modal response rewriter that transforms raw annotations into a more\nappealing, \"polite\" format. Polite Flamingo is trained to reconstruct\nhigh-quality responses from their automatically distorted counterparts and is\nsubsequently applied to a vast array of vision-language datasets for response\nrewriting. After rigorous filtering, we generate the PF-1M dataset and further\nvalidate its value by fine-tuning a multi-modal LLM with it. Combined with\nnovel methodologies including U-shaped multi-stage tuning and multi-turn\naugmentation, the resulting model, Clever Flamingo, demonstrates its advantages\nin both multi-modal understanding and response politeness according to\nautomated and human evaluations.", "published": "2023-07-03 13:37:00", "link": "http://arxiv.org/abs/2307.01003v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction", "abstract": "In the current digitalization era, capturing and effectively representing\nknowledge is crucial in most real-world scenarios. In this context, knowledge\ngraphs represent a potent tool for retrieving and organizing a vast amount of\ninformation in a properly interconnected and interpretable structure. However,\ntheir generation is still challenging and often requires considerable human\neffort and domain expertise, hampering the scalability and flexibility across\ndifferent application fields. This paper proposes an innovative knowledge graph\ngeneration approach that leverages the potential of the latest generative large\nlanguage models, such as GPT-3.5, that can address all the main critical issues\nin knowledge graph building. The approach is conveyed in a pipeline that\ncomprises novel iterative zero-shot and external knowledge-agnostic strategies\nin the main stages of the generation process. Our unique manifold approach may\nencompass significant benefits to the scientific community. In particular, the\nmain contribution can be summarized by: (i) an innovative strategy for\niteratively prompting large language models to extract relevant components of\nthe final graph; (ii) a zero-shot strategy for each prompt, meaning that there\nis no need for providing examples for \"guiding\" the prompt result; (iii) a\nscalable solution, as the adoption of LLMs avoids the need for any external\nresources or human expertise. To assess the effectiveness of our proposed\nmodel, we performed experiments on a dataset that covered a specific domain. We\nclaim that our proposal is a suitable solution for scalable and versatile\nknowledge graph construction and may be applied to different and novel\ncontexts.", "published": "2023-07-03 16:01:45", "link": "http://arxiv.org/abs/2307.01128v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the In-context Learning Ability of Large Language Model for\n  Biomedical Concept Linking", "abstract": "The biomedical field relies heavily on concept linking in various areas such\nas literature mining, graph alignment, information retrieval,\nquestion-answering, data, and knowledge integration. Although large language\nmodels (LLMs) have made significant strides in many natural language processing\ntasks, their effectiveness in biomedical concept mapping is yet to be fully\nexplored. This research investigates a method that exploits the in-context\nlearning (ICL) capabilities of large models for biomedical concept linking. The\nproposed approach adopts a two-stage retrieve-and-rank framework. Initially,\nbiomedical concepts are embedded using language models, and then embedding\nsimilarity is utilized to retrieve the top candidates. These candidates'\ncontextual information is subsequently incorporated into the prompt and\nprocessed by a large language model to re-rank the concepts. This approach\nachieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7%\nin chemical entity normalization, exhibiting a competitive performance relative\nto supervised learning methods. Further, it showed a significant improvement,\nwith an over 20-point absolute increase in F1 score on an oncology matching\ndataset. Extensive qualitative assessments were conducted, and the benefits and\npotential shortcomings of using large language models within the biomedical\ndomain were discussed. were discussed.", "published": "2023-07-03 16:19:50", "link": "http://arxiv.org/abs/2307.01137v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Trainable Transformer in Transformer", "abstract": "Recent works attribute the capability of in-context learning (ICL) in large\npre-trained language models to implicitly simulating and fine-tuning an\ninternal model (e.g., linear or 2-layer MLP) during inference. However, such\nconstructions require large memory overhead, which makes simulation of more\nsophisticated internal models intractable. In this work, we propose an\nefficient construction, Transformer in Transformer (in short, TinT), that\nallows a transformer to simulate and fine-tune complex models internally during\ninference (e.g., pre-trained language models). In particular, we introduce\ninnovative approximation techniques that allow a TinT model with less than 2\nbillion parameters to simulate and fine-tune a 125 million parameter\ntransformer model within a single forward pass. TinT accommodates many common\ntransformer variants and its design ideas also improve the efficiency of past\ninstantiations of simple models inside transformers. We conduct end-to-end\nexperiments to validate the internal fine-tuning procedure of TinT on various\nlanguage modeling and downstream tasks. For example, even with a limited\none-step budget, we observe TinT for a OPT-125M model improves performance by\n4-16% absolute on average compared to OPT-125M. These findings suggest that\nlarge pre-trained language models are capable of performing intricate\nsubroutines. To facilitate further work, a modular and extensible codebase for\nTinT is included.", "published": "2023-07-03 17:53:39", "link": "http://arxiv.org/abs/2307.01189v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Shiftable Context: Addressing Training-Inference Context Mismatch in\n  Simultaneous Speech Translation", "abstract": "Transformer models using segment-based processing have been an effective\narchitecture for simultaneous speech translation. However, such models create a\ncontext mismatch between training and inference environments, hindering\npotential translation accuracy. We solve this issue by proposing Shiftable\nContext, a simple yet effective scheme to ensure that consistent segment and\ncontext sizes are maintained throughout training and inference, even with the\npresence of partially filled segments due to the streaming nature of\nsimultaneous translation. Shiftable Context is also broadly applicable to\nsegment-based transformers for streaming tasks. Our experiments on the\nEnglish-German, English-French, and English-Spanish language pairs from the\nMUST-C dataset demonstrate that when applied to the Augmented Memory\nTransformer, a state-of-the-art model for simultaneous speech translation, the\nproposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU\nscores across each wait-k value for the three language pairs, respectively,\nwith a minimal impact on computation-aware Average Lagging.", "published": "2023-07-03 22:11:51", "link": "http://arxiv.org/abs/2307.01377v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Implicit Memory Transformer for Computationally Efficient Simultaneous\n  Speech Translation", "abstract": "Simultaneous speech translation is an essential communication task difficult\nfor humans whereby a translation is generated concurrently with oncoming speech\ninputs. For such a streaming task, transformers using block processing to break\nan input sequence into segments have achieved state-of-the-art performance at a\nreduced cost. Current methods to allow information to propagate across\nsegments, including left context and memory banks, have faltered as they are\nboth insufficient representations and unnecessarily expensive to compute. In\nthis paper, we propose an Implicit Memory Transformer that implicitly retains\nmemory through a new left context method, removing the need to explicitly\nrepresent memory with memory banks. We generate the left context from the\nattention output of the previous segment and include it in the keys and values\nof the current segment's attention calculation. Experiments on the MuST-C\ndataset show that the Implicit Memory Transformer provides a substantial\nspeedup on the encoder forward pass with nearly identical translation quality\nwhen compared with the state-of-the-art approach that employs both left context\nand memory banks.", "published": "2023-07-03 22:20:21", "link": "http://arxiv.org/abs/2307.01381v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty\n  Using Model Souping on the Example of Check-Worthiness Classification", "abstract": "This paper describes the second-placed approach developed by the Fraunhofer\nSIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text\nsnippet from a political debate, the aim of this task is to determine whether\nit should be assessed for check-worthiness. Detecting check-worthy statements\naims to facilitate manual fact-checking efforts by prioritizing the claims that\nfact-checkers should consider first. It can also be considered as primary step\nof a fact-checking system. Our best-performing method took advantage of an\nensemble classification scheme centered on Model Souping. When applied to the\nEnglish data set, our submitted model achieved an overall F1 score of 0.878 and\nwas ranked as the second-best model in the competition.", "published": "2023-07-03 09:27:46", "link": "http://arxiv.org/abs/2307.02377v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An End-to-End Multi-Module Audio Deepfake Generation System for ADD\n  Challenge 2023", "abstract": "The task of synthetic speech generation is to generate language content from\na given text, then simulating fake human voice.The key factors that determine\nthe effect of synthetic speech generation mainly include speed of generation,\naccuracy of word segmentation, naturalness of synthesized speech, etc. This\npaper builds an end-to-end multi-module synthetic speech generation model,\nincluding speaker encoder, synthesizer based on Tacotron2, and vocoder based on\nWaveRNN. In addition, we perform a lot of comparative experiments on different\ndatasets and various model structures. Finally, we won the first place in the\nADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of\n44.97%.", "published": "2023-07-03 03:21:23", "link": "http://arxiv.org/abs/2307.00729v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multilingual Contextual Adapters To Improve Custom Word Recognition In\n  Low-resource Languages", "abstract": "Connectionist Temporal Classification (CTC) models are popular for their\nbalance between speed and performance for Automatic Speech Recognition (ASR).\nHowever, these CTC models still struggle in other areas, such as\npersonalization towards custom words. A recent approach explores Contextual\nAdapters, wherein an attention-based biasing model for CTC is used to improve\nthe recognition of custom entities. While this approach works well with enough\ndata, we showcase that it isn't an effective strategy for low-resource\nlanguages. In this work, we propose a supervision loss for smoother training of\nthe Contextual Adapters. Further, we explore a multilingual strategy to improve\nperformance with limited training data. Our method achieves 48% F1 improvement\nin retrieving unseen custom entities for a low-resource language.\nInterestingly, as a by-product of training the Contextual Adapters, we see a\n5-11% Word Error Rate (WER) reduction in the performance of the base CTC model\nas well.", "published": "2023-07-03 05:29:38", "link": "http://arxiv.org/abs/2307.00759v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph\n  Reading", "abstract": "While state-of-the-art Text-to-Speech systems can generate natural speech of\nvery high quality at sentence level, they still meet great challenges in speech\ngeneration for paragraph / long-form reading. Such deficiencies are due to i)\nignorance of cross-sentence contextual information, and ii) high computation\nand memory cost for long-form synthesis. To address these issues, this work\ndevelops a lightweight yet effective TTS system, ContextSpeech. Specifically,\nwe first design a memory-cached recurrence mechanism to incorporate global text\nand speech context into sentence encoding. Then we construct\nhierarchically-structured textual semantics to broaden the scope for global\ncontext enhancement. Additionally, we integrate linearized self-attention to\nimprove model efficiency. Experiments show that ContextSpeech significantly\nimproves the voice quality and prosody expressiveness in paragraph reading with\ncompetitive model efficiency. Audio samples are available at:\nhttps://contextspeech.github.io/demo/", "published": "2023-07-03 06:55:03", "link": "http://arxiv.org/abs/2307.00782v2", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Data-Driven Information Extraction and Enrichment of Molecular Profiling\n  Data for Cancer Cell Lines", "abstract": "With the proliferation of research means and computational methodologies,\npublished biomedical literature is growing exponentially in numbers and volume.\nCancer cell lines are frequently used models in biological and medical research\nthat are currently applied for a wide range of purposes, from studies of\ncellular mechanisms to drug development, which has led to a wealth of related\ndata and publications. Sifting through large quantities of text to gather\nrelevant information on the cell lines of interest is tedious and extremely\nslow when performed by humans. Hence, novel computational information\nextraction and correlation mechanisms are required to boost meaningful\nknowledge extraction. In this work, we present the design, implementation and\napplication of a novel data extraction and exploration system. This system\nextracts deep semantic relations between textual entities from scientific\nliterature to enrich existing structured clinical data in the domain of cancer\ncell lines. We introduce a new public data exploration portal, which enables\nautomatic linking of genomic copy number variants plots with ranked, related\nentities such as affected genes. Each relation is accompanied by\nliterature-derived evidences, allowing for deep, yet rapid, literature search,\nusing existing structured data as a springboard. Our system is publicly\navailable on the web at https://cancercelllines.org", "published": "2023-07-03 11:15:42", "link": "http://arxiv.org/abs/2307.00933v2", "categories": ["cs.CL", "cs.CE", "cs.DB"], "primary_category": "cs.CL"}
{"title": "SCITUNE: Aligning Large Language Models with Scientific Multimodal\n  Instructions", "abstract": "Instruction finetuning is a popular paradigm to align large language models\n(LLM) with human intent. Despite its popularity, this idea is less explored in\nimproving the LLMs to align existing foundation models with scientific\ndisciplines, concepts and goals. In this work, we present SciTune as a tuning\nframework to improve the ability of LLMs to follow scientific multimodal\ninstructions. To test our methodology, we use a human-generated scientific\ninstruction tuning dataset and train a large multimodal model LLaMA-SciTune\nthat connects a vision encoder and LLM for science-focused visual and language\nunderstanding. In comparison to the models that are finetuned with machine\ngenerated data only, LLaMA-SciTune surpasses human performance on average and\nin many sub-categories on the ScienceQA benchmark.", "published": "2023-07-03 16:25:49", "link": "http://arxiv.org/abs/2307.01139v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving Language Plasticity via Pretraining with Active Forgetting", "abstract": "Pretrained language models (PLMs) are today the primary model for natural\nlanguage processing. Despite their impressive downstream performance, it can be\ndifficult to apply PLMs to new languages, a barrier to making their\ncapabilities universally accessible. While prior work has shown it possible to\naddress this issue by learning a new embedding layer for the new language,\ndoing so is both data and compute inefficient. We propose to use an active\nforgetting mechanism during pretraining, as a simple way of creating PLMs that\ncan quickly adapt to new languages. Concretely, by resetting the embedding\nlayer every K updates during pretraining, we encourage the PLM to improve its\nability of learning new embeddings within a limited number of updates, similar\nto a meta-learning effect. Experiments with RoBERTa show that models pretrained\nwith our forgetting mechanism not only demonstrate faster convergence during\nlanguage adaptation but also outperform standard ones in a low-data regime,\nparticularly for languages that are distant from English.", "published": "2023-07-03 17:12:44", "link": "http://arxiv.org/abs/2307.01163v3", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Interpretability and Transparency-Driven Detection and Transformation of\n  Textual Adversarial Examples (IT-DT)", "abstract": "Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have\nshown impressive performance in NLP. However, their vulnerability to\nadversarial examples poses a security risk. Existing defense methods lack\ninterpretability, making it hard to understand adversarial classifications and\nidentify model vulnerabilities. To address this, we propose the\nInterpretability and Transparency-Driven Detection and Transformation (IT-DT)\nframework. It focuses on interpretability and transparency in detecting and\ntransforming textual adversarial examples. IT-DT utilizes techniques like\nattention maps, integrated gradients, and model feedback for interpretability\nduring detection. This helps identify salient features and perturbed words\ncontributing to adversarial classifications. In the transformation phase, IT-DT\nuses pre-trained embeddings and model feedback to generate optimal replacements\nfor perturbed words. By finding suitable substitutions, we aim to convert\nadversarial examples into non-adversarial counterparts that align with the\nmodel's intended behavior while preserving the text's meaning. Transparency is\nemphasized through human expert involvement. Experts review and provide\nfeedback on detection and transformation results, enhancing decision-making,\nespecially in complex scenarios. The framework generates insights and threat\nintelligence empowering analysts to identify vulnerabilities and improve model\nrobustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in\ndetecting and transforming adversarial examples. The approach enhances\ninterpretability, provides transparency, and enables accurate identification\nand successful transformation of adversarial inputs. By combining technical\nanalysis and human expertise, IT-DT significantly improves the resilience and\ntrustworthiness of transformer-based text classifiers against adversarial\nattacks.", "published": "2023-07-03 03:17:20", "link": "http://arxiv.org/abs/2307.01225v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language and Text-to-3D Models for Engineering Design Optimization", "abstract": "The current advances in generative AI for learning large neural network\nmodels with the capability to produce essays, images, music and even 3D assets\nfrom text prompts create opportunities for a manifold of disciplines. In the\npresent paper, we study the potential of deep text-to-3D models in the\nengineering domain, with focus on the chances and challenges when integrating\nand interacting with 3D assets in computational simulation-based design\noptimization. In contrast to traditional design optimization of 3D geometries\nthat often searches for the optimum designs using numerical representations,\nsuch as B-Spline surface or deformation parameters in vehicle aerodynamic\noptimization, natural language challenges the optimization framework by\nrequiring a different interpretation of variation operators while at the same\ntime may ease and motivate the human user interaction. Here, we propose and\nrealize a fully automated evolutionary design optimization framework using\nShap-E, a recently published text-to-3D asset network by OpenAI, in the context\nof aerodynamic vehicle optimization. For representing text prompts in the\nevolutionary optimization, we evaluate (a) a bag-of-words approach based on\nprompt templates and Wordnet samples, and (b) a tokenisation approach based on\nprompt templates and the byte pair encoding method from GPT4. Our main findings\nfrom the optimizations indicate that, first, it is important to ensure that the\ndesigns generated from prompts are within the object class of application, i.e.\ndiverse and novel designs need to be realistic, and, second, that more research\nis required to develop methods where the strength of text prompt variations and\nthe resulting variations of the 3D designs share causal relations to some\ndegree to improve the optimization.", "published": "2023-07-03 07:54:09", "link": "http://arxiv.org/abs/2307.01230v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Semantic enrichment towards efficient speech representations", "abstract": "Over the past few years, self-supervised learned speech representations have\nemerged as fruitful replacements for conventional surface representations when\nsolving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual\nmodels trained on massive textual data were introduced to encode language\nagnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make\nprofit from such textual models to enrich multilingual speech representations\nwith language agnostic semantics. By aiming for better semantic extraction on a\nchallenging Spoken Language Understanding task and in consideration with\ncomputation costs, this study investigates a specific in-domain semantic\nenrichment of the SAMU-XLSR model by specializing it on a small amount of\ntranscribed data from the downstream task. In addition, we show the benefits of\nthe use of same-domain French and Italian benchmarks for low-resource language\nportability and explore cross-domain capacities of the enriched SAMU-XLSR.", "published": "2023-07-03 19:52:56", "link": "http://arxiv.org/abs/2307.01323v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Shifting Attention to Relevance: Towards the Predictive Uncertainty\n  Quantification of Free-Form Large Language Models", "abstract": "Large Language Models (LLMs) show promising results in language generation\nand instruction following but frequently \"hallucinate\", making their outputs\nless reliable. Despite Uncertainty Quantification's (UQ) potential solutions,\nimplementing it accurately within LLMs is challenging. Our research introduces\na simple heuristic: not all tokens in auto-regressive LLM text equally\nrepresent the underlying meaning, as \"linguistic redundancy\" often allows a few\nkeywords to convey the essence of long sentences. However, current methods\nunderestimate this inequality when assessing uncertainty, causing tokens with\nlimited semantics to be equally or excessively weighted in UQ. To correct this,\nwe propose Shifting Attention to more Relevant (SAR) components at both token-\nand sentence-levels for better UQ. We conduct extensive experiments involving a\nrange of popular \"off-the-shelf\" LLMs, such as Vicuna, WizardLM, and\nLLaMA-2-chat, with model sizes extending up to 33B parameters. We evaluate\nvarious free-form question-answering tasks, encompassing domains such as\nreading comprehension, science Q&A, and medical Q&A. Our experimental results,\ncoupled with a comprehensive demographic analysis, demonstrate the superior\nperformance of SAR. The code is available at https://github.com/jinhaoduan/SAR.", "published": "2023-07-03 22:17:16", "link": "http://arxiv.org/abs/2307.01379v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translating scientific Latin texts with artificial intelligence: the\n  works of Euler and contemporaries", "abstract": "The major hindrance in the study of earlier scientific literature is the\navailability of Latin translations into modern languages. This is particular\ntrue for the works of Euler who authored about 850 manuscripts and wrote a\nthousand letters and received back almost two thousand more. The translation of\nmany of these manuscripts, books and letters have been published in various\nsources over the last two centuries, but many more have not yet appeared.\nFortunately, nowadays, artificial intelligence (AI) translation can be used to\ncircumvent the challenges of translating such substantial number of texts. To\nvalidate this tool, benchmark tests have been performed to compare the\nperformance of two popular AI translating algorithms, namely Google Translate\nand ChatGPT. Additional tests were accomplished in translating an excerpt of a\n1739 letter from Johann Bernoulli to Euler, where he announces that he was\nsending Euler the first part of his manuscript Hydraulica. Overall, the\ncomparative results show that ChatGPT performed better that Google Translate\nnot only in the benchmark tests but also in the translation of this letter,\nhighlighting the superiority of ChatGPT as a translation tool, catering not\nonly to general Latin practitioners but also proving beneficial for specialized\nLatin translators.", "published": "2023-07-03 16:27:32", "link": "http://arxiv.org/abs/2307.07520v2", "categories": ["math.HO", "cs.CL", "physics.ed-ph"], "primary_category": "math.HO"}
{"title": "vONTSS: vMF based semi-supervised neural topic modeling with optimal\n  transport", "abstract": "Recently, Neural Topic Models (NTM), inspired by variational autoencoders,\nhave attracted a lot of research interest; however, these methods have limited\napplications in the real world due to the challenge of incorporating human\nknowledge. This work presents a semi-supervised neural topic modeling method,\nvONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and\noptimal transport. When a few keywords per topic are provided, vONTSS in the\nsemi-supervised setting generates potential topics and optimizes topic-keyword\nquality and topic classification. Experiments show that vONTSS outperforms\nexisting semi-supervised topic modeling methods in classification accuracy and\ndiversity. vONTSS also supports unsupervised topic modeling. Quantitative and\nqualitative experiments show that vONTSS in the unsupervised setting\noutperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered\nand coherent topics on benchmark datasets. It is also much faster than the\nstate-of-the-art weakly supervised text classification method while achieving\nsimilar classification performance. We further prove the equivalence of optimal\ntransport loss and cross-entropy loss at the global minimum.", "published": "2023-07-03 04:23:41", "link": "http://arxiv.org/abs/2307.01226v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Spatial-temporal Graph Based Multi-channel Speaker Verification With\n  Ad-hoc Microphone Arrays", "abstract": "The performance of speaker verification degrades significantly in adverse\nacoustic environments with strong reverberation and noise. To address this\nissue, this paper proposes a spatial-temporal graph convolutional network (GCN)\nmethod for the multi-channel speaker verification with ad-hoc microphone\narrays. It includes a feature aggregation block and a channel selection block,\nboth of which are built on graphs. The feature aggregation block fuses speaker\nfeatures among different time and channels by a spatial-temporal GCN. The\ngraph-based channel selection block discards the noisy channels that may\ncontribute negatively to the system. The proposed method is flexible in\nincorporating various kinds of graphs and prior knowledge. We compared the\nproposed method with six representative methods in both real-world and\nsimulated environments.\n  Experimental results show that the proposed method achieves a relative equal\nerror rate (EER) reduction of $\\mathbf{15.39\\%}$ lower than the strongest\nreferenced method in the simulated datasets, and $\\mathbf{17.70\\%}$ lower than\nthe latter in the real datasets. Moreover, its performance is robust across\ndifferent signal-to-noise ratios and reverberation time.", "published": "2023-07-03 22:49:25", "link": "http://arxiv.org/abs/2307.01386v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "musif: a Python package for symbolic music feature extraction", "abstract": "In this work, we introduce musif, a Python package that facilitates the\nautomatic extraction of features from symbolic music scores. The package\nincludes the implementation of a large number of features, which have been\ndeveloped by a team of experts in musicology, music theory, statistics, and\ncomputer science. Additionally, the package allows for the easy creation of\ncustom features using commonly available Python libraries. musif is primarily\ngeared towards processing high-quality musicological data encoded in MusicXML\nformat, but also supports other formats commonly used in music information\nretrieval tasks, including MIDI, MEI, Kern, and others. We provide\ncomprehensive documentation and tutorials to aid in the extension of the\nframework and to facilitate the introduction of new and inexperienced users to\nits usage.", "published": "2023-07-03 15:49:15", "link": "http://arxiv.org/abs/2307.01120v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AVSegFormer: Audio-Visual Segmentation with Transformer", "abstract": "The combination of audio and vision has long been a topic of interest in the\nmulti-modal community. Recently, a new audio-visual segmentation (AVS) task has\nbeen introduced, aiming to locate and segment the sounding objects in a given\nvideo. This task demands audio-driven pixel-level scene understanding for the\nfirst time, posing significant challenges. In this paper, we propose\nAVSegFormer, a novel framework for AVS tasks that leverages the transformer\narchitecture. Specifically, we introduce audio queries and learnable queries\ninto the transformer decoder, enabling the network to selectively attend to\ninterested visual features. Besides, we present an audio-visual mixer, which\ncan dynamically adjust visual features by amplifying relevant and suppressing\nirrelevant spatial channels. Additionally, we devise an intermediate mask loss\nto enhance the supervision of the decoder, encouraging the network to produce\nmore accurate intermediate predictions. Extensive experiments demonstrate that\nAVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is\navailable at https://github.com/vvvb-github/AVSegFormer.", "published": "2023-07-03 16:37:10", "link": "http://arxiv.org/abs/2307.01146v4", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting\n  Self-Supervised Representations", "abstract": "Significant progress has been made in speaker dependent Lip-to-Speech\nsynthesis, which aims to generate speech from silent videos of talking faces.\nCurrent state-of-the-art approaches primarily employ non-autoregressive\nsequence-to-sequence architectures to directly predict mel-spectrograms or\naudio waveforms from lip representations. We hypothesize that the direct\nmel-prediction hampers training/model efficiency due to the entanglement of\nspeech content with ambient information and speaker characteristics. To this\nend, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis.\nFirst, a non-autoregressive sequence-to-sequence model maps self-supervised\nvisual features to a representation of disentangled speech content. A vocoder\nthen converts the speech features into raw waveforms. Extensive evaluations\nconfirm the effectiveness of our setup, achieving state-of-the-art performance\non the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT\ndatasets. Speech samples from RobustL2S can be found at\nhttps://neha-sherin.github.io/RobustL2S/", "published": "2023-07-03 09:13:57", "link": "http://arxiv.org/abs/2307.01233v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EmoGen: Eliminating Subjective Bias in Emotional Music Generation", "abstract": "Music is used to convey emotions, and thus generating emotional music is\nimportant in automatic music generation. Previous work on emotional music\ngeneration directly uses annotated emotion labels as control signals, which\nsuffers from subjective bias: different people may annotate different emotions\non the same music, and one person may feel different emotions under different\nsituations. Therefore, directly mapping emotion labels to music sequences in an\nend-to-end way would confuse the learning process and hinder the model from\ngenerating music with general emotions. In this paper, we propose EmoGen, an\nemotional music generation system that leverages a set of emotion-related music\nattributes as the bridge between emotion and music, and divides the generation\ninto two stages: emotion-to-attribute mapping with supervised clustering, and\nattribute-to-music generation with self-supervised learning. Both stages are\nbeneficial: in the first stage, the attribute values around the clustering\ncenter represent the general emotions of these samples, which help eliminate\nthe impacts of the subjective bias of emotion labels; in the second stage, the\ngeneration is completely disentangled from emotion labels and thus free from\nthe subjective bias. Both subjective and objective evaluations show that EmoGen\noutperforms previous methods on emotion control accuracy and music quality\nrespectively, which demonstrate our superiority in generating emotional music.\nMusic samples generated by EmoGen are available via this\nlink:https://ai-muzic.github.io/emogen/, and the code is available at this\nlink:https://github.com/microsoft/muzic/.", "published": "2023-07-03 05:54:29", "link": "http://arxiv.org/abs/2307.01229v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
