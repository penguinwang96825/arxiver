{"title": "Text Generation with Exemplar-based Adaptive Decoding", "abstract": "We propose a novel conditioned text generation model. It draws inspiration\nfrom traditional template-based text generation techniques, where the source\nprovides the content (i.e., what to say), and the template influences how to\nsay it. Building on the successful encoder-decoder paradigm, it first encodes\nthe content representation from the given input text; to produce the output, it\nretrieves exemplar text from the training data as \"soft templates,\" which are\nthen used to construct an exemplar-specific decoder. We evaluate the proposed\nmodel on abstractive text summarization and data-to-text generation. Empirical\nresults show that this model achieves strong performance and outperforms\ncomparable baselines.", "published": "2019-04-09 02:34:30", "link": "http://arxiv.org/abs/1904.04428v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion\n  Recognition", "abstract": "In this paper, we address three challenges in utterance-level emotion\nrecognition in dialogue systems: (1) the same word can deliver different\nemotions in different contexts; (2) some emotions are rarely seen in general\ndialogues; (3) long-range contextual information is hard to be effectively\ncaptured. We therefore propose a hierarchical Gated Recurrent Unit (HiGRU)\nframework with a lower-level GRU to model the word-level inputs and an\nupper-level GRU to capture the contexts of utterance-level embeddings.\nMoreover, we promote the framework to two variants, HiGRU with individual\nfeatures fusion (HiGRU-f) and HiGRU with self-attention and features fusion\n(HiGRU-sf), so that the word/utterance-level individual inputs and the\nlong-range contextual information can be sufficiently utilized. Experiments on\nthree dialogue emotion datasets, IEMOCAP, Friends, and EmotionPush demonstrate\nthat our proposed HiGRU models attain at least 8.7%, 7.5%, 6.0% improvement\nover the state-of-the-art methods on each dataset, respectively. Particularly,\nby utilizing only the textual feature in IEMOCAP, our HiGRU models gain at\nleast 3.8% improvement over the state-of-the-art conversational memory network\n(CMN) with the trimodal features of text, video, and audio.", "published": "2019-04-09 03:25:53", "link": "http://arxiv.org/abs/1904.04446v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixing syntagmatic and paradigmatic information for concept detection", "abstract": "In the last decades, philosophers have begun using empirical data for\nconceptual analysis, but corpus-based conceptual analysis has so far failed to\ndevelop, in part because of the absence of reliable methods to automatically\ndetect concepts in textual data. Previous attempts have shown that topic models\ncan constitute efficient concept detection heuristics, but while they leverage\nthe syntagmatic relations in a corpus, they fail to exploit paradigmatic\nrelations, and thus probably fail to model concepts accurately. In this\narticle, we show that using a topic model that models concepts on a space of\nword embeddings (Hu and Tsujii, 2016) can lead to significant increases in\nconcept detection performance, as well as enable the target concept to be\nexpressed in more flexible ways using word vectors.", "published": "2019-04-09 04:27:31", "link": "http://arxiv.org/abs/1904.04461v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Needs Words? Lexicon-Free Speech Recognition", "abstract": "Lexicon-free speech recognition naturally deals with the problem of\nout-of-vocabulary (OOV) words. In this paper, we show that character-based\nlanguage models (LM) can perform as well as word-based LMs for speech\nrecognition, in word error rates (WER), even without restricting the decoding\nto a lexicon. We study character-based LMs and show that convolutional LMs can\neffectively leverage large (character) contexts, which is key for good speech\nrecognition performance downstream. We specifically show that the lexicon-free\ndecoding performance (WER) on utterances with OOV words using character-based\nLMs is better than lexicon-based decoding, both with character or word-based\nLMs.", "published": "2019-04-09 06:06:54", "link": "http://arxiv.org/abs/1904.04479v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Decoding Model For Spoken Language Understanding From\n  Unaligned Data", "abstract": "Spoken language understanding (SLU) systems can be trained on two types of\nlabelled data: aligned or unaligned. Unaligned data do not require word by word\nannotation and is easier to be obtained. In the paper, we focus on spoken\nlanguage understanding from unaligned data whose annotation is a set of\nact-slot-value triples. Previous works usually focus on improve slot-value pair\nprediction and estimate dialogue act types separately, which ignores the\nhierarchical structure of the act-slot-value triples. Here, we propose a novel\nhierarchical decoding model which dynamically parses act, slot and value in a\nstructured way and employs pointer network to handle out-of-vocabulary (OOV)\nvalues. Experiments on DSTC2 dataset, a benchmark unaligned dataset, show that\nthe proposed model not only outperforms previous state-of-the-art model, but\nalso can be generalized effectively and efficiently to unseen act-slot type\npairs and OOV values.", "published": "2019-04-09 07:26:25", "link": "http://arxiv.org/abs/1904.04498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "APE at Scale and its Implications on MT Evaluation Biases", "abstract": "In this work, we train an Automatic Post-Editing (APE) model and use it to\nreveal biases in standard Machine Translation (MT) evaluation procedures. The\ngoal of our APE model is to correct typical errors introduced by the\ntranslation process, and convert the \"translationese\" output into natural text.\nOur APE model is trained entirely on monolingual data that has been round-trip\ntranslated through English, to mimic errors that are similar to the ones\nintroduced by NMT. We apply our model to the output of existing NMT systems,\nand demonstrate that, while the human-judged quality improves in all cases,\nBLEU scores drop with forward-translated test sets. We verify these results for\nthe WMT18 English to German, WMT15 English to French, and WMT16 English to\nRomanian tasks. Furthermore, we selectively apply our APE model on the output\nof the top submissions of the most recent WMT evaluation campaigns. We see\nquality improvements on all tasks of up to 2.5 BLEU points.", "published": "2019-04-09 17:10:15", "link": "http://arxiv.org/abs/1904.04790v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quizbowl: The Case for Incremental Question Answering", "abstract": "Scholastic trivia competitions test knowledge and intelligence through\nmastery of question answering. Modern question answering benchmarks are one\nvariant of the Turing test. Specifically, answering a set of questions as well\nas a human is a minimum bar towards demonstrating human-like intelligence. This\npaper makes the case that the format of one competition -- where participants\ncan answer in the middle of hearing a question (incremental) -- better\ndifferentiates the skill between (human or machine) players. Additionally,\nmerging a sequential decision-making sub-task with question answering (QA)\nprovides a good setting for research in model calibration and opponent\nmodeling. Thus, embedded in this task are three machine learning challenges:\n(1) factoid QA over thousands of Wikipedia-like answers, (2) calibration of the\nQA model's confidence scores, and (3) sequential decision-making that\nincorporates knowledge of the QA model, its calibration, and what the opponent\nmay do. We make two contributions: (1) collecting and curating a large factoid\nQA dataset and an accompanying gameplay dataset, and (2) developing a model\nthat addresses these three machine learning challenges. In addition to offline\nevaluation, we pitted our model against some of the most accomplished trivia\nplayers in the world in a series of exhibition matches spanning several years.\nThroughout this paper, we show that collaborations with the vibrant trivia\ncommunity have contributed to the quality of our dataset, spawned new research\ndirections, and doubled as an exciting way to engage the public with research\nin machine learning and natural language processing.", "published": "2019-04-09 17:13:36", "link": "http://arxiv.org/abs/1904.04792v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance Monitoring for End-to-End Speech Recognition", "abstract": "Measuring performance of an automatic speech recognition (ASR) system without\nground-truth could be beneficial in many scenarios, especially with data from\nunseen domains, where performance can be highly inconsistent. In conventional\nASR systems, several performance monitoring (PM) techniques have been\nwell-developed to monitor performance by looking at tri-phone posteriors or\npre-softmax activations from neural network acoustic modeling. However,\nstrategies for monitoring more recently developed end-to-end ASR systems have\nnot yet been explored, and so that is the focus of this paper. We adapt\nprevious PM measures (Entropy, M-measure and Auto-encoder) and apply our\nproposed RNN predictor in the end-to-end setting. These measures utilize the\ndecoder output layer and attention probability vectors, and their predictive\npower is measured with simple linear models. Our findings suggest that\ndecoder-level features are more feasible and informative than attention-level\nprobabilities for PM measures, and that M-measure on the decoder posteriors\nachieves the best overall predictive performance with an average prediction\nerror 8.8%. Entropy measures and RNN-based prediction also show competitive\npredictability, especially for unseen conditions.", "published": "2019-04-09 20:35:25", "link": "http://arxiv.org/abs/1904.04896v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Selection with Cluster-Based Language Difference Models and Cynical\n  Selection", "abstract": "We present and apply two methods for addressing the problem of selecting\nrelevant training data out of a general pool for use in tasks such as machine\ntranslation. Building on existing work on class-based language difference\nmodels, we first introduce a cluster-based method that uses Brown clusters to\ncondense the vocabulary of the corpora. Secondly, we implement the cynical data\nselection method, which incrementally constructs a training corpus to\nefficiently model the task corpus. Both the cluster-based and the cynical data\nselection approaches are used for the first time within a machine translation\nsystem, and we perform a head-to-head comparison. Our intrinsic evaluations\nshow that both new methods outperform the standard Moore-Lewis approach\n(cross-entropy difference), in terms of better perplexity and OOV rates on\nin-domain data. The cynical approach converges much quicker, covering nearly\nall of the in-domain vocabulary with 84% less data than the other methods.\nFurthermore, the new approaches can be used to select machine translation\ntraining data for training better systems. Our results confirm that class-based\nselection using Brown clusters is a viable alternative to POS-based class-based\nmethods, and removes the reliance on a part-of-speech tagger. Additionally, we\nare able to validate the recently proposed cynical data selection method,\nshowing that its performance in SMT models surpasses that of traditional\ncross-entropy difference methods and more closely matches the sentence length\nof the task corpus.", "published": "2019-04-09 20:39:07", "link": "http://arxiv.org/abs/1904.04900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-based Multi-instance Neural Network for Medical Diagnosis from\n  Incomplete and Low Quality Data", "abstract": "One way to extract patterns from clinical records is to consider each patient\nrecord as a bag with various number of instances in the form of symptoms.\nMedical diagnosis is to discover informative ones first and then map them to\none or more diseases. In many cases, patients are represented as vectors in\nsome feature space and a classifier is applied after to generate diagnosis\nresults. However, in many real-world cases, data is often of low-quality due to\na variety of reasons, such as data consistency, integrity, completeness,\naccuracy, etc. In this paper, we propose a novel approach, attention based\nmulti-instance neural network (AMI-Net), to make the single disease\nclassification only based on the existing and valid information in the\nreal-world outpatient records. In the context of a patient, it takes a bag of\ninstances as input and output the bag label directly in end-to-end way.\nEmbedding layer is adopted at the beginning, mapping instances into an\nembedding space which represents the individual patient condition. The\ncorrelations among instances and their importance for the final classification\nare captured by multi-head attention transformer, instance-level multi-instance\npooling and bag-level multi-instance pooling. The proposed approach was test on\ntwo non-standardized and highly imbalanced datasets, one in the Traditional\nChinese Medicine (TCM) domain and the other in the Western Medicine (WM)\ndomain. Our preliminary results show that the proposed approach outperforms all\nbaselines results by a significant margin.", "published": "2019-04-09 04:21:52", "link": "http://arxiv.org/abs/1904.04460v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Graph-based Model for Joint Chinese Word Segmentation and Dependency\n  Parsing", "abstract": "Chinese word segmentation and dependency parsing are two fundamental tasks\nfor Chinese natural language processing. The dependency parsing is defined on\nword-level. Therefore word segmentation is the precondition of dependency\nparsing, which makes dependency parsing suffer from error propagation and\nunable to directly make use of the character-level pre-trained language model\n(such as BERT). In this paper, we propose a graph-based model to integrate\nChinese word segmentation and dependency parsing. Different from previous\ntransition-based joint models, our proposed model is more concise, which\nresults in fewer efforts of feature engineering. Our graph-based joint model\nachieves better performance than previous joint models and state-of-the-art\nresults in both Chinese word segmentation and dependency parsing. Besides, when\nBERT is combined, our model can substantially reduce the performance gap of\ndependency parsing between joint models and gold-segmented word-based models.\nOur code is publicly available at https://github.com/fastnlp/JointCwsParser.", "published": "2019-04-09 14:25:17", "link": "http://arxiv.org/abs/1904.04697v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Seq2Biseq: Bidirectional Output-wise Recurrent Neural Networks for\n  Sequence Modelling", "abstract": "During the last couple of years, Recurrent Neural Networks (RNN) have reached\nstate-of-the-art performances on most of the sequence modelling problems. In\nparticular, the \"sequence to sequence\" model and the neural CRF have proved to\nbe very effective in this domain. In this article, we propose a new RNN\narchitecture for sequence labelling, leveraging gated recurrent layers to take\narbitrarily long contexts into account, and using two decoders operating\nforward and backward. We compare several variants of the proposed solution and\ntheir performances to the state-of-the-art. Most of our results are better than\nthe state-of-the-art or very close to it and thanks to the use of recent\ntechnologies, our architecture can scale on corpora larger than those used in\nthis work.", "published": "2019-04-09 15:33:59", "link": "http://arxiv.org/abs/1904.04733v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bilingual-GAN: A Step Towards Parallel Text Generation", "abstract": "Latent space based GAN methods and attention based sequence to sequence\nmodels have achieved impressive results in text generation and unsupervised\nmachine translation respectively. Leveraging the two domains, we propose an\nadversarial latent space based model capable of generating parallel sentences\nin two languages concurrently and translating bidirectionally. The bilingual\ngeneration goal is achieved by sampling from the latent space that is shared\nbetween both languages. First two denoising autoencoders are trained, with\nshared encoders and back-translation to enforce a shared latent state between\nthe two languages. The decoder is shared for the two translation directions.\nNext, a GAN is trained to generate synthetic \"code\" mimicking the languages'\nshared latent space. This code is then fed into the decoder to generate text in\neither language. We perform our experiments on Europarl and Multi30k datasets,\non the English-French language pair, and document our performance using both\nsupervised and unsupervised machine translation.", "published": "2019-04-09 15:42:08", "link": "http://arxiv.org/abs/1904.04742v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Self-Attention Network for Referring Image Segmentation", "abstract": "We consider the problem of referring image segmentation. Given an input image\nand a natural language expression, the goal is to segment the object referred\nby the language expression in the image. Existing works in this area treat the\nlanguage expression and the input image separately in their representations.\nThey do not sufficiently capture long-range correlations between these two\nmodalities. In this paper, we propose a cross-modal self-attention (CMSA)\nmodule that effectively captures the long-range dependencies between linguistic\nand visual features. Our model can adaptively focus on informative words in the\nreferring expression and important regions in the input image. In addition, we\npropose a gated multi-level fusion module to selectively integrate\nself-attentive cross-modal features corresponding to different levels in the\nimage. This module controls the information flow of features at different\nlevels. We validate the proposed approach on four evaluation datasets. Our\nproposed approach consistently outperforms existing state-of-the-art methods.", "published": "2019-04-09 15:51:07", "link": "http://arxiv.org/abs/1904.04745v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Characterizing the impact of geometric properties of word embeddings on\n  task performance", "abstract": "Analysis of word embedding properties to inform their use in downstream NLP\ntasks has largely been studied by assessing nearest neighbors. However,\ngeometric properties of the continuous feature space contribute directly to the\nuse of embedding features in downstream models, and are largely unexplored. We\nconsider four properties of word embedding geometry, namely: position relative\nto the origin, distribution of features in the vector space, global pairwise\ndistances, and local pairwise distances. We define a sequence of\ntransformations to generate new embeddings that expose subsets of these\nproperties to downstream models and evaluate change in task performance to\nunderstand the contribution of each property to NLP models. We transform\npublicly available pretrained embeddings from three popular toolkits (word2vec,\nGloVe, and FastText) and evaluate on a variety of intrinsic tasks, which model\nlinguistic information in the vector space, and extrinsic tasks, which use\nvectors as input to machine learning models. We find that intrinsic evaluations\nare highly sensitive to absolute position, while extrinsic tasks rely primarily\non local similarity. Our findings suggest that future embedding models and\npost-processing techniques should focus primarily on similarity to nearby\npoints in vector space.", "published": "2019-04-09 18:53:00", "link": "http://arxiv.org/abs/1904.04866v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parallel Hardware for Faster Morphological Analysis", "abstract": "Morphological analysis in the Arabic language is computationally intensive,\nhas numerous forms and rules, and is intrinsically parallel. The investigation\npresented in this paper confirms that the effective development of parallel\nalgorithms and the derivation of corresponding processors in hardware enable\nimplementations with appealing performance characteristics. The presented\ndevelopments of parallel hardware comprise the application of a variety of\nalgorithm modelling techniques, strategies for concurrent processing, and the\ncreation of pioneering hardware implementations that target modern programmable\ndevices. The investigation includes the creation of a linguistic-based stemmer\nfor Arabic verb root extraction with extended infix processing to attain\nhigh-levels of accuracy. The implementations comprise three versions, namely,\nsoftware, non-pipelined processor, and pipelined processor with high\nthroughput. The targeted systems are high-performance multi-core processors for\nsoftware implementations and high-end Field Programmable Gate Array systems for\nhardware implementations. The investigation includes a thorough evaluation of\nthe methodology, and performance and accuracy analyses of the developed\nsoftware and hardware implementations. The pipelined processor achieved a\nsignificant speedup of 5571.4 over the software implementation. The developed\nstemmer for verb root extraction with infix processing attained accuracies of\n87% and 90.7% for analyzing the texts of the Holy Quran and its Chapter 29 -\nSurat Al-Ankabut.", "published": "2019-04-09 23:46:52", "link": "http://arxiv.org/abs/1904.07148v1", "categories": ["cs.DC", "cs.CL", "B.1.1; B.1.2; B.2.2; B.5.1"], "primary_category": "cs.DC"}
{"title": "Knowledge-Augmented Language Model and its Application to Unsupervised\n  Named-Entity Recognition", "abstract": "Traditional language models are unable to efficiently model entity names\nobserved in text. All but the most popular named entities appear infrequently\nin text providing insufficient context. Recent efforts have recognized that\ncontext can be generalized between entity names that share the same type (e.g.,\n\\emph{person} or \\emph{location}) and have equipped language models with access\nto an external knowledge base (KB). Our Knowledge-Augmented Language Model\n(KALM) continues this line of work by augmenting a traditional model with a KB.\nUnlike previous methods, however, we train with an end-to-end predictive\nobjective optimizing the perplexity of text. We do not require any additional\ninformation such as named entity tags. In addition to improving language\nmodeling performance, KALM learns to recognize named entities in an entirely\nunsupervised way by using entity type information latent in the model. On a\nNamed Entity Recognition (NER) task, KALM achieves performance comparable with\nstate-of-the-art supervised models. Our work demonstrates that named entities\n(and possibly other types of world knowledge) can be modeled successfully using\npredictive learning and training on large corpora of text without any\nadditional information.", "published": "2019-04-09 04:09:45", "link": "http://arxiv.org/abs/1904.04458v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS", "abstract": "The end-to-end TTS, which can predict speech directly from a given sequence\nof graphemes or phonemes, has shown improved performance over the conventional\nTTS. However, its predicting capability is still limited by the\nacoustic/phonetic coverage of the training data, usually constrained by the\ntraining set size. To further improve the TTS quality in pronunciation, prosody\nand perceived naturalness, we propose to exploit the information embedded in a\nsyntactically parsed tree where the inter-phrase/word information of a sentence\nis organized in a multilevel tree structure. Specifically, two key features:\nphrase structure and relations between adjacent words are investigated.\nExperimental results in subjective listening, measured on three test sets, show\nthat the proposed approach is effective to improve the pronunciation clarity,\nprosody and naturalness of the synthesized speech of the baseline system.", "published": "2019-04-09 16:20:52", "link": "http://arxiv.org/abs/1904.04764v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "A New GAN-based End-to-End TTS Training Algorithm", "abstract": "End-to-end, autoregressive model-based TTS has shown significant performance\nimprovements over the conventional one. However, the autoregressive module\ntraining is affected by the exposure bias, or the mismatch between the\ndifferent distributions of real and predicted data. While real data is\navailable in training, but in testing, only predicted data is available to feed\nthe autoregressive module. By introducing both real and generated data\nsequences in training, we can alleviate the effects of the exposure bias. We\npropose to use Generative Adversarial Network (GAN) along with the key idea of\nProfessor Forcing in training. A discriminator in GAN is jointly trained to\nequalize the difference between real and predicted data. In AB subjective\nlistening test, the results show that the new approach is preferred over the\nstandard transfer learning with a CMOS improvement of 0.1. Sentence level\nintelligibility tests show significant improvement in a pathological test set.\nThe GAN-trained new model is also more stable than the baseline to produce\nbetter alignments for the Tacotron output.", "published": "2019-04-09 16:37:35", "link": "http://arxiv.org/abs/1904.04775v1", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Exploring Uncertainty Measures for Image-Caption Embedding-and-Retrieval\n  Task", "abstract": "With the wide development of black-box machine learning algorithms,\nparticularly deep neural network (DNN), the practical demand for the\nreliability assessment is rapidly rising. On the basis of the concept that\n`Bayesian deep learning knows what it does not know,' the uncertainty of DNN\noutputs has been investigated as a reliability measure for the classification\nand regression tasks. However, in the image-caption retrieval task, well-known\nsamples are not always easy-to-retrieve samples. This study investigates two\naspects of image-caption embedding-and-retrieval systems. On one hand, we\nquantify feature uncertainty by considering image-caption embedding as a\nregression task, and use it for model averaging, which can improve the\nretrieval performance. On the other hand, we further quantify posterior\nuncertainty by considering the retrieval as a classification task, and use it\nas a reliability measure, which can greatly improve the retrieval performance\nby rejecting uncertain queries. The consistent performance of two uncertainty\nmeasures is observed with different datasets (MS COCO and Flickr30k), different\ndeep learning architectures (dropout and batch normalization), and different\nsimilarity functions.", "published": "2019-04-09 12:19:09", "link": "http://arxiv.org/abs/1904.08504v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Progressive Speech Enhancement with Residual Connections", "abstract": "This paper studies the Speech Enhancement based on Deep Neural Networks. The\nproposed architecture gradually follows the signal transformation during\nenhancement by means of a visualization probe at each network block. Alongside\nthe process, the enhancement performance is visually inspected and evaluated in\nterms of regression cost. This progressive scheme is based on Residual\nNetworks. During the process, we investigate a residual connection with a\nconstant number of channels, including internal state between blocks, and\nadding progressive supervision. The insights provided by the interpretation of\nthe network enhancement process leads us to design an improved architecture for\nthe enhancement purpose. Following this strategy, we are able to obtain speech\nenhancement results beyond the state-of-the-art, achieving a favorable\ntrade-off between dereverberation and the amount of spectral distortion.", "published": "2019-04-09 08:04:45", "link": "http://arxiv.org/abs/1904.04511v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Probability density distillation with generative adversarial networks\n  for high-quality parallel waveform generation", "abstract": "This paper proposes an effective probability density distillation (PDD)\nalgorithm for WaveNet-based parallel waveform generation (PWG) systems.\nRecently proposed teacher-student frameworks in the PWG system have\nsuccessfully achieved a real-time generation of speech signals. However, the\ndifficulties optimizing the PDD criteria without auxiliary losses result in\nquality degradation of synthesized speech. To generate more natural speech\nsignals within the teacher-student framework, we propose a novel optimization\ncriterion based on generative adversarial networks (GANs). In the proposed\nmethod, the inverse autoregressive flow-based student model is incorporated as\na generator in the GAN framework, and jointly optimized by the PDD mechanism\nwith the proposed adversarial learning method. As this process encourages the\nstudent to model the distribution of realistic speech waveform, the perceptual\nquality of the synthesized speech becomes much more natural. Our experimental\nresults verify that the PWG systems with the proposed method outperform both\nthose using conventional approaches, and also autoregressive generation systems\nwith a well-trained teacher WaveNet.", "published": "2019-04-09 05:38:18", "link": "http://arxiv.org/abs/1904.04472v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ensemble Models for Spoofing Detection in Automatic Speaker Verification", "abstract": "Detecting spoofing attempts of automatic speaker verification (ASV) systems\nis challenging, especially when using only one modeling approach. For\nrobustness, we use both deep neural networks and traditional machine learning\nmodels and combine them as ensemble models through logistic regression. They\nare trained to detect logical access (LA) and physical access (PA) attacks on\nthe dataset released as part of the ASV Spoofing and Countermeasures Challenge\n2019. We propose dataset partitions that ensure different attack types are\npresent during training and validation to improve system robustness. Our\nensemble model outperforms all our single models and the baselines from the\nchallenge for both attack types. We investigate why some models on the PA\ndataset strongly outperform others and find that spoofed recordings in the\ndataset tend to have longer silences at the end than genuine ones. By removing\nthem, the PA task becomes much more challenging, with the tandem detection cost\nfunction (t-DCF) of our best single model rising from 0.1672 to 0.5018 and\nequal error rate (EER) increasing from 5.98% to 19.8% on the development set.", "published": "2019-04-09 11:10:26", "link": "http://arxiv.org/abs/1904.04589v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement with Wide Residual Networks in Reverberant\n  Environments", "abstract": "This paper proposes a speech enhancement method which exploits the high\npotential of residual connections in a Wide Residual Network architecture. This\nis supported on single dimensional convolutions computed alongside the time\ndomain, which is a powerful approach to process contextually correlated\nrepresentations through the temporal domain, such as speech feature sequences.\nWe find the residual mechanism extremely useful for the enhancement task since\nthe signal always has a linear shortcut and the non-linear path enhances it in\nseveral steps by adding or subtracting corrections. The enhancement capability\nof the proposal is assessed by objective quality metrics evaluated with\nsimulated and real samples of reverberated speech signals. Results show that\nthe proposal outperforms the state-of-the-art method called WPE, which is known\nto effectively reduce reverberation and greatly enhance the signal. The\nproposed model, trained with artificial synthesized reverberation data, was\nable to generalize to real room impulse responses for a variety of conditions\n(e.g. different room sizes, $RT_{60}$, near & far field). Furthermore, it\nachieves accuracy for real speech with reverberation from two different\ndatasets.", "published": "2019-04-09 07:46:25", "link": "http://arxiv.org/abs/1904.05167v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CycleGAN-VC2: Improved CycleGAN-based Non-parallel Voice Conversion", "abstract": "Non-parallel voice conversion (VC) is a technique for learning the mapping\nfrom source to target speech without relying on parallel data. This is an\nimportant task, but it has been challenging due to the disadvantages of the\ntraining conditions. Recently, CycleGAN-VC has provided a breakthrough and\nperformed comparably to a parallel VC method without relying on any extra data,\nmodules, or time alignment procedures. However, there is still a large gap\nbetween the real target and converted speech, and bridging this gap remains a\nchallenge. To reduce this gap, we propose CycleGAN-VC2, which is an improved\nversion of CycleGAN-VC incorporating three new techniques: an improved\nobjective (two-step adversarial losses), improved generator (2-1-2D CNN), and\nimproved discriminator (PatchGAN). We evaluated our method on a non-parallel VC\ntask and analyzed the effect of each technique in detail. An objective\nevaluation showed that these techniques help bring the converted feature\nsequence closer to the target in terms of both global and local structures,\nwhich we assess by using Mel-cepstral distortion and modulation spectra\ndistance, respectively. A subjective evaluation showed that CycleGAN-VC2\noutperforms CycleGAN-VC in terms of naturalness and similarity for every\nspeaker pair, including intra-gender and inter-gender pairs.", "published": "2019-04-09 12:55:35", "link": "http://arxiv.org/abs/1904.04631v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection", "abstract": "ASVspoof, now in its third edition, is a series of community-led challenges\nwhich promote the development of countermeasures to protect automatic speaker\nverification (ASV) from the threat of spoofing. Advances in the 2019 edition\ninclude: (i) a consideration of both logical access (LA) and physical access\n(PA) scenarios and the three major forms of spoofing attack, namely synthetic,\nconverted and replayed speech; (ii) spoofing attacks generated with\nstate-of-the-art neural acoustic and waveform models; (iii) an improved,\ncontrolled simulation of replay attacks; (iv) use of the tandem detection cost\nfunction (t-DCF) that reflects the impact of both spoofing and countermeasures\nupon ASV reliability. Even if ASV remains the core focus, in retaining the\nequal error rate (EER) as a secondary metric, ASYspoof also embraces the\ngrowing importance of fake audio detection. ASVspoof 2019 attracted the\nparticipation of 63 research teams, with more than half of these reporting\nsystems that improve upon the performance of two baseline spoofing\ncountermeasures. This paper describes the 2019 database, protocols and\nchallenge results. It also outlines major findings which demonstrate the real\nprogress made in protecting against the threat of spoofing and fake audio.", "published": "2019-04-09 15:12:52", "link": "http://arxiv.org/abs/1904.05441v2", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
