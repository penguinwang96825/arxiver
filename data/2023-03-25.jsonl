{"title": "GPT is becoming a Turing machine: Here are some ways to program it", "abstract": "We demonstrate that, through appropriate prompting, GPT-3 family of models\ncan be triggered to perform iterative behaviours necessary to execute (rather\nthan just write or recall) programs that involve loops, including several\npopular algorithms found in computer science curricula or software developer\ninterviews. We trigger execution and description of Iterations by Regimenting\nSelf-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong\nrepetitive structure in an example of an execution path of a target program for\none particular input, 2) Prompting with fragments of execution paths, and 3)\nExplicitly forbidding (skipping) self-attention to parts of the generated text.\nOn a dynamic program execution, IRSA leads to larger accuracy gains than\nreplacing the model with the much more powerful GPT-4. IRSA has promising\napplications in education, as the prompts and responses resemble student\nassignments in data structures and algorithms classes. Our findings hold\nimplications for evaluating LLMs, which typically target the in-context\nlearning: We show that prompts that may not even cover one full task example\ncan trigger algorithmic behaviour, allowing solving problems previously thought\nof as hard for LLMs, such as logical puzzles. Consequently, prompt design plays\nan even more critical role in LLM performance than previously recognized.", "published": "2023-03-25 00:43:41", "link": "http://arxiv.org/abs/2303.14310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Backdoor Attacks with Input-unique Triggers in NLP", "abstract": "Backdoor attack aims at inducing neural models to make incorrect predictions\nfor poison data while keeping predictions on the clean dataset unchanged, which\ncreates a considerable threat to current natural language processing (NLP)\nsystems. Existing backdoor attacking systems face two severe issues:firstly,\nmost backdoor triggers follow a uniform and usually input-independent pattern,\ne.g., insertion of specific trigger words, synonym replacement. This\nsignificantly hinders the stealthiness of the attacking model, leading the\ntrained backdoor model being easily identified as malicious by model probes.\nSecondly, trigger-inserted poisoned sentences are usually disfluent,\nungrammatical, or even change the semantic meaning from the original sentence,\nmaking them being easily filtered in the pre-processing stage. To resolve these\ntwo issues, in this paper, we propose an input-unique backdoor attack(NURA),\nwhere we generate backdoor triggers unique to inputs. IDBA generates\ncontext-related triggers by continuing writing the input with a language model\nlike GPT2. The generated sentence is used as the backdoor trigger. This\nstrategy not only creates input-unique backdoor triggers, but also preserves\nthe semantics of the original input, simultaneously resolving the two issues\nabove. Experimental results show that the IDBA attack is effective for attack\nand difficult to defend: it achieves high attack success rate across all the\nwidely applied benchmarks, while is immune to existing defending methods. In\naddition, it is able to generate fluent, grammatical, and diverse backdoor\ninputs, which can hardly be recognized through human inspection.", "published": "2023-03-25 01:41:54", "link": "http://arxiv.org/abs/2303.14325v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SmartBook: AI-Assisted Situation Report Generation for Intelligence\n  Analysts", "abstract": "Timely and comprehensive understanding of emerging events is crucial for\neffective decision-making; automating situation report generation can\nsignificantly reduce the time, effort, and cost for intelligence analysts. In\nthis work, we identify intelligence analysts' practices and preferences for AI\nassistance in situation report generation to guide the design strategies for an\neffective, trust-building interface that aligns with their thought processes\nand needs. Next, we introduce SmartBook, an automated framework designed to\ngenerate situation reports from large volumes of news data, creating structured\nreports by automatically discovering event-related strategic questions. These\nreports include multiple hypotheses (claims), summarized and grounded to\nsources with factual evidence, to promote in-depth situation understanding. Our\ncomprehensive evaluation of SmartBook, encompassing a user study alongside a\ncontent review with an editing study, reveals SmartBook's effectiveness in\ngenerating accurate and relevant situation reports. Qualitative evaluations\nindicate over 80% of questions probe for strategic information, and over 90% of\nsummaries produce tactically useful content, being consistently favored over\nsummaries from a large language model integrated with web search. The editing\nstudy reveals that minimal information is removed from the generated text\n(under 2.5%), suggesting that SmartBook provides analysts with a valuable\nfoundation for situation reports", "published": "2023-03-25 03:03:00", "link": "http://arxiv.org/abs/2303.14337v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Performance of GPT-3.5 and GPT-4 in Grammatical Error\n  Correction", "abstract": "GPT-3 and GPT-4 models are powerful, achieving high performance on a variety\nof Natural Language Processing tasks. However, there is a relative lack of\ndetailed published analysis of their performance on the task of grammatical\nerror correction (GEC). To address this, we perform experiments testing the\ncapabilities of a GPT-3.5 model (text-davinci-003) and a GPT-4 model\n(gpt-4-0314) on major GEC benchmarks. We compare the performance of different\nprompts in both zero-shot and few-shot settings, analyzing intriguing or\nproblematic outputs encountered with different prompt formats. We report the\nperformance of our best prompt on the BEA-2019 and JFLEG datasets, finding that\nthe GPT models can perform well in a sentence-level revision setting, with\nGPT-4 achieving a new high score on the JFLEG benchmark. Through human\nevaluation experiments, we compare the GPT models' corrections to source, human\nreference, and baseline GEC system sentences and observe differences in editing\nstrategies and how they are scored by human raters.", "published": "2023-03-25 03:08:49", "link": "http://arxiv.org/abs/2303.14342v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-augmented Frame Semantic Parsing with Hybrid Prompt-tuning", "abstract": "Frame semantics-based approaches have been widely used in semantic parsing\ntasks and have become mainstream. It remains challenging to disambiguate frame\nrepresentations evoked by target lexical units under different contexts.\nPre-trained Language Models (PLMs) have been used in semantic parsing and\nsignificantly improve the accuracy of neural parsers. However, the PLMs-based\napproaches tend to favor collocated patterns presented in the training data,\nleading to inaccurate outcomes. The intuition here is to design a mechanism to\noptimally use knowledge captured in semantic frames in conjunction with PLMs to\ndisambiguate frames. We propose a novel Knowledge-Augmented Frame Semantic\nParsing Architecture (KAF-SPA) to enhance semantic representation by\nincorporating accurate frame knowledge into PLMs during frame semantic parsing.\nSpecifically, a Memory-based Knowledge Extraction Module (MKEM) is devised to\nselect accurate frame knowledge and construct the continuous templates in the\nhigh dimensional vector space. Moreover, we design a Task-oriented Knowledge\nProbing Module (TKPM) using hybrid prompts (in terms of continuous and discrete\nprompts) to incorporate the selected knowledge into the PLMs and adapt PLMs to\nthe tasks of frame and argument identification. Experimental results on two\npublic FrameNet datasets demonstrate that our method significantly outperforms\nstrong baselines (by more than +3$\\%$ in F1), achieving state-of-art results on\nthe current benchmark. Ablation studies verify the effectiveness of KAF-SPA.", "published": "2023-03-25 06:41:19", "link": "http://arxiv.org/abs/2303.14375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing in Ethiopian Languages: Current State,\n  Challenges, and Opportunities", "abstract": "This survey delves into the current state of natural language processing\n(NLP) for four Ethiopian languages: Amharic, Afaan Oromo, Tigrinya, and\nWolaytta. Through this paper, we identify key challenges and opportunities for\nNLP research in Ethiopia. Furthermore, we provide a centralized repository on\nGitHub that contains publicly available resources for various NLP tasks in\nthese languages. This repository can be updated periodically with contributions\nfrom other researchers. Our objective is to identify research gaps and\ndisseminate the information to NLP researchers interested in Ethiopian\nlanguages and encourage future research in this domain.", "published": "2023-03-25 09:04:29", "link": "http://arxiv.org/abs/2303.14406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COFFEE: A Contrastive Oracle-Free Framework for Event Extraction", "abstract": "Event extraction is a complex information extraction task that involves\nextracting events from unstructured text. Prior classification-based methods\nrequire comprehensive entity annotations for joint training, while newer\ngeneration-based methods rely on heuristic templates containing oracle\ninformation such as event type, which is often unavailable in real-world\nscenarios. In this study, we consider a more realistic setting of this task,\nnamely the Oracle-Free Event Extraction (OFEE) task, where only the input\ncontext is given without any oracle information, including event type, event\nontology and trigger word. To solve this task, we propose a new framework,\ncalled COFFEE, which extracts the events solely based on the document context\nwithout referring to any oracle information. In particular, a contrastive\nselection model is introduced in COFFEE to rectify the generated triggers and\nhandle multi-event instances. The proposed COFFEE outperforms state-of-the-art\napproaches under the oracle-free setting of the event extraction task, as\nevaluated on a public event extraction benchmark ACE05.", "published": "2023-03-25 12:12:30", "link": "http://arxiv.org/abs/2303.14452v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Indian Language Summarization using Pretrained Sequence-to-Sequence\n  Models", "abstract": "The ILSUM shared task focuses on text summarization for two major Indian\nlanguages- Hindi and Gujarati, along with English. In this task, we experiment\nwith various pretrained sequence-to-sequence models to find out the best model\nfor each of the languages. We present a detailed overview of the models and our\napproaches in this paper. We secure the first rank across all three sub-tasks\n(English, Hindi and Gujarati). This paper also extensively analyzes the impact\nof k-fold cross-validation while experimenting with limited data size, and we\nalso perform various experiments with a combination of the original and a\nfiltered version of the data to determine the efficacy of the pretrained\nmodels.", "published": "2023-03-25 13:05:54", "link": "http://arxiv.org/abs/2303.14461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Generation of Multiple-Choice Questions", "abstract": "Creating multiple-choice questions to assess reading comprehension of a given\narticle involves generating question-answer pairs (QAPs) and adequate\ndistractors. We present two methods to tackle the challenge of QAP generations:\n(1) A deep-learning-based end-to-end question generation system based on T5\nTransformer with Preprocessing and Postprocessing Pipelines (TP3). We use the\nfinetuned T5 model for our downstream task of question generation and improve\naccuracy using a combination of various NLP tools and algorithms in\npreprocessing and postprocessing to select appropriate answers and filter\nundesirable questions. (2) A sequence-learning-based scheme to generate\nadequate QAPs via meta-sequence representations of sentences. A meta-sequence\nis a sequence of vectors comprising semantic and syntactic tags. we devise a\nscheme called MetaQA to learn meta sequences from training data to form pairs\nof a meta sequence for a declarative sentence and a corresponding interrogative\nsentence. The TP3 works well on unseen data, which is complemented by MetaQA.\nBoth methods can generate well-formed and grammatically correct questions.\nMoreover, we present a novel approach to automatically generate adequate\ndistractors for a given QAP. The method is a combination of part-of-speech\ntagging, named-entity tagging, semantic-role labeling, regular expressions,\ndomain knowledge bases, word embeddings, word edit distance, WordNet, and other\nalgorithms.", "published": "2023-03-25 22:45:54", "link": "http://arxiv.org/abs/2303.14576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tashkeel: Finetuning Byte-Level Models for Accurate Arabic Text\n  Diacritization", "abstract": "Most of previous work on learning diacritization of the Arabic language\nrelied on training models from scratch. In this paper, we investigate how to\nleverage pre-trained language models to learn diacritization. We finetune\ntoken-free pre-trained multilingual models (ByT5) to learn to predict and\ninsert missing diacritics in Arabic text, a complex task that requires\nunderstanding the sentence semantics and the morphological structure of the\ntokens. We show that we can achieve state-of-the-art on the diacritization task\nwith minimal amount of training and no feature engineering, reducing WER by\n40%. We release our finetuned models for the greater benefit of the researchers\nin the community.", "published": "2023-03-25 23:41:33", "link": "http://arxiv.org/abs/2303.14588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For\n  Language Model Synonym-Aware Pretraining", "abstract": "The model's ability to understand synonymous expression is crucial in many\nkinds of downstream tasks. It will make the model to better understand the\nsimilarity between context, and more robust to the synonym substitution attack.\nHowever, many Pretrained Language Model (PLM) lack synonym knowledge due to\nlimitation of small-scale synsets and PLM's pretraining objectives. In this\npaper, we propose a framework called Sem4SAP to mine synsets from Open\nKnowledge Graph (Open-KG) and using the mined synsets to do synonym-aware\npretraining for language models. We propose to coarsly filter the content in\nOpen-KG and use the frequency information to better help the clustering process\nunder low-resource unsupervised conditions. We expand the mined synsets by\nmigrating core semantics between synonymous expressions.We also propose two\nnovel and effective synonym-aware pre-training methods for injecting synonym\nknowledge into PLMs.Extensive experiments demonstrate that Sem4SAP can\ndramatically outperform the original PLMs and other baselines on ten different\ntasks.", "published": "2023-03-25 10:19:14", "link": "http://arxiv.org/abs/2303.14425v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GANTEE: Generative Adversatial Network for Taxonomy Entering Evaluation", "abstract": "Taxonomy is formulated as directed acyclic concepts graphs or trees that\nsupport many downstream tasks. Many new coming concepts need to be added to an\nexisting taxonomy. The traditional taxonomy expansion task aims only at finding\nthe best position for new coming concepts in the existing taxonomy. However,\nthey have two drawbacks when being applied to the real-scenarios. The previous\nmethods suffer from low-efficiency since they waste much time when most of the\nnew coming concepts are indeed noisy concepts. They also suffer from\nlow-effectiveness since they collect training samples only from the existing\ntaxonomy, which limits the ability of the model to mine more hypernym-hyponym\nrelationships among real concepts. This paper proposes a pluggable framework\ncalled Generative Adversarial Network for Taxonomy Entering Evaluation (GANTEE)\nto alleviate these drawbacks. A generative adversarial network is designed in\nthis framework by discriminative models to alleviate the first drawback and the\ngenerative model to alleviate the second drawback. Two discriminators are used\nin GANTEE to provide long-term and short-term rewards, respectively. Moreover,\nto further improve the efficiency, pre-trained language models are used to\nretrieve the representation of the concepts quickly. The experiments on three\nreal-world large-scale datasets with two different languages show that GANTEE\nimproves the performance of the existing taxonomy expansion methods in both\neffectiveness and efficiency.", "published": "2023-03-25 14:24:50", "link": "http://arxiv.org/abs/2303.14480v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The Semantic Reader Project: Augmenting Scholarly Documents through\n  AI-Powered Interactive Reading Interfaces", "abstract": "Scholarly publications are key to the transfer of knowledge from scholars to\nothers. However, research papers are information-dense, and as the volume of\nthe scientific literature grows, the need for new technology to support the\nreading process grows. In contrast to the process of finding papers, which has\nbeen transformed by Internet technology, the experience of reading research\npapers has changed little in decades. The PDF format for sharing research\npapers is widely used due to its portability, but it has significant downsides\nincluding: static content, poor accessibility for low-vision readers, and\ndifficulty reading on mobile devices. This paper explores the question \"Can\nrecent advances in AI and HCI power intelligent, interactive, and accessible\nreading interfaces -- even for legacy PDFs?\" We describe the Semantic Reader\nProject, a collaborative effort across multiple institutions to explore\nautomatic creation of dynamic reading interfaces for research papers. Through\nthis project, we've developed ten research prototype interfaces and conducted\nusability studies with more than 300 participants and real-world users showing\nimproved reading experiences for scholars. We've also released a production\nreading interface for research papers that will incorporate the best features\nas they mature. We structure this paper around challenges scholars and the\npublic face when reading research papers -- Discovery, Efficiency,\nComprehension, Synthesis, and Accessibility -- and present an overview of our\nprogress and remaining open challenges.", "published": "2023-03-25 02:47:09", "link": "http://arxiv.org/abs/2303.14334v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender\n  System", "abstract": "Large language models (LLMs) have demonstrated their significant potential to\nbe applied for addressing various application tasks. However, traditional\nrecommender systems continue to face great challenges such as poor\ninteractivity and explainability, which actually also hinder their broad\ndeployment in real-world systems. To address these limitations, this paper\nproposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender\nSystem) that innovatively augments LLMs for building conversational recommender\nsystems by converting user profiles and historical interactions into prompts.\nChat-Rec is demonstrated to be effective in learning user preferences and\nestablishing connections between users and products through in-context\nlearning, which also makes the recommendation process more interactive and\nexplainable. What's more, within the Chat-Rec framework, user's preferences can\ntransfer to different products for cross-domain recommendations, and\nprompt-based injection of information into LLMs can also handle the cold-start\nscenarios with new items. In our experiments, Chat-Rec effectively improve the\nresults of top-k recommendations and performs better in zero-shot rating\nprediction task. Chat-Rec offers a novel approach to improving recommender\nsystems and presents new practical scenarios for the implementation of AIGC (AI\ngenerated content) in recommender system studies.", "published": "2023-03-25 17:37:43", "link": "http://arxiv.org/abs/2303.14524v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning", "abstract": "Despite dropout's ubiquity in machine learning, its effectiveness as a form\nof data augmentation remains under-explored. We address two key questions: (i)\nWhen is dropout effective as an augmentation strategy? (ii) Is dropout uniquely\neffective under these conditions? To explore these questions, we propose Deep\nAugmentation, a network- and modality-agnostic method that applies dropout or\nPCA transformations to targeted layers in neural networks. Through extensive\nexperiments on contrastive learning tasks in NLP, computer vision, and graph\nlearning, we find that uniformly applying dropout across layers does not\nconsistently improve performance. Instead, dropout proves most beneficial in\ndeeper layers and can be matched by alternative augmentations (e.g., PCA). We\nalso show that a stop-gradient operation is critical for ensuring dropout\nfunctions effectively as an augmentation, and that performance trends invert\nwhen moving from contrastive tasks to supervised tasks. Our analysis suggests\nthat Deep Augmentation helps mitigate inter-layer co-adaptation -- a notable\nissue in self-supervised learning due to the absence of labeled data. Drawing\non these insights, we outline a procedure for selecting the optimal\naugmentation layer and demonstrate that Deep Augmentation can outperform\ntraditional input-level augmentations. This simple yet powerful approach can be\nseamlessly integrated into a wide range of architectures and modalities,\nyielding notable gains in both performance and generalization.", "published": "2023-03-25 19:03:57", "link": "http://arxiv.org/abs/2303.14537v4", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Identification of Negative Transfers in Multitask Learning Using\n  Surrogate Models", "abstract": "Multitask learning is widely used in practice to train a low-resource target\ntask by augmenting it with multiple related source tasks. Yet, naively\ncombining all the source tasks with a target task does not always improve the\nprediction performance for the target task due to negative transfers. Thus, a\ncritical problem in multitask learning is identifying subsets of source tasks\nthat would benefit the target task. This problem is computationally challenging\nsince the number of subsets grows exponentially with the number of source\ntasks; efficient heuristics for subset selection do not always capture the\nrelationship between task subsets and multitask learning performances. In this\npaper, we introduce an efficient procedure to address this problem via\nsurrogate modeling. In surrogate modeling, we sample (random) subsets of source\ntasks and precompute their multitask learning performances. Then, we\napproximate the precomputed performances with a linear regression model that\ncan also predict the multitask performance of unseen task subsets. We show\ntheoretically and empirically that fitting this model only requires sampling\nlinearly many subsets in the number of source tasks. The fitted model provides\na relevance score between each source and target task. We use the relevance\nscores to perform subset selection for multitask learning by thresholding.\nThrough extensive experiments, we show that our approach predicts negative\ntransfers from multiple source tasks to target tasks much more accurately than\nexisting task affinity measures. Additionally, we demonstrate that for several\nweak supervision datasets, our approach consistently improves upon existing\noptimization methods for multitask learning.", "published": "2023-03-25 23:16:11", "link": "http://arxiv.org/abs/2303.14582v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Informed Machine Learning, Centrality, CNN, Relevant Document Detection,\n  Repatriation of Indigenous Human Remains", "abstract": "Among the pressing issues facing Australian and other First Nations peoples\nis the repatriation of the bodily remains of their ancestors, which are\ncurrently held in Western scientific institutions. The success of securing the\nreturn of these remains to their communities for reburial depends largely on\nlocating information within scientific and other literature published between\n1790 and 1970 documenting their theft, donation, sale, or exchange between\ninstitutions. This article reports on collaborative research by data scientists\nand social science researchers in the Research, Reconcile, Renew Network (RRR)\nto develop and apply text mining techniques to identify this vital information.\nWe describe our work to date on developing a machine learning-based solution to\nautomate the process of finding and semantically analysing relevant texts.\nClassification models, particularly deep learning-based models, are known to\nhave low accuracy when trained with small amounts of labelled (i.e.\nrelevant/non-relevant) documents. To improve the accuracy of our detection\nmodel, we explore the use of an Informed Neural Network (INN) model that\ndescribes documentary content using expert-informed contextual knowledge. Only\na few labelled documents are used to provide specificity to the model, using\nconceptually related keywords identified by RRR experts in provenance research.\nThe results confirm the value of using an INN network model for identifying\nrelevant documents related to the investigation of the global commercial trade\nin Indigenous human remains. Empirical analysis suggests that this INN model\ncan be generalized for use by other researchers in the social sciences and\nhumanities who want to extract relevant information from large textual corpora.", "published": "2023-03-25 14:08:21", "link": "http://arxiv.org/abs/2303.14475v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels", "abstract": "Audio-visual speech recognition has received a lot of attention due to its\nrobustness against acoustic noise. Recently, the performance of automatic,\nvisual, and audio-visual speech recognition (ASR, VSR, and AV-ASR,\nrespectively) has been substantially improved, mainly due to the use of larger\nmodels and training sets. However, accurate labelling of datasets is\ntime-consuming and expensive. Hence, in this work, we investigate the use of\nautomatically-generated transcriptions of unlabelled datasets to increase the\ntraining set size. For this purpose, we use publicly-available pre-trained ASR\nmodels to automatically transcribe unlabelled datasets such as AVSpeech and\nVoxCeleb2. Then, we train ASR, VSR and AV-ASR models on the augmented training\nset, which consists of the LRS2 and LRS3 datasets as well as the additional\nautomatically-transcribed data. We demonstrate that increasing the size of the\ntraining set, a recent trend in the literature, leads to reduced WER despite\nusing noisy transcriptions. The proposed model achieves new state-of-the-art\nperformance on AV-ASR on LRS2 and LRS3. In particular, it achieves a WER of\n0.9% on LRS3, a relative improvement of 30% over the current state-of-the-art\napproach, and outperforms methods that have been trained on non-publicly\navailable datasets with 26 times more training data.", "published": "2023-03-25 00:37:34", "link": "http://arxiv.org/abs/2303.14307v3", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
