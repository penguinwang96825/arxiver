{"title": "Combining Word and Character Vector Representation on Neural Machine\n  Translation", "abstract": "This paper describes combinations of word vector representation and character\nvector representation in English-Indonesian neural machine translation (NMT).\nSix configurations of NMT models were built with different input vector\nrepresentations: word-based, combination of word and character representation\nusing bidirectional LSTM(bi-LSTM), combination of word and character\nrepresentation using CNN, combination of word and character representation by\ncombining bi-LSTM and CNN by three different vector operations: addition,\npointwise multiplication, and averaging. The experiment results showed that NMT\nmodels with concatenation of word and character representation obtained BLEU\nscore higher than baseline model, ranging from 9.14 points to 11.65 points, for\nall models that combining both word and character representation, except the\nmodel that combining word and character representation using both bi-LSTM and\nCNN by addition operation. The highest BLEU score achieved was 42.48 compared\nto the 30.83 of the baseline model.", "published": "2020-09-13 06:53:40", "link": "http://arxiv.org/abs/2009.05935v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Span-based Semantic Parsing for Compositional Generalization", "abstract": "Despite the success of sequence-to-sequence (seq2seq) models in semantic\nparsing, recent work has shown that they fail in compositional generalization,\ni.e., the ability to generalize to new structures built of components observed\nduring training. In this work, we posit that a span-based parser should lead to\nbetter compositional generalization. we propose SpanBasedSP, a parser that\npredicts a span tree over an input utterance, explicitly encoding how partial\nprograms compose over spans in the input. SpanBasedSP extends Pasupat et al.\n(2019) to be comparable to seq2seq models by (i) training from programs,\nwithout access to gold trees, treating trees as latent variables, (ii) parsing\na class of non-projective trees through an extension to standard CKY. On\nGeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong\nseq2seq baselines on random splits, but dramatically improves performance\ncompared to baselines on splits that require compositional generalization: from\n$61.0 \\rightarrow 88.9$ average accuracy.", "published": "2020-09-13 16:42:18", "link": "http://arxiv.org/abs/2009.06040v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range\n  Dependency Encoding", "abstract": "Transformer has become ubiquitous in the deep learning field. One of the key\ningredients that destined its success is the self-attention mechanism, which\nallows fully-connected contextual encoding over input tokens. However, despite\nits effectiveness in modeling short sequences, self-attention suffers when\nhandling inputs with extreme long-range dependencies, as its complexity grows\nquadratically with respect to the sequence length. Therefore, long sequences\nare often encoded by Transformer in chunks using a sliding window. In this\npaper, we propose Cluster-Former, a novel clustering-based sparse Transformer\nto perform attention across chunked sequences. The proposed framework is\npivoted on two unique types of Transformer layer: Sliding-Window Layer and\nCluster-Former Layer, which encode local sequence information and global\ncontext jointly and iteratively. This new design allows information integration\nbeyond local windows, which is especially beneficial for question answering\n(QA) tasks that rely on long-range dependencies. Experiments show that\nCluster-Former achieves state-of-the-art performance on several major QA\nbenchmarks.", "published": "2020-09-13 22:09:30", "link": "http://arxiv.org/abs/2009.06097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BoostingBERT:Integrating Multi-Class Boosting into BERT for NLP Tasks", "abstract": "As a pre-trained Transformer model, BERT (Bidirectional Encoder\nRepresentations from Transformers) has achieved ground-breaking performance on\nmultiple NLP tasks. On the other hand, Boosting is a popular ensemble learning\ntechnique which combines many base classifiers and has been demonstrated to\nyield better generalization performance in many machine learning tasks. Some\nworks have indicated that ensemble of BERT can further improve the application\nperformance. However, current ensemble approaches focus on bagging or stacking\nand there has not been much effort on exploring the boosting. In this work, we\nproposed a novel Boosting BERT model to integrate multi-class boosting into the\nBERT. Our proposed model uses the pre-trained Transformer as the base\nclassifier to choose harder training sets to fine-tune and gains the benefits\nof both the pre-training language knowledge and boosting ensemble in NLP tasks.\nWe evaluate the proposed model on the GLUE dataset and 3 popular Chinese NLU\nbenchmarks. Experimental results demonstrate that our proposed model\nsignificantly outperforms BERT on all datasets and proves its effectiveness in\nmany NLP tasks. Replacing the BERT base with RoBERTa as base classifier,\nBoostingBERT achieves new state-of-the-art results in several NLP Tasks. We\nalso use knowledge distillation within the \"teacher-student\" framework to\nreduce the computational overhead and model storage of BoostingBERT while\nkeeping its performance for practical application.", "published": "2020-09-13 09:07:14", "link": "http://arxiv.org/abs/2009.05959v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cosine meets Softmax: A tough-to-beat baseline for visual grounding", "abstract": "In this paper, we present a simple baseline for visual grounding for\nautonomous driving which outperforms the state of the art methods, while\nretaining minimal design choices. Our framework minimizes the cross-entropy\nloss over the cosine distance between multiple image ROI features with a text\nembedding (representing the give sentence/phrase). We use pre-trained networks\nfor obtaining the initial embeddings and learn a transformation layer on top of\nthe text embedding. We perform experiments on the Talk2Car dataset and achieve\n68.7% AP50 accuracy, improving upon the previous state of the art by 8.6%. Our\ninvestigation suggests reconsideration towards more approaches employing\nsophisticated attention mechanisms or multi-stage reasoning or complex metric\nlearning loss functions by showing promise in simpler alternatives.", "published": "2020-09-13 19:35:43", "link": "http://arxiv.org/abs/2009.06066v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Identity-Based Patterns in Deep Convolutional Networks: Generative\n  Adversarial Phonology and Reduplication", "abstract": "This paper models unsupervised learning of an identity-based pattern (or\ncopying) in speech called reduplication from raw continuous data with deep\nconvolutional neural networks. We use the ciwGAN architecture Begu\\v{s} (2021a;\narXiv:2006.02951) in which learning of meaningful representations in speech\nemerges from a requirement that the CNNs generate informative data. We propose\na technique to wug-test CNNs trained on speech and, based on four generative\ntests, argue that the network learns to represent an identity-based pattern in\nits latent space. By manipulating only two categorical variables in the latent\nspace, we can actively turn an unreduplicated form into a reduplicated form\nwith no other substantial changes to the output in the majority of cases. We\nalso argue that the network extends the identity-based pattern to unobserved\ndata. Exploration of how meaningful representations of identity-based patterns\nemerge in CNNs and how the latent space variables outside of the training range\ncorrelate with identity-based patterns in the output has general implications\nfor neural network interpretability.", "published": "2020-09-13 23:12:49", "link": "http://arxiv.org/abs/2009.06110v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Differentially Private Language Models Benefit from Public Pre-training", "abstract": "Language modeling is a keystone task in natural language processing. When\ntraining a language model on sensitive information, differential privacy (DP)\nallows us to quantify the degree to which our private data is protected.\nHowever, training algorithms which enforce differential privacy often lead to\ndegradation in model quality. We study the feasibility of learning a language\nmodel which is simultaneously high-quality and privacy preserving by tuning a\npublic base model on a private corpus. We find that DP fine-tuning boosts the\nperformance of language models in the private domain, making the training of\nsuch models possible.", "published": "2020-09-13 00:50:44", "link": "http://arxiv.org/abs/2009.05886v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Pow-Wow: A Dataset and Study on Collaborative Communication in Pommerman", "abstract": "In multi-agent learning, agents must coordinate with each other in order to\nsucceed. For humans, this coordination is typically accomplished through the\nuse of language. In this work we perform a controlled study of human language\nuse in a competitive team-based game, and search for useful lessons for\nstructuring communication protocol between autonomous agents. We construct\nPow-Wow, a new dataset for studying situated goal-directed human communication.\nUsing the Pommerman game environment, we enlisted teams of humans to play\nagainst teams of AI agents, recording their observations, actions, and\ncommunications. We analyze the types of communications which result in\neffective game strategies, annotate them accordingly, and present corpus-level\nstatistical analysis of how trends in communications affect game outcomes.\nBased on this analysis, we design a communication policy for learning agents,\nand show that agents which utilize communication achieve higher win-rates\nagainst baseline systems than those which do not.", "published": "2020-09-13 07:11:37", "link": "http://arxiv.org/abs/2009.05940v1", "categories": ["cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
