{"title": "Pair-Level Supervised Contrastive Learning for Natural Language\n  Inference", "abstract": "Natural language inference (NLI) is an increasingly important task for\nnatural language understanding, which requires one to infer the relationship\nbetween the sentence pair (premise and hypothesis). Many recent works have used\ncontrastive learning by incorporating the relationship of the sentence pair\nfrom NLI datasets to learn sentence representation. However, these methods only\nfocus on comparisons with sentence-level representations. In this paper, we\npropose a Pair-level Supervised Contrastive Learning approach (PairSCL). We\nadopt a cross attention module to learn the joint representations of the\nsentence pairs. A contrastive learning objective is designed to distinguish the\nvaried classes of sentence pairs by pulling those in one class together and\npushing apart the pairs in other classes. We evaluate PairSCL on two public\ndatasets of NLI where the accuracy of PairSCL outperforms other methods by 2.1%\non average. Furthermore, our method outperforms the previous state-of-the-art\nmethod on seven transfer tasks of text classification.", "published": "2022-01-26 13:34:52", "link": "http://arxiv.org/abs/2201.10927v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Twitter-Demographer: A Flow-based Tool to Enrich Twitter Data", "abstract": "Twitter data have become essential to Natural Language Processing (NLP) and\nsocial science research, driving various scientific discoveries in recent\nyears. However, the textual data alone are often not enough to conduct studies:\nespecially social scientists need more variables to perform their analysis and\ncontrol for various factors. How we augment this information, such as users'\nlocation, age, or tweet sentiment, has ramifications for anonymity and\nreproducibility, and requires dedicated effort. This paper describes\nTwitter-Demographer, a simple, flow-based tool to enrich Twitter data with\nadditional information about tweets and users. Twitter-Demographer is aimed at\nNLP practitioners and (computational) social scientists who want to enrich\ntheir datasets with aggregated information, facilitating reproducibility, and\nproviding algorithmic privacy-by-design measures for pseudo-anonymity. We\ndiscuss our design choices, inspired by the flow-based programming paradigm, to\nuse black-box components that can easily be chained together and extended. We\nalso analyze the ethical issues related to the use of this tool, and the\nbuilt-in measures to facilitate pseudo-anonymity.", "published": "2022-01-26 14:59:17", "link": "http://arxiv.org/abs/2201.10986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence", "abstract": "Recently, there has been a growing interest in designing text generation\nsystems from a discourse coherence perspective, e.g., modeling the\ninterdependence between sentences. Still, recent BERT-based evaluation metrics\nare weak in recognizing coherence, and thus are not reliable in a way to spot\nthe discourse-level improvements of those text generation systems. In this\nwork, we introduce DiscoScore, a parametrized discourse metric, which uses BERT\nto model discourse coherence from different perspectives, driven by Centering\ntheory. Our experiments encompass 16 non-discourse and discourse metrics,\nincluding DiscoScore and popular coherence models, evaluated on summarization\nand document-level machine translation (MT). We find that (i) the majority of\nBERT-based metrics correlate much worse with human rated coherence than early\ndiscourse metrics, invented a decade ago; (ii) the recent state-of-the-art\nBARTScore is weak when operated at system level -- which is particularly\nproblematic as systems are typically compared in this manner. DiscoScore, in\ncontrast, achieves strong system-level correlation with human ratings, not only\nin coherence but also in factual consistency and other aspects, and surpasses\nBARTScore by over 10 correlation points on average. Further, aiming to\nunderstand DiscoScore, we provide justifications to the importance of discourse\ncoherence for evaluation metrics, and explain the superiority of one variant\nover another. Our code is available at\n\\url{https://github.com/AIPHES/DiscoScore}.", "published": "2022-01-26 20:28:26", "link": "http://arxiv.org/abs/2201.11176v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Strategy for Multilingual Grammatical Error Correction with\n  Pre-trained Cross-Lingual Language Model", "abstract": "Synthetic data construction of Grammatical Error Correction (GEC) for\nnon-English languages relies heavily on human-designed and language-specific\nrules, which produce limited error-corrected patterns. In this paper, we\npropose a generic and language-independent strategy for multilingual GEC, which\ncan train a GEC system effectively for a new non-English language with only two\neasy-to-access resources: 1) a pretrained cross-lingual language model (PXLM)\nand 2) parallel translation data between English and the language. Our approach\ncreates diverse parallel GEC data without any language-specific operations by\ntaking the non-autoregressive translation generated by PXLM and the gold\ntranslation as error-corrected sentence pairs. Then, we reuse PXLM to\ninitialize the GEC model and pretrain it with the synthetic data generated by\nitself, which yields further improvement. We evaluate our approach on three\npublic benchmarks of GEC in different languages. It achieves the\nstate-of-the-art results on the NLPCC 2018 Task 2 dataset (Chinese) and obtains\ncompetitive performance on Falko-Merlin (German) and RULEC-GEC (Russian).\nFurther analysis demonstrates that our data construction method is\ncomplementary to rule-based approaches.", "published": "2022-01-26 02:10:32", "link": "http://arxiv.org/abs/2201.10707v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models", "abstract": "Neural network models have achieved state-of-the-art performance on\ngrapheme-to-phoneme (G2P) conversion. However, their performance relies on\nlarge-scale pronunciation dictionaries, which may not be available for a lot of\nlanguages. Inspired by the success of the pre-trained language model BERT, this\npaper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which\nis built by self-supervised training on a large, language-specific word list\nwith only grapheme information. Furthermore, two approaches are developed to\nincorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e.,\nfine-tuning GBERT or fusing GBERT into the Transformer model by attention.\nExperimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean\ndatasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our\nGBERT-based G2P models under both medium-resource and low-resource data\nconditions.", "published": "2022-01-26 02:49:56", "link": "http://arxiv.org/abs/2201.10716v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Both the validity of the cultural tightness index and the association\n  with creativity and order are spurious -- a comment on Jackson et al", "abstract": "It was recently suggested in a study published in Nature Human Behaviour that\nthe historical loosening of American culture was associated with a trade-off\nbetween higher creativity and lower order. To this end, Jackson et al. generate\na linguistic index of cultural tightness based on the Google Books Ngram corpus\nand use this index to show that American norms loosened between 1800 and 2000.\nWhile we remain agnostic toward a potential loosening of American culture and a\nstatistical association with creativity/order, we show here that the methods\nused by Jackson et al. are neither suitable for testing the validity of the\nindex nor for establishing possible relationships with creativity/order.", "published": "2022-01-26 08:32:44", "link": "http://arxiv.org/abs/2201.10812v1", "categories": ["stat.AP", "cs.CL"], "primary_category": "stat.AP"}
{"title": "CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search", "abstract": "In this paper, we propose the CodeRetriever model, which learns the\nfunction-level code semantic representations through large-scale code-text\ncontrastive pre-training. We adopt two contrastive learning schemes in\nCodeRetriever: unimodal contrastive learning and bimodal contrastive learning.\nFor unimodal contrastive learning, we design an unsupervised learning approach\nto build semantic-related code pairs based on the documentation and function\nname. For bimodal contrastive learning, we leverage the documentation and\nin-line comments of code to build code-text pairs. Both contrastive objectives\ncan fully leverage large-scale code corpus for pre-training. Extensive\nexperimental results show that CodeRetriever achieves new state-of-the-art with\nsignificant improvement over existing code pre-trained models, on eleven\ndomain/language-specific code search tasks with six programming languages in\ndifferent code granularity (function-level, snippet-level and statement-level).\nThese results demonstrate the effectiveness and robustness of CodeRetriever.", "published": "2022-01-26 10:54:30", "link": "http://arxiv.org/abs/2201.10866v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "SCAI-QReCC Shared Task on Conversational Question Answering", "abstract": "Search-Oriented Conversational AI (SCAI) is an established venue that\nregularly puts a spotlight upon the recent work advancing the field of\nconversational search. SCAI'21 was organised as an independent on-line event\nand featured a shared task on conversational question answering. Since all of\nthe participant teams experimented with answer generation models for this task,\nwe identified evaluation of answer correctness in this settings as the major\nchallenge and a current research gap. Alongside the automatic evaluation, we\nconducted two crowdsourcing experiments to collect annotations for answer\nplausibility and faithfulness. As a result of this shared task, the original\nconversational QA dataset used for evaluation was further extended with\nalternative correct answers produced by the participant systems.", "published": "2022-01-26 18:03:21", "link": "http://arxiv.org/abs/2201.11094v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CsFEVER and CTKFacts: Acquiring Czech data for fact verification", "abstract": "In this paper, we examine several methods of acquiring Czech data for\nautomated fact-checking, which is a task commonly modeled as a classification\nof textual claim veracity w.r.t. a corpus of trusted ground truths. We attempt\nto collect sets of data in form of a factual claim, evidence within the ground\ntruth corpus, and its veracity label (supported, refuted or not enough info).\nAs a first attempt, we generate a Czech version of the large-scale FEVER\ndataset built on top of Wikipedia corpus. We take a hybrid approach of machine\ntranslation and document alignment; the approach and the tools we provide can\nbe easily applied to other languages. We discuss its weaknesses and\ninaccuracies, propose a future approach for their cleaning and publish the 127k\nresulting translations, as well as a version of such dataset reliably\napplicable for the Natural Language Inference task - the CsFEVER-NLI.\nFurthermore, we collect a novel dataset of 3,097 claims, which is annotated\nusing the corpus of 2.2M articles of Czech News Agency. We present its extended\nannotation methodology based on the FEVER approach, and, as the underlying\ncorpus is kept a trade secret, we also publish a standalone version of the\ndataset for the task of Natural Language Inference we call CTKFactsNLI. We\nanalyze both acquired datasets for spurious cues - annotation patterns leading\nto model overfitting. CTKFacts is further examined for inter-annotator\nagreement, thoroughly cleaned, and a typology of common annotator errors is\nextracted. Finally, we provide baseline models for all stages of the\nfact-checking pipeline and publish the NLI datasets, as well as our annotation\nplatform and other experimental data.", "published": "2022-01-26 18:48:42", "link": "http://arxiv.org/abs/2201.11115v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Addressing Issues of Cross-Linguality in Open-Retrieval Question\n  Answering Systems For Emergent Domains", "abstract": "Open-retrieval question answering systems are generally trained and tested on\nlarge datasets in well-established domains. However, low-resource settings such\nas new and emerging domains would especially benefit from reliable question\nanswering systems. Furthermore, multilingual and cross-lingual resources in\nemergent domains are scarce, leading to few or no such systems. In this paper,\nwe demonstrate a cross-lingual open-retrieval question answering system for the\nemergent domain of COVID-19. Our system adopts a corpus of scientific articles\nto ensure that retrieved documents are reliable. To address the scarcity of\ncross-lingual training data in emergent domains, we present a method utilizing\nautomatic translation, alignment, and filtering to produce English-to-all\ndatasets. We show that a deep semantic retriever greatly benefits from training\non our English-to-all data and significantly outperforms a BM25 baseline in the\ncross-lingual setting. We illustrate the capabilities of our system with\nexamples and release all code necessary to train and deploy such a system.", "published": "2022-01-26 19:27:32", "link": "http://arxiv.org/abs/2201.11153v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Explainable Patterns for Distinction and Prediction of Moral Judgement\n  on Reddit", "abstract": "The forum r/AmITheAsshole in Reddit hosts discussion on moral issues based on\nconcrete narratives presented by users. Existing analysis of the forum focuses\non its comments, and does not make the underlying data publicly available. In\nthis paper we build a new dataset of comments and also investigate the\nclassification of the posts in the forum. Further, we identify textual patterns\nassociated with the provocation of moral judgement by posts, with the\nexpression of moral stance in comments, and with the decisions of trained\nclassifiers of posts and comments.", "published": "2022-01-26 19:39:52", "link": "http://arxiv.org/abs/2201.11155v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Learning Invariable Semantical Representation from Language for\n  Extensible Policy Generalization", "abstract": "Recently, incorporating natural language instructions into reinforcement\nlearning (RL) to learn semantically meaningful representations and foster\ngeneralization has caught many concerns. However, the semantical information in\nlanguage instructions is usually entangled with task-specific state\ninformation, which hampers the learning of semantically invariant and reusable\nrepresentations. In this paper, we propose a method to learn such\nrepresentations called element randomization, which extracts task-relevant but\nenvironment-agnostic semantics from instructions using a set of environments\nwith randomized elements, e.g., topological structures or textures, yet the\nsame language instruction. We theoretically prove the feasibility of learning\nsemantically invariant representations through randomization. In practice, we\naccordingly develop a hierarchy of policies, where a high-level policy is\ndesigned to modulate the behavior of a goal-conditioned low-level policy by\nproposing subgoals as semantically invariant representations. Experiments on\nchallenging long-horizon tasks show that (1) our low-level policy reliably\ngeneralizes to tasks against environment changes; (2) our hierarchical policy\nexhibits extensible generalization in unseen new tasks that can be decomposed\ninto several solvable sub-tasks; and (3) by storing and replaying language\ntrajectories as succinct policy representations, the agent can complete tasks\nin a one-shot fashion, i.e., once one successful trajectory has been attained.", "published": "2022-01-26 08:04:27", "link": "http://arxiv.org/abs/2202.00466v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Assessment of the Impact of OCR Noise on Language Models", "abstract": "Neural language models are the backbone of modern-day natural language\nprocessing applications. Their use on textual heritage collections which have\nundergone Optical Character Recognition (OCR) is therefore also increasing.\nNevertheless, our understanding of the impact OCR noise could have on language\nmodels is still limited. We perform an assessment of the impact OCR noise has\non a variety of language models, using data in Dutch, English, French and\nGerman. We find that OCR noise poses a significant obstacle to language\nmodelling, with language models increasingly diverging from their noiseless\ntargets as OCR quality lowers. In the presence of small corpora, simpler models\nincluding PPMI and Word2Vec consistently outperform transformer-based models in\nthis respect.", "published": "2022-01-26 21:56:14", "link": "http://arxiv.org/abs/2202.00470v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FiNCAT: Financial Numeral Claim Analysis Tool", "abstract": "While making investment decisions by reading financial documents, investors\nneed to differentiate between in-claim and outof-claim numerals. In this paper,\nwe present a tool which does it automatically. It extracts context embeddings\nof the numerals using one of the transformer based pre-trained language model\ncalled BERT. After this, it uses a Logistic Regression based model to detect\nwhether the numerals is in-claim or out-of-claim. We use FinNum-3 (English)\ndataset to train our model. After conducting rigorous experiments we achieve a\nMacro F1 score of 0.8223 on the validation set. We have open-sourced this tool\nand it can be accessed from\nhttps://github.com/sohomghosh/FiNCAT_Financial_Numeral_Claim_Analysis_Tool", "published": "2022-01-26 11:53:34", "link": "http://arxiv.org/abs/2202.00631v1", "categories": ["q-fin.GN", "cs.CL", "I.7.0; I.2.7"], "primary_category": "q-fin.GN"}
{"title": "On the Effectiveness of Pinyin-Character Dual-Decoding for End-to-End\n  Mandarin Chinese ASR", "abstract": "End-to-end automatic speech recognition (ASR) has achieved promising results.\nHowever, most existing end-to-end ASR methods neglect the use of specific\nlanguage characteristics. For Mandarin Chinese ASR tasks, there exist mutual\npromotion relationship between Pinyin and Character where Chinese characters\ncan be romanized by Pinyin. Based on the above intuition, we first investigate\ntypes of end-to-end encoder-decoder based models in the single-input\ndual-output (SIDO) multi-task framework, after which a novel asynchronous\ndecoding with fuzzy Pinyin sampling method is proposed according to the\none-to-one correspondence characteristics between Pinyin and Character.\nFurthermore, we proposed a two-stage training strategy to make training more\nstable and converge faster. The results on the test sets of AISHELL-1 dataset\nshow that the proposed enhanced dual-decoder model without a language model is\nimproved by a big margin compared to strong baseline models.", "published": "2022-01-26 07:59:03", "link": "http://arxiv.org/abs/2201.10792v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Automated Question-Answering Framework Based on Evolution Algorithm", "abstract": "Building a deep learning model for a Question-Answering (QA) task requires a\nlot of human effort, it may need several months to carefully tune various model\narchitectures and find a best one. It's even harder to find different excellent\nmodels for multiple datasets. Recent works show that the best model structure\nis related to the dataset used, and one single model cannot adapt to all tasks.\nIn this paper, we propose an automated Question-Answering framework, which\ncould automatically adjust network architecture for multiple datasets. Our\nframework is based on an innovative evolution algorithm, which is stable and\nsuitable for multiple dataset scenario. The evolution algorithm for search\ncombine prior knowledge into initial population and use a performance estimator\nto avoid inefficient mutation by predicting the performance of candidate model\narchitecture. The prior knowledge used in initial population could improve the\nfinal result of the evolution algorithm. The performance estimator could\nquickly filter out models with bad performance in population as the number of\ntrials increases, to speed up the convergence. Our framework achieves 78.9 EM\nand 86.1 F1 on SQuAD 1.1, 69.9 EM and 72.5 F1 on SQuAD 2.0. On NewsQA dataset,\nthe found model achieves 47.0 EM and 62.9 F1.", "published": "2022-01-26 08:13:24", "link": "http://arxiv.org/abs/2201.10797v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "The Norwegian Parliamentary Speech Corpus", "abstract": "The Norwegian Parliamentary Speech Corpus (NPSC) is a speech dataset with\nrecordings of meetings from Stortinget, the Norwegian parliament. It is the\nfirst, publicly available dataset containing unscripted, Norwegian speech\ndesigned for training of automatic speech recognition (ASR) systems. The\nrecordings are manually transcribed and annotated with language codes and\nspeakers, and there are detailed metadata about the speakers. The\ntranscriptions exist in both normalized and non-normalized form, and\nnon-standardized words are explicitly marked and annotated with standardized\nequivalents. To test the usefulness of this dataset, we have compared an ASR\nsystem trained on the NPSC with a baseline system trained on only\nmanuscript-read speech. These systems were tested on an independent dataset\ncontaining spontaneous, dialectal speech. The NPSC-trained system performed\nsignificantly better, with a 22.9% relative improvement in word error rate\n(WER). Moreover, training on the NPSC is shown to have a \"democratizing\" effect\nin terms of dialects, as improvements are generally larger for dialects with\nhigher WER from the baseline system.", "published": "2022-01-26 11:41:55", "link": "http://arxiv.org/abs/2201.10881v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "One Student Knows All Experts Know: From Sparse to Dense", "abstract": "Human education system trains one student by multiple experts.\nMixture-of-experts (MoE) is a powerful sparse architecture including multiple\nexperts. However, sparse MoE model is easy to overfit, hard to deploy, and not\nhardware-friendly for practitioners. In this work, inspired by the human\neducation model, we propose a novel task, knowledge integration, to obtain a\ndense student model (OneS) as knowledgeable as one sparse MoE. We investigate\nthis task by proposing a general training framework including knowledge\ngathering and knowledge distillation. Specifically, to gather key knowledge\nfrom different pre-trained experts, we first investigate four different\npossible knowledge gathering methods, \\ie summation, averaging, Top-K Knowledge\nGathering (Top-KG), and Singular Value Decomposition Knowledge Gathering\n(SVD-KG) proposed in this paper. We then refine the dense student model by\nknowledge distillation to offset the noise from gathering. On ImageNet, our\nOneS preserves $61.7\\%$ benefits from MoE and achieves $78.4\\%$ top-1 accuracy\nImageNet with only $15$M parameters. On four natural language processing\ndatasets, OneS obtains $88.2\\%$ MoE benefits and outperforms the best baseline\nby $51.7\\%$ using the same architecture and training data. In addition,\ncompared with the MoE counterpart, OneS can achieve $3.7 \\times$ inference\nspeedup due to less computation and hardware-friendly architecture.", "published": "2022-01-26 12:11:02", "link": "http://arxiv.org/abs/2201.10890v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Language-biased image classification: evaluation based on semantic\n  representations", "abstract": "Humans show language-biased image recognition for a word-embedded image,\nknown as picture-word interference. Such interference depends on hierarchical\nsemantic categories and reflects that human language processing highly\ninteracts with visual processing. Similar to humans, recent artificial models\njointly trained on texts and images, e.g., OpenAI CLIP, show language-biased\nimage classification. Exploring whether the bias leads to interference similar\nto those observed in humans can contribute to understanding how much the model\nacquires hierarchical semantic representations from joint learning of language\nand vision. The present study introduces methodological tools from the\ncognitive science literature to assess the biases of artificial models.\nSpecifically, we introduce a benchmark task to test whether words superimposed\non images can distort the image classification across different category levels\nand, if it can, whether the perturbation is due to the shared semantic\nrepresentation between language and vision. Our dataset is a set of\nword-embedded images and consists of a mixture of natural image datasets and\nhierarchical word labels with superordinate/basic category levels. Using this\nbenchmark test, we evaluate the CLIP model. We show that presenting words\ndistorts the image classification by the model across different category\nlevels, but the effect does not depend on the semantic relationship between\nimages and embedded words. This suggests that the semantic word representation\nin the CLIP visual processing is not shared with the image representation,\nalthough the word representation strongly dominates for word-embedded images.", "published": "2022-01-26 15:46:36", "link": "http://arxiv.org/abs/2201.11014v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Natural Language Descriptions of Deep Visual Features", "abstract": "Some neurons in deep networks specialize in recognizing highly specific\nperceptual, structural, or semantic features of inputs. In computer vision,\ntechniques exist for identifying neurons that respond to individual concept\ncategories like colors, textures, and object classes. But these techniques are\nlimited in scope, labeling only a small subset of neurons and behaviors in any\nnetwork. Is a richer characterization of neuron-level computation possible? We\nintroduce a procedure (called MILAN, for mutual-information-guided linguistic\nannotation of neurons) that automatically labels neurons with open-ended,\ncompositional, natural language descriptions. Given a neuron, MILAN generates a\ndescription by searching for a natural language string that maximizes pointwise\nmutual information with the image regions in which the neuron is active. MILAN\nproduces fine-grained descriptions that capture categorical, relational, and\nlogical structure in learned features. These descriptions obtain high agreement\nwith human-generated feature descriptions across a diverse set of model\narchitectures and tasks, and can aid in understanding and controlling learned\nmodels. We highlight three applications of natural language neuron\ndescriptions. First, we use MILAN for analysis, characterizing the distribution\nand importance of neurons selective for attribute, category, and relational\ninformation in vision models. Second, we use MILAN for auditing, surfacing\nneurons sensitive to human faces in datasets designed to obscure them. Finally,\nwe use MILAN for editing, improving robustness in an image classifier by\ndeleting neurons sensitive to text features spuriously correlated with class\nlabels.", "published": "2022-01-26 18:48:02", "link": "http://arxiv.org/abs/2201.11114v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Tackling data scarcity in speech translation using zero-shot\n  multilingual machine translation techniques", "abstract": "Recently, end-to-end speech translation (ST) has gained significant attention\nas it avoids error propagation. However, the approach suffers from data\nscarcity. It heavily depends on direct ST data and is less efficient in making\nuse of speech transcription and text translation data, which is often more\neasily available. In the related field of multilingual text translation,\nseveral techniques have been proposed for zero-shot translation. A main idea is\nto increase the similarity of semantically similar sentences in different\nlanguages. We investigate whether these ideas can be applied to speech\ntranslation, by building ST models trained on speech transcription and text\ntranslation data. We investigate the effects of data augmentation and auxiliary\nloss function. The techniques were successfully applied to few-shot ST using\nlimited ST data, with improvements of up to +12.9 BLEU points compared to\ndirect end-to-end ST and +3.1 BLEU points compared to ST models fine-tuned from\nASR model.", "published": "2022-01-26 20:20:59", "link": "http://arxiv.org/abs/2201.11172v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Discovering Phonetic Inventories with Crosslingual Automatic Speech\n  Recognition", "abstract": "The high cost of data acquisition makes Automatic Speech Recognition (ASR)\nmodel training problematic for most existing languages, including languages\nthat do not even have a written script, or for which the phone inventories\nremain unknown. Past works explored multilingual training, transfer learning,\nas well as zero-shot learning in order to build ASR systems for these\nlow-resource languages. While it has been shown that the pooling of resources\nfrom multiple languages is helpful, we have not yet seen a successful\napplication of an ASR model to a language unseen during training. A crucial\nstep in the adaptation of ASR from seen to unseen languages is the creation of\nthe phone inventory of the unseen language. The ultimate goal of our work is to\nbuild the phone inventory of a language unseen during training in an\nunsupervised way without any knowledge about the language. In this paper, we 1)\ninvestigate the influence of different factors (i.e., model architecture,\nphonotactic model, type of speech representation) on phone recognition in an\nunknown language; 2) provide an analysis of which phones transfer well across\nlanguages and which do not in order to understand the limitations of and areas\nfor further improvement for automatic phone inventory creation; and 3) present\ndifferent methods to build a phone inventory of an unseen language in an\nunsupervised way. To that end, we conducted mono-, multi-, and crosslingual\nexperiments on a set of 13 phonetically diverse languages and several in-depth\nanalyses. We found a number of universal phone tokens (IPA symbols) that are\nwell-recognized cross-linguistically. Through a detailed analysis of results,\nwe conclude that unique sounds, similar sounds, and tone languages remain a\nmajor challenge for phonetic inventory discovery.", "published": "2022-01-26 22:12:55", "link": "http://arxiv.org/abs/2201.11207v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noise-robust voice conversion with domain adversarial training", "abstract": "Voice conversion has made great progress in the past few years under the\nstudio-quality test scenario in terms of speech quality and speaker similarity.\nHowever, in real applications, test speech from source speaker or target\nspeaker can be corrupted by various environment noises, which seriously degrade\nthe speech quality and speaker similarity. In this paper, we propose a novel\nencoder-decoder based noise-robust voice conversion framework, which consists\nof a speaker encoder, a content encoder, a decoder, and two domain adversarial\nneural networks. Specifically, we integrate disentangling speaker and content\nrepresentation technique with domain adversarial training technique. Domain\nadversarial training makes speaker representations and content representations\nextracted by speaker encoder and content encoder from clean speech and noisy\nspeech in the same space, respectively. In this way, the learned speaker and\ncontent representations are noise-invariant. Therefore, the two noise-invariant\nrepresentations can be taken as input by the decoder to predict the clean\nconverted spectrum. The experimental results demonstrate that our proposed\nmethod can synthesize clean converted speech under noisy test scenarios, where\nthe source speech and target speech can be corrupted by seen or unseen noise\ntypes during the training process. Additionally, both speech quality and\nspeaker similarity are improved.", "published": "2022-01-26 00:57:47", "link": "http://arxiv.org/abs/2201.10693v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SkiM: Skipping Memory LSTM for Low-Latency Real-Time Continuous Speech\n  Separation", "abstract": "Continuous speech separation for meeting pre-processing has recently become a\nfocused research topic. Compared to the data in utterance-level speech\nseparation, the meeting-style audio stream lasts longer, has an uncertain\nnumber of speakers. We adopt the time-domain speech separation method and the\nrecently proposed Graph-PIT to build a super low-latency online speech\nseparation model, which is very important for the real application. The\nlow-latency time-domain encoder with a small stride leads to an extremely long\nfeature sequence. We proposed a simple yet efficient model named Skipping\nMemory (SkiM) for the long sequence modeling. Experimental results show that\nSkiM achieves on par or even better separation performance than DPRNN.\nMeanwhile, the computational cost of SkiM is reduced by 75% compared to DPRNN.\nThe strong long sequence modeling capability and low computational cost make\nSkiM a suitable model for online CSS applications. Our fastest real-time model\ngets 17.1 dB signal-to-distortion (SDR) improvement with less than\n1-millisecond latency in the simulated meeting-style evaluation.", "published": "2022-01-26 08:16:56", "link": "http://arxiv.org/abs/2201.10800v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A two-step backward compatible fullband speech enhancement system", "abstract": "Speech enhancement methods based on deep learning have surpassed traditional\nmethods. While many of these new approaches are operating on the wideband\n(16kHz) sample rate, a new fullband (48kHz) speech enhancement system is\nproposed in this paper. Compared to the existing fullband systems that utilizes\nperceptually motivated features to train the fullband speech enhancement using\na single network structure, the proposed system is a two-step system ensuring\ngood fullband speech enhancement quality while backward compatible to the\nexisting wideband systems.", "published": "2022-01-26 08:29:33", "link": "http://arxiv.org/abs/2201.10809v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "J-MAC: Japanese multi-speaker audiobook corpus for speech synthesis", "abstract": "In this paper, we construct a Japanese audiobook speech corpus called \"J-MAC\"\nfor speech synthesis research. With the success of reading-style speech\nsynthesis, the research target is shifting to tasks that use complicated\ncontexts. Audiobook speech synthesis is a good example that requires\ncross-sentence, expressiveness, etc. Unlike reading-style speech,\nspeaker-specific expressiveness in audiobook speech also becomes the context.\nTo enhance this research, we propose a method of constructing a corpus from\naudiobooks read by professional speakers. From many audiobooks and their texts,\nour method can automatically extract and refine the data without any language\ndependency. Specifically, we use vocal-instrumental separation to extract clean\ndata, connectionist temporal classification to roughly align text and audio,\nand voice activity detection to refine the alignment. J-MAC is open-sourced in\nour project page. We also conduct audiobook speech synthesis evaluations, and\nthe results give insights into audiobook speech synthesis.", "published": "2022-01-26 12:22:53", "link": "http://arxiv.org/abs/2201.10896v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Invertible Voice Conversion", "abstract": "In this paper, we propose an invertible deep learning framework called INVVC\nfor voice conversion. It is designed against the possible threats that\ninherently come along with voice conversion systems. Specifically, we develop\nan invertible framework that makes the source identity traceable. The framework\nis built on a series of invertible $1\\times1$ convolutions and flows consisting\nof affine coupling layers. We apply the proposed framework to one-to-one voice\nconversion and many-to-one conversion using parallel training data.\nExperimental results show that this approach yields impressive performance on\nvoice conversion and, moreover, the converted results can be reversed back to\nthe source inputs utilizing the same parameters as in forwarding.", "published": "2022-01-26 00:25:27", "link": "http://arxiv.org/abs/2201.10687v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control", "abstract": "Generating music with deep neural networks has been an area of active\nresearch in recent years. While the quality of generated samples has been\nsteadily increasing, most methods are only able to exert minimal control over\nthe generated sequence, if any. We propose the self-supervised\ndescription-to-sequence task, which allows for fine-grained controllable\ngeneration on a global level. We do so by extracting high-level features about\nthe target sequence and learning the conditional distribution of sequences\ngiven the corresponding high-level description in a sequence-to-sequence\nmodelling setup. We train FIGARO (FIne-grained music Generation via\nAttention-based, RObust control) by applying description-to-sequence modelling\nto symbolic music. By combining learned high level features with domain\nknowledge, which acts as a strong inductive bias, the model achieves\nstate-of-the-art results in controllable symbolic music generation and\ngeneralizes well beyond the training distribution.", "published": "2022-01-26 13:51:19", "link": "http://arxiv.org/abs/2201.10936v4", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Learnable Wavelet Packet Transform for Data-Adapted Spectrograms", "abstract": "Capturing high-frequency data concerning the condition of complex systems,\ne.g. by acoustic monitoring, has become increasingly prevalent. Such\nhigh-frequency signals typically contain time dependencies ranging over\ndifferent time scales and different types of cyclic behaviors. Processing such\nsignals requires careful feature engineering, particularly the extraction of\nmeaningful time-frequency features. This can be time-consuming and the\nperformance is often dependent on the choice of parameters. To address these\nlimitations, we propose a deep learning framework for learnable wavelet packet\ntransforms, enabling to learn features automatically from data and optimise\nthem with respect to the defined objective function. The learned features can\nbe represented as a spectrogram, containing the important time-frequency\ninformation of the dataset. We evaluate the properties and performance of the\nproposed approach by evaluating its improved spectral leakage and by applying\nit to an anomaly detection task for acoustic monitoring.", "published": "2022-01-26 17:28:17", "link": "http://arxiv.org/abs/2201.11069v1", "categories": ["cs.SD", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Self-attention fusion for audiovisual emotion recognition with\n  incomplete data", "abstract": "In this paper, we consider the problem of multimodal data analysis with a use\ncase of audiovisual emotion recognition. We propose an architecture capable of\nlearning from raw data and describe three variants of it with distinct modality\nfusion mechanisms. While most of the previous works consider the ideal scenario\nof presence of both modalities at all times during inference, we evaluate the\nrobustness of the model in the unconstrained settings where one modality is\nabsent or noisy, and propose a method to mitigate these limitations in a form\nof modality dropout. Most importantly, we find that following this approach not\nonly improves performance drastically under the absence/noisy representations\nof one modality, but also improves the performance in a standard ideal setting,\noutperforming the competing methods.", "published": "2022-01-26 18:04:29", "link": "http://arxiv.org/abs/2201.11095v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Rapid solution for searching similar audio items", "abstract": "A naive approach for finding similar audio items would be to compare each\nentry from the feature vector of the test example with each feature vector of\nthe candidates in a k-nearest neighbors fashion. There are already two problems\nwith this approach: audio signals are represented by high dimensional vectors\nand the number of candidates can be very large - think thousands. The search\nprocess would have a high complexity. Our paper will treat this problem through\nhashing methodologies more specifically the Locality Sensitive Hashing. This\nproject will be in the spirit of classification and clustering problems. The\ncomputer sound production principles will be used to determine which features\nthat describe an audio signal are the most useful. That will down-sample the\nsize of the feature vectors and speed up the process subsequently.", "published": "2022-01-26 20:30:42", "link": "http://arxiv.org/abs/2201.11178v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Internal Language Model Estimation Through Explicit Context Vector\n  Learning for Attention-based Encoder-decoder ASR", "abstract": "An end-to-end (E2E) ASR model implicitly learns a prior Internal Language\nModel (ILM) from the training transcripts. To fuse an external LM using Bayes\nposterior theory, the log likelihood produced by the ILM has to be accurately\nestimated and subtracted. In this paper we propose two novel approaches to\nestimate the ILM based on Listen-Attend-Spell (LAS) framework. The first method\nis to replace the context vector of the LAS decoder at every time step with a\nvector that is learned with training transcripts. Furthermore, we propose\nanother method that uses a lightweight feed-forward network to directly map\nquery vector to context vector in a dynamic sense. Since the context vectors\nare learned by minimizing the perplexities on training transcripts, and their\nestimation is independent of encoder output, hence the ILMs are accurately\nlearned for both methods. Experiments show that the ILMs achieve the lowest\nperplexity, indicating the efficacy of the proposed methods. In addition, they\nalso significantly outperform the shallow fusion method, as well as two\npreviously proposed ILM Estimation (ILME) approaches on several datasets.", "published": "2022-01-26 07:47:27", "link": "http://arxiv.org/abs/2201.11627v3", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Closing the sim-to-real gap in guided wave damage detection with\n  adversarial training of variational auto-encoders", "abstract": "Guided wave testing is a popular approach for monitoring the structural\nintegrity of infrastructures. We focus on the primary task of damage detection,\nwhere signal processing techniques are commonly employed. The detection\nperformance is affected by a mismatch between the wave propagation model and\nexperimental wave data. External variations, such as temperature, which are\ndifficult to model, also affect the performance. While deep learning models can\nbe an alternative detection method, there is often a lack of real-world\ntraining datasets. In this work, we counter this challenge by training an\nensemble of variational autoencoders only on simulation data with a wave\nphysics-guided adversarial component. We set up an experiment with non-uniform\ntemperature variations to test the robustness of the methods. We compare our\nscheme with existing deep learning detection schemes and observe superior\nperformance on experimental data.", "published": "2022-01-26 17:36:11", "link": "http://arxiv.org/abs/2202.00570v1", "categories": ["eess.SP", "cs.LG", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Understanding and Compressing Music with Maximal Transformable Patterns", "abstract": "We present a polynomial-time algorithm that discovers all maximal patterns in\na point set, $D\\subset\\mathbb{R}^k$, that are related by transformations in a\nuser-specified class, $F$, of bijections over $\\mathbb{R}^k$. We also present a\nsecond algorithm that discovers the set of occurrences for each of these\nmaximal patterns and then uses compact encodings of these occurrence sets to\ncompute a losslessly compressed encoding of the input point set. This encoding\ntakes the form of a set of pairs, $E=\\left\\lbrace\\left\\langle P_1,\nT_1\\right\\rangle,\\left\\langle P_2, T_2\\right\\rangle,\\ldots\\left\\langle\nP_{\\ell}, T_{\\ell}\\right\\rangle\\right\\rbrace$, where each $\\langle\nP_i,T_i\\rangle$ consists of a maximal pattern, $P_i\\subseteq D$, and a set,\n$T_i\\subset F$, of transformations that map $P_i$ onto other subsets of $D$.\nEach transformation is encoded by a vector of real values that uniquely\nidentifies it within $F$ and the length of this vector is used as a measure of\nthe complexity of $F$. We evaluate the new compression algorithm with three\ntransformation classes of differing complexity, on the task of classifying\nfolk-song melodies into tune families. The most complex of the classes tested\nincludes all combinations of the musical transformations of transposition,\ninversion, retrograde, augmentation and diminution. We found that broadening\nthe transformation class improved performance on this task. However, it did\nnot, on average, improve compression factor, which may be due to the datasets\n(in this case, folk-song melodies) being too short and simple to benefit from\nthe potentially greater number of pattern relationships that are discoverable\nwith larger transformation classes.", "published": "2022-01-26 17:47:26", "link": "http://arxiv.org/abs/2201.11085v2", "categories": ["cs.LG", "cs.IT", "cs.SD", "eess.AS", "math.IT"], "primary_category": "cs.LG"}
