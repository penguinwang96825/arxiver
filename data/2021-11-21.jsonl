{"title": "Capitalization and Punctuation Restoration: a Survey", "abstract": "Ensuring proper punctuation and letter casing is a key pre-processing step\ntowards applying complex natural language processing algorithms. This is\nespecially significant for textual sources where punctuation and casing are\nmissing, such as the raw output of automatic speech recognition systems.\nAdditionally, short text messages and micro-blogging platforms offer unreliable\nand often wrong punctuation and casing. This survey offers an overview of both\nhistorical and state-of-the-art techniques for restoring punctuation and\ncorrecting word casing. Furthermore, current challenges and research directions\nare highlighted.", "published": "2021-11-21 05:48:32", "link": "http://arxiv.org/abs/2111.10746v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "More Romanian word embeddings from the RETEROM project", "abstract": "Automatically learned vector representations of words, also known as \"word\nembeddings\", are becoming a basic building block for more and more natural\nlanguage processing algorithms. There are different ways and tools for\nconstructing word embeddings. Most of the approaches rely on raw texts, the\nconstruction items being the word occurrences and/or letter n-grams. More\nelaborated research is using additional linguistic features extracted after\ntext preprocessing. Morphology is clearly served by vector representations\nconstructed from raw texts and letter n-grams. Syntax and semantics studies may\nprofit more from the vector representations constructed with additional\nfeatures such as lemma, part-of-speech, syntactic or semantic dependants\nassociated with each word. One of the key objectives of the ReTeRom project is\nthe development of advanced technologies for Romanian natural language\nprocessing, including morphological, syntactic and semantic analysis of text.\nAs such, we plan to develop an open-access large library of ready-to-use word\nembeddings sets, each set being characterized by different parameters: used\nfeatures (wordforms, letter n-grams, lemmas, POSes etc.), vector lengths,\nwindow/context size and frequency thresholds. To this end, the previously\ncreated sets of word embeddings (based on word occurrences) on the CoRoLa\ncorpus (P\\u{a}i\\c{s} and Tufi\\c{s}, 2018) are and will be further augmented\nwith new representations learned from the same corpus by using specific\nfeatures such as lemmas and parts of speech. Furthermore, in order to better\nunderstand and explore the vectors, graphical representations will be available\nby customized interfaces.", "published": "2021-11-21 06:05:12", "link": "http://arxiv.org/abs/2111.10750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Isomer: Transfer enhanced Dual-Channel Heterogeneous Dependency\n  Attention Network for Aspect-based Sentiment Classification", "abstract": "Aspect-based sentiment classification aims to predict the sentiment polarity\nof a specific aspect in a sentence. However, most existing methods attempt to\nconstruct dependency relations into a homogeneous dependency graph with the\nsparsity and ambiguity, which cannot cover the comprehensive contextualized\nfeatures of short texts or consider any additional node types or semantic\nrelation information. To solve those issues, we present a sentiment analysis\nmodel named Isomer, which performs a dual-channel attention on heterogeneous\ndependency graphs incorporating external knowledge, to effectively integrate\nother additional information. Specifically, a transfer-enhanced dual-channel\nheterogeneous dependency attention network is devised in Isomer to model short\ntexts using heterogeneous dependency graphs. These heterogeneous dependency\ngraphs not only consider different types of information but also incorporate\nexternal knowledge. Experiments studies show that our model outperforms recent\nmodels on benchmark datasets. Furthermore, the results suggest that our method\ncaptures the importance of various information features to focus on informative\ncontextual words.", "published": "2021-11-21 11:21:50", "link": "http://arxiv.org/abs/2112.03011v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TraVLR: Now You See It, Now You Don't! A Bimodal Dataset for Evaluating\n  Visio-Linguistic Reasoning", "abstract": "Numerous visio-linguistic (V+L) representation learning methods have been\ndeveloped, yet existing datasets do not adequately evaluate the extent to which\nthey represent visual and linguistic concepts in a unified space. We propose\nseveral novel evaluation settings for V+L models, including cross-modal\ntransfer. Furthermore, existing V+L benchmarks often report global accuracy\nscores on the entire dataset, making it difficult to pinpoint the specific\nreasoning tasks that models fail and succeed at. We present TraVLR, a synthetic\ndataset comprising four V+L reasoning tasks. TraVLR's synthetic nature allows\nus to constrain its training and testing distributions along task-relevant\ndimensions, enabling the evaluation of out-of-distribution generalisation. Each\nexample in TraVLR redundantly encodes the scene in two modalities, allowing\neither to be dropped or added during training or testing without losing\nrelevant information. We compare the performance of four state-of-the-art V+L\nmodels, finding that while they perform well on test examples from the same\nmodality, they all fail at cross-modal transfer and have limited success\naccommodating the addition or deletion of one modality. We release TraVLR as an\nopen challenge for the research community.", "published": "2021-11-21 07:22:44", "link": "http://arxiv.org/abs/2111.10756v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Case Study on the Independence of Speech Emotion Recognition in Bangla\n  and English Languages using Language-Independent Prosodic Features", "abstract": "A language agnostic approach to recognizing emotions from speech remains an\nincomplete and challenging task. In this paper, we performed a step-by-step\ncomparative analysis of Speech Emotion Recognition (SER) using Bangla and\nEnglish languages to assess whether distinguishing emotions from speech is\nindependent of language. Six emotions were categorized for this study, such as\n- happy, angry, neutral, sad, disgust, and fear. We employed three Emotional\nSpeech Sets (ESS), of which the first two were developed by native Bengali\nspeakers in Bangla and English languages separately. The third was a subset of\nthe Toronto Emotional Speech Set (TESS), which was developed by native English\nspeakers from Canada. We carefully selected language-independent prosodic\nfeatures, adopted a Support Vector Machine (SVM) model, and conducted three\nexperiments to carry out our proposition. In the first experiment, we measured\nthe performance of the three speech sets individually, followed by the second\nexperiment, where different ESS pairs were integrated to analyze the impact on\nSER. Finally, we measured the recognition rate by training and testing the\nmodel with different speech sets in the third experiment. Although this study\nreveals that SER in Bangla and English languages is mostly\nlanguage-independent, some disparities were observed while recognizing\nemotional states like disgust and fear in these two languages. Moreover, our\ninvestigations revealed that non-native speakers convey emotions through\nspeech, much like expressing themselves in their native tongue.", "published": "2021-11-21 09:28:49", "link": "http://arxiv.org/abs/2111.10776v3", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jointly Dynamic Topic Model for Recognition of Lead-lag Relationship in\n  Two Text Corpora", "abstract": "Topic evolution modeling has received significant attentions in recent\ndecades. Although various topic evolution models have been proposed, most\nstudies focus on the single document corpus. However in practice, we can easily\naccess data from multiple sources and also observe relationships between them.\nThen it is of great interest to recognize the relationship between multiple\ntext corpora and further utilize this relationship to improve topic modeling.\nIn this work, we focus on a special type of relationship between two text\ncorpora, which we define as the \"lead-lag relationship\". This relationship\ncharacterizes the phenomenon that one text corpus would influence the topics to\nbe discussed in the other text corpus in the future. To discover the lead-lag\nrelationship, we propose a jointly dynamic topic model and also develop an\nembedding extension to address the modeling problem of large-scale text corpus.\nWith the recognized lead-lag relationship, the similarities of the two text\ncorpora can be figured out and the quality of topic learning in both corpora\ncan be improved. We numerically investigate the performance of the jointly\ndynamic topic modeling approach using synthetic data. Finally, we apply the\nproposed model on two text corpora consisting of statistical papers and the\ngraduation theses. Results show the proposed model can well recognize the\nlead-lag relationship between the two corpora, and the specific and shared\ntopic patterns in the two corpora are also discovered.", "published": "2021-11-21 15:53:15", "link": "http://arxiv.org/abs/2111.10846v1", "categories": ["cs.CL", "stat.ME", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Fake News Detection Tools and Methods -- A Review", "abstract": "In the past decade, the social networks platforms and micro-blogging sites\nsuch as Facebook, Twitter, Instagram, and Weibo have become an integral part of\nour day-to-day activities and is widely used all over the world by billions of\nusers to share their views and circulate information in the form of messages,\npictures, and videos. These are even used by government agencies to spread\nimportant information through their verified Facebook accounts and official\nTwitter handles, as they can reach a huge population within a limited time\nwindow. However, many deceptive activities like propaganda and rumor can\nmislead users on a daily basis. In these COVID times, fake news and rumors are\nvery prevalent and are shared in a huge number which has created chaos in this\ntough time. And hence, the need for Fake News Detection in the present scenario\nis inevitable. In this paper, we survey the recent literature about different\napproaches to detect fake news over the Internet. In particular, we firstly\ndiscuss fake news and the various terms related to it that have been considered\nin the literature. Secondly, we highlight the various publicly available\ndatasets and various online tools that are available and can debunk Fake News\nin real-time. Thirdly, we describe fake news detection methods based on two\nbroader areas i.e., its content and the social context. Finally, we provide a\ncomparison of various techniques that are used to debunk fake news.", "published": "2021-11-21 13:19:23", "link": "http://arxiv.org/abs/2112.11185v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Automatic Detection of Depression from Stratified Samples of Audio Data", "abstract": "Depression is a common mental disorder which has been affecting millions of\npeople around the world and becoming more severe with the arrival of COVID-19.\nNevertheless proper diagnosis is not accessible in many regions due to a severe\nshortage of psychiatrists. This scarcity is worsened in low-income countries\nwhich have a psychiatrist to population ratio 210 times lower than that of\ncountries with better economies. This study aimed to explore applications of\ndeep learning in diagnosing depression from voice samples. We collected data\nfrom the DAIC-WOZ database which contained 189 vocal recordings from 154\nindividuals. Voice samples from a patient with a PHQ-8 score equal or higher\nthan 10 were deemed as depressed and those with a PHQ-8 score lower than 10\nwere considered healthy. We applied mel-spectrogram to extract relevant\nfeatures from the audio. Three types of encoders were tested i.e. 1D CNN, 1D\nCNN-LSTM, and 1D CNN-GRU. After tuning hyperparameters systematically, we found\nthat 1D CNN-GRU encoder with a kernel size of 5 and 15 seconds of recording\ndata appeared to have the best performance with F1 score of 0.75, precision of\n0.64, and recall of 0.92.", "published": "2021-11-21 10:16:32", "link": "http://arxiv.org/abs/2111.10783v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Geometry-Aware Multi-Task Learning for Binaural Audio Generation from\n  Video", "abstract": "Binaural audio provides human listeners with an immersive spatial sound\nexperience, but most existing videos lack binaural audio recordings. We propose\nan audio spatialization method that draws on visual information in videos to\nconvert their monaural (single-channel) audio to binaural audio. Whereas\nexisting approaches leverage visual features extracted directly from video\nframes, our approach explicitly disentangles the geometric cues present in the\nvisual stream to guide the learning process. In particular, we develop a\nmulti-task framework that learns geometry-aware features for binaural audio\ngeneration by accounting for the underlying room impulse response, the visual\nstream's coherence with the sound source(s) positions, and the consistency in\ngeometry of the sounding objects over time. Furthermore, we introduce a new\nlarge video dataset with realistic binaural audio simulated for real-world\nscanned environments. On two datasets, we demonstrate the efficacy of our\nmethod, which achieves state-of-the-art results.", "published": "2021-11-21 19:26:45", "link": "http://arxiv.org/abs/2111.10882v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Active Restoration of Lost Audio Signals Using Machine Learning and\n  Latent Information", "abstract": "Digital audio signal reconstruction of a lost or corrupt segment using deep\nlearning algorithms has been explored intensively in recent years.\nNevertheless, prior traditional methods with linear interpolation, phase coding\nand tone insertion techniques are still in vogue. However, we found no research\nwork on reconstructing audio signals with the fusion of dithering,\nsteganography, and machine learning regressors. Therefore, this paper proposes\nthe combination of steganography, halftoning (dithering), and state-of-the-art\nshallow and deep learning methods. The results (including comparing the SPAIN,\nAutoregressive, deep learning-based, graph-based, and other methods) are\nevaluated with three different metrics. The observations from the results show\nthat the proposed solution is effective and can enhance the reconstruction of\naudio signals performed by the side information (e.g., Latent representation)\nsteganography provides. Moreover, this paper proposes a novel framework for\nreconstruction from heavily compressed embedded audio data using halftoning\n(i.e., dithering) and machine learning, which we termed the HCR (halftone-based\ncompression and reconstruction). This work may trigger interest in optimising\nthis approach and/or transferring it to different domains (i.e., image\nreconstruction). Compared to existing methods, we show improvement in the\ninpainting performance in terms of signal-to-noise ratio (SNR), the objective\ndifference grade (ODG) and Hansen's audio quality metric. In particular, our\nproposed framework outperformed the learning-based methods (D2WGAN and SG) and\nthe traditional statistical algorithms (e.g., SPAIN, TDC, WCP).", "published": "2021-11-21 20:11:33", "link": "http://arxiv.org/abs/2111.10891v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Health Monitoring of Industrial machines using Scene-Aware Threshold\n  Selection", "abstract": "This paper presents an autoencoder based unsupervised approach to identify\nanomaly in an industrial machine using sounds produced by the machine. The\nproposed framework is trained using log-melspectrogram representations of the\nsound signal. In classification, our hypothesis is that the reconstruction\nerror computed for an abnormal machine is larger than that of the a normal\nmachine, since only normal machine sounds are being used to train the\nautoencoder. A threshold is chosen to discriminate between normal and abnormal\nmachines. However, the threshold changes as surrounding conditions vary. To\nselect an appropriate threshold irrespective of the surrounding, we propose a\nscene classification framework, which can classify the underlying surrounding.\nHence, the threshold can be selected adaptively irrespective of the\nsurrounding. The experiment evaluation is performed on MIMII dataset for\nindustrial machines namely fan, pump, valve and slide rail. Our experiment\nanalysis shows that utilizing adaptive threshold, the performance improves\nsignificantly as that obtained using the fixed threshold computed for a given\nsurrounding only.", "published": "2021-11-21 21:01:38", "link": "http://arxiv.org/abs/2111.10897v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
