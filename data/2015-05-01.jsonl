{"title": "Embedding Semantic Relations into Word Representations", "abstract": "Learning representations for semantic relations is important for various\ntasks such as analogy detection, relational search, and relation\nclassification. Although there have been several proposals for learning\nrepresentations for individual words, learning word representations that\nexplicitly capture the semantic relations between words remains under\ndeveloped. We propose an unsupervised method for learning vector\nrepresentations for words such that the learnt representations are sensitive to\nthe semantic relations that exist between two words. First, we extract lexical\npatterns from the co-occurrence contexts of two words in a corpus to represent\nthe semantic relations that exist between those two words. Second, we represent\na lexical pattern as the weighted sum of the representations of the words that\nco-occur with that lexical pattern. Third, we train a binary classifier to\ndetect relationally similar vs. non-similar lexical pattern pairs. The proposed\nmethod is unsupervised in the sense that the lexical pattern pairs we use as\ntrain data are automatically sampled from a corpus, without requiring any\nmanual intervention. Our proposed method statistically significantly\noutperforms the current state-of-the-art word representations on three\nbenchmark datasets for proportional analogy detection, demonstrating its\nability to accurately capture the semantic relations among words.", "published": "2015-05-01 11:43:34", "link": "http://arxiv.org/abs/1505.00161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchy of Scales in Language Dynamics", "abstract": "Methods and insights from statistical physics are finding an increasing\nvariety of applications where one seeks to understand the emergent properties\nof a complex interacting system. One such area concerns the dynamics of\nlanguage at a variety of levels of description, from the behaviour of\nindividual agents learning simple artificial languages from each other, up to\nchanges in the structure of languages shared by large groups of speakers over\nhistorical timescales. In this Colloquium, we survey a hierarchy of scales at\nwhich language and linguistic behaviour can be described, along with the main\nprogress in understanding that has been made at each of them---much of which\nhas come from the statistical physics community. We argue that future\ndevelopments may arise by linking the different levels of the hierarchy\ntogether in a more coherent fashion, in particular where this allows more\neffective use of rich empirical data sets.", "published": "2015-05-01 08:44:45", "link": "http://arxiv.org/abs/1505.00122v2", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "Grounded Discovery of Coordinate Term Relationships between Software\n  Entities", "abstract": "We present an approach for the detection of coordinate-term relationships\nbetween entities from the software domain, that refer to Java classes. Usually,\nrelations are found by examining corpus statistics associated with text\nentities. In some technical domains, however, we have access to additional\ninformation about the real-world objects named by the entities, suggesting that\ncoupling information about the \"grounded\" entities with corpus statistics might\nlead to improved methods for relation discovery. To this end, we develop a\nsimilarity measure for Java classes using distributional information about how\nthey are used in software, which we combine with corpus statistics on the\ndistribution of contexts in which the classes appear in text. Using our\napproach, cross-validation accuracy on this dataset can be improved\ndramatically, from around 60% to 88%. Human labeling results show that our\nclassifier has an F1 score of 86% over the top 1000 predicted pairs.", "published": "2015-05-01 20:40:00", "link": "http://arxiv.org/abs/1505.00277v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Compositional Distributional Semantics with Compact Closed Categories\n  and Frobenius Algebras", "abstract": "This thesis contributes to ongoing research related to the categorical\ncompositional model for natural language of Coecke, Sadrzadeh and Clark in\nthree ways: Firstly, I propose a concrete instantiation of the abstract\nframework based on Frobenius algebras (joint work with Sadrzadeh). The theory\nimproves shortcomings of previous proposals, extends the coverage of the\nlanguage, and is supported by experimental work that improves existing results.\nThe proposed framework describes a new class of compositional models that find\nintuitive interpretations for a number of linguistic phenomena. Secondly, I\npropose and evaluate in practice a new compositional methodology which\nexplicitly deals with the different levels of lexical ambiguity (joint work\nwith Pulman). A concrete algorithm is presented, based on the separation of\nvector disambiguation from composition in an explicit prior step. Extensive\nexperimental work shows that the proposed methodology indeed results in more\naccurate composite representations for the framework of Coecke et al. in\nparticular and every other class of compositional models in general. As a last\ncontribution, I formalize the explicit treatment of lexical ambiguity in the\ncontext of the categorical framework by resorting to categorical quantum\nmechanics (joint work with Coecke). In the proposed extension, the concept of a\ndistributional vector is replaced with that of a density matrix, which\ncompactly represents a probability distribution over the potential different\nmeanings of the specific word. Composition takes the form of quantum\nmeasurements, leading to interesting analogies between quantum physics and\nlinguistics.", "published": "2015-05-01 10:00:33", "link": "http://arxiv.org/abs/1505.00138v1", "categories": ["cs.CL", "cs.AI", "math.CT", "math.QA", "quant-ph"], "primary_category": "cs.CL"}
