{"title": "Efficient infusion of self-supervised representations in Automatic\n  Speech Recognition", "abstract": "Self-supervised learned (SSL) models such as Wav2vec and HuBERT yield\nstate-of-the-art results on speech-related tasks. Given the effectiveness of\nsuch models, it is advantageous to use them in conventional ASR systems. While\nsome approaches suggest incorporating these models as a trainable encoder or a\nlearnable frontend, training such systems is extremely slow and requires a lot\nof computation cycles. In this work, we propose two simple approaches that use\n(1) framewise addition and (2) cross-attention mechanisms to efficiently\nincorporate the representations from the SSL model(s) into the ASR\narchitecture, resulting in models that are comparable in size with standard\nencoder-decoder conformer systems while also avoiding the usage of SSL models\nduring training. Our approach results in faster training and yields significant\nperformance gains on the Librispeech and Tedlium datasets compared to\nbaselines. We further provide detailed analysis and ablation studies that\ndemonstrate the effectiveness of our approach.", "published": "2024-04-19 05:01:12", "link": "http://arxiv.org/abs/2404.12628v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SOS-1K: A Fine-grained Suicide Risk Classification Dataset for Chinese\n  Social Media Analysis", "abstract": "In the social media, users frequently express personal emotions, a subset of\nwhich may indicate potential suicidal tendencies. The implicit and varied forms\nof expression in internet language complicate accurate and rapid identification\nof suicidal intent on social media, thus creating challenges for timely\nintervention efforts. The development of deep learning models for suicide risk\ndetection is a promising solution, but there is a notable lack of relevant\ndatasets, especially in the Chinese context. To address this gap, this study\npresents a Chinese social media dataset designed for fine-grained suicide risk\nclassification, focusing on indicators such as expressions of suicide intent,\nmethods of suicide, and urgency of timing. Seven pre-trained models were\nevaluated in two tasks: high and low suicide risk, and fine-grained suicide\nrisk classification on a level of 0 to 10. In our experiments, deep learning\nmodels show good performance in distinguishing between high and low suicide\nrisk, with the best model achieving an F1 score of 88.39%. However, the results\nfor fine-grained suicide risk classification were still unsatisfactory, with an\nweighted F1 score of 50.89%. To address the issues of data imbalance and\nlimited dataset size, we investigated both traditional and advanced, large\nlanguage model based data augmentation techniques, demonstrating that data\naugmentation can enhance model performance by up to 4.65% points in F1-score.\nNotably, the Chinese MentalBERT model, which was pre-trained on psychological\ndomain data, shows superior performance in both tasks. This study provides\nvaluable insights for automatic identification of suicidal individuals,\nfacilitating timely psychological intervention on social media platforms. The\nsource code and data are publicly available.", "published": "2024-04-19 06:58:51", "link": "http://arxiv.org/abs/2404.12659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Semantic Parsing with Extremely Rich Symbolic Meaning\n  Representations", "abstract": "Current open-domain neural semantics parsers show impressive performance.\nHowever, closer inspection of the symbolic meaning representations they produce\nreveals significant weaknesses: sometimes they tend to merely copy character\nsequences from the source text to form symbolic concepts, defaulting to the\nmost frequent word sense based in the training distribution. By leveraging the\nhierarchical structure of a lexical ontology, we introduce a novel\ncompositional symbolic representation for concepts based on their position in\nthe taxonomical hierarchy. This representation provides richer semantic\ninformation and enhances interpretability. We introduce a neural \"taxonomical\"\nsemantic parser to utilize this new representation system of predicates, and\ncompare it with a standard neural semantic parser trained on the traditional\nmeaning representation format, employing a novel challenge set and evaluation\nmetric for evaluation. Our experimental findings demonstrate that the\ntaxonomical model, trained on much richer and complex meaning representations,\nis slightly subordinate in performance to the traditional model using the\nstandard metrics for evaluation, but outperforms it when dealing with\nout-of-vocabulary concepts. This finding is encouraging for research in\ncomputational semantics that aims to combine data-driven distributional\nmeanings with knowledge-based symbolic representations.", "published": "2024-04-19 08:06:01", "link": "http://arxiv.org/abs/2404.12698v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble Learning for Heterogeneous Large Language Models with Deep\n  Parallel Collaboration", "abstract": "Large language models (LLMs) exhibit complementary strengths in various\ntasks, motivating the research of LLM ensembling. However, existing work\nfocuses on training an extra reward model or fusion model to select or combine\nall candidate answers, posing a great challenge to the generalization on unseen\ndata distributions. Besides, prior methods use textual responses as\ncommunication media, ignoring the valuable information in the internal\nrepresentations. In this work, we propose a training-free ensemble framework\nDeePEn, fusing the informative probability distributions yielded by different\nLLMs at each decoding step. Unfortunately, the vocabulary discrepancy between\nheterogeneous LLMs directly makes averaging the distributions unfeasible due to\nthe token misalignment. To address this challenge, DeePEn maps the probability\ndistribution of each model from its own probability space to a universal\nrelative space based on the relative representation theory, and performs\naggregation. Next, we devise a search-based inverse transformation to transform\nthe aggregated result back to the probability space of one of the ensembling\nLLMs (main model), in order to determine the next token. We conduct extensive\nexperiments on ensembles of different number of LLMs, ensembles of LLMs with\ndifferent architectures, and ensembles between the LLM and the specialist\nmodel. Experimental results show that (i) DeePEn achieves consistent\nimprovements across six benchmarks covering subject examination, reasoning, and\nknowledge, (ii) a well-performing specialist model can benefit from a less\neffective LLM through distribution fusion, and (iii) DeePEn has complementary\nstrengths with other ensemble methods such as voting.", "published": "2024-04-19 08:52:22", "link": "http://arxiv.org/abs/2404.12715v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Character Understanding of Large Language Models via\n  Character Profiling from Fictional Works", "abstract": "Large language models (LLMs) have demonstrated impressive performance and\nspurred numerous AI applications, in which role-playing agents (RPAs) are\nparticularly popular, especially for fictional characters. The prerequisite for\nthese RPAs lies in the capability of LLMs to understand characters from\nfictional works. Previous efforts have evaluated this capability via basic\nclassification tasks or characteristic imitation, failing to capture the\nnuanced character understanding with LLMs. In this paper, we propose evaluating\nLLMs' character understanding capability via the character profiling task,\ni.e., summarizing character profiles from corresponding materials, a widely\nadopted yet understudied practice for RPA development. Specifically, we\nconstruct the CroSS dataset from literature experts and assess the generated\nprofiles by comparing them with ground truth references and evaluating their\napplicability in downstream tasks. Our experiments, which cover various\nsummarization methods and LLMs, have yielded promising results. These results\nstrongly validate the character understanding capability of LLMs. Resources are\navailable at https://github.com/Joanna0123/character_profiling.", "published": "2024-04-19 09:10:29", "link": "http://arxiv.org/abs/2404.12726v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?", "abstract": "Analogical reasoning is a unique ability of humans to address unfamiliar\nchallenges by transferring strategies from relevant past experiences. One key\nfinding in psychology is that compared with irrelevant past experiences,\nrecalling relevant ones can help humans better handle new tasks.\nCoincidentally, the NLP community has also recently found that self-generating\nrelevant examples in the context can help large language models (LLMs) better\nsolve a given problem than hand-crafted prompts. However, it is yet not clear\nwhether relevance is the key factor eliciting such capability, i.e., can LLMs\nbenefit more from self-generated relevant examples than irrelevant ones? In\nthis work, we systematically explore whether LLMs can truly perform analogical\nreasoning on a diverse set of reasoning tasks. With extensive experiments and\nanalysis, we show that self-generated random examples can surprisingly achieve\ncomparable or even better performance, e.g., 4% performance boost on GSM8K with\nrandom biological examples. We find that the accuracy of self-generated\nexamples is the key factor and subsequently design two improved methods with\nsignificantly reduced inference costs. Overall, we aim to advance a deeper\nunderstanding of LLM analogical reasoning and hope this work stimulates further\nresearch in the design of self-generated contexts.", "published": "2024-04-19 09:15:07", "link": "http://arxiv.org/abs/2404.12728v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REXEL: An End-to-end Model for Document-Level Relation Extraction and\n  Entity Linking", "abstract": "Extracting structured information from unstructured text is critical for many\ndownstream NLP applications and is traditionally achieved by closed information\nextraction (cIE). However, existing approaches for cIE suffer from two\nlimitations: (i) they are often pipelines which makes them prone to error\npropagation, and/or (ii) they are restricted to sentence level which prevents\nthem from capturing long-range dependencies and results in expensive inference\ntime. We address these limitations by proposing REXEL, a highly efficient and\naccurate model for the joint task of document level cIE (DocIE). REXEL performs\nmention detection, entity typing, entity disambiguation, coreference resolution\nand document-level relation classification in a single forward pass to yield\nfacts fully linked to a reference knowledge graph. It is on average 11 times\nfaster than competitive existing approaches in a similar setting and performs\ncompetitively both when optimised for any of the individual subtasks and a\nvariety of combinations of different joint tasks, surpassing the baselines by\nan average of more than 6 F1 points. The combination of speed and accuracy\nmakes REXEL an accurate cost-efficient system for extracting structured\ninformation at web-scale. We also release an extension of the DocRED dataset to\nenable benchmarking of future work on DocIE, which is available at\nhttps://github.com/amazon-science/e2e-docie.", "published": "2024-04-19 11:04:27", "link": "http://arxiv.org/abs/2404.12788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical\n  Trial Results", "abstract": "Adverse drug events (ADEs) are a major safety issue in clinical trials. Thus,\npredicting ADEs is key to developing safer medications and enhancing patient\noutcomes. To support this effort, we introduce CT-ADE, a dataset for multilabel\nADE prediction in monopharmacy treatments. CT-ADE encompasses 2,497 drugs and\n168,984 drug-ADE pairs from clinical trial results, annotated using the MedDRA\nontology. Unlike existing resources, CT-ADE integrates treatment and target\npopulation data, enabling comparative analyses under varying conditions, such\nas dosage, administration route, and demographics. In addition, CT-ADE\nsystematically collects all ADEs in the study population, including positive\nand negative cases. To provide a baseline for ADE prediction performance using\nthe CT-ADE dataset, we conducted analyses using large language models (LLMs).\nThe best LLM achieved an F1-score of 56%, with models incorporating treatment\nand patient information outperforming by 21%-38% those relying solely on the\nchemical structure. These findings underscore the importance of contextual\ninformation in ADE prediction and establish CT-ADE as a robust resource for\nsafety risk assessment in pharmaceutical research and development.", "published": "2024-04-19 12:04:32", "link": "http://arxiv.org/abs/2404.12827v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LiMe: a Latin Corpus of Late Medieval Criminal Sentences", "abstract": "The Latin language has received attention from the computational linguistics\nresearch community, which has built, over the years, several valuable\nresources, ranging from detailed annotated corpora to sophisticated tools for\nlinguistic analysis. With the recent advent of large language models,\nresearchers have also started developing models capable of generating vector\nrepresentations of Latin texts. The performances of such models remain behind\nthe ones for modern languages, given the disparity in available data. In this\npaper, we present the LiMe dataset, a corpus of 325 documents extracted from a\nseries of medieval manuscripts called Libri sententiarum potestatis Mediolani,\nand thoroughly annotated by experts, in order to be employed for masked\nlanguage model, as well as supervised natural language processing tasks.", "published": "2024-04-19 12:06:28", "link": "http://arxiv.org/abs/2404.12829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TartuNLP @ SIGTYP 2024 Shared Task: Adapting XLM-RoBERTa for Ancient and\n  Historical Languages", "abstract": "We present our submission to the unconstrained subtask of the SIGTYP 2024\nShared Task on Word Embedding Evaluation for Ancient and Historical Languages\nfor morphological annotation, POS-tagging, lemmatization, character- and\nword-level gap-filling. We developed a simple, uniform, and computationally\nlightweight approach based on the adapters framework using parameter-efficient\nfine-tuning. We applied the same adapter-based approach uniformly to all tasks\nand 16 languages by fine-tuning stacked language- and task-specific adapters.\nOur submission obtained an overall second place out of three submissions, with\nthe first place in word-level gap-filling. Our results show the feasibility of\nadapting language models pre-trained on modern languages to historical and\nancient languages via adapter training.", "published": "2024-04-19 12:26:28", "link": "http://arxiv.org/abs/2404.12845v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented\n  Generation", "abstract": "While Retrieval-Augmented Generation (RAG) plays a crucial role in the\napplication of Large Language Models (LLMs), existing retrieval methods in\nknowledge-dense domains like law and medicine still suffer from a lack of\nmulti-perspective views, which are essential for improving interpretability and\nreliability. Previous research on multi-view retrieval often focused solely on\ndifferent semantic forms of queries, neglecting the expression of specific\ndomain knowledge perspectives. This paper introduces a novel multi-view RAG\nframework, MVRAG, tailored for knowledge-dense domains that utilizes\nintention-aware query rewriting from multiple domain viewpoints to enhance\nretrieval precision, thereby improving the effectiveness of the final\ninference. Experiments conducted on legal and medical case retrieval\ndemonstrate significant improvements in recall and precision rates with our\nframework. Our multi-perspective retrieval approach unleashes the potential of\nmulti-view information enhancing RAG tasks, accelerating the further\napplication of LLMs in knowledge-intensive fields.", "published": "2024-04-19 13:27:38", "link": "http://arxiv.org/abs/2404.12879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enabling Natural Zero-Shot Prompting on Encoder Models via\n  Statement-Tuning", "abstract": "While Large Language Models (LLMs) exhibit remarkable capabilities in\nzero-shot and few-shot scenarios, they often require computationally\nprohibitive sizes. Conversely, smaller Masked Language Models (MLMs) like BERT\nand RoBERTa achieve state-of-the-art results through fine-tuning but struggle\nwith extending to few-shot and zero-shot settings due to their architectural\nconstraints. Hence, we propose Statement-Tuning, a technique that models\ndiscriminative tasks as a set of finite statements and trains an encoder model\nto discriminate between the potential statements to determine the label. We do\nStatement-Tuning on multiple tasks to enable cross-task generalization.\nExperimental results demonstrate that Statement-Tuning achieves competitive\nperformance compared to state-of-the-art LLMs with significantly fewer\nparameters. Moreover, the study investigates the impact of several design\nchoices on few-shot and zero-shot generalization, revealing that\nStatement-Tuning can achieve strong performance with modest training data and\nbenefits from task and statement diversity for unseen task generalizability.", "published": "2024-04-19 14:05:03", "link": "http://arxiv.org/abs/2404.12897v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sample Design Engineering: An Empirical Study of What Makes Good\n  Downstream Fine-Tuning Samples for LLMs", "abstract": "In the burgeoning field of Large Language Models (LLMs) like ChatGPT and\nLLaMA, Prompt Engineering (PE) is renowned for boosting zero-shot or in-context\nlearning (ICL) through prompt modifications. Yet, the realm of the sample\ndesign for downstream fine-tuning, crucial for task-specific LLM adaptation, is\nlargely unexplored. This paper introduces Sample Design Engineering (SDE), a\nmethodical approach to enhancing LLMs' post-tuning performance by refining\ninput, output, and reasoning designs. We conduct a series of in-domain (ID) and\nout-of-domain (OOD) experiments to assess the impact of various design options\non LLMs' downstream performance, revealing several intriguing patterns that\nhold consistently across different LLMs. Based on these insights, we propose an\nintegrated SDE strategy, combining the most effective options, and validate its\nconsistent superiority over heuristic sample designs in complex downstream\ntasks like multi-aspect sentiment analysis, event extraction, and nested entity\nrecognition. Additionally, analyses of LLMs' inherent prompt/output perplexity,\nzero-shot, and ICL abilities illustrate that good PE strategies may not always\ntranslate to good SDE strategies. Code available at\nhttps://github.com/beyondguo/LLM-Tuning.", "published": "2024-04-19 17:47:02", "link": "http://arxiv.org/abs/2404.13033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for\n  Text-to-SQL", "abstract": "The current state-of-the-art (SOTA) for automated text-to-SQL still falls\nwell short of expert human performance as measured by execution accuracy (EX)\non the BIRD-SQL benchmark. The most accurate methods are also slow and\nexpensive. To advance the SOTA for text-to-SQL while reducing cost and\nimproving speed, we explore the combination of low-cost fine tuning, novel\nmethods for diverse retrieval-augmented generation (RAG) and new input and\noutput formats that help large language models (LLMs) achieve higher EX. We\nintroduce two new methods, Dubo-SQL v1 and v2. Dubo-SQL v1 sets a new record\nfor EX on the holdout test set of BIRD-SQL. Dubo-SQL v2 achieves even higher\nperformance on the BIRD-SQL dev set. Dubo-SQL v1 relies on LLMs from OpenAI,\nbut uses the low-cost GPT-3.5 Turbo while exceeding the performance of the\nnext-best model using OpenAI, which instead uses the more expensive GPT-4.\nDubo-SQL v1 exceeds the performance of the next-best model using GPT-3.5 by\nover 20%. Dubo-SQL v2 uses GPT-4 Turbo and RAG in place of fine tuning to push\nEX higher.", "published": "2024-04-19 00:48:30", "link": "http://arxiv.org/abs/2404.12560v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Cooperative Sentiment Agents for Multimodal Sentiment Analysis", "abstract": "In this paper, we propose a new Multimodal Representation Learning (MRL)\nmethod for Multimodal Sentiment Analysis (MSA), which facilitates the adaptive\ninteraction between modalities through Cooperative Sentiment Agents, named\nCo-SA. Co-SA comprises two critical components: the Sentiment Agents\nEstablishment (SAE) phase and the Sentiment Agents Cooperation (SAC) phase.\nDuring the SAE phase, each sentiment agent deals with an unimodal signal and\nhighlights explicit dynamic sentiment variations within the modality via the\nModality-Sentiment Disentanglement (MSD) and Deep Phase Space Reconstruction\n(DPSR) modules. Subsequently, in the SAC phase, Co-SA meticulously designs\ntask-specific interaction mechanisms for sentiment agents so that coordinating\nmultimodal signals to learn the joint representation. Specifically, Co-SA\nequips an independent policy model for each sentiment agent that captures\nsignificant properties within the modality. These policies are optimized\nmutually through the unified reward adaptive to downstream tasks. Benefitting\nfrom the rewarding mechanism, Co-SA transcends the limitation of pre-defined\nfusion modes and adaptively captures unimodal properties for MRL in the\nmultimodal interaction setting. To demonstrate the effectiveness of Co-SA, we\napply it to address Multimodal Sentiment Analysis (MSA) and Multimodal Emotion\nRecognition (MER) tasks. Our comprehensive experimental results demonstrate\nthat Co-SA excels at discovering diverse cross-modal features, encompassing\nboth common and complementary aspects. The code can be available at\nhttps://github.com/smwanghhh/Co-SA.", "published": "2024-04-19 05:48:09", "link": "http://arxiv.org/abs/2404.12642v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PDF-MVQA: A Dataset for Multimodal Information Retrieval in PDF-based\n  Visual Question Answering", "abstract": "Document Question Answering (QA) presents a challenge in understanding\nvisually-rich documents (VRD), particularly those dominated by lengthy textual\ncontent like research journal articles. Existing studies primarily focus on\nreal-world documents with sparse text, while challenges persist in\ncomprehending the hierarchical semantic relations among multiple pages to\nlocate multimodal components. To address this gap, we propose PDF-MVQA, which\nis tailored for research journal articles, encompassing multiple pages and\nmultimodal information retrieval. Unlike traditional machine reading\ncomprehension (MRC) tasks, our approach aims to retrieve entire paragraphs\ncontaining answers or visually rich document entities like tables and figures.\nOur contributions include the introduction of a comprehensive PDF Document VQA\ndataset, allowing the examination of semantically hierarchical layout\nstructures in text-dominant documents. We also present new VRD-QA frameworks\ndesigned to grasp textual contents and relations among document layouts\nsimultaneously, extending page-level understanding to the entire multi-page\ndocument. Through this work, we aim to enhance the capabilities of existing\nvision-and-language models in handling challenges posed by text-dominant\ndocuments in VRD-QA.", "published": "2024-04-19 09:00:05", "link": "http://arxiv.org/abs/2404.12720v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Beyond Human Norms: Unveiling Unique Values of Large Language Models\n  through Interdisciplinary Approaches", "abstract": "Recent advancements in Large Language Models (LLMs) have revolutionized the\nAI field but also pose potential safety and ethical risks. Deciphering LLMs'\nembedded values becomes crucial for assessing and mitigating their risks.\nDespite extensive investigation into LLMs' values, previous studies heavily\nrely on human-oriented value systems in social sciences. Then, a natural\nquestion arises: Do LLMs possess unique values beyond those of humans? Delving\ninto it, this work proposes a novel framework, ValueLex, to reconstruct LLMs'\nunique value system from scratch, leveraging psychological methodologies from\nhuman personality/value research. Based on Lexical Hypothesis, ValueLex\nintroduces a generative approach to elicit diverse values from 30+ LLMs,\nsynthesizing a taxonomy that culminates in a comprehensive value framework via\nfactor analysis and semantic clustering. We identify three core value\ndimensions, Competence, Character, and Integrity, each with specific\nsubdimensions, revealing that LLMs possess a structured, albeit non-human,\nvalue system. Based on this system, we further develop tailored projective\ntests to evaluate and analyze the value inclinations of LLMs across different\nmodel sizes, training methods, and data sources. Our framework fosters an\ninterdisciplinary paradigm of understanding LLMs, paving the way for future AI\nalignment and regulation.", "published": "2024-04-19 09:44:51", "link": "http://arxiv.org/abs/2404.12744v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AutoScraper: A Progressive Understanding Web Agent for Web Scraper\n  Generation", "abstract": "Web scraping is a powerful technique that extracts data from websites,\nenabling automated data collection, enhancing data analysis capabilities, and\nminimizing manual data entry efforts. Existing methods, wrappers-based methods\nsuffer from limited adaptability and scalability when faced with a new website,\nwhile language agents, empowered by large language models (LLMs), exhibit poor\nreusability in diverse web environments. In this work, we introduce the\nparadigm of generating web scrapers with LLMs and propose AutoScraper, a\ntwo-stage framework that can handle diverse and changing web environments more\nefficiently. AutoScraper leverages the hierarchical structure of HTML and\nsimilarity across different web pages for generating web scrapers. Besides, we\npropose a new executability metric for better measuring the performance of web\nscraper generation tasks. We conduct comprehensive experiments with multiple\nLLMs and demonstrate the effectiveness of our framework. Resources of this\npaper can be found at \\url{https://github.com/EZ-hwh/AutoScraper}", "published": "2024-04-19 09:59:44", "link": "http://arxiv.org/abs/2404.12753v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Logically Consistent Language Models via Probabilistic Reasoning", "abstract": "Large language models (LLMs) are a promising venue for natural language\nunderstanding and generation tasks. However, current LLMs are far from\nreliable: they are prone to generate non-factual information and, more\ncrucially, to contradict themselves when prompted to reason about beliefs of\nthe world. These problems are currently addressed with large scale fine-tuning\nor by delegating consistent reasoning to external tools. In this work, we\nstrive for a middle ground and introduce a training objective based on\nprincipled probabilistic reasoning that teaches a LLM to be consistent with\nexternal knowledge in the form of a set of facts and rules. Fine-tuning with\nour loss on a limited set of facts enables our LLMs to be more logically\nconsistent than previous baselines and allows them to extrapolate to unseen but\nsemantically similar factual knowledge more systematically.", "published": "2024-04-19 12:23:57", "link": "http://arxiv.org/abs/2404.12843v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for\n  Boosting Query Efficiency", "abstract": "Query rewrite, which aims to generate more efficient queries by altering a\nSQL query's structure without changing the query result, has been an important\nresearch problem. In order to maintain equivalence between the rewritten query\nand the original one during rewriting, traditional query rewrite methods always\nrewrite the queries following certain rewrite rules. However, some problems\nstill remain. Firstly, existing methods of finding the optimal choice or\nsequence of rewrite rules are still limited and the process always costs a lot\nof resources. Methods involving discovering new rewrite rules typically require\ncomplicated proofs of structural logic or extensive user interactions.\nSecondly, current query rewrite methods usually rely highly on DBMS cost\nestimators which are often not accurate. In this paper, we address these\nproblems by proposing a novel method of query rewrite named LLM-R2, adopting a\nlarge language model (LLM) to propose possible rewrite rules for a database\nrewrite system. To further improve the inference ability of LLM in recommending\nrewrite rules, we train a contrastive model by curriculum to learn query\nrepresentations and select effective query demonstrations for the LLM.\nExperimental results have shown that our method can significantly improve the\nquery execution efficiency and outperform the baseline methods. In addition,\nour method enjoys high robustness across different datasets.", "published": "2024-04-19 13:17:07", "link": "http://arxiv.org/abs/2404.12872v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Cross-cultural Inspiration Detection and Analysis in Real and\n  LLM-generated Social Media Data", "abstract": "Inspiration is linked to various positive outcomes, such as increased\ncreativity, productivity, and happiness. Although inspiration has great\npotential, there has been limited effort toward identifying content that is\ninspiring, as opposed to just engaging or positive. Additionally, most research\nhas concentrated on Western data, with little attention paid to other cultures.\nThis work is the first to study cross-cultural inspiration through machine\nlearning methods. We aim to identify and analyze real and AI-generated\ncross-cultural inspiring posts. To this end, we compile and make publicly\navailable the InspAIred dataset, which consists of 2,000 real inspiring posts,\n2,000 real non-inspiring posts, and 2,000 generated inspiring posts evenly\ndistributed across India and the UK. The real posts are sourced from Reddit,\nwhile the generated posts are created using the GPT-4 model. Using this\ndataset, we conduct extensive computational linguistic analyses to (1) compare\ninspiring content across cultures, (2) compare AI-generated inspiring posts to\nreal inspiring posts, and (3) determine if detection models can accurately\ndistinguish between inspiring content across cultures and data sources.", "published": "2024-04-19 15:04:30", "link": "http://arxiv.org/abs/2404.12933v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel\n  Reviews", "abstract": "Deceptive reviews are becoming increasingly common, especially given the\nincrease in performance and the prevalence of LLMs. While work to date has\naddressed the development of models to differentiate between truthful and\ndeceptive human reviews, much less is known about the distinction between real\nreviews and AI-authored fake reviews. Moreover, most of the research so far has\nfocused primarily on English, with very little work dedicated to other\nlanguages. In this paper, we compile and make publicly available the MAiDE-up\ndataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews,\nbalanced across ten languages. Using this dataset, we conduct extensive\nlinguistic analyses to (1) compare the AI fake hotel reviews to real hotel\nreviews, and (2) identify the factors that influence the deception detection\nmodel performance. We explore the effectiveness of several models for deception\ndetection in hotel reviews across three main dimensions: sentiment, location,\nand language. We find that these dimensions influence how well we can detect\nAI-generated fake reviews.", "published": "2024-04-19 15:08:06", "link": "http://arxiv.org/abs/2404.12938v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt\n  Many-Shot Based Factual Knowledge Extraction", "abstract": "In this paper, we focus on the challenging task of reliably estimating\nfactual knowledge that is embedded inside large language models (LLMs). To\navoid reliability concerns with prior approaches, we propose to eliminate\nprompt engineering when probing LLMs for factual knowledge. Our approach,\ncalled Zero-Prompt Latent Knowledge Estimator (ZP-LKE), leverages the\nin-context learning ability of LLMs to communicate both the factual knowledge\nquestion as well as the expected answer format. Our knowledge estimator is both\nconceptually simpler (i.e., doesn't depend on meta-linguistic judgments of\nLLMs) and easier to apply (i.e., is not LLM-specific), and we demonstrate that\nit can surface more of the latent knowledge embedded in LLMs. We also\ninvestigate how different design choices affect the performance of ZP-LKE.\nUsing the proposed estimator, we perform a large-scale evaluation of the\nfactual knowledge of a variety of open-source LLMs, like OPT, Pythia, Llama(2),\nMistral, Gemma, etc. over a large set of relations and facts from the Wikidata\nknowledge base. We observe differences in the factual knowledge between\ndifferent model families and models of different sizes, that some relations are\nconsistently better known than others but that models differ in the precise\nfacts they know, and differences in the knowledge of base models and their\nfinetuned counterparts. Code available at:\nhttps://github.com/QinyuanWu0710/ZeroPrompt_LKE", "published": "2024-04-19 15:40:39", "link": "http://arxiv.org/abs/2404.12957v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking the Evaluation of Dialogue Systems: Effects of User Feedback\n  on Crowdworkers and LLMs", "abstract": "In ad-hoc retrieval, evaluation relies heavily on user actions, including\nimplicit feedback. In a conversational setting such signals are usually\nunavailable due to the nature of the interactions, and, instead, the evaluation\noften relies on crowdsourced evaluation labels. The role of user feedback in\nannotators' assessment of turns in a conversational perception has been little\nstudied. We focus on how the evaluation of task-oriented dialogue systems\n(TDSs), is affected by considering user feedback, explicit or implicit, as\nprovided through the follow-up utterance of a turn being evaluated. We explore\nand compare two methodologies for assessing TDSs: one includes the user's\nfollow-up utterance and one without. We use both crowdworkers and large\nlanguage models (LLMs) as annotators to assess system responses across four\naspects: relevance, usefulness, interestingness, and explanation quality. Our\nfindings indicate that there is a distinct difference in ratings assigned by\nboth annotator groups in the two setups, indicating user feedback does\ninfluence system evaluation. Workers are more susceptible to user feedback on\nusefulness and interestingness compared to LLMs on interestingness and\nrelevance. User feedback leads to a more personalized assessment of usefulness\nby workers, aligning closely with the user's explicit feedback. Additionally,\nin cases of ambiguous or complex user requests, user feedback improves\nagreement among crowdworkers. These findings emphasize the significance of user\nfeedback in refining system evaluations and suggest the potential for automated\nfeedback integration in future research. We publicly release the annotated data\nto foster research in this area.", "published": "2024-04-19 16:45:50", "link": "http://arxiv.org/abs/2404.12994v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Stronger Random Baselines for In-Context Learning", "abstract": "Evaluating the in-context learning classification performance of language\nmodels poses challenges due to small dataset sizes, extensive prompt-selection\nusing the validation set, and intentionally difficult tasks that lead to\nnear-random performance. The standard random baseline--the expected accuracy of\nguessing labels uniformly at random--is stable when the evaluation set is used\nonly once or when the dataset is large. We account for the common practice of\nvalidation set reuse and existing small datasets with a stronger random\nbaseline: the expected maximum accuracy across multiple random classifiers.\nWhen choosing the best prompt demonstrations across six quantized language\nmodels applied to 16 BIG-bench Lite tasks, more than 20% of the few-shot\nresults that exceed the standard baseline do not exceed this stronger random\nbaseline. When held-out test sets are available, this stronger baseline is also\na better predictor of held-out performance than the standard baseline, avoiding\nunnecessary test set evaluations. This maximum random baseline provides an\neasily calculated drop-in replacement for the standard baseline.", "published": "2024-04-19 17:30:10", "link": "http://arxiv.org/abs/2404.13020v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LaPA: Latent Prompt Assist Model For Medical Visual Question Answering", "abstract": "Medical visual question answering (Med-VQA) aims to automate the prediction\nof correct answers for medical images and questions, thereby assisting\nphysicians in reducing repetitive tasks and alleviating their workload.\nExisting approaches primarily focus on pre-training models using additional and\ncomprehensive datasets, followed by fine-tuning to enhance performance in\ndownstream tasks. However, there is also significant value in exploring\nexisting models to extract clinically relevant information. In this paper, we\npropose the Latent Prompt Assist model (LaPA) for medical visual question\nanswering. Firstly, we design a latent prompt generation module to generate the\nlatent prompt with the constraint of the target answer. Subsequently, we\npropose a multi-modal fusion block with latent prompt fusion module that\nutilizes the latent prompt to extract clinical-relevant information from\nuni-modal and multi-modal features. Additionally, we introduce a prior\nknowledge fusion module to integrate the relationship between diseases and\norgans with the clinical-relevant information. Finally, we combine the final\nintegrated information with image-language cross-modal information to predict\nthe final answers. Experimental results on three publicly available Med-VQA\ndatasets demonstrate that LaPA outperforms the state-of-the-art model ARL,\nachieving improvements of 1.83%, 0.63%, and 1.80% on VQA-RAD, SLAKE, and\nVQA-2019, respectively. The code is publicly available at\nhttps://github.com/GaryGuTC/LaPA_model.", "published": "2024-04-19 17:51:52", "link": "http://arxiv.org/abs/2404.13039v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mathify: Evaluating Large Language Models on Mathematical Problem\n  Solving Tasks", "abstract": "The rapid progress in the field of natural language processing (NLP) systems\nand the expansion of large language models (LLMs) have opened up numerous\nopportunities in the field of education and instructional methods. These\nadvancements offer the potential for tailored learning experiences and\nimmediate feedback, all delivered through accessible and cost-effective\nservices. One notable application area for this technological advancement is in\nthe realm of solving mathematical problems. Mathematical problem-solving not\nonly requires the ability to decipher complex problem statements but also the\nskill to perform precise arithmetic calculations at each step of the\nproblem-solving process. However, the evaluation of the arithmetic capabilities\nof large language models remains an area that has received relatively little\nattention. In response, we introduce an extensive mathematics dataset called\n\"MathQuest\" sourced from the 11th and 12th standard Mathematics NCERT\ntextbooks. This dataset encompasses mathematical challenges of varying\ncomplexity and covers a wide range of mathematical concepts. Utilizing this\ndataset, we conduct fine-tuning experiments with three prominent LLMs: LLaMA-2,\nWizardMath, and MAmmoTH. These fine-tuned models serve as benchmarks for\nevaluating their performance on our dataset. Our experiments reveal that among\nthe three models, MAmmoTH-13B emerges as the most proficient, achieving the\nhighest level of competence in solving the presented mathematical problems.\nConsequently, MAmmoTH-13B establishes itself as a robust and dependable\nbenchmark for addressing NCERT mathematics problems.", "published": "2024-04-19 08:45:42", "link": "http://arxiv.org/abs/2404.13099v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi Class Depression Detection Through Tweets using Artificial\n  Intelligence", "abstract": "Depression is a significant issue nowadays. As per the World Health\nOrganization (WHO), in 2023, over 280 million individuals are grappling with\ndepression. This is a huge number; if not taken seriously, these numbers will\nincrease rapidly. About 4.89 billion individuals are social media users. People\nexpress their feelings and emotions on platforms like Twitter, Facebook,\nReddit, Instagram, etc. These platforms contain valuable information which can\nbe used for research purposes. Considerable research has been conducted across\nvarious social media platforms. However, certain limitations persist in these\nendeavors. Particularly, previous studies were only focused on detecting\ndepression and the intensity of depression in tweets. Also, there existed\ninaccuracies in dataset labeling. In this research work, five types of\ndepression (Bipolar, major, psychotic, atypical, and postpartum) were predicted\nusing tweets from the Twitter database based on lexicon labeling. Explainable\nAI was used to provide reasoning by highlighting the parts of tweets that\nrepresent type of depression. Bidirectional Encoder Representations from\nTransformers (BERT) was used for feature extraction and training. Machine\nlearning and deep learning methodologies were used to train the model. The BERT\nmodel presented the most promising results, achieving an overall accuracy of\n0.96.", "published": "2024-04-19 12:47:56", "link": "http://arxiv.org/abs/2404.13104v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and\n  Accuracy of LLMs in Cancer Staging", "abstract": "Advances in large language models (LLMs) have encouraged their adoption in\nthe healthcare domain where vital clinical information is often contained in\nunstructured notes. Cancer staging status is available in clinical reports, but\nit requires natural language processing to extract the status from the\nunstructured text. With the advance in clinical-oriented LLMs, it is promising\nto extract such status without extensive efforts in training the algorithms.\nPrompting approaches of the pre-trained LLMs that elicit a model's reasoning\nprocess, such as chain-of-thought, may help to improve the trustworthiness of\nthe generated responses. Using self-consistency further improves model\nperformance, but often results in inconsistent generations across the multiple\nreasoning paths. In this study, we propose an ensemble reasoning approach with\nthe aim of improving the consistency of the model generations. Using an open\naccess clinical large language model to determine the pathologic cancer stage\nfrom real-world pathology reports, we show that the ensemble reasoning approach\nis able to improve both the consistency and performance of the LLM in\ndetermining cancer stage, thereby demonstrating the potential to use these\nmodels in clinical or other domains where reliability and trustworthiness are\ncritical.", "published": "2024-04-19 19:34:35", "link": "http://arxiv.org/abs/2404.13149v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Heterogeneous Subgraph Transformer for Fake News Detection", "abstract": "Fake news is pervasive on social media, inflicting substantial harm on public\ndiscourse and societal well-being. We investigate the explicit structural\ninformation and textual features of news pieces by constructing a heterogeneous\ngraph concerning the relations among news topics, entities, and content.\nThrough our study, we reveal that fake news can be effectively detected in\nterms of the atypical heterogeneous subgraphs centered on them, which\nencapsulate the essential semantics and intricate relations between news\nelements. However, suffering from the heterogeneity, exploring such\nheterogeneous subgraphs remains an open problem. To bridge the gap, this work\nproposes a heterogeneous subgraph transformer (HeteroSGT) to exploit subgraphs\nin our constructed heterogeneous graph. In HeteroSGT, we first employ a\npre-trained language model to derive both word-level and sentence-level\nsemantics. Then the random walk with restart (RWR) is applied to extract\nsubgraphs centered on each news, which are further fed to our proposed subgraph\nTransformer to quantify the authenticity. Extensive experiments on five\nreal-world datasets demonstrate the superior performance of HeteroSGT over five\nbaselines. Further case and ablation studies validate our motivation and\ndemonstrate that performance improvement stems from our specially designed\ncomponents.", "published": "2024-04-19 21:39:37", "link": "http://arxiv.org/abs/2404.13192v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TopoLedgerBERT: Topological Learning of Ledger Description Embeddings\n  using Siamese BERT-Networks", "abstract": "This paper addresses a long-standing problem in the field of accounting:\nmapping company-specific ledger accounts to a standardized chart of accounts.\nWe propose a novel solution, TopoLedgerBERT, a unique sentence embedding method\ndevised specifically for ledger account mapping. This model integrates\nhierarchical information from the charts of accounts into the sentence\nembedding process, aiming to accurately capture both the semantic similarity\nand the hierarchical structure of the ledger accounts. In addition, we\nintroduce a data augmentation strategy that enriches the training data and, as\na result, increases the performance of our proposed model. Compared to\nbenchmark methods, TopoLedgerBERT demonstrates superior performance in terms of\naccuracy and mean reciprocal rank.", "published": "2024-04-19 13:20:05", "link": "http://arxiv.org/abs/2407.05175v1", "categories": ["cs.CE", "cs.CL"], "primary_category": "cs.CE"}
{"title": "iTBLS: A Dataset of Interactive Conversations Over Tabular Information", "abstract": "This paper introduces Interactive Tables (iTBLS), a dataset of interactive\nconversations situated in tables from scientific articles. This dataset is\ndesigned to facilitate human-AI collaborative problem-solving through\nAI-powered multi-task tabular capabilities. In contrast to prior work that\nmodels interactions as factoid QA or procedure synthesis, iTBLS broadens the\nscope of interactions to include mathematical reasoning, natural language\nmanipulation, and expansion of existing tables from natural language\nconversation by delineating interactions into one of three tasks:\ninterpretation, modification, or generation. Additionally, the paper presents a\nsuite of baseline approaches to iTBLS, utilizing zero-shot prompting and\nparameter-efficient fine-tuning for different computing situations. We also\nintroduce a novel multi-step approach and show how it can be leveraged in\nconjunction with parameter-efficient fine-tuning to achieve the\nstate-of-the-art on iTBLS; outperforming standard parameter-efficient\nfine-tuning by up to 15% on interpretation, 18% on modification, and 38% on\ngeneration.", "published": "2024-04-19 02:11:41", "link": "http://arxiv.org/abs/2404.12580v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter Efficient Diverse Paraphrase Generation Using Sequence-Level\n  Knowledge Distillation", "abstract": "Over the past year, the field of Natural Language Generation (NLG) has\nexperienced an exponential surge, largely due to the introduction of Large\nLanguage Models (LLMs). These models have exhibited the most effective\nperformance in a range of domains within the Natural Language Processing and\nGeneration domains. However, their application in domain-specific tasks, such\nas paraphrasing, presents significant challenges. The extensive number of\nparameters makes them difficult to operate on commercial hardware, and they\nrequire substantial time for inference, leading to high costs in a production\nsetting. In this study, we tackle these obstacles by employing LLMs to develop\nthree distinct models for the paraphrasing field, applying a method referred to\nas sequence-level knowledge distillation. These distilled models are capable of\nmaintaining the quality of paraphrases generated by the LLM. They demonstrate\nfaster inference times and the ability to generate diverse paraphrases of\ncomparable quality. A notable characteristic of these models is their ability\nto exhibit syntactic diversity while also preserving lexical diversity,\nfeatures previously uncommon due to existing data quality issues in datasets\nand not typically observed in neural-based approaches. Human evaluation of our\nmodels shows that there is only a 4% drop in performance compared to the LLM\nteacher model used in the distillation process, despite being 1000 times\nsmaller. This research provides a significant contribution to the NLG field,\noffering a more efficient and cost-effective solution for paraphrasing tasks.", "published": "2024-04-19 02:59:09", "link": "http://arxiv.org/abs/2404.12596v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive\n  Learning for Table Representations", "abstract": "Spreadsheets are widely recognized as the most popular end-user programming\ntools, which blend the power of formula-based computation, with an intuitive\ntable-based interface. Today, spreadsheets are used by billions of users to\nmanipulate tables, most of whom are neither database experts nor professional\nprogrammers.\n  Despite the success of spreadsheets, authoring complex formulas remains\nchallenging, as non-technical users need to look up and understand non-trivial\nformula syntax. To address this pain point, we leverage the observation that\nthere is often an abundance of similar-looking spreadsheets in the same\norganization, which not only have similar data, but also share similar\ncomputation logic encoded as formulas. We develop an Auto-Formula system that\ncan accurately predict formulas that users want to author in a target\nspreadsheet cell, by learning and adapting formulas that already exist in\nsimilar spreadsheets, using contrastive-learning techniques inspired by\n\"similar-face recognition\" from compute vision.\n  Extensive evaluations on over 2K test formulas extracted from real enterprise\nspreadsheets show the effectiveness of Auto-Formula over alternatives. Our\nbenchmark data is available at https://github.com/microsoft/Auto-Formula to\nfacilitate future research.", "published": "2024-04-19 03:28:18", "link": "http://arxiv.org/abs/2404.12608v1", "categories": ["cs.DB", "cs.CL", "cs.PL"], "primary_category": "cs.DB"}
{"title": "CORI: CJKV Benchmark with Romanization Integration -- A step towards\n  Cross-lingual Transfer Beyond Textual Scripts", "abstract": "Naively assuming English as a source language may hinder cross-lingual\ntransfer for many languages by failing to consider the importance of language\ncontact. Some languages are more well-connected than others, and target\nlanguages can benefit from transferring from closely related languages; for\nmany languages, the set of closely related languages does not include English.\nIn this work, we study the impact of source language for cross-lingual\ntransfer, demonstrating the importance of selecting source languages that have\nhigh contact with the target language. We also construct a novel benchmark\ndataset for close contact Chinese-Japanese-Korean-Vietnamese (CJKV) languages\nto further encourage in-depth studies of language contact. To comprehensively\ncapture contact between these languages, we propose to integrate Romanized\ntranscription beyond textual scripts via Contrastive Learning objectives,\nleading to enhanced cross-lingual representations and effective zero-shot\ncross-lingual transfer.", "published": "2024-04-19 04:02:50", "link": "http://arxiv.org/abs/2404.12618v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pre-trained Vision-Language Models Learn Discoverable Visual Concepts", "abstract": "Do vision-language models (VLMs) pre-trained to caption an image of a\n\"durian\" learn visual concepts such as \"brown\" (color) and \"spiky\" (texture) at\nthe same time? We aim to answer this question as visual concepts learned \"for\nfree\" would enable wide applications such as neuro-symbolic reasoning or\nhuman-interpretable object classification. We assume that the visual concepts,\nif captured by pre-trained VLMs, can be extracted by their vision-language\ninterface with text-based concept prompts. We observe that recent works\nprompting VLMs with concepts often differ in their strategies to define and\nevaluate the visual concepts, leading to conflicting conclusions. We propose a\nnew concept definition strategy based on two observations: First, certain\nconcept prompts include shortcuts that recognize correct concepts for wrong\nreasons; Second, multimodal information (e.g. visual discriminativeness, and\ntextual knowledge) should be leveraged when selecting the concepts. Our\nproposed concept discovery and learning (CDL) framework is thus designed to\nidentify a diverse list of generic visual concepts (e.g. \"spiky\" as opposed to\n\"spiky durian\"), which are ranked and selected based on visual and language\nmutual information. We carefully design quantitative and human evaluations of\nthe discovered concepts on six diverse visual recognition datasets, which\nconfirm that pre-trained VLMs do learn visual concepts that provide accurate\nand thorough descriptions for the recognized objects. All code and models are\npublicly released.", "published": "2024-04-19 06:41:32", "link": "http://arxiv.org/abs/2404.12652v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards Human-centered Proactive Conversational Agents", "abstract": "Recent research on proactive conversational agents (PCAs) mainly focuses on\nimproving the system's capabilities in anticipating and planning action\nsequences to accomplish tasks and achieve goals before users articulate their\nrequests. This perspectives paper highlights the importance of moving towards\nbuilding human-centered PCAs that emphasize human needs and expectations, and\nthat considers ethical and social implications of these agents, rather than\nsolely focusing on technological capabilities. The distinction between a\nproactive and a reactive system lies in the proactive system's\ninitiative-taking nature. Without thoughtful design, proactive systems risk\nbeing perceived as intrusive by human users. We address the issue by\nestablishing a new taxonomy concerning three key dimensions of human-centered\nPCAs, namely Intelligence, Adaptivity, and Civility. We discuss potential\nresearch opportunities and challenges based on this new taxonomy upon the five\nstages of PCA system construction. This perspectives paper lays a foundation\nfor the emerging area of conversational information retrieval research and\npaves the way towards advancing human-centered proactive conversational\nsystems.", "published": "2024-04-19 07:14:31", "link": "http://arxiv.org/abs/2404.12670v1", "categories": ["cs.IR", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "How Does the Textual Information Affect the Retrieval of Multimodal\n  In-Context Learning?", "abstract": "The increase in parameter size of multimodal large language models (MLLMs)\nintroduces significant capabilities, particularly in-context learning, where\nMLLMs enhance task performance without updating pre-trained parameters. This\neffectiveness, however, hinges on the appropriate selection of in-context\nexamples, a process that is currently biased towards visual data, overlooking\ntextual information. Furthermore, the area of supervised retrievers for MLLMs,\ncrucial for optimal in-context example selection, continues to be\nuninvestigated. Our study offers an in-depth evaluation of the impact of\ntextual information on the unsupervised selection of in-context examples in\nmultimodal contexts, uncovering a notable sensitivity of retriever performance\nto the employed modalities. Responding to this, we introduce a novel supervised\nMLLM-retriever MSIER that employs a neural network to select examples that\nenhance multimodal in-context learning efficiency. This approach is validated\nthrough extensive testing across three distinct tasks, demonstrating the\nmethod's effectiveness. Additionally, we investigate the influence of\nmodalities on our supervised retrieval method's training and pinpoint factors\ncontributing to our model's success. This exploration paves the way for future\nadvancements, highlighting the potential for refined in-context learning in\nMLLMs through the strategic use of multimodal data.", "published": "2024-04-19 13:05:37", "link": "http://arxiv.org/abs/2404.12866v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large\n  Language Models", "abstract": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded\nand fine-grained visual perception ability. Beyond holistic image\nunderstanding, Groma is adept at region-level tasks such as region captioning\nand visual grounding. Such capabilities are built upon a localized visual\ntokenization mechanism, where an image input is decomposed into regions of\ninterest and subsequently encoded into region tokens. By integrating region\ntokens into user instructions and model responses, we seamlessly enable Groma\nto understand user-specified region inputs and ground its textual output to\nimages. Besides, to enhance the grounded chat ability of Groma, we curate a\nvisually grounded instruction dataset by leveraging the powerful GPT-4V and\nvisual prompting techniques. Compared with MLLMs that rely on the language\nmodel or external module for localization, Groma consistently demonstrates\nsuperior performances in standard referring and grounding benchmarks,\nhighlighting the advantages of embedding localization into image tokenization.\nProject page: https://groma-mllm.github.io/.", "published": "2024-04-19 17:22:51", "link": "http://arxiv.org/abs/2404.13013v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Data Alignment for Zero-Shot Concept Generation in Dermatology AI", "abstract": "AI in dermatology is evolving at a rapid pace but the major limitation to\ntraining trustworthy classifiers is the scarcity of data with ground-truth\nconcept level labels, which are meta-labels semantically meaningful to humans.\nFoundation models like CLIP providing zero-shot capabilities can help alleviate\nthis challenge by leveraging vast amounts of image-caption pairs available on\nthe internet. CLIP can be fine-tuned using domain specific image-caption pairs\nto improve classification performance. However, CLIP's pre-training data is not\nwell-aligned with the medical jargon that clinicians use to perform diagnoses.\nThe development of large language models (LLMs) in recent years has led to the\npossibility of leveraging the expressive nature of these models to generate\nrich text. Our goal is to use these models to generate caption text that aligns\nwell with both the clinical lexicon and with the natural human language used in\nCLIP's pre-training data. Starting with captions used for images in PubMed\narticles, we extend them by passing the raw captions through an LLM fine-tuned\non the field's several textbooks. We find that using captions generated by an\nexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept\nclassification performance.", "published": "2024-04-19 17:57:29", "link": "http://arxiv.org/abs/2404.13043v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Course-Skill Atlas: A national longitudinal dataset of skills taught in\n  U.S. higher education curricula", "abstract": "Higher education plays a critical role in driving an innovative economy by\nequipping students with knowledge and skills demanded by the workforce. While\nresearchers and practitioners have developed data systems to track detailed\noccupational skills, such as those established by the U.S. Department of Labor\n(DOL), much less effort has been made to document which of these skills are\nbeing developed in higher education at a similar granularity. Here, we fill\nthis gap by presenting Course-Skill Atlas -- a longitudinal dataset of skills\ninferred from over three million course syllabi taught at nearly three thousand\nU.S. higher education institutions. To construct Course-Skill Atlas, we apply\nnatural language processing to quantify the alignment between course syllabi\nand detailed workplace activities (DWAs) used by the DOL to describe\noccupations. We then aggregate these alignment scores to create skill profiles\nfor institutions and academic majors. Our dataset offers a large-scale\nrepresentation of college education's role in preparing students for the labor\nmarket. Overall, Course-Skill Atlas can enable new research on the source of\nskills in the context of workforce development and provide actionable insights\nfor shaping the future of higher education to meet evolving labor demands,\nespecially in the face of new technologies.", "published": "2024-04-19 20:14:15", "link": "http://arxiv.org/abs/2404.13163v2", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged\n  Instructions", "abstract": "Today's LLMs are susceptible to prompt injections, jailbreaks, and other\nattacks that allow adversaries to overwrite a model's original instructions\nwith their own malicious prompts. In this work, we argue that one of the\nprimary vulnerabilities underlying these attacks is that LLMs often consider\nsystem prompts (e.g., text from an application developer) to be the same\npriority as text from untrusted users and third parties. To address this, we\npropose an instruction hierarchy that explicitly defines how models should\nbehave when instructions of different priorities conflict. We then propose a\ndata generation method to demonstrate this hierarchical instruction following\nbehavior, which teaches LLMs to selectively ignore lower-privileged\ninstructions. We apply this method to GPT-3.5, showing that it drastically\nincreases robustness -- even for attack types not seen during training -- while\nimposing minimal degradations on standard capabilities.", "published": "2024-04-19 22:55:23", "link": "http://arxiv.org/abs/2404.13208v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Enhancing Generalization in Audio Deepfake Detection: A Neural Collapse\n  based Sampling and Training Approach", "abstract": "Generalization in audio deepfake detection presents a significant challenge,\nwith models trained on specific datasets often struggling to detect deepfakes\ngenerated under varying conditions and unknown algorithms. While collectively\ntraining a model using diverse datasets can enhance its generalization ability,\nit comes with high computational costs. To address this, we propose a neural\ncollapse-based sampling approach applied to pre-trained models trained on\ndistinct datasets to create a new training database. Using ASVspoof 2019\ndataset as a proof-of-concept, we implement pre-trained models with Resnet and\nConvNext architectures. Our approach demonstrates comparable generalization on\nunseen data while being computationally efficient, requiring less training\ndata. Evaluation is conducted using the In-the-wild dataset.", "published": "2024-04-19 17:13:21", "link": "http://arxiv.org/abs/2404.13008v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On fusing active and passive acoustic sensing for simultaneous\n  localization and mapping", "abstract": "Studies on the social behaviors of bats show that they have the ability to\neavesdrop on the signals emitted by conspecifics in their vicinity. They can\nfuse this ``passive\" data with actively collected data from their own signals\nto get more information about their environment, allowing them to fly and hunt\nmore efficiently and to avoid or cause jamming when competing for prey.\nAcoustic sensors are capable of similar feats but are generally used in only an\nactive or passive capacity at one time. Is there a benefit to using both active\nand passive sensing simultaneously in the same array? In this work we define a\nfamily of models for active, passive, and fused sensing systems to measure\nrange and bearing data from an environment defined by point-based landmarks.\nThese measurements are used to solve the problem of simultaneous localization\nand mapping (SLAM) with extended Kalman filter (EKF) and FastSLAM 2.0\napproaches. Our results show agreement with previous findings. Specifically,\nwhen active sensing is limited to a narrow angular range, fused sensing can\nperform just as accurately if not better, while also allowing the sensor to\nperceive more of the surrounding environment.", "published": "2024-04-19 18:06:20", "link": "http://arxiv.org/abs/2404.13116v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "TRNet: Two-level Refinement Network leveraging Speech Enhancement for\n  Noise Robust Speech Emotion Recognition", "abstract": "One persistent challenge in Speech Emotion Recognition (SER) is the\nubiquitous environmental noise, which frequently results in deteriorating SER\nperformance in practice. In this paper, we introduce a Two-level Refinement\nNetwork, dubbed TRNet, to address this challenge. Specifically, a pre-trained\nspeech enhancement module is employed for front-end noise reduction and noise\nlevel estimation. Later, we utilize clean speech spectrograms and their\ncorresponding deep representations as reference signals to refine the\nspectrogram distortion and representation shift of enhanced speech during model\ntraining. Experimental results validate that the proposed TRNet substantially\npromotes the robustness of the proposed system in both matched and unmatched\nnoisy environments, without compromising its performance in noise-free\nenvironments.", "published": "2024-04-19 16:09:17", "link": "http://arxiv.org/abs/2404.12979v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Intro to Quantum Harmony: Chords in Superposition", "abstract": "Correlations between quantum theory and music theory - specifically between\nprinciples of quantum computing and musical harmony - can lead to new\nunderstandings and new methodologies for music theorists and composers. The\nquantum principle of superposition is shown to be closely related to different\ninterpretations of musical meaning. Superposition is implemented directly in\nthe authors' simulations of quantum computing, as applied in the\ndecision-making processes of computer-generated music composition.", "published": "2024-04-19 18:59:43", "link": "http://arxiv.org/abs/2404.13140v1", "categories": ["quant-ph", "cs.ET", "cs.SD", "eess.AS"], "primary_category": "quant-ph"}
{"title": "Separate in the Speech Chain: Cross-Modal Conditional Audio-Visual\n  Target Speech Extraction", "abstract": "The integration of visual cues has revitalized the performance of the target\nspeech extraction task, elevating it to the forefront of the field.\nNevertheless, this multi-modal learning paradigm often encounters the challenge\nof modality imbalance. In audio-visual target speech extraction tasks, the\naudio modality tends to dominate, potentially overshadowing the importance of\nvisual guidance. To tackle this issue, we propose AVSepChain, drawing\ninspiration from the speech chain concept. Our approach partitions the\naudio-visual target speech extraction task into two stages: speech perception\nand speech production. In the speech perception stage, audio serves as the\ndominant modality, while visual information acts as the conditional modality.\nConversely, in the speech production stage, the roles are reversed. This\ntransformation of modality status aims to alleviate the problem of modality\nimbalance. Additionally, we introduce a contrastive semantic matching loss to\nensure that the semantic information conveyed by the generated speech aligns\nwith the semantic information conveyed by lip movements during the speech\nproduction stage. Through extensive experiments conducted on multiple benchmark\ndatasets for audio-visual target speech extraction, we showcase the superior\nperformance achieved by our proposed method.", "published": "2024-04-19 09:08:44", "link": "http://arxiv.org/abs/2404.12725v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
