{"title": "Detecting Context Dependence in Exercise Item Candidates Selected from\n  Corpora", "abstract": "We explore the factors influencing the dependence of single sentences on\ntheir larger textual context in order to automatically identify candidate\nsentences for language learning exercises from corpora which are presentable in\nisolation. An in-depth investigation of this question has not been previously\ncarried out. Understanding this aspect can contribute to a more efficient\nselection of candidate sentences which, besides reducing the time required for\nitem writing, can also ensure a higher degree of variability and authenticity.\nWe present a set of relevant aspects collected based on the qualitative\nanalysis of a smaller set of context-dependent corpus example sentences.\nFurthermore, we implemented a rule-based algorithm using these criteria which\nachieved an average precision of 0.76 for the identification of different\nissues related to context dependence. The method has also been evaluated\nempirically where 80% of the sentences in which our system did not detect\ncontext-dependent elements were also considered context-independent by human\nraters.", "published": "2016-05-06 07:30:53", "link": "http://arxiv.org/abs/1605.01845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec", "abstract": "Distributed dense word vectors have been shown to be effective at capturing\ntoken-level semantic and syntactic regularities in language, while topic models\ncan form interpretable representations over documents. In this work, we\ndescribe lda2vec, a model that learns dense word vectors jointly with\nDirichlet-distributed latent document-level mixtures of topic vectors. In\ncontrast to continuous dense document representations, this formulation\nproduces sparse, interpretable document mixtures through a non-negative simplex\nconstraint. Our method is simple to incorporate into existing automatic\ndifferentiation frameworks and allows for unsupervised document representations\ngeared for use by scientists while simultaneously learning word vectors and the\nlinear relationships between them.", "published": "2016-05-06 18:13:18", "link": "http://arxiv.org/abs/1605.02019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User Reviews and Language: How Language Influences Ratings", "abstract": "The number of user reviews of tourist attractions, restaurants, mobile apps,\netc. is increasing for all languages; yet, research is lacking on how reviews\nin multiple languages should be aggregated and displayed. Speakers of different\nlanguages may have consistently different experiences, e.g., different\ninformation available in different languages at tourist attractions or\ndifferent user experiences with software due to\ninternationalization/localization choices. This paper assesses the similarity\nin the ratings given by speakers of different languages to London tourist\nattractions on TripAdvisor. The correlations between different languages are\ngenerally high, but some language pairs are more correlated than others. The\nresults question the common practice of computing average ratings from reviews\nin many languages.", "published": "2016-05-06 12:52:09", "link": "http://arxiv.org/abs/1605.01919v1", "categories": ["cs.HC", "cs.CL", "cs.CY", "H.5.m; H.3.5"], "primary_category": "cs.HC"}
