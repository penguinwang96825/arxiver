{"title": "Embracing Ambiguity: Shifting the Training Target of NLI Models", "abstract": "Natural Language Inference (NLI) datasets contain examples with highly\nambiguous labels. While many research works do not pay much attention to this\nfact, several recent efforts have been made to acknowledge and embrace the\nexistence of ambiguity, such as UNLI and ChaosNLI. In this paper, we explore\nthe option of training directly on the estimated label distribution of the\nannotators in the NLI task, using a learning loss based on this ambiguity\ndistribution instead of the gold-labels. We prepare AmbiNLI, a trial dataset\nobtained from readily available sources, and show it is possible to reduce\nChaosNLI divergence scores when finetuning on this data, a promising first step\ntowards learning how to capture linguistic ambiguity. Additionally, we show\nthat training on the same amount of data but targeting the ambiguity\ndistribution instead of gold-labels can result in models that achieve higher\nperformance and learn better representations for downstream tasks.", "published": "2021-06-06 03:18:53", "link": "http://arxiv.org/abs/2106.03020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Grammatical Error Correction Models Realize Grammatical\n  Generalization?", "abstract": "There has been an increased interest in data generation approaches to\ngrammatical error correction (GEC) using pseudo data. However, these approaches\nsuffer from several issues that make them inconvenient for real-world\ndeployment including a demand for large amounts of training data. On the other\nhand, some errors based on grammatical rules may not necessarily require a\nlarge amount of data if GEC models can realize grammatical generalization. This\nstudy explores to what extent GEC models generalize grammatical knowledge\nrequired for correcting errors. We introduce an analysis method using synthetic\nand real GEC datasets with controlled vocabularies to evaluate whether models\ncan generalize to unseen errors. We found that a current standard\nTransformer-based GEC model fails to realize grammatical generalization even in\nsimple settings with limited vocabulary and syntax, suggesting that it lacks\nthe generalization ability required to correct errors from provided training\nexamples.", "published": "2021-06-06 04:59:29", "link": "http://arxiv.org/abs/2106.03031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic-Enhanced Explainable Finetuning for Open-Domain Dialogues", "abstract": "This paper propose to combine pretrained language models with the modular\ndialogue paradigm for open-domain dialogue modeling. Our method,\nsemantic-enhanced finetuning, instantiates conversation understanding,\nplanning, and response generation as a language model finetuning task. At\ninference, we disentangle semantic and token variations by specifying sampling\nmethods and constraints for each module separately. For training and\nevaluation, we present X-Weibo, a Chinese multi-turn open-domain dialogue\ndataset with automatic annotation for emotions, DAs, and topical words.\nExperiments show that semantic-enhanced finetuning outperforms strong baselines\non non-semantic and semantic metrics, improves the human-evaluated relevance,\ncoherence, and informativeness, and exhibits considerable controllability over\nsemantic variables.", "published": "2021-06-06 09:03:41", "link": "http://arxiv.org/abs/2106.03065v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Static Word Embeddings and Contextual Representations for\n  Bilingual Lexicon Induction", "abstract": "Bilingual Lexicon Induction (BLI) aims to map words in one language to their\ntranslations in another, and is typically through learning linear projections\nto align monolingual word representation spaces. Two classes of word\nrepresentations have been explored for BLI: static word embeddings and\ncontextual representations, but there is no studies to combine both. In this\npaper, we propose a simple yet effective mechanism to combine the static word\nembeddings and the contextual representations to utilize the advantages of both\nparadigms. We test the combination mechanism on various language pairs under\nthe supervised and unsupervised BLI benchmark settings. Experiments show that\nour mechanism consistently improves performances over robust BLI baselines on\nall language pairs by averagely improving 3.2 points in the supervised setting,\nand 3.1 points in the unsupervised setting.", "published": "2021-06-06 10:31:02", "link": "http://arxiv.org/abs/2106.03084v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Label Correlation Feedback in Multi-Label Text Classification\n  via Multi-Task Learning", "abstract": "In multi-label text classification (MLTC), each given document is associated\nwith a set of correlated labels. To capture label correlations, previous\nclassifier-chain and sequence-to-sequence models transform MLTC to a sequence\nprediction task. However, they tend to suffer from label order dependency,\nlabel combination over-fitting and error propagation problems. To address these\nproblems, we introduce a novel approach with multi-task learning to enhance\nlabel correlation feedback. We first utilize a joint embedding (JE) mechanism\nto obtain the text and label representation simultaneously. In MLTC task, a\ndocument-label cross attention (CA) mechanism is adopted to generate a more\ndiscriminative document representation. Furthermore, we propose two auxiliary\nlabel co-occurrence prediction tasks to enhance label correlation learning: 1)\nPairwise Label Co-occurrence Prediction (PLCP), and 2) Conditional Label\nCo-occurrence Prediction (CLCP). Experimental results on AAPD and RCV1-V2\ndatasets show that our method outperforms competitive baselines by a large\nmargin. We analyze low-frequency label performance, label dependency, label\ncombination diversity and coverage speed to show the effectiveness of our\nproposed method on label correlation learning.", "published": "2021-06-06 12:26:14", "link": "http://arxiv.org/abs/2106.03103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Semantic Change Discovery", "abstract": "While there is a large amount of research in the field of Lexical Semantic\nChange Detection, only few approaches go beyond a standard benchmark evaluation\nof existing models. In this paper, we propose a shift of focus from change\ndetection to change discovery, i.e., discovering novel word senses over time\nfrom the full corpus vocabulary. By heavily fine-tuning a type-based and a\ntoken-based approach on recently published German data, we demonstrate that\nboth models can successfully be applied to discover new words undergoing\nmeaning change. Furthermore, we provide an almost fully automated framework for\nboth evaluation and discovery.", "published": "2021-06-06 13:02:38", "link": "http://arxiv.org/abs/2106.03111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Populist Paragraphs in Text: A machine-learning approach", "abstract": "Abstract: In this paper we present an approach to develop a\ntext-classification model which would be able to identify populist content in\ntext. The developed BERT-based model is largely successful in identifying\npopulist content in text and produces only a negligible amount of False\nNegatives, which makes it well-suited as a content analysis automation tool,\nwhich shortlists potentially relevant content for human validation.", "published": "2021-06-06 15:58:34", "link": "http://arxiv.org/abs/2106.03161v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Effectiveness of Adapter-based Tuning for Pretrained Language\n  Model Adaptation", "abstract": "Adapter-based tuning has recently arisen as an alternative to fine-tuning. It\nworks by adding light-weight adapter modules to a pretrained language model\n(PrLM) and only updating the parameters of adapter modules when learning on a\ndownstream task. As such, it adds only a few trainable parameters per new task,\nallowing a high degree of parameter sharing. Prior studies have shown that\nadapter-based tuning often achieves comparable results to fine-tuning. However,\nexisting work only focuses on the parameter-efficient aspect of adapter-based\ntuning while lacking further investigation on its effectiveness. In this paper,\nwe study the latter. We first show that adapter-based tuning better mitigates\nforgetting issues than fine-tuning since it yields representations with less\ndeviation from those generated by the initial PrLM. We then empirically compare\nthe two tuning methods on several downstream NLP tasks and settings. We\ndemonstrate that 1) adapter-based tuning outperforms fine-tuning on\nlow-resource and cross-lingual tasks; 2) it is more robust to overfitting and\nless sensitive to changes in learning rates.", "published": "2021-06-06 16:10:12", "link": "http://arxiv.org/abs/2106.03164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let's be explicit about that: Distant supervision for implicit discourse\n  relation classification via connective prediction", "abstract": "In implicit discourse relation classification, we want to predict the\nrelation between adjacent sentences in the absence of any overt discourse\nconnectives. This is challenging even for humans, leading to shortage of\nannotated data, a fact that makes the task even more difficult for supervised\nmachine learning approaches. In the current study, we perform implicit\ndiscourse relation classification without relying on any labeled implicit\nrelation. We sidestep the lack of data through explicitation of implicit\nrelations to reduce the task to two sub-problems: language modeling and\nexplicit discourse relation classification, a much easier problem. Our\nexperimental results show that this method can even marginally outperform the\nstate-of-the-art, in spite of being much simpler than alternative models of\ncomparable performance. Moreover, we show that the achieved performance is\nrobust across domains as suggested by the zero-shot experiments on a completely\ndifferent domain. This indicates that recent advances in language modeling have\nmade language models sufficiently good at capturing inter-sentence relations\nwithout the help of explicit discourse markers.", "published": "2021-06-06 17:57:32", "link": "http://arxiv.org/abs/2106.03192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Targeted Assessment of Incremental Processing in Neural LanguageModels\n  and Humans", "abstract": "We present a targeted, scaled-up comparison of incremental processing in\nhumans and neural language models by collecting by-word reaction time data for\nsixteen different syntactic test suites across a range of structural phenomena.\nHuman reaction time data comes from a novel online experimental paradigm called\nthe Interpolated Maze task. We compare human reaction times to by-word\nprobabilities for four contemporary language models, with different\narchitectures and trained on a range of data set sizes. We find that across\nmany phenomena, both humans and language models show increased processing\ndifficulty in ungrammatical sentence regions with human and model `accuracy'\nscores (a la Marvin and Linzen(2018)) about equal. However, although language\nmodel outputs match humans in direction, we show that models systematically\nunder-predict the difference in magnitude of incremental processing difficulty\nbetween grammatical and ungrammatical sentences. Specifically, when models\nencounter syntactic violations they fail to accurately predict the longer\nreaction times observed in the human data. These results call into question\nwhether contemporary language models are approaching human-like performance for\nsensitivity to syntactic violations.", "published": "2021-06-06 20:04:39", "link": "http://arxiv.org/abs/2106.03232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Itihasa: A large-scale corpus for Sanskrit to English translation", "abstract": "This work introduces Itihasa, a large-scale translation dataset containing\n93,000 pairs of Sanskrit shlokas and their English translations. The shlokas\nare extracted from two Indian epics viz., The Ramayana and The Mahabharata. We\nfirst describe the motivation behind the curation of such a dataset and follow\nup with empirical analysis to bring out its nuances. We then benchmark the\nperformance of standard translation models on this corpus and show that even\nstate-of-the-art transformer architectures perform poorly, emphasizing the\ncomplexity of the dataset.", "published": "2021-06-06 22:58:13", "link": "http://arxiv.org/abs/2106.03269v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion-aware Chat Machine: Automatic Emotional Response Generation for\n  Human-like Emotional Interaction", "abstract": "The consistency of a response to a given post at semantic-level and\nemotional-level is essential for a dialogue system to deliver human-like\ninteractions. However, this challenge is not well addressed in the literature,\nsince most of the approaches neglect the emotional information conveyed by a\npost while generating responses. This article addresses this problem by\nproposing a unifed end-to-end neural architecture, which is capable of\nsimultaneously encoding the semantics and the emotions in a post for generating\nmore intelligent responses with appropriately expressed emotions. Extensive\nexperiments on real-world data demonstrate that the proposed method outperforms\nthe state-of-the-art methods in terms of both content coherence and emotion\nappropriateness.", "published": "2021-06-06 06:26:15", "link": "http://arxiv.org/abs/2106.03044v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empowering Language Understanding with Counterfactual Reasoning", "abstract": "Present language understanding methods have demonstrated extraordinary\nability of recognizing patterns in texts via machine learning. However,\nexisting methods indiscriminately use the recognized patterns in the testing\nphase that is inherently different from us humans who have counterfactual\nthinking, e.g., to scrutinize for the hard testing samples. Inspired by this,\nwe propose a Counterfactual Reasoning Model, which mimics the counterfactual\nthinking by learning from few counterfactual samples. In particular, we devise\na generation module to generate representative counterfactual samples for each\nfactual sample, and a retrospective module to retrospect the model prediction\nby comparing the counterfactual and factual samples. Extensive experiments on\nsentiment analysis (SA) and natural language inference (NLI) validate the\neffectiveness of our method.", "published": "2021-06-06 06:36:52", "link": "http://arxiv.org/abs/2106.03046v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Did This Get Funded?! Automatically Identifying Quirky Scientific\n  Achievements", "abstract": "Humor is an important social phenomenon, serving complex social and\npsychological functions. However, despite being studied for millennia humor is\ncomputationally not well understood, often considered an AI-complete problem.\nIn this work, we introduce a novel setting in humor mining: automatically\ndetecting funny and unusual scientific papers. We are inspired by the Ig Nobel\nprize, a satirical prize awarded annually to celebrate funny scientific\nachievements (example past winner: \"Are cows more likely to lie down the longer\nthey stand?\"). This challenging task has unique characteristics that make it\nparticularly suitable for automatic learning. We construct a dataset containing\nthousands of funny papers and use it to learn classifiers, combining findings\nfrom psychology and linguistics with recent advances in NLP. We use our models\nto identify potentially funny papers in a large dataset of over 630,000\narticles. The results demonstrate the potential of our methods, and more\nbroadly the utility of integrating state-of-the-art NLP methods with insights\nfrom more traditional disciplines.", "published": "2021-06-06 06:54:40", "link": "http://arxiv.org/abs/2106.03048v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual\n  Machine Translation", "abstract": "One of the biggest challenges hindering progress in low-resource and\nmultilingual machine translation is the lack of good evaluation benchmarks.\nCurrent evaluation benchmarks either lack good coverage of low-resource\nlanguages, consider only restricted domains, or are low quality because they\nare constructed using semi-automatic procedures. In this work, we introduce the\nFLORES-101 evaluation benchmark, consisting of 3001 sentences extracted from\nEnglish Wikipedia and covering a variety of different topics and domains. These\nsentences have been translated in 101 languages by professional translators\nthrough a carefully controlled process. The resulting dataset enables better\nassessment of model quality on the long tail of low-resource languages,\nincluding the evaluation of many-to-many multilingual translation systems, as\nall translations are multilingually aligned. By publicly releasing such a\nhigh-quality and high-coverage dataset, we hope to foster progress in the\nmachine translation community and beyond.", "published": "2021-06-06 17:58:12", "link": "http://arxiv.org/abs/2106.03193v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extractive Research Slide Generation Using Windowed Labeling Ranking", "abstract": "Presentation slides describing the content of scientific and technical papers\nare an efficient and effective way to present that work. However, manually\ngenerating presentation slides is labor intensive. We propose a method to\nautomatically generate slides for scientific papers based on a corpus of 5000\npaper-slide pairs compiled from conference proceedings websites. The sentence\nlabeling module of our method is based on SummaRuNNer, a neural sequence model\nfor extractive summarization. Instead of ranking sentences based on semantic\nsimilarities in the whole document, our algorithm measures importance and\nnovelty of sentences by combining semantic and lexical features within a\nsentence window. Our method outperforms several baseline methods including\nSummaRuNNer by a significant margin in terms of ROUGE score.", "published": "2021-06-06 20:56:43", "link": "http://arxiv.org/abs/2106.03246v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structured Reordering for Modeling Latent Alignments in Sequence\n  Transduction", "abstract": "Despite success in many domains, neural models struggle in settings where\ntrain and test examples are drawn from different distributions. In particular,\nin contrast to humans, conventional sequence-to-sequence (seq2seq) models fail\nto generalize systematically, i.e., interpret sentences representing novel\ncombinations of concepts (e.g., text segments) seen in training. Traditional\ngrammar formalisms excel in such settings by implicitly encoding alignments\nbetween input and output segments, but are hard to scale and maintain. Instead\nof engineering a grammar, we directly model segment-to-segment alignments as\ndiscrete structured latent variables within a neural seq2seq model. To\nefficiently explore the large space of alignments, we introduce a reorder-first\nalign-later framework whose central component is a neural reordering module\nproducing {\\it separable} permutations. We present an efficient dynamic\nprogramming algorithm performing exact marginal inference of separable\npermutations, and, thus, enabling end-to-end differentiable training of our\nmodel. The resulting seq2seq model exhibits better systematic generalization\nthan standard models on synthetic problems and NLP tasks (i.e., semantic\nparsing and machine translation).", "published": "2021-06-06 21:53:54", "link": "http://arxiv.org/abs/2106.03257v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Meta-learning for downstream aware and agnostic pretraining", "abstract": "Neural network pretraining is gaining attention due to its outstanding\nperformance in natural language processing applications. However, pretraining\nusually leverages predefined task sequences to learn general linguistic clues.\nThe lack of mechanisms in choosing proper tasks during pretraining makes the\nlearning and knowledge encoding inefficient. We thus propose using\nmeta-learning to select tasks that provide the most informative learning\nsignals in each episode of pretraining. With the proposed method, we aim to\nachieve better efficiency in computation and memory usage for the pretraining\nprocess and resulting networks while maintaining the performance. In this\npreliminary work, we discuss the algorithm of the method and its two variants,\ndownstream-aware and downstream-agnostic pretraining. Our experiment plan is\nalso summarized, while empirical results will be shared in our future works.", "published": "2021-06-06 23:08:09", "link": "http://arxiv.org/abs/2106.03270v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Adversarial Learning based Multi-Step Spoken Language Understanding\n  System through Human-Computer Interaction", "abstract": "Most of the existing spoken language understanding systems can perform only\nsemantic frame parsing based on a single-round user query. They cannot take\nusers' feedback to update/add/remove slot values through multiround\ninteractions with users. In this paper, we introduce a novel multi-step spoken\nlanguage understanding system based on adversarial learning that can leverage\nthe multiround user's feedback to update slot values. We perform two\nexperiments on the benchmark ATIS dataset and demonstrate that the new system\ncan improve parsing performance by at least $2.5\\%$ in terms of F1, with only\none round of feedback. The improvement becomes even larger when the number of\nfeedback rounds increases. Furthermore, we also compare the new system with\nstate-of-the-art dialogue state tracking systems and demonstrate that the new\ninteractive system can perform better on multiround spoken language\nunderstanding tasks in terms of slot- and sentence-level accuracy.", "published": "2021-06-06 03:46:53", "link": "http://arxiv.org/abs/2106.14611v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Theory of Language Learning", "abstract": "A theory of language learning is described, which uses Bayesian induction of\nfeature structures (scripts) and script functions. Each word sense in a\nlanguage is mentally represented by an m-script, a script function which\nembodies all the syntax and semantics of the word. M-scripts form a\nfully-lexicalised unification grammar, which can support adult language. Each\nword m-script can be learnt robustly from about six learning examples. The\ntheory has been implemented as a computer model, which can bootstrap-learn a\nlanguage from zero vocabulary. The Bayesian learning mechanism is (1) Capable:\nto learn arbitrarily complex meanings and syntactic structures; (2) Fast:\nlearning these structures from a few examples each; (3) Robust: learning in the\npresence of much irrelevant noise, and (4) Self-repairing: able to acquire\nimplicit negative evidence, using it to learn exceptions. Children learning\nlanguage are clearly all of (1) - (4), whereas connectionist theories fail on\n(1) and (2), and symbolic theories fail on (3) and (4). The theory is in good\nagreement with many key facts of language acquisition, including facts which\nare problematic for other theories. It is compared with over 100 key\ncross-linguistic findings about acquisition of the lexicon, phrase structure,\nmorphology, complementation and control, auxiliaries, verb argument structures,\ngaps and movement - in nearly all cases giving unforced agreement without extra\nassumptions.", "published": "2021-06-06 11:06:42", "link": "http://arxiv.org/abs/2106.14612v1", "categories": ["cs.CL", "q-bio.NC", "J.3"], "primary_category": "cs.CL"}
{"title": "An evaluation of template and ML-based generation of user-readable text\n  from a knowledge graph", "abstract": "Typical user-friendly renderings of knowledge graphs are visualisations and\nnatural language text. Within the latter HCI solution approach, data-driven\nnatural language generation systems receive increased attention, but they are\noften outperformed by template-based systems due to suffering from errors such\nas content dropping, hallucination, or repetition. It is unknown which of those\nerrors are associated significantly with low quality judgements by humans who\nthe text is aimed for, which hampers addressing errors based on their impact on\nimproving human evaluations. We assessed their possible association with an\nexperiment availing of expert and crowdsourced evaluations of human authored\ntext, template generated text, and sequence-to-sequence model generated text.\nThe results showed that there was no significant association between human\nauthored texts with errors and the low human judgements of naturalness and\nquality. There was also no significant association between machine learning\ngenerated texts with dropped or hallucinated slots and the low human judgements\nof naturalness and quality. Thus, both approaches appear to be viable options\nfor designing a natural language interface for knowledge graphs.", "published": "2021-06-06 14:47:19", "link": "http://arxiv.org/abs/2106.14613v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Real-Time Cognitive Evaluation of Online Learners through Automatically\n  Generated Questions", "abstract": "With the increased adoption of E-learning platforms, keeping online learners\nengaged throughout a lesson is challenging. One approach to tackle this\nchallenge is to probe learn-ers periodically by asking questions. The paper\npresents an approach to generate questions from a given video lecture\nautomatically. The generated questions are aimed to evaluate learners'\nlower-level cognitive abilities. The approach automatically extracts text from\nvideo lectures to generates wh-kinds of questions. When learners respond with\nan answer, the proposed approach further evaluates the response and provides\nfeedback. Besides enhancing learner's engagement, this approach's main benefits\nare that it frees instructors from design-ing questions to check the\ncomprehension of a topic. Thus, instructors can spend this time productively on\nother activities.", "published": "2021-06-06 05:45:56", "link": "http://arxiv.org/abs/2106.03036v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional\n  Embeddings", "abstract": "Without positional information, attention-based Transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed Transformer models with positional information.\nAbsolute positional embeddings are simple to implement, but suffer from\ngeneralization issues when evaluating on sequences longer than seen at training\ntime. Relative positions are more robust to input length change, but are more\ncomplex to implement and yield inferior model throughput due to extra\ncomputational and memory costs. In this paper, we propose an augmentation-based\napproach (CAPE) for absolute positional embeddings, which keeps the advantages\nof both absolute (simplicity and speed) and relative positional embeddings\n(better generalization). In addition, our empirical evaluation on\nstate-of-the-art models in machine translation, image and speech recognition\ndemonstrates that CAPE leads to better generalization performance as well as\nincreased stability with respect to training hyper-parameters.", "published": "2021-06-06 14:54:55", "link": "http://arxiv.org/abs/2106.03143v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Attend and select: A segment selective transformer for microblog hashtag\n  generation", "abstract": "Hashtag generation aims to generate short and informal topical tags from a\nmicroblog post, in which tokens or phrases form the hashtags. These tokens or\nphrases may originate from primary fragmental textual pieces (e.g., segments)\nin the original text and are separated into different segments. However,\nconventional sequence-to-sequence generation methods are hard to filter out\nsecondary information from different textual granularity and are not good at\nselecting crucial tokens. Thus, they are suboptimal in generating more\ncondensed hashtags. In this work, we propose a modified Transformer-based\ngeneration model with adding a segments-selection procedure for the original\nencoding and decoding phases. The segments-selection phase is based on a novel\nSegments Selection Mechanism (SSM) to model different textual granularity on\nglobal text, local segments, and tokens, contributing to generating condensed\nhashtags. Specifically, it first attends to primary semantic segments and then\ntransforms discontinuous segments from the source text into a sequence of\nhashtags by selecting crucial tokens. Extensive evaluations on the two datasets\nreveal our approach's superiority with significant improvements to the\nextraction and generation baselines. The code and datasets are available at\nhttps://github.com/OpenSUM/HashtagGen.", "published": "2021-06-06 15:13:58", "link": "http://arxiv.org/abs/2106.03151v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation", "abstract": "With rapid progress in neural text-to-speech (TTS) models, personalized\nspeech generation is now in high demand for many applications. For practical\napplicability, a TTS model should generate high-quality speech with only a few\naudio samples from the given speaker, that are also short in length. However,\nexisting methods either require to fine-tune the model or achieve low\nadaptation quality without fine-tuning. In this work, we propose StyleSpeech, a\nnew TTS model which not only synthesizes high-quality speech but also\neffectively adapts to new speakers. Specifically, we propose Style-Adaptive\nLayer Normalization (SALN) which aligns gain and bias of the text input\naccording to the style extracted from a reference speech audio. With SALN, our\nmodel effectively synthesizes speech in the style of the target speaker even\nfrom single speech audio. Furthermore, to enhance StyleSpeech's adaptation to\nspeech from new speakers, we extend it to Meta-StyleSpeech by introducing two\ndiscriminators trained with style prototypes, and performing episodic training.\nThe experimental results show that our models generate high-quality speech\nwhich accurately follows the speaker's voice with single short-duration (1-3\nsec) speech audio, significantly outperforming baselines.", "published": "2021-06-06 15:34:11", "link": "http://arxiv.org/abs/2106.03153v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FlexParser -- the adaptive log file parser for continuous results in a\n  changing world", "abstract": "Any modern system writes events into files, called log files. Those contain\ncrucial information which are subject to various analyses. Examples range from\ncybersecurity, intrusion detection over usage analyses to trouble shooting.\nBefore data analysis is possible, desired information needs to be extracted\nfirst out of the semi-structured log messages. State-of-the-art event parsing\noften assumes static log events. However, any modern system is updated\nconsistently and with updates also log file structures can change. We call\nthose changes \"mutation\" and study parsing performance for different mutation\ncases. Latest research discovers mutations using anomaly detection post mortem,\nhowever, does not cover actual continuous parsing. Thus, we propose a novel and\nflexible parser, called FlexParser, which can extract desired values despite\ngradual changes in the log messages. It implies basic text preprocessing\nfollowed by a supervised Deep Learning method. We train a stateful LSTM on\nparsing one event per data set. Statefulness enforces the model to learn log\nmessage structures across several examples. Our model was tested on seven\ndifferent, publicly available log file data sets and various kinds of\nmutations. Exhibiting an average F1-Score of 0.98, it outperforms other Deep\nLearning methods as well as state-of-the-art unsupervised parsers.", "published": "2021-06-06 16:30:01", "link": "http://arxiv.org/abs/2106.03170v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Transient Chaos in BERT", "abstract": "Language is an outcome of our complex and dynamic human-interactions and the\ntechnique of natural language processing (NLP) is hence built on human\nlinguistic activities. Bidirectional Encoder Representations from Transformers\n(BERT) has recently gained its popularity by establishing the state-of-the-art\nscores in several NLP benchmarks. A Lite BERT (ALBERT) is literally\ncharacterized as a lightweight version of BERT, in which the number of BERT\nparameters is reduced by repeatedly applying the same neural network called\nTransformer's encoder layer. By pre-training the parameters with a massive\namount of natural language data, ALBERT can convert input sentences into\nversatile high-dimensional vectors potentially capable of solving multiple NLP\ntasks. In that sense, ALBERT can be regarded as a well-designed\nhigh-dimensional dynamical system whose operator is the Transformer's encoder,\nand essential structures of human language are thus expected to be encapsulated\nin its dynamics. In this study, we investigated the embedded properties of\nALBERT to reveal how NLP tasks are effectively solved by exploiting its\ndynamics. We thereby aimed to explore the nature of human language from the\ndynamical expressions of the NLP model. Our short-term analysis clarified that\nthe pre-trained model stably yields trajectories with higher dimensionality,\nwhich would enhance the expressive capacity required for NLP tasks. Also, our\nlong-term analysis revealed that ALBERT intrinsically shows transient chaos, a\ntypical nonlinear phenomenon showing chaotic dynamics only in its transient,\nand the pre-trained ALBERT model tends to produce the chaotic trajectory for a\nsignificantly longer time period compared to a randomly-initialized one. Our\nresults imply that local chaoticity would contribute to improving NLP\nperformance, uncovering a novel aspect in the role of chaotic dynamics in human\nlanguage behaviors.", "published": "2021-06-06 17:02:29", "link": "http://arxiv.org/abs/2106.03181v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "math.DS", "nlin.CD"], "primary_category": "cs.CL"}
{"title": "RTNeural: Fast Neural Inferencing for Real-Time Systems", "abstract": "RTNeural is a neural inferencing library written in C++. RTNeural is designed\nto be used in systems with hard real-time constraints, with additional emphasis\non speed, flexibility, size, and convenience. The motivation and design of the\nlibrary are described, as well as real-world use-cases, and performance\ncomparisons with other neural inferencing libraries.", "published": "2021-06-06 05:46:25", "link": "http://arxiv.org/abs/2106.03037v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Improving Channel Decorrelation for Multi-Channel Target Speech\n  Extraction", "abstract": "Target speech extraction has attracted widespread attention. When microphone\narrays are available, the additional spatial information can be helpful in\nextracting the target speech. We have recently proposed a channel decorrelation\n(CD) mechanism to extract the inter-channel differential information to enhance\nthe reference channel encoder representation. Although the proposed mechanism\nhas shown promising results for extracting the target speech from mixtures, the\nextraction performance is still limited by the nature of the original\ndecorrelation theory. In this paper, we propose two methods to broaden the\nhorizon of the original channel decorrelation, by replacing the original\nsoftmax-based inter-channel similarity between encoder representations, using\nan unrolled probability and a normalized cosine-based similarity at the\ndimensional-level. Moreover, new combination strategies of the CD-based spatial\ninformation and target speaker adaptation of parallel encoder outputs are also\ninvestigated. Experiments on the reverberant WSJ0 2-mix show that the improved\nCD can result in more discriminative differential information and the new\nadaptation strategy is also very effective to improve the target speech\nextraction.", "published": "2021-06-06 13:08:47", "link": "http://arxiv.org/abs/2106.03113v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Mathematical Vocoder Algorithm : Modified Spectral Inversion for\n  Efficient Neural Speech Synthesis", "abstract": "In this work, we propose a new mathematical vocoder algorithm(modified\nspectral inversion) that generates a waveform from acoustic features without\nphase estimation. The main benefit of using our proposed method is that it\nexcludes the training stage of the neural vocoder from the end-to-end speech\nsynthesis model. Our implementation can synthesize high fidelity speech at\napproximately 20 Mhz on CPU and 59.6MHz on GPU. This is 909 and 2,702 times\nfaster compared to real-time. Since the proposed methodology is not a\ndata-driven method, it is applicable to unseen voices and multiple languages\nwithout any additional work. The proposed method is expected to adapt for\nresearching on neural network models capable of synthesizing speech at the\nstudio recording level.", "published": "2021-06-06 16:16:08", "link": "http://arxiv.org/abs/2106.03167v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
