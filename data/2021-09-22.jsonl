{"title": "The NiuTrans Machine Translation Systems for WMT21", "abstract": "This paper describes NiuTrans neural machine translation systems of the WMT\n2021 news translation tasks. We made submissions to 9 language directions,\nincluding English$\\leftrightarrow$$\\{$Chinese, Japanese, Russian, Icelandic$\\}$\nand English$\\rightarrow$Hausa tasks. Our primary systems are built on several\neffective variants of Transformer, e.g., Transformer-DLCL, ODE-Transformer. We\nalso utilize back-translation, knowledge distillation, post-ensemble, and\niterative fine-tuning techniques to enhance the model performance further.", "published": "2021-09-22 02:00:24", "link": "http://arxiv.org/abs/2109.10485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Approach to Jointly Rank Passages and Select Relevant Sentences\n  in the OBQA Context", "abstract": "In the open book question answering (OBQA) task, selecting the relevant\npassages and sentences from distracting information is crucial to reason the\nanswer to a question. HotpotQA dataset is designed to teach and evaluate\nsystems to do both passage ranking and sentence selection. Many existing\nframeworks use separate models to select relevant passages and sentences\nrespectively. Such systems not only have high complexity in terms of the\nparameters of models but also fail to take the advantage of training these two\ntasks together since one task can be beneficial for the other one. In this\nwork, we present a simple yet effective framework to address these limitations\nby jointly ranking passages and selecting sentences. Furthermore, we propose\nconsistency and similarity constraints to promote the correlation and\ninteraction between passage ranking and sentence selection.The experiments\ndemonstrate that our framework can achieve competitive results with previous\nsystems and outperform the baseline by 28\\% in terms of exact matching of\nrelevant sentences on the HotpotQA dataset.", "published": "2021-09-22 03:11:17", "link": "http://arxiv.org/abs/2109.10497v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FCM: A Fine-grained Comparison Model for Multi-turn Dialogue Reasoning", "abstract": "Despite the success of neural dialogue systems in achieving high performance\non the leader-board, they cannot meet users' requirements in practice, due to\ntheir poor reasoning skills. The underlying reason is that most neural dialogue\nmodels only capture the syntactic and semantic information, but fail to model\nthe logical consistency between the dialogue history and the generated\nresponse. Recently, a new multi-turn dialogue reasoning task has been proposed,\nto facilitate dialogue reasoning research. However, this task is challenging,\nbecause there are only slight differences between the illogical response and\nthe dialogue history. How to effectively solve this challenge is still worth\nexploring. This paper proposes a Fine-grained Comparison Model (FCM) to tackle\nthis problem. Inspired by human's behavior in reading comprehension, a\ncomparison mechanism is proposed to focus on the fine-grained differences in\nthe representation of each response candidate. Specifically, each candidate\nrepresentation is compared with the whole history to obtain a history\nconsistency representation. Furthermore, the consistency signals between each\ncandidate and the speaker's own history are considered to drive a model to\nprefer a candidate that is logically consistent with the speaker's history\nlogic. Finally, the above consistency representations are employed to output a\nranking list of the candidate responses for multi-turn dialogue reasoning.\nExperimental results on two public dialogue datasets show that our method\nobtains higher ranking scores than the baseline models.", "published": "2021-09-22 04:16:11", "link": "http://arxiv.org/abs/2109.10510v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Role of Language Relatedness in Multilingual Fine-tuning of Language\n  Models: A Case Study in Indo-Aryan Languages", "abstract": "We explore the impact of leveraging the relatedness of languages that belong\nto the same family in NLP models using multilingual fine-tuning. We hypothesize\nand validate that multilingual fine-tuning of pre-trained language models can\nyield better performance on downstream NLP applications, compared to models\nfine-tuned on individual languages. A first of its kind detailed study is\npresented to track performance change as languages are added to a base language\nin a graded and greedy (in the sense of best boost of performance) manner;\nwhich reveals that careful selection of subset of related languages can\nsignificantly improve performance than utilizing all related languages. The\nIndo-Aryan (IA) language family is chosen for the study, the exact languages\nbeing Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script\nbarrier is crossed by simple rule-based transliteration of the text of all\nlanguages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL\nand two RoBERTa-based LMs, the last two being pre-trained by us. Low resource\nlanguages, such as Oriya and Punjabi, are found to be the largest beneficiaries\nof multilingual fine-tuning. Textual Entailment, Entity Classification, Section\nTitle Prediction, tasks of IndicGLUE and POS tagging form our test bed.\nCompared to monolingual fine tuning we get relative performance improvement of\nup to 150% in the downstream tasks. The surprise take-away is that for any\nlanguage there is a particular combination of other languages which yields the\nbest performance, and any additional language is in fact detrimental.", "published": "2021-09-22 06:37:39", "link": "http://arxiv.org/abs/2109.10534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering\n  Dataset", "abstract": "While diverse question answering (QA) datasets have been proposed and\ncontributed significantly to the development of deep learning models for QA\ntasks, the existing datasets fall short in two aspects. First, we lack QA\ndatasets covering complex questions that involve answers as well as the\nreasoning processes to get the answers. As a result, the state-of-the-art QA\nresearch on numerical reasoning still focuses on simple calculations and does\nnot provide the mathematical expressions or evidences justifying the answers.\nSecond, the QA community has contributed much effort to improving the\ninterpretability of QA models. However, these models fail to explicitly show\nthe reasoning process, such as the evidence order for reasoning and the\ninteractions between different pieces of evidence. To address the above\nshortcomings, we introduce NOAHQA, a conversational and bilingual QA dataset\nwith questions requiring numerical reasoning with compound mathematical\nexpressions. With NOAHQA, we develop an interpretable reasoning graph as well\nas the appropriate evaluation metric to measure the answer quality. We evaluate\nthe state-of-the-art QA models trained using existing QA datasets on NOAHQA and\nshow that the best among them can only achieve 55.5 exact match scores, while\nthe human performance is 89.7. We also present a new QA model for generating a\nreasoning graph where the reasoning graph metric still has a large gap compared\nwith that of humans, e.g., 28 scores.", "published": "2021-09-22 09:17:09", "link": "http://arxiv.org/abs/2109.10604v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COVR: A test-bed for Visually Grounded Compositional Generalization with\n  real images", "abstract": "While interest in models that generalize at test time to new compositions has\nrisen in recent years, benchmarks in the visually-grounded domain have thus far\nbeen restricted to synthetic images. In this work, we propose COVR, a new\ntest-bed for visually-grounded compositional generalization with real images.\nTo create COVR, we use real images annotated with scene graphs, and propose an\nalmost fully automatic procedure for generating question-answer pairs along\nwith a set of context images. COVR focuses on questions that require complex\nreasoning, including higher-order operations such as quantification and\naggregation. Due to the automatic generation process, COVR facilitates the\ncreation of compositional splits, where models at test time need to generalize\nto new concepts and compositions in a zero- or few-shot setting. We construct\ncompositional splits using COVR and demonstrate a myriad of cases where\nstate-of-the-art pre-trained language-and-vision models struggle to\ncompositionally generalize.", "published": "2021-09-22 09:25:41", "link": "http://arxiv.org/abs/2109.10613v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching and Controlling Global Semantics for Text Summarization", "abstract": "Recently, Transformer-based models have been proven effective in the\nabstractive summarization task by creating fluent and informative summaries.\nNevertheless, these models still suffer from the short-range dependency\nproblem, causing them to produce summaries that miss the key points of\ndocument. In this paper, we attempt to address this issue by introducing a\nneural topic model empowered with normalizing flow to capture the global\nsemantics of the document, which are then integrated into the summarization\nmodel. In addition, to avoid the overwhelming effect of global semantics on\ncontextualized representation, we introduce a mechanism to control the amount\nof global semantics supplied to the text generation module. Our method\noutperforms state-of-the-art summarization models on five common text\nsummarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and\nPubMed.", "published": "2021-09-22 09:31:50", "link": "http://arxiv.org/abs/2109.10616v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News\n  Summarization", "abstract": "One of the most challenging aspects of current single-document news\nsummarization is that the summary often contains 'extrinsic hallucinations',\ni.e., facts that are not present in the source document, which are often\nderived via world knowledge. This causes summarization systems to act more like\nopen-ended language models tending to hallucinate facts that are erroneous. In\nthis paper, we mitigate this problem with the help of multiple supplementary\nresource documents assisting the task. We present a new dataset MiRANews and\nbenchmark existing summarization models. In contrast to multi-document\nsummarization, which addresses multiple events from several source documents,\nwe still aim at generating a summary for a single document. We show via data\nanalysis that it's not only the models which are to blame: more than 27% of\nfacts mentioned in the gold summaries of MiRANews are better grounded on\nassisting documents than in the main source articles. An error analysis of\ngenerated summaries from pretrained models fine-tuned on MiRANews reveals that\nthis has an even bigger effects on models: assisted summarization reduces 55%\nof hallucinations when compared to single-document summarization models trained\non the main article only. Our code and data are available at\nhttps://github.com/XinnuoXu/MiRANews.", "published": "2021-09-22 10:58:40", "link": "http://arxiv.org/abs/2109.10650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simulated Annealing for Emotional Dialogue Systems", "abstract": "Explicitly modeling emotions in dialogue generation has important\napplications, such as building empathetic personal companions. In this study,\nwe consider the task of expressing a specific emotion for dialogue generation.\nPrevious approaches take the emotion as an input signal, which may be ignored\nduring inference. We instead propose a search-based emotional dialogue system\nby simulated annealing (SA). Specifically, we first define a scoring function\nthat combines contextual coherence and emotional correctness. Then, SA\niteratively edits a general response and searches for a sentence with a higher\nscore, enforcing the presence of the desired emotion. We evaluate our system on\nthe NLPCC2017 dataset. Our proposed method shows 12% improvements in emotion\naccuracy compared with the previous state-of-the-art method, without hurting\nthe generation quality (measured by BLEU).", "published": "2021-09-22 13:17:17", "link": "http://arxiv.org/abs/2109.10715v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Cross-linguistically Consistent Semantic and Syntactic Annotation of\n  Child-directed Speech", "abstract": "This paper proposes a methodology for constructing such corpora of child\ndirected speech (CDS) paired with sentential logical forms, and uses this\nmethod to create two such corpora, in English and Hebrew. The approach enforces\na cross-linguistically consistent representation, building on recent advances\nin dependency representation and semantic parsing. Specifically, the approach\ninvolves two steps. First, we annotate the corpora using the Universal\nDependencies (UD) scheme for syntactic annotation, which has been developed to\napply consistently to a wide variety of domains and typologically diverse\nlanguages. Next, we further annotate these data by applying an automatic method\nfor transducing sentential logical forms (LFs) from UD structures. The UD and\nLF representations have complementary strengths: UD structures are\nlanguage-neutral and support consistent and reliable annotation by multiple\nannotators, whereas LFs are neutral as to their syntactic derivation and\ntransparently encode semantic relations.\n  Using this approach, we provide syntactic and semantic annotation for two\ncorpora from CHILDES: Brown's Adam corpus (English; we annotate ~80% of its\nchild-directed utterances), all child-directed utterances from Berman's Hagar\ncorpus (Hebrew). We verify the quality of the UD annotation using an\ninter-annotator agreement study, and manually evaluate the transduced meaning\nrepresentations. We then demonstrate the utility of the compiled corpora\nthrough (1) a longitudinal corpus study of the prevalence of different\nsyntactic and semantic phenomena in the CDS, and (2) applying an existing\ncomputational model of language acquisition to the two corpora and briefly\ncomparing the results across languages.", "published": "2021-09-22 18:17:06", "link": "http://arxiv.org/abs/2109.10952v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable Fact-checking with Human-in-the-Loop", "abstract": "Researchers have been investigating automated solutions for fact-checking in\na variety of fronts. However, current approaches often overlook the fact that\nthe amount of information released every day is escalating, and a large amount\nof them overlap. Intending to accelerate fact-checking, we bridge this gap by\ngrouping similar messages and summarizing them into aggregated claims.\nSpecifically, we first clean a set of social media posts (e.g., tweets) and\nbuild a graph of all posts based on their semantics; Then, we perform two\nclustering methods to group the messages for further claim summarization. We\nevaluate the summaries both quantitatively with ROUGE scores and qualitatively\nwith human evaluation. We also generate a graph of summaries to verify that\nthere is no significant overlap among them. The results reduced 28,818 original\nmessages to 700 summary claims, showing the potential to speed up the\nfact-checking process by organizing and selecting representative claims from\nmassive disorganized and redundant messages.", "published": "2021-09-22 19:19:59", "link": "http://arxiv.org/abs/2109.10992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese\n  Language Models", "abstract": "Prior work has shown that structural supervision helps English language\nmodels learn generalizations about syntactic phenomena such as subject-verb\nagreement. However, it remains unclear if such an inductive bias would also\nimprove language models' ability to learn grammatical dependencies in\ntypologically different languages. Here we investigate this question in\nMandarin Chinese, which has a logographic, largely syllable-based writing\nsystem; different word order; and sparser morphology than English. We train\nLSTMs, Recurrent Neural Network Grammars, Transformer language models, and\nTransformer-parameterized generative parsing models on two Mandarin Chinese\ndatasets of different sizes. We evaluate the models' ability to learn different\naspects of Mandarin grammar that assess syntactic and semantic relationships.\nWe find suggestive evidence that structural supervision helps with representing\nsyntactic state across intervening content and improves performance in low-data\nsettings, suggesting that the benefits of hierarchical inductive biases in\nacquiring dependency relationships may extend beyond English.", "published": "2021-09-22 22:11:30", "link": "http://arxiv.org/abs/2109.11058v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Salience-Aware Event Chain Modeling for Narrative Understanding", "abstract": "Storytelling, whether via fables, news reports, documentaries, or memoirs,\ncan be thought of as the communication of interesting and related events that,\ntaken together, form a concrete process. It is desirable to extract the event\nchains that represent such processes. However, this extraction remains a\nchallenging problem. We posit that this is due to the nature of the texts from\nwhich chains are discovered. Natural language text interleaves a narrative of\nconcrete, salient events with background information, contextualization,\nopinion, and other elements that are important for a variety of necessary\ndiscourse and pragmatics acts but are not part of the principal chain of events\nbeing communicated. We introduce methods for extracting this principal chain\nfrom natural language text, by filtering away non-salient events and supportive\nsentences. We demonstrate the effectiveness of our methods at isolating\ncritical event chains by comparing their effect on downstream tasks. We show\nthat by pre-training large language models on our extracted chains, we obtain\nimprovements in two tasks that benefit from a clear understanding of event\nchains: narrative prediction and event-based temporal question answering. The\ndemonstrated improvements and ablative studies confirm that our extraction\nmethod isolates critical event chains.", "published": "2021-09-22 01:34:03", "link": "http://arxiv.org/abs/2109.10475v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DialogueBERT: A Self-Supervised Learning based Dialogue Pre-training\n  Encoder", "abstract": "With the rapid development of artificial intelligence, conversational bots\nhave became prevalent in mainstream E-commerce platforms, which can provide\nconvenient customer service timely. To satisfy the user, the conversational\nbots need to understand the user's intention, detect the user's emotion, and\nextract the key entities from the conversational utterances. However,\nunderstanding dialogues is regarded as a very challenging task. Different from\ncommon language understanding, utterances in dialogues appear alternately from\ndifferent roles and are usually organized as hierarchical structures. To\nfacilitate the understanding of dialogues, in this paper, we propose a novel\ncontextual dialogue encoder (i.e. DialogueBERT) based on the popular\npre-trained language model BERT. Five self-supervised learning pre-training\ntasks are devised for learning the particularity of dialouge utterances. Four\ndifferent input embeddings are integrated to catch the relationship between\nutterances, including turn embedding, role embedding, token embedding and\nposition embedding. DialogueBERT was pre-trained with 70 million dialogues in\nreal scenario, and then fine-tuned in three different downstream dialogue\nunderstanding tasks. Experimental results show that DialogueBERT achieves\nexciting results with 88.63% accuracy for intent recognition, 94.25% accuracy\nfor emotion recognition and 97.04% F1 score for named entity recognition, which\noutperforms several strong baselines by a large margin.", "published": "2021-09-22 01:41:28", "link": "http://arxiv.org/abs/2109.10480v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HyperExpan: Taxonomy Expansion with Hyperbolic Representation Learning", "abstract": "Taxonomies are valuable resources for many applications, but the limited\ncoverage due to the expensive manual curation process hinders their general\napplicability. Prior works attempt to automatically expand existing taxonomies\nto improve their coverage by learning concept embeddings in Euclidean space,\nwhile taxonomies, inherently hierarchical, more naturally align with the\ngeometric properties of a hyperbolic space. In this paper, we present\nHyperExpan, a taxonomy expansion algorithm that seeks to preserve the structure\nof a taxonomy in a more expressive hyperbolic embedding space and learn to\nrepresent concepts and their relations with a Hyperbolic Graph Neural Network\n(HGNN). Specifically, HyperExpan leverages position embeddings to exploit the\nstructure of the existing taxonomies, and characterizes the concept profile\ninformation to support the inference on unseen concepts during training.\nExperiments show that our proposed HyperExpan outperforms baseline models with\nrepresentation learning in a Euclidean feature space and achieves\nstate-of-the-art performance on the taxonomy expansion benchmarks.", "published": "2021-09-22 03:27:04", "link": "http://arxiv.org/abs/2109.10500v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tecnologica cosa: Modeling Storyteller Personalities in Boccaccio's\n  Decameron", "abstract": "We explore Boccaccio's Decameron to see how digital humanities tools can be\nused for tasks that have limited data in a language no longer in contemporary\nuse: medieval Italian. We focus our analysis on the question: Do the different\nstorytellers in the text exhibit distinct personalities? To answer this\nquestion, we curate and release a dataset based on the authoritative edition of\nthe text. We use supervised classification methods to predict storytellers\nbased on the stories they tell, confirming the difficulty of the task, and\ndemonstrate that topic modeling can extract thematic storyteller \"profiles.\"", "published": "2021-09-22 03:42:14", "link": "http://arxiv.org/abs/2109.10506v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards The Automatic Coding of Medical Transcripts to Improve\n  Patient-Centered Communication", "abstract": "This paper aims to provide an approach for automatic coding of\nphysician-patient communication transcripts to improve patient-centered\ncommunication (PCC). PCC is a central part of high-quality health care. To\nimprove PCC, dialogues between physicians and patients have been recorded and\ntagged with predefined codes. Trained human coders have manually coded the\ntranscripts. Since it entails huge labor costs and poses possible human errors,\nautomatic coding methods should be considered for efficiency and effectiveness.\nWe adopted three machine learning algorithms (Na\\\"ive Bayes, Random Forest, and\nSupport Vector Machine) to categorize lines in transcripts into corresponding\ncodes. The result showed that there is evidence to distinguish the codes, and\nthis is considered to be sufficient for training of human annotators.", "published": "2021-09-22 04:37:05", "link": "http://arxiv.org/abs/2109.10514v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Awakening Latent Grounding from Pretrained Language Models for Semantic\n  Parsing", "abstract": "Recent years pretrained language models (PLMs) hit a success on several\ndownstream tasks, showing their power on modeling language. To better\nunderstand and leverage what PLMs have learned, several techniques have emerged\nto explore syntactic structures entailed by PLMs. However, few efforts have\nbeen made to explore grounding capabilities of PLMs, which are also essential.\nIn this paper, we highlight the ability of PLMs to discover which token should\nbe grounded to which concept, if combined with our proposed\nerasing-then-awakening approach. Empirical studies on four datasets demonstrate\nthat our approach can awaken latent grounding which is understandable to human\nexperts, even if it is not exposed to such labels during training. More\nimportantly, our approach shows great potential to benefit downstream semantic\nparsing models. Taking text-to-SQL as a case study, we successfully couple our\napproach with two off-the-shelf parsers, obtaining an absolute improvement of\nup to 9.8%.", "published": "2021-09-22 06:46:29", "link": "http://arxiv.org/abs/2109.10540v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning for Fair Representations", "abstract": "Trained classification models can unintentionally lead to biased\nrepresentations and predictions, which can reinforce societal preconceptions\nand stereotypes. Existing debiasing methods for classification models, such as\nadversarial training, are often expensive to train and difficult to optimise.\nIn this paper, we propose a method for mitigating bias in classifier training\nby incorporating contrastive learning, in which instances sharing the same\nclass label are encouraged to have similar representations, while instances\nsharing a protected attribute are forced further apart. In such a way our\nmethod learns representations which capture the task label in focused regions,\nwhile ensuring the protected attribute has diverse spread, and thus has limited\nimpact on prediction and thereby results in fairer models. Extensive\nexperimental results across four tasks in NLP and computer vision show (a) that\nour proposed method can achieve fairer representations and realises bias\nreductions compared with competitive baselines; and (b) that it can do so\nwithout sacrificing main task performance; (c) that it sets a new\nstate-of-the-art performance in one task despite reducing the bias. Finally,\nour method is conceptually simple and agnostic to network architectures, and\nincurs minimal additional compute cost.", "published": "2021-09-22 10:47:51", "link": "http://arxiv.org/abs/2109.10645v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Small-Bench NLP: Benchmark for small single GPU trained models in\n  Natural Language Processing", "abstract": "Recent progress in the Natural Language Processing domain has given us\nseveral State-of-the-Art (SOTA) pretrained models which can be finetuned for\nspecific tasks. These large models with billions of parameters trained on\nnumerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In\nthis paper, we discuss the need for a benchmark for cost and time effective\nsmaller models trained on a single GPU. This will enable researchers with\nresource constraints experiment with novel and innovative ideas on\ntokenization, pretraining tasks, architecture, fine tuning methods etc. We set\nup Small-Bench NLP, a benchmark for small efficient neural language models\ntrained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks\non the publicly available GLUE datasets and a leaderboard to track the progress\nof the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture\nachieves an average score of 81.53 which is comparable to that of BERT-Base's\n82.20 (110M parameters). Our models, code and leaderboard are available at\nhttps://github.com/smallbenchnlp", "published": "2021-09-22 17:18:55", "link": "http://arxiv.org/abs/2109.10847v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BFClass: A Backdoor-free Text Classification Framework", "abstract": "Backdoor attack introduces artificial vulnerabilities into the model by\npoisoning a subset of the training data via injecting triggers and modifying\nlabels. Various trigger design strategies have been explored to attack text\nclassifiers, however, defending such attacks remains an open problem. In this\nwork, we propose BFClass, a novel efficient backdoor-free training framework\nfor text classification. The backbone of BFClass is a pre-trained discriminator\nthat predicts whether each token in the corrupted input was replaced by a\nmasked language model. To identify triggers, we utilize this discriminator to\nlocate the most suspicious token from each training sample and then distill a\nconcise set by considering their association strengths with particular labels.\nTo recognize the poisoned subset, we examine the training samples with these\nidentified triggers as the most suspicious token, and check if removing the\ntrigger will change the poisoned model's prediction. Extensive experiments\ndemonstrate that BFClass can identify all the triggers, remove 95% poisoned\ntraining samples with very limited false alarms, and achieve almost the same\nperformance as the models trained on the benign training data.", "published": "2021-09-22 17:28:21", "link": "http://arxiv.org/abs/2109.10855v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Coarse2Fine: Fine-grained Text Classification on Coarsely-grained\n  Annotated Data", "abstract": "Existing text classification methods mainly focus on a fixed label set,\nwhereas many real-world applications require extending to new fine-grained\nclasses as the number of samples per label increases. To accommodate such\nrequirements, we introduce a new problem called coarse-to-fine grained\nclassification, which aims to perform fine-grained classification on coarsely\nannotated data. Instead of asking for new fine-grained human annotations, we\nopt to leverage label surface names as the only human guidance and weave in\nrich pre-trained generative language models into the iterative weak supervision\nstrategy. Specifically, we first propose a label-conditioned finetuning\nformulation to attune these generators for our task. Furthermore, we devise a\nregularization objective based on the coarse-fine label constraints derived\nfrom our problem setting, giving us even further improvements over the prior\nformulation. Our framework uses the fine-tuned generative models to sample\npseudo-training data for training the classifier, and bootstraps on real\nunlabeled data for model refinement. Extensive experiments and case studies on\ntwo real-world datasets demonstrate superior performance over SOTA zero-shot\nclassification baselines.", "published": "2021-09-22 17:29:01", "link": "http://arxiv.org/abs/2109.10856v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pushing the Right Buttons: Adversarial Evaluation of Quality Estimation", "abstract": "Current Machine Translation (MT) systems achieve very good results on a\ngrowing variety of language pairs and datasets. However, they are known to\nproduce fluent translation outputs that can contain important meaning errors,\nthus undermining their reliability in practice. Quality Estimation (QE) is the\ntask of automatically assessing the performance of MT systems at test time.\nThus, in order to be useful, QE systems should be able to detect such errors.\nHowever, this ability is yet to be tested in the current evaluation practices,\nwhere QE systems are assessed only in terms of their correlation with human\njudgements. In this work, we bridge this gap by proposing a general methodology\nfor adversarial testing of QE for MT. First, we show that despite a high\ncorrelation with human judgements achieved by the recent SOTA, certain types of\nmeaning errors are still problematic for QE to detect. Second, we show that on\naverage, the ability of a given model to discriminate between\nmeaning-preserving and meaning-altering perturbations is predictive of its\noverall performance, thus potentially allowing for comparing QE systems without\nrelying on manual quality annotation.", "published": "2021-09-22 17:32:18", "link": "http://arxiv.org/abs/2109.10859v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conditional Poisson Stochastic Beam Search", "abstract": "Beam search is the default decoding strategy for many sequence generation\ntasks in NLP. The set of approximate K-best items returned by the algorithm is\na useful summary of the distribution for many applications; however, the\ncandidates typically exhibit high overlap and may give a highly biased estimate\nfor expectations under our model. These problems can be addressed by instead\nusing stochastic decoding strategies. In this work, we propose a new method for\nturning beam search into a stochastic process: Conditional Poisson stochastic\nbeam search. Rather than taking the maximizing set at each iteration, we sample\nK candidates without replacement according to the conditional Poisson sampling\ndesign. We view this as a more natural alternative to Kool et. al. 2019's\nstochastic beam search (SBS). Furthermore, we show how samples generated under\nthe CPSBS design can be used to build consistent estimators and sample diverse\nsets from sequence models. In our experiments, we observe CPSBS produces lower\nvariance and more efficient estimators than SBS, even showing improvements in\nhigh entropy settings.", "published": "2021-09-22 20:49:16", "link": "http://arxiv.org/abs/2109.11034v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Sociolinguistic Variables to Reveal Changing Attitudes Towards\n  Sexuality and Gender", "abstract": "Individuals signal aspects of their identity and beliefs through linguistic\nchoices. Studying these choices in aggregate allows us to examine large-scale\nattitude shifts within a population. Here, we develop computational methods to\nstudy word choice within a sociolinguistic lexical variable -- alternate words\nused to express the same concept -- in order to test for change in the United\nStates towards sexuality and gender. We examine two variables: i) referents to\nsignificant others, such as the word \"partner\" and ii) referents to an\nindefinite person, both of which could optionally be marked with gender. The\nlinguistic choices in each variable allow us to study increased rates of\nacceptances of gay marriage and gender equality, respectively. In longitudinal\nanalyses across Twitter and Reddit over 87M messages, we demonstrate that\nattitudes are changing but that these changes are driven by specific\ndemographics within the United States. Further, in a quasi-causal analysis, we\nshow that passages of Marriage Equality Acts in different states are drivers of\nlinguistic change.", "published": "2021-09-22 22:28:07", "link": "http://arxiv.org/abs/2109.11061v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Actionable Conversational Quality Indicators for Improving Task-Oriented\n  Dialog Systems", "abstract": "Automatic dialog systems have become a mainstream part of online customer\nservice. Many such systems are built, maintained, and improved by customer\nservice specialists, rather than dialog systems engineers and computer\nprogrammers. As conversations between people and machines become commonplace,\nit is critical to understand what is working, what is not, and what actions can\nbe taken to reduce the frequency of inappropriate system responses. These\nanalyses and recommendations need to be presented in terms that directly\nreflect the user experience rather than the internal dialog processing.\n  This paper introduces and explains the use of Actionable Conversational\nQuality Indicators (ACQIs), which are used both to recognize parts of dialogs\nthat can be improved, and to recommend how to improve them. This combines\nbenefits of previous approaches, some of which have focused on producing dialog\nquality scoring while others have sought to categorize the types of errors the\ndialog system is making.\n  We demonstrate the effectiveness of using ACQIs on LivePerson internal dialog\nsystems used in commercial customer service applications, and on the publicly\navailable CMU LEGOv2 conversational dataset (Raux et al. 2005). We report on\nthe annotation and analysis of conversational datasets showing which ACQIs are\nimportant to fix in various situations.\n  The annotated datasets are then used to build a predictive model which uses a\nturn-based vector embedding of the message texts and achieves an 79% weighted\naverage f1-measure at the task of finding the correct ACQI for a given\nconversation. We predict that if such a model worked perfectly, the range of\npotential improvement actions a bot-builder must consider at each turn could be\nreduced by an average of 81%.", "published": "2021-09-22 22:41:42", "link": "http://arxiv.org/abs/2109.11064v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Second Pandemic? Analysis of Fake News About COVID-19 Vaccines in\n  Qatar", "abstract": "While COVID-19 vaccines are finally becoming widely available, a second\npandemic that revolves around the circulation of anti-vaxxer fake news may\nhinder efforts to recover from the first one. With this in mind, we performed\nan extensive analysis of Arabic and English tweets about COVID-19 vaccines,\nwith focus on messages originating from Qatar. We found that Arabic tweets\ncontain a lot of false information and rumors, while English tweets are mostly\nfactual. However, English tweets are much more propagandistic than Arabic ones.\nIn terms of propaganda techniques, about half of the Arabic tweets express\ndoubt, and 1/5 use loaded language, while English tweets are abundant in loaded\nlanguage, exaggeration, fear, name-calling, doubt, and flag-waving. Finally, in\nterms of framing, Arabic tweets adopt a health and safety perspective, while in\nEnglish economic concerns dominate.", "published": "2021-09-22 14:26:08", "link": "http://arxiv.org/abs/2109.11372v1", "categories": ["cs.CL", "cs.SI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Scalable and Efficient MoE Training for Multitask Multilingual Models", "abstract": "The Mixture of Experts (MoE) models are an emerging class of sparsely\nactivated deep learning models that have sublinear compute costs with respect\nto their parameters. In contrast with dense models, the sparse architecture of\nMoE offers opportunities for drastically growing model size with significant\naccuracy gain while consuming much lower compute budget. However, supporting\nlarge scale MoE training also has its own set of system and modeling\nchallenges. To overcome the challenges and embrace the opportunities of MoE, we\nfirst develop a system capable of scaling MoE models efficiently to trillions\nof parameters. It combines multi-dimensional parallelism and heterogeneous\nmemory technologies harmoniously with MoE to empower 8x larger models on the\nsame hardware compared with existing work. Besides boosting system efficiency,\nwe also present new training methods to improve MoE sample efficiency and\nleverage expert pruning strategy to improve inference time efficiency. By\ncombining the efficient system and training methods, we are able to\nsignificantly scale up large multitask multilingual models for language\ngeneration which results in a great improvement in model accuracy. A model\ntrained with 10 billion parameters on 50 languages can achieve state-of-the-art\nperformance in Machine Translation (MT) and multilingual natural language\ngeneration tasks. The system support of efficient MoE training has been\nimplemented and open-sourced with the DeepSpeed library.", "published": "2021-09-22 00:57:46", "link": "http://arxiv.org/abs/2109.10465v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Contextualized Document Representation", "abstract": "Several NLP tasks need the effective representation of text documents. Arora\net. al., 2017 demonstrate that simple weighted averaging of word vectors\nfrequently outperforms neural models. SCDV (Mekala et. al., 2017) further\nextends this from sentences to documents by employing soft and sparse\nclustering over pre-computed word vectors. However, both techniques ignore the\npolysemy and contextual character of words. In this paper, we address this\nissue by proposing SCDV+BERT(ctxd), a simple and effective unsupervised\nrepresentation that combines contextualized BERT (Devlin et al., 2019) based\nword embedding for word sense disambiguation with SCDV soft clustering\napproach. We show that our embeddings outperform original SCDV, pre-train BERT,\nand several other baselines on many classification datasets. We also\ndemonstrate our embeddings effectiveness on other tasks, such as concept\nmatching and sentence similarity. In addition, we show that SCDV+BERT(ctxd)\noutperforms fine-tune BERT and different embedding approaches in scenarios with\nlimited data and only few shots examples.", "published": "2021-09-22 03:56:33", "link": "http://arxiv.org/abs/2109.10509v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diarisation using location tracking with agglomerative clustering", "abstract": "Previous works have shown that spatial location information can be\ncomplementary to speaker embeddings for a speaker diarisation task. However,\nthe models used often assume that speakers are fairly stationary throughout a\nmeeting. This paper proposes to relax this assumption, by explicitly modelling\nthe movements of speakers within an Agglomerative Hierarchical Clustering (AHC)\ndiarisation framework. Kalman filters, which track the locations of speakers,\nare used to compute log-likelihood ratios that contribute to the cluster\naffinity computations for the AHC merging and stopping decisions. Experiments\nshow that the proposed approach is able to yield improvements on a Microsoft\nrich meeting transcription task, compared to methods that do not use location\ninformation or that make stationarity assumptions.", "published": "2021-09-22 08:54:10", "link": "http://arxiv.org/abs/2109.10598v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Scale Efficiently: Insights from Pre-training and Fine-tuning\n  Transformers", "abstract": "There remain many open questions pertaining to the scaling behaviour of\nTransformer architectures. These scaling decisions and findings can be\ncritical, as training runs often come with an associated computational cost\nwhich have both financial and/or environmental impact. The goal of this paper\nis to present scaling insights from pretraining and finetuning Transformers.\nWhile Kaplan et al. presents a comprehensive study of the scaling behaviour of\nTransformer language models, the scope is only on the upstream (pretraining)\nloss. Therefore, it is still unclear if these set of findings transfer to\ndownstream task within the context of the pretrain-finetune paradigm. The key\nfindings of this paper are as follows: (1) we show that aside from only the\nmodel size, model shape matters for downstream fine-tuning, (2) scaling\nprotocols operate differently at different compute regions, (3) widely adopted\nT5-base and T5-large sizes are Pareto-inefficient. To this end, we present\nimproved scaling protocols whereby our redesigned models achieve similar\ndownstream fine-tuning quality while having 50\\% fewer parameters and training\n40\\% faster compared to the widely adopted T5-base model. We publicly release\nover 100 pretrained checkpoints of different T5 configurations to facilitate\nfuture research and analysis.", "published": "2021-09-22 12:29:15", "link": "http://arxiv.org/abs/2109.10686v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Low-Latency Incremental Text-to-Speech Synthesis with Distilled Context\n  Prediction Network", "abstract": "Incremental text-to-speech (TTS) synthesis generates utterances in small\nlinguistic units for the sake of real-time and low-latency applications. We\npreviously proposed an incremental TTS method that leverages a large\npre-trained language model to take unobserved future context into account\nwithout waiting for the subsequent segment. Although this method achieves\ncomparable speech quality to that of a method that waits for the future\ncontext, it entails a huge amount of processing for sampling from the language\nmodel at each time step. In this paper, we propose an incremental TTS method\nthat directly predicts the unobserved future context with a lightweight model,\ninstead of sampling words from the large-scale language model. We perform\nknowledge distillation from a GPT2-based context prediction network into a\nsimple recurrent model by minimizing a teacher-student loss defined between the\ncontext embedding vectors of those models. Experimental results show that the\nproposed method requires about ten times less inference time to achieve\ncomparable synthetic speech quality to that of our previous method, and it can\nperform incremental synthesis much faster than the average speaking speed of\nhuman English speakers, demonstrating the availability of our method to\nreal-time applications.", "published": "2021-09-22 13:29:10", "link": "http://arxiv.org/abs/2109.10724v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pix2seq: A Language Modeling Framework for Object Detection", "abstract": "We present Pix2Seq, a simple and generic framework for object detection.\nUnlike existing approaches that explicitly integrate prior knowledge about the\ntask, we cast object detection as a language modeling task conditioned on the\nobserved pixel inputs. Object descriptions (e.g., bounding boxes and class\nlabels) are expressed as sequences of discrete tokens, and we train a neural\nnetwork to perceive the image and generate the desired sequence. Our approach\nis based mainly on the intuition that if a neural network knows about where and\nwhat the objects are, we just need to teach it how to read them out. Beyond the\nuse of task-specific data augmentations, our approach makes minimal assumptions\nabout the task, yet it achieves competitive results on the challenging COCO\ndataset, compared to highly specialized and well optimized detection\nalgorithms.", "published": "2021-09-22 17:26:36", "link": "http://arxiv.org/abs/2109.10852v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Recursively Summarizing Books with Human Feedback", "abstract": "A major challenge for scaling machine learning is training models to perform\ntasks that are very difficult or time-consuming for humans to evaluate. We\npresent progress on this problem on the task of abstractive summarization of\nentire fiction novels. Our method combines learning from human feedback with\nrecursive task decomposition: we use models trained on smaller parts of the\ntask to assist humans in giving feedback on the broader task. We collect a\nlarge volume of demonstrations and comparisons from human labelers, and\nfine-tune GPT-3 using behavioral cloning and reward modeling to do\nsummarization recursively. At inference time, the model first summarizes small\nsections of the book and then recursively summarizes these summaries to produce\na summary of the entire book. Our human labelers are able to supervise and\nevaluate the models quickly, despite not having read the entire books\nthemselves. Our resulting model generates sensible summaries of entire books,\neven matching the quality of human-written summaries in a few cases ($\\sim5\\%$\nof books). We achieve state-of-the-art results on the recent BookSum dataset\nfor book-length summarization. A zero-shot question-answering model using these\nsummaries achieves state-of-the-art results on the challenging NarrativeQA\nbenchmark for answering questions about books and movie scripts. We release\ndatasets of samples from our model.", "published": "2021-09-22 17:34:18", "link": "http://arxiv.org/abs/2109.10862v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Alzheimers Dementia Detection using Acoustic & Linguistic features and\n  Pre-Trained BERT", "abstract": "Alzheimers disease is a fatal progressive brain disorder that worsens with\ntime. It is high time we have inexpensive and quick clinical diagnostic\ntechniques for early detection and care. In previous studies, various Machine\nLearning techniques and Pre-trained Deep Learning models have been used in\nconjunction with the extraction of various acoustic and linguistic features.\nOur study focuses on three models for the classification task in the ADReSS\n(The Alzheimers Dementia Recognition through Spontaneous Speech) 2021\nChallenge. We use the well-balanced dataset provided by the ADReSS Challenge\nfor training and validating our models. Model 1 uses various acoustic features\nfrom the eGeMAPs feature-set, Model 2 uses various linguistic features that we\ngenerated from auto-generated transcripts and Model 3 uses the auto-generated\ntranscripts directly to extract features using a Pre-trained BERT and TF-IDF.\nThese models are described in detail in the models section.", "published": "2021-09-22 19:57:04", "link": "http://arxiv.org/abs/2109.11010v2", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Exploring Decomposition for Table-based Fact Verification", "abstract": "Fact verification based on structured data is challenging as it requires\nmodels to understand both natural language and symbolic operations performed\nover tables. Although pre-trained language models have demonstrated a strong\ncapability in verifying simple statements, they struggle with complex\nstatements that involve multiple operations. In this paper, we improve fact\nverification by decomposing complex statements into simpler subproblems.\nLeveraging the programs synthesized by a weakly supervised semantic parser, we\npropose a program-guided approach to constructing a pseudo dataset for\ndecomposition model training. The subproblems, together with their predicted\nanswers, serve as the intermediate evidence to enhance our fact verification\nmodel. Experiments show that our proposed approach achieves the new\nstate-of-the-art performance, an 82.7\\% accuracy, on the TabFact benchmark.", "published": "2021-09-22 20:15:05", "link": "http://arxiv.org/abs/2109.11020v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Few-Shot Learning Approach for Sound Source Distance Estimation Using\n  Relation Networks", "abstract": "In this paper, we study the performance of few-shot learning, specifically\nmeta learning empowered few-shot relation networks, over supervised deep\nlearning and conventional machine learning approaches in the problem of Sound\nSource Distance Estimation (SSDE). In previous research on deep supervised\nSSDE, low accuracies have often resulted from the mismatch between the training\ndata (from known environments) and the test data (from unknown environments).\nBy performing comparative experiments on a sufficient amount of data, we show\nthat the few-shot relation network outperforms other competitors including\neXtreme Gradient Boosting (XGBoost), Support Vector Machine (SVM),\nConvolutional Neural Network (CNN), and MultiLayer Perceptron (MLP). Hence it\nis possible to calibrate a microphone-equipped system, with a few labeled\nsamples of audio recorded in a particular unknown environment to adjust and\ngeneralize our classifier to the possible input data and gain higher\naccuracies.", "published": "2021-09-22 07:44:01", "link": "http://arxiv.org/abs/2109.10561v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noisy-to-Noisy Voice Conversion Framework with Denoising Model", "abstract": "In a conventional voice conversion (VC) framework, a VC model is often\ntrained with a clean dataset consisting of speech data carefully recorded and\nselected by minimizing background interference. However, collecting such a\nhigh-quality dataset is expensive and time-consuming. Leveraging crowd-sourced\nspeech data in training is more economical. Moreover, for some real-world VC\nscenarios such as VC in video and VC-based data augmentation for speech\nrecognition systems, the background sounds themselves are also informative and\nneed to be maintained. In this paper, to explore VC with the flexibility of\nhandling background sounds, we propose a noisy-to-noisy (N2N) VC framework\ncomposed of a denoising module and a VC module. With the proposed framework, we\ncan convert the speaker's identity while preserving the background sounds. Both\nobjective and subjective evaluations are conducted, and the results reveal the\neffectiveness of the proposed framework.", "published": "2021-09-22 09:21:52", "link": "http://arxiv.org/abs/2109.10608v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
