{"title": "Urdu Word Segmentation using Conditional Random Fields (CRFs)", "abstract": "State-of-the-art Natural Language Processing algorithms rely heavily on\nefficient word segmentation. Urdu is amongst languages for which word\nsegmentation is a complex task as it exhibits space omission as well as space\ninsertion issues. This is partly due to the Arabic script which although\ncursive in nature, consists of characters that have inherent joining and\nnon-joining attributes regardless of word boundary. This paper presents a word\nsegmentation system for Urdu which uses a Conditional Random Field sequence\nmodeler with orthographic, linguistic and morphological features. Our proposed\nmodel automatically learns to predict white space as word boundary as well as\nZero Width Non-Joiner (ZWNJ) as sub-word boundary. Using a manually annotated\ncorpus, our model achieves F1 score of 0.97 for word boundary identification\nand 0.85 for sub-word boundary identification tasks. We have made our code and\ncorpus publicly available to make our results reproducible.", "published": "2018-06-14 09:39:32", "link": "http://arxiv.org/abs/1806.05432v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Context-Aware Question Matching in\n  Information-seeking Conversations in E-commerce", "abstract": "Building multi-turn information-seeking conversation systems is an important\nand challenging research topic. Although several advanced neural text matching\nmodels have been proposed for this task, they are generally not efficient for\nindustrial applications. Furthermore, they rely on a large amount of labeled\ndata, which may not be available in real-world applications. To alleviate these\nproblems, we study transfer learning for multi-turn information seeking\nconversations in this paper. We first propose an efficient and effective\nmulti-turn conversation model based on convolutional neural networks. After\nthat, we extend our model to adapt the knowledge learned from a resource-rich\ndomain to enhance the performance. Finally, we deployed our model in an\nindustrial chatbot called AliMe Assist\n(https://consumerservice.taobao.com/online-help) and observed a significant\nimprovement over the existing online model.", "published": "2018-06-14 09:44:59", "link": "http://arxiv.org/abs/1806.05434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Cross-lingual Distributed Logical Representations for Semantic\n  Parsing", "abstract": "With the development of several multilingual datasets used for semantic\nparsing, recent research efforts have looked into the problem of learning\nsemantic parsers in a multilingual setup. However, how to improve the\nperformance of a monolingual semantic parser for a specific language by\nleveraging data annotated in different languages remains a research question\nthat is under-explored. In this work, we present a study to show how learning\ndistributed representations of the logical forms from data annotated in\ndifferent languages can be used for improving the performance of a monolingual\nsemantic parser. We extend two existing monolingual semantic parsers to\nincorporate such cross-lingual distributed logical representations as features.\nExperiments show that our proposed approach is able to yield improved semantic\nparsing results on the standard multilingual GeoQuery dataset.", "published": "2018-06-14 10:59:22", "link": "http://arxiv.org/abs/1806.05461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphological and Language-Agnostic Word Segmentation for NMT", "abstract": "The state of the art of handling rich morphology in neural machine\ntranslation (NMT) is to break word forms into subword units, so that the\noverall vocabulary size of these units fits the practical limits given by the\nNMT model and GPU memory capacity. In this paper, we compare two common but\nlinguistically uninformed methods of subword construction (BPE and STE, the\nmethod implemented in Tensor2Tensor toolkit) and two linguistically-motivated\nmethods: Morfessor and one novel method, based on a derivational dictionary.\nOur experiments with German-to-Czech translation, both morphologically rich,\ndocument that so far, the non-motivated methods perform better. Furthermore, we\niden- tify a critical difference between BPE and STE and show a simple pre-\nprocessing step for BPE that considerably increases translation quality as\nevaluated by automatic measures.", "published": "2018-06-14 11:44:48", "link": "http://arxiv.org/abs/1806.05482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect Sentiment Model for Micro Reviews", "abstract": "This paper aims at an aspect sentiment model for aspect-based sentiment\nanalysis (ABSA) focused on micro reviews. This task is important in order to\nunderstand short reviews majority of the users write, while existing topic\nmodels are targeted for expert-level long reviews with sufficient co-occurrence\npatterns to observe. Current methods on aggregating micro reviews using\nmetadata information may not be effective as well due to metadata absence,\ntopical heterogeneity, and cold start problems. To this end, we propose a model\ncalled Micro Aspect Sentiment Model (MicroASM). MicroASM is based on the\nobservation that short reviews 1) are viewed with sentiment-aspect word pairs\nas building blocks of information, and 2) can be clustered into larger reviews.\nWhen compared to the current state-of-the-art aspect sentiment models,\nexperiments show that our model provides better performance on aspect-level\ntasks such as aspect term extraction and document-level tasks such as sentiment\nclassification.", "published": "2018-06-14 12:28:43", "link": "http://arxiv.org/abs/1806.05499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Commonsense Representation for Neural Abstractive Summarization", "abstract": "A major proportion of a text summary includes important entities found in the\noriginal text. These entities build up the topic of the summary. Moreover, they\nhold commonsense information once they are linked to a knowledge base. Based on\nthese observations, this paper investigates the usage of linked entities to\nguide the decoder of a neural text summarizer to generate concise and better\nsummaries. To this end, we leverage on an off-the-shelf entity linking system\n(ELS) to extract linked entities and propose Entity2Topic (E2T), a module\neasily attachable to a sequence-to-sequence model that transforms a list of\nentities into a vector representation of the topic of the summary. Current\navailable ELS's are still not sufficiently effective, possibly introducing\nunresolved ambiguities and irrelevant entities. We resolve the imperfections of\nthe ELS by (a) encoding entities with selective disambiguation, and (b) pooling\nentity vectors using firm attention. By applying E2T to a simple\nsequence-to-sequence model with attention mechanism as base model, we see\nsignificant improvements of the performance in the Gigaword (sentence to title)\nand CNN (long document to multi-sentence highlights) summarization datasets by\nat least 2 ROUGE points.", "published": "2018-06-14 12:41:50", "link": "http://arxiv.org/abs/1806.05504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cold-Start Aware User and Product Attention for Sentiment Classification", "abstract": "The use of user/product information in sentiment analysis is important,\nespecially for cold-start users/products, whose number of reviews are very\nlimited. However, current models do not deal with the cold-start problem which\nis typical in review websites. In this paper, we present Hybrid Contextualized\nSentiment Classifier (HCSC), which contains two modules: (1) a fast word\nencoder that returns word vectors embedded with short and long range dependency\nfeatures; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism\nthat considers the existence of cold-start problem when attentively pooling the\nencoded word vectors. HCSC introduces shared vectors that are constructed from\nsimilar users/products, and are used when the original distinct vectors do not\nhave sufficient information (i.e. cold-start). This is decided by a\nfrequency-guided selective gate vector. Our experiments show that in terms of\nRMSE, HCSC performs significantly better when compared with on famous datasets,\ndespite having less complexity, and thus can be trained much faster. More\nimportantly, our model performs significantly better than previous models when\nthe training data is sparse and has cold-start problems.", "published": "2018-06-14 12:48:31", "link": "http://arxiv.org/abs/1806.05507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Humor Detection in English-Hindi Code-Mixed Social Media Content :\n  Corpus and Baseline System", "abstract": "The tremendous amount of user generated data through social networking sites\nled to the gaining popularity of automatic text classification in the field of\ncomputational linguistics over the past decade. Within this domain, one problem\nthat has drawn the attention of many researchers is automatic humor detection\nin texts. In depth semantic understanding of the text is required to detect\nhumor which makes the problem difficult to automate. With increase in the\nnumber of social media users, many multilingual speakers often interchange\nbetween languages while posting on social media which is called code-mixing. It\nintroduces some challenges in the field of linguistic analysis of social media\ncontent (Barman et al., 2014), like spelling variations and non-grammatical\nstructures in a sentence. Past researches include detecting puns in texts (Kao\net al., 2016) and humor in one-lines (Mihalcea et al., 2010) in a single\nlanguage, but with the tremendous amount of code-mixed data available online,\nthere is a need to develop techniques which detects humor in code-mixed tweets.\nIn this paper, we analyze the task of humor detection in texts and describe a\nfreely available corpus containing English-Hindi code-mixed tweets annotated\nwith humorous(H) or non-humorous(N) tags. We also tagged the words in the\ntweets with Language tags (English/Hindi/Others). Moreover, we describe the\nexperiments carried out on the corpus and provide a baseline classification\nsystem which distinguishes between humorous and non-humorous texts.", "published": "2018-06-14 12:57:13", "link": "http://arxiv.org/abs/1806.05513v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translations as Additional Contexts for Sentence Classification", "abstract": "In sentence classification tasks, additional contexts, such as the\nneighboring sentences, may improve the accuracy of the classifier. However,\nsuch contexts are domain-dependent and thus cannot be used for another\nclassification task with an inappropriate domain. In contrast, we propose the\nuse of translated sentences as context that is always available regardless of\nthe domain. We find that naive feature expansion of translations gains only\nmarginal improvements and may decrease the performance of the classifier, due\nto possible inaccurate translations thus producing noisy sentence vectors. To\nthis end, we present multiple context fixing attachment (MCFA), a series of\nmodules attached to multiple sentence vectors to fix the noise in the vectors\nusing the other sentence vectors as context. We show that our method performs\ncompetitively compared to previous models, achieving best classification\nperformance on multiple data sets. We are the first to use translations as\ndomain-free contexts for sentence classification.", "published": "2018-06-14 13:01:04", "link": "http://arxiv.org/abs/1806.05516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Open Information Extraction", "abstract": "We provide a detailed overview of the various approaches that were proposed\nto date to solve the task of Open Information Extraction. We present the major\nchallenges that such systems face, show the evolution of the suggested\napproaches over time and depict the specific issues they address. In addition,\nwe provide a critique of the commonly applied evaluation procedures for\nassessing the performance of Open IE systems and highlight some directions for\nfuture work.", "published": "2018-06-14 15:07:46", "link": "http://arxiv.org/abs/1806.05599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Prediction in English-Hindi Code-Mixed Social Media Content :\n  Corpus and Baseline System", "abstract": "The rapid expansion in the usage of social media networking sites leads to a\nhuge amount of unprocessed user generated data which can be used for text\nmining. Author profiling is the problem of automatically determining profiling\naspects like the author's gender and age group through a text is gaining much\npopularity in computational linguistics. Most of the past research in author\nprofiling is concentrated on English texts \\cite{1,2}. However many users often\nchange the language while posting on social media which is called code-mixing,\nand it develops some challenges in the field of text classification and author\nprofiling like variations in spelling, non-grammatical structure and\ntransliteration \\cite{3}. There are very few English-Hindi code-mixed annotated\ndatasets of social media content present online \\cite{4}. In this paper, we\nanalyze the task of author's gender prediction in code-mixed content and\npresent a corpus of English-Hindi texts collected from Twitter which is\nannotated with author's gender. We also explore language identification of\nevery word in this corpus. We present a supervised classification baseline\nsystem which uses various machine learning algorithms to identify the gender of\nan author using a text, based on character and word level features.", "published": "2018-06-14 15:08:22", "link": "http://arxiv.org/abs/1806.05600v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NCRF++: An Open-source Neural Sequence Labeling Toolkit", "abstract": "This paper describes NCRF++, a toolkit for neural sequence labeling. NCRF++\nis designed for quick implementation of different neural sequence labeling\nmodels with a CRF inference layer. It provides users with an inference for\nbuilding the custom model structure through configuration file with flexible\nneural feature design and utilization. Built on PyTorch, the core operations\nare calculated in batch, making the toolkit efficient with the acceleration of\nGPU. It also includes the implementations of most state-of-the-art neural\nsequence labeling models such as LSTM-CRF, facilitating reproducing and\nrefinement on those methods.", "published": "2018-06-14 16:10:57", "link": "http://arxiv.org/abs/1806.05626v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstract Meaning Representation for Multi-Document Summarization", "abstract": "Generating an abstract from a collection of documents is a desirable\ncapability for many real-world applications. However, abstractive approaches to\nmulti-document summarization have not been thoroughly investigated. This paper\nstudies the feasibility of using Abstract Meaning Representation (AMR), a\nsemantic representation of natural language grounded in linguistic theory, as a\nform of content representation. Our approach condenses source documents to a\nset of summary graphs following the AMR formalism. The summary graphs are then\ntransformed to a set of summary sentences in a surface realization step. The\nframework is fully data-driven and flexible. Each component can be optimized\nindependently using small-scale, in-domain training data. We perform\nexperiments on benchmark summarization datasets and report promising results.\nWe also describe opportunities and challenges for advancing this line of\nresearch.", "published": "2018-06-14 17:25:43", "link": "http://arxiv.org/abs/1806.05655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structure-Infused Copy Mechanisms for Abstractive Summarization", "abstract": "Seq2seq learning has produced promising results on summarization. However, in\nmany cases, system summaries still struggle to keep the meaning of the original\nintact. They may miss out important words or relations that play critical roles\nin the syntactic structure of source sentences. In this paper, we present\nstructure-infused copy mechanisms to facilitate copying important words and\nrelations from the source sentence to summary sentence. The approach naturally\ncombines source dependency structure with the copy mechanism of an abstractive\nsentence summarizer. Experimental results demonstrate the effectiveness of\nincorporating source-side syntactic information in the system, and our proposed\napproach compares favorably to state-of-the-art methods.", "published": "2018-06-14 17:31:18", "link": "http://arxiv.org/abs/1806.05658v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Language Identification for Romance Languages using Stop Words\n  and Diacritics", "abstract": "Automatic language identification is a natural language processing problem\nthat tries to determine the natural language of a given content. In this paper\nwe present a statistical method for automatic language identification of\nwritten text using dictionaries containing stop words and diacritics. We\npropose different approaches that combine the two dictionaries to accurately\ndetermine the language of textual corpora. This method was chosen because stop\nwords and diacritics are very specific to a language, although some languages\nhave some similar words and special characters they are not all common. The\nlanguages taken into account were romance languages because they are very\nsimilar and usually it is hard to distinguish between them from a computational\npoint of view. We have tested our method using a Twitter corpus and a news\narticle corpus. Both corpora consists of UTF-8 encoded text, so the diacritics\ncould be taken into account, in the case that the text has no diacritics only\nthe stop words are used to determine the language of the text. The experimental\nresults show that the proposed method has an accuracy of over 90% for small\ntexts and over 99.8% for", "published": "2018-06-14 11:38:24", "link": "http://arxiv.org/abs/1806.05480v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Nearly Zero-Shot Learning for Semantic Decoding in Spoken Dialogue\n  Systems", "abstract": "This paper presents two ways of dealing with scarce data in semantic decoding\nusing N-Best speech recognition hypotheses. First, we learn features by using a\ndeep learning architecture in which the weights for the unknown and known\ncategories are jointly optimised. Second, an unsupervised method is used for\nfurther tuning the weights. Sharing weights injects prior knowledge to unknown\ncategories. The unsupervised tuning (i.e. the risk minimisation) improves the\nF-Measure when recognising nearly zero-shot data on the DSTC3 corpus. This\nunsupervised method can be applied subject to two assumptions: the rank of the\nclass marginal is assumed to be known and the class-conditional scores of the\nclassifier are assumed to follow a Gaussian distribution.", "published": "2018-06-14 11:47:14", "link": "http://arxiv.org/abs/1806.05484v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SemAxis: A Lightweight Framework to Characterize Domain-Specific Word\n  Semantics Beyond Sentiment", "abstract": "Because word semantics can substantially change across communities and\ncontexts, capturing domain-specific word semantics is an important challenge.\nHere, we propose SEMAXIS, a simple yet powerful framework to characterize word\nsemantics using many semantic axes in word- vector spaces beyond sentiment. We\ndemonstrate that SEMAXIS can capture nuanced semantic representations in\nmultiple online communities. We also show that, when the sentiment axis is\nexamined, SEMAXIS outperforms the state-of-the-art approaches in building\ndomain-specific sentiment lexicons.", "published": "2018-06-14 13:11:36", "link": "http://arxiv.org/abs/1806.05521v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Grounded Textual Entailment", "abstract": "Capturing semantic relations between sentences, such as entailment, is a\nlong-standing challenge for computational semantics. Logic-based models analyse\nentailment in terms of possible worlds (interpretations, or situations) where a\npremise P entails a hypothesis H iff in all worlds where P is true, H is also\ntrue. Statistical models view this relationship probabilistically, addressing\nit in terms of whether a human would likely infer H from P. In this paper, we\nwish to bridge these two perspectives, by arguing for a visually-grounded\nversion of the Textual Entailment task. Specifically, we ask whether models can\nperform better if, in addition to P and H, there is also an image\n(corresponding to the relevant \"world\" or \"situation\"). We use a multimodal\nversion of the SNLI dataset (Bowman et al., 2015) and we compare \"blind\" and\nvisually-augmented models of textual entailment. We show that visual\ninformation is beneficial, but we also conduct an in-depth error analysis that\nreveals that current multimodal models are not performing \"grounding\" in an\noptimal fashion.", "published": "2018-06-14 16:56:44", "link": "http://arxiv.org/abs/1806.05645v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "GLoMo: Unsupervisedly Learned Relational Graphs as Transferable\n  Representations", "abstract": "Modern deep transfer learning approaches have mainly focused on learning\ngeneric feature vectors from one task that are transferable to other tasks,\nsuch as word embeddings in language and pretrained convolutional features in\nvision. However, these approaches usually transfer unary features and largely\nignore more structured graphical representations. This work explores the\npossibility of learning generic latent relational graphs that capture\ndependencies between pairs of data units (e.g., words or pixels) from\nlarge-scale unlabeled data and transferring the graphs to downstream tasks. Our\nproposed transfer learning framework improves performance on various tasks\nincluding question answering, natural language inference, sentiment analysis,\nand image classification. We also show that the learned graphs are generic\nenough to be transferred to different embeddings on which the graphs have not\nbeen trained (including GloVe embeddings, ELMo embeddings, and task-specific\nRNN hidden unit), or embedding-free units such as image pixels.", "published": "2018-06-14 17:41:19", "link": "http://arxiv.org/abs/1806.05662v3", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Using Search Queries to Understand Health Information Needs in Africa", "abstract": "The lack of comprehensive, high-quality health data in developing nations\ncreates a roadblock for combating the impacts of disease. One key challenge is\nunderstanding the health information needs of people in these nations. Without\nunderstanding people's everyday needs, concerns, and misconceptions, health\norganizations and policymakers lack the ability to effectively target education\nand programming efforts. In this paper, we propose a bottom-up approach that\nuses search data from individuals to uncover and gain insight into health\ninformation needs in Africa. We analyze Bing searches related to HIV/AIDS,\nmalaria, and tuberculosis from all 54 African nations. For each disease, we\nautomatically derive a set of common search themes or topics, revealing a\nwide-spread interest in various types of information, including disease\nsymptoms, drugs, concerns about breastfeeding, as well as stigma, beliefs in\nnatural cures, and other topics that may be hard to uncover through traditional\nsurveys. We expose the different patterns that emerge in health information\nneeds by demographic groups (age and sex) and country. We also uncover\ndiscrepancies in the quality of content returned by search engines to users by\ntopic. Combined, our results suggest that search data can help illuminate\nhealth information needs in Africa and inform discussions on health policy and\ntargeted education efforts both on- and offline.", "published": "2018-06-14 20:48:41", "link": "http://arxiv.org/abs/1806.05740v2", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Hierarchical interpretations for neural network predictions", "abstract": "Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.", "published": "2018-06-14 02:41:03", "link": "http://arxiv.org/abs/1806.05337v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "VoxCeleb2: Deep Speaker Recognition", "abstract": "The objective of this paper is speaker recognition under noisy and\nunconstrained conditions.\n  We make two key contributions. First, we introduce a very large-scale\naudio-visual speaker recognition dataset collected from open-source media.\nUsing a fully automated pipeline, we curate VoxCeleb2 which contains over a\nmillion utterances from over 6,000 speakers. This is several times larger than\nany publicly available speaker recognition dataset.\n  Second, we develop and compare Convolutional Neural Network (CNN) models and\ntraining strategies that can effectively recognise identities from voice under\nvarious conditions. The models trained on the VoxCeleb2 dataset surpass the\nperformance of previous works on a benchmark dataset by a significant margin.", "published": "2018-06-14 15:59:12", "link": "http://arxiv.org/abs/1806.05622v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
