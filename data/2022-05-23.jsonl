{"title": "Computational Storytelling and Emotions: A Survey", "abstract": "Storytelling has always been vital for human nature. From ancient times,\nhumans have used stories for several objectives including entertainment,\nadvertisement, and education. Various analyses have been conducted by\nresearchers and creators to determine the way of producing good stories. The\ndeep relationship between stories and emotions is a prime example. With the\nadvancement in deep learning technology, computers are expected to understand\nand generate stories. This survey paper is intended to summarize and further\ncontribute to the development of research being conducted on the relationship\nbetween stories and emotions. We believe creativity research is not to replace\nhumans with computers, but to find a way of collaboration between humans and\ncomputers to enhance the creativity. With the intention of creating a new\nintersection between computational storytelling research and human creative\nwriting, we introduced creative techniques used by professional storytellers.", "published": "2022-05-23 00:21:59", "link": "http://arxiv.org/abs/2205.10967v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Short Text Classification With Augmented Data Using GPT-3", "abstract": "GPT-3 is a large-scale natural language model developed by OpenAI that can\nperform many different tasks, including topic classification. Although\nresearchers claim that it requires only a small number of in-context examples\nto learn a task, in practice GPT-3 requires these training examples to be\neither of exceptional quality or a higher quantity than easily created by hand.\nTo address this issue, this study teaches GPT-3 to classify whether a question\nis related to data science by augmenting a small training set with additional\nexamples generated by GPT-3 itself. This study compares two classifiers: the\nGPT-3 Classification Endpoint with augmented examples, and the GPT-3 Completion\nEndpoint with an optimal training set chosen using a genetic algorithm. We find\nthat while the augmented Completion Endpoint achieves upwards of 80 percent\nvalidation accuracy, using the augmented Classification Endpoint yields more\nconsistent accuracy on unseen examples. In this way, giving large-scale machine\nlearning models like GPT-3 the ability to propose their own additional training\nexamples can result in improved classification performance.", "published": "2022-05-23 01:10:38", "link": "http://arxiv.org/abs/2205.10981v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Artificial intelligence for topic modelling in Hindu philosophy: mapping\n  themes between the Upanishads and the Bhagavad Gita", "abstract": "A distinct feature of Hindu religious and philosophical text is that they\ncome from a library of texts rather than single source. The Upanishads is known\nas one of the oldest philosophical texts in the world that forms the foundation\nof Hindu philosophy. The Bhagavad Gita is core text of Hindu philosophy and is\nknown as a text that summarises the key philosophies of the Upanishads with\nmajor focus on the philosophy of karma. These texts have been translated into\nmany languages and there exists studies about themes and topics that are\nprominent; however, there is not much study of topic modelling using language\nmodels which are powered by deep learning. In this paper, we use advanced\nlanguage produces such as BERT to provide topic modelling of the key texts of\nthe Upanishads and the Bhagavad Gita. We analyse the distinct and overlapping\ntopics amongst the texts and visualise the link of selected texts of the\nUpanishads with Bhagavad Gita. Our results show a very high similarity between\nthe topics of these two texts with the mean cosine similarity of 73%. We find\nthat out of the fourteen topics extracted from the Bhagavad Gita, nine of them\nhave a cosine similarity of more than 70% with the topics of the Upanishads. We\nalso found that topics generated by the BERT-based models show very high\ncoherence as compared to that of conventional models. Our best performing model\ngives a coherence score of 73% on the Bhagavad Gita and 69% on The Upanishads.\nThe visualization of the low dimensional embeddings of these texts shows very\nclear overlapping among their topics adding another level of validation to our\nresults.", "published": "2022-05-23 03:39:00", "link": "http://arxiv.org/abs/2205.11020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vector-Quantized Input-Contextualized Soft Prompts for Natural Language\n  Understanding", "abstract": "Prompt Tuning has been largely successful as a parameter-efficient method of\nconditioning large-scale pre-trained language models to perform downstream\ntasks. Thus far, soft prompt tuning learns a fixed set of task-specific\ncontinuous vectors, i.e., soft tokens that remain static across the task\nsamples. A fixed prompt, however, may not generalize well to the diverse kinds\nof inputs the task comprises. In order to address this, we propose\nVector-quantized Input-contextualized Prompts (VIP) as an extension to the soft\nprompt tuning framework. VIP particularly focuses on two aspects -- contextual\nprompts that learns input-specific contextualization of the soft prompt tokens\nthrough a small-scale sentence encoder and quantized prompts that maps the\ncontextualized prompts to a set of learnable codebook vectors through a Vector\nquantization network. On various language understanding tasks like SuperGLUE,\nQA, Relation classification, NER and NLI, VIP outperforms the soft prompt\ntuning (PT) baseline by an average margin of 1.19%. Further, our generalization\nstudies show that VIP learns more robust prompt representations, surpassing PT\nby a margin of 0.6% - 5.3% on Out-of-domain QA and NLI tasks respectively, and\nby 0.75% on Multi-Task setup over 4 tasks spanning across 12 domains.", "published": "2022-05-23 03:51:27", "link": "http://arxiv.org/abs/2205.11024v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla", "abstract": "This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.", "published": "2022-05-23 06:54:56", "link": "http://arxiv.org/abs/2205.11081v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Fine-grained Interpretability Evaluation Benchmark for Neural NLP", "abstract": "While there is increasing concern about the interpretability of neural\nmodels, the evaluation of interpretability remains an open problem, due to the\nlack of proper evaluation datasets and metrics. In this paper, we present a\nnovel benchmark to evaluate the interpretability of both neural models and\nsaliency methods. This benchmark covers three representative NLP tasks:\nsentiment analysis, textual similarity and reading comprehension, each provided\nwith both English and Chinese annotated data. In order to precisely evaluate\nthe interpretability, we provide token-level rationales that are carefully\nannotated to be sufficient, compact and comprehensive. We also design a new\nmetric, i.e., the consistency between the rationales before and after\nperturbations, to uniformly evaluate the interpretability on different types of\ntasks. Based on this benchmark, we conduct experiments on three typical models\nwith three saliency methods, and unveil their strengths and weakness in terms\nof interpretability. We will release this benchmark\nhttps://www.luge.ai/#/luge/task/taskDetail?taskId=15 and hope it can facilitate\nthe research in building trustworthy systems.", "published": "2022-05-23 07:37:04", "link": "http://arxiv.org/abs/2205.11097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Drives the Use of Metaphorical Language? Negative Insights from\n  Abstractness, Affect, Discourse Coherence and Contextualized Word\n  Representations", "abstract": "Given a specific discourse, which discourse properties trigger the use of\nmetaphorical language, rather than using literal alternatives? For example,\nwhat drives people to say \"grasp the meaning\" rather than \"understand the\nmeaning\" within a specific context? Many NLP approaches to metaphorical\nlanguage rely on cognitive and (psycho-)linguistic insights and have\nsuccessfully defined models of discourse coherence, abstractness and affect. In\nthis work, we build five simple models relying on established cognitive and\nlinguistic properties -- frequency, abstractness, affect, discourse coherence\nand contextualized word representations -- to predict the use of a metaphorical\nvs. synonymous literal expression in context. By comparing the models' outputs\nto human judgments, our study indicates that our selected properties are not\nsufficient to systematically explain metaphorical vs. literal language choices.", "published": "2022-05-23 08:08:53", "link": "http://arxiv.org/abs/2205.11113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Please, Don't Forget the Difference and the Confidence Interval when\n  Seeking for the State-of-the-Art Status", "abstract": "This paper argues for the widest possible use of bootstrap confidence\nintervals for comparing NLP system performances instead of the state-of-the-art\nstatus (SOTA) and statistical significance testing. Their main benefits are to\ndraw attention to the difference in performance between two systems and to help\nassessing the degree of superiority of one system over another. Two cases\nstudies, one comparing several systems and the other based on a K-fold\ncross-validation procedure, illustrate these benefits. A python module for\nobtaining these confidence intervals as well as a second function implementing\nthe Fisher-Pitman test for paired samples are freely available on PyPi.", "published": "2022-05-23 08:42:17", "link": "http://arxiv.org/abs/2205.11134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Lifelong Learning", "abstract": "The longstanding goal of multi-lingual learning has been to develop a\nuniversal cross-lingual model that can withstand the changes in multi-lingual\ndata distributions. There has been a large amount of work to adapt such\nmulti-lingual models to unseen target languages. However, the majority of work\nin this direction focuses on the standard one-hop transfer learning pipeline\nfrom source to target languages, whereas in realistic scenarios, new languages\ncan be incorporated at any time in a sequential manner. In this paper, we\npresent a principled Cross-lingual Continual Learning (CCL) evaluation\nparadigm, where we analyze different categories of approaches used to\ncontinually adapt to emerging data from different languages. We provide\ninsights into what makes multilingual sequential learning particularly\nchallenging. To surmount such challenges, we benchmark a representative set of\ncross-lingual continual learning algorithms and analyze their knowledge\npreservation, accumulation, and generalization capabilities compared to\nbaselines on carefully curated datastreams. The implications of this analysis\ninclude a recipe for how to measure and balance different cross-lingual\ncontinual learning desiderata, which go beyond conventional transfer learning.", "published": "2022-05-23 09:25:43", "link": "http://arxiv.org/abs/2205.11152v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RuNNE-2022 Shared Task: Recognizing Nested Named Entities", "abstract": "The RuNNE Shared Task approaches the problem of nested named entity\nrecognition. The annotation schema is designed in such a way, that an entity\nmay partially overlap or even be nested into another entity. This way, the\nnamed entity \"The Yermolova Theatre\" of type \"organization\" houses another\nentity \"Yermolova\" of type \"person\". We adopt the Russian NEREL dataset for the\nRuNNE Shared Task. NEREL comprises news texts written in the Russian language\nand collected from the Wikinews portal. The annotation schema includes 29\nentity types. The nestedness of named entities in NEREL reaches up to six\nlevels. The RuNNE Shared Task explores two setups. (i) In the general setup all\nentities occur more or less with the same frequency. (ii) In the few-shot setup\nthe majority of entity types occur often in the training set. However, some of\nthe entity types are have lower frequency, being thus challenging to recognize.\nIn the test set the frequency of all entity types is even.\n  This paper reports on the results of the RuNNE Shared Task. Overall the\nshared task has received 156 submissions from nine teams. Half of the\nsubmissions outperform a straightforward BERT-based baseline in both setups.\nThis paper overviews the shared task setup and discusses the submitted systems,\ndiscovering meaning insights for the problem of nested NER. The links to the\nevaluation platform and the data from the shared task are available in our\ngithub repository: https://github.com/dialogue-evaluation/RuNNE.", "published": "2022-05-23 09:50:42", "link": "http://arxiv.org/abs/2205.11159v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Self-Paced Mixed Distillation Method for Non-Autoregressive Generation", "abstract": "Non-Autoregressive generation is a sequence generation paradigm, which\nremoves the dependency between target tokens. It could efficiently reduce the\ntext generation latency with parallel decoding in place of token-by-token\nsequential decoding. However, due to the known multi-modality problem,\nNon-Autoregressive (NAR) models significantly under-perform Auto-regressive\n(AR) models on various language generation tasks. Among the NAR models, BANG is\nthe first large-scale pre-training model on English un-labeled raw text corpus.\nIt considers different generation paradigms as its pre-training tasks including\nAuto-regressive (AR), Non-Autoregressive (NAR), and semi-Non-Autoregressive\n(semi-NAR) information flow with multi-stream strategy. It achieves\nstate-of-the-art performance without any distillation techniques. However, AR\ndistillation has been shown to be a very effective solution for improving NAR\nperformance. In this paper, we propose a novel self-paced mixed distillation\nmethod to further improve the generation quality of BANG. Firstly, we propose\nthe mixed distillation strategy based on the AR stream knowledge. Secondly, we\nencourage the model to focus on the samples with the same modality by\nself-paced learning. The proposed self-paced mixed distillation algorithm\nimproves the generation quality and has no influence on the inference latency.\nWe carry out extensive experiments on summarization and question generation\ntasks to validate the effectiveness. To further illustrate the commercial value\nof our approach, we conduct experiments on three generation tasks in real-world\nadvertisements applications. Experimental results on commercial data show the\neffectiveness of the proposed model. Compared with BANG, it achieves\nsignificant BLEU score improvement. On the other hand, compared with\nauto-regressive generation method, it achieves more than 7x speedup.", "published": "2022-05-23 09:54:53", "link": "http://arxiv.org/abs/2205.11162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Template-based Method for Constrained Neural Machine Translation", "abstract": "Machine translation systems are expected to cope with various types of\nconstraints in many practical scenarios. While neural machine translation (NMT)\nhas achieved strong performance in unconstrained cases, it is non-trivial to\nimpose pre-specified constraints into the translation process of NMT models.\nAlthough many approaches have been proposed to address this issue, most\nexisting methods can not satisfy the following three desiderata at the same\ntime: (1) high translation quality, (2) high match accuracy, and (3) low\nlatency. In this work, we propose a template-based method that can yield\nresults with high translation quality and match accuracy and the inference\nspeed of our method is comparable with unconstrained NMT models. Our basic idea\nis to rearrange the generation of constrained and unconstrained tokens through\na template. Our method does not require any changes in the model architecture\nand the decoding algorithm. Experimental results show that the proposed\ntemplate-based approach can outperform several representative baselines in both\nlexically and structurally constrained translation tasks.", "published": "2022-05-23 12:24:34", "link": "http://arxiv.org/abs/2205.11255v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When does Parameter-Efficient Transfer Learning Work for Machine\n  Translation?", "abstract": "Parameter-efficient fine-tuning methods (PEFTs) offer the promise of adapting\nlarge pre-trained models while only tuning a small number of parameters. They\nhave been shown to be competitive with full model fine-tuning for many\ndownstream tasks. However, prior work indicates that PEFTs may not work as well\nfor machine translation (MT), and there is no comprehensive study showing when\nPEFTs work for MT. We conduct a comprehensive empirical study of PEFTs for MT,\nconsidering (1) various parameter budgets, (2) a diverse set of language-pairs,\nand (3) different pre-trained models. We find that 'adapters', in which small\nfeed-forward networks are added after every layer, are indeed on par with full\nmodel fine-tuning when the parameter budget corresponds to 10% of total model\nparameters. Nevertheless, as the number of tuned parameters decreases, the\nperformance of PEFTs decreases. The magnitude of this decrease depends on the\nlanguage pair, with PEFTs particularly struggling for distantly related\nlanguage-pairs. We find that using PEFTs with a larger pre-trained model\noutperforms full fine-tuning with a smaller model, and for smaller training\ndata sizes, PEFTs outperform full fine-tuning for the same pre-trained model.", "published": "2022-05-23 12:49:46", "link": "http://arxiv.org/abs/2205.11277v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sample Efficient Approaches for Idiomaticity Detection", "abstract": "Deep neural models, in particular Transformer-based pre-trained language\nmodels, require a significant amount of data to train. This need for data tends\nto lead to problems when dealing with idiomatic multiword expressions (MWEs),\nwhich are inherently less frequent in natural text. As such, this work explores\nsample efficient methods of idiomaticity detection. In particular we study the\nimpact of Pattern Exploit Training (PET), a few-shot method of classification,\nand BERTRAM, an efficient method of creating contextual embeddings, on the task\nof idiomaticity detection. In addition, to further explore generalisability, we\nfocus on the identification of MWEs not present in the training data. Our\nexperiments show that while these methods improve performance on English, they\nare much less effective on Portuguese and Galician, leading to an overall\nperformance about on par with vanilla mBERT. Regardless, we believe sample\nefficient methods for both identifying and representing potentially idiomatic\nMWEs are very encouraging and hold significant potential for future\nexploration.", "published": "2022-05-23 13:46:35", "link": "http://arxiv.org/abs/2205.11306v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Symptom Identification for Interpretable Detection of Multiple Mental\n  Disorders", "abstract": "Mental disease detection (MDD) from social media has suffered from poor\ngeneralizability and interpretability, due to lack of symptom modeling. This\npaper introduces PsySym, the first annotated symptom identification corpus of\nmultiple psychiatric disorders, to facilitate further research progress. PsySym\nis annotated according to a knowledge graph of the 38 symptom classes related\nto 7 mental diseases complied from established clinical manuals and scales, and\na novel annotation framework for diversity and quality. Experiments show that\nsymptom-assisted MDD enabled by PsySym can outperform strong pure-text\nbaselines. We also exhibit the convincing MDD explanations provided by symptom\npredictions with case studies, and point to their further potential\napplications.", "published": "2022-05-23 13:51:48", "link": "http://arxiv.org/abs/2205.11308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Use of Transformer-Based Models for Word-Level Transliteration of the\n  Book of the Dean of Lismore", "abstract": "The Book of the Dean of Lismore (BDL) is a 16th-century Scottish Gaelic\nmanuscript written in a non-standard orthography. In this work, we outline the\nproblem of transliterating the text of the BDL into a standardised orthography,\nand perform exploratory experiments using Transformer-based models for this\ntask. In particular, we focus on the task of word-level transliteration, and\nachieve a character-level BLEU score of 54.15 with our best model, a BART\narchitecture pre-trained on the text of Scottish Gaelic Wikipedia and then\nfine-tuned on around 2,000 word-level parallel examples. Our initial\nexperiments give promising results, but we highlight the shortcomings of our\nmodel, and discuss directions for future work.", "published": "2022-05-23 15:04:26", "link": "http://arxiv.org/abs/2205.11370v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Many-Class Text Classification with Matching", "abstract": "In this work, we formulate \\textbf{T}ext \\textbf{C}lassification as a\n\\textbf{M}atching problem between the text and the labels, and propose a simple\nyet effective framework named TCM. Compared with previous text classification\napproaches, TCM takes advantage of the fine-grained semantic information of the\nclassification labels, which helps distinguish each class better when the class\nnumber is large, especially in low-resource scenarios. TCM is also easy to\nimplement and is compatible with various large pretrained language models. We\nevaluate TCM on 4 text classification datasets (each with 20+ labels) in both\nfew-shot and full-data settings, and this model demonstrates significant\nimprovements over other text classification paradigms. We also conduct\nextensive experiments with different variants of TCM and discuss the underlying\nfactors of its success. Our method and analyses offer a new perspective on text\nclassification.", "published": "2022-05-23 15:51:19", "link": "http://arxiv.org/abs/2205.11409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QASem Parsing: Text-to-text Modeling of QA-based Semantics", "abstract": "Several recent works have suggested to represent semantic relations with\nquestions and answers, decomposing textual information into separate\ninterrogative natural language statements. In this paper, we consider three\nQA-based semantic tasks - namely, QA-SRL, QANom and QADiscourse, each targeting\na certain type of predication - and propose to regard them as jointly providing\na comprehensive representation of textual information. To promote this goal, we\ninvestigate how to best utilize the power of sequence-to-sequence (seq2seq)\npre-trained language models, within the unique setup of semi-structured\noutputs, consisting of an unordered set of question-answer pairs. We examine\ndifferent input and output linearization strategies, and assess the effect of\nmultitask learning and of simple data augmentation techniques in the setting of\nimbalanced training data. Consequently, we release the first unified QASem\nparsing tool, practical for downstream applications who can benefit from an\nexplicit, QA-based account of information units in a text.", "published": "2022-05-23 15:56:07", "link": "http://arxiv.org/abs/2205.11413v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Importance of Being Parameters: An Intra-Distillation Method for\n  Serious Gains", "abstract": "Recent model pruning methods have demonstrated the ability to remove\nredundant parameters without sacrificing model performance. Common methods\nremove redundant parameters according to the parameter sensitivity, a\ngradient-based measure reflecting the contribution of the parameters. In this\npaper, however, we argue that redundant parameters can be trained to make\nbeneficial contributions. We first highlight the large sensitivity\n(contribution) gap among high-sensitivity and low-sensitivity parameters and\nshow that the model generalization performance can be significantly improved\nafter balancing the contribution of all parameters. Our goal is to balance the\nsensitivity of all parameters and encourage all of them to contribute equally.\nWe propose a general task-agnostic method, namely intra-distillation, appended\nto the regular training loss to balance parameter sensitivity. Moreover, we\nalso design a novel adaptive learning method to control the strength of\nintra-distillation loss for faster convergence. Our experiments show the strong\neffectiveness of our methods on machine translation, natural language\nunderstanding, and zero-shot cross-lingual transfer across up to 48 languages,\ne.g., a gain of 3.54 BLEU on average across 8 language pairs from the IWSLT'14\ntranslation dataset.", "published": "2022-05-23 16:01:46", "link": "http://arxiv.org/abs/2205.11416v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Representation Learning for Cross-Document Coreference\n  Resolution of Events and Entities", "abstract": "Identifying related entities and events within and across documents is\nfundamental to natural language understanding. We present an approach to entity\nand event coreference resolution utilizing contrastive representation learning.\nEarlier state-of-the-art methods have formulated this problem as a binary\nclassification problem and leveraged large transformers in a cross-encoder\narchitecture to achieve their results. For large collections of documents and\ncorresponding set of $n$ mentions, the necessity of performing $n^{2}$\ntransformer computations in these earlier approaches can be computationally\nintensive. We show that it is possible to reduce this burden by applying\ncontrastive learning techniques that only require $n$ transformer computations\nat inference time. Our method achieves state-of-the-art results on a number of\nkey metrics on the ECB+ corpus and is competitive on others.", "published": "2022-05-23 16:30:20", "link": "http://arxiv.org/abs/2205.11438v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Extraction and Categorization of Lexical Collocations with\n  Graph-aware Transformers", "abstract": "Recognizing and categorizing lexical collocations in context is useful for\nlanguage learning, dictionary compilation and downstream NLP. However, it is a\nchallenging task due to the varying degrees of frozenness lexical collocations\nexhibit. In this paper, we put forward a sequence tagging BERT-based model\nenhanced with a graph-aware transformer architecture, which we evaluate on the\ntask of collocation recognition in context. Our results suggest that explicitly\nencoding syntactic dependencies in the model architecture is helpful, and\nprovide insights on differences in collocation typification in English, Spanish\nand French.", "published": "2022-05-23 16:47:37", "link": "http://arxiv.org/abs/2205.11456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context Limitations Make Neural Language Models More Human-Like", "abstract": "Language models (LMs) have been used in cognitive modeling as well as\nengineering studies -- they compute information-theoretic complexity metrics\nthat simulate humans' cognitive load during reading. This study highlights a\nlimitation of modern neural LMs as the model of choice for this purpose: there\nis a discrepancy between their context access capacities and that of humans.\nOur results showed that constraining the LMs' context access improved their\nsimulation of human reading behavior. We also showed that LM-human gaps in\ncontext access were associated with specific syntactic constructions;\nincorporating syntactic biases into LMs' context access might enhance their\ncognitive plausibility.", "published": "2022-05-23 17:01:13", "link": "http://arxiv.org/abs/2205.11463v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Question-Answer Driven Approach to Reveal Affirmative Interpretations\n  from Verbal Negations", "abstract": "This paper explores a question-answer driven approach to reveal affirmative\ninterpretations from verbal negations (i.e., when a negation cue grammatically\nmodifies a verb). We create a new corpus consisting of 4,472 verbal negations\nand discover that 67.1% of them convey that an event actually occurred.\nAnnotators generate and answer 7,277 questions for the 3,001 negations that\nconvey an affirmative interpretation. We first cast the problem of revealing\naffirmative interpretations from negations as a natural language inference\n(NLI) classification task. Experimental results show that state-of-the-art\ntransformers trained with existing NLI corpora are insufficient to reveal\naffirmative interpretations. We also observe, however, that fine-tuning brings\nsmall improvements. In addition to NLI classification, we also explore the more\nrealistic task of generating affirmative interpretations directly from\nnegations with the T5 transformer. We conclude that the generation task remains\na challenge as T5 substantially underperforms humans.", "published": "2022-05-23 17:08:30", "link": "http://arxiv.org/abs/2205.11467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diversity Over Size: On the Effect of Sample and Topic Sizes for\n  Topic-Dependent Argument Mining Datasets", "abstract": "The task of Argument Mining, that is extracting and classifying argument\ncomponents for a specific topic from large document sources, is an inherently\ndifficult task for machine learning models and humans alike, as large Argument\nMining datasets are rare and recognition of argument components requires expert\nknowledge. The task becomes even more difficult if it also involves stance\ndetection of retrieved arguments. In this work, we investigate the effect of\nArgument Mining dataset composition in few- and zero-shot settings. Our\nfindings show that, while fine-tuning is mandatory to achieve acceptable model\nperformance, using carefully composed training samples and reducing the\ntraining sample size by up to almost 90% can still yield 95% of the maximum\nperformance. This gain is consistent across three Argument Mining tasks on\nthree different datasets. We also publish a new dataset for future\nbenchmarking.", "published": "2022-05-23 17:14:32", "link": "http://arxiv.org/abs/2205.11472v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Automated Document Revision: Grammatical Error Correction,\n  Fluency Edits, and Beyond", "abstract": "Natural language processing technology has rapidly improved automated\ngrammatical error correction tasks, and the community begins to explore\ndocument-level revision as one of the next challenges. To go beyond\nsentence-level automated grammatical error correction to NLP-based\ndocument-level revision assistant, there are two major obstacles: (1) there are\nfew public corpora with document-level revisions being annotated by\nprofessional editors, and (2) it is not feasible to elicit all possible\nreferences and evaluate the quality of revision with such references because\nthere are infinite possibilities of revision. This paper tackles these\nchallenges. First, we introduce a new document-revision corpus, TETRA, where\nprofessional editors revised academic papers sampled from the ACL anthology\nwhich contain few trivial grammatical errors that enable us to focus more on\ndocument- and paragraph-level edits such as coherence and consistency. Second,\nwe explore reference-less and interpretable methods for meta-evaluation that\ncan detect quality improvements by document revision. We show the uniqueness of\nTETRA compared with existing document revision corpora and demonstrate that a\nfine-tuned pre-trained language model can discriminate the quality of documents\nafter revision even when the difference is subtle. This promising result will\nencourage the community to further explore automated document revision models\nand metrics in future.", "published": "2022-05-23 17:37:20", "link": "http://arxiv.org/abs/2205.11484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual\n  Style Transfer with Small Language Models", "abstract": "We propose a method for arbitrary textual style transfer (TST)--the task of\ntransforming a text into any given style--utilizing general-purpose pre-trained\nlanguage models. Our method, Prompt-and-Rerank, is based on a mathematical\nformulation of the TST task, decomposing it into three constituent components:\ntextual similarity, target style strength, and fluency. Specifically, our\nmethod first uses zero-shot or few-shot prompting to obtain a set of candidate\ngenerations in the target style, and then re-ranks these candidates according\nto a combination of the three components above. Empirically, our method enables\nsmall pre-trained language models to perform on par with state-of-the-art\nlarge-scale models while consuming two orders of magnitude less compute and\nmemory. Finally, we conduct a systematic investigation of the effect of model\nsize and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on\nstyle transfer quality across seven diverse textual style transfer datasets.", "published": "2022-05-23 17:57:15", "link": "http://arxiv.org/abs/2205.11503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeded Hierarchical Clustering for Expert-Crafted Taxonomies", "abstract": "Practitioners from many disciplines (e.g., political science) use\nexpert-crafted taxonomies to make sense of large, unlabeled corpora. In this\nwork, we study Seeded Hierarchical Clustering (SHC): the task of automatically\nfitting unlabeled data to such taxonomies using only a small set of labeled\nexamples. We propose HierSeed, a novel weakly supervised algorithm for this\ntask that uses only a small set of labeled seed examples. It is both data and\ncomputationally efficient. HierSeed assigns documents to topics by weighing\ndocument density against topic hierarchical structure. It outperforms both\nunsupervised and supervised baselines for the SHC task on three real-world\ndatasets.", "published": "2022-05-23 19:58:06", "link": "http://arxiv.org/abs/2205.11602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representation Projection Invariance Mitigates Representation Collapse", "abstract": "Fine-tuning contextualized representations learned by pre-trained language\nmodels remains a prevalent practice in NLP. However, fine-tuning can lead to\nrepresentation degradation (also known as representation collapse), which may\nresult in instability, sub-optimal performance, and weak generalization.\n  In this paper, we propose Representation Projection Invariance (REPINA), a\nnovel regularization method to maintain the information content of\nrepresentation and reduce representation collapse during fine-tuning by\ndiscouraging undesirable changes in the representations. We study the empirical\nbehavior of the proposed regularization in comparison to 5 comparable baselines\nacross 13 language understanding tasks (GLUE benchmark and six additional\ndatasets). When evaluating in-domain performance, REPINA consistently\noutperforms other baselines on most tasks (10 out of 13). We also demonstrate\nits effectiveness in few-shot settings and robustness to label perturbation. As\na by-product, we extend previous studies of representation collapse and propose\nseveral metrics to quantify it. Our empirical findings show that our approach\nis significantly more effective at mitigating representation collapse.", "published": "2022-05-23 20:00:22", "link": "http://arxiv.org/abs/2205.11603v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Opening the Black Box of Neural Machine Translation: Source and\n  Target Interpretations of the Transformer", "abstract": "In Neural Machine Translation (NMT), each token prediction is conditioned on\nthe source sentence and the target prefix (what has been previously translated\nat a decoding step). However, previous work on interpretability in NMT has\nmainly focused solely on source sentence tokens' attributions. Therefore, we\nlack a full understanding of the influences of every input token (source\nsentence and target prefix) in the model predictions. In this work, we propose\nan interpretability method that tracks input tokens' attributions for both\ncontexts. Our method, which can be extended to any encoder-decoder\nTransformer-based model, allows us to better comprehend the inner workings of\ncurrent NMT models. We apply the proposed method to both bilingual and\nmultilingual Transformers and present insights into their behaviour.", "published": "2022-05-23 20:59:14", "link": "http://arxiv.org/abs/2205.11631v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Penguins Don't Fly: Reasoning about Generics through Instantiations and\n  Exceptions", "abstract": "Generics express generalizations about the world (e.g., birds can fly) that\nare not universally true (e.g., newborn birds and penguins cannot fly).\nCommonsense knowledge bases, used extensively in NLP, encode some generic\nknowledge but rarely enumerate such exceptions and knowing when a generic\nstatement holds or does not hold true is crucial for developing a comprehensive\nunderstanding of generics. We present a novel framework informed by linguistic\ntheory to generate exemplars -- specific cases when a generic holds true or\nfalse. We generate ~19k exemplars for ~650 generics and show that our framework\noutperforms a strong GPT-3 baseline by 12.8 precision points. Our analysis\nhighlights the importance of linguistic theory-based controllability for\ngenerating exemplars, the insufficiency of knowledge bases as a source of\nexemplars, and the challenges exemplars pose for the task of natural language\ninference.", "published": "2022-05-23 22:45:53", "link": "http://arxiv.org/abs/2205.11658v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Subgraph Explorer: Reducing Noisy Information via Target-Oriented\n  Syntax Graph Pruning", "abstract": "Recent years have witnessed the emerging success of leveraging syntax graphs\nfor the target sentiment classification task. However, we discover that\nexisting syntax-based models suffer from two issues: noisy information\naggregation and loss of distant correlations. In this paper, we propose a novel\nmodel termed Neural Subgraph Explorer, which (1) reduces the noisy information\nvia pruning target-irrelevant nodes on the syntax graph; (2) introduces\nbeneficial first-order connections between the target and its related words\ninto the obtained graph. Specifically, we design a multi-hop actions score\nestimator to evaluate the value of each word regarding the specific target. The\ndiscrete action sequence is sampled through Gumble-Softmax and then used for\nboth of the syntax graph and the self-attention graph. To introduce the\nfirst-order connections between the target and its relevant words, the two\npruned graphs are merged. Finally, graph convolution is conducted on the\nobtained unified graph to update the hidden states. And this process is stacked\nwith multiple layers. To our knowledge, this is the first attempt of\ntarget-oriented syntax graph pruning in this task. Experimental results\ndemonstrate the superiority of our model, which achieves new state-of-the-art\nperformance.", "published": "2022-05-23 00:29:32", "link": "http://arxiv.org/abs/2205.10970v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What should I Ask: A Knowledge-driven Approach for Follow-up Questions\n  Generation in Conversational Surveys", "abstract": "Generating follow-up questions on the fly could significantly improve\nconversational survey quality and user experiences by enabling a more dynamic\nand personalized survey structure. In this paper, we proposed a novel task for\nknowledge-driven follow-up question generation in conversational surveys. We\nconstructed a new human-annotated dataset of human-written follow-up questions\nwith dialogue history and labeled knowledge in the context of conversational\nsurveys. Along with the dataset, we designed and validated a set of\nreference-free Gricean-inspired evaluation metrics to systematically evaluate\nthe quality of generated follow-up questions. We then propose a two-staged\nknowledge-driven model for the task, which generates informative and coherent\nfollow-up questions by using knowledge to steer the generation process. The\nexperiments demonstrate that compared to GPT-based baseline models, our\ntwo-staged model generates more informative, coherent, and clear follow-up\nquestions.", "published": "2022-05-23 00:57:33", "link": "http://arxiv.org/abs/2205.10977v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "AdaptivePaste: Code Adaptation through Learning Semantics-aware Variable\n  Usage Representations", "abstract": "In software development, it is common for programmers to copy-paste or port\ncode snippets and then adapt them to their use case. This scenario motivates\nthe code adaptation task -- a variant of program repair which aims to adapt\nvariable identifiers in a pasted snippet of code to the surrounding,\npreexisting source code. However, no existing approach has been shown to\neffectively address this task. In this paper, we introduce AdaptivePaste, a\nlearning-based approach to source code adaptation, based on transformers and a\ndedicated dataflow-aware deobfuscation pre-training task to learn meaningful\nrepresentations of variable usage patterns. We evaluate AdaptivePaste on a\ndataset of code snippets in Python. Results suggest that our model can learn to\nadapt source code with 79.8% accuracy. To evaluate how valuable is\nAdaptivePaste in practice, we perform a user study with 10 Python developers on\na hundred real-world copy-paste instances. The results show that AdaptivePaste\nreduces the dwell time to nearly half the time it takes for manual code\nadaptation, and helps to avoid bugs. In addition, we utilize the participant\nfeedback to identify potential avenues for improvement of AdaptivePaste.", "published": "2022-05-23 03:49:48", "link": "http://arxiv.org/abs/2205.11023v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI", "abstract": "Task-oriented dialogue (TOD) systems have been widely used by mobile phone\nintelligent assistants to accomplish tasks such as calendar scheduling or hotel\nreservation. Current TOD systems usually focus on multi-turn text/speech\ninteraction, then they would call back-end APIs designed for TODs to perform\nthe task. However, this API-based architecture greatly limits the\ninformation-searching capability of intelligent assistants and may even lead to\ntask failure if TOD-specific APIs are not available or the task is too\ncomplicated to be executed by the provided APIs. In this paper, we propose a\nnew TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A\nGUI-TOD system can directly perform GUI operations on real APPs and execute\ntasks without invoking TOD-specific backend APIs. Furthermore, we release\nMETA-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile\nGUI. We also propose a multi-model action prediction and response model, which\nshow promising results on META-GUI. The dataset, codes and leaderboard are\npublicly available.", "published": "2022-05-23 04:05:37", "link": "http://arxiv.org/abs/2205.11029v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TempLM: Distilling Language Models into Template-Based Generators", "abstract": "While pretrained language models (PLMs) have greatly improved text\ngeneration, they have also been known to produce unfaithful or inappropriate\ncontent. In contrast, classic template-based systems provide strong guarantees\nof faithfulness at the cost of fluency. We propose TempLM, which achieves the\nbest of both worlds by distilling a PLM into a template-based generator. On the\nE2E and SynthBio data-to-text datasets, we show that TempLM is more faithful\nthan the original PLM and is more fluent than prior template systems. Notably,\non an out-of-domain evaluation, TempLM reduces a finetuned BART model's\nunfaithfulness rate from 83% to 0%. In a human study, we find that TempLM's\ntemplates substantially improve upon human-written ones in BERTScore.", "published": "2022-05-23 05:46:59", "link": "http://arxiv.org/abs/2205.11055v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DistilCamemBERT: a distillation of the French model CamemBERT", "abstract": "Modern Natural Language Processing (NLP) models based on Transformer\nstructures represent the state of the art in terms of performance on very\ndiverse tasks. However, these models are complex and represent several hundred\nmillion parameters for the smallest of them. This may hinder their adoption at\nthe industrial level, making it difficult to scale up to a reasonable\ninfrastructure and/or to comply with societal and environmental\nresponsibilities. To this end, we present in this paper a model that\ndrastically reduces the computational cost of a well-known French model\n(CamemBERT), while preserving good performance.", "published": "2022-05-23 08:04:58", "link": "http://arxiv.org/abs/2205.11111v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Summarize and Generate to Back-translate: Unsupervised Translation of\n  Programming Languages", "abstract": "Back-translation is widely known for its effectiveness in neural machine\ntranslation when there is little to no parallel data. In this approach, a\nsource-to-target model is coupled with a target-to-source model trained in\nparallel. The target-to-source model generates noisy sources, while the\nsource-to-target model is trained to reconstruct the targets and vice versa.\nRecent developments of multilingual pre-trained sequence-to-sequence models for\nprogramming languages have been very effective for a broad spectrum of\ndownstream software engineering tasks. Hence, training them to build\nprogramming language translation systems via back-translation is compelling.\nHowever, these models cannot be further trained via back-translation since they\nlearn to output sequences in the same language as the inputs during\npre-training. As an alternative, we propose performing back-translation via\ncode summarization and generation. In code summarization, a model learns to\ngenerate natural language (NL) summaries given code snippets. In code\ngeneration, the model learns to do the opposite. Therefore, target-to-source\ngeneration in back-translation can be viewed as a target-to-NL-to-source\ngeneration. We show that our proposed approach performs competitively with\nstate-of-the-art methods. We have made the code publicly available.", "published": "2022-05-23 08:20:41", "link": "http://arxiv.org/abs/2205.11116v2", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Prompt Tuning for Discriminative Pre-trained Language Models", "abstract": "Recent works have shown promising results of prompt tuning in stimulating\npre-trained language models (PLMs) for natural language processing (NLP) tasks.\nHowever, to the best of our knowledge, existing works focus on prompt-tuning\ngenerative PLMs that are pre-trained to generate target tokens, such as BERT.\nIt is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be\neffectively prompt-tuned. In this work, we present DPT, the first prompt tuning\nframework for discriminative PLMs, which reformulates NLP tasks into a\ndiscriminative language modeling problem. Comprehensive experiments on text\nclassification and question answering show that, compared with vanilla\nfine-tuning, DPT achieves significantly higher performance, and also prevents\nthe unstable problem in tuning large PLMs in both full-set and low-resource\nsettings. The source code and experiment details of this paper can be obtained\nfrom https://github.com/thunlp/DPT.", "published": "2022-05-23 10:11:50", "link": "http://arxiv.org/abs/2205.11166v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UnifieR: A Unified Retriever for Large-Scale Retrieval", "abstract": "Large-scale retrieval is to recall relevant documents from a huge collection\ngiven a query. It relies on representation learning to embed documents and\nqueries into a common semantic encoding space. According to the encoding space,\nrecent retrieval methods based on pre-trained language models (PLM) can be\ncoarsely categorized into either dense-vector or lexicon-based paradigms. These\ntwo paradigms unveil the PLMs' representation capability in different\ngranularities, i.e., global sequence-level compression and local word-level\ncontexts, respectively. Inspired by their complementary global-local\ncontextualization and distinct representing views, we propose a new learning\nframework, UnifieR which unifies dense-vector and lexicon-based retrieval in\none model with a dual-representing capability. Experiments on passage retrieval\nbenchmarks verify its effectiveness in both paradigms. A uni-retrieval scheme\nis further presented with even better retrieval quality. We lastly evaluate the\nmodel on BEIR benchmark to verify its transferability.", "published": "2022-05-23 11:01:59", "link": "http://arxiv.org/abs/2205.11194v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "BBTv2: Towards a Gradient-Free Future with Large Language Models", "abstract": "Most downstream adaptation methods tune all or part of the parameters of\npre-trained models (PTMs) through gradient descent, where the tuning cost\nincreases linearly with the growth of the model size. By contrast,\ngradient-free methods only require the forward computation of the PTM to tune\nthe prompt, retaining the benefits of efficient tuning and deployment. Though,\npast work on gradient-free tuning often introduces gradient descent to seek a\ngood initialization of prompt and lacks versatility across tasks and PTMs. In\nthis paper, we present BBTv2, an improved version of Black-Box Tuning, to drive\nPTMs for few-shot learning. We prepend continuous prompts to every layer of the\nPTM and propose a divide-and-conquer gradient-free algorithm to optimize the\nprompts at different layers alternately. Extensive experiments across various\ntasks and PTMs show that BBTv2 can achieve comparable performance to full model\ntuning and state-of-the-art parameter-efficient methods (e.g., Adapter, LoRA,\nBitFit, etc.) under few-shot settings while maintaining much fewer tunable\nparameters.", "published": "2022-05-23 11:10:19", "link": "http://arxiv.org/abs/2205.11200v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stop Filtering: Multi-View Attribute-Enhanced Dialogue Learning", "abstract": "There is a growing interest in improving the conversational ability of models\nby filtering the raw dialogue corpora. Previous filtering strategies usually\nrely on a scoring method to assess and discard samples from one perspective,\nenabling the model to enhance the corresponding dialogue attributes (e.g.,\nconsistency) more easily. However, the discarded samples may obtain high scores\nin other perspectives and can provide regularization effects on the model\nlearning, which causes the performance improvement to be sensitive to the\nfiltering ratio. In this work, we propose a multi-view attribute-enhanced\ndialogue learning framework that strengthens the attribute-related features\nmore robustly and comprehensively. Instead of filtering the raw dataset to\ntrain the model, our framework first pre-trains the model on the raw dataset\nand then fine-tunes it through adapters on the selected sub-sets, which also\nenhances certain attributes of responses but without suffering from the\nproblems mentioned above. Considering the variety of the dialogue attribute, we\nfurther design a multi-view enhancement mechanism, including multi-view\nselection and inter-view fusion. It groups the high-quality samples from\nmultiple perspectives, respectively, and enhances different attributes of\nresponses with the corresponding sample sets and adapters, keeping knowledge\nindependent and allowing flexible integration. Empirical results and analysis\nshow that our framework can improve the performance significantly in terms of\nenhancing dialogue attributes and fusing view-specific knowledge.", "published": "2022-05-23 11:28:36", "link": "http://arxiv.org/abs/2205.11206v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Non-Parametric Domain Adaptation for End-to-End Speech Translation", "abstract": "End-to-End Speech Translation (E2E-ST) has received increasing attention due\nto the potential of its less error propagation, lower latency, and fewer\nparameters. However, the effectiveness of neural-based approaches to this task\nis severely limited by the available training corpus, especially for domain\nadaptation where in-domain triplet training data is scarce or nonexistent. In\nthis paper, we propose a novel non-parametric method that leverages\ndomain-specific text translation corpus to achieve domain adaptation for the\nE2E-ST system. To this end, we first incorporate an additional encoder into the\npre-trained E2E-ST model to realize text translation modelling, and then unify\nthe decoder's output representation for text and speech translation tasks by\nreducing the correspondent representation mismatch in available triplet\ntraining data. During domain adaptation, a k-nearest-neighbor (kNN) classifier\nis introduced to produce the final translation distribution using the external\ndatastore built by the domain-specific text translation corpus, while the\nuniversal output representation is adopted to perform a similarity search.\nExperiments on the Europarl-ST benchmark demonstrate that when in-domain text\ntranslation data is involved only, our proposed approach significantly improves\nbaseline by 12.82 BLEU on average in all translation directions, even\noutperforming the strong in-domain fine-tuning method.", "published": "2022-05-23 11:41:02", "link": "http://arxiv.org/abs/2205.11211v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KOLD: Korean Offensive Language Dataset", "abstract": "Recent directions for offensive language detection are hierarchical modeling,\nidentifying the type and the target of offensive language, and interpretability\nwith offensive span annotation and prediction. These improvements are focused\non English and do not transfer well to other languages because of cultural and\nlinguistic differences. In this paper, we present the Korean Offensive Language\nDataset (KOLD) comprising 40,429 comments, which are annotated hierarchically\nwith the type and the target of offensive language, accompanied by annotations\nof the corresponding text spans. We collect the comments from NAVER news and\nYouTube platform and provide the titles of the articles and videos as the\ncontext information for the annotation process. We use these annotated comments\nas training data for Korean BERT and RoBERTa models and find that they are\neffective at offensiveness detection, target classification, and target span\ndetection while having room for improvement for target group classification and\noffensive span detection. We discover that the target group distribution\ndiffers drastically from the existing English datasets, and observe that\nproviding the context information improves the model performance in\noffensiveness detection (+0.3), target classification (+1.5), and target group\nclassification (+13.1). We publicly release the dataset and baseline models.", "published": "2022-05-23 13:58:45", "link": "http://arxiv.org/abs/2205.11315v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Diminishing Returns of Masked Language Models to Science", "abstract": "Transformer-based masked language models such as BERT, trained on general\ncorpora, have shown impressive performance on downstream tasks. It has also\nbeen demonstrated that the downstream task performance of such models can be\nimproved by pretraining larger models for longer on more data. In this work, we\nempirically evaluate the extent to which these results extend to tasks in\nscience. We use 14 domain-specific transformer-based models (including\nScholarBERT, a new 770M-parameter science-focused masked language model\npretrained on up to 225B tokens) to evaluate the impact of training data, model\nsize, pretraining and finetuning time on 12 downstream scientific tasks.\nInterestingly, we find that increasing model sizes, training data, or compute\ntime does not always lead to significant improvements (i.e., >1% F1), if at\nall, in scientific information extraction tasks and offered possible\nexplanations for the surprising performance differences.", "published": "2022-05-23 14:35:08", "link": "http://arxiv.org/abs/2205.11342v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements", "abstract": "The growing capability and availability of generative language models has\nenabled a wide range of new downstream tasks. Academic research has identified,\nquantified and mitigated biases present in language models but is rarely\ntailored to downstream tasks where wider impact on individuals and society can\nbe felt. In this work, we leverage one popular generative language model,\nGPT-3, with the goal of writing unbiased and realistic job advertisements. We\nfirst assess the bias and realism of zero-shot generated advertisements and\ncompare them to real-world advertisements. We then evaluate prompt-engineering\nand fine-tuning as debiasing methods. We find that prompt-engineering with\ndiversity-encouraging prompts gives no significant improvement to bias, nor\nrealism. Conversely, fine-tuning, especially on unbiased real advertisements,\ncan improve realism and reduce bias.", "published": "2022-05-23 15:05:27", "link": "http://arxiv.org/abs/2205.11374v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Outliers Dimensions that Disrupt Transformers Are Driven by Frequency", "abstract": "While Transformer-based language models are generally very robust to pruning,\nthere is the recently discovered outlier phenomenon: disabling only 48 out of\n110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We\nreplicate the original evidence for the outlier phenomenon and we link it to\nthe geometry of the embedding space. We find that in both BERT and RoBERTa the\nmagnitude of hidden state coefficients corresponding to outlier dimensions\ncorrelates with the frequency of encoded tokens in pre-training data, and it\nalso contributes to the \"vertical\" self-attention pattern enabling the model to\nfocus on the special tokens. This explains the drop in performance from\ndisabling the outliers, and it suggests that to decrease anisotropicity in\nfuture models we need pre-training schemas that would better take into account\nthe skewed token distributions.", "published": "2022-05-23 15:19:09", "link": "http://arxiv.org/abs/2205.11380v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in\n  Question Answering Models", "abstract": "Knowledge and language understanding of models evaluated through question\nanswering (QA) has been usually studied on static snapshots of knowledge, like\nWikipedia. However, our world is dynamic, evolves over time, and our models'\nknowledge becomes outdated. To study how semi-parametric QA models and their\nunderlying parametric language models (LMs) adapt to evolving knowledge, we\nconstruct a new large-scale dataset, StreamingQA, with human written and\ngenerated questions asked on a given date, to be answered from 14 years of\ntime-stamped news articles. We evaluate our models quarterly as they read new\narticles not seen in pre-training. We show that parametric models can be\nupdated without full retraining, while avoiding catastrophic forgetting. For\nsemi-parametric models, adding new articles into the search space allows for\nrapid adaptation, however, models with an outdated underlying LM under-perform\nthose with a retrained LM. For questions about higher-frequency named entities,\nparametric updates are particularly beneficial. In our dynamic world, the\nStreamingQA dataset enables a more realistic evaluation of QA models, and our\nexperiments highlight several promising directions for future research.", "published": "2022-05-23 15:33:41", "link": "http://arxiv.org/abs/2205.11388v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Logical Reasoning with Span-Level Predictions for Interpretable and\n  Robust NLI Models", "abstract": "Current Natural Language Inference (NLI) models achieve impressive results,\nsometimes outperforming humans when evaluating on in-distribution test sets.\nHowever, as these models are known to learn from annotation artefacts and\ndataset biases, it is unclear to what extent the models are learning the task\nof NLI instead of learning from shallow heuristics in their training data. We\naddress this issue by introducing a logical reasoning framework for NLI,\ncreating highly transparent model decisions that are based on logical rules.\nUnlike prior work, we show that improved interpretability can be achieved\nwithout decreasing the predictive accuracy. We almost fully retain performance\non SNLI, while also identifying the exact hypothesis spans that are responsible\nfor each model prediction. Using the e-SNLI human explanations, we verify that\nour model makes sensible decisions at a span level, despite not using any span\nlabels during training. We can further improve model performance and span-level\ndecisions by using the e-SNLI explanations during training. Finally, our model\nis more robust in a reduced data setting. When training with only 1,000\nexamples, out-of-distribution performance improves on the MNLI matched and\nmismatched validation sets by 13% and 16% relative to the baseline. Training\nwith fewer observations yields further improvements, both in-distribution and\nout-of-distribution.", "published": "2022-05-23 16:24:27", "link": "http://arxiv.org/abs/2205.11432v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SQuALITY: Building a Long-Document Summarization Dataset the Hard Way", "abstract": "Summarization datasets are often assembled either by scraping naturally\noccurring public-domain summaries -- which are nearly always in\ndifficult-to-work-with technical domains -- or by using approximate heuristics\nto extract them from everyday text -- which frequently yields unfaithful\nsummaries. In this work, we turn to a slower but more straightforward approach\nto developing summarization benchmark data: We hire highly-qualified\ncontractors to read stories and write original summaries from scratch. To\namortize reading time, we collect five summaries per document, with the first\ngiving an overview and the subsequent four addressing specific questions. We\nuse this protocol to collect SQuALITY, a dataset of question-focused summaries\nbuilt on the same public-domain short stories as the multiple-choice dataset\nQuALITY (Pang et al., 2021). Experiments with state-of-the-art summarization\nsystems show that our dataset is challenging and that existing automatic\nevaluation metrics are weak indicators of quality.", "published": "2022-05-23 17:02:07", "link": "http://arxiv.org/abs/2205.11465v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Tracing Factual Knowledge in Language Models Back to the\n  Training Data", "abstract": "Language models (LMs) have been shown to memorize a great deal of factual\nknowledge contained in their training data. But when an LM generates an\nassertion, it is often difficult to determine where it learned this information\nand whether it is true. In this paper, we propose the problem of fact tracing:\nidentifying which training examples taught an LM to generate a particular\nfactual assertion. Prior work on training data attribution (TDA) may offer\neffective tools for identifying such examples, known as \"proponents\". We\npresent the first quantitative benchmark to evaluate this. We compare two\npopular families of TDA methods -- gradient-based and embedding-based -- and\nfind that much headroom remains. For example, both methods have lower\nproponent-retrieval precision than an information retrieval baseline (BM25)\nthat does not have access to the LM at all. We identify key challenges that may\nbe necessary for further improvement such as overcoming the problem of gradient\nsaturation, and also show how several nuanced implementation details of\nexisting neural TDA methods can significantly improve overall fact tracing\nperformance.", "published": "2022-05-23 17:34:16", "link": "http://arxiv.org/abs/2205.11482v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Local Byte Fusion for Neural Machine Translation", "abstract": "Subword tokenization schemes are the dominant technique used in current NLP\nmodels. However, such schemes can be rigid and tokenizers built on one corpus\ndo not adapt well to other parallel corpora. It has also been observed that in\nmultilingual corpora, subword tokenization schemes over-segment low-resource\nlanguages leading to a drop in translation performance. A simple alternative to\nsubword tokenizers is byte-based methods i.e. tokenization into byte sequences\nusing encoding schemes such as UTF-8. Byte tokens often represent inputs at a\nsub-character granularity i.e. one character can be represented by a sequence\nof multiple byte tokens. This results in byte sequences that are significantly\nlonger than character sequences. Enforcing aggregation of local information in\nthe lower layers can guide the model to build higher-level semantic\ninformation. We propose a Local Byte Fusion (LOBEF) method for byte-based\nmachine translation -- utilizing byte $n$-gram and word boundaries -- to\naggregate local semantic information. Extensive experiments on multilingual\ntranslation, zero-shot cross-lingual transfer, and domain adaptation reveal a\nconsistent improvement over traditional byte-based models and even over subword\ntechniques. Further analysis also indicates that our byte-based models are\nparameter-efficient and can be trained faster than subword models.", "published": "2022-05-23 17:49:02", "link": "http://arxiv.org/abs/2205.11490v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HyperTree Proof Search for Neural Theorem Proving", "abstract": "We propose an online training procedure for a transformer-based automated\ntheorem prover. Our approach leverages a new search algorithm, HyperTree Proof\nSearch (HTPS), inspired by the recent success of AlphaZero. Our model learns\nfrom previous proof searches through online training, allowing it to generalize\nto domains far from the training distribution. We report detailed ablations of\nour pipeline's main components by studying performance on three environments of\nincreasing complexity. In particular, we show that with HTPS alone, a model\ntrained on annotated proofs manages to prove 65.4% of a held-out set of\nMetamath theorems, significantly outperforming the previous state of the art of\n56.5% by GPT-f. Online training on these unproved theorems increases accuracy\nto 82.6%. With a similar computational budget, we improve the state of the art\non the Lean-based miniF2F-curriculum dataset from 31% to 42% proving accuracy.", "published": "2022-05-23 17:49:55", "link": "http://arxiv.org/abs/2205.11491v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Paradox of Learning to Reason from Data", "abstract": "Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be\ntrained end-to-end to solve logical reasoning problems presented in natural\nlanguage? We attempt to answer this question in a confined problem space where\nthere exists a set of parameters that perfectly simulates logical reasoning. We\nmake observations that seem to contradict each other: BERT attains near-perfect\naccuracy on in-distribution test examples while failing to generalize to other\ndata distributions over the exact same problem space. Our study provides an\nexplanation for this paradox: instead of learning to emulate the correct\nreasoning function, BERT has in fact learned statistical features that\ninherently exist in logical reasoning problems. We also show that it is\ninfeasible to jointly remove statistical features from data, illustrating the\ndifficulty of learning to reason in general. Our result naturally extends to\nother neural models and unveils the fundamental difference between learning to\nreason and learning to achieve high performance on NLP benchmarks using\nstatistical features.", "published": "2022-05-23 17:56:48", "link": "http://arxiv.org/abs/2205.11502v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Ignore Adversarial Attacks", "abstract": "Despite the strong performance of current NLP models, they can be brittle\nagainst adversarial attacks. To enable effective learning against adversarial\ninputs, we introduce the use of rationale models that can explicitly learn to\nignore attack tokens. We find that the rationale models can successfully ignore\nover 90% of attack tokens. This approach leads to consistent sizable\nimprovements ($\\sim$10%) over baseline models in robustness on three datasets\nfor both BERT and RoBERTa, and also reliably outperforms data augmentation with\nadversarial examples alone. In many cases, we find that our method is able to\nclose the gap between model performance on a clean test set and an attacked\ntest set and hence reduce the effect of adversarial attacks.", "published": "2022-05-23 18:01:30", "link": "http://arxiv.org/abs/2205.11551v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simple Recurrence Improves Masked Language Models", "abstract": "In this work, we explore whether modeling recurrence into the Transformer\narchitecture can both be beneficial and efficient, by building an extremely\nsimple recurrent module into the Transformer. We compare our model to baselines\nfollowing the training and evaluation recipe of BERT. Our results confirm that\nrecurrence can indeed improve Transformer models by a consistent margin,\nwithout requiring low-level performance optimizations, and while keeping the\nnumber of parameters constant. For example, our base model achieves an absolute\nimprovement of 2.1 points averaged across 10 tasks and also demonstrates\nincreased stability in fine-tuning over a range of learning rates.", "published": "2022-05-23 19:38:23", "link": "http://arxiv.org/abs/2205.11588v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Challenges in Measuring Bias via Open-Ended Language Generation", "abstract": "Researchers have devised numerous ways to quantify social biases vested in\npretrained language models. As some language models are capable of generating\ncoherent completions given a set of textual prompts, several prompting datasets\nhave been proposed to measure biases between social groups -- posing language\ngeneration as a way of identifying biases. In this opinion paper, we analyze\nhow specific choices of prompt sets, metrics, automatic tools and sampling\nstrategies affect bias results. We find out that the practice of measuring\nbiases through text completion is prone to yielding contradicting results under\ndifferent experiment settings. We additionally provide recommendations for\nreporting biases in open-ended language generation for a more complete outlook\nof biases exhibited by a given language model. Code to reproduce the results is\nreleased under https://github.com/feyzaakyurek/bias-textgen.", "published": "2022-05-23 19:57:15", "link": "http://arxiv.org/abs/2205.11601v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "On Measuring Social Biases in Prompt-Based Multi-Task Learning", "abstract": "Large language models trained on a mixture of NLP tasks that are converted\ninto a text-to-text format using prompts, can generalize into novel forms of\nlanguage and handle novel tasks. A large body of work within prompt engineering\nattempts to understand the effects of input forms and prompts in achieving\nsuperior performance. We consider an alternative measure and inquire whether\nthe way in which an input is encoded affects social biases promoted in outputs.\nIn this paper, we study T0, a large-scale multi-task text-to-text language\nmodel trained using prompt-based learning. We consider two different forms of\nsemantically equivalent inputs: question-answer format and premise-hypothesis\nformat. We use an existing bias benchmark for the former BBQ and create the\nfirst bias benchmark in natural language inference BBNLI with hand-written\nhypotheses while also converting each benchmark into the other form. The\nresults on two benchmarks suggest that given two different formulations of\nessentially the same input, T0 conspicuously acts more biased in question\nanswering form, which is seen during training, compared to premise-hypothesis\nform which is unlike its training examples. Code and data are released under\nhttps://github.com/feyzaakyurek/bbnli.", "published": "2022-05-23 20:01:20", "link": "http://arxiv.org/abs/2205.11605v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Utilizing Language-Image Pretraining for Efficient and Robust Bilingual\n  Word Alignment", "abstract": "Word translation without parallel corpora has become feasible, rivaling the\nperformance of supervised methods. Recent findings have shown that the accuracy\nand robustness of unsupervised word translation (UWT) can be improved by making\nuse of visual observations, which are universal representations across\nlanguages. In this work, we investigate the potential of using not only visual\nobservations but also pretrained language-image models for enabling a more\nefficient and robust UWT. Specifically, we develop a novel UWT method dubbed\nWord Alignment using Language-Image Pretraining (WALIP), which leverages visual\nobservations via the shared embedding space of images and texts provided by\nCLIP models (Radford et al., 2021). WALIP has a two-step procedure. First, we\nretrieve word pairs with high confidences of similarity, computed using our\nproposed image-based fingerprints, which define the initial pivot for the word\nalignment. Second, we apply our robust Procrustes algorithm to estimate the\nlinear mapping between two embedding spaces, which iteratively corrects and\nrefines the estimated alignment. Our extensive experiments show that WALIP\nimproves upon the state-of-the-art performance of bilingual word alignment for\na few language pairs across different word embeddings and displays great\nrobustness to the dissimilarity of language pairs or training corpora for two\nword embeddings.", "published": "2022-05-23 20:29:26", "link": "http://arxiv.org/abs/2205.11616v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FlexiBERT: Are Current Transformer Architectures too Homogeneous and\n  Rigid?", "abstract": "The existence of a plethora of language models makes the problem of selecting\nthe best one for a custom task challenging. Most state-of-the-art methods\nleverage transformer-based models (e.g., BERT) or their variants. Training such\nmodels and exploring their hyperparameter space, however, is computationally\nexpensive. Prior work proposes several neural architecture search (NAS) methods\nthat employ performance predictors (e.g., surrogate models) to address this\nissue; however, analysis has been limited to homogeneous models that use fixed\ndimensionality throughout the network. This leads to sub-optimal architectures.\nTo address this limitation, we propose a suite of heterogeneous and flexible\nmodels, namely FlexiBERT, that have varied encoder layers with a diverse set of\npossible operations and different hidden dimensions. For better-posed surrogate\nmodeling in this expanded design space, we propose a new graph-similarity-based\nembedding scheme. We also propose a novel NAS policy, called BOSHNAS, that\nleverages this new scheme, Bayesian modeling, and second-order optimization, to\nquickly train and use a neural surrogate model to converge to the optimal\narchitecture. A comprehensive set of experiments shows that the proposed\npolicy, when applied to the FlexiBERT design space, pushes the performance\nfrontier upwards compared to traditional models. FlexiBERT-Mini, one of our\nproposed models, has 3% fewer parameters than BERT-Mini and achieves 8.9%\nhigher GLUE score. A FlexiBERT model with equivalent performance as the best\nhomogeneous model achieves 2.6x smaller size. FlexiBERT-Large, another proposed\nmodel, achieves state-of-the-art results, outperforming the baseline models by\nat least 5.7% on the GLUE benchmark.", "published": "2022-05-23 22:44:34", "link": "http://arxiv.org/abs/2205.11656v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Calibrate and Refine! A Novel and Agile Framework for ASR-error Robust\n  Intent Detection", "abstract": "The past ten years have witnessed the rapid development of text-based intent\ndetection, whose benchmark performances have already been taken to a remarkable\nlevel by deep learning techniques. However, automatic speech recognition (ASR)\nerrors are inevitable in real-world applications due to the environment noise,\nunique speech patterns and etc, leading to sharp performance drop in\nstate-of-the-art text-based intent detection models. Essentially, this\nphenomenon is caused by the semantic drift brought by ASR errors and most\nexisting works tend to focus on designing new model structures to reduce its\nimpact, which is at the expense of versatility and flexibility. Different from\nprevious one-piece model, in this paper, we propose a novel and agile framework\ncalled CR-ID for ASR error robust intent detection with two plug-and-play\nmodules, namely semantic drift calibration module (SDCM) and phonemic\nrefinement module (PRM), which are both model-agnostic and thus could be easily\nintegrated to any existing intent detection models without modifying their\nstructures. Experimental results on SNIPS dataset show that, our proposed CR-ID\nframework achieves competitive performance and outperform all the baseline\nmethods on ASR outputs, which verifies that CR-ID can effectively alleviate the\nsemantic drift caused by ASR errors.", "published": "2022-05-23 02:54:11", "link": "http://arxiv.org/abs/2205.11008v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PEVL: Position-enhanced Pre-training and Prompt Tuning for\n  Vision-language Models", "abstract": "Vision-language pre-training (VLP) has shown impressive performance on a wide\nrange of cross-modal tasks, where VLP models without reliance on object\ndetectors are becoming the mainstream due to their superior computation\nefficiency and competitive performance. However, the removal of object\ndetectors also deprives the capability of VLP models in explicit object\nmodeling, which is essential to various position-sensitive vision-language (VL)\ntasks, such as referring expression comprehension and visual commonsense\nreasoning. To address the challenge, we introduce PEVL that enhances the\npre-training and prompt tuning of VLP models with explicit object position\nmodeling. Specifically, PEVL reformulates discretized object positions and\nlanguage in a unified language modeling framework, which facilitates explicit\nVL alignment during pre-training, and also enables flexible prompt tuning for\nvarious downstream tasks. We show that PEVL enables state-of-the-art\nperformance of detector-free VLP models on position-sensitive tasks such as\nreferring expression comprehension and phrase grounding, and also improves the\nperformance on position-insensitive tasks with grounded inputs. We make the\ndata and code for this paper publicly available at\nhttps://github.com/thunlp/PEVL.", "published": "2022-05-23 10:17:53", "link": "http://arxiv.org/abs/2205.11169v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Consistency of UML class, object and statechart diagrams using ontology\n  reasoners", "abstract": "We propose an automatic approach to analyze the consistency and\nsatisfiability of Unified Modeling Language UML models containing multiple\nclass, object and statechart diagrams using logic reasoners for the Web\nOntology Language OWL 2. We describe how to translate UML models in OWL 2 and\nwe present a tool chain implementing this translation that can be used with any\nstandard compliant UML modeling tool. The proposed approach is limited in\nscope, but is fully automatic and does not require any expertise about OWL 2\nand its reasoners from the designer.", "published": "2022-05-23 10:29:32", "link": "http://arxiv.org/abs/2205.11177v1", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Unsupervised Tokenization Learning", "abstract": "In the presented study, we discover that the so-called \"transition freedom\"\nmetric appears superior for unsupervised tokenization purposes in comparison to\nstatistical metrics such as mutual information and conditional probability,\nproviding F-measure scores in range from 0.71 to 1.0 across explored\nmultilingual corpora. We find that different languages require different\noffshoots of that metric (such as derivative, variance, and \"peak values\") for\nsuccessful tokenization. Larger training corpora do not necessarily result in\nbetter tokenization quality, while compressing the models by eliminating\nstatistically weak evidence tends to improve performance. The proposed\nunsupervised tokenization technique provides quality better than or comparable\nto lexicon-based ones, depending on the language.", "published": "2022-05-23 16:33:41", "link": "http://arxiv.org/abs/2205.11443v4", "categories": ["cs.CL", "cs.AI", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Conditional Supervised Contrastive Learning for Fair Text Classification", "abstract": "Contrastive representation learning has gained much attention due to its\nsuperior performance in learning representations from both image and sequential\ndata. However, the learned representations could potentially lead to\nperformance disparities in downstream tasks, such as increased silencing of\nunderrepresented groups in toxicity comment classification. In light of this\nchallenge, in this work, we study learning fair representations that satisfy a\nnotion of fairness known as equalized odds for text classification via\ncontrastive learning. Specifically, we first theoretically analyze the\nconnections between learning representations with a fairness constraint and\nconditional supervised contrastive objectives, and then propose to use\nconditional supervised contrastive objectives to learn fair representations for\ntext classification. We conduct experiments on two text datasets to demonstrate\nthe effectiveness of our approaches in balancing the trade-offs between task\nperformance and bias mitigation among existing baselines for text\nclassification. Furthermore, we also show that the proposed methods are stable\nin different hyperparameter settings.", "published": "2022-05-23 17:38:30", "link": "http://arxiv.org/abs/2205.11485v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks\n  for Visual Question Answering", "abstract": "Visual question answering (VQA) requires systems to perform concept-level\nreasoning by unifying unstructured (e.g., the context in question and answer;\n\"QA context\") and structured (e.g., knowledge graph for the QA context and\nscene; \"concept graph\") multimodal knowledge. Existing works typically combine\na scene graph and a concept graph of the scene by connecting corresponding\nvisual nodes and concept nodes, then incorporate the QA context representation\nto perform question answering. However, these methods only perform a\nunidirectional fusion from unstructured knowledge to structured knowledge,\nlimiting their potential to capture joint reasoning over the heterogeneous\nmodalities of knowledge. To perform more expressive reasoning, we propose\nVQA-GNN, a new VQA model that performs bidirectional fusion between\nunstructured and structured multimodal knowledge to obtain unified knowledge\nrepresentations. Specifically, we inter-connect the scene graph and the concept\ngraph through a super node that represents the QA context, and introduce a new\nmultimodal GNN technique to perform inter-modal message passing for reasoning\nthat mitigates representational gaps between modalities. On two challenging VQA\ntasks (VCR and GQA), our method outperforms strong baseline VQA methods by 3.2%\non VCR (Q-AR) and 4.6% on GQA, suggesting its strength in performing\nconcept-level reasoning. Ablation studies further demonstrate the efficacy of\nthe bidirectional fusion and multimodal GNN method in unifying unstructured and\nstructured multimodal knowledge.", "published": "2022-05-23 17:55:34", "link": "http://arxiv.org/abs/2205.11501v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "What Makes Data-to-Text Generation Hard for Pretrained Language Models?", "abstract": "Expressing natural language descriptions of structured facts or relations --\ndata-to-text generation (D2T) -- increases the accessibility of structured\nknowledge repositories. Previous work shows that pre-trained language\nmodels(PLMs) perform remarkably well on this task after fine-tuning on a\nsignificant amount of task-specific training data. On the other hand, while\nauto-regressive PLMs can generalize from a few task examples, their efficacy at\nD2T is largely unexplored. Furthermore, we have an incomplete understanding of\nthe limits of PLMs on D2T.\n  In this work, we conduct an empirical study of both fine-tuned and\nauto-regressive PLMs on the DART multi-domain D2T dataset. We consider their\nperformance as a function of the amount of task-specific data and how these\ndata are incorporated into the models: zero and few-shot learning, and\nfine-tuning of model weights. In addition, we probe the limits of PLMs by\nmeasuring performance on subsets of the evaluation data: novel predicates and\nabstractive test examples. To improve the performance on these subsets, we\ninvestigate two techniques: providing predicate descriptions in the context and\nre-ranking generated candidates by information reflected in the source.\nFinally, we conduct a human evaluation of model errors and show that D2T\ngeneration tasks would benefit from datasets with more careful manual curation.", "published": "2022-05-23 17:58:39", "link": "http://arxiv.org/abs/2205.11505v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Information Propagation by Composited Labels in Natural Language\n  Processing", "abstract": "In natural language processing (NLP), labeling on regions of text, such as\nwords, sentences and paragraphs, is a basic task. In this paper, label is\ndefined as map between mention of entity in a region on text and context of\nentity in a broader region on text containing the mention. This definition\nnaturally introduces linkage of entities induced from inclusion relation of\nregions, and connected entities form a graph representing information flow\ndefined by map. It also enables calculation of information loss through map\nusing entropy, and entropy lost is regarded as distance between two entities\nover a path on graph.", "published": "2022-05-23 23:19:14", "link": "http://arxiv.org/abs/2205.11509v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Natural Language Processing Pipeline for Detecting Informal Data\n  References in Academic Literature", "abstract": "Discovering authoritative links between publications and the datasets that\nthey use can be a labor-intensive process. We introduce a natural language\nprocessing pipeline that retrieves and reviews publications for informal\nreferences to research datasets, which complements the work of data librarians.\nWe first describe the components of the pipeline and then apply it to expand an\nauthoritative bibliography linking thousands of social science studies to the\ndata-related publications in which they are used. The pipeline increases recall\nfor literature to review for inclusion in data-related collections of\npublications and makes it possible to detect informal data references at scale.\nWe contribute (1) a novel Named Entity Recognition (NER) model that reliably\ndetects informal data references and (2) a dataset connecting items from social\nscience literature with datasets they reference. Together, these contributions\nenable future work on data reference, data citation networks, and data reuse.", "published": "2022-05-23 22:06:46", "link": "http://arxiv.org/abs/2205.11651v1", "categories": ["cs.DL", "cs.CL", "cs.LG"], "primary_category": "cs.DL"}
{"title": "Markedness in Visual Semantic AI", "abstract": "We evaluate the state-of-the-art multimodal \"visual semantic\" model CLIP\n(\"Contrastive Language Image Pretraining\") for biases related to the marking of\nage, gender, and race or ethnicity. Given the option to label an image as \"a\nphoto of a person\" or to select a label denoting race or ethnicity, CLIP\nchooses the \"person\" label 47.9% of the time for White individuals, compared\nwith 5.0% or less for individuals who are Black, East Asian, Southeast Asian,\nIndian, or Latino or Hispanic. The model is more likely to rank the unmarked\n\"person\" label higher than labels denoting gender for Male individuals (26.7%\nof the time) vs. Female individuals (15.2% of the time). Age affects whether an\nindividual is marked by the model: Female individuals under the age of 20 are\nmore likely than Male individuals to be marked with a gender label, but less\nlikely to be marked with an age label, while Female individuals over the age of\n40 are more likely to be marked based on age than Male individuals. We also\nexamine the self-similarity (mean pairwise cosine similarity) for each social\ngroup, where higher self-similarity denotes greater attention directed by CLIP\nto the shared characteristics (age, race, or gender) of the social group. As\nage increases, the self-similarity of representations of Female individuals\nincreases at a higher rate than for Male individuals, with the disparity most\npronounced at the \"more than 70\" age range. All ten of the most self-similar\nsocial groups are individuals under the age of 10 or over the age of 70, and\nsix of the ten are Female individuals. Existing biases of self-similarity and\nmarkedness between Male and Female gender groups are further exacerbated when\nthe groups compared are individuals who are White and Male and individuals who\nare Black and Female. Results indicate that CLIP reflects the biases of the\nlanguage and society which produced its training data.", "published": "2022-05-23 15:14:41", "link": "http://arxiv.org/abs/2205.11378v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multiple Offsets Multilateration: a new paradigm for sensor network\n  calibration with unsynchronized reference nodes", "abstract": "Positioning using wave signal measurements is used in several applications,\nsuch as GPS systems, structure from sound and Wifi based positioning.\nMathematically, such problems require the computation of the positions of\nreceivers and/or transmitters as well as time offsets if the devices are\nunsynchronized. In this paper, we expand the previous state-of-the-art on\npositioning formulations by introducing Multiple Offsets Multilateration (MOM),\na new mathematical framework to compute the receivers positions with\npseudoranges from unsynchronized reference transmitters at known positions.\nThis could be applied in several scenarios, for example structure from sound\nand positioning with LEO satellites. We mathematically describe MOM,\ndetermining how many receivers and transmitters are needed for the network to\nbe solvable, a study on the number of possible distinct solutions is presented\nand stable solvers based on homotopy continuation are derived. The solvers are\nshown to be efficient and robust to noise both for synthetic and real audio\ndata.", "published": "2022-05-23 13:38:39", "link": "http://arxiv.org/abs/2205.11299v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
