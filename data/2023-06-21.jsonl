{"title": "Towards Accurate Translation via Semantically Appropriate Application of\n  Lexical Constraints", "abstract": "Lexically-constrained NMT (LNMT) aims to incorporate user-provided\nterminology into translations. Despite its practical advantages, existing work\nhas not evaluated LNMT models under challenging real-world conditions. In this\npaper, we focus on two important but under-studied issues that lie in the\ncurrent evaluation process of LNMT studies. The model needs to cope with\nchallenging lexical constraints that are \"homographs\" or \"unseen\" during\ntraining. To this end, we first design a homograph disambiguation module to\ndifferentiate the meanings of homographs. Moreover, we propose PLUMCOT, which\nintegrates contextually rich information about unseen lexical constraints from\npre-trained language models and strengthens a copy mechanism of the pointer\nnetwork via direct supervision of a copying score. We also release HOLLY, an\nevaluation benchmark for assessing the ability of a model to cope with\n\"homographic\" and \"unseen\" lexical constraints. Experiments on HOLLY and the\nprevious test setup show the effectiveness of our method. The effects of\nPLUMCOT are shown to be remarkable in \"unseen\" constraints. Our dataset is\navailable at https://github.com/papago-lab/HOLLY-benchmark", "published": "2023-06-21 08:08:15", "link": "http://arxiv.org/abs/2306.12089v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature Interactions Reveal Linguistic Structure in Language Models", "abstract": "We study feature interactions in the context of feature attribution methods\nfor post-hoc interpretability. In interpretability research, getting to grips\nwith feature interactions is increasingly recognised as an important challenge,\nbecause interacting features are key to the success of neural networks. Feature\ninteractions allow a model to build up hierarchical representations for its\ninput, and might provide an ideal starting point for the investigation into\nlinguistic structure in language models. However, uncovering the exact role\nthat these interactions play is also difficult, and a diverse range of\ninteraction attribution methods has been proposed. In this paper, we focus on\nthe question which of these methods most faithfully reflects the inner workings\nof the target models. We work out a grey box methodology, in which we train\nmodels to perfection on a formal language classification task, using PCFGs. We\nshow that under specific configurations, some methods are indeed able to\nuncover the grammatical rules acquired by a model. Based on these findings we\nextend our evaluation to a case study on language models, providing novel\ninsights into the linguistic structure that these models have acquired.", "published": "2023-06-21 11:24:41", "link": "http://arxiv.org/abs/2306.12181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Limits for Learning with Language Models", "abstract": "With the advent of large language models (LLMs), the trend in NLP has been to\ntrain LLMs on vast amounts of data to solve diverse language understanding and\ngeneration tasks. The list of LLM successes is long and varied. Nevertheless,\nseveral recent papers provide empirical evidence that LLMs fail to capture\nimportant aspects of linguistic meaning. Focusing on universal quantification,\nwe provide a theoretical foundation for these empirical findings by proving\nthat LLMs cannot learn certain fundamental semantic properties including\nsemantic entailment and consistency as they are defined in formal semantics.\nMore generally, we show that LLMs are unable to learn concepts beyond the first\nlevel of the Borel Hierarchy, which imposes severe limits on the ability of\nLMs, both large and small, to capture many aspects of linguistic meaning. This\nmeans that LLMs will continue to operate without formal guarantees on tasks\nthat require entailments and deep linguistic understanding.", "published": "2023-06-21 12:11:31", "link": "http://arxiv.org/abs/2306.12213v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bidirectional End-to-End Learning of Retriever-Reader Paradigm for\n  Entity Linking", "abstract": "Entity Linking (EL) is a fundamental task for Information Extraction and\nKnowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first\nfind mentions in the given input document and then link the mentions to\ncorresponding entities in a specific knowledge base. Recently, the paradigm of\nretriever-reader promotes the progress of end-to-end EL, benefiting from the\nadvantages of dense entity retrieval and machine reading comprehension.\nHowever, the existing study only trains the retriever and the reader separately\nin a pipeline manner, which ignores the benefit that the interaction between\nthe retriever and the reader can bring to the task. To advance the\nretriever-reader paradigm to perform more perfectly on end-to-end EL, we\npropose BEER$^2$, a Bidirectional End-to-End training framework for Retriever\nand Reader. Through our designed bidirectional end-to-end training, BEER$^2$\nguides the retriever and the reader to learn from each other, make progress\ntogether, and ultimately improve EL performance. Extensive experiments on\nbenchmarks of multiple domains demonstrate the effectiveness of our proposed\nBEER$^2$.", "published": "2023-06-21 13:04:30", "link": "http://arxiv.org/abs/2306.12245v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving and Generating NPR Sunday Puzzles with Large Language Models", "abstract": "We explore the ability of large language models to solve and generate puzzles\nfrom the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15\nyears of on-air puzzles. We evaluate four large language models using PUZZLEQA,\nin both multiple choice and free response formats, and explore two prompt\nengineering techniques to improve free response performance: chain-of-thought\nreasoning and prompt summarization. We find that state-of-the-art large\nlanguage models can solve many PUZZLEQA puzzles: the best model, GPT-3.5,\nachieves 50.2% loose accuracy. However, in our few-shot puzzle generation\nexperiment, we find no evidence that models can generate puzzles: GPT-3.5\ngenerates puzzles with answers that do not conform to the generated rules.\nPuzzle generation remains a challenging task for future work.", "published": "2023-06-21 13:23:48", "link": "http://arxiv.org/abs/2306.12255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Medical ministrations through web scraping", "abstract": "Web scraping is a technique that allows us to extract data from websites\nautomatically. in the field of medicine, web scraping can be used to collect\ninformation about medical procedures, treatments, and healthcare providers.\nthis information can be used to improve patient care, monitor the quality of\nhealthcare services, and identify areas for improvement. one area where web\nscraping can be particularly useful is in medical ministrations. medical\nministrations are the actions taken to provide medical care to patients, and\nweb scraping can help healthcare providers identify the most effective\nministrations for their patients. for example, healthcare providers can use web\nscraping to collect data about the symptoms and medical histories of their\npatients, and then use this information to determine the most appropriate\nministrations. they can also use web scraping to gather information about the\nlatest medical research and clinical trials, which can help them stay\nup-to-date with the latest treatments and procedures.", "published": "2023-06-21 14:43:25", "link": "http://arxiv.org/abs/2306.12310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterated Piecewise Affine (IPA) Approximation for Language Modeling", "abstract": "In this work, we demonstrate the application of a first-order Taylor\nexpansion to approximate a generic function $F: R^{n \\times m} \\to R^{n \\times\nm}$ and utilize it in language modeling. To enhance the basic Taylor expansion,\nwe introduce iteration and piecewise modeling, leading us to name the algorithm\nthe Iterative Piecewise Affine (IPA) approximation. The final algorithm\nexhibits interesting resemblances to the Transformers decoder architecture. By\ncomparing parameter arrangements in IPA and Transformers, we observe a\nstrikingly similar performance, with IPA outperforming Transformers by 1.5\\% in\nthe next token prediction task with cross-entropy loss for smaller sequence\nlengths.", "published": "2023-06-21 14:58:31", "link": "http://arxiv.org/abs/2306.12317v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Evaluation of Document Classification using RVL-CDIP", "abstract": "The RVL-CDIP benchmark is widely used for measuring performance on the task\nof document classification. Despite its widespread use, we reveal several\nundesirable characteristics of the RVL-CDIP benchmark. These include (1)\nsubstantial amounts of label noise, which we estimate to be 8.1% (ranging\nbetween 1.6% to 16.9% per document category); (2) presence of many ambiguous or\nmulti-label documents; (3) a large overlap between test and train splits, which\ncan inflate model performance metrics; and (4) presence of sensitive\npersonally-identifiable information like US Social Security numbers (SSNs). We\nargue that there is a risk in using RVL-CDIP for benchmarking document\nclassifiers, as its limited scope, presence of errors (state-of-the-art models\nnow achieve accuracy error rates that are within our estimated label error\nrate), and lack of diversity make it less than ideal for benchmarking. We\nfurther advocate for the creation of a new document classification benchmark,\nand provide recommendations for what characteristics such a resource should\ninclude.", "published": "2023-06-21 20:32:22", "link": "http://arxiv.org/abs/2306.12550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SituatedGen: Incorporating Geographical and Temporal Contexts into\n  Generative Commonsense Reasoning", "abstract": "Recently, commonsense reasoning in text generation has attracted much\nattention. Generative commonsense reasoning is the task that requires machines,\ngiven a group of keywords, to compose a single coherent sentence with\ncommonsense plausibility. While existing datasets targeting generative\ncommonsense reasoning focus on everyday scenarios, it is unclear how well\nmachines reason under specific geographical and temporal contexts. We formalize\nthis challenging task as SituatedGen, where machines with commonsense should\ngenerate a pair of contrastive sentences given a group of keywords including\ngeographical or temporal entities. We introduce a corresponding English dataset\nconsisting of 8,268 contrastive sentence pairs, which are built upon several\nexisting commonsense reasoning benchmarks with minimal manual labor.\nExperiments show that state-of-the-art generative language models struggle to\ngenerate sentences with commonsense plausibility and still lag far behind human\nperformance. Our dataset is publicly available at\nhttps://github.com/yunx-z/situated_gen.", "published": "2023-06-21 20:36:55", "link": "http://arxiv.org/abs/2306.12552v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphological Inflection with Phonological Features", "abstract": "Recent years have brought great advances into solving morphological tasks,\nmostly due to powerful neural models applied to various tasks as (re)inflection\nand analysis. Yet, such morphological tasks cannot be considered solved,\nespecially when little training data is available or when generalizing to\npreviously unseen lemmas. This work explores effects on performance obtained\nthrough various ways in which morphological models get access to subcharacter\nphonological features that are the targets of morphological processes. We\ndesign two methods to achieve this goal: one that leaves models as is but\nmanipulates the data to include features instead of characters, and another\nthat manipulates models to take phonological features into account when\nbuilding representations for phonemes. We elicit phonemic data from standard\ngraphemic data using language-specific grammars for languages with shallow\ngrapheme-to-phoneme mapping, and we experiment with two reinflection models\nover eight languages. Our results show that our methods yield comparable\nresults to the grapheme-based baseline overall, with minor improvements in some\nof the languages. All in all, we conclude that patterns in character\ndistributions are likely to allow models to infer the underlying phonological\ncharacteristics, even when phonemes are not explicitly represented.", "published": "2023-06-21 21:34:39", "link": "http://arxiv.org/abs/2306.12581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer\n  Reviews", "abstract": "We introduce the task of automatically revising scientific papers based on\npeer feedback and release ARIES, a dataset of review comments and their\ncorresponding paper edits. The data is drawn from real reviewer-author\ninteractions from computer science, and we provide labels linking each reviewer\ncomment to the specific paper edits made by the author in response. We\nautomatically create a high-precision silver training set, as well as an\nexpert-labeled test set that shows high inter-annotator agreement. In\nexperiments with 10 models covering the state of the art, we find that they\nstruggle even to identify which edits correspond to a comment -- especially\nwhen the relationship between the edit and the comment is indirect and requires\nreasoning to uncover. We also extensively analyze GPT-4's ability to generate\nedits given a comment and the original paper. We find that it often succeeds on\na superficial level, but tends to rigidly follow the wording of the feedback\nrather than the underlying intent, and lacks technical details compared to\nhuman-written edits.", "published": "2023-06-21 22:00:03", "link": "http://arxiv.org/abs/2306.12587v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Semi-Autoregressive Graph Generative Model for Dependency Graph\n  Parsing", "abstract": "Recent years have witnessed the impressive progress in Neural Dependency\nParsing. According to the different factorization approaches to the graph joint\nprobabilities, existing parsers can be roughly divided into autoregressive and\nnon-autoregressive patterns. The former means that the graph should be\nfactorized into multiple sequentially dependent components, then it can be\nbuilt up component by component. And the latter assumes these components to be\nindependent so that they can be outputted in a one-shot manner. However, when\ntreating the directed edge as an explicit dependency relationship, we discover\nthat there is a mixture of independent and interdependent components in the\ndependency graph, signifying that both aforementioned models fail to precisely\ncapture the explicit dependencies among nodes and edges. Based on this\nproperty, we design a Semi-Autoregressive Dependency Parser to generate\ndependency graphs via adding node groups and edge groups autoregressively while\npouring out all group elements in parallel. The model gains a trade-off between\nnon-autoregression and autoregression, which respectively suffer from the lack\nof target inter-dependencies and the uncertainty of graph generation orders.\nThe experiments show the proposed parser outperforms strong baselines on\nEnhanced Universal Dependencies of multiple languages, especially achieving\n$4\\%$ average promotion at graph-level accuracy. Also, the performances of\nmodel variations show the importance of specific parts.", "published": "2023-06-21 05:07:40", "link": "http://arxiv.org/abs/2306.12018v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Strategies in Transfer Learning for Low-Resource Speech Synthesis: Phone\n  Mapping, Features Input, and Source Language Selection", "abstract": "We compare using a PHOIBLE-based phone mapping method and using phonological\nfeatures input in transfer learning for TTS in low-resource languages. We use\ndiverse source languages (English, Finnish, Hindi, Japanese, and Russian) and\ntarget languages (Bulgarian, Georgian, Kazakh, Swahili, Urdu, and Uzbek) to\ntest the language-independence of the methods and enhance the findings'\napplicability. We use Character Error Rates from automatic speech recognition\nand predicted Mean Opinion Scores for evaluation. Results show that both phone\nmapping and features input improve the output quality and the latter performs\nbetter, but these effects also depend on the specific language combination. We\nalso compare the recently-proposed Angular Similarity of Phone Frequencies\n(ASPF) with a family tree-based distance measure as a criterion to select\nsource languages in transfer learning. ASPF proves effective if label-based\nphone input is used, while the language distance does not have expected\neffects.", "published": "2023-06-21 06:12:14", "link": "http://arxiv.org/abs/2306.12040v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sample Attackability in Natural Language Adversarial Attacks", "abstract": "Adversarial attack research in natural language processing (NLP) has made\nsignificant progress in designing powerful attack methods and defence\napproaches. However, few efforts have sought to identify which source samples\nare the most attackable or robust, i.e. can we determine for an unseen target\nmodel, which samples are the most vulnerable to an adversarial attack. This\nwork formally extends the definition of sample attackability/robustness for NLP\nattacks. Experiments on two popular NLP datasets, four state of the art models\nand four different NLP adversarial attack methods, demonstrate that sample\nuncertainty is insufficient for describing characteristics of attackable/robust\nsamples and hence a deep learning based detector can perform much better at\nidentifying the most attackable and robust samples for an unseen target model.\nNevertheless, further analysis finds that there is little agreement in which\nsamples are considered the most attackable/robust across different NLP attack\nmethods, explaining a lack of portability of attackability detection methods\nacross attack methods.", "published": "2023-06-21 06:20:51", "link": "http://arxiv.org/abs/2306.12043v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Hierarchical Reasoning Chains by Linking Discourse Units and\n  Key Phrases for Reading Comprehension", "abstract": "Machine reading comprehension (MRC) poses new challenges over logical\nreasoning, which aims to understand the implicit logical relations entailed in\nthe given contexts and perform inference over them. Due to the complexity of\nlogic, logical relations exist at different granularity levels. However, most\nexisting methods of logical reasoning individually focus on either entity-aware\nor discourse-based information but ignore the hierarchical relations that may\neven have mutual effects. In this paper, we propose a holistic graph network\n(HGN) which deals with context at both discourse level and word level, as the\nbasis for logical reasoning, to provide a more fine-grained relation\nextraction. Specifically, node-level and type-level relations, which can be\ninterpreted as bridges in the reasoning process, are modeled by a hierarchical\ninteraction mechanism to improve the interpretation of MRC systems.\nExperimental results on logical reasoning QA datasets (ReClor and LogiQA) and\nnatural language inference datasets (SNLI and ANLI) show the effectiveness and\ngeneralization of our method, and in-depth analysis verifies its capability to\nunderstand complex logical relations.", "published": "2023-06-21 07:34:27", "link": "http://arxiv.org/abs/2306.12069v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Which Spurious Correlations Impact Reasoning in NLI Models? A Visual\n  Interactive Diagnosis through Data-Constrained Counterfactuals", "abstract": "We present a human-in-the-loop dashboard tailored to diagnosing potential\nspurious features that NLI models rely on for predictions. The dashboard\nenables users to generate diverse and challenging examples by drawing\ninspiration from GPT-3 suggestions. Additionally, users can receive feedback\nfrom a trained NLI model on how challenging the newly created example is and\nmake refinements based on the feedback. Through our investigation, we discover\nseveral categories of spurious correlations that impact the reasoning of NLI\nmodels, which we group into three categories: Semantic Relevance, Logical\nFallacies, and Bias. Based on our findings, we identify and describe various\nresearch opportunities, including diversifying training data and assessing NLI\nmodels' robustness by creating adversarial test suites.", "published": "2023-06-21 09:50:48", "link": "http://arxiv.org/abs/2306.12146v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Mixture Encoder for Joint Speech Separation and Recognition", "abstract": "Multi-speaker automatic speech recognition (ASR) is crucial for many\nreal-world applications, but it requires dedicated modeling techniques.\nExisting approaches can be divided into modular and end-to-end methods. Modular\napproaches separate speakers and recognize each of them with a single-speaker\nASR system. End-to-end models process overlapped speech directly in a single,\npowerful neural network. This work proposes a middle-ground approach that\nleverages explicit speech separation similarly to the modular approach but also\nincorporates mixture speech information directly into the ASR module in order\nto mitigate the propagation of errors made by the speech separator. We also\nexplore a way to exchange cross-speaker context information through a layer\nthat combines information of the individual speakers. Our system is optimized\nthrough separate and joint training stages and achieves a relative improvement\nof 7% in word error rate over a purely modular setup on the SMS-WSJ task.", "published": "2023-06-21 11:01:31", "link": "http://arxiv.org/abs/2306.12173v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating Pre-trained Language Models on Cross-Domain Datasets, a\n  Step Closer to General AI", "abstract": "Pre-trained language models have recently emerged as a powerful tool for\nfine-tuning a variety of language tasks. Ideally, when models are pre-trained\non large amount of data, they are expected to gain implicit knowledge. In this\npaper, we investigate the ability of pre-trained language models to generalize\nto different non-language tasks. In particular, we test them on tasks from\ndifferent domains such as computer vision, reasoning on hierarchical data, and\nprotein fold prediction. The four pre-trained models that we used, T5, BART,\nBERT, and GPT-2 achieve outstanding results. They all have similar performance\nand they outperform transformers that are trained from scratch by a large\nmargin. For instance, pre-trained language models perform better on the Listops\ndataset, with an average accuracy of 58.7\\%, compared to transformers trained\nfrom scratch, which have an average accuracy of 29.0\\%. The significant\nimprovement demonstrated across three types of datasets suggests that\npre-training on language helps the models to acquire general knowledge,\nbringing us a step closer to general AI. We also showed that reducing the\nnumber of parameters in pre-trained language models does not have a great\nimpact as the performance drops slightly when using T5-Small instead of\nT5-Base. In fact, when using only 2\\% of the parameters, we achieved a great\nimprovement compared to training from scratch. Finally, in contrast to prior\nwork, we find out that using pre-trained embeddings for the input layer is\nnecessary to achieve the desired results.", "published": "2023-06-21 11:55:17", "link": "http://arxiv.org/abs/2306.12205v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence\n  Embeddings", "abstract": "The paradigm of pre-training followed by fine-tuning on downstream tasks has\nbecome the mainstream method in natural language processing tasks. Although\npre-trained models have the advantage of generalization, their performance may\nstill vary significantly across different domain tasks. This is because the\ndata distribution in different domains varies. For example, the different parts\nof the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy\nmarried life' may have different impact for downstream tasks. For similarity\ncalculations, words such as 'led' and 'life' are more important. On the other\nhand, for sentiment analysis, the word 'happy' is crucial. This indicates that\ndifferent downstream tasks have different levels of sensitivity to sentence\ncomponents. Our starting point is to scale information of the model and data\naccording to the specifics of downstream tasks, enhancing domain information of\nrelevant parts for these tasks and reducing irrelevant elements for different\ndomain tasks, called SIFTER. In the experimental part, we use the SIFTER to\nimprove SimCSE by constructing positive sample pairs based on enhancing the\nsentence stem and reducing the unimportant components in the sentence, and\nmaximize the similarity between three sentences. Similarly, SIFTER can improve\nthe gate mechanism of the LSTM model by short-circuiting the input gate of\nimportant words so that the LSTM model remembers the important parts of the\nsentence. Our experiments demonstrate that SIFTER outperforms the SimCSE and\nLSTM baselines.", "published": "2023-06-21 14:08:08", "link": "http://arxiv.org/abs/2306.12280v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Solving Dialogue Grounding Embodied Task in a Simulated Environment\n  using Further Masked Language Modeling", "abstract": "Enhancing AI systems with efficient communication skills that align with\nhuman understanding is crucial for their effective assistance to human users.\nProactive initiatives from the system side are needed to discern specific\ncircumstances and interact aptly with users to solve these scenarios. In this\nresearch, we opt for a collective building assignment taken from the Minecraft\ndataset. Our proposed method employs language modeling to enhance task\nunderstanding through state-of-the-art (SOTA) methods using language models.\nThese models focus on grounding multi-modal understandinging and task-oriented\ndialogue comprehension tasks. This focus aids in gaining insights into how well\nthese models interpret and respond to a variety of inputs and tasks. Our\nexperimental results provide compelling evidence of the superiority of our\nproposed method. This showcases a substantial improvement and points towards a\npromising direction for future research in this domain.", "published": "2023-06-21 17:17:09", "link": "http://arxiv.org/abs/2306.12387v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LMFlow: An Extensible Toolkit for Finetuning and Inference of Large\n  Foundation Models", "abstract": "Foundation models have demonstrated a great ability to achieve general\nhuman-level intelligence far beyond traditional approaches. As the technique\nkeeps attracting attention from the AI community, an increasing number of\nfoundation models are becoming publicly accessible. However, a significant\nshortcoming of most of these models lies in their performance in\nspecialized-domain and task-specific applications, necessitating domain- and\ntask-aware fine-tuning to develop effective scientific language models. As the\nnumber of available foundation models and specialized tasks keeps growing, the\njob of training scientific language models becomes highly nontrivial. In this\npaper, we initiate steps to tackle this issue. We introduce an extensible and\nlightweight toolkit, LMFlow, which aims to simplify the domain- and task-aware\nfinetuning of general foundation models. LMFlow offers a complete finetuning\nworkflow for a foundation model to support specialized training with limited\ncomputing resources. Furthermore, it supports continuous pretraining,\ninstruction tuning, parameter-efficient finetuning, alignment tuning, inference\nacceleration, long context generalization, model customization, and even\nmultimodal finetuning, along with carefully designed and extensible APIs. This\ntoolkit has been thoroughly tested and is available at\nhttps://github.com/OptimalScale/LMFlow.", "published": "2023-06-21 17:58:25", "link": "http://arxiv.org/abs/2306.12420v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VisoGender: A dataset for benchmarking gender bias in image-text pronoun\n  resolution", "abstract": "We introduce VisoGender, a novel dataset for benchmarking gender bias in\nvision-language models. We focus on occupation-related biases within a\nhegemonic system of binary gender, inspired by Winograd and Winogender schemas,\nwhere each image is associated with a caption containing a pronoun relationship\nof subjects and objects in the scene. VisoGender is balanced by gender\nrepresentation in professional roles, supporting bias evaluation in two ways:\ni) resolution bias, where we evaluate the difference between pronoun resolution\naccuracies for image subjects with gender presentations perceived as masculine\nversus feminine by human annotators and ii) retrieval bias, where we compare\nratios of professionals perceived to have masculine and feminine gender\npresentations retrieved for a gender-neutral search query. We benchmark several\nstate-of-the-art vision-language models and find that they demonstrate bias in\nresolving binary gender in complex scenes. While the direction and magnitude of\ngender bias depends on the task and the model being evaluated, captioning\nmodels are generally less biased than Vision-Language Encoders. Dataset and\ncode are available at https://github.com/oxai/visogender", "published": "2023-06-21 17:59:51", "link": "http://arxiv.org/abs/2306.12424v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Misinformation as Information Pollution", "abstract": "Social media feed algorithms are designed to optimize online social\nengagements for the purpose of maximizing advertising profits, and therefore\nhave an incentive to promote controversial posts including misinformation. By\nthinking about misinformation as information pollution, we can draw parallels\nwith environmental policy for countering pollution such as carbon taxes.\nSimilar to pollution, a Pigouvian tax on misinformation provides economic\nincentives for social media companies to control the spread of misinformation\nmore effectively to avoid or reduce their misinformation tax, while preserving\nsome degree of freedom in platforms' response. In this paper, we highlight a\nbird's eye view of a Pigouvian misinformation tax and discuss the key questions\nand next steps for implementing such a taxing scheme.", "published": "2023-06-21 17:30:02", "link": "http://arxiv.org/abs/2306.12466v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Joint Prompt Optimization of Stacked LLMs using Variational Inference", "abstract": "Large language models (LLMs) can be seen as atomic units of computation\nmapping sequences to a distribution over sequences. Thus, they can be seen as\nstochastic language layers in a language network, where the learnable\nparameters are the natural language prompts at each layer. By stacking two such\nlayers and feeding the output of one layer to the next, we obtain a Deep\nLanguage Network (DLN). We first show how to effectively perform prompt\noptimization for a 1-Layer language network (DLN-1). Then, we present an\nextension that applies to 2-layer DLNs (DLN-2), where two prompts must be\nlearned. The key idea is to consider the output of the first layer as a latent\nvariable, which requires inference, and prompts to be learned as the parameters\nof the generative distribution. We first test the effectiveness of DLN-1 in\nmultiple reasoning and natural language understanding tasks. Then, we show that\nDLN-2 can reach higher performance than a single layer, showing promise that we\nmight reach comparable performance to GPT-4, even when each LLM in the network\nis smaller and less powerful.", "published": "2023-06-21 18:45:56", "link": "http://arxiv.org/abs/2306.12509v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning\n  Ability and Human-like Biases", "abstract": "This paper investigates whether current large language models exhibit biases\nin logical reasoning, similar to humans. Specifically, we focus on syllogistic\nreasoning, a well-studied form of inference in the cognitive science of human\ndeduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO,\noriginally designed for psychological experiments that assess human logical\nabilities in syllogistic reasoning. The dataset consists of syllogistic\ninferences in both English and Japanese. We examine three types of biases\nobserved in human syllogistic reasoning: belief biases, conversion errors, and\natmosphere effects. Our findings demonstrate that current large language models\nstruggle more with problems involving these three types of biases.", "published": "2023-06-21 21:04:11", "link": "http://arxiv.org/abs/2306.12567v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Approach to exploiting Multiple Datasets from TalkBank", "abstract": "TalkBank is an online database that facilitates the sharing of linguistics\nresearch data. However, the existing TalkBank's API has limited data filtering\nand batch processing capabilities. To overcome these limitations, this paper\nintroduces a pipeline framework that employs a hierarchical search approach,\nenabling efficient complex data selection. This approach involves a quick\npreliminary screening of relevant corpora that a researcher may need, and then\nperform an in-depth search for target data based on specific criteria. The\nidentified files are then indexed, providing easier access for future analysis.\nFurthermore, the paper demonstrates how data from different studies curated\nwith the framework can be integrated by standardizing and cleaning metadata,\nallowing researchers to extract insights from a large, integrated dataset.\nWhile being designed for TalkBank, the framework can also be adapted to process\ndata from other open-science platforms.", "published": "2023-06-21 22:37:51", "link": "http://arxiv.org/abs/2306.12596v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "A Reference-less Quality Metric for Automatic Speech Recognition via\n  Contrastive-Learning of a Multi-Language Model with Self-Supervision", "abstract": "The common standard for quality evaluation of automatic speech recognition\n(ASR) systems is reference-based metrics such as the Word Error Rate (WER),\ncomputed using manual ground-truth transcriptions that are time-consuming and\nexpensive to obtain. This work proposes a multi-language referenceless quality\nmetric, which allows comparing the performance of different ASR models on a\nspeech dataset without ground truth transcriptions. To estimate the quality of\nASR hypotheses, a pre-trained language model (LM) is fine-tuned with\ncontrastive learning in a self-supervised learning manner. In experiments\nconducted on several unseen test datasets consisting of outputs from top\ncommercial ASR engines in various languages, the proposed referenceless metric\nobtains a much higher correlation with WER scores and their ranks than the\nperplexity metric from the state-of-art multi-lingual LM in all experiments,\nand also reduces WER by more than $7\\%$ when used for ensembling hypotheses.\nThe fine-tuned model and experiments are made available for the\nreproducibility: https://github.com/aixplain/NoRefER", "published": "2023-06-21 21:33:39", "link": "http://arxiv.org/abs/2306.13114v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Interactive Molecular Discovery with Natural Language", "abstract": "Natural language is expected to be a key medium for various human-machine\ninteractions in the era of large language models. When it comes to the\nbiochemistry field, a series of tasks around molecules (e.g., property\nprediction, molecule mining, etc.) are of great significance while having a\nhigh technical threshold. Bridging the molecule expressions in natural language\nand chemical language can not only hugely improve the interpretability and\nreduce the operation difficulty of these tasks, but also fuse the chemical\nknowledge scattered in complementary materials for a deeper comprehension of\nmolecules. Based on these benefits, we propose the conversational molecular\ndesign, a novel task adopting natural language for describing and editing\ntarget molecules. To better accomplish this task, we design ChatMol, a\nknowledgeable and versatile generative pre-trained model, enhanced by injecting\nexperimental property information, molecular spatial knowledge, and the\nassociations between natural and chemical languages into it. Several typical\nsolutions including large language models (e.g., ChatGPT) are evaluated,\nproving the challenge of conversational molecular design and the effectiveness\nof our knowledge enhancement method. Case observations and analysis are\nconducted to provide directions for further exploration of natural-language\ninteraction in molecular discovery.", "published": "2023-06-21 02:05:48", "link": "http://arxiv.org/abs/2306.11976v1", "categories": ["cs.CL", "physics.chem-ph", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "3HAN: A Deep Neural Network for Fake News Detection", "abstract": "The rapid spread of fake news is a serious problem calling for AI solutions.\nWe employ a deep learning based automated detector through a three level\nhierarchical attention network (3HAN) for fast, accurate detection of fake\nnews. 3HAN has three levels, one each for words, sentences, and the headline,\nand constructs a news vector: an effective representation of an input news\narticle, by processing an article in an hierarchical bottom-up manner. The\nheadline is known to be a distinguishing feature of fake news, and furthermore,\nrelatively few words and sentences in an article are more important than the\nrest. 3HAN gives a differential importance to parts of an article, on account\nof its three layers of attention. By experiments on a large real-world data\nset, we observe the effectiveness of 3HAN with an accuracy of 96.77%. Unlike\nsome other deep learning models, 3HAN provides an understandable output through\nthe attention weights given to different parts of an article, which can be\nvisualized through a heatmap to enable further manual fact checking.", "published": "2023-06-21 04:34:27", "link": "http://arxiv.org/abs/2306.12014v1", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Visual-Aware Text-to-Speech", "abstract": "Dynamically synthesizing talking speech that actively responds to a listening\nhead is critical during the face-to-face interaction. For example, the speaker\ncould take advantage of the listener's facial expression to adjust the tones,\nstressed syllables, or pauses. In this work, we present a new visual-aware\ntext-to-speech (VA-TTS) task to synthesize speech conditioned on both textual\ninputs and sequential visual feedback (e.g., nod, smile) of the listener in\nface-to-face communication. Different from traditional text-to-speech, VA-TTS\nhighlights the impact of visual modality. On this newly-minted task, we devise\na baseline model to fuse phoneme linguistic information and listener visual\nsignals for speech synthesis. Extensive experiments on multimodal conversation\ndataset ViCo-X verify our proposal for generating more natural audio with\nscenario-appropriate rhythm and prosody.", "published": "2023-06-21 05:11:39", "link": "http://arxiv.org/abs/2306.12020v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mass-Producing Failures of Multimodal Systems with Language Models", "abstract": "Deployed multimodal systems can fail in ways that evaluators did not\nanticipate. In order to find these failures before deployment, we introduce\nMultiMon, a system that automatically identifies systematic failures --\ngeneralizable, natural-language descriptions of patterns of model failures. To\nuncover systematic failures, MultiMon scrapes a corpus for examples of\nerroneous agreement: inputs that produce the same output, but should not. It\nthen prompts a language model (e.g., GPT-4) to find systematic patterns of\nfailure and describe them in natural language. We use MultiMon to find 14\nsystematic failures (e.g., \"ignores quantifiers\") of the CLIP text-encoder,\neach comprising hundreds of distinct inputs (e.g., \"a shelf with a few/many\nbooks\"). Because CLIP is the backbone for most state-of-the-art multimodal\nsystems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion,\nand others. MultiMon can also steer towards failures relevant to specific use\ncases, such as self-driving cars. We see MultiMon as a step towards evaluation\nthat autonomously explores the long tail of potential system failures. Code for\nMULTIMON is available at https://github.com/tsb0601/MultiMon.", "published": "2023-06-21 08:43:29", "link": "http://arxiv.org/abs/2306.12105v2", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Opening the Black Box: Analyzing Attention Weights and Hidden States in\n  Pre-trained Language Models for Non-language Tasks", "abstract": "Investigating deep learning language models has always been a significant\nresearch area due to the ``black box\" nature of most advanced models. With the\nrecent advancements in pre-trained language models based on transformers and\ntheir increasing integration into daily life, addressing this issue has become\nmore pressing. In order to achieve an explainable AI model, it is essential to\ncomprehend the procedural steps involved and compare them with human thought\nprocesses. Thus, in this paper, we use simple, well-understood non-language\ntasks to explore these models' inner workings. Specifically, we apply a\npre-trained language model to constrained arithmetic problems with hierarchical\nstructure, to analyze their attention weight scores and hidden states. The\ninvestigation reveals promising results, with the model addressing hierarchical\nproblems in a moderately structured manner, similar to human problem-solving\nstrategies. Additionally, by inspecting the attention weights layer by layer,\nwe uncover an unconventional finding that layer 10, rather than the model's\nfinal layer, is the optimal layer to unfreeze for the least parameter-intensive\napproach to fine-tune the model. We support these findings with entropy\nanalysis and token embeddings similarity analysis. The attention analysis\nallows us to hypothesize that the model can generalize to longer sequences in\nListOps dataset, a conclusion later confirmed through testing on sequences\nlonger than those in the training set. Lastly, by utilizing a straightforward\ntask in which the model predicts the winner of a Tic Tac Toe game, we identify\nlimitations in attention analysis, particularly its inability to capture 2D\npatterns.", "published": "2023-06-21 11:48:07", "link": "http://arxiv.org/abs/2306.12198v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition\n  via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning", "abstract": "This paper introduces NoRefER, a novel referenceless quality metric for\nautomatic speech recognition (ASR) systems. Traditional reference-based metrics\nfor evaluating ASR systems require costly ground-truth transcripts. NoRefER\novercomes this limitation by fine-tuning a multilingual language model for\npair-wise ranking ASR hypotheses using contrastive learning with Siamese\nnetwork architecture. The self-supervised NoRefER exploits the known quality\nrelationships between hypotheses from multiple compression levels of an ASR for\nlearning to rank intra-sample hypotheses by quality, which is essential for\nmodel comparisons. The semi-supervised version also uses a referenced dataset\nto improve its inter-sample quality ranking, which is crucial for selecting\npotentially erroneous samples. The results indicate that NoRefER correlates\nhighly with reference-based metrics and their intra-sample ranks, indicating a\nhigh potential for referenceless ASR evaluation or a/b testing.", "published": "2023-06-21 21:26:19", "link": "http://arxiv.org/abs/2306.12577v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards Enriched Controllability for Educational Question Generation", "abstract": "Question Generation (QG) is a task within Natural Language Processing (NLP)\nthat involves automatically generating questions given an input, typically\ncomposed of a text and a target answer. Recent work on QG aims to control the\ntype of generated questions so that they meet educational needs. A remarkable\nexample of controllability in educational QG is the generation of questions\nunderlying certain narrative elements, e.g., causal relationship, outcome\nresolution, or prediction. This study aims to enrich controllability in QG by\nintroducing a new guidance attribute: question explicitness. We propose to\ncontrol the generation of explicit and implicit wh-questions from\nchildren-friendly stories. We show preliminary evidence of controlling QG via\nquestion explicitness alone and simultaneously with another target attribute:\nthe question's narrative element. The code is publicly available at\ngithub.com/bernardoleite/question-generation-control.", "published": "2023-06-21 11:21:08", "link": "http://arxiv.org/abs/2306.14917v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Utilizing Natural Language Processing for Automated Assessment of\n  Classroom Discussion", "abstract": "Rigorous and interactive class discussions that support students to engage in\nhigh-level thinking and reasoning are essential to learning and are a central\ncomponent of most teaching interventions. However, formally assessing\ndiscussion quality 'at scale' is expensive and infeasible for most researchers.\nIn this work, we experimented with various modern natural language processing\n(NLP) techniques to automatically generate rubric scores for individual\ndimensions of classroom text discussion quality. Specifically, we worked on a\ndataset of 90 classroom discussion transcripts consisting of over 18000 turns\nannotated with fine-grained Analyzing Teaching Moves (ATM) codes and focused on\nfour Instructional Quality Assessment (IQA) rubrics. Despite the limited amount\nof data, our work shows encouraging results in some of the rubrics while\nsuggesting that there is room for improvement in the others. We also found that\ncertain NLP approaches work better for certain rubrics.", "published": "2023-06-21 16:45:24", "link": "http://arxiv.org/abs/2306.14918v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Social Reasoning in Language Models with Language Models", "abstract": "As Large Language Models (LLMs) become increasingly integrated into our\neveryday lives, understanding their ability to comprehend human mental states\nbecomes critical for ensuring effective interactions. However, despite the\nrecent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of\nLLMs, the degree to which these models can align with human ToM remains a\nnuanced topic of exploration. This is primarily due to two distinct challenges:\n(1) the presence of inconsistent results from previous evaluations, and (2)\nconcerns surrounding the validity of existing evaluation methodologies. To\naddress these challenges, we present a novel framework for procedurally\ngenerating evaluations with LLMs by populating causal templates. Using our\nframework, we create a new social reasoning benchmark (BigToM) for LLMs which\nconsists of 25 controls and 5,000 model-written evaluations. We find that human\nparticipants rate the quality of our benchmark higher than previous\ncrowd-sourced evaluations and comparable to expert-written evaluations. Using\nBigToM, we evaluate the social reasoning capabilities of a variety of LLMs and\ncompare model performances with human performance. Our results suggest that\nGPT4 has ToM capabilities that mirror human inference patterns, though less\nreliable, while other LLMs struggle.", "published": "2023-06-21 16:42:15", "link": "http://arxiv.org/abs/2306.15448v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Testing of Detection Tools for AI-Generated Text", "abstract": "Recent advances in generative pre-trained transformer large language models\nhave emphasised the potential risks of unfair use of artificial intelligence\n(AI) generated content in an academic environment and intensified efforts in\nsearching for solutions to detect such content. The paper examines the general\nfunctionality of detection tools for artificial intelligence generated text and\nevaluates them based on accuracy and error type analysis. Specifically, the\nstudy seeks to answer research questions about whether existing detection tools\ncan reliably differentiate between human-written text and ChatGPT-generated\ntext, and whether machine translation and content obfuscation techniques affect\nthe detection of AI-generated text. The research covers 12 publicly available\ntools and two commercial systems (Turnitin and PlagiarismCheck) that are widely\nused in the academic setting. The researchers conclude that the available\ndetection tools are neither accurate nor reliable and have a main bias towards\nclassifying the output as human-written rather than detecting AI-generated\ntext. Furthermore, content obfuscation techniques significantly worsen the\nperformance of tools. The study makes several significant contributions. First,\nit summarises up-to-date similar scientific and non-scientific efforts in the\nfield. Second, it presents the result of one of the most comprehensive tests\nconducted so far, based on a rigorous research methodology, an original\ndocument set, and a broad coverage of tools. Third, it discusses the\nimplications and drawbacks of using detection tools for AI-generated text in\nacademic settings.", "published": "2023-06-21 16:29:44", "link": "http://arxiv.org/abs/2306.15666v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.m"], "primary_category": "cs.CL"}
{"title": "Learning When to Trust Which Teacher for Weakly Supervised ASR", "abstract": "Automatic speech recognition (ASR) training can utilize multiple experts as\nteacher models, each trained on a specific domain or accent. Teacher models may\nbe opaque in nature since their architecture may be not be known or their\ntraining cadence is different from that of the student ASR model. Still, the\nstudent models are updated incrementally using the pseudo-labels generated\nindependently by the expert teachers. In this paper, we exploit supervision\nfrom multiple domain experts in training student ASR models. This training\nstrategy is especially useful in scenarios where few or no human transcriptions\nare available. To that end, we propose a Smart-Weighter mechanism that selects\nan appropriate expert based on the input audio, and then trains the student\nmodel in an unsupervised setting. We show the efficacy of our approach using\nLibriSpeech and LibriLight benchmarks and find an improvement of 4 to 25\\% over\nbaselines that uniformly weight all the experts, use a single expert model, or\ncombine experts using ROVER.", "published": "2023-06-21 04:23:26", "link": "http://arxiv.org/abs/2306.12012v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Federated Self-Learning with Weak Supervision for Speech Recognition", "abstract": "Automatic speech recognition (ASR) models with low-footprint are increasingly\nbeing deployed on edge devices for conversational agents, which enhances\nprivacy. We study the problem of federated continual incremental learning for\nrecurrent neural network-transducer (RNN-T) ASR models in the privacy-enhancing\nscheme of learning on-device, without access to ground truth human transcripts\nor machine transcriptions from a stronger ASR model. In particular, we study\nthe performance of a self-learning based scheme, with a paired teacher model\nupdated through an exponential moving average of ASR models. Further, we\npropose using possibly noisy weak-supervision signals such as feedback scores\nand natural language understanding semantics determined from user behavior\nacross multiple turns in a session of interactions with the conversational\nagent. These signals are leveraged in a multi-task policy-gradient training\napproach to improve the performance of self-learning for ASR. Finally, we show\nhow catastrophic forgetting can be mitigated by combining on-device learning\nwith a memory-replay approach using selected historical datasets. These\ninnovations allow for 10% relative improvement in WER on new use cases with\nminimal degradation on other test sets in the absence of strong-supervision\nsignals such as ground-truth transcriptions.", "published": "2023-06-21 04:41:42", "link": "http://arxiv.org/abs/2306.12015v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Multimodal Prototypical Approach for Unsupervised Sound Classification", "abstract": "In the context of environmental sound classification, the adaptability of\nsystems is key: which sound classes are interesting depends on the context and\nthe user's needs. Recent advances in text-to-audio retrieval allow for\nzero-shot audio classification, but performance compared to supervised models\nremains limited. This work proposes a multimodal prototypical approach that\nexploits local audio-text embeddings to provide more relevant answers to audio\nqueries, augmenting the adaptability of sound detection in the wild. We do this\nby first using text to query a nearby community of audio embeddings that best\ncharacterize each query sound, and select the group's centroids as our\nprototypes. Second, we compare unseen audio to these prototypes for\nclassification. We perform multiple ablation studies to understand the impact\nof the embedding models and prompts. Our unsupervised approach improves upon\nthe zero-shot state-of-the-art in three sound recognition benchmarks by an\naverage of 12%.", "published": "2023-06-21 14:29:22", "link": "http://arxiv.org/abs/2306.12300v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Speech Disentanglement for Voice Conversion using Rank Module\n  and Speech Augmentation", "abstract": "Voice Conversion (VC) converts the voice of a source speech to that of a\ntarget while maintaining the source's content. Speech can be mainly decomposed\ninto four components: content, timbre, rhythm and pitch. Unfortunately, most\nrelated works only take into account content and timbre, which results in less\nnatural speech. Some recent works are able to disentangle speech into several\ncomponents, but they require laborious bottleneck tuning or various\nhand-crafted features, each assumed to contain disentangled speech information.\nIn this paper, we propose a VC model that can automatically disentangle speech\ninto four components using only two augmentation functions, without the\nrequirement of multiple hand-crafted features or laborious bottleneck tuning.\nThe proposed model is straightforward yet efficient, and the empirical results\ndemonstrate that our model can achieve a better performance than the baseline,\nregarding disentanglement effectiveness and speech naturalness.", "published": "2023-06-21 13:28:06", "link": "http://arxiv.org/abs/2306.12259v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diffusion Posterior Sampling for Informed Single-Channel Dereverberation", "abstract": "We present in this paper an informed single-channel dereverberation method\nbased on conditional generation with diffusion models. With knowledge of the\nroom impulse response, the anechoic utterance is generated via reverse\ndiffusion using a measurement consistency criterion coupled with a neural\nnetwork that represents the clean speech prior. The proposed approach is\nlargely more robust to measurement noise compared to a state-of-the-art\ninformed single-channel dereverberation method, especially for non-stationary\nnoise. Furthermore, we compare to other blind dereverberation methods using\ndiffusion models and show superiority of the proposed approach for large\nreverberation times. We motivate the presented algorithm by introducing an\nextension for blind dereverberation allowing joint estimation of the room\nimpulse response and anechoic speech. Audio samples and code can be found\nonline (https://uhh.de/inf-sp-derev-dps).", "published": "2023-06-21 14:14:05", "link": "http://arxiv.org/abs/2306.12286v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring the Role of Audio in Video Captioning", "abstract": "Recent focus in video captioning has been on designing architectures that can\nconsume both video and text modalities, and using large-scale video datasets\nwith text transcripts for pre-training, such as HowTo100M. Though these\napproaches have achieved significant improvement, the audio modality is often\nignored in video captioning. In this work, we present an audio-visual\nframework, which aims to fully exploit the potential of the audio modality for\ncaptioning. Instead of relying on text transcripts extracted via automatic\nspeech recognition (ASR), we argue that learning with raw audio signals can be\nmore beneficial, as audio has additional information including acoustic events,\nspeaker identity, etc. Our contributions are twofold. First, we observed that\nthe model overspecializes to the audio modality when pre-training with both\nvideo and audio modality, since the ground truth (i.e., text transcripts) can\nbe solely predicted using audio. We proposed a Modality Balanced Pre-training\n(MBP) loss to mitigate this issue and significantly improve the performance on\ndownstream tasks. Second, we slice and dice different design choices of the\ncross-modal module, which may become an information bottleneck and generate\ninferior results. We proposed new local-global fusion mechanisms to improve\ninformation exchange across audio and video. We demonstrate significant\nimprovements by leveraging the audio modality on four datasets, and even\noutperform the state of the art on some metrics without relying on the text\nmodality as the input.", "published": "2023-06-21 20:54:52", "link": "http://arxiv.org/abs/2306.12559v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Knowledge-based Multimodal Music Similarity", "abstract": "Music similarity is an essential aspect of music retrieval, recommendation\nsystems, and music analysis. Moreover, similarity is of vital interest for\nmusic experts, as it allows studying analogies and influences among composers\nand historical periods. Current approaches to musical similarity rely mainly on\nsymbolic content, which can be expensive to produce and is not always readily\navailable. Conversely, approaches using audio signals typically fail to provide\nany insight about the reasons behind the observed similarity. This research\naddresses the limitations of current approaches by focusing on the study of\nmusical similarity using both symbolic and audio content. The aim of this\nresearch is to develop a fully explainable and interpretable system that can\nprovide end-users with more control and understanding of music similarity and\nclassification systems.", "published": "2023-06-21 13:12:12", "link": "http://arxiv.org/abs/2306.12249v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
