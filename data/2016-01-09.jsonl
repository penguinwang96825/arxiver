{"title": "Empirical Gaussian priors for cross-lingual transfer learning", "abstract": "Sequence model learning algorithms typically maximize log-likelihood minus\nthe norm of the model (or minimize Hamming loss + norm). In cross-lingual\npart-of-speech (POS) tagging, our target language training data consists of\nsequences of sentences with word-by-word labels projected from translations in\n$k$ languages for which we have labeled data, via word alignments. Our training\ndata is therefore very noisy, and if Rademacher complexity is high, learning\nalgorithms are prone to overfit. Norm-based regularization assumes a constant\nwidth and zero mean prior. We instead propose to use the $k$ source language\nmodels to estimate the parameters of a Gaussian prior for learning new POS\ntaggers. This leads to significantly better performance in multi-source\ntransfer set-ups. We also present a drop-out version that injects (empirical)\nGaussian noise during online learning. Finally, we note that using empirical\nGaussian priors leads to much lower Rademacher complexity, and is superior to\noptimally weighted model interpolation.", "published": "2016-01-09 23:34:05", "link": "http://arxiv.org/abs/1601.02166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
