{"title": "Automatic Inference of Minimalist Grammars using an SMT-Solver", "abstract": "We introduce (1) a novel parser for Minimalist Grammars (MG), encoded as a\nsystem of first-order logic formulae that may be evaluated using an SMT-solver,\nand (2) a novel procedure for inferring Minimalist Grammars using this parser.\nThe input to this procedure is a sequence of sentences that have been annotated\nwith syntactic relations such as semantic role labels (connecting arguments to\npredicates) and subject-verb agreement. The output of this procedure is a set\nof minimalist grammars, each of which is able to parse the sentences in the\ninput sequence such that the parse for a sentence has the same syntactic\nrelations as those specified in the annotation for that sentence. We applied\nthis procedure to a set of sentences annotated with syntactic relations and\nevaluated the inferred grammars using cost functions inspired by the Minimum\nDescription Length principle and the Subset principle. Inferred grammars that\nwere optimal with respect to certain combinations of these cost functions were\nfound to align with contemporary theories of syntax.", "published": "2019-05-08 02:12:18", "link": "http://arxiv.org/abs/1905.02869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word\n  Representations", "abstract": "Syntax has been demonstrated highly effective in neural machine translation\n(NMT). Previous NMT models integrate syntax by representing 1-best tree outputs\nfrom a well-trained parsing system, e.g., the representative Tree-RNN and\nTree-Linearization methods, which may suffer from error propagation. In this\nwork, we propose a novel method to integrate source-side syntax implicitly for\nNMT. The basic idea is to use the intermediate hidden representations of a\nwell-trained end-to-end dependency parser, which are referred to as\nsyntax-aware word representations (SAWRs). Then, we simply concatenate such\nSAWRs with ordinary word embeddings to enhance basic NMT models. The method can\nbe straightforwardly integrated into the widely-used sequence-to-sequence\n(Seq2Seq) NMT models. We start with a representative RNN-based Seq2Seq baseline\nsystem, and test the effectiveness of our proposed method on two benchmark\ndatasets of the Chinese-English and English-Vietnamese translation tasks,\nrespectively. Experimental results show that the proposed approach is able to\nbring significant BLEU score improvements on the two datasets compared with the\nbaseline, 1.74 points for Chinese-English translation and 0.80 point for\nEnglish-Vietnamese translation, respectively. In addition, the approach also\noutperforms the explicit Tree-RNN and Tree-Linearization methods.", "published": "2019-05-08 02:56:43", "link": "http://arxiv.org/abs/1905.02878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Feasibility of Automated Detection of Allusive Text Reuse", "abstract": "The detection of allusive text reuse is particularly challenging due to the\nsparse evidence on which allusive references rely---commonly based on none or\nvery few shared words. Arguably, lexical semantics can be resorted to since\nuncovering semantic relations between words has the potential to increase the\nsupport underlying the allusion and alleviate the lexical sparsity. A further\nobstacle is the lack of evaluation benchmark corpora, largely due to the highly\ninterpretative character of the annotation process. In the present paper, we\naim to elucidate the feasibility of automated allusion detection. We approach\nthe matter from an Information Retrieval perspective in which referencing texts\nact as queries and referenced texts as relevant documents to be retrieved, and\nestimate the difficulty of benchmark corpus compilation by a novel\ninter-annotator agreement study on query segmentation. Furthermore, we\ninvestigate to what extent the integration of lexical semantic information\nderived from distributional models and ontologies can aid retrieving cases of\nallusive reuse. The results show that (i) despite low agreement scores, using\nmanual queries considerably improves retrieval performance with respect to a\nwindowing approach, and that (ii) retrieval performance can be moderately\nboosted with distributional semantics.", "published": "2019-05-08 09:24:48", "link": "http://arxiv.org/abs/1905.02973v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Language Model Pre-training for Natural Language Understanding\n  and Generation", "abstract": "This paper presents a new Unified pre-trained Language Model (UniLM) that can\nbe fine-tuned for both natural language understanding and generation tasks. The\nmodel is pre-trained using three types of language modeling tasks:\nunidirectional, bidirectional, and sequence-to-sequence prediction. The unified\nmodeling is achieved by employing a shared Transformer network and utilizing\nspecific self-attention masks to control what context the prediction conditions\non. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\nand CoQA question answering tasks. Moreover, UniLM achieves new\nstate-of-the-art results on five natural language generation datasets,\nincluding improving the CNN/DailyMail abstractive summarization ROUGE-L to\n40.51 (2.04 absolute improvement), the Gigaword abstractive summarization\nROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question\nanswering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question\ngeneration BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7\ndocument-grounded dialog response generation NIST-4 to 2.67 (human performance\nis 2.65). The code and pre-trained models are available at\nhttps://github.com/microsoft/unilm.", "published": "2019-05-08 16:33:51", "link": "http://arxiv.org/abs/1905.03197v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FAQ Retrieval using Query-Question Similarity and BERT-Based\n  Query-Answer Relevance", "abstract": "Frequently Asked Question (FAQ) retrieval is an important task where the\nobjective is to retrieve an appropriate Question-Answer (QA) pair from a\ndatabase based on a user's query. We propose a FAQ retrieval system that\nconsiders the similarity between a user's query and a question as well as the\nrelevance between the query and an answer. Although a common approach to FAQ\nretrieval is to construct labeled data for training, it takes annotation costs.\nTherefore, we use a traditional unsupervised information retrieval system to\ncalculate the similarity between the query and question. On the other hand, the\nrelevance between the query and answer can be learned by using QA pairs in a\nFAQ database. The recently-proposed BERT model is used for the relevance\ncalculation. Since the number of QA pairs in FAQ page is not enough to train a\nmodel, we cope with this issue by leveraging FAQ sets that are similar to the\none in question. We evaluate our approach on two datasets. The first one is\nlocalgovFAQ, a dataset we construct in a Japanese administrative municipality\ndomain. The second is StackExchange dataset, which is the public dataset in\nEnglish. We demonstrate that our proposed method outperforms baseline methods\non these datasets.", "published": "2019-05-08 00:33:37", "link": "http://arxiv.org/abs/1905.02851v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ShapeGlot: Learning Language for Shape Differentiation", "abstract": "In this work we explore how fine-grained differences between the shapes of\ncommon objects are expressed in language, grounded on images and 3D models of\nthe objects. We first build a large scale, carefully controlled dataset of\nhuman utterances that each refers to a 2D rendering of a 3D CAD model so as to\ndistinguish it from a set of shape-wise similar alternatives. Using this\ndataset, we develop neural language understanding (listening) and production\n(speaking) models that vary in their grounding (pure 3D forms via point-clouds\nvs. rendered 2D images), the degree of pragmatic reasoning captured (e.g.\nspeakers that reason about a listener or not), and the neural architecture\n(e.g. with or without attention). We find models that perform well with both\nsynthetic and human partners, and with held out utterances and objects. We also\nfind that these models are amenable to zero-shot transfer learning to novel\nobject classes (e.g. transfer from training on chairs to testing on lamps), as\nwell as to real-world images drawn from furniture catalogs. Lesion studies\nindicate that the neural listeners depend heavily on part-related words and\nassociate these words correctly with visual parts of objects (without any\nexplicit network training on object parts), and that transfer to novel classes\nis most successful when known part-words are available. This work illustrates a\npractical approach to language grounding, and provides a case study in the\nrelationship between object shape and linguistic structure when it comes to\nobject differentiation.", "published": "2019-05-08 06:01:33", "link": "http://arxiv.org/abs/1905.02925v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Emotion Recognition in Conversation: Research Challenges, Datasets, and\n  Recent Advances", "abstract": "Emotion is intrinsic to humans and consequently emotion understanding is a\nkey part of human-like artificial intelligence (AI). Emotion recognition in\nconversation (ERC) is becoming increasingly popular as a new research frontier\nin natural language processing (NLP) due to its ability to mine opinions from\nthe plethora of publicly available conversational data in platforms such as\nFacebook, Youtube, Reddit, Twitter, and others. Moreover, it has potential\napplications in health-care systems (as a tool for psychological analysis),\neducation (understanding student frustration) and more. Additionally, ERC is\nalso extremely important for generating emotion-aware dialogues that require an\nunderstanding of the user's emotions. Catering to these needs calls for\neffective and scalable conversational emotion-recognition algorithms. However,\nit is a strenuous problem to solve because of several research challenges. In\nthis paper, we discuss these challenges and shed light on the recent research\nin this field. We also describe the drawbacks of these approaches and discuss\nthe reasons why they fail to successfully overcome the research challenges in\nERC.", "published": "2019-05-08 07:46:30", "link": "http://arxiv.org/abs/1905.02947v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RWTH ASR Systems for LibriSpeech: Hybrid vs Attention -- w/o Data\n  Augmentation", "abstract": "We present state-of-the-art automatic speech recognition (ASR) systems\nemploying a standard hybrid DNN/HMM architecture compared to an attention-based\nencoder-decoder design for the LibriSpeech task. Detailed descriptions of the\nsystem development, including model design, pretraining schemes, training\nschedules, and optimization approaches are provided for both system\narchitectures. Both hybrid DNN/HMM and attention-based systems employ\nbi-directional LSTMs for acoustic modeling/encoding. For language modeling, we\nemploy both LSTM and Transformer based architectures. All our systems are built\nusing RWTHs open-source toolkits RASR and RETURNN. To the best knowledge of the\nauthors, the results obtained when training on the full LibriSpeech training\nset, are the best published currently, both for the hybrid DNN/HMM and the\nattention-based systems. Our single hybrid system even outperforms previous\nresults obtained from combining eight single systems. Our comparison shows that\non the LibriSpeech 960h task, the hybrid DNN/HMM system outperforms the\nattention-based system by 15% relative on the clean and 40% relative on the\nother test sets in terms of word error rate. Moreover, experiments on a reduced\n100h-subset of the LibriSpeech training corpus even show a more pronounced\nmargin between the hybrid DNN/HMM and attention-based architectures.", "published": "2019-05-08 13:57:28", "link": "http://arxiv.org/abs/1905.03072v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Confirmatory Factor Analysis -- A Case study", "abstract": "Confirmatory Factor Analysis (CFA) is a particular form of factor analysis,\nmost commonly used in social research. In confirmatory factor analysis, the\nresearcher first develops a hypothesis about what factors they believe are\nunderlying the used measures and may impose constraints on the model based on\nthese a priori hypotheses. For example, if two factors are accounting for the\ncovariance in the measures, and these factors are unrelated to one another, we\ncan create a model where the correlation between factor X and factor Y is set\nto zero. Measures could then be obtained to assess how well the fitted model\ncaptured the covariance between all the items or measures in the model. Thus,\nif the results of statistical tests of the model fit indicate a poor fit, the\nmodel will be rejected. If the fit is weak, it may be due to a variety of\nreasons. We propose to introduce state of the art techniques to do CFA in R\nlanguage. Then, we propose to do some examples of CFA with R and some datasets,\nrevealing several scenarios where CFA is relevant.", "published": "2019-05-08 19:27:22", "link": "http://arxiv.org/abs/1905.05598v1", "categories": ["stat.AP", "cs.CL", "cs.IR"], "primary_category": "stat.AP"}
{"title": "Semi-Supervised Speech Emotion Recognition with Ladder Networks", "abstract": "Speech emotion recognition (SER) systems find applications in various fields\nsuch as healthcare, education, and security and defense. A major drawback of\nthese systems is their lack of generalization across different conditions. This\nproblem can be solved by training models on large amounts of labeled data from\nthe target domain, which is expensive and time-consuming. Another approach is\nto increase the generalization of the models. An effective way to achieve this\ngoal is by regularizing the models through multitask learning (MTL), where\nauxiliary tasks are learned along with the primary task. These methods often\nrequire the use of labeled data which is computationally expensive to collect\nfor emotion recognition (gender, speaker identity, age or other emotional\ndescriptors). This study proposes the use of ladder networks for emotion\nrecognition, which utilizes an unsupervised auxiliary task. The primary task is\na regression problem to predict emotional attributes. The auxiliary task is the\nreconstruction of intermediate feature representations using a denoising\nautoencoder. This auxiliary task does not require labels so it is possible to\ntrain the framework in a semi-supervised fashion with abundant unlabeled data\nfrom the target domain. This study shows that the proposed approach creates a\npowerful framework for SER, achieving superior performance than fully\nsupervised single-task learning (STL) and MTL baselines. The approach is\nimplemented with several acoustic features, showing that ladder networks\ngeneralize significantly better in cross-corpus settings. Compared to the STL\nbaselines, the proposed approach achieves relative gains in concordance\ncorrelation coefficient (CCC) between 3.0% and 3.5% for within corpus\nevaluations, and between 16.1% and 74.1% for cross corpus evaluations,\nhighlighting the power of the architecture.", "published": "2019-05-08 05:37:28", "link": "http://arxiv.org/abs/1905.02921v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On the representation of speech and music", "abstract": "In most automatic speech recognition (ASR) systems, the audio signal is\nprocessed to produce a time series of sensor measurements (e.g., filterbank\noutputs). This time series encodes semantic information in a speaker-dependent\nway. An earlier paper showed how to use the sequence of sensor measurements to\nderive an \"inner\" time series that is unaffected by any previous invertible\ntransformation of the sensor measurements. The current paper considers two or\nmore speakers, who mimic one another in the following sense: when they say the\nsame words, they produce sensor states that are invertibly mapped onto one\nanother. It follows that the inner time series of their utterances must be the\nsame when they say the same words. In other words, the inner time series\nencodes their speech in a manner that is speaker-independent. Consequently, the\nASR training process can be simplified by collecting and labelling the inner\ntime series of the utterances of just one speaker, instead of training on the\nsensor time series of the utterances of a large variety of speakers. A similar\nargument suggests that the inner time series of music is\ninstrument-independent. This is demonstrated in experiments on monophonic\nelectronic music.", "published": "2019-05-08 18:11:50", "link": "http://arxiv.org/abs/1905.03278v1", "categories": ["cs.SD", "eess.AS", "stat.ME"], "primary_category": "cs.SD"}
{"title": "Universal Sound Separation", "abstract": "Recent deep learning approaches have achieved impressive performance on\nspeech enhancement and separation tasks. However, these approaches have not\nbeen investigated for separating mixtures of arbitrary sounds of different\ntypes, a task we refer to as universal sound separation, and it is unknown how\nperformance on speech tasks carries over to non-speech tasks. To study this\nquestion, we develop a dataset of mixtures containing arbitrary sounds, and use\nit to investigate the space of mask-based separation architectures, varying\nboth the overall network architecture and the framewise analysis-synthesis\nbasis for signal transformations. These network architectures include\nconvolutional long short-term memory networks and time-dilated convolution\nstacks inspired by the recent success of time-domain enhancement networks like\nConvTasNet. For the latter architecture, we also propose novel modifications\nthat further improve separation performance. In terms of the framewise\nanalysis-synthesis basis, we explore both a short-time Fourier transform (STFT)\nand a learnable basis, as used in ConvTasNet. For both of these bases, we also\nexamine the effect of window size. In particular, for STFTs, we find that\nlonger windows (25-50 ms) work best for speech/non-speech separation, while\nshorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases,\nshorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal\nsound separation, STFTs outperform learnable bases. Our best methods produce an\nimprovement in scale-invariant signal-to-distortion ratio of over 13 dB for\nspeech/non-speech separation and close to 10 dB for universal sound separation.", "published": "2019-05-08 20:48:49", "link": "http://arxiv.org/abs/1905.03330v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
