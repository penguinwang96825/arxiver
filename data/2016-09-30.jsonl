{"title": "Controlling Output Length in Neural Encoder-Decoders", "abstract": "Neural encoder-decoder models have shown great success in many sequence\ngeneration tasks. However, previous work has not investigated situations in\nwhich we would like to control the length of encoder-decoder outputs. This\ncapability is crucial for applications such as text summarization, in which we\nhave to generate concise summaries with a desired length. In this paper, we\npropose methods for controlling the output sequence length for neural\nencoder-decoder models: two decoding-based methods and two learning-based\nmethods. Results show that our learning-based methods have the capability to\ncontrol length without degrading summary quality in a summarization task.", "published": "2016-09-30 00:01:27", "link": "http://arxiv.org/abs/1609.09552v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Referential Uncertainty and Word Learning in High-dimensional,\n  Continuous Meaning Spaces", "abstract": "This paper discusses lexicon word learning in high-dimensional meaning spaces\nfrom the viewpoint of referential uncertainty. We investigate various\nstate-of-the-art Machine Learning algorithms and discuss the impact of scaling,\nrepresentation and meaning space structure. We demonstrate that current Machine\nLearning techniques successfully deal with high-dimensional meaning spaces. In\nparticular, we show that exponentially increasing dimensions linearly impact\nlearner performance and that referential uncertainty from word sensitivity has\nno impact.", "published": "2016-09-30 03:20:52", "link": "http://arxiv.org/abs/1609.09580v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Language Change in Historical Corpora: The Case of Portuguese", "abstract": "This paper presents a number of experiments to model changes in a historical\nPortuguese corpus composed of literary texts for the purpose of temporal text\nclassification. Algorithms were trained to classify texts with respect to their\npublication date taking into account lexical variation represented as word\nn-grams, and morphosyntactic variation represented by part-of-speech (POS)\ndistribution. We report results of 99.8% accuracy using word unigram features\nwith a Support Vector Machines classifier to predict the publication date of\ndocuments in time intervals of both one century and half a century. A feature\nanalysis is performed to investigate the most informative features for this\ntask and how they are linked to language change.", "published": "2016-09-30 20:57:01", "link": "http://arxiv.org/abs/1610.00030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discriminating Similar Languages: Evaluations and Explorations", "abstract": "We present an analysis of the performance of machine learning classifiers on\ndiscriminating between similar languages and language varieties. We carried out\na number of experiments using the results of the two editions of the\nDiscriminating between Similar Languages (DSL) shared task. We investigate the\nprogress made between the two tasks, estimate an upper bound on possible\nperformance using ensemble and oracle combination, and provide learning curves\nto help us understand which languages are more challenging. A number of\ndifficult sentences are identified and investigated further with human\nannotation.", "published": "2016-09-30 20:57:52", "link": "http://arxiv.org/abs/1610.00031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks", "abstract": "In this paper, a neural network based real-time speech recognition (SR)\nsystem is developed using an FPGA for very low-power operation. The implemented\nsystem employs two recurrent neural networks (RNNs); one is a\nspeech-to-character RNN for acoustic modeling (AM) and the other is for\ncharacter-level language modeling (LM). The system also employs a statistical\nword-level LM to improve the recognition accuracy. The results of the AM, the\ncharacter-level LM, and the word-level LM are combined using a fairly simple\nN-best search algorithm instead of the hidden Markov model (HMM) based network.\nThe RNNs are implemented using massively parallel processing elements (PEs) for\nlow latency and high throughput. The weights are quantized to 6 bits to store\nall of them in the on-chip memory of an FPGA. The proposed algorithm is\nimplemented on a Xilinx XC7Z045, and the system can operate much faster than\nreal-time.", "published": "2016-09-30 10:44:32", "link": "http://arxiv.org/abs/1610.00552v1", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
