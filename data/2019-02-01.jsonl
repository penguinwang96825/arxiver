{"title": "DREAM: A Challenge Dataset and Models for Dialogue-Based Reading\n  Comprehension", "abstract": "We present DREAM, the first dialogue-based multiple-choice reading\ncomprehension dataset. Collected from English-as-a-foreign-language\nexaminations designed by human experts to evaluate the comprehension level of\nChinese learners of English, our dataset contains 10,197 multiple-choice\nquestions for 6,444 dialogues. In contrast to existing reading comprehension\ndatasets, DREAM is the first to focus on in-depth multi-turn multi-party\ndialogue understanding. DREAM is likely to present significant challenges for\nexisting reading comprehension systems: 84% of answers are non-extractive, 85%\nof questions require reasoning beyond a single sentence, and 34% of questions\nalso involve commonsense knowledge.\n  We apply several popular neural reading comprehension models that primarily\nexploit surface information within the text and find them to, at best, just\nbarely outperform a rule-based approach. We next investigate the effects of\nincorporating dialogue structure and different kinds of general world knowledge\ninto both rule-based and (neural and non-neural) machine learning-based reading\ncomprehension models. Experimental results on the DREAM dataset show the\neffectiveness of dialogue structure and general world knowledge. DREAM will be\navailable at https://dataset.org/dream/.", "published": "2019-02-01 03:43:51", "link": "http://arxiv.org/abs/1902.00164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Regularization-based Algorithm for Learning Cross-Domain Word\n  Embeddings", "abstract": "Learning word embeddings has received a significant amount of attention\nrecently. Often, word embeddings are learned in an unsupervised manner from a\nlarge collection of text. The genre of the text typically plays an important\nrole in the effectiveness of the resulting embeddings. How to effectively train\nword embedding models using data from different domains remains a problem that\nis underexplored. In this paper, we present a simple yet effective method for\nlearning word embeddings based on text from different domains. We demonstrate\nthe effectiveness of our approach through extensive experiments on various\ndown-stream NLP tasks.", "published": "2019-02-01 05:02:59", "link": "http://arxiv.org/abs/1902.00184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual Transfer for NER", "abstract": "In cross-lingual transfer, NLP models over one or more source languages are\napplied to a low-resource target language. While most prior work has used a\nsingle source model or a few carefully selected models, here we consider a\n`massive' setting with many such models. This setting raises the problem of\npoor transfer, particularly from distant languages. We propose two techniques\nfor modulating the transfer, suitable for zero-shot or few-shot learning,\nrespectively. Evaluating on named entity recognition, we show that our\ntechniques are much more effective than strong baselines, including standard\nensembling, and our unsupervised method rivals oracle selection of the single\nbest individual model.", "published": "2019-02-01 05:49:45", "link": "http://arxiv.org/abs/1902.00193v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Entity Linking with Deep Reinforcement Learning", "abstract": "Entity linking is the task of aligning mentions to corresponding entities in\na given knowledge base. Previous studies have highlighted the necessity for\nentity linking systems to capture the global coherence. However, there are two\ncommon weaknesses in previous global models. First, most of them calculate the\npairwise scores between all candidate entities and select the most relevant\ngroup of entities as the final result. In this process, the consistency among\nwrong entities as well as that among right ones are involved, which may\nintroduce noise data and increase the model complexity. Second, the cues of\npreviously disambiguated entities, which could contribute to the disambiguation\nof the subsequent mentions, are usually ignored by previous models. To address\nthese problems, we convert the global linking into a sequence decision problem\nand propose a reinforcement learning model which makes decisions from a global\nperspective. Our model makes full use of the previous referred entities and\nexplores the long-term influence of current selection on subsequent decisions.\nWe conduct experiments on different types of datasets, the results show that\nour model outperforms state-of-the-art systems and has better generalization\nperformance.", "published": "2019-02-01 13:58:37", "link": "http://arxiv.org/abs/1902.00330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "tax2vec: Constructing Interpretable Features from Taxonomies for Short\n  Text Classification", "abstract": "The use of background knowledge is largely unexploited in text classification\ntasks. This paper explores word taxonomies as means for constructing new\nsemantic features, which may improve the performance and robustness of the\nlearned classifiers. We propose tax2vec, a parallel algorithm for constructing\ntaxonomy-based features, and demonstrate its use on six short text\nclassification problems: prediction of gender, personality type, age, news\ntopics, drug side effects and drug effectiveness. The constructed semantic\nfeatures, in combination with fast linear classifiers, tested against strong\nbaselines such as hierarchical attention neural networks, achieves comparable\nclassification results on short text documents. The algorithm's performance is\nalso tested in a few-shot learning setting, indicating that the inclusion of\nsemantic features can improve the performance in data-scarce situations. The\ntax2vec capability to extract corpus-specific semantic keywords is also\ndemonstrated. Finally, we investigate the semantic space of potential features,\nwhere we observe a similarity with the well known Zipf's law.", "published": "2019-02-01 16:23:17", "link": "http://arxiv.org/abs/1902.00438v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human acceptability judgements for extractive sentence compression", "abstract": "Recent approaches to English-language sentence compression rely on parallel\ncorpora consisting of sentence-compression pairs. However, a sentence may be\nshortened in many different ways, which each might be suited to the needs of a\nparticular application. Therefore, in this work, we collect and model\ncrowdsourced judgements of the acceptability of many possible sentence\nshortenings. We then show how a model of such judgements can be used to support\na flexible approach to the compression task. We release our model and dataset\nfor future work.", "published": "2019-02-01 18:22:33", "link": "http://arxiv.org/abs/1902.00489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining the Presence of Gender Bias in Customer Reviews Using Word\n  Embedding", "abstract": "Humans have entered the age of algorithms. Each minute, algorithms shape\ncountless preferences from suggesting a product to a potential life partner. In\nthe marketplace algorithms are trained to learn consumer preferences from\ncustomer reviews because user-generated reviews are considered the voice of\ncustomers and a valuable source of information to firms. Insights mined from\nreviews play an indispensable role in several business activities ranging from\nproduct recommendation, targeted advertising, promotions, segmentation etc. In\nthis research, we question whether reviews might hold stereotypic gender bias\nthat algorithms learn and propagate Utilizing data from millions of\nobservations and a word embedding approach, GloVe, we show that algorithms\ndesigned to learn from human language output also learn gender bias. We also\nexamine why such biases occur: whether the bias is caused because of a negative\nbias against females or a positive bias for males. We examine the impact of\ngender bias in reviews on choice and conclude with policy implications for\nfemale consumers, especially when they are unaware of the bias, and the ethical\nimplications for firms.", "published": "2019-02-01 18:36:09", "link": "http://arxiv.org/abs/1902.00496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong\n  Baselines, Comparative Analyses, and Some Misconceptions", "abstract": "Cross-lingual word embeddings (CLEs) enable multilingual modeling of meaning\nand facilitate cross-lingual transfer of NLP models. Despite their ubiquitous\nusage in downstream tasks, recent increasingly popular projection-based CLE\nmodels are almost exclusively evaluated on a single task only: bilingual\nlexicon induction (BLI). Even BLI evaluations vary greatly, hindering our\nability to correctly interpret performance and properties of different CLE\nmodels. In this work, we make the first step towards a comprehensive evaluation\nof cross-lingual word embeddings. We thoroughly evaluate both supervised and\nunsupervised CLE models on a large number of language pairs in the BLI task and\nthree downstream tasks, providing new insights concerning the ability of\ncutting-edge CLE models to support cross-lingual NLP. We empirically\ndemonstrate that the performance of CLE models largely depends on the task at\nhand and that optimizing CLE models for BLI can result in deteriorated\ndownstream performance. We indicate the most robust supervised and unsupervised\nCLE models and emphasize the need to reassess existing baselines, which still\ndisplay competitive performance across the board. We hope that our work will\ncatalyze further work on CLE evaluation and model analysis.", "published": "2019-02-01 18:59:27", "link": "http://arxiv.org/abs/1902.00508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Generating Long and Coherent Text with Multi-Level Latent\n  Variable Models", "abstract": "Variational autoencoders (VAEs) have received much attention recently as an\nend-to-end architecture for text generation with latent variables. In this\npaper, we investigate several multi-level structures to learn a VAE model to\ngenerate long, and coherent text. In particular, we use a hierarchy of\nstochastic layers between the encoder and decoder networks to generate more\ninformative latent codes. We also investigate a multi-level decoder structure\nto learn a coherent long-term structure by generating intermediate sentence\nrepresentations as high-level plan vectors. Empirical results demonstrate that\na multi-level VAE model produces more coherent and less repetitive long text\ncompared to the standard VAE models and can further mitigate the\nposterior-collapse issue.", "published": "2019-02-01 02:42:55", "link": "http://arxiv.org/abs/1902.00154v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog", "abstract": "This paper presents a new model for visual dialog, Recurrent Dual Attention\nNetwork (ReDAN), using multi-step reasoning to answer a series of questions\nabout an image. In each question-answering turn of a dialog, ReDAN infers the\nanswer progressively through multiple reasoning steps. In each step of the\nreasoning process, the semantic representation of the question is updated based\non the image and the previous dialog history, and the recurrently-refined\nrepresentation is used for further reasoning in the subsequent step. On the\nVisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art\nof 64.47% NDCG score. Visualization on the reasoning process further\ndemonstrates that ReDAN can locate context-relevant visual and textual clues\nvia iterative refinement, which can lead to the correct answer step-by-step.", "published": "2019-02-01 22:48:26", "link": "http://arxiv.org/abs/1902.00579v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Dating Documents using Graph Convolution Networks", "abstract": "Document date is essential for many important tasks, such as document\nretrieval, summarization, event detection, etc. While existing approaches for\nthese tasks assume accurate knowledge of the document date, this is not always\navailable, especially for arbitrary documents from the Web. Document Dating is\na challenging problem which requires inference over the temporal structure of\nthe document. Prior document dating systems have largely relied on handcrafted\nfeatures while ignoring such document internal structures. In this paper, we\npropose NeuralDater, a Graph Convolutional Network (GCN) based document dating\napproach which jointly exploits syntactic and temporal graph structures of\ndocument in a principled way. To the best of our knowledge, this is the first\napplication of deep learning for the problem of document dating. Through\nextensive experiments on real-world datasets, we find that NeuralDater\nsignificantly outperforms state-of-the-art baseline by 19% absolute (45%\nrelative) accuracy points.", "published": "2019-02-01 04:30:42", "link": "http://arxiv.org/abs/1902.00175v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring attention mechanism for acoustic-based classification of\n  speech utterances into system-directed and non-system-directed", "abstract": "Voice controlled virtual assistants (VAs) are now available in smartphones,\ncars, and standalone devices in homes. In most cases, the user needs to first\n\"wake-up\" the VA by saying a particular word/phrase every time he or she wants\nthe VA to do something. Eliminating the need for saying the wake-up word for\nevery interaction could improve the user experience. This would require the VA\nto have the capability to detect the speech that is being directed at it and\nrespond accordingly. In other words, the challenge is to distinguish between\nsystem-directed and non-system-directed speech utterances. In this paper, we\npresent a number of neural network architectures for tackling this\nclassification problem based on using only acoustic features. These\narchitectures are based on using convolutional, recurrent and feed-forward\nlayers. In addition, we investigate the use of an attention mechanism applied\nto the output of the convolutional and the recurrent layers. It is shown that\nincorporating the proposed attention mechanism into the models always leads to\nsignificant improvement in classification accuracy. The best model achieved\nequal error rates of 16.25 and 15.62 percents on two distinct realistic\ndatasets.", "published": "2019-02-01 21:48:45", "link": "http://arxiv.org/abs/1902.00570v1", "categories": ["cs.HC", "cs.CL", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Multi-layered Cepstrum for Instantaneous Frequency Estimation", "abstract": "We propose the multi-layered cepstrum (MLC) method to estimate multiple\nfundamental frequencies (MF0) of a signal under challenging contamination such\nas high-pass filter noise. Taking the operation of cepstrum (i.e., Fourier\ntransform, filtering, and nonlinear activation) recursively, MLC is shown as an\nefficient method to enhance MF0 saliency in a step-by-step manner. Evaluation\non a real-world polyphonic music dataset under both normal and low-fidelity\nconditions demonstrates the potential of MLC.", "published": "2019-02-01 20:03:15", "link": "http://arxiv.org/abs/1902.00539v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
