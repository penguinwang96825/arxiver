{"title": "On Zero-Shot Counterspeech Generation by LLMs", "abstract": "With the emergence of numerous Large Language Models (LLM), the usage of such\nmodels in various Natural Language Processing (NLP) applications is increasing\nextensively. Counterspeech generation is one such key task where efforts are\nmade to develop generative models by fine-tuning LLMs with hatespeech -\ncounterspeech pairs, but none of these attempts explores the intrinsic\nproperties of large language models in zero-shot settings. In this work, we\npresent a comprehensive analysis of the performances of four LLMs namely GPT-2,\nDialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech\ngeneration, which is the first of its kind. For GPT-2 and DialoGPT, we further\ninvestigate the deviation in performance with respect to the sizes (small,\nmedium, large) of the models. On the other hand, we propose three different\nprompting strategies for generating different types of counterspeech and\nanalyse the impact of such strategies on the performance of the models. Our\nanalysis shows that there is an improvement in generation quality for two\ndatasets (17%), however the toxicity increase (25%) with increase in model\nsize. Considering type of model, GPT-2 and FlanT5 models are significantly\nbetter in terms of counterspeech quality but also have high toxicity as\ncompared to DialoGPT. ChatGPT are much better at generating counter speech than\nother models across all metrics. In terms of prompting, we find that our\nproposed strategies help in improving counter speech generation across all the\nmodels.", "published": "2024-03-22 04:13:10", "link": "http://arxiv.org/abs/2403.14938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of\n  Chain-of-Thoughts", "abstract": "Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 -\nwhich provides a dataset of puzzles for testing natural language understanding.\nWe employ large language models (LLMs) to solve this task through several\nprompting techniques. Zero-shot and few-shot prompting generate reasonably good\nresults when tested with proprietary LLMs, compared to the open-source models.\nWe obtain further improved results with chain-of-thought prompting, an\niterative prompting method that breaks down the reasoning process step-by-step.\nWe obtain our best results by utilizing an ensemble of chain-of-thought\nprompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle\nsubtask. The strong performance of prompted LLMs demonstrates their capability\nfor complex reasoning when provided with a decomposition of the thought\nprocess. Our work sheds light on how step-wise explanatory prompts can unlock\nmore of the knowledge encoded in the parameters of large models.", "published": "2024-03-22 06:31:49", "link": "http://arxiv.org/abs/2403.14982v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Risk and Response in Large Language Models: Evaluating Key Threat\n  Categories", "abstract": "This paper explores the pressing issue of risk assessment in Large Language\nModels (LLMs) as they become increasingly prevalent in various applications.\nFocusing on how reward models, which are designed to fine-tune pretrained LLMs\nto align with human values, perceive and categorize different types of risks,\nwe delve into the challenges posed by the subjective nature of preference-based\ntraining data. By utilizing the Anthropic Red-team dataset, we analyze major\nrisk categories, including Information Hazards, Malicious Uses, and\nDiscrimination/Hateful content. Our findings indicate that LLMs tend to\nconsider Information Hazards less harmful, a finding confirmed by a specially\ndeveloped regression model. Additionally, our analysis shows that LLMs respond\nless stringently to Information Hazards compared to other risks. The study\nfurther reveals a significant vulnerability of LLMs to jailbreaking attacks in\nInformation Hazard scenarios, highlighting a critical security concern in LLM\nrisk assessment and emphasizing the need for improved AI safety measures.", "published": "2024-03-22 06:46:40", "link": "http://arxiv.org/abs/2403.14988v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MasonTigers at SemEval-2024 Task 8: Performance Analysis of\n  Transformer-based Models on Machine-Generated Text Detection", "abstract": "This paper presents the MasonTigers entry to the SemEval-2024 Task 8 -\nMultigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text\nDetection. The task encompasses Binary Human-Written vs. Machine-Generated Text\nClassification (Track A), Multi-Way Machine-Generated Text Classification\n(Track B), and Human-Machine Mixed Text Detection (Track C). Our best\nperforming approaches utilize mainly the ensemble of discriminator transformer\nmodels along with sentence transformer and statistical machine learning\napproaches in specific cases. Moreover, zero-shot prompting and fine-tuning of\nFLAN-T5 are used for Track A and B.", "published": "2024-03-22 06:47:28", "link": "http://arxiv.org/abs/2403.14989v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic\n  Textual Relatedness", "abstract": "This paper presents the MasonTigers entry to the SemEval-2024 Task 1 -\nSemantic Textual Relatedness. The task encompasses supervised (Track A),\nunsupervised (Track B), and cross-lingual (Track C) approaches across 14\ndifferent languages. MasonTigers stands out as one of the two teams who\nparticipated in all languages across the three tracks. Our approaches achieved\nrankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and\nfrom 5th to 12th in Track C. Adhering to the task-specific constraints, our\nbest performing approaches utilize ensemble of statistical machine learning\napproaches combined with language-specific BERT based models and sentence\ntransformers.", "published": "2024-03-22 06:47:42", "link": "http://arxiv.org/abs/2403.14990v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ESG Classification by Implicit Rule Learning via GPT-4", "abstract": "Environmental, social, and governance (ESG) factors are widely adopted as\nhigher investment return indicators. Accordingly, ongoing efforts are being\nmade to automate ESG evaluation with language models to extract signals from\nmassive web text easily. However, recent approaches suffer from a lack of\ntraining data, as rating agencies keep their evaluation metrics confidential.\nThis paper investigates whether state-of-the-art language models like GPT-4 can\nbe guided to align with unknown ESG evaluation criteria through strategies such\nas prompting, chain-of-thought reasoning, and dynamic in-context learning. We\ndemonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task\nML-ESG-3 Impact Type track for Korean without updating the model on the\nprovided training data. We also explore how adjusting prompts impacts the\nability of language models to address financial tasks leveraging smaller models\nwith openly available weights. We observe longer general pre-training to\ncorrelate with enhanced performance in financial downstream tasks. Our findings\nshowcase the potential of language models to navigate complex, subjective\nevaluation guidelines despite lacking explicit training examples, revealing\nopportunities for training-free solutions for financial downstream tasks.", "published": "2024-03-22 08:45:30", "link": "http://arxiv.org/abs/2403.15040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement", "abstract": "Pretrained large language models (LLMs) are currently state-of-the-art for\nsolving the vast majority of natural language processing tasks. While many\nreal-world applications still require fine-tuning to reach satisfactory levels\nof performance, many of them are in the low-data regime, making fine-tuning\nchallenging. To address this, we propose LLM2LLM, a targeted and iterative data\naugmentation strategy that uses a teacher LLM to enhance a small seed dataset\nby augmenting additional data that can be used for fine-tuning on a specific\ntask. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,\n(2) evaluates and extracts data points that the model gets wrong, and (3) uses\na teacher LLM to generate synthetic data based on these incorrect data points,\nwhich are then added back into the training data. This approach amplifies the\nsignal from incorrectly predicted data points by the LLM during training and\nreintegrates them into the dataset to focus on more challenging examples for\nthe LLM. Our results show that LLM2LLM significantly enhances the performance\nof LLMs in the low-data regime, outperforming both traditional fine-tuning and\nother data augmentation baselines. LLM2LLM reduces the dependence on\nlabor-intensive data curation and paves the way for more scalable and\nperformant LLM solutions, allowing us to tackle data-constrained domains and\ntasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on\nCaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular\nfine-tuning in the low-data regime using a Llama-2-7B student model. Our code\nis available at https://github.com/SqueezeAILab/LLM2LLM .", "published": "2024-03-22 08:57:07", "link": "http://arxiv.org/abs/2403.15042v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CHisIEC: An Information Extraction Corpus for Ancient Chinese History", "abstract": "Natural Language Processing (NLP) plays a pivotal role in the realm of\nDigital Humanities (DH) and serves as the cornerstone for advancing the\nstructural analysis of historical and cultural heritage texts. This is\nparticularly true for the domains of named entity recognition (NER) and\nrelation extraction (RE). In our commitment to expediting ancient history and\nculture, we present the ``Chinese Historical Information Extraction\nCorpus''(CHisIEC). CHisIEC is a meticulously curated dataset designed to\ndevelop and evaluate NER and RE tasks, offering a resource to facilitate\nresearch in the field. Spanning a remarkable historical timeline encompassing\ndata from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the\nextensive temporal range and text heterogeneity inherent in Chinese historical\ndocuments. The dataset encompasses four distinct entity types and twelve\nrelation types, resulting in a meticulously labeled dataset comprising 14,194\nentities and 8,609 relations. To establish the robustness and versatility of\nour dataset, we have undertaken comprehensive experimentation involving models\nof various sizes and paradigms. Additionally, we have evaluated the\ncapabilities of Large Language Models (LLMs) in the context of tasks related to\nancient Chinese history. The dataset and code are available at\n\\url{https://github.com/tangxuemei1995/CHisIEC}.", "published": "2024-03-22 10:12:10", "link": "http://arxiv.org/abs/2403.15088v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Performance of Language Models for Completing Code in\n  Functional Programming Languages: a Haskell Case Study", "abstract": "Language model-based code completion models have quickly grown in use,\nhelping thousands of developers write code in many different programming\nlanguages. However, research on code completion models typically focuses on\nimperative languages such as Python and JavaScript, which results in a lack of\nrepresentation for functional programming languages. Consequently, these models\noften perform poorly on functional languages such as Haskell. To investigate\nwhether this can be alleviated, we evaluate the performance of two language\nmodels for code, CodeGPT and UniXcoder, on the functional programming language\nHaskell. We fine-tune and evaluate the models on Haskell functions sourced from\na publicly accessible Haskell dataset on HuggingFace. Additionally, we manually\nevaluate the models using our novel translated HumanEval dataset. Our automatic\nevaluation shows that knowledge of imperative programming languages in the\npre-training of LLMs may not transfer well to functional languages, but that\ncode completion on functional languages is feasible. Consequently, this shows\nthe need for more high-quality Haskell datasets. A manual evaluation on\nHumanEval-Haskell indicates CodeGPT frequently generates empty predictions and\nextra comments, while UniXcoder more often produces incomplete or incorrect\npredictions. Finally, we release HumanEval-Haskell, along with the fine-tuned\nmodels and all code required to reproduce our experiments on GitHub\n(https://github.com/AISE-TUDelft/HaskellCCEval).", "published": "2024-03-22 13:13:13", "link": "http://arxiv.org/abs/2403.15185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Awakening Augmented Generation: Learning to Awaken Internal Knowledge of\n  Large Language Models for Question Answering", "abstract": "Retrieval-Augmented-Generation and Generation-Augmented-Generation have been\nproposed to enhance the knowledge required for question answering with Large\nLanguage Models (LLMs) by leveraging richer context. However, the former relies\non external resources, and both require incorporating explicit documents into\nthe context, which increases execution costs and susceptibility to noise data\nduring inference. Recent works indicate that LLMs model rich knowledge, but it\nis often not effectively activated and awakened. Inspired by this, we propose a\nnovel knowledge-augmented framework, $\\textbf{Awakening-Augmented-Generation}$\n(AAG), which mimics the human ability to answer questions using only thinking\nand recalling to compensate for knowledge gaps, thereby awaking relevant\nknowledge in LLMs without relying on external resources. AAG consists of two\nkey components for awakening richer context. Explicit awakening fine-tunes a\ncontext generator to create a synthetic, compressed document that functions as\nsymbolic context. Implicit awakening utilizes a hypernetwork to generate\nadapters based on the question and synthetic document, which are inserted into\nLLMs to serve as parameter context. Experimental results on three datasets\ndemonstrate that AAG exhibits significant advantages in both open-domain and\nclosed-book settings, as well as in out-of-distribution generalization. Our\ncode will be available at \\url{https://github.com/Xnhyacinth/IAG}.", "published": "2024-03-22 15:06:45", "link": "http://arxiv.org/abs/2403.15268v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs", "abstract": "Event temporal relation (TempRel) is a primary subject of the event relation\nextraction task. However, the inherent ambiguity of TempRel increases the\ndifficulty of the task. With the rise of prompt engineering, it is important to\ndesign effective prompt templates and verbalizers to extract relevant\nknowledge. The traditional manually designed templates struggle to extract\nprecise temporal knowledge. This paper introduces a novel retrieval-augmented\nTempRel extraction approach, leveraging knowledge retrieved from large language\nmodels (LLMs) to enhance prompt templates and verbalizers. Our method\ncapitalizes on the diverse capabilities of various LLMs to generate a wide\narray of ideas for template and verbalizer design. Our proposed method fully\nexploits the potential of LLMs for generation tasks and contributes more\nknowledge to our design. Empirical evaluations across three widely recognized\ndatasets demonstrate the efficacy of our method in improving the performance of\nevent temporal relation extraction tasks.", "published": "2024-03-22 15:16:10", "link": "http://arxiv.org/abs/2403.15273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Specifying Genericity through Inclusiveness and Abstractness Continuous\n  Scales", "abstract": "This paper introduces a novel annotation framework for the fine-grained\nmodeling of Noun Phrases' (NPs) genericity in natural language. The framework\nis designed to be simple and intuitive, making it accessible to non-expert\nannotators and suitable for crowd-sourced tasks. Drawing from theoretical and\ncognitive literature on genericity, this framework is grounded in established\nlinguistic theory. Through a pilot study, we created a small but crucial\nannotated dataset of 324 sentences, serving as a foundation for future\nresearch. To validate our approach, we conducted an evaluation comparing our\ncontinuous annotations with existing binary annotations on the same dataset,\ndemonstrating the framework's effectiveness in capturing nuanced aspects of\ngenericity. Our work offers a practical resource for linguists, providing a\nfirst annotated dataset and an annotation scheme designed to build\nreal-language datasets that can be used in studies on the semantics of\ngenericity, and NLP practitioners, contributing to the development of\ncommonsense knowledge repositories valuable in enhancing various NLP\napplications.", "published": "2024-03-22 15:21:07", "link": "http://arxiv.org/abs/2403.15278v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for\n  Named Entity Recognition and Relation Extraction", "abstract": "The process of cyber mapping gives insights in relationships among financial\nentities and service providers. Centered around the outsourcing practices of\ncompanies within fund prospectuses in Germany, we introduce a dataset\nspecifically designed for named entity recognition and relation extraction\ntasks. The labeling process on 948 sentences was carried out by three experts\nwhich yields to 5,969 annotations for four entity types (Outsourcing, Company,\nLocation and Software) and 4,102 relation annotations (Outsourcing-Company,\nCompany-Location). State-of-the-art deep learning models were trained to\nrecognize entities and extract relations showing first promising results. An\nanonymized version of the dataset, along with guidelines and the code used for\nmodel training, are publicly available at\nhttps://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip.", "published": "2024-03-22 16:17:55", "link": "http://arxiv.org/abs/2403.15322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Review Fusion-in-Context", "abstract": "Grounded text generation, encompassing tasks such as long-form\nquestion-answering and summarization, necessitates both content selection and\ncontent consolidation. Current end-to-end methods are difficult to control and\ninterpret due to their opaqueness. Accordingly, recent works have proposed a\nmodular approach, with separate components for each step. Specifically, we\nfocus on the second subtask, of generating coherent text given pre-selected\ncontent in a multi-document setting. Concretely, we formalize Fusion-in-Context\n(FiC) as a standalone task, whose input consists of source texts with\nhighlighted spans of targeted content. A model then needs to generate a\ncoherent passage that includes all and only the target information. Our work\nincludes the development of a curated dataset of 1000 instances in the reviews\ndomain, alongside a novel evaluation framework for assessing the faithfulness\nand coverage of highlights, which strongly correlate to human judgment. Several\nbaseline models exhibit promising outcomes and provide insightful analyses.\nThis study lays the groundwork for further exploration of modular text\ngeneration in the multi-document setting, offering potential improvements in\nthe quality and reliability of generated content. Our benchmark, FuseReviews,\nincluding the dataset, evaluation framework, and designated leaderboard, can be\nfound at https://fusereviews.github.io/.", "published": "2024-03-22 17:06:05", "link": "http://arxiv.org/abs/2403.15351v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Knowledge-Grounded Natural Language Understanding and Generation", "abstract": "This thesis investigates how natural language understanding and generation\nwith transformer models can benefit from grounding the models with knowledge\nrepresentations and addresses the following key research questions: (i) Can\nknowledge of entities extend its benefits beyond entity-centric tasks, such as\nentity linking? (ii) How can we faithfully and effectively extract such\nstructured knowledge from raw text, especially noisy web text? (iii) How do\nother types of knowledge, beyond structured knowledge, contribute to improving\nNLP tasks?\n  Studies in this thesis find that incorporating relevant and up-to-date\nknowledge of entities benefits fake news detection, and entity-focused\ncode-switching significantly enhances zero-shot cross-lingual transfer on\nentity-centric tasks. In terms of effective and faithful approaches to\nextracting structured knowledge, it is observed that integrating negative\nexamples and training with entity planning significantly improves performance.\nAdditionally, it is established that other general forms of knowledge, such as\nparametric and distilled knowledge, enhance multimodal and multilingual\nknowledge-intensive tasks. This research shows the tangible benefits of diverse\nknowledge integration and motivates further exploration in this direction.", "published": "2024-03-22 17:32:43", "link": "http://arxiv.org/abs/2403.15364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NaturalTurn: A Method to Segment Transcripts into Naturalistic\n  Conversational Turns", "abstract": "Conversation is the subject of increasing interest in the social, cognitive,\nand computational sciences. And yet, as conversational datasets continue to\nincrease in size and complexity, researchers lack scalable methods to segment\nspeech-to-text transcripts into conversational turns-the basic building blocks\nof social interaction. We discuss this challenge and then introduce\n\"NaturalTurn,\" a turn segmentation algorithm designed to accurately capture the\ndynamics of naturalistic exchange. NaturalTurn operates by distinguishing\nspeakers' primary conversational turns from listeners' secondary utterances,\nsuch as backchannels, brief interjections, and other forms of parallel speech\nthat characterize conversation. Using data from a large conversation corpus, we\nshow how NaturalTurn-derived transcripts demonstrate favorable statistical and\ninferential characteristics compared to transcripts derived from existing\nmethods. The NaturalTurn algorithm represents an improvement in\nmachine-generated transcript processing methods, or \"turn models\" that will\nenable researchers to link turn-taking dynamics with the broader outcomes that\nresult from social interaction, a central goal of conversation science.", "published": "2024-03-22 21:05:54", "link": "http://arxiv.org/abs/2403.15615v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stance Reasoner: Zero-Shot Stance Detection on Social Media with\n  Explicit Reasoning", "abstract": "Social media platforms are rich sources of opinionated content. Stance\ndetection allows the automatic extraction of users' opinions on various topics\nfrom such content. We focus on zero-shot stance detection, where the model's\nsuccess relies on (a) having knowledge about the target topic; and (b) learning\ngeneral reasoning strategies that can be employed for new topics. We present\nStance Reasoner, an approach to zero-shot stance detection on social media that\nleverages explicit reasoning over background knowledge to guide the model's\ninference about the document's stance on a target. Specifically, our method\nuses a pre-trained language model as a source of world knowledge, with the\nchain-of-thought in-context learning approach to generate intermediate\nreasoning steps. Stance Reasoner outperforms the current state-of-the-art\nmodels on 3 Twitter datasets, including fully supervised models. It can better\ngeneralize across targets, while at the same time providing explicit and\ninterpretable explanations for its predictions.", "published": "2024-03-22 00:58:28", "link": "http://arxiv.org/abs/2403.14895v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Skip Decoding for Efficient Autoregressive Text Generation", "abstract": "Autoregressive decoding strategy is a commonly used method for text\ngeneration tasks with pre-trained language models, while early-exiting is an\neffective approach to speedup the inference stage. In this work, we propose a\nnovel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient\nautoregressive text generation. Different from existing methods that require\nadditional trainable components, HSD is a plug-and-play method applicable to\nautoregressive text generation models, it adaptively skips decoding layers in a\nhierarchical manner based on the current sequence length, thereby reducing\ncomputational workload and allocating computation resources. Comprehensive\nexperiments on five text generation datasets with pre-trained language models\ndemonstrate HSD's advantages in balancing efficiency and text quality. With\nalmost half of the layers skipped, HSD can sustain 90% of the text quality\ncompared to vanilla autoregressive decoding, outperforming the competitive\napproaches.", "published": "2024-03-22 02:44:05", "link": "http://arxiv.org/abs/2403.14919v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extending Token Computation for LLM Reasoning", "abstract": "Large Language Models (LLMs) are pivotal in advancing natural language\nprocessing but often struggle with complex reasoning tasks due to inefficient\nattention distributions. In this paper, we explore the effect of increased\ncomputed tokens on LLM performance and introduce a novel method for extending\ncomputed tokens in the Chain-of-Thought (CoT) process, utilizing attention\nmechanism optimization. By fine-tuning an LLM on a domain-specific, highly\nstructured dataset, we analyze attention patterns across layers, identifying\ninefficiencies caused by non-semantic tokens with outlier high attention\nscores. To address this, we propose an algorithm that emulates early layer\nattention patterns across downstream layers to re-balance skewed attention\ndistributions and enhance knowledge abstraction. Our findings demonstrate that\nour approach not only facilitates a deeper understanding of the internal\ndynamics of LLMs but also significantly improves their reasoning capabilities,\nparticularly in non-STEM domains. Our study lays the groundwork for further\ninnovations in LLM design, aiming to create more powerful, versatile, and\nresponsible models capable of tackling a broad range of real-world\napplications.", "published": "2024-03-22 03:23:58", "link": "http://arxiv.org/abs/2403.14932v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable\n  Adaptation", "abstract": "Parameter-efficient finetuning (PEFT) is a key technique for adapting large\nlanguage models (LLMs) to downstream tasks. In this paper, we study leveraging\nknowledge graph embeddings to improve the effectiveness of PEFT. We propose a\nknowledgeable adaptation method called KnowLA. It inserts an adaptation layer\ninto an LLM to integrate the embeddings of entities appearing in the input\ntext. The adaptation layer is trained in combination with LoRA on instruction\ndata. Experiments on six benchmarks with two popular LLMs and three knowledge\ngraphs demonstrate the effectiveness and robustness of KnowLA. We show that\n\\modelname can help activate the relevant parameterized knowledge in an LLM to\nanswer a question without changing its parameters or input prompts.", "published": "2024-03-22 04:48:41", "link": "http://arxiv.org/abs/2403.14950v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evidence-Driven Retrieval Augmented Response Generation for Online\n  Misinformation", "abstract": "The proliferation of online misinformation has posed significant threats to\npublic interest. While numerous online users actively participate in the combat\nagainst misinformation, many of such responses can be characterized by the lack\nof politeness and supporting facts. As a solution, text generation approaches\nare proposed to automatically produce counter-misinformation responses.\nNevertheless, existing methods are often trained end-to-end without leveraging\nexternal knowledge, resulting in subpar text quality and excessively repetitive\nresponses. In this paper, we propose retrieval augmented response generation\nfor online misinformation (RARG), which collects supporting evidence from\nscientific sources and generates counter-misinformation responses based on the\nevidences. In particular, our RARG consists of two stages: (1) evidence\ncollection, where we design a retrieval pipeline to retrieve and rerank\nevidence documents using a database comprising over 1M academic articles; (2)\nresponse generation, in which we align large language models (LLMs) to generate\nevidence-based responses via reinforcement learning from human feedback (RLHF).\nWe propose a reward function to maximize the utilization of the retrieved\nevidence while maintaining the quality of the generated text, which yields\npolite and factual responses that clearly refutes misinformation. To\ndemonstrate the effectiveness of our method, we study the case of COVID-19 and\nperform extensive experiments with both in- and cross-domain datasets, where\nRARG consistently outperforms baselines by generating high-quality\ncounter-misinformation responses.", "published": "2024-03-22 05:05:45", "link": "http://arxiv.org/abs/2403.14952v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Construction of a Japanese Financial Benchmark for Large Language Models", "abstract": "With the recent development of large language models (LLMs), models that\nfocus on certain domains and languages have been discussed for their necessity.\nThere is also a growing need for benchmarks to evaluate the performance of\ncurrent LLMs in each domain. Therefore, in this study, we constructed a\nbenchmark comprising multiple tasks specific to the Japanese and financial\ndomains and performed benchmark measurements on some models. Consequently, we\nconfirmed that GPT-4 is currently outstanding, and that the constructed\nbenchmarks function effectively. According to our analysis, our benchmark can\ndifferentiate benchmark scores among models in all performance ranges by\ncombining tasks with different difficulties.", "published": "2024-03-22 09:40:27", "link": "http://arxiv.org/abs/2403.15062v1", "categories": ["q-fin.CP", "cs.CL"], "primary_category": "q-fin.CP"}
{"title": "Argument-Aware Approach To Event Linking", "abstract": "Event linking connects event mentions in text with relevant nodes in a\nknowledge base (KB). Prior research in event linking has mainly borrowed\nmethods from entity linking, overlooking the distinct features of events.\nCompared to the extensively explored entity linking task, events have more\ncomplex structures and can be more effectively distinguished by examining their\nassociated arguments. Moreover, the information-rich nature of events leads to\nthe scarcity of event KBs. This emphasizes the need for event linking models to\nidentify and classify event mentions not in the KB as ``out-of-KB,'' an area\nthat has received limited attention. In this work, we tackle these challenges\nby introducing an argument-aware approach. First, we improve event linking\nmodels by augmenting input text with tagged event argument information,\nfacilitating the recognition of key information about event mentions.\nSubsequently, to help the model handle ``out-of-KB'' scenarios, we synthesize\nout-of-KB training examples from in-KB instances through controlled\nmanipulation of event arguments. Our experiment across two test datasets showed\nsignificant enhancements in both in-KB and out-of-KB scenarios, with a notable\n22% improvement in out-of-KB evaluations.", "published": "2024-03-22 10:32:43", "link": "http://arxiv.org/abs/2403.15097v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Not All Attention is Needed: Parameter and Computation Efficient\n  Transfer Learning for Multi-modal Large Language Models", "abstract": "In this paper, we propose a novel parameter and computation efficient tuning\nmethod for Multi-modal Large Language Models (MLLMs), termed Efficient\nAttention Skipping (EAS). Concretely, we first reveal that multi-head\nattentions (MHAs), the main computational overhead of MLLMs, are often\nredundant to downstream tasks. Based on this observation, EAS evaluates the\nattention redundancy and skips the less important MHAs to speed up inference.\nBesides, we also propose a novel propagation-of-information adapter (PIA) to\nserve the attention skipping of EAS and keep parameter efficiency, which can be\nfurther re-parameterized into feed-forward networks (FFNs) for zero-extra\nlatency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN\nand a classic VL pre-trained model called METER, and conduct extensive\nexperiments on a set of benchmarks. The experiments show that EAS not only\nretains high performance and parameter efficiency, but also greatly speeds up\ninference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on\nScineceQA while speeding up inference by 2.2 times to LaVIN", "published": "2024-03-22 14:20:34", "link": "http://arxiv.org/abs/2403.15226v2", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Fundus: A Simple-to-Use News Scraper Optimized for High Quality\n  Extractions", "abstract": "This paper introduces Fundus, a user-friendly news scraper that enables users\nto obtain millions of high-quality news articles with just a few lines of code.\nUnlike existing news scrapers, we use manually crafted, bespoke content\nextractors that are specifically tailored to the formatting guidelines of each\nsupported online newspaper. This allows us to optimize our scraping for quality\nsuch that retrieved news articles are textually complete and without HTML\nartifacts. Further, our framework combines both crawling (retrieving HTML from\nthe web or large web archives) and content extraction into a single pipeline.\nBy providing a unified interface for a predefined collection of newspapers, we\naim to make Fundus broadly usable even for non-technical users. This paper\ngives an overview of the framework, discusses our design choices, and presents\na comparative evaluation against other popular news scrapers. Our evaluation\nshows that Fundus yields significantly higher quality extractions (complete and\nartifact-free news articles) than prior work. The framework is available on\nGitHub under https://github.com/flairNLP/fundus and can be simply installed\nusing pip.", "published": "2024-03-22 15:22:06", "link": "http://arxiv.org/abs/2403.15279v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CoLLEGe: Concept Embedding Generation for Large Language Models", "abstract": "Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training. Code\nand data for our project can be found at\nhttps://college-concept-learning.github.io/", "published": "2024-03-22 17:26:05", "link": "http://arxiv.org/abs/2403.15362v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CTSM: Combining Trait and State Emotions for Empathetic Response Model", "abstract": "Empathetic response generation endeavors to empower dialogue systems to\nperceive speakers' emotions and generate empathetic responses accordingly.\nPsychological research demonstrates that emotion, as an essential factor in\nempathy, encompasses trait emotions, which are static and context-independent,\nand state emotions, which are dynamic and context-dependent. However, previous\nstudies treat them in isolation, leading to insufficient emotional perception\nof the context, and subsequently, less effective empathetic expression. To\naddress this problem, we propose Combining Trait and State emotions for\nEmpathetic Response Model (CTSM). Specifically, to sufficiently perceive\nemotions in dialogue, we first construct and encode trait and state emotion\nembeddings, and then we further enhance emotional perception capability through\nan emotion guidance module that guides emotion representation. In addition, we\npropose a cross-contrastive learning decoder to enhance the model's empathetic\nexpression capability by aligning trait and state emotions between generated\nresponses and contexts. Both automatic and manual evaluation results\ndemonstrate that CTSM outperforms state-of-the-art baselines and can generate\nmore empathetic responses. Our code is available at\nhttps://github.com/wangyufeng-empty/CTSM", "published": "2024-03-22 10:45:13", "link": "http://arxiv.org/abs/2403.15516v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Selecting Query-bag as Pseudo Relevance Feedback for Information-seeking\n  Conversations", "abstract": "Information-seeking dialogue systems are widely used in e-commerce systems,\nwith answers that must be tailored to fit the specific settings of the online\nsystem. Given the user query, the information-seeking dialogue systems first\nretrieve a subset of response candidates, then further select the best response\nfrom the candidate set through re-ranking. Current methods mainly retrieve\nresponse candidates based solely on the current query, however, incorporating\nsimilar questions could introduce more diverse content, potentially refining\nthe representation and improving the matching process. Hence, in this paper, we\nproposed a Query-bag based Pseudo Relevance Feedback framework (QB-PRF), which\nconstructs a query-bag with related queries to serve as pseudo signals to guide\ninformation-seeking conversations. Concretely, we first propose a Query-bag\nSelection module (QBS), which utilizes contrastive learning to train the\nselection of synonymous queries in an unsupervised manner by leveraging the\nrepresentations learned from pre-trained VAE. Secondly, we come up with a\nQuery-bag Fusion module (QBF) that fuses synonymous queries to enhance the\nsemantic representation of the original query through multidimensional\nattention computation. We verify the effectiveness of the QB-PRF framework on\ntwo competitive pretrained backbone models, including BERT and GPT-2.\nExperimental results on two benchmark datasets show that our framework achieves\nsuperior performance over strong baselines.", "published": "2024-03-22 08:10:32", "link": "http://arxiv.org/abs/2404.04272v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Optimal path for Biomedical Text Summarization Using Pointer GPT", "abstract": "Biomedical text summarization is a critical tool that enables clinicians to\neffectively ascertain patient status. Traditionally, text summarization has\nbeen accomplished with transformer models, which are capable of compressing\nlong documents into brief summaries. However, transformer models are known to\nbe among the most challenging natural language processing (NLP) tasks.\nSpecifically, GPT models have a tendency to generate factual errors, lack\ncontext, and oversimplify words. To address these limitations, we replaced the\nattention mechanism in the GPT model with a pointer network. This modification\nwas designed to preserve the core values of the original text during the\nsummarization process. The effectiveness of the Pointer-GPT model was evaluated\nusing the ROUGE score. The results demonstrated that Pointer-GPT outperformed\nthe original GPT model. These findings suggest that pointer networks can be a\nvaluable addition to EMR systems and can provide clinicians with more accurate\nand informative summaries of patient medical records. This research has the\npotential to usher in a new paradigm in EMR systems and to revolutionize the\nway that clinicians interact with patient medical records.", "published": "2024-03-22 02:13:23", "link": "http://arxiv.org/abs/2404.08654v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Single Linear Layer Yields Task-Adapted Low-Rank Matrices", "abstract": "Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning\n(PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix\n$\\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study\nsuggested that there is correlation between $W_0$ and $\\Delta W$. In this\nstudy, we aim to delve deeper into relationships between $W_0$ and low-rank\nmatrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular,\nwe analyze a conversion matrix that transform $W_0$ into low-rank matrices,\nwhich encapsulates information about the relationships. Our analysis reveals\nthat the conversion matrices are similar across each layer. Inspired by these\nfindings, we hypothesize that a single linear layer, which takes each layer's\n$W_0$ as input, can yield task-adapted low-rank matrices. To confirm this\nhypothesis, we devise a method named Conditionally Parameterized LoRA\n(CondLoRA) that updates initial weight matrices with low-rank matrices derived\nfrom a single linear layer. Our empirical results show that CondLoRA maintains\na performance on par with LoRA, despite the fact that the trainable parameters\nof CondLoRA are fewer than those of LoRA. Therefore, we conclude that \"a single\nlinear layer yields task-adapted low-rank matrices.\"", "published": "2024-03-22 04:38:42", "link": "http://arxiv.org/abs/2403.14946v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapprox: Adaptive Approximation in Adam Optimization via Randomized\n  Low-Rank Matrices", "abstract": "As deep learning models exponentially increase in size, optimizers such as\nAdam encounter significant memory consumption challenges due to the storage of\nfirst and second moment data. Current memory-efficient methods like Adafactor\nand CAME often compromise accuracy with their matrix factorization techniques.\nAddressing this, we introduce Adapprox, a novel approach that employs\nrandomized low-rank matrix approximation for a more effective and accurate\napproximation of Adam's second moment. Adapprox features an adaptive rank\nselection mechanism, finely balancing accuracy and memory efficiency, and\nincludes an optional cosine similarity guidance strategy to enhance stability\nand expedite convergence. In GPT-2 training and downstream tasks, Adapprox\nsurpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings\nfor the 117M and 345M models, respectively, with the first moment enabled, and\nfurther increases these savings without the first moment. Besides, it enhances\nconvergence speed and improves downstream task performance relative to its\ncounterparts.", "published": "2024-03-22 05:23:31", "link": "http://arxiv.org/abs/2403.14958v1", "categories": ["cs.LG", "cs.CL", "math.OC"], "primary_category": "cs.LG"}
{"title": "A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal\n  Reasoning", "abstract": "This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate that BDoG is able to achieve state-of-the-art\nresults in ScienceQA and MMBench with significant improvements over previous\nmethods. The source code can be accessed at https://github.com/thecharm/BDoG.", "published": "2024-03-22 06:03:07", "link": "http://arxiv.org/abs/2403.14972v2", "categories": ["cs.AI", "cs.CL", "cs.MA", "cs.MM"], "primary_category": "cs.AI"}
{"title": "Text Clustering with Large Language Model Embeddings", "abstract": "Text clustering is an important method for organising the increasing volume\nof digital content, aiding in the structuring and discovery of hidden patterns\nin uncategorised data. The effectiveness of text clustering largely depends on\nthe selection of textual embeddings and clustering algorithms. This study\nargues that recent advancements in large language models (LLMs) have the\npotential to enhance this task. The research investigates how different textual\nembeddings, particularly those utilised in LLMs, and various clustering\nalgorithms influence the clustering of text datasets. A series of experiments\nwere conducted to evaluate the impact of embeddings on clustering results, the\nrole of dimensionality reduction through summarisation, and the adjustment of\nmodel size. The findings indicate that LLM embeddings are superior at capturing\nsubtleties in structured language. OpenAI's GPT-3.5 Turbo model yields better\nresults in three out of five clustering metrics across most tested datasets.\nMost LLM embeddings show improvements in cluster purity and provide a more\ninformative silhouette score, reflecting a refined structural understanding of\ntext data compared to traditional methods. Among the more lightweight models,\nBERT demonstrates leading performance. Additionally, it was observed that\nincreasing model dimensionality and employing summarisation techniques do not\nconsistently enhance clustering efficiency, suggesting that these strategies\nrequire careful consideration for practical application. These results\nhighlight a complex balance between the need for refined text representation\nand computational feasibility in text clustering applications. This study\nextends traditional text clustering frameworks by integrating embeddings from\nLLMs, offering improved methodologies and suggesting new avenues for future\nresearch in various types of textual analysis.", "published": "2024-03-22 11:08:48", "link": "http://arxiv.org/abs/2403.15112v5", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary), 62H30 (Secondary)", "I.2.6; I.2.7; I.7.m"], "primary_category": "cs.CL"}
{"title": "Language Models in Dialogue: Conversational Maxims for Human-AI\n  Interactions", "abstract": "Modern language models, while sophisticated, exhibit some inherent\nshortcomings, particularly in conversational settings. We claim that many of\nthe observed shortcomings can be attributed to violation of one or more\nconversational principles. By drawing upon extensive research from both the\nsocial science and AI communities, we propose a set of maxims -- quantity,\nquality, relevance, manner, benevolence, and transparency -- for describing\neffective human-AI conversation. We first justify the applicability of the\nfirst four maxims (from Grice) in the context of human-AI interactions. We then\nargue that two new maxims, benevolence (concerning the generation of, and\nengagement with, harmful content) and transparency (concerning recognition of\none's knowledge boundaries, operational constraints, and intents), are\nnecessary for addressing behavior unique to modern human-AI interactions. We\nevaluate the degree to which various language models are able to understand\nthese maxims and find that models possess an internal prioritization of\nprinciples that can significantly impact their ability to interpret the maxims\naccurately.", "published": "2024-03-22 11:16:43", "link": "http://arxiv.org/abs/2403.15115v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "CACA Agent: Capability Collaboration based AI Agent", "abstract": "As AI Agents based on Large Language Models (LLMs) have shown potential in\npractical applications across various fields, how to quickly deploy an AI agent\nand how to conveniently expand the application scenario of AI agents has become\na challenge. Previous studies mainly focused on implementing all the reasoning\ncapabilities of AI agents within a single LLM, which often makes the model more\ncomplex and also reduces the extensibility of AI agent functionality. In this\npaper, we propose CACA Agent (Capability Collaboration based AI Agent), using\nan open architecture inspired by service computing. CACA Agent integrates a set\nof collaborative capabilities to implement AI Agents, not only reducing the\ndependence on a single LLM, but also enhancing the extensibility of both the\nplanning abilities and the tools available to AI agents. Utilizing the proposed\nsystem, we present a demo to illustrate the operation and the application\nscenario extension of CACA Agent.", "published": "2024-03-22 11:42:47", "link": "http://arxiv.org/abs/2403.15137v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "InstaSynth: Opportunities and Challenges in Generating Synthetic\n  Instagram Data with ChatGPT for Sponsored Content Detection", "abstract": "Large Language Models (LLMs) raise concerns about lowering the cost of\ngenerating texts that could be used for unethical or illegal purposes,\nespecially on social media. This paper investigates the promise of such models\nto help enforce legal requirements related to the disclosure of sponsored\ncontent online. We investigate the use of LLMs for generating synthetic\nInstagram captions with two objectives: The first objective (fidelity) is to\nproduce realistic synthetic datasets. For this, we implement content-level and\nnetwork-level metrics to assess whether synthetic captions are realistic. The\nsecond objective (utility) is to create synthetic data that is useful for\nsponsored content detection. For this, we evaluate the effectiveness of the\ngenerated synthetic data for training classifiers to identify undisclosed\nadvertisements on Instagram. Our investigations show that the objectives of\nfidelity and utility may conflict and that prompt engineering is a useful but\ninsufficient strategy. Additionally, we find that while individual synthetic\nposts may appear realistic, collectively they lack diversity, topic\nconnectivity, and realistic user interaction patterns.", "published": "2024-03-22 13:58:42", "link": "http://arxiv.org/abs/2403.15214v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions", "abstract": "Modern Language Models (LMs) are capable of following long and complex\ninstructions that enable a large and diverse set of user requests. While\nInformation Retrieval (IR) models use these LMs as the backbone of their\narchitectures, virtually none of them allow users to provide detailed\ninstructions alongside queries, thus limiting their ability to satisfy complex\ninformation needs. In this work, we study the use of instructions in IR\nsystems. First, we introduce our dataset FollowIR, which contains a rigorous\ninstruction evaluation benchmark as well as a training set for helping IR\nmodels learn to better follow real-world instructions. FollowIR repurposes\ndetailed instructions -- also known as narratives -- developed for professional\nassessors to evaluate retrieval systems. In particular, we build our benchmark\nfrom three collections curated for shared tasks at the Text REtrieval\nConference (TREC). These collections contains hundreds to thousands of labeled\ndocuments per query, making them suitable for our exploration. Through this\nprocess, we can measure how well IR models follow instructions, through a new\npairwise evaluation framework. Our results indicate that existing retrieval\nmodels fail to correctly use instructions, using them for basic keywords and\nstruggling to understand long-form information. However, we show that it is\npossible for IR models to learn to follow complex instructions: our new\nFollowIR-7B model has significant improvements after fine-tuning on our\ntraining set.", "published": "2024-03-22 14:42:29", "link": "http://arxiv.org/abs/2403.15246v3", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A\n  Multifaceted Statistical Approach", "abstract": "Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.", "published": "2024-03-22 14:47:35", "link": "http://arxiv.org/abs/2403.15250v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controlled Training Data Generation with Diffusion Models", "abstract": "In this work, we present a method to control a text-to-image generative model\nto produce training data specifically \"useful\" for supervised learning. Unlike\nprevious works that employ an open-loop approach and pre-define prompts to\ngenerate new data using either a language model or human expertise, we develop\nan automated closed-loop system which involves two feedback mechanisms. The\nfirst mechanism uses feedback from a given supervised model and finds\nadversarial prompts that result in image generations that maximize the model\nloss. While these adversarial prompts result in diverse data informed by the\nmodel, they are not informed of the target distribution, which can be\ninefficient. Therefore, we introduce the second feedback mechanism that guides\nthe generation process towards a certain target distribution. We call the\nmethod combining these two mechanisms Guided Adversarial Prompts. We perform\nour evaluations on different tasks, datasets and architectures, with different\ntypes of distribution shifts (spuriously correlated data, unseen domains) and\ndemonstrate the efficiency of the proposed feedback mechanisms compared to\nopen-loop approaches.", "published": "2024-03-22 15:59:24", "link": "http://arxiv.org/abs/2403.15309v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Transfer Attack to Image Watermarks", "abstract": "Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In this work, we\npropose a new transfer evasion attack to image watermark in the no-box setting.\nOur transfer attack adds a perturbation to a watermarked image to evade\nmultiple surrogate watermarking models trained by the attacker itself, and the\nperturbed watermarked image also evades the target watermarking model. Our\nmajor contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector based on existing watermarking\nmethods is not robust to evasion attacks even if the attacker does not have\naccess to the watermarking model nor the detection API. Our code is available\nat: https://github.com/hifi-hyp/Watermark-Transfer-Attack.", "published": "2024-03-22 17:33:11", "link": "http://arxiv.org/abs/2403.15365v4", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Can large language models explore in-context?", "abstract": "We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.", "published": "2024-03-22 17:50:43", "link": "http://arxiv.org/abs/2403.15371v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models", "abstract": "Large Multimodal Models (LMMs) have shown significant visual reasoning\ncapabilities by connecting a visual encoder and a large language model. LMMs\ntypically take in a fixed and large amount of visual tokens, such as the\npenultimate layer features in the CLIP visual encoder, as the prefix content.\nRecent LMMs incorporate more complex visual inputs, such as high-resolution\nimages and videos, which further increases the number of visual tokens\nsignificantly. However, due to the inherent design of the Transformer\narchitecture, the computational costs of these models tend to increase\nquadratically with the number of input tokens. To tackle this problem, we\nexplore a token reduction mechanism that identifies significant spatial\nredundancy among visual tokens. In response, we propose PruMerge, a novel\nadaptive visual token reduction strategy that significantly reduces the number\nof visual tokens without compromising the performance of LMMs. Specifically, to\nmetric the importance of each token, we exploit the sparsity observed in the\nvisual encoder, characterized by the sparse distribution of attention scores\nbetween the class token and visual tokens. This sparsity enables us to\ndynamically select the most crucial visual tokens to retain. Subsequently, we\ncluster the selected (unpruned) tokens based on their key similarity and merge\nthem with the unpruned tokens, effectively supplementing and enhancing their\ninformational content. Empirically, when applied to LLaVA-1.5, our approach can\ncompress the visual tokens by 14 times on average, and achieve comparable\nperformance across diverse visual question-answering and reasoning tasks. Code\nand checkpoints are at https://llava-prumerge.github.io/.", "published": "2024-03-22 17:59:52", "link": "http://arxiv.org/abs/2403.15388v5", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Effectiveness and Robustness in a Low-Resource Regime via\n  Decision-Boundary-aware Data Augmentation", "abstract": "Efforts to leverage deep learning models in low-resource regimes have led to\nnumerous augmentation studies. However, the direct application of methods such\nas mixup and cutout to text data, is limited due to their discrete\ncharacteristics. While methods using pretrained language models have exhibited\nefficiency, they require additional considerations for robustness. Inspired by\nrecent studies on decision boundaries, this paper proposes a\ndecision-boundary-aware data augmentation strategy to enhance robustness using\npretrained language models. The proposed technique first focuses on shifting\nthe latent features closer to the decision boundary, followed by reconstruction\nto generate an ambiguous version with a soft label. Additionally, mid-K\nsampling is suggested to enhance the diversity of the generated sentences. This\npaper demonstrates the performance of the proposed augmentation strategy\ncompared to other methods through extensive experiments. Furthermore, the\nablation study reveals the effect of soft labels and mid-K sampling and the\nextensibility of the method with curriculum data augmentation.", "published": "2024-03-22 05:18:08", "link": "http://arxiv.org/abs/2403.15512v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LimGen: Probing the LLMs for Generating Suggestive Limitations of\n  Research Papers", "abstract": "Examining limitations is a crucial step in the scholarly research reviewing\nprocess, revealing aspects where a study might lack decisiveness or require\nenhancement. This aids readers in considering broader implications for further\nresearch. In this article, we present a novel and challenging task of\nSuggestive Limitation Generation (SLG) for research papers. We compile a\ndataset called \\textbf{\\textit{LimGen}}, encompassing 4068 research papers and\ntheir associated limitations from the ACL anthology. We investigate several\napproaches to harness large language models (LLMs) for producing suggestive\nlimitations, by thoroughly examining the related challenges, practical\ninsights, and potential opportunities. Our LimGen dataset and code can be\naccessed at \\url{https://github.com/arbmf/LimGen}.", "published": "2024-03-22 17:31:43", "link": "http://arxiv.org/abs/2403.15529v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Differentially Private Next-Token Prediction of Large Language Models", "abstract": "Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly\nimportant. The most widely adopted technique to accomplish this is DP-SGD,\nwhich trains a model to guarantee Differential Privacy (DP). However, DP-SGD\noverestimates an adversary's capabilities in having white box access to the\nmodel and, as a result, causes longer training times and larger memory usage\nthan SGD. On the other hand, commercial LLM deployments are predominantly\ncloud-based; hence, adversarial access to LLMs is black-box. Motivated by these\nobservations, we present Private Mixing of Ensemble Distributions (PMixED): a\nprivate prediction protocol for next-token prediction that utilizes the\ninherent stochasticity of next-token sampling and a public model to achieve\nDifferential Privacy. We formalize this by introducing RD-mollifers which\nproject each of the model's output distribution from an ensemble of fine-tuned\nLLMs onto a set around a public LLM's output distribution, then average the\nprojected distributions and sample from it. Unlike DP-SGD which needs to\nconsider the model architecture during training, PMixED is model agnostic,\nwhich makes PMixED a very appealing solution for current deployments. Our\nresults show that PMixED achieves a stronger privacy guarantee than\nsample-level privacy and outperforms DP-SGD for privacy $\\epsilon = 8$ on\nlarge-scale datasets. Thus, PMixED offers a practical alternative to DP\ntraining methods for achieving strong generative utility without compromising\nprivacy.", "published": "2024-03-22 22:27:44", "link": "http://arxiv.org/abs/2403.15638v3", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy\n  with Semantic Search and Hybrid Query-Based Retrievers", "abstract": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a\nprivate knowledge base of documents with Large Language Models (LLM) to build\nGenerative Q\\&A (Question-Answering) systems. However, RAG accuracy becomes\nincreasingly challenging as the corpus of documents scales up, with Retrievers\nplaying an outsized role in the overall RAG accuracy by extracting the most\nrelevant document from the corpus to provide context to the LLM. In this paper,\nwe propose the 'Blended RAG' method of leveraging semantic search techniques,\nsuch as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid\nquery strategies. Our study achieves better retrieval results and sets new\nbenchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID\ndatasets. We further extend such a 'Blended Retriever' to the RAG system to\ndemonstrate far superior results on Generative Q\\&A datasets like SQUAD, even\nsurpassing fine-tuning performance.", "published": "2024-03-22 17:13:46", "link": "http://arxiv.org/abs/2404.07220v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Human behaviour through a LENS: How Linguistic content triggers Emotions\n  and Norms and determines Strategy choices", "abstract": "Over the last two decades, a growing body of experimental research has\nprovided evidence that linguistic frames influence human behaviour in economic\ngames, beyond the economic consequences of the available actions. This article\nproposes a novel framework that transcends the traditional confines of\noutcome-based preference models. According to the LENS model, the Linguistic\ndescription of the decision problem triggers Emotional responses and suggests\npotential Norms of behaviour, which then interact to shape an individual's\nStrategic choice. The article reviews experimental evidence that supports each\npath of the LENS model. Furthermore, it identifies and discusses several\ncritical research questions that arise from this model, pointing towards\navenues for future inquiry.", "published": "2024-03-22 15:40:11", "link": "http://arxiv.org/abs/2403.15293v1", "categories": ["cs.CL", "cs.GT", "econ.GN", "physics.soc-ph", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition", "abstract": "In this article, we introduce a novel problem of audio-visual autism behavior\nrecognition, which includes social behavior recognition, an essential aspect\npreviously omitted in AI-assisted autism screening research. We define the task\nat hand as one that is audio-visual autism behavior recognition, which uses\naudio and visual cues, including any speech present in the audio, to recognize\nautism-related behaviors. To facilitate this new research direction, we\ncollected an audio-visual autism spectrum dataset (AV-ASD), currently the\nlargest video dataset for autism screening using a behavioral approach. It\ncovers an extensive range of autism-associated behaviors, including those\nrelated to social communication and interaction. To pave the way for further\nresearch on this new problem, we intensively explored leveraging foundation\nmodels and multimodal large language models across different modalities. Our\nexperiments on the AV-ASD dataset demonstrate that integrating audio, visual,\nand speech modalities significantly enhances the performance in autism behavior\nrecognition. Additionally, we explored the use of a post-hoc to ad-hoc pipeline\nin a multimodal large language model to investigate its potential to augment\nthe model's explanatory capability during autism behavior recognition. We will\nrelease our dataset, code, and pre-trained models.", "published": "2024-03-22 22:52:35", "link": "http://arxiv.org/abs/2406.02554v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Dialogue Understandability: Why are we streaming movies with subtitles?", "abstract": "Watching movies and TV shows with subtitles enabled is not simply down to\naudibility or speech intelligibility. A variety of evolving factors related to\ntechnological advances, cinema production and social behaviour challenge our\nperception and understanding. This study seeks to formalise and give context to\nthese influential factors under a wider and novel term referred to as Dialogue\nUnderstandability. We propose a working definition for Dialogue\nUnderstandability being a listener's capacity to follow the story without undue\ncognitive effort or concentration being required that impacts their Quality of\nExperience (QoE). The paper identifies, describes and categorises the factors\nthat influence Dialogue Understandability mapping them over the QoE framework,\na media streaming lifecycle, and the stakeholders involved. We then explore\navailable measurement tools in the literature and link them to the factors they\ncould potentially be used for. The maturity and suitability of these tools is\nevaluated over a set of pilot experiments. Finally, we reflect on the gaps that\nstill need to be filled, what we can measure and what not, future subjective\nexperiments, and new research trends that could help us to fully characterise\nDialogue Understandability.", "published": "2024-03-22 16:41:45", "link": "http://arxiv.org/abs/2403.15336v1", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Privacy-Preserving End-to-End Spoken Language Understanding", "abstract": "Spoken language understanding (SLU), one of the key enabling technologies for\nhuman-computer interaction in IoT devices, provides an easy-to-use user\ninterface. Human speech can contain a lot of user-sensitive information, such\nas gender, identity, and sensitive content. New types of security and privacy\nbreaches have thus emerged. Users do not want to expose their personal\nsensitive information to malicious attacks by untrusted third parties. Thus,\nthe SLU system needs to ensure that a potential malicious attacker cannot\ndeduce the sensitive attributes of the users, while it should avoid greatly\ncompromising the SLU accuracy. To address the above challenge, this paper\nproposes a novel SLU multi-task privacy-preserving model to prevent both the\nspeech recognition (ASR) and identity recognition (IR) attacks. The model uses\nthe hidden layer separation technique so that SLU information is distributed\nonly in a specific portion of the hidden layer, and the other two types of\ninformation are removed to obtain a privacy-secure hidden layer. In order to\nachieve good balance between efficiency and privacy, we introduce a new\nmechanism of model pre-training, namely joint adversarial training, to further\nenhance the user privacy. Experiments over two SLU datasets show that the\nproposed method can reduce the accuracy of both the ASR and IR attacks close to\nthat of a random guess, while leaving the SLU performance largely unaffected.", "published": "2024-03-22 03:41:57", "link": "http://arxiv.org/abs/2403.15510v1", "categories": ["cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Music to Dance as Language Translation using Sequence Models", "abstract": "Synthesising appropriate choreographies from music remains an open problem.\nWe introduce MDLT, a novel approach that frames the choreography generation\nproblem as a translation task. Our method leverages an existing data set to\nlearn to translate sequences of audio into corresponding dance poses. We\npresent two variants of MDLT: one utilising the Transformer architecture and\nthe other employing the Mamba architecture. We train our method on AIST++ and\nPhantomDance data sets to teach a robotic arm to dance, but our method can be\napplied to a full humanoid robot. Evaluation metrics, including Average Joint\nError and Fr\\'echet Inception Distance, consistently demonstrate that, when\ngiven a piece of music, MDLT excels at producing realistic and high-quality\nchoreography. The code can be found at github.com/meowatthemoon/MDLT.", "published": "2024-03-22 18:47:54", "link": "http://arxiv.org/abs/2403.15569v2", "categories": ["cs.SD", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards auditory attention decoding with noise-tagging: A pilot study", "abstract": "Auditory attention decoding (AAD) aims to extract from brain activity the\nattended speaker amidst candidate speakers, offering promising applications for\nneuro-steered hearing devices and brain-computer interfacing. This pilot study\nmakes a first step towards AAD using the noise-tagging stimulus protocol, which\nevokes reliable code-modulated evoked potentials, but is minimally explored in\nthe auditory modality. Participants were sequentially presented with two Dutch\nspeech stimuli that were amplitude-modulated with a unique binary pseudo-random\nnoise-code, effectively tagging these with additional decodable information. We\ncompared the decoding of unmodulated audio against audio modulated with various\nmodulation depths, and a conventional AAD method against a standard method to\ndecode noise-codes. Our pilot study revealed higher performances for the\nconventional method with 70 to 100 percent modulation depths compared to\nunmodulated audio. The noise-code decoder did not further improve these\nresults. These fundamental insights highlight the potential of integrating\nnoise-codes in speech to enhance auditory speaker detection when multiple\nspeakers are presented simultaneously.", "published": "2024-03-22 13:35:34", "link": "http://arxiv.org/abs/2403.15523v2", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
