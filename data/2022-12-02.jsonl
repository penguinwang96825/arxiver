{"title": "UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question\n  Answering Over Knowledge Graph", "abstract": "Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the\nanswer entities that are multiple hops away from the topic entities mentioned\nin a natural language question on a large-scale Knowledge Graph (KG). To cope\nwith the vast search space, existing work usually adopts a two-stage approach:\nit first retrieves a relatively small subgraph related to the question and then\nperforms the reasoning on the subgraph to find the answer entities accurately.\nAlthough these two stages are highly related, previous work employs very\ndifferent technical solutions for developing the retrieval and reasoning\nmodels, neglecting their relatedness in task essence. In this paper, we propose\nUniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and\nreasoning in both model architecture and parameter learning. For model\narchitecture, UniKGQA consists of a semantic matching module based on a\npre-trained language model~(PLM) for question-relation semantic matching, and a\nmatching information propagation module to propagate the matching information\nalong the directed edges on KGs. For parameter learning, we design a shared\npre-training task based on question-relation matching for both retrieval and\nreasoning models, and then propose retrieval- and reasoning-oriented\nfine-tuning strategies. Compared with previous studies, our approach is more\nunified, tightly relating the retrieval and reasoning stages. Extensive\nexperiments on three benchmark datasets have demonstrated the effectiveness of\nour method on the multi-hop KGQA task. Our codes and data are publicly\navailable at~\\url{https://github.com/RUCAIBox/UniKGQA}.", "published": "2022-12-02 04:08:09", "link": "http://arxiv.org/abs/2212.00959v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning", "abstract": "The spread of rumors along with breaking events seriously hinders the truth\nin the era of social media. Previous studies reveal that due to the lack of\nannotated resources, rumors presented in minority languages are hard to be\ndetected. Furthermore, the unforeseen breaking events not involved in\nyesterday's news exacerbate the scarcity of data resources. In this work, we\npropose a novel zero-shot framework based on prompt learning to detect rumors\nfalling in different domains or presented in different languages. More\nspecifically, we firstly represent rumor circulated on social media as diverse\npropagation threads, then design a hierarchical prompt encoding mechanism to\nlearn language-agnostic contextual representations for both prompts and rumor\ndata. To further enhance domain adaptation, we model the domain-invariant\nstructural features from the propagation threads, to incorporate structural\nposition representations of influential community response. In addition, a new\nvirtual response augmentation method is used to improve model training.\nExtensive experiments conducted on three real-world datasets demonstrate that\nour proposed model achieves much better performance than state-of-the-art\nmethods and exhibits a superior capacity for detecting rumors at early stages.", "published": "2022-12-02 12:04:48", "link": "http://arxiv.org/abs/2212.01117v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SumREN: Summarizing Reported Speech about Events in News", "abstract": "A primary objective of news articles is to establish the factual record for\nan event, frequently achieved by conveying both the details of the specified\nevent (i.e., the 5 Ws; Who, What, Where, When and Why regarding the event) and\nhow people reacted to it (i.e., reported statements). However, existing work on\nnews summarization almost exclusively focuses on the event details. In this\nwork, we propose the novel task of summarizing the reactions of different\nspeakers, as expressed by their reported statements, to a given event. To this\nend, we create a new multi-document summarization benchmark, SUMREN, comprising\n745 summaries of reported statements from various public figures obtained from\n633 news articles discussing 132 events. We propose an automatic silver\ntraining data generation approach for our task, which helps smaller models like\nBART achieve GPT-3 level performance on this task. Finally, we introduce a\npipeline-based framework for summarizing reported speech, which we empirically\nshow to generate summaries that are more abstractive and factual than baseline\nquery-focused summarization approaches.", "published": "2022-12-02 12:51:39", "link": "http://arxiv.org/abs/2212.01146v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Simultaneous Machine Translation with Monolingual Data", "abstract": "Simultaneous machine translation (SiMT) is usually done via sequence-level\nknowledge distillation (Seq-KD) from a full-sentence neural machine translation\n(NMT) model. However, there is still a significant performance gap between NMT\nand SiMT. In this work, we propose to leverage monolingual data to improve\nSiMT, which trains a SiMT student on the combination of bilingual data and\nexternal monolingual data distilled by Seq-KD. Preliminary experiments on En-Zh\nand En-Ja news domain corpora demonstrate that monolingual data can\nsignificantly improve translation quality (e.g., +3.15 BLEU on En-Zh). Inspired\nby the behavior of human simultaneous interpreters, we propose a novel\nmonolingual sampling strategy for SiMT, considering both chunk length and\nmonotonicity. Experimental results show that our sampling strategy consistently\noutperforms the random sampling strategy (and other conventional typical NMT\nmonolingual sampling strategies) by avoiding the key problem of SiMT --\nhallucination, and has better scalability. We achieve +0.72 BLEU improvements\non average against random sampling on En-Zh and En-Ja. Data and codes can be\nfound at https://github.com/hexuandeng/Mono4SiMT.", "published": "2022-12-02 14:13:53", "link": "http://arxiv.org/abs/2212.01188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subword-Delimited Downsampling for Better Character-Level Translation", "abstract": "Subword-level models have been the dominant paradigm in NLP. However,\ncharacter-level models have the benefit of seeing each character individually,\nproviding the model with more detailed information that ultimately could lead\nto better models. Recent works have shown character-level models to be\ncompetitive with subword models, but costly in terms of time and computation.\nCharacter-level models with a downsampling component alleviate this, but at the\ncost of quality, particularly for machine translation. This work analyzes the\nproblems of previous downsampling methods and introduces a novel downsampling\nmethod which is informed by subwords. This new downsampling method not only\noutperforms existing downsampling methods, showing that downsampling characters\ncan be done without sacrificing quality, but also leads to promising\nperformance compared to subword models for translation.", "published": "2022-12-02 16:56:50", "link": "http://arxiv.org/abs/2212.01304v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Iterative Text Revision by Learning Where to Edit from Other\n  Revision Tasks", "abstract": "Iterative text revision improves text quality by fixing grammatical errors,\nrephrasing for better readability or contextual appropriateness, or\nreorganizing sentence structures throughout a document. Most recent research\nhas focused on understanding and classifying different types of edits in the\niterative revision process from human-written text instead of building accurate\nand robust systems for iterative text revision. In this work, we aim to build\nan end-to-end text revision system that can iteratively generate helpful edits\nby explicitly detecting editable spans (where-to-edit) with their corresponding\nedit intents and then instructing a revision model to revise the detected edit\nspans. Leveraging datasets from other related text editing NLP tasks, combined\nwith the specification of editable spans, leads our system to more accurately\nmodel the process of iterative text refinement, as evidenced by empirical\nresults and human evaluations. Our system significantly outperforms previous\nbaselines on our text revision tasks and other standard text revision tasks,\nincluding grammatical error correction, text simplification, sentence fusion,\nand style transfer. Through extensive qualitative and quantitative analysis, we\nmake vital connections between edit intentions and writing quality, and better\ncomputational modeling of iterative text revisions.", "published": "2022-12-02 18:10:43", "link": "http://arxiv.org/abs/2212.01350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Modeling on Clinical Social Work Notes for Exploring Social\n  Determinants of Health Factors", "abstract": "Most research studying social determinants of health (SDoH) has focused on\nphysician notes or structured elements of the electronic medical record (EMR).\nWe hypothesize that clinical notes from social workers, whose role is to\nameliorate social and economic factors, might provide a richer source of data\non SDoH. We sought to perform topic modeling to identify robust topics of\ndiscussion within a large cohort of social work notes. We retrieved a diverse,\ndeidentified corpus of 0.95 million clinical social work notes from 181,644\npatients at the University of California, San Francisco. We used word frequency\nanalysis and Latent Dirichlet Allocation (LDA) topic modeling analysis to\ncharacterize this corpus and identify potential topics of discussion. Word\nfrequency analysis identified both medical and non-medical terms associated\nwith specific ICD10 chapters. The LDA topic modeling analysis extracted 11\ntopics related to social determinants of health risk factors including\nfinancial status, abuse history, social support, risk of death, and mental\nhealth. In addition, the topic modeling approach captured the variation between\ndifferent types of social work notes and across patients with different types\nof diseases or conditions. We demonstrated that social work notes contain rich,\nunique, and otherwise unobtainable information on an individual's SDoH.", "published": "2022-12-02 21:54:55", "link": "http://arxiv.org/abs/2212.01462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NarraSum: A Large-Scale Dataset for Abstractive Narrative Summarization", "abstract": "Narrative summarization aims to produce a distilled version of a narrative to\ndescribe its most salient events and characters. Summarizing a narrative is\nchallenging as it requires an understanding of event causality and character\nbehaviors. To encourage research in this direction, we propose NarraSum, a\nlarge-scale narrative summarization dataset. It contains 122K narrative\ndocuments, which are collected from plot descriptions of movies and TV episodes\nwith diverse genres, and their corresponding abstractive summaries. Experiments\nshow that there is a large performance gap between humans and the\nstate-of-the-art summarization models on NarraSum. We hope that this dataset\nwill promote future research in summarization, as well as broader studies of\nnatural language understanding and generation. The dataset is available at\nhttps://github.com/zhaochaocs/narrasum.", "published": "2022-12-02 22:51:51", "link": "http://arxiv.org/abs/2212.01476v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Nested Named Entity Recognition", "abstract": "While Named Entity Recognition (NER) is a widely studied task, making\ninferences of entities with only a few labeled data has been challenging,\nespecially for entities with nested structures. Unlike flat entities, entities\nand their nested entities are more likely to have similar semantic feature\nrepresentations, drastically increasing difficulties in classifying different\nentity categories in the few-shot setting. Although prior work has briefly\ndiscussed nested structures in the context of few-shot learning, to our best\nknowledge, this paper is the first one specifically dedicated to studying the\nfew-shot nested NER task. Leveraging contextual dependency to distinguish\nnested entities, we propose a Biaffine-based Contrastive Learning (BCL)\nframework. We first design a Biaffine span representation module for learning\nthe contextual span dependency representation for each entity span rather than\nonly learning its semantic representation. We then merge these two\nrepresentations by the residual connection to distinguish nested entities.\nFinally, we build a contrastive learning framework to adjust the representation\ndistribution for larger margin boundaries and more generalized domain transfer\nlearning ability. We conducted experimental studies on three English, German,\nand Russian nested NER datasets. The results show that the BCL outperformed\nthree baseline models on the 1-shot and 5-shot tasks in terms of F1 score.", "published": "2022-12-02 03:42:23", "link": "http://arxiv.org/abs/2212.00953v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Relation-Aware Language-Graph Transformer for Question Answering", "abstract": "Question Answering (QA) is a task that entails reasoning over natural\nlanguage contexts, and many relevant works augment language models (LMs) with\ngraph neural networks (GNNs) to encode the Knowledge Graph (KG) information.\nHowever, most existing GNN-based modules for QA do not take advantage of rich\nrelational information of KGs and depend on limited information interaction\nbetween the LM and the KG. To address these issues, we propose Question\nAnswering Transformer (QAT), which is designed to jointly reason over language\nand graphs with respect to entity relations in a unified manner. Specifically,\nQAT constructs Meta-Path tokens, which learn relation-centric embeddings based\non diverse structural and semantic relations. Then, our Relation-Aware\nSelf-Attention module comprehensively integrates different modalities via the\nCross-Modal Relative Position Bias, which guides information exchange between\nrelevant entites of different modalities. We validate the effectiveness of QAT\non commonsense question answering datasets like CommonsenseQA and OpenBookQA,\nand on a medical question answering dataset, MedQA-USMLE. On all the datasets,\nour method achieves state-of-the-art performance. Our code is available at\nhttp://github.com/mlvlab/QAT.", "published": "2022-12-02 05:10:10", "link": "http://arxiv.org/abs/2212.00975v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Systematic Analysis for Pretrained Language Model Priming for\n  Parameter-Efficient Fine-tuning", "abstract": "Parameter-efficient (PE) methods (like Prompts or Adapters) for adapting\npre-trained language models (PLM) to downstream tasks have been popular\nrecently. However, hindrances still prevent these methods from reaching their\nfull potential. For example, two significant challenges are few-shot adaptation\nand cross-task generalization. To tackle these issues, we propose a general PE\npriming framework to enhance and explore the few-shot adaptation and\ngeneralization ability of PE methods. In this framework, PLMs are primed with\nPE methods for rapidly adapting to various target tasks. To evaluate the\ngeneralization ability of these PE methods, we conduct experiments on a\nfew-shot cross-domain benchmark containing 160 diverse NLP tasks. Our\nexperiment not only reveals the best priming strategy but also verifies that\npriming facilitates the adaptation to target tasks.", "published": "2022-12-02 08:56:53", "link": "http://arxiv.org/abs/2212.01032v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Faithful Rationale for Multi-hop Fact Verification via\n  Salience-Aware Graph Learning", "abstract": "The opaqueness of the multi-hop fact verification model imposes imperative\nrequirements for explainability. One feasible way is to extract rationales, a\nsubset of inputs, where the performance of prediction drops dramatically when\nbeing removed. Though being explainable, most rationale extraction methods for\nmulti-hop fact verification explore the semantic information within each piece\nof evidence individually, while ignoring the topological information\ninteraction among different pieces of evidence. Intuitively, a faithful\nrationale bears complementary information being able to extract other\nrationales through the multi-hop reasoning process. To tackle such\ndisadvantages, we cast explainable multi-hop fact verification as subgraph\nextraction, which can be solved based on graph convolutional network (GCN) with\nsalience-aware graph learning. In specific, GCN is utilized to incorporate the\ntopological interaction information among multiple pieces of evidence for\nlearning evidence representation. Meanwhile, to alleviate the influence of\nnoisy evidence, the salience-aware graph perturbation is induced into the\nmessage passing of GCN. Moreover, the multi-task model with three diagnostic\nproperties of rationale is elaborately designed to improve the quality of an\nexplanation without any explicit annotations. Experimental results on the\nFEVEROUS benchmark show significant gains over previous state-of-the-art\nmethods for both rationale extraction and fact verification.", "published": "2022-12-02 09:54:05", "link": "http://arxiv.org/abs/2212.01060v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22", "abstract": "This paper describes the system developed at the Universitat Polit\\`ecnica de\nCatalunya for the Workshop on Machine Translation 2022 Sign Language\nTranslation Task, in particular, for the sign-to-text direction. We use a\nTransformer model implemented with the Fairseq modeling toolkit. We have\nexperimented with the vocabulary size, data augmentation techniques and\npretraining the model with the PHOENIX-14T dataset. Our system obtains 0.50\nBLEU score for the test set, improving the organizers' baseline by 0.38 BLEU.\nWe remark the poor results for both the baseline and our system, and thus, the\nunreliability of our findings.", "published": "2022-12-02 12:42:24", "link": "http://arxiv.org/abs/2212.01140v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards Diverse, Relevant and Coherent Open-Domain Dialogue Generation\n  via Hybrid Latent Variables", "abstract": "Conditional variational models, using either continuous or discrete latent\nvariables, are powerful for open-domain dialogue response generation. However,\nprevious works show that continuous latent variables tend to reduce the\ncoherence of generated responses. In this paper, we also found that discrete\nlatent variables have difficulty capturing more diverse expressions. To tackle\nthese problems, we combine the merits of both continuous and discrete latent\nvariables and propose a Hybrid Latent Variable (HLV) method. Specifically, HLV\nconstrains the global semantics of responses through discrete latent variables\nand enriches responses with continuous latent variables. Thus, we diversify the\ngenerated responses while maintaining relevance and coherence. In addition, we\npropose Conditional Hybrid Variational Transformer (CHVT) to construct and to\nutilize HLV with transformers for dialogue generation. Through fine-grained\nsymbolic-level semantic information and additive Gaussian mixing, we construct\nthe distribution of continuous variables, prompting the generation of diverse\nexpressions. Meanwhile, to maintain the relevance and coherence, the discrete\nlatent variable is optimized by self-separation training. Experimental results\non two dialogue generation datasets (DailyDialog and Opensubtitles) show that\nCHVT is superior to traditional transformer-based variational mechanism w.r.t.\ndiversity, relevance and coherence metrics. Moreover, we also demonstrate the\nbenefit of applying HLV to fine-tuning two pre-trained dialogue models (PLATO\nand BART-base).", "published": "2022-12-02 12:48:01", "link": "http://arxiv.org/abs/2212.01145v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Joint Open Knowledge Base Canonicalization and Linking", "abstract": "Open Information Extraction (OIE) methods extract a large number of OIE\ntriples (noun phrase, relation phrase, noun phrase) from text, which compose\nlarge Open Knowledge Bases (OKBs). However, noun phrases (NPs) and relation\nphrases (RPs) in OKBs are not canonicalized and often appear in different\nparaphrased textual variants, which leads to redundant and ambiguous facts. To\naddress this problem, there are two related tasks: OKB canonicalization (i.e.,\nconvert NPs and RPs to canonicalized form) and OKB linking (i.e., link NPs and\nRPs with their corresponding entities and relations in a curated Knowledge Base\n(e.g., DBPedia). These two tasks are tightly coupled, and one task can benefit\nsignificantly from the other. However, they have been studied in isolation so\nfar. In this paper, we explore the task of joint OKB canonicalization and\nlinking for the first time, and propose a novel framework JOCL based on factor\ngraph model to make them reinforce each other. JOCL is flexible enough to\ncombine different signals from both tasks, and able to extend to fit any new\nsignals. A thorough experimental study over two large scale OIE triple data\nsets shows that our framework outperforms all the baseline methods for the task\nof OKB canonicalization (OKB linking) in terms of average F1 (accuracy).", "published": "2022-12-02 14:38:58", "link": "http://arxiv.org/abs/2212.01207v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Legal Prompting: Teaching a Language Model to Think Like a Lawyer", "abstract": "Large language models that are capable of zero or few-shot prompting\napproaches have given rise to the new research area of prompt engineering.\nRecent advances showed that for example Chain-of-Thought (CoT) prompts can\nimprove arithmetic or common sense tasks significantly. We explore how such\napproaches fare with legal reasoning tasks and take the COLIEE entailment task\nbased on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning\napproaches. Our findings show that while CoT prompting and fine-tuning with\nexplanations approaches show improvements, the best results are produced by\nprompts that are derived from specific legal reasoning techniques such as IRAC\n(Issue, Rule, Application, Conclusion). Based on our experiments we improve the\n2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best\nsystem of 0.6789 accuracy with an accuracy of 0.7431.", "published": "2022-12-02 17:41:22", "link": "http://arxiv.org/abs/2212.01326v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Moving Beyond Downstream Task Accuracy for Information Retrieval\n  Benchmarking", "abstract": "Neural information retrieval (IR) systems have progressed rapidly in recent\nyears, in large part due to the release of publicly available benchmarking\ntasks. Unfortunately, some dimensions of this progress are illusory: the\nmajority of the popular IR benchmarks today focus exclusively on downstream\ntask accuracy and thus conceal the costs incurred by systems that trade away\nefficiency for quality. Latency, hardware cost, and other efficiency\nconsiderations are paramount to the deployment of IR systems in user-facing\nsettings. We propose that IR benchmarks structure their evaluation methodology\nto include not only metrics of accuracy, but also efficiency considerations\nsuch as a query latency and the corresponding cost budget for a reproducible\nhardware setting. For the popular IR benchmarks MS MARCO and XOR-TyDi, we show\nhow the best choice of IR system varies according to how these efficiency\nconsiderations are chosen and weighed. We hope that future benchmarks will\nadopt these guidelines toward more holistic IR evaluation.", "published": "2022-12-02 17:57:06", "link": "http://arxiv.org/abs/2212.01340v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Event knowledge in large language models: the gap between the impossible\n  and the unlikely", "abstract": "Word co-occurrence patterns in language corpora contain a surprising amount\nof conceptual knowledge. Large language models (LLMs), trained to predict words\nin context, leverage these patterns to achieve impressive performance on\ndiverse semantic tasks requiring world knowledge. An important but understudied\nquestion about LLMs' semantic abilities is whether they acquire generalized\nknowledge of common events. Here, we test whether five pre-trained LLMs (from\n2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions\nof agent-patient interactions than to minimally different implausible versions\nof the same event. Using three curated sets of minimal sentence pairs (total\nn=1,215), we found that pre-trained LLMs possess substantial event knowledge,\noutperforming other distributional language models. In particular, they almost\nalways assign higher likelihood to possible vs. impossible events (The teacher\nbought the laptop vs. The laptop bought the teacher). However, LLMs show less\nconsistent preferences for likely vs. unlikely events (The nanny tutored the\nboy vs. The boy tutored the nanny). In follow-up analyses, we show that (i) LLM\nscores are driven by both plausibility and surface-level sentence features,\n(ii) LLM scores generalize well across syntactic variants (active vs. passive\nconstructions) but less well across semantic variants (synonymous sentences),\n(iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence\nplausibility serves as an organizing dimension in internal LLM representations.\nOverall, our results show that important aspects of event knowledge naturally\nemerge from distributional linguistic patterns, but also highlight a gap\nbetween representations of possible/impossible and likely/unlikely events.", "published": "2022-12-02 23:43:18", "link": "http://arxiv.org/abs/2212.01488v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AGRO: Adversarial Discovery of Error-prone groups for Robust\n  Optimization", "abstract": "Models trained via empirical risk minimization (ERM) are known to rely on\nspurious correlations between labels and task-independent input features,\nresulting in poor generalization to distributional shifts. Group\ndistributionally robust optimization (G-DRO) can alleviate this problem by\nminimizing the worst-case loss over a set of pre-defined groups over training\ndata. G-DRO successfully improves performance of the worst-group, where the\ncorrelation does not hold. However, G-DRO assumes that the spurious\ncorrelations and associated worst groups are known in advance, making it\nchallenging to apply it to new tasks with potentially multiple unknown spurious\ncorrelations. We propose AGRO -- Adversarial Group discovery for\nDistributionally Robust Optimization -- an end-to-end approach that jointly\nidentifies error-prone groups and improves accuracy on them. AGRO equips G-DRO\nwith an adversarial slicing model to find a group assignment for training\nexamples which maximizes worst-case loss over the discovered groups. On the\nWILDS benchmark, AGRO results in 8% higher model performance on average on\nknown worst-groups, compared to prior group discovery approaches used with\nG-DRO. AGRO also improves out-of-distribution performance on SST2, QQP, and\nMS-COCO -- datasets where potential spurious correlations are as yet\nuncharacterized. Human evaluation of ARGO groups shows that they contain\nwell-defined, yet previously unstudied spurious correlations that lead to model\nerrors.", "published": "2022-12-02 00:57:03", "link": "http://arxiv.org/abs/2212.00921v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Masked Contrastive Pre-Training for Efficient Video-Text Retrieval", "abstract": "We present a simple yet effective end-to-end Video-language Pre-training\n(VidLP) framework, Masked Contrastive Video-language Pretraining (MAC), for\nvideo-text retrieval tasks. Our MAC aims to reduce video representation's\nspatial and temporal redundancy in the VidLP model by a mask sampling mechanism\nto improve pre-training efficiency. Comparing conventional temporal sparse\nsampling, we propose to randomly mask a high ratio of spatial regions and only\nfeed visible regions into the encoder as sparse spatial sampling. Similarly, we\nadopt the mask sampling technique for text inputs for consistency. Instead of\nblindly applying the mask-then-prediction paradigm from MAE, we propose a\nmasked-then-alignment paradigm for efficient video-text alignment. The\nmotivation is that video-text retrieval tasks rely on high-level alignment\nrather than low-level reconstruction, and multimodal alignment with masked\nmodeling encourages the model to learn a robust and general multimodal\nrepresentation from incomplete and unstable inputs. Coupling these designs\nenables efficient end-to-end pre-training: reduce FLOPs (60% off), accelerate\npre-training (by 3x), and improve performance. Our MAC achieves\nstate-of-the-art results on various video-text retrieval datasets, including\nMSR-VTT, DiDeMo, and ActivityNet. Our approach is omnivorous to input\nmodalities. With minimal modifications, we achieve competitive results on\nimage-text retrieval tasks.", "published": "2022-12-02 05:44:23", "link": "http://arxiv.org/abs/2212.00986v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AGO: Boosting Mobile AI Inference Performance by Removing Constraints on\n  Graph Optimization", "abstract": "Traditional deep learning compilers rely on heuristics for subgraph\ngeneration, which impose extra constraints on graph optimization, e.g., each\nsubgraph can only contain at most one complex operator. In this paper, we\npropose AGO, a framework for graph optimization with arbitrary structures to\nboost the inference performance of deep models by removing such constraints. To\ncreate new optimization opportunities for complicated subgraphs, we propose\nintensive operator fusion, which can effectively stitch multiple complex\noperators together for better performance. Further, we design a graph\npartitioning scheme that allows an arbitrary structure for each subgraph while\nguaranteeing the acyclic property among all generated subgraphs. Additionally,\nto enable efficient performance tuning on complicated subgraphs, we devise a\nnovel divide-and-conquer tuning mechanism to orchestrate different system\ncomponents. Through extensive experiments on various neural networks and mobile\ndevices, we show that our system can improve the inference performance by up to\n3.3x when compared with state-of-the-art deep compilers.", "published": "2022-12-02 07:16:49", "link": "http://arxiv.org/abs/2212.01005v1", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "SoftCorrect: Error Correction with Soft Detection for Automatic Speech\n  Recognition", "abstract": "Error correction in automatic speech recognition (ASR) aims to correct those\nincorrect words in sentences generated by ASR models. Since recent ASR models\nusually have low word error rate (WER), to avoid affecting originally correct\ntokens, error correction models should only modify incorrect words, and\ntherefore detecting incorrect words is important for error correction. Previous\nworks on error correction either implicitly detect error words through\ntarget-source attention or CTC (connectionist temporal classification) loss, or\nexplicitly locate specific deletion/substitution/insertion errors. However,\nimplicit error detection does not provide clear signal about which tokens are\nincorrect and explicit error detection suffers from low detection accuracy. In\nthis paper, we propose SoftCorrect with a soft error detection mechanism to\navoid the limitations of both explicit and implicit error detection.\nSpecifically, we first detect whether a token is correct or not through a\nprobability produced by a dedicatedly designed language model, and then design\na constrained CTC loss that only duplicates the detected incorrect tokens to\nlet the decoder focus on the correction of error tokens. Compared with implicit\nerror detection with CTC loss, SoftCorrect provides explicit signal about which\nwords are incorrect and thus does not need to duplicate every token but only\nincorrect tokens; compared with explicit error detection, SoftCorrect does not\ndetect specific deletion/substitution/insertion errors but just leaves it to\nCTC loss. Experiments on AISHELL-1 and Aidatatang datasets show that\nSoftCorrect achieves 26.1% and 9.4% CER reduction respectively, outperforming\nprevious works by a large margin, while still enjoying fast speed of parallel\ngeneration.", "published": "2022-12-02 09:11:32", "link": "http://arxiv.org/abs/2212.01039v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Mutual Learning for Cued Speech Recognition", "abstract": "Automatic Cued Speech Recognition (ACSR) provides an intelligent\nhuman-machine interface for visual communications, where the Cued Speech (CS)\nsystem utilizes lip movements and hand gestures to code spoken language for\nhearing-impaired people. Previous ACSR approaches often utilize direct feature\nconcatenation as the main fusion paradigm. However, the asynchronous modalities\ni.e., lip, hand shape and hand position) in CS may cause interference for\nfeature concatenation. To address this challenge, we propose a transformer\nbased cross-modal mutual learning framework to prompt multi-modal interaction.\nCompared with the vanilla self-attention, our model forces modality-specific\ninformation of different modalities to pass through a modality-invariant\ncodebook, collating linguistic representations for tokens of each modality.\nThen the shared linguistic knowledge is used to re-synchronize multi-modal\nsequences. Moreover, we establish a novel large-scale multi-speaker CS dataset\nfor Mandarin Chinese. To our knowledge, this is the first work on ACSR for\nMandarin Chinese. Extensive experiments are conducted for different languages\ni.e., Chinese, French, and British English). Results demonstrate that our model\nexhibits superior recognition performance to the state-of-the-art by a large\nmargin.", "published": "2022-12-02 10:45:33", "link": "http://arxiv.org/abs/2212.01083v2", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Semantic Role Labeling Meets Definition Modeling: Using Natural Language\n  to Describe Predicate-Argument Structures", "abstract": "One of the common traits of past and present approaches for Semantic Role\nLabeling (SRL) is that they rely upon discrete labels drawn from a predefined\nlinguistic inventory to classify predicate senses and their arguments. However,\nwe argue this need not be the case. In this paper, we present an approach that\nleverages Definition Modeling to introduce a generalized formulation of SRL as\nthe task of describing predicate-argument structures using natural language\ndefinitions instead of discrete labels. Our novel formulation takes a first\nstep towards placing interpretability and flexibility foremost, and yet our\nexperiments and analyses on PropBank-style and FrameNet-style, dependency-based\nand span-based SRL also demonstrate that a flexible model with an interpretable\noutput does not necessarily come at the expense of performance. We release our\nsoftware for research purposes at https://github.com/SapienzaNLP/dsrl.", "published": "2022-12-02 11:19:16", "link": "http://arxiv.org/abs/2212.01094v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Nonparametric Masked Language Modeling", "abstract": "Existing language models (LMs) predict tokens with a softmax over a finite\nvocabulary, which can make it difficult to predict rare tokens or phrases. We\nintroduce NPM, the first nonparametric masked language model that replaces this\nsoftmax with a nonparametric distribution over every phrase in a reference\ncorpus. NPM fills in the [MASK] solely from retrieving a token from a text\ncorpus. We show that NPM can be efficiently trained with a contrastive\nobjective and an in-batch approximation to full corpus retrieval. Zero-shot\nevaluation on 16 tasks including classification, fact probing and question\nanswering demonstrates that NPM outperforms significantly larger parametric\nmodels, either with or without a retrieve-and-generate approach. It is\nparticularly better at dealing with rare patterns (word senses or facts) and\npredicting rare or nearly unseen words (e.g., non-Latin script). We release the\nmodel and code at github.com/facebookresearch/NPM.", "published": "2022-12-02 18:10:42", "link": "http://arxiv.org/abs/2212.01349v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning", "abstract": "We propose a new paradigm to continually evolve pretrained models, denoted\nColD Fusion. It provides the benefits of multitask learning but leverages\ndistributed computation with limited communication and eliminates the need for\nshared data. Consequentially, ColD Fusion can give rise to a synergistic loop,\nwhere finetuned models can be recycled to continually improve the pretrained\nmodel they are based upon. We show that ColD Fusion yields comparable benefits\nto multitask training by producing a model that (a) attains strong performance\non all of the datasets it was trained on; and (b) is a better starting point\nfor finetuning on unseen datasets. We show that ColD Fusion outperforms RoBERTa\nand even previous multitask models. Specifically, when training and testing on\n35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.33 points\non average without any changes to the architecture.", "published": "2022-12-02 18:59:04", "link": "http://arxiv.org/abs/2212.01378v2", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Continual Learning for On-Device Speech Recognition using Disentangled\n  Conformers", "abstract": "Automatic speech recognition research focuses on training and evaluating on\nstatic datasets. Yet, as speech models are increasingly deployed on personal\ndevices, such models encounter user-specific distributional shifts. To simulate\nthis real-world scenario, we introduce LibriContinual, a continual learning\nbenchmark for speaker-specific domain adaptation derived from LibriVox\naudiobooks, with data corresponding to 118 individual speakers and 6 train\nsplits per speaker of different sizes. Additionally, current speech recognition\nmodels and continual learning algorithms are not optimized to be\ncompute-efficient. We adapt a general-purpose training algorithm NetAug for ASR\nand create a novel Conformer variant called the DisConformer (Disentangled\nConformer). This algorithm produces ASR models consisting of a frozen 'core'\nnetwork for general-purpose use and several tunable 'augment' networks for\nspeaker-specific tuning. Using such models, we propose a novel\ncompute-efficient continual learning algorithm called DisentangledCL. Our\nexperiments show that the DisConformer models significantly outperform\nbaselines on general ASR i.e. LibriSpeech (15.58% rel. WER on test-other). On\nspeaker-specific LibriContinual they significantly outperform\ntrainable-parameter-matched baselines (by 20.65% rel. WER on test) and even\nmatch fully finetuned baselines in some settings.", "published": "2022-12-02 18:58:51", "link": "http://arxiv.org/abs/2212.01393v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Twitter Data Analysis: Izmir Earthquake Case", "abstract": "T\\\"urkiye is located on a fault line; earthquakes often occur on a large and\nsmall scale. There is a need for effective solutions for gathering current\ninformation during disasters. We can use social media to get insight into\npublic opinion. This insight can be used in public relations and disaster\nmanagement. In this study, Twitter posts on Izmir Earthquake that took place on\nOctober 2020 are analyzed. We question if this analysis can be used to make\nsocial inferences on time. Data mining and natural language processing (NLP)\nmethods are used for this analysis. NLP is used for sentiment analysis and\ntopic modelling. The latent Dirichlet Allocation (LDA) algorithm is used for\ntopic modelling. We used the Bidirectional Encoder Representations from\nTransformers (BERT) model working with Transformers architecture for sentiment\nanalysis. It is shown that the users shared their goodwill wishes and aimed to\ncontribute to the initiated aid activities after the earthquake. The users\ndesired to make their voices heard by competent institutions and organizations.\nThe proposed methods work effectively. Future studies are also discussed.", "published": "2022-12-02 21:30:34", "link": "http://arxiv.org/abs/2212.01453v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.m"], "primary_category": "cs.CL"}
{"title": "Thread With Caution: Proactively Helping Users Assess and Deescalate\n  Tension in Their Online Discussions", "abstract": "Incivility remains a major challenge for online discussion platforms, to such\nan extent that even conversations between well-intentioned users can often\nderail into uncivil behavior. Traditionally, platforms have relied on\nmoderators to -- with or without algorithmic assistance -- take corrective\nactions such as removing comments or banning users. In this work we propose a\ncomplementary paradigm that directly empowers users by proactively enhancing\ntheir awareness about existing tension in the conversation they are engaging in\nand actively guides them as they are drafting their replies to avoid further\nescalation.\n  As a proof of concept for this paradigm, we design an algorithmic tool that\nprovides such proactive information directly to users, and conduct a user study\nin a popular discussion platform. Through a mixed methods approach combining\nsurveys with a randomized controlled experiment, we uncover qualitative and\nquantitative insights regarding how the participants utilize and react to this\ninformation. Most participants report finding this proactive paradigm valuable,\nnoting that it helps them to identify tension that they may have otherwise\nmissed and prompts them to further reflect on their own replies and to revise\nthem. These effects are corroborated by a comparison of how the participants\ndraft their reply when our tool warns them that their conversation is at risk\nof derailing into uncivil behavior versus in a control condition where the tool\nis disabled. These preliminary findings highlight the potential of this\nuser-centered paradigm and point to concrete directions for future\nimplementations.", "published": "2022-12-02 19:00:03", "link": "http://arxiv.org/abs/2212.01401v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "physics.soc-ph"], "primary_category": "cs.HC"}
{"title": "Avoiding spurious correlations via logit correction", "abstract": "Empirical studies suggest that machine learning models trained with empirical\nrisk minimization (ERM) often rely on attributes that may be spuriously\ncorrelated with the class labels. Such models typically lead to poor\nperformance during inference for data lacking such correlations. In this work,\nwe explicitly consider a situation where potential spurious correlations are\npresent in the majority of training data. In contrast with existing approaches,\nwhich use the ERM model outputs to detect the samples without spurious\ncorrelations and either heuristically upweight or upsample those samples, we\npropose the logit correction (LC) loss, a simple yet effective improvement on\nthe softmax cross-entropy loss, to correct the sample logit. We demonstrate\nthat minimizing the LC loss is equivalent to maximizing the group-balanced\naccuracy, so the proposed LC could mitigate the negative impacts of spurious\ncorrelations. Our extensive experimental results further reveal that the\nproposed LC loss outperforms state-of-the-art solutions on multiple popular\nbenchmarks by a large margin, an average 5.5\\% absolute improvement, without\naccess to spurious attribute labels. LC is also competitive with oracle methods\nthat make use of the attribute labels. Code is available at\nhttps://github.com/shengliu66/LC.", "published": "2022-12-02 20:30:59", "link": "http://arxiv.org/abs/2212.01433v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "eess.IV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Injecting Spatial Information for Monaural Speech Enhancement via\n  Knowledge Distillation", "abstract": "Monaural speech enhancement (SE) provides a versatile and cost-effective\napproach to SE tasks by utilizing recordings from a single microphone. However,\nthe monaural SE lags performance behind multi-channel SE as the monaural SE\nmethods are unable to extract spatial information from one-channel recordings,\nwhich greatly limits their application scenarios. To address this issue, we\ninject spatial information into the monaural SE model and propose a knowledge\ndistillation strategy to enable the monaural SE model to learn binaural speech\nfeatures from the binaural SE model, which makes monaural SE model possible to\nreconstruct higher intelligibility and quality enhanced speeches under low\nsignal-to-noise ratio (SNR) conditions. Extensive experiments show that our\nproposed monaural SE model by injecting spatial information via knowledge\ndistillation achieves favorable performance against other monaural SE models\nwith fewer parameters.", "published": "2022-12-02 07:49:37", "link": "http://arxiv.org/abs/2212.01012v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ExARN: self-attending RNN for target speaker extraction", "abstract": "Target speaker extraction is to extract the target speaker, specified by\nenrollment utterance, in an environment with other competing speakers.\nTherefore, the task needs to solve two problems, speaker identification and\nseparation, at the same time. In this paper, we combine self-attention and\nRecurrent Neural Networks (RNN). Further, we exploit various ways to combining\ndifferent auxiliary information with mixed representations. Experimental\nresults show that our proposed model achieves excellent performance on the task\nof target speaker extraction.", "published": "2022-12-02 11:38:42", "link": "http://arxiv.org/abs/2212.01106v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Relative Acoustic Features for Distance Estimation in Smart-Homes", "abstract": "Any audio recording encapsulates the unique fingerprint of the associated\nacoustic environment, namely the background noise and reverberation.\nConsidering the scenario of a room equipped with a fixed smart speaker device\nwith one or more microphones and a wearable smart device (watch, glasses or\nsmartphone), we employed the improved proportionate normalized least mean\nsquare adaptive filter to estimate the relative room impulse response mapping\nthe audio recordings of the two devices. We performed inter-device distance\nestimation by exploiting a new set of features obtained extending the\ndefinition of some acoustic attributes of the room impulse response to its\nrelative version. In combination with the sparseness measure of the estimated\nrelative room impulse response, the relative features allow precise\ninter-device distance estimation which can be exploited for tasks such as best\nmicrophone selection or acoustic scene analysis. Experimental results from\nsimulated rooms of different dimensions and reverberation times demonstrate the\neffectiveness of this computationally lightweight approach for smart home\nacoustic ranging applications", "published": "2022-12-02 16:58:35", "link": "http://arxiv.org/abs/2212.01306v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigations on the Influence of Combined Inter-Aural Cue Distortions\n  on Overall Audio Quality", "abstract": "There is a considerable interest in developing algorithms that can predict\naudio quality of perceptually coded signals to avoid the cost of extensive\nlistening tests during development time. While many established algorithms for\npredicting the perceived quality of signals with monaural (timbral) distortions\nare available (PEAQ, POLQA), predicting the quality degradation of stereo and\nmulti-channel spatial signals is still considered a challenge. Audio quality\ndegradation arising from spatial distortions is usually measured in terms of\nwell known inter-aural cue distortion measures such as Inter-aural Level\nDifference Distortions (ILDD), Inter-aural Time Difference Distortions (ITDD)\nand Inter-aural Cross Correlation Distortions (IACCD). However, the extent to\nwhich their interaction influences the overall audio quality degradation in\ncomplex signals as expressed - for example - in a multiple stimuli test is not\nyet thoroughly studied. We propose a systematic approach that introduces\ncontrolled combinations of spatial distortions on a representative set of\nsignals and evaluates their influence on overall perceived quality degradation\nby analyzing listening test scores over said signals. From this study we derive\nguidelines for designing meaningful distortion measures that consider\ninter-aural cue distortion interactions.", "published": "2022-12-02 20:20:16", "link": "http://arxiv.org/abs/2212.01427v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NEAL: An open-source tool for audio annotation", "abstract": "Passive acoustic monitoring is used widely in ecology, biodiversity, and\nconservation studies. Data sets collected via acoustic monitoring are often\nextremely large and built to be processed automatically using Artificial\nIntelligence and Machine learning models, which aim to replicate the work of\ndomain experts. These models, being supervised learning algorithms, need to be\ntrained on high quality annotations produced by experts. Since the experts are\noften resource-limited, a cost-effective process for annotating audio is needed\nto get maximal use out of the data. We present an open-source interactive audio\ndata annotation tool, NEAL (Nature+Energy Audio Labeller). Built using R and\nthe associated Shiny framework, the tool provides a reactive environment where\nusers can quickly annotate audio files and adjust settings that automatically\nchange the corresponding elements of the user interface. The app has been\ndesigned with the goal of having both expert birders and citizen scientists\ncontribute to acoustic annotation projects. The popularity and flexibility of R\nprogramming in bioacoustics means that the Shiny app can be modified for other\nbird labelling data sets, or even to generic audio labelling tasks. We\ndemonstrate the app by labelling data collected from wind farm sites across\nIreland.", "published": "2022-12-02 21:45:12", "link": "http://arxiv.org/abs/2212.01457v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can we still use PEAQ? A Performance Analysis of the ITU Standard for\n  the Objective Assessment of Perceived Audio Quality", "abstract": "The Perceptual Evaluation of Audio Quality (PEAQ) method as described in the\nInternational Telecommunication Union (ITU) recommendation ITU-R BS.1387 has\nbeen widely used for computationally estimating the quality of perceptually\ncoded audio signals without the need for extensive subjective listening tests.\nHowever, many reports have highlighted clear limitations of the scheme after\nthe end of its standardization, particularly involving signals coded with newer\ntechnologies such as bandwidth extension or parametric multi-channel coding.\nUntil now, no other method for measuring the quality of both speech and audio\nsignals has been standardized by the ITU. Therefore, a further investigation of\nthe causes for these limitations would be beneficial to a possible update of\nsaid scheme. Our experimental results indicate that the performance of PEAQ's\nmodel of disturbance loudness is still as good as (and sometimes superior to)\nother state-of-the-art objective measures, albeit with varying performance\ndepending on the type of degraded signal content (i.e. speech or music). This\nfinding evidences the need for an improved cognitive model. In addition,\nresults indicate that an updated mapping of Model Output Values (MOVs) to\nPEAQ's Distortion Index (DI) based on newer training data can greatly improve\nperformance. Finally, some suggestions for the improvement of PEAQ are provided\nbased on the reported results and comparison to other systems.", "published": "2022-12-02 22:09:15", "link": "http://arxiv.org/abs/2212.01467v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention\n  Mechanism for Symbolic Music Modeling", "abstract": "Following the success of the transformer architecture in the natural language\ndomain, transformer-like architectures have been widely applied to the domain\nof symbolic music recently. Symbolic music and text, however, are two different\nmodalities. Symbolic music contains multiple attributes, both absolute\nattributes (e.g., pitch) and relative attributes (e.g., pitch interval). These\nrelative attributes shape human perception of musical motifs. These important\nrelative attributes, however, are mostly ignored in existing symbolic music\nmodeling methods with the main reason being the lack of a musically-meaningful\nembedding space where both the absolute and relative embeddings of the symbolic\nmusic tokens can be efficiently represented. In this paper, we propose the\nFundamental Music Embedding (FME) for symbolic music based on a bias-adjusted\nsinusoidal encoding within which both the absolute and the relative attributes\ncan be embedded and the fundamental musical properties (e.g., translational\ninvariance) are explicitly preserved. Taking advantage of the proposed FME, we\nfurther propose a novel attention mechanism based on the relative index, pitch\nand onset embeddings (RIPO attention) such that the musical domain knowledge\ncan be fully utilized for symbolic music modeling. Experiment results show that\nour proposed model: RIPO transformer which utilizes FME and RIPO attention\noutperforms the state-of-the-art transformers (i.e., music transformer, linear\ntransformer) in a melody completion task. Moreover, using the RIPO transformer\nin a downstream music generation task, we notice that the notorious\ndegeneration phenomenon no longer exists and the music generated by the RIPO\ntransformer outperforms the music generated by state-of-the-art transformer\nmodels in both subjective and objective evaluations.", "published": "2022-12-02 05:04:31", "link": "http://arxiv.org/abs/2212.00973v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Sonus Texere! Automated Dense Soundtrack Construction for Books using\n  Movie Adaptations", "abstract": "Reading, much like music listening, is an immersive experience that\ntransports readers while taking them on an emotional journey. Listening to\ncomplementary music has the potential to amplify the reading experience,\nespecially when the music is stylistically cohesive and emotionally relevant.\nIn this paper, we propose the first fully automatic method to build a dense\nsoundtrack for books, which can play high-quality instrumental music for the\nentirety of the reading duration. Our work employs a unique text processing and\nmusic weaving pipeline that determines the context and emotional composition of\nscenes in a chapter. This allows our method to identify and play relevant\nexcerpts from the soundtrack of the book's movie adaptation. By relying on the\nmovie composer's craftsmanship, our book soundtracks include expert-made motifs\nand other scene-specific musical characteristics. We validate the design\ndecisions of our approach through a perceptual study. Our readers note that the\nbook soundtrack greatly enhanced their reading experience, due to high\nimmersiveness granted via uninterrupted and style-consistent music, and a\nheightened emotional state attained via high precision emotion and scene\ncontext recognition.", "published": "2022-12-02 08:57:20", "link": "http://arxiv.org/abs/2212.01033v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Role of Audio in Audio-Visual Video Summarization", "abstract": "Video summarization attracts attention for efficient video representation,\nretrieval, and browsing to ease volume and traffic surge problems. Although\nvideo summarization mostly uses the visual channel for compaction, the benefits\nof audio-visual modeling appeared in recent literature. The information coming\nfrom the audio channel can be a result of audio-visual correlation in the video\ncontent. In this study, we propose a new audio-visual video summarization\nframework integrating four ways of audio-visual information fusion with\nGRU-based and attention-based networks. Furthermore, we investigate a new\nexplainability methodology using audio-visual canonical correlation analysis\n(CCA) to better understand and explain the role of audio in the video\nsummarization task. Experimental evaluations on the TVSum dataset attain F1\nscore and Kendall-tau score improvements for the audio-visual video\nsummarization. Furthermore, splitting video content on TVSum and COGNIMUSE\ndatasets based on audio-visual CCA as positively and negatively correlated\nvideos yields a strong performance improvement over the positively correlated\nvideos for audio-only and audio-visual video summarization.", "published": "2022-12-02 09:11:49", "link": "http://arxiv.org/abs/2212.01040v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "AccEar: Accelerometer Acoustic Eavesdropping with Unconstrained\n  Vocabulary", "abstract": "With the increasing popularity of voice-based applications, acoustic\neavesdropping has become a serious threat to users' privacy. While on\nsmartphones the access to microphones needs an explicit user permission,\nacoustic eavesdropping attacks can rely on motion sensors (such as\naccelerometer and gyroscope), which access is unrestricted. However, previous\ninstances of such attacks can only recognize a limited set of pre-trained words\nor phrases. In this paper, we present AccEar, an accelerometerbased acoustic\neavesdropping attack that can reconstruct any audio played on the smartphone's\nloudspeaker with unconstrained vocabulary. We show that an attacker can employ\na conditional Generative Adversarial Network (cGAN) to reconstruct highfidelity\naudio from low-frequency accelerometer signals. The presented cGAN model learns\nto recreate high-frequency components of the user's voice from low-frequency\naccelerometer signals through spectrogram enhancement. We assess the\nfeasibility and effectiveness of AccEar attack in a thorough set of experiments\nusing audio from 16 public personalities. As shown by the results in both\nobjective and subjective evaluations, AccEar successfully reconstructs user\nspeeches from accelerometer signals in different scenarios including varying\nsampling rate, audio volume, device model, etc.", "published": "2022-12-02 09:13:28", "link": "http://arxiv.org/abs/2212.01042v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Objective Assessment of Spatial Audio Quality using Directional Loudness\n  Maps", "abstract": "This work introduces a feature extracted from stereophonic/binaural audio\nsignals aiming to represent a measure of perceived quality degradation in\nprocessed spatial auditory scenes. The feature extraction technique is based on\na simplified stereo signal model considering auditory events positioned towards\na given direction in the stereo field using amplitude panning (AP) techniques.\nWe decompose the stereo signal into a set of directional signals for given AP\nvalues in the Short-Time Fourier Transform domain and calculate their overall\nloudness to form a directional loudness representation or maps. Then, we\ncompare directional loudness maps of a reference signal and a deteriorated\nversion to derive a distortion measure aiming to describe the associated\nperceived degradation scores reported in listening tests. The measure is then\ntested on an extensive listening test database with stereo signals processed by\nstate-of-the-art perceptual audio codecs using non waveform-preserving\ntechniques such as bandwidth extension and joint stereo coding, known for\npresenting a challenge to existing quality predictors. Results suggest that the\nderived distortion measure can be incorporated as an extension to existing\nautomated perceptual quality assessment algorithms for improving prediction on\nspatially coded audio signals.", "published": "2022-12-02 21:22:29", "link": "http://arxiv.org/abs/2212.01451v1", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
