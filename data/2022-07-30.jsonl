{"title": "Dynamically Retrieving Knowledge via Query Generation for Informative\n  Dialogue Generation", "abstract": "Knowledge-driven dialog system has recently made remarkable breakthroughs.\nCompared with general dialog systems, superior knowledge-driven dialog systems\ncan generate more informative and knowledgeable responses with pre-provided\nknowledge. However, in practical applications, the dialog system cannot be\nprovided with corresponding knowledge in advance because it cannot know in\nadvance the development of the conversation. Therefore, in order to make the\nknowledge dialogue system more practical, it is vital to find a way to retrieve\nrelevant knowledge based on the dialogue history. To solve this problem, we\ndesign a knowledge-driven dialog system named DRKQG (Dynamically Retrieving\nKnowledge via Query Generation for informative dialog response). Specifically,\nthe system can be divided into two modules: the query generation module and the\ndialog generation module. First, a time-aware mechanism is utilized to capture\ncontext information, and a query can be generated for retrieving knowledge\nthrough search engine. Then, we integrate the copy mechanism and transformers,\nwhich allows the response generation module to produce responses derived from\nthe context and retrieved knowledge. Experimental results at LIC2022, Language\nand Intelligence Technology Competition, show that our module outperforms the\nbaseline model by a large margin on automatic evaluation metrics, while human\nevaluation by the Baidu Linguistics team shows that our system achieves\nimpressive results in Factually Correct and Knowledgeable.", "published": "2022-07-30 03:05:43", "link": "http://arxiv.org/abs/2208.00128v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ELF22: A Context-based Counter Trolling Dataset to Combat Internet\n  Trolls", "abstract": "Online trolls increase social costs and cause psychological damage to\nindividuals. With the proliferation of automated accounts making use of bots\nfor trolling, it is difficult for targeted individual users to handle the\nsituation both quantitatively and qualitatively. To address this issue, we\nfocus on automating the method to counter trolls, as counter responses to\ncombat trolls encourage community users to maintain ongoing discussion without\ncompromising freedom of expression. For this purpose, we propose a novel\ndataset for automatic counter response generation. In particular, we\nconstructed a pair-wise dataset that includes troll comments and counter\nresponses with labeled response strategies, which enables models fine-tuned on\nour dataset to generate responses by varying counter responses according to the\nspecified strategy. We conducted three tasks to assess the effectiveness of our\ndataset and evaluated the results through both automatic and human evaluation.\nIn human evaluation, we demonstrate that the model fine-tuned on our dataset\nshows a significantly improved performance in strategy-controlled sentence\ngeneration.", "published": "2022-07-30 10:14:41", "link": "http://arxiv.org/abs/2208.00176v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Masked Autoencoders As The Unified Learners For Pre-Trained Sentence\n  Representation", "abstract": "Despite the progresses on pre-trained language models, there is a lack of\nunified frameworks for pre-trained sentence representation. As such, it calls\nfor different pre-training methods for specific scenarios, and the pre-trained\nmodels are likely to be limited by their universality and representation\nquality. In this work, we extend the recently proposed MAE style pre-training\nstrategy, RetroMAE, such that it may effectively support a wide variety of\nsentence representation tasks. The extended framework consists of two stages,\nwith RetroMAE conducted throughout the process. The first stage performs\nRetroMAE over generic corpora, like Wikipedia, BookCorpus, etc., from which the\nbase model is learned. The second stage takes place on domain-specific data,\ne.g., MS MARCO and NLI, where the base model is continuingly trained based on\nRetroMAE and contrastive learning. The pre-training outputs at the two stages\nmay serve different applications, whose effectiveness are verified with\ncomprehensive experiments. Concretely, the base model are proved to be\neffective for zero-shot retrieval, with remarkable performances achieved on\nBEIR benchmark. The continuingly pre-trained models further benefit more\ndownstream tasks, including the domain-specific dense retrieval on MS MARCO,\nNatural Questions, and the sentence embeddings' quality for standard STS and\ntransfer tasks in SentEval. The empirical insights of this work may inspire the\nfuture design of sentence representation pre-training. Our pre-trained models\nand source code will be released to the public communities.", "published": "2022-07-30 14:34:55", "link": "http://arxiv.org/abs/2208.00231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cause-and-Effect Analysis of ADAS: A Comparison Study between Literature\n  Review and Complaint Data", "abstract": "Advanced driver assistance systems (ADAS) are designed to improve vehicle\nsafety. However, it is difficult to achieve such benefits without understanding\nthe causes and limitations of the current ADAS and their possible solutions.\nThis study 1) investigated the limitations and solutions of ADAS through a\nliterature review, 2) identified the causes and effects of ADAS through\nconsumer complaints using natural language processing models, and 3) compared\nthe major differences between the two. These two lines of research identified\nsimilar categories of ADAS causes, including human factors, environmental\nfactors, and vehicle factors. However, academic research focused more on human\nfactors of ADAS issues and proposed advanced algorithms to mitigate such issues\nwhile drivers complained more of vehicle factors of ADAS failures, which led to\nassociated top consequences. The findings from these two sources tend to\ncomplement each other and provide important implications for the improvement of\nADAS in the future.", "published": "2022-07-30 15:15:58", "link": "http://arxiv.org/abs/2208.00249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Smoothing Entailment Graphs with Language Models", "abstract": "The diversity and Zipfian frequency distribution of natural language\npredicates in corpora leads to sparsity in Entailment Graphs (EGs) built by\nOpen Relation Extraction (ORE). EGs are computationally efficient and\nexplainable models of natural language inference, but as symbolic models, they\nfail if a novel premise or hypothesis vertex is missing at test-time. We\npresent theory and methodology for overcoming such sparsity in symbolic models.\nFirst, we introduce a theory of optimal smoothing of EGs by constructing\ntransitive chains. We then demonstrate an efficient, open-domain, and\nunsupervised smoothing method using an off-the-shelf Language Model to find\napproximations of missing premise predicates. This improves recall by 25.1 and\n16.3 percentage points on two difficult directional entailment datasets, while\nraising average precision and maintaining model explainability. Further, in a\nQA task we show that EG smoothing is most useful for answering questions with\nlesser supporting text, where missing premise predicates are more costly.\nFinally, controlled experiments with WordNet confirm our theory and show that\nhypothesis smoothing is difficult, but possible in principle.", "published": "2022-07-30 22:15:22", "link": "http://arxiv.org/abs/2208.00318v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
