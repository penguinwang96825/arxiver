{"title": "A Sentence Compression Based Framework to Query-Focused Multi-Document\n  Summarization", "abstract": "We consider the problem of using sentence compression techniques to\nfacilitate query-focused multi-document summarization. We present a\nsentence-compression-based framework for the task, and design a series of\nlearning-based compression models built on parse trees. An innovative beam\nsearch decoder is proposed to efficiently find highly probable compressions.\nUnder this framework, we show how to integrate various indicative metrics such\nas linguistic motivation and query relevance into the compression process by\nderiving a novel formulation of a compression scoring function. Our best model\nachieves statistically significant improvement over the state-of-the-art\nsystems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2\nrespectively) for the DUC 2006 and 2007 summarization task.", "published": "2016-06-24 02:57:04", "link": "http://arxiv.org/abs/1606.07548v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation method of word embedding by roots and affixes", "abstract": "Word embedding has been shown to be remarkably effective in a lot of Natural\nLanguage Processing tasks. However, existing models still have a couple of\nlimitations in interpreting the dimensions of word vector. In this paper, we\nprovide a new approach---roots and affixes model(RAAM)---to interpret it from\nthe intrinsic structures of natural language. Also it can be used as an\nevaluation measure of the quality of word embedding. We introduce the\ninformation entropy into our model and divide the dimensions into two\ncategories, just like roots and affixes in lexical semantics. Then considering\neach category as a whole rather than individually. We experimented with English\nWikipedia corpus. Our result show that there is a negative linear relation\nbetween the two attributes and a high positive correlation between our model\nand downstream semantic evaluation tasks.", "published": "2016-06-24 08:35:01", "link": "http://arxiv.org/abs/1606.07601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Issues in evaluating semantic spaces using word analogies", "abstract": "The offset method for solving word analogies has become a standard evaluation\ntool for vector-space semantic models: it is considered desirable for a space\nto represent semantic relations as consistent vector offsets. We show that the\nmethod's reliance on cosine similarity conflates offset consistency with\nlargely irrelevant neighborhood structure, and propose simple baselines that\nshould be used to improve the utility of the method in vector space evaluation.", "published": "2016-06-24 15:59:14", "link": "http://arxiv.org/abs/1606.07736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The emotional arcs of stories are dominated by six basic shapes", "abstract": "Advances in computing power, natural language processing, and digitization of\ntext now make it possible to study a culture's evolution through its texts\nusing a \"big data\" lens. Our ability to communicate relies in part upon a\nshared emotional experience, with stories often following distinct emotional\ntrajectories and forming patterns that are meaningful to us. Here, by\nclassifying the emotional arcs for a filtered subset of 1,327 stories from\nProject Gutenberg's fiction collection, we find a set of six core emotional\narcs which form the essential building blocks of complex emotional\ntrajectories. We strengthen our findings by separately applying Matrix\ndecomposition, supervised learning, and unsupervised learning. For each of\nthese six core emotional arcs, we examine the closest characteristic stories in\npublication today and find that particular emotional arcs enjoy greater\nsuccess, as measured by downloads.", "published": "2016-06-24 18:00:42", "link": "http://arxiv.org/abs/1606.07772v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequential Convolutional Neural Networks for Slot Filling in Spoken\n  Language Understanding", "abstract": "We investigate the usage of convolutional neural networks (CNNs) for the slot\nfilling task in spoken language understanding. We propose a novel CNN\narchitecture for sequence labeling which takes into account the previous\ncontext words with preserved order information and pays special attention to\nthe current word with its surrounding context. Moreover, it combines the\ninformation from the past and the future words for classification. Our proposed\nCNN architecture outperforms even the previously best ensembling recurrent\nneural network model and achieves state-of-the-art results with an F1-score of\n95.61% on the ATIS benchmark dataset without using any additional linguistic\nknowledge and resources.", "published": "2016-06-24 18:35:56", "link": "http://arxiv.org/abs/1606.07783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Topic Modeling Approaches to Decision Summarization in\n  Spoken Meetings", "abstract": "We present a token-level decision summarization framework that utilizes the\nlatent topic structures of utterances to identify \"summary-worthy\" words.\nConcretely, a series of unsupervised topic models is explored and experimental\nresults show that fine-grained topic models, which discover topics at the\nutterance-level rather than the document-level, can better identify the gist of\nthe decision-making process. Moreover, our proposed token-level summarization\napproach, which is able to remove redundancies within utterances, outperforms\nexisting utterance ranking based summarization methods. Finally, context\ninformation is also investigated to add additional relevant information to the\nsummary.", "published": "2016-06-24 20:17:44", "link": "http://arxiv.org/abs/1606.07829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Focused Meeting Summarization via Unsupervised Relation Extraction", "abstract": "We present a novel unsupervised framework for focused meeting summarization\nthat views the problem as an instance of relation extraction. We adapt an\nexisting in-domain relation learner (Chen et al., 2011) by exploiting a set of\ntask-specific constraints and features. We evaluate the approach on a decision\nsummarization task and show that it outperforms unsupervised utterance-level\nextractive summarization baselines as well as an existing generic\nrelation-extraction-based summarization method. Moreover, our approach produces\nsummaries competitive with those generated by supervised methods in terms of\nthe standard ROUGE score.", "published": "2016-06-24 22:49:56", "link": "http://arxiv.org/abs/1606.07849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interactive Semantic Featuring for Text Classification", "abstract": "In text classification, dictionaries can be used to define\nhuman-comprehensible features. We propose an improvement to dictionary features\ncalled smoothed dictionary features. These features recognize document contexts\ninstead of n-grams. We describe a principled methodology to solicit dictionary\nfeatures from a teacher, and present results showing that models built using\nthese human-comprehensible features are competitive with models trained with\nBag of Words features.", "published": "2016-06-24 02:28:24", "link": "http://arxiv.org/abs/1606.07545v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Adaptability of Neural Networks on Varying Granularity IR Tasks", "abstract": "Recent work in Information Retrieval (IR) using Deep Learning models has\nyielded state of the art results on a variety of IR tasks. Deep neural networks\n(DNN) are capable of learning ideal representations of data during the training\nprocess, removing the need for independently extracting features. However, the\nstructures of these DNNs are often tailored to perform on specific datasets. In\naddition, IR tasks deal with text at varying levels of granularity from single\nfactoids to documents containing thousands of words. In this paper, we examine\nthe role of the granularity on the performance of common state of the art DNN\nstructures in IR.", "published": "2016-06-24 04:40:48", "link": "http://arxiv.org/abs/1606.07565v1", "categories": ["cs.IR", "cs.CL", "H.3.3; I.5.1"], "primary_category": "cs.IR"}
{"title": "Captioning Images with Diverse Objects", "abstract": "Recent captioning models are limited in their ability to scale and describe\nconcepts unseen in paired image-text corpora. We propose the Novel Object\nCaptioner (NOC), a deep visual semantic captioning model that can describe a\nlarge number of object categories not present in existing image-caption\ndatasets. Our model takes advantage of external sources -- labeled images from\nobject recognition datasets, and semantic knowledge extracted from unannotated\ntext. We propose minimizing a joint objective which can learn from these\ndiverse data sources and leverage distributional semantic embeddings, enabling\nthe model to generalize and describe novel objects outside of image-caption\ndatasets. We demonstrate that our model exploits semantic information to\ngenerate captions for hundreds of object categories in the ImageNet object\nrecognition dataset that are not observed in MSCOCO image-caption training\ndata, as well as many categories that are observed very rarely. Both automatic\nevaluations and human judgements show that our model considerably outperforms\nprior work in being able to describe many more categories of objects.", "published": "2016-06-24 17:53:45", "link": "http://arxiv.org/abs/1606.07770v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Efficient Parallel Learning of Word2Vec", "abstract": "Since its introduction, Word2Vec and its variants are widely used to learn\nsemantics-preserving representations of words or entities in an embedding\nspace, which can be used to produce state-of-art results for various Natural\nLanguage Processing tasks. Existing implementations aim to learn efficiently by\nrunning multiple threads in parallel while operating on a single model in\nshared memory, ignoring incidental memory update collisions. We show that these\ncollisions can degrade the efficiency of parallel learning, and propose a\nstraightforward caching strategy that improves the efficiency by a factor of 4.", "published": "2016-06-24 20:05:53", "link": "http://arxiv.org/abs/1606.07822v1", "categories": ["cs.CL", "cs.DC"], "primary_category": "cs.CL"}
{"title": "Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles", "abstract": "Many practical perception systems exist within larger processes that include\ninteractions with users or additional components capable of evaluating the\nquality of predicted solutions. In these contexts, it is beneficial to provide\nthese oracle mechanisms with multiple highly likely hypotheses rather than a\nsingle prediction. In this work, we pose the task of producing multiple outputs\nas a learning problem over an ensemble of deep networks -- introducing a novel\nstochastic gradient descent based approach to minimize the loss with respect to\nan oracle. Our method is simple to implement, agnostic to both architecture and\nloss function, and parameter-free. Our approach achieves lower oracle error\ncompared to existing methods on a wide range of tasks and deep architectures.\nWe also show qualitatively that the diverse solutions produced often provide\ninterpretable representations of task ambiguity.", "published": "2016-06-24 21:48:55", "link": "http://arxiv.org/abs/1606.07839v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enriching Linked Datasets with New Object Properties", "abstract": "Although several RDF knowledge bases are available through the LOD\ninitiative, the ontology schema of such linked datasets is not very rich. In\nparticular, they lack object properties. The problem of finding new object\nproperties (and their instances) between any two given classes has not been\ninvestigated in detail in the context of Linked Data. In this paper, we present\nDART (Detecting Arbitrary Relations for enriching T-Boxes of Linked Data) - an\nunsupervised solution to enrich the LOD cloud with new object properties\nbetween two given classes. DART exploits contextual similarity to identify text\npatterns from the web corpus that can potentially represent relations between\nindividuals. These text patterns are then clustered by means of paraphrase\ndetection to capture the object properties between the two given LOD classes.\nDART also performs fully automated mapping of the discovered relations to the\nproperties in the linked dataset. This serves many purposes such as\nidentification of completely new relations, elimination of irrelevant\nrelations, and generation of prospective property axioms. We have empirically\nevaluated our approach on several pairs of classes and found that the system\ncan indeed be used for enriching the linked datasets with new object properties\nand their instances. We compared DART with newOntExt system which is an\noffshoot of the NELL (Never-Ending Language Learning) effort. Our experiments\nreveal that DART gives better results than newOntExt with respect to both the\ncorrectness, as well as the number of relations.", "published": "2016-06-24 06:00:42", "link": "http://arxiv.org/abs/1606.07572v3", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "A Game-Theoretic Approach to Word Sense Disambiguation", "abstract": "This paper presents a new model for word sense disambiguation formulated in\nterms of evolutionary game theory, where each word to be disambiguated is\nrepresented as a node on a graph whose edges represent word relations and\nsenses are represented as classes. The words simultaneously update their class\nmembership preferences according to the senses that neighboring words are\nlikely to choose. We use distributional information to weigh the influence that\neach word has on the decisions of the others and semantic similarity\ninformation to measure the strength of compatibility among the choices. With\nthis information we can formulate the word sense disambiguation problem as a\nconstraint satisfaction problem and solve it using tools derived from game\ntheory, maintaining the textual coherence. The model is based on two ideas:\nsimilar words should be assigned to similar classes and the meaning of a word\ndoes not depend on all the words in a text but just on some of them. The paper\nprovides an in-depth motivation of the idea of modeling the word sense\ndisambiguation problem in terms of game theory, which is illustrated by an\nexample. The conclusion presents an extensive analysis on the combination of\nsimilarity measures to use in the framework and a comparison with\nstate-of-the-art systems. The results show that our model outperforms\nstate-of-the-art algorithms and can be applied to different tasks and in\ndifferent scenarios.", "published": "2016-06-24 14:45:27", "link": "http://arxiv.org/abs/1606.07711v4", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI"}
