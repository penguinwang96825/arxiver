{"title": "Markov-Functional Models with Local Drift", "abstract": "We introduce a Markov-functional approach to construct local volatility\nmodels that are calibrated to a discrete set of marginal distributions. The\nmethod is inspired by and extends the volatility interpolation of Bass (1983)\nand Conze and Henry-Labord\\`ere (2022). The method is illustrated with\nefficient numerical algorithms in the cases where the constructed local\nvolatility functions are: (1) time-homogeneous between or (2) continuous\nacross, the successive maturities. The step-wise time-homogeneous construction\nproduces a parsimonious representation of the local volatility term structure.", "published": "2024-11-22 16:40:38", "link": "http://arxiv.org/abs/2411.15053v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "A New Way: Kronecker-Factored Approximate Curvature Deep Hedging and its Benefits", "abstract": "This paper advances the computational efficiency of Deep Hedging frameworks\nthrough the novel integration of Kronecker-Factored Approximate Curvature\n(K-FAC) optimization. While recent literature has established Deep Hedging as a\ndata-driven alternative to traditional risk management strategies, the\ncomputational burden of training neural networks with first-order methods\nremains a significant impediment to practical implementation. The proposed\narchitecture couples Long Short-Term Memory (LSTM) networks with K-FAC\nsecond-order optimization, specifically addressing the challenges of sequential\nfinancial data and curvature estimation in recurrent networks. Empirical\nvalidation using simulated paths from a calibrated Heston stochastic volatility\nmodel demonstrates that the K-FAC implementation achieves marked improvements\nin convergence dynamics and hedging efficacy. The methodology yields a 78.3%\nreduction in transaction costs ($t = 56.88$, $p < 0.001$) and a 34.4% decrease\nin profit and loss (P&L) variance compared to Adam optimization. Moreover, the\nK-FAC-enhanced model exhibits superior risk-adjusted performance with a Sharpe\nratio of 0.0401, contrasting with $-0.0025$ for the baseline model. These\nresults provide compelling evidence that second-order optimization methods can\nmaterially enhance the tractability of Deep Hedging implementations. The\nfindings contribute to the growing literature on computational methods in\nquantitative finance while highlighting the potential for advanced optimization\ntechniques to bridge the gap between theoretical frameworks and practical\napplications in financial markets.", "published": "2024-11-22 15:19:40", "link": "http://arxiv.org/abs/2411.15002v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Understanding the Impact of News Articles on the Movement of Market Index: A Case on Nifty 50", "abstract": "In the recent past, there were several works on the prediction of stock price\nusing different methods. Sentiment analysis of news and tweets and relating\nthem to the movement of stock prices have already been explored. But, when we\ntalk about the news, there can be several topics such as politics, markets,\nsports etc. It was observed that most of the prior analyses dealt with news or\ncomments associated with particular stock prices only or the researchers dealt\nwith overall sentiment scores only. However, it is quite possible that\ndifferent topics having different levels of impact on the movement of the stock\nprice or an index. The current study focused on bridging this gap by analysing\nthe movement of Nifty 50 index with respect to the sentiments associated with\nnews items related to various different topic such as sports, politics, markets\netc. The study established that sentiment scores of news items of different\nother topics also have a significant impact on the movement of the index.", "published": "2024-11-22 06:09:14", "link": "http://arxiv.org/abs/2412.06794v1", "categories": ["cs.CL", "cs.LG", "q-fin.ST"], "primary_category": "cs.CL"}
{"title": "Benchmarking Multimodal Models for Ukrainian Language Understanding\n  Across Academic and Cultural Domains", "abstract": "While the evaluation of multimodal English-centric models is an active area\nof research with numerous benchmarks, there is a profound lack of benchmarks or\nevaluation suites for low- and mid-resource languages. We introduce ZNO-Vision,\na comprehensive multimodal Ukrainian-centric benchmark derived from\nstandardized university entrance examination (ZNO). The benchmark consists of\nover 4,300 expert-crafted questions spanning 12 academic disciplines, including\nmathematics, physics, chemistry, and humanities. We evaluated the performance\nof both open-source models and API providers, finding that only a handful of\nmodels performed above baseline. Alongside the new benchmark, we performed the\nfirst evaluation study of multimodal text generation for the Ukrainian\nlanguage: we measured caption generation quality on the Multi30K-UK dataset,\ntranslated the VQA benchmark into Ukrainian, and measured performance\ndegradation relative to original English versions. Lastly, we tested a few\nmodels from a cultural perspective on knowledge of national cuisine. We believe\nour work will advance multimodal generation capabilities for the Ukrainian\nlanguage and our approach could be useful for other low-resource languages.", "published": "2024-11-22 00:37:49", "link": "http://arxiv.org/abs/2411.14647v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Social Media Annotation of HPV Vaccine Skepticism and\n  Misinformation Using Large Language Models: An Experimental Evaluation of\n  In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models", "abstract": "This paper leverages large-language models (LLMs) to experimentally determine\noptimal strategies for scaling up social media content annotation for stance\ndetection on HPV vaccine-related tweets. We examine both conventional\nfine-tuning and emergent in-context learning methods, systematically varying\nstrategies of prompt engineering across widely used LLMs and their variants\n(e.g., GPT4, Mistral, and Llama3, etc.). Specifically, we varied prompt\ntemplate design, shot sampling methods, and shot quantity to detect stance on\nHPV vaccination. Our findings reveal that 1) in general, in-context learning\noutperforms fine-tuning in stance detection for HPV vaccine social media\ncontent; 2) increasing shot quantity does not necessarily enhance performance\nacross models; and 3) different LLMs and their variants present differing\nsensitivity to in-context learning conditions. We uncovered that the optimal\nin-context learning configuration for stance detection on HPV vaccine tweets\ninvolves six stratified shots paired with detailed contextual prompts. This\nstudy highlights the potential and provides an applicable approach for applying\nLLMs to research on social media stance and skepticism detection.", "published": "2024-11-22 04:19:32", "link": "http://arxiv.org/abs/2411.14720v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "De-biased Multimodal Electrocardiogram Analysis", "abstract": "Multimodal large language models (MLLMs) are increasingly being applied in\nthe medical field, particularly in medical imaging. However, developing MLLMs\nfor ECG signals, which are crucial in clinical settings, has been a significant\nchallenge beyond medical imaging. Previous studies have attempted to address\nthis by converting ECGs into several text tags using an external classifier in\na training-free manner. However, this approach significantly compresses the\ninformation in ECGs and underutilizes the reasoning capabilities of LLMs. In\nthis work, we directly feed the embeddings of ECGs into the LLM through a\nprojection layer, retaining more information about ECGs and better leveraging\nthe reasoning abilities of LLMs. Our method can also effectively handle a\ncommon situation in clinical practice where it is necessary to compare two ECGs\ntaken at different times. Recent studies found that MLLMs may rely solely on\ntext input to provide answers, ignoring inputs from other modalities. We\nanalyzed this phenomenon from a causal perspective in the context of ECG MLLMs\nand discovered that the confounder, severity of illness, introduces a spurious\ncorrelation between the question and answer, leading the model to rely on this\nspurious correlation and ignore the ECG input. Such models do not comprehend\nthe ECG input and perform poorly in adversarial tests where different\nexpressions of the same question are used in the training and testing sets. We\ndesigned a de-biased pre-training method to eliminate the confounder's effect\naccording to the theory of backdoor adjustment. Our model performed well on the\nECG-QA task under adversarial testing and demonstrated zero-shot capabilities.\nAn interesting random ECG test further validated that our model effectively\nunderstands and utilizes the input ECG signal.", "published": "2024-11-22 08:35:35", "link": "http://arxiv.org/abs/2411.14795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Hierarchical Prototypes as the Verbalizer for Implicit\n  Discourse Relation Recognition", "abstract": "Implicit discourse relation recognition involves determining relationships\nthat hold between spans of text that are not linked by an explicit discourse\nconnective. In recent years, the pre-train, prompt, and predict paradigm has\nemerged as a promising approach for tackling this task. However, previous work\nsolely relied on manual verbalizers for implicit discourse relation\nrecognition, which suffer from issues of ambiguity and even incorrectness. To\novercome these limitations, we leverage the prototypes that capture certain\nclass-level semantic features and the hierarchical label structure for\ndifferent classes as the verbalizer. We show that our method improves on\ncompetitive baselines. Besides, our proposed approach can be extended to enable\nzero-shot cross-lingual learning, facilitating the recognition of discourse\nrelations in languages with scarce resources. These advancement validate the\npracticality and versatility of our approach in addressing the issues of\nimplicit discourse relation recognition across different languages.", "published": "2024-11-22 12:01:04", "link": "http://arxiv.org/abs/2411.14880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Extraction from Heterogeneous Documents without Ground Truth\n  Labels using Synthetic Label Generation and Knowledge Distillation", "abstract": "Invoices and receipts submitted by employees are visually rich documents\n(VRDs) with textual, visual and layout information. To protect against the risk\nof fraud and abuse, it is crucial for organizations to efficiently extract\ndesired information from submitted receipts. This helps in the assessment of\nkey factors such as appropriateness of the expense claim, adherence to spending\nand transaction policies, the validity of the receipt, as well as downstream\nanomaly detection at various levels. These documents are heterogeneous, with\nmultiple formats and languages, uploaded with different image qualities, and\noften do not contain ground truth labels for the efficient training of models.\nIn this paper we propose Task Aware Instruction-based Labelling (TAIL), a\nmethod for synthetic label generation in VRD corpuses without labels, and\nfine-tune a multimodal Visually Rich Document Understanding Model (VRDU) on\nTAIL labels using response-based knowledge distillation without using the\nteacher model's weights or training dataset to conditionally generate\nannotations in the appropriate format. Using a benchmark external dataset where\nground truth labels are available, we demonstrate conditions under which our\napproach performs at par with Claude 3 Sonnet through empirical studies. We\nthen show that the resulting model performs at par or better on the internal\nexpense documents of a large multinational organization than state-of-the-art\nLMM (large multimodal model) Claude 3 Sonnet while being 85% less costly and\n~5X faster, and outperforms layout-aware baselines by more than 10% in Average\nNormalized Levenshtein Similarity (ANLS) scores due to its ability to reason\nand extract information from rare formats. Finally, we illustrate the usage of\nour approach in overpayment prevention.", "published": "2024-11-22 14:16:09", "link": "http://arxiv.org/abs/2411.14957v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locating the Leading Edge of Cultural Change", "abstract": "Measures of textual similarity and divergence are increasingly used to study\ncultural change. But which measures align, in practice, with social evidence\nabout change? We apply three different representations of text (topic models,\ndocument embeddings, and word-level perplexity) to three different corpora\n(literary studies, economics, and fiction). In every case, works by\nhighly-cited authors and younger authors are textually ahead of the curve. We\ndon't find clear evidence that one representation of text is to be preferred\nover the others. But alignment with social evidence is strongest when texts are\nrepresented through the top quartile of passages, suggesting that a text's\nimpact may depend more on its most forward-looking moments than on sustaining a\nhigh level of innovation throughout.", "published": "2024-11-22 16:56:38", "link": "http://arxiv.org/abs/2411.15068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tulu 3: Pushing Frontiers in Open Language Model Post-Training", "abstract": "Language model post-training is applied to refine behaviors and unlock new\nskills across a wide range of recent language models, but open recipes for\napplying these techniques lag behind proprietary ones. The underlying training\ndata and recipes for post-training are simultaneously the most important pieces\nof the puzzle and the portion with the least transparency. To bridge this gap,\nwe introduce Tulu 3, a family of fully-open state-of-the-art post-trained\nmodels, alongside its data, code, and training recipes, serving as a\ncomprehensive guide for modern post-training techniques. Tulu 3, which builds\non Llama 3.1 base models, achieves results surpassing the instruct versions of\nLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and\nClaude 3.5-Haiku. The training algorithms for our models include supervised\nfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method we\ncall Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we\nintroduce a multi-task evaluation scheme for post-training recipes with\ndevelopment and unseen evaluations, standard benchmark implementations, and\nsubstantial decontamination of existing open datasets on said benchmarks. We\nconclude with analysis and discussion of training methods that did not reliably\nimprove performance.\n  In addition to the Tulu 3 model weights and demo, we release the complete\nrecipe -- including datasets for diverse core skills, a robust toolkit for data\ncuration and evaluation, the training code and infrastructure, and, most\nimportantly, a detailed report for reproducing and further adapting the Tulu 3\napproach to more domains.", "published": "2024-11-22 18:44:04", "link": "http://arxiv.org/abs/2411.15124v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning", "abstract": "Large language models have emerged as powerful tools for general\nintelligence, showcasing advanced natural language processing capabilities that\nfind applications across diverse domains. Despite their impressive performance,\nrecent studies have highlighted the potential for significant enhancements in\nLLMs' task-specific performance through fine-tuning strategies like\nReinforcement Learning with Human Feedback (RLHF), supervised fine-tuning\n(SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works\nhave shown that while fine-tuning offers significant performance gains, it also\nleads to challenges such as catastrophic forgetting and privacy and safety\nrisks. To this end, there has been little to no work in \\textit{understanding\nthe impact of fine-tuning on the reasoning capabilities of LLMs}. Our research\ninvestigates the effect of fine-tuning on the reasoning abilities of LLMs,\naddressing critical questions regarding the impact of task-specific fine-tuning\non overall reasoning capabilities, the influence of fine-tuning on\nChain-of-Thought (CoT) reasoning performance, and the implications for the\nfaithfulness of CoT reasonings. By exploring these dimensions, our study shows\nthe impact of fine-tuning on LLM reasoning capabilities, where the faithfulness\nof CoT reasoning, on average across four datasets, decreases, highlighting\npotential shifts in internal mechanisms of the LLMs resulting from fine-tuning\nprocesses.", "published": "2024-11-22 23:54:37", "link": "http://arxiv.org/abs/2411.15382v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis\n  Perspective", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks.", "published": "2024-11-22 00:59:25", "link": "http://arxiv.org/abs/2411.14654v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multiverse of Greatness: Generating Story Branches with LLMs", "abstract": "This paper presents Dynamic Context Prompting/Programming (DCP/P), a novel\nframework for interacting with LLMs to generate graph-based content with a\ndynamic context window history. While there is an existing study utilizing LLMs\nto generate a visual novel game, the previous study involved a manual process\nof output extraction and did not provide flexibility in generating a longer,\ncoherent story. We evaluate DCP/P against our baseline, which does not provide\ncontext history to an LLM and only relies on the initial story data. Through\nobjective evaluation, we show that simply providing the LLM with a summary\nleads to a subpar story compared to additionally providing the LLM with the\nproper context of the story. We also provide an extensive qualitative analysis\nand discussion. We qualitatively examine the quality of the objectively\nbest-performing generated game from each approach. In addition, we examine\nbiases in word choices and word sentiment of the generated content. We find a\nconsistent observation with previous studies that LLMs are biased towards\ncertain words, even with a different LLM family. Finally, we provide a\ncomprehensive discussion on opportunities for future studies.", "published": "2024-11-22 02:11:37", "link": "http://arxiv.org/abs/2411.14672v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Mathematical Reasoning Capabilities of Small Language Models\n  via Feedback-Driven Distillation", "abstract": "Large Language Models (LLMs) demonstrate exceptional reasoning capabilities,\noften achieving state-of-the-art performance in various tasks. However, their\nsubstantial computational and memory demands, due to billions of parameters,\nhinder deployment in resource-constrained environments. A promising solution is\nknowledge distillation, where LLMs transfer reasoning capabilities to Small\nLanguage Models (SLMs, $\\le$ 1B parameters), enabling wider deployment on\nlow-resource devices. Existing methods primarily focus on generating\nhigh-quality reasoning rationales for distillation datasets but often neglect\nthe critical role of data quantity and quality. To address these challenges, we\npropose a Feedback-Driven Distillation (FDD) framework to enhance SLMs'\nmathematical reasoning capabilities. In the initialization stage, a\ndistillation dataset is constructed by prompting LLMs to pair mathematical\nproblems with corresponding reasoning rationales. We classify problems into\neasy and hard categories based on SLM performance. For easy problems, LLMs\ngenerate more complex variations, while for hard problems, new questions of\nsimilar complexity are synthesized. In addition, we propose a multi-round\ndistillation paradigm to iteratively enrich the distillation datasets, thereby\nprogressively improving the mathematical reasoning abilities of SLMs.\nExperimental results demonstrate that our method can make SLMs achieve SOTA\nmathematical reasoning performance.", "published": "2024-11-22 03:12:39", "link": "http://arxiv.org/abs/2411.14698v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query\n  Generation for Conversational Search", "abstract": "The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing\nconversational assistants, able to adapt their interaction and responses from\npersonalized user knowledge. The track incorporates a Personal Textual\nKnowledge Base (PTKB) alongside Conversational AI tasks, such as passage\nranking and response generation. Query Rewrite being an effective approach for\nresolving conversational context, we explore Large Language Models (LLMs), as\nquery rewriters. Specifically, our submitted runs explore multi-aspect query\ngeneration using the MQ4CS framework, which we further enhance with Learned\nSparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder\nmodels. We also propose an alternative to the previous interleaving strategy,\naggregating multiple aspects during the reranking phase. Our findings indicate\nthat multi-aspect query generation is effective in enhancing performance when\nintegrated with advanced retrieval and reranking models. Our results also lead\nthe way for better personalization in Conversational Search, relying on LLMs to\nintegrate personalization within query rewrite, and outperforming human rewrite\nperformance.", "published": "2024-11-22 05:18:35", "link": "http://arxiv.org/abs/2411.14739v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "KBAlign: Efficient Self Adaptation on Specific Knowledge Bases", "abstract": "Humans can utilize techniques to quickly acquire knowledge from specific\nmaterials in advance, such as creating self-assessment questions, enabling us\nto achieving related tasks more efficiently. In contrast, large language models\n(LLMs) usually relies on retrieval-augmented generation to exploit knowledge\nmaterials in an instant manner, or requires external signals such as human\npreference data and stronger LLM annotations to conduct knowledge adaptation.\nTo unleash the self-learning potential of LLMs, we propose KBAlign, an approach\ndesigned for efficient adaptation to downstream tasks involving knowledge\nbases. Our method utilizes iterative training with self-annotated data such as\nQ&A pairs and revision suggestions, enabling the model to grasp the knowledge\ncontent efficiently. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach, significantly boosting model performance in\ndownstream tasks that require specific knowledge at a low cost. Notably, our\napproach achieves over 90% of the performance improvement that can be obtained\nby using GPT-4-turbo annotation, while relying entirely on self-supervision. We\nrelease our experimental data, models, and process analyses to the community\nfor further exploration (https://github.com/thunlp/KBAlign).", "published": "2024-11-22 08:21:03", "link": "http://arxiv.org/abs/2411.14790v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Astro-HEP-BERT: A bidirectional language model for studying the meanings\n  of concepts in astrophysics and high energy physics", "abstract": "I present Astro-HEP-BERT, a transformer-based language model specifically\ndesigned for generating contextualized word embeddings (CWEs) to study the\nmeanings of concepts in astrophysics and high-energy physics. Built on a\ngeneral pretrained BERT model, Astro-HEP-BERT underwent further training over\nthree epochs using the Astro-HEP Corpus, a dataset I curated from 21.84 million\nparagraphs extracted from more than 600,000 scholarly articles on arXiv, all\nbelonging to at least one of these two scientific domains. The project\ndemonstrates both the effectiveness and feasibility of adapting a bidirectional\ntransformer for applications in the history, philosophy, and sociology of\nscience (HPSS). The entire training process was conducted using freely\navailable code, pretrained weights, and text inputs, completed on a single\nMacBook Pro Laptop (M2/96GB). Preliminary evaluations indicate that\nAstro-HEP-BERT's CWEs perform comparably to domain-adapted BERT models trained\nfrom scratch on larger datasets for domain-specific word sense disambiguation\nand induction and related semantic change analyses. This suggests that\nretraining general language models for specific scientific domains can be a\ncost-effective and efficient strategy for HPSS researchers, enabling high\nperformance without the need for extensive training from scratch.", "published": "2024-11-22 11:59:15", "link": "http://arxiv.org/abs/2411.14877v1", "categories": ["cs.CL", "physics.hist-ph", "I.2.6; I.2.7; J.4"], "primary_category": "cs.CL"}
{"title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in\n  Hour-Long Videos", "abstract": "Large language models (LLMs) excel at retrieving information from lengthy\ntext, but their vision-language counterparts (VLMs) face difficulties with\nhour-long videos, especially for temporal grounding. Specifically, these VLMs\nare constrained by frame limitations, often losing essential temporal details\nneeded for accurate event localization in extended video content. We propose\nReVisionLLM, a recursive vision-language model designed to locate events in\nhour-long videos. Inspired by human search strategies, our model initially\ntargets broad segments of interest, progressively revising its focus to\npinpoint exact temporal boundaries. Our model can seamlessly handle videos of\nvastly different lengths, from minutes to hours. We also introduce a\nhierarchical training strategy that starts with short clips to capture distinct\nevents and progressively extends to longer videos. To our knowledge,\nReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,\noutperforming previous state-of-the-art methods across multiple datasets by a\nsignificant margin (+2.6% R1@0.1 on MAD). The code is available at\nhttps://github.com/Tanveer81/ReVisionLLM.", "published": "2024-11-22 12:46:50", "link": "http://arxiv.org/abs/2411.14901v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Large Multi-modal Models Can Interpret Features in Large Multi-modal\n  Models", "abstract": "Recent advances in Large Multimodal Models (LMMs) lead to significant\nbreakthroughs in both academia and industry. One question that arises is how\nwe, as humans, can understand their internal neural representations. This paper\ntakes an initial step towards addressing this question by presenting a\nversatile framework to identify and interpret the semantics within LMMs.\nSpecifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the\nrepresentations into human understandable features. 2) We then present an\nautomatic interpretation framework to interpreted the open-semantic features\nlearned in SAE by the LMMs themselves. We employ this framework to analyze the\nLLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these\nfeatures can effectively steer the model's behavior. Our results contribute to\na deeper understanding of why LMMs excel in specific tasks, including EQ tests,\nand illuminate the nature of their mistakes along with potential strategies for\ntheir rectification. These findings offer new insights into the internal\nmechanisms of LMMs and suggest parallels with the cognitive processes of the\nhuman brain.", "published": "2024-11-22 14:41:36", "link": "http://arxiv.org/abs/2411.14982v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale\n  Workflow Data", "abstract": "Large Language Model (LLM) agents are rapidly improving to handle\nincreasingly complex web-based tasks. Most of these agents rely on\ngeneral-purpose, proprietary models like GPT-4 and focus on designing better\nprompts to improve their planning abilities. However, general-purpose LLMs are\nnot specifically trained to understand specialized web contexts such as HTML,\nand they often struggle with long-horizon planning. We explore an alternative\napproach that fine-tunes open-source LLMs using production-scale workflow data\ncollected from over 250 domains corresponding to 6 billion tokens. This simple\nyet effective approach shows substantial gains over prompting-based agents on\nexisting benchmarks -- ScribeAgent achieves state-of-the-art direct generation\nperformance on Mind2Web and improves the task success rate by 7.3% over the\nprevious best text-only web agents on WebArena. We further perform detailed\nablation studies on various fine-tuning design choices and provide insights\ninto LLM selection, training recipes, context window optimization, and effect\nof dataset sizes.", "published": "2024-11-22 15:26:23", "link": "http://arxiv.org/abs/2411.15004v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evolutionary Automata and Deep Evolutionary Computation", "abstract": "Evolution by natural selection, which is one of the most compelling themes of\nmodern science, brought forth evolutionary algorithms and evolutionary\ncomputation, applying mechanisms of evolution in nature to various problems\nsolved by computers. In this paper we concentrate on evolutionary automata that\nconstitute an analogous model of evolutionary computation compared to\nwell-known evolutionary algorithms. Evolutionary automata provide a more\ncomplete dual model of evolutionary computation, similar like abstract automata\n(e.g., Turing machines) form a more formal and precise model compared to\nrecursive algorithms and their subset - evolutionary algorithms. An\nevolutionary automaton is an automaton that evolves performing evolutionary\ncomputation perhaps using an infinite number of generations. This model allows\nfor a direct modeling evolution of evolution, and leads to tremendous\nexpressiveness of evolutionary automata and evolutionary computation. This also\ngives the hint to the power of natural evolution that is self-evolving by\ninteractive feedback with the environment.", "published": "2024-11-22 15:31:50", "link": "http://arxiv.org/abs/2411.15008v1", "categories": ["cs.NE", "cs.CL", "68"], "primary_category": "cs.NE"}
{"title": "mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for\n  Knowledge-Based VQA", "abstract": "Advanced Multimodal Large Language Models (MLLMs) struggle with recent\nKnowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their\nlimited and frozen knowledge scope, often leading to ambiguous and inaccurate\nresponses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally\nintroduced to provide MLLMs with comprehensive and up-to-date knowledge,\neffectively expanding the knowledge scope. However, current mRAG methods have\ninherent drawbacks, including: 1) Performing retrieval even when external\nknowledge is not needed. 2) Lacking of identification of evidence that supports\nthe query. 3) Increasing model complexity due to additional information\nfiltering modules or rules. To address these shortcomings, we propose a novel\ngeneralized framework called \\textbf{m}ultimodal\n\\textbf{R}etrieval-\\textbf{R}eflection-\\textbf{A}ugmented \\textbf{G}eneration\n(mR$^2$AG), which achieves adaptive retrieval and useful information\nlocalization to enable answers through two easy-to-implement reflection\noperations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection\nis designed to distinguish different user queries and avoids redundant\nretrieval calls, and Relevance-Reflection is introduced to guide the MLLM in\nlocating beneficial evidence of the retrieved content and generating answers\naccordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM\nwith efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset\n(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,\nGPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while\nmaintaining the exceptional capabilities of base MLLMs across a wide range of\nVisual-dependent tasks.", "published": "2024-11-22 16:15:50", "link": "http://arxiv.org/abs/2411.15041v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object\n  Hallucination in Large Vision-Language Models", "abstract": "Despite the recent breakthroughs achieved by Large Vision Language Models\n(LVLMs) in understanding and responding to complex visual-textual contexts,\ntheir inherent hallucination tendencies limit their practical application in\nreal-world scenarios that demand high levels of precision. Existing methods\ntypically either fine-tune the LVLMs using additional data, which incurs extra\ncosts in manual annotation and computational resources or perform comparisons\nat the decoding stage, which may eliminate useful language priors for reasoning\nwhile introducing inference time overhead. Therefore, we propose ICT, a\nlightweight, training-free method that calculates an intervention direction to\nshift the model's focus towards different levels of visual information,\nenhancing its attention to high-level and fine-grained visual details. During\nthe forward pass stage, the intervention is applied to the attention heads that\nencode the overall image information and the fine-grained object details,\neffectively mitigating the phenomenon of overly language priors, and thereby\nalleviating hallucinations. Extensive experiments demonstrate that ICT achieves\nstrong performance with a small amount of data and generalizes well across\ndifferent datasets and models. Our code will be public.", "published": "2024-11-22 12:22:21", "link": "http://arxiv.org/abs/2411.15268v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "BanglaEmbed: Efficient Sentence Embedding Models for a Low-Resource\n  Language Using Cross-Lingual Distillation Techniques", "abstract": "Sentence-level embedding is essential for various tasks that require\nunderstanding natural language. Many studies have explored such embeddings for\nhigh-resource languages like English. However, low-resource languages like\nBengali (a language spoken by almost two hundred and thirty million people) are\nstill under-explored. This work introduces two lightweight sentence\ntransformers for the Bangla language, leveraging a novel cross-lingual\nknowledge distillation approach. This method distills knowledge from a\npre-trained, high-performing English sentence transformer. Proposed models are\nevaluated across multiple downstream tasks, including paraphrase detection,\nsemantic textual similarity (STS), and Bangla hate speech detection. The new\nmethod consistently outperformed existing Bangla sentence transformers.\nMoreover, the lightweight architecture and shorter inference time make the\nmodels highly suitable for deployment in resource-constrained environments,\nmaking them valuable for practical NLP applications in low-resource languages.", "published": "2024-11-22 13:03:25", "link": "http://arxiv.org/abs/2411.15270v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sycophancy in Large Language Models: Causes and Mitigations", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of natural language processing tasks. However, their tendency to\nexhibit sycophantic behavior - excessively agreeing with or flattering users -\nposes significant risks to their reliability and ethical deployment. This paper\nprovides a technical survey of sycophancy in LLMs, analyzing its causes,\nimpacts, and potential mitigation strategies. We review recent work on\nmeasuring and quantifying sycophantic tendencies, examine the relationship\nbetween sycophancy and other challenges like hallucination and bias, and\nevaluate promising techniques for reducing sycophancy while maintaining model\nperformance. Key approaches explored include improved training data, novel\nfine-tuning methods, post-deployment control mechanisms, and decoding\nstrategies. We also discuss the broader implications of sycophancy for AI\nalignment and propose directions for future research. Our analysis suggests\nthat mitigating sycophancy is crucial for developing more robust, reliable, and\nethically-aligned language models.", "published": "2024-11-22 16:56:49", "link": "http://arxiv.org/abs/2411.15287v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PPLqa: An Unsupervised Information-Theoretic Quality Metric for\n  Comparing Generative Large Language Models", "abstract": "We propose PPLqa, an easy to compute, language independent,\ninformation-theoretic metric to measure the quality of responses of generative\nLarge Language Models (LLMs) in an unsupervised way, without requiring ground\ntruth annotations or human supervision. The method and metric enables users to\nrank generative language models for quality of responses, so as to make a\nselection of the best model for a given task. Our single metric assesses LLMs\nwith an approach that subsumes, but is not explicitly based on, coherence and\nfluency (quality of writing) and relevance and consistency (appropriateness of\nresponse) to the query. PPLqa performs as well as other related metrics, and\nworks better with long-form Q\\&A. Thus, PPLqa enables bypassing the lengthy\nannotation process required for ground truth evaluations, and it also\ncorrelates well with human and LLM rankings.", "published": "2024-11-22 19:28:06", "link": "http://arxiv.org/abs/2411.15320v1", "categories": ["cs.CL", "cs.AI", "I.2.m; E.4"], "primary_category": "cs.CL"}
{"title": "A Brief Summary of Explanatory Virtues", "abstract": "In this report, I provide a brief summary of the literature in philosophy,\npsychology and cognitive science about Explanatory Virtues, and link these\nconcepts to eXplainable AI.", "published": "2024-11-22 13:27:56", "link": "http://arxiv.org/abs/2411.16709v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Whats in a Video: Factorized Autoregressive Decoding for Online Dense\n  Video Captioning", "abstract": "Generating automatic dense captions for videos that accurately describe their\ncontents remains a challenging area of research. Most current models require\nprocessing the entire video at once. Instead, we propose an efficient, online\napproach which outputs frequent, detailed and temporally aligned captions,\nwithout access to future frames. Our model uses a novel autoregressive\nfactorized decoding architecture, which models the sequence of visual features\nfor each time segment, outputting localized descriptions and efficiently\nleverages the context from the previous video segments. This allows the model\nto output frequent, detailed captions to more comprehensively describe the\nvideo, according to its actual local content, rather than mimic the training\ndata. Second, we propose an optimization for efficient training and inference,\nwhich enables scaling to longer videos. Our approach shows excellent\nperformance compared to both offline and online methods, and uses 20\\% less\ncompute. The annotations produced are much more comprehensive and frequent, and\ncan further be utilized in automatic video tagging and in large-scale video\ndata harvesting.", "published": "2024-11-22 02:46:44", "link": "http://arxiv.org/abs/2411.14688v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Understanding LLM Embeddings for Regression", "abstract": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.", "published": "2024-11-22 03:33:51", "link": "http://arxiv.org/abs/2411.14708v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FedMLLM: Federated Fine-tuning MLLM on Multimodal Heterogeneity Data", "abstract": "Multimodal Large Language Models (MLLMs) have made significant advancements,\ndemonstrating powerful capabilities in processing and understanding multimodal\ndata. Fine-tuning MLLMs with Federated Learning (FL) allows for expanding the\ntraining data scope by including private data sources, thereby enhancing their\npractical applicability in privacy-sensitive domains. However, current research\nremains in the early stage, particularly in addressing the \\textbf{multimodal\nheterogeneities} in real-world applications. In this paper, we introduce a\nbenchmark to evaluate the performance of federated fine-tuning of MLLMs across\nvarious multimodal heterogeneous scenarios, laying the groundwork for future\nresearch in the field. Our benchmark includes two lightweight MLLMs, two\ndownstream tasks, three evaluation metrics, and five datasets across three\ndomains, along with six comparison baselines, covering over ten types of\nmodality heterogeneities across four multimodal scenarios. To address the\nchallenges posed by multimodal heterogeneity, we develop a general FedMLLM\nframework that integrates classic FL methods alongside two modality-agnostic\nstrategies. Extensive experimental results show that our proposed FL paradigm\nimproves the performance of MLLMs by broadening the range of training data and\nmitigating multimodal heterogeneity. Code is available in supplementary\nmaterials.", "published": "2024-11-22 04:09:23", "link": "http://arxiv.org/abs/2411.14717v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "MolReFlect: Towards In-Context Fine-grained Alignments between Molecules\n  and Texts", "abstract": "Molecule discovery is a pivotal research field, impacting everything from the\nmedicines we take to the materials we use. Recently, Large Language Models\n(LLMs) have been widely adopted in molecule understanding and generation, yet\nthe alignments between molecules and their corresponding captions remain a\nsignificant challenge. Previous endeavours often treat the molecule as a\ngeneral SMILES string or molecular graph, neglecting the fine-grained\nalignments between the molecular sub-structures and the descriptive textual\nphrases, which are crucial for accurate and explainable predictions. In this\ncase, we introduce MolReFlect, a novel teacher-student framework designed to\ncontextually perform the molecule-caption alignments in a fine-grained way. Our\napproach initially leverages a larger teacher LLM to label the detailed\nalignments by directly extracting critical phrases from molecule captions or\nSMILES strings and implying them to corresponding sub-structures or\ncharacteristics. To refine these alignments, we propose In-Context Selective\nReflection, which retrieves previous extraction results as context examples for\nteacher LLM to reflect and lets a smaller student LLM select from in-context\nreflection and previous extraction results. Finally, we enhance the learning\nprocess of the student LLM through Chain-of-Thought In-Context Molecule Tuning,\nintegrating the fine-grained alignments and the reasoning processes within the\nChain-of-Thought format. Our experimental results demonstrate that MolReFlect\nenables LLMs like Mistral-7B to significantly outperform the previous\nbaselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement\nnot only enhances the generative capabilities of LLMs in the molecule-caption\ntranslation task, but also contributes to a more explainable framework.", "published": "2024-11-22 04:28:56", "link": "http://arxiv.org/abs/2411.14721v1", "categories": ["cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Evaluating and Advancing Multimodal Large Language Models in Ability\n  Lens", "abstract": "As multimodal large language models (MLLMs) advance rapidly, rigorous\nevaluation has become essential, providing further guidance for their\ndevelopment. In this work, we focus on a unified and robust evaluation of\n\\textbf{vision perception} abilities, the foundational skill of MLLMs. We find\nthat existing perception benchmarks, each focusing on different question types,\ndomains, and evaluation metrics, introduce significant evaluation variance,\ncomplicating comprehensive assessments of perception abilities when relying on\nany single benchmark. To address this, we introduce \\textbf{AbilityLens}, a\nunified benchmark designed to evaluate MLLMs across six key perception\nabilities, focusing on both accuracy and stability, with each ability\nencompassing diverse question types, domains, and metrics. With the assistance\nof AbilityLens, we: (1) identify the strengths and weaknesses of current\nmodels, highlighting stability patterns and revealing a notable performance gap\nbetween open-source and closed-source models; (2) introduce an online\nevaluation mode, which uncovers interesting ability conflict and early\nconvergence phenomena during MLLM training; and (3) design a simple\nability-specific model merging method that combines the best ability checkpoint\nfrom early training stages, effectively mitigating performance decline due to\nability conflict. The benchmark and online leaderboard will be released soon.", "published": "2024-11-22 04:41:20", "link": "http://arxiv.org/abs/2411.14725v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Universal and Context-Independent Triggers for Precise Control of LLM\n  Outputs", "abstract": "Large language models (LLMs) have been widely adopted in applications such as\nautomated content generation and even critical decision-making systems.\nHowever, the risk of prompt injection allows for potential manipulation of LLM\noutputs. While numerous attack methods have been documented, achieving full\ncontrol over these outputs remains challenging, often requiring experienced\nattackers to make multiple attempts and depending heavily on the prompt\ncontext. Recent advancements in gradient-based white-box attack techniques have\nshown promise in tasks like jailbreaks and system prompt leaks. Our research\ngeneralizes gradient-based attacks to find a trigger that is (1) Universal:\neffective irrespective of the target output; (2) Context-Independent: robust\nacross diverse prompt contexts; and (3) Precise Output: capable of manipulating\nLLM inputs to yield any specified output with high accuracy. We propose a novel\nmethod to efficiently discover such triggers and assess the effectiveness of\nthe proposed attack. Furthermore, we discuss the substantial threats posed by\nsuch attacks to LLM-based applications, highlighting the potential for\nadversaries to taking over the decisions and actions made by AI agents.", "published": "2024-11-22 05:17:18", "link": "http://arxiv.org/abs/2411.14738v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained\n  Video Reasoning via Core Frame Selection", "abstract": "The advancement of Large Vision Language Models (LVLMs) has significantly\nimproved multimodal understanding, yet challenges remain in video reasoning\ntasks due to the scarcity of high-quality, large-scale datasets. Existing video\nquestion-answering (VideoQA) datasets often rely on costly manual annotations\nwith insufficient granularity or automatic construction methods with redundant\nframe-by-frame analysis, limiting their scalability and effectiveness for\ncomplex reasoning. To address these challenges, we introduce VideoEspresso, a\nnovel dataset that features VideoQA pairs preserving essential spatial details\nand temporal coherence, along with multimodal annotations of intermediate\nreasoning steps. Our construction pipeline employs a semantic-aware method to\nreduce redundancy, followed by generating QA pairs using GPT-4o. We further\ndevelop video Chain-of-Thought (CoT) annotations to enrich reasoning processes,\nguiding GPT-4o in extracting logical relationships from QA pairs and video\ncontent. To exploit the potential of high-quality VideoQA pairs, we propose a\nHybrid LVLMs Collaboration framework, featuring a Frame Selector and a\ntwo-stage instruction fine-tuned reasoning LVLM. This framework adaptively\nselects core frames and performs CoT reasoning using multimodal evidence.\nEvaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our\nmethod outperforms existing baselines on most tasks, demonstrating superior\nvideo reasoning capabilities. Our code and dataset will be released at:\nhttps://github.com/hshjerry/VideoEspresso", "published": "2024-11-22 08:33:36", "link": "http://arxiv.org/abs/2411.14794v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Continual SFT Matches Multimodal RLHF with Negative Supervision", "abstract": "Multimodal RLHF usually happens after supervised finetuning (SFT) stage to\ncontinually improve vision-language models' (VLMs) comprehension. Conventional\nwisdom holds its superiority over continual SFT during this preference\nalignment stage. In this paper, we observe that the inherent value of\nmultimodal RLHF lies in its negative supervision, the logit of the rejected\nresponses. We thus propose a novel negative supervised finetuning (nSFT)\napproach that fully excavates these information resided. Our nSFT disentangles\nthis negative supervision in RLHF paradigm, and continually aligns VLMs with a\nsimple SFT loss. This is more memory efficient than multimodal RLHF where 2\n(e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The\neffectiveness of nSFT is rigorously proved by comparing it with various\nmultimodal RLHF approaches, across different dataset sources, base VLMs and\nevaluation metrics. Besides, fruitful of ablations are provided to support our\nhypothesis. We hope this paper will stimulate further research to properly\nalign large vision language models.", "published": "2024-11-22 08:48:30", "link": "http://arxiv.org/abs/2411.14797v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Harlequin: Color-driven Generation of Synthetic Data for Referring\n  Expression Comprehension", "abstract": "Referring Expression Comprehension (REC) aims to identify a particular object\nin a scene by a natural language expression, and is an important topic in\nvisual language understanding. State-of-the-art methods for this task are based\non deep learning, which generally requires expensive and manually labeled\nannotations. Some works tackle the problem with limited-supervision learning or\nrelying on Large Vision and Language Models. However, the development of\ntechniques to synthesize labeled data is overlooked. In this paper, we propose\na novel framework that generates artificial data for the REC task, taking into\naccount both textual and visual modalities. At first, our pipeline processes\nexisting data to create variations in the annotations. Then, it generates an\nimage using altered annotations as guidance. The result of this pipeline is a\nnew dataset, called Harlequin, made by more than 1M queries. This approach\neliminates manual data collection and annotation, enabling scalability and\nfacilitating arbitrary complexity. We pre-train three REC models on Harlequin,\nthen fine-tuned and evaluated on human-annotated datasets. Our experiments show\nthat the pre-training on artificial data is beneficial for performance.", "published": "2024-11-22 09:08:36", "link": "http://arxiv.org/abs/2411.14807v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Fine-Grained Alignment in Vision-and-Language Navigation through\n  Bayesian Optimization", "abstract": "This paper addresses the challenge of fine-grained alignment in\nVision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D\nenvironments based on natural language instructions. Current approaches use\ncontrastive learning to align language with visual trajectory sequences.\nNevertheless, they encounter difficulties with fine-grained vision negatives.\nTo enhance cross-modal embeddings, we introduce a novel Bayesian\nOptimization-based adversarial optimization framework for creating fine-grained\ncontrastive vision samples. To validate the proposed methodology, we conduct a\nseries of experiments to assess the effectiveness of the enriched embeddings on\nfine-grained vision negatives. We conduct experiments on two common VLN\nbenchmarks R2R and REVERIE, experiments on the them demonstrate that these\nembeddings benefit navigation, and can lead to a promising performance\nenhancement. Our source code and trained models are available at:\nhttps://anonymous.4open.science/r/FGVLN.", "published": "2024-11-22 09:12:02", "link": "http://arxiv.org/abs/2411.14811v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "VisGraphVar: A Benchmark Generator for Assessing Variability in Graph\n  Analysis Using Large Vision-Language Models", "abstract": "The fast advancement of Large Vision-Language Models (LVLMs) has shown\nimmense potential. These models are increasingly capable of tackling abstract\nvisual tasks. Geometric structures, particularly graphs with their inherent\nflexibility and complexity, serve as an excellent benchmark for evaluating\nthese models' predictive capabilities. While human observers can readily\nidentify subtle visual details and perform accurate analyses, our investigation\nreveals that state-of-the-art LVLMs exhibit consistent limitations in specific\nvisual graph scenarios, especially when confronted with stylistic variations.\nIn response to these challenges, we introduce VisGraphVar (Visual Graph\nVariability), a customizable benchmark generator able to produce graph images\nfor seven distinct task categories (detection, classification, segmentation,\npattern recognition, link prediction, reasoning, matching), designed to\nsystematically evaluate the strengths and limitations of individual LVLMs. We\nuse VisGraphVar to produce 990 graph images and evaluate six LVLMs, employing\ntwo distinct prompting strategies, namely zero-shot and chain-of-thought. The\nfindings demonstrate that variations in visual attributes of images (e.g., node\nlabeling and layout) and the deliberate inclusion of visual imperfections, such\nas overlapping nodes, significantly affect model performance. This research\nemphasizes the importance of a comprehensive evaluation across graph-related\ntasks, extending beyond reasoning alone. VisGraphVar offers valuable insights\nto guide the development of more reliable and robust systems capable of\nperforming advanced visual graph analysis.", "published": "2024-11-22 10:10:53", "link": "http://arxiv.org/abs/2411.14832v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "68T50"], "primary_category": "cs.CV"}
{"title": "Preference Alignment for Diffusion Model via Explicit Denoised\n  Distribution Estimation", "abstract": "Diffusion models have shown remarkable success in text-to-image generation,\nmaking preference alignment for these models increasingly important. The\npreference labels are typically available only at the terminal of denoising\ntrajectories, which poses challenges in optimizing the intermediate denoising\nsteps. In this paper, we propose to conduct Denoised Distribution Estimation\n(DDE) that explicitly connects intermediate steps to the terminal denoised\ndistribution. Therefore, preference labels can be used for the entire\ntrajectory optimization. To this end, we design two estimation strategies for\nour DDE. The first is stepwise estimation, which utilizes the conditional\ndenoised distribution to estimate the model denoised distribution. The second\nis single-shot estimation, which converts the model output into the terminal\ndenoised distribution via DDIM modeling. Analytically and empirically, we\nreveal that DDE equipped with two estimation strategies naturally derives a\nnovel credit assignment scheme that prioritizes optimizing the middle part of\nthe denoising trajectory. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively.", "published": "2024-11-22 11:45:33", "link": "http://arxiv.org/abs/2411.14871v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Evaluating LLM Prompts for Data Augmentation in Multi-label\n  Classification of Ecological Texts", "abstract": "Large language models (LLMs) play a crucial role in natural language\nprocessing (NLP) tasks, improving the understanding, generation, and\nmanipulation of human language across domains such as translating, summarizing,\nand classifying text. Previous studies have demonstrated that instruction-based\nLLMs can be effectively utilized for data augmentation to generate diverse and\nrealistic text samples. This study applied prompt-based data augmentation to\ndetect mentions of green practices in Russian social media. Detecting green\npractices in social media aids in understanding their prevalence and helps\nformulate recommendations for scaling eco-friendly actions to mitigate\nenvironmental issues. We evaluated several prompts for augmenting texts in a\nmulti-label classification task, either by rewriting existing datasets using\nLLMs, generating new data, or combining both approaches. Our results revealed\nthat all strategies improved classification performance compared to the models\nfine-tuned only on the original dataset, outperforming baselines in most cases.\nThe best results were obtained with the prompt that paraphrased the original\ntext while clearly indicating the relevant categories.", "published": "2024-11-22 12:37:41", "link": "http://arxiv.org/abs/2411.14896v1", "categories": ["cs.CL", "cs.CY", "cs.SI", "68T50", "I.2.7; J.4; I.7.2"], "primary_category": "cs.CL"}
{"title": "LLM for Barcodes: Generating Diverse Synthetic Data for Identity\n  Documents", "abstract": "Accurate barcode detection and decoding in Identity documents is crucial for\napplications like security, healthcare, and education, where reliable data\nextraction and verification are essential. However, building robust detection\nmodels is challenging due to the lack of diverse, realistic datasets an issue\noften tied to privacy concerns and the wide variety of document formats.\nTraditional tools like Faker rely on predefined templates, making them less\neffective for capturing the complexity of real-world identity documents. In\nthis paper, we introduce a new approach to synthetic data generation that uses\nLLMs to create contextually rich and realistic data without relying on\npredefined field. Using the vast knowledge LLMs have about different documents\nand content, our method creates data that reflects the variety found in real\nidentity documents. This data is then encoded into barcode and overlayed on\ntemplates for documents such as Driver's licenses, Insurance cards, Student\nIDs. Our approach simplifies the process of dataset creation, eliminating the\nneed for extensive domain knowledge or predefined fields. Compared to\ntraditional methods like Faker, data generated by LLM demonstrates greater\ndiversity and contextual relevance, leading to improved performance in barcode\ndetection models. This scalable, privacy-first solution is a big step forward\nin advancing machine learning for automated document processing and identity\nverification.", "published": "2024-11-22 14:21:18", "link": "http://arxiv.org/abs/2411.14962v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "SwissADT: An Audio Description Translation System for Swiss Languages", "abstract": "Audio description (AD) is a crucial accessibility service provided to blind\npersons and persons with visual impairment, designed to convey visual\ninformation in acoustic form. Despite recent advancements in multilingual\nmachine translation research, the lack of well-crafted and time-synchronized AD\ndata impedes the development of audio description translation (ADT) systems\nthat address the needs of multilingual countries such as Switzerland.\nFurthermore, since the majority of ADT systems rely solely on text, uncertainty\nexists as to whether incorporating visual information from the corresponding\nvideo clips can enhance the quality of ADT outputs. In this work, we present\nSwissADT, the first ADT system implemented for three main Swiss languages and\nEnglish. By collecting well-crafted AD data augmented with video clips in\nGerman, French, Italian, and English, and leveraging the power of Large\nLanguage Models (LLMs), we aim to enhance information accessibility for diverse\nlanguage populations in Switzerland by automatically translating AD scripts to\nthe desired Swiss language. Our extensive experimental ADT results, composed of\nboth automatic and human evaluations of ADT quality, demonstrate the promising\ncapability of SwissADT for the ADT task. We believe that combining human\nexpertise with the generation power of LLMs can further enhance the performance\nof ADT systems, ultimately benefiting a larger multilingual target population.", "published": "2024-11-22 14:23:07", "link": "http://arxiv.org/abs/2411.14967v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Fantastic Biases (What are They) and Where to Find Them", "abstract": "Deep Learning models tend to learn correlations of patterns on huge datasets.\nThe bigger these systems are, the more complex are the phenomena they can\ndetect, and the more data they need for this. The use of Artificial\nIntelligence (AI) is becoming increasingly ubiquitous in our society, and its\nimpact is growing everyday. The promises it holds strongly depend on their fair\nand universal use, such as access to information or education for all. In a\nworld of inequalities, they can help to reach the most disadvantaged areas.\nHowever, such a universal systems must be able to represent society, without\nbenefiting some at the expense of others. We must not reproduce the\ninequalities observed throughout the world, but educate these IAs to go beyond\nthem. We have seen cases where these systems use gender, race, or even class\ninformation in ways that are not appropriate for resolving their tasks. Instead\nof real causal reasoning, they rely on spurious correlations, which is what we\nusually call a bias. In this paper, we first attempt to define what is a bias\nin general terms. It helps us to demystify the concept of bias, to understand\nwhy we can find them everywhere and why they are sometimes useful. Second, we\nfocus over the notion of what is generally seen as negative bias, the one we\nwant to avoid in machine learning, before presenting a general zoology\ncontaining the most common of these biases. We finally conclude by looking at\nclassical methods to detect them, by means of specially crafted datasets of\ntemplates and specific algorithms, and also classical methods to mitigate them.", "published": "2024-11-22 16:38:03", "link": "http://arxiv.org/abs/2411.15051v1", "categories": ["cs.CL", "cs.CV", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Instance-Aware Generalized Referring Expression Segmentation", "abstract": "Recent works on Generalized Referring Expression Segmentation (GRES) struggle\nwith handling complex expressions referring to multiple distinct objects. This\nis because these methods typically employ an end-to-end foreground-background\nsegmentation and lack a mechanism to explicitly differentiate and associate\ndifferent object instances to the text query. To this end, we propose\nInstAlign, a method that incorporates object-level reasoning into the\nsegmentation process. Our model leverages both text and image inputs to extract\na set of object-level tokens that capture both the semantic information in the\ninput prompt and the objects within the image. By modeling the text-object\nalignment via instance-level supervision, each token uniquely represents an\nobject segment in the image, while also aligning with relevant semantic\ninformation from the text. Extensive experiments on the gRefCOCO and Ref-ZOM\nbenchmarks demonstrate that our method significantly advances state-of-the-art\nperformance, setting a new standard for precise and flexible GRES.", "published": "2024-11-22 17:28:43", "link": "http://arxiv.org/abs/2411.15087v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Context-Aware Multimodal Pretraining", "abstract": "Large-scale multimodal representation learning successfully optimizes for\nzero-shot transfer at test time. Yet the standard pretraining paradigm\n(contrastive learning on large amounts of image-text data) does not explicitly\nencourage representations to support few-shot adaptation. In this work, we\npropose a simple, but carefully designed extension to multimodal pretraining\nwhich enables representations to accommodate additional context. Using this\nobjective, we show that vision-language models can be trained to exhibit\nsignificantly increased few-shot adaptation: across 21 downstream tasks, we\nfind up to four-fold improvements in test-time sample efficiency, and average\nfew-shot adaptation gains of over 5%, while retaining zero-shot generalization\nperformance across model scales and training durations. In particular, equipped\nwith simple, training-free, metric-based adaptation mechanisms, our\nrepresentations easily surpass more complex and expensive optimization-based\nschemes, vastly simplifying generalization to new domains.", "published": "2024-11-22 17:55:39", "link": "http://arxiv.org/abs/2411.15099v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models", "abstract": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving.", "published": "2024-11-22 18:01:37", "link": "http://arxiv.org/abs/2411.15100v2", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable\n  Diffusion", "abstract": "As text-to-image models grow increasingly powerful and complex, their\nburgeoning size presents a significant obstacle to widespread adoption,\nespecially on resource-constrained devices. This paper presents a pioneering\nstudy on post-training pruning of Stable Diffusion 2, addressing the critical\nneed for model compression in text-to-image domain. Our study tackles the\npruning techniques for the previously unexplored multi-modal generation models,\nand particularly examines the pruning impact on the textual component and the\nimage generation component separately. We conduct a comprehensive comparison on\npruning the model or the single component of the model in various sparsities.\nOur results yield previously undocumented findings. For example, contrary to\nestablished trends in language model pruning, we discover that simple magnitude\npruning outperforms more advanced techniques in text-to-image context.\nFurthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%\nsparsity with minimal quality loss, achieving a significant reduction in model\nsize. We propose an optimal pruning configuration that prunes the text encoder\nto 47.5% and the diffusion generator to 35%. This configuration maintains image\ngeneration quality while substantially reducing computational requirements. In\naddition, our work uncovers intriguing questions about information encoding in\ntext-to-image models: we observe that pruning beyond certain thresholds leads\nto sudden performance drops (unreadable images), suggesting that specific\nweights encode critical semantics information. This finding opens new avenues\nfor future research in model compression, interoperability, and bias\nidentification in text-to-image models. By providing crucial insights into the\npruning behavior of text-to-image models, our study lays the groundwork for\ndeveloping more efficient and accessible AI-driven image generation systems", "published": "2024-11-22 18:29:37", "link": "http://arxiv.org/abs/2411.15113v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "VideoRepair: Improving Text-to-Video Generation via Misalignment\n  Evaluation and Localized Refinement", "abstract": "Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of two stages: In (1) video refinement planning, we first\ndetect misalignments by generating fine-grained evaluation questions and\nanswering them using an MLLM. Based on video evaluation outputs, we identify\naccurately generated objects and construct localized prompts to precisely\nrefine misaligned regions. In (2) localized refinement, we enhance video\nalignment by 'repairing' the misaligned regions from the original video while\npreserving the correctly generated areas. This is achieved by frame-wise region\ndecomposition using our Region-Preserving Segmentation (RPS) module. On two\npopular video generation benchmarks (EvalCrafter and T2V-CompBench),\nVideoRepair substantially outperforms recent baselines across various\ntext-video alignment metrics. We provide a comprehensive analysis of\nVideoRepair components and qualitative examples.", "published": "2024-11-22 18:31:47", "link": "http://arxiv.org/abs/2411.15115v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation", "abstract": "AI-driven models have demonstrated significant potential in automating\nradiology report generation for chest X-rays. However, there is no standardized\nbenchmark for objectively evaluating their performance. To address this, we\npresent ReXrank, https://rexrank.ai, a public leaderboard and challenge for\nassessing AI-powered radiology report generation. Our framework incorporates\nReXGradient, the largest test dataset consisting of 10,000 studies, and three\npublic datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation\nassessment. ReXrank employs 8 evaluation metrics and separately assesses models\ncapable of generating only findings sections and those providing both findings\nand impressions sections. By providing this standardized evaluation framework,\nReXrank enables meaningful comparisons of model performance and offers crucial\ninsights into their robustness across diverse clinical settings. Beyond its\ncurrent focus on chest X-rays, ReXrank's framework sets the stage for\ncomprehensive evaluation of automated reporting across the full spectrum of\nmedical imaging.", "published": "2024-11-22 18:40:02", "link": "http://arxiv.org/abs/2411.15122v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Measuring Bullshit in the Language Games played by ChatGPT", "abstract": "Generative large language models (LLMs), which create text without direct\ncorrespondence to truth value, are widely understood to resemble the uses of\nlanguage described in Frankfurt's popular monograph On Bullshit. In this paper,\nwe offer a rigorous investigation of this topic, identifying how the phenomenon\nhas arisen, and how it might be analysed. In this paper, we elaborate on this\nargument to propose that LLM-based chatbots play the 'language game of\nbullshit'. We use statistical text analysis to investigate the features of this\nWittgensteinian language game, based on a dataset constructed to contrast the\nlanguage of 1,000 scientific publications with typical pseudo-scientific text\ngenerated by ChatGPT. We then explore whether the same language features can be\ndetected in two well-known contexts of social dysfunction: George Orwell's\ncritique of politics and language, and David Graeber's characterisation of\nbullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a\nstatistical model of the language of bullshit can reliably relate the\nFrankfurtian artificial bullshit of ChatGPT to the political and workplace\nfunctions of bullshit as observed in natural human language.", "published": "2024-11-22 18:55:21", "link": "http://arxiv.org/abs/2411.15129v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The Zamba2 Suite: Technical Report", "abstract": "In this technical report, we present the Zamba2 series -- a suite of 1.2B,\n2.7B, and 7.4B parameter hybrid Mamba2-transformer models that achieve state of\nthe art performance against the leading open-weights models of their class,\nwhile achieving substantial gains in inference latency, throughput, and memory\nefficiency. The Zamba2 series builds upon our initial work with Zamba1-7B,\noptimizing its architecture, training and annealing datasets, and training for\nup to three trillion tokens. We provide open-source weights for all models of\nthe Zamba2 series as well as instruction-tuned variants that are strongly\ncompetitive against comparable instruct-tuned models of their class. We\nadditionally open-source the pretraining dataset, which we call Zyda-2, used to\ntrain the Zamba2 series of models. The models and datasets used in this work\nare openly available at https://huggingface.co/Zyphra", "published": "2024-11-22 02:55:20", "link": "http://arxiv.org/abs/2411.15242v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TPLogAD: Unsupervised Log Anomaly Detection Based on Event Templates and\n  Key Parameters", "abstract": "Log-system is an important mechanism for recording the runtime status and\nevents of Web service systems, and anomaly detection in logs is an effective\nmethod of detecting problems. However, manual anomaly detection in logs is\ninefficient, error-prone, and unrealistic. Existing log anomaly detection\nmethods either use the indexes of event templates, or form vectors by embedding\nthe fixed string part of the template as a sentence, or use time parameters for\nsequence analysis. However, log entries often contain features and semantic\ninformation that cannot be fully represented by these methods, resulting in\nmissed and false alarms. In this paper, we propose TPLogAD, a universal\nunsupervised method for analyzing unstructured logs, which performs anomaly\ndetection based on event templates and key parameters. The itemplate2vec and\npara2vec included in TPLogAD are two efficient and easy-to-implement semantic\nrepresentation methods for logs, detecting anomalies in event templates and\nparameters respectively, which has not been achieved in previous work.\nAdditionally, TPLogAD can avoid the interference of log diversity and dynamics\non anomaly detection. Our experiments on four public log datasets show that\nTPLogAD outperforms existing log anomaly detection methods.", "published": "2024-11-22 08:25:21", "link": "http://arxiv.org/abs/2411.15250v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs", "abstract": "As a prominent direction of Artificial General Intelligence (AGI), Multimodal\nLarge Language Models (MLLMs) have garnered increased attention from both\nindustry and academia. Building upon pre-trained LLMs, this family of models\nfurther develops multimodal perception and reasoning capabilities that are\nimpressive, such as writing code given a flow chart or creating stories based\non an image. In the development process, evaluation is critical since it\nprovides intuitive feedback and guidance on improving models. Distinct from the\ntraditional train-eval-test paradigm that only favors a single task like image\nclassification, the versatility of MLLMs has spurred the rise of various new\nbenchmarks and evaluation methods. In this paper, we aim to present a\ncomprehensive survey of MLLM evaluation, discussing four key aspects: 1) the\nsummarised benchmarks types divided by the evaluation capabilities, including\nfoundation capabilities, model self-analysis, and extented applications; 2) the\ntypical process of benchmark counstruction, consisting of data collection,\nannotation, and precautions; 3) the systematic evaluation manner composed of\njudge, metric, and toolkit; 4) the outlook for the next benchmark. This work\naims to offer researchers an easy grasp of how to effectively evaluate MLLMs\naccording to different needs and to inspire better evaluation methods, thereby\ndriving the progress of MLLM research.", "published": "2024-11-22 18:59:54", "link": "http://arxiv.org/abs/2411.15296v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploring Facets of Language Generation in the Limit", "abstract": "The recent work of Kleinberg & Mullainathan [KM24] provides a concrete model\nfor language generation in the limit: given a sequence of examples from an\nunknown target language, the goal is to generate new examples from the target\nlanguage such that no incorrect examples are generated beyond some point. In\nsharp contrast to strong negative results for the closely related problem of\nlanguage identification, they establish positive results for language\ngeneration in the limit for all countable collections of languages. Follow-up\nwork by Raman & Tewari [RT24] studies bounds on the number of distinct inputs\nrequired by an algorithm before correct language generation is achieved --\nnamely, whether this is a constant for all languages in the collection (uniform\ngeneration) or a language-dependent constant (non-uniform generation).\n  We show that every countable language collection has a generator which has\nthe stronger property of non-uniform generation in the limit. However, while\nthe generation algorithm of [KM24] can be implemented using membership queries,\nwe show that any algorithm cannot non-uniformly generate even for collections\nof just two languages, using only membership queries.\n  We also formalize the tension between validity and breadth in the generation\nalgorithm of [KM24] by introducing a definition of exhaustive generation, and\nshow a strong negative result for exhaustive generation. Our result shows that\na tradeoff between validity and breadth is inherent for generation in the\nlimit. We also provide a precise characterization of the language collections\nfor which exhaustive generation is possible. Finally, inspired by algorithms\nthat can choose to obtain feedback, we consider a model of uniform generation\nwith feedback, completely characterizing language collections for which such\nuniform generation with feedback is possible in terms of a complexity measure\nof the collection.", "published": "2024-11-22 22:13:40", "link": "http://arxiv.org/abs/2411.15364v2", "categories": ["cs.DS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DS"}
{"title": "Transforming NLU with Babylon: A Case Study in Development of Real-time,\n  Edge-Efficient, Multi-Intent Translation System for Automated Drive-Thru\n  Ordering", "abstract": "Real-time conversational AI agents face challenges in performing Natural\nLanguage Understanding (NLU) in dynamic, outdoor environments like automated\ndrive-thru systems. These settings require NLU models to handle background\nnoise, diverse accents, and multi-intent queries while operating under strict\nlatency and memory constraints on edge devices. Additionally, robustness to\nerrors from upstream Automatic Speech Recognition (ASR) is crucial, as ASR\noutputs in these environments are often noisy. We introduce Babylon, a\ntransformer-based architecture that tackles NLU as an intent translation task,\nconverting natural language inputs into sequences of regular language units\n('transcodes') that encode both intents and slot information. This formulation\nallows Babylon to manage multi-intent scenarios in a single dialogue turn.\nFurthermore, Babylon incorporates an LSTM-based token pooling mechanism to\npreprocess phoneme sequences, reducing input length and optimizing for\nlow-latency, low-memory edge deployment. This also helps mitigate inaccuracies\nin ASR outputs, enhancing system robustness. While this work focuses on\ndrive-thru ordering, Babylon's design extends to similar noise-prone scenarios,\nfor e.g. ticketing kiosks. Our experiments show that Babylon achieves\nsignificantly better accuracy-latency-memory footprint trade-offs over\ntypically employed NMT models like Flan-T5 and BART, demonstrating its\neffectiveness for real-time NLU in edge deployment settings.", "published": "2024-11-22 23:03:35", "link": "http://arxiv.org/abs/2411.15372v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bio-inspired AI: Integrating Biological Complexity into Artificial\n  Intelligence", "abstract": "The pursuit of creating artificial intelligence (AI) mirrors our longstanding\nfascination with understanding our own intelligence. From the myths of Talos to\nAristotelian logic and Heron's inventions, we have sought to replicate the\nmarvels of the mind. While recent advances in AI hold promise, singular\napproaches often fall short in capturing the essence of intelligence. This\npaper explores how fundamental principles from biological\ncomputation--particularly context-dependent, hierarchical information\nprocessing, trial-and-error heuristics, and multi-scale organization--can guide\nthe design of truly intelligent systems. By examining the nuanced mechanisms of\nbiological intelligence, such as top-down causality and adaptive interaction\nwith the environment, we aim to illuminate potential limitations in artificial\nconstructs. Our goal is to provide a framework inspired by biological systems\nfor designing more adaptable and robust artificial intelligent systems.", "published": "2024-11-22 02:55:39", "link": "http://arxiv.org/abs/2411.15243v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.CV", "cs.NE", "cs.SC"], "primary_category": "q-bio.NC"}
{"title": "VQalAttent: a Transparent Speech Generation Pipeline based on\n  Transformer-learned VQ-VAE Latent Space", "abstract": "Generating high-quality speech efficiently remains a key challenge for\ngenerative models in speech synthesis. This paper introduces VQalAttent, a\nlightweight model designed to generate fake speech with tunable performance and\ninterpretability. Leveraging the AudioMNIST dataset, consisting of human\nutterances of decimal digits (0-9), our method employs a two-step architecture:\nfirst, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio\nspectrograms into discrete latent representations, and second, a decoder-only\ntransformer that learns the probability model of these latents. Trained\ntransformer generates similar latent sequences, convertible to audio\nspectrograms by the VQ-VAE decoder, from which we generate fake utterances.\nInterpreting statistical and perceptual quality of the fakes, depending on the\ndimension and the extrinsic information of the latent space, enables guided\nimprovements in larger, commercial generative models. As a valuable tool for\nunderstanding and refining audio synthesis, our results demonstrate\nVQalAttent's capacity to generate intelligible speech samples with limited\ncomputational resources, while the modularity and transparency of the training\npipeline helps easily correlate the analytics with modular modifications, hence\nproviding insights for the more complex models.", "published": "2024-11-22 00:21:39", "link": "http://arxiv.org/abs/2411.14642v1", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Mode-conditioned music learning and composition: a spiking neural\n  network inspired by neuroscience and psychology", "abstract": "Musical mode is one of the most critical element that establishes the\nframework of pitch organization and determines the harmonic relationships.\nPrevious works often use the simplistic and rigid alignment method, and\noverlook the diversity of modes. However, in contrast to AI models, humans\npossess cognitive mechanisms for perceiving the various modes and keys. In this\npaper, we propose a spiking neural network inspired by brain mechanisms and\npsychological theories to represent musical modes and keys, ultimately\ngenerating musical pieces that incorporate tonality features. Specifically, the\ncontributions are detailed as follows: 1) The model is designed with multiple\ncollaborated subsystems inspired by the structures and functions of\ncorresponding brain regions; 2)We incorporate mechanisms for neural circuit\nevolutionary learning that enable the network to learn and generate\nmode-related features in music, reflecting the cognitive processes involved in\nhuman music perception. 3)The results demonstrate that the proposed model shows\na connection framework closely similar to the Krumhansl-Schmuckler model, which\nis one of the most significant key perception models in the music psychology\ndomain. 4) Experiments show that the model can generate music pieces with\ncharacteristics of the given modes and keys. Additionally, the quantitative\nassessments of generated pieces reveals that the generating music pieces have\nboth tonality characteristics and the melodic adaptability needed to generate\ndiverse and musical content. By combining insights from neuroscience,\npsychology, and music theory with advanced neural network architectures, our\nresearch aims to create a system that not only learns and generates music but\nalso bridges the gap between human cognition and artificial intelligence.", "published": "2024-11-22 07:29:26", "link": "http://arxiv.org/abs/2411.14773v2", "categories": ["cs.SD", "cs.AI", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large\n  Language Models", "abstract": "Adversarial audio attacks pose a significant threat to the growing use of\nlarge language models (LLMs) in voice-based human-machine interactions. While\nexisting research has primarily focused on model-specific adversarial methods,\nreal-world applications demand a more generalizable and universal approach to\naudio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks\n(CAA) benchmark including four distinct types of audio attacks, which aims to\nexplore the the vulnerabilities of LLMs to these audio attacks in\nconversational scenarios. To evaluate the robustness of LLMs, we propose three\nevaluation strategies: Standard Evaluation, utilizing traditional metrics to\nquantify model performance under attacks; GPT-4o-Based Evaluation, which\nsimulates real-world conversational complexities; and Human Evaluation,\noffering insights into user perception and trust. We evaluate six\nstate-of-the-art LLMs with voice interaction capabilities, including\nGemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on\nthe CAA benchmark. Our comprehensive analysis reveals the impact of four types\nof audio attacks on the performance of these models, demonstrating that GPT-4o\nexhibits the highest level of resilience.", "published": "2024-11-22 10:30:48", "link": "http://arxiv.org/abs/2411.14842v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DAIRHuM: A Platform for Directly Aligning AI Representations with Human\n  Musical Judgments applied to Carnatic Music", "abstract": "Quantifying and aligning music AI model representations with human behavior\nis an important challenge in the field of MIR. This paper presents a platform\nfor exploring the Direct alignment between AI music model Representations and\nHuman Musical judgments (DAIRHuM). It is designed to enable musicians and\nexperimentalists to label similarities in a dataset of music recordings, and\nexamine a pre-trained model's alignment with their labels using quantitative\nscores and visual plots. DAIRHuM is applied to analyze alignment between NSynth\nrepresentations, and a rhythmic duet between two percussionists in a Carnatic\nquartet ensemble, an example of a genre where annotated data is scarce and\nassessing alignment is non-trivial. The results demonstrate significant\nfindings on model alignment with human judgments of rhythmic harmony, while\nhighlighting key differences in rhythm perception and music similarity\njudgments specific to Carnatic music. This work is among the first efforts to\nenable users to explore human-AI model alignment in Carnatic music and advance\nMIR research in Indian music while dealing with data scarcity and cultural\nspecificity. The development of this platform provides greater accessibility to\nmusic AI tools for under-represented genres.", "published": "2024-11-22 13:04:51", "link": "http://arxiv.org/abs/2411.14907v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Open-Amp: Synthetic Data Framework for Audio Effect Foundation Models", "abstract": "This paper introduces Open-Amp, a synthetic data framework for generating\nlarge-scale and diverse audio effects data. Audio effects are relevant to many\nmusical audio processing and Music Information Retrieval (MIR) tasks, such as\nmodelling of analog audio effects, automatic mixing, tone matching and\ntranscription. Existing audio effects datasets are limited in scope, usually\nincluding relatively few audio effects processors and a limited amount of input\naudio signals. Our proposed framework overcomes these issues, by crowdsourcing\nneural network emulations of guitar amplifiers and effects, created by users of\nopen-source audio effects emulation software. This allows users of Open-Amp\ncomplete control over the input signals to be processed by the effects models,\nas well as providing high-quality emulations of hundreds of devices. Open-Amp\ncan render audio online during training, allowing great flexibility in data\naugmentation. Our experiments show that using Open-Amp to train a guitar\neffects encoder achieves new state-of-the-art results on multiple guitar\neffects classification tasks. Furthermore, we train a one-to-many guitar\neffects model using Open-Amp, and use it to emulate unseen analog effects via\nmanipulation of its learned latent space, indicating transferability to analog\nguitar effects data.", "published": "2024-11-22 14:27:59", "link": "http://arxiv.org/abs/2411.14972v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Speaker Identification with Minimal Dataset and Constrained\n  Resources using 1D-Convolution Neural Network", "abstract": "Voice recognition and speaker identification are vital for applications in\nsecurity and personal assistants. This paper presents a lightweight\n1D-Convolutional Neural Network (1D-CNN) designed to perform speaker\nidentification on minimal datasets. Our approach achieves a validation accuracy\nof 97.87%, leveraging data augmentation techniques to handle background noise\nand limited training samples. Future improvements include testing on larger\ndatasets and integrating transfer learning methods to enhance generalizability.\nWe provide all code, the custom dataset, and the trained models to facilitate\nreproducibility. These resources are available on our GitHub repository:\nhttps://github.com/IrfanNafiz/RecMe.", "published": "2024-11-22 17:18:08", "link": "http://arxiv.org/abs/2411.15082v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Comparison of Tiny Machine Learning Techniques for Embedded Acoustic\n  Emission Analysis", "abstract": "This paper compares machine learning approaches with different input data\nformats for the classification of acoustic emission (AE) signals. AE signals\nare a promising monitoring technique in many structural health monitoring\napplications. Machine learning has been demonstrated as an effective data\nanalysis method, classifying different AE signals according to the damage\nmechanism they represent. These classifications can be performed based on the\nentire AE waveform or specific features that have been extracted from it.\nHowever, it is currently unknown which of these approaches is preferred. With\nthe goal of model deployment on resource-constrained embedded Internet of\nThings (IoT) systems, this work evaluates and compares both approaches in terms\nof classification accuracy, memory requirement, processing time, and energy\nconsumption. To accomplish this, features are extracted and carefully selected,\nneural network models are designed and optimized for each input data scenario,\nand the models are deployed on a low-power IoT node. The comparative analysis\nreveals that all models can achieve high classification accuracies of over\n99\\%, but that embedded feature extraction is computationally expensive.\nConsequently, models utilizing the raw AE signal as input have the fastest\nprocessing speed and thus the lowest energy consumption, which comes at the\ncost of a larger memory requirement.", "published": "2024-11-22 15:58:25", "link": "http://arxiv.org/abs/2411.17733v1", "categories": ["eess.SP", "cs.LG", "eess.AS"], "primary_category": "eess.SP"}
