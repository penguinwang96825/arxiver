{"title": "Developing and Using Special-Purpose Lexicons for Cohort Selection from\n  Clinical Notes", "abstract": "Background and Significance: Selecting cohorts for a clinical trial typically\nrequires costly and time-consuming manual chart reviews resulting in poor\nparticipation. To help automate the process, National NLP Clinical Challenges\n(N2C2) conducted a shared challenge by defining 13 criteria for clinical trial\ncohort selection and by providing training and test datasets. This research was\nmotivated by the N2C2 challenge.\n  Methods: We broke down the task into 13 independent subtasks corresponding to\neach criterion and implemented subtasks using rules or a supervised machine\nlearning model. Each task critically depended on knowledge resources in the\nform of task-specific lexicons, for which we developed a novel model-driven\napproach. The approach allowed us to first expand the lexicon from a seed set\nand then remove noise from the list, thus improving the accuracy.\n  Results: Our system achieved an overall F measure of 0.9003 at the challenge,\nand was statistically tied for the first place out of 45 participants. The\nmodel-driven lexicon development and further debugging the rules/code on the\ntraining set improved overall F measure to 0.9140, overtaking the best\nnumerical result at the challenge.\n  Discussion: Cohort selection, like phenotype extraction and classification,\nis amenable to rule-based or simple machine learning methods, however, the\nlexicons involved, such as medication names or medical terms referring to a\nmedical problem, critically determine the overall accuracy. Automated lexicon\ndevelopment has the potential for scalability and accuracy.", "published": "2019-02-26 00:45:56", "link": "http://arxiv.org/abs/1902.09674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Polyglot Contextual Representations Improve Crosslingual Transfer", "abstract": "We introduce Rosita, a method to produce multilingual contextual word\nrepresentations by training a single language model on text from multiple\nlanguages. Our method combines the advantages of contextual word\nrepresentations with those of multilingual representation learning. We produce\nlanguage models from dissimilar language pairs (English/Arabic and\nEnglish/Chinese) and use them in dependency parsing, semantic role labeling,\nand named entity recognition, with comparisons to monolingual and\nnon-contextual variants. Our results provide further evidence for the benefits\nof polyglot learning, in which representations are shared across multiple\nlanguages.", "published": "2019-02-26 01:50:09", "link": "http://arxiv.org/abs/1902.09697v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recursive Subtree Composition in LSTM-Based Dependency Parsing", "abstract": "The need for tree structure modelling on top of sequence modelling is an open\nissue in neural dependency parsing. We investigate the impact of adding a tree\nlayer on top of a sequential model by recursively composing subtree\nrepresentations (composition) in a transition-based parser that uses features\nextracted by a BiLSTM. Composition seems superfluous with such a model,\nsuggesting that BiLSTMs capture information about subtrees. We perform model\nablations to tease out the conditions under which composition helps. When\nablating the backward LSTM, performance drops and composition does not recover\nmuch of the gap. When ablating the forward LSTM, performance drops less\ndramatically and composition recovers a substantial part of the gap, indicating\nthat a forward LSTM and composition capture similar information. We take the\nbackward LSTM to be related to lookahead features and the forward LSTM to the\nrich history-based features both crucial for transition-based parsers. To\ncapture history-based information, composition is better than a forward LSTM on\nits own, but it is even better to have a forward LSTM as part of a BiLSTM. We\ncorrelate results with language properties, showing that the improved lookahead\nof a backward LSTM is especially important for head-final languages.", "published": "2019-02-26 07:57:22", "link": "http://arxiv.org/abs/1902.09781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Hilbert Space for Text Representation Learning", "abstract": "Capturing the meaning of sentences has long been a challenging task. Current\nmodels tend to apply linear combinations of word features to conduct semantic\ncomposition for bigger-granularity units e.g. phrases, sentences, and\ndocuments. However, the semantic linearity does not always hold in human\nlanguage. For instance, the meaning of the phrase `ivory tower' can not be\ndeduced by linearly combining the meanings of `ivory' and `tower'. To address\nthis issue, we propose a new framework that models different levels of semantic\nunits (e.g. sememe, word, sentence, and semantic abstraction) on a single\n\\textit{Semantic Hilbert Space}, which naturally admits a non-linear semantic\ncomposition by means of a complex-valued vector word representation. An\nend-to-end neural network~\\footnote{https://github.com/wabyking/qnn} is\nproposed to implement the framework in the text classification task, and\nevaluation results on six benchmarking text classification datasets demonstrate\nthe effectiveness, robustness and self-explanation power of the proposed model.\nFurthermore, intuitive case studies are conducted to help end users to\nunderstand how the framework works.", "published": "2019-02-26 08:46:15", "link": "http://arxiv.org/abs/1902.09802v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving a tf-idf weighted document vector embedding", "abstract": "We examine a number of methods to compute a dense vector embedding for a\ndocument in a corpus, given a set of word vectors such as those from word2vec\nor GloVe. We describe two methods that can improve upon a simple weighted sum,\nthat are optimal in the sense that they maximizes a particular weighted cosine\nsimilarity measure.\n  We consider several weighting functions, including inverse document frequency\n(idf), smooth inverse frequency (SIF), and the sub-sampling function used in\nword2vec. We find that idf works best for our applications. We also use common\ncomponent removal proposed by Arora et al. as a post-process and find it is\nhelpful in most cases.\n  We compare these embeddings variations to the doc2vec embedding on a new\nevaluation task using TripAdvisor reviews, and also on the CQADupStack\nbenchmark from the literature.", "published": "2019-02-26 11:55:35", "link": "http://arxiv.org/abs/1902.09875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Recognition at First Sight: Improving NER with Eye Movement\n  Information", "abstract": "Previous research shows that eye-tracking data contains information about the\nlexical and syntactic properties of text, which can be used to improve natural\nlanguage processing models. In this work, we leverage eye movement features\nfrom three corpora with recorded gaze information to augment a state-of-the-art\nneural model for named entity recognition (NER) with gaze embeddings. These\ncorpora were manually annotated with named entity labels. Moreover, we show how\ngaze features, generalized on word type level, eliminate the need for recorded\neye-tracking data at test time. The gaze-augmented models for NER using\ntoken-level and type-level features outperform the baselines. We present the\nbenefits of eye-tracking features by evaluating the NER models on both\nindividual datasets as well as in cross-domain settings.", "published": "2019-02-26 17:29:43", "link": "http://arxiv.org/abs/1902.10068v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning with Contextualized Word Representations for\n  Extented Named Entity Recognition", "abstract": "Fine-Grained Named Entity Recognition (FG-NER) is critical for many NLP\napplications. While classical named entity recognition (NER) has attracted a\nsubstantial amount of research, FG-NER is still an open research domain. The\ncurrent state-of-the-art (SOTA) model for FG-NER relies heavily on manual\nefforts for building a dictionary and designing hand-crafted features. The\nend-to-end framework which achieved the SOTA result for NER did not get the\ncompetitive result compared to SOTA model for FG-NER. In this paper, we\ninvestigate how effective multi-task learning approaches are in an end-to-end\nframework for FG-NER in different aspects. Our experiments show that using\nmulti-task learning approaches with contextualized word representation can help\nan end-to-end neural network model achieve SOTA results without using any\nadditional manual effort for creating data and designing features.", "published": "2019-02-26 18:53:22", "link": "http://arxiv.org/abs/1902.10118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Idiosyncrasies of the Mandarin Chinese Classifier System", "abstract": "While idiosyncrasies of the Chinese classifier system have been a richly\nstudied topic among linguists (Adams and Conklin, 1973; Erbaugh, 1986; Lakoff,\n1986), not much work has been done to quantify them with statistical methods.\nIn this paper, we introduce an information-theoretic approach to measuring\nidiosyncrasy; we examine how much the uncertainty in Mandarin Chinese\nclassifiers can be reduced by knowing semantic information about the nouns that\nthe classifiers modify. Using the empirical distribution of classifiers from\nthe parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the mutual\ninformation (in bits) between the distribution over classifiers and\ndistributions over other linguistic quantities. We investigate whether semantic\nclasses of nouns and adjectives differ in how much they reduce uncertainty in\nclassifier choice, and find that it is not fully idiosyncratic; while there are\nno obvious trends for the majority of semantic classes, shape nouns reduce\nuncertainty in classifier choice the most.", "published": "2019-02-26 20:11:27", "link": "http://arxiv.org/abs/1902.10193v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning When Not to Answer: A Ternary Reward Structure for\n  Reinforcement Learning based Question Answering", "abstract": "In this paper, we investigate the challenges of using reinforcement learning\nagents for question-answering over knowledge graphs for real-world\napplications. We examine the performance metrics used by state-of-the-art\nsystems and determine that they are inadequate for such settings. More\nspecifically, they do not evaluate the systems correctly for situations when\nthere is no answer available and thus agents optimized for these metrics are\npoor at modeling confidence. We introduce a simple new performance metric for\nevaluating question-answering agents that is more representative of practical\nusage conditions, and optimize for this metric by extending the binary reward\nstructure used in prior work to a ternary reward structure which also rewards\nan agent for not answering a question rather than giving an incorrect answer.\nWe show that this can drastically improve the precision of answered questions\nwhile only not answering a limited number of previously correctly answered\nquestions. Employing a supervised learning strategy using depth-first-search\npaths to bootstrap the reinforcement learning algorithm further improves\nperformance.", "published": "2019-02-26 21:33:48", "link": "http://arxiv.org/abs/1902.10236v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Recurrent Neural Network for Authorship Attribution", "abstract": "Writing style is a combination of consistent decisions at different levels of\nlanguage production including lexical, syntactic, and structural associated to\na specific author (or author groups). While lexical-based models have been\nwidely explored in style-based text classification, relying on content makes\nthe model less scalable when dealing with heterogeneous data comprised of\nvarious topics. On the other hand, syntactic models which are\ncontent-independent, are more robust against topic variance. In this paper, we\nintroduce a syntactic recurrent neural network to encode the syntactic patterns\nof a document in a hierarchical structure. The model first learns the syntactic\nrepresentation of sentences from the sequence of part-of-speech tags. For this\npurpose, we exploit both convolutional filters and long short-term memories to\ninvestigate the short-term and long-term dependencies of part-of-speech tags in\nthe sentences. Subsequently, the syntactic representations of sentences are\naggregated into document representation using recurrent neural networks. Our\nexperimental results on PAN 2012 dataset for authorship attribution task shows\nthat syntactic recurrent neural network outperforms the lexical model with the\nidentical architecture by approximately 14% in terms of accuracy.", "published": "2019-02-26 04:32:42", "link": "http://arxiv.org/abs/1902.09723v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Image-Question-Answer Synergistic Network for Visual Dialog", "abstract": "The image, question (combined with the history for de-referencing), and the\ncorresponding answer are three vital components of visual dialog. Classical\nvisual dialog systems integrate the image, question, and history to search for\nor generate the best matched answer, and so, this approach significantly\nignores the role of the answer. In this paper, we devise a novel\nimage-question-answer synergistic network to value the role of the answer for\nprecise visual dialog. We extend the traditional one-stage solution to a\ntwo-stage solution. In the first stage, candidate answers are coarsely scored\naccording to their relevance to the image and question pair. Afterward, in the\nsecond stage, answers with high probability of being correct are re-ranked by\nsynergizing with image and question. On the Visual Dialog v1.0 dataset, the\nproposed synergistic network boosts the discriminative visual dialog model to\nachieve a new state-of-the-art of 57.88\\% normalized discounted cumulative\ngain. A generative visual dialog model equipped with the proposed technique\nalso shows promising improvements.", "published": "2019-02-26 07:30:43", "link": "http://arxiv.org/abs/1902.09774v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A multimodal movie review corpus for fine-grained opinion mining", "abstract": "In this paper, we introduce a set of opinion annotations for the POM movie\nreview dataset, composed of 1000 videos. The annotation campaign is motivated\nby the development of a hierarchical opinion prediction framework allowing one\nto predict the different components of the opinions (e.g. polarity and aspect)\nand to identify the corresponding textual spans. The resulting annotations have\nbeen gathered at two granularity levels: a coarse one (opinionated span) and a\nfiner one (span of opinion components). We introduce specific categories in\norder to make the annotation of opinions easier for movie reviews. For example,\nsome categories allow the discovery of user recommendation and preference in\nmovie reviews. We provide a quantitative analysis of the annotations and report\nthe inter-annotator agreement under the different levels of granularity. We\nprovide thus the first set of ground-truth annotations which can be used for\nthe task of fine-grained multimodal opinion prediction. We provide an analysis\nof the data gathered through an inter-annotator study and show that a linear\nstructured predictor learns meaningful features even for the prediction of\nscarce labels. Both the annotations and the baseline system are made publicly\navailable. https://github.com/eusip/POM/", "published": "2019-02-26 18:30:50", "link": "http://arxiv.org/abs/1902.10102v2", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Attention is not Explanation", "abstract": "Attention mechanisms have seen wide adoption in neural NLP models. In\naddition to improving predictive performance, these are often touted as\naffording transparency: models equipped with attention provide a distribution\nover attended-to input units, and this is often presented (at least implicitly)\nas communicating the relative importance of inputs. However, it is unclear what\nrelationship exists between attention weights and model outputs. In this work,\nwe perform extensive experiments across a variety of NLP tasks that aim to\nassess the degree to which attention weights provide meaningful `explanations'\nfor predictions. We find that they largely do not. For example, learned\nattention weights are frequently uncorrelated with gradient-based measures of\nfeature importance, and one can identify very different attention distributions\nthat nonetheless yield equivalent predictions. Our findings show that standard\nattention modules do not provide meaningful explanations and should not be\ntreated as though they do. Code for all experiments is available at\nhttps://github.com/successar/AttentionExplanation.", "published": "2019-02-26 19:59:15", "link": "http://arxiv.org/abs/1902.10186v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpretable Structure-aware Document Encoders with Hierarchical\n  Attention", "abstract": "We propose a method to create document representations that reflect their\ninternal structure. We modify Tree-LSTMs to hierarchically merge basic elements\nsuch as words and sentences into blocks of increasing complexity. Our Structure\nTree-LSTM implements a hierarchical attention mechanism over individual\ncomponents and combinations thereof. We thus emphasize the usefulness of\nTree-LSTMs for texts larger than a sentence. We show that structure-aware\nencoders can be used to improve the performance of document classification. We\ndemonstrate that our method is resilient to changes to the basic building\nblocks, as it performs well with both sentence and word embeddings. The\nStructure Tree-LSTM outperforms all the baselines on two datasets by leveraging\nstructural clues. We show our model's interpretability by visualizing how our\nmodel distributes attention inside a document. On a third dataset from the\nmedical domain, our model achieves competitive performance with the state of\nthe art. This result shows the Structure Tree-LSTM can leverage dependency\nrelations other than text structure, such as a set of reports on the same\npatient.", "published": "2019-02-26 02:54:03", "link": "http://arxiv.org/abs/1902.09713v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Context Vectors are Reflections of Word Vectors in Half the Dimensions", "abstract": "This paper takes a step towards theoretical analysis of the relationship\nbetween word embeddings and context embeddings in models such as word2vec. We\nstart from basic probabilistic assumptions on the nature of word vectors,\ncontext vectors, and text generation. These assumptions are well supported\neither empirically or theoretically by the existing literature. Next, we show\nthat under these assumptions the widely-used word-word PMI matrix is\napproximately a random symmetric Gaussian ensemble. This, in turn, implies that\ncontext vectors are reflections of word vectors in approximately half the\ndimensions. As a direct application of our result, we suggest a theoretically\ngrounded way of tying weights in the SGNS model.", "published": "2019-02-26 11:09:29", "link": "http://arxiv.org/abs/1902.09859v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A framework for information extraction from tables in biomedical\n  literature", "abstract": "The scientific literature is growing exponentially, and professionals are no\nmore able to cope with the current amount of publications. Text mining provided\nin the past methods to retrieve and extract information from text; however,\nmost of these approaches ignored tables and figures. The research done in\nmining table data still does not have an integrated approach for mining that\nwould consider all complexities and challenges of a table. Our research is\nexamining the methods for extracting numerical (number of patients, age, gender\ndistribution) and textual (adverse reactions) information from tables in the\nclinical literature. We present a requirement analysis template and an integral\nmethodology for information extraction from tables in clinical domain that\ncontains 7 steps: (1) table detection, (2) functional processing, (3)\nstructural processing, (4) semantic tagging, (5) pragmatic processing, (6) cell\nselection and (7) syntactic processing and extraction. Our approach performed\nwith the F-measure ranged between 82 and 92%, depending on the variable, task\nand its complexity.", "published": "2019-02-26 16:22:15", "link": "http://arxiv.org/abs/1902.10031v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex\n  Space", "abstract": "We study the problem of learning representations of entities and relations in\nknowledge graphs for predicting missing links. The success of such a task\nheavily relies on the ability of modeling and inferring the patterns of (or\nbetween) the relations. In this paper, we present a new approach for knowledge\ngraph embedding called RotatE, which is able to model and infer various\nrelation patterns including: symmetry/antisymmetry, inversion, and composition.\nSpecifically, the RotatE model defines each relation as a rotation from the\nsource entity to the target entity in the complex vector space. In addition, we\npropose a novel self-adversarial negative sampling technique for efficiently\nand effectively training the RotatE model. Experimental results on multiple\nbenchmark knowledge graphs show that the proposed RotatE model is not only\nscalable, but also able to infer and model various relation patterns and\nsignificantly outperform existing state-of-the-art models for link prediction.", "published": "2019-02-26 20:15:09", "link": "http://arxiv.org/abs/1902.10197v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Beyond the Self: Using Grounded Affordances to Interpret and Describe\n  Others' Actions", "abstract": "We propose a developmental approach that allows a robot to interpret and\ndescribe the actions of human agents by reusing previous experience. The robot\nfirst learns the association between words and object affordances by\nmanipulating the objects in its environment. It then uses this information to\nlearn a mapping between its own actions and those performed by a human in a\nshared environment. It finally fuses the information from these two models to\ninterpret and describe human actions in light of its own experience. In our\nexperiments, we show that the model can be used flexibly to do inference on\ndifferent aspects of the scene. We can predict the effects of an action on the\nbasis of object properties. We can revise the belief that a certain action\noccurred, given the observed effects of the human action. In an early action\nrecognition fashion, we can anticipate the effects when the action has only\nbeen partially observed. By estimating the probability of words given the\nevidence and feeding them into a pre-defined grammar, we can generate relevant\ndescriptions of the scene. We believe that this is a step towards providing\nrobots with the fundamental skills to engage in social collaboration with\nhumans.", "published": "2019-02-26 02:14:10", "link": "http://arxiv.org/abs/1902.09705v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Acoustic scene classification using multi-layer temporal pooling based\n  on convolutional neural network", "abstract": "The performance of an Acoustic Scene Classification (ASC) system is highly\ndepending on the latent temporal dynamics of the audio signal. In this paper,\nwe proposed a multiple layers temporal pooling method using CNN feature\nsequence as in-put, which can effectively capture the temporal dynamics for an\nentire audio signal with arbitrary duration by building direct connections\nbetween the sequence and its time indexes. We applied our novel framework on\nDCASE 2018 task 1, ASC. For evaluation, we trained a Support Vector Machine\n(SVM) with the proposed Multi-Layered Temporal Pooling (MLTP) learned features.\nExperimental results on the development dataset, usage of the MLTP features\nsignificantly improved the ASC performance. The best performance with 75.28%\naccuracy was achieved by using the optimal setting found in our experiments.", "published": "2019-02-26 17:15:13", "link": "http://arxiv.org/abs/1902.10063v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Directional Embedding Based Semi-supervised Framework For Bird\n  Vocalization Segmentation", "abstract": "This paper proposes a data-efficient, semi-supervised, two-pass framework for\nsegmenting bird vocalizations. The framework utilizes a binary classification\nmodel to categorize frames of an input audio recording into the background or\nbird vocalization. The first pass of the framework automatically generates\ntraining labels from the input recording itself, while model training and\nclassification is done during the second pass. The proposed framework utilizes\na reference directional model for obtaining a feature representation called\ndirectional embeddings (DE). This reference directional model acts as an\nacoustic model for bird vocalizations and is obtained using the mixtures of\nVon-Mises Fisher distribution (moVMF). The proposed DE space only contains\ninformation about bird vocalizations, while no information about the background\ndisturbances is reflected. The framework employs supervised information only\nfor obtaining the reference directional model and avoids the background\nmodeling. Hence, it can be regarded as semi-supervised in nature. The proposed\nframework is tested on approximately 79000 vocalizations of seven different\nbird species. The performance of the framework is also analyzed in the presence\nof noise at different SNRs. Experimental results convey that the proposed\nframework performs better than the existing bird vocalization segmentation\nmethods.", "published": "2019-02-26 07:08:31", "link": "http://arxiv.org/abs/1902.09765v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Utterance-level Aggregation For Speaker Recognition In The Wild", "abstract": "The objective of this paper is speaker recognition \"in the wild\"-where\nutterances may be of variable length and also contain irrelevant signals.\nCrucial elements in the design of deep networks for this task are the type of\ntrunk (frame level) network, and the method of temporal aggregation. We propose\na powerful speaker recognition deep network, using a \"thin-ResNet\" trunk\narchitecture, and a dictionary-based NetVLAD or GhostVLAD layer to aggregate\nfeatures across time, that can be trained end-to-end. We show that our network\nachieves state of the art performance by a significant margin on the VoxCeleb1\ntest set for speaker recognition, whilst requiring fewer parameters than\nprevious methods. We also investigate the effect of utterance length on\nperformance, and conclude that for \"in the wild\" data, a longer length is\nbeneficial.", "published": "2019-02-26 18:34:05", "link": "http://arxiv.org/abs/1902.10107v2", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
