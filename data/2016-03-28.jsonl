{"title": "Audio Visual Emotion Recognition with Temporal Alignment and Perception\n  Attention", "abstract": "This paper focuses on two key problems for audio-visual emotion recognition\nin the video. One is the audio and visual streams temporal alignment for\nfeature level fusion. The other one is locating and re-weighting the perception\nattentions in the whole audio-visual stream for better recognition. The Long\nShort Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the main\nclassification architecture. Firstly, soft attention mechanism aligns the audio\nand visual streams. Secondly, seven emotion embedding vectors, which are\ncorresponding to each classification emotion type, are added to locate the\nperception attentions. The locating and re-weighting process is also based on\nthe soft attention mechanism. The experiment results on EmotiW2015 dataset and\nthe qualitative analysis show the efficiency of the proposed two techniques.", "published": "2016-03-28 06:06:10", "link": "http://arxiv.org/abs/1603.08321v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Longitudinal Analysis of Discussion Topics in an Online Breast Cancer\n  Community using Convolutional Neural Networks", "abstract": "Identifying topics of discussions in online health communities (OHC) is\ncritical to various applications, but can be difficult because topics of OHC\ncontent are usually heterogeneous and domain-dependent. In this paper, we\nprovide a multi-class schema, an annotated dataset, and supervised classifiers\nbased on convolutional neural network (CNN) and other models for the task of\nclassifying discussion topics. We apply the CNN classifier to the most popular\nbreast cancer online community, and carry out a longitudinal analysis to show\ntopic distributions and topic changes throughout members' participation. Our\nexperimental results suggest that CNN outperforms other classifiers in the task\nof topic classification, and that certain trajectories can be detected with\nrespect to topic changes.", "published": "2016-03-28 17:47:42", "link": "http://arxiv.org/abs/1603.08458v3", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Deep Embedding for Spatial Role Labeling", "abstract": "This paper introduces the visually informed embedding of word (VIEW), a\ncontinuous vector representation for a word extracted from a deep neural model\ntrained using the Microsoft COCO data set to forecast the spatial arrangements\nbetween visual objects, given a textual description. The model is composed of a\ndeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory\n(LSTM) network, the latter being preceded by an embedding layer. The VIEW is\napplied to transferring multimodal background knowledge to Spatial Role\nLabeling (SpRL) algorithms, which recognize spatial relations between objects\nmentioned in the text. This work also contributes with a new method to select\ncomplementary features and a fine-tuning method for MLP that improves the $F1$\nmeasure in classifying the words into spatial roles. The VIEW is evaluated with\nthe Task 3 of SemEval-2013 benchmark data set, SpaceEval.", "published": "2016-03-28 18:38:46", "link": "http://arxiv.org/abs/1603.08474v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Generating Visual Explanations", "abstract": "Clearly explaining a rationale for a classification decision to an end-user\ncan be as important as the decision itself. Existing approaches for deep visual\nrecognition are generally opaque and do not output any justification text;\ncontemporary vision-language models can describe image content but fail to take\ninto account class-discriminative image aspects which justify visual\npredictions. We propose a new model that focuses on the discriminating\nproperties of the visible object, jointly predicts a class label, and explains\nwhy the predicted label is appropriate for the image. We propose a novel loss\nfunction based on sampling and reinforcement learning that learns to generate\nsentences that realize a global sentence property, such as class specificity.\nOur results on a fine-grained bird species classification dataset show that our\nmodel is able to generate explanations which are not only consistent with an\nimage but also more discriminative than descriptions produced by existing\ncaptioning methods.", "published": "2016-03-28 19:54:12", "link": "http://arxiv.org/abs/1603.08507v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
