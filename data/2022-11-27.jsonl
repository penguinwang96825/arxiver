{"title": "Understanding BLOOM: An empirical study on diverse NLP tasks", "abstract": "We view the landscape of large language models (LLMs) through the lens of the\nrecently released BLOOM model to understand the performance of BLOOM and other\ndecoder-only LLMs compared to BERT-style encoder-only models. We achieve this\nby evaluating the smaller BLOOM model variants (\\textit{350m/560m} and\n\\textit{1b3/1b7}) on several NLP benchmark datasets and popular leaderboards.\nWe make the following observations: (1) BLOOM performance does not scale with\nparameter size, unlike other LLMs like GPT and BERT. Experiments fine-tuning\nBLOOM models show that the 560m variant performs similarly to or better than\nthe 1b7 variant, (2) Zero-shot cross-lingual and multi-lingual fine-tuning\nexperiments show that BLOOM is at par or worse than monolingual GPT-2 models,\nand (3) Toxicity analysis of prompt-based text generation using the\nRealToxicityPrompts dataset shows that the text generated by BLOOM is at least\n17\\% less toxic than GPT-2 and GPT-3 models.", "published": "2022-11-27 15:48:14", "link": "http://arxiv.org/abs/2211.14865v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A novel multimodal dynamic fusion network for disfluency detection in\n  spoken utterances", "abstract": "Disfluency, though originating from human spoken utterances, is primarily\nstudied as a uni-modal text-based Natural Language Processing (NLP) task. Based\non early-fusion and self-attention-based multimodal interaction between text\nand acoustic modalities, in this paper, we propose a novel multimodal\narchitecture for disfluency detection from individual utterances. Our\narchitecture leverages a multimodal dynamic fusion network that adds minimal\nparameters over an existing text encoder commonly used in prior art to leverage\nthe prosodic and acoustic cues hidden in speech. Through experiments, we show\nthat our proposed model achieves state-of-the-art results on the widely used\nEnglish Switchboard for disfluency detection and outperforms prior unimodal and\nmultimodal systems in literature by a significant margin. In addition, we make\na thorough qualitative analysis and show that, unlike text-only systems, which\nsuffer from spurious correlations in the data, our system overcomes this\nproblem through additional cues from speech signals. We make all our codes\npublicly available on GitHub.", "published": "2022-11-27 01:54:22", "link": "http://arxiv.org/abs/2211.14700v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BadPrompt: Backdoor Attacks on Continuous Prompts", "abstract": "The prompt-based learning paradigm has gained much research attention\nrecently. It has achieved state-of-the-art performance on several NLP tasks,\nespecially in the few-shot scenarios. While steering the downstream tasks, few\nworks have been reported to investigate the security problems of the\nprompt-based models. In this paper, we conduct the first study on the\nvulnerability of the continuous prompt learning algorithm to backdoor attacks.\nWe observe that the few-shot scenarios have posed a great challenge to backdoor\nattacks on the prompt-based models, limiting the usability of existing NLP\nbackdoor methods. To address this challenge, we propose BadPrompt, a\nlightweight and task-adaptive algorithm, to backdoor attack continuous prompts.\nSpecially, BadPrompt first generates candidate triggers which are indicative\nfor predicting the targeted label and dissimilar to the samples of the\nnon-targeted labels. Then, it automatically selects the most effective and\ninvisible trigger for each sample with an adaptive trigger optimization\nalgorithm. We evaluate the performance of BadPrompt on five datasets and two\ncontinuous prompt models. The results exhibit the abilities of BadPrompt to\neffectively attack continuous prompts while maintaining high performance on the\nclean test sets, outperforming the baseline models by a large margin. The\nsource code of BadPrompt is publicly available at\nhttps://github.com/papersPapers/BadPrompt.", "published": "2022-11-27 04:23:18", "link": "http://arxiv.org/abs/2211.14719v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "X-PuDu at SemEval-2022 Task 7: A Replaced Token Detection Task\n  Pre-trained Model with Pattern-aware Ensembling for Identifying Plausible\n  Clarifications", "abstract": "This paper describes our winning system on SemEval 2022 Task 7: Identifying\nPlausible Clarifications of Implicit and Underspecified Phrases in\nInstructional Texts. A replaced token detection pre-trained model is utilized\nwith minorly different task-specific heads for SubTask-A: Multi-class\nClassification and SubTask-B: Ranking. Incorporating a pattern-aware ensemble\nmethod, our system achieves a 68.90% accuracy score and 0.8070 spearman's rank\ncorrelation score surpassing the 2nd place with a large margin by 2.7 and 2.2\npercent points for SubTask-A and SubTask-B, respectively. Our approach is\nsimple and easy to implement, and we conducted ablation studies and qualitative\nand quantitative analyses for the working strategies used in our system.", "published": "2022-11-27 05:46:46", "link": "http://arxiv.org/abs/2211.14734v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Alignment-Enriched Tuning for Patch-Level Pre-trained Document Image\n  Models", "abstract": "Alignment between image and text has shown promising improvements on\npatch-level pre-trained document image models. However, investigating more\neffective or finer-grained alignment techniques during pre-training requires a\nlarge amount of computation cost and time. Thus, a question naturally arises:\nCould we fine-tune the pre-trained models adaptive to downstream tasks with\nalignment objectives and achieve comparable or better performance? In this\npaper, we propose a new model architecture with alignment-enriched tuning\n(dubbed AETNet) upon pre-trained document image models, to adapt downstream\ntasks with the joint task-specific supervised and alignment-aware contrastive\nobjective. Specifically, we introduce an extra visual transformer as the\nalignment-ware image encoder and an extra text transformer as the\nalignment-ware text encoder before multimodal fusion. We consider alignment in\nthe following three aspects: 1) document-level alignment by leveraging the\ncross-modal and intra-modal contrastive loss; 2) global-local alignment for\nmodeling localized and structural information in document images; and 3)\nlocal-level alignment for more accurate patch-level information. Experiments on\nvarious downstream tasks show that AETNet can achieve state-of-the-art\nperformance on various downstream tasks. Notably, AETNet consistently\noutperforms state-of-the-art pre-trained models, such as LayoutLMv3 with\nfine-tuning techniques, on three different downstream tasks.", "published": "2022-11-27 09:55:15", "link": "http://arxiv.org/abs/2211.14777v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ESIE-BERT: Enriching Sub-words Information Explicitly with BERT for\n  Joint Intent Classification and SlotFilling", "abstract": "Natural language understanding (NLU) has two core tasks: intent\nclassification and slot filling. The success of pre-training language models\nresulted in a significant breakthrough in the two tasks. One of the promising\nsolutions called BERT can jointly optimize the two tasks. We note that\nBERT-based models convert each complex token into multiple sub-tokens by\nwordpiece algorithm, which generates a mismatch between the lengths of the\ntokens and the labels. This leads to BERT-based models do not do well in label\nprediction which limits model performance improvement. Many existing models can\nbe compatible with this issue but some hidden semantic information is discarded\nin the fine-tuning process. We address the problem by introducing a novel joint\nmethod on top of BERT which explicitly models the multiple sub-tokens features\nafter wordpiece tokenization, thereby contributing to the two tasks. Our method\ncan well extract the contextual features from complex tokens by the proposed\nsub-words attention adapter (SAA), which preserves overall utterance\ninformation. Additionally, we propose an intent attention adapter (IAA) to\nobtain the full sentence features to aid users to predict intent. Experimental\nresults confirm that our proposed model is significantly improved on two public\nbenchmark datasets. In particular, the slot filling F1 score is improved from\n96.1 to 98.2 (2.1% absolute) on the Airline Travel Information Systems (ATIS)\ndataset.", "published": "2022-11-27 13:49:19", "link": "http://arxiv.org/abs/2211.14829v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PUnifiedNER: A Prompting-based Unified NER System for Diverse Datasets", "abstract": "Much of named entity recognition (NER) research focuses on developing\ndataset-specific models based on data from the domain of interest, and a\nlimited set of related entity types. This is frustrating as each new dataset\nrequires a new model to be trained and stored. In this work, we present a\n``versatile'' model -- the Prompting-based Unified NER system (PUnifiedNER) --\nthat works with data from different domains and can recognise up to 37 entity\ntypes simultaneously, and theoretically it could be as many as possible. By\nusing prompt learning, PUnifiedNER is a novel approach that is able to jointly\ntrain across multiple corpora, implementing intelligent on-demand entity\nrecognition. Experimental results show that PUnifiedNER leads to significant\nprediction benefits compared to dataset-specific models with impressively\nreduced model deployment costs. Furthermore, the performance of PUnifiedNER can\nachieve competitive or even better performance than state-of-the-art\ndomain-specific methods for some datasets. We also perform comprehensive pilot\nand ablation studies to support in-depth analysis of each component in\nPUnifiedNER.", "published": "2022-11-27 14:25:48", "link": "http://arxiv.org/abs/2211.14838v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detect-Localize-Repair: A Unified Framework for Learning to Debug with\n  CodeT5", "abstract": "Automated software debugging is a crucial task for improving the productivity\nof software developers. Many neural-based techniques have been proven effective\nfor debugging-related tasks such as bug localization and program repair (or bug\nfixing). However, these techniques often focus only on either one of them or\napproach them in a stage-wise manner, ignoring the mutual benefits between\nthem. In this work, we propose a novel unified \\emph{Detect-Localize-Repair}\nframework based on a pretrained programming language model CodeT5 to seamlessly\naddress these tasks, named CodeT5-DLR. Specifically, we propose three\nobjectives to adapt the generic CodeT5 for debugging: a bug detection objective\nto determine whether a given code snippet is buggy or not, a bug localization\nobjective to identify the buggy lines, and a program repair objective to\ntranslate the buggy code to its fixed version. We evaluate it on each of these\ntasks and their combined setting on two newly collected line-level debugging\ndatasets in Java and Python. Extensive results show that our model\nsignificantly outperforms existing baselines from both NLP and software\nengineering domains.", "published": "2022-11-27 16:11:29", "link": "http://arxiv.org/abs/2211.14875v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Combining Data Generation and Active Learning for Low-Resource Question\n  Answering", "abstract": "Neural approaches have become very popular in Question Answering (QA),\nhowever, they require a large amount of annotated data. In this work, we\npropose a novel approach that combines data augmentation via question-answer\ngeneration with Active Learning to improve performance in low-resource\nsettings, where the target domains are diverse in terms of difficulty and\nsimilarity to the source domain. We also investigate Active Learning for\nquestion answering in different stages, overall reducing the annotation effort\nof humans. For this purpose, we consider target domains in realistic settings,\nwith an extremely low amount of annotated samples but with many unlabeled\ndocuments, which we assume can be obtained with little effort. Additionally, we\nassume a sufficient amount of labeled data from the source domain being\navailable. We perform extensive experiments to find the best setup for\nincorporating domain experts. Our findings show that our novel approach, where\nhumans are incorporated in a data generation approach, boosts performance in\nthe low-resource, domain-specific setting, allowing for low-labeling-effort\nquestion answering systems in new, specialized domains. They further\ndemonstrate how human annotation affects the performance of QA depending on the\nstage it is performed.", "published": "2022-11-27 16:31:33", "link": "http://arxiv.org/abs/2211.14880v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "EPIK: Eliminating multi-model Pipelines with Knowledge-distillation", "abstract": "Real-world tasks are largely composed of multiple models, each performing a\nsub-task in a larger chain of tasks, i.e., using the output from a model as\ninput for another model in a multi-model pipeline. A model like MATRa performs\nthe task of Crosslingual Transliteration in two stages, using English as an\nintermediate transliteration target when transliterating between two indic\nlanguages. We propose a novel distillation technique, EPIK, that condenses\ntwo-stage pipelines for hierarchical tasks into a single end-to-end model\nwithout compromising performance. This method can create end-to-end models for\ntasks without needing a dedicated end-to-end dataset, solving the data scarcity\nproblem. The EPIK model has been distilled from the MATra model using this\ntechnique of knowledge distillation. The MATra model can perform crosslingual\ntransliteration between 5 languages - English, Hindi, Tamil, Kannada and\nBengali. The EPIK model executes the task of transliteration without any\nintermediate English output while retaining the performance and accuracy of the\nMATra model. The EPIK model can perform transliteration with an average CER\nscore of 0.015 and average phonetic accuracy of 92.1%. In addition, the average\ntime for execution has reduced by 54.3% as compared to the teacher model and\nhas a similarity score of 97.5% with the teacher encoder. In a few cases, the\nEPIK model (student model) can outperform the MATra model (teacher model) even\nthough it has been distilled from the MATra model.", "published": "2022-11-27 19:36:44", "link": "http://arxiv.org/abs/2211.14920v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Opinion Summarisation in the Wasserstein Space", "abstract": "Opinion summarisation synthesises opinions expressed in a group of documents\ndiscussing the same topic to produce a single summary. Recent work has looked\nat opinion summarisation of clusters of social media posts. Such posts are\nnoisy and have unpredictable structure, posing additional challenges for the\nconstruction of the summary distribution and the preservation of meaning\ncompared to online reviews, which has been so far the focus of opinion\nsummarisation. To address these challenges we present \\textit{WassOS}, an\nunsupervised abstractive summarization model which makes use of the Wasserstein\ndistance. A Variational Autoencoder is used to get the distribution of\ndocuments/posts, and the distributions are disentangled into separate semantic\nand syntactic spaces. The summary distribution is obtained using the\nWasserstein barycenter of the semantic and syntactic distributions. A latent\nvariable sampled from the summary distribution is fed into a GRU decoder with a\ntransformer layer to produce the final summary. Our experiments on multiple\ndatasets including Twitter clusters, Reddit threads, and reviews show that\nWassOS almost always outperforms the state-of-the-art on ROUGE metrics and\nconsistently produces the best summaries with respect to meaning preservation\naccording to human evaluations.", "published": "2022-11-27 19:45:38", "link": "http://arxiv.org/abs/2211.14923v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topic Segmentation in the Wild: Towards Segmentation of Semi-structured\n  & Unstructured Chats", "abstract": "Breaking down a document or a conversation into multiple contiguous segments\nbased on its semantic structure is an important and challenging problem in NLP,\nwhich can assist many downstream tasks. However, current works on topic\nsegmentation often focus on segmentation of structured texts. In this paper, we\ncomprehensively analyze the generalization capabilities of state-of-the-art\ntopic segmentation models on unstructured texts. We find that: (a) Current\nstrategies of pre-training on a large corpus of structured text such as\nWiki-727K do not help in transferability to unstructured texts. (b) Training\nfrom scratch with only a relatively small-sized dataset of the target\nunstructured domain improves the segmentation results by a significant margin.", "published": "2022-11-27 22:17:16", "link": "http://arxiv.org/abs/2211.14954v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MNER-QG: An End-to-End MRC framework for Multimodal Named Entity\n  Recognition with Query Grounding", "abstract": "Multimodal named entity recognition (MNER) is a critical step in information\nextraction, which aims to detect entity spans and classify them to\ncorresponding entity types given a sentence-image pair. Existing methods either\n(1) obtain named entities with coarse-grained visual clues from attention\nmechanisms, or (2) first detect fine-grained visual regions with toolkits and\nthen recognize named entities. However, they suffer from improper alignment\nbetween entity types and visual regions or error propagation in the two-stage\nmanner, which finally imports irrelevant visual information into texts. In this\npaper, we propose a novel end-to-end framework named MNER-QG that can\nsimultaneously perform MRC-based multimodal named entity recognition and query\ngrounding. Specifically, with the assistance of queries, MNER-QG can provide\nprior knowledge of entity types and visual regions, and further enhance\nrepresentations of both texts and images. To conduct the query grounding task,\nwe provide manual annotations and weak supervisions that are obtained via\ntraining a highly flexible visual grounding model with transfer learning. We\nconduct extensive experiments on two public MNER datasets, Twitter2015 and\nTwitter2017. Experimental results show that MNER-QG outperforms the current\nstate-of-the-art models on the MNER task, and also improves the query grounding\nperformance.", "published": "2022-11-27 06:10:03", "link": "http://arxiv.org/abs/2211.14739v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Navigation as Attackers Wish? Towards Building Robust Embodied Agents\n  under Federated Learning", "abstract": "Federated embodied agent learning protects the data privacy of individual\nvisual environments by keeping data locally at each client (the individual\nenvironment) during training. However, since the local data is inaccessible to\nthe server under federated learning, attackers may easily poison the training\ndata of the local client to build a backdoor in the agent without notice.\nDeploying such an agent raises the risk of potential harm to humans, as the\nattackers may easily navigate and control the agent as they wish via the\nbackdoor. Towards Byzantine-robust federated embodied agent learning, in this\npaper, we study the attack and defense for the task of vision-and-language\nnavigation (VLN), where the agent is required to follow natural language\ninstructions to navigate indoor environments. First, we introduce a simple but\neffective attack strategy, Navigation as Wish (NAW), in which the malicious\nclient manipulates local trajectory data to implant a backdoor into the global\nmodel. Results on two VLN datasets (R2R and RxR) show that NAW can easily\nnavigate the deployed VLN agent regardless of the language instruction, without\naffecting its performance on normal test sets. Then, we propose a new\nPrompt-Based Aggregation (PBA) to defend against the NAW attack in federated\nVLN, which provides the server with a ''prompt'' of the vision-and-language\nalignment variance between the benign and malicious clients so that they can be\ndistinguished during training. We validate the effectiveness of the PBA method\non protecting the global model from the NAW attack, which outperforms other\nstate-of-the-art defense methods by a large margin in the defense metrics on\nR2R and RxR.", "published": "2022-11-27 09:01:31", "link": "http://arxiv.org/abs/2211.14769v4", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Multi-Modal Few-Shot Temporal Action Detection", "abstract": "Few-shot (FS) and zero-shot (ZS) learning are two different approaches for\nscaling temporal action detection (TAD) to new classes. The former adapts a\npretrained vision model to a new task represented by as few as a single video\nper class, whilst the latter requires no training examples by exploiting a\nsemantic description of the new class. In this work, we introduce a new\nmulti-modality few-shot (MMFS) TAD problem, which can be considered as a\nmarriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new\nclass names jointly. To tackle this problem, we further introduce a novel\nMUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by\nefficiently bridging pretrained vision and language models whilst maximally\nreusing already learned capacity. Concretely, we construct multi-modal prompts\nby mapping support videos into the textual token space of a vision-language\nmodel using a meta-learned adapter-equipped visual semantics tokenizer. To\ntackle large intra-class variation, we further design a query feature\nregulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14\ndemonstrate that our MUPPET outperforms state-of-the-art alternative methods,\noften by a large margin. We also show that our MUPPET can be easily extended to\ntackle the few-shot object detection problem and again achieves the\nstate-of-the-art performance on MS-COCO dataset. The code will be available in\nhttps://github.com/sauradip/MUPPET", "published": "2022-11-27 18:13:05", "link": "http://arxiv.org/abs/2211.14905v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
