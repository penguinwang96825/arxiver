{"title": "First order linear logic and tensor type calculus for categorial\n  grammars", "abstract": "We study relationship between first order multiplicative linear logic (MLL1),\nwhich has been known to provide representations to different categorial\ngrammars, and the recently introduced extended tensor type calculus (ETTC). We\nidentify a fragment of MLL1, which seems sufficient for many grammar\nrepresentations, and establish a correspondence between ETTC and this fragment.\nThe system ETTC, thus, can be seen as an alternative syntax and intrinsic\ndeductive system together with a geometric representation for the latter. We\nalso give a natural deduction formulation of ETTC, which might be convenient.", "published": "2021-12-31 00:35:48", "link": "http://arxiv.org/abs/2112.15253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clustering Vietnamese Conversations From Facebook Page To Build Training\n  Dataset For Chatbot", "abstract": "The biggest challenge of building chatbots is training data. The required\ndata must be realistic and large enough to train chatbots. We create a tool to\nget actual training data from Facebook messenger of a Facebook page. After text\npreprocessing steps, the newly obtained dataset generates FVnC and Sample\ndataset. We use the Retraining of BERT for Vietnamese (PhoBERT) to extract\nfeatures of our text data. K-Means and DBSCAN clustering algorithms are used\nfor clustering tasks based on output embeddings from PhoBERT$_{base}$. We apply\nV-measure score and Silhouette score to evaluate the performance of clustering\nalgorithms. We also demonstrate the efficiency of PhoBERT compared to other\nmodels in feature extraction on the Sample dataset and wiki dataset. A\nGridSearch algorithm that combines both clustering evaluations is also proposed\nto find optimal parameters. Thanks to clustering such a number of\nconversations, we save a lot of time and effort to build data and storylines\nfor training chatbot.", "published": "2021-12-31 07:56:12", "link": "http://arxiv.org/abs/2112.15338v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenQA: Hybrid QA System Relying on Structured Knowledge Base as well as\n  Non-structured Data", "abstract": "Search engines based on keyword retrieval can no longer adapt to the way of\ninformation acquisition in the era of intelligent Internet of Things due to the\nreturn of keyword related Internet pages. How to quickly, accurately and\neffectively obtain the information needed by users from massive Internet data\nhas become one of the key issues urgently needed to be solved. We propose an\nintelligent question-answering system based on structured KB and unstructured\ndata, called OpenQA, in which users can give query questions and the model can\nquickly give accurate answers back to users. We integrate KBQA structured\nquestion answering based on semantic parsing and deep representation learning,\nand two-stage unstructured question answering based on retrieval and neural\nmachine reading comprehension into OpenQA, and return the final answer with the\nhighest probability through the Transformer answer selection module in OpenQA.\nWe carry out preliminary experiments on our constructed dataset, and the\nexperimental results prove the effectiveness of the proposed intelligent\nquestion answering system. At the same time, the core technology of each module\nof OpenQA platform is still in the forefront of academic hot spots, and the\ntheoretical essence and enrichment of OpenQA will be further explored based on\nthese academic hot spots.", "published": "2021-12-31 09:15:39", "link": "http://arxiv.org/abs/2112.15356v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hypers at ComMA@ICON: Modelling Aggressiveness, Gender Bias and Communal\n  Bias Identification", "abstract": "Due to the exponentially increasing reach of social media, it is essential to\nfocus on its negative aspects as it can potentially divide society and incite\npeople into violence. In this paper, we present our system description of work\non the shared task ComMA@ICON, where we have to classify how aggressive the\nsentence is and if the sentence is gender-biased or communal biased. These\nthree could be the primary reasons to cause significant problems in society. As\nteam Hypers we have proposed an approach that utilizes different pretrained\nmodels with Attention and mean pooling methods. We were able to get Rank 3 with\n0.223 Instance F1 score on Bengali, Rank 2 with 0.322 Instance F1 score on\nMulti-lingual set, Rank 4 with 0.129 Instance F1 score on Meitei and Rank 5\nwith 0.336 Instance F1 score on Hindi. The source code and the pretrained\nmodels of this work can be found here.", "published": "2021-12-31 12:50:38", "link": "http://arxiv.org/abs/2112.15417v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How do lexical semantics affect translation? An empirical study", "abstract": "Neural machine translation (NMT) systems aim to map text from one language\ninto another. While there are a wide variety of applications of NMT, one of the\nmost important is translation of natural language. A distinguishing factor of\nnatural language is that words are typically ordered according to the rules of\nthe grammar of a given language. Although many advances have been made in\ndeveloping NMT systems for translating natural language, little research has\nbeen done on understanding how the word ordering of and lexical similarity\nbetween the source and target language affect translation performance. Here, we\ninvestigate these relationships on a variety of low-resource language pairs\nfrom the OpenSubtitles2016 database, where the source language is English, and\nfind that the more similar the target language is to English, the greater the\ntranslation performance. In addition, we study the impact of providing NMT\nmodels with part of speech of words (POS) in the English sequence and find\nthat, for Transformer-based models, the more dissimilar the target language is\nfrom English, the greater the benefit provided by POS.", "published": "2021-12-31 23:28:28", "link": "http://arxiv.org/abs/2201.00075v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Deep Learning Approach to Integrate Human-Level Understanding in a\n  Chatbot", "abstract": "In recent times, a large number of people have been involved in establishing\ntheir own businesses. Unlike humans, chatbots can serve multiple customers at a\ntime, are available 24/7 and reply in less than a fraction of a second. Though\nchatbots perform well in task-oriented activities, in most cases they fail to\nunderstand personalized opinions, statements or even queries which later impact\nthe organization for poor service management. Lack of understanding\ncapabilities in bots disinterest humans to continue conversations with them.\nUsually, chatbots give absurd responses when they are unable to interpret a\nuser's text accurately. Extracting the client reviews from conversations by\nusing chatbots, organizations can reduce the major gap of understanding between\nthe users and the chatbot and improve their quality of products and\nservices.Thus, in our research we incorporated all the key elements that are\nnecessary for a chatbot to analyse and understand an input text precisely and\naccurately. We performed sentiment analysis, emotion detection, intent\nclassification and named-entity recognition using deep learning to develop\nchatbots with humanistic understanding and intelligence. The efficiency of our\napproach can be demonstrated accordingly by the detailed analysis.", "published": "2021-12-31 22:26:41", "link": "http://arxiv.org/abs/2201.02735v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ViNMT: Neural Machine Translation Toolkit", "abstract": "We present an open-source toolkit for neural machine translation (NMT). The\nnew toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along\nwith many other improvements detailed below, in order to create a\nself-contained, simple to use, consistent and comprehensive framework for\nMachine Translation tasks of various domains. It is tooled to support both\nbilingual and multilingual translation tasks, starting from building the model\nfrom respective corpora, to inferring new predictions or packaging the model to\nserving-capable JIT format.", "published": "2021-12-31 02:42:39", "link": "http://arxiv.org/abs/2112.15272v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ERNIE-ViLG: Unified Generative Pre-training for Bidirectional\n  Vision-Language Generation", "abstract": "Conventional methods for the image-text generation tasks mainly tackle the\nnaturally bidirectional generation tasks separately, focusing on designing\ntask-specific frameworks to improve the quality and fidelity of the generated\nsamples. Recently, Vision-Language Pre-training models have greatly improved\nthe performance of the image-to-text generation tasks, but large-scale\npre-training models for text-to-image synthesis task are still under-developed.\nIn this paper, we propose ERNIE-ViLG, a unified generative pre-training\nframework for bidirectional image-text generation with transformer model. Based\non the image quantization models, we formulate both image generation and text\ngeneration as autoregressive generative tasks conditioned on the text/image\ninput. The bidirectional image-text generative modeling eases the semantic\nalignments across vision and language. For the text-to-image generation\nprocess, we further propose an end-to-end training method to jointly learn the\nvisual sequence generator and the image reconstructor. To explore the landscape\nof large-scale pre-training for bidirectional text-image generation, we train a\n10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million\n(Chinese) image-text pairs which achieves state-of-the-art performance for both\ntext-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for\ntext-to-image synthesis and best results on COCO-CN and AIC-ICC for image\ncaptioning.", "published": "2021-12-31 03:53:33", "link": "http://arxiv.org/abs/2112.15283v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Domain Adaptation with Category Attention Network for Deep Sentiment\n  Analysis", "abstract": "Domain adaptation tasks such as cross-domain sentiment classification aim to\nutilize existing labeled data in the source domain and unlabeled or few labeled\ndata in the target domain to improve the performance in the target domain via\nreducing the shift between the data distributions. Existing cross-domain\nsentiment classification methods need to distinguish pivots, i.e., the\ndomain-shared sentiment words, and non-pivots, i.e., the domain-specific\nsentiment words, for excellent adaptation performance. In this paper, we first\ndesign a Category Attention Network (CAN), and then propose a model named\nCAN-CNN to integrate CAN and a Convolutional Neural Network (CNN). On the one\nhand, the model regards pivots and non-pivots as unified category attribute\nwords and can automatically capture them to improve the domain adaptation\nperformance; on the other hand, the model makes an attempt at interpretability\nto learn the transferred category attribute words. Specifically, the\noptimization objective of our model has three different components: 1) the\nsupervised classification loss; 2) the distributions loss of category feature\nweights; 3) the domain invariance loss. Finally, the proposed model is\nevaluated on three public sentiment analysis datasets and the results\ndemonstrate that CAN-CNN can outperform other various baseline methods.", "published": "2021-12-31 04:03:48", "link": "http://arxiv.org/abs/2112.15290v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deconfounded Visual Grounding", "abstract": "We focus on the confounding bias between language and location in the visual\ngrounding pipeline, where we find that the bias is the major visual reasoning\nbottleneck. For example, the grounding process is usually a trivial\nlanguage-location association without visual reasoning, e.g., grounding any\nlanguage query containing sheep to the nearly central regions, due to that most\nqueries about sheep have ground-truth locations at the image center. First, we\nframe the visual grounding pipeline into a causal graph, which shows the\ncausalities among image, query, target location and underlying confounder.\nThrough the causal graph, we know how to break the grounding bottleneck:\ndeconfounded visual grounding. Second, to tackle the challenge that the\nconfounder is unobserved in general, we propose a confounder-agnostic approach\ncalled: Referring Expression Deconfounder (RED), to remove the confounding\nbias. Third, we implement RED as a simple language attention, which can be\napplied in any grounding method. On popular benchmarks, RED improves various\nstate-of-the-art grounding methods by a significant margin. Code will soon be\navailable at: https://github.com/JianqiangH/Deconfounded_VG.", "published": "2021-12-31 07:14:59", "link": "http://arxiv.org/abs/2112.15324v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Using Graph-Aware Reinforcement Learning to Identify Winning Strategies\n  in Diplomacy Games (Student Abstract)", "abstract": "This abstract proposes an approach towards goal-oriented modeling of the\ndetection and modeling complex social phenomena in multiparty discourse in an\nonline political strategy game. We developed a two-tier approach that first\nencodes sociolinguistic behavior as linguistic features then use reinforcement\nlearning to estimate the advantage afforded to any player. In the first tier,\nsociolinguistic behavior, such as Friendship and Reasoning, that speakers use\nto influence others are encoded as linguistic features to identify the\npersuasive strategies applied by each player in simultaneous two-party\ndialogues. In the second tier, a reinforcement learning approach is used to\nestimate a graph-aware reward function to quantify the advantage afforded to\neach player based on their standing in this multiparty setup. We apply this\ntechnique to the game Diplomacy, using a dataset comprising of over 15,000\nmessages exchanged between 78 users. Our graph-aware approach shows robust\nperformance compared to a context-agnostic setup.", "published": "2021-12-31 07:38:36", "link": "http://arxiv.org/abs/2112.15331v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Training and Generating Neural Networks in Compressed Weight Space", "abstract": "The inputs and/or outputs of some neural nets are weight matrices of other\nneural nets. Indirect encodings or end-to-end compression of weight matrices\ncould help to scale such approaches. Our goal is to open a discussion on this\ntopic, starting with recurrent neural networks for character-level language\nmodelling whose weight matrices are encoded by the discrete cosine transform.\nOur fast weight version thereof uses a recurrent neural network to parameterise\nthe compressed weights. We present experimental results on the enwik8 dataset.", "published": "2021-12-31 16:50:31", "link": "http://arxiv.org/abs/2112.15545v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "What is Event Knowledge Graph: A Survey", "abstract": "Besides entity-centric knowledge, usually organized as Knowledge Graph (KG),\nevents are also an essential kind of knowledge in the world, which trigger the\nspring up of event-centric knowledge representation form like Event KG (EKG).\nIt plays an increasingly important role in many downstream applications, such\nas search, question-answering, recommendation, financial quantitative\ninvestments, and text generation. This paper provides a comprehensive survey of\nEKG from history, ontology, instance, and application views. Specifically, to\ncharacterize EKG thoroughly, we focus on its history, definition, schema\ninduction, acquisition, related representative graphs/systems, and\napplications. The development processes and trends are studied therein. We\nfurther summarize prospective directions to facilitate future research on EKG.", "published": "2021-12-31 03:42:55", "link": "http://arxiv.org/abs/2112.15280v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating Deep Music Generation Methods Using Data Augmentation", "abstract": "Despite advances in deep algorithmic music generation, evaluation of\ngenerated samples often relies on human evaluation, which is subjective and\ncostly. We focus on designing a homogeneous, objective framework for evaluating\nsamples of algorithmically generated music. Any engineered measures to evaluate\ngenerated music typically attempt to define the samples' musicality, but do not\ncapture qualities of music such as theme or mood. We do not seek to assess the\nmusical merit of generated music, but instead explore whether generated samples\ncontain meaningful information pertaining to emotion or mood/theme. We achieve\nthis by measuring the change in predictive performance of a music mood/theme\nclassifier after augmenting its training data with generated samples. We\nanalyse music samples generated by three models -- SampleRNN, Jukebox, and DDSP\n-- and employ a homogeneous framework across all methods to allow for objective\ncomparison. This is the first attempt at augmenting a music genre\nclassification dataset with conditionally generated music. We investigate the\nclassification performance improvement using deep music generation and the\nability of the generators to make emotional music by using an additional,\nemotion annotation of the dataset. Finally, we use a classifier trained on real\ndata to evaluate the label validity of class-conditionally generated samples.", "published": "2021-12-31 20:35:46", "link": "http://arxiv.org/abs/2201.00052v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
