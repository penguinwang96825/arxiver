{"title": "Text Level Graph Neural Network for Text Classification", "abstract": "Recently, researches have explored the graph neural network (GNN) techniques\non text classification, since GNN does well in handling complex structures and\npreserving global information. However, previous methods based on GNN are\nmainly faced with the practical problems of fixed corpus level graph structure\nwhich do not support online testing and high memory consumption. To tackle the\nproblems, we propose a new GNN based model that builds graphs for each input\ntext with global parameters sharing instead of a single graph for the whole\ncorpus. This method removes the burden of dependence between an individual text\nand entire corpus which support online testing, but still preserve global\ninformation. Besides, we build graphs by much smaller windows in the text,\nwhich not only extract more local features but also significantly reduce the\nedge numbers as well as memory consumption. Experiments show that our model\noutperforms existing models on several text classification datasets even with\nconsuming less memory.", "published": "2019-10-06 02:38:25", "link": "http://arxiv.org/abs/1910.02356v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition -- Is there a glass ceiling?", "abstract": "Recent developments in Named Entity Recognition (NER) have resulted in better\nand better models. However, is there a glass ceiling? Do we know which types of\nerrors are still hard or even impossible to correct? In this paper, we present\na detailed analysis of the types of errors in state-of-the-art machine learning\n(ML) methods. Our study reveals the weak and strong points of the Stanford,\nCMU, FLAIR, ELMO and BERT models, as well as their shared limitations. We also\nintroduce new techniques for improving annotation, for training processes and\nfor checking a model's quality and stability. Presented results are based on\nthe CoNLL 2003 data set for the English language. A new enriched semantic\nannotation of errors for this data set and new diagnostic data sets are\nattached in the supplementary materials.", "published": "2019-10-06 09:35:52", "link": "http://arxiv.org/abs/1910.02403v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Dialogue Generation with Shared-Private Memory", "abstract": "Existing dialog systems are all monolingual, where features shared among\ndifferent languages are rarely explored. In this paper, we introduce a novel\nmultilingual dialogue system. Specifically, we augment the sequence to sequence\nframework with improved shared-private memory. The shared memory learns common\nfeatures among different languages and facilitates a cross-lingual transfer to\nboost dialogue systems, while the private memory is owned by each separate\nlanguage to capture its unique feature. Experiments conducted on Chinese and\nEnglish conversation corpora of different scales show that our proposed\narchitecture outperforms the individually learned model with the help of the\nother language, where the improvement is particularly distinct when the\ntraining data is limited.", "published": "2019-10-06 04:02:55", "link": "http://arxiv.org/abs/1910.02365v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Design and Use of Loop-Transformation Pragmas", "abstract": "Adding a pragma directive into the source code is arguably easier than\nrewriting it, for instance for loop unrolling. Moreover, if the application is\nmaintained for multiple platforms, their difference in performance\ncharacteristics may require different code transformations. Code transformation\ndirectives allow replacing the directives depending on the platform, i.e.\nseparation of code semantics and its performance optimization.\n  In this paper, we explore the design space (syntax and semantics) of adding\nsuch directive into a future OpenMP specification. Using a prototype\nimplementation in Clang, we demonstrate the usefulness of such directives on a\nfew benchmarks.", "published": "2019-10-06 05:06:38", "link": "http://arxiv.org/abs/1910.02375v1", "categories": ["cs.PL", "cs.CL"], "primary_category": "cs.PL"}
{"title": "Measuring Sentences Similarity: A Survey", "abstract": "This study is to review the approaches used for measuring sentences\nsimilarity. Measuring similarity between natural language sentences is a\ncrucial task for many Natural Language Processing applications such as text\nclassification, information retrieval, question answering, and plagiarism\ndetection. This survey classifies approaches of calculating sentences\nsimilarity based on the adopted methodology into three categories. Word-to-word\nbased, structure based, and vector-based are the most widely used approaches to\nfind sentences similarity. Each approach measures relatedness between short\ntexts based on a specific perspective. In addition, datasets that are mostly\nused as benchmarks for evaluating techniques in this field are introduced to\nprovide a complete view on this issue. The approaches that combine more than\none perspective give better results. Moreover, structure based similarity that\nmeasures similarity between sentences structures needs more investigation.", "published": "2019-10-06 09:21:21", "link": "http://arxiv.org/abs/1910.03940v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Analysis of Propaganda in News Articles", "abstract": "Propaganda aims at influencing people's mindset with the purpose of advancing\na specific agenda. Previous work has addressed propaganda detection at the\ndocument level, typically labelling all articles from a propagandistic news\noutlet as propaganda. Such noisy gold labels inevitably affect the quality of\nany learning system trained on them. A further issue with most existing systems\nis the lack of explainability. To overcome these limitations, we propose a\nnovel task: performing fine-grained analysis of texts by detecting all\nfragments that contain propaganda techniques as well as their type. In\nparticular, we create a corpus of news articles manually annotated at the\nfragment level with eighteen propaganda techniques and we propose a suitable\nevaluation measure. We further design a novel multi-granularity neural network,\nand we show that it outperforms several strong BERT-based baselines.", "published": "2019-10-06 20:26:12", "link": "http://arxiv.org/abs/1910.02517v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Early Prediction of 30-day ICU Re-admissions Using Natural Language\n  Processing and Machine Learning", "abstract": "ICU readmission is associated with longer hospitalization, mortality and\nadverse outcomes. An early recognition of ICU re-admission can help prevent\npatients from worse situation and lower treatment cost. As the abundance of\nElectronics Health Records (EHR), it is popular to design clinical decision\ntools with machine learning technique manipulating on healthcare large scale\ndata. We designed data-driven predictive models to estimate the risk of ICU\nreadmission. The discharge summary of each hospital admission was carefully\nrepresented by natural language processing techniques. Unified Medical Language\nSystem (UMLS) was further used to standardize inconsistency of discharge\nsummaries. 5 machine learning classifiers were adopted to construct predictive\nmodels. The best configuration yielded a competitive AUC of 0.748. Our work\nsuggests that natural language processing of discharge summaries is capable to\nsend clinicians warning of unplanned 30-day readmission upon discharge.", "published": "2019-10-06 22:54:00", "link": "http://arxiv.org/abs/1910.02545v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
