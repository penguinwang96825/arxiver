{"title": "Improving Graph-based Sentence Ordering with Iteratively Predicted\n  Pairwise Orderings", "abstract": "Dominant sentence ordering models can be classified into pairwise ordering\nmodels and set-to-sequence models. However, there is little attempt to combine\nthese two types of models, which inituitively possess complementary advantages.\nIn this paper, we propose a novel sentence ordering framework which introduces\ntwo classifiers to make better use of pairwise orderings for graph-based\nsentence ordering. Specially, given an initial sentence-entity graph, we first\nintroduce a graph-based classifier to predict pairwise orderings between linked\nsentences. Then, in an iterative manner, based on the graph updated by\npreviously predicted high-confident pairwise orderings, another classifier is\nused to predict the remaining uncertain pairwise orderings. At last, we adapt a\nGRN-based sentence ordering model on the basis of final graph. Experiments on\nfive commonly-used datasets demonstrate the effectiveness and generality of our\nmodel. Particularly, when equipped with BERT and FHDecoder, our model achieves\nstate-of-the-art performance.", "published": "2021-10-13 02:18:16", "link": "http://arxiv.org/abs/2110.06446v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual COVID-19 Fake News Detection", "abstract": "The COVID-19 pandemic poses a great threat to global public health.\nMeanwhile, there is massive misinformation associated with the pandemic which\nadvocates unfounded or unscientific claims. Even major social media and news\noutlets have made an extra effort in debunking COVID-19 misinformation, most of\nthe fact-checking information is in English, whereas some unmoderated COVID-19\nmisinformation is still circulating in other languages, threatening the health\nof less-informed people in immigrant communities and developing countries. In\nthis paper, we make the first attempt to detect COVID-19 misinformation in a\nlow-resource language (Chinese) only using the fact-checked news in a\nhigh-resource language (English). We start by curating a Chinese real&fake news\ndataset according to existing fact-checking information. Then, we propose a\ndeep learning framework named CrossFake to jointly encode the cross-lingual\nnews body texts and capture the news content as much as possible. Empirical\nresults on our dataset demonstrate the effectiveness of CrossFake under the\ncross-lingual setting and it also outperforms several monolingual and\ncross-lingual fake news detectors. The dataset is available at\nhttps://github.com/YingtongDou/CrossFake.", "published": "2021-10-13 04:44:02", "link": "http://arxiv.org/abs/2110.06495v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-tuning in ASR systems for efficient domain-adaptation", "abstract": "Automatic Speech Recognition (ASR) systems have found their use in numerous\nindustrial applications in very diverse domains. Since domain-specific systems\nperform better than their generic counterparts on in-domain evaluation, the\nneed for memory and compute-efficient domain adaptation is obvious.\nParticularly, adapting parameter-heavy transformer-based language models used\nfor rescoring ASR hypothesis is challenging. In this work, we overcome the\nproblem using prompt-tuning, a methodology that trains a small number of domain\ntoken embedding parameters to prime a transformer-based LM to a particular\ndomain. With just a handful of extra parameters per domain, we achieve much\nbetter perplexity scores over the baseline of using an unadapted LM. Despite\nbeing parameter-efficient, these improvements are comparable to those of\nfully-fine-tuned models with hundreds of millions of parameters. We replicate\nour findings in perplexity numbers to Word Error Rate in a domain-specific ASR\nsystem for one such domain.", "published": "2021-10-13 05:20:57", "link": "http://arxiv.org/abs/2110.06502v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perception Point: Identifying Critical Learning Periods in Speech for\n  Bilingual Networks", "abstract": "Recent studies in speech perception have been closely linked to fields of\ncognitive psychology, phonology, and phonetics in linguistics. During\nperceptual attunement, a critical and sensitive developmental trajectory has\nbeen examined in bilingual and monolingual infants where they can best\ndiscriminate common phonemes. In this paper, we compare and identify these\ncognitive aspects on deep neural-based visual lip-reading models. We conduct\nexperiments on the two most extensive public visual speech recognition datasets\nfor English and Mandarin. Through our experimental results, we observe a strong\ncorrelation between these theories in cognitive psychology and our unique\nmodeling. We inspect how these computational models develop similar phases in\nspeech perception and acquisitions.", "published": "2021-10-13 05:30:50", "link": "http://arxiv.org/abs/2110.06507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EventBERT: A Pre-Trained Model for Event Correlation Reasoning", "abstract": "Event correlation reasoning infers whether a natural language paragraph\ncontaining multiple events conforms to human common sense. For example, \"Andrew\nwas very drowsy, so he took a long nap, and now he is very alert\" is sound and\nreasonable. In contrast, \"Andrew was very drowsy, so he stayed up a long time,\nnow he is very alert\" does not comply with human common sense. Such reasoning\ncapability is essential for many downstream tasks, such as script reasoning,\nabductive reasoning, narrative incoherence, story cloze test, etc. However,\nconducting event correlation reasoning is challenging due to a lack of large\namounts of diverse event-based knowledge and difficulty in capturing\ncorrelation among multiple events. In this paper, we propose EventBERT, a\npre-trained model to encapsulate eventuality knowledge from unlabeled text.\nSpecifically, we collect a large volume of training examples by identifying\nnatural language paragraphs that describe multiple correlated events and\nfurther extracting event spans in an unsupervised manner. We then propose three\nnovel event- and correlation-based learning objectives to pre-train an event\ncorrelation model on our created training corpus. Empirical results show\nEventBERT outperforms strong baselines on four downstream tasks, and achieves\nSoTA results on most of them. Besides, it outperforms existing pre-trained\nmodels by a large margin, e.g., 6.5~23%, in zero-shot learning of these tasks.", "published": "2021-10-13 06:57:47", "link": "http://arxiv.org/abs/2110.06533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple or Complex? Complexity-Controllable Question Generation with Soft\n  Templates and Deep Mixture of Experts Model", "abstract": "The ability to generate natural-language questions with controlled complexity\nlevels is highly desirable as it further expands the applicability of question\ngeneration. In this paper, we propose an end-to-end neural\ncomplexity-controllable question generation model, which incorporates a mixture\nof experts (MoE) as the selector of soft templates to improve the accuracy of\ncomplexity control and the quality of generated questions. The soft templates\ncapture question similarity while avoiding the expensive construction of actual\ntemplates. Our method introduces a novel, cross-domain complexity estimator to\nassess the complexity of a question, taking into account the passage, the\nquestion, the answer and their interactions. The experimental results on two\nbenchmark QA datasets demonstrate that our QG model is superior to\nstate-of-the-art methods in both automatic and manual evaluation. Moreover, our\ncomplexity estimator is significantly more accurate than the baselines in both\nin-domain and out-domain settings.", "published": "2021-10-13 08:16:52", "link": "http://arxiv.org/abs/2110.06560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better\n  Translators", "abstract": "Prompting has recently been shown as a promising approach for applying\npre-trained language models to perform downstream tasks. We present Multi-Stage\nPrompting (MSP), a simple and automatic approach for leveraging pre-trained\nlanguage models to translation tasks. To better mitigate the discrepancy\nbetween pre-training and translation, MSP divides the translation process via\npre-trained language models into multiple separate stages: the encoding stage,\nthe re-encoding stage, and the decoding stage. During each stage, we\nindependently apply different continuous prompts for allowing pre-trained\nlanguage models better shift to translation tasks. We conduct extensive\nexperiments on three translation tasks. Experiments show that our method can\nsignificantly improve the translation performance of pre-trained language\nmodels.", "published": "2021-10-13 10:06:21", "link": "http://arxiv.org/abs/2110.06609v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Systematic Inequalities in Language Technology Performance across the\n  World's Languages", "abstract": "Natural language processing (NLP) systems have become a central technology in\ncommunication, education, medicine, artificial intelligence, and many other\ndomains of research and development. While the performance of NLP methods has\ngrown enormously over the last decade, this progress has been restricted to a\nminuscule subset of the world's 6,500 languages. We introduce a framework for\nestimating the global utility of language technologies as revealed in a\ncomprehensive snapshot of recent publications in NLP. Our analyses involve the\nfield at large, but also more in-depth studies on both user-facing technologies\n(machine translation, language understanding, question answering,\ntext-to-speech synthesis) as well as more linguistic NLP tasks (dependency\nparsing, morphological inflection). In the process, we (1) quantify disparities\nin the current state of NLP research, (2) explore some of its associated\nsocietal and academic factors, and (3) produce tailored recommendations for\nevidence-based policy making aimed at promoting more global and equitable\nlanguage technologies.", "published": "2021-10-13 14:03:07", "link": "http://arxiv.org/abs/2110.06733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Masader: Metadata Sourcing for Arabic Text and Speech Data Resources", "abstract": "The NLP pipeline has evolved dramatically in the last few years. The first\nstep in the pipeline is to find suitable annotated datasets to evaluate the\ntasks we are trying to solve. Unfortunately, most of the published datasets\nlack metadata annotations that describe their attributes. Not to mention, the\nabsence of a public catalogue that indexes all the publicly available datasets\nrelated to specific regions or languages. When we consider low-resource\ndialectical languages, for example, this issue becomes more prominent. In this\npaper we create \\textit{Masader}, the largest public catalogue for Arabic NLP\ndatasets, which consists of 200 datasets annotated with 25 attributes.\nFurthermore, We develop a metadata annotation strategy that could be extended\nto other languages. We also make remarks and highlight some issues about the\ncurrent status of Arabic NLP datasets and suggest recommendations to address\nthem.", "published": "2021-10-13 14:25:21", "link": "http://arxiv.org/abs/2110.06744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue\n  Systems", "abstract": "Zero/few-shot transfer to unseen services is a critical challenge in\ntask-oriented dialogue research. The Schema-Guided Dialogue (SGD) dataset\nintroduced a paradigm for enabling models to support any service in zero-shot\nthrough schemas, which describe service APIs to models in natural language. We\nexplore the robustness of dialogue systems to linguistic variations in schemas\nby designing SGD-X - a benchmark extending SGD with semantically similar yet\nstylistically diverse variants for every schema. We observe that two top state\ntracking models fail to generalize well across schema variants, measured by\njoint goal accuracy and a novel metric for measuring schema sensitivity.\nAdditionally, we present a simple model-agnostic data augmentation method to\nimprove schema robustness.", "published": "2021-10-13 15:38:29", "link": "http://arxiv.org/abs/2110.06800v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization in Dependency Parsing", "abstract": "Compositionality -- the ability to combine familiar units like words into\nnovel phrases and sentences -- has been the focus of intense interest in\nartificial intelligence in recent years. To test compositional generalization\nin semantic parsing, Keysers et al. (2020) introduced Compositional Freebase\nQueries (CFQ). This dataset maximizes the similarity between the test and train\ndistributions over primitive units, like words, while maximizing the compound\ndivergence: the dissimilarity between test and train distributions over larger\nstructures, like phrases. Dependency parsing, however, lacks a compositional\ngeneralization benchmark. In this work, we introduce a gold-standard set of\ndependency parses for CFQ, and use this to analyze the behavior of a\nstate-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We\nfind that increasing compound divergence degrades dependency parsing\nperformance, although not as dramatically as semantic parsing performance.\nAdditionally, we find the performance of the dependency parser does not\nuniformly degrade relative to compound divergence, and the parser performs\ndifferently on different splits with the same compound divergence. We explore a\nnumber of hypotheses for what causes the non-uniform degradation in dependency\nparsing performance, and identify a number of syntactic structures that drive\nthe dependency parser's lower performance on the most challenging splits.", "published": "2021-10-13 16:32:24", "link": "http://arxiv.org/abs/2110.06843v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphosyntactic Tagging with Pre-trained Language Models for Arabic and\n  its Dialects", "abstract": "We present state-of-the-art results on morphosyntactic tagging across\ndifferent varieties of Arabic using fine-tuned pre-trained transformer language\nmodels. Our models consistently outperform existing systems in Modern Standard\nArabic and all the Arabic dialects we study, achieving 2.6% absolute\nimprovement over the previous state-of-the-art in Modern Standard Arabic, 2.8%\nin Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training\nsetups for fine-tuning pre-trained transformer language models, including\ntraining data size, the use of external linguistic resources, and the use of\nannotated data from other dialects in a low-resource scenario. Our results show\nthat strategic fine-tuning using datasets from other high-resource dialects is\nbeneficial for a low-resource dialect. Additionally, we show that high-quality\nmorphological analyzers as external linguistic resources are beneficial\nespecially in low-resource settings.", "published": "2021-10-13 16:43:44", "link": "http://arxiv.org/abs/2110.06852v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Role Labeling as Dependency Parsing: Exploring Latent Tree\n  Structures Inside Arguments", "abstract": "Semantic role labeling (SRL) is a fundamental yet challenging task in the NLP\ncommunity. Recent works of SRL mainly fall into two lines: 1) BIO-based; 2)\nspan-based. Despite ubiquity, they share some intrinsic drawbacks of not\nconsidering internal argument structures, potentially hindering the model's\nexpressiveness. The key challenge is arguments are flat structures, and there\nare no determined subtree realizations for words inside arguments. To remedy\nthis, in this paper, we propose to regard flat argument spans as latent\nsubtrees, accordingly reducing SRL to a tree parsing task. In particular, we\nequip our formulation with a novel span-constrained TreeCRF to make tree\nstructures span-aware and further extend it to the second-order case. We\nconduct extensive experiments on CoNLL05 and CoNLL12 benchmarks. Results reveal\nthat our methods perform favorably better than all previous syntax-agnostic\nworks, achieving new state-of-the-art under both end-to-end and w/ gold\npredicates settings.", "published": "2021-10-13 17:02:29", "link": "http://arxiv.org/abs/2110.06865v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Essay Scoring Using Transformer Models", "abstract": "Automated essay scoring (AES) is gaining increasing attention in the\neducation sector as it significantly reduces the burden of manual scoring and\nallows ad hoc feedback for learners. Natural language processing based on\nmachine learning has been shown to be particularly suitable for text\nclassification and AES. While many machine-learning approaches for AES still\nrely on a bag-of-words (BOW) approach, we consider a transformer-based approach\nin this paper, compare its performance to a logistic regression model based on\nthe BOW approach and discuss their differences. The analysis is based on 2,088\nemail responses to a problem-solving task, that were manually labeled in terms\nof politeness. Both transformer models considered in that analysis outperformed\nwithout any hyper-parameter tuning the regression-based model. We argue that\nfor AES tasks such as politeness classification, the transformer-based approach\nhas significant advantages, while a BOW approach suffers from not taking word\norder into account and reducing the words to their stem. Further, we show how\nsuch models can help increase the accuracy of human raters, and we provide a\ndetailed instruction on how to implement transformer-based models for one's own\npurpose.", "published": "2021-10-13 17:09:47", "link": "http://arxiv.org/abs/2110.06874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Audio-Visual Scene-Aware Dialog and Reasoning using Audio-Visual\n  Transformers with Joint Student-Teacher Learning", "abstract": "In previous work, we have proposed the Audio-Visual Scene-Aware Dialog (AVSD)\ntask, collected an AVSD dataset, developed AVSD technologies, and hosted an\nAVSD challenge track at both the 7th and 8th Dialog System Technology\nChallenges (DSTC7, DSTC8). In these challenges, the best-performing systems\nrelied heavily on human-generated descriptions of the video content, which were\navailable in the datasets but would be unavailable in real-world applications.\nTo promote further advancements for real-world applications, we proposed a\nthird AVSD challenge, at DSTC10, with two modifications: 1) the human-created\ndescription is unavailable at inference time, and 2) systems must demonstrate\ntemporal reasoning by finding evidence from the video to support each answer.\nThis paper introduces the new task that includes temporal reasoning and our new\nextension of the AVSD dataset for DSTC10, for which we collected\nhuman-generated temporal reasoning data. We also introduce a baseline system\nbuilt using an AV-transformer, which we released along with the new dataset.\nFinally, this paper introduces a new system that extends our baseline system\nwith attentional multimodal fusion, joint student-teacher learning (JSTL), and\nmodel combination techniques, achieving state-of-the-art performances on the\nAVSD datasets for DSTC7, DSTC8, and DSTC10. We also propose two temporal\nreasoning methods for AVSD: one attention-based, and one based on a time-domain\nregion proposal network.", "published": "2021-10-13 17:24:16", "link": "http://arxiv.org/abs/2110.06894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching Models new APIs: Domain-Agnostic Simulators for Task Oriented\n  Dialogue", "abstract": "We demonstrate that large language models are able to simulate Task Oriented\nDialogues in novel domains, provided only with an API implementation and a list\nof goals. We show these simulations can formulate online, automatic metrics\nthat correlate well with human evaluations. Furthermore, by checking for\nwhether the User's goals are met, we can use simulation to repeatedly generate\ntraining data and improve the quality of simulations themselves. With no human\nintervention or domain-specific training data, our simulations bootstrap\nend-to-end models which achieve a 37\\% error reduction in previously unseen\ndomains. By including as few as 32 domain-specific conversations, bootstrapped\nmodels can match the performance of a fully-supervised model with $10\\times$\nmore data. To our knowledge, this is the first time simulations have been shown\nto be effective at bootstrapping models without explicitly requiring any\ndomain-specific training data, rule-engineering, or humans-in-the-loop.", "published": "2021-10-13 17:39:25", "link": "http://arxiv.org/abs/2110.06905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantics-aware Attention Improves Neural Machine Translation", "abstract": "The integration of syntactic structures into Transformer machine translation\nhas shown positive results, but to our knowledge, no work has attempted to do\nso with semantic structures. In this work we propose two novel parameter-free\nmethods for injecting semantic information into Transformers, both rely on\nsemantics-aware masking of (some of) the attention heads. One such method\noperates on the encoder, through a Scene-Aware Self-Attention (SASA) head.\nAnother on the decoder, through a Scene-Aware Cross-Attention (SACrA) head. We\nshow a consistent improvement over the vanilla Transformer and syntax-aware\nmodels for four language pairs. We further show an additional gain when using\nboth semantic and syntactic structures in some language pairs.", "published": "2021-10-13 17:58:22", "link": "http://arxiv.org/abs/2110.06920v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FlexiTerm: A more efficient implementation of flexible multi-word term\n  recognition", "abstract": "Terms are linguistic signifiers of domain-specific concepts. Automated\nrecognition of multi-word terms (MWT) in free text is a sequence labelling\nproblem, which is commonly addressed using supervised machine learning methods.\nTheir need for manual annotation of training data makes it difficult to port\nsuch methods across domains. FlexiTerm, on the other hand, is a fully\nunsupervised method for MWT recognition from domain-specific corpora.\nOriginally implemented in Java as a proof of concept, it did not scale well,\nthus offering little practical value in the context of big data. In this paper,\nwe describe its re-implementation in Python and compare the performance of\nthese two implementations. The results demonstrated major improvements in terms\nof efficiency, which allow FlexiTerm to transition from the proof of concept to\nthe production-grade application.", "published": "2021-10-13 18:49:46", "link": "http://arxiv.org/abs/2110.06981v2", "categories": ["cs.CL", "I.2.7; H.3.1"], "primary_category": "cs.CL"}
{"title": "Federated Natural Language Generation for Personalized Dialogue System", "abstract": "Neural conversational models have long suffered from the problem of\ninconsistency and lacking coherent personality. To address the issue,\npersona-based models capturing individual characteristics have been proposed,\nbut they still face the dilemma of model adaption and data privacy. To break\nthis dilemma, we propose a novel Federated Natural Language Generation (FedNLG)\nframework, which learns personalized representations from various dataset on\ndistributed devices, and thus implements the personalized dialogue system\nefficiently and safely. FedNLG first pre-trains parameters of standard neural\nconversational model over a large dialogue corpus, and then fine-tune the model\nparameters and persona embeddings on specific datasets, in a federated manner.\nThus, the model could simultaneously learn the persona embeddings in local\nclients and learn shared model parameters by federated aggregation, which\nachieves accuracyprivacy balance. By conducting extensive experiments, we\ndemonstrate the effectiveness of our model by pre-training model over Cornell\nMovie-Dialogs Corpus and fine-tuning the model over two TV series dataset.", "published": "2021-10-13 00:59:52", "link": "http://arxiv.org/abs/2110.06419v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ActiveEA: Active Learning for Neural Entity Alignment", "abstract": "Entity Alignment (EA) aims to match equivalent entities across different\nKnowledge Graphs (KGs) and is an essential step of KG fusion. Current\nmainstream methods -- neural EA models -- rely on training with seed alignment,\ni.e., a set of pre-aligned entity pairs which are very costly to annotate. In\nthis paper, we devise a novel Active Learning (AL) framework for neural EA,\naiming to create highly informative seed alignment to obtain more effective EA\nmodels with less annotation cost. Our framework tackles two main challenges\nencountered when applying AL to EA: (1) How to exploit dependencies between\nentities within the AL strategy. Most AL strategies assume that the data\ninstances to sample are independent and identically distributed. However,\nentities in KGs are related. To address this challenge, we propose a\nstructure-aware uncertainty sampling strategy that can measure the uncertainty\nof each entity as well as its impact on its neighbour entities in the KG. (2)\nHow to recognise entities that appear in one KG but not in the other KG (i.e.,\nbachelors). Identifying bachelors would likely save annotation budget. To\naddress this challenge, we devise a bachelor recognizer paying attention to\nalleviate the effect of sampling bias. Empirical results show that our proposed\nAL strategy can significantly improve sampling quality with good generality\nacross different datasets, EA models and amount of bachelors.", "published": "2021-10-13 03:38:04", "link": "http://arxiv.org/abs/2110.06474v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding of Emotion Perception from Art", "abstract": "Computational modeling of the emotions evoked by art in humans is a\nchallenging problem because of the subjective and nuanced nature of art and\naffective signals. In this paper, we consider the above-mentioned problem of\nunderstanding emotions evoked in viewers by artwork using both text and visual\nmodalities. Specifically, we analyze images and the accompanying text captions\nfrom the viewers expressing emotions as a multimodal classification task. Our\nresults show that single-stream multimodal transformer-based models like MMBT\nand VisualBERT perform better compared to both image-only models and\ndual-stream multimodal models having separate pathways for text and image\nmodalities. We also observe improvements in performance for extreme positive\nand negative emotion classes, when a single-stream model like MMBT is compared\nwith a text-only transformer model like BERT.", "published": "2021-10-13 04:14:49", "link": "http://arxiv.org/abs/2110.06486v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploring Dense Retrieval for Dialogue Response Selection", "abstract": "Recent progress in deep learning has continuously improved the accuracy of\ndialogue response selection. In particular, sophisticated neural network\narchitectures are leveraged to capture the rich interactions between dialogue\ncontext and response candidates. While remarkably effective, these models also\nbring in a steep increase in computational cost. Consequently, such models can\nonly be used as a re-rank module in practice. In this study, we present a\nsolution to directly select proper responses from a large corpus or even a\nnonparallel corpus that only consists of unpaired sentences, using a dense\nretrieval model. To push the limits of dense retrieval, we design an\ninteraction layer upon the dense retrieval models and apply a set of\ntailor-designed learning strategies. Our model shows superiority over strong\nbaselines on the conventional re-rank evaluation setting, which is remarkable\ngiven its efficiency. To verify the effectiveness of our approach in realistic\nscenarios, we also conduct full-rank evaluation, where the target is to select\nproper responses from a full candidate pool that may contain millions of\ncandidates and evaluate them fairly through human annotations. Our proposed\nmodel notably outperforms pipeline baselines that integrate fast recall and\nexpressive re-rank modules. Human evaluation results show that enlarging the\ncandidate pool with nonparallel corpora improves response quality further.", "published": "2021-10-13 10:10:32", "link": "http://arxiv.org/abs/2110.06612v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Maximizing Efficiency of Language Model Pre-training for Learning\n  Representation", "abstract": "Pre-trained language models in the past years have shown exponential growth\nin model parameters and compute time. ELECTRA is a novel approach for improving\nthe compute efficiency of pre-trained language models (e.g. BERT) based on\nmasked language modeling (MLM) by addressing the sample inefficiency problem\nwith the replaced token detection (RTD) task. Our work proposes adaptive early\nexit strategy to maximize the efficiency of the pre-training process by\nrelieving the model's subsequent layers of the need to process latent features\nby leveraging earlier layer representations. Moreover, we evaluate an initial\napproach to the problem that has not succeeded in maintaining the accuracy of\nthe model while showing a promising compute efficiency by thoroughly\ninvestigating the necessity of the generator module of ELECTRA.", "published": "2021-10-13 10:25:06", "link": "http://arxiv.org/abs/2110.06620v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MDERank: A Masked Document Embedding Rank Approach for Unsupervised\n  Keyphrase Extraction", "abstract": "Keyphrase extraction (KPE) automatically extracts phrases in a document that\nprovide a concise summary of the core content, which benefits downstream\ninformation retrieval and NLP tasks. Previous state-of-the-art (SOTA) methods\nselect candidate keyphrases based on the similarity between learned\nrepresentations of the candidates and the document. They suffer performance\ndegradation on long documents due to discrepancy between sequence lengths which\ncauses mismatch between representations of keyphrase candidates and the\ndocument. In this work, we propose a novel unsupervised embedding-based KPE\napproach, Masked Document Embedding Rank (MDERank), to address this problem by\nleveraging a mask strategy and ranking candidates by the similarity between\nembeddings of the source document and the masked document. We further develop a\nKPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised\ncontrastive learning method, which is more compatible to MDERank than vanilla\nBERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the\nproposed MDERank outperforms state-of-the-art unsupervised KPE approach by\naverage 1.80 $F1@15$ improvement. MDERank further benefits from KPEBERT and\noverall achieves average 3.53 $F1@15$ improvement over the SOTA SIFRank. Our\ncode is available at \\url{https://github.com/LinhanZ/mderank}.", "published": "2021-10-13 11:29:17", "link": "http://arxiv.org/abs/2110.06651v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese", "abstract": "Although pre-trained models (PLMs) have achieved remarkable improvements in a\nwide range of NLP tasks, they are expensive in terms of time and resources.\nThis calls for the study of training more efficient models with less\ncomputation but still ensures impressive performance. Instead of pursuing a\nlarger scale, we are committed to developing lightweight yet more powerful\nmodels trained with equal or less computation and friendly to rapid deployment.\nThis technical report releases our pre-trained model called Mengzi, which\nstands for a family of discriminative, generative, domain-specific, and\nmultimodal pre-trained model variants, capable of a wide range of language and\nvision tasks. Compared with public Chinese PLMs, Mengzi is simple but more\npowerful. Our lightweight model has achieved new state-of-the-art results on\nthe widely-used CLUE benchmark with our optimized pre-training and fine-tuning\ntechniques. Without modifying the model architecture, our model can be easily\nemployed as an alternative to existing PLMs. Our sources are available at\nhttps://github.com/Langboat/Mengzi.", "published": "2021-10-13 13:14:32", "link": "http://arxiv.org/abs/2110.06696v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Speaker-aware Parallel Hierarchical Attentive Encoder-Decoder Model\n  for Multi-turn Dialogue Generation", "abstract": "This paper presents a novel open-domain dialogue generation model emphasizing\nthe differentiation of speakers in multi-turn conversations. Differing from\nprior work that solely relies on the content of conversation history to\ngenerate a response, we argue that capturing relative social relations among\nutterances (i.e., generated by either the same speaker or different persons)\nbenefits the machine capturing fine-grained context information from a\nconversation history to improve context coherence in the generated response.\nGiven that, we propose a speaker-aware Parallel Hierarchical Attentive\nEncoder-Decoder (PHAED) model that aims to model each utterance with the\nawareness of its speaker and contextual associations with the same speaker's\nprevious messages. Specifically, in a conversation involving two speakers, we\nregard the utterances from one speaker as responses and those from the other as\nqueries. After understanding queries via our encoder with inner-query and\ninter-query encodings, our decoder reuses the hidden states of previously\ngenerated responses, instead of reconstructing these by the encoder, to\ngenerate a new response. Our empirical results show that PHAED outperforms the\nstate-of-the-art in both automatic and human evaluations. Furthermore, our\nablation study shows that dialogue models with speaker tokens can generally\ndecrease the possibility of generating non-coherent responses regarding the\nconversation context.", "published": "2021-10-13 16:08:29", "link": "http://arxiv.org/abs/2110.06823v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Language Model Integration for RNN Transducer based Speech\n  Recognition", "abstract": "The mismatch between an external language model (LM) and the implicitly\nlearned internal LM (ILM) of RNN-Transducer (RNN-T) can limit the performance\nof LM integration such as simple shallow fusion. A Bayesian interpretation\nsuggests to remove this sequence prior as ILM correction. In this work, we\nstudy various ILM correction-based LM integration methods formulated in a\ncommon RNN-T framework. We provide a decoding interpretation on two major\nreasons for performance improvement with ILM correction, which is further\nexperimentally verified with detailed analysis. We also propose an exact-ILM\ntraining framework by extending the proof given in the hybrid autoregressive\ntransducer, which enables a theoretical justification for other ILM approaches.\nSystematic comparison is conducted for both in-domain and cross-domain\nevaluation on the Librispeech and TED-LIUM Release 2 corpora, respectively. Our\nproposed exact-ILM training can further improve the best ILM method.", "published": "2021-10-13 16:30:46", "link": "http://arxiv.org/abs/2110.06841v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ConditionalQA: A Complex Reading Comprehension Dataset with Conditional\n  Answers", "abstract": "We describe a Question Answering (QA) dataset that contains complex questions\nwith conditional answers, i.e. the answers are only applicable when certain\nconditions apply. We call this dataset ConditionalQA. In addition to\nconditional answers, the dataset also features: (1) long context documents with\ninformation that is related in logically complex ways; (2) multi-hop questions\nthat require compositional logical reasoning; (3) a combination of extractive\nquestions, yes/no questions, questions with multiple answers, and\nnot-answerable questions; (4) questions asked without knowing the answers. We\nshow that ConditionalQA is challenging for many of the existing QA models,\nespecially in selecting answer conditions. We believe that this dataset will\nmotivate further research in answering complex questions over long documents.\nData and leaderboard are publicly available at\n\\url{https://github.com/haitian-sun/ConditionalQA}.", "published": "2021-10-13 17:16:46", "link": "http://arxiv.org/abs/2110.06884v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Modelling via Learning to Rank", "abstract": "We consider language modelling (LM) as a multi-label structured prediction\ntask by re-framing training from solely predicting a single ground-truth word\nto ranking a set of words which could continue a given context. To avoid\nannotating top-$k$ ranks, we generate them using pre-trained LMs: GPT-2, BERT,\nand Born-Again models. This leads to a rank-based form of knowledge\ndistillation (KD). We also develop a method using $N$-grams to create a\nnon-probabilistic teacher which generates the ranks without the need of a\npre-trained LM.\n  We confirm the hypotheses that we can treat LMing as a ranking task and that\nwe can do so without the use of a pre-trained LM. We show that rank-based KD\ngenerally improves perplexity (PPL), often with statistical significance, when\ncompared to Kullback-Leibler-based KD. Surprisingly, given the simplicity of\nthe method, $N$-grams act as competitive teachers and achieve similar\nperformance as using either BERT or a Born-Again model teachers. GPT-2 always\nacts as the best teacher, though, and using it and a Transformer-XL student on\nWiki-02, rank-based KD reduces a cross-entropy baseline from 65.27 to 55.94 and\nagainst a KL-based KD of 56.70.", "published": "2021-10-13 18:03:47", "link": "http://arxiv.org/abs/2110.06961v2", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Open-Domain Question-Answering for COVID-19 and Other Emergent Domains", "abstract": "Since late 2019, COVID-19 has quickly emerged as the newest biomedical\ndomain, resulting in a surge of new information. As with other emergent\ndomains, the discussion surrounding the topic has been rapidly changing,\nleading to the spread of misinformation. This has created the need for a public\nspace for users to ask questions and receive credible, scientific answers. To\nfulfill this need, we turn to the task of open-domain question-answering, which\nwe can use to efficiently find answers to free-text questions from a large set\nof documents. In this work, we present such a system for the emergent domain of\nCOVID-19. Despite the small data size available, we are able to successfully\ntrain the system to retrieve answers from a large-scale corpus of published\nCOVID-19 scientific papers. Furthermore, we incorporate effective re-ranking\nand question-answering techniques, such as document diversity and multiple\nanswer spans. Our open-domain question-answering system can further act as a\nmodel for the quick development of similar systems that can be adapted and\nmodified for other developing emergent domains.", "published": "2021-10-13 18:06:14", "link": "http://arxiv.org/abs/2110.06962v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation\n  with Multi-Armed Bandits", "abstract": "Training data for machine translation (MT) is often sourced from a multitude\nof large corpora that are multi-faceted in nature, e.g. containing contents\nfrom multiple domains or different levels of quality or complexity. Naturally,\nthese facets do not occur with equal frequency, nor are they equally important\nfor the test scenario at hand. In this work, we propose to optimize this\nbalance jointly with MT model parameters to relieve system developers from\nmanual schedule design. A multi-armed bandit is trained to dynamically choose\nbetween facets in a way that is most beneficial for the MT system. We evaluate\nit on three different multi-facet applications: balancing translationese and\nnatural training data, or data from multiple domains or multiple language\npairs. We find that bandit learning leads to competitive MT systems across\ntasks, and our analysis provides insights into its learned strategies and the\nunderlying data sets.", "published": "2021-10-13 19:16:59", "link": "http://arxiv.org/abs/2110.06997v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Efficient NLP: A Standard Evaluation and A Strong Baseline", "abstract": "Supersized pre-trained language models have pushed the accuracy of various\nnatural language processing (NLP) tasks to a new state-of-the-art (SOTA).\nRather than pursuing the reachless SOTA accuracy, more and more researchers\nstart paying attention on model efficiency and usability. Different from\naccuracy, the metric for efficiency varies across different studies, making\nthem hard to be fairly compared. To that end, this work presents ELUE\n(Efficient Language Understanding Evaluation), a standard evaluation, and a\npublic leaderboard for efficient NLP models. ELUE is dedicated to depict the\nPareto Frontier for various language understanding tasks, such that it can tell\nwhether and how much a method achieves Pareto improvement. Along with the\nbenchmark, we also release a strong baseline, ElasticBERT, which allows BERT to\nexit at any layer in both static and dynamic ways. We demonstrate the\nElasticBERT, despite its simplicity, outperforms or performs on par with SOTA\ncompressed and early exiting models. With ElasticBERT, the proposed ELUE has a\nstrong Pareto Frontier and makes a better evaluation for efficient NLP models.", "published": "2021-10-13 21:17:15", "link": "http://arxiv.org/abs/2110.07038v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continual learning using lattice-free MMI for speech recognition", "abstract": "Continual learning (CL), or domain expansion, recently became a popular topic\nfor automatic speech recognition (ASR) acoustic modeling because practical\nsystems have to be updated frequently in order to work robustly on types of\nspeech not observed during initial training. While sequential adaptation allows\ntuning a system to a new domain, it may result in performance degradation on\nthe old domains due to catastrophic forgetting. In this work we explore\nregularization-based CL for neural network acoustic models trained with the\nlattice-free maximum mutual information (LF-MMI) criterion. We simulate domain\nexpansion by incrementally adapting the acoustic model on different public\ndatasets that include several accents and speaking styles. We investigate two\nwell-known CL techniques, elastic weight consolidation (EWC) and learning\nwithout forgetting (LWF), which aim to reduce forgetting by preserving model\nweights or network outputs. We additionally introduce a sequence-level LWF\nregularization, which exploits posteriors from the denominator graph of LF-MMI\nto further reduce forgetting. Empirical results show that the proposed\nsequence-level LWF can improve the best average word error rate across all\ndomains by up to 9.4% relative compared with using regular LWF.", "published": "2021-10-13 22:11:11", "link": "http://arxiv.org/abs/2110.07055v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Inconsistent Few-Shot Relation Classification via Cross-Attentional\n  Prototype Networks with Contrastive Learning", "abstract": "Standard few-shot relation classification (RC) is designed to learn a robust\nclassifier with only few labeled data for each class. However, previous works\nrarely investigate the effects of a different number of classes (i.e., $N$-way)\nand number of labeled data per class (i.e., $K$-shot) during training vs.\ntesting. In this work, we define a new task, \\textit{inconsistent few-shot RC},\nwhere the model needs to handle the inconsistency of $N$ and $K$ between\ntraining and testing. To address this new task, we propose Prototype\nNetwork-based cross-attention contrastive learning (ProtoCACL) to capture the\nrich mutual interactions between the support set and query set. Experimental\nresults demonstrate that our ProtoCACL can outperform the state-of-the-art\nbaseline model under both inconsistent $K$ and inconsistent $N$ settings, owing\nto its more robust and discriminate representations. Moreover, we identify that\nin the inconsistent few-shot learning setting, models can achieve better\nperformance with \\textit{less data} than the standard few-shot setting with\ncarefully-selected $N$ and $K$. In the end of the paper, we provide further\nanalyses and suggestions to systematically guide the selection of $N$ and $K$\nunder different scenarios.", "published": "2021-10-13 07:47:13", "link": "http://arxiv.org/abs/2110.08254v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "E-Commerce Dispute Resolution Prediction", "abstract": "E-Commerce marketplaces support millions of daily transactions, and some\ndisagreements between buyers and sellers are unavoidable. Resolving disputes in\nan accurate, fast, and fair manner is of great importance for maintaining a\ntrustworthy platform. Simple cases can be automated, but intricate cases are\nnot sufficiently addressed by hard-coded rules, and therefore most disputes are\ncurrently resolved by people. In this work we take a first step towards\nautomatically assisting human agents in dispute resolution at scale. We\nconstruct a large dataset of disputes from the eBay online marketplace, and\nidentify several interesting behavioral and linguistic patterns. We then train\nclassifiers to predict dispute outcomes with high accuracy. We explore the\nmodel and the dataset, reporting interesting correlations, important features,\nand insights.", "published": "2021-10-13 09:45:06", "link": "http://arxiv.org/abs/2110.15730v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fake News Detection in Spanish Using Deep Learning Techniques", "abstract": "This paper addresses the problem of fake news detection in Spanish using\nMachine Learning techniques. It is fundamentally the same problem tackled for\nthe English language; however, there is not a significant amount of publicly\navailable and adequately labeled fake news in Spanish to effectively train a\nMachine Learning model, similarly to those proposed for the English language.\nTherefore, this work explores different training strategies and architectures\nto establish a baseline for further research in this area. Four datasets were\nused, two in English and two in Spanish, and four experimental schemes were\ntested, including a baseline with classical Machine Learning models, trained\nand validated using a small dataset in Spanish. The remaining schemes include\nstate-of-the-art Deep Learning models trained (or fine-tuned) and validated in\nEnglish, trained and validated in Spanish, and fitted in English and validated\nwith automatic translated Spanish sentences. The Deep Learning architectures\nwere built on top of different pre-trained Word Embedding representations,\nincluding GloVe, ELMo, BERT, and BETO (a BERT version trained on a large corpus\nin Spanish). According to the results, the best strategy was a combination of a\npre-trained BETO model and a Recurrent Neural Network based on LSTM layers,\nyielding an accuracy of up to 80%; nonetheless, a baseline model using a Random\nForest estimator obtained similar outcomes. Additionally, the translation\nstrategy did not yield acceptable results because of the propagation error;\nthere was also observed a significant difference in models performance when\ntrained in English or Spanish, mainly attributable to the number of samples\navailable for each language.", "published": "2021-10-13 02:56:16", "link": "http://arxiv.org/abs/2110.06461v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dict-BERT: Enhancing Language Model Pre-training with Dictionary", "abstract": "Pre-trained language models (PLMs) aim to learn universal language\nrepresentations by conducting self-supervised training tasks on large-scale\ncorpora. Since PLMs capture word semantics in different contexts, the quality\nof word representations highly depends on word frequency, which usually follows\na heavy-tailed distributions in the pre-training corpus. Therefore, the\nembeddings of rare words on the tail are usually poorly optimized. In this\nwork, we focus on enhancing language model pre-training by leveraging\ndefinitions of the rare words in dictionaries (e.g., Wiktionary). To\nincorporate a rare word definition as a part of input, we fetch its definition\nfrom the dictionary and append it to the end of the input text sequence. In\naddition to training with the masked language modeling objective, we propose\ntwo novel self-supervised pre-training tasks on word and sentence-level\nalignment between input text sequence and rare word definitions to enhance\nlanguage modeling representation with dictionary. We evaluate the proposed\nDict-BERT model on the language understanding benchmark GLUE and eight\nspecialized domain benchmark datasets. Extensive experiments demonstrate that\nDict-BERT can significantly improve the understanding of rare words and boost\nmodel performance on various NLP downstream tasks.", "published": "2021-10-13 04:29:14", "link": "http://arxiv.org/abs/2110.06490v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Differentially Private Fine-tuning of Language Models", "abstract": "We give simpler, sparser, and faster algorithms for differentially private\nfine-tuning of large-scale pre-trained language models, which achieve the\nstate-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.\nWe propose a meta-framework for this problem, inspired by the recent success of\nhighly parameter-efficient methods for fine-tuning. Our experiments show that\ndifferentially private adaptations of these approaches outperform previous\nprivate algorithms in three important dimensions: utility, privacy, and the\ncomputational and memory cost of private training. On many commonly studied\ndatasets, the utility of private models approaches that of non-private models.\nFor example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using\nRoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of\n$\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large\nachieves an accuracy of $90.2\\%$. Our findings are similar for natural language\ngeneration tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,\nGPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8\nrespectively (privacy budget of $\\epsilon = 6.8,\\delta=$ 1e-5) whereas the\nnon-private baseline is $48.1$. All our experiments suggest that larger models\nare better suited for private fine-tuning: while they are well known to achieve\nsuperior accuracy non-privately, we find that they also better maintain their\naccuracy when privacy is introduced.", "published": "2021-10-13 05:15:00", "link": "http://arxiv.org/abs/2110.06500v2", "categories": ["cs.LG", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Well-classified Examples are Underestimated in Classification with Deep\n  Neural Networks", "abstract": "The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and margin growth. To counteract this deficiency, we propose to\nreward well-classified examples with additive bonuses to revive their\ncontribution to the learning process. This counterexample theoretically\naddresses these three issues. We empirically support this claim by directly\nverifying the theoretical results or significant performance improvement with\nour counterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that we\ncan deal with complex scenarios, such as imbalanced classification, OOD\ndetection, and applications under adversarial attacks because our idea can\nsolve these three issues. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.", "published": "2021-10-13 07:19:47", "link": "http://arxiv.org/abs/2110.06537v6", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "End-to-end translation of human neural activity to speech with a\n  dual-dual generative adversarial network", "abstract": "In a recent study of auditory evoked potential (AEP) based brain-computer\ninterface (BCI), it was shown that, with an encoder-decoder framework, it is\npossible to translate human neural activity to speech (T-CAS). However, current\nencoder-decoder-based methods achieve T-CAS often with a two-step method where\nthe information is passed between the encoder and decoder with a shared\ndimension reduction vector, which may result in a loss of information. A\npotential approach to this problem is to design an end-to-end method by using a\ndual generative adversarial network (DualGAN) without dimension reduction of\npassing information, but it cannot realize one-to-one signal-to-signal\ntranslation (see Fig.1 (a) and (b)). In this paper, we propose an end-to-end\nmodel to translate human neural activity to speech directly, create a new\nelectroencephalogram (EEG) datasets for participants with good attention by\ndesign a device to detect participants' attention, and introduce a dual-dual\ngenerative adversarial network (Dual-DualGAN) (see Fig. 1 (c) and (d)) to\naddress an end-to-end translation of human neural activity to speech (ET-CAS)\nproblem by group labelling EEG signals and speech signals, inserting a\ntransition domain to realize cross-domain mapping. In the transition domain,\nthe transition signals are cascaded by the corresponding EEG and speech signals\nin a certain proportion, which can build bridges for EEG and speech signals\nwithout corresponding features, and realize one-to-one cross-domain\nEEG-to-speech translation. The proposed method can translate word-length and\nsentence-length sequences of neural activity to speech. Experimental evaluation\nhas been conducted to show that the proposed method significantly outperforms\nstate-of-the-art methods on both words and sentences of auditory stimulus.", "published": "2021-10-13 10:54:41", "link": "http://arxiv.org/abs/2110.06634v3", "categories": ["cs.SD", "cs.CL", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Truthful AI: Developing and governing AI that does not lie", "abstract": "In many contexts, lying -- the use of verbal falsehoods to deceive -- is\nharmful. While lying has traditionally been a human affair, AI systems that\nmake sophisticated verbal statements are becoming increasingly prevalent. This\nraises the question of how we should limit the harm caused by AI \"lies\" (i.e.\nfalsehoods that are actively selected for). Human truthfulness is governed by\nsocial norms and by laws (against defamation, perjury, and fraud). Differences\nbetween AI and humans present an opportunity to have more precise standards of\ntruthfulness for AI, and to have these standards rise over time. This could\nprovide significant benefits to public epistemics and the economy, and mitigate\nrisks of worst-case AI futures.\n  Establishing norms or laws of AI truthfulness will require significant work\nto: (1) identify clear truthfulness standards; (2) create institutions that can\njudge adherence to those standards; and (3) develop AI systems that are\nrobustly truthful.\n  Our initial proposals for these areas include: (1) a standard of avoiding\n\"negligent falsehoods\" (a generalisation of lies that is easier to assess); (2)\ninstitutions to evaluate AI systems before and after real-world deployment; and\n(3) explicitly training AI systems to be truthful via curated datasets and\nhuman interaction.\n  A concerning possibility is that evaluation mechanisms for eventual\ntruthfulness standards could be captured by political interests, leading to\nharmful censorship and propaganda. Avoiding this might take careful attention.\nAnd since the scale of AI speech acts might grow dramatically over the coming\ndecades, early truthfulness standards might be particularly important because\nof the precedents they set.", "published": "2021-10-13 12:18:09", "link": "http://arxiv.org/abs/2110.06674v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "I.2.0"], "primary_category": "cs.CY"}
{"title": "Leveraging Automated Unit Tests for Unsupervised Code Translation", "abstract": "With little to no parallel data available for programming languages,\nunsupervised methods are well-suited to source code translation. However, the\nmajority of unsupervised machine translation approaches rely on\nback-translation, a method developed in the context of natural language\ntranslation and one that inherently involves training on noisy inputs.\nUnfortunately, source code is highly sensitive to small changes; a single token\ncan result in compilation failures or erroneous programs, unlike natural\nlanguages where small inaccuracies may not change the meaning of a sentence. To\naddress this issue, we propose to leverage an automated unit-testing system to\nfilter out invalid translations, thereby creating a fully tested parallel\ncorpus. We found that fine-tuning an unsupervised model with this filtered data\nset significantly reduces the noise in the translations so-generated,\ncomfortably outperforming the state-of-the-art for all language pairs studied.\nIn particular, for Java $\\to$ Python and Python $\\to$ C++ we outperform the\nbest previous methods by more than 16% and 24% respectively, reducing the error\nrate by more than 35%.", "published": "2021-10-13 15:08:43", "link": "http://arxiv.org/abs/2110.06773v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Leveraging redundancy in attention with Reuse Transformers", "abstract": "Pairwise dot product-based attention allows Transformers to exchange\ninformation between tokens in an input-dependent way, and is key to their\nsuccess across diverse applications in language and vision. However, a typical\nTransformer model computes such pairwise attention scores repeatedly for the\nsame sequence, in multiple heads in multiple layers. We systematically analyze\nthe empirical similarity of these scores across heads and layers and find them\nto be considerably redundant, especially adjacent layers showing high\nsimilarity. Motivated by these findings, we propose a novel architecture that\nreuses attention scores computed in one layer in multiple subsequent layers.\nExperiments on a number of standard benchmarks show that reusing attention\ndelivers performance equivalent to or better than standard transformers, while\nreducing both compute and memory usage.", "published": "2021-10-13 16:08:02", "link": "http://arxiv.org/abs/2110.06821v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Ousiometrics and Telegnomics: The essence of meaning conforms to a\n  two-dimensional powerful-weak and dangerous-safe framework with diverse\n  corpora presenting a safety bias", "abstract": "We define `ousiometrics' to be the study of essential meaning in whatever\ncontext that meaningful signals are communicated, and `telegnomics' as the\nstudy of remotely sensed knowledge. From work emerging through the middle of\nthe 20th century, the essence of meaning has become generally accepted as being\nwell captured by the three orthogonal dimensions of evaluation, potency, and\nactivation (EPA). By re-examining first types and then tokens for the English\nlanguage, and through the use of automatically annotated histograms --\n`ousiograms' -- we find here that: 1. The essence of meaning conveyed by words\nis instead best described by a compass-like power-danger (PD) framework, and 2.\nAnalysis of a disparate collection of large-scale English language corpora --\nliterature, news, Wikipedia, talk radio, and social media -- shows that natural\nlanguage exhibits a systematic bias toward safe, low danger words -- a\nreinterpretation of the Pollyanna principle's positivity bias for written\nexpression. To help justify our choice of dimension names and to help address\nthe problems with representing observed ousiometric dimensions by bipolar\nadjective pairs, we introduce and explore `synousionyms' and `antousionyms' --\nousiometric counterparts of synonyms and antonyms. We further show that the PD\nframework revises the circumplex model of affect as a more general model of\nstate of mind. Finally, we use our findings to construct and test a prototype\n`ousiometer', a telegnomic instrument that measures ousiometric time series for\ntemporal corpora. We contend that our power-danger ousiometric framework\nprovides a complement for entropy-based measurements, and may be of value for\nthe study of a wide variety of communication across biological and artificial\nlife.", "published": "2021-10-13 16:35:22", "link": "http://arxiv.org/abs/2110.06847v2", "categories": ["cs.CL", "cs.CY", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a\n  Sparse One?", "abstract": "Despite their recent popularity and well-known advantages, dense retrievers\nstill lag behind sparse methods such as BM25 in their ability to reliably match\nsalient phrases and rare entities in the query and to generalize to\nout-of-domain data. It has been argued that this is an inherent limitation of\ndense models. We rebut this claim by introducing the Salient Phrase Aware\nRetriever (SPAR), a dense retriever with the lexical matching capacity of a\nsparse model. We show that a dense Lexical Model {\\Lambda} can be trained to\nimitate a sparse one, and SPAR is built by augmenting a standard dense\nretriever with {\\Lambda}. Empirically, SPAR shows superior performance on a\nrange of tasks including five question answering datasets, MS MARCO passage\nretrieval, as well as the EntityQuestions and BEIR benchmarks for out-of-domain\nevaluation, exceeding the performance of state-of-the-art dense and sparse\nretrievers. The code and models of SPAR are available at:\nhttps://github.com/facebookresearch/dpr-scale/tree/main/spar", "published": "2021-10-13 17:56:19", "link": "http://arxiv.org/abs/2110.06918v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation", "abstract": "Text autoencoders are often used for unsupervised conditional text generation\nby applying mappings in the latent space to change attributes to the desired\nvalues. Recently, Mai et al. (2020) proposed Emb2Emb, a method to learn these\nmappings in the embedding space of an autoencoder. However, their method is\nrestricted to autoencoders with a single-vector embedding, which limits how\nmuch information can be retained. We address this issue by extending their\nmethod to Bag-of-Vectors Autoencoders (BoV-AEs), which encode the text into a\nvariable-size bag of vectors that grows with the size of the text, as in\nattention-based models. This allows to encode and reconstruct much longer texts\nthan standard autoencoders. Analogous to conventional autoencoders, we propose\nregularization techniques that facilitate learning meaningful operations in the\nlatent space. Finally, we adapt Emb2Emb for a training scheme that learns to\nmap an input bag to an output bag, including a novel loss function and neural\narchitecture. Our empirical evaluations on unsupervised sentiment transfer show\nthat our method performs substantially better than a standard autoencoder.", "published": "2021-10-13 19:30:40", "link": "http://arxiv.org/abs/2110.07002v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparison of SVD and factorized TDNN approaches for speech to text", "abstract": "This work concentrates on reducing the RTF and word error rate of a hybrid\nHMM-DNN. Our baseline system uses an architecture with TDNN and LSTM layers. We\nfind this architecture particularly useful for lightly reverberated\nenvironments. However, these models tend to demand more computation than is\ndesirable. In this work, we explore alternate architectures employing singular\nvalue decomposition (SVD) is applied to the TDNN layers to reduce the RTF, as\nwell as to the affine transforms of every LSTM cell. We compare this approach\nwith specifying bottleneck layers similar to those introduced by SVD before\ntraining. Additionally, we reduced the search space of the decoding graph to\nmake it a better fit to operate in real-time applications. We report -61.57%\nrelative reduction in RTF and almost 1% relative decrease in WER for our\narchitecture trained on Fisher data along with reverberated versions of this\ndataset in order to match one of our target test distributions.", "published": "2021-10-13 20:54:37", "link": "http://arxiv.org/abs/2110.07027v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving the Robustness to Variations of Objects and Instructions with\n  a Neuro-Symbolic Approach for Interactive Instruction Following", "abstract": "An interactive instruction following task has been proposed as a benchmark\nfor learning to map natural language instructions and first-person vision into\nsequences of actions to interact with objects in 3D environments. We found that\nan existing end-to-end neural model for this task tends to fail to interact\nwith objects of unseen attributes and follow various instructions. We assume\nthat this problem is caused by the high sensitivity of neural feature\nextraction to small changes in vision and language inputs. To mitigate this\nproblem, we propose a neuro-symbolic approach that utilizes high-level symbolic\nfeatures, which are robust to small changes in raw inputs, as intermediate\nrepresentations. We verify the effectiveness of our model with the subtask\nevaluation on the ALFRED benchmark. Our experiments show that our approach\nsignificantly outperforms the end-to-end neural model by 9, 46, and 74 points\nin the success rate on the ToggleObject, PickupObject, and SliceObject subtasks\nin unseen environments respectively.", "published": "2021-10-13 21:00:00", "link": "http://arxiv.org/abs/2110.07031v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Decision Attentive Regularization to Improve Simultaneous Speech\n  Translation Systems", "abstract": "Simultaneous translation systems start producing the output while processing\nthe partial source sentence in the incoming input stream. These systems need to\ndecide when to read more input and when to write the output. These decisions\ndepend on the structure of source/target language and the information contained\nin the partial input sequence. Hence, read/write decision policy remains the\nsame across different input modalities, i.e., speech and text. This motivates\nus to leverage the text transcripts corresponding to the speech input for\nimproving simultaneous speech-to-text translation (SimulST). We propose\nDecision Attentive Regularization (DAR) to improve the decision policy of\nSimulST systems by using the simultaneous text-to-text translation (SimulMT)\ntask. We also extend several techniques from the offline speech translation\ndomain to explore the role of SimulMT task in improving SimulST performance.\nOverall, we achieve 34.66% / 4.5 BLEU improvement over the baseline model\nacross different latency regimes for the MuST-C English-German (EnDe) SimulST\ntask.", "published": "2021-10-13 08:33:31", "link": "http://arxiv.org/abs/2110.15729v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Dawn of Quantum Natural Language Processing", "abstract": "In this paper, we discuss the initial attempts at boosting understanding\nhuman language based on deep-learning models with quantum computing. We\nsuccessfully train a quantum-enhanced Long Short-Term Memory network to perform\nthe parts-of-speech tagging task via numerical simulations. Moreover, a\nquantum-enhanced Transformer is proposed to perform the sentiment analysis\nbased on the existing dataset.", "published": "2021-10-13 05:46:57", "link": "http://arxiv.org/abs/2110.06510v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Independence-based Joint Dereverberation and Separation with Neural\n  Source Model", "abstract": "We propose an independence-based joint dereverberation and separation method\nwith a neural source model. We introduce a neural network in the framework of\ntime-decorrelation iterative source steering, which is an extension of\nindependent vector analysis to joint dereverberation and separation. The\nnetwork is trained in an end-to-end manner with a permutation invariant loss on\nthe time-domain separation output signals. Our proposed method can be applied\nin any situation with at least as many microphones as sources, regardless of\ntheir number. In experiments, we demonstrate that our method results in high\nperformance in terms of both speech quality metrics and word error rate (WER),\neven for mixtures with a different number of speakers than training.\nFurthermore, the model, trained on synthetic mixtures, without any\nmodifications, greatly reduces the WER on the recorded dataset LibriCSS.", "published": "2021-10-13 07:40:01", "link": "http://arxiv.org/abs/2110.06545v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Revisiting Speech Content Privacy", "abstract": "In this paper, we discuss an important aspect of speech privacy: protecting\nspoken content. New capabilities from the field of machine learning provide a\nunique and timely opportunity to revisit speech content protection. There are\nmany different applications of content privacy, even though this area has been\nunder-explored in speech technology research. This paper presents several\nscenarios that indicate a need for speech content privacy even as the specific\ntechniques to achieve content privacy may necessarily vary. Our discussion\nincludes several different types of content privacy including recoverable and\nnon-recoverable content. Finally, we introduce evaluation strategies as well as\ndescribe some of the difficulties that may be encountered.", "published": "2021-10-13 14:45:31", "link": "http://arxiv.org/abs/2110.06760v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Exploring the Importance of F0 Trajectories for Speaker Anonymization\n  using X-vectors and Neural Waveform Models", "abstract": "Voice conversion for speaker anonymization is an emerging field in speech\nprocessing research. Many state-of-the-art approaches are based on the\nresynthesis of the phoneme posteriorgrams (PPG), the fundamental frequency (F0)\nof the input signal together with modified X-vectors. Our research focuses on\nthe role of F0 for speaker anonymization, which is an understudied area.\nUtilizing the VoicePrivacy Challenge 2020 framework and its datasets we\ndeveloped and evaluated eight low-complexity F0 modifications prior\nresynthesis. We found that modifying the F0 can improve speaker anonymization\nby as much as 8% with minor word-error rate degradation.", "published": "2021-10-13 17:19:51", "link": "http://arxiv.org/abs/2110.06887v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "All-neural beamformer for continuous speech separation", "abstract": "Continuous speech separation (CSS) aims to separate overlapping voices from a\ncontinuous influx of conversational audio containing an unknown number of\nutterances spoken by an unknown number of speakers. A common application\nscenario is transcribing a meeting conversation recorded by a microphone array.\nPrior studies explored various deep learning models for time-frequency mask\nestimation, followed by a minimum variance distortionless response (MVDR)\nfilter to improve the automatic speech recognition (ASR) accuracy. The\nperformance of these methods is fundamentally upper-bounded by MVDR's spatial\nselectivity. Recently, the all deep learning MVDR (ADL-MVDR) model was proposed\nfor neural beamforming and demonstrated superior performance in a target speech\nextraction task using pre-segmented input. In this paper, we further adapt\nADL-MVDR to the CSS task with several enhancements to enable end-to-end neural\nbeamforming. The proposed system achieves significant word error rate reduction\nover a baseline spectral masking system on the LibriCSS dataset. Moreover, the\nproposed neural beamformer is shown to be comparable to a state-of-the-art\nMVDR-based system in real meeting transcription tasks, including AMI, while\nshowing potentials to further simplify the runtime implementation and reduce\nthe system latency with frame-wise processing.", "published": "2021-10-13 01:24:32", "link": "http://arxiv.org/abs/2110.06428v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music Source Separation with Deep Equilibrium Models", "abstract": "While deep neural network-based music source separation (MSS) is very\neffective and achieves high performance, its model size is often a problem for\npractical deployment. Deep implicit architectures such as deep equilibrium\nmodels (DEQ) were recently proposed, which can achieve higher performance than\ntheir explicit counterparts with limited depth while keeping the number of\nparameters small. This makes DEQ also attractive for MSS, especially as it was\noriginally applied to sequential modeling tasks in natural language processing\nand thus should in principle be also suited for MSS. However, an investigation\nof a good architecture and training scheme for MSS with DEQ is needed as the\ncharacteristics of acoustic signals are different from those of natural\nlanguage data. Hence, in this paper we propose an architecture and training\nscheme for MSS with DEQ. Starting with the architecture of Open-Unmix (UMX), we\nreplace its sequence model with DEQ. We refer to our proposed method as\nDEQ-based UMX (DEQ-UMX). Experimental results show that DEQ-UMX performs better\nthan the original UMX while reducing its number of parameters by 30%.", "published": "2021-10-13 04:34:46", "link": "http://arxiv.org/abs/2110.06494v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spatial Data Augmentation with Simulated Room Impulse Responses for\n  Sound Event Localization and Detection", "abstract": "Recording and annotating real sound events for a sound event localization and\ndetection (SELD) task is time consuming, and data augmentation techniques are\noften favored when the amount of data is limited. However, how to augment the\nspatial information in a dataset, including unlabeled directional interference\nevents, remains an open research question. Furthermore, directional\ninterference events make it difficult to accurately extract spatial\ncharacteristics from target sound events. To address this problem, we propose\nan impulse response simulation framework (IRS) that augments spatial\ncharacteristics using simulated room impulse responses (RIR). RIRs\ncorresponding to a microphone array assumed to be placed in various rooms are\naccurately simulated, and the source signals of the target sound events are\nextracted from a mixture. The simulated RIRs are then convolved with the\nextracted source signals to obtain an augmented multi-channel training dataset.\nEvaluation results obtained using the TAU-NIGENS Spatial Sound Events 2021\ndataset show that the IRS contributes to improving the overall SELD\nperformance. Additionally, we conducted an ablation study to discuss the\ncontribution and need for each component within the IRS.", "published": "2021-10-13 05:20:50", "link": "http://arxiv.org/abs/2110.06501v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Simple Attention Module based Speaker Verification with Iterative noisy\n  label detection", "abstract": "Recently, the attention mechanism such as squeeze-and-excitation module (SE)\nand convolutional block attention module (CBAM) has achieved great success in\ndeep learning-based speaker verification system. This paper introduces an\nalternative effective yet simple one, i.e., simple attention module (SimAM),\nfor speaker verification. The SimAM module is a plug-and-play module without\nextra modal parameters. In addition, we propose a noisy label detection method\nto iteratively filter out the data samples with a noisy label from the training\ndata, considering that a large-scale dataset labeled with human annotation or\nother automated processes may contain noisy labels. Data with the noisy label\nmay over parameterize a deep neural network (DNN) and result in a performance\ndrop due to the memorization effect of the DNN. Experiments are conducted on\nVoxCeleb dataset. The speaker verification model with SimAM achieves the 0.675%\nequal error rate (EER) on VoxCeleb1 original test trials. Our proposed\niterative noisy label detection method further reduces the EER to 0.643%.", "published": "2021-10-13 07:00:20", "link": "http://arxiv.org/abs/2110.06534v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Duality Temporal-channel-frequency Attention Enhanced Speaker\n  Representation Learning", "abstract": "The use of channel-wise attention in CNN based speaker representation\nnetworks has achieved remarkable performance in speaker verification (SV). But\nthese approaches do simple averaging on time and frequency feature maps before\nchannel-wise attention learning and ignore the essential mutual interaction\namong temporal, channel as well as frequency scales. To address this problem,\nwe propose the Duality Temporal-Channel-Frequency (DTCF) attention to\nre-calibrate the channel-wise features with aggregation of global context on\ntemporal and frequency dimensions. Specifically, the duality attention -\ntime-channel (T-C) attention as well as frequency-channel (F-C) attention -\naims to focus on salient regions along the T-C and F-C feature maps that may\nhave more considerable impact on the global context, leading to more\ndiscriminative speaker representations. We evaluate the effectiveness of the\nproposed DTCF attention on the CN-Celeb and VoxCeleb datasets. On the CN-Celeb\nevaluation set, the EER/minDCF of ResNet34-DTCF are reduced by 0.63%/0.0718\ncompared with those of ResNet34-SE. On VoxCeleb1-O, VoxCeleb1-E and VoxCeleb1-H\nevaluation sets, the EER/minDCF of ResNet34-DTCF achieve 0.36%/0.0263,\n0.39%/0.0382 and 0.74%/0.0753 reductions compared with those of ResNet34-SE.", "published": "2021-10-13 08:27:09", "link": "http://arxiv.org/abs/2110.06565v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diverse Audio Captioning via Adversarial Training", "abstract": "Audio captioning aims at generating natural language descriptions for audio\nclips automatically. Existing audio captioning models have shown promising\nimprovement in recent years. However, these models are mostly trained via\nmaximum likelihood estimation (MLE),which tends to make captions generic,\nsimple and deterministic. As different people may describe an audio clip from\ndifferent aspects using distinct words and grammars, we argue that an audio\ncaptioning system should have the ability to generate diverse captions for a\nfixed audio clip and across similar audio clips. To address this problem, we\npropose an adversarial training framework for audio captioning based on a\nconditional generative adversarial network (C-GAN), which aims at improving the\nnaturalness and diversity of generated captions. Unlike processing data of\ncontinuous values in a classical GAN, a sentence is composed of discrete tokens\nand the discrete sampling process is non-differentiable. To address this issue,\npolicy gradient, a reinforcement learning technique, is used to back-propagate\nthe reward to the generator. The results show that our proposed model can\ngenerate more diverse captions, as compared to state-of-the-art methods.", "published": "2021-10-13 13:03:08", "link": "http://arxiv.org/abs/2110.06691v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DeepA: A Deep Neural Analyzer For Speech And Singing Vocoding", "abstract": "Conventional vocoders are commonly used as analysis tools to provide\ninterpretable features for downstream tasks such as speech synthesis and voice\nconversion. They are built under certain assumptions about the signals\nfollowing signal processing principle, therefore, not easily generalizable to\ndifferent audio, for example, from speech to singing. In this paper, we propose\na deep neural analyzer, denoted as DeepA - a neural vocoder that extracts F0\nand timbre/aperiodicity encoding from the input speech that emulate those\ndefined in conventional vocoders. Therefore, the resulting parameters are more\ninterpretable than other latent neural representations. At the same time, as\nthe deep neural analyzer is learnable, it is expected to be more accurate for\nsignal reconstruction and manipulation, and generalizable from speech to\nsinging. The proposed neural analyzer is built based on a variational\nautoencoder (VAE) architecture. We show that DeepA improves F0 estimation over\nthe conventional vocoder (WORLD). To our best knowledge, this is the first\nstudy dedicated to the development of a neural framework for extracting\nlearnable vocoder-like parameters.", "published": "2021-10-13 01:39:57", "link": "http://arxiv.org/abs/2110.06434v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SDR -- Medium Rare with Fast Computations", "abstract": "We revisit the widely used bss eval metrics for source separation with an eye\nout for performance. We propose a fast algorithm fixing shortcomings of\npublicly available implementations. First, we show that the metrics are fully\nspecified by the squared cosine of just two angles between estimate and\nreference subspaces. Second, large linear systems are involved. However, they\nare structured, and we apply a fast iterative method based on conjugate\ngradient descent. The complexity of this step is thus reduced by a factor\nquadratic in the distortion filter size used in bss eval, usually 512. In\nexperiments, we assess speed and numerical accuracy. Not only is the loss of\naccuracy due to the approximate solver acceptable for most applications, but\nthe speed-up is up to two orders of magnitude in some, not so extreme, cases.\nWe confirm that our implementation can train neural networks, and find that\nlonger distortion filters may be beneficial.", "published": "2021-10-13 01:57:44", "link": "http://arxiv.org/abs/2110.06440v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Dual-branch Attention-In-Attention Transformer for single-channel speech\n  enhancement", "abstract": "Curriculum learning begins to thrive in the speech enhancement area, which\ndecouples the original spectrum estimation task into multiple easier sub-tasks\nto achieve better performance. Motivated by that, we propose a dual-branch\nattention-in-attention transformer dubbed DB-AIAT to handle both coarse- and\nfine-grained regions of the spectrum in parallel. From a complementary\nperspective, a magnitude masking branch is proposed to coarsely estimate the\noverall magnitude spectrum, and simultaneously a complex refining branch is\nelaborately designed to compensate for the missing spectral details and\nimplicitly derive phase information. Within each branch, we propose a novel\nattention-in-attention transformer-based module to replace the conventional\nRNNs and temporal convolutional networks for temporal sequence modeling.\nSpecifically, the proposed attention-in-attention transformer consists of\nadaptive temporal-frequency attention transformer blocks and an adaptive\nhierarchical attention module, aiming to capture long-term temporal-frequency\ndependencies and further aggregate global hierarchical contextual information.\nExperimental results on Voice Bank + DEMAND demonstrate that DB-AIAT yields\nstate-of-the-art performance (e.g., 3.31 PESQ, 95.6% STOI and 10.79dB SSNR)\nover previous advanced systems with a relatively small model size (2.81M).", "published": "2021-10-13 03:03:49", "link": "http://arxiv.org/abs/2110.06467v5", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic DJ Transitions with Differentiable Audio Effects and\n  Generative Adversarial Networks", "abstract": "A central task of a Disc Jockey (DJ) is to create a mixset of mu-sic with\nseamless transitions between adjacent tracks. In this paper, we explore a\ndata-driven approach that uses a generative adversarial network to create the\nsong transition by learning from real-world DJ mixes. In particular, the\ngenerator of the model uses two differentiable digital signal processing\ncomponents, an equalizer (EQ) and a fader, to mix two tracks selected by a data\ngeneration pipeline. The generator has to set the parameters of the EQs and\nfader in such away that the resulting mix resembles real mixes created by\nhumanDJ, as judged by the discriminator counterpart. Result of a listening test\nshows that the model can achieve competitive results compared with a number of\nbaselines.", "published": "2021-10-13 06:25:52", "link": "http://arxiv.org/abs/2110.06525v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EIHW-MTG DiCOVA 2021 Challenge System Report", "abstract": "This paper aims to automatically detect COVID-19 patients by analysing the\nacoustic information embedded in coughs. COVID-19 affects the respiratory\nsystem, and, consequently, respiratory-related signals have the potential to\ncontain salient information for the task at hand. We focus on analysing the\nspectrogram representations of coughing samples with the aim to investigate\nwhether COVID-19 alters the frequency content of these signals. Furthermore,\nthis work also assesses the impact of gender in the automatic detection of\nCOVID-19. To extract deep learnt representations of the spectrograms, we\ncompare the performance of a cough-specific, and a Resnet18 pre-trained\nConvolutional Neural Network (CNN). Additionally, our approach explores the use\nof contextual attention, so the model can learn to highlight the most relevant\ndeep learnt features extracted by the CNN. We conduct our experiments on the\ndataset released for the Cough Sound Track of the DiCOVA 2021 Challenge. The\nbest performance on the test set is obtained using the Resnet18 pre-trained CNN\nwith contextual attention, which scored an Area Under the Curve (AUC) of 70.91\nat 80% sensitivity.", "published": "2021-10-13 07:38:54", "link": "http://arxiv.org/abs/2110.06543v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Melody-Unsupervision Model for Singing Voice Synthesis", "abstract": "Recent studies in singing voice synthesis have achieved high-quality results\nleveraging advances in text-to-speech models based on deep neural networks. One\nof the main issues in training singing voice synthesis models is that they\nrequire melody and lyric labels to be temporally aligned with audio data. The\ntemporal alignment is a time-exhausting manual work in preparing for the\ntraining data. To address the issue, we propose a melody-unsupervision model\nthat requires only audio-and-lyrics pairs without temporal alignment in\ntraining time but generates singing voice audio given a melody and lyrics input\nin inference time. The proposed model is composed of a phoneme classifier and a\nsinging voice generator jointly trained in an end-to-end manner. The model can\nbe fine-tuned by adjusting the amount of supervision with temporally aligned\nmelody labels. Through experiments in melody-unsupervision and semi-supervision\nsettings, we compare the audio quality of synthesized singing voice. We also\nshow that the proposed model is capable of being trained with speech audio and\ntext labels but can generate singing voice in inference time.", "published": "2021-10-13 07:42:35", "link": "http://arxiv.org/abs/2110.06546v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Singer separation for karaoke content generation", "abstract": "Due to the rapid development of deep learning, we can now successfully\nseparate singing voice from mono audio music. However, this separation can only\nextract human voices from other musical instruments, which is undesirable for\nkaraoke content generation applications that only require the separation of\nlead singers. For this karaoke application, we need to separate the music\ncontaining male and female duets into two vocals, or extract a single lead\nvocal from the music containing vocal harmony. For this reason, we propose in\nthis article to use a singer separation system, which generates karaoke content\nfor one or two separated lead singers. In particular, we introduced three\nmodels for the singer separation task and designed an automatic model selection\nscheme to distinguish how many lead singers are in the song. We also collected\na large enough data set, MIR-SingerSeparation, which has been publicly released\nto advance the frontier of this research. Our singer separation is most\nsuitable for sentimental ballads and can be directly applied to karaoke content\ngeneration. As far as we know, this is the first singer-separation work for\nreal-world karaoke applications.", "published": "2021-10-13 13:30:54", "link": "http://arxiv.org/abs/2110.06707v4", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Study of positional encoding approaches for Audio Spectrogram\n  Transformers", "abstract": "Transformers have revolutionized the world of deep learning, specially in the\nfield of natural language processing. Recently, the Audio Spectrogram\nTransformer (AST) was proposed for audio classification, leading to state of\nthe art results in several datasets. However, in order for ASTs to outperform\nCNNs, pretraining with ImageNet is needed. In this paper, we study one\ncomponent of the AST, the positional encoding, and propose several variants to\nimprove the performance of ASTs trained from scratch, without ImageNet\npretraining. Our best model, which incorporates conditional positional\nencodings, significantly improves performance on Audioset and ESC-50 compared\nto the original AST.", "published": "2021-10-13 19:20:20", "link": "http://arxiv.org/abs/2110.06999v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
