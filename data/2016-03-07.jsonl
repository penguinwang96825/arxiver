{"title": "A matter of words: NLP for quality evaluation of Wikipedia medical\n  articles", "abstract": "Automatic quality evaluation of Web information is a task with many fields of\napplications and of great relevance, especially in critical domains like the\nmedical one. We move from the intuition that the quality of content of medical\nWeb documents is affected by features related with the specific domain. First,\nthe usage of a specific vocabulary (Domain Informativeness); then, the adoption\nof specific codes (like those used in the infoboxes of Wikipedia articles) and\nthe type of document (e.g., historical and technical ones). In this paper, we\npropose to leverage specific domain features to improve the results of the\nevaluation of Wikipedia medical articles. In particular, we evaluate the\narticles adopting an \"actionable\" model, whose features are related to the\ncontent of the articles, so that the model can also directly suggest strategies\nfor improving a given article quality. We rely on Natural Language Processing\n(NLP) and dictionaries-based techniques in order to extract the bio-medical\nconcepts in a text. We prove the effectiveness of our approach by classifying\nthe medical articles of the Wikipedia Medicine Portal, which have been\npreviously manually labeled by the Wiki Project team. The results of our\nexperiments confirm that, by considering domain-oriented features, it is\npossible to obtain sensible improvements with respect to existing solutions,\nmainly for those articles that other approaches have less correctly classified.\nOther than being interesting by their own, the results call for further\nresearch in the area of domain specific features suitable for Web data quality\nassessment.", "published": "2016-03-07 09:54:11", "link": "http://arxiv.org/abs/1603.01987v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Latent Variable Recurrent Neural Network for Discourse Relation\n  Language Models", "abstract": "This paper presents a novel latent variable recurrent neural network\narchitecture for jointly modeling sequences of words and (possibly latent)\ndiscourse relations between adjacent sentences. A recurrent neural network\ngenerates individual words, thus reaping the benefits of\ndiscriminatively-trained vector representations. The discourse relations are\nrepresented with a latent variable, which can be predicted or marginalized,\ndepending on the task. The resulting model can therefore employ a training\nobjective that includes not only discourse relation classification, but also\nword prediction. As a result, it outperforms state-of-the-art alternatives for\ntwo tasks: implicit discourse relation classification in the Penn Discourse\nTreebank, and dialog act classification in the Switchboard corpus. Furthermore,\nby marginalizing over latent discourse relations at test time, we obtain a\ndiscourse informed language model, which improves over a strong LSTM baseline.", "published": "2016-03-07 01:54:56", "link": "http://arxiv.org/abs/1603.01913v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
