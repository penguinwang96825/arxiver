{"title": "End-to-end Emotion-Cause Pair Extraction via Learning to Link", "abstract": "Emotion-cause pair extraction (ECPE), as an emergent natural language\nprocessing task, aims at jointly investigating emotions and their underlying\ncauses in documents. It extends the previous emotion cause extraction (ECE)\ntask, yet without requiring a set of pre-given emotion clauses as in ECE.\nExisting approaches to ECPE generally adopt a two-stage method, i.e., (1)\nemotion and cause detection, and then (2) pairing the detected emotions and\ncauses. Such pipeline method, while intuitive, suffers from two critical\nissues, including error propagation across stages that may hinder the\neffectiveness, and high computational cost that would limit the practical\napplication of the method. To tackle these issues, we propose a multi-task\nlearning model that can extract emotions, causes and emotion-cause pairs\nsimultaneously in an end-to-end manner. Specifically, our model regards pair\nextraction as a link prediction task, and learns to link from emotion clauses\nto cause clauses, i.e., the links are directional. Emotion extraction and cause\nextraction are incorporated into the model as auxiliary tasks, which further\nboost the pair extraction. Experiments are conducted on an ECPE benchmarking\ndataset. The results show that our proposed model outperforms a range of\nstate-of-the-art approaches.", "published": "2020-02-25 07:49:12", "link": "http://arxiv.org/abs/2002.10710v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Edge-Enhanced Graph Convolution Networks for Event Detection with\n  Syntactic Relation", "abstract": "Event detection (ED), a key subtask of information extraction, aims to\nrecognize instances of specific event types in text. Previous studies on the\ntask have verified the effectiveness of integrating syntactic dependency into\ngraph convolutional networks. However, these methods usually ignore dependency\nlabel information, which conveys rich and useful linguistic knowledge for ED.\nIn this paper, we propose a novel architecture named Edge-Enhanced Graph\nConvolution Networks (EE-GCN), which simultaneously exploits syntactic\nstructure and typed dependency label information to perform ED. Specifically,\nan edge-aware node update module is designed to generate expressive word\nrepresentations by aggregating syntactically-connected words through specific\ndependency types. Furthermore, to fully explore clues hidden in dependency\nedges, a node-aware edge update module is introduced, which refines the\nrelation representations with contextual information. These two modules are\ncomplementary to each other and work in a mutual promotion way. We conduct\nexperiments on the widely used ACE2005 dataset and the results show significant\nimprovement over competitive baseline methods.", "published": "2020-02-25 09:18:26", "link": "http://arxiv.org/abs/2002.10757v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuST-Cinema: a Speech-to-Subtitles corpus", "abstract": "Growing needs in localising audiovisual content in multiple languages through\nsubtitles call for the development of automatic solutions for human subtitling.\nNeural Machine Translation (NMT) can contribute to the automatisation of\nsubtitling, facilitating the work of human subtitlers and reducing turn-around\ntimes and related costs. NMT requires high-quality, large, task-specific\ntraining data. The existing subtitling corpora, however, are missing both\nalignments to the source language audio and important information about\nsubtitle breaks. This poses a significant limitation for developing efficient\nautomatic approaches for subtitling, since the length and form of a subtitle\ndirectly depends on the duration of the utterance. In this work, we present\nMuST-Cinema, a multilingual speech translation corpus built from TED subtitles.\nThe corpus is comprised of (audio, transcription, translation) triplets.\nSubtitle breaks are preserved by inserting special symbols. We show that the\ncorpus can be used to build models that efficiently segment sentences into\nsubtitles and propose a method for annotating existing subtitling corpora with\nsubtitle breaks, conforming to the constraint of length.", "published": "2020-02-25 12:40:06", "link": "http://arxiv.org/abs/2002.10829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small-Footprint Open-Vocabulary Keyword Spotting with Quantized LSTM\n  Networks", "abstract": "We explore a keyword-based spoken language understanding system, in which the\nintent of the user can directly be derived from the detection of a sequence of\nkeywords in the query. In this paper, we focus on an open-vocabulary keyword\nspotting method, allowing the user to define their own keywords without having\nto retrain the whole model. We describe the different design choices leading to\na fast and small-footprint system, able to run on tiny devices, for any\narbitrary set of user-defined keywords, without training data specific to those\nkeywords. The model, based on a quantized long short-term memory (LSTM) neural\nnetwork, trained with connectionist temporal classification (CTC), weighs less\nthan 500KB. Our approach takes advantage of some properties of the predictions\nof CTC-trained networks to calibrate the confidence scores and implement a fast\ndetection algorithm. The proposed system outperforms a standard keyword-filler\nmodel approach.", "published": "2020-02-25 13:27:31", "link": "http://arxiv.org/abs/2002.10851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KEML: A Knowledge-Enriched Meta-Learning Framework for Lexical Relation\n  Classification", "abstract": "Lexical relations describe how concepts are semantically related, in the form\nof relation triples. The accurate prediction of lexical relations between\nconcepts is challenging, due to the sparsity of patterns indicating the\nexistence of such relations. We propose the Knowledge-Enriched Meta-Learning\n(KEML) framework to address the task of lexical relation classification. In\nKEML, the LKB-BERT (Lexical Knowledge Base-BERT) model is presented to learn\nconcept representations from massive text corpora, with rich lexical knowledge\ninjected by distant supervision. A probabilistic distribution of auxiliary\ntasks is defined to increase the model's ability to recognize different types\nof lexical relations. We further combine a meta-learning process over the\nauxiliary task distribution and supervised learning to train the neural lexical\nrelation classifier. Experiments over multiple datasets show that KEML\noutperforms state-of-the-art methods.", "published": "2020-02-25 14:43:56", "link": "http://arxiv.org/abs/2002.10903v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Asks in SE attacks: Impact of Linguistic and Structural\n  Knowledge", "abstract": "Social engineers attempt to manipulate users into undertaking actions such as\ndownloading malware by clicking links or providing access to money or sensitive\ninformation. Natural language processing, computational sociolinguistics, and\nmedia-specific structural clues provide a means for detecting both the ask\n(e.g., buy gift card) and the risk/reward implied by the ask, which we call\nframing (e.g., lose your job, get a raise). We apply linguistic resources such\nas Lexical Conceptual Structure to tackle ask detection and also leverage\nstructural clues such as links and their proximity to identified asks to\nimprove confidence in our results. Our experiments indicate that the\nperformance of ask detection, framing detection, and identification of the top\nask is improved by linguistically motivated classes coupled with structural\nclues such as links. Our approach is implemented in a system that informs users\nabout social engineering risk situations.", "published": "2020-02-25 15:05:06", "link": "http://arxiv.org/abs/2002.10931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression\n  of Pre-Trained Transformers", "abstract": "Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its\nvariants) have achieved remarkable success in varieties of NLP tasks. However,\nthese models usually consist of hundreds of millions of parameters which brings\nchallenges for fine-tuning and online serving in real-life applications due to\nlatency and capacity constraints. In this work, we present a simple and\neffective approach to compress large Transformer (Vaswani et al., 2017) based\npre-trained models, termed as deep self-attention distillation. The small model\n(student) is trained by deeply mimicking the self-attention module, which plays\na vital role in Transformer networks, of the large model (teacher).\nSpecifically, we propose distilling the self-attention module of the last\nTransformer layer of the teacher, which is effective and flexible for the\nstudent. Furthermore, we introduce the scaled dot-product between values in the\nself-attention module as the new deep self-attention knowledge, in addition to\nthe attention distributions (i.e., the scaled dot-product of queries and keys)\nthat have been used in existing works. Moreover, we show that introducing a\nteacher assistant (Mirzadeh et al., 2019) also helps the distillation of large\npre-trained Transformer models. Experimental results demonstrate that our\nmonolingual model outperforms state-of-the-art baselines in different parameter\nsize of student models. In particular, it retains more than 99% accuracy on\nSQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer\nparameters and computations of the teacher model. We also obtain competitive\nresults in applying deep self-attention distillation to multilingual\npre-trained models.", "published": "2020-02-25 15:21:10", "link": "http://arxiv.org/abs/2002.10957v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A more abstractive summarization model", "abstract": "Pointer-generator network is an extremely popular method of text\nsummarization. More recent works in this domain still build on top of the\nbaseline pointer generator by augmenting a content selection phase, or by\ndecomposing the decoder into a contextual network and a language model.\nHowever, all such models that are based on the pointer-generator base\narchitecture cannot generate novel words in the summary and mostly copy words\nfrom the source text. In our work, we first thoroughly investigate why the\npointer-generator network is unable to generate novel words, and then address\nthat by adding an Out-of-vocabulary (OOV) penalty. This enables us to improve\nthe amount of novelty/abstraction significantly. We use normalized n-gram\nnovelty scores as a metric for determining the level of abstraction. Moreover,\nwe also report rouge scores of our model since most summarization models are\nevaluated with R-1, R-2, R-L scores.", "published": "2020-02-25 15:22:23", "link": "http://arxiv.org/abs/2002.10959v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Relatedness for Keyword Disambiguation: Exploiting Different\n  Embeddings", "abstract": "Understanding the meaning of words is crucial for many tasks that involve\nhuman-machine interaction. This has been tackled by research in Word Sense\nDisambiguation (WSD) in the Natural Language Processing (NLP) field. Recently,\nWSD and many other NLP tasks have taken advantage of embeddings-based\nrepresentation of words, sentences, and documents. However, when it comes to\nWSD, most embeddings models suffer from ambiguity as they do not capture the\ndifferent possible meanings of the words. Even when they do, the list of\npossible meanings for a word (sense inventory) has to be known in advance at\ntraining time to be included in the embeddings space. Unfortunately, there are\nsituations in which such a sense inventory is not known in advance (e.g., an\nontology selected at run-time), or it evolves with time and its status diverges\nfrom the one at training time. This hampers the use of embeddings models for\nWSD. Furthermore, traditional WSD techniques do not perform well in situations\nin which the available linguistic information is very scarce, such as the case\nof keyword-based queries. In this paper, we propose an approach to keyword\ndisambiguation which grounds on a semantic relatedness between words and senses\nprovided by an external inventory (ontology) that is not known at training\ntime. Building on previous works, we present a semantic relatedness measure\nthat uses word embeddings, and explore different disambiguation algorithms to\nalso exploit both word and sentence representations. Experimental results show\nthat this approach achieves results comparable with the state of the art when\napplied for WSD, without training for a particular domain.", "published": "2020-02-25 16:44:50", "link": "http://arxiv.org/abs/2002.11023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Entity Linking and Disambiguation leveraging Word and\n  Knowledge Graph Embeddings", "abstract": "Entity linking - connecting entity mentions in a natural language utterance\nto knowledge graph (KG) entities is a crucial step for question answering over\nKGs. It is often based on measuring the string similarity between the entity\nlabel and its mention in the question. The relation referred to in the question\ncan help to disambiguate between entities with the same label. This can be\nmisleading if an incorrect relation has been identified in the relation linking\nstep. However, an incorrect relation may still be semantically similar to the\nrelation in which the correct entity forms a triple within the KG; which could\nbe captured by the similarity of their KG embeddings. Based on this idea, we\npropose the first end-to-end neural network approach that employs KG as well as\nword embeddings to perform joint relation and entity classification of simple\nquestions while implicitly performing entity disambiguation with the help of a\nnovel gating mechanism. An empirical evaluation shows that the proposed\napproach achieves a performance comparable to state-of-the-art entity linking\nwhile requiring less post-processing.", "published": "2020-02-25 19:07:54", "link": "http://arxiv.org/abs/2002.11143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Differentiable Reasoning over a Virtual Knowledge Base", "abstract": "We consider the task of answering complex multi-hop questions using a corpus\nas a virtual knowledge base (KB). In particular, we describe a neural module,\nDrKIT, that traverses textual data like a KB, softly following paths of\nrelations between mentions of entities in the corpus. At each step the module\nuses a combination of sparse-matrix TFIDF indices and a maximum inner product\nsearch (MIPS) on a special index of contextual representations of the mentions.\nThis module is differentiable, so the full system can be trained end-to-end\nusing gradient based methods, starting from natural language inputs. We also\ndescribe a pretraining scheme for the contextual representation encoder by\ngenerating hard negative examples using existing knowledge bases. We show that\nDrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset,\ncutting the gap between text-based and KB-based state-of-the-art by 70%. On\nHotpotQA, DrKIT leads to a 10% improvement over a BERT-based re-ranking\napproach to retrieving the relevant passages required to answer a question.\nDrKIT is also very efficient, processing 10-100x more queries per second than\nexisting multi-hop systems.", "published": "2020-02-25 03:13:32", "link": "http://arxiv.org/abs/2002.10640v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Declarative Memory-based Structure for the Representation of Text Data", "abstract": "In the era of intelligent computing, computational progress in text\nprocessing is an essential consideration. Many systems have been developed to\nprocess text over different languages. Though, there is considerable\ndevelopment, they still lack in understanding of the text, i.e., instead of\nkeeping text as knowledge, many treat text as a data. In this work we introduce\na text representation scheme which is influenced by human memory\ninfrastructure. Since texts are declarative in nature, a structural\norganization would foster efficient computation over text. We exploit long term\nepisodic memory to keep text information observed over time. This not only keep\nfragments of text in an organized fashion but also reduces redundancy and\nstores the temporal relation among them. Wordnet has been used to imitate\nsemantic memory, which works at word level to facilitate the understanding\nabout individual words within text. Experimental results of various operation\nperformed over episodic memory and growth of knowledge infrastructure over time\nis reported.", "published": "2020-02-25 04:56:47", "link": "http://arxiv.org/abs/2002.10665v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Label-guided Learning for Text Classification", "abstract": "Text classification is one of the most important and fundamental tasks in\nnatural language processing. Performance of this task mainly dependents on text\nrepresentation learning. Currently, most existing learning frameworks mainly\nfocus on encoding local contextual information between words. These methods\nalways neglect to exploit global clues, such as label information, for encoding\ntext information. In this study, we propose a label-guided learning framework\nLguidedLearn for text representation and classification. Our method is novel\nbut simple that we only insert a label-guided encoding layer into the commonly\nused text representation learning schemas. That label-guided layer performs\nlabel-based attentive encoding to map the universal text embedding (encoded by\na contextual information learner) into different label spaces, resulting in\nlabel-wise embeddings. In our proposed framework, the label-guided layer can be\neasily and directly applied with a contextual encoding method to perform\njointly learning. Text information is encoded based on both the local\ncontextual information and the global label clues. Therefore, the obtained text\nembeddings are more robust and discriminative for text classification.\nExtensive experiments are conducted on benchmark datasets to illustrate the\neffectiveness of our proposed method.", "published": "2020-02-25 10:05:56", "link": "http://arxiv.org/abs/2002.10772v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Abstractive Snippet Generation", "abstract": "An abstractive snippet is an originally created piece of text to summarize a\nweb page on a search engine results page. Compared to the conventional\nextractive snippets, which are generated by extracting phrases and sentences\nverbatim from a web page, abstractive snippets circumvent copyright issues;\neven more interesting is the fact that they open the door for personalization.\nAbstractive snippets have been evaluated as equally powerful in terms of user\nacceptance and expressiveness---but the key question remains: Can abstractive\nsnippets be automatically generated with sufficient quality?\n  This paper introduces a new approach to abstractive snippet generation: We\nidentify the first two large-scale sources for distant supervision, namely\nanchor contexts and web directories. By mining the entire ClueWeb09 and\nClueWeb12 for anchor contexts and by utilizing the DMOZ Open Directory Project,\nwe compile the Webis Abstractive Snippet Corpus 2020, comprising more than 3.5\nmillion triples of the form $\\langle$query, snippet, document$\\rangle$ as\ntraining examples, where the snippet is either an anchor context or a web\ndirectory description in lieu of a genuine query-biased abstractive snippet of\nthe web document. We propose a bidirectional abstractive snippet generation\nmodel and assess the quality of both our corpus and the generated abstractive\nsnippets with standard measures, crowdsourcing, and in comparison to the state\nof the art. The evaluation shows that our novel data sources along with the\nproposed model allow for producing usable query-biased abstractive snippets\nwhile minimizing text reuse.", "published": "2020-02-25 10:36:17", "link": "http://arxiv.org/abs/2002.10782v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via\n  Pre-training", "abstract": "Learning to navigate in a visual environment following natural-language\ninstructions is a challenging task, because the multimodal inputs to the agent\nare highly variable, and the training data on a new task is often limited. In\nthis paper, we present the first pre-training and fine-tuning paradigm for\nvision-and-language navigation (VLN) tasks. By training on a large amount of\nimage-text-action triplets in a self-supervised learning manner, the\npre-trained model provides generic representations of visual environments and\nlanguage instructions. It can be easily used as a drop-in for existing VLN\nframeworks, leading to the proposed agent called Prevalent. It learns more\neffectively in new tasks and generalizes better in a previously unseen\nenvironment. The performance is validated on three VLN tasks. On the\nRoom-to-Room benchmark, our model improves the state-of-the-art from 47% to 51%\non success rate weighted by path length. Further, the learned representation is\ntransferable to other VLN tasks. On two recent tasks, vision-and-dialog\nnavigation and \"Help, Anna!\" the proposed Prevalent leads to significant\nimprovement over existing methods, achieving a new state of the art.", "published": "2020-02-25 03:08:12", "link": "http://arxiv.org/abs/2002.10638v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Exploring BERT Parameter Efficiency on the Stanford Question Answering\n  Dataset v2.0", "abstract": "In this paper we explore the parameter efficiency of BERT arXiv:1810.04805 on\nversion 2.0 of the Stanford Question Answering dataset (SQuAD2.0). We evaluate\nthe parameter efficiency of BERT while freezing a varying number of final\ntransformer layers as well as including the adapter layers proposed in\narXiv:1902.00751. Additionally, we experiment with the use of context-aware\nconvolutional (CACNN) filters, as described in arXiv:1709.08294v3, as a final\naugmentation layer for the SQuAD2.0 tasks.\n  This exploration is motivated in part by arXiv:1907.10597, which made a\ncompelling case for broadening the evaluation criteria of artificial\nintelligence models to include various measures of resource efficiency. While\nwe do not evaluate these models based on their floating point operation\nefficiency as proposed in arXiv:1907.10597, we examine efficiency with respect\nto training time, inference time, and total number of model parameters. Our\nresults largely corroborate those of arXiv:1902.00751 for adapter modules,\nwhile also demonstrating that gains in F1 score from adding context-aware\nconvolutional filters are not practical due to the increase in training and\ninference time.", "published": "2020-02-25 05:09:48", "link": "http://arxiv.org/abs/2002.10670v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge", "abstract": "Audio-Visual Scene-Aware Dialog (AVSD) is an extension from Video Question\nAnswering (QA) whereby the dialogue agent is required to generate natural\nlanguage responses to address user queries and carry on conversations. This is\na challenging task as it consists of video features of multiple modalities,\nincluding text, visual, and audio features. The agent also needs to learn\nsemantic dependencies among user utterances and system responses to make\ncoherent conversations with humans. In this work, we describe our submission to\nthe AVSD track of the 8th Dialogue System Technology Challenge. We adopt\ndot-product attention to combine text and non-text features of input video. We\nfurther enhance the generation capability of the dialogue agent by adopting\npointer networks to point to tokens from multiple source sequences in each\ngeneration step. Our systems achieve high performance in automatic metrics and\nobtain 5th and 6th place in human evaluation among all submissions.", "published": "2020-02-25 06:41:07", "link": "http://arxiv.org/abs/2002.10695v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What BERT Sees: Cross-Modal Transfer for Visual Question Generation", "abstract": "Pre-trained language models have recently contributed to significant advances\nin NLP tasks. Recently, multi-modal versions of BERT have been developed, using\nheavy pre-training relying on vast corpora of aligned textual and image data,\nprimarily applied to classification tasks such as VQA. In this paper, we are\ninterested in evaluating the visual capabilities of BERT out-of-the-box, by\navoiding pre-training made on supplementary data. We choose to study Visual\nQuestion Generation, a task of great interest for grounded dialog, that enables\nto study the impact of each modality (as input can be visual and/or textual).\nMoreover, the generation aspect of the task requires an adaptation since BERT\nis primarily designed as an encoder. We introduce BERT-gen, a BERT-based\narchitecture for text generation, able to leverage on either mono- or multi-\nmodal representations. The results reported under different configurations\nindicate an innate capacity for BERT-gen to adapt to multi-modal data and text\ngeneration, even with few data available, avoiding expensive pre-training. The\nproposed model obtains substantial improvements over the state-of-the-art on\ntwo established VQG datasets.", "published": "2020-02-25 12:44:36", "link": "http://arxiv.org/abs/2002.10832v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diversity-Based Generalization for Unsupervised Text Classification\n  under Domain Shift", "abstract": "Domain adaptation approaches seek to learn from a source domain and\ngeneralize it to an unseen target domain. At present, the state-of-the-art\nunsupervised domain adaptation approaches for subjective text classification\nproblems leverage unlabeled target data along with labeled source data. In this\npaper, we propose a novel method for domain adaptation of single-task text\nclassification problems based on a simple but effective idea of diversity-based\ngeneralization that does not require unlabeled target data but still matches\nthe state-of-the-art in performance. Diversity plays the role of promoting the\nmodel to better generalize and be indiscriminate towards domain shift by\nforcing the model not to rely on same features for prediction. We apply this\nconcept on the most explainable component of neural networks, the attention\nlayer. To generate sufficient diversity, we create a multi-head attention model\nand infuse a diversity constraint between the attention heads such that each\nhead will learn differently. We further expand upon our model by tri-training\nand designing a procedure with an additional diversity constraint between the\nattention heads of the tri-trained classifiers. Extensive evaluation using the\nstandard benchmark dataset of Amazon reviews and a newly constructed dataset of\nCrisis events shows that our fully unsupervised method matches with the\ncompeting baselines that uses unlabeled target data. Our results demonstrate\nthat machine learning architectures that ensure sufficient diversity can\ngeneralize better; encouraging future research to design ubiquitously usable\nlearning models without using unlabeled target data.", "published": "2020-02-25 15:11:02", "link": "http://arxiv.org/abs/2002.10937v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Language-Independent Tokenisation Rivals Language-Specific Tokenisation\n  for Word Similarity Prediction", "abstract": "Language-independent tokenisation (LIT) methods that do not require labelled\nlanguage resources or lexicons have recently gained popularity because of their\napplicability in resource-poor languages. Moreover, they compactly represent a\nlanguage using a fixed size vocabulary and can efficiently handle unseen or\nrare words. On the other hand, language-specific tokenisation (LST) methods\nhave a long and established history, and are developed using carefully created\nlexicons and training resources. Unlike subtokens produced by LIT methods, LST\nmethods produce valid morphological subwords. Despite the contrasting\ntrade-offs between LIT vs. LST methods, their performance on downstream NLP\ntasks remain unclear. In this paper, we empirically compare the two approaches\nusing semantic similarity measurement as an evaluation task across a diverse\nset of languages. Our experimental results covering eight languages show that\nLST consistently outperforms LIT when the vocabulary size is large, but LIT can\nproduce comparable or better results than LST in many languages with\ncomparatively smaller (i.e. less than 100K words) vocabulary sizes, encouraging\nthe use of LIT when language-specific resources are unavailable, incomplete or\na smaller model is required. Moreover, we find that smoothed inverse frequency\n(SIF) to be an accurate method to create word embeddings from subword\nembeddings for multilingual semantic similarity prediction tasks. Further\nanalysis of the nearest neighbours of tokens show that semantically and\nsyntactically related tokens are closely embedded in subword embedding spaces", "published": "2020-02-25 16:24:42", "link": "http://arxiv.org/abs/2002.11004v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Feature Normalization and Data Augmentation", "abstract": "The moments (a.k.a., mean and standard deviation) of latent features are\noften removed as noise when training image recognition models, to increase\nstability and reduce training time. However, in the field of image generation,\nthe moments play a much more central role. Studies have shown that the moments\nextracted from instance normalization and positional normalization can roughly\ncapture style and shape information of an image. Instead of being discarded,\nthese moments are instrumental to the generation process. In this paper we\npropose Moment Exchange, an implicit data augmentation method that encourages\nthe model to utilize the moment information also for recognition models.\nSpecifically, we replace the moments of the learned features of one training\nimage by those of another, and also interpolate the target labels -- forcing\nthe model to extract training signal from the moments in addition to the\nnormalized features. As our approach is fast, operates entirely in feature\nspace, and mixes different signals than prior methods, one can effectively\ncombine it with existing augmentation approaches. We demonstrate its efficacy\nacross several recognition benchmark data sets where it improves the\ngeneralization capability of highly competitive baseline networks with\nremarkable consistency.", "published": "2020-02-25 18:59:05", "link": "http://arxiv.org/abs/2002.11102v3", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Speech2Phone: A Novel and Efficient Method for Training Speaker\n  Recognition Models", "abstract": "In this paper we present an efficient method for training models for speaker\nrecognition using small or under-resourced datasets. This method requires less\ndata than other SOTA (State-Of-The-Art) methods, e.g. the Angular Prototypical\nand GE2E loss functions, while achieving similar results to those methods. This\nis done using the knowledge of the reconstruction of a phoneme in the speaker's\nvoice. For this purpose, a new dataset was built, composed of 40 male speakers,\nwho read sentences in Portuguese, totaling approximately 3h. We compare the\nthree best architectures trained using our method to select the best one, which\nis the one with a shallow architecture. Then, we compared this model with the\nSOTA method for the speaker recognition task: the Fast ResNet-34 trained with\napproximately 2,000 hours, using the loss functions Angular Prototypical and\nGE2E. Three experiments were carried out with datasets in different languages.\nAmong these three experiments, our model achieved the second best result in two\nexperiments and the best result in one of them. This highlights the importance\nof our method, which proved to be a great competitor to SOTA speaker\nrecognition models, with 500x less data and a simpler approach.", "published": "2020-02-25 22:59:36", "link": "http://arxiv.org/abs/2002.11213v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Controllable Sequence-To-Sequence Neural TTS with LPCNET Backend for\n  Real-time Speech Synthesis on CPU", "abstract": "State-of-the-art sequence-to-sequence acoustic networks, that convert a\nphonetic sequence to a sequence of spectral features with no explicit prosody\nprediction, generate speech with close to natural quality, when cascaded with\nneural vocoders, such as Wavenet. However, the combined system is typically too\nheavy for real-time speech synthesis on a CPU. In this work we present a\nsequence-to-sequence acoustic network combined with lightweight LPCNet neural\nvocoder, designed for real-time speech synthesis on a CPU. In addition, the\nsystem allows sentence-level pace and expressivity control at inference time.\nWe demonstrate that the proposed system can synthesize high quality 22 kHz\nspeech in real-time on a general-purpose CPU. In terms of MOS score degradation\nrelative to PCM, the system attained as low as 6.1-6.5% for quality and 6.3-\n7.0% for expressiveness, reaching equivalent or better quality when compared to\na similar system with a Wavenet vocoder backend.", "published": "2020-02-25 07:43:11", "link": "http://arxiv.org/abs/2002.10708v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "An LSTM Based Architecture to Relate Speech Stimulus to EEG", "abstract": "Modeling the relationship between natural speech and a recorded\nelectroencephalogram (EEG) helps us understand how the brain processes speech\nand has various applications in neuroscience and brain-computer interfaces. In\nthis context, so far mainly linear models have been used. However, the decoding\nperformance of the linear model is limited due to the complex and highly\nnon-linear nature of the auditory processing in the human brain. We present a\nnovel Long Short-Term Memory (LSTM)-based architecture as a non-linear model\nfor the classification problem of whether a given pair of (EEG, speech\nenvelope) correspond to each other or not. The model maps short segments of the\nEEG and the envelope to a common embedding space using a CNN in the EEG path\nand an LSTM in the speech path. The latter also compensates for the brain\nresponse delay. In addition, we use transfer learning to fine-tune the model\nfor each subject. The mean classification accuracy of the proposed model\nreaches 85%, which is significantly higher than that of a state of the art\nConvolutional Neural Network (CNN)-based model (73%) and the linear model\n(69%).", "published": "2020-02-25 16:00:47", "link": "http://arxiv.org/abs/2002.10988v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "IoT Based Real Time Noise Mapping System for Urban Sound Pollution Study", "abstract": "This paper describes the development of a system that enables real time data\nvisualization via a webapp regarding sound intensity using multiple node\ndevices connected through internet. The prototypes were realized using\nATmega328 (Arduino Nano) and ESP8266 hardware modules, NodeMCU Arduino wrapper\nlibrary, Google maps and firebase API along with JavaScript webapp. System\narchitecture is such that multiple node devices will be installed in different\nlocations of the target area. On each node device, an Arduino Nano interfaced\nwith a Sound Sensor measures the ambient sound intensity and ESP8266 Wi-Fi\nmodule transmits the data to a database via web API. On the webapp, it plots\nall the real-time data from the devices over Google maps according to the\nlocations of the node devices. The logged data that is collected can then be\nused to carry out researches regarding sound pollution in targeted areas.", "published": "2020-02-25 21:53:06", "link": "http://arxiv.org/abs/2002.11188v1", "categories": ["cs.NI", "cs.IR", "eess.AS"], "primary_category": "cs.NI"}
{"title": "Dataset of raw and pre-processed speech signals, Mel Frequency Cepstral\n  Coefficients of Speech and Heart Rate measurements", "abstract": "Heart rate is an important vital sign used in the diagnosis of many medical\nconditions. Conventionally, heart rate is measured using a medical device such\nas pulse oxymeter. Physiological parameters such as heart rate bear a\ncorrelation to speech characteristics of an individual. Hence, there is a\npossibility to measure heart rate from speech signals using machine learning\nand deep learning, which would also allow non-invasive, non contact based and\nremote monitoring of patients. However, to design such a scheme and verify its\naccuracy, it is necessary to collect speech recordings along with heart rates\nmeasured using a medical device, simultaneously during the recording. This\narticle provides a dataset as well as the procedure used to create the dataset\nwhich could be used to facilitate research in developing techniques to estimate\nheart rate accurately by observing speech signal.", "published": "2020-02-25 16:32:31", "link": "http://arxiv.org/abs/2002.11250v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Towards Learning a Universal Non-Semantic Representation of Speech", "abstract": "The ultimate goal of transfer learning is to reduce labeled data requirements\nby exploiting a pre-existing embedding model trained for different datasets or\ntasks. The visual and language communities have established benchmarks to\ncompare embeddings, but the speech community has yet to do so. This paper\nproposes a benchmark for comparing speech representations on non-semantic\ntasks, and proposes a representation based on an unsupervised triplet-loss\nobjective. The proposed representation outperforms other representations on the\nbenchmark, and even exceeds state-of-the-art performance on a number of\ntransfer learning tasks. The embedding is trained on a publicly available\ndataset, and it is tested on a variety of low-resource downstream tasks,\nincluding personalization tasks and medical domain. The benchmark, models, and\nevaluation code are publicly released.", "published": "2020-02-25 21:38:24", "link": "http://arxiv.org/abs/2002.12764v6", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A.I. based Embedded Speech to Text Using Deepspeech", "abstract": "Deepspeech was very useful for development IoT devices that need voice\nrecognition. One of the voice recognition systems is deepspeech from Mozilla.\nDeepspeech is an open-source voice recognition that was using a neural network\nto convert speech spectrogram into a text transcript. This paper shows the\nimplementation process of speech recognition on a low-end computational device.\nDevelopment of English-language speech recognition that has many datasets\nbecome a good point for starting. The model that used results from pre-trained\nmodel that provide by each version of deepspeech, without change of the model\nthat already released, furthermore the benefit of using raspberry pi as a media\nend-to-end speech recognition device become a good thing, user can change and\nmodify of the speech recognition, and also deepspeech can be standalone device\nwithout need continuously internet connection to process speech recognition,\nand even this paper show the power of Tensorflow Lite can make a significant\ndifference on inference by deepspeech rather than using Tensorflow\nnon-Lite.This paper shows the experiment using Deepspeech version 0.1.0, 0.1.1,\nand 0.6.0, and there is some improvement on Deepspeech version 0.6.0, faster\nwhile processing speech-to-text on old hardware raspberry pi 3 b+.", "published": "2020-02-25 08:27:41", "link": "http://arxiv.org/abs/2002.12830v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
