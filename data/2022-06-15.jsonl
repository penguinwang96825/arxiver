{"title": "Detecting Harmful Online Conversational Content towards LGBTQIA+\n  Individuals", "abstract": "Online discussions, panels, talk page edits, etc., often contain harmful\nconversational content i.e., hate speech, death threats and offensive language,\nespecially towards certain demographic groups. For example, individuals who\nidentify as members of the LGBTQIA+ community and/or BIPOC (Black, Indigenous,\nPeople of Color) are at higher risk for abuse and harassment online. In this\nwork, we first introduce a real-world dataset that will enable us to study and\nunderstand harmful online conversational content. Then, we conduct several\nexploratory data analysis experiments to gain deeper insights from the dataset.\nWe later describe our approach for detecting harmful online Anti-LGBTQIA+\nconversational content, and finally, we implement two baseline machine learning\nmodels (i.e., Support Vector Machine and Logistic Regression), and fine-tune 3\npre-trained large language models (BERT, RoBERTa, and HateBERT). Our findings\nverify that large language models can achieve very promising performance on\ndetecting online Anti-LGBTQIA+ conversational content detection tasks.", "published": "2022-06-15 20:14:02", "link": "http://arxiv.org/abs/2207.10032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Born for Auto-Tagging: Faster and better with new objective functions", "abstract": "Keyword extraction is a task of text mining. It is applied to increase search\nvolume in SEO and ads. Implemented in auto-tagging, it makes tagging on a mass\nscale of online articles and photos efficiently and accurately. BAT is invented\nfor auto-tagging which served as awoo's AI marketing platform (AMP). awoo AMP\nnot only provides service as a customized recommender system but also increases\nthe converting rate in E-commerce. The strength of BAT converges faster and\nbetter than other SOTA models, as its 4-layer structure achieves the best F\nscores at 50 epochs. In other words, it performs better than other models which\nrequire deeper layers at 100 epochs. To generate rich and clean tags, awoo\ncreates new objective functions to maintain similar ${\\rm F_1}$ scores with\ncross-entropy while enhancing ${\\rm F_2}$ scores simultaneously. To assure the\neven better performance of F scores awoo revamps the learning rate strategy\nproposed by Transformer \\cite{Transformer} to increase ${\\rm F_1}$ and ${\\rm\nF_2}$ scores at the same time.", "published": "2022-06-15 02:52:22", "link": "http://arxiv.org/abs/2206.07264v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhanced Knowledge Selection for Grounded Dialogues via Document\n  Semantic Graphs", "abstract": "Providing conversation models with background knowledge has been shown to\nmake open-domain dialogues more informative and engaging. Existing models treat\nknowledge selection as a sentence ranking or classification problem where each\nsentence is handled individually, ignoring the internal semantic connection\namong sentences in the background document. In this work, we propose to\nautomatically convert the background knowledge documents into document semantic\ngraphs and then perform knowledge selection over such graphs. Our document\nsemantic graphs preserve sentence-level information through the use of sentence\nnodes and provide concept connections between sentences. We jointly apply\nmulti-task learning for sentence-level and concept-level knowledge selection\nand show that it improves sentence-level selection. Our experiments show that\nour semantic graph-based knowledge selection improves over sentence selection\nbaselines for both the knowledge selection task and the end-to-end response\ngeneration task on HollE and improves generalization on unseen topics in WoW.", "published": "2022-06-15 04:51:32", "link": "http://arxiv.org/abs/2206.07296v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMNEROne at SemEval-2022 Task 11: Code-Mixed Named Entity Recognition by\n  leveraging multilingual data", "abstract": "Identifying named entities is, in general, a practical and challenging task\nin the field of Natural Language Processing. Named Entity Recognition on the\ncode-mixed text is further challenging due to the linguistic complexity\nresulting from the nature of the mixing. This paper addresses the submission of\nteam CMNEROne to the SEMEVAL 2022 shared task 11 MultiCoNER. The Code-mixed NER\ntask aimed to identify named entities on the code-mixed dataset. Our work\nconsists of Named Entity Recognition (NER) on the code-mixed dataset by\nleveraging the multilingual data. We achieved a weighted average F1 score of\n0.7044, i.e., 6% greater than the baseline.", "published": "2022-06-15 06:33:13", "link": "http://arxiv.org/abs/2206.07318v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Structure Search for Parameter-Efficient Tuning", "abstract": "Adapting large pre-trained models (PTMs) through fine-tuning imposes\nprohibitive computational and storage burdens. Recent studies of\nparameter-efficient tuning (PET) find that only optimizing a small portion of\nparameters conditioned on PTMs could yield on-par performance compared to\nconventional fine-tuning. Generally, PET methods exquisitely design\nparameter-efficient modules (PET modules) which could be applied to arbitrary\nfine-grained positions inside PTMs. However, the effectiveness of these\nfine-grained positions largely relies on sophisticated manual designation,\nthereby usually producing sub-optimal results. In contrast to the manual\ndesignation, we explore constructing PET modules in an automatic manner. We\nautomatically \\textbf{S}earch for the \\textbf{S}parse \\textbf{S}tructure of\n\\textbf{P}arameter-\\textbf{E}fficient \\textbf{T}uning (S$^3$PET). Based on a\nunified framework of various PET methods, S$^3$PET conducts the differentiable\nPET structure search through bi-level optimization and proposes shifted global\nsigmoid method to explicitly control the number of trainable parameters.\nExtensive experiments show that S$^3$PET surpasses manual and random structures\nwith less trainable parameters. The searched structures preserve more than 99\\%\nfine-tuning performance with 0.01\\% trainable parameters. Moreover, the\nadvantage of S$^3$PET is amplified with extremely low trainable parameters\nbudgets (0.0009\\%$\\sim$0.01\\%). The searched structures are transferable and\nexplainable, providing suggestions and guidance for the future design of PET\nmethods.", "published": "2022-06-15 08:45:21", "link": "http://arxiv.org/abs/2206.07382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating Confidence of Predictions of Individual Classifiers and Their\n  Ensembles for the Genre Classification Task", "abstract": "Genre identification is a subclass of non-topical text classification. The\nmain difference between this task and topical classification is that genres,\nunlike topics, usually do not correspond to simple keywords, and thus they need\nto be defined in terms of their functions in communication. Neural models based\non pre-trained transformers, such as BERT or XLM-RoBERTa, demonstrate SOTA\nresults in many NLP tasks, including non-topical classification. However, in\nmany cases, their downstream application to very large corpora, such as those\nextracted from social media, can lead to unreliable results because of dataset\nshifts, when some raw texts do not match the profile of the training set. To\nmitigate this problem, we experiment with individual models as well as with\ntheir ensembles. To evaluate the robustness of all models we use a prediction\nconfidence metric, which estimates the reliability of a prediction in the\nabsence of a gold standard label. We can evaluate robustness via the confidence\ngap between the correctly classified texts and the misclassified ones on a\nlabeled test corpus, higher gaps make it easier to improve our confidence that\nour classifier made the right decision. Our results show that for all of the\nclassifiers tested in this study, there is a confidence gap, but for the\nensembles, the gap is bigger, meaning that ensembles are more robust than their\nindividual models.", "published": "2022-06-15 09:59:05", "link": "http://arxiv.org/abs/2206.07427v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KE-QI: A Knowledge Enhanced Article Quality Identification Dataset", "abstract": "With so many articles of varying qualities being produced every moment, it is\na very urgent task to screen outstanding articles and commit them to social\nmedia. To our best knowledge, there is a lack of datasets and mature research\nworks in identifying high-quality articles. Consequently, we conduct some\nsurveys and finalize 7 objective indicators to annotate the quality of 10k\narticles. During annotation, we find that many characteristics of high-quality\narticles (e.g., background) rely more on extensive external knowledge than\ninner semantic information of articles. In response, we link extracted article\nentities to Baidu Encyclopedia, then propose Knowledge Enhanced article Quality\nIdentification (KE-QI) dataset. To make better use of external knowledge, we\npropose a compound model which fuses the text and external knowledge\ninformation via a gate unit to classify the quality of an article. Our\nexperimental results on KE-QI show that with initialization of our pre-trained\nNode2Vec model, our model achieves about 78\\% $F_1$, outperforming other\nbaselines.", "published": "2022-06-15 14:15:41", "link": "http://arxiv.org/abs/2206.07556v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual AMR Aligner: Paying Attention to Cross-Attention", "abstract": "This paper introduces a novel aligner for Abstract Meaning Representation\n(AMR) graphs that can scale cross-lingually, and is thus capable of aligning\nunits and spans in sentences of different languages. Our approach leverages\nmodern Transformer-based parsers, which inherently encode alignment information\nin their cross-attention weights, allowing us to extract this information\nduring parsing. This eliminates the need for English-specific rules or the\nExpectation Maximization (EM) algorithm that have been used in previous\napproaches. In addition, we propose a guided supervised method using alignment\nto further enhance the performance of our aligner. We achieve state-of-the-art\nresults in the benchmarks for AMR alignment and demonstrate our aligner's\nability to obtain them across multiple languages. Our code will be available at\n\\href{https://www.github.com/Babelscape/AMR-alignment}{github.com/Babelscape/AMR-alignment}.", "published": "2022-06-15 15:12:24", "link": "http://arxiv.org/abs/2206.07587v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HICEM: A High-Coverage Emotion Model for Artificial Emotional\n  Intelligence", "abstract": "As social robots and other intelligent machines enter the home, artificial\nemotional intelligence (AEI) is taking center stage to address users' desire\nfor deeper, more meaningful human-machine interaction. To accomplish such\nefficacious interaction, the next-generation AEI need comprehensive human\nemotion models for training. Unlike theory of emotion, which has been the\nhistorical focus in psychology, emotion models are a descriptive tools. In\npractice, the strongest models need robust coverage, which means defining the\nsmallest core set of emotions from which all others can be derived. To achieve\nthe desired coverage, we turn to word embeddings from natural language\nprocessing. Using unsupervised clustering techniques, our experiments show that\nwith as few as 15 discrete emotion categories, we can provide maximum coverage\nacross six major languages--Arabic, Chinese, English, French, Spanish, and\nRussian. In support of our findings, we also examine annotations from two\nlarge-scale emotion recognition datasets to assess the validity of existing\nemotion models compared to human perception at scale. Because robust,\ncomprehensive emotion models are foundational for developing real-world\naffective computing applications, this work has broad implications in social\nrobotics, human-machine interaction, mental healthcare, and computational\npsychology.", "published": "2022-06-15 15:21:30", "link": "http://arxiv.org/abs/2206.07593v1", "categories": ["cs.CL", "68T01 (Primary) 62P15, 68T50 (Secondary)"], "primary_category": "cs.CL"}
{"title": "The SIGMORPHON 2022 Shared Task on Morpheme Segmentation", "abstract": "The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems\nto decompose a word into a sequence of morphemes and covered most types of\nmorphology: compounds, derivations, and inflections. Subtask 1, word-level\nmorpheme segmentation, covered 5 million words in 9 languages (Czech, English,\nSpanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13\nsystem submissions from 7 teams and the best system averaged 97.29% F1 score\nacross all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2,\nsentence-level morpheme segmentation, covered 18,735 sentences in 3 languages\n(Czech, English, Mongolian), received 10 system submissions from 3 teams, and\nthe best systems outperformed all three state-of-the-art subword tokenization\nmethods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis\nand support any type of future studies, we released all system predictions, the\nevaluation script, and all gold standard datasets.", "published": "2022-06-15 15:57:22", "link": "http://arxiv.org/abs/2206.07615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Automatic Speech Recognition of Formal and Colloquial\n  Czech in MALACH Project", "abstract": "Czech is a very specific language due to its large differences between the\nformal and the colloquial form of speech. While the formal (written) form is\nused mainly in official documents, literature, and public speeches, the\ncolloquial (spoken) form is used widely among people in casual speeches. This\ngap introduces serious problems for ASR systems, especially when training or\nevaluating ASR models on datasets containing a lot of colloquial speech, such\nas the MALACH project. In this paper, we are addressing this problem in the\nlight of a new paradigm in end-to-end ASR systems -- recently introduced\nself-supervised audio Transformers. Specifically, we are investigating the\ninfluence of colloquial speech on the performance of Wav2Vec 2.0 models and\ntheir ability to transcribe colloquial speech directly into formal transcripts.\nWe are presenting results with both formal and colloquial forms in the training\ntranscripts, language models, and evaluation transcripts.", "published": "2022-06-15 17:01:20", "link": "http://arxiv.org/abs/2206.07666v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emergent Abilities of Large Language Models", "abstract": "Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.", "published": "2022-06-15 17:32:01", "link": "http://arxiv.org/abs/2206.07682v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIRECTOR: Generator-Classifiers For Supervised Language Modeling", "abstract": "Current language models achieve low perplexity but their resulting\ngenerations still suffer from toxic responses, repetitiveness and\ncontradictions. The standard language modeling setup fails to address these\nissues. In this paper, we introduce a new architecture, {\\sc Director}, that\nconsists of a unified generator-classifier with both a language modeling and a\nclassification head for each output token. Training is conducted jointly using\nboth standard language modeling data, and data labeled with desirable and\nundesirable sequences. Experiments in several settings show that the model has\ncompetitive training and decoding speed compared to standard language models\nwhile yielding superior results, alleviating known issues while maintaining\ngeneration quality. It also outperforms existing model guiding approaches in\nterms of both accuracy and efficiency.", "published": "2022-06-15 17:44:08", "link": "http://arxiv.org/abs/2206.07694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Adults Understand What Young Children Say", "abstract": "Children's early speech often bears little resemblance to that of adults, and\nyet parents and other caregivers are able to interpret that speech and react\naccordingly. Here we investigate how these adult inferences as listeners\nreflect sophisticated beliefs about what children are trying to communicate, as\nwell as how children are likely to pronounce words. Using a Bayesian framework\nfor modeling spoken word recognition, we find that computational models can\nreplicate adult interpretations of children's speech only when they include\nstrong, context-specific prior expectations about the messages that children\nwill want to communicate. This points to a critical role of adult cognitive\nprocesses in supporting early communication and reveals how children can\nactively prompt adults to take actions on their behalf even when they have only\na nascent understanding of the adult language. We discuss the wide-ranging\nimplications of the powerful listening capabilities of adults for theories of\nfirst language acquisition.", "published": "2022-06-15 20:37:32", "link": "http://arxiv.org/abs/2206.07807v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TOKEN is a MASK: Few-shot Named Entity Recognition with Pre-trained\n  Language Models", "abstract": "Transferring knowledge from one domain to another is of practical importance\nfor many tasks in natural language processing, especially when the amount of\navailable data in the target domain is limited. In this work, we propose a\nnovel few-shot approach to domain adaptation in the context of Named Entity\nRecognition (NER). We propose a two-step approach consisting of a variable base\nmodule and a template module that leverages the knowledge captured in\npre-trained language models with the help of simple descriptive patterns. Our\napproach is simple yet versatile and can be applied in few-shot and zero-shot\nsettings. Evaluating our lightweight approach across a number of different\ndatasets shows that it can boost the performance of state-of-the-art baselines\nby 2-5% F1-score.", "published": "2022-06-15 22:49:14", "link": "http://arxiv.org/abs/2206.07841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Location-based Twitter Filtering for the Creation of Low-Resource\n  Language Datasets in Indonesian Local Languages", "abstract": "Twitter contains an abundance of linguistic data from the real world. We\nexamine Twitter for user-generated content in low-resource languages such as\nlocal Indonesian. For NLP to work in Indonesian, it must consider local\ndialects, geographic context, and regional culture influence Indonesian\nlanguages. This paper identifies the problems we faced when constructing a\nLocal Indonesian NLP dataset. Furthermore, we are developing a framework for\ncreating, collecting, and classifying Local Indonesian datasets for NLP. Using\ntwitter's geolocation tool for automatic annotating.", "published": "2022-06-15 01:53:43", "link": "http://arxiv.org/abs/2206.07238v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Survey : Neural Networks for AMR-to-Text", "abstract": "AMR-to-text is one of the key techniques in the NLP community that aims at\ngenerating sentences from the Abstract Meaning Representation (AMR) graphs.\nSince AMR was proposed in 2013, the study on AMR-to-Text has become\nincreasingly prevalent as an essential branch of structured data to text\nbecause of the unique advantages of AMR as a high-level semantic description of\nnatural language. In this paper, we provide a brief survey of AMR-to-Text.\nFirstly, we introduce the current scenario of this technique and point out its\ndifficulties. Secondly, based on the methods used in previous studies, we\nroughly divided them into five categories according to their respective\nmechanisms, i.e., Rules-based, Seq-to-Seq-based, Graph-to-Seq-based,\nTransformer-based, and Pre-trained Language Model (PLM)-based. In particular,\nwe detail the neural network-based method and present the latest progress of\nAMR-to-Text, which refers to AMR reconstruction, Decoder optimization, etc.\nFurthermore, we present the benchmarks and evaluation methods of AMR-to-Text.\nEventually, we provide a summary of current techniques and the outlook for\nfuture research.", "published": "2022-06-15 07:20:28", "link": "http://arxiv.org/abs/2206.07328v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Emotion is Not One-hot Encoding: Learning with Grayscale Label for\n  Emotion Recognition in Conversation", "abstract": "In emotion recognition in conversation (ERC), the emotion of the current\nutterance is predicted by considering the previous context, which can be\nutilized in many natural language processing tasks. Although multiple emotions\ncan coexist in a given sentence, most previous approaches take the perspective\nof a classification task to predict only a given label. However, it is\nexpensive and difficult to label the emotion of a sentence with confidence or\nmulti-label. In this paper, we automatically construct a grayscale label\nconsidering the correlation between emotions and use it for learning. That is,\ninstead of using a given label as a one-hot encoding, we construct a grayscale\nlabel by measuring scores for different emotions. We introduce several methods\nfor constructing grayscale labels and confirm that each method improves the\nemotion recognition performance. Our method is simple, effective, and\nuniversally applicable to previous systems. The experiments show a significant\nimprovement in the performance of baselines.", "published": "2022-06-15 08:14:42", "link": "http://arxiv.org/abs/2206.07359v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BaIT: Barometer for Information Trustworthiness", "abstract": "This paper presents a new approach to the FNC-1 fake news classification task\nwhich involves employing pre-trained encoder models from similar NLP tasks,\nnamely sentence similarity and natural language inference, and two neural\nnetwork architectures using this approach are proposed. Methods in data\naugmentation are explored as a means of tackling class imbalance in the\ndataset, employing common pre-existing methods and proposing a method for\nsample generation in the under-represented class using a novel sentence\nnegation algorithm. Comparable overall performance with existing baselines is\nachieved, while significantly increasing accuracy on an under-represented but\nnonetheless important class for FNC-1.", "published": "2022-06-15 13:42:55", "link": "http://arxiv.org/abs/2206.07535v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Contextualization and Generalization in Entity and Relation Extraction", "abstract": "During the past decade, neural networks have become prominent in Natural\nLanguage Processing (NLP), notably for their capacity to learn relevant word\nrepresentations from large unlabeled corpora. These word embeddings can then be\ntransferred and finetuned for diverse end applications during a supervised\ntraining phase. More recently, in 2018, the transfer of entire pretrained\nLanguage Models and the preservation of their contextualization capacities\nenabled to reach unprecedented performance on virtually every NLP benchmark,\nsometimes even outperforming human baselines. However, as models reach such\nimpressive scores, their comprehension abilities still appear as shallow, which\nreveal limitations of benchmarks to provide useful insights on their factors of\nperformance and to accurately measure understanding capabilities.\n  In this thesis, we study the behaviour of state-of-the-art models regarding\ngeneralization to facts unseen during training in two important Information\nExtraction tasks: Named Entity Recognition (NER) and Relation Extraction (RE).\nIndeed, traditional benchmarks present important lexical overlap between\nmentions and relations used for training and evaluating models, whereas the\nmain interest of Information Extraction is to extract previously unknown\ninformation. We propose empirical studies to separate performance based on\nmention and relation overlap with the training set and find that pretrained\nLanguage Models are mainly beneficial to detect unseen mentions, in particular\nout-of-domain. While this makes them suited for real use cases, there is still\na gap in performance between seen and unseen mentions that hurts generalization\nto new facts. In particular, even state-of-the-art ERE models rely on a shallow\nretention heuristic, basing their prediction more on arguments surface forms\nthan context.", "published": "2022-06-15 14:16:42", "link": "http://arxiv.org/abs/2206.07558v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personal Entity, Concept, and Named Entity Linking in Conversations", "abstract": "Building conversational agents that can have natural and knowledge-grounded\ninteractions with humans requires understanding user utterances. Entity Linking\n(EL) is an effective and widely used method for understanding natural language\ntext and connecting it to external knowledge. It is, however, shown that\nexisting EL methods developed for annotating documents are suboptimal for\nconversations, where personal entities (e.g., \"my cars\") and concepts are\nessential for understanding user utterances. In this paper, we introduce a\ncollection and a tool for entity linking in conversations. We collect EL\nannotations for 1327 conversational utterances, consisting of links to named\nentities, concepts, and personal entities. The dataset is used for training our\ntoolkit for conversational entity linking, CREL. Unlike existing EL methods,\nCREL is developed to identify both named entities and concepts. It also\nutilizes coreference resolution techniques to identify personal entities and\nreferences to the explicit entity mentions in the conversations. We compare\nCREL with state-of-the-art techniques and show that it outperforms all existing\nbaselines.", "published": "2022-06-15 22:32:29", "link": "http://arxiv.org/abs/2206.07836v3", "categories": ["cs.CL", "cs.IR", "H.3"], "primary_category": "cs.CL"}
{"title": "A smile is all you need: Predicting limiting activity coefficients from\n  SMILES with natural language processing", "abstract": "Knowledge of mixtures' phase equilibria is crucial in nature and technical\nchemistry. Phase equilibria calculations of mixtures require activity\ncoefficients. However, experimental data on activity coefficients is often\nlimited due to high cost of experiments. For an accurate and efficient\nprediction of activity coefficients, machine learning approaches have been\nrecently developed. However, current machine learning approaches still\nextrapolate poorly for activity coefficients of unknown molecules. In this\nwork, we introduce the SMILES-to-Properties-Transformer (SPT), a natural\nlanguage processing network to predict binary limiting activity coefficients\nfrom SMILES codes. To overcome the limitations of available experimental data,\nwe initially train our network on a large dataset of synthetic data sampled\nfrom COSMO-RS (10 Million data points) and then fine-tune the model on\nexperimental data (20 870 data points). This training strategy enables SPT to\naccurately predict limiting activity coefficients even for unknown molecules,\ncutting the mean prediction error in half compared to state-of-the-art models\nfor activity coefficient predictions such as COSMO-RS, UNIFAC, and improving on\nrecent machine learning approaches.", "published": "2022-06-15 07:11:37", "link": "http://arxiv.org/abs/2206.07048v1", "categories": ["physics.chem-ph", "cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "physics.chem-ph"}
{"title": "TeKo: Text-Rich Graph Neural Networks with External Knowledge", "abstract": "Graph Neural Networks (GNNs) have gained great popularity in tackling various\nanalytical tasks on graph-structured data (i.e., networks). Typical GNNs and\ntheir variants follow a message-passing manner that obtains network\nrepresentations by the feature propagation process along network topology,\nwhich however ignore the rich textual semantics (e.g., local word-sequence)\nthat exist in many real-world networks. Existing methods for text-rich networks\nintegrate textual semantics by mainly utilizing internal information such as\ntopics or phrases/words, which often suffer from an inability to\ncomprehensively mine the text semantics, limiting the reciprocal guidance\nbetween network structure and text semantics. To address these problems, we\npropose a novel text-rich graph neural network with external knowledge (TeKo),\nin order to take full advantage of both structural and textual information\nwithin text-rich networks. Specifically, we first present a flexible\nheterogeneous semantic network that incorporates high-quality entities and\ninteractions among documents and entities. We then introduce two types of\nexternal knowledge, that is, structured triplets and unstructured entity\ndescription, to gain a deeper insight into textual semantics. We further design\na reciprocal convolutional mechanism for the constructed heterogeneous semantic\nnetwork, enabling network structure and textual semantics to collaboratively\nenhance each other and learn high-level network representations. Extensive\nexperimental results on four public text-rich networks as well as a large-scale\ne-commerce searching dataset illustrate the superior performance of TeKo over\nstate-of-the-art baselines.", "published": "2022-06-15 02:33:10", "link": "http://arxiv.org/abs/2206.07253v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Human heuristics for AI-generated language are flawed", "abstract": "Human communication is increasingly intermixed with language generated by AI.\nAcross chat, email, and social media, AI systems suggest words, complete\nsentences, or produce entire conversations. AI-generated language is often not\nidentified as such but presented as language written by humans, raising\nconcerns about novel forms of deception and manipulation. Here, we study how\nhumans discern whether verbal self-presentations, one of the most personal and\nconsequential forms of language, were generated by AI. In six experiments,\nparticipants (N = 4,600) were unable to detect self-presentations generated by\nstate-of-the-art AI language models in professional, hospitality, and dating\ncontexts. A computational analysis of language features shows that human\njudgments of AI-generated language are hindered by intuitive but flawed\nheuristics such as associating first-person pronouns, use of contractions, or\nfamily topics with human-written language. We experimentally demonstrate that\nthese heuristics make human judgment of AI-generated language predictable and\nmanipulable, allowing AI systems to produce text perceived as \"more human than\nhuman.\" We discuss solutions, such as AI accents, to reduce the deceptive\npotential of language generated by AI, limiting the subversion of human\nintuition.", "published": "2022-06-15 03:18:56", "link": "http://arxiv.org/abs/2206.07271v4", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "SciTweets -- A Dataset and Annotation Framework for Detecting Scientific\n  Online Discourse", "abstract": "Scientific topics, claims and resources are increasingly debated as part of\nonline discourse, where prominent examples include discourse related to\nCOVID-19 or climate change. This has led to both significant societal impact\nand increased interest in scientific online discourse from various disciplines.\nFor instance, communication studies aim at a deeper understanding of biases,\nquality or spreading pattern of scientific information whereas computational\nmethods have been proposed to extract, classify or verify scientific claims\nusing NLP and IR techniques. However, research across disciplines currently\nsuffers from both a lack of robust definitions of the various forms of\nscience-relatedness as well as appropriate ground truth data for distinguishing\nthem. In this work, we contribute (a) an annotation framework and corresponding\ndefinitions for different forms of scientific relatedness of online discourse\nin Tweets, (b) an expert-annotated dataset of 1261 tweets obtained through our\nlabeling framework reaching an average Fleiss Kappa $\\kappa$ of 0.63, (c) a\nmulti-label classifier trained on our data able to detect science-relatedness\nwith 89% F1 and also able to detect distinct forms of scientific knowledge\n(claims, references). With this work we aim to lay the foundation for\ndeveloping and evaluating robust methods for analysing science as part of\nlarge-scale online discourse.", "published": "2022-06-15 08:14:55", "link": "http://arxiv.org/abs/2206.07360v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "NatiQ: An End-to-end Text-to-Speech System for Arabic", "abstract": "NatiQ is end-to-end text-to-speech system for Arabic. Our speech synthesizer\nuses an encoder-decoder architecture with attention. We used both\ntacotron-based models (tacotron-1 and tacotron-2) and the faster transformer\nmodel for generating mel-spectrograms from characters. We concatenated\nTacotron1 with the WaveRNN vocoder, Tacotron2 with the WaveGlow vocoder and\nESPnet transformer with the parallel wavegan vocoder to synthesize waveforms\nfrom the spectrograms. We used in-house speech data for two voices: 1) neutral\nmale \"Hamza\"- narrating general content and news, and 2) expressive female\n\"Amina\"- narrating children story books to train our models. Our best systems\nachieve an average Mean Opinion Score (MOS) of 4.21 and 4.40 for Amina and\nHamza respectively. The objective evaluation of the systems using word and\ncharacter error rate (WER and CER) as well as the response time measured by\nreal-time factor favored the end-to-end architecture ESPnet. NatiQ demo is\navailable on-line at https://tts.qcri.org", "published": "2022-06-15 08:28:08", "link": "http://arxiv.org/abs/2206.07373v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring Capabilities of Monolingual Audio Transformers using Large\n  Datasets in Automatic Speech Recognition of Czech", "abstract": "In this paper, we present our progress in pretraining Czech monolingual audio\ntransformers from a large dataset containing more than 80 thousand hours of\nunlabeled speech, and subsequently fine-tuning the model on automatic speech\nrecognition tasks using a combination of in-domain data and almost 6 thousand\nhours of out-of-domain transcribed speech. We are presenting a large palette of\nexperiments with various fine-tuning setups evaluated on two public datasets\n(CommonVoice and VoxPopuli) and one extremely challenging dataset from the\nMALACH project. Our results show that monolingual Wav2Vec 2.0 models are robust\nASR systems, which can take advantage of large labeled and unlabeled datasets\nand successfully compete with state-of-the-art LVCSR systems. Moreover, Wav2Vec\nmodels proved to be good zero-shot learners when no training data are available\nfor the target ASR task.", "published": "2022-06-15 16:14:37", "link": "http://arxiv.org/abs/2206.07627v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone", "abstract": "Vision-language (VL) pre-training has recently received considerable\nattention. However, most existing end-to-end pre-training approaches either\nonly aim to tackle VL tasks such as image-text retrieval, visual question\nanswering (VQA) and image captioning that test high-level understanding of\nimages, or only target region-level understanding for tasks such as phrase\ngrounding and object detection. We present FIBER (Fusion-In-the-Backbone-based\ntransformER), a new VL model architecture that can seamlessly handle both these\ntypes of tasks. Instead of having dedicated transformer layers for fusion after\nthe uni-modal backbones, FIBER pushes multimodal fusion deep into the model by\ninserting cross-attention into the image and text backbones, bringing gains in\nterms of memory and performance. In addition, unlike previous work that is\neither only pre-trained on image-text data or on fine-grained data with\nbox-level annotations, we present a two-stage pre-training strategy that uses\nboth these kinds of data efficiently: (i) coarse-grained pre-training based on\nimage-text data; followed by (ii) fine-grained pre-training based on\nimage-text-box data. We conduct comprehensive experiments on a wide range of VL\ntasks, ranging from VQA, image captioning, and retrieval, to phrase grounding,\nreferring expression comprehension, and object detection. Using deep multimodal\nfusion coupled with the two-stage pre-training, FIBER provides consistent\nperformance improvements over strong baselines across all tasks, often\noutperforming methods using magnitudes more data. Code is available at\nhttps://github.com/microsoft/FIBER.", "published": "2022-06-15 16:41:29", "link": "http://arxiv.org/abs/2206.07643v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Unified Sequence Interface for Vision Tasks", "abstract": "While language tasks are naturally expressed in a single, unified, modeling\nframework, i.e., generating sequences of tokens, this has not been the case in\ncomputer vision. As a result, there is a proliferation of distinct\narchitectures and loss functions for different vision tasks. In this work we\nshow that a diverse set of \"core\" computer vision tasks can also be unified if\nformulated in terms of a shared pixel-to-sequence interface. We focus on four\ntasks, namely, object detection, instance segmentation, keypoint detection, and\nimage captioning, all with diverse types of outputs, e.g., bounding boxes or\ndense masks. Despite that, by formulating the output of each task as a sequence\nof discrete tokens with a unified interface, we show that one can train a\nneural network with a single model architecture and loss function on all these\ntasks, with no task-specific customization. To solve a specific task, we use a\nshort prompt as task description, and the sequence output adapts to the prompt\nso it can produce task-specific output. We show that such a model can achieve\ncompetitive performance compared to well-established task-specific models.", "published": "2022-06-15 17:08:53", "link": "http://arxiv.org/abs/2206.07669v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Write and Paint: Generative Vision-Language Models are Unified Modal\n  Learners", "abstract": "Recent advances in vision-language pre-training have pushed the\nstate-of-the-art on various vision-language tasks, making machines more capable\nof multi-modal writing (image-to-text generation) and painting (text-to-image\ngeneration). However, few studies investigate if these two essential\ncapabilities can be learned together and boost each other, making a versatile\nand powerful multi-modal foundation model. In this work, we disclose the\npotential of symmetric generative vision-language pre-training in learning to\nwrite and paint concurrently, and propose a new unified modal model, named\nDaVinci, trained with prefix language modeling and prefix image modeling, a\nsimple generative self-supervised objective on image-text pairs. Thanks to the\nproposed prefix multi-modal modeling framework, DaVinci is simple to train,\nscalable to huge data, adaptable to both writing and painting tasks, and also\nstrong on other vision, text, and multi-modal understanding tasks. DaVinci\nachieves competitive performance on a wide range of 27 generation/understanding\ntasks and demonstrates the superiority of combining vision/language generative\npre-training. Furthermore, we carefully benchmark the performance of different\nvision-language pre-training objectives on different scales of pre-training\ndatasets on a heterogeneous and broad distribution coverage. Our results\ndemonstrate the potential of exploiting self-supervision in both language and\nvision inputs, and establish new, stronger baselines for future comparisons at\ndifferent data scales. The code and pre-trained models are available at\nhttps://github.com/shizhediao/DaVinci.", "published": "2022-06-15 17:49:38", "link": "http://arxiv.org/abs/2206.07699v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter\n  Encoders for Natural Language Understanding Systems", "abstract": "We present results from a large-scale experiment on pretraining encoders with\nnon-embedding parameter counts ranging from 700M to 9.3B, their subsequent\ndistillation into smaller models ranging from 17M-170M parameters, and their\napplication to the Natural Language Understanding (NLU) component of a virtual\nassistant system. Though we train using 70% spoken-form data, our teacher\nmodels perform comparably to XLM-R and mT5 when evaluated on the written-form\nCross-lingual Natural Language Inference (XNLI) corpus. We perform a second\nstage of pretraining on our teacher models using in-domain data from our\nsystem, improving error rates by 3.86% relative for intent classification and\n7.01% relative for slot filling. We find that even a 170M-parameter model\ndistilled from our Stage 2 teacher model has 2.88% better intent classification\nand 7.69% better slot filling error rates when compared to the 2.3B-parameter\nteacher trained only on public data (Stage 1), emphasizing the importance of\nin-domain data for pretraining. When evaluated offline using labeled NLU data,\nour 17M-parameter Stage 2 distilled model outperforms both XLM-R Base (85M\nparams) and DistillBERT (42M params) by 4.23% to 6.14%, respectively. Finally,\nwe present results from a full virtual assistant experimentation platform,\nwhere we find that models trained using our pretraining and distillation\npipeline outperform models distilled from 85M-parameter teachers by 3.74%-4.91%\non an automatic measurement of full-system user dissatisfaction.", "published": "2022-06-15 20:44:23", "link": "http://arxiv.org/abs/2206.07808v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Proposed Bi-LSTM Method to Fake News Detection", "abstract": "Recent years have seen an explosion in social media usage, allowing people to\nconnect with others. Since the appearance of platforms such as Facebook and\nTwitter, such platforms influence how we speak, think, and behave. This problem\nnegatively undermines confidence in content because of the existence of fake\nnews. For instance, false news was a determining factor in influencing the\noutcome of the U.S. presidential election and other sites. Because this\ninformation is so harmful, it is essential to make sure we have the necessary\ntools to detect and resist it. We applied Bidirectional Long Short-Term Memory\n(Bi-LSTM) to determine if the news is false or real in order to showcase this\nstudy. A number of foreign websites and newspapers were used for data\ncollection. After creating & running the model, the work achieved 84% model\naccuracy and 62.0 F1-macro scores with training data.", "published": "2022-06-15 06:36:42", "link": "http://arxiv.org/abs/2206.13982v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Management System with NLP-Assisted Annotations: A Brief\n  Survey and Outlook", "abstract": "Knowledge management systems (KMS) are in high demand for industrial\nresearchers, chemical or research enterprises, or evidence-based decision\nmaking. However, existing systems have limitations in categorizing and\norganizing paper insights or relationships. Traditional databases are usually\ndisjoint with logging systems, which limit its utility in generating concise,\ncollated overviews. In this work, we briefly survey existing approaches of this\nproblem space and propose a unified framework that utilizes relational\ndatabases to log hierarchical information to facilitate the research and\nwriting process, or generate useful knowledge from references or insights from\nconnected concepts. Our framework of bidirectional knowledge management system\n(BKMS) enables novel functionalities encompassing improved hierarchical\nnote-taking, AI-assisted brainstorming, and multi-directional relationships.\nPotential applications include managing inventories and changes for manufacture\nor research enterprises, or generating analytic reports with evidence-based\ndecision making.", "published": "2022-06-15 05:20:38", "link": "http://arxiv.org/abs/2206.07304v2", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.DB"}
{"title": "The ZevoMOS entry to VoiceMOS Challenge 2022", "abstract": "This paper introduces the ZevoMOS entry to the main track of the VoiceMOS\nChallenge 2022. The ZevoMOS submission is based on a two-step finetuning of\npretrained self-supervised learning (SSL) speech models. The first step uses a\ntask of classifying natural versus synthetic speech, while the second step's\ntask is to predict the MOS scores associated with each training sample. The\nresults of the finetuning process are then combined with the confidence scores\nextracted from an automatic speech recognition model, as well as the raw\nembeddings of the training samples obtained from a wav2vec SSL speech model.\n  The team id assigned to the ZevoMOS system within the VoiceMOS Challenge is\nT01. The submission was placed on the 14th place with respect to the\nsystem-level SRCC, and on the 9th place with respect to the utterance-level\nMSE. The paper also introduces additional evaluations of the intermediate\nresults.", "published": "2022-06-15 10:53:22", "link": "http://arxiv.org/abs/2206.07448v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "EDITnet: A Lightweight Network for Unsupervised Domain Adaptation in\n  Speaker Verification", "abstract": "Performance degradation caused by language mismatch is a common problem when\napplying a speaker verification system on speech data in different languages.\nThis paper proposes a domain transfer network, named EDITnet, to alleviate the\nlanguage-mismatch problem on speaker embeddings without requiring speaker\nlabels. The network leverages a conditional variational auto-encoder to\ntransfer embeddings from the target domain into the source domain. A\nself-supervised learning strategy is imposed on the transferred embeddings so\nas to increase the cosine distance between embeddings from different speakers.\nIn the training process of the EDITnet, the embedding extraction model is fixed\nwithout fine-tuning, which renders the training efficient and low-cost.\nExperiments on Voxceleb and CN-Celeb show that the embeddings transferred by\nEDITnet outperform the un-transferred ones by around 30% with the\nECAPA-TDNN512. Performance improvement can also be achieved with other\nembedding extraction models, e.g., TDNN, SE-ResNet34.", "published": "2022-06-15 14:10:00", "link": "http://arxiv.org/abs/2206.07548v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learnable Frequency Filters for Speech Feature Extraction in Speaker\n  Verification", "abstract": "Mel-scale spectrum features are used in various recognition and\nclassification tasks on speech signals. There is no reason to expect that these\nfeatures are optimal for all different tasks, including speaker verification\n(SV). This paper describes a learnable front-end feature extraction model. The\nmodel comprises a group of filters to transform the Fourier spectrum. Model\nparameters that define these filters are trained end-to-end and optimized\nspecifically for the task of speaker verification. Compared to the standard\nMel-scale filter-bank, the filters' bandwidths and center frequencies are\nadjustable. Experimental results show that applying the learnable acoustic\nfront-end improves speaker verification performance over conventional Mel-scale\nspectrum features. Analysis on the learned filter parameters suggests that\nnarrow-band information benefits the SV system performance. The proposed model\nachieves a good balance between performance and computation cost. In\nresource-constrained computation settings, the model significantly outperforms\nCNN-based learnable front-ends. The generalization ability of the proposed\nmodel is also demonstrated on different embedding extraction models and\ndatasets.", "published": "2022-06-15 14:28:43", "link": "http://arxiv.org/abs/2206.07563v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Streaming non-autoregressive model for any-to-many voice conversion", "abstract": "Voice conversion models have developed for decades, and current mainstream\nresearch focuses on non-streaming voice conversion. However, streaming voice\nconversion is more suitable for practical application scenarios than\nnon-streaming voice conversion. In this paper, we propose a streaming\nany-to-many voice conversion based on fully non-autoregressive model, which\nincludes a streaming transformer based acoustic model and a streaming vocoder.\nStreaming transformer based acoustic model is composed of a pre-trained encoder\nfrom streaming end-to-end based automatic speech recognition model and a\ndecoder modified on FastSpeech blocks. Streaming vocoder is designed for\nstreaming task with pseudo quadrature mirror filter bank and causal\nconvolution. Experimental results show that the proposed method achieves\nsignificant performance both in latency and conversion quality and can be\nreal-time on CPU and GPU.", "published": "2022-06-15 04:04:14", "link": "http://arxiv.org/abs/2206.07288v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FRCRN: Boosting Feature Representation using Frequency Recurrence for\n  Monaural Speech Enhancement", "abstract": "Convolutional recurrent networks (CRN) integrating a convolutional\nencoder-decoder (CED) structure and a recurrent structure have achieved\npromising performance for monaural speech enhancement. However, feature\nrepresentation across frequency context is highly constrained due to limited\nreceptive fields in the convolutions of CED. In this paper, we propose a\nconvolutional recurrent encoder-decoder (CRED) structure to boost feature\nrepresentation along the frequency axis. The CRED applies frequency recurrence\non 3D convolutional feature maps along the frequency axis following each\nconvolution, therefore, it is capable of catching long-range frequency\ncorrelations and enhancing feature representations of speech inputs. The\nproposed frequency recurrence is realized efficiently using a feedforward\nsequential memory network (FSMN). Besides the CRED, we insert two stacked FSMN\nlayers between the encoder and the decoder to model further temporal dynamics.\nWe name the proposed framework as Frequency Recurrent CRN (FRCRN). We design\nFRCRN to predict complex Ideal Ratio Mask (cIRM) in complex-valued domain and\noptimize FRCRN using both time-frequency-domain and time-domain losses. Our\nproposed approach achieved state-of-the-art performance on wideband benchmark\ndatasets and achieved 2nd place for the real-time fullband track in terms of\nMean Opinion Score (MOS) and Word Accuracy (WAcc) in the ICASSP 2022 Deep Noise\nSuppression (DNS) challenge\n(https://github.com/modelscope/ClearerVoice-Studio).", "published": "2022-06-15 04:29:10", "link": "http://arxiv.org/abs/2206.07293v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting Cross-domain And Cross-Lingual Ultrasound Tongue Imaging\n  Features For Elderly And Dysarthric Speech Recognition", "abstract": "Articulatory features are inherently invariant to acoustic signal distortion\nand have been successfully incorporated into automatic speech recognition (ASR)\nsystems designed for normal speech. Their practical application to atypical\ntask domains such as elderly and disordered speech across languages is often\nlimited by the difficulty in collecting such specialist data from target\nspeakers. This paper presents a cross-domain and cross-lingual A2A inversion\napproach that utilizes the parallel audio and ultrasound tongue imaging (UTI)\ndata of the 24-hour TaL corpus in A2A model pre-training before being\ncross-domain and cross-lingual adapted to three datasets across two languages:\nthe English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech corpora;\nand the English TORGO dysarthric speech data, to produce UTI based articulatory\nfeatures. Experiments conducted on three tasks suggested incorporating the\ngenerated articulatory features consistently outperformed the baseline TDNN and\nConformer ASR systems constructed using acoustic features only by statistically\nsignificant word or character error rate reductions up to 4.75%, 2.59% and\n2.07% absolute (14.69%, 10.64% and 22.72% relative) after data augmentation,\nspeaker adaptation and cross system multi-pass decoding were applied.", "published": "2022-06-15 07:20:28", "link": "http://arxiv.org/abs/2206.07327v3", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "On the Design and Training Strategies for RNN-based Online Neural Speech\n  Separation Systems", "abstract": "While the performance of offline neural speech separation systems has been\ngreatly advanced by the recent development of novel neural network\narchitectures, there is typically an inevitable performance gap between the\nsystems and their online variants. In this paper, we investigate how RNN-based\noffline neural speech separation systems can be changed into their online\ncounterparts while mitigating the performance degradation. We decompose or\nreorganize the forward and backward RNN layers in a bidirectional RNN layer to\nform an online path and an offline path, which enables the model to perform\nboth online and offline processing with a same set of model parameters. We\nfurther introduce two training strategies for improving the online model via\neither a pretrained offline model or a multitask training objective. Experiment\nresults show that compared to the online models that are trained from scratch,\nthe proposed layer decomposition and reorganization schemes and training\nstrategies can effectively mitigate the performance gap between two RNN-based\noffline separation models and their online variants.", "published": "2022-06-15 07:48:39", "link": "http://arxiv.org/abs/2206.07340v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Use of Deep Mask Estimation Module for Neural Source Separation\n  Systems", "abstract": "Most of the recent neural source separation systems rely on a masking-based\npipeline where a set of multiplicative masks are estimated from and applied to\na signal representation of the input mixture. The estimation of such masks, in\nalmost all network architectures, is done by a single layer followed by an\noptional nonlinear activation function. However, recent literatures have\ninvestigated the use of a deep mask estimation module and observed performance\nimprovement compared to a shallow mask estimation module. In this paper, we\nanalyze the role of such deeper mask estimation module by connecting it to a\nrecently proposed unsupervised source separation method, and empirically show\nthat the deep mask estimation module is an efficient approximation of the\nso-called overseparation-grouping paradigm with the conventional shallow mask\nestimation layers.", "published": "2022-06-15 07:57:39", "link": "http://arxiv.org/abs/2206.07347v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Residual Language Model for End-to-end Speech Recognition", "abstract": "End-to-end automatic speech recognition suffers from adaptation to unknown\ntarget domain speech despite being trained with a large amount of paired\naudio--text data. Recent studies estimate a linguistic bias of the model as the\ninternal language model (LM). To effectively adapt to the target domain, the\ninternal LM is subtracted from the posterior during inference and fused with an\nexternal target-domain LM. However, this fusion complicates the inference and\nthe estimation of the internal LM may not always be accurate. In this paper, we\npropose a simple external LM fusion method for domain adaptation, which\nconsiders the internal LM estimation in its training. We directly model the\nresidual factor of the external and internal LMs, namely the residual LM. To\nstably train the residual LM, we propose smoothing the estimated internal LM\nand optimizing it with a combination of cross-entropy and mean-squared-error\nlosses, which consider the statistical behaviors of the internal LM in the\ntarget domain data. We experimentally confirmed that the proposed residual LM\nperforms better than the internal LM estimation in most of the cross-domain and\nintra-domain scenarios.", "published": "2022-06-15 10:04:30", "link": "http://arxiv.org/abs/2206.07430v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Voice Conversion with Information Perturbation", "abstract": "The ideal goal of voice conversion is to convert the source speaker's speech\nto sound naturally like the target speaker while maintaining the linguistic\ncontent and the prosody of the source speech. However, current approaches are\ninsufficient to achieve comprehensive source prosody transfer and target\nspeaker timbre preservation in the converted speech, and the quality of the\nconverted speech is also unsatisfied due to the mismatch between the acoustic\nmodel and the vocoder. In this paper, we leverage the recent advances in\ninformation perturbation and propose a fully end-to-end approach to conduct\nhigh-quality voice conversion. We first adopt information perturbation to\nremove speaker-related information in the source speech to disentangle speaker\ntimbre and linguistic content and thus the linguistic information is\nsubsequently modeled by a content encoder. To better transfer the prosody of\nthe source speech to the target, we particularly introduce a speaker-related\npitch encoder which can maintain the general pitch pattern of the source\nspeaker while flexibly modifying the pitch intensity of the generated speech.\nFinally, one-shot voice conversion is set up through continuous speaker space\nmodeling. Experimental results indicate that the proposed end-to-end approach\nsignificantly outperforms the state-of-the-art models in terms of\nintelligibility, naturalness, and speaker similarity.", "published": "2022-06-15 14:38:31", "link": "http://arxiv.org/abs/2206.07569v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on\n  Data-Driven Deep Learning", "abstract": "Emotion classification of speech and assessment of the emotion strength are\nrequired in applications such as emotional text-to-speech and voice conversion.\nThe emotion attribute ranking function based on Support Vector Machine (SVM)\nwas proposed to predict emotion strength for emotional speech corpus. However,\nthe trained ranking function doesn't generalize to new domains, which limits\nthe scope of applications, especially for out-of-domain or unseen speech. In\nthis paper, we propose a data-driven deep learning model, i.e. StrengthNet, to\nimprove the generalization of emotion strength assessment for seen and unseen\nspeech. This is achieved by the fusion of emotional data from various domains.\nWe follow a multi-task learning network architecture that includes an acoustic\nencoder, a strength predictor, and an auxiliary emotion predictor. Experiments\nshow that the predicted emotion strength of the proposed StrengthNet is highly\ncorrelated with ground truth scores for both seen and unseen speech. We release\nthe source codes at: https://github.com/ttslr/StrengthNet.", "published": "2022-06-15 01:25:32", "link": "http://arxiv.org/abs/2206.07229v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Latency Control for Keyword Spotting", "abstract": "Conversational agents commonly utilize keyword spotting (KWS) to initiate\nvoice interaction with the user. For user experience and privacy\nconsiderations, existing approaches to KWS largely focus on accuracy, which can\noften come at the expense of introduced latency. To address this tradeoff, we\npropose a novel approach to control KWS model latency and which generalizes to\nany loss function without explicit knowledge of the keyword endpoint. Through a\nsingle, tunable hyperparameter, our approach enables one to balance detection\nlatency and accuracy for the targeted application. Empirically, we show that\nour approach gives superior performance under latency constraints when compared\nto existing methods. Namely, we make a substantial 25\\% relative false accepts\nimprovement for a fixed latency target when compared to the baseline\nstate-of-the-art. We also show that when our approach is used in conjunction\nwith a max-pooling loss, we are able to improve relative false accepts by 25 %\nat a fixed latency when compared to cross entropy loss.", "published": "2022-06-15 02:45:28", "link": "http://arxiv.org/abs/2206.07261v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Text-Aware End-to-end Mispronunciation Detection and Diagnosis", "abstract": "Mispronunciation detection and diagnosis (MDD) technology is a key component\nof computer-assisted pronunciation training system (CAPT). In the field of\nassessing the pronunciation quality of constrained speech, the given\ntranscriptions can play the role of a teacher. Conventional methods have fully\nutilized the prior texts for the model construction or improving the system\nperformance, e.g. forced-alignment and extended recognition networks. Recently,\nsome end-to-end based methods attempt to incorporate the prior texts into model\ntraining and preliminarily show the effectiveness. However, previous studies\nmostly consider applying raw attention mechanism to fuse audio representations\nwith text representations, without taking possible text-pronunciation mismatch\ninto account. In this paper, we present a gating strategy that assigns more\nimportance to the relevant audio features while suppressing irrelevant text\ninformation. Moreover, given the transcriptions, we design an extra contrastive\nloss to reduce the gap between the learning objective of phoneme recognition\nand MDD. We conducted experiments using two publicly available datasets (TIMIT\nand L2-Arctic) and our best model improved the F1 score from $57.51\\%$ to\n$61.75\\%$ compared to the baselines. Besides, we provide a detailed analysis to\nshed light on the effectiveness of gating mechanism and contrastive learning on\nMDD.", "published": "2022-06-15 04:08:10", "link": "http://arxiv.org/abs/2206.07289v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via\n  Speech-Visage Feature Selection", "abstract": "The goal of this work is to reconstruct speech from a silent talking face\nvideo. Recent studies have shown impressive performance on synthesizing speech\nfrom silent talking face videos. However, they have not explicitly considered\non varying identity characteristics of different speakers, which place a\nchallenge in the video-to-speech synthesis, and this becomes more critical in\nunseen-speaker settings. Our approach is to separate the speech content and the\nvisage-style from a given silent talking face video. By guiding the model to\nindependently focus on modeling the two representations, we can obtain the\nspeech of high intelligibility from the model even when the input video of an\nunseen subject is given. To this end, we introduce speech-visage selection that\nseparates the speech content and the speaker identity from the visual features\nof the input video. The disentangled representations are jointly incorporated\nto synthesize speech through visage-style based synthesizer which generates\nspeech by coating the visage-styles while maintaining the speech content. Thus,\nthe proposed framework brings the advantage of synthesizing the speech\ncontaining the right content even with the silent talking face video of an\nunseen subject. We validate the effectiveness of the proposed framework on the\nGRID, TCD-TIMIT volunteer, and LRW datasets.", "published": "2022-06-15 11:29:58", "link": "http://arxiv.org/abs/2206.07458v2", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Investigating Multi-Feature Selection and Ensembling for Audio\n  Classification", "abstract": "Deep Learning (DL) algorithms have shown impressive performance in diverse\ndomains. Among them, audio has attracted many researchers over the last couple\nof decades due to some interesting patterns--particularly in classification of\naudio data. For better performance of audio classification, feature selection\nand combination play a key role as they have the potential to make or break the\nperformance of any DL model. To investigate this role, we conduct an extensive\nevaluation of the performance of several cutting-edge DL models (i.e.,\nConvolutional Neural Network, EfficientNet, MobileNet, Supper Vector Machine\nand Multi-Perceptron) with various state-of-the-art audio features (i.e., Mel\nSpectrogram, Mel Frequency Cepstral Coefficients, and Zero Crossing Rate)\neither independently or as a combination (i.e., through ensembling) on three\ndifferent datasets (i.e., Free Spoken Digits Dataset, Audio Urdu Digits\nDataset, and Audio Gujarati Digits Dataset). Overall, results suggest feature\nselection depends on both the dataset and the model. However, feature\ncombinations should be restricted to the only features that already achieve\ngood performances when used individually (i.e., mostly Mel Spectrogram, Mel\nFrequency Cepstral Coefficients). Such feature combination/ensembling enabled\nus to outperform the previous state-of-the-art results irrespective of our\nchoice of DL model.", "published": "2022-06-15 13:11:08", "link": "http://arxiv.org/abs/2206.07511v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AVATAR: Unconstrained Audiovisual Speech Recognition", "abstract": "Audio-visual automatic speech recognition (AV-ASR) is an extension of ASR\nthat incorporates visual cues, often from the movements of a speaker's mouth.\nUnlike works that simply focus on the lip motion, we investigate the\ncontribution of entire visual frames (visual actions, objects, background\netc.). This is particularly useful for unconstrained videos, where the speaker\nis not necessarily visible. To solve this task, we propose a new\nsequence-to-sequence AudioVisual ASR TrAnsformeR (AVATAR) which is trained\nend-to-end from spectrograms and full-frame RGB. To prevent the audio stream\nfrom dominating training, we propose different word-masking strategies, thereby\nencouraging our model to pay attention to the visual stream. We demonstrate the\ncontribution of the visual modality on the How2 AV-ASR benchmark, especially in\nthe presence of simulated noise, and show that our model outperforms all other\nprior work by a large margin. Finally, we also create a new, real-world test\nbed for AV-ASR called VisSpeech, which demonstrates the contribution of the\nvisual modality under challenging audio conditions.", "published": "2022-06-15 17:33:19", "link": "http://arxiv.org/abs/2206.07684v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
