{"title": "A Survey for Biomedical Text Summarization: From Pre-trained to Large\n  Language Models", "abstract": "The exponential growth of biomedical texts such as biomedical literature and\nelectronic health records (EHRs), poses a significant challenge for clinicians\nand researchers to access clinical information efficiently. To tackle this\nchallenge, biomedical text summarization (BTS) has been proposed as a solution\nto support clinical information retrieval and management. BTS aims at\ngenerating concise summaries that distill key information from single or\nmultiple biomedical documents. In recent years, the rapid advancement of\nfundamental natural language processing (NLP) techniques, from pre-trained\nlanguage models (PLMs) to large language models (LLMs), has greatly facilitated\nthe progress of BTS. This growth has led to numerous proposed summarization\nmethods, datasets, and evaluation metrics, raising the need for a comprehensive\nand up-to-date survey for BTS. In this paper, we present a systematic review of\nrecent advancements in BTS, leveraging cutting-edge NLP techniques from PLMs to\nLLMs, to help understand the latest progress, challenges, and future\ndirections. We begin by introducing the foundational concepts of BTS, PLMs and\nLLMs, followed by an in-depth review of available datasets, recent approaches,\nand evaluation metrics in BTS. We finally discuss existing challenges and\npromising future directions in the era of LLMs. To facilitate the research\ncommunity, we line up open resources including available datasets, recent\napproaches, codes, evaluation metrics, and the leaderboard in a public project:\nhttps://github.com/KenZLuo/Biomedical-Text-Summarization-Survey/tree/master. We\nbelieve that this survey will be a useful resource to researchers, allowing\nthem to quickly track recent advancements and provide guidelines for future BTS\nresearch within the research community.", "published": "2023-04-18 06:38:40", "link": "http://arxiv.org/abs/2304.08763v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Role of Similarity and Dissimilarity in Best Counter\n  Argument Retrieval", "abstract": "This paper studies the task of best counter-argument retrieval given an input\nargument. Following the definition that the best counter-argument addresses the\nsame aspects as the input argument while having the opposite stance, we aim to\ndevelop an efficient and effective model for scoring counter-arguments based on\nsimilarity and dissimilarity metrics. We first conduct an experimental study on\nthe effectiveness of available scoring methods, including traditional\nLearning-To-Rank (LTR) and recent neural scoring models. We then propose\nBipolar-encoder, a novel BERT-based model to learn an optimal representation\nfor simultaneous similarity and dissimilarity. Experimental results show that\nour proposed method can achieve the accuracy@1 of 49.04\\%, which significantly\noutperforms other baselines by a large margin. When combined with an\nappropriate caching technique, Bipolar-encoder is comparably efficient at\nprediction time.", "published": "2023-04-18 08:13:48", "link": "http://arxiv.org/abs/2304.08807v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer to a Low-Resource Language via Close Relatives: The Case Study\n  on Faroese", "abstract": "Multilingual language models have pushed state-of-the-art in cross-lingual\nNLP transfer. The majority of zero-shot cross-lingual transfer, however, use\none and the same massively multilingual transformer (e.g., mBERT or XLM-R) to\ntransfer to all target languages, irrespective of their typological,\netymological, and phylogenetic relations to other languages. In particular,\nreadily available data and models of resource-rich sibling languages are often\nignored. In this work, we empirically show, in a case study for Faroese -- a\nlow-resource language from a high-resource language family -- that by\nleveraging the phylogenetic information and departing from the\n'one-size-fits-all' paradigm, one can improve cross-lingual transfer to\nlow-resource languages. In particular, we leverage abundant resources of other\nScandinavian languages (i.e., Danish, Norwegian, Swedish, and Icelandic) for\nthe benefit of Faroese. Our evaluation results show that we can substantially\nimprove the transfer performance to Faroese by exploiting data and models of\nclosely-related high-resource languages. Further, we release a new web corpus\nof Faroese and Faroese datasets for named entity recognition (NER), semantic\ntext similarity (STS), and new language models trained on all Scandinavian\nlanguages.", "published": "2023-04-18 08:42:38", "link": "http://arxiv.org/abs/2304.08823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tailoring Domain Adaptation for Machine Translation Quality Estimation", "abstract": "While quality estimation (QE) can play an important role in the translation\nprocess, its effectiveness relies on the availability and quality of training\ndata. For QE in particular, high-quality labeled data is often lacking due to\nthe high cost and effort associated with labeling such data. Aside from the\ndata scarcity challenge, QE models should also be generalizable, i.e., they\nshould be able to handle data from different domains, both generic and\nspecific. To alleviate these two main issues -- data scarcity and domain\nmismatch -- this paper combines domain adaptation and data augmentation within\na robust QE system. Our method first trains a generic QE model and then\nfine-tunes it on a specific domain while retaining generic knowledge. Our\nresults show a significant improvement for all the language pairs investigated,\nbetter cross-lingual inference, and a superior performance in zero-shot\nlearning scenarios as compared to state-of-the-art baselines.", "published": "2023-04-18 10:36:50", "link": "http://arxiv.org/abs/2304.08891v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Zero-Shot Personalized Table-to-Text Generation with Contrastive\n  Persona Distillation", "abstract": "Existing neural methods have shown great potentials towards generating\ninformative text from structured tabular data as well as maintaining high\ncontent fidelity. However, few of them shed light on generating personalized\nexpressions, which often requires well-aligned persona-table-text datasets that\nare difficult to obtain. To overcome these obstacles, we explore personalized\ntable-to-text generation under a zero-shot setting, by assuming no well-aligned\npersona-table-text triples are required during training. To this end, we\nfirstly collect a set of unpaired persona information and then propose a\nsemi-supervised approach with contrastive persona distillation (S2P-CPD) to\ngenerate personalized context. Specifically, tabular data and persona\ninformation are firstly represented as latent variables separately. Then, we\ndevise a latent space fusion technique to distill persona information into the\ntable representation. Besides, a contrastive-based discriminator is employed to\nguarantee the style consistency between the generated context and its\ncorresponding persona. Experimental results on two benchmarks demonstrate\nS2P-CPD's ability on keeping both content fidelity and personalized\nexpressions.", "published": "2023-04-18 11:32:33", "link": "http://arxiv.org/abs/2304.08911v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Trade-Offs: Unified Large Language Models vs Local\n  Fine-Tuned Models for Highly-Specific Radiology NLI Task", "abstract": "Recently, ChatGPT and GPT-4 have emerged and gained immense global attention\ndue to their unparalleled performance in language processing. Despite\ndemonstrating impressive capability in various open-domain tasks, their\nadequacy in highly specific fields like radiology remains untested. Radiology\npresents unique linguistic phenomena distinct from open-domain data due to its\nspecificity and complexity. Assessing the performance of large language models\n(LLMs) in such specific domains is crucial not only for a thorough evaluation\nof their overall performance but also for providing valuable insights into\nfuture model design directions: whether model design should be generic or\ndomain-specific. To this end, in this study, we evaluate the performance of\nChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned\nspecifically on task-related data samples. We also conduct a comprehensive\ninvestigation on ChatGPT/GPT-4's reasoning ability by introducing varying\nlevels of inference difficulty. Our results show that 1) GPT-4 outperforms\nChatGPT in the radiology NLI task; 2) other specifically fine-tuned models\nrequire significant amounts of data samples to achieve comparable performance\nto ChatGPT/GPT-4. These findings demonstrate that constructing a generic model\nthat is capable of solving various tasks across different domains is feasible.", "published": "2023-04-18 17:21:48", "link": "http://arxiv.org/abs/2304.09138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Outlier Suppression+: Accurate quantization of large language models by\n  equivalent and optimal shifting and scaling", "abstract": "Post-training quantization~(PTQ) of transformer language models faces\nsignificant challenges due to the existence of detrimental outliers in\nactivations. We observe that these outliers are concentrated in specific\nchannels and are asymmetric across channels. To address this issue, we propose\nthe Outlier Suppression+~(OS+) framework, which contains the channel-wise\nshifting for asymmetry and channel-wise scaling for concentration. We show that\nthese operations can be seamlessly migrated into subsequent modules while\nmaintaining equivalence. Second, we propose a fast and stable scheme to\ncalculate effective shifting and scaling values. The channel-wise shifting\naligns the center of each channel for removal of outlier asymmetry. The\nchannel-wise scaling quantitatively evaluates changes brought by migration and\nquantization for better quantization burden balance. We validate our OS+ under\nboth standard and fine-grained quantization settings with models including\nBERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks\ndemonstrate the superiority of our approach. Especially, with standard\nquantization, OS+ can achieve near-floating-point performance on both small\nmodels and large language models on 8-bit and 6-bit. Besides, we establish a\nnew state-of-the-art for 4-bit BERT with 15.5\\% improvement. Our code is\navailable at \\url{https://github.com/ModelTC/Outlier_Suppression_Plus}.", "published": "2023-04-18 17:34:23", "link": "http://arxiv.org/abs/2304.09145v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniMax: Fairer and more Effective Language Sampling for Large-Scale\n  Multilingual Pretraining", "abstract": "Pretrained multilingual large language models have typically used heuristic\ntemperature-based sampling to balance between different languages. However\nprevious work has not systematically evaluated the efficacy of different\npretraining language distributions across model scales. In this paper, we\npropose a new sampling method, UniMax, that delivers more uniform coverage of\nhead languages while mitigating overfitting on tail languages by explicitly\ncapping the number of repeats over each language's corpus. We perform an\nextensive series of ablations testing a range of sampling strategies on a suite\nof multilingual benchmarks, while varying model scale. We find that UniMax\noutperforms standard temperature-based sampling, and the benefits persist as\nscale increases. As part of our contribution, we release: (i) an improved and\nrefreshed mC4 multilingual corpus consisting of 29 trillion characters across\n107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained\nwith UniMax sampling.", "published": "2023-04-18 17:45:50", "link": "http://arxiv.org/abs/2304.09151v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating Large Language Model Resistant Exams: Guidelines and Strategies", "abstract": "The proliferation of Large Language Models (LLMs), such as ChatGPT, has\nraised concerns about their potential impact on academic integrity, prompting\nthe need for LLM-resistant exam designs. This article investigates the\nperformance of LLMs on exams and their implications for assessment, focusing on\nChatGPT's abilities and limitations. We propose guidelines for creating\nLLM-resistant exams, including content moderation, deliberate inaccuracies,\nreal-world scenarios beyond the model's knowledge base, effective distractor\noptions, evaluating soft skills, and incorporating non-textual information. The\narticle also highlights the significance of adapting assessments to modern\ntools and promoting essential skills development in students. By adopting these\nstrategies, educators can maintain academic integrity while ensuring that\nassessments accurately reflect contemporary professional settings and address\nthe challenges and opportunities posed by artificial intelligence in education.", "published": "2023-04-18 18:01:32", "link": "http://arxiv.org/abs/2304.12203v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speaker Profiling in Multiparty Conversations", "abstract": "In conversational settings, individuals exhibit unique behaviors, rendering a\none-size-fits-all approach insufficient for generating responses by dialogue\nagents. Although past studies have aimed to create personalized dialogue agents\nusing speaker persona information, they have relied on the assumption that the\nspeaker's persona is already provided. However, this assumption is not always\nvalid, especially when it comes to chatbots utilized in industries like\nbanking, hotel reservations, and airline bookings. This research paper aims to\nfill this gap by exploring the task of Speaker Profiling in Conversations\n(SPC). The primary objective of SPC is to produce a summary of persona\ncharacteristics for each individual speaker present in a dialogue. To\naccomplish this, we have divided the task into three subtasks: persona\ndiscovery, persona-type identification, and persona-value extraction. Given a\ndialogue, the first subtask aims to identify all utterances that contain\npersona information. Subsequently, the second task evaluates these utterances\nto identify the type of persona information they contain, while the third\nsubtask identifies the specific persona values for each identified type. To\naddress the task of SPC, we have curated a new dataset named SPICE, which comes\nwith specific labels. We have evaluated various baselines on this dataset and\nbenchmarked it with a new neural model, SPOT, which we introduce in this paper.\nFurthermore, we present a comprehensive analysis of SPOT, examining the\nlimitations of individual modules both quantitatively and qualitatively.", "published": "2023-04-18 08:04:46", "link": "http://arxiv.org/abs/2304.08801v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Approximate Nearest Neighbour Phrase Mining for Contextual Speech\n  Recognition", "abstract": "This paper presents an extension to train end-to-end Context-Aware\nTransformer Transducer ( CATT ) models by using a simple, yet efficient method\nof mining hard negative phrases from the latent space of the context encoder.\nDuring training, given a reference query, we mine a number of similar phrases\nusing approximate nearest neighbour search. These sampled phrases are then used\nas negative examples in the context list alongside random and ground truth\ncontextual information. By including approximate nearest neighbour phrases\n(ANN-P) in the context list, we encourage the learned representation to\ndisambiguate between similar, but not identical, biasing phrases. This improves\nbiasing accuracy when there are several similar phrases in the biasing\ninventory. We carry out experiments in a large-scale data regime obtaining up\nto 7% relative word error rate reductions for the contextual portion of test\ndata. We also extend and evaluate CATT approach in streaming applications.", "published": "2023-04-18 09:52:11", "link": "http://arxiv.org/abs/2304.08862v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Romanization-based Large-scale Adaptation of Multilingual Language\n  Models", "abstract": "Large multilingual pretrained language models (mPLMs) have become the de\nfacto state of the art for cross-lingual transfer in NLP. However, their\nlarge-scale deployment to many languages, besides pretraining data scarcity, is\nalso hindered by the increase in vocabulary size and limitations in their\nparameter budget. In order to boost the capacity of mPLMs to deal with\nlow-resource and unseen languages, we explore the potential of leveraging\ntransliteration on a massive scale. In particular, we explore the UROMAN\ntransliteration tool, which provides mappings from UTF-8 to Latin characters\nfor all the writing systems, enabling inexpensive romanization for virtually\nany language. We first focus on establishing how UROMAN compares against other\nlanguage-specific and manually curated transliterators for adapting\nmultilingual PLMs. We then study and compare a plethora of data- and\nparameter-efficient strategies for adapting the mPLMs to romanized and\nnon-romanized corpora of 14 diverse low-resource languages. Our results reveal\nthat UROMAN-based transliteration can offer strong performance for many\nlanguages, with particular gains achieved in the most challenging setups: on\nlanguages with unseen scripts and with limited training data without any\nvocabulary augmentation. Further analyses reveal that an improved tokenizer\nbased on romanized data can even outperform non-transliteration-based methods\nin the majority of languages.", "published": "2023-04-18 09:58:34", "link": "http://arxiv.org/abs/2304.08865v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Textbooks with Visuals from the Web for Improved Learning", "abstract": "Textbooks are one of the main mediums for delivering high-quality education\nto students. In particular, explanatory and illustrative visuals play a key\nrole in retention, comprehension and general transfer of knowledge. However,\nmany textbooks lack these interesting visuals to support student learning. In\nthis paper, we investigate the effectiveness of vision-language models to\nautomatically enhance textbooks with images from the web. We collect a dataset\nof e-textbooks in the math, science, social science and business domains. We\nthen set up a text-image matching task that involves retrieving and\nappropriately assigning web images to textbooks, which we frame as a matching\noptimization problem. Through a crowd-sourced evaluation, we verify that (1)\nwhile the original textbook images are rated higher, automatically assigned\nones are not far behind, and (2) the precise formulation of the optimization\nproblem matters. We release the dataset of textbooks with an associated image\nbank to inspire further research in this intersectional area of computer vision\nand NLP for education.", "published": "2023-04-18 12:16:39", "link": "http://arxiv.org/abs/2304.08931v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised\n  Learning", "abstract": "The first Multimodal Emotion Recognition Challenge (MER 2023) was\nsuccessfully held at ACM Multimedia. The challenge focuses on system robustness\nand consists of three distinct tracks: (1) MER-MULTI, where participants are\nrequired to recognize both discrete and dimensional emotions; (2) MER-NOISE, in\nwhich noise is added to test videos for modality robustness evaluation; (3)\nMER-SEMI, which provides a large amount of unlabeled samples for\nsemi-supervised learning. In this paper, we introduce the motivation behind\nthis challenge, describe the benchmark dataset, and provide some statistics\nabout participants. To continue using this dataset after MER 2023, please sign\na new End User License Agreement and send it to our official email address\nmerchallenge.contact@gmail.com. We believe this high-quality dataset can become\na new benchmark in multimodal emotion recognition, especially for the Chinese\nresearch community.", "published": "2023-04-18 13:23:42", "link": "http://arxiv.org/abs/2304.08981v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence\n  Embeddings", "abstract": "This paper describes Difference-aware Deep continuous prompt for Contrastive\nSentence Embeddings (D2CSE) that learns sentence embeddings. Compared to\nstate-of-the-art approaches, D2CSE computes sentence vectors that are\nexceptional to distinguish a subtle difference in similar sentences by\nemploying a simple neural architecture for continuous prompts. Unlike existing\narchitectures that require multiple pretrained language models (PLMs) to\nprocess a pair of the original and corrupted (subtly modified) sentences, D2CSE\navoids cumbersome fine-tuning of multiple PLMs by only optimizing continuous\nprompts by performing multiple tasks -- i.e., contrastive learning and\nconditional replaced token detection all done in a self-guided manner. D2CSE\noverloads a single PLM on continuous prompts and greatly saves memory\nconsumption as a result. The number of training parameters in D2CSE is reduced\nto about 1\\% of existing approaches while substantially improving the quality\nof sentence embeddings. We evaluate D2CSE on seven Semantic Textual Similarity\n(STS) benchmarks, using three different metrics, namely, Spearman's rank\ncorrelation, recall@K for a retrieval task, and the anisotropy of an embedding\nspace measured in alignment and uniformity. Our empirical results suggest that\nshallow (not too meticulously devised) continuous prompts can be honed\neffectively for multiple NLP tasks and lead to improvements upon existing\nstate-of-the-art approaches.", "published": "2023-04-18 13:45:07", "link": "http://arxiv.org/abs/2304.08991v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Biomedical Entity Extraction Pipeline for Oncology Health Records in\n  Portuguese", "abstract": "Textual health records of cancer patients are usually protracted and highly\nunstructured, making it very time-consuming for health professionals to get a\ncomplete overview of the patient's therapeutic course. As such limitations can\nlead to suboptimal and/or inefficient treatment procedures, healthcare\nproviders would greatly benefit from a system that effectively summarizes the\ninformation of those records. With the advent of deep neural models, this\nobjective has been partially attained for English clinical texts, however, the\nresearch community still lacks an effective solution for languages with limited\nresources. In this paper, we present the approach we developed to extract\nprocedures, drugs, and diseases from oncology health records written in\nEuropean Portuguese. This project was conducted in collaboration with the\nPortuguese Institute for Oncology which, besides holding over $10$ years of\nduly protected medical records, also provided oncologist expertise throughout\nthe development of the project. Since there is no annotated corpus for\nbiomedical entity extraction in Portuguese, we also present the strategy we\nfollowed in annotating the corpus for the development of the models. The final\nmodels, which combined a neural architecture with entity linking, achieved\n$F_1$ scores of $88.6$, $95.0$, and $55.8$ per cent in the mention extraction\nof procedures, drugs, and diseases, respectively.", "published": "2023-04-18 14:10:34", "link": "http://arxiv.org/abs/2304.08999v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Token Imbalance Adaptation for Radiology Report Generation", "abstract": "Imbalanced token distributions naturally exist in text documents, leading\nneural language models to overfit on frequent tokens. The token imbalance may\ndampen the robustness of radiology report generators, as complex medical terms\nappear less frequently but reflect more medical information. In this study, we\ndemonstrate how current state-of-the-art models fail to generate infrequent\ntokens on two standard benchmark datasets (IU X-RAY and MIMIC-CXR) of radiology\nreport generation. % However, no prior study has proposed methods to adapt\ninfrequent tokens for text generators feeding with medical images. To solve the\nchallenge, we propose the \\textbf{T}oken \\textbf{Im}balance Adapt\\textbf{er}\n(\\textit{TIMER}), aiming to improve generation robustness on infrequent tokens.\nThe model automatically leverages token imbalance by an unlikelihood loss and\ndynamically optimizes generation processes to augment infrequent tokens. We\ncompare our approach with multiple state-of-the-art methods on the two\nbenchmarks. Experiments demonstrate the effectiveness of our approach in\nenhancing model robustness overall and infrequent tokens. Our ablation analysis\nshows that our reinforcement learning method has a major effect in adapting\ntoken imbalance for radiology report generation.", "published": "2023-04-18 23:09:36", "link": "http://arxiv.org/abs/2304.09185v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information\n  Retrieval", "abstract": "Efficient information retrieval (IR) from building information models (BIMs)\nposes significant challenges due to the necessity for deep BIM knowledge or\nextensive engineering efforts for automation. We introduce BIM-GPT, a\nprompt-based virtual assistant (VA) framework integrating BIM and generative\npre-trained transformer (GPT) technologies to support NL-based IR. A prompt\nmanager and dynamic template generate prompts for GPT models, enabling\ninterpretation of NL queries, summarization of retrieved information, and\nanswering BIM-related questions. In tests on a BIM IR dataset, our approach\nachieved 83.5% and 99.5% accuracy rates for classifying NL queries with no data\nand 2% data incorporated in prompts, respectively. Additionally, we validated\nthe functionality of BIM-GPT through a VA prototype for a hospital building.\nThis research contributes to the development of effective and versatile VAs for\nBIM IR in the construction industry, significantly enhancing BIM accessibility\nand reducing engineering efforts and training data requirements for processing\nNL queries.", "published": "2023-04-18 22:46:02", "link": "http://arxiv.org/abs/2304.09333v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain\n  Text Classification", "abstract": "Cross-domain text classification aims to adapt models to a target domain that\nlacks labeled data. It leverages or reuses rich labeled data from the different\nbut related source domain(s) and unlabeled data from the target domain. To this\nend, previous work focuses on either extracting domain-invariant features or\ntask-agnostic features, ignoring domain-aware features that may be present in\nthe target domain and could be useful for the downstream task. In this paper,\nwe propose a two-stage framework for cross-domain text classification. In the\nfirst stage, we finetune the model with mask language modeling (MLM) and\nlabeled data from the source domain. In the second stage, we further fine-tune\nthe model with self-supervised distillation (SSD) and unlabeled data from the\ntarget domain. We evaluate its performance on a public cross-domain text\nclassification benchmark and the experiment results show that our method\nachieves new state-of-the-art results for both single-source domain adaptations\n(94.17% $\\uparrow$1.03%) and multi-source domain adaptations (95.09%\n$\\uparrow$1.34%).", "published": "2023-04-18 06:21:40", "link": "http://arxiv.org/abs/2304.09820v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Think Before You Act: Unified Policy for Interleaving Language Reasoning\n  with Actions", "abstract": "The success of transformer models trained with a language modeling objective\nbrings a promising opportunity to the reinforcement learning framework.\nDecision Transformer is a step towards this direction, showing how to train\ntransformers with a similar next-step prediction objective on offline data.\nAnother important development in this area is the recent emergence of\nlarge-scale datasets collected from the internet, such as the ones composed of\ntutorial videos with captions where people talk about what they are doing. To\ntake advantage of this language component, we propose a novel method for\nunifying language reasoning with actions in a single policy. Specifically, we\naugment a transformer policy with word outputs, so it can generate textual\ncaptions interleaved with actions. When tested on the most challenging task in\nBabyAI, with captions describing next subgoals, our reasoning policy\nconsistently outperforms the caption-free baseline.", "published": "2023-04-18 16:12:38", "link": "http://arxiv.org/abs/2304.11063v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HeRo: RoBERTa and Longformer Hebrew Language Models", "abstract": "In this paper, we fill in an existing gap in resources available to the\nHebrew NLP community by providing it with the largest so far pre-train dataset\nHeDC4, a state-of-the-art pre-trained language model HeRo for standard length\ninputs and an efficient transformer LongHeRo for long input sequences. The HeRo\nmodel was evaluated on the sentiment analysis, the named entity recognition,\nand the question answering tasks while the LongHeRo model was evaluated on the\ndocument classification task with a dataset composed of long documents. Both\nHeRo and LongHeRo presented state-of-the-art performance. The dataset and model\ncheckpoints used in this work are publicly available.", "published": "2023-04-18 05:56:32", "link": "http://arxiv.org/abs/2304.11077v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparative study on Judgment Text Classification for Transformer Based\n  Models", "abstract": "This work involves the usage of various NLP models to predict the winner of a\nparticular judgment by the means of text extraction and summarization from a\njudgment document. These documents are useful when it comes to legal\nproceedings. One such advantage is that these can be used for citations and\nprecedence reference in Lawsuits and cases which makes a strong argument for\ntheir case by the ones using it. When it comes to precedence, it is necessary\nto refer to an ample number of documents in order to collect legal points with\nrespect to the case. However, reviewing these documents takes a long time to\nanalyze due to the complex word structure and the size of the document. This\nwork involves the comparative study of 6 different self-attention-based\ntransformer models and how they perform when they are being tweaked in 4\ndifferent activation functions. These models which are trained with 200\njudgement contexts and their results are being judged based on different\nbenchmark parameters. These models finally have a confidence level up to 99%\nwhile predicting the judgment. This can be used to get a particular judgment\ndocument without spending too much time searching relevant cases and reading\nthem completely.", "published": "2023-04-18 02:20:43", "link": "http://arxiv.org/abs/2306.01739v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TTIDA: Controllable Generative Data Augmentation via Text-to-Text and\n  Text-to-Image Models", "abstract": "Data augmentation has been established as an efficacious approach to\nsupplement useful information for low-resource datasets. Traditional\naugmentation techniques such as noise injection and image transformations have\nbeen widely used. In addition, generative data augmentation (GDA) has been\nshown to produce more diverse and flexible data. While generative adversarial\nnetworks (GANs) have been frequently used for GDA, they lack diversity and\ncontrollability compared to text-to-image diffusion models. In this paper, we\npropose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the\ncapabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image\n(T2I) generative models for data augmentation. By conditioning the T2I model on\ndetailed descriptions produced by T2T models, we are able to generate\nphoto-realistic labeled images in a flexible and controllable manner.\nExperiments on in-domain classification, cross-domain classification, and image\ncaptioning tasks show consistent improvements over other data augmentation\nbaselines. Analytical studies in varied settings, including few-shot,\nlong-tail, and adversarial, further reinforce the effectiveness of TTIDA in\nenhancing performance and increasing robustness.", "published": "2023-04-18 08:40:30", "link": "http://arxiv.org/abs/2304.08821v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to\n  Fine-Tune and Hard to Detect with other LLMs", "abstract": "The self-attention revolution allowed generative language models to scale and\nachieve increasingly impressive abilities. Such models - commonly referred to\nas Large Language Models (LLMs) - have recently gained prominence with the\ngeneral public, thanks to conversational fine-tuning, putting their behavior in\nline with public expectations regarding AI. This prominence amplified prior\nconcerns regarding the misuse of LLMs and led to the emergence of numerous\ntools to detect LLMs in the wild.\n  Unfortunately, most such tools are critically flawed. While major\npublications in the LLM detectability field suggested that LLMs were easy to\ndetect with fine-tuned autoencoders, the limitations of their results are easy\nto overlook. Specifically, they assumed publicly available generative models\nwithout fine-tunes or non-trivial prompts. While the importance of these\nassumptions has been demonstrated, until now, it remained unclear how well such\ndetection could be countered.\n  Here, we show that an attacker with access to such detectors' reference human\ntexts and output not only evades detection but can fully frustrate the detector\ntraining - with a reasonable budget and all its outputs labeled as such.\nAchieving it required combining common \"reinforcement from critic\" loss\nfunction modification and AdamW optimizer, which led to surprisingly good\nfine-tuning generalization. Finally, we warn against the temptation to\ntranspose the conclusions obtained in RNN-driven text GANs to LLMs due to their\nbetter representative ability.\n  These results have critical implications for the detection and prevention of\nmalicious use of generative language models, and we hope they will aid the\ndesigners of generative models and detectors.", "published": "2023-04-18 13:05:01", "link": "http://arxiv.org/abs/2304.08968v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "I.2.7; K.6.5"], "primary_category": "cs.CL"}
{"title": "A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of\n  computing and functional programming", "abstract": "Over the last decades, deep neural networks based-models became the dominant\nparadigm in machine learning. Further, the use of artificial neural networks in\nsymbolic learning has been seen as increasingly relevant recently. To study the\ncapabilities of neural networks in the symbolic AI domain, researchers have\nexplored the ability of deep neural networks to learn mathematical\nconstructions, such as addition and multiplication, logic inference, such as\ntheorem provers, and even the execution of computer programs. The latter is\nknown to be too complex a task for neural networks. Therefore, the results were\nnot always successful, and often required the introduction of biased elements\nin the learning process, in addition to restricting the scope of possible\nprograms to be executed. In this work, we will analyze the ability of neural\nnetworks to learn how to execute programs as a whole. To do so, we propose a\ndifferent approach. Instead of using an imperative programming language, with\ncomplex structures, we use the Lambda Calculus ({\\lambda}-Calculus), a simple,\nbut Turing-Complete mathematical formalism, which serves as the basis for\nmodern functional programming languages and is at the heart of computability\ntheory. We will introduce the use of integrated neural learning and lambda\ncalculi formalization. Finally, we explore execution of a program in\n{\\lambda}-Calculus is based on reductions, we will show that it is enough to\nlearn how to perform these reductions so that we can execute any program.\nKeywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural Networks,\nTransformer Model, Sequence-to-Sequence Models, Computational Models", "published": "2023-04-18 20:30:16", "link": "http://arxiv.org/abs/2304.09276v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.LG"}
{"title": "The Unintended Consequences of Censoring Digital Technology -- Evidence\n  from Italy's ChatGPT Ban", "abstract": "We analyse the effects of the ban of ChatGPT, a generative pre-trained\ntransformer chatbot, on individual productivity. We first compile data on the\nhourly coding output of over 8,000 professional GitHub users in Italy and other\nEuropean countries to analyse the impact of the ban on individual productivity.\nCombining the high-frequency data with the sudden announcement of the ban in a\ndifference-in-differences framework, we find that the output of Italian\ndevelopers decreased by around 50% in the first two business days after the ban\nand recovered after that. Applying a synthetic control approach to daily Google\nsearch and Tor usage data shows that the ban led to a significant increase in\nthe use of censorship bypassing tools. Our findings show that users swiftly\nimplement strategies to bypass Internet restrictions but this adaptation\nactivity creates short-term disruptions and hampers productivity.", "published": "2023-04-18 23:11:04", "link": "http://arxiv.org/abs/2304.09339v1", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained\n  Language Models", "abstract": "Large pre-trained language models (LLMs) have been shown to have significant\npotential in few-shot learning across various fields, even with minimal\ntraining data. However, their ability to generalize to unseen tasks in more\ncomplex fields, such as biology, has yet to be fully evaluated. LLMs can offer\na promising alternative approach for biological inference, particularly in\ncases where structured data and sample size are limited, by extracting prior\nknowledge from text corpora. Our proposed few-shot learning approach uses LLMs\nto predict the synergy of drug pairs in rare tissues that lack structured data\nand features. Our experiments, which involved seven rare tissues from different\ncancer types, demonstrated that the LLM-based prediction model achieved\nsignificant accuracy with very few or zero samples. Our proposed model, the\nCancerGPT (with $\\sim$ 124M parameters), was even comparable to the larger\nfine-tuned GPT-3 model (with $\\sim$ 175B parameters). Our research is the first\nto tackle drug pair synergy prediction in rare tissues with limited data. We\nare also the first to utilize an LLM-based prediction model for biological\nreaction prediction tasks.", "published": "2023-04-18 02:49:53", "link": "http://arxiv.org/abs/2304.10946v1", "categories": ["cs.CL", "cs.LG", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "CodeKGC: Code Language Model for Generative Knowledge Graph Construction", "abstract": "Current generative knowledge graph construction approaches usually fail to\ncapture structural knowledge by simply flattening natural language into\nserialized texts or a specification language. However, large generative\nlanguage model trained on structured data such as code has demonstrated\nimpressive capability in understanding natural language for structural\nprediction and reasoning tasks. Intuitively, we address the task of generative\nknowledge graph construction with code language model: given a code-format\nnatural language input, the target is to generate triples which can be\nrepresented as code completion tasks. Specifically, we develop schema-aware\nprompts that effectively utilize the semantic structure within the knowledge\ngraph. As code inherently possesses structure, such as class and function\ndefinitions, it serves as a useful model for prior semantic structural\nknowledge. Furthermore, we employ a rationale-enhanced generation method to\nboost the performance. Rationales provide intermediate steps, thereby improving\nknowledge extraction abilities. Experimental results indicate that the proposed\napproach can obtain better performance on benchmark datasets compared with\nbaselines. Code and datasets are available in\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.", "published": "2023-04-18 15:12:34", "link": "http://arxiv.org/abs/2304.09048v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Revisiting k-NN for Fine-tuning Pre-trained Language Models", "abstract": "Pre-trained Language Models (PLMs), as parametric-based eager learners, have\nbecome the de-facto choice for current paradigms of Natural Language Processing\n(NLP). In contrast, k-Nearest-Neighbor (kNN) classifiers, as the lazy learning\nparadigm, tend to mitigate over-fitting and isolated noise. In this paper, we\nrevisit kNN classifiers for augmenting the PLMs-based classifiers. From the\nmethodological level, we propose to adopt kNN with textual representations of\nPLMs in two steps: (1) Utilize kNN as prior knowledge to calibrate the training\nprocess. (2) Linearly interpolate the probability distribution predicted by kNN\nwith that of the PLMs' classifier. At the heart of our approach is the\nimplementation of kNN-calibrated training, which treats predicted results as\nindicators for easy versus hard examples during the training process. From the\nperspective of the diversity of application scenarios, we conduct extensive\nexperiments on fine-tuning, prompt-tuning paradigms and zero-shot, few-shot and\nfully-supervised settings, respectively, across eight diverse end-tasks. We\nhope our exploration will encourage the community to revisit the power of\nclassical methods for efficient NLP. Code and datasets are available in\nhttps://github.com/zjunlp/Revisit-KNN.", "published": "2023-04-18 15:28:47", "link": "http://arxiv.org/abs/2304.09058v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot\n  Speech and Singing Synthesizers", "abstract": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild\ndatasets is important to capture the diversity in human speech such as speaker\nidentities, prosodies, and styles (e.g., singing). Current large TTS systems\nusually quantize speech into discrete tokens and use language models to\ngenerate these tokens one by one, which suffer from unstable prosody, word\nskipping/repeating issue, and poor voice quality. In this paper, we develop\nNaturalSpeech 2, a TTS system that leverages a neural audio codec with residual\nvector quantizers to get the quantized latent vectors and uses a diffusion\nmodel to generate these latent vectors conditioned on text input. To enhance\nthe zero-shot capability that is important to achieve diverse speech synthesis,\nwe design a speech prompting mechanism to facilitate in-context learning in the\ndiffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to\nlarge-scale datasets with 44K hours of speech and singing data and evaluate its\nvoice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS\nsystems by a large margin in terms of prosody/timbre similarity, robustness,\nand voice quality in a zero-shot setting, and performs novel zero-shot singing\nsynthesis with only a speech prompt. Audio samples are available at\nhttps://speechresearch.github.io/naturalspeech2.", "published": "2023-04-18 16:31:59", "link": "http://arxiv.org/abs/2304.09116v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Array Configuration-Agnostic Personal Voice Activity Detection Based on\n  Spatial Coherence", "abstract": "Personal voice activity detection has received increased attention due to the\ngrowing popularity of personal mobile devices and smart speakers. PVAD is often\nan integral element to speech enhancement and recognition for these\napplications in which lightweight signal processing is only enabled for the\ntarget user. However, in real-world scenarios, the detection performance may\ndegrade because of competing speakers, background noise, and reverberation. To\naddress this problem, we proposed to use equivalent rectangular bandwidth\nERB-scaled spatial coherence as the input feature to train an array\nconfiguration-agnostic PVAD network. Whereas the network model requires only\n112k parameters, it exhibits excellent detection performance and robustness in\nadverse acoustic conditions. Notably, the proposed ARCA-PVAD system is scalable\nto array configurations. Experimental results have demonstrated the superior\nperformance achieved by the proposed ARCA-PVAD system over a baseline in terms\nof the area under receiver operating characteristic curve and equal error rate.", "published": "2023-04-18 10:27:32", "link": "http://arxiv.org/abs/2304.08887v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "AIRCADE: an Anechoic and IR Convolution-based Auralization\n  Data-compilation Ensemble", "abstract": "In this paper, we introduce a data-compilation ensemble, primarily intended\nto serve as a resource for researchers in the field of dereverberation,\nparticularly for data-driven approaches. It comprises speech and song samples,\ntogether with acoustic guitar sounds, with original annotations pertinent to\nemotion recognition and Music Information Retrieval (MIR). Moreover, it\nincludes a selection of impulse response (IR) samples with varying\nReverberation Time (RT) values, providing a wide range of conditions for\nevaluation. This data-compilation can be used together with provided Python\nscripts, for generating auralized data ensembles in different sizes: tiny,\nsmall, medium and large. Additionally, the provided metadata annotations also\nallow for further analysis and investigation of the performance of\ndereverberation algorithms under different conditions. All data is licensed\nunder Creative Commons Attribution 4.0 International License.", "published": "2023-04-18 22:07:50", "link": "http://arxiv.org/abs/2304.09318v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Neural Speech Enhancement with Very Low Algorithmic Latency and\n  Complexity via Integrated Full- and Sub-Band Modeling", "abstract": "We propose FSB-LSTM, a novel long short-term memory (LSTM) based architecture\nthat integrates full- and sub-band (FSB) modeling, for single- and\nmulti-channel speech enhancement in the short-time Fourier transform (STFT)\ndomain. The model maintains an information highway to flow an over-complete\ninput representation through multiple FSB-LSTM modules. Each FSB-LSTM module\nconsists of a full-band block to model spectro-temporal patterns at all\nfrequencies and a sub-band block to model patterns within each sub-band, where\neach of the two blocks takes a down-sampled representation as input and returns\nan up-sampled discriminative representation to be added to the block input via\na residual connection. The model is designed to have a low algorithmic\ncomplexity, a small run-time buffer and a very low algorithmic latency, at the\nsame time producing a strong enhancement performance on a noisy-reverberant\nspeech enhancement task even if the hop size is as low as $2$ ms.", "published": "2023-04-18 02:37:54", "link": "http://arxiv.org/abs/2304.08707v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Voice Disease Detection Method Based on MFCCs and Shallow CNN", "abstract": "The incidence rate of voice diseases is increasing year by year. The use of\nsoftware for remote diagnosis is a technical development trend and has\nimportant practical value. Among voice diseases, common diseases that cause\nhoarseness include spasmodic dysphonia, vocal cord paralysis, vocal nodule, and\nvocal cord polyp. This paper presents a voice disease detection method that can\nbe applied in a wide range of clinical. We cooperated with Xiangya Hospital of\nCentral South University to collect voice samples from sixty-one different\npatients. The Mel Frequency Cepstrum Coefficient (MFCC) parameters are\nextracted as input features to describe the voice in the form of data. An\ninnovative model combining MFCC parameters and single convolution layer CNN is\nproposed for fast calculation and classification. The highest accuracy we\nachieved was 92%, it is fully ahead of the original research results and\ninternationally advanced. And we use Advanced Voice Function Assessment\nDatabases (AVFAD) to evaluate the generalization ability of the method we\nproposed, which achieved an accuracy rate of 98%. Experiments on clinical and\nstandard datasets show that for the pathological detection of voice diseases,\nour method has greatly improved in accuracy and computational efficiency.", "published": "2023-04-18 02:41:59", "link": "http://arxiv.org/abs/2304.08708v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN", "abstract": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech\ncommunication. Evaluation of coded speech quality is often performed\nsubjectively by an absolute category rating (ACR) listening test. However, the\nACR test is impractical for online monitoring of speech communication networks.\nPerceptual evaluation of speech quality (PESQ) is one of the widely used\nmetrics instrumentally predicting the results of an ACR test. However, the PESQ\nalgorithm requires an original reference signal, which is usually unavailable\nin network monitoring, thus limiting its applicability. NISQA is a new\nnon-intrusive neural-network-based speech quality measure, focusing on\nsuper-wideband speech signals. In this work, however, we aim at predicting the\nwell-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the\npotential of this model by predicting the PESQ scores of wideband-coded speech\nobtained from AMR-WB or EVS codecs operating at different bitrates in noisy,\ntandeming, and error-prone transmission conditions. We compare our methods with\nthe state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS --\nall applied to PESQ prediction -- by measuring the mean absolute error (MAE)\nand the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the\nbest total MAE and LCC of 0.11 and 0.92, respectively, in conditions without\nframe loss, and still is best when including frame loss. Note that our model\ncould be similarly used to non-intrusively predict POLQA or other (intrusive)\nmetrics. Upon article acceptance, code will be provided at GitHub.", "published": "2023-04-18 18:26:56", "link": "http://arxiv.org/abs/2304.09226v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Chunk Convolution for Unified Streaming and Non-Streaming\n  Conformer ASR", "abstract": "Recently, there has been an increasing interest in unifying streaming and\nnon-streaming speech recognition models to reduce development, training and\ndeployment cost. The best-known approaches rely on either window-based or\ndynamic chunk-based attention strategy and causal convolutions to minimize the\ndegradation due to streaming. However, the performance gap still remains\nrelatively large between non-streaming and a full-contextual model trained\nindependently. To address this, we propose a dynamic chunk-based convolution\nreplacing the causal convolution in a hybrid Connectionist Temporal\nClassification (CTC)-Attention Conformer architecture. Additionally, we\ndemonstrate further improvements through initialization of weights from a\nfull-contextual model and parallelization of the convolution and self-attention\nmodules. We evaluate our models on the open-source Voxpopuli, LibriSpeech and\nin-house conversational datasets. Overall, our proposed model reduces the\ndegradation of the streaming mode over the non-streaming full-contextual model\nfrom 41.7% and 45.7% to 16.7% and 26.2% on the LibriSpeech test-clean and\ntest-other datasets respectively, while improving by a relative 15.5% WER over\nthe previous state-of-the-art unified model.", "published": "2023-04-18 22:18:40", "link": "http://arxiv.org/abs/2304.09325v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards the Transferable Audio Adversarial Attack via Ensemble Methods", "abstract": "In recent years, deep learning (DL) models have achieved significant progress\nin many domains, such as autonomous driving, facial recognition, and speech\nrecognition. However, the vulnerability of deep learning models to adversarial\nattacks has raised serious concerns in the community because of their\ninsufficient robustness and generalization. Also, transferable attacks have\nbecome a prominent method for black-box attacks. In this work, we explore the\npotential factors that impact adversarial examples (AEs) transferability in\nDL-based speech recognition. We also discuss the vulnerability of different DL\nsystems and the irregular nature of decision boundaries. Our results show a\nremarkable difference in the transferability of AEs between speech and images,\nwith the data relevance being low in images but opposite in speech recognition.\nMotivated by dropout-based ensemble approaches, we propose random gradient\nensembles and dynamic gradient-weighted ensembles, and we evaluate the impact\nof ensembles on the transferability of AEs. The results show that the AEs\ncreated by both approaches are valid for transfer to the black box API.", "published": "2023-04-18 08:21:49", "link": "http://arxiv.org/abs/2304.08811v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "From Words to Music: A Study of Subword Tokenization Techniques in\n  Symbolic Music Generation", "abstract": "Subword tokenization has been widely successful in text-based natural\nlanguage processing (NLP) tasks with Transformer-based models. As Transformer\nmodels become increasingly popular in symbolic music-related studies, it is\nimperative to investigate the efficacy of subword tokenization in the symbolic\nmusic domain. In this paper, we explore subword tokenization techniques, such\nas byte-pair encoding (BPE), in symbolic music generation and its impact on the\noverall structure of generated songs. Our experiments are based on three types\nof MIDI datasets: single track-melody only, multi-track with a single\ninstrument, and multi-track and multi-instrument. We apply subword tokenization\non post-musical tokenization schemes and find that it enables the generation of\nlonger songs at the same time and improves the overall structure of the\ngenerated music in terms of objective metrics like structure indicator (SI),\nPitch Class Entropy, etc. We also compare two subword tokenization methods, BPE\nand Unigram, and observe that both methods lead to consistent improvements. Our\nstudy suggests that subword tokenization is a promising technique for symbolic\nmusic generation and may have broader implications for music composition,\nparticularly in cases involving complex data such as multi-track songs.", "published": "2023-04-18 12:46:12", "link": "http://arxiv.org/abs/2304.08953v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
