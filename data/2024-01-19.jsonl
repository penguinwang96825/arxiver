{"title": "Breaking the Curse of Multilinguality with Cross-lingual Expert Language\n  Models", "abstract": "Despite their popularity in non-English NLP, multilingual language models\noften underperform monolingual ones due to inter-language competition for model\nparameters. We propose Cross-lingual Expert Language Models (X-ELM), which\nmitigate this competition by independently training language models on subsets\nof the multilingual corpus. This process specializes X-ELMs to different\nlanguages while remaining effective as a multilingual ensemble. Our experiments\nshow that when given the same compute budget, X-ELM outperforms jointly trained\nmultilingual models across all considered languages and that these gains\ntransfer to downstream tasks. X-ELM provides additional benefits over\nperformance improvements: new experts can be iteratively added, adapting X-ELM\nto new languages without catastrophic forgetting. Furthermore, training is\nasynchronous, reducing the hardware requirements for multilingual training and\ndemocratizing multilingual modeling.", "published": "2024-01-19 01:07:50", "link": "http://arxiv.org/abs/2401.10440v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Recognition Under Domain Shift via Metric Learning for Life\n  Sciences", "abstract": "Named entity recognition is a key component of Information Extraction (IE),\nparticularly in scientific domains such as biomedicine and chemistry, where\nlarge language models (LLMs), e.g., ChatGPT, fall short. We investigate the\napplicability of transfer learning for enhancing a named entity recognition\nmodel trained in the biomedical domain (the source domain) to be used in the\nchemical domain (the target domain). A common practice for training such a\nmodel in a few-shot learning setting is to pretrain the model on the labeled\nsource data, and then, to finetune it on a hand-full of labeled target\nexamples. In our experiments, we observed that such a model is prone to\nmislabeling the source entities, which can often appear in the text, as the\ntarget entities. To alleviate this problem, we propose a model to transfer the\nknowledge from the source domain to the target domain, but, at the same time,\nto project the source entities and target entities into separate regions of the\nfeature space. This diminishes the risk of mislabeling the source entities as\nthe target entities. Our model consists of two stages: 1) entity grouping in\nthe source domain, which incorporates knowledge from annotated events to\nestablish relations between entities, and 2) entity discrimination in the\ntarget domain, which relies on pseudo labeling and contrastive learning to\nenhance discrimination between the entities in the two domains. We conduct our\nextensive experiments across three source and three target datasets,\ndemonstrating that our method outperforms the baselines by up to 5% absolute\nvalue.", "published": "2024-01-19 03:49:28", "link": "http://arxiv.org/abs/2401.10472v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Fusion of Large Language Models", "abstract": "While training large language models (LLMs) from scratch can generate models\nwith distinct functionalities and strengths, it comes at significant costs and\nmay result in redundant capabilities. Alternatively, a cost-effective and\ncompelling approach is to merge existing pre-trained LLMs into a more potent\nmodel. However, due to the varying architectures of these LLMs, directly\nblending their weights is impractical. In this paper, we introduce the notion\nof knowledge fusion for LLMs, aimed at combining the capabilities of existing\nLLMs and transferring them into a single LLM. By leveraging the generative\ndistributions of source LLMs, we externalize their collective knowledge and\nunique strengths, thereby potentially elevating the capabilities of the target\nmodel beyond those of any individual source LLM. We validate our approach using\nthree popular LLMs with different architectures--Llama-2, MPT, and\nOpenLLaMA--across various benchmarks and tasks. Our findings confirm that the\nfusion of LLMs can improve the performance of the target model across a range\nof capabilities such as reasoning, commonsense, and code generation. Our code,\nmodel weights, and data are public at\n\\url{https://github.com/fanqiwan/FuseLLM}.", "published": "2024-01-19 05:02:46", "link": "http://arxiv.org/abs/2401.10491v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech Swin-Transformer: Exploring a Hierarchical Transformer with\n  Shifted Windows for Speech Emotion Recognition", "abstract": "Swin-Transformer has demonstrated remarkable success in computer vision by\nleveraging its hierarchical feature representation based on Transformer. In\nspeech signals, emotional information is distributed across different scales of\nspeech features, e.\\,g., word, phrase, and utterance. Drawing above\ninspiration, this paper presents a hierarchical speech Transformer with shifted\nwindows to aggregate multi-scale emotion features for speech emotion\nrecognition (SER), called Speech Swin-Transformer. Specifically, we first\ndivide the speech spectrogram into segment-level patches in the time domain,\ncomposed of multiple frame patches. These segment-level patches are then\nencoded using a stack of Swin blocks, in which a local window Transformer is\nutilized to explore local inter-frame emotional information across frame\npatches of each segment patch. After that, we also design a shifted window\nTransformer to compensate for patch correlations near the boundaries of segment\npatches. Finally, we employ a patch merging operation to aggregate\nsegment-level emotional features for hierarchical speech representation by\nexpanding the receptive field of Transformer from frame-level to segment-level.\nExperimental results demonstrate that our proposed Speech Swin-Transformer\noutperforms the state-of-the-art methods.", "published": "2024-01-19 07:30:57", "link": "http://arxiv.org/abs/2401.10536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-training from Self-memory in Data-to-text Generation", "abstract": "This paper introduces a novel training model, self-training from self-memory\n(STSM) in data-to-text generation (DTG), allowing the model to self-train on\nsubsets, including self-memory as outputs inferred directly from the trained\nmodels and/or the new data. The quality of self-memory is validated by two\nmodels, data-to-text (D2T) and text-to-data (T2D), by two pre-defined\nconditions: (1) the appearance of all source values in the outputs of the D2T\nmodel and (2) the ability to convert back to source data in the outputs in the\nT2D model. We utilize a greedy algorithm to generate shorter D2T outputs if\nthey contain all source values. Subsequently, we use the T2D model to confirm\nthat these outputs can capture input relationships by demonstrating their\ncapacity to convert text back into data. With 30% of the dataset, we can train\nthe D2T model with a competitive performance compared to full training in the\nsame setup. We experiment with our model on two datasets, E2E NLG and DART.\nSTSM offers the D2T model a generalization capability from its subset memory\nwhile reducing training data volume. Ultimately, we anticipate that this paper\nwill contribute to continual learning solutions that adapt to new training\ndata, incorporating it as a form of self-memory in DTG tasks. The curated\ndataset is publicly available at: https://github.com/hoangthangta/STSM.", "published": "2024-01-19 09:13:28", "link": "http://arxiv.org/abs/2401.10567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PHOENIX: Open-Source Language Adaption for Direct Preference\n  Optimization", "abstract": "Large language models have gained immense importance in recent years and have\ndemonstrated outstanding results in solving various tasks. However, despite\nthese achievements, many questions remain unanswered in the context of large\nlanguage models. Besides the optimal use of the models for inference and the\nalignment of the results to the desired specifications, the transfer of models\nto other languages is still an underdeveloped area of research. The recent\npublication of models such as Llama-2 and Zephyr has provided new insights into\narchitectural improvements and the use of human feedback. However, insights\ninto adapting these techniques to other languages remain scarce. In this paper,\nwe build on latest improvements and apply the Direct Preference\nOptimization(DPO) approach to the German language. The model is available at\nhttps://huggingface.co/DRXD1000/Phoenix.", "published": "2024-01-19 09:46:08", "link": "http://arxiv.org/abs/2401.10580v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language\n  Models", "abstract": "In the rapidly advancing field of artificial intelligence, the concept of\nRed-Teaming or Jailbreaking large language models (LLMs) has emerged as a\ncrucial area of study. This approach is especially significant in terms of\nassessing and enhancing the safety and robustness of these models. This paper\ninvestigates the intricate consequences of such modifications through model\nediting, uncovering a complex relationship between enhancing model accuracy and\npreserving its ethical integrity. Our in-depth analysis reveals a striking\nparadox: while injecting accurate information is crucial for model reliability,\nit can paradoxically destabilize the model's foundational framework, resulting\nin unpredictable and potentially unsafe behaviors. Additionally, we propose a\nbenchmark dataset NicheHazardQA to investigate this unsafe behavior both within\nthe same and cross topical domain. This aspect of our research sheds light on\nhow the edits, impact the model's safety metrics and guardrails. Our findings\nshow that model editing serves as a cost-effective tool for topical red-teaming\nby methodically applying targeted edits and evaluating the resultant model\nbehavior.", "published": "2024-01-19 11:48:09", "link": "http://arxiv.org/abs/2401.10647v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LangBridge: Multilingual Reasoning Without Multilingual Supervision", "abstract": "We introduce LangBridge, a zero-shot approach to adapt language models for\nmultilingual reasoning tasks without multilingual supervision. LangBridge\noperates by bridging two models, each specialized in different aspects: (1) one\nspecialized in understanding multiple languages (e.g., mT5 encoder) and (2) one\nspecialized in reasoning (e.g., MetaMath). LangBridge connects the two models\nby introducing minimal trainable parameters between them. Despite utilizing\nonly English data for training, LangBridge considerably enhances the\nperformance of language models on low-resource languages across mathematical\nreasoning, code completion, logical reasoning, and commonsense reasoning. Our\nanalysis suggests that the efficacy of LangBridge stems from the\nlanguage-agnostic characteristics of multilingual representations. We publicly\nrelease our code and models.", "published": "2024-01-19 14:00:19", "link": "http://arxiv.org/abs/2401.10695v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Code Representations Enable Data-Efficient Adaptation of Code\n  Language Models", "abstract": "Current language models tailored for code tasks often adopt the\npre-training-then-fine-tuning paradigm from natural language processing,\nmodeling source code as plain text. This approach, however, overlooks the\nunambiguous structures inherent in programming languages. In this work, we\nexplore data-efficient adaptation of pre-trained code models by further\npre-training and fine-tuning them with program structures. Specifically, we\nrepresent programs as parse trees -- also known as concrete syntax trees (CSTs)\n-- and adapt pre-trained models on serialized CSTs. Although the models that we\nadapt have been pre-trained only on the surface form of programs, we find that\na small amount of continual pre-training and fine-tuning on CSTs without\nchanging the model architecture yields improvements over the baseline approach\nacross various code tasks. The improvements are found to be particularly\nsignificant when there are limited training examples, demonstrating the\neffectiveness of integrating program structures with plain-text representation\neven when working with backbone models that have not been pre-trained with\nstructures.", "published": "2024-01-19 14:27:44", "link": "http://arxiv.org/abs/2401.10716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Verification to Nip Hallucination in the Bud", "abstract": "While large language models (LLMs) have demonstrated exceptional performance\nacross various tasks following human alignment, they may still generate\nresponses that sound plausible but contradict factual knowledge, a phenomenon\nknown as hallucination. In this paper, we demonstrate the feasibility of\nmitigating hallucinations by verifying and minimizing the inconsistency between\nexternal knowledge present in the alignment data and the intrinsic knowledge\nembedded within foundation LLMs. Specifically, we propose a novel approach\ncalled Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM\nto automatically formulate assessments based on external knowledge to evaluate\nthe knowledge boundaries of foundation LLMs. To address knowledge\ninconsistencies in the alignment data, KCA implements several specific\nstrategies to deal with these data instances. We demonstrate the superior\nefficacy of KCA in reducing hallucinations across six benchmarks, utilizing\nfoundation LLMs of varying backbones and scales. This confirms the\neffectiveness of mitigating hallucinations by reducing knowledge inconsistency.\nOur code, model weights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/KCA}.", "published": "2024-01-19 15:39:49", "link": "http://arxiv.org/abs/2401.10768v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for\n  Large Language Models' Training?", "abstract": "The rapid evolution of Large Language Models (LLMs) highlights the necessity\nfor ethical considerations and data integrity in AI development, particularly\nemphasizing the role of FAIR (Findable, Accessible, Interoperable, Reusable)\ndata principles. While these principles are crucial for ethical data\nstewardship, their specific application in the context of LLM training data\nremains an under-explored area. This research gap is the focus of our study,\nwhich begins with an examination of existing literature to underline the\nimportance of FAIR principles in managing data for LLM training. Building upon\nthis, we propose a novel framework designed to integrate FAIR principles into\nthe LLM development lifecycle. A contribution of our work is the development of\na comprehensive checklist intended to guide researchers and developers in\napplying FAIR data principles consistently across the model development\nprocess. The utility and effectiveness of our framework are validated through a\ncase study on creating a FAIR-compliant dataset aimed at detecting and\nmitigating biases in LLMs. We present this framework to the community as a tool\nto foster the creation of technologically advanced, ethically grounded, and\nsocially responsible AI models.", "published": "2024-01-19 21:21:02", "link": "http://arxiv.org/abs/2401.11033v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining experimental data from Materials Science literature with Large\n  Language Models: an evaluation study", "abstract": "This study is dedicated to assessing the capabilities of large language\nmodels (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in extracting\nstructured information from scientific documents in materials science. To this\nend, we primarily focus on two critical tasks of information extraction: (i) a\nnamed entity recognition (NER) of studied materials and physical properties and\n(ii) a relation extraction (RE) between these entities. Due to the evident lack\nof datasets within Materials Informatics (MI), we evaluated using SuperMat,\nbased on superconductor research, and MeasEval, a generic measurement\nevaluation corpus. The performance of LLMs in executing these tasks is\nbenchmarked against traditional models based on the BERT architecture and\nrule-based approaches (baseline). We introduce a novel methodology for the\ncomparative analysis of intricate material expressions, emphasising the\nstandardisation of chemical formulas to tackle the complexities inherent in\nmaterials science information assessment. For NER, LLMs fail to outperform the\nbaseline with zero-shot prompting and exhibit only limited improvement with\nfew-shot prompting. However, a GPT-3.5-Turbo fine-tuned with the appropriate\nstrategy for RE outperforms all models, including the baseline. Without any\nfine-tuning, GPT-4 and GPT-4-Turbo display remarkable reasoning and\nrelationship extraction capabilities after being provided with merely a couple\nof examples, surpassing the baseline. Overall, the results suggest that\nalthough LLMs demonstrate relevant reasoning skills in connecting concepts,\nspecialised models are currently a better choice for tasks requiring extracting\ncomplex domain-specific entities like materials. These insights provide initial\nguidance applicable to other materials science sub-domains in future work.", "published": "2024-01-19 23:00:31", "link": "http://arxiv.org/abs/2401.11052v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressive Distillation Based on Masked Generation Feature Method for\n  Knowledge Graph Completion", "abstract": "In recent years, knowledge graph completion (KGC) models based on pre-trained\nlanguage model (PLM) have shown promising results. However, the large number of\nparameters and high computational cost of PLM models pose challenges for their\napplication in downstream tasks. This paper proposes a progressive distillation\nmethod based on masked generation features for KGC task, aiming to\nsignificantly reduce the complexity of pre-trained models. Specifically, we\nperform pre-distillation on PLM to obtain high-quality teacher models, and\ncompress the PLM network to obtain multi-grade student models. However,\ntraditional feature distillation suffers from the limitation of having a single\nrepresentation of information in teacher models. To solve this problem, we\npropose masked generation of teacher-student features, which contain richer\nrepresentation information. Furthermore, there is a significant gap in\nrepresentation ability between teacher and student. Therefore, we design a\nprogressive distillation method to distill student models at each grade level,\nenabling efficient knowledge transfer from teachers to students. The\nexperimental results demonstrate that the model in the pre-distillation stage\nsurpasses the existing state-of-the-art methods. Furthermore, in the\nprogressive distillation stage, the model significantly reduces the model\nparameters while maintaining a certain level of performance. Specifically, the\nmodel parameters of the lower-grade student model are reduced by 56.7\\%\ncompared to the baseline.", "published": "2024-01-19 07:34:36", "link": "http://arxiv.org/abs/2401.12997v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepEdit: Knowledge Editing as Decoding with Constraints", "abstract": "How to edit the knowledge in multi-step reasoning has become the major\nchallenge in the knowledge editing (KE) of large language models (LLMs). The\ndifficulty arises because the hallucinations of LLMs during multi-step\nreasoning often lead to incorrect use of new knowledge and incorrect answers.\nTo address this issue, we design decoding constraints to \"regulate\" LLMs'\nreasoning, enhancing logical coherence when incorporating new knowledge. We\npropose a new KE framework: DEEPEDIT (Depth-first Search-based Constrained\nDecoding for Knowledge Editing), which enhances LLMs's ability to generate\ncoherent reasoning chains with new knowledge through depth-first search. Our\nsearch selects the most important knowledge that satisfies our constraints as\nthe reasoning step to efficiently increase the reasoning depth. In addition to\nDEEPEDIT, we propose two new KE benchmarks: MQUAKE-2002 and MQUAKE-HARD, which\nprovide more precise and challenging assessments of KE approaches.\nQualitatively, DEEPEDIT enables LLMs to produce succinct and coherent reasoning\nchains involving new knowledge. Quantitatively, it yields significant\nimprovements on multiple KE benchmarks.", "published": "2024-01-19 03:48:27", "link": "http://arxiv.org/abs/2401.10471v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step\n  Reasoning", "abstract": "Self-consistency (SC) has been a widely used decoding strategy for\nchain-of-thought reasoning. Despite bringing significant performance\nimprovements across a variety of multi-step reasoning tasks, it is a high-cost\nmethod that requires multiple sampling with the preset size. In this paper, we\npropose a simple and scalable sampling process, \\textbf{E}arly-Stopping\n\\textbf{S}elf-\\textbf{C}onsistency (ESC), to greatly reduce the cost of SC\nwithout sacrificing performance. On this basis, one control scheme for ESC is\nfurther derivated to dynamically choose the performance-cost balance for\ndifferent tasks and models. To demonstrate ESC's effectiveness, we conducted\nextensive experiments on three popular categories of reasoning tasks:\narithmetic, commonsense and symbolic reasoning over language models with\nvarying scales. The empirical results show that ESC reduces the average number\nof sampling of chain-of-thought reasoning by a significant margin on six\nbenchmarks, including MATH (-33.8%), GSM8K (-80.1%), StrategyQA (-76.8%),\nCommonsenseQA (-78.5%), Coin Flip (-84.2%) and Last Letters (-67.4%), while\nattaining comparable performances.", "published": "2024-01-19 04:03:59", "link": "http://arxiv.org/abs/2401.10480v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Dense Retrieval: Memory Can Be a Burden", "abstract": "Generative Retrieval (GR), autoregressively decoding relevant document\nidentifiers given a query, has been shown to perform well under the setting of\nsmall-scale corpora. By memorizing the document corpus with model parameters,\nGR implicitly achieves deep interaction between query and document. However,\nsuch a memorizing mechanism faces three drawbacks: (1) Poor memory accuracy for\nfine-grained features of documents; (2) Memory confusion gets worse as the\ncorpus size increases; (3) Huge memory update costs for new documents. To\nalleviate these problems, we propose the Generative Dense Retrieval (GDR)\nparadigm. Specifically, GDR first uses the limited memory volume to achieve\ninter-cluster matching from query to relevant document clusters.\nMemorizing-free matching mechanism from Dense Retrieval (DR) is then introduced\nto conduct fine-grained intra-cluster matching from clusters to relevant\ndocuments. The coarse-to-fine process maximizes the advantages of GR's deep\ninteraction and DR's scalability. Besides, we design a cluster identifier\nconstructing strategy to facilitate corpus memory and a cluster-adaptive\nnegative sampling strategy to enhance the intra-cluster mapping ability.\nEmpirical results show that GDR obtains an average of 3.0 R@100 improvement on\nNQ dataset under multiple settings and has better scalability.", "published": "2024-01-19 04:24:07", "link": "http://arxiv.org/abs/2401.10487v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Cross-lingual Editing in Multilingual Language Models", "abstract": "The training of large language models (LLMs) necessitates substantial data\nand computational resources, and updating outdated LLMs entails significant\nefforts and resources. While numerous model editing techniques (METs) have\nemerged to efficiently update model outputs without retraining, their\neffectiveness in multilingual LLMs, where knowledge is stored in diverse\nlanguages, remains an underexplored research area. This research paper\nintroduces the cross-lingual model editing (\\textbf{XME}) paradigm, wherein a\nfact is edited in one language, and the subsequent update propagation is\nobserved across other languages. To investigate the XME paradigm, we conducted\nexperiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts:\n\\textit{Latin} (English, French, and Spanish) and \\textit{Indic} (Hindi,\nGujarati, and Bengali). The results reveal notable performance limitations of\nstate-of-the-art METs under the XME setting, mainly when the languages involved\nbelong to two distinct script families. These findings highlight the need for\nfurther research and development of XME techniques to address these challenges.\nFor more comprehensive information, the dataset used in this research and the\nassociated code are publicly available at the following\nURL\\url{https://github.com/lingo-iitgn/XME}.", "published": "2024-01-19 06:54:39", "link": "http://arxiv.org/abs/2401.10521v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Accelerating Multilingual Language Model for Excessively Tokenized\n  Languages", "abstract": "Recent advancements in large language models (LLMs) have remarkably enhanced\nperformances on a variety of tasks in multiple languages. However, tokenizers\nin LLMs trained primarily on English-centric corpora often overly fragment a\ntext into character or Unicode-level tokens in non-Roman alphabetic languages,\nleading to inefficient text generation. We introduce a simple yet effective\nframework to accelerate text generation in such languages. Our approach\ninvolves employing a new language model head with a vocabulary set tailored to\na specific target language for a pre-trained LLM. This is followed by\nfine-tuning the new head while incorporating a verification step to ensure the\nmodel's performance is preserved. We show that this targeted fine-tuning, while\nfreezing other model parameters, effectively reduces token fragmentation for\nthe target language. Our extensive experiments demonstrate that the proposed\nframework increases the generation speed by a factor of 1.7 while maintaining\nthe performance of pre-trained multilingual models on target monolingual tasks.", "published": "2024-01-19 12:26:57", "link": "http://arxiv.org/abs/2401.10660v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple\n  Decoding Heads", "abstract": "Large Language Models (LLMs) employ auto-regressive decoding that requires\nsequential computation, with each step reliant on the previous one's output.\nThis creates a bottleneck as each step necessitates moving the full model\nparameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While\nmethods such as speculative decoding have been suggested to address this issue,\ntheir implementation is impeded by the challenges associated with acquiring and\nmaintaining a separate draft model. In this paper, we present Medusa, an\nefficient method that augments LLM inference by adding extra decoding heads to\npredict multiple subsequent tokens in parallel. Using a tree-based attention\nmechanism, Medusa constructs multiple candidate continuations and verifies them\nsimultaneously in each decoding step. By leveraging parallel processing, Medusa\nsubstantially reduces the number of decoding steps required. We present two\nlevels of fine-tuning procedures for Medusa to meet the needs of different use\ncases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM,\nenabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned\ntogether with the backbone LLM, enabling better prediction accuracy of Medusa\nheads and higher speedup but needing a special training recipe that preserves\nthe backbone model's capabilities.\n  Moreover, we propose several extensions that improve or expand the utility of\nMedusa, including a self-distillation to handle situations where no training\ndata is available and a typical acceptance scheme to boost the acceptance rate\nwhile maintaining generation quality. We evaluate Medusa on models of various\nsizes and training procedures. Our experiments demonstrate that Medusa-1 can\nachieve over 2.2x speedup without compromising generation quality, while\nMedusa-2 further improves the speedup to 2.3-3.6x.", "published": "2024-01-19 15:48:40", "link": "http://arxiv.org/abs/2401.10774v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and\n  Comparative Study", "abstract": "Named Entity Recognition seeks to extract substrings within a text that name\nreal-world objects and to determine their type (for example, whether they refer\nto persons or organizations). In this survey, we first present an overview of\nrecent popular approaches, including advancements in Transformer-based methods\nand Large Language Models (LLMs) that have not had much coverage in other\nsurveys. In addition, we discuss reinforcement learning and graph-based\napproaches, highlighting their role in enhancing NER performance. Second, we\nfocus on methods designed for datasets with scarce annotations. Third, we\nevaluate the performance of the main NER implementations on a variety of\ndatasets with differing characteristics (as regards their domain, their size,\nand their number of classes). We thus provide a deep comparison of algorithms\nthat have never been considered together. Our experiments shed some light on\nhow the characteristics of datasets affect the behavior of the methods we\ncompare.", "published": "2024-01-19 17:21:05", "link": "http://arxiv.org/abs/2401.10825v3", "categories": ["cs.CL", "cs.LG", "68T50, 68Q32"], "primary_category": "cs.CL"}
{"title": "Advancements in eHealth Data Analytics through Natural Language\n  Processing and Deep Learning", "abstract": "The healthcare environment is commonly referred to as \"information-rich\" but\nalso \"knowledge poor\". Healthcare systems collect huge amounts of data from\nvarious sources: lab reports, medical letters, logs of medical tools or\nprograms, medical prescriptions, etc. These massive sets of data can provide\ngreat knowledge and information that can improve the medical services, and\noverall the healthcare domain, such as disease prediction by analyzing the\npatient's symptoms or disease prevention, by facilitating the discovery of\nbehavioral factors for diseases. Unfortunately, only a relatively small volume\nof the textual eHealth data is processed and interpreted, an important factor\nbeing the difficulty in efficiently performing Big Data operations. In the\nmedical field, detecting domain-specific multi-word terms is a crucial task as\nthey can define an entire concept with a few words. A term can be defined as a\nlinguistic structure or a concept, and it is composed of one or more words with\na specific meaning to a domain. All the terms of a domain create its\nterminology. This chapter offers a critical study of the current, most\nperformant solutions for analyzing unstructured (image and textual) eHealth\ndata. This study also provides a comparison of the current Natural Language\nProcessing and Deep Learning techniques in the eHealth context. Finally, we\nexamine and discuss some of the current issues, and we define a set of research\ndirections in this area.", "published": "2024-01-19 17:51:11", "link": "http://arxiv.org/abs/2401.10850v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Radiation Oncology NLP Database", "abstract": "We present the Radiation Oncology NLP Database (ROND), the first dedicated\nNatural Language Processing (NLP) dataset for radiation oncology, an important\nmedical specialty that has received limited attention from the NLP community in\nthe past. With the advent of Artificial General Intelligence (AGI), there is an\nincreasing need for specialized datasets and benchmarks to facilitate research\nand development. ROND is specifically designed to address this gap in the\ndomain of radiation oncology, a field that offers many opportunities for NLP\nexploration. It encompasses various NLP tasks including Logic Reasoning, Text\nClassification, Named Entity Recognition (NER), Question Answering (QA), Text\nSummarization, and Patient-Clinician Conversations, each with a distinct focus\non radiation oncology concepts and application cases. In addition, we have\ndeveloped an instruction-tuning dataset consisting of over 20k instruction\npairs (based on ROND) and trained a large language model, CancerChat. This\nserves to demonstrate the potential of instruction-tuning large language models\nwithin a highly-specialized medical domain. The evaluation results in this\nstudy could serve as baseline results for future research. ROND aims to\nstimulate advancements in radiation oncology and clinical NLP by offering a\nplatform for testing and improving algorithms and models in a domain-specific\ncontext. The ROND dataset is a joint effort of multiple U.S. health\ninstitutions. The data is available at\nhttps://github.com/zl-liu/Radiation-Oncology-NLP-Database.", "published": "2024-01-19 19:23:37", "link": "http://arxiv.org/abs/2401.10995v1", "categories": ["cs.CL", "physics.med-ph"], "primary_category": "cs.CL"}
{"title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical\n  Knowledge", "abstract": "PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a\nbiomedical literature resource using state-of-the-art AI techniques to offer\nsemantic and relation searches for key concepts like proteins, genetic\nvariants, diseases, and chemicals. It currently provides over one billion\nentity and relation annotations across approximately 36 million PubMed\nabstracts and 6 million full-text articles from the PMC open access subset,\nupdated weekly. PubTator 3.0's online interface and API utilize these\nprecomputed entity relations and synonyms to provide advanced search\ncapabilities and enable large-scale analyses, streamlining many complex\ninformation needs. We showcase the retrieval quality of PubTator 3.0 using a\nseries of entity pair queries, demonstrating that PubTator 3.0 retrieves a\ngreater number of articles than either PubMed or Google Scholar, with higher\nprecision in the top 20 results. We further show that integrating ChatGPT\n(GPT-4) with PubTator APIs dramatically improves the factuality and\nverifiability of its responses. In summary, PubTator 3.0 offers a comprehensive\nset of features and tools that allow researchers to navigate the ever-expanding\nwealth of biomedical literature, expediting research and unlocking valuable\ninsights for scientific discovery.", "published": "2024-01-19 22:24:39", "link": "http://arxiv.org/abs/2401.11048v1", "categories": ["cs.CL", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Language models align with human judgments on key grammatical\n  constructions", "abstract": "Do large language models (LLMs) make human-like linguistic generalizations?\nDentella et al. (2023) (\"DGL\") prompt several LLMs (\"Is the following sentence\ngrammatically correct in English?\") to elicit grammaticality judgments of 80\nEnglish sentences, concluding that LLMs demonstrate a \"yes-response bias\" and a\n\"failure to distinguish grammatical from ungrammatical sentences\". We\nre-evaluate LLM performance using well-established practices and find that\nDGL's data in fact provide evidence for just how well LLMs capture human\nbehaviors. Models not only achieve high accuracy overall, but also capture\nfine-grained variation in human linguistic judgments.", "published": "2024-01-19 19:36:54", "link": "http://arxiv.org/abs/2402.01676v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contextualized Automatic Speech Recognition with Attention-Based Bias\n  Phrase Boosted Beam Search", "abstract": "End-to-end (E2E) automatic speech recognition (ASR) methods exhibit\nremarkable performance. However, since the performance of such methods is\nintrinsically linked to the context present in the training data, E2E-ASR\nmethods do not perform as desired for unseen user contexts (e.g., technical\nterms, personal names, and playlists). Thus, E2E-ASR methods must be easily\ncontextualized by the user or developer. This paper proposes an attention-based\ncontextual biasing method that can be customized using an editable phrase list\n(referred to as a bias list). The proposed method can be trained effectively by\ncombining a bias phrase index loss and special tokens to detect the bias\nphrases in the input speech data. In addition, to improve the contextualization\nperformance during inference further, we propose a bias phrase boosted (BPB)\nbeam search algorithm based on the bias phrase index probability. Experimental\nresults demonstrate that the proposed method consistently improves the word\nerror rate and the character error rate of the target phrases in the bias list\non both the Librispeech-960 (English) and our in-house (Japanese) dataset,\nrespectively.", "published": "2024-01-19 01:36:07", "link": "http://arxiv.org/abs/2401.10449v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Critical Data Size of Language Models from a Grokking Perspective", "abstract": "We explore the critical data size in language models, a threshold that marks\na fundamental shift from quick memorization to slow generalization. We\nformalize the phase transition under the grokking configuration into the Data\nEfficiency Hypothesis and identify data insufficiency, sufficiency, and surplus\nregimes in language models training dynamics. We develop a grokking\nconfiguration to reproduce grokking on simplistic language models stably by\nrescaling initialization and weight decay. We show that generalization occurs\nonly when language models reach a critical size. We analyze grokking across\nsample-wise and model-wise, verifying the proposed data efficiency hypothesis.\nOur experiments reveal smoother phase transitions occurring at the critical\ndataset size for language datasets. As the model size increases, this critical\npoint also becomes larger, indicating that larger models require more data. Our\nresults deepen the understanding of language model training, offering a novel\nperspective on the role of data in the learning mechanism of language models.", "published": "2024-01-19 03:24:36", "link": "http://arxiv.org/abs/2401.10463v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data-driven grapheme-to-phoneme representations for a lexicon-free\n  text-to-speech", "abstract": "Grapheme-to-Phoneme (G2P) is an essential first step in any modern,\nhigh-quality Text-to-Speech (TTS) system. Most of the current G2P systems rely\non carefully hand-crafted lexicons developed by experts. This poses a two-fold\nproblem. Firstly, the lexicons are generated using a fixed phoneme set,\nusually, ARPABET or IPA, which might not be the most optimal way to represent\nphonemes for all languages. Secondly, the man-hours required to produce such an\nexpert lexicon are very high. In this paper, we eliminate both of these issues\nby using recent advances in self-supervised learning to obtain data-driven\nphoneme representations instead of fixed representations. We compare our\nlexicon-free approach against strong baselines that utilize a well-crafted\nlexicon. Furthermore, we show that our data-driven lexicon-free method performs\nas good or even marginally better than the conventional rule-based or\nlexicon-based neural G2Ps in terms of Mean Opinion Score (MOS) while using no\nprior language lexicon or phoneme set, i.e. no linguistic expertise.", "published": "2024-01-19 03:37:27", "link": "http://arxiv.org/abs/2401.10465v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial\n  Analysis", "abstract": "Text-to-SQL, which provides zero-code interface for operating relational\ndatabases, has gained much attention in financial analysis; because, financial\nprofessionals may not well-skilled in SQL programming. However, until now,\nthere is no practical Text-to-SQL benchmark dataset for financial analysis, and\nexisting Text-to-SQL methods have not considered the unique characteristics of\ndatabases in financial applications, such as commonly existing wide tables. To\naddress these issues, we collect a practical Text-to-SQL benchmark dataset and\npropose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL\nframework for financial analysis. The benchmark dataset, BULL, is collected\nfrom the practical financial analysis business of Hundsun Technologies Inc.,\nincluding databases for fund, stock, and macro economy. Besides, the proposed\nLLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for\nfinancial Text-to-SQL from the perspectives of prompt construction,\nparameter-efficient fine-tuning and output calibration. Extensive experimental\nresults on BULL demonstrate that FinSQL achieves the state-of-the-art\nText-to-SQL performance at a small cost; furthermore, FinSQL can bring up to\n36.64% performance improvement in scenarios requiring few-shot cross-database\nmodel transfer.", "published": "2024-01-19 05:48:07", "link": "http://arxiv.org/abs/2401.10506v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "When Large Language Models Meet Evolutionary Algorithms: Potential\n  Enhancements and Challenges", "abstract": "Pre-trained large language models (LLMs) exhibit powerful capabilities for\ngenerating natural text. Evolutionary algorithms (EAs) can discover diverse\nsolutions to complex real-world problems. Motivated by the common collective\nand directionality of text generation and evolution, this paper first\nillustrates the conceptual parallels between LLMs and EAs at a micro level,\nwhich includes multiple one-to-one key characteristics: token representation\nand individual representation, position encoding and fitness shaping, position\nembedding and selection, Transformers block and reproduction, and model\ntraining and parameter adaptation. These parallels highlight potential\nopportunities for technical advancements in both LLMs and EAs. Subsequently, we\nanalyze existing interdisciplinary research from a macro perspective to uncover\ncritical challenges, with a particular focus on evolutionary fine-tuning and\nLLM-enhanced EAs. These analyses not only provide insights into the\nevolutionary mechanisms behind LLMs but also offer potential directions for\nenhancing the capabilities of artificial agents.", "published": "2024-01-19 05:58:30", "link": "http://arxiv.org/abs/2401.10510v3", "categories": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
{"title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model\n  Reasoning over Image Sequences", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated proficiency in\nhandling a variety of visual-language tasks. However, current MLLM benchmarks\nare predominantly designed to evaluate reasoning based on static information\nabout a single image, and the ability of modern MLLMs to extrapolate from image\nsequences, which is essential for understanding our ever-changing world, has\nbeen less investigated. To address this challenge, this paper introduces\nMementos, a new benchmark designed to assess MLLMs' sequential image reasoning\nabilities. Mementos features 4,761 diverse image sequences with varying\nlengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning\nperformance. Through a careful evaluation of nine recent MLLMs on Mementos,\nincluding GPT-4V and Gemini, we find that they struggle to accurately describe\ndynamic information about given image sequences, often leading to\nhallucinations/misrepresentations of objects and their corresponding behaviors.\nOur quantitative analysis and case studies identify three key factors impacting\nMLLMs' sequential image reasoning: the correlation between object and\nbehavioral hallucinations, the influence of cooccurring behaviors, and the\ncompounding impact of behavioral hallucinations. Our dataset is available at\nhttps://github.com/umd-huang-lab/Mementos.", "published": "2024-01-19 07:10:13", "link": "http://arxiv.org/abs/2401.10529v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The \"Colonial Impulse\" of Natural Language Processing: An Audit of\n  Bengali Sentiment Analysis Tools and Their Identity-based Biases", "abstract": "While colonization has sociohistorically impacted people's identities across\nvarious dimensions, those colonial values and biases continue to be perpetuated\nby sociotechnical systems. One category of sociotechnical systems--sentiment\nanalysis tools--can also perpetuate colonial values and bias, yet less\nattention has been paid to how such tools may be complicit in perpetuating\ncoloniality, although they are often used to guide various practices (e.g.,\ncontent moderation). In this paper, we explore potential bias in sentiment\nanalysis tools in the context of Bengali communities that have experienced and\ncontinue to experience the impacts of colonialism. Drawing on identity\ncategories most impacted by colonialism amongst local Bengali communities, we\nfocused our analytic attention on gender, religion, and nationality. We\nconducted an algorithmic audit of all sentiment analysis tools for Bengali,\navailable on the Python package index (PyPI) and GitHub. Despite similar\nsemantic content and structure, our analyses showed that in addition to\ninconsistencies in output from different tools, Bengali sentiment analysis\ntools exhibit bias between different identity categories and respond\ndifferently to different ways of identity expression. Connecting our findings\nwith colonially shaped sociocultural structures of Bengali communities, we\ndiscuss the implications of downstream bias of sentiment analysis tools.", "published": "2024-01-19 07:21:45", "link": "http://arxiv.org/abs/2401.10535v1", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual acoustic word embeddings for zero-resource languages", "abstract": "This research addresses the challenge of developing speech applications for\nzero-resource languages that lack labelled data. It specifically uses acoustic\nword embedding (AWE) -- fixed-dimensional representations of variable-duration\nspeech segments -- employing multilingual transfer, where labelled data from\nseveral well-resourced languages are used for pertaining. The study introduces\na new neural network that outperforms existing AWE models on zero-resource\nlanguages. It explores the impact of the choice of well-resourced languages.\nAWEs are applied to a keyword-spotting system for hate speech detection in\nSwahili radio broadcasts, demonstrating robustness in real-world scenarios.\nAdditionally, novel semantic AWE models improve semantic query-by-example\nsearch.", "published": "2024-01-19 08:02:37", "link": "http://arxiv.org/abs/2401.10543v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy", "abstract": "We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel\nmulti-adapter method, OrchMoE, which capitalizes on modular skill architecture\nfor enhanced forward transfer in neural networks. Unlike prior models that\ndepend on explicit task identification inputs, OrchMoE automatically discerns\ntask categories, streamlining the learning process. This is achieved through an\nintegrated mechanism comprising an Automatic Task Classification module and a\nTask-Skill Allocation module, which collectively deduce task-specific\nclassifications and tailor skill allocation matrices. Our extensive evaluations\non the 'Super Natural Instructions' dataset, featuring 1,600 diverse\ninstructional tasks, indicate that OrchMoE substantially outperforms comparable\nmulti-adapter baselines in terms of both performance and sample utilization\nefficiency, all while operating within the same parameter constraints. These\nfindings suggest that OrchMoE offers a significant leap forward in multi-task\nlearning efficiency.", "published": "2024-01-19 08:50:54", "link": "http://arxiv.org/abs/2401.10559v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal\n  Models for Video Question Answering", "abstract": "Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the information observed in videos. Despite the recent success of\nLarge Multimodal Models (LMMs) in image-language understanding and reasoning,\nthey deal with VideoQA insufficiently, by simply taking uniformly sampled\nframes as visual inputs, which ignores question-relevant visual clues.\nMoreover, there are no human annotations for question-critical timestamps in\nexisting VideoQA datasets. In light of this, we propose a novel weakly\nsupervised framework to enforce the LMMs to reason out the answers with\nquestion-critical moments as visual inputs. Specifically, we first fuse the\nquestion and answer pairs as event descriptions to find multiple keyframes as\ntarget moments and pseudo-labels, with the visual-language alignment capability\nof the CLIP models. With these pseudo-labeled keyframes as additionally weak\nsupervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG)\nmodule. GCG learns multiple Gaussian functions to characterize the temporal\nstructure of the video, and sample question-critical frames as positive moments\nto be the visual inputs of LMMs. Extensive experiments on several benchmarks\nverify the effectiveness of our framework, and we achieve substantial\nimprovements compared to previous state-of-the-art methods.", "published": "2024-01-19 14:21:46", "link": "http://arxiv.org/abs/2401.10711v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Q&A Prompts: Discovering Rich Visual Clues through Mining\n  Question-Answer Prompts for VQA requiring Diverse World Knowledge", "abstract": "With the breakthrough of multi-modal large language models, answering complex\nvisual questions that demand advanced reasoning abilities and world knowledge\nhas become a much more important testbed for developing AI models than ever.\nHowever, equipping AI models with robust cross-modality reasoning ability\nremains challenging since the cognition scheme of humans has not been\nunderstood systematically. In this paper, we believe that if we can collect\nvisual clues in the given image as much as possible, we will recognize the\nimage more accurately, understand the question better, recall relevant\nknowledge more easily, and finally reason out the answer. We discover these\nrich visual clues by mining question-answer pairs in images and sending them\ninto multi-modal large language models as prompts. We call the proposed method\nQ&A Prompts. Specifically, we first use the image-answer pairs and the\ncorresponding questions in the training set as inputs and outputs to train a\nvisual question generation model. Then, we use an image tagging model to\nidentify various instances and send packaged image-tag pairs into the visual\nquestion generation model to generate relevant questions with the extracted\nimage tags as answers. Finally, we encode these generated question-answer pairs\nas prompts with a visual-aware prompting module and send them into pre-trained\nmulti-modal large language models to reason out the final answers. Experimental\nresults show that, compared with state-of-the-art methods, our Q&A Prompts\nachieves substantial improvements on the challenging visual question answering\ndatasets requiring reasoning over diverse world knowledge, such as OK-VQA and\nA-OKVQA.", "published": "2024-01-19 14:22:29", "link": "http://arxiv.org/abs/2401.10712v5", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Using LLMs to discover emerging coded antisemitic hate-speech in\n  extremist social media", "abstract": "Online hate speech proliferation has created a difficult problem for social\nmedia platforms. A particular challenge relates to the use of coded language by\ngroups interested in both creating a sense of belonging for its users and\nevading detection. Coded language evolves quickly and its use varies over time.\nThis paper proposes a methodology for detecting emerging coded hate-laden\nterminology. The methodology is tested in the context of online antisemitic\ndiscourse. The approach considers posts scraped from social media platforms,\noften used by extremist users. The posts are scraped using seed expressions\nrelated to previously known discourse of hatred towards Jews. The method begins\nby identifying the expressions most representative of each post and calculating\ntheir frequency in the whole corpus. It filters out grammatically incoherent\nexpressions as well as previously encountered ones so as to focus on emergent\nwell-formed terminology. This is followed by an assessment of semantic\nsimilarity to known antisemitic terminology using a fine-tuned large language\nmodel, and subsequent filtering out of the expressions that are too distant\nfrom known expressions of hatred. Emergent antisemitic expressions containing\nterms clearly relating to Jewish topics are then removed to return only coded\nexpressions of hatred.", "published": "2024-01-19 17:40:50", "link": "http://arxiv.org/abs/2401.10841v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs\n  Without Fine-Tuning", "abstract": "This paper investigates the impact of model compression on the way Large\nLanguage Models (LLMs) process prompts, particularly concerning jailbreak\nresistance. We show that moderate WANDA pruning can enhance resistance to\njailbreaking attacks without fine-tuning, while maintaining performance on\nstandard benchmarks. To systematically evaluate this safety enhancement, we\nintroduce a dataset of 225 harmful tasks across five categories. Our analysis\nof LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2 reveals that pruning\nbenefits correlate with initial model safety levels. We interpret these results\nby examining changes in attention patterns and perplexity shifts, demonstrating\nthat pruned models exhibit sharper attention and increased sensitivity to\nartificial jailbreak constructs. We extend our evaluation to the AdvBench\nharmful behavior tasks and the GCG attack method. We find that LLaMA-2 is much\nsafer on AdvBench prompts than on our dataset when evaluated with manual\njailbreak attempts, and that pruning is effective against both automated\nattacks and manual jailbreaking on Advbench.", "published": "2024-01-19 18:05:34", "link": "http://arxiv.org/abs/2401.10862v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Reinforcement learning for question answering in programming domain\n  using public community scoring as a human feedback", "abstract": "In this study, we investigate the enhancement of the GPT Neo 125M performance\nin Community Question Answering (CQA) with a focus on programming, through the\nintegration of Reinforcement Learning from Human Feedback (RLHF) and the\nutilization of scores from Stack Overflow. Two distinct reward model training\nstrategies are employed for fine-tuning with Proximal Policy Optimization\n(PPO). Notably, the improvements in performance achieved through this method\nare comparable to those of GPT Neo 2.7B parameter variant. Additionally, an\nauxiliary scoring mechanism is introduced, which demonstrates the limitations\nof conventional linguistic metrics in evaluating responses in the programming\ndomain. Through accurate analysis, this paper looks at the divergence between\ntraditional linguistic metrics and our human-preferences-based reward model,\nunderscoring the imperative for domain-specific evaluation methods. By\nelucidating the complexities involved in applying RLHF to programming CQA and\naccentuating the significance of context-aware evaluation, this study\ncontributes to the ongoing efforts in refining Large Language Models through\nfocused human feedback.", "published": "2024-01-19 18:49:36", "link": "http://arxiv.org/abs/2401.10882v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Analysis and Detection of Multilingual Hate Speech Using Transformer\n  Based Deep Learning", "abstract": "Hate speech is harmful content that directly attacks or promotes hatred\nagainst members of groups or individuals based on actual or perceived aspects\nof identity, such as racism, religion, or sexual orientation. This can affect\nsocial life on social media platforms as hateful content shared through social\nmedia can harm both individuals and communities. As the prevalence of hate\nspeech increases online, the demand for automated detection as an NLP task is\nincreasing. In this work, the proposed method is using transformer-based model\nto detect hate speech in social media, like twitter, Facebook, WhatsApp,\nInstagram, etc. The proposed model is independent of languages and has been\ntested on Italian, English, German, Bengali. The Gold standard datasets were\ncollected from renowned researcher Zeerak Talat, Sara Tonelli, Melanie Siegel,\nand Rezaul Karim. The success rate of the proposed model for hate speech\ndetection is higher than the existing baseline and state-of-the-art models with\naccuracy in Bengali dataset is 89%, in English: 91%, in German dataset 91% and\nin Italian dataset it is 77%. The proposed algorithm shows substantial\nimprovement to the benchmark method.", "published": "2024-01-19 20:40:23", "link": "http://arxiv.org/abs/2401.11021v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Combining topic modelling and citation network analysis to study case\n  law from the European Court on Human Rights on the right to respect for\n  private and family life", "abstract": "As legal case law databases such as HUDOC continue to grow rapidly, it has\nbecome essential for legal researchers to find efficient methods to handle such\nlarge-scale data sets. Such case law databases usually consist of the textual\ncontent of cases together with the citations between them. This paper focuses\non case law from the European Court of Human Rights on Article 8 of the\nEuropean Convention of Human Rights, the right to respect private and family\nlife, home and correspondence. In this study, we demonstrate and compare the\npotential of topic modelling and citation network to find and organize case law\non Article 8 based on their general themes and citation patterns, respectively.\nAdditionally, we explore whether combining these two techniques leads to better\nresults compared to the application of only one of the methods. We evaluate the\neffectiveness of the combined method on a unique manually collected and\nannotated dataset of Aricle 8 case law on evictions. The results of our\nexperiments show that our combined (text and citation-based) approach provides\nthe best results in finding and grouping case law, providing scholars with an\neffective way to extract and analyse relevant cases on a specific issue.", "published": "2024-01-19 14:30:35", "link": "http://arxiv.org/abs/2401.16429v1", "categories": ["cs.IR", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and\n  Google Bard Content in Relation to BioMedical Literature", "abstract": "Background: The emergence of generative AI tools, empowered by Large Language\nModels (LLMs), has shown powerful capabilities in generating content. To date,\nthe assessment of the usefulness of such content, generated by what is known as\nprompt engineering, has become an interesting research question. Objectives\nUsing the mean of prompt engineering, we assess the similarity and closeness of\nsuch contents to real literature produced by scientists. Methods In this\nexploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to\ngenerate clinical content to be compared with literature counterparts, (2) we\nassess the similarities of the contents generated by comparing them with\ncounterparts from biomedical literature. Our approach is to use text-mining\napproaches to compare documents and associated bigrams and to use network\nanalysis to assess the terms' centrality. Results The experiments demonstrated\nthat ChatGPT outperformed Google Bard in cosine document similarity (38% to\n34%), Jaccard document similarity (23% to 19%), TF-IDF bigram similarity (47%\nto 41%), and term network centrality (degree and closeness). We also found new\nlinks that emerged in ChatGPT bigram networks that did not exist in literature\nbigram networks. Conclusions: The obtained similarity results show that ChatGPT\noutperformed Google Bard in document similarity, bigrams, and degree and\ncloseness centrality. We also observed that ChatGPT offers linkage to terms\nthat are connected in the literature. Such connections could inspire asking\ninteresting questions and generate new hypotheses.", "published": "2024-01-19 17:14:46", "link": "http://arxiv.org/abs/2402.05116v2", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SocraSynth: Multi-LLM Reasoning with Conditional Statistics", "abstract": "Large language models (LLMs), while promising, face criticisms for biases,\nhallucinations, and a lack of reasoning capability. This paper introduces\nSocraSynth, a multi-LLM agent reasoning platform developed to mitigate these\nissues. SocraSynth utilizes conditional statistics and systematic context\nenhancement through continuous arguments, alongside adjustable debate\ncontentiousness levels. The platform typically involves a human moderator and\ntwo LLM agents representing opposing viewpoints on a given subject. SocraSynth\noperates in two main phases: knowledge generation and reasoning evaluation. In\nthe knowledge generation phase, the moderator defines the debate topic and\ncontentiousness level, prompting the agents to formulate supporting arguments\nfor their respective stances. The reasoning evaluation phase then employs\nSocratic reasoning and formal logic principles to appraise the quality of the\narguments presented. The dialogue concludes with the moderator adjusting the\ncontentiousness from confrontational to collaborative, gathering final,\nconciliatory remarks to aid in human reasoning and decision-making. Through\ncase studies in three distinct application domains, this paper showcases\nSocraSynth's effectiveness in fostering rigorous research, dynamic reasoning,\ncomprehensive assessment, and enhanced collaboration. This underscores the\nvalue of multi-agent interactions in leveraging LLMs for advanced knowledge\nextraction and decision-making support.", "published": "2024-01-19 07:16:21", "link": "http://arxiv.org/abs/2402.06634v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Large Language Models are Efficient Learners of Noise-Robust Speech\n  Recognition", "abstract": "Recent advances in large language models (LLMs) have promoted generative\nerror correction (GER) for automatic speech recognition (ASR), which leverages\nthe rich linguistic knowledge and powerful reasoning ability of LLMs to improve\nrecognition results. The latest work proposes a GER benchmark with HyPoradise\ndataset to learn the mapping from ASR N-best hypotheses to ground-truth\ntranscription by efficient LLM finetuning, which shows great effectiveness but\nlacks specificity on noise-robust ASR. In this work, we extend the benchmark to\nnoisy conditions and investigate if we can teach LLMs to perform denoising for\nGER just like what robust ASR do}, where one solution is introducing noise\ninformation as a conditioner into LLM. However, directly incorporating noise\nembeddings from audio encoder could harm the LLM tuning due to cross-modality\ngap. To this end, we propose to extract a language-space noise embedding from\nthe N-best list to represent the noise conditions of source speech, which can\npromote the denoising process in GER. Furthermore, in order to enhance its\nrepresentation ability of audio noise, we design a knowledge distillation (KD)\napproach via mutual information estimation to distill the real noise\ninformation in audio embeddings to our language embedding. Experiments on\nvarious latest LLMs demonstrate our approach achieves a new breakthrough with\nup to 53.9% correction improvement in terms of word error rate while with\nlimited training data. Analysis shows that our language-space noise embedding\ncan well represent the noise conditions of source speech, under which\noff-the-shelf LLMs show strong ability of language-space denoising.", "published": "2024-01-19 01:29:27", "link": "http://arxiv.org/abs/2401.10446v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Attentive Fusion: A Transformer-based Approach to Multimodal Hate Speech\n  Detection", "abstract": "With the recent surge and exponential growth of social media usage,\nscrutinizing social media content for the presence of any hateful content is of\nutmost importance. Researchers have been diligently working since the past\ndecade on distinguishing between content that promotes hatred and content that\ndoes not. Traditionally, the main focus has been on analyzing textual content.\nHowever, recent research attempts have also commenced into the identification\nof audio-based content. Nevertheless, studies have shown that relying solely on\naudio or text-based content may be ineffective, as recent upsurge indicates\nthat individuals often employ sarcasm in their speech and writing. To overcome\nthese challenges, we present an approach to identify whether a speech promotes\nhate or not utilizing both audio and textual representations. Our methodology\nis based on the Transformer framework that incorporates both audio and text\nsampling, accompanied by our very own layer called \"Attentive Fusion\". The\nresults of our study surpassed previous state-of-the-art techniques, achieving\nan impressive macro F1 score of 0.927 on the Test Set.", "published": "2024-01-19 11:59:13", "link": "http://arxiv.org/abs/2401.10653v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Investigating Training Strategies and Model Robustness of Low-Rank\n  Adaptation for Language Modeling in Speech Recognition", "abstract": "The use of low-rank adaptation (LoRA) with frozen pretrained language models\n(PLMs) has become increasing popular as a mainstream, resource-efficient\nmodeling approach for memory-constrained hardware. In this study, we first\nexplore how to enhance model performance by introducing various LoRA training\nstrategies, achieving relative word error rate reductions of 3.50\\% on the\npublic Librispeech dataset and of 3.67\\% on an internal dataset in the\nmessaging domain. To further characterize the stability of LoRA-based\nsecond-pass speech recognition models, we examine robustness against input\nperturbations. These perturbations are rooted in homophone replacements and a\nnovel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both\ndesigned to measure the relative degradation in the performance of rescoring\nmodels. Our experimental results indicate that while advanced variants of LoRA,\nsuch as dynamic rank-allocated LoRA, lead to performance degradation in\n$1$-best perturbation, they alleviate the degradation in $N$-best perturbation.\nThis finding is in comparison to fully-tuned models and vanilla LoRA tuning\nbaselines, suggesting that a comprehensive selection is needed when using\nLoRA-based adaptation for compute-cost savings and robust language modeling.", "published": "2024-01-19 01:30:16", "link": "http://arxiv.org/abs/2401.10447v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "3D Room Geometry Inference from Multichannel Room Impulse Response using\n  Deep Neural Network", "abstract": "Room geometry inference (RGI) aims at estimating room shapes from measured\nroom impulse responses (RIRs) and has received lots of attention for its\nimportance in environment-aware audio rendering and virtual acoustic\nrepresentation of a real venue. A lot of estimation models utilizing time\ndifference of arrival (TDoA) or time of arrival (ToA) information in RIRs have\nbeen proposed. However, an estimation model should be able to handle more\ngeneral features and complex relations between reflections to cope with various\nroom shapes and uncertainties such as the unknown number of walls. In this\nstudy, we propose a deep neural network that can estimate various room shapes\nwithout prior assumptions on the shape or number of walls. The proposed model\nconsists of three sub-networks: a feature extractor, parameter estimation, and\nevaluation networks, which extract key features from RIRs, estimate parameters,\nand evaluate the confidence of estimated parameters, respectively. The network\nis trained by about 40,000 RIRs simulated in rooms of different shapes using a\nsingle source and spherical microphone array and tested for rooms of unseen\nshapes and dimensions. The proposed algorithm achieves almost perfect accuracy\nin finding the true number of walls and shows negligible errors in room shapes.", "published": "2024-01-19 01:50:29", "link": "http://arxiv.org/abs/2401.10453v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Two-Stage Framework in Cross-Spectrum Domain for Real-Time Speech\n  Enhancement", "abstract": "Two-stage pipeline is popular in speech enhancement tasks due to its\nsuperiority over traditional single-stage methods. The current two-stage\napproaches usually enhance the magnitude spectrum in the first stage, and\nfurther modify the complex spectrum to suppress the residual noise and recover\nthe speech phase in the second stage. The above whole process is performed in\nthe short-time Fourier transform (STFT) spectrum domain. In this paper, we\nre-implement the above second sub-process in the short-time discrete cosine\ntransform (STDCT) spectrum domain. The reason is that we have found STDCT\nperforms greater noise suppression capability than STFT. Additionally, the\nimplicit phase of STDCT ensures simpler and more efficient phase recovery,\nwhich is challenging and computationally expensive in the STFT-based methods.\nTherefore, we propose a novel two-stage framework called the STFT-STDCT\nspectrum fusion network (FDFNet) for speech enhancement in cross-spectrum\ndomain. Experimental results demonstrate that the proposed FDFNet outperforms\nthe previous two-stage methods and also exhibits superior performance compared\nto other advanced systems.", "published": "2024-01-19 05:12:23", "link": "http://arxiv.org/abs/2401.10494v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time\n  Zero-Shot Voice Conversion", "abstract": "Recent language model (LM) advancements have showcased impressive zero-shot\nvoice conversion (VC) performance. However, existing LM-based VC models usually\napply offline conversion from source semantics to acoustic features, demanding\nthe complete source speech and limiting their deployment to real-time\napplications. In this paper, we introduce StreamVoice, a novel streaming\nLM-based model for zero-shot VC, facilitating real-time conversion given\narbitrary speaker prompts and source speech. Specifically, to enable streaming\ncapability, StreamVoice employs a fully causal context-aware LM with a\ntemporal-independent acoustic predictor, while alternately processing semantic\nand acoustic features at each time step of autoregression which eliminates the\ndependence on complete source speech. To address the potential performance\ndegradation from the incomplete context in streaming processing, we enhance the\ncontext-awareness of the LM through two strategies: 1) teacher-guided context\nforesight, using a teacher model to summarize the present and future semantic\ncontext during training to guide the model's forecasting for missing context;\n2) semantic masking strategy, promoting acoustic prediction from preceding\ncorrupted semantic and acoustic input, enhancing context-learning ability.\nNotably, StreamVoice is the first LM-based streaming zero-shot VC model without\nany future look-ahead. Experiments demonstrate StreamVoice's streaming\nconversion capability while achieving zero-shot performance comparable to\nnon-streaming VC systems.", "published": "2024-01-19 23:05:05", "link": "http://arxiv.org/abs/2401.11053v5", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ultra-lightweight Neural Differential DSP Vocoder For High Quality\n  Speech Synthesis", "abstract": "Neural vocoders model the raw audio waveform and synthesize high-quality\naudio, but even the highly efficient ones, like MB-MelGAN and LPCNet, fail to\nrun real-time on a low-end device like a smartglass. A pure digital signal\nprocessing (DSP) based vocoder can be implemented via lightweight fast Fourier\ntransforms (FFT), and therefore, is a magnitude faster than any neural vocoder.\nA DSP vocoder often gets a lower audio quality due to consuming over-smoothed\nacoustic model predictions of approximate representations for the vocal tract.\nIn this paper, we propose an ultra-lightweight differential DSP (DDSP) vocoder\nthat uses a jointly optimized acoustic model with a DSP vocoder, and learns\nwithout an extracted spectral feature for the vocal tract. The model achieves\naudio quality comparable to neural vocoders with a high average MOS of 4.36\nwhile being efficient as a DSP vocoder. Our C++ implementation, without any\nhardware-specific optimization, is at 15 MFLOPS, surpasses MB-MelGAN by 340\ntimes in terms of FLOPS, and achieves a vocoder-only RTF of 0.003 and overall\nRTF of 0.044 while running single-threaded on a 2GHz Intel Xeon CPU.", "published": "2024-01-19 02:51:00", "link": "http://arxiv.org/abs/2401.10460v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks", "abstract": "Recently, Transformers have been introduced into the field of acoustics\nrecognition. They are pre-trained on large-scale datasets using methods such as\nsupervised learning and semi-supervised learning, demonstrating robust\ngenerality--It fine-tunes easily to downstream tasks and shows more robust\nperformance. However, the predominant fine-tuning method currently used is\nstill full fine-tuning, which involves updating all parameters during training.\nThis not only incurs significant memory usage and time costs but also\ncompromises the model's generality. Other fine-tuning methods either struggle\nto address this issue or fail to achieve matching performance. Therefore, we\nconducted a comprehensive analysis of existing fine-tuning methods and proposed\nan efficient fine-tuning approach based on Adapter tuning, namely AAT. The core\nidea is to freeze the audio Transformer model and insert extra learnable\nAdapters, efficiently acquiring downstream task knowledge without compromising\nthe model's original generality. Extensive experiments have shown that our\nmethod achieves performance comparable to or even superior to full fine-tuning\nwhile optimizing only 7.118% of the parameters. It also demonstrates\nsuperiority over other fine-tuning methods.", "published": "2024-01-19 08:07:59", "link": "http://arxiv.org/abs/2401.10544v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Revealing Emotional Clusters in Speaker Embeddings: A Contrastive\n  Learning Strategy for Speech Emotion Recognition", "abstract": "Speaker embeddings carry valuable emotion-related information, which makes\nthem a promising resource for enhancing speech emotion recognition (SER),\nespecially with limited labeled data. Traditionally, it has been assumed that\nemotion information is indirectly embedded within speaker embeddings, leading\nto their under-utilization. Our study reveals a direct and useful link between\nemotion and state-of-the-art speaker embeddings in the form of intra-speaker\nclusters. By conducting a thorough clustering analysis, we demonstrate that\nemotion information can be readily extracted from speaker embeddings. In order\nto leverage this information, we introduce a novel contrastive pretraining\napproach applied to emotion-unlabeled data for speech emotion recognition. The\nproposed approach involves the sampling of positive and the negative examples\nbased on the intra-speaker clusters of speaker embeddings. The proposed\nstrategy, which leverages extensive emotion-unlabeled data, leads to a\nsignificant improvement in SER performance, whether employed as a standalone\npretraining task or integrated into a multi-task pretraining setting.", "published": "2024-01-19 20:31:53", "link": "http://arxiv.org/abs/2401.11017v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound\n  Event Localization and Detection in Realistic Rooms", "abstract": "Sound event localization and detection (SELD) is an important task in machine\nlistening. Major advancements rely on simulated data with sound events in\nspecific rooms and strong spatio-temporal labels. SELD data is simulated by\nconvolving spatialy-localized room impulse responses (RIRs) with sound\nwaveforms to place sound events in a soundscape. However, RIRs require manual\ncollection in specific rooms. We present SpatialScaper, a library for SELD data\nsimulation and augmentation. Compared to existing tools, SpatialScaper emulates\nvirtual rooms via parameters such as size and wall absorption. This allows for\nparameterized placement (including movement) of foreground and background sound\nsources. SpatialScaper also includes data augmentation pipelines that can be\napplied to existing SELD data. As a case study, we use SpatialScaper to add\nrooms to the DCASE SELD data. Training a model with our data led to progressive\nperformance improves as a direct function of acoustic diversity. These results\nshow that SpatialScaper is valuable to train robust SELD models.", "published": "2024-01-19 19:01:13", "link": "http://arxiv.org/abs/2401.12238v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
