{"title": "Best Practices for Data-Efficient Modeling in NLG:How to Train\n  Production-Ready Neural Models with Less Data", "abstract": "Natural language generation (NLG) is a critical component in conversational\nsystems, owing to its role of formulating a correct and natural text response.\nTraditionally, NLG components have been deployed using template-based\nsolutions. Although neural network solutions recently developed in the research\ncommunity have been shown to provide several benefits, deployment of such\nmodel-based solutions has been challenging due to high latency, correctness\nissues, and high data needs. In this paper, we present approaches that have\nhelped us deploy data-efficient neural solutions for NLG in conversational\nsystems to production. We describe a family of sampling and modeling techniques\nto attain production quality with light-weight neural network models using only\na fraction of the data that would be necessary otherwise, and show a thorough\ncomparison between each. Our results show that domain complexity dictates the\nappropriate approach to achieve high data efficiency. Finally, we distill the\nlessons from our experimental findings into a list of best practices for\nproduction-level NLG model development, and present them in a brief runbook.\nImportantly, the end products of all of the techniques are small\nsequence-to-sequence models (2Mb) that we can reliably deploy in production.", "published": "2020-11-08 00:38:08", "link": "http://arxiv.org/abs/2011.03877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Denoising Relation Extraction from Document-level Distant Supervision", "abstract": "Distant supervision (DS) has been widely used to generate auto-labeled data\nfor sentence-level relation extraction (RE), which improves RE performance.\nHowever, the existing success of DS cannot be directly transferred to the more\nchallenging document-level relation extraction (DocRE), since the inherent\nnoise in DS may be even multiplied in document level and significantly harm the\nperformance of RE. To address this challenge, we propose a novel pre-trained\nmodel for DocRE, which denoises the document-level DS data via multiple\npre-training tasks. Experimental results on the large-scale DocRE benchmark\nshow that our model can capture useful information from noisy DS data and\nachieve promising results.", "published": "2020-11-08 02:05:25", "link": "http://arxiv.org/abs/2011.03888v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Gold Standard Methodology for Evaluating Accuracy in Data-To-Text\n  Systems", "abstract": "Most Natural Language Generation systems need to produce accurate texts. We\npropose a methodology for high-quality human evaluation of the accuracy of\ngenerated texts, which is intended to serve as a gold-standard for accuracy\nevaluations of data-to-text systems. We use our methodology to evaluate the\naccuracy of computer generated basketball summaries. We then show how our gold\nstandard evaluation can be used to validate automated metrics", "published": "2020-11-08 14:49:18", "link": "http://arxiv.org/abs/2011.03992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Practical Ability of Recurrent Neural Networks to Recognize\n  Hierarchical Languages", "abstract": "While recurrent models have been effective in NLP tasks, their performance on\ncontext-free languages (CFLs) has been found to be quite weak. Given that CFLs\nare believed to capture important phenomena such as hierarchical structure in\nnatural languages, this discrepancy in performance calls for an explanation. We\nstudy the performance of recurrent models on Dyck-n languages, a particularly\nimportant and well-studied class of CFLs. We find that while recurrent models\ngeneralize nearly perfectly if the lengths of the training and test strings are\nfrom the same range, they perform poorly if the test strings are longer. At the\nsame time, we observe that recurrent models are expressive enough to recognize\nDyck words of arbitrary lengths in finite precision if their depths are\nbounded. Hence, we evaluate our models on samples generated from Dyck languages\nwith bounded depth and find that they are indeed able to generalize to much\nhigher lengths. Since natural language datasets have nested dependencies of\nbounded depth, this may help explain why they perform well in modeling\nhierarchical dependencies in natural language data despite prior works\nindicating poor generalization performance on Dyck languages. We perform\nprobing studies to support our results and provide comparisons with\nTransformers.", "published": "2020-11-08 12:15:31", "link": "http://arxiv.org/abs/2011.03965v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring End-to-End Differentiable Natural Logic Modeling", "abstract": "We explore end-to-end trained differentiable models that integrate natural\nlogic with neural networks, aiming to keep the backbone of natural language\nreasoning based on the natural logic formalism while introducing subsymbolic\nvector representations and neural components. The proposed model adapts module\nnetworks to model natural logic operations, which is enhanced with a memory\ncomponent to model contextual information. Experiments show that the proposed\nframework can effectively model monotonicity-based reasoning, compared to the\nbaseline neural network models without built-in inductive bias for\nmonotonicity-based reasoning. Our proposed model shows to be robust when\ntransferred from upward to downward inference. We perform further analyses on\nthe performance of the proposed model on aggregation, showing the effectiveness\nof the proposed subcomponents on helping achieve better intermediate\naggregation performance.", "published": "2020-11-08 18:18:15", "link": "http://arxiv.org/abs/2011.04044v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Metrics also Disagree in the Low Scoring Range: Revisiting Summarization\n  Evaluation Metrics", "abstract": "In text summarization, evaluating the efficacy of automatic metrics without\nhuman judgments has become recently popular. One exemplar work concludes that\nautomatic metrics strongly disagree when ranking high-scoring summaries. In\nthis paper, we revisit their experiments and find that their observations stem\nfrom the fact that metrics disagree in ranking summaries from any narrow\nscoring range. We hypothesize that this may be because summaries are similar to\neach other in a narrow scoring range and are thus, difficult to rank. Apart\nfrom the width of the scoring range of summaries, we analyze three other\nproperties that impact inter-metric agreement - Ease of Summarization,\nAbstractiveness, and Coverage. To encourage reproducible research, we make all\nour analysis code and data publicly available.", "published": "2020-11-08 22:26:06", "link": "http://arxiv.org/abs/2011.04096v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting Emerging Symptoms of COVID-19 using Context-based Twitter\n  Embeddings", "abstract": "In this paper, we present an iterative graph-based approach for the detection\nof symptoms of COVID-19, the pathology of which seems to be evolving. More\ngenerally, the method can be applied to finding context-specific words and\ntexts (e.g. symptom mentions) in large imbalanced corpora (e.g. all tweets\nmentioning #COVID-19). Given the novelty of COVID-19, we also test if the\nproposed approach generalizes to the problem of detecting Adverse Drug Reaction\n(ADR). We find that the approach applied to Twitter data can detect symptom\nmentions substantially before being reported by the Centers for Disease Control\n(CDC).", "published": "2020-11-08 13:56:05", "link": "http://arxiv.org/abs/2011.03983v1", "categories": ["cs.CL", "cs.HC", "cs.SI"], "primary_category": "cs.CL"}
{"title": "DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for\n  Temporal Knowledge Graph Completion", "abstract": "There has recently been increasing interest in learning representations of\ntemporal knowledge graphs (KGs), which record the dynamic relationships between\nentities over time. Temporal KGs often exhibit multiple simultaneous\nnon-Euclidean structures, such as hierarchical and cyclic structures. However,\nexisting embedding approaches for temporal KGs typically learn entity\nrepresentations and their dynamic evolution in the Euclidean space, which might\nnot capture such intrinsic structures very well. To this end, we propose Dy-\nERNIE, a non-Euclidean embedding approach that learns evolving entity\nrepresentations in a product of Riemannian manifolds, where the composed spaces\nare estimated from the sectional curvatures of underlying data. Product\nmanifolds enable our approach to better reflect a wide variety of geometric\nstructures on temporal KGs. Besides, to capture the evolutionary dynamics of\ntemporal KGs, we let the entity representations evolve according to a velocity\nvector defined in the tangent space at each timestamp. We analyze in detail the\ncontribution of geometric spaces to representation learning of temporal KGs and\nevaluate our model on temporal knowledge graph completion tasks. Extensive\nexperiments on three real-world datasets demonstrate significantly improved\nperformance, indicating that the dynamics of multi-relational graph data can be\nmore properly modeled by the evolution of embeddings on Riemannian manifolds.", "published": "2020-11-08 14:04:16", "link": "http://arxiv.org/abs/2011.03984v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adapting a Language Model for Controlled Affective Text Generation", "abstract": "Human use language not just to convey information but also to express their\ninner feelings and mental states. In this work, we adapt the state-of-the-art\nlanguage generation models to generate affective (emotional) text. We posit a\nmodel capable of generating affect-driven and topic-focused sentences without\nlosing grammatical correctness as the affect intensity increases. We propose to\nincorporate emotion as prior for the probabilistic state-of-the-art text\ngeneration model such as GPT-2. The model gives a user the flexibility to\ncontrol the category and intensity of emotion as well as the topic of the\ngenerated text. Previous attempts at modelling fine-grained emotions fall out\non grammatical correctness at extreme intensities, but our model is resilient\nto this and delivers robust results at all intensities. We conduct automated\nevaluations and human studies to test the performance of our model and provide\na detailed comparison of the results with other models. In all evaluations, our\nmodel outperforms existing affective text generation models.", "published": "2020-11-08 15:24:39", "link": "http://arxiv.org/abs/2011.04000v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stochastic Attention Head Removal: A simple and effective method for\n  improving Transformer Based ASR Models", "abstract": "Recently, Transformer based models have shown competitive automatic speech\nrecognition (ASR) performance. One key factor in the success of these models is\nthe multi-head attention mechanism. However, for trained models, we have\npreviously observed that many attention matrices are close to diagonal,\nindicating the redundancy of the corresponding attention heads. We have also\nfound that some architectures with reduced numbers of attention heads have\nbetter performance. Since the search for the best structure is time\nprohibitive, we propose to randomly remove attention heads during training and\nkeep all attention heads at test time, thus the final model is an ensemble of\nmodels with different architectures. The proposed method also forces each head\nindependently learn the most useful patterns. We apply the proposed method to\ntrain Transformer based and Convolution-augmented Transformer (Conformer) based\nASR models. Our method gives consistent performance gains over strong baselines\non the Wall Street Journal, AISHELL, Switchboard and AMI datasets. To the best\nof our knowledge, we have achieved state-of-the-art end-to-end Transformer\nbased model performance on Switchboard and AMI.", "published": "2020-11-08 15:41:03", "link": "http://arxiv.org/abs/2011.04004v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On the Usefulness of Self-Attention for Automatic Speech Recognition\n  with Transformers", "abstract": "Self-attention models such as Transformers, which can capture temporal\nrelationships without being limited by the distance between events, have given\ncompetitive speech recognition results. However, we note the range of the\nlearned context increases from the lower to upper self-attention layers, whilst\nacoustic events often happen within short time spans in a left-to-right order.\nThis leads to a question: for speech recognition, is a global view of the\nentire sequence useful for the upper self-attention encoder layers in\nTransformers? To investigate this, we train models with lower\nself-attention/upper feed-forward layers encoders on Wall Street Journal and\nSwitchboard. Compared to baseline Transformers, no performance drop but minor\ngains are observed. We further developed a novel metric of the diagonality of\nattention matrices and found the learned diagonality indeed increases from the\nlower to upper encoder self-attention layers. We conclude the global view is\nunnecessary in training upper encoder layers.", "published": "2020-11-08 16:01:38", "link": "http://arxiv.org/abs/2011.04906v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Long Range Arena: A Benchmark for Efficient Transformers", "abstract": "Transformers do not scale very well to long sequence lengths largely because\nof quadratic self-attention complexity. In the recent months, a wide spectrum\nof efficient, fast Transformers have been proposed to tackle this problem, more\noften than not claiming superior or comparable model quality to vanilla\nTransformer models. To this date, there is no well-established consensus on how\nto evaluate this class of models. Moreover, inconsistent benchmarking on a wide\nspectrum of tasks and datasets makes it difficult to assess relative model\nquality amongst many models. This paper proposes a systematic and unified\nbenchmark, LRA, specifically focused on evaluating model quality under\nlong-context scenarios. Our benchmark is a suite of tasks consisting of\nsequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data\ntypes and modalities such as text, natural, synthetic images, and mathematical\nexpressions requiring similarity, structural, and visual-spatial reasoning. We\nsystematically evaluate ten well-established long-range Transformer models\n(Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers,\nSynthesizers, Sparse Transformers, and Longformers) on our newly proposed\nbenchmark suite. LRA paves the way towards better understanding this class of\nefficient Transformer models, facilitates more research in this direction, and\npresents new challenging tasks to tackle. Our benchmark code will be released\nat https://github.com/google-research/long-range-arena.", "published": "2020-11-08 15:53:56", "link": "http://arxiv.org/abs/2011.04006v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Fine-grained Style Modeling, Transfer and Prediction in Text-to-Speech\n  Synthesis via Phone-Level Content-Style Disentanglement", "abstract": "This paper presents a novel design of neural network system for fine-grained\nstyle modeling, transfer and prediction in expressive text-to-speech (TTS)\nsynthesis. Fine-grained modeling is realized by extracting style embeddings\nfrom the mel-spectrograms of phone-level speech segments. Collaborative\nlearning and adversarial learning strategies are applied in order to achieve\neffective disentanglement of content and style factors in speech and alleviate\nthe \"content leakage\" problem in style modeling. The proposed system can be\nused for varying-content speech style transfer in the single-speaker scenario.\nThe results of objective and subjective evaluation show that our system\nperforms better than other fine-grained speech style transfer models,\nespecially in the aspect of content preservation. By incorporating a style\npredictor, the proposed system can also be used for text-to-speech synthesis.\nAudio samples are provided for system demonstration\nhttps://daxintan-cuhk.github.io/pl-csd-speech .", "published": "2020-11-08 09:51:21", "link": "http://arxiv.org/abs/2011.03943v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Denoising-and-Dereverberation Hierarchical Neural Vocoder for Robust\n  Waveform Generation", "abstract": "This paper presents a denoising and dereverberation hierarchical neural\nvocoder (DNR-HiNet) to convert noisy and reverberant acoustic features into a\nclean speech waveform. We implement it mainly by modifying the amplitude\nspectrum predictor (ASP) in the original HiNet vocoder. This modified denoising\nand dereverberation ASP (DNR-ASP) can predict clean log amplitude spectra (LAS)\nfrom input degraded acoustic features. To achieve this, the DNR-ASP first\npredicts the noisy and reverberant LAS, noise LAS related to the noise\ninformation, and room impulse response related to the reverberation information\nthen performs initial denoising and dereverberation. The initial processed LAS\nare then enhanced by another neural network as the final clean LAS. To further\nimprove the quality of the generated clean LAS, we also introduce a bandwidth\nextension model and frequency resolution extension model in the DNR-ASP. The\nexperimental results indicate that the DNR-HiNet vocoder was able to generate a\ndenoised and dereverberated waveform given noisy and reverberant acoustic\nfeatures and outperformed the original HiNet vocoder and a few other neural\nvocoders. We also applied the DNR-HiNet vocoder to speech enhancement tasks,\nand its performance was competitive with several advanced speech enhancement\nmethods.", "published": "2020-11-08 11:09:00", "link": "http://arxiv.org/abs/2011.03955v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Listen, Look and Deliberate: Visual context-aware speech recognition\n  using pre-trained text-video representations", "abstract": "In this study, we try to address the problem of leveraging visual signals to\nimprove Automatic Speech Recognition (ASR), also known as visual context-aware\nASR (VC-ASR). We explore novel VC-ASR approaches to leverage video and text\nrepresentations extracted by a self-supervised pre-trained text-video embedding\nmodel. Firstly, we propose a multi-stream attention architecture to leverage\nsignals from both audio and video modalities. This architecture consists of\nseparate encoders for the two modalities and a single decoder that attends over\nthem. We show that this architecture is better than fusing modalities at the\nsignal level. Additionally, we also explore leveraging the visual information\nin a second pass model, which has also been referred to as a `deliberation\nmodel'. The deliberation model accepts audio representations and text\nhypotheses from the first pass ASR and combines them with a visual stream for\nan improved visual context-aware recognition. The proposed deliberation scheme\ncan work on top of any well trained ASR and also enabled us to leverage the\npre-trained text model to ground the hypotheses with the visual features. Our\nexperiments on HOW2 dataset show that multi-stream and deliberation\narchitectures are very effective at the VC-ASR task. We evaluate the proposed\nmodels for two scenarios; clean audio stream and distorted audio in which we\nmask out some specific words in the audio. The deliberation model outperforms\nthe multi-stream model and achieves a relative WER improvement of 6% and 8.7%\nfor the clean and masked data, respectively, compared to an audio-only model.\nThe deliberation model also improves recovering the masked words by 59%\nrelative.", "published": "2020-11-08 21:26:17", "link": "http://arxiv.org/abs/2011.04084v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Frequency Gating: Improved Convolutional Neural Networks for Speech\n  Enhancement in the Time-Frequency Domain", "abstract": "One of the strengths of traditional convolutional neural networks (CNNs) is\ntheir inherent translational invariance. However, for the task of speech\nenhancement in the time-frequency domain, this property cannot be fully\nexploited due to a lack of invariance in the frequency direction. In this paper\nwe propose to remedy this inefficiency by introducing a method, which we call\nFrequency Gating, to compute multiplicative weights for the kernels of the CNN\nin order to make them frequency dependent. Several mechanisms are explored:\ntemporal gating, in which weights are dependent on prior time frames, local\ngating, whose weights are generated based on a single time frame and the ones\nadjacent to it, and frequency-wise gating, where each kernel is assigned a\nweight independent of the input data. Experiments with an autoencoder neural\nnetwork with skip connections show that both local and frequency-wise gating\noutperform the baseline and are therefore viable ways to improve CNN-based\nspeech enhancement neural networks. In addition, a loss function based on the\nextended short-time objective intelligibility score (ESTOI) is introduced,\nwhich we show to outperform the standard mean squared error (MSE) loss\nfunction.", "published": "2020-11-08 22:04:00", "link": "http://arxiv.org/abs/2011.04092v1", "categories": ["cs.SD", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
