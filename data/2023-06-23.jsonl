{"title": "Stress Testing BERT Anaphora Resolution Models for Reaction Extraction\n  in Chemical Patents", "abstract": "The high volume of published chemical patents and the importance of a timely\nacquisition of their information gives rise to automating information\nextraction from chemical patents. Anaphora resolution is an important component\nof comprehensive information extraction, and is critical for extracting\nreactions. In chemical patents, there are five anaphoric relations of interest:\nco-reference, transformed, reaction associated, work up, and contained. Our\ngoal is to investigate how the performance of anaphora resolution models for\nreaction texts in chemical patents differs in a noise-free and noisy\nenvironment and to what extent we can improve the robustness against noise of\nthe model.", "published": "2023-06-23 09:01:56", "link": "http://arxiv.org/abs/2306.13379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-Pretrained Transformer: Long-range Language Modeling with\n  Self-retrieval", "abstract": "Retrieval-augmented language models (LMs) have received much attention\nrecently. However, typically the retriever is not trained jointly as a native\ncomponent of the LM, but added post-hoc to an already-pretrained LM, which\nlimits the ability of the LM and the retriever to adapt to one another. In this\nwork, we propose the Retrieval-Pretrained Transformer (RPT), an architecture\nand training procedure for jointly training a retrieval-augmented LM from\nscratch and apply it to the task of modeling long texts. Given a recently\ngenerated text chunk in a long document, the LM computes query representations,\nwhich are then used to retrieve earlier chunks in the document, located\npotentially tens of thousands of tokens before. Information from retrieved\nchunks is fused into the LM representations to predict the next target chunk.\nWe train the retriever component with a semantic objective, where the goal is\nto retrieve chunks that increase the probability of the next chunk, according\nto a reference LM. We evaluate RPT on four long-range language modeling tasks,\nspanning books, code, and mathematical writing, and demonstrate that RPT\nimproves retrieval quality and subsequently perplexity across the board\ncompared to strong baselines.", "published": "2023-06-23 10:18:02", "link": "http://arxiv.org/abs/2306.13421v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Descriptive Image Captioning via Semipermeable Maximum\n  Likelihood Estimation", "abstract": "Image captioning aims to describe visual content in natural language. As 'a\npicture is worth a thousand words', there could be various correct descriptions\nfor an image. However, with maximum likelihood estimation as the training\nobjective, the captioning model is penalized whenever its prediction mismatches\nwith the label. For instance, when the model predicts a word expressing richer\nsemantics than the label, it will be penalized and optimized to prefer more\nconcise expressions, referred to as conciseness optimization. In contrast,\npredictions that are more concise than labels lead to richness optimization.\nSuch conflicting optimization directions could eventually result in the model\ngenerating general descriptions. In this work, we introduce Semipermeable\nMaxImum Likelihood Estimation (SMILE), which allows richness optimization while\nblocking conciseness optimization, thus encouraging the model to generate\nlonger captions with more details. Extensive experiments on two mainstream\nimage captioning datasets MSCOCO and Flickr30K demonstrate that SMILE\nsignificantly enhances the descriptiveness of generated captions. We further\nprovide in-depth investigations to facilitate a better understanding of how\nSMILE works.", "published": "2023-06-23 12:03:07", "link": "http://arxiv.org/abs/2306.13460v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Infused Self Attention Transformers", "abstract": "Transformer-based language models have achieved impressive success in various\nnatural language processing tasks due to their ability to capture complex\ndependencies and contextual information using self-attention mechanisms.\nHowever, they are not without limitations. These limitations include\nhallucinations, where they produce incorrect outputs with high confidence, and\nalignment issues, where they generate unhelpful and unsafe outputs for human\nusers. These limitations stem from the absence of implicit and missing context\nin the data alone. To address this, researchers have explored augmenting these\nmodels with external knowledge from knowledge graphs to provide the necessary\nadditional context. However, the ad-hoc nature of existing methods makes it\ndifficult to properly analyze the effects of knowledge infusion on the many\nmoving parts or components of a transformer. This paper introduces a systematic\nmethod for infusing knowledge into different components of a transformer-based\nmodel. A modular framework is proposed to identify specific components within\nthe transformer architecture, such as the self-attention mechanism, encoder\nlayers, or the input embedding layer, where knowledge infusion can be applied.\nAdditionally, extensive experiments are conducted on the General Language\nUnderstanding Evaluation (GLUE) benchmark tasks, and the findings are reported.\nThis systematic approach aims to facilitate more principled approaches to\nincorporating knowledge into language model architectures.", "published": "2023-06-23 13:55:01", "link": "http://arxiv.org/abs/2306.13501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT may excel in States Medical Licensing Examination but falters in\n  basic Linear Algebra", "abstract": "The emergence of ChatGPT has been rapid, and although it has demonstrated\npositive impacts in certain domains, its influence is not universally\nadvantageous. Our analysis focuses on ChatGPT's capabilities in Mathematics\nEducation, particularly in teaching basic Linear Algebra. While there are\ninstances where ChatGPT delivers accurate and well-motivated answers, it is\ncrucial to recognize numerous cases where it makes significant mathematical\nerrors and fails in logical inference. These occurrences raise concerns\nregarding the system's genuine understanding of mathematics, as it appears to\nrely more on visual patterns rather than true comprehension. Additionally, the\nsuitability of ChatGPT as a teacher for students also warrants consideration.", "published": "2023-06-23 15:19:29", "link": "http://arxiv.org/abs/2306.16282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToolQA: A Dataset for LLM Question Answering with External Tools", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in\nvarious NLP tasks, but they still suffer from challenges such as hallucination\nand weak numerical reasoning. To overcome these challenges, external tools can\nbe used to enhance LLMs' question-answering abilities. However, current\nevaluation methods do not distinguish between questions that can be answered\nusing LLMs' internal knowledge and those that require external information\nthrough tool use. To address this issue, we introduce a new dataset called\nToolQA, which is designed to faithfully evaluate LLMs' ability to use external\ntools for question answering. Our development of ToolQA involved a scalable,\nautomated process for dataset curation, along with 13 specialized tools\ndesigned for interaction with external knowledge in order to answer questions.\nImportantly, we strive to minimize the overlap between our benchmark data and\nLLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use\nreasoning abilities. We conducted an in-depth diagnosis of existing tool-use\nLLMs to highlight their strengths, weaknesses, and potential improvements. Our\nfindings set a new benchmark for evaluating LLMs and suggest new directions for\nfuture advancements. Our data and code are freely available to the broader\nscientific community on GitHub.", "published": "2023-06-23 05:43:28", "link": "http://arxiv.org/abs/2306.13304v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Effective and Compact Contextual Representation for Conformer\n  Transducer Speech Recognition Systems", "abstract": "Current ASR systems are mainly trained and evaluated at the utterance level.\nLong range cross utterance context can be incorporated. A key task is to derive\na suitable compact representation of the most relevant history contexts. In\ncontrast to previous researches based on either LSTM-RNN encoded histories that\nattenuate the information from longer range contexts, or frame level\nconcatenation of transformer context embeddings, in this paper compact\nlow-dimensional cross utterance contextual features are learned in the\nConformer-Transducer Encoder using specially designed attention pooling layers\nthat are applied over efficiently cached preceding utterances history vectors.\nExperiments on the 1000-hr Gigaspeech corpus demonstrate that the proposed\ncontextualized streaming Conformer-Transducers outperform the baseline using\nutterance internal context only with statistically significant WER reductions\nof 0.7% to 0.5% absolute (4.3% to 3.1% relative) on the dev and test data.", "published": "2023-06-23 05:55:19", "link": "http://arxiv.org/abs/2306.13307v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Mutually Guided Few-shot Learning for Relational Triple Extraction", "abstract": "Knowledge graphs (KGs), containing many entity-relation-entity triples,\nprovide rich information for downstream applications. Although extracting\ntriples from unstructured texts has been widely explored, most of them require\na large number of labeled instances. The performance will drop dramatically\nwhen only few labeled data are available. To tackle this problem, we propose\nthe Mutually Guided Few-shot learning framework for Relational Triple\nExtraction (MG-FTE). Specifically, our method consists of an entity-guided\nrelation proto-decoder to classify the relations firstly and a relation-guided\nentity proto-decoder to extract entities based on the classified relations. To\ndraw the connection between entity and relation, we design a proto-level fusion\nmodule to boost the performance of both entity extraction and relation\nclassification. Moreover, a new cross-domain few-shot triple extraction task is\nintroduced. Extensive experiments show that our method outperforms many\nstate-of-the-art methods by 12.6 F1 score on FewRel 1.0 (single-domain) and\n20.5 F1 score on FewRel 2.0 (cross-domain).", "published": "2023-06-23 06:15:54", "link": "http://arxiv.org/abs/2306.13310v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Abstractive Text Summarization for Resumes With Cutting Edge NLP\n  Transformers and LSTM", "abstract": "Text summarization is a fundamental task in natural language processing that\naims to condense large amounts of textual information into concise and coherent\nsummaries. With the exponential growth of content and the need to extract key\ninformation efficiently, text summarization has gained significant attention in\nrecent years. In this study, LSTM and pre-trained T5, Pegasus, BART and\nBART-Large model performances were evaluated on the open source dataset (Xsum,\nCNN/Daily Mail, Amazon Fine Food Review and News Summary) and the prepared\nresume dataset. This resume dataset consists of many information such as\nlanguage, education, experience, personal information, skills, and this data\nincludes 75 resumes. The primary objective of this research was to classify\nresume text. Various techniques such as LSTM, pre-trained models, and\nfine-tuned models were assessed using a dataset of resumes. The BART-Large\nmodel fine-tuned with the resume dataset gave the best performance.", "published": "2023-06-23 06:33:20", "link": "http://arxiv.org/abs/2306.13315v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Incorporating Graph Information in Transformer-based AMR Parsing", "abstract": "Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that\naims at providing a semantic graph abstraction representing a given text.\nCurrent approaches are based on autoregressive language models such as BART or\nT5, fine-tuned through Teacher Forcing to obtain a linearized version of the\nAMR graph from a sentence. In this paper, we present LeakDistill, a model and\nmethod that explores a modification to the Transformer architecture, using\nstructural adapters to explicitly incorporate graph information into the\nlearned representations and improve AMR parsing performance. Our experiments\nshow how, by employing word-to-node alignment to embed graph structural\ninformation into the encoder at training time, we can obtain state-of-the-art\nAMR parsing through self-knowledge distillation, even without the use of\nadditional data. We release the code at\n\\url{http://www.github.com/sapienzanlp/LeakDistill}.", "published": "2023-06-23 12:12:08", "link": "http://arxiv.org/abs/2306.13467v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "System-Level Natural Language Feedback", "abstract": "Natural language (NL) feedback offers rich insights into user experience.\nWhile existing studies focus on an instance-level approach, where feedback is\nused to refine specific examples, we introduce a framework for system-level use\nof NL feedback. We show how to use feedback to formalize system-level design\ndecisions in a human-in-the-loop-process -- in order to produce better models.\nIn particular this is done through: (i) metric design for tasks; and (ii)\nlanguage model prompt design for refining model responses. We conduct two case\nstudies of this approach for improving search query and dialog response\ngeneration, demonstrating the effectiveness of system-level feedback. We show\nthe combination of system-level and instance-level feedback brings further\ngains, and that human written instance-level feedback results in more grounded\nrefinements than GPT-3.5 written ones, underlying the importance of human\nfeedback for building systems. We release our code and data at\nhttps://github.com/yyy-Apple/Sys-NL-Feedback.", "published": "2023-06-23 16:21:40", "link": "http://arxiv.org/abs/2306.13588v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language\n  Models", "abstract": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment\nin diverse domains, measuring language model behavior on realistic data is\nimperative. For example, a company deploying a client-facing chatbot must\nensure that the model will not respond to client requests with profanity.\nCurrent evaluations approach this problem using small, domain-specific datasets\nwith human-curated labels. These evaluation sets are often sampled from a\nnarrow and simplified distribution, and data sources can unknowingly be leaked\ninto the training set which can lead to misleading evaluations. To bypass these\ndrawbacks, we propose a framework for self-supervised evaluation of LLMs by\nanalyzing their sensitivity or invariance to transformations on the input text.\nSelf-supervised evaluation can directly monitor LLM behavior on datasets\ncollected in the wild or streamed during live model deployment. We demonstrate\nself-supervised evaluation strategies for measuring closed-book knowledge,\ntoxicity, and long-range context dependence, in addition to sensitivity to\ngrammatical structure and tokenization errors. When comparisons to similar\nhuman-labeled benchmarks are available, we find strong correlations between\nself-supervised and human-supervised evaluations. The self-supervised paradigm\ncomplements current evaluation strategies that rely on labeled data.", "published": "2023-06-23 17:59:09", "link": "http://arxiv.org/abs/2306.13651v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Resume Information Extraction via Post-OCR Text Processing", "abstract": "Information extraction (IE), one of the main tasks of natural language\nprocessing (NLP), has recently increased importance in the use of resumes. In\nstudies on the text to extract information from the CV, sentence classification\nwas generally made using NLP models. In this study, it is aimed to extract\ninformation by classifying all of the text groups after pre-processing such as\nOptical Character Recognition (OCT) and object recognition with the YOLOv8\nmodel of the resumes. The text dataset consists of 286 resumes collected for 5\ndifferent (education, experience, talent, personal and language) job\ndescriptions in the IT industry. The dataset created for object recognition\nconsists of 1198 resumes, which were collected from the open-source internet\nand labeled as sets of text. BERT, BERT-t, DistilBERT, RoBERTa and XLNet were\nused as models. F1 score variances were used to compare the model results. In\naddition, the YOLOv8 model has also been reported comparatively in itself. As a\nresult of the comparison, DistilBERT was showed better results despite having a\nlower number of parameters than other models.", "published": "2023-06-23 20:14:07", "link": "http://arxiv.org/abs/2306.13775v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "An analysis of vaccine-related sentiments from development to deployment\n  of COVID-19 vaccines", "abstract": "Anti-vaccine sentiments have been well-known and reported throughout the\nhistory of viral outbreaks and vaccination programmes. The COVID-19 pandemic\nhad fear and uncertainty about vaccines which has been well expressed on social\nmedia platforms such as Twitter. We analyse Twitter sentiments from the\nbeginning of the COVID-19 pandemic and study the public behaviour during the\nplanning, development and deployment of vaccines expressed in tweets worldwide\nusing a sentiment analysis framework via deep learning models. In this way, we\nprovide visualisation and analysis of anti-vaccine sentiments over the course\nof the COVID-19 pandemic. Our results show a link between the number of tweets,\nthe number of cases, and the change in sentiment polarity scores during major\nwaves of COVID-19 cases. We also found that the first half of the pandemic had\ndrastic changes in the sentiment polarity scores that later stabilised which\nimplies that the vaccine rollout had an impact on the nature of discussions on\nsocial media.", "published": "2023-06-23 22:10:05", "link": "http://arxiv.org/abs/2306.13797v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "The Double Helix inside the NLP Transformer", "abstract": "We introduce a framework for analyzing various types of information in an NLP\nTransformer. In this approach, we distinguish four layers of information:\npositional, syntactic, semantic, and contextual. We also argue that the common\npractice of adding positional information to semantic embedding is sub-optimal\nand propose instead a Linear-and-Add approach. Our analysis reveals an\nautogenetic separation of positional information through the deep layers. We\nshow that the distilled positional components of the embedding vectors follow\nthe path of a helix, both on the encoder side and on the decoder side. We\nadditionally show that on the encoder side, the conceptual dimensions generate\nPart-of-Speech (PoS) clusters. On the decoder side, we show that a di-gram\napproach helps to reveal the PoS clusters of the next token. Our approach paves\na way to elucidate the processing of information through the deep layers of an\nNLP Transformer.", "published": "2023-06-23 23:53:49", "link": "http://arxiv.org/abs/2306.13817v1", "categories": ["cs.AI", "cs.CL", "I.2"], "primary_category": "cs.AI"}
{"title": "Product Information Extraction using ChatGPT", "abstract": "Structured product data in the form of attribute/value pairs is the\nfoundation of many e-commerce applications such as faceted product search,\nproduct comparison, and product recommendation. Product offers often only\ncontain textual descriptions of the product attributes in the form of titles or\nfree text. Hence, extracting attribute/value pairs from textual product\ndescriptions is an essential enabler for e-commerce applications. In order to\nexcel, state-of-the-art product information extraction methods require large\nquantities of task-specific training data. The methods also struggle with\ngeneralizing to out-of-distribution attributes and attribute values that were\nnot a part of the training data. Due to being pre-trained on huge amounts of\ntext as well as due to emergent effects resulting from the model size, Large\nLanguage Models like ChatGPT have the potential to address both of these\nshortcomings. This paper explores the potential of ChatGPT for extracting\nattribute/value pairs from product descriptions. We experiment with different\nzero-shot and few-shot prompt designs. Our results show that ChatGPT achieves a\nperformance similar to a pre-trained language model but requires much smaller\namounts of training data and computation for fine-tuning.", "published": "2023-06-23 09:30:01", "link": "http://arxiv.org/abs/2306.14921v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Implementing contextual biasing in GPU decoder for online ASR", "abstract": "GPU decoding significantly accelerates the output of ASR predictions. While\nGPUs are already being used for online ASR decoding, post-processing and\nrescoring on GPUs have not been properly investigated yet. Rescoring with\navailable contextual information can considerably improve ASR predictions.\nPrevious studies have proven the viability of lattice rescoring in decoding and\nbiasing language model (LM) weights in offline and online CPU scenarios. In\nreal-time GPU decoding, partial recognition hypotheses are produced without\nlattice generation, which makes the implementation of biasing more complex. The\npaper proposes and describes an approach to integrate contextual biasing in\nreal-time GPU decoding while exploiting the standard Kaldi GPU decoder. Besides\nthe biasing of partial ASR predictions, our approach also permits dynamic\ncontext switching allowing a flexible rescoring per each speech segment\ndirectly on GPU. The code is publicly released and tested with open-sourced\ntest sets.", "published": "2023-06-23 08:59:50", "link": "http://arxiv.org/abs/2306.15685v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Master-ASR: Achieving Multilingual Scalability and Low-Resource\n  Adaptation in ASR with Modular Learning", "abstract": "Despite the impressive performance recently achieved by automatic speech\nrecognition (ASR), we observe two primary challenges that hinder its broader\napplications: (1) The difficulty of introducing scalability into the model to\nsupport more languages with limited training, inference, and storage overhead;\n(2) The low-resource adaptation ability that enables effective low-resource\nadaptation while avoiding over-fitting and catastrophic forgetting issues.\nInspired by recent findings, we hypothesize that we can address the above\nchallenges with modules widely shared across languages. To this end, we propose\nan ASR framework, dubbed \\METHODNS, that, \\textit{for the first time},\nsimultaneously achieves strong multilingual scalability and low-resource\nadaptation ability thanks to its modularize-then-assemble strategy.\nSpecifically, \\METHOD learns a small set of generalizable sub-modules and\nadaptively assembles them for different languages to reduce the multilingual\noverhead and enable effective knowledge transfer for low-resource adaptation.\nExtensive experiments and visualizations demonstrate that \\METHOD can\neffectively discover language similarity and improve multilingual and\nlow-resource ASR performance over state-of-the-art (SOTA) methods, e.g., under\nmultilingual-ASR, our framework achieves a 0.13$\\sim$2.41 lower character error\nrate (CER) with 30\\% smaller inference overhead over SOTA solutions on\nmultilingual ASR and a comparable CER, with nearly 50 times fewer trainable\nparameters over SOTA solutions on low-resource tuning, respectively.", "published": "2023-06-23 16:23:00", "link": "http://arxiv.org/abs/2306.15686v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "A Survey on Multimodal Large Language Models", "abstract": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and OCR-free math\nreasoning, are rare in traditional multimodal methods, suggesting a potential\npath to artificial general intelligence. To this end, both academia and\nindustry have endeavored to develop MLLMs that can compete with or even better\nthan GPT-4V, pushing the limit of research at a surprising speed. In this\npaper, we aim to trace and summarize the recent progress of MLLMs. First of\nall, we present the basic formulation of MLLM and delineate its related\nconcepts, including architecture, training strategy and data, as well as\nevaluation. Then, we introduce research topics about how MLLMs can be extended\nto support more granularity, modalities, languages, and scenarios. We continue\nwith multimodal hallucination and extended techniques, including Multimodal ICL\n(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To\nconclude the paper, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.", "published": "2023-06-23 15:21:52", "link": "http://arxiv.org/abs/2306.13549v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Max-Margin Token Selection in Attention Mechanism", "abstract": "Attention mechanism is a central component of the transformer architecture\nwhich led to the phenomenal success of large language models. However, the\ntheoretical principles underlying the attention mechanism are poorly\nunderstood, especially its nonconvex optimization dynamics. In this work, we\nexplore the seminal softmax-attention model $f(\\boldsymbol{X})=\\langle\n\\boldsymbol{Xv}, \\texttt{softmax}(\\boldsymbol{XWp})\\rangle$, where\n$\\boldsymbol{X}$ is the token sequence and\n$(\\boldsymbol{v},\\boldsymbol{W},\\boldsymbol{p})$ are trainable parameters. We\nprove that running gradient descent on $\\boldsymbol{p}$, or equivalently\n$\\boldsymbol{W}$, converges in direction to a max-margin solution that\nseparates $\\textit{locally-optimal}$ tokens from non-optimal ones. This clearly\nformalizes attention as an optimal token selection mechanism. Remarkably, our\nresults are applicable to general data and precisely characterize\n$\\textit{optimality}$ of tokens in terms of the value embeddings\n$\\boldsymbol{Xv}$ and problem geometry. We also provide a broader\nregularization path analysis that establishes the margin maximizing nature of\nattention even for nonlinear prediction heads. When optimizing $\\boldsymbol{v}$\nand $\\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions\nunder which the regularization paths directionally converge to their respective\nhard-margin SVM solutions where $\\boldsymbol{v}$ separates the input features\nbased on their labels. Interestingly, the SVM formulation of $\\boldsymbol{p}$\nis influenced by the support vector geometry of $\\boldsymbol{v}$. Finally, we\nverify our theoretical findings via numerical experiments and provide insights.", "published": "2023-06-23 16:35:46", "link": "http://arxiv.org/abs/2306.13596v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "primary_category": "cs.LG"}
{"title": "On-Policy Distillation of Language Models: Learning from Self-Generated\n  Mistakes", "abstract": "Knowledge distillation (KD) is widely used for compressing a teacher model to\nreduce its inference cost and memory footprint, by training a smaller student\nmodel. However, current KD methods for auto-regressive sequence models suffer\nfrom distribution mismatch between output sequences seen during training and\nthose generated by the student during inference. To address this issue, we\nintroduce Generalized Knowledge Distillation (GKD). Instead of solely relying\non a fixed set of output sequences, GKD trains the student on its\nself-generated output sequences by leveraging feedback from the teacher on such\nsequences. Unlike supervised KD approaches, GKD also offers the flexibility to\nemploy alternative loss functions between the student and teacher, which can be\nuseful when the student lacks the expressivity to mimic the teacher's\ndistribution. Furthermore, GKD facilitates the seamless integration of\ndistillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for\ndistilling auto-regressive language models on summarization, translation, and\narithmetic reasoning tasks, and task-agnostic distillation for\ninstruction-tuning.", "published": "2023-06-23 17:56:26", "link": "http://arxiv.org/abs/2306.13649v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple\n  Devices in Diverse Scenarios", "abstract": "The CHiME challenges have played a significant role in the development and\nevaluation of robust automatic speech recognition (ASR) systems. We introduce\nthe CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task\ncomprises joint ASR and diarization in far-field settings with multiple, and\npossibly heterogeneous, recording devices. Different from previous challenges,\nwe evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The\ngoal is for participants to devise a single system that can generalize across\ndifferent array geometries and use cases with no a-priori information. Another\ndeparture from earlier CHiME iterations is that participants are allowed to use\nopen-source pre-trained models and datasets. In this paper, we describe the\nchallenge design, motivation, and fundamental research questions in detail. We\nalso present the baseline system, which is fully array-topology agnostic and\nfeatures multi-channel diarization, channel selection, guided source separation\nand a robust ASR model that leverages self-supervised speech representations\n(SSLR).", "published": "2023-06-23 18:49:20", "link": "http://arxiv.org/abs/2306.13734v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deconstructing Classifiers: Towards A Data Reconstruction Attack Against\n  Text Classification Models", "abstract": "Natural language processing (NLP) models have become increasingly popular in\nreal-world applications, such as text classification. However, they are\nvulnerable to privacy attacks, including data reconstruction attacks that aim\nto extract the data used to train the model. Most previous studies on data\nreconstruction attacks have focused on LLM, while classification models were\nassumed to be more secure. In this work, we propose a new targeted data\nreconstruction attack called the Mix And Match attack, which takes advantage of\nthe fact that most classification models are based on LLM. The Mix And Match\nattack uses the base model of the target model to generate candidate tokens and\nthen prunes them using the classification head. We extensively demonstrate the\neffectiveness of the attack using both random and organic canaries. This work\nhighlights the importance of considering the privacy risks associated with data\nreconstruction attacks in classification models and offers insights into\npossible leakages.", "published": "2023-06-23 21:25:38", "link": "http://arxiv.org/abs/2306.13789v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Language Speech Emotion Recognition Using Multimodal Dual\n  Attention Transformers", "abstract": "Despite the recent progress in speech emotion recognition (SER),\nstate-of-the-art systems are unable to achieve improved performance in\ncross-language settings. In this paper, we propose a Multimodal Dual Attention\nTransformer (MDAT) model to improve cross-language SER. Our model utilises\npre-trained models for multimodal feature extraction and is equipped with a\ndual attention mechanism including graph attention and co-attention to capture\ncomplex dependencies across different modalities and achieve improved\ncross-language SER results using minimal target language data. In addition, our\nmodel also exploits a transformer encoder layer for high-level feature\nrepresentation to improve emotion classification accuracy. In this way, MDAT\nperforms refinement of feature representation at various stages and provides\nemotional salient features to the classification layer. This novel approach\nalso ensures the preservation of modality-specific emotional information while\nenhancing cross-modality and cross-language interactions. We assess our model's\nperformance on four publicly available SER datasets and establish its superior\neffectiveness compared to recent approaches and baseline models.", "published": "2023-06-23 22:38:32", "link": "http://arxiv.org/abs/2306.13804v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LLM-Assisted Content Analysis: Using Large Language Models to Support\n  Deductive Coding", "abstract": "Deductive coding is a widely used qualitative research method for determining\nthe prevalence of themes across documents. While useful, deductive coding is\noften burdensome and time consuming since it requires researchers to read,\ninterpret, and reliably categorize a large body of unstructured text documents.\nLarge language models (LLMs), like ChatGPT, are a class of quickly evolving AI\ntools that can perform a range of natural language processing and reasoning\ntasks. In this study, we explore the use of LLMs to reduce the time it takes\nfor deductive coding while retaining the flexibility of a traditional content\nanalysis. We outline the proposed approach, called LLM-assisted content\nanalysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a\npublicly available deductive coding data set. Additionally, we conduct an\nempirical benchmark using LACA on 4 publicly available data sets to assess the\nbroader question of how well GPT-3.5 performs across a range of deductive\ncoding tasks. Overall, we find that GPT-3.5 can often perform deductive coding\nat levels of agreement comparable to human coders. Additionally, we demonstrate\nthat LACA can help refine prompts for deductive coding, identify codes for\nwhich an LLM is randomly guessing, and help assess when to use LLMs vs. human\ncoders for deductive coding. We conclude with several implications for future\npractice of deductive coding and related research methods.", "published": "2023-06-23 20:57:32", "link": "http://arxiv.org/abs/2306.14924v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale", "abstract": "Large-scale generative models such as GPT and DALL-E have revolutionized the\nresearch community. These models not only generate high fidelity outputs, but\nare also generalists which can solve tasks not explicitly taught. In contrast,\nspeech generative models are still primitive in terms of scale and task\ngeneralization. In this paper, we present Voicebox, the most versatile\ntext-guided generative model for speech at scale. Voicebox is a\nnon-autoregressive flow-matching model trained to infill speech, given audio\ncontext and text, trained on over 50K hours of speech that are not filtered or\nenhanced. Similar to GPT, Voicebox can perform many different tasks through\nin-context learning, but is more flexible as it can also condition on future\ncontext. Voicebox can be used for mono or cross-lingual zero-shot\ntext-to-speech synthesis, noise removal, content editing, style conversion, and\ndiverse sample generation. In particular, Voicebox outperforms the\nstate-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs\n1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to\n20 times faster. Audio samples can be found in\n\\url{https://voicebox.metademolab.com}.", "published": "2023-06-23 16:23:24", "link": "http://arxiv.org/abs/2306.15687v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DISCO-10M: A Large-Scale Music Dataset", "abstract": "Music datasets play a crucial role in advancing research in machine learning\nfor music. However, existing music datasets suffer from limited size,\naccessibility, and lack of audio resources. To address these shortcomings, we\npresent DISCO-10M, a novel and extensive music dataset that surpasses the\nlargest previously available music dataset by an order of magnitude. To ensure\nhigh-quality data, we implement a multi-stage filtering process. This process\nincorporates similarities based on textual descriptions and audio embeddings.\nMoreover, we provide precomputed CLAP embeddings alongside DISCO-10M,\nfacilitating direct application on various downstream tasks. These embeddings\nenable efficient exploration of machine learning applications on the provided\ndata. With DISCO-10M, we aim to democratize and facilitate new research to help\nadvance the development of novel machine learning models for music.", "published": "2023-06-23 14:27:14", "link": "http://arxiv.org/abs/2306.13512v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TACOformer:Token-channel compounded Cross Attention for Multimodal\n  Emotion Recognition", "abstract": "Recently, emotion recognition based on physiological signals has emerged as a\nfield with intensive research. The utilization of multi-modal, multi-channel\nphysiological signals has significantly improved the performance of emotion\nrecognition systems, due to their complementarity. However, effectively\nintegrating emotion-related semantic information from different modalities and\ncapturing inter-modal dependencies remains a challenging issue. Many existing\nmultimodal fusion methods ignore either token-to-token or channel-to-channel\ncorrelations of multichannel signals from different modalities, which limits\nthe classification capability of the models to some extent. In this paper, we\npropose a comprehensive perspective of multimodal fusion that integrates\nchannel-level and token-level cross-modal interactions. Specifically, we\nintroduce a unified cross attention module called Token-chAnnel COmpound (TACO)\nCross Attention to perform multimodal fusion, which simultaneously models\nchannel-level and token-level dependencies between modalities. Additionally, we\npropose a 2D position encoding method to preserve information about the spatial\ndistribution of EEG signal channels, then we use two transformer encoders ahead\nof the fusion module to capture long-term temporal dependencies from the EEG\nsignal and the peripheral physiological signal, respectively.\nSubject-independent experiments on emotional dataset DEAP and Dreamer\ndemonstrate that the proposed model achieves state-of-the-art performance.", "published": "2023-06-23 16:28:12", "link": "http://arxiv.org/abs/2306.13592v2", "categories": ["cs.MM", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
