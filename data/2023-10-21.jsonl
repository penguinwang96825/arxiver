{"title": "RECAP: Towards Precise Radiology Report Generation via Dynamic Disease\n  Progression Reasoning", "abstract": "Automating radiology report generation can significantly alleviate\nradiologists' workloads. Previous research has primarily focused on realizing\nhighly concise observations while neglecting the precise attributes that\ndetermine the severity of diseases (e.g., small pleural effusion). Since\nincorrect attributes will lead to imprecise radiology reports, strengthening\nthe generation process with precise attribute modeling becomes necessary.\nAdditionally, the temporal information contained in the historical records,\nwhich is crucial in evaluating a patient's current condition (e.g., heart size\nis unchanged), has also been largely disregarded. To address these issues, we\npropose RECAP, which generates precise and accurate radiology reports via\ndynamic disease progression reasoning. Specifically, RECAP first predicts the\nobservations and progressions (i.e., spatiotemporal information) given two\nconsecutive radiographs. It then combines the historical records,\nspatiotemporal information, and radiographs for report generation, where a\ndisease progression graph and dynamic progression reasoning mechanism are\ndevised to accurately select the attributes of each observation and\nprogression. Extensive experiments on two publicly available datasets\ndemonstrate the effectiveness of our model.", "published": "2023-10-21 00:05:32", "link": "http://arxiv.org/abs/2310.13864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online\n  Posts using Large Language Models", "abstract": "Hate speech has become pervasive in today's digital age. Although there has\nbeen considerable research to detect hate speech or generate counter speech to\ncombat hateful views, these approaches still cannot completely eliminate the\npotential harmful societal consequences of hate speech -- hate speech, even\nwhen detected, can often not be taken down or is often not taken down enough;\nand hate speech unfortunately spreads quickly, often much faster than any\ngenerated counter speech.\n  This paper investigates a relatively new yet simple and effective approach of\nsuggesting a rephrasing of potential hate speech content even before the post\nis made. We show that Large Language Models (LLMs) perform well on this task,\noutperforming state-of-the-art baselines such as BART-Detox. We develop 4\ndifferent prompts based on task description, hate definition, few-shot\ndemonstrations and chain-of-thoughts for comprehensive experiments and conduct\nexperiments on open-source LLMs such as LLaMA-1, LLaMA-2 chat, Vicuna as well\nas OpenAI's GPT-3.5. We propose various evaluation metrics to measure the\nefficacy of the generated text and ensure the generated text has reduced hate\nintensity without drastically changing the semantic meaning of the original\ntext.\n  We find that LLMs with a few-shot demonstrations prompt work the best in\ngenerating acceptable hate-rephrased text with semantic meaning similar to the\noriginal text. Overall, we find that GPT-3.5 outperforms the baseline and\nopen-source models for all the different kinds of prompts. We also perform\nhuman evaluations and interestingly, find that the rephrasings generated by\nGPT-3.5 outperform even the human-generated ground-truth rephrasings in the\ndataset. We also conduct detailed ablation studies to investigate why LLMs work\nsatisfactorily on this task and conduct a failure analysis to understand the\ngaps.", "published": "2023-10-21 12:18:29", "link": "http://arxiv.org/abs/2310.13985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4", "abstract": "This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to\ndetect translation quality errors, specifically for the quality estimation\nsetting without the need for human reference translations. Based on the power\nof large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting\ntechnique, querying the GPT-4 model to mark error quality spans. Compared to\nprevious works, our method has language-agnostic prompts, thus avoiding the\nneed for manual prompt preparation for new languages.\n  While preliminary results indicate that GEMBA-MQM achieves state-of-the-art\naccuracy for system ranking, we advise caution when using it in academic works\nto demonstrate improvements over other methods due to its dependence on the\nproprietary, black-box GPT model.", "published": "2023-10-21 12:30:33", "link": "http://arxiv.org/abs/2310.13988v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emulating the Human Mind: A Neural-symbolic Link Prediction Model with\n  Fast and Slow Reasoning and Filtered Rules", "abstract": "Link prediction is an important task in addressing the incompleteness problem\nof knowledge graphs (KG). Previous link prediction models suffer from issues\nrelated to either performance or explanatory capability. Furthermore, models\nthat are capable of generating explanations, often struggle with erroneous\npaths or reasoning leading to the correct answer. To address these challenges,\nwe introduce a novel Neural-Symbolic model named FaSt-FLiP (stands for Fast and\nSlow Thinking with Filtered rules for Link Prediction task), inspired by two\ndistinct aspects of human cognition: \"commonsense reasoning\" and \"thinking,\nfast and slow.\" Our objective is to combine a logical and neural model for\nenhanced link prediction. To tackle the challenge of dealing with incorrect\npaths or rules generated by the logical model, we propose a semi-supervised\nmethod to convert rules into sentences. These sentences are then subjected to\nassessment and removal of incorrect rules using an NLI (Natural Language\nInference) model. Our approach to combining logical and neural models involves\nfirst obtaining answers from both the logical and neural models. These answers\nare subsequently unified using an Inference Engine module, which has been\nrealized through both algorithmic implementation and a novel neural model\narchitecture. To validate the efficacy of our model, we conducted a series of\nexperiments. The results demonstrate the superior performance of our model in\nboth link prediction metrics and the generation of more reliable explanations.", "published": "2023-10-21 12:45:11", "link": "http://arxiv.org/abs/2310.13996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transductive Learning for Textual Few-Shot Classification in API-based\n  Embedding Models", "abstract": "Proprietary and closed APIs are becoming increasingly common to process\nnatural language, and are impacting the practical applications of natural\nlanguage processing, including few-shot classification. Few-shot classification\ninvolves training a model to perform a new classification task with a handful\nof labeled data. This paper presents three contributions. First, we introduce a\nscenario where the embedding of a pre-trained model is served through a gated\nAPI with compute-cost and data-privacy constraints. Second, we propose a\ntransductive inference, a learning paradigm that has been overlooked by the NLP\ncommunity. Transductive inference, unlike traditional inductive learning,\nleverages the statistics of unlabeled data. We also introduce a new\nparameter-free transductive regularizer based on the Fisher-Rao loss, which can\nbe used on top of the gated API embeddings. This method fully utilizes\nunlabeled data, does not share any label with the third-party API provider and\ncould serve as a baseline for future research. Third, we propose an improved\nexperimental setting and compile a benchmark of eight datasets involving\nmulticlass classification in four different languages, with up to 151 classes.\nWe evaluate our methods using eight backbone models, along with an episodic\nevaluation over 1,000 episodes, which demonstrate the superiority of\ntransductive inference over the standard inductive setting.", "published": "2023-10-21 12:47:10", "link": "http://arxiv.org/abs/2310.13998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Stronger Textual Attack Detectors", "abstract": "The landscape of available textual adversarial attacks keeps growing, posing\nsevere threats and raising concerns regarding the deep NLP system's integrity.\nHowever, the crucial problem of defending against malicious attacks has only\ndrawn the attention of the NLP community. The latter is nonetheless\ninstrumental in developing robust and trustworthy systems. This paper makes two\nimportant contributions in this line of search: (i) we introduce LAROUSSE, a\nnew framework to detect textual adversarial attacks and (ii) we introduce\nSTAKEOUT, a new benchmark composed of nine popular attack methods, three\ndatasets, and two pre-trained models. LAROUSSE is ready-to-use in production as\nit is unsupervised, hyperparameter-free, and non-differentiable, protecting it\nagainst gradient-based methods. Our new benchmark STAKEOUT allows for a robust\nevaluation framework: we conduct extensive numerical experiments which\ndemonstrate that LAROUSSE outperforms previous methods, and which allows to\nidentify interesting factors of detection rate variations.", "published": "2023-10-21 13:01:29", "link": "http://arxiv.org/abs/2310.14001v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models and Multimodal Retrieval for Visual Word Sense\n  Disambiguation", "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the\ngoal of retrieving an image among a set of candidates, which better represents\nthe meaning of an ambiguous word within a given context. In this paper, we make\na substantial step towards unveiling this interesting task by applying a\nvarying set of approaches. Since VWSD is primarily a text-image retrieval task,\nwe explore the latest transformer-based methods for multimodal retrieval.\nAdditionally, we utilize Large Language Models (LLMs) as knowledge bases to\nenhance the given phrases and resolve ambiguity related to the target word. We\nalso study VWSD as a unimodal problem by converting to text-to-text and\nimage-to-image retrieval, as well as question-answering (QA), to fully explore\nthe capabilities of relevant models. To tap into the implicit knowledge of\nLLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable\nanswer generation. On top of all, we train a learn to rank (LTR) model in order\nto combine our different modules, achieving competitive ranking results.\nExtensive experiments on VWSD demonstrate valuable insights to effectively\ndrive future directions.", "published": "2023-10-21 14:35:42", "link": "http://arxiv.org/abs/2310.14025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GASCOM: Graph-based Attentive Semantic Context Modeling for Online\n  Conversation Understanding", "abstract": "Online conversation understanding is an important yet challenging NLP problem\nwhich has many useful applications (e.g., hate speech detection). However,\nonline conversations typically unfold over a series of posts and replies to\nthose posts, forming a tree structure within which individual posts may refer\nto semantic context from higher up the tree. Such semantic cross-referencing\nmakes it difficult to understand a single post by itself; yet considering the\nentire conversation tree is not only difficult to scale but can also be\nmisleading as a single conversation may have several distinct threads or\npoints, not all of which are relevant to the post being considered. In this\npaper, we propose a Graph-based Attentive Semantic COntext Modeling (GASCOM)\nframework for online conversation understanding. Specifically, we design two\nnovel algorithms that utilise both the graph structure of the online\nconversation as well as the semantic information from individual posts for\nretrieving relevant context nodes from the whole conversation. We further\ndesign a token-level multi-head graph attention mechanism to pay different\nattentions to different tokens from different selected context utterances for\nfine-grained conversation context modeling. Using this semantic conversational\ncontext, we re-examine two well-studied problems: polarity prediction and hate\nspeech detection. Our proposed framework significantly outperforms\nstate-of-the-art methods on both tasks, improving macro-F1 scores by 4.5% for\npolarity prediction and by 5% for hate speech detection. The GASCOM context\nweights also enhance interpretability.", "published": "2023-10-21 14:45:26", "link": "http://arxiv.org/abs/2310.14028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic\n  Study", "abstract": "This paper analyses two hitherto unstudied sites sharing state-backed\ndisinformation, Reliable Recent News (rrn.world) and WarOnFakes\n(waronfakes.com), which publish content in Arabic, Chinese, English, French,\nGerman, and Spanish. We describe our content acquisition methodology and\nperform cross-site unsupervised topic clustering on the resulting multilingual\ndataset. We also perform linguistic and temporal analysis of the web page\ntranslations and topics over time, and investigate articles with false\npublication dates. We make publicly available this new dataset of 14,053\narticles, annotated with each language version, and additional metadata such as\nlinks and images. The main contribution of this paper for the NLP community is\nin the novel dataset which enables studies of disinformation networks, and the\ntraining of NLP tools for disinformation detection.", "published": "2023-10-21 15:00:27", "link": "http://arxiv.org/abs/2310.14032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MeaeQ: Mount Model Extraction Attacks with Efficient Queries", "abstract": "We study model extraction attacks in natural language processing (NLP) where\nattackers aim to steal victim models by repeatedly querying the open\nApplication Programming Interfaces (APIs). Recent works focus on limited-query\nbudget settings and adopt random sampling or active learning-based sampling\nstrategies on publicly available, unannotated data sources. However, these\nmethods often result in selected queries that lack task relevance and data\ndiversity, leading to limited success in achieving satisfactory results with\nlow query costs. In this paper, we propose MeaeQ (Model extraction attack with\nefficient Queries), a straightforward yet effective method to address these\nissues. Specifically, we initially utilize a zero-shot sequence inference\nclassifier, combined with API service information, to filter task-relevant data\nfrom a public text corpus instead of a problem domain-specific dataset.\nFurthermore, we employ a clustering-based data reduction technique to obtain\nrepresentative data as queries for the attack. Extensive experiments conducted\non four benchmark datasets demonstrate that MeaeQ achieves higher functional\nsimilarity to the victim model than baselines while requiring fewer queries.\nOur code is available at https://github.com/C-W-D/MeaeQ.", "published": "2023-10-21 16:07:16", "link": "http://arxiv.org/abs/2310.14047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code-Switching with Word Senses for Pretraining in Neural Machine\n  Translation", "abstract": "Lexical ambiguity is a significant and pervasive challenge in Neural Machine\nTranslation (NMT), with many state-of-the-art (SOTA) NMT systems struggling to\nhandle polysemous words (Campolungo et al., 2022). The same holds for the NMT\npretraining paradigm of denoising synthetic \"code-switched\" text (Pan et al.,\n2021; Iyer et al., 2023), where word senses are ignored in the noising stage --\nleading to harmful sense biases in the pretraining data that are subsequently\ninherited by the resulting models. In this work, we introduce Word Sense\nPretraining for Neural Machine Translation (WSP-NMT) - an end-to-end approach\nfor pretraining multilingual NMT models leveraging word sense-specific\ninformation from Knowledge Bases. Our experiments show significant improvements\nin overall translation quality. Then, we show the robustness of our approach to\nscale to various challenging data and resource-scarce scenarios and, finally,\nreport fine-grained accuracy improvements on the DiBiMT disambiguation\nbenchmark. Our studies yield interesting and novel insights into the merits and\nchallenges of integrating word sense information and structured knowledge in\nmultilingual pretraining for NMT.", "published": "2023-10-21 16:13:01", "link": "http://arxiv.org/abs/2310.14050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark\n  for Language Model Evaluation", "abstract": "Curated datasets for healthcare are often limited due to the need of human\nannotations from experts. In this paper, we present MedEval, a multi-level,\nmulti-task, and multi-domain medical benchmark to facilitate the development of\nlanguage models for healthcare. MedEval is comprehensive and consists of data\nfrom several healthcare systems and spans 35 human body regions from 8\nexamination modalities. With 22,779 collected sentences and 21,228 reports, we\nprovide expert annotations at multiple levels, offering a granular potential\nusage of the data and supporting a wide range of tasks. Moreover, we\nsystematically evaluated 10 generic and domain-specific language models under\nzero-shot and finetuning settings, from domain-adapted baselines in healthcare\nto general-purposed state-of-the-art large language models (e.g., ChatGPT). Our\nevaluations reveal varying effectiveness of the two categories of language\nmodels across different tasks, from which we notice the importance of\ninstruction tuning for few-shot usage of large language models. Our\ninvestigation paves the way toward benchmarking language models for healthcare\nand provides valuable insights into the strengths and limitations of adopting\nlarge language models in medical domains, informing their practical\napplications and future advancements.", "published": "2023-10-21 18:59:41", "link": "http://arxiv.org/abs/2310.14088v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Knowledge Graphs for Orphan Entity Allocation in Resume\n  Processing", "abstract": "Significant challenges are posed in talent acquisition and recruitment by\nprocessing and analyzing unstructured data, particularly resumes. This research\npresents a novel approach for orphan entity allocation in resume processing\nusing knowledge graphs. Techniques of association mining, concept extraction,\nexternal knowledge linking, named entity recognition, and knowledge graph\nconstruction are integrated into our pipeline. By leveraging these techniques,\nthe aim is to automate and enhance the efficiency of the job screening process\nby successfully bucketing orphan entities within resumes. This allows for more\neffective matching between candidates and job positions, streamlining the\nresume screening process, and enhancing the accuracy of candidate-job matching.\nThe approach's exceptional effectiveness and resilience are highlighted through\nextensive experimentation and evaluation, ensuring that alternative measures\ncan be relied upon for seamless processing and orphan entity allocation in case\nof any component failure. The capabilities of knowledge graphs in generating\nvaluable insights through intelligent information extraction and\nrepresentation, specifically in the domain of categorizing orphan entities, are\nhighlighted by the results of our research.", "published": "2023-10-21 19:10:30", "link": "http://arxiv.org/abs/2310.14093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finite-context Indexing of Restricted Output Space for NLP Models Facing\n  Noisy Input", "abstract": "NLP models excel on tasks with clean inputs, but are less accurate with noisy\ninputs. In particular, character-level noise such as human-written typos and\nadversarially-engineered realistic-looking misspellings often appears in text\nand can easily trip up NLP models. Prior solutions to address character-level\nnoise often alter the content of the inputs (low fidelity), thus inadvertently\nlowering model accuracy on clean inputs. We proposed FiRo, an approach to boost\nNLP model performance on noisy inputs without sacrificing performance on clean\ninputs. FiRo sanitizes the input text while preserving its fidelity by\ninferring the noise-free form for each token in the input. FiRo uses\nfinite-context aggregation to obtain contextual embeddings which is then used\nto find the noise-free form within a restricted output space. The output space\nis restricted to a small cluster of probable candidates in order to predict the\nnoise-free tokens more accurately. Although the clusters are small, FiRo's\neffective vocabulary (union of all clusters) can be scaled up to better\npreserve the input content. Experimental results show NLP models that use FiRo\noutperforming baselines on six classification tasks and one sequence labeling\ntask at various degrees of noise.", "published": "2023-10-21 20:28:26", "link": "http://arxiv.org/abs/2310.14110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural generalization in COGS: Supertagging is (almost) all you need", "abstract": "In many Natural Language Processing applications, neural networks have been\nfound to fail to generalize on out-of-distribution examples. In particular,\nseveral recent semantic parsing datasets have put forward important limitations\nof neural networks in cases where compositional generalization is required. In\nthis work, we extend a neural graph-based semantic parsing framework in several\nways to alleviate this issue. Notably, we propose: (1) the introduction of a\nsupertagging step with valency constraints, expressed as an integer linear\nprogram; (2) a reduction of the graph prediction problem to the maximum\nmatching problem; (3) the design of an incremental early-stopping training\nstrategy to prevent overfitting. Experimentally, our approach significantly\nimproves results on examples that require structural generalization in the COGS\ndataset, a known challenging benchmark for compositional generalization.\nOverall, our results confirm that structural constraints are important for\ngeneralization in semantic parsing.", "published": "2023-10-21 21:51:25", "link": "http://arxiv.org/abs/2310.14124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RTSUM: Relation Triple-based Interpretable Summarization with\n  Multi-level Salience Visualization", "abstract": "In this paper, we present RTSUM, an unsupervised summarization framework that\nutilizes relation triples as the basic unit for summarization. Given an input\ndocument, RTSUM first selects salient relation triples via multi-level salience\nscoring and then generates a concise summary from the selected relation triples\nby using a text-to-text language model. On the basis of RTSUM, we also develop\na web demo for an interpretable summarizing tool, providing fine-grained\ninterpretations with the output summary. With support for customization\noptions, our tool visualizes the salience for textual units at three distinct\nlevels: sentences, relation triples, and phrases. The codes,are publicly\navailable.", "published": "2023-10-21 02:46:03", "link": "http://arxiv.org/abs/2310.13895v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research", "abstract": "With language technology increasingly affecting individuals' lives, many\nrecent works have investigated the ethical aspects of NLP. Among other topics,\nresearchers focused on the notion of morality, investigating, for example,\nwhich moral judgements language models make. However, there has been little to\nno discussion of the terminology and the theories underpinning those efforts\nand their implications. This lack is highly problematic, as it hides the works'\nunderlying assumptions and hinders a thorough and targeted scientific debate of\nmorality in NLP. In this work, we address this research gap by (a) providing an\noverview of some important ethical concepts stemming from philosophy and (b)\nsystematically surveying the existing literature on moral NLP w.r.t. their\nphilosophical foundation, terminology, and data basis. For instance, we analyse\nwhat ethical theory an approach is based on, how this decision is justified,\nand what implications it entails. Our findings surveying 92 papers show that,\nfor instance, most papers neither provide a clear definition of the terms they\nuse nor adhere to definitions from philosophy. Finally, (c) we give three\nrecommendations for future research in the field. We hope our work will lead to\na more informed, careful, and sound discussion of morality in language\ntechnology.", "published": "2023-10-21 06:04:10", "link": "http://arxiv.org/abs/2310.13915v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Linguistically Motivated Sign Language Segmentation", "abstract": "Sign language segmentation is a crucial task in sign language processing\nsystems. It enables downstream tasks such as sign recognition, transcription,\nand machine translation. In this work, we consider two kinds of segmentation:\nsegmentation into individual signs and segmentation into phrases, larger units\ncomprising several signs. We propose a novel approach to jointly model these\ntwo tasks.\n  Our method is motivated by linguistic cues observed in sign language corpora.\nWe replace the predominant IO tagging scheme with BIO tagging to account for\ncontinuous signing. Given that prosody plays a significant role in phrase\nboundaries, we explore the use of optical flow features. We also provide an\nextensive analysis of hand shapes and 3D hand normalization.\n  We find that introducing BIO tagging is necessary to model sign boundaries.\nExplicitly encoding prosody by optical flow improves segmentation in shallow\nmodels, but its contribution is negligible in deeper models. Careful tuning of\nthe decoding algorithm atop the models further improves the segmentation\nquality.\n  We demonstrate that our final models generalize to out-of-domain video\ncontent in a different signed language, even under a zero-shot setting. We\nobserve that including optical flow and 3D hand normalization enhances the\nrobustness of the model in this context.", "published": "2023-10-21 10:09:34", "link": "http://arxiv.org/abs/2310.13960v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Ensemble-Instruct: Generating Instruction-Tuning Data with a\n  Heterogeneous Mixture of LMs", "abstract": "Using in-context learning (ICL) for data generation, techniques such as\nSelf-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023)\ncan train strong conversational agents with only a small amount of human\nsupervision. One limitation of these approaches is that they resort to very\nlarge language models (around 175B parameters) that are also proprietary and\nnon-public. Here we explore the application of such techniques to language\nmodels that are much smaller (around 10B--40B parameters) and have permissive\nlicenses. We find the Self-Instruct approach to be less effective at these\nsizes and propose new ICL methods that draw on two main ideas: (a)\nCategorization and simplification of the ICL templates to make prompt learning\neasier for the LM, and (b) Ensembling over multiple LM outputs to help select\nhigh-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct\nseed tasks and employs separate pipelines for instructions that require an\ninput and instructions that do not. Empirical investigations with different LMs\nshow that: (1) Our proposed method yields higher-quality instruction tuning\ndata than Self-Instruct, (2) It improves performances of both vanilla and\ninstruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned\nLMs generate more useful outputs than their larger un-tuned counterparts. Our\ncodebase is available at https://github.com/IBM/ensemble-instruct.", "published": "2023-10-21 10:21:17", "link": "http://arxiv.org/abs/2310.13961v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Novel Information-Theoretic Objective to Disentangle Representations\n  for Fair Classification", "abstract": "One of the pursued objectives of deep learning is to provide tools that learn\nabstract representations of reality from the observation of multiple contextual\nsituations. More precisely, one wishes to extract disentangled representations\nwhich are (i) low dimensional and (ii) whose components are independent and\ncorrespond to concepts capturing the essence of the objects under consideration\n(Locatello et al., 2019b). One step towards this ambitious project consists in\nlearning disentangled representations with respect to a predefined (sensitive)\nattribute, e.g., the gender or age of the writer. Perhaps one of the main\napplication for such disentangled representations is fair classification.\nExisting methods extract the last layer of a neural network trained with a loss\nthat is composed of a cross-entropy objective and a disentanglement\nregularizer. In this work, we adopt an information-theoretic view of this\nproblem which motivates a novel family of regularizers that minimizes the\nmutual information between the latent representation and the sensitive\nattribute conditional to the target. The resulting set of losses, called\nCLINIC, is parameter free and thus, it is easier and faster to train. CLINIC\nlosses are studied through extensive numerical experiments by training over 2k\nneural networks. We demonstrate that our methods offer a better\ndisentanglement/accuracy trade-off than previous techniques, and generalize\nbetter than training with cross-entropy loss solely provided that the\ndisentanglement task is not too constraining.", "published": "2023-10-21 12:35:48", "link": "http://arxiv.org/abs/2310.13990v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline\n  Solids From Their Text Descriptions", "abstract": "The prediction of crystal properties plays a crucial role in the crystal\ndesign process. Current methods for predicting crystal properties focus on\nmodeling crystal structures using graph neural networks (GNNs). Although GNNs\nare powerful, accurately modeling the complex interactions between atoms and\nmolecules within a crystal remains a challenge. Surprisingly, predicting\ncrystal properties from crystal text descriptions is understudied, despite the\nrich information and expressiveness that text data offer. One of the main\nreasons is the lack of publicly available data for this task. In this paper, we\ndevelop and make public a benchmark dataset (called TextEdge) that contains\ntext descriptions of crystal structures with their properties. We then propose\nLLM-Prop, a method that leverages the general-purpose learning capabilities of\nlarge language models (LLMs) to predict the physical and electronic properties\nof crystals from their text descriptions. LLM-Prop outperforms the current\nstate-of-the-art GNN-based crystal property predictor by about 4% in predicting\nband gap, 3% in classifying whether the band gap is direct or indirect, and 66%\nin predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT,\na domain-specific pre-trained BERT model, despite having 3 times fewer\nparameters. Our empirical results may highlight the current inability of GNNs\nto capture information pertaining to space group symmetry and Wyckoff sites for\naccurate crystal property prediction.", "published": "2023-10-21 14:49:58", "link": "http://arxiv.org/abs/2310.14029v1", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "primary_category": "cs.CL"}
{"title": "Tree Prompting: Efficient Task Adaptation without Fine-Tuning", "abstract": "Prompting language models (LMs) is the main interface for applying them to\nnew tasks. However, for smaller LMs, prompting provides low accuracy compared\nto gradient-based finetuning. Tree Prompting is an approach to prompting which\nbuilds a decision tree of prompts, linking multiple LM calls together to solve\na task. At inference time, each call to the LM is determined by efficiently\nrouting the outcome of the previous call using the tree. Experiments on\nclassification datasets show that Tree Prompting improves accuracy over\ncompeting methods and is competitive with fine-tuning. We also show that\nvariants of Tree Prompting allow inspection of a model's decision-making\nprocess.", "published": "2023-10-21 15:18:22", "link": "http://arxiv.org/abs/2310.14034v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Transferability of Visually Grounded PCFGs", "abstract": "There has been a significant surge of interest in visually grounded grammar\ninduction in recent times. While a variety of models have been developed for\nthe task and have demonstrated impressive performance, they have not been\nevaluated on text domains that are different from the training domain, so it is\nunclear if the improvements brought by visual groundings are transferable. Our\nstudy aims to fill this gap and assess the degree of transferability. We start\nby extending VC-PCFG (short for Visually-grounded Compound\nPCFG~\\citep{zhao-titov-2020-visually}) in such a way that it can transfer\nacross text domains. We consider a zero-shot transfer learning setting where a\nmodel is trained on the source domain and is directly applied to target\ndomains, without any further training. Our experimental results suggest that:\nthe benefits from using visual groundings transfer to text in a domain similar\nto the training domain but fail to transfer to remote domains. Further, we\nconduct data and result analysis; we find that the lexicon overlap between the\nsource domain and the target domain is the most important factor in the\ntransferability of VC-PCFG.", "published": "2023-10-21 20:19:51", "link": "http://arxiv.org/abs/2310.14107v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis Across Multiple African Languages: A Current\n  Benchmark", "abstract": "Sentiment analysis is a fundamental and valuable task in NLP. However, due to\nlimitations in data and technological availability, research into sentiment\nanalysis of African languages has been fragmented and lacking. With the recent\nrelease of the AfriSenti-SemEval Shared Task 12, hosted as a part of The 17th\nInternational Workshop on Semantic Evaluation, an annotated sentiment analysis\nof 14 African languages was made available. We benchmarked and compared current\nstate-of-art transformer models across 12 languages and compared the\nperformance of training one-model-per-language versus\nsingle-model-all-languages. We also evaluated the performance of standard\nmultilingual models and their ability to learn and transfer cross-lingual\nrepresentation from non-African to African languages. Our results show that\ndespite work in low resource modeling, more data still produces better models\non a per-language basis. Models explicitly developed for African languages\noutperform other models on all tasks. Additionally, no one-model-fits-all\nsolution exists for a per-language evaluation of the models evaluated.\nMoreover, for some languages with a smaller sample size, a larger multilingual\nmodel may perform better than a dedicated per-language model for sentiment\nclassification.", "published": "2023-10-21 21:38:06", "link": "http://arxiv.org/abs/2310.14120v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ask To The Point: Open-Domain Entity-Centric Question Generation", "abstract": "We introduce a new task called *entity-centric question generation* (ECQG),\nmotivated by real-world applications such as topic-specific learning, assisted\nreading, and fact-checking. The task aims to generate questions from an entity\nperspective. To solve ECQG, we propose a coherent PLM-based framework GenCONE\nwith two novel modules: content focusing and question verification. The content\nfocusing module first identifies a focus as \"what to ask\" to form draft\nquestions, and the question verification module refines the questions\nafterwards by verifying the answerability. We also construct a large-scale\nopen-domain dataset from SQuAD to support this task. Our extensive experiments\ndemonstrate that GenCONE significantly and consistently outperforms various\nbaselines, and two modules are effective and complementary in generating\nhigh-quality questions.", "published": "2023-10-21 22:19:19", "link": "http://arxiv.org/abs/2310.14126v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AITA Generating Moral Judgements of the Crowd with Reasoning", "abstract": "Morality is a fundamental aspect of human behavior and ethics, influencing\nhow we interact with each other and the world around us. When faced with a\nmoral dilemma, a person's ability to make clear moral judgments can be clouded.\nDue to many factors such as personal biases, emotions and situational factors\npeople can find it difficult to decide their best course of action. The\nAmITheAsshole (AITA) subreddit is a forum on the social media platform Reddit\nthat helps people get clarity and objectivity on their predicaments. In the\nforum people post anecdotes about moral dilemmas they are facing in their\nlives, seeking validation for their actions or advice on how to navigate the\nsituation from the community. The morality of the actions in each post is\nclassified based on the collective opinion of the community into mainly two\nlabels, \"Not The Asshole\" (NTA) and \"You Are The Asshole\" (YTA). This project\naims to generate comments with moral reasoning for stories with moral dilemmas\nusing the AITA subreddit as a dataset. While past literature has explored the\nclassification of posts into labels (Alhassan et al., 2022), the generation of\ncomments remains a novel and challenging task. It involves understanding the\ncomplex social and ethical considerations in each situation. To address this\nchallenge, we will leverage the vast amount of data on the forum with the goal\nof generating coherent comments that align with the norms and values of the\nAITA community. In this endeavor, we aim to evaluate state-of-the-art seq2seq\ntext generation models for their ability to make moral judgments similarly to\nhumans, ultimately producing concise comments providing clear moral stances and\nadvice for the poster.", "published": "2023-10-21 10:27:22", "link": "http://arxiv.org/abs/2310.18336v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Small Language Models Fine-tuned to Coordinate Larger Language Models\n  improve Complex Reasoning", "abstract": "Large Language Models (LLMs) prompted to generate chain-of-thought (CoT)\nexhibit impressive reasoning capabilities. Recent attempts at prompt\ndecomposition toward solving complex, multi-step reasoning problems depend on\nthe ability of the LLM to simultaneously decompose and solve the problem. A\nsignificant disadvantage is that foundational LLMs are typically not available\nfor fine-tuning, making adaptation computationally prohibitive. We believe (and\ndemonstrate) that problem decomposition and solution generation are distinct\ncapabilites, better addressed in separate modules, than by one monolithic LLM.\nWe introduce DaSLaM, which uses a decomposition generator to decompose complex\nproblems into subproblems that require fewer reasoning steps. These subproblems\nare answered by a solver. We use a relatively small (13B parameters) LM as the\ndecomposition generator, which we train using policy gradient optimization to\ninteract with a solver LM (regarded as black-box) and guide it through\nsubproblems, thereby rendering our method solver-agnostic. Evaluation on\nmultiple different reasoning datasets reveal that with our method, a 175\nbillion parameter LM (text-davinci-003) can produce competitive or even better\nperformance, compared to its orders-of-magnitude larger successor, GPT-4.\nAdditionally, we show that DaSLaM is not limited by the solver's capabilities\nas a function of scale; e.g., solver LMs with diverse sizes give significant\nperformance improvement with our solver-agnostic decomposition technique.\nExhaustive ablation studies evince the superiority of our modular finetuning\ntechnique over exorbitantly large decomposer LLMs, based on prompting alone.", "published": "2023-10-21 15:23:20", "link": "http://arxiv.org/abs/2310.18338v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task\n  Medical Applications", "abstract": "The recent surge in Large Language Models (LLMs) has garnered significant\nattention across numerous fields. Fine-tuning is often required to fit general\nLLMs for a specific domain, like the web-based healthcare system. However, two\nproblems arise during fine-tuning LLMs for medical applications. One is the\ntask variety problem, which involves distinct tasks in real-world medical\nscenarios. The variety often leads to sub-optimal fine-tuning for data\nimbalance and seesaw problems. Besides, the large amount of parameters in LLMs\nleads to huge time and computation consumption by fine-tuning. To address these\ntwo problems, we propose a novel parameter efficient fine-tuning framework for\nmulti-task medical applications, dubbed as MOELoRA. The designed framework aims\nto absorb both the benefits of mixture-of-expert (MOE) for multi-task learning\nand low-rank adaptation (LoRA) for parameter efficient fine-tuning. For\nunifying MOE and LoRA, we devise multiple experts as the trainable parameters,\nwhere each expert consists of a pair of low-rank matrices to retain the small\nsize of trainable parameters. Then, a task-motivated gate function for all\nMOELoRA layers is proposed, which can control the contributions of each expert\nand produce distinct parameters for various tasks. We conduct experiments on a\nmulti-task medical dataset, indicating MOELoRA outperforms the existing\nparameter efficient fine-tuning methods. The code is available online.", "published": "2023-10-21 17:18:09", "link": "http://arxiv.org/abs/2310.18339v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Pronunciation Assessment -- A Review", "abstract": "Pronunciation assessment and its application in computer-aided pronunciation\ntraining (CAPT) have seen impressive progress in recent years. With the rapid\ngrowth in language processing and deep learning over the past few years, there\nis a need for an updated review. In this paper, we review methods employed in\npronunciation assessment for both phonemic and prosodic. We categorize the main\nchallenges observed in prominent research trends, and highlight existing\nlimitations, and available resources. This is followed by a discussion of the\nremaining challenges and possible directions for future work.", "published": "2023-10-21 11:26:24", "link": "http://arxiv.org/abs/2310.13974v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On Bilingual Lexicon Induction with Large Language Models", "abstract": "Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that\nstill, to a large extent, relies on calculating cross-lingual word\nrepresentations. Inspired by the global paradigm shift in NLP towards Large\nLanguage Models (LLMs), we examine the potential of the latest generation of\nLLMs for the development of bilingual lexicons. We ask the following research\nquestion: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for\nBLI, and how does this approach compare against and complement current BLI\napproaches? To this end, we systematically study 1) zero-shot prompting for\nunsupervised BLI and 2) few-shot in-context prompting with a set of seed\ntranslation pairs, both without any LLM fine-tuning, as well as 3) standard\nBLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source\ntext-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two\nstandard BLI benchmarks covering a range of typologically diverse languages.\nOur work is the first to demonstrate strong BLI capabilities of text-to-text\nmLLMs. The results reveal that few-shot prompting with in-context examples from\nnearest neighbours achieves the best performance, establishing new\nstate-of-the-art BLI scores for many language pairs. We also conduct a series\nof in-depth analyses and ablation studies, providing more insights on BLI with\n(m)LLMs, also along with their limitations.", "published": "2023-10-21 12:43:27", "link": "http://arxiv.org/abs/2310.13995v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Accuracy: Evaluating Self-Consistency of Code Large Language\n  Models with IdentityChain", "abstract": "Code Large Language Models (Code LLMs) are being increasingly employed in\nreal-life applications, so evaluating them is critical. While the conventional\naccuracy evaluates the performance of Code LLMs on a set of individual tasks,\ntheir self-consistency across different tasks is overlooked. Intuitively, a\ntrustworthy model should be self-consistent when generating natural language\nspecifications for its own code and generating code for its own specifications.\nFailure to preserve self-consistency reveals a lack of understanding of the\nshared semantics underlying natural language and programming language, and\ntherefore undermines the trustworthiness of a model. In this paper, we first\nformally define the self-consistency of Code LLMs and then design a framework,\nIdentityChain, which effectively and efficiently evaluates the self-consistency\nand conventional accuracy of a model at the same time. We study eleven Code\nLLMs and show that they fail to preserve self-consistency, which is indeed a\ndistinct aspect from conventional accuracy. Furthermore, we show that\nIdentityChain can be used as a model debugging tool to expose weaknesses of\nCode LLMs by demonstrating three major weaknesses that we identify in current\nmodels using IdentityChain. Our code is available at\nhttps://github.com/marcusm117/IdentityChain.", "published": "2023-10-21 16:14:56", "link": "http://arxiv.org/abs/2310.14053v3", "categories": ["cs.LG", "cs.CL", "cs.SE", "68", "I.2; D.2"], "primary_category": "cs.LG"}
{"title": "Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial\n  Applications", "abstract": "Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the\nzero-shot capabilities of Large Language Models (LLMs), but in doing so induces\nnew evaluation metric requirements. We show LLM-based metrics to be well\nadapted to these requirements, and leverage them to conduct an investigation of\ntask-specialization strategies, quantifying the trade-offs that emerge in\npractical industrial settings. Our findings offer practitioners actionable\ninsights for real-world IFT model deployment.", "published": "2023-10-21 20:04:55", "link": "http://arxiv.org/abs/2310.14103v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SwG-former: A Sliding-Window Graph Convolutional Network for\n  Simultaneous Spatial-Temporal Information Extraction in Sound Event\n  Localization and Detection", "abstract": "Sound event localization and detection (SELD) involves sound event detection\n(SED) and direction of arrival (DoA) estimation tasks. SED mainly relies on\ntemporal dependencies to distinguish different sound classes, while DoA\nestimation depends on spatial correlations to estimate source directions. This\npaper addresses the need to simultaneously extract spatial-temporal information\nin audio signals to improve SELD performance. A novel block, the sliding-window\ngraph-former (SwG-former), is designed to learn temporal context information of\nsound events based on their spatial correlations. The SwG-former block\ntransforms audio signals into a graph representation and constructs graph\nvertices to capture higher abstraction levels for spatial correlations. It uses\ndifferent-sized sliding windows to adapt various sound event durations and\naggregates temporal features with similar spatial information while\nincorporating multi-head self-attention (MHSA) to model global information.\nFurthermore, as the cornerstone of message passing, a robust Conv2dAgg function\nis proposed and embedded into the block to aggregate the features of neighbor\nvertices. As a result, a SwG-former model, which stacks the SwG-former blocks,\ndemonstrates superior performance compared to recent advanced SELD models. The\nSwG-former block is also integrated into the event-independent network version\n2 (EINV2), called SwG-EINV2, which surpasses the state-of-the-art (SOTA)\nmethods under the same acoustic environment.", "published": "2023-10-21 13:56:23", "link": "http://arxiv.org/abs/2310.14016v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Temporal convolutional neural networks to generate a head-related\n  impulse response from one direction to another", "abstract": "Virtual sound synthesis is a technology that allows users to perceive spatial\nsound through headphones or earphones. However, accurate virtual sound requires\nan individual head-related transfer function (HRTF), which can be difficult to\nmeasure due to the need for a specialized environment. In this study, we\nproposed a method to generate HRTFs from one direction to the other. To this\nend, we used temporal convolutional neural networks (TCNs) to generate\nhead-related impulse responses (HRIRs). To train the TCNs, publicly available\ndatasets in the horizontal plane were used. Using the trained networks, we\nsuccessfully generated HRIRs for directions other than the front direction in\nthe dataset. We found that the proposed method successfully generated HRIRs for\npublicly available datasets. To test the generalization of the method, we\nmeasured the HRIRs of a new dataset and tested whether the trained networks\ncould be used for this new dataset. Although the similarity evaluated by\nspectral distortion was slightly degraded, behavioral experiments with human\nparticipants showed that the generated HRIRs were equivalent to the measured\nones. These results suggest that the proposed TCNs can be used to generate\npersonalized HRIRs from one direction to another, which could contribute to the\npersonalization of virtual sound.", "published": "2023-10-21 14:02:23", "link": "http://arxiv.org/abs/2310.14018v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fast Diffusion GAN Model for Symbolic Music Generation Controlled by\n  Emotions", "abstract": "Diffusion models have shown promising results for a wide range of generative\ntasks with continuous data, such as image and audio synthesis. However, little\nprogress has been made on using diffusion models to generate discrete symbolic\nmusic because this new class of generative models are not well suited for\ndiscrete data while its iterative sampling process is computationally\nexpensive. In this work, we propose a diffusion model combined with a\nGenerative Adversarial Network, aiming to (i) alleviate one of the remaining\nchallenges in algorithmic music generation which is the control of generation\ntowards a target emotion, and (ii) mitigate the slow sampling drawback of\ndiffusion models applied to symbolic music generation. We first used a trained\nVariational Autoencoder to obtain embeddings of a symbolic music dataset with\nemotion labels and then used those to train a diffusion model. Our results\ndemonstrate the successful control of our diffusion model to generate symbolic\nmusic with a desired emotion. Our model achieves several orders of magnitude\nimprovement in computational cost, requiring merely four time steps to denoise\nwhile the steps required by current state-of-the-art diffusion models for\nsymbolic music generation is in the order of thousands.", "published": "2023-10-21 15:35:43", "link": "http://arxiv.org/abs/2310.14040v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Composer Style-specific Symbolic Music Generation Using Vector Quantized\n  Discrete Diffusion Models", "abstract": "Emerging Denoising Diffusion Probabilistic Models (DDPM) have become\nincreasingly utilised because of promising results they have achieved in\ndiverse generative tasks with continuous data, such as image and sound\nsynthesis. Nonetheless, the success of diffusion models has not been fully\nextended to discrete symbolic music. We propose to combine a vector quantized\nvariational autoencoder (VQ-VAE) and discrete diffusion models for the\ngeneration of symbolic music with desired composer styles. The trained VQ-VAE\ncan represent symbolic music as a sequence of indexes that correspond to\nspecific entries in a learned codebook. Subsequently, a discrete diffusion\nmodel is used to model the VQ-VAE's discrete latent space. The diffusion model\nis trained to generate intermediate music sequences consisting of codebook\nindexes, which are then decoded to symbolic music using the VQ-VAE's decoder.\nThe evaluation results demonstrate our model can generate symbolic music with\ntarget composer styles that meet the given conditions with a high accuracy of\n72.36%. Our code is available at\nhttps://github.com/jinchengzhanggg/VQVAE-Diffusion.", "published": "2023-10-21 15:41:50", "link": "http://arxiv.org/abs/2310.14044v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
