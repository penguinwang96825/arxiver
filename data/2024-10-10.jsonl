{"title": "Variance-Hawkes Process and its Application to Energy Markets", "abstract": "We define a new model using a Hawkes process as a subordinator in a standard\nBrownian motion. We demonstrate that this Hawkes subordinated Brownian motion\nor more succinctly, variance-Hawkes process can be fit to 2018 and 2019 natural\ngas and crude oil front-month futures log returns. This variance-Hawkes process\nallows financial models to easily have clustering effects encoded into their\nbehaviour in a simple and tractable way. We also compare the simulations of a\nsquare of a variance Hawkes process with its Ito formula. We simulate both\nprocesses and compare their distributions, trajectories, and percent errors\nacross multiple runs. We derive the generator relating to this Hawkes\nsubordinated Brownian motion, calculate several moments, and conjecture its\ndistribution. We also provide explicit solutions to the second moments of the\nHawkes process and its intensity as well as the cross moment between the Hawkes\nprocess and its intensity in the case of an exponential kernel.", "published": "2024-10-10 23:44:26", "link": "http://arxiv.org/abs/2410.08420v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Optimal mutual insurance against systematic longevity risk", "abstract": "We mathematically demonstrate how and what it means for two collective\npension funds to mutually insure one another against systematic longevity risk.\nThe key equation that facilitates the exchange of insurance is a market\nclearing condition. This enables an insurance market to be established even if\nthe two funds face the same mortality risk, so long as they have different risk\npreferences. Provided the preferences of the two funds are not too dissimilar,\ninsurance provides little benefit, implying the base scheme is effectively\noptimal. When preferences vary significantly, insurance can be beneficial.", "published": "2024-10-10 09:26:04", "link": "http://arxiv.org/abs/2410.07749v1", "categories": ["q-fin.MF", "q-fin.PM", "q-fin.RM"], "primary_category": "q-fin.MF"}
{"title": "Fitting the seven-parameter Generalized Tempered Stable distribution to the financial data", "abstract": "The paper proposes and implements a methodology to fit a seven-parameter\nGeneralized Tempered Stable (GTS) distribution to financial data. The\nnonexistence of the mathematical expression of the GTS probability density\nfunction makes the maximum likelihood estimation (MLE) inadequate for providing\nparameter estimations. Based on the function characteristic and the fractional\nFourier transform (FRFT), we provide a comprehensive approach to circumvent the\nproblem and yield a good parameter estimation of the GTS probability. The\nmethodology was applied to fit two heavily tailed data (Bitcoin and Ethereum\nreturns) and two peaked data (S\\&P 500 and SPY ETF returns). For each index,\nthe estimation results show that the six-parameter estimations are\nstatistically significant except for the local parameter, $\\mu$. The\ngoodness-of-fit was assessed through Kolmogorov-Smirnov, Anderson-Darling, and\nPearson's chi-squared statistics. While the two-parameter geometric Brownian\nmotion (GBM) hypothesis is always rejected, the GTS distribution fits\nsignificantly with a very high p-value; and outperforms the Kobol,\nCarr-Geman-Madan-Yor, and Bilateral Gamma distributions.", "published": "2024-10-10 23:05:08", "link": "http://arxiv.org/abs/2410.19751v2", "categories": ["q-fin.ST", "math.PR"], "primary_category": "q-fin.ST"}
{"title": "TraderTalk: An LLM Behavioural ABM applied to Simulating Human Bilateral Trading Interactions", "abstract": "We introduce a novel hybrid approach that augments Agent-Based Models (ABMs)\nwith behaviors generated by Large Language Models (LLMs) to simulate human\ntrading interactions. We call our model TraderTalk. Leveraging LLMs trained on\nextensive human-authored text, we capture detailed and nuanced representations\nof bilateral conversations in financial trading. Applying this Generative\nAgent-Based Model (GABM) to government bond markets, we replicate trading\ndecisions between two stylised virtual humans. Our method addresses both\nstructural challenges, such as coordinating turn-taking between realistic\nLLM-based agents, and design challenges, including the interpretation of LLM\noutputs by the agent model. By exploring prompt design opportunistically rather\nthan systematically, we enhance the realism of agent interactions without\nexhaustive overfitting or model reliance. Our approach successfully replicates\ntrade-to-order volume ratios observed in related asset markets, demonstrating\nthe potential of LLM-augmented ABMs in financial simulations", "published": "2024-10-10 23:58:07", "link": "http://arxiv.org/abs/2410.21280v1", "categories": ["q-fin.TR", "cs.AI"], "primary_category": "q-fin.TR"}
{"title": "PublicHearingBR: A Brazilian Portuguese Dataset of Public Hearing\n  Transcripts for Summarization of Long Documents", "abstract": "This paper introduces PublicHearingBR, a Brazilian Portuguese dataset\ndesigned for summarizing long documents. The dataset consists of transcripts of\npublic hearings held by the Brazilian Chamber of Deputies, paired with news\narticles and structured summaries containing the individuals participating in\nthe hearing and their statements or opinions. The dataset supports the\ndevelopment and evaluation of long document summarization systems in\nPortuguese. Our contributions include the dataset, a hybrid summarization\nsystem to establish a baseline for future studies, and a discussion on\nevaluation metrics for summarization involving large language models,\naddressing the challenge of hallucination in the generated summaries. As a\nresult of this discussion, the dataset also provides annotated data that can be\nused in Natural Language Inference tasks in Portuguese.", "published": "2024-10-10 00:13:59", "link": "http://arxiv.org/abs/2410.07495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thought2Text: Text Generation from EEG Signal using Large Language\n  Models (LLMs)", "abstract": "Decoding and expressing brain activity in a comprehensible form is a\nchallenging frontier in AI. This paper presents Thought2Text, which uses\ninstruction-tuned Large Language Models (LLMs) fine-tuned with EEG data to\nachieve this goal. The approach involves three stages: (1) training an EEG\nencoder for visual feature extraction, (2) fine-tuning LLMs on image and text\ndata, enabling multimodal description generation, and (3) further fine-tuning\non EEG embeddings to generate text directly from EEG during inference.\nExperiments on a public EEG dataset collected for six subjects with image\nstimuli and text captions demonstrate the efficacy of multimodal LLMs\n(LLaMA-v3, Mistral-v0.3, Qwen2.5), validated using traditional language\ngeneration evaluation metrics, as well as fluency and adequacy measures. This\napproach marks a significant advancement towards portable, low-cost\n\"thoughts-to-text\" technology with potential applications in both neuroscience\nand natural language processing.", "published": "2024-10-10 00:47:59", "link": "http://arxiv.org/abs/2410.07507v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News", "abstract": "Large Language Models (LLMs) have fast become an essential tools to many\nconversational chatbots due to their ability to provide coherent answers for\nvaried queries. Datasets used to train these LLMs are often a mix of generic\nand synthetic samples, thus lacking the verification needed to provide correct\nand verifiable answers for T.V. News.\n  We collect and share a large collection of QA pairs extracted from\ntranscripts of news recordings from various news-channels across the United\nStates. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM\nmodel. Our model surpasses base models of similar size on several open LLM\nbenchmarks. We further integrate and propose a RAG method to improve\ncontextualization of our answers and also point it to a verifiable news\nrecording.", "published": "2024-10-10 01:21:48", "link": "http://arxiv.org/abs/2410.07520v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI-Press: A Multi-Agent News Generating and Feedback Simulation System\n  Powered by Large Language Models", "abstract": "The rise of various social platforms has transformed journalism. The growing\ndemand for news content has led to the increased use of large language models\n(LLMs) in news production due to their speed and cost-effectiveness. However,\nLLMs still encounter limitations in professionalism and ethical judgment in\nnews generation. Additionally, predicting public feedback is usually difficult\nbefore news is released. To tackle these challenges, we introduce AI-Press, an\nautomated news drafting and polishing system based on multi-agent collaboration\nand Retrieval-Augmented Generation. We develop a feedback simulation system\nthat generates public feedback considering demographic distributions. Through\nextensive quantitative and qualitative evaluations, our system shows\nsignificant improvements in news-generating capabilities and verifies the\neffectiveness of public feedback simulation.", "published": "2024-10-10 02:58:52", "link": "http://arxiv.org/abs/2410.07561v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for\n  Large Language Models", "abstract": "Finding appropriate prompts for the specific task has become an important\nissue as the usage of Large Language Models (LLM) has expanded. Reinforcement\nLearning (RL) is widely used for prompt tuning, but its inherent instability\nand environmental dependency make it difficult to use in practice. In this\npaper, we propose StablePrompt, which strikes a balance between training\nstability and search space, mitigating the instability of RL and producing\nhigh-performance prompts. We formulate prompt tuning as an online RL problem\nbetween the agent and target LLM and introduce Adaptive Proximal Policy\nOptimization (APPO). APPO introduces an LLM anchor model to adaptively adjust\nthe rate of policy updates. This allows for flexible prompt search while\npreserving the linguistic ability of the pre-trained LLM. StablePrompt\noutperforms previous methods on various tasks including text classification,\nquestion answering, and text generation. Our code can be found in github.", "published": "2024-10-10 06:35:51", "link": "http://arxiv.org/abs/2410.07652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Smart Audit System Empowered by LLM", "abstract": "Manufacturing quality audits are pivotal for ensuring high product standards\nin mass production environments. Traditional auditing processes, however, are\nlabor-intensive and reliant on human expertise, posing challenges in\nmaintaining transparency, accountability, and continuous improvement across\ncomplex global supply chains. To address these challenges, we propose a smart\naudit system empowered by large language models (LLMs). Our approach introduces\nthree innovations: a dynamic risk assessment model that streamlines audit\nprocedures and optimizes resource allocation; a manufacturing compliance\ncopilot that enhances data processing, retrieval, and evaluation for a\nself-evolving manufacturing knowledge base; and a Re-act framework commonality\nanalysis agent that provides real-time, customized analysis to empower\nengineers with insights for supplier improvement. These enhancements elevate\naudit efficiency and effectiveness, with testing scenarios demonstrating an\nimprovement of over 24%.", "published": "2024-10-10 07:36:15", "link": "http://arxiv.org/abs/2410.07677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Facet Counterfactual Learning for Content Quality Evaluation", "abstract": "Evaluating the quality of documents is essential for filtering valuable\ncontent from the current massive amount of information. Conventional approaches\ntypically rely on a single score as a supervision signal for training content\nquality evaluators, which is inadequate to differentiate documents with quality\nvariations across multiple facets. In this paper, we propose Multi-facet\ncOunterfactual LEarning (MOLE), a framework for efficiently constructing\nevaluators that perceive multiple facets of content quality evaluation. Given a\nspecific scenario, we prompt large language models to generate counterfactual\ncontent that exhibits variations in critical quality facets compared to the\noriginal document. Furthermore, we leverage a joint training strategy based on\ncontrastive learning and supervised learning to enable the evaluator to\ndistinguish between different quality facets, resulting in more accurate\npredictions of content quality scores. Experimental results on 2 datasets\nacross different scenarios demonstrate that our proposed MOLE framework\neffectively improves the correlation of document content quality evaluations\nwith human judgments, which serve as a valuable toolkit for effective\ninformation acquisition.", "published": "2024-10-10 08:04:10", "link": "http://arxiv.org/abs/2410.07693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StepTool: Enhancing Multi-Step Tool Usage in LLMs through Step-Grained\n  Reinforcement Learning", "abstract": "Despite powerful text generation capabilities, large language models (LLMs)\nstill need to learn how to utilize external tools to solve complex tasks, a\nprocess known as tool learning. Existing methods primarily rely on supervised\nfine-tuning to enhance tool-use capabilities, treating tool learning as a\ntext-generation task while overlooking the decision-making complexities\ninherent in multi-step contexts. In this work, we propose modeling tool\nlearning as a dynamic decision-making task and introduce StepTool, a novel\nstep-grained reinforcement learning framework that enhances the multi-step tool\nuse capabilities of LLMs. StepTool consists of two main components:\nStep-grained Reward Shaping, which assigns rewards at each tool interaction\nbased on the success of tool invocation and its contribution to the task; and\nStep-grained Optimization, which uses policy gradient methods to optimize the\nmodel in a multi-step manner. Experimental results demonstrate that StepTool\nsignificantly outperforms existing methods in multi-step, tool-based tasks,\noffering a robust solution for tool learning.", "published": "2024-10-10 09:23:26", "link": "http://arxiv.org/abs/2410.07745v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling User Preferences with Automatic Metrics: Creating a\n  High-Quality Preference Dataset for Machine Translation", "abstract": "Alignment with human preferences is an important step in developing accurate\nand safe large language models. This is no exception in machine translation\n(MT), where better handling of language nuances and context-specific variations\nleads to improved quality. However, preference data based on human feedback can\nbe very expensive to obtain and curate at a large scale. Automatic metrics, on\nthe other hand, can induce preferences, but they might not match human\nexpectations perfectly. In this paper, we propose an approach that leverages\nthe best of both worlds. We first collect sentence-level quality assessments\nfrom professional linguists on translations generated by multiple high-quality\nMT systems and evaluate the ability of current automatic metrics to recover\nthese preferences. We then use this analysis to curate a new dataset, MT-Pref\n(metric induced translation preference) dataset, which comprises 18k instances\ncovering 18 language directions, using texts sourced from multiple domains\npost-2022. We show that aligning TOWER models on MT-Pref significantly improves\ntranslation quality on WMT23 and FLORES benchmarks.", "published": "2024-10-10 10:09:54", "link": "http://arxiv.org/abs/2410.07779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering Overfitting in Large Language Model Editing", "abstract": "Knowledge editing has been proposed as an effective method for updating and\ncorrecting the internal knowledge of Large Language Models (LLMs). However,\nexisting editing methods often struggle with complex tasks, such as multi-hop\nreasoning. In this paper, we identify and investigate the phenomenon of Editing\nOverfit, where edited models assign disproportionately high probabilities to\nthe edit target, hindering the generalization of new knowledge in complex\nscenarios. We attribute this issue to the current editing paradigm, which\nplaces excessive emphasis on the direct correspondence between the input prompt\nand the edit target for each edit sample. To further explore this issue, we\nintroduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge\nEditing), along with fine-grained evaluation metrics. Through comprehensive\nexperiments and analysis, we demonstrate that Editing Overfit is prevalent in\ncurrent editing methods and that common overfitting mitigation strategies are\nof limited effectiveness in knowledge editing. To overcome this, inspired by\nLLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy\ncalled Learn to Inference (LTI), which introduce a Multi-stage Inference\nConstraint module to guide the edited models in recalling new knowledge\nsimilarly to how unedited LLMs leverage knowledge through in-context learning.\nExtensive experimental results across a wide range of tasks validate the\neffectiveness of LTI in mitigating Editing Overfit.", "published": "2024-10-10 11:09:00", "link": "http://arxiv.org/abs/2410.07819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting and Transferring Abilities For Building Multi-lingual\n  Ability-enhanced Large Language Models", "abstract": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at\n\\url{https://github.com/RUCAIBox/MAET}.", "published": "2024-10-10 11:23:18", "link": "http://arxiv.org/abs/2410.07825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study\n  of Alignment with Human Responses", "abstract": "Language models often misinterpret human intentions due to their handling of\nambiguity, a limitation well-recognized in NLP research. While morally clear\nscenarios are more discernible to LLMs, greater difficulty is encountered in\nmorally ambiguous contexts. In this investigation, we explored LLM calibration\nto show that human and LLM judgments are poorly aligned in such scenarios. We\nused two curated datasets from the Scruples project for evaluation: DILEMMAS,\nwhich involves pairs of distinct moral scenarios to assess the model's ability\nto compare and contrast ethical situations, and ANECDOTES, which presents\nindividual narratives to evaluate the model's skill in drawing out details,\ninterpreting, and analyzing distinct moral scenarios. Model answer\nprobabilities were extracted for all possible choices and compared with human\nannotations to benchmark the alignment of three models: Llama-3.1-8b,\nZephyr-7b-beta, and Mistral-7b. Significant improvements were observed after\nfine-tuning, with notable enhancements in both cross-entropy and Dirichlet\nscores, particularly in the latter. Notably, after fine-tuning, the performance\nof Mistral-7B-Instruct-v0.3 was on par with GPT-4o. However, the experimental\nmodels that were examined were all still outperformed by the BERT and RoBERTa\nmodels in terms of cross-entropy scores. Our fine-tuning approach, which\nimproves the model's understanding of text distributions in a text-to-text\nformat, effectively enhances performance and alignment in complex\ndecision-making contexts, underscoring the need for further research to refine\nethical reasoning techniques and capture human judgment nuances.", "published": "2024-10-10 11:24:04", "link": "http://arxiv.org/abs/2410.07826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why do objects have many names? A study on word informativeness in\n  language use and lexical systems", "abstract": "Human lexicons contain many different words that speakers can use to refer to\nthe same object, e.g., \"purple\" or \"magenta\" for the same shade of color. On\nthe one hand, studies on language use have explored how speakers adapt their\nreferring expressions to successfully communicate in context, without focusing\non properties of the lexical system. On the other hand, studies in language\nevolution have discussed how competing pressures for informativeness and\nsimplicity shape lexical systems, without tackling in-context communication. We\naim at bridging the gap between these traditions, and explore why a soft\nmapping between referents and words is a good solution for communication, by\ntaking into account both in-context communication and the structure of the\nlexicon. We propose a simple measure of informativeness for words and lexical\nsystems, grounded in a visual space, and analyze color naming data for English\nand Mandarin Chinese. We conclude that optimal lexical systems are those where\nmultiple words can apply to the same referent, conveying different amounts of\ninformation. Such systems allow speakers to maximize communication accuracy and\nminimize the amount of information they convey when communicating about\nreferents in contexts.", "published": "2024-10-10 11:29:08", "link": "http://arxiv.org/abs/2410.07827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages\n  with Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated exceptional promise in\ntranslation tasks for high-resource languages. However, their performance in\nlow-resource languages is limited by the scarcity of both parallel and\nmonolingual corpora, as well as the presence of noise. Consequently, such LLMs\nsuffer with alignment and have lagged behind State-of-The-Art (SoTA) neural\nmachine translation (NMT) models in these settings. This paper introduces\nNusaMT-7B, an LLM-based machine translation model for low-resource Indonesian\nlanguages, starting with Balinese and Minangkabau. Leveraging the pretrained\nLLaMA2-7B, our approach integrates continued pre-training on monolingual data,\nSupervised Fine-Tuning (SFT), self-learning, and an LLM-based data cleaner to\nreduce noise in parallel sentences. In the FLORES-200 multilingual translation\nbenchmark, NusaMT-7B outperforms SoTA models in the spBLEU metric by up to\n+6.69 spBLEU in translations into Balinese and Minangkabau, but underperforms\nby up to -3.38 spBLEU in translations into higher-resource languages. Our\nresults show that fine-tuned LLMs can enhance translation quality for\nlow-resource languages, aiding in linguistic preservation and cross-cultural\ncommunication.", "published": "2024-10-10 11:33:25", "link": "http://arxiv.org/abs/2410.07830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Self-Consistency: Enhancing Language Model Reasoning via\n  Semantic Weighting", "abstract": "While large language models (LLMs) have rapidly improved their performance on\na broad number of tasks, they still often fall short on reasoning tasks. As\nLLMs become more integrated in diverse real-world tasks, advancing their\nreasoning capabilities is crucial to their effectiveness in nuanced, complex\nproblems. Wang et al.'s self-consistency framework reveals that sampling\nmultiple rationales before taking a majority vote reliably improves model\nperformance across various closed-answer reasoning tasks. Standard methods\nbased on this framework aggregate the final decisions of these rationales but\nfail to utilize the semantic information detailed in the step-by-step reasoning\npaths. Our work introduces semantic self-consistency, enhancing this approach\nby incorporating and analyzing both the reasoning paths of these rationales in\naddition to their final decisions before taking a majority vote. These methods\nnot only improve the reliability of reasoning paths but also cause more robust\nperformance on complex reasoning tasks.", "published": "2024-10-10 11:58:48", "link": "http://arxiv.org/abs/2410.07839v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large\n  Language Models", "abstract": "Recent advancements in large language models (LLMs) have led to significant\nbreakthroughs in mathematical reasoning capabilities. However, existing\nbenchmarks like GSM8K or MATH are now being solved with high accuracy (e.g.,\nOpenAI o1 achieves 94.8\\% on MATH dataset), indicating their inadequacy for\ntruly challenging these models. To bridge this gap, we propose a comprehensive\nand challenging benchmark specifically designed to assess LLMs' mathematical\nreasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks,\nour dataset focuses exclusively on mathematics and comprises a vast collection\nof 4428 competition-level problems with rigorous human annotation. These\nproblems are meticulously categorized into over 33 sub-domains and span more\nthan 10 distinct difficulty levels, enabling a holistic assessment of model\nperformance in Olympiad-mathematical reasoning. Furthermore, we conducted an\nin-depth analysis based on this benchmark. Our experimental results show that\neven the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle\nwith highly challenging Olympiad-level problems, with 60.54\\% and 52.55\\%\naccuracy, highlighting significant challenges in Olympiad-level mathematical\nreasoning.", "published": "2024-10-10 14:39:33", "link": "http://arxiv.org/abs/2410.07985v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Privacy-preserved LLM Cascade via CoT-enhanced Policy Learning", "abstract": "Large Language Models (LLMs) have gained significant attention in on-device\napplications due to their remarkable performance across real-world tasks.\nHowever, on-device LLMs often suffer from suboptimal performance due to\nhardware limitations. A promising solution to this challenge is cascading a\nweaker local (on-device) LLM with a more powerful server LLM. While existing\nresearch on LLM cascade primarily optimizes the performance-cost trade-off,\nreal-world applications impose additional requirements, such as privacy\npreservation, which remain largely unaddressed. In this work, we move beyond\nexisting confidence- and logit-based LLM cascade methods and propose\n$\\mathbf{P^{3}Defer}$, a novel Chain-of-Thought (CoT)-enhanced \\textbf{p}olicy\nlearning framework for \\textbf{p}rivacy-\\textbf{p}reserved \\textbf{defer}ral\ndecision-making. Our approach effectively improves cascade efficiency while\nmitigating privacy risks. Extensive experiments on three benchmark datasets\ndemonstrate the effectiveness and superiority of $\\mathbf{P^{3}Defer}$ over\nexisting methods.", "published": "2024-10-10 15:09:52", "link": "http://arxiv.org/abs/2410.08014v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Rise of AI-Generated Content in Wikipedia", "abstract": "The rise of AI-generated content in popular information sources raises\nsignificant concerns about accountability, accuracy, and bias amplification.\nBeyond directly impacting consumers, the widespread presence of this content\nposes questions for the long-term viability of training language models on vast\ninternet sweeps. We use GPTZero, a proprietary AI detector, and Binoculars, an\nopen-source alternative, to establish lower bounds on the presence of\nAI-generated content in recently created Wikipedia pages. Both detectors reveal\na marked increase in AI-generated content in recent pages compared to those\nfrom before the release of GPT-3.5. With thresholds calibrated to achieve a 1%\nfalse positive rate on pre-GPT-3.5 articles, detectors flag over 5% of newly\ncreated English Wikipedia articles as AI-generated, with lower percentages for\nGerman, French, and Italian articles. Flagged Wikipedia articles are typically\nof lower quality and are often self-promotional or partial towards a specific\nviewpoint on controversial topics.", "published": "2024-10-10 15:36:10", "link": "http://arxiv.org/abs/2410.08044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Divide and Translate: Compositional First-Order Logic Translation and\n  Verification for Complex Logical Reasoning", "abstract": "Complex logical reasoning tasks require a long sequence of reasoning, which a\nlarge language model (LLM) with chain-of-thought prompting still falls short.\nTo alleviate this issue, neurosymbolic approaches incorporate a symbolic\nsolver. Specifically, an LLM only translates a natural language problem into a\nsatisfiability (SAT) problem that consists of first-order logic formulas, and a\nsound symbolic solver returns a mathematically correct solution. However, we\ndiscover that LLMs have difficulties to capture complex logical semantics\nhidden in the natural language during translation. To resolve this limitation,\nwe propose a Compositional First-Order Logic Translation. An LLM first parses a\nnatural language sentence into newly defined logical dependency structures that\nconsist of an atomic subsentence and its dependents, then sequentially\ntranslate the parsed subsentences. Since multiple logical dependency structures\nand sequential translations are possible for a single sentence, we also\nintroduce two Verification algorithms to ensure more reliable results. We\nutilize an SAT solver to rigorously compare semantics of generated first-order\nlogic formulas and select the most probable one. We evaluate the proposed\nmethod, dubbed CLOVER, on seven logical reasoning benchmarks and show that it\noutperforms the previous neurosymbolic approaches and achieves new\nstate-of-the-art results.", "published": "2024-10-10 15:42:39", "link": "http://arxiv.org/abs/2410.08047v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Target-Aware Analysis of Data Augmentation for Hate Speech Detection", "abstract": "Hate speech is one of the main threats posed by the widespread use of social\nnetworks, despite efforts to limit it. Although attention has been devoted to\nthis issue, the lack of datasets and case studies centered around scarcely\nrepresented phenomena, such as ableism or ageism, can lead to hate speech\ndetection systems that do not perform well on underrepresented identity groups.\nGiven the unpreceded capabilities of LLMs in producing high-quality data, we\ninvestigate the possibility of augmenting existing data with generative\nlanguage models, reducing target imbalance. We experiment with augmenting 1,000\nposts from the Measuring Hate Speech corpus, an English dataset annotated with\ntarget identity information, adding around 30,000 synthetic examples using both\nsimple data augmentation methods and different types of generative models,\ncomparing autoregressive and sequence-to-sequence approaches. We find\ntraditional DA methods to often be preferable to generative models, but the\ncombination of the two tends to lead to the best results. Indeed, for some hate\ncategories such as origin, religion, and disability, hate speech classification\nusing augmented data for training improves by more than 10% F1 over the no\naugmentation baseline. This work contributes to the development of systems for\nhate speech detection that are not only better performing but also fairer and\nmore inclusive towards targets that have been neglected so far.", "published": "2024-10-10 15:46:27", "link": "http://arxiv.org/abs/2410.08053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining", "abstract": "Efficient data selection is crucial to accelerate the pretraining of large\nlanguage models (LLMs). While various methods have been proposed to enhance\ndata efficiency, limited research has addressed the inherent conflicts between\nthese approaches to achieve optimal data selection for LLM pretraining. To\ntackle this problem, we propose a novel multi-agent collaborative data\nselection mechanism. In this framework, each data selection method serves as an\nindependent agent, and an agent console is designed to dynamically integrate\nthe information from all agents throughout the LLM training process. We conduct\nextensive empirical studies to evaluate our multi-agent framework. The\nexperimental results demonstrate that our approach significantly improves data\nefficiency, accelerates convergence in LLM training, and achieves an average\nperformance gain up to 10.5% across multiple language model benchmarks compared\nto the state-of-the-art methods.", "published": "2024-10-10 16:45:28", "link": "http://arxiv.org/abs/2410.08102v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Makes Large Language Models Reason in (Multi-Turn) Code Generation?", "abstract": "Prompting techniques such as chain-of-thought have established themselves as\na popular vehicle for improving the outputs of large language models (LLMs).\nFor code generation, however, their exact mechanics and efficacy are\nunder-explored. We thus investigate the effects of a wide range of prompting\nstrategies with a focus on automatic re-prompting over multiple turns and\ncomputational requirements. After systematically decomposing reasoning,\ninstruction, and execution feedback prompts, we conduct an extensive grid\nsearch on the competitive programming benchmarks CodeContests and TACO for\nmultiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o).\nOur study reveals strategies that consistently improve performance across all\nmodels with small and large sampling budgets. We then show how finetuning with\nsuch an optimal configuration allows models to internalize the induced\nreasoning process and obtain improvements in performance and scalability for\nmulti-turn code generation.", "published": "2024-10-10 16:53:10", "link": "http://arxiv.org/abs/2410.08105v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effect of Surprisal on Reading Times in Information Seeking and\n  Repeated Reading", "abstract": "The effect of surprisal on processing difficulty has been a central topic of\ninvestigation in psycholinguistics. Here, we use eyetracking data to examine\nthree language processing regimes that are common in daily life but have not\nbeen addressed with respect to this question: information seeking, repeated\nprocessing, and the combination of the two. Using standard regime-agnostic\nsurprisal estimates we find that the prediction of surprisal theory regarding\nthe presence of a linear effect of surprisal on processing times, extends to\nthese regimes. However, when using surprisal estimates from regime-specific\ncontexts that match the contexts and tasks given to humans, we find that in\ninformation seeking, such estimates do not improve the predictive power of\nprocessing times compared to standard surprisals. Further, regime-specific\ncontexts yield near zero surprisal estimates with no predictive power for\nprocessing times in repeated reading. These findings point to misalignments of\ntask and memory representations between humans and current language models, and\nquestion the extent to which such models can be used for estimating cognitively\nrelevant quantities. We further discuss theoretical challenges posed by these\nresults.", "published": "2024-10-10 17:43:34", "link": "http://arxiv.org/abs/2410.08162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment", "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining. Our project page is\navailable at: https://genarm.github.io.", "published": "2024-10-10 17:58:24", "link": "http://arxiv.org/abs/2410.08193v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MELO: An Evaluation Benchmark for Multilingual Entity Linking of\n  Occupations", "abstract": "We present the Multilingual Entity Linking of Occupations (MELO) Benchmark, a\nnew collection of 48 datasets for evaluating the linking of entity mentions in\n21 languages to the ESCO Occupations multilingual taxonomy. MELO was built\nusing high-quality, pre-existent human annotations. We conduct experiments with\nsimple lexical models and general-purpose sentence encoders, evaluated as\nbi-encoders in a zero-shot setup, to establish baselines for future research.\nThe datasets and source code for standardized evaluation are publicly available\nat https://github.com/Avature/melo-benchmark", "published": "2024-10-10 19:14:54", "link": "http://arxiv.org/abs/2410.08319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Differentially Private Synthetic Data Generation in\n  High-Stakes Domains", "abstract": "The difficulty of anonymizing text data hinders the development and\ndeployment of NLP in high-stakes domains that involve private data, such as\nhealthcare and social services. Poorly anonymized sensitive data cannot be\neasily shared with annotators or external researchers, nor can it be used to\ntrain public models. In this work, we explore the feasibility of using\nsynthetic data generated from differentially private language models in place\nof real data to facilitate the development of NLP in these domains without\ncompromising privacy. In contrast to prior work, we generate synthetic data for\nreal high-stakes domains, and we propose and conduct use-inspired evaluations\nto assess data quality. Our results show that prior simplistic evaluations have\nfailed to highlight utility, privacy, and fairness issues in the synthetic\ndata. Overall, our work underscores the need for further improvements to\nsynthetic data generation for it to be a viable way to enable\nprivacy-preserving data sharing.", "published": "2024-10-10 19:31:02", "link": "http://arxiv.org/abs/2410.08327v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Transformer Models for Suicide Risk Detection on Social Media", "abstract": "The detection of suicide risk in social media is a critical task with\npotential life-saving implications. This paper presents a study on leveraging\nstate-of-the-art natural language processing solutions for identifying suicide\nrisk in social media posts as a submission for the \"IEEE BigData 2024 Cup:\nDetection of Suicide Risk on Social Media\" conducted by the kubapok team. We\nexperimented with the following configurations of transformer-based models:\nfine-tuned DeBERTa, GPT-4o with CoT and few-shot prompting, and fine-tuned\nGPT-4o. The task setup was to classify social media posts into four categories:\nindicator, ideation, behavior, and attempt. Our findings demonstrate that the\nfine-tuned GPT-4o model outperforms two other configurations, achieving high\naccuracy in identifying suicide risk. Notably, our model achieved second place\nin the competition. By demonstrating that straightforward, general-purpose\nmodels can achieve state-of-the-art results, we propose that these models,\ncombined with minimal tuning, may have the potential to be effective solutions\nfor automated suicide risk detection on social media.", "published": "2024-10-10 21:15:25", "link": "http://arxiv.org/abs/2410.08375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Interplay between Parametric and Contextual Knowledge\n  for Large Language Models", "abstract": "Large language models (LLMs) encode vast amounts of knowledge during\npre-training (parametric knowledge, or PK) and can further be enhanced by\nincorporating contextual knowledge (CK). Can LLMs effectively integrate their\ninternal PK with external CK to solve complex problems? In this paper, we\ninvestigate the dynamic interaction between PK and CK, categorizing their\nrelationships into four types: Supportive, Complementary, Conflicting, and\nIrrelevant. To support this investigation, we introduce ECHOQA, a benchmark\nspanning scientific, factual, and commonsense knowledge. Our results show that\nLLMs tend to suppress their PK when contextual information is available, even\nwhen it is complementary or irrelevant. While tailored instructions can\nencourage LLMs to rely more on their PK, they still struggle to fully leverage\nit. These findings reveal a key vulnerability in LLMs, raising concerns about\ntheir reliability in knowledge-intensive tasks. Resources are available at\nhttps://github.com/sitaocheng/Knowledge_Interplay", "published": "2024-10-10 23:09:08", "link": "http://arxiv.org/abs/2410.08414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using LLMs to Discover Legal Factors", "abstract": "Factors are a foundational component of legal analysis and computational\nmodels of legal reasoning. These factor-based representations enable lawyers,\njudges, and AI and Law researchers to reason about legal cases. In this paper,\nwe introduce a methodology that leverages large language models (LLMs) to\ndiscover lists of factors that effectively represent a legal domain. Our method\ntakes as input raw court opinions and produces a set of factors and associated\ndefinitions. We demonstrate that a semi-automated approach, incorporating\nminimal human involvement, produces factor representations that can predict\ncase outcomes with moderate success, if not yet as well as expert-defined\nfactors can.", "published": "2024-10-10 00:42:10", "link": "http://arxiv.org/abs/2410.07504v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MKGL: Mastery of a Three-Word Language", "abstract": "Large language models (LLMs) have significantly advanced performance across a\nspectrum of natural language processing (NLP) tasks. Yet, their application to\nknowledge graphs (KGs), which describe facts in the form of triplets and allow\nminimal hallucinations, remains an underexplored frontier. In this paper, we\ninvestigate the integration of LLMs with KGs by introducing a specialized KG\nLanguage (KGL), where a sentence precisely consists of an entity noun, a\nrelation verb, and ends with another entity noun. Despite KGL's unfamiliar\nvocabulary to the LLM, we facilitate its learning through a tailored dictionary\nand illustrative sentences, and enhance context understanding via real-time KG\ncontext retrieval and KGL token embedding augmentation. Our results reveal that\nLLMs can achieve fluency in KGL, drastically reducing errors compared to\nconventional KG embedding methods on KG completion. Furthermore, our enhanced\nLLM shows exceptional competence in generating accurate three-word sentences\nfrom an initial entity and interpreting new unseen terms out of KGs.", "published": "2024-10-10 01:39:26", "link": "http://arxiv.org/abs/2410.07526v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via\n  Large Language Model Prompting", "abstract": "Entity Linking (EL) is the process of associating ambiguous textual mentions\nto specific entities in a knowledge base. Traditional EL methods heavily rely\non large datasets to enhance their performance, a dependency that becomes\nproblematic in the context of few-shot entity linking, where only a limited\nnumber of examples are available for training. To address this challenge, we\npresent OneNet, an innovative framework that utilizes the few-shot learning\ncapabilities of Large Language Models (LLMs) without the need for fine-tuning.\nTo the best of our knowledge, this marks a pioneering approach to applying LLMs\nto few-shot entity linking tasks. OneNet is structured around three key\ncomponents prompted by LLMs: (1) an entity reduction processor that simplifies\ninputs by summarizing and filtering out irrelevant entities, (2) a\ndual-perspective entity linker that combines contextual cues and prior\nknowledge for precise entity linking, and (3) an entity consensus judger that\nemploys a unique consistency algorithm to alleviate the hallucination in the\nentity linking reasoning. Comprehensive evaluations across seven benchmark\ndatasets reveal that OneNet outperforms current state-of-the-art entity linking\nmethods.", "published": "2024-10-10 02:45:23", "link": "http://arxiv.org/abs/2410.07549v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KRAG Framework for Enhancing LLMs in the Legal Domain", "abstract": "This paper introduces Knowledge Representation Augmented Generation (KRAG), a\nnovel framework designed to enhance the capabilities of Large Language Models\n(LLMs) within domain-specific applications. KRAG points to the strategic\ninclusion of critical knowledge entities and relationships that are typically\nabsent in standard data sets and which LLMs do not inherently learn. In the\ncontext of legal applications, we present Soft PROLEG, an implementation model\nunder KRAG, which uses inference graphs to aid LLMs in delivering structured\nlegal reasoning, argumentation, and explanations tailored to user inquiries.\nThe integration of KRAG, either as a standalone framework or in tandem with\nretrieval augmented generation (RAG), markedly improves the ability of language\nmodels to navigate and solve the intricate challenges posed by legal texts and\nterminologies. This paper details KRAG's methodology, its implementation\nthrough Soft PROLEG, and potential broader applications, underscoring its\nsignificant role in advancing natural language understanding and processing in\nspecialized knowledge domains.", "published": "2024-10-10 02:48:06", "link": "http://arxiv.org/abs/2410.07551v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When and Where Did it Happen? An Encoder-Decoder Model to Identify\n  Scenario Context", "abstract": "We introduce a neural architecture finetuned for the task of scenario context\ngeneration: The relevant location and time of an event or entity mentioned in\ntext. Contextualizing information extraction helps to scope the validity of\nautomated finings when aggregating them as knowledge graphs. Our approach uses\na high-quality curated dataset of time and location annotations in a corpus of\nepidemiology papers to train an encoder-decoder architecture. We also explored\nthe use of data augmentation techniques during training. Our findings suggest\nthat a relatively small fine-tuned encoder-decoder model performs better than\nout-of-the-box LLMs and semantic role labeling parsers to accurate predict the\nrelevant scenario information of a particular entity or event.", "published": "2024-10-10 03:05:48", "link": "http://arxiv.org/abs/2410.07567v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Does Vision-Language Adaptation Impact the Safety of Vision Language\n  Models?", "abstract": "Vision-Language adaptation (VL adaptation) transforms Large Language Models\n(LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this\nprocess often compromises the inherent safety capabilities embedded in the\noriginal LLMs. Despite potential harmfulness due to weakened safety measures,\nin-depth analysis on the effects of VL adaptation on safety remains\nunder-explored. This study examines how VL adaptation influences safety and\nevaluates the impact of safety fine-tuning methods. Our analysis reveals that\nsafety degradation occurs during VL adaptation, even when the training data is\nsafe. While safety tuning techniques like supervised fine-tuning with safety\ndatasets or reinforcement learning from human feedback mitigate some risks,\nthey still lead to safety degradation and a reduction in helpfulness due to\nover-rejection issues. Further analysis of internal model weights suggests that\nVL adaptation may impact certain safety-related layers, potentially lowering\noverall safety levels. Additionally, our findings demonstrate that the\nobjectives of VL adaptation and safety tuning are divergent, which often\nresults in their simultaneous application being suboptimal. To address this, we\nsuggest the weight merging approach as an optimal solution effectively reducing\nsafety degradation while maintaining helpfulness. These insights help guide the\ndevelopment of more reliable and secure LVLMs for real-world applications.", "published": "2024-10-10 03:12:03", "link": "http://arxiv.org/abs/2410.07571v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "RealVul: Can We Detect Vulnerabilities in Web Applications with LLM?", "abstract": "The latest advancements in large language models (LLMs) have sparked interest\nin their potential for software vulnerability detection. However, there is\ncurrently a lack of research specifically focused on vulnerabilities in the PHP\nlanguage, and challenges in extracting samples and processing persist,\nhindering the model's ability to effectively capture the characteristics of\nspecific vulnerabilities. In this paper, we present RealVul, the first\nLLM-based framework designed for PHP vulnerability detection, addressing these\nissues. By vulnerability candidate detection methods and employing techniques\nsuch as normalization, we can isolate potential vulnerability triggers while\nstreamlining the code and eliminating unnecessary semantic information,\nenabling the model to better understand and learn from the generated\nvulnerability samples. We also address the issue of insufficient PHP\nvulnerability samples by improving data synthesis methods. To evaluate\nRealVul's performance, we conduct an extensive analysis using five distinct\ncode LLMs on vulnerability data from 180 PHP projects. The results demonstrate\na significant improvement in both effectiveness and generalization compared to\nexisting methods, effectively boosting the vulnerability detection capabilities\nof these models.", "published": "2024-10-10 03:16:34", "link": "http://arxiv.org/abs/2410.07573v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in\n  LLMs, Even for Vigilant Users", "abstract": "Retrieval-Augmented Generation (RAG) is widely adopted for its effectiveness\nand cost-efficiency in mitigating hallucinations and enhancing the\ndomain-specific generation capabilities of large language models (LLMs).\nHowever, is this effectiveness and cost-efficiency truly a free lunch? In this\nstudy, we comprehensively investigate the fairness costs associated with RAG by\nproposing a practical three-level threat model from the perspective of user\nawareness of fairness. Specifically, varying levels of user fairness awareness\nresult in different degrees of fairness censorship on the external dataset. We\nexamine the fairness implications of RAG using uncensored, partially censored,\nand fully censored datasets. Our experiments demonstrate that fairness\nalignment can be easily undermined through RAG without the need for fine-tuning\nor retraining. Even with fully censored and supposedly unbiased external\ndatasets, RAG can lead to biased outputs. Our findings underscore the\nlimitations of current alignment methods in the context of RAG-based LLMs and\nhighlight the urgent need for new strategies to ensure fairness. We propose\npotential mitigations and call for further research to develop robust fairness\nsafeguards in RAG-based LLMs.", "published": "2024-10-10 03:51:58", "link": "http://arxiv.org/abs/2410.07589v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text", "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.", "published": "2024-10-10 03:52:54", "link": "http://arxiv.org/abs/2410.07590v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference\n  Optimization", "abstract": "As large language models (LLMs) are rapidly advancing and achieving\nnear-human capabilities on specific tasks, aligning them with human values is\nbecoming more urgent. In scenarios where LLMs outperform humans, we face a\nweak-to-strong alignment problem where we need to effectively align strong\nstudent LLMs through weak supervision generated by weak teachers. Existing\nalignment methods mainly focus on strong-to-weak alignment and self-alignment\nsettings, and it is impractical to adapt them to the much harder weak-to-strong\nalignment setting. To fill this gap, we propose a multi-agent contrastive\npreference optimization (MACPO) framework. MACPO facilitates weak teachers and\nstrong students to learn from each other by iteratively reinforcing unfamiliar\npositive behaviors while penalizing familiar negative ones. To get this, we\ndevise a mutual positive behavior augmentation strategy to encourage weak\nteachers and strong students to learn from each other's positive behavior and\nfurther provide higher quality positive behavior for the next iteration.\nAdditionally, we propose a hard negative behavior construction strategy to\ninduce weak teachers and strong students to generate familiar negative behavior\nby fine-tuning on negative behavioral data. Experimental results on the HH-RLHF\nand PKU-SafeRLHF datasets, evaluated using both automatic metrics and human\njudgments, demonstrate that MACPO simultaneously improves the alignment\nperformance of strong students and weak teachers. Moreover, as the number of\nweak teachers increases, MACPO achieves better weak-to-strong alignment\nperformance through more iteration optimization rounds.", "published": "2024-10-10 07:29:35", "link": "http://arxiv.org/abs/2410.07672v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+\n  Interaction Trajectories", "abstract": "Fine-tuning on agent-environment interaction trajectory data holds\nsignificant promise for surfacing generalized agent capabilities in open-source\nlarge language models (LLMs). In this work, we introduce AgentBank, by far the\nlargest trajectory tuning data collection featuring more than 50k diverse\nhigh-quality interaction trajectories which comprises 16 tasks covering five\ndistinct agent skill dimensions. Leveraging a novel annotation pipeline, we are\nable to scale the annotated trajectories and generate a trajectory dataset with\nminimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a\nseries of agent models, Samoyed. Our comparative experiments demonstrate the\neffectiveness of scaling the interaction trajectory data to acquire generalized\nagent capabilities. Additional studies also reveal some key observations\nregarding trajectory tuning and agent skill generalization.", "published": "2024-10-10 08:19:12", "link": "http://arxiv.org/abs/2410.07706v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity\n  Mixture", "abstract": "Although many efforts have been made, it is still a challenge to balance the\ntraining budget, downstream performance, and the general capabilities of the\nLLMs in many applications. Training the whole model for downstream tasks is\nexpensive, and could easily result in catastrophic forgetting. By introducing\nparameter-efficient fine-tuning (PEFT), the training cost could be reduced, but\nit still suffers from forgetting, and limits the learning on the downstream\ntasks. To efficiently fine-tune the LLMs with less limitation to their\ndownstream performance while mitigating the forgetting of general capabilities,\nwe propose a novel mixture of expert (MoE) framework based on Soft LoRA and\nIdentity Mixture (SLIM), that allows dynamic routing between LoRA adapters and\nskipping connection, enables the suppression of forgetting. We adopt\nweight-yielding with sliding clustering for better out-of-domain distinguish to\nenhance the routing. We also propose to convert the mixture of low-rank\nadapters to the model merging formulation and introduce fast dynamic merging of\nLoRA adapters to keep the general capabilities of the base model. Extensive\nexperiments demonstrate that the proposed SLIM is comparable to the\nstate-of-the-art PEFT approaches on the downstream tasks while achieving the\nleading performance in mitigating catastrophic forgetting.", "published": "2024-10-10 09:16:05", "link": "http://arxiv.org/abs/2410.07739v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language\n  Models Through Traversing 2D Game Maps", "abstract": "Large language models (LLMs) have recently demonstrated great success in\ngenerating and understanding natural language. While they have also shown\npotential beyond the domain of natural language, it remains an open question as\nto what extent and in which way these LLMs can plan. We investigate their\nplanning capabilities by proposing GameTraversalBenchmark (GTB), a benchmark\nconsisting of diverse 2D grid-based game maps. An LLM succeeds if it can\ntraverse through given objectives, with a minimum number of steps and a minimum\nnumber of generation errors. We evaluate a number of LLMs on GTB and found that\nGPT-4-Turbo achieved the highest score of 44.97% on GTB\\_Score (GTBS), a\ncomposite score that combines the three above criteria. Furthermore, we\npreliminarily test large reasoning models, namely o1, which scores $67.84\\%$ on\nGTBS, indicating that the benchmark remains challenging for current models.\nCode, data, and documentation are available at\nhttps://github.com/umair-nasir14/Game-Traversal-Benchmark.", "published": "2024-10-10 09:54:28", "link": "http://arxiv.org/abs/2410.07765v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialectical Behavior Therapy Approach to LLM Prompting", "abstract": "Large language models demonstrated state-of-the-art results on various\nreasoning tasks when applying the chain-of-thought (CoT) prompting technique.\nCoT prompting guides the model into breaking tasks into a few intermediate\nsteps and provides step-by-step demonstrations. However, solving complex\nreasoning tasks remains a challenge. In this paper, we propose a novel\nprompting strategy inspired by Dialectical Behavioral Therapy (DBT). DBT, a\nform of cognitive-behavioral therapy, aims to help individuals cope with stress\nby developing a system of reasoning. We applied DBT's basic concepts of shaping\ndialog to construct prompts and conducted experiments on different datasets and\nLLMs with various numbers of parameters. Our results show that prompts crafted\nwith DBT techniques significantly improve results on smaller models, achieving\na 7% increase in accuracy on the StrategyQA, 4.8% on Aqua dataset using 8b\nparameters model, and a 16.2% increase on the StrategyQA, 5.3% on GSM8K dataset\nwith 14b parameters model.", "published": "2024-10-10 09:58:03", "link": "http://arxiv.org/abs/2410.07768v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linguistically-Informed Multilingual Instruction Tuning: Is There an\n  Optimal Set of Languages to Tune?", "abstract": "Multilingual language models often perform unevenly across different\nlanguages due to limited generalization capabilities for some languages. This\nissue is significant because of the growing interest in making universal\nlanguage models that work well for all languages. Instruction tuning with\nmultilingual instruction-response pairs has been used to improve model\nperformance across various languages. However, this approach is challenged by\nhigh computational costs, a lack of quality tuning data for all languages, and\nthe \"curse of multilinguality\" -- the performance drop per language after\nadding many languages. Recent studies have found that working with datasets\nwith few languages and a smaller number of instances can be beneficial. Yet,\nthere exists no systematic investigation into how choosing different languages\naffects multilingual instruction tuning. Our study proposes a method to select\nlanguages for instruction tuning in a linguistically informed way, aiming to\nboost model performance across languages and tasks. We use a simple algorithm\nto choose diverse languages and test their effectiveness on various benchmarks\nand open-ended questions. Our results show that this careful selection\ngenerally leads to better outcomes than choosing languages at random. We\nsuggest a new and simple way of enhancing multilingual models by selecting\ndiverse languages based on linguistic features that could help develop better\nmultilingual systems and guide dataset creation efforts. All resources,\nincluding the code for language selection and multilingual instruction tuning,\nare made available in our official repository at\nhttps://github.com/GGLAB-KU/ling-informed-mit enabling reproducibility and\nfurther research in this area.", "published": "2024-10-10 10:57:24", "link": "http://arxiv.org/abs/2410.07809v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Unsupervised Data Validation Methods for Efficient Model Training", "abstract": "This paper investigates the challenges and potential solutions for improving\nmachine learning systems for low-resource languages. State-of-the-art models in\nnatural language processing (NLP), text-to-speech (TTS), speech-to-text (STT),\nand vision-language models (VLM) rely heavily on large datasets, which are\noften unavailable for low-resource languages. This research explores key areas\nsuch as defining \"quality data,\" developing methods for generating appropriate\ndata and enhancing accessibility to model training. A comprehensive review of\ncurrent methodologies, including data augmentation, multilingual transfer\nlearning, synthetic data generation, and data selection techniques, highlights\nboth advancements and limitations. Several open research questions are\nidentified, providing a framework for future studies aimed at optimizing data\nutilization, reducing the required data quantity, and maintaining high-quality\nmodel performance. By addressing these challenges, the paper aims to make\nadvanced machine learning models more accessible for low-resource languages,\nenhancing their utility and impact across various sectors.", "published": "2024-10-10 13:00:53", "link": "http://arxiv.org/abs/2410.07880v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InstructBioMol: Advancing Biomolecule Understanding and Design Following\n  Human Instructions", "abstract": "Understanding and designing biomolecules, such as proteins and small\nmolecules, is central to advancing drug discovery, synthetic biology, and\nenzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have\nrevolutionized biomolecular research, achieving remarkable accuracy in\nbiomolecular prediction and design. However, a critical gap remains between\nAI's computational power and researchers' intuition, using natural language to\nalign molecular complexity with human intentions. Large Language Models (LLMs)\nhave shown potential to interpret human intentions, yet their application to\nbiomolecular research remains nascent due to challenges including specialized\nknowledge requirements, multimodal data integration, and semantic alignment\nbetween natural language and biomolecules. To address these limitations, we\npresent InstructBioMol, a novel LLM designed to bridge natural language and\nbiomolecules through a comprehensive any-to-any alignment of natural language,\nmolecules, and proteins. This model can integrate multimodal biomolecules as\ninput, and enable researchers to articulate design goals in natural language,\nproviding biomolecular outputs that meet precise biological needs. Experimental\nresults demonstrate InstructBioMol can understand and design biomolecules\nfollowing human instructions. Notably, it can generate drug molecules with a\n10% improvement in binding affinity and design enzymes that achieve an ESP\nScore of 70.4, making it the only method to surpass the enzyme-substrate\ninteraction threshold of 60.0 recommended by the ESP developer. This highlights\nits potential to transform real-world biomolecular research.", "published": "2024-10-10 13:45:56", "link": "http://arxiv.org/abs/2410.07919v1", "categories": ["cs.CL", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "Disease Entity Recognition and Normalization is Improved with Large\n  Language Model Derived Synthetic Normalized Mentions", "abstract": "Background: Machine learning methods for clinical named entity recognition\nand entity normalization systems can utilize both labeled corpora and Knowledge\nGraphs (KGs) for learning. However, infrequently occurring concepts may have\nfew mentions in training corpora and lack detailed descriptions or synonyms,\neven in large KGs. For Disease Entity Recognition (DER) and Disease Entity\nNormalization (DEN), this can result in fewer high quality training examples\nrelative to the number of known diseases. Large Language Model (LLM) generation\nof synthetic training examples could improve performance in these information\nextraction tasks.\n  Methods: We fine-tuned a LLaMa-2 13B Chat LLM to generate a synthetic corpus\ncontaining normalized mentions of concepts from the Unified Medical Language\nSystem (UMLS) Disease Semantic Group. We measured overall and Out of\nDistribution (OOD) performance for DER and DEN, with and without synthetic data\naugmentation. We evaluated performance on 3 different disease corpora using 4\ndifferent data augmentation strategies, assessed using BioBERT for DER and\nSapBERT and KrissBERT for DEN.\n  Results: Our synthetic data yielded a substantial improvement for DEN, in all\n3 training corpora the top 1 accuracy of both SapBERT and KrissBERT improved by\n3-9 points in overall performance and by 20-55 points in OOD data. A small\nimprovement (1-2 points) was also seen for DER in overall performance, but only\none dataset showed OOD improvement.\n  Conclusion: LLM generation of normalized disease mentions can improve DEN\nrelative to normalization approaches that do not utilize LLMs to augment data\nwith synthetic mentions. Ablation studies indicate that performance gains for\nDEN were only partially attributable to improvements in OOD performance. The\nsame approach has only a limited ability to improve DER. We make our software\nand dataset publicly available.", "published": "2024-10-10 14:18:34", "link": "http://arxiv.org/abs/2410.07951v1", "categories": ["cs.CL", "cs.LG", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based\n  Verifiers", "abstract": "Recent advancements in test time compute, particularly through the use of\nverifier models, have significantly enhanced the reasoning capabilities of\nLarge Language Models (LLMs). This generator-verifier approach closely\nresembles the actor-critic framework in reinforcement learning (RL). However,\ncurrent verifier models in LLMs often rely on supervised fine-tuning without\ntemporal difference learning such as Q-learning. This paper introduces\nVerifierQ, a novel approach that integrates Offline Q-learning into LLM\nverifier models. We address three key challenges in applying Q-learning to\nLLMs: (1) handling utterance-level Markov Decision Processes (MDPs), (2)\nmanaging large action spaces, and (3) mitigating overestimation bias. VerifierQ\nintroduces a modified Bellman update for bounded Q-values, incorporates\nImplicit Q-learning (IQL) for efficient action space management, and integrates\na novel Conservative Q-learning (CQL) formulation for balanced Q-value\nestimation. Our method enables parallel Q-value computation and improving\ntraining efficiency. While recent work has explored RL techniques like MCTS for\ngenerators, VerifierQ is among the first to investigate the verifier (critic)\naspect in LLMs through Q-learning. This integration of RL principles into\nverifier models complements existing advancements in generator techniques,\npotentially enabling more robust and adaptive reasoning in LLMs. Experimental\nresults on mathematical reasoning tasks demonstrate VerifierQ's superior\nperformance compared to traditional supervised fine-tuning approaches, with\nimprovements in efficiency, accuracy and robustness. By enhancing the synergy\nbetween generation and evaluation capabilities, VerifierQ contributes to the\nongoing evolution of AI systems in addressing complex cognitive tasks across\nvarious domains.", "published": "2024-10-10 15:43:55", "link": "http://arxiv.org/abs/2410.08048v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Teaching-Inspired Integrated Prompting Framework: A Novel Approach for\n  Enhancing Reasoning in Large Language Models", "abstract": "Large Language Models (LLMs) exhibit impressive performance across various\ndomains but still struggle with arithmetic reasoning tasks. Recent work shows\nthe effectiveness of prompt design methods in enhancing reasoning capabilities.\nHowever, these approaches overlook crucial requirements for prior knowledge of\nspecific concepts, theorems, and tricks to tackle most arithmetic reasoning\nproblems successfully. To address this issue, we propose a novel and effective\nTeaching-Inspired Integrated Framework, which emulates the instructional\nprocess of a teacher guiding students. This method equips LLMs with essential\nconcepts, relevant theorems, and similar problems with analogous solution\napproaches, facilitating the enhancement of reasoning abilities. Additionally,\nwe introduce two new Chinese datasets, MathMC and MathToF, both with detailed\nexplanations and answers. Experiments are conducted on nine benchmarks which\ndemonstrates that our approach improves the reasoning accuracy of LLMs. With\nGPT-4 and our framework, we achieve new state-of-the-art performance on four\nmath benchmarks (AddSub, SVAMP, Math23K and AQuA) with accuracies of 98.2%\n(+3.3%), 93.9% (+0.2%), 94.3% (+7.2%) and 81.1% (+1.2%). Our data and code are\navailable at https://github.com/SallyTan13/Teaching-Inspired-Prompting.", "published": "2024-10-10 16:02:36", "link": "http://arxiv.org/abs/2410.08068v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An\n  Empirical Study Over Open-ended Question Answering", "abstract": "Recent works integrating Knowledge Graphs (KGs) have led to promising\nimprovements in enhancing the reasoning accuracy of Large Language Models\n(LLMs). However, current benchmarks focus mainly on closed-ended tasks, leaving\na gap in the assessment of more complex real-world scenarios. This gap has also\nobscured the evaluation of KGs' potential to mitigate the problem of\nhallucination in LLMs. To fill the gap, we introduce OKGQA, a new benchmark\nspecifically designed to assess LLMs enhanced with KGs under open-ended,\nreal-world question answering scenarios. OKGQA is designed to closely reflect\nthe complexities of practical applications using questions from different\ntypes, and incorporates specific metrics to measure both hallucination ratio\nand the enhancement in reasoning capabilities. To consider the scenario in\nwhich KGs may have varying levels of mistakes, we propose another benchmark\nvariant OKGQA-P to assess model performance when the semantics and structure of\nKGs are deliberately perturbed and contaminated. OKGQA aims to (1) explore\nwhether KGs can make LLMs more trustworthy in an open-ended setting, and (2)\nconduct a comparative analysis to shed light on method design. We believe that\nthis study can facilitate a more complete performance comparison and encourage\ncontinuous improvement in integrating KGs with LLMs to reduce hallucination.", "published": "2024-10-10 16:29:21", "link": "http://arxiv.org/abs/2410.08085v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based\n  Multi-Agent System", "abstract": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable\npotential in collaborative problem-solving, yet they still face critical\nchallenges: low communication efficiency, poor scalability, and a lack of\neffective parameter-updating optimization methods. We present Optima, a novel\nframework that addresses these issues by significantly enhancing both\ncommunication efficiency and task effectiveness in LLM-based MAS through LLM\ntraining. Optima employs an iterative generate, rank, select, and train\nparadigm with a reward function balancing task performance, token efficiency,\nand communication readability. We explore various RL algorithms, including\nSupervised Fine-Tuning, Direct Preference Optimization, and their hybrid\napproaches, providing insights into their effectiveness-efficiency trade-offs.\nWe integrate Monte Carlo Tree Search-inspired techniques for DPO data\ngeneration, treating conversation turns as tree nodes to explore diverse\ninteraction paths. Evaluated on common multi-agent tasks, including\ninformation-asymmetric question answering and complex reasoning, Optima shows\nconsistent and substantial improvements over single-agent baselines and vanilla\nMAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than\n10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's\nefficiency gains open new possibilities for leveraging inference-compute more\neffectively, leading to improved inference-time scaling laws. By addressing\nfundamental challenges in LLM-based MAS, Optima shows the potential towards\nscalable, efficient, and effective MAS\n(https://chenweize1998.github.io/optima-project-page).", "published": "2024-10-10 17:00:06", "link": "http://arxiv.org/abs/2410.08115v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Think Beyond Size: Adaptive Prompting for More Effective Reasoning", "abstract": "Pretrained large language models (LLMs) are increasingly utilized across a\nwide range of natural language processing (NLP) tasks due to their impressive\ncapabilities as few-shot learners. Recent techniques, such as chain-of-thought\n(CoT) prompting, have significantly advanced multi-step reasoning by\nintroducing step-by-step decomposition, achieving state-of-the-art results on\ncomplex reasoning benchmarks. However, these approaches often rely on static\nprompting templates that do not adapt to task complexity or errors during the\nreasoning process. In this work, we introduce Adaptive Prompting, a dynamic and\niterative framework designed to enhance reasoning by incorporating real-time\nadjustments to prompt structures and validation mechanisms.Experimental results\ndemonstrate that Adaptive Prompting significantly improves performance on\ndiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,\nMultiArith), logical reasoning and commonsense tasks, achieving substantial\naccuracy gains compared to static prompting baselines. By integrating guided\nprompts, intermediate validation, and self-corrective steps, our approach\nenables smaller models to achieve competitive performance with larger\ncounterparts, such as GPT-4, while maintaining computational efficiency. The\nframework achieves this without requiring fine-tuning or task-specific training\ndata, highlighting the untapped potential of iterative reasoning methods.", "published": "2024-10-10 17:14:36", "link": "http://arxiv.org/abs/2410.08130v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level\n  Memory", "abstract": "Large language models (LLMs) have achieved reasonable quality improvements in\nmachine translation (MT). However, most current research on MT-LLMs still faces\nsignificant challenges in maintaining translation consistency and accuracy when\nprocessing entire documents. In this paper, we introduce DelTA, a\nDocument-levEL Translation Agent designed to overcome these limitations. DelTA\nfeatures a multi-level memory structure that stores information across various\ngranularities and spans, including Proper Noun Records, Bilingual Summary,\nLong-Term Memory, and Short-Term Memory, which are continuously retrieved and\nupdated by auxiliary LLM-based components. Experimental results indicate that\nDelTA significantly outperforms strong baselines in terms of translation\nconsistency and quality across four open/closed-source LLMs and two\nrepresentative document translation datasets, achieving an increase in\nconsistency scores by up to 4.58 percentage points and in COMET scores by up to\n3.16 points on average. DelTA employs a sentence-by-sentence translation\nstrategy, ensuring no sentence omissions and offering a memory-efficient\nsolution compared to the mainstream method. Furthermore, DelTA improves pronoun\nand context-dependent translation accuracy, and the summary component of the\nagent also shows promise as a tool for query-based summarization tasks. The\ncode and data of our approach are released at\nhttps://github.com/YutongWang1216/DocMTAgent.", "published": "2024-10-10 17:30:09", "link": "http://arxiv.org/abs/2410.08143v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Insight Over Sight? Exploring the Vision-Knowledge Conflicts in\n  Multimodal LLMs", "abstract": "This paper explores the problem of commonsense-level vision-knowledge\nconflict in Multimodal Large Language Models (MLLMs), where visual information\ncontradicts model's internal commonsense knowledge (see Figure 1). To study\nthis issue, we introduce an automated pipeline, augmented with\nhuman-in-the-loop quality control, to establish a benchmark aimed at simulating\nand assessing the conflicts in MLLMs. Utilizing this pipeline, we have crafted\na diagnostic benchmark comprising 374 original images and 1,122 high-quality\nquestion-answer (QA) pairs. This benchmark covers two types of conflict target\nand three question difficulty levels, providing a thorough assessment tool.\nThrough this benchmark, we evaluate the conflict-resolution capabilities of\nnine representative MLLMs across various model families and find a noticeable\nover-reliance on textual queries. Drawing on these findings, we propose a novel\nprompting strategy, \"Focus-on-Vision\" (FoV), which markedly enhances MLLMs'\nability to favor visual data over conflicting textual knowledge. Our detailed\nanalysis and the newly proposed strategy significantly advance the\nunderstanding and mitigating of vision-knowledge conflicts in MLLMs. The data\nand code are made publicly available.", "published": "2024-10-10 17:31:17", "link": "http://arxiv.org/abs/2410.08145v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM\n  Reasoning", "abstract": "A promising approach for improving reasoning in large language models is to\nuse process reward models (PRMs). PRMs provide feedback at each step of a\nmulti-step reasoning trace, potentially improving credit assignment over\noutcome reward models (ORMs) that only provide feedback at the final step.\nHowever, collecting dense, per-step human labels is not scalable, and training\nPRMs from automatically-labeled data has thus far led to limited gains. To\nimprove a base policy by running search against a PRM or using it as dense\nrewards for reinforcement learning (RL), we ask: \"How should we design process\nrewards?\". Our key insight is that, to be effective, the process reward for a\nstep should measure progress: a change in the likelihood of producing a correct\nresponse in the future, before and after taking the step, corresponding to the\nnotion of step-level advantages in RL. Crucially, this progress should be\nmeasured under a prover policy distinct from the base policy. We theoretically\ncharacterize the set of good provers and our results show that optimizing\nprocess rewards from such provers improves exploration during test-time search\nand online RL. In fact, our characterization shows that weak prover policies\ncan substantially improve a stronger base policy, which we also observe\nempirically. We validate our claims by training process advantage verifiers\n(PAVs) to predict progress under such provers, and show that compared to ORMs,\ntest-time search against PAVs is $>8\\%$ more accurate, and $1.5-5\\times$ more\ncompute-efficient. Online RL with dense rewards from PAVs enables one of the\nfirst results with $5-6\\times$ gain in sample efficiency, and $>6\\%$ gain in\naccuracy, over ORMs.", "published": "2024-10-10 17:31:23", "link": "http://arxiv.org/abs/2410.08146v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "From Exploration to Mastery: Enabling LLMs to Master Tools via\n  Self-Driven Interactions", "abstract": "Tool learning enables Large Language Models (LLMs) to interact with external\nenvironments by invoking tools, serving as an effective strategy to mitigate\nthe limitations inherent in their pre-training data. In this process, tool\ndocumentation plays a crucial role by providing usage instructions for LLMs,\nthereby facilitating effective tool utilization. This paper concentrates on the\ncritical challenge of bridging the comprehension gap between LLMs and external\ntools due to the inadequacies and inaccuracies inherent in existing\nhuman-centric tool documentation. We propose a novel framework, DRAFT, aimed at\nDynamically Refining tool documentation through the Analysis of Feedback and\nTrials emanating from LLMs' interactions with external tools. This methodology\npivots on an innovative trial-and-error approach, consisting of three distinct\nlearning phases: experience gathering, learning from experience, and\ndocumentation rewriting, to iteratively enhance the tool documentation. This\nprocess is further optimized by implementing a diversity-promoting exploration\nstrategy to ensure explorative diversity and a tool-adaptive termination\nmechanism to prevent overfitting while enhancing efficiency. Extensive\nexperiments on multiple datasets demonstrate that DRAFT's iterative,\nfeedback-based refinement significantly ameliorates documentation quality,\nfostering a deeper comprehension and more effective utilization of tools by\nLLMs. Notably, our analysis reveals that the tool documentation refined via our\napproach demonstrates robust cross-model generalization capabilities.", "published": "2024-10-10 17:58:44", "link": "http://arxiv.org/abs/2410.08197v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training", "abstract": "In this paper, we focus on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nIn particular, we identify that existing pre-training strategies for monolithic\nMLLMs often suffer from unstable optimization or catastrophic forgetting. To\naddress this issue, our core idea is to embed a new visual parameter space into\na pre-trained LLM, thereby stably learning visual knowledge from noisy data\nwhile freezing the LLM. Based on this principle, we present Mono-InternVL, a\nnovel monolithic MLLM that seamlessly integrates a set of visual experts via a\nmultimodal mixture-of-experts structure. Moreover, we propose an innovative\npre-training strategy to maximize the visual capability of Mono-InternVL,\nnamely Endogenous Visual Pre-training (EViP). In particular, EViP is designed\nas a progressive learning process for visual experts, which aims to fully\nexploit the visual knowledge from noisy data to high-quality data. To validate\nour approach, we conduct extensive experiments on 16 benchmarks. Experimental\nresults confirm the superior performance of Mono-InternVL than existing\nmonolithic MLLMs on 13 of 16 multimodal benchmarks, e.g., +80 points over Emu3\non OCRBench. Compared to the modular baseline, i.e., InternVL-1.5,\nMono-InternVL still retains comparable multimodal performance while reducing up\nto 67% first token latency. Code and model are released at\nhttps://github.com/OpenGVLab/Mono-InternVL.", "published": "2024-10-10 17:59:22", "link": "http://arxiv.org/abs/2410.08202v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Increasing the Difficulty of Automatically Generated Questions via\n  Reinforcement Learning with Synthetic Preference", "abstract": "As the cultural heritage sector increasingly adopts technologies like\nRetrieval-Augmented Generation (RAG) to provide more personalised search\nexperiences and enable conversations with collections data, the demand for\nspecialised evaluation datasets has grown. While end-to-end system testing is\nessential, it's equally important to assess individual components. We target\nthe final, answering task, which is well-suited to Machine Reading\nComprehension (MRC). Although existing MRC datasets address general domains,\nthey lack the specificity needed for cultural heritage information.\nUnfortunately, the manual creation of such datasets is prohibitively expensive\nfor most heritage institutions. This paper presents a cost-effective approach\nfor generating domain-specific MRC datasets with increased difficulty using\nReinforcement Learning from Human Feedback (RLHF) from synthetic preference\ndata. Our method leverages the performance of existing question-answering\nmodels on a subset of SQuAD to create a difficulty metric, assuming that more\nchallenging questions are answered correctly less frequently. This research\ncontributes: (1) A methodology for increasing question difficulty using PPO and\nsynthetic data; (2) Empirical evidence of the method's effectiveness, including\nhuman evaluation; (3) An in-depth error analysis and study of emergent\nphenomena; and (4) An open-source codebase and set of three llama-2-chat\nadapters for reproducibility and adaptation.", "published": "2024-10-10 18:21:00", "link": "http://arxiv.org/abs/2410.08289v1", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F20 (Secondary)", "I.2.7; J.5"], "primary_category": "cs.CL"}
{"title": "Do You Know What You Are Talking About? Characterizing Query-Knowledge\n  Relevance For Reliable Retrieval Augmented Generation", "abstract": "Language models (LMs) are known to suffer from hallucinations and\nmisinformation. Retrieval augmented generation (RAG) that retrieves verifiable\ninformation from an external knowledge corpus to complement the parametric\nknowledge in LMs provides a tangible solution to these problems. However, the\ngeneration quality of RAG is highly dependent on the relevance between a user's\nquery and the retrieved documents. Inaccurate responses may be generated when\nthe query is outside of the scope of knowledge represented in the external\nknowledge corpus or if the information in the corpus is out-of-date. In this\nwork, we establish a statistical framework that assesses how well a query can\nbe answered by an RAG system by capturing the relevance of knowledge. We\nintroduce an online testing procedure that employs goodness-of-fit (GoF) tests\nto inspect the relevance of each user query to detect out-of-knowledge queries\nwith low knowledge relevance. Additionally, we develop an offline testing\nframework that examines a collection of user queries, aiming to detect\nsignificant shifts in the query distribution which indicates the knowledge\ncorpus is no longer sufficiently capable of supporting the interests of the\nusers. We demonstrate the capabilities of these strategies through a systematic\nevaluation on eight question-answering (QA) datasets, the results of which\nindicate that the new testing framework is an efficient solution to enhance the\nreliability of existing RAG systems.", "published": "2024-10-10 19:14:55", "link": "http://arxiv.org/abs/2410.08320v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Nonlinear second-order dynamics describe labial constriction\n  trajectories across languages and contexts", "abstract": "We investigate the dynamics of labial constriction trajectories during the\nproduction of /b/ and /m/ in English and Mandarin. We find that, across\nlanguages and contexts, the ratio of instantaneous displacement to\ninstantaneous velocity generally follows an exponential decay curve from\nmovement onset to movement offset. We formalize this empirical discovery in a\ndifferential equation and, in combination with an assumption of point attractor\ndynamics, derive a nonlinear second-order dynamical system describing labial\nconstriction trajectories. The equation has only two parameters, T and r. T\ncorresponds to the target state and r corresponds to movement rapidity. Thus,\neach of the parameters corresponds to a phonetically relevant dimension of\ncontrol. Nonlinear regression demonstrates that the model provides excellent\nfits to individual movement trajectories. Moreover, trajectories simulated from\nthe model qualitatively match empirical trajectories, and capture key kinematic\nvariables like duration, peak velocity, and time to achieve peak velocity. The\nmodel constitutes a proposal for the dynamics of individual articulatory\nmovements, and thus offers a novel foundation from which to understand\nadditional influences on articulatory kinematics like prosody, inter-movement\ncoordination, and stochastic noise.", "published": "2024-10-10 20:13:51", "link": "http://arxiv.org/abs/2410.08351v2", "categories": ["cs.CL", "nlin.AO"], "primary_category": "cs.CL"}
{"title": "The GUS Framework: Benchmarking Social Bias Classification with\n  Discriminative (Encoder-Only) and Generative (Decoder-Only) Language Models", "abstract": "The detection of social bias in text is a critical challenge, particularly\ndue to the limitations of binary classification methods. These methods often\noversimplify nuanced biases, leading to high emotional impact when content is\nmisclassified as either \"biased\" or \"fair.\" To address these shortcomings, we\npropose a more nuanced framework that focuses on three key linguistic\ncomponents underlying social bias: Generalizations, Unfairness, and Stereotypes\n(the GUS framework). The GUS framework employs a semi-automated approach to\ncreate a comprehensive synthetic dataset, which is then verified by humans to\nmaintain ethical standards. This dataset enables robust multi-label token\nclassification. Our methodology, which combines discriminative (encoder-only)\nmodels and generative (auto-regressive large language models), identifies\nbiased entities in text. Through extensive experiments, we demonstrate that\nencoder-only models are effective for this complex task, often outperforming\nstate-of-the-art methods, both in terms of macro and entity-wise F1-score and\nHamming loss. These findings can guide the choice of model for different use\ncases, highlighting the GUS framework's effectiveness in capturing explicit and\nimplicit biases across diverse contexts, and offering a pathway for future\nresearch and applications in various fields.", "published": "2024-10-10 21:51:22", "link": "http://arxiv.org/abs/2410.08388v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KV Prediction for Improved Time to First Token", "abstract": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .", "published": "2024-10-10 21:55:11", "link": "http://arxiv.org/abs/2410.08391v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Relieving Clinician Burden by Automatically Generating Progress\n  Notes using Interim Hospital Data", "abstract": "Regular documentation of progress notes is one of the main contributors to\nclinician burden. The abundance of structured chart information in medical\nrecords further exacerbates the burden, however, it also presents an\nopportunity to automate the generation of progress notes. In this paper, we\npropose a task to automate progress note generation using structured or tabular\ninformation present in electronic health records. To this end, we present a\nnovel framework and a large dataset, ChartPNG, for the task which contains\n$7089$ annotation instances (each having a pair of progress notes and interim\nstructured chart data) across $1616$ patients. We establish baselines on the\ndataset using large language models from general and biomedical domains. We\nperform both automated (where the best performing Biomistral model achieved a\nBERTScore F1 of $80.53$ and MEDCON score of $19.61$) and manual (where we found\nthat the model was able to leverage relevant structured data with $76.9\\%$\naccuracy) analyses to identify the challenges with the proposed task and\nopportunities for future research.", "published": "2024-10-10 02:03:27", "link": "http://arxiv.org/abs/2410.12845v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Accurate and Regret-aware Numerical Problem Solver for Tabular Question\n  Answering", "abstract": "Question answering on free-form tables (a.k.a. TableQA) is a challenging task\nbecause of the flexible structure and complex schema of tables. Recent studies\nuse Large Language Models (LLMs) for this task, exploiting their capability in\nunderstanding the questions and tabular data, which are typically given in\nnatural language and contain many textual fields, respectively. While this\napproach has shown promising results, it overlooks the challenges brought by\nnumerical values which are common in tabular data, and LLMs are known to\nstruggle with such values. We aim to address this issue, and we propose a model\nnamed TabLaP that uses LLMs as a planner rather than an answer generator. This\napproach exploits LLMs' capability in multi-step reasoning while leaving the\nactual numerical calculations to a Python interpreter for accurate calculation.\nRecognizing the inaccurate nature of LLMs, we further make a first attempt to\nquantify the trustworthiness of the answers produced by TabLaP, such that users\ncan use TabLaP in a regret-aware manner. Experimental results on two benchmark\ndatasets show that TabLaP is substantially more accurate than the\nstate-of-the-art models, improving the answer accuracy by 5.7% and 5.8% on the\ntwo datasets, respectively.", "published": "2024-10-10 05:34:00", "link": "http://arxiv.org/abs/2410.12846v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "abstract": "Prompt Tuning has been a popular Parameter-Efficient Fine-Tuning method\nattributed to its remarkable performance with few updated parameters on various\nlarge-scale pretrained Language Models (PLMs). Traditionally, each prompt has\nbeen considered indivisible and updated independently, leading the parameters\nincrease proportionally as prompt length grows. To address this issue, we\npropose Adaptive Codebook for Composite and Efficient Prompt Tuning (ACCEPT).\nIn our method, we refer to the concept of product quantization (PQ), allowing\nall soft prompts to share a set of learnable codebook vectors in each subspace,\nwith each prompt differentiated by a set of adaptive weights. We achieve the\nsuperior performance on 17 diverse natural language tasks including natural\nlanguage understanding (NLU) and question answering (QA) tasks by tuning only\n0.3% of parameters of the PLMs. Our approach also excels in few-shot and large\nmodel settings, highlighting its significant potential.", "published": "2024-10-10 07:48:53", "link": "http://arxiv.org/abs/2410.12847v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VibeCheck: Discover and Quantify Qualitative Differences in Large\n  Language Models", "abstract": "Large language models (LLMs) often exhibit subtle yet distinctive\ncharacteristics in their outputs that users intuitively recognize, but struggle\nto quantify. These \"vibes\" -- such as tone, formatting, or writing style --\ninfluence user preferences, yet traditional evaluations focus primarily on the\nsingular axis of correctness. We introduce VibeCheck, a system for\nautomatically comparing a pair of LLMs by discovering identifying traits of a\nmodel (vibes) that are well-defined, differentiating, and user-aligned.\nVibeCheck iteratively discovers vibes from model outputs and then utilizes a\npanel of LLM judges to quantitatively measure the utility of each vibe. We\nvalidate that the vibes generated by VibeCheck align with those found in human\ndiscovery and run VibeCheck on pairwise preference data from real-world user\nconversations with Llama-3-70b vs GPT-4. VibeCheck reveals that Llama has a\nfriendly, funny, and somewhat controversial vibe. These vibes predict model\nidentity with 80% accuracy and human preference with 61% accuracy. Lastly, we\nrun VibeCheck on a variety of models and tasks including summarization, math,\nand captioning to provide insight into differences in model behavior. VibeCheck\ndiscovers vibes like Command X prefers to add concrete intros and conclusions\nwhen summarizing in comparison to TNGL, Llama-405b often overexplains its\nthought process on math problems compared to GPT-4o, and GPT-4 prefers to focus\non the mood and emotions of the scene when captioning compared to\nGemini-1.5-Flash. Code can be found at https://github.com/lisadunlap/VibeCheck", "published": "2024-10-10 17:59:17", "link": "http://arxiv.org/abs/2410.12851v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Large Language Model GreekLegalRoBERTa", "abstract": "We develop four versions of GreekLegalRoBERTa, which are four large language\nmodels trained on Greek legal and nonlegal text. We show that our models\nsurpass the performance of GreekLegalBERT, Greek- LegalBERT-v2, and GreekBERT\nin two tasks involving Greek legal documents: named entity recognition and\nmulti-class legal topic classification. We view our work as a contribution to\nthe study of domain-specific NLP tasks in low-resource languages, like Greek,\nusing modern NLP techniques and methodologies.", "published": "2024-10-10 20:54:39", "link": "http://arxiv.org/abs/2410.12852v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TPO: Aligning Large Language Models with Multi-branch & Multi-step\n  Preference Trees", "abstract": "In the domain of complex reasoning tasks, such as mathematical reasoning,\nrecent advancements have proposed the use of Direct Preference Optimization\n(DPO) to suppress output of dispreferred responses, thereby enhancing the\nlong-chain reasoning capabilities of large language models (LLMs). To this end,\nthese studies employed LLMs to generate preference trees via Tree-of-thoughts\n(ToT) and sample the paired preference responses required by the DPO algorithm.\nHowever, the DPO algorithm based on binary preference optimization is unable to\nlearn multiple responses with varying degrees of preference/dispreference that\nprovided by the preference trees, resulting in incomplete preference learning.\nIn this work, we introduce Tree Preference Optimization (TPO), that does not\nsample paired preference responses from the preference tree; instead, it\ndirectly learns from the entire preference tree during the fine-tuning.\nSpecifically, TPO formulates the language model alignment as a Preference List\nRanking problem, where the policy can potentially learn more effectively from a\nranked preference list of responses given the prompt. In addition, to further\nassist LLMs in identifying discriminative steps within long-chain reasoning and\nincrease the relative reward margin in the preference list, TPO utilizes\nAdaptive Step Reward to adjust the reward values of each step in trajectory for\nperforming fine-grained preference optimization. We carry out extensive\nexperiments on mathematical reasoning tasks to evaluate TPO. The experimental\nresults indicate that TPO consistently outperforms DPO across five public large\nlanguage models on four datasets.", "published": "2024-10-10 22:22:05", "link": "http://arxiv.org/abs/2410.12854v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evolutionary Contrastive Distillation for Language Model Alignment", "abstract": "The ability of large language models (LLMs) to execute complex instructions\nis essential for their real-world applications. However, several recent studies\nindicate that LLMs struggle with challenging instructions. In this paper, we\npropose Evolutionary Contrastive Distillation (ECD), a novel method for\ngenerating high-quality synthetic preference data designed to enhance the\ncomplex instruction-following capability of language models. ECD generates data\nthat specifically illustrates the difference between a response that\nsuccessfully follows a set of complex instructions and a response that is\nhigh-quality, but nevertheless makes some subtle mistakes. This is done by\nprompting LLMs to progressively evolve simple instructions to more complex\ninstructions. When the complexity of an instruction is increased, the original\nsuccessful response to the original instruction becomes a \"hard negative\"\nresponse for the new instruction, mostly meeting requirements of the new\ninstruction, but barely missing one or two. By pairing a good response with\nsuch a hard negative response, and employing contrastive learning algorithms\nsuch as DPO, we improve language models' ability to follow complex\ninstructions. Empirically, we observe that our method yields a 7B model that\nexceeds the complex instruction-following performance of current SOTA 7B models\nand is competitive even with open-source 70B models.", "published": "2024-10-10 01:04:03", "link": "http://arxiv.org/abs/2410.07513v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DemoShapley: Valuation of Demonstrations for In-Context Learning", "abstract": "Large language models (LLMs) leveraging in-context learning (ICL) have set\nnew benchmarks in few-shot learning across various tasks without needing\ntask-specific fine-tuning. However, extensive research has demonstrated that\nthe effectiveness of ICL is significantly influenced by the selection and\nordering of demonstrations. Considering the critical role of demonstration\nselection in ICL, we introduce DemoShapley which is inspired by the Data\nShapley valuation theorem. This approach assesses the influence of individual\ndemonstration instances, distinguishing between those that contribute\npositively and those that may hinder performance. Our findings reveal that\nDemoShapley not only enhances model performance in terms of accuracy and\nfairness but also generalizes queries from domains distinct from those of the\nin-context demonstrations, highlighting its versatility and effectiveness in\noptimizing ICL demonstration selection. Last but not least, DemoShapley\ndemonstrates its ability to aid in identifying noisy data within the\ndemonstration set.", "published": "2024-10-10 01:35:03", "link": "http://arxiv.org/abs/2410.07523v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Upcycling Large Language Models into Mixture of Experts", "abstract": "Upcycling pre-trained dense language models into sparse mixture-of-experts\n(MoE) models is an efficient approach to increase the model capacity of already\ntrained models. However, optimal techniques for upcycling at scale remain\nunclear. In this work, we conduct an extensive study of upcycling methods and\nhyperparameters for billion-parameter scale language models. We propose a novel\n\"virtual group\" initialization scheme and weight scaling approach to enable\nupcycling into fine-grained MoE architectures. Through ablations, we find that\nupcycling outperforms continued dense model training. In addition, we show that\nsoftmax-then-topK expert routing improves over topK-then-softmax approach and\nhigher granularity MoEs can help improve accuracy. Finally, we upcycled\nNemotron-4 15B on 1T tokens and compared it to a continuously trained version\nof the same model on the same 1T tokens: the continuous trained model achieved\n65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer\ninsights and best practices to effectively leverage upcycling for building MoE\nlanguage models.", "published": "2024-10-10 01:36:03", "link": "http://arxiv.org/abs/2410.07524v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency", "abstract": "We introduce PLaMo-100B, a large-scale language model designed for Japanese\nproficiency. The model was trained from scratch using 2 trillion tokens, with\narchitecture such as QK Normalization and Z-Loss to ensure training stability\nduring the training process. Post-training techniques, including Supervised\nFine-Tuning and Direct Preference Optimization, were applied to refine the\nmodel's performance. Benchmark evaluations suggest that PLaMo-100B performs\nwell, particularly in Japanese-specific tasks, achieving results that are\ncompetitive with frontier models like GPT-4. The base model is available at\nhttps://huggingface.co/pfnet/plamo-100b.", "published": "2024-10-10 02:59:36", "link": "http://arxiv.org/abs/2410.07563v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Training Data of Large Language Models via Expectation\n  Maximization", "abstract": "The widespread deployment of large language models (LLMs) has led to\nimpressive advancements, yet information about their training data, a critical\nfactor in their performance, remains undisclosed. Membership inference attacks\n(MIAs) aim to determine whether a specific instance was part of a target\nmodel's training data. MIAs can offer insights into LLM outputs and help detect\nand address concerns such as data contamination and compliance with privacy and\ncopyright standards. However, applying MIAs to LLMs presents unique challenges\ndue to the massive scale of pre-training data and the ambiguous nature of\nmembership. Additionally, creating appropriate benchmarks to evaluate MIA\nmethods is not straightforward, as training and test data distributions are\noften unknown. In this paper, we introduce EM-MIA, a novel MIA method for LLMs\nthat iteratively refines membership scores and prefix scores via an\nexpectation-maximization algorithm, leveraging the duality that the estimates\nof these scores can be improved by each other. Membership scores and prefix\nscores assess how each instance is likely to be a member and discriminative as\na prefix, respectively. Our method achieves state-of-the-art results on the\nWikiMIA dataset. To further evaluate EM-MIA, we present OLMoMIA, a benchmark\nbuilt from OLMo resources, which allows us to control the difficulty of MIA\ntasks with varying degrees of overlap between training and test data\ndistributions. We believe that EM-MIA serves as a robust MIA method for LLMs\nand that OLMoMIA provides a valuable resource for comprehensively evaluating\nMIA approaches, thereby driving future research in this critical area.", "published": "2024-10-10 03:31:16", "link": "http://arxiv.org/abs/2410.07582v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Curriculum Expert Iteration for Reliable LLM Reasoning", "abstract": "Hallucinations (i.e., generating plausible but inaccurate content) and\nlaziness (i.e. excessive refusals or defaulting to \"I don't know\") persist as\nmajor challenges in LLM reasoning. Current efforts to reduce hallucinations\nprimarily focus on factual errors in knowledge-grounded tasks, often neglecting\nhallucinations related to faulty reasoning. Meanwhile, some approaches render\nLLMs overly conservative, limiting their problem-solving capabilities. To\nmitigate hallucination and laziness in reasoning tasks, we propose Automatic\nCurriculum Expert Iteration (Auto-CEI) to enhance LLM reasoning and align\nresponses to the model's capabilities--assertively answering within its limits\nand declining when tasks exceed them. In our method, Expert Iteration explores\nthe reasoning trajectories near the LLM policy, guiding incorrect paths back on\ntrack to reduce compounding errors and improve robustness; it also promotes\nappropriate \"I don't know\" responses after sufficient reasoning attempts. The\ncurriculum automatically adjusts rewards, incentivizing extended reasoning\nbefore acknowledging incapability, thereby pushing the limits of LLM reasoning\nand aligning its behaviour with these limits. We compare Auto-CEI with various\nSOTA baselines across logical reasoning, mathematics, and planning tasks, where\nAuto-CEI achieves superior alignment by effectively balancing assertiveness and\nconservativeness. The code is available at\nhttps://github.com/SalesforceAIResearch/Auto-CEI .", "published": "2024-10-10 05:43:07", "link": "http://arxiv.org/abs/2410.07627v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "$\\textit{Jump Your Steps}$: Optimizing Sampling Schedule of Discrete\n  Diffusion Models", "abstract": "Diffusion models have seen notable success in continuous domains, leading to\nthe development of discrete diffusion models (DDMs) for discrete variables.\nDespite recent advances, DDMs face the challenge of slow sampling speeds. While\nparallel sampling methods like $\\tau$-leaping accelerate this process, they\nintroduce $\\textit{Compounding Decoding Error}$ (CDE), where discrepancies\narise between the true distribution and the approximation from parallel token\ngeneration, leading to degraded sample quality. In this work, we present\n$\\textit{Jump Your Steps}$ (JYS), a novel approach that optimizes the\nallocation of discrete sampling timesteps by minimizing CDE without extra\ncomputational cost. More precisely, we derive a practical upper bound on CDE\nand propose an efficient algorithm for searching for the optimal sampling\nschedule. Extensive experiments across image, music, and text generation show\nthat JYS significantly improves sampling quality, establishing it as a\nversatile framework for enhancing DDM performance for fast sampling.", "published": "2024-10-10 09:44:25", "link": "http://arxiv.org/abs/2410.07761v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Rewriting Conversational Utterances with Instructed Large Language\n  Models", "abstract": "Many recent studies have shown the ability of large language models (LLMs) to\nachieve state-of-the-art performance on many NLP tasks, such as question\nanswering, text summarization, coding, and translation. In some cases, the\nresults provided by LLMs are on par with those of human experts. These models'\nmost disruptive innovation is their ability to perform tasks via zero-shot or\nfew-shot prompting. This capability has been successfully exploited to train\ninstructed LLMs, where reinforcement learning with human feedback is used to\nguide the model to follow the user's requests directly. In this paper, we\ninvestigate the ability of instructed LLMs to improve conversational search\neffectiveness by rewriting user questions in a conversational setting. We study\nwhich prompts provide the most informative rewritten utterances that lead to\nthe best retrieval performance. Reproducible experiments are conducted on\npublicly-available TREC CAST datasets. The results show that rewriting\nconversational utterances with instructed LLMs achieves significant\nimprovements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and\n11.5% in Recall@500 over state-of-the-art techniques.", "published": "2024-10-10 10:30:28", "link": "http://arxiv.org/abs/2410.07797v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mitigating Gender Bias in Code Large Language Models via Model Editing", "abstract": "In recent years, with the maturation of large language model (LLM) technology\nand the emergence of high-quality programming code datasets, researchers have\nbecome increasingly confident in addressing the challenges of program synthesis\nautomatically. However, since most of the training samples for LLMs are\nunscreened, it is inevitable that LLMs' performance may not align with\nreal-world scenarios, leading to the presence of social bias. To evaluate and\nquantify the gender bias in code LLMs, we propose a dataset named CodeGenBias\n(Gender Bias in the Code Generation) and an evaluation metric called FB-Score\n(Factual Bias Score) based on the actual gender distribution of correlative\nprofessions. With the help of CodeGenBias and FB-Score, we evaluate and analyze\nthe gender bias in eight mainstream Code LLMs. Previous work has demonstrated\nthat model editing methods that perform well in knowledge editing have the\npotential to mitigate social bias in LLMs. Therefore, we develop a model\nediting approach named MG-Editing (Multi-Granularity model Editing), which\nincludes the locating and editing phases. Our model editing method MG-Editing\ncan be applied at five different levels of model parameter granularity: full\nparameters level, layer level, module level, row level, and neuron level.\nExtensive experiments not only demonstrate that our MG-Editing can effectively\nmitigate the gender bias in code LLMs while maintaining their general code\ngeneration capabilities, but also showcase its excellent generalization. At the\nsame time, the experimental results show that, considering both the gender bias\nof the model and its general code generation capability, MG-Editing is most\neffective when applied at the row and neuron levels of granularity.", "published": "2024-10-10 11:11:32", "link": "http://arxiv.org/abs/2410.07820v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking\n  Suite for the EU Artificial Intelligence Act", "abstract": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards\nresponsible AI development, but lacks clear technical interpretation, making it\ndifficult to assess models' compliance. This work presents COMPL-AI, a\ncomprehensive framework consisting of (i) the first technical interpretation of\nthe EU AI Act, translating its broad regulatory requirements into measurable\ntechnical requirements, with the focus on large language models (LLMs), and\n(ii) an open-source Act-centered benchmarking suite, based on thorough\nsurveying and implementation of state-of-the-art LLM benchmarks. By evaluating\n12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in\nexisting models and benchmarks, particularly in areas like robustness, safety,\ndiversity, and fairness. This work highlights the need for a shift in focus\ntowards these aspects, encouraging balanced development of LLMs and more\ncomprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the\nfirst time demonstrates the possibilities and difficulties of bringing the\nAct's obligations to a more concrete, technical level. As such, our work can\nserve as a useful first step towards having actionable recommendations for\nmodel providers, and contributes to ongoing efforts of the EU to enable\napplication of the Act, such as the drafting of the GPAI Code of Practice.", "published": "2024-10-10 14:23:51", "link": "http://arxiv.org/abs/2410.07959v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets", "abstract": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.", "published": "2024-10-10 14:48:57", "link": "http://arxiv.org/abs/2410.07991v5", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Private Language Models via Truncated Laplacian Mechanism", "abstract": "Deep learning models for NLP tasks are prone to variants of privacy attacks.\nTo prevent privacy leakage, researchers have investigated word-level\nperturbations, relying on the formal guarantees of differential privacy (DP) in\nthe embedding space. However, many existing approaches either achieve\nunsatisfactory performance in the high privacy regime when using the Laplacian\nor Gaussian mechanism, or resort to weaker relaxations of DP that are inferior\nto the canonical DP in terms of privacy strength. This raises the question of\nwhether a new method for private word embedding can be designed to overcome\nthese limitations. In this paper, we propose a novel private embedding method\ncalled the high dimensional truncated Laplacian mechanism. Specifically, we\nintroduce a non-trivial extension of the truncated Laplacian mechanism, which\nwas previously only investigated in one-dimensional space cases. Theoretically,\nwe show that our method has a lower variance compared to the previous private\nword embedding methods. To further validate its effectiveness, we conduct\ncomprehensive experiments on private embedding and downstream tasks using three\ndatasets. Remarkably, even in the high privacy regime, our approach only incurs\na slight decrease in utility compared to the non-private scenario.", "published": "2024-10-10 15:25:02", "link": "http://arxiv.org/abs/2410.08027v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Closing the Loop: Learning to Generate Writing Feedback via Language\n  Model Simulated Student Revisions", "abstract": "Providing feedback is widely recognized as crucial for refining students'\nwriting skills. Recent advances in language models (LMs) have made it possible\nto automatically generate feedback that is actionable and well-aligned with\nhuman-specified attributes. However, it remains unclear whether the feedback\ngenerated by these models is truly effective in enhancing the quality of\nstudent revisions. Moreover, prompting LMs with a precise set of instructions\nto generate feedback is nontrivial due to the lack of consensus regarding the\nspecific attributes that can lead to improved revising performance. To address\nthese challenges, we propose PROF that PROduces Feedback via learning from LM\nsimulated student revisions. PROF aims to iteratively optimize the feedback\ngenerator by directly maximizing the effectiveness of students' overall\nrevising performance as simulated by LMs. Focusing on an economic essay\nassignment, we empirically test the efficacy of PROF and observe that our\napproach not only surpasses a variety of baseline methods in effectiveness of\nimproving students' writing but also demonstrates enhanced pedagogical values,\neven though it was not explicitly trained for this aspect.", "published": "2024-10-10 15:52:48", "link": "http://arxiv.org/abs/2410.08058v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Packing Analysis: Packing Is More Appropriate for Large Models or\n  Datasets in Supervised Fine-tuning", "abstract": "Packing, initially utilized in the pre-training phase, is an optimization\ntechnique designed to maximize hardware resource efficiency by combining\ndifferent training sequences to fit the model's maximum input length. Although\nit has demonstrated effectiveness during pre-training, there remains a lack of\ncomprehensive analysis for the supervised fine-tuning (SFT) stage on the\nfollowing points: (1) whether packing can effectively enhance training\nefficiency while maintaining performance, (2) the suitable size of the model\nand dataset for fine-tuning with the packing method, and (3) whether packing\nunrelated or related training samples might cause the model to either\nexcessively disregard or over-rely on the context.\n  In this paper, we perform extensive comparisons between SFT methods using\npadding and packing, covering SFT datasets ranging from 69K to 1.2M and models\nfrom 8B to 70B. This provides the first comprehensive analysis of the\nadvantages and limitations of packing versus padding, as well as practical\nconsiderations for implementing packing in various training scenarios. Our\nanalysis covers various benchmarks, including knowledge, reasoning, and coding,\nas well as GPT-based evaluations, time efficiency, and other fine-tuning\nparameters. We also open-source our code for fine-tuning and evaluation and\nprovide checkpoints fine-tuned on datasets of different sizes, aiming to\nadvance future research on packing methods. Code is available at:\nhttps://github.com/ShuheWang1998/Packing-Analysis?tab=readme-ov-file.", "published": "2024-10-10 16:25:34", "link": "http://arxiv.org/abs/2410.08081v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Closer Look at Machine Unlearning for Large Language Models", "abstract": "Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.", "published": "2024-10-10 16:56:05", "link": "http://arxiv.org/abs/2410.08109v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust AI-Generated Text Detection by Restricted Embeddings", "abstract": "Growing amount and quality of AI-generated texts makes detecting such content\nmore difficult. In most real-world scenarios, the domain (style and topic) of\ngenerated data and the generator model are not known in advance. In this work,\nwe focus on the robustness of classifier-based detectors of AI-generated text,\nnamely their ability to transfer to unseen generators or semantic domains. We\ninvestigate the geometry of the embedding space of Transformer-based text\nencoders and show that clearing out harmful linear subspaces helps to train a\nrobust classifier, ignoring domain-specific spurious features. We investigate\nseveral subspace decomposition and feature selection strategies and achieve\nsignificant improvements over state of the art methods in cross-domain and\ncross-generator transfer. Our best approaches for head-wise and\ncoordinate-based subspace removal increase the mean out-of-distribution (OOD)\nclassification score by up to 9% and 14% in particular setups for RoBERTa and\nBERT embeddings respectively. We release our code and data:\nhttps://github.com/SilverSolver/RobustATD", "published": "2024-10-10 16:58:42", "link": "http://arxiv.org/abs/2410.08113v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Mars: Situated Inductive Reasoning in an Open-World Environment", "abstract": "Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way.", "published": "2024-10-10 17:10:34", "link": "http://arxiv.org/abs/2410.08126v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks", "abstract": "Current LLM benchmarks focus on evaluating models' memory of facts and\nsemantic relations, primarily assessing semantic aspects of long-term memory.\nHowever, in humans, long-term memory also includes episodic memory, which links\nmemories to their contexts, such as the time and place they occurred. The\nability to contextualize memories is crucial for many cognitive tasks and\neveryday functions. This form of memory has not been evaluated in LLMs with\nexisting benchmarks. To address the gap in evaluating memory in LLMs, we\nintroduce Sequence Order Recall Tasks (SORT), which we adapt from tasks used to\nstudy episodic memory in cognitive psychology. SORT requires LLMs to recall the\ncorrect order of text segments, and provides a general framework that is both\neasily extendable and does not require any additional annotations. We present\nan initial evaluation dataset, Book-SORT, comprising 36k pairs of segments\nextracted from 9 books recently added to the public domain. Based on a human\nexperiment with 155 participants, we show that humans can recall sequence order\nbased on long-term memory of a book. We find that models can perform the task\nwith high accuracy when relevant text is given in-context during the SORT\nevaluation. However, when presented with the book text only during training,\nLLMs' performance on SORT falls short. By allowing to evaluate more aspects of\nmemory, we believe that SORT will aid in the emerging development of\nmemory-augmented models.", "published": "2024-10-10 17:17:38", "link": "http://arxiv.org/abs/2410.08133v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Agent S: An Open Agentic Framework that Uses Computers Like a Human", "abstract": "We present Agent S, an open agentic framework that enables autonomous\ninteraction with computers through a Graphical User Interface (GUI), aimed at\ntransforming human-computer interaction by automating complex, multi-step\ntasks. Agent S aims to address three key challenges in automating computer\ntasks: acquiring domain-specific knowledge, planning over long task horizons,\nand handling dynamic, non-uniform interfaces. To this end, Agent S introduces\nexperience-augmented hierarchical planning, which learns from external\nknowledge search and internal experience retrieval at multiple levels,\nfacilitating efficient task planning and subtask execution. In addition, it\nemploys an Agent-Computer Interface (ACI) to better elicit the reasoning and\ncontrol capabilities of GUI agents based on Multimodal Large Language Models\n(MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the\nbaseline by 9.37% on success rate (an 83.6% relative improvement) and achieves\na new state-of-the-art. Comprehensive analysis highlights the effectiveness of\nindividual components and provides insights for future improvements.\nFurthermore, Agent S demonstrates broad generalizability to different operating\nsystems on a newly-released WindowsAgentArena benchmark. Code available at\nhttps://github.com/simular-ai/Agent-S.", "published": "2024-10-10 17:43:51", "link": "http://arxiv.org/abs/2410.08164v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Sample then Identify: A General Framework for Risk Control and\n  Assessment in Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) exhibit promising advancements\nacross various tasks, yet they still encounter significant trustworthiness\nissues. Prior studies apply Split Conformal Prediction (SCP) in language\nmodeling to construct prediction sets with statistical guarantees. However,\nthese methods typically rely on internal model logits or are restricted to\nmultiple-choice settings, which hampers their generalizability and adaptability\nin dynamic, open-ended environments. In this paper, we introduce TRON, a\ntwo-step framework for risk control and assessment, applicable to any MLLM that\nsupports sampling in both open-ended and closed-ended scenarios. TRON comprises\ntwo main components: (1) a novel conformal score to sample response sets of\nminimum size, and (2) a nonconformity score to identify high-quality responses\nbased on self-consistency theory, controlling the error rates by two specific\nrisk levels. Furthermore, we investigate semantic redundancy in prediction sets\nwithin open-ended contexts for the first time, leading to a promising\nevaluation metric for MLLMs based on average set size. Our comprehensive\nexperiments across four Video Question-Answering (VideoQA) datasets utilizing\neight MLLMs show that TRON achieves desired error rates bounded by two\nuser-specified risk levels. Additionally, deduplicated prediction sets maintain\nadaptiveness while being more efficient and stable for risk assessment under\ndifferent risk levels.", "published": "2024-10-10 17:50:42", "link": "http://arxiv.org/abs/2410.08174v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal\n  Models", "abstract": "Existing multimodal retrieval benchmarks primarily focus on evaluating\nwhether models can retrieve and utilize external textual knowledge for question\nanswering. However, there are scenarios where retrieving visual information is\neither more beneficial or easier to access than textual data. In this paper, we\nintroduce a multimodal retrieval-augmented generation benchmark, MRAG-Bench, in\nwhich we systematically identify and categorize scenarios where visually\naugmented knowledge is better than textual knowledge, for instance, more images\nfrom varying viewpoints. MRAG-Bench consists of 16,130 images and 1,353\nhuman-annotated multiple-choice questions across 9 distinct scenarios. With\nMRAG-Bench, we conduct an evaluation of 10 open-source and 4 proprietary large\nvision-language models (LVLMs). Our results show that all LVLMs exhibit greater\nimprovements when augmented with images compared to textual knowledge,\nconfirming that MRAG-Bench is vision-centric. Additionally, we conduct\nextensive analysis with MRAG-Bench, which offers valuable insights into\nretrieval-augmented LVLMs. Notably, the top-performing model, GPT-4o, faces\nchallenges in effectively leveraging retrieved knowledge, achieving only a\n5.82% improvement with ground-truth information, in contrast to a 33.16%\nimprovement observed in human participants. These findings highlight the\nimportance of MRAG-Bench in encouraging the community to enhance LVLMs' ability\nto utilize retrieved visual knowledge more effectively.", "published": "2024-10-10 17:55:02", "link": "http://arxiv.org/abs/2410.08182v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MathCoder2: Better Math Reasoning from Continued Pretraining on\n  Model-translated Mathematical Code", "abstract": "Code has been shown to be effective in enhancing the mathematical reasoning\nabilities of large language models due to its precision and accuracy. Previous\nworks involving continued mathematical pretraining often include code that\nutilizes math-related packages, which are primarily designed for fields such as\nengineering, machine learning, signal processing, or module testing, rather\nthan being directly focused on mathematical reasoning. In this paper, we\nintroduce a novel method for generating mathematical code accompanied with\ncorresponding reasoning steps for continued pretraining. Our approach begins\nwith the construction of a high-quality mathematical continued pretraining\ndataset by incorporating math-related web data, code using mathematical\npackages, math textbooks, and synthetic data. Next, we construct reasoning\nsteps by extracting LaTeX expressions, the conditions needed for the\nexpressions, and the results of the expressions from the previously collected\ndataset. Based on this extracted information, we generate corresponding code to\naccurately capture the mathematical reasoning process. Appending the generated\ncode to each reasoning step results in data consisting of paired natural\nlanguage reasoning steps and their corresponding code. Combining this data with\nthe original dataset results in a 19.2B-token high-performing mathematical\npretraining corpus, which we name MathCode-Pile. Training several popular base\nmodels with this corpus significantly improves their mathematical abilities,\nleading to the creation of the MathCoder2 family of models. All of our data\nprocessing and training code is open-sourced, ensuring full transparency and\neasy reproducibility of the entire data collection and training pipeline. The\ncode is released at https://github.com/mathllm/MathCoder2 .", "published": "2024-10-10 17:58:40", "link": "http://arxiv.org/abs/2410.08196v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts", "abstract": "Large-scale vision-language pre-trained (VLP) models (e.g., CLIP) are\nrenowned for their versatility, as they can be applied to diverse applications\nin a zero-shot setup. However, when these models are used in specific domains,\ntheir performance often falls short due to domain gaps or the\nunder-representation of these domains in the training data. While fine-tuning\nVLP models on custom datasets with human-annotated labels can address this\nissue, annotating even a small-scale dataset (e.g., 100k samples) can be an\nexpensive endeavor, often requiring expert annotators if the task is complex.\nTo address these challenges, we propose LatteCLIP, an unsupervised method for\nfine-tuning CLIP models on classification with known class names in custom\ndomains, without relying on human annotations. Our method leverages Large\nMultimodal Models (LMMs) to generate expressive textual descriptions for both\nindividual images and groups of images. These provide additional contextual\ninformation to guide the fine-tuning process in the custom domains. Since\nLMM-generated descriptions are prone to hallucination or missing details, we\nintroduce a novel strategy to distill only the useful information and stabilize\nthe training. Specifically, we learn rich per-class prototype representations\nfrom noisy generated texts and dual pseudo-labels. Our experiments on 10\ndomain-specific datasets show that LatteCLIP outperforms pre-trained zero-shot\nmethods by an average improvement of +4.74 points in top-1 accuracy and other\nstate-of-the-art unsupervised methods by +3.45 points.", "published": "2024-10-10 17:59:59", "link": "http://arxiv.org/abs/2410.08211v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploring ASR-Based Wav2Vec2 for Automated Speech Disorder Assessment:\n  Insights and Analysis", "abstract": "With the rise of SSL and ASR technologies, the Wav2Vec2 ASR-based model has\nbeen fine-tuned for automated speech disorder quality assessment tasks,\nyielding impressive results and setting a new baseline for Head and Neck Cancer\nspeech contexts. This demonstrates that the ASR dimension from Wav2Vec2 closely\naligns with assessment dimensions. Despite its effectiveness, this system\nremains a black box with no clear interpretation of the connection between the\nmodel ASR dimension and clinical assessments. This paper presents the first\nanalysis of this baseline model for speech quality assessment, focusing on\nintelligibility and severity tasks. We conduct a layer-wise analysis to\nidentify key layers and compare different SSL and ASR Wav2Vec2 models based on\npre-trained data. Additionally, post-hoc XAI methods, including Canonical\nCorrelation Analysis (CCA) and visualization techniques, are used to track\nmodel evolution and visualize embeddings for enhanced interpretability.", "published": "2024-10-10 13:12:17", "link": "http://arxiv.org/abs/2410.08250v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Privately Learning from Graphs with Applications in Fine-tuning Large\n  Language Models", "abstract": "Graphs offer unique insights into relationships and interactions between\nentities, complementing data modalities like text, images, and videos. By\nincorporating relational information from graph data, AI models can extend\ntheir capabilities beyond traditional tasks. However, relational data in\nsensitive domains such as finance and healthcare often contain private\ninformation, making privacy preservation crucial. Existing privacy-preserving\nmethods, such as DP-SGD, which rely on gradient decoupling assumptions, are not\nwell-suited for relational learning due to the inherent dependencies between\ncoupled training samples. To address this challenge, we propose a\nprivacy-preserving relational learning pipeline that decouples dependencies in\nsampled relations during training, ensuring differential privacy through a\ntailored application of DP-SGD. We apply this method to fine-tune large\nlanguage models (LLMs) on sensitive graph data, and tackle the associated\ncomputational complexities. Our approach is evaluated on LLMs of varying sizes\n(e.g., BERT, Llama2) using real-world relational data from four text-attributed\ngraphs. The results demonstrate significant improvements in relational learning\ntasks, all while maintaining robust privacy guarantees during training.\nAdditionally, we explore the trade-offs between privacy, utility, and\ncomputational efficiency, offering insights into the practical deployment of\nour approach. Code is available at https://github.com/Graph-COM/PvGaLM.", "published": "2024-10-10 18:38:38", "link": "http://arxiv.org/abs/2410.08299v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "HyperDPO: Conditioned One-Shot Multi-Objective Fine-Tuning Framework", "abstract": "In LLM alignment and many other ML applications, one often faces the\nMulti-Objective Fine-Tuning (MOFT) problem, i.e. fine-tuning an existing model\nwith datasets labeled w.r.t. different objectives simultaneously. To address\nthe challenge, we propose the HyperDPO framework, a conditioned one-shot\nfine-tuning approach that extends the Direct Preference Optimization (DPO)\ntechnique, originally developed for efficient LLM alignment with preference\ndata, to accommodate the MOFT settings. By substituting the Bradley-Terry-Luce\nmodel in DPO with the Plackett-Luce model, our framework is capable of handling\na wide range of MOFT tasks that involve listwise ranking datasets. Compared\nwith previous approaches, HyperDPO enjoys an efficient one-shot training\nprocess for profiling the Pareto front of auxiliary objectives, and offers\npost-training control over trade-offs. Additionally, we propose a novel Hyper\nPrompt Tuning design, that conveys continuous importance weight across\nobjectives to transformer-based models without altering their architecture, and\ninvestigate the potential of temperature-conditioned networks for enhancing the\nflexibility of post-training control. We demonstrate the effectiveness and\nefficiency of the HyperDPO framework through its applications to various tasks,\nincluding Learning-to-Rank (LTR) and LLM alignment, highlighting its viability\nfor large-scale ML deployments.", "published": "2024-10-10 19:06:39", "link": "http://arxiv.org/abs/2410.08316v2", "categories": ["cs.LG", "cs.CL", "math.OC"], "primary_category": "cs.LG"}
{"title": "Agents Thinking Fast and Slow: A Talker-Reasoner Architecture", "abstract": "Large language models have enabled agents of all kinds to interact with users\nthrough natural conversation. Consequently, agents now have two jobs:\nconversing and planning/reasoning. Their conversational responses must be\ninformed by all available information, and their actions must help to achieve\ngoals. This dichotomy between conversing with the user and doing multi-step\nreasoning and planning can be seen as analogous to the human systems of\n\"thinking fast and slow\" as introduced by Kahneman. Our approach is comprised\nof a \"Talker\" agent (System 1) that is fast and intuitive, and tasked with\nsynthesizing the conversational response; and a \"Reasoner\" agent (System 2)\nthat is slower, more deliberative, and more logical, and is tasked with\nmulti-step reasoning and planning, calling tools, performing actions in the\nworld, and thereby producing the new agent state. We describe the new\nTalker-Reasoner architecture and discuss its advantages, including modularity\nand decreased latency. We ground the discussion in the context of a sleep\ncoaching agent, in order to demonstrate real-world relevance.", "published": "2024-10-10 19:31:35", "link": "http://arxiv.org/abs/2410.08328v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Exploring Natural Language-Based Strategies for Efficient Number\n  Learning in Children through Reinforcement Learning", "abstract": "This paper investigates how children learn numbers using the framework of\nreinforcement learning (RL), with a focus on the impact of language\ninstructions. The motivation for using reinforcement learning stems from its\nparallels with psychological learning theories in controlled environments. By\nusing state of the art deep reinforcement learning models, we simulate and\nanalyze the effects of various forms of language instructions on number\nacquisition. Our findings indicate that certain linguistic structures more\neffectively improve numerical comprehension in RL agents. Additionally, our\nmodel predicts optimal sequences for presenting numbers to RL agents which\nenhance their speed of learning. This research provides valuable insights into\nthe interplay between language and numerical cognition, with implications for\nboth educational strategies and the development of artificial intelligence\nsystems designed to support early childhood learning.", "published": "2024-10-10 19:49:13", "link": "http://arxiv.org/abs/2410.08334v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Revealing COVID-19's Social Dynamics: Diachronic Semantic Analysis of\n  Vaccine and Symptom Discourse on Twitter", "abstract": "Social media is recognized as an important source for deriving insights into\npublic opinion dynamics and social impacts due to the vast textual data\ngenerated daily and the 'unconstrained' behavior of people interacting on these\nplatforms. However, such analyses prove challenging due to the semantic shift\nphenomenon, where word meanings evolve over time. This paper proposes an\nunsupervised dynamic word embedding method to capture longitudinal semantic\nshifts in social media data without predefined anchor words. The method\nleverages word co-occurrence statistics and dynamic updating to adapt\nembeddings over time, addressing the challenges of data sparseness, imbalanced\ndistributions, and synergistic semantic effects. Evaluated on a large COVID-19\nTwitter dataset, the method reveals semantic evolution patterns of vaccine- and\nsymptom-related entities across different pandemic stages, and their potential\ncorrelations with real-world statistics. Our key contributions include the\ndynamic embedding technique, empirical analysis of COVID-19 semantic shifts,\nand discussions on enhancing semantic shift modeling for computational social\nscience research. This study enables capturing longitudinal semantic dynamics\non social media to understand public discourse and collective phenomena.", "published": "2024-10-10 20:15:28", "link": "http://arxiv.org/abs/2410.08352v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path\n  from Averaging to Automation", "abstract": "By merging models, AI systems can combine the distinct strengths of separate\nlanguage models, achieving a balance between multiple capabilities without\nrequiring substantial retraining. However, the integration process can be\nintricate due to differences in training methods and fine-tuning, typically\nnecessitating specialized knowledge and repeated refinement. This paper\nexplores model merging techniques across a spectrum of complexity, examining\nwhere automated methods like evolutionary strategies stand compared to\nhyperparameter-driven approaches such as DARE, TIES-Merging and simpler methods\nlike Model Soups. In addition, we introduce Differentiable Adaptive Merging\n(DAM), an efficient, adaptive merging approach as an alternative to\nevolutionary merging that optimizes model integration through scaling\ncoefficients, minimizing computational demands. Our findings reveal that even\nsimple averaging methods, like Model Soups, perform competitively when model\nsimilarity is high, underscoring each technique's unique strengths and\nlimitations. We open-sourced DAM, including the implementation code and\nexperiment pipeline, on GitHub: https://github.com/arcee-ai/DAM.", "published": "2024-10-10 20:58:29", "link": "http://arxiv.org/abs/2410.08371v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Effects of Hallucinations in Synthetic Training Data for Relation\n  Extraction", "abstract": "Relation extraction is crucial for constructing knowledge graphs, with large\nhigh-quality datasets serving as the foundation for training, fine-tuning, and\nevaluating models. Generative data augmentation (GDA) is a common approach to\nexpand such datasets. However, this approach often introduces hallucinations,\nsuch as spurious facts, whose impact on relation extraction remains\nunderexplored. In this paper, we examine the effects of hallucinations on the\nperformance of relation extraction on the document and sentence levels. Our\nempirical study reveals that hallucinations considerably compromise the ability\nof models to extract relations from text, with recall reductions between 19.1%\nand 39.2%. We identify that relevant hallucinations impair the model's\nperformance, while irrelevant hallucinations have a minimal impact.\nAdditionally, we develop methods for the detection of hallucinations to improve\ndata quality and model performance. Our approaches successfully classify texts\nas either 'hallucinated' or 'clean,' achieving high F1-scores of 83.8% and\n92.2%. These methods not only assist in removing hallucinations but also help\nin estimating their prevalence within datasets, which is crucial for selecting\nhigh-quality data. Overall, our work confirms the profound impact of relevant\nhallucinations on the effectiveness of relation extraction models.", "published": "2024-10-10 22:00:16", "link": "http://arxiv.org/abs/2410.08393v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "HLM-Cite: Hybrid Language Model Workflow for Text-based Scientific\n  Citation Prediction", "abstract": "Citation networks are critical in modern science, and predicting which\nprevious papers (candidates) will a new paper (query) cite is a critical\nproblem. However, the roles of a paper's citations vary significantly, ranging\nfrom foundational knowledge basis to superficial contexts. Distinguishing these\nroles requires a deeper understanding of the logical relationships among\npapers, beyond simple edges in citation networks. The emergence of LLMs with\ntextual reasoning capabilities offers new possibilities for discerning these\nrelationships, but there are two major challenges. First, in practice, a new\npaper may select its citations from gigantic existing papers, where the texts\nexceed the context length of LLMs. Second, logical relationships between papers\nare implicit, and directly prompting an LLM to predict citations may result in\nsurface-level textual similarities rather than the deeper logical reasoning. In\nthis paper, we introduce the novel concept of core citation, which identifies\nthe critical references that go beyond superficial mentions. Thereby, we\nelevate the citation prediction task from a simple binary classification to\ndistinguishing core citations from both superficial citations and\nnon-citations. To address this, we propose $\\textbf{HLM-Cite}$, a\n$\\textbf{H}$ybrid $\\textbf{L}$anguage $\\textbf{M}$odel workflow for citation\nprediction, which combines embedding and generative LMs. We design a curriculum\nfinetune procedure to adapt a pretrained text embedding model to coarsely\nretrieve high-likelihood core citations from vast candidates and then design an\nLLM agentic workflow to rank the retrieved papers through one-shot reasoning,\nrevealing the implicit relationships among papers. With the pipeline, we can\nscale the candidate sets to 100K papers. We evaluate HLM-Cite across 19\nscientific fields, demonstrating a 17.6% performance improvement comparing SOTA\nmethods.", "published": "2024-10-10 10:46:06", "link": "http://arxiv.org/abs/2410.09112v1", "categories": ["cs.DL", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.DL"}
{"title": "Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent\n  Approach for Enhanced Compliance with Prompt Instructions", "abstract": "Patients with schizophrenia often present with cognitive impairments that may\nhinder their ability to learn about their condition. These individuals could\nbenefit greatly from education platforms that leverage the adaptability of\nLarge Language Models (LLMs) such as GPT-4. While LLMs have the potential to\nmake topical mental health information more accessible and engaging, their\nblack-box nature raises concerns about ethics and safety. Prompting offers a\nway to produce semi-scripted chatbots with responses anchored in instructions\nand validated information, but prompt-engineered chatbots may drift from their\nintended identity as the conversation progresses. We propose a Critical\nAnalysis Filter for achieving better control over chatbot behavior. In this\nsystem, a team of prompted LLM agents are prompt-engineered to critically\nanalyze and refine the chatbot's response and deliver real-time feedback to the\nchatbot. To test this approach, we develop an informational schizophrenia\nchatbot and converse with it (with the filter deactivated) until it oversteps\nits scope. Once drift has been observed, AI-agents are used to automatically\ngenerate sample conversations in which the chatbot is being enticed to talk\nabout out-of-bounds topics. We manually assign to each response a compliance\nscore that quantifies the chatbot's compliance to its instructions;\nspecifically the rules about accurately conveying sources and being transparent\nabout limitations. Activating the Critical Analysis Filter resulted in an\nacceptable compliance score (>=2) in 67.0% of responses, compared to only 8.7%\nwhen the filter was deactivated. These results suggest that a self-reflection\nlayer could enable LLMs to be used effectively and safely in mental health\nplatforms, maintaining adaptability while reliably limiting their scope to\nappropriate use cases.", "published": "2024-10-10 09:49:24", "link": "http://arxiv.org/abs/2410.12848v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "RecurFormer: Not All Transformer Heads Need Self-Attention", "abstract": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs.", "published": "2024-10-10 15:24:12", "link": "http://arxiv.org/abs/2410.12850v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Diversity of Thought Elicits Stronger Reasoning Capabilities in\n  Multi-Agent Debate Frameworks", "abstract": "Large language models (LLMs) excel in natural language generation but often\nconfidently produce incorrect responses, especially in tasks like mathematical\nreasoning. Chain-of-thought prompting, self-verification, and multi-agent\ndebate are among the strategies proposed to improve the reasoning and factual\naccuracy of LLMs. Building on Du et al.'s multi-agent debate framework, we find\nthat multi-agent debate helps at any model scale, and that diversity of thought\nelicits stronger reasoning in debating LLMs. Across various model sizes,\nperformance on mathematical reasoning tasks benefits most when diverse trained\nmodels are used. Remarkably, after 4 rounds of debate, a diverse set of\nmedium-capacity models (Gemini-Pro, Mixtral 7BX8, and PaLM 2-M) outperforms\nGPT-4 on the GSM-8K benchmark, scoring 91% accuracy. By comparison, when 3\ninstances of Gemini-Pro are used, performance only reaches 82%. Finally, this\ndiverse set of medium-capacity models sets a new state-of-the-art performance\non the ASDiv benchmark (94%). These results underscore the idea that the future\nof AI is agentic, with diverse cooperating agents yielding emergent\ncapabilities beyond even the most powerful individual models.", "published": "2024-10-10 21:59:01", "link": "http://arxiv.org/abs/2410.12853v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Full-Rank No More: Low-Rank Weight Training for Modern Speech\n  Recognition Models", "abstract": "This paper investigates the under-explored area of low-rank weight training\nfor large-scale Conformer-based speech recognition models from scratch. Our\nstudy demonstrates the viability of this training paradigm for such models,\nyielding several notable findings. Firstly, we discover that applying a\nlow-rank structure exclusively to the attention modules can unexpectedly\nenhance performance, even with a significant rank reduction of 12%. In\ncontrast, feed-forward layers present greater challenges, as they begin to\nexhibit performance degradation with a moderate 50% rank reduction.\nFurthermore, we find that both initialization and layer-wise rank assignment\nplay critical roles in successful low-rank training. Specifically, employing\nSVD initialization and linear layer-wise rank mapping significantly boosts the\nefficacy of low-rank weight training. Building on these insights, we introduce\nthe Low-Rank Speech Model from Scratch (LR-SMS), an approach that achieves\nperformance parity with full-rank training while delivering substantial\nreductions in parameters count (by at least 2x), and training time speedups (by\n1.3x for ASR and 1.15x for AVSR).", "published": "2024-10-10 09:58:35", "link": "http://arxiv.org/abs/2410.07771v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Benchmarking Agentic Workflow Generation", "abstract": "Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorfBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorfEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset are available at\nhttps://github.com/zjunlp/WorfBench.", "published": "2024-10-10 12:41:19", "link": "http://arxiv.org/abs/2410.07869v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "The language of sound search: Examining User Queries in Audio Search\n  Engines", "abstract": "This study examines textual, user-written search queries within the context\nof sound search engines, encompassing various applications such as foley, sound\neffects, and general audio retrieval. Current research inadequately addresses\nreal-world user needs and behaviours in designing text-based audio retrieval\nsystems. To bridge this gap, we analysed search queries from two sources: a\ncustom survey and Freesound website query logs. The survey was designed to\ncollect queries for an unrestricted, hypothetical sound search engine,\nresulting in a dataset that captures user intentions without the constraints of\nexisting systems. This dataset is also made available for sharing with the\nresearch community. In contrast, the Freesound query logs encompass\napproximately 9 million search requests, providing a comprehensive view of\nreal-world usage patterns. Our findings indicate that survey queries are\ngenerally longer than Freesound queries, suggesting users prefer detailed\nqueries when not limited by system constraints. Both datasets predominantly\nfeature keyword-based queries, with few survey participants using full\nsentences. Key factors influencing survey queries include the primary sound\nsource, intended usage, perceived location, and the number of sound sources.\nThese insights are crucial for developing user-centred, effective text-based\naudio retrieval systems, enhancing our understanding of user behaviour in sound\nsearch contexts.", "published": "2024-10-10 19:24:13", "link": "http://arxiv.org/abs/2410.08324v1", "categories": ["cs.CL", "cs.HC", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Window Function-less DFT with Reduced Noise and Latency for Real-Time\n  Music Analysis", "abstract": "Music analysis applications demand algorithms that can provide both high time\nand frequency resolution while minimizing noise in an already-noisy signal.\nReal-time analysis additionally demands low latency and low computational\nrequirements. We propose a DFT-based algorithm that accomplishes all these\nrequirements by extending a method that post-processes DFT output without the\nuse of window functions. Our approach yields greatly reduced sidelobes and\nnoise, and improves time resolution without sacrificing frequency resolution.\nWe use exponentially spaced output bins which directly map to notes in music.\nThe resulting improved performance, compared to existing FFT and DFT-based\napproaches, creates possibilities for improved real-time visualizations, and\ncontributes to improved analysis quality in other applications such as\nautomatic transcription.", "published": "2024-10-10 14:37:10", "link": "http://arxiv.org/abs/2410.07982v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer", "abstract": "Discrete Audio codecs (or audio tokenizers) have recently regained interest\ndue to the ability of Large Language Models (LLMs) to learn their compressed\nacoustic representations. Various publicly available trainable discrete\ntokenizers recently demonstrated impressive results for audio tokenization, yet\nthey mostly require high token rates to gain high-quality reconstruction. In\nthis study, we fine-tuned an open-source general audio RVQGAN model using\ndiverse open-source speech data, considering various recording conditions and\nquality levels. The resulting wideband (24kHz) speech-only model achieves\nspeech reconstruction, which is nearly indistinguishable from PCM (pulse-code\nmodulation) with a rate of 150-300 tokens per second (1500-3000 bps). The\nevaluation used comprehensive English speech data encompassing different\nrecording conditions, including studio settings. Speech samples are made\npublicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially\nreleased in https://huggingface.co/ibm/DAC.speech.v1.0", "published": "2024-10-10 19:29:05", "link": "http://arxiv.org/abs/2410.08325v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Robust Fixed-Filter Sound Zone Control with Audio-Based Position\n  Tracking", "abstract": "Performance of sound zone control (SZC) systems deployed in practical\nscenarios are highly sensitive to the location of the listener(s) and can\ndegrade significantly when listener(s) are moving. This paper presents a robust\nSZC system that adapts to dynamic changes such as moving listeners and varying\nzone locations using a dictionary-based approach. The proposed system\ncontinuously monitors the environment and updates the fixed control filters by\ntracking the listener position using audio signals only. To test the\neffectiveness of the proposed SZC method, simulation studies are carried out\nusing practically measured impulse responses. These studies show that SZC, when\nincorporated with the proposed audio-only position tracking scheme, achieves\noptimal performance when all listener positions are available in the\ndictionary. Moreover, even when not all listener positions are included in the\ndictionary, the method still provides good performance improvement compared to\na traditional fixed filter SZC scheme.", "published": "2024-10-10 14:01:07", "link": "http://arxiv.org/abs/2410.07935v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Sound Zone Control Robust To Sound Speed Change", "abstract": "Sound zone control (SZC) implemented using static optimal filters is\nsignificantly affected by various perturbations in the acoustic environment, an\nimportant one being the fluctuation in the speed of sound, which is in turn\ninfluenced by changes in temperature and humidity (TH). This issue arises\nbecause control algorithms typically use pre-recorded, static impulse responses\n(IRs) to design the optimal control filters. The IRs, however, may change with\ntime due to TH changes, which renders the derived control filters to become\nnon-optimal. To address this challenge, we propose a straightforward model\ncalled sinc interpolation-compression/expansion-resampling (SICER), which\nadjusts the IRs to account for both sound speed reduction and increase. Using\nthe proposed technique, IRs measured at a certain TH can be corrected for any\nTH change and control filters can be re-derived without the need of\nre-measuring the new IRs (which is impractical when SZC is deployed). We\nintegrate the proposed SICER IR correction method with the recently introduced\nvariable span trade-off (VAST) framework for SZC, and propose a SICER-corrected\nVAST method that is resilient to sound speed variations. Simulation studies\nshow that the proposed SICER-corrected VAST approach significantly improves\nacoustic contrast and reduces signal distortion in the presence of sound speed\nchanges.", "published": "2024-10-10 14:35:33", "link": "http://arxiv.org/abs/2410.07978v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Explanation Synthesis with Generative Foundation Models", "abstract": "The increasing success of audio foundation models across various tasks has\nled to a growing need for improved interpretability to understand their\nintricate decision-making processes better. Existing methods primarily focus on\nexplaining these models by attributing importance to elements within the input\nspace based on their influence on the final decision. In this paper, we\nintroduce a novel audio explanation method that capitalises on the generative\ncapacity of audio foundation models. Our method leverages the intrinsic\nrepresentational power of the embedding space within these models by\nintegrating established feature attribution techniques to identify significant\nfeatures in this space. The method then generates listenable audio explanations\nby prioritising the most important features. Through rigorous benchmarking\nagainst standard datasets, including keyword spotting and speech emotion\nrecognition, our model demonstrates its efficacy in producing audio\nexplanations.", "published": "2024-10-10 01:55:58", "link": "http://arxiv.org/abs/2410.07530v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Genre Classification using Large Language Models", "abstract": "This paper exploits the zero-shot capabilities of pre-trained large language\nmodels (LLMs) for music genre classification. The proposed approach splits\naudio signals into 20 ms chunks and processes them through convolutional\nfeature encoders, a transformer encoder, and additional layers for coding audio\nunits and generating feature vectors. The extracted feature vectors are used to\ntrain a classification head. During inference, predictions on individual chunks\nare aggregated for a final genre classification. We conducted a comprehensive\ncomparison of LLMs, including WavLM, HuBERT, and wav2vec 2.0, with traditional\ndeep learning architectures like 1D and 2D convolutional neural networks (CNNs)\nand the audio spectrogram transformer (AST). Our findings demonstrate the\nsuperior performance of the AST model, achieving an overall accuracy of 85.5%,\nsurpassing all other models evaluated. These results highlight the potential of\nLLMs and transformer-based architectures for advancing music information\nretrieval tasks, even in zero-shot scenarios.", "published": "2024-10-10 19:17:56", "link": "http://arxiv.org/abs/2410.08321v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
