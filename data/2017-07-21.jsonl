{"title": "Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling\n  Tasks", "abstract": "Selecting optimal parameters for a neural network architecture can often make\nthe difference between mediocre and state-of-the-art performance. However,\nlittle is published which parameters and design choices should be evaluated or\nselected making the correct hyperparameter optimization often a \"black art that\nrequires expert experiences\" (Snoek et al., 2012). In this paper, we evaluate\nthe importance of different network design choices and hyperparameters for five\ncommon linguistic sequence tagging tasks (POS, Chunking, NER, Entity\nRecognition, and Event Detection). We evaluated over 50.000 different setups\nand found, that some parameters, like the pre-trained word embeddings or the\nlast layer of the network, have a large impact on the performance, while other\nparameters, for example the number of LSTM layers or the number of recurrent\nunits, are of minor importance. We give a recommendation on a configuration\nthat performs well among different tasks.", "published": "2017-07-21 08:36:31", "link": "http://arxiv.org/abs/1707.06799v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shallow reading with Deep Learning: Predicting popularity of online\n  content using only its title", "abstract": "With the ever decreasing attention span of contemporary Internet users, the\ntitle of online content (such as a news article or video) can be a major factor\nin determining its popularity. To take advantage of this phenomenon, we propose\na new method based on a bidirectional Long Short-Term Memory (LSTM) neural\nnetwork designed to predict the popularity of online content using only its\ntitle. We evaluate the proposed architecture on two distinct datasets of news\narticles and news videos distributed in social media that contain over 40,000\nsamples in total. On those datasets, our approach improves the performance over\ntraditional shallow approaches by a margin of 15%. Additionally, we show that\nusing pre-trained word vectors in the embedding layer improves the results of\nLSTM models, especially when the training set is small. To our knowledge, this\nis the first attempt of applying popularity prediction using only textual\ninformation from the title.", "published": "2017-07-21 09:02:55", "link": "http://arxiv.org/abs/1707.06806v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why We Need New Evaluation Metrics for NLG", "abstract": "The majority of NLG evaluation relies on automatic metrics, such as BLEU . In\nthis paper, we motivate the need for novel, system- and data-independent\nautomatic evaluation methods: We investigate a wide range of metrics, including\nstate-of-the-art word-based and novel grammar-based ones, and demonstrate that\nthey only weakly reflect human judgements of system outputs as generated by\ndata-driven, end-to-end NLG. We also show that metric performance is data- and\nsystem-specific. Nevertheless, our results also suggest that automatic metrics\nperform reliably at system-level and can support system development by finding\ncases where a system performs poorly.", "published": "2017-07-21 12:47:03", "link": "http://arxiv.org/abs/1707.06875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised, Knowledge-Free, and Interpretable Word Sense\n  Disambiguation", "abstract": "Interpretability of a predictive model is a powerful feature that gains the\ntrust of users in the correctness of the predictions. In word sense\ndisambiguation (WSD), knowledge-based systems tend to be much more\ninterpretable than knowledge-free counterparts as they rely on the wealth of\nmanually-encoded elements representing word senses, such as hypernyms, usage\nexamples, and images. We present a WSD system that bridges the gap between\nthese two so far disconnected groups of methods. Namely, our system, providing\naccess to several state-of-the-art WSD models, aims to be interpretable as a\nknowledge-based system while it remains completely unsupervised and\nknowledge-free. The presented tool features a Web interface for all-word\ndisambiguation of texts that makes the sense predictions human readable by\nproviding interpretable word sense inventories, sense representations, and\ndisambiguation results. We provide a public API, enabling seamless integration.", "published": "2017-07-21 12:56:06", "link": "http://arxiv.org/abs/1707.06878v1", "categories": ["cs.CL", "I.2.6; I.5.3; I.2.4"], "primary_category": "cs.CL"}
{"title": "SGNMT -- A Flexible NMT Decoding Platform for Quick Prototyping of New\n  Models and Search Strategies", "abstract": "This paper introduces SGNMT, our experimental platform for machine\ntranslation research. SGNMT provides a generic interface to neural and symbolic\nscoring modules (predictors) with left-to-right semantic such as translation\nmodels like NMT, language models, translation lattices, $n$-best lists or other\nkinds of scores and constraints. Predictors can be combined with other\npredictors to form complex decoding tasks. SGNMT implements a number of search\nstrategies for traversing the space spanned by the predictors which are\nappropriate for different predictor constellations. Adding new predictors or\ndecoding strategies is particularly easy, making it a very efficient tool for\nprototyping new research ideas. SGNMT is actively being used by students in the\nMPhil program in Machine Learning, Speech and Language Technology at the\nUniversity of Cambridge for course work and theses, as well as for most of the\nresearch work in our group.", "published": "2017-07-21 13:14:25", "link": "http://arxiv.org/abs/1707.06885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Induction and Transfer of Verb Classes Based on Word\n  Vector Space Specialisation", "abstract": "Existing approaches to automatic VerbNet-style verb classification are\nheavily dependent on feature engineering and therefore limited to languages\nwith mature NLP pipelines. In this work, we propose a novel cross-lingual\ntransfer method for inducing VerbNets for multiple languages. To the best of\nour knowledge, this is the first study which demonstrates how the architectures\nfor learning word embeddings can be applied to this challenging\nsyntactic-semantic task. Our method uses cross-lingual translation pairs to tie\neach of the six target languages into a bilingual vector space with English,\njointly specialising the representations to encode the relational information\nfrom English VerbNet. A standard clustering algorithm is then run on top of the\nVerbNet-specialised representations, using vector dimensions as features for\nlearning verb classes. Our results show that the proposed cross-lingual\ntransfer approach sets new state-of-the-art verb classification performance\nacross all six target languages explored in this work.", "published": "2017-07-21 15:52:54", "link": "http://arxiv.org/abs/1707.06945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reconstruction of Word Embeddings from Sub-Word Parameters", "abstract": "Pre-trained word embeddings improve the performance of a neural model at the\ncost of increasing the model size. We propose to benefit from this resource\nwithout paying the cost by operating strictly at the sub-lexical level. Our\napproach is quite simple: before task-specific training, we first optimize\nsub-word parameters to reconstruct pre-trained word embeddings using various\ndistance measures. We report interesting results on a variety of tasks: word\nsimilarity, word analogy, and part-of-speech tagging.", "published": "2017-07-21 16:10:51", "link": "http://arxiv.org/abs/1707.06957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mimicking Word Embeddings using Subword RNNs", "abstract": "Word embeddings improve generalization over lexical features by placing each\nword in a lower-dimensional space, using distributional information obtained\nfrom unlabeled data. However, the effectiveness of word embeddings for\ndownstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which\nembeddings do not exist. In this paper, we present MIMICK, an approach to\ngenerating OOV word embeddings compositionally, by learning a function from\nspellings to distributional embeddings. Unlike prior work, MIMICK does not\nrequire re-training on the original word embedding corpus; instead, learning is\nperformed at the type level. Intrinsic and extrinsic evaluations demonstrate\nthe power of this simple approach. On 23 languages, MIMICK improves performance\nover a word-based baseline for tagging part-of-speech and morphosyntactic\nattributes. It is competitive with (and complementary to) a supervised\ncharacter-based model in low-resource settings.", "published": "2017-07-21 16:18:10", "link": "http://arxiv.org/abs/1707.06961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Split and Rephrase", "abstract": "We propose a new sentence simplification task (Split-and-Rephrase) where the\naim is to split a complex sentence into a meaning preserving sequence of\nshorter sentences. Like sentence simplification, splitting-and-rephrasing has\nthe potential of benefiting both natural language processing and societal\napplications. Because shorter sentences are generally better processed by NLP\nsystems, it could be used as a preprocessing step which facilitates and\nimproves the performance of parsers, semantic role labellers and machine\ntranslation systems. It should also be of use for people with reading\ndisabilities because it allows the conversion of longer sentences into shorter\nones. This paper makes two contributions towards this new task. First, we\ncreate and make available a benchmark consisting of 1,066,115 tuples mapping a\nsingle complex sentence to a sequence of sentences expressing the same meaning.\nSecond, we propose five models (vanilla sequence-to-sequence to\nsemantically-motivated models) to understand the difficulty of the proposed\ntask.", "published": "2017-07-21 16:47:56", "link": "http://arxiv.org/abs/1707.06971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sentiment-and-Semantics-Based Approach for Emotion Detection in\n  Textual Conversations", "abstract": "Emotions are physiological states generated in humans in reaction to internal\nor external events. They are complex and studied across numerous fields\nincluding computer science. As humans, on reading \"Why don't you ever text me!\"\nwe can either interpret it as a sad or angry emotion and the same ambiguity\nexists for machines. Lack of facial expressions and voice modulations make\ndetecting emotions from text a challenging problem. However, as humans\nincreasingly communicate using text messaging applications, and digital agents\ngain popularity in our society, it is essential that these digital agents are\nemotion aware, and respond accordingly.\n  In this paper, we propose a novel approach to detect emotions like happy, sad\nor angry in textual conversations using an LSTM based Deep Learning model. Our\napproach consists of semi-automated techniques to gather training data for our\nmodel. We exploit advantages of semantic and sentiment based embeddings and\npropose a solution combining both. Our work is evaluated on real-world\nconversations and significantly outperforms traditional Machine Learning\nbaselines as well as other off-the-shelf Deep Learning models.", "published": "2017-07-21 11:52:45", "link": "http://arxiv.org/abs/1707.06996v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Neural Coreference Resolution", "abstract": "We introduce the first end-to-end coreference resolution model and show that\nit significantly outperforms all previous work without using a syntactic parser\nor hand-engineered mention detector. The key idea is to directly consider all\nspans in a document as potential mentions and learn distributions over possible\nantecedents for each. The model computes span embeddings that combine\ncontext-dependent boundary representations with a head-finding attention\nmechanism. It is trained to maximize the marginal likelihood of gold antecedent\nspans from coreference clusters and is factored to enable aggressive pruning of\npotential mentions. Experiments demonstrate state-of-the-art performance, with\na gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model\nensemble, despite the fact that this is the first approach to be successfully\ntrained with no external resources.", "published": "2017-07-21 21:05:04", "link": "http://arxiv.org/abs/1707.07045v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Pilot Study of Domain Adaptation Effect for Neural Abstractive\n  Summarization", "abstract": "We study the problem of domain adaptation for neural abstractive\nsummarization. We make initial efforts in investigating what information can be\ntransferred to a new domain. Experimental results on news stories and opinion\narticles indicate that neural summarization model benefits from pre-training\nbased on extractive summaries. We also find that the combination of in-domain\nand out-of-domain setup yields better summaries when in-domain data is\ninsufficient. Further analysis shows that, the model is capable to select\nsalient content even trained on out-of-domain data, but requires in-domain data\nto capture the style for a target domain.", "published": "2017-07-21 22:42:52", "link": "http://arxiv.org/abs/1707.07062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressive Joint Modeling in Unsupervised Single-channel Overlapped\n  Speech Recognition", "abstract": "Unsupervised single-channel overlapped speech recognition is one of the\nhardest problems in automatic speech recognition (ASR). Permutation invariant\ntraining (PIT) is a state of the art model-based approach, which applies a\nsingle neural network to solve this single-input, multiple-output modeling\nproblem. We propose to advance the current state of the art by imposing a\nmodular structure on the neural network, applying a progressive pretraining\nregimen, and improving the objective function with transfer learning and a\ndiscriminative training criterion. The modular structure splits the problem\ninto three sub-tasks: frame-wise interpreting, utterance-level speaker tracing,\nand speech recognition. The pretraining regimen uses these modules to solve\nprogressively harder tasks. Transfer learning leverages parallel clean speech\nto improve the training targets for the network. Our discriminative training\nformulation is a modification of standard formulations, that also penalizes\ncompeting outputs of the system. Experiments are conducted on the artificial\noverlapped Switchboard and hub5e-swb dataset. The proposed framework achieves\nover 30% relative improvement of WER over both a strong jointly trained system,\nPIT for ASR, and a separately optimized system, PIT for speech separation with\nclean speech ASR model. The improvement comes from better model generalization,\ntraining efficiency and the sequence level linguistic knowledge integration.", "published": "2017-07-21 21:21:09", "link": "http://arxiv.org/abs/1707.07048v2", "categories": ["cs.CL", "cs.AI", "68T10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An Error-Oriented Approach to Word Embedding Pre-Training", "abstract": "We propose a novel word embedding pre-training approach that exploits writing\nerrors in learners' scripts. We compare our method to previous models that tune\nthe embeddings based on script scores and the discrimination between correct\nand corrupt word contexts in addition to the generic commonly-used embeddings\npre-trained on large corpora. The comparison is achieved by using the\naforementioned models to bootstrap a neural network that learns to predict a\nholistic score for scripts. Furthermore, we investigate augmenting our model\nwith error corrections and monitor the impact on performance. Our results show\nthat our error-oriented approach outperforms other comparable ones which is\nfurther demonstrated when training on more data. Additionally, extending the\nmodel with corrections provides further performance gains when data sparsity is\nan issue.", "published": "2017-07-21 11:06:12", "link": "http://arxiv.org/abs/1707.06841v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A study on text-score disagreement in online reviews", "abstract": "In this paper, we focus on online reviews and employ artificial intelligence\ntools, taken from the cognitive computing field, to help understanding the\nrelationships between the textual part of the review and the assigned numerical\nscore. We move from the intuitions that 1) a set of textual reviews expressing\ndifferent sentiments may feature the same score (and vice-versa); and 2)\ndetecting and analyzing the mismatches between the review content and the\nactual score may benefit both service providers and consumers, by highlighting\nspecific factors of satisfaction (and dissatisfaction) in texts.\n  To prove the intuitions, we adopt sentiment analysis techniques and we\nconcentrate on hotel reviews, to find polarity mismatches therein. In\nparticular, we first train a text classifier with a set of annotated hotel\nreviews, taken from the Booking website. Then, we analyze a large dataset, with\naround 160k hotel reviews collected from Tripadvisor, with the aim of detecting\na polarity mismatch, indicating if the textual content of the review is in\nline, or not, with the associated score.\n  Using well established artificial intelligence techniques and analyzing in\ndepth the reviews featuring a mismatch between the text polarity and the score,\nwe find that -on a scale of five stars- those reviews ranked with middle scores\ninclude a mixture of positive and negative aspects.\n  The approach proposed here, beside acting as a polarity detector, provides an\neffective selection of reviews -on an initial very large dataset- that may\nallow both consumers and providers to focus directly on the review subset\nfeaturing a text/score disagreement, which conveniently convey to the user a\nsummary of positive and negative features of the review target.", "published": "2017-07-21 15:19:09", "link": "http://arxiv.org/abs/1707.06932v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Autocompletion interfaces make crowd workers slower, but their use\n  promotes response diversity", "abstract": "Creative tasks such as ideation or question proposal are powerful\napplications of crowdsourcing, yet the quantity of workers available for\naddressing practical problems is often insufficient. To enable scalable\ncrowdsourcing thus requires gaining all possible efficiency and information\nfrom available workers. One option for text-focused tasks is to allow assistive\ntechnology, such as an autocompletion user interface (AUI), to help workers\ninput text responses. But support for the efficacy of AUIs is mixed. Here we\ndesigned and conducted a randomized experiment where workers were asked to\nprovide short text responses to given questions. Our experimental goal was to\ndetermine if an AUI helps workers respond more quickly and with improved\nconsistency by mitigating typos and misspellings. Surprisingly, we found that\nneither occurred: workers assigned to the AUI treatment were slower than those\nassigned to the non-AUI control and their responses were more diverse, not\nless, than those of the control. Both the lexical and semantic diversities of\nresponses were higher, with the latter measured using word2vec. A crowdsourcer\ninterested in worker speed may want to avoid using an AUI, but using an AUI to\nboost response diversity may be valuable to crowdsourcers interested in\nreceiving as much novel information from workers as possible.", "published": "2017-07-21 15:41:38", "link": "http://arxiv.org/abs/1707.06939v1", "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.HC"}
{"title": "Ultraslow diffusion in language: Dynamics of appearance of already\n  popular adjectives on Japanese blogs", "abstract": "What dynamics govern a time series representing the appearance of words in\nsocial media data? In this paper, we investigate an elementary dynamics, from\nwhich word-dependent special effects are segregated, such as breaking news,\nincreasing (or decreasing) concerns, or seasonality. To elucidate this problem,\nwe investigated approximately three billion Japanese blog articles over a\nperiod of six years, and analysed some corresponding solvable mathematical\nmodels. From the analysis, we found that a word appearance can be explained by\nthe random diffusion model based on the power-law forgetting process, which is\na type of long memory point process related to ARFIMA(0,0.5,0). In particular,\nwe confirmed that ultraslow diffusion (where the mean squared displacement\ngrows logarithmically), which the model predicts in an approximate manner,\nreproduces the actual data. In addition, we also show that the model can\nreproduce other statistical properties of a time series: (i) the fluctuation\nscaling, (ii) spectrum density, and (iii) shapes of the probability density\nfunctions.", "published": "2017-07-21 23:13:50", "link": "http://arxiv.org/abs/1707.07066v3", "categories": ["physics.soc-ph", "cs.CL", "cs.CY", "stat.AP"], "primary_category": "physics.soc-ph"}
