{"title": "Improving Persian Relation Extraction Models by Data Augmentation", "abstract": "Relation extraction that is the task of predicting semantic relation type\nbetween entities in a sentence or document is an important task in natural\nlanguage processing. Although there are many researches and datasets for\nEnglish, Persian suffers from sufficient researches and comprehensive datasets.\nThe only available Persian dataset for this task is PERLEX, which is a Persian\nexpert-translated version of the SemEval-2010-Task-8 dataset. In this paper, we\npresent our augmented dataset and the results and findings of our system,\nparticipated in the Persian relation Extraction shared task of NSURL 2021\nworkshop. We use PERLEX as the base dataset and enhance it by applying some\ntext preprocessing steps and by increasing its size via data augmentation\ntechniques to improve the generalization and robustness of applied models. We\nthen employ two different models including ParsBERT and multilingual BERT for\nrelation extraction on the augmented PERLEX dataset. Our best model obtained\n64.67% of Macro-F1 on the test phase of the contest and it achieved 83.68% of\nMacro-F1 on the test set of PERLEX.", "published": "2022-03-29 08:08:47", "link": "http://arxiv.org/abs/2203.15323v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LDKP: A Dataset for Identifying Keyphrases from Long Scientific\n  Documents", "abstract": "Identifying keyphrases (KPs) from text documents is a fundamental task in\nnatural language processing and information retrieval. Vast majority of the\nbenchmark datasets for this task are from the scientific domain containing only\nthe document title and abstract information. This limits keyphrase extraction\n(KPE) and keyphrase generation (KPG) algorithms to identify keyphrases from\nhuman-written summaries that are often very short (approx 8 sentences). This\npresents three challenges for real-world applications: human-written summaries\nare unavailable for most documents, the documents are almost always long, and a\nhigh percentage of KPs are directly found beyond the limited context of title\nand abstract. Therefore, we release two extensive corpora mapping KPs of ~1.3M\nand ~100K scientific articles with their fully extracted text and additional\nmetadata including publication venue, year, author, field of study, and\ncitations for facilitating research on this real-world problem.", "published": "2022-03-29 08:44:57", "link": "http://arxiv.org/abs/2203.15349v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Short-Term Word-Learning in a Dynamically Changing Environment", "abstract": "Neural sequence-to-sequence automatic speech recognition (ASR) systems are in\nprinciple open vocabulary systems, when using appropriate modeling units. In\npractice, however, they often fail to recognize words not seen during training,\ne.g., named entities, numbers or technical terms. To alleviate this problem,\nHuber et al. proposed to supplement an end-to-end ASR system with a word/phrase\nmemory and a mechanism to access this memory to recognize the words and phrases\ncorrectly. In this paper we study, a) methods to acquire important words for\nthis memory dynamically and, b) the trade-off between improvement in\nrecognition accuracy of new words and the potential danger of false alarms for\nthose added words. We demonstrate significant improvements in the detection\nrate of new words with only a minor increase in false alarms (F1 score 0.30\n$\\rightarrow$ 0.80), when using an appropriate number of new words. In\naddition, we show that important keywords can be extracted from supporting\ndocuments and used effectively.", "published": "2022-03-29 10:05:39", "link": "http://arxiv.org/abs/2203.15404v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic properties of English nominal pluralization: Insights from word\n  embeddings", "abstract": "Semantic differentiation of nominal pluralization is grammaticalized in many\nlanguages. For example, plural markers may only be relevant for human nouns.\nEnglish does not appear to make such distinctions. Using distributional\nsemantics, we show that English nominal pluralization exhibits semantic\nclusters. For instance, pluralization of fruit words is more similar to one\nanother and less similar to pluralization of other semantic classes. Therefore,\nreduction of the meaning shift in plural formation to the addition of an\nabstract plural meaning is too simplistic. A semantically informed method,\ncalled CosClassAvg, is introduced that outperforms pluralization methods in\ndistributional semantics which assume plural formation amounts to the addition\nof a fixed plural vector. In comparison with our approach, a method from\ncompositional distributional semantics, called FRACSS, predicted plural vectors\nthat were more similar to the corpus-extracted plural vectors in terms of\ndirection but not vector length. A modeling study reveals that the observed\ndifference between the two predicted semantic spaces by CosClassAvg and FRACSS\ncarries over to how well a computational model of the listener can understand\npreviously unencountered plural forms. Mappings from word forms, represented\nwith triphone vectors, to predicted semantic vectors are more productive when\nCosClassAvg-generated semantic vectors are employed as gold standard vectors\ninstead of FRACSS-generated vectors.", "published": "2022-03-29 10:42:47", "link": "http://arxiv.org/abs/2203.15424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representing 'how you say' with 'what you say': English corpus of\n  focused speech and text reflecting corresponding implications", "abstract": "In speech communication, how something is said (paralinguistic information)\nis as crucial as what is said (linguistic information). As a type of\nparalinguistic information, English speech uses sentence stress, the heaviest\nprominence within a sentence, to convey emphasis. While different placements of\nsentence stress communicate different emphatic implications, current speech\ntranslation systems return the same translations if the utterances are\nlinguistically identical, losing paralinguistic information. Concentrating on\nfocus, a type of emphasis, we propose mapping paralinguistic information into\nthe linguistic domain within the source language using lexical and grammatical\ndevices. This method enables us to translate the paraphrased text\nrepresentations instead of the transcription of the original speech and obtain\ntranslations that preserve paralinguistic information. As a first step, we\npresent the collection of an English corpus containing speech that differed in\nthe placement of focus along with the corresponding text, which was designed to\nreflect the implied meaning of the speech. Also, analyses of our corpus\ndemonstrated that mapping of focus from the paralinguistic domain into the\nlinguistic domain involved various lexical and grammatical methods. The data\nand insights from our analysis will further advance research into\nparalinguistic translation. The corpus will be published via LDC and our\nwebsite.", "published": "2022-03-29 12:29:22", "link": "http://arxiv.org/abs/2203.15483v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Heuristic-based Inter-training to Improve Few-shot Multi-perspective\n  Dialog Summarization", "abstract": "Many organizations require their customer-care agents to manually summarize\ntheir conversations with customers. These summaries are vital for decision\nmaking purposes of the organizations. The perspective of the summary that is\nrequired to be created depends on the application of the summaries. With this\nwork, we study the multi-perspective summarization of customer-care\nconversations between support agents and customers. We observe that there are\ndifferent heuristics that are associated with summaries of different\nperspectives, and explore these heuristics to create weak-labeled data for\nintermediate training of the models before fine-tuning with scarce human\nannotated summaries. Most importantly, we show that our approach supports\nmodels to generate multi-perspective summaries with a very small amount of\nannotated data. For example, our approach achieves 94\\% of the performance\n(Rouge-2) of a model trained with the original data, by training only with 7\\%\nof the original data.", "published": "2022-03-29 14:02:40", "link": "http://arxiv.org/abs/2203.15590v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Earnings-22: A Practical Benchmark for Accents in the Wild", "abstract": "Modern automatic speech recognition (ASR) systems have achieved superhuman\nWord Error Rate (WER) on many common corpora despite lacking adequate\nperformance on speech in the wild. Beyond that, there is a lack of real-world,\naccented corpora to properly benchmark academic and commercial models. To\nensure this type of speech is represented in ASR benchmarking, we present\nEarnings-22, a 125 file, 119 hour corpus of English-language earnings calls\ngathered from global companies. We run a comparison across 4 commercial models\nshowing the variation in performance when taking country of origin into\nconsideration. Looking at hypothesis transcriptions, we explore errors common\nto all ASR systems tested. By examining Individual Word Error Rate (IWER), we\nfind that key speech features impact model performance more for certain accents\nthan others. Earnings-22 provides a free-to-use benchmark of real-world,\naccented audio to bridge academic and industrial research.", "published": "2022-03-29 14:02:57", "link": "http://arxiv.org/abs/2203.15591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autoregressive Co-Training for Learning Discrete Speech Representations", "abstract": "While several self-supervised approaches for learning discrete speech\nrepresentation have been proposed, it is unclear how these seemingly similar\napproaches relate to each other. In this paper, we consider a generative model\nwith discrete latent variables that learns a discrete representation for\nspeech. The objective of learning the generative model is formulated as\ninformation-theoretic co-training. Besides the wide generality, the objective\ncan be optimized with several approaches, subsuming HuBERT-like training and\nvector quantization for learning discrete representation. Empirically, we find\nthat the proposed approach learns discrete representation that is highly\ncorrelated with phonetic units, more correlated than HuBERT-like training and\nvector quantization.", "published": "2022-03-29 18:17:18", "link": "http://arxiv.org/abs/2203.15840v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shallow Fusion of Weighted Finite-State Transducer and Language Model\n  for Text Normalization", "abstract": "Text normalization (TN) systems in production are largely rule-based using\nweighted finite-state transducers (WFST). However, WFST-based systems struggle\nwith ambiguous input when the normalized form is context-dependent. On the\nother hand, neural text normalization systems can take context into account but\nthey suffer from unrecoverable errors and require labeled normalization\ndatasets, which are hard to collect. We propose a new hybrid approach that\ncombines the benefits of rule-based and neural systems. First, a\nnon-deterministic WFST outputs all normalization candidates, and then a neural\nlanguage model picks the best one -- similar to shallow fusion for automatic\nspeech recognition. While the WFST prevents unrecoverable errors, the language\nmodel resolves contextual ambiguity. The approach is easy to extend and we show\nit is effective. It achieves comparable or better results than existing\nstate-of-the-art TN models.", "published": "2022-03-29 21:34:35", "link": "http://arxiv.org/abs/2203.15917v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Inefficiency of Language Models in Scholarly Retrieval: An\n  Experimental Walk-through", "abstract": "Language models are increasingly becoming popular in AI-powered scientific IR\nsystems. This paper evaluates popular scientific language models in handling\n(i) short-query texts and (ii) textual neighbors. Our experiments showcase the\ninability to retrieve relevant documents for a short-query text even under the\nmost relaxed conditions. Additionally, we leverage textual neighbors, generated\nby small perturbations to the original text, to demonstrate that not all\nperturbations lead to close neighbors in the embedding space. Further, an\nexhaustive categorization yields several classes of orthographically and\nsemantically related, partially related, and completely unrelated neighbors.\nRetrieval performance turns out to be more influenced by the surface form\nrather than the semantics of the text.", "published": "2022-03-29 09:01:26", "link": "http://arxiv.org/abs/2203.15364v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Training Compute-Optimal Large Language Models", "abstract": "We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.", "published": "2022-03-29 13:38:03", "link": "http://arxiv.org/abs/2203.15556v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Decoding Strategies for Neural Text Generators", "abstract": "When generating text from probabilistic models, the chosen decoding strategy\nhas a profound effect on the resulting text. Yet the properties elicited by\nvarious decoding strategies do not always transfer across natural language\ngeneration tasks. For example, while mode-seeking methods like beam search\nperform remarkably well for machine translation, they have been observed to\nlead to incoherent and repetitive text in story generation. Despite such\nobservations, the effectiveness of decoding strategies is often assessed with\nrespect to only a single task. This work -- in contrast -- provides a\ncomprehensive analysis of the interaction between language generation tasks and\ndecoding strategies. Specifically, we measure changes in attributes of\ngenerated text as a function of both decoding strategy and task using human and\nautomatic evaluation. Our results reveal both previously-observed and\nsurprising findings. For example, the nature of the diversity-quality trade-off\nin language generation is very task-specific; the length bias often attributed\nto beam search is not constant across tasks.", "published": "2022-03-29 16:25:30", "link": "http://arxiv.org/abs/2203.15721v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LinkBERT: Pretraining Language Models with Document Links", "abstract": "Language model (LM) pretraining can learn various knowledge from text\ncorpora, helping downstream tasks. However, existing methods such as BERT model\na single document, and do not capture dependencies or knowledge that span\nacross documents. In this work, we propose LinkBERT, an LM pretraining method\nthat leverages links between documents, e.g., hyperlinks. Given a text corpus,\nwe view it as a graph of documents and create LM inputs by placing linked\ndocuments in the same context. We then pretrain the LM with two joint\nself-supervised objectives: masked language modeling and our new proposal,\ndocument relation prediction. We show that LinkBERT outperforms BERT on various\ndownstream tasks across two domains: the general domain (pretrained on\nWikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with\ncitation links). LinkBERT is especially effective for multi-hop reasoning and\nfew-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our\nbiomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on\nBioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT,\nas well as code and data at https://github.com/michiyasunaga/LinkBERT.", "published": "2022-03-29 18:01:24", "link": "http://arxiv.org/abs/2203.15827v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture", "abstract": "Person name capture from human speech is a difficult task in human-machine\nconversations. In this paper, we propose a novel approach to capture the person\nnames from the caller utterances in response to the prompt \"say and spell your\nfirst/last name\". Inspired from work on spell correction, disfluency removal\nand text normalization, we propose a lightweight Seq-2-Seq system which\ngenerates a name spell from a varying user input. Our proposed method\noutperforms the strong baseline which is based on LM-driven rule-based\napproach.", "published": "2022-03-29 18:04:51", "link": "http://arxiv.org/abs/2203.15833v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Investigating Data Variance in Evaluations of Automatic Machine\n  Translation Metrics", "abstract": "Current practices in metric evaluation focus on one single dataset, e.g.,\nNewstest dataset in each year's WMT Metrics Shared Task. However, in this\npaper, we qualitatively and quantitatively show that the performances of\nmetrics are sensitive to data. The ranking of metrics varies when the\nevaluation is conducted on different datasets. Then this paper further\ninvestigates two potential hypotheses, i.e., insignificant data points and the\ndeviation of Independent and Identically Distributed (i.i.d) assumption, which\nmay take responsibility for the issue of data variance. In conclusion, our\nfindings suggest that when evaluating automatic translation metrics,\nresearchers should take data variance into account and be cautious to claim the\nresult on a single dataset, because it may leads to inconsistent results with\nmost of other datasets.", "published": "2022-03-29 18:58:28", "link": "http://arxiv.org/abs/2203.15858v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visualizing the Relationship Between Encoded Linguistic Information and\n  Task Performance", "abstract": "Probing is popular to analyze whether linguistic information can be captured\nby a well-trained deep neural model, but it is hard to answer how the change of\nthe encoded linguistic information will affect task performance. To this end,\nwe study the dynamic relationship between the encoded linguistic information\nand task performance from the viewpoint of Pareto Optimality. Its key idea is\nto obtain a set of models which are Pareto-optimal in terms of both objectives.\nFrom this viewpoint, we propose a method to optimize the Pareto-optimal models\nby formalizing it as a multi-objective optimization problem. We conduct\nexperiments on two popular NLP tasks, i.e., machine translation and language\nmodeling, and investigate the relationship between several kinds of linguistic\ninformation and task performances. Experimental results demonstrate that the\nproposed method is better than a baseline method. Our empirical findings\nsuggest that some syntactic information is helpful for NLP tasks whereas\nencoding more syntactic information does not necessarily lead to better\nperformance, because the model architecture is also an important factor.", "published": "2022-03-29 19:03:10", "link": "http://arxiv.org/abs/2203.15860v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Image Retrieval from Contextual Descriptions", "abstract": "The ability to integrate context, including perceptual and temporal cues,\nplays a pivotal role in grounding the meaning of a linguistic utterance. In\norder to measure to what extent current vision-and-language models master this\nability, we devise a new multimodal challenge, Image Retrieval from Contextual\nDescriptions (ImageCoDe). In particular, models are tasked with retrieving the\ncorrect image from a set of 10 minimally contrastive candidates based on a\ncontextual description. As such, each description contains only the details\nthat help distinguish between images. Because of this, descriptions tend to be\ncomplex in terms of syntax and discourse and require drawing pragmatic\ninferences. Images are sourced from both static pictures and video frames. We\nbenchmark several state-of-the-art models, including both cross-encoders such\nas ViLBERT and bi-encoders such as CLIP, on ImageCoDe. Our results reveal that\nthese models dramatically lag behind human performance: the best variant\nachieves an accuracy of 20.9 on video frames and 59.4 on static pictures,\ncompared with 90.8 in humans. Furthermore, we experiment with new model\nvariants that are better equipped to incorporate visual and temporal context\ninto their representations, which achieve modest gains. Our hope is that\nImageCoDE will foster progress in grounded language understanding by\nencouraging models to focus on fine-grained visual differences.", "published": "2022-03-29 19:18:12", "link": "http://arxiv.org/abs/2203.15867v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Worldwide city transport typology prediction with sentence-BERT based\n  supervised learning via Wikipedia", "abstract": "An overwhelming majority of the world's human population lives in urban areas\nand cities. Understanding a city's transportation typology is immensely\nvaluable for planners and policy makers whose decisions can potentially impact\nmillions of city residents. Despite the value of understanding a city's\ntypology, labeled data (city and it's typology) is scarce, and spans at most a\nfew hundred cities in the current transportation literature. To break this\nbarrier, we propose a supervised machine learning approach to predict a city's\ntypology given the information in its Wikipedia page. Our method leverages\nrecent breakthroughs in natural language processing, namely sentence-BERT, and\nshows how the text-based information from Wikipedia can be effectively used as\na data source for city typology prediction tasks that can be applied to over\n2000 cities worldwide. We propose a novel method for low-dimensional city\nrepresentation using a city's Wikipedia page, which makes supervised learning\nof city typology labels tractable even with a few hundred labeled samples.\nThese features are used with labeled city samples to train binary classifiers\n(logistic regression) for four different city typologies: (i) congestion, (ii)\nauto-heavy, (iii) transit-heavy, and (iv) bike-friendly cities resulting in\nreasonably high AUC scores of 0.87, 0.86, 0.61 and 0.94 respectively. Our\napproach provides sufficient flexibility for incorporating additional variables\nin the city typology models and can be applied to study other city typologies\nas well. Our findings can assist a diverse group of stakeholders in\ntransportation and urban planning fields, and opens up new opportunities for\nusing text-based information from Wikipedia (or similar platforms) as data\nsources in such fields.", "published": "2022-03-29 00:09:55", "link": "http://arxiv.org/abs/2204.05193v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Fast Post-Training Pruning Framework for Transformers", "abstract": "Pruning is an effective way to reduce the huge inference cost of Transformer\nmodels. However, prior work on pruning Transformers requires retraining the\nmodels. This can add high training cost and high complexity to model\ndeployment, making it difficult to use in many practical situations. To address\nthis, we propose a fast post-training pruning framework for Transformers that\ndoes not require any retraining. Given a resource constraint and a sample\ndataset, our framework automatically prunes the Transformer model using\nstructured sparsity methods. To retain high accuracy without retraining, we\nintroduce three novel techniques: (i) a lightweight mask search algorithm that\nfinds which heads and filters to prune based on the Fisher information; (ii)\nmask rearrangement that complements the search algorithm; and (iii) mask tuning\nthat reconstructs the output activations for each layer. We apply our method to\nBERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD\nbenchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x\nspeedup in inference latency, while maintaining < 1% loss in accuracy.\nImportantly, our framework prunes Transformers in less than 3 minutes on a\nsingle GPU, which is over two orders of magnitude faster than existing pruning\napproaches that retrain the models.", "published": "2022-03-29 07:41:11", "link": "http://arxiv.org/abs/2204.09656v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Evaluation Dataset for Legal Word Embedding: A Case Study On Chinese\n  Codex", "abstract": "Word embedding is a modern distributed word representations approach widely\nused in many natural language processing tasks. Converting the vocabulary in a\nlegal document into a word embedding model facilitates subjecting legal\ndocuments to machine learning, deep learning, and other algorithms and\nsubsequently performing the downstream tasks of natural language processing\nvis-\\`a-vis, for instance, document classification, contract review, and\nmachine translation. The most common and practical approach of accuracy\nevaluation with the word embedding model uses a benchmark set with linguistic\nrules or the relationship between words to perform analogy reasoning via\nalgebraic calculation. This paper proposes establishing a 1,134 Legal\nAnalogical Reasoning Questions Set (LARQS) from the 2,388 Chinese Codex corpus\nusing five kinds of legal relations, which are then used to evaluate the\naccuracy of the Chinese word embedding model. Moreover, we discovered that\nlegal relations might be ubiquitous in the word embedding model.", "published": "2022-03-29 01:26:26", "link": "http://arxiv.org/abs/2203.15173v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Generalization of Deep Neural Network Acoustic Models with\n  Length Perturbation and N-best Based Label Smoothing", "abstract": "We introduce two techniques, length perturbation and n-best based label\nsmoothing, to improve generalization of deep neural network (DNN) acoustic\nmodels for automatic speech recognition (ASR). Length perturbation is a data\naugmentation algorithm that randomly drops and inserts frames of an utterance\nto alter the length of the speech feature sequence. N-best based label\nsmoothing randomly injects noise to ground truth labels during training in\norder to avoid overfitting, where the noisy labels are generated from n-best\nhypotheses. We evaluate these two techniques extensively on the 300-hour\nSwitchboard (SWB300) dataset and an in-house 500-hour Japanese (JPN500) dataset\nusing recurrent neural network transducer (RNNT) acoustic models for ASR. We\nshow that both techniques improve the generalization of RNNT models\nindividually and they can also be complementary. In particular, they yield good\nimprovements over a strong SWB300 baseline and give state-of-art performance on\nSWB300 using RNNT models.", "published": "2022-03-29 01:40:22", "link": "http://arxiv.org/abs/2203.15176v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Visualizations of Complex Sequences of Family-Infant Vocalizations Using\n  Bag-of-Audio-Words Approach Based on Wav2vec 2.0 Features", "abstract": "In the U.S., approximately 15-17% of children 2-8 years of age are estimated\nto have at least one diagnosed mental, behavioral or developmental disorder.\nHowever, such disorders often go undiagnosed, and the ability to evaluate and\ntreat disorders in the first years of life is limited. To analyze infant\ndevelopmental changes, previous studies have shown advanced ML models excel at\nclassifying infant and/or parent vocalizations collected using cell phone,\nvideo, or audio-only recording device like LENA. In this study, we pilot test\nthe audio component of a new infant wearable multi-modal device that we have\ndeveloped called LittleBeats (LB). LB audio pipeline is advanced in that it\nprovides reliable labels for both speaker diarization and vocalization\nclassification tasks, compared with other platforms that only record audio\nand/or provide speaker diarization labels. We leverage wav2vec 2.0 to obtain\nsuperior and more nuanced results with the LB family audio stream. We use a\nbag-of-audio-words method with wav2vec 2.0 features to create high-level\nvisualizations to understand family-infant vocalization interactions. We\ndemonstrate that our high-quality visualizations capture major types of family\nvocalization interactions, in categories indicative of mental, behavioral, and\ndevelopmental health, for both labeled and unlabeled LB audio.", "published": "2022-03-29 01:46:14", "link": "http://arxiv.org/abs/2203.15183v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Shifted Chunk Encoder for Transformer Based Streaming End-to-End ASR", "abstract": "Currently, there are mainly three kinds of Transformer encoder based\nstreaming End to End (E2E) Automatic Speech Recognition (ASR) approaches,\nnamely time-restricted methods, chunk-wise methods, and memory-based methods.\nGenerally, all of them have limitations in aspects of linear computational\ncomplexity, global context modeling, and parallel training. In this work, we\naim to build a model to take all these three advantages for streaming\nTransformer ASR. Particularly, we propose a shifted chunk mechanism for the\nchunk-wise Transformer which provides cross-chunk connections between chunks.\nTherefore, the global context modeling ability of chunk-wise models can be\nsignificantly enhanced while all the original merits inherited. We integrate\nthis scheme with the chunk-wise Transformer and Conformer, and identify them as\nSChunk-Transformer and SChunk-Conformer, respectively. Experiments on AISHELL-1\nshow that the SChunk-Transformer and SChunk-Conformer can respectively achieve\nCER 6.43% and 5.77%. And the linear complexity makes them possible to train\nwith large batches and infer more efficiently. Our models can significantly\noutperform their conventional chunk-wise counterparts, while being competitive,\nwith only 0.22 absolute CER drop, when compared with U2 which has quadratic\ncomplexity. A better CER can be achieved if compared with existing chunk-wise\nor memory-based methods, such as HS-DACS and MMA. Code is released.", "published": "2022-03-29 03:02:35", "link": "http://arxiv.org/abs/2203.15206v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Applying Syntax$\\unicode{x2013}$Prosody Mapping Hypothesis and Prosodic\n  Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis", "abstract": "End-to-end text-to-speech synthesis (TTS), which generates speech sounds\ndirectly from strings of texts or phonemes, has improved the quality of speech\nsynthesis over the conventional TTS. However, most previous studies have been\nevaluated based on subjective naturalness and have not objectively examined\nwhether they can reproduce pitch patterns of phonological phenomena such as\ndownstep, rhythmic boost, and initial lowering that reflect syntactic\nstructures in Japanese. These phenomena can be linguistically explained by\nphonological constraints and the syntax$\\unicode{x2013}$prosody mapping\nhypothesis (SPMH), which assumes projections from syntactic structures to\nphonological hierarchy. Although some experiments in psycholinguistics have\nverified the validity of the SPMH, it is crucial to investigate whether it can\nbe implemented in TTS. To synthesize linguistic phenomena involving syntactic\nor phonological constraints, we propose a model using phonological symbols\nbased on the SPMH and prosodic well-formedness constraints. Experimental\nresults showed that the proposed method synthesized similar pitch patterns to\nthose reported in linguistics experiments for the phenomena of initial lowering\nand rhythmic boost. The proposed model efficiently synthesizes phonological\nphenomena in the test data that were not explicitly included in the training\ndata.", "published": "2022-03-29 06:45:28", "link": "http://arxiv.org/abs/2203.15276v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT\n  Models for Code Generation", "abstract": "Neural Machine Translation (NMT) has reached a level of maturity to be\nrecognized as the premier method for the translation between different\nlanguages and aroused interest in different research areas, including software\nengineering. A key step to validate the robustness of the NMT models consists\nin evaluating the performance of the models on adversarial inputs, i.e., inputs\nobtained from the original ones by adding small amounts of perturbation.\nHowever, when dealing with the specific task of the code generation (i.e., the\ngeneration of code starting from a description in natural language), it has not\nyet been defined an approach to validate the robustness of the NMT models. In\nthis work, we address the problem by identifying a set of perturbations and\nmetrics tailored for the robustness assessment of such models. We present a\npreliminary experimental evaluation, showing what type of perturbations affect\nthe model the most and deriving useful insights for future directions.", "published": "2022-03-29 08:01:39", "link": "http://arxiv.org/abs/2203.15319v2", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Noise-robust Speech Recognition with 10 Minutes Unparalleled In-domain\n  Data", "abstract": "Noise-robust speech recognition systems require large amounts of training\ndata including noisy speech data and corresponding transcripts to achieve\nstate-of-the-art performances in face of various practical environments.\nHowever, such plenty of in-domain data is not always available in the real-life\nworld. In this paper, we propose a generative adversarial network to simulate\nnoisy spectrum from the clean spectrum (Simu-GAN), where only 10 minutes of\nunparalleled in-domain noisy speech data is required as labels. Furthermore, we\nalso propose a dual-path speech recognition system to improve the robustness of\nthe system under noisy conditions. Experimental results show that the proposed\nspeech recognition system achieves 7.3% absolute improvement with simulated\nnoisy data by Simu-GAN over the best baseline in terms of word error rate\n(WER).", "published": "2022-03-29 08:06:01", "link": "http://arxiv.org/abs/2203.15321v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quality Assurance of Generative Dialog Models in an Evolving\n  Conversational Agent Used for Swedish Language Practice", "abstract": "Due to the migration megatrend, efficient and effective second-language\nacquisition is vital. One proposed solution involves AI-enabled conversational\nagents for person-centered interactive language practice. We present results\nfrom ongoing action research targeting quality assurance of proprietary\ngenerative dialog models trained for virtual job interviews. The action team\nelicited a set of 38 requirements for which we designed corresponding automated\ntest cases for 15 of particular interest to the evolving solution. Our results\nshow that six of the test case designs can detect meaningful differences\nbetween candidate models. While quality assurance of natural language\nprocessing applications is complex, we provide initial steps toward an\nautomated framework for machine learning model selection in the context of an\nevolving conversational agent. Future work will focus on model selection in an\nMLOps setting.", "published": "2022-03-29 10:25:13", "link": "http://arxiv.org/abs/2203.15414v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit", "abstract": "Recently, we made available WeNet, a production-oriented end-to-end speech\nrecognition toolkit, which introduces a unified two-pass (U2) framework and a\nbuilt-in runtime to address the streaming and non-streaming decoding modes in a\nsingle model. To further improve ASR performance and facilitate various\nproduction requirements, in this paper, we present WeNet 2.0 with four\nimportant updates. (1) We propose U2++, a unified two-pass framework with\nbidirectional attention decoders, which includes the future contextual\ninformation by a right-to-left attention decoder to improve the representative\nability of the shared encoder and the performance during the rescoring stage.\n(2) We introduce an n-gram based language model and a WFST-based decoder into\nWeNet 2.0, promoting the use of rich text data in production scenarios. (3) We\ndesign a unified contextual biasing framework, which leverages user-specific\ncontext (e.g., contact lists) to provide rapid adaptation ability for\nproduction and improves ASR accuracy in both with-LM and without-LM scenarios.\n(4) We design a unified IO to support large-scale data for effective model\ntraining. In summary, the brand-new WeNet 2.0 achieves up to 10\\% relative\nrecognition performance improvement over the original WeNet on various corpora\nand makes available several important production-oriented features.", "published": "2022-03-29 11:54:34", "link": "http://arxiv.org/abs/2203.15455v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Segmentation Optimization using Segmented Bilingual Speech Corpus\n  for End-to-end Speech Translation", "abstract": "Speech segmentation, which splits long speech into short segments, is\nessential for speech translation (ST). Popular VAD tools like WebRTC VAD have\ngenerally relied on pause-based segmentation. Unfortunately, pauses in speech\ndo not necessarily match sentence boundaries, and sentences can be connected by\na very short pause that is difficult to detect by VAD. In this study, we\npropose a speech segmentation method using a binary classification model\ntrained using a segmented bilingual speech corpus. We also propose a hybrid\nmethod that combines VAD and the above speech segmentation method. Experimental\nresults revealed that the proposed method is more suitable for cascade and\nend-to-end ST systems than conventional segmentation methods. The hybrid\napproach further improved the translation performance.", "published": "2022-03-29 12:26:56", "link": "http://arxiv.org/abs/2203.15479v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Interactive Audio-text Representation for Automated Audio Captioning\n  with Contrastive Learning", "abstract": "Automated Audio captioning (AAC) is a cross-modal task that generates natural\nlanguage to describe the content of input audio. Most prior works usually\nextract single-modality acoustic features and are therefore sub-optimal for the\ncross-modal decoding task. In this work, we propose a novel AAC system called\nCLIP-AAC to learn interactive cross-modality representation with both acoustic\nand textual information. Specifically, the proposed CLIP-AAC introduces an\naudio-head and a text-head in the pre-trained encoder to extract audio-text\ninformation. Furthermore, we also apply contrastive learning to narrow the\ndomain difference by learning the correspondence between the audio signal and\nits paired captions. Experimental results show that the proposed CLIP-AAC\napproach surpasses the best baseline by a significant margin on the Clotho\ndataset in terms of NLP evaluation metrics. The ablation study indicates that\nboth the pre-trained model and contrastive learning contribute to the\nperformance gain of the AAC model.", "published": "2022-03-29 13:06:46", "link": "http://arxiv.org/abs/2203.15526v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LightHuBERT: Lightweight and Configurable Speech Representation Learning\n  with Once-for-All Hidden-Unit BERT", "abstract": "Self-supervised speech representation learning has shown promising results in\nvarious speech processing tasks. However, the pre-trained models, e.g., HuBERT,\nare storage-intensive Transformers, limiting their scope of applications under\nlow-resource settings. To this end, we propose LightHuBERT, a once-for-all\nTransformer compression framework, to find the desired architectures\nautomatically by pruning structured parameters. More precisely, we create a\nTransformer-based supernet that is nested with thousands of weight-sharing\nsubnets and design a two-stage distillation strategy to leverage the\ncontextualized latent representations from HuBERT. Experiments on automatic\nspeech recognition (ASR) and the SUPERB benchmark show the proposed LightHuBERT\nenables over $10^9$ architectures concerning the embedding dimension, attention\ndimension, head number, feed-forward network ratio, and network depth.\nLightHuBERT outperforms the original HuBERT on ASR and five SUPERB tasks with\nthe HuBERT size, achieves comparable performance to the teacher model in most\ntasks with a reduction of 29% parameters, and obtains a $3.5\\times$ compression\nratio in three SUPERB tasks, e.g., automatic speaker verification, keyword\nspotting, and intent classification, with a slight accuracy loss. The code and\npre-trained models are available at\nhttps://github.com/mechanicalsea/lighthubert.", "published": "2022-03-29 14:20:55", "link": "http://arxiv.org/abs/2203.15610v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Latency for CTC-Based Streaming Automatic Speech Recognition\n  With Emformer", "abstract": "An inferior performance of the streaming automatic speech recognition models\nversus non-streaming model is frequently seen due to the absence of future\ncontext. In order to improve the performance of the streaming model and reduce\nthe computational complexity, a frame-level model using efficient augment\nmemory transformer block and dynamic latency training method is employed for\nstreaming automatic speech recognition in this paper. The long-range history\ncontext is stored into the augment memory bank as a complement to the limited\nhistory context used in the encoder. Key and value are cached by a cache\nmechanism and reused for next chunk to reduce computation. Afterwards, a\ndynamic latency training method is proposed to obtain better performance and\nsupport low and high latency inference simultaneously. Our experiments are\nconducted on benchmark 960h LibriSpeech data set. With an average latency of\n640ms, our model achieves a relative WER reduction of 6.0% on test-clean and\n3.0% on test-other versus the truncate chunk-wise Transformer.", "published": "2022-03-29 14:31:06", "link": "http://arxiv.org/abs/2203.15613v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Integrating Lattice-Free MMI into End-to-End Speech Recognition", "abstract": "In automatic speech recognition (ASR) research, discriminative criteria have\nachieved superior performance in DNN-HMM systems. Given this success, the\nadoption of discriminative criteria is promising to boost the performance of\nend-to-end (E2E) ASR systems. With this motivation, previous works have\nintroduced the minimum Bayesian risk (MBR, one of the discriminative criteria)\ninto E2E ASR systems. However, the effectiveness and efficiency of the\nMBR-based methods are compromised: the MBR criterion is only used in system\ntraining, which creates a mismatch between training and decoding; the\non-the-fly decoding process in MBR-based methods results in the need for\npre-trained models and slow training speeds. To this end, novel algorithms are\nproposed in this work to integrate another widely used discriminative\ncriterion, lattice-free maximum mutual information (LF-MMI), into E2E ASR\nsystems not only in the training stage but also in the decoding process. The\nproposed LF-MMI training and decoding methods show their effectiveness on two\nwidely used E2E frameworks: Attention-Based Encoder-Decoders (AEDs) and Neural\nTransducers (NTs). Compared with MBR-based methods, the proposed LF-MMI method:\nmaintains the consistency between training and decoding; eschews the on-the-fly\ndecoding process; trains from randomly initialized models with superior\ntraining efficiency. Experiments suggest that the LF-MMI method outperforms its\nMBR counterparts and consistently leads to statistically significant\nperformance improvements on various frameworks and datasets from 30 hours to\n14.3k hours. The proposed method achieves state-of-the-art (SOTA) results on\nAishell-1 (CER 4.10%) and Aishell-2 (CER 5.02%) datasets. Code is released.", "published": "2022-03-29 14:32:46", "link": "http://arxiv.org/abs/2203.15614v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "EnvEdit: Environment Editing for Vision-and-Language Navigation", "abstract": "In Vision-and-Language Navigation (VLN), an agent needs to navigate through\nthe environment based on natural language instructions. Due to limited\navailable data for agent training and finite diversity in navigation\nenvironments, it is challenging for the agent to generalize to new, unseen\nenvironments. To address this problem, we propose EnvEdit, a data augmentation\nmethod that creates new environments by editing existing environments, which\nare used to train a more generalizable agent. Our augmented environments can\ndiffer from the seen environments in three diverse aspects: style, object\nappearance, and object classes. Training on these edit-augmented environments\nprevents the agent from overfitting to existing environments and helps\ngeneralize better to new, unseen environments. Empirically, on both the\nRoom-to-Room and the multi-lingual Room-Across-Room datasets, we show that our\nproposed EnvEdit method gets significant improvements in all metrics on both\npre-trained and non-pre-trained VLN agents, and achieves the new\nstate-of-the-art on the test leaderboard. We further ensemble the VLN agents\naugmented on different edited environments and show that these edit methods are\ncomplementary. Code and data are available at\nhttps://github.com/jialuli-luka/EnvEdit", "published": "2022-03-29 15:44:32", "link": "http://arxiv.org/abs/2203.15685v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Forecasting with Economic News", "abstract": "The goal of this paper is to evaluate the informational content of sentiment\nextracted from news articles about the state of the economy. We propose a\nfine-grained aspect-based sentiment analysis that has two main characteristics:\n1) we consider only the text in the article that is semantically dependent on a\nterm of interest (aspect-based) and, 2) assign a sentiment score to each word\nbased on a dictionary that we develop for applications in economics and finance\n(fine-grained). Our data set includes six large US newspapers, for a total of\nover 6.6 million articles and 4.2 billion words. Our findings suggest that\nseveral measures of economic sentiment track closely business cycle\nfluctuations and that they are relevant predictors for four major macroeconomic\nvariables. We find that there are significant improvements in forecasting when\nsentiment is considered along with macroeconomic factors. In addition, we also\nfind that sentiment matters to explains the tails of the probability\ndistribution across several macroeconomic variables.", "published": "2022-03-29 15:46:42", "link": "http://arxiv.org/abs/2203.15686v1", "categories": ["cs.CE", "cs.AI", "cs.CL", "stat.AP"], "primary_category": "cs.CE"}
{"title": "Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting", "abstract": "Large language models have shown that impressive zero-shot performance can be\nachieved through natural language prompts (Radford et al., 2019; Brown et al.,\n2020; Sanh et al., 2021). Creating an effective prompt, however, requires\nsignificant trial and error. That \\textit{prompts} the question: how do the\nqualities of a prompt effects its performance? To this end, we collect and\nstandardize prompts from a diverse range of tasks for use with tasks they were\nnot designed for. We then evaluate these prompts across fixed multiple choice\ndatasets for a quantitative analysis of how certain attributes of a prompt\naffect performance. We find that including the choices and using prompts not\nused during pre-training provide significant improvements. All experiments and\ncode can be found https://github.com/gabeorlanski/zero-shot-cross-task.", "published": "2022-03-29 17:04:17", "link": "http://arxiv.org/abs/2203.15754v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Streaming parallel transducer beam search with fast-slow cascaded\n  encoders", "abstract": "Streaming ASR with strict latency constraints is required in many speech\nrecognition applications. In order to achieve the required latency, streaming\nASR models sacrifice accuracy compared to non-streaming ASR models due to lack\nof future input context. Previous research has shown that streaming and\nnon-streaming ASR for RNN Transducers can be unified by cascading causal and\nnon-causal encoders. This work improves upon this cascaded encoders framework\nby leveraging two streaming non-causal encoders with variable input context\nsizes that can produce outputs at different audio intervals (e.g. fast and\nslow). We propose a novel parallel time-synchronous beam search algorithm for\ntransducers that decodes from fast-slow encoders, where the slow encoder\ncorrects the mistakes generated from the fast encoder. The proposed algorithm,\nachieves up to 20% WER reduction with a slight increase in token emission\ndelays on the public Librispeech dataset and in-house datasets. We also explore\ntechniques to reduce the computation by distributing processing between the\nfast and slow encoders. Lastly, we explore sharing the parameters in the fast\nencoder to reduce the memory footprint. This enables low latency processing on\nedge devices with low computation cost and a low memory footprint.", "published": "2022-03-29 17:29:39", "link": "http://arxiv.org/abs/2203.15773v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen\n  Language Models", "abstract": "Large-scale auto-regressive language models pretrained on massive text have\ndemonstrated their impressive ability to perform new natural language tasks\nwith only a few text examples, without the need for fine-tuning. Recent studies\nfurther show that such a few-shot learning ability can be extended to the\ntext-image setting by training an encoder to encode the images into embeddings\nfunctioning like the text embeddings of the language model. Interested in\nexploring the possibility of transferring the few-shot learning ability to the\naudio-text setting, we propose a novel speech understanding framework,\nWavPrompt, where we finetune a wav2vec model to generate a sequence of audio\nembeddings understood by the language model. We show that WavPrompt is a\nfew-shot learner that can perform speech understanding tasks better than a\nnaive text baseline. We conduct detailed ablation studies on different\ncomponents and hyperparameters to empirically identify the best model\nconfiguration. In addition, we conduct a non-speech understanding experiment to\nshow WavPrompt can extract more information than just the transcriptions. Code\nis available at https://github.com/Hertin/WavPrompt", "published": "2022-03-29 19:08:55", "link": "http://arxiv.org/abs/2203.15863v2", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Improving Mispronunciation Detection with Wav2vec2-based Momentum\n  Pseudo-Labeling for Accentedness and Intelligibility Assessment", "abstract": "Current leading mispronunciation detection and diagnosis (MDD) systems\nachieve promising performance via end-to-end phoneme recognition. One challenge\nof such end-to-end solutions is the scarcity of human-annotated phonemes on\nnatural L2 speech. In this work, we leverage unlabeled L2 speech via a\npseudo-labeling (PL) procedure and extend the fine-tuning approach based on\npre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec\n2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples\nplus the created pseudo-labeled L2 speech samples. Our pseudo labels are\ndynamic and are produced by an ensemble of the online model on-the-fly, which\nensures that our model is robust to pseudo label noise. We show that\nfine-tuning with pseudo labels achieves a 5.35% phoneme error rate reduction\nand 2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning\nbaseline. The proposed PL method is also shown to outperform conventional\noffline PL methods. Compared to the state-of-the-art MDD systems, our MDD\nsolution produces a more accurate and consistent phonetic error diagnosis. In\naddition, we conduct an open test on a separate UTD-4Accents dataset, where our\nsystem recognition outputs show a strong correlation with human perception,\nbased on accentedness and intelligibility.", "published": "2022-03-29 22:40:31", "link": "http://arxiv.org/abs/2203.15937v3", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "A Cognitive Architecture for Machine Consciousness and Artificial\n  Superintelligence: Thought Is Structured by the Iterative Updating of Working\n  Memory", "abstract": "This article provides an analytical framework for how to simulate human-like\nthought processes within a computer. It describes how attention and memory\nshould be structured, updated, and utilized to search for associative additions\nto the stream of thought. The focus is on replicating the dynamics of the\nmammalian working memory system, which features two forms of persistent\nactivity: sustained firing (preserving information on the order of seconds) and\nsynaptic potentiation (preserving information from minutes to hours). The\narticle uses a series of figures to systematically demonstrate how the\niterative updating of these working memory stores provides functional\norganization to behavior, cognition, and awareness.\n  In a machine learning implementation, these two memory stores should be\nupdated continuously and in an iterative fashion. This means each state should\npreserve a proportion of the coactive representations from the state before it\n(where each representation is an ensemble of neural network nodes). This makes\neach state a revised iteration of the preceding state and causes successive\nconfigurations to overlap and blend with respect to the information they\ncontain. Thus, the set of concepts in working memory will evolve gradually and\nincrementally over time. Transitions between states happen as persistent\nactivity spreads activation energy throughout the hierarchical network,\nsearching long-term memory for the most appropriate representation to be added\nto the global workspace. The result is a chain of associatively linked\nintermediate states capable of advancing toward a solution or goal. Iterative\nupdating is conceptualized here as an information processing strategy, a model\nof working memory, a theory of consciousness, and an algorithm for designing\nand programming artificial intelligence (AI, AGI, and ASI).", "published": "2022-03-29 22:28:30", "link": "http://arxiv.org/abs/2203.17255v7", "categories": ["q-bio.NC", "cs.CL", "cs.CV"], "primary_category": "q-bio.NC"}
{"title": "ASR data augmentation in low-resource settings using cross-lingual\n  multi-speaker TTS and cross-lingual voice conversion", "abstract": "We explore cross-lingual multi-speaker speech synthesis and cross-lingual\nvoice conversion applied to data augmentation for automatic speech recognition\n(ASR) systems in low/medium-resource scenarios. Through extensive experiments,\nwe show that our approach permits the application of speech synthesis and voice\nconversion to improve ASR systems using only one target-language speaker during\nmodel training. We also managed to close the gap between ASR models trained\nwith synthesized versus human speech compared to other works that use many\nspeakers. Finally, we show that it is possible to obtain promising ASR training\nresults with our data augmentation method using only a single real speaker in a\ntarget language.", "published": "2022-03-29 11:55:30", "link": "http://arxiv.org/abs/2204.00618v5", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analysis of EEG frequency bands for Envisioned Speech Recognition", "abstract": "The use of Automatic speech recognition (ASR) interfaces have become\nincreasingly popular in daily life for use in interaction and control of\nelectronic devices. The interfaces currently being used are not feasible for a\nvariety of users such as those suffering from a speech disorder, locked-in\nsyndrome, paralysis or people with utmost privacy requirements. In such cases,\nan interface that can identify envisioned speech using electroencephalogram\n(EEG) signals can be of great benefit. Various works targeting this problem\nhave been done in the past. However, there has been limited work in identifying\nthe frequency bands ($\\delta, \\theta, \\alpha, \\beta, \\gamma$) of the EEG signal\nthat contribute towards envisioned speech recognition. Therefore, in this work,\nwe aim to analyze the significance of different EEG frequency bands and signals\nobtained from different lobes of the brain and their contribution towards\nrecognizing envisioned speech. Signals obtained from different lobes and\nbandpass filtered for different frequency bands are fed to a spatio-temporal\ndeep learning architecture with Convolutional Neural Network (CNN) and Long\nShort-Term Memory (LSTM). The performance is evaluated on a publicly available\ndataset comprising of three classification tasks - digit, character and images.\nWe obtain a classification accuracy of $85.93\\%$, $87.27\\%$ and $87.51\\%$ for\nthe three tasks respectively. The code for the implementation has been made\navailable at https://github.com/ayushayt/ImaginedSpeechRecognition.", "published": "2022-03-29 05:50:09", "link": "http://arxiv.org/abs/2203.15250v1", "categories": ["eess.SP", "cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "eess.SP"}
{"title": "Nix-TTS: Lightweight and End-to-End Text-to-Speech via Module-wise\n  Distillation", "abstract": "Several solutions for lightweight TTS have shown promising results. Still,\nthey either rely on a hand-crafted design that reaches non-optimum size or use\na neural architecture search but often suffer training costs. We present\nNix-TTS, a lightweight TTS achieved via knowledge distillation to a\nhigh-quality yet large-sized, non-autoregressive, and end-to-end (vocoder-free)\nTTS teacher model. Specifically, we offer module-wise distillation, enabling\nflexible and independent distillation to the encoder and decoder module. The\nresulting Nix-TTS inherited the advantageous properties of being\nnon-autoregressive and end-to-end from the teacher, yet significantly smaller\nin size, with only 5.23M parameters or up to 89.34% reduction of the teacher\nmodel; it also achieves over 3.04x and 8.36x inference speedup on Intel-i7 CPU\nand Raspberry Pi 3B respectively and still retains a fair voice naturalness and\nintelligibility compared to the teacher model. We provide pretrained models and\naudio samples of Nix-TTS.", "published": "2022-03-29 15:04:26", "link": "http://arxiv.org/abs/2203.15643v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "cs.NE", "eess.AS", "68T50 (Primary) 68T07, 68T10, 68T99 (Secondary)", "I.2.7; I.2.6; H.5.5"], "primary_category": "cs.SD"}
{"title": "Frequency Dynamic Convolution: Frequency-Adaptive Pattern Recognition\n  for Sound Event Detection", "abstract": "2D convolution is widely used in sound event detection (SED) to recognize two\ndimensional time-frequency patterns of sound events. However, 2D convolution\nenforces translation equivariance on sound events along both time and frequency\naxis while frequency is not shift-invariant dimension. In order to improve\nphysical consistency of 2D convolution on SED, we propose frequency dynamic\nconvolution which applies kernel that adapts to frequency components of input.\nFrequency dynamic convolution outperforms the baseline by 6.3% in DESED\nvalidation dataset in terms of polyphonic sound detection score (PSDS). It also\nsignificantly outperforms other pre-existing content-adaptive methods on SED.\nIn addition, by comparing class-wise F1 scores of baseline and frequency\ndynamic convolution, we showed that frequency dynamic convolution is especially\nmore effective for detection of non-stationary sound events with intricate\ntime-frequency patterns. From this result, we verified that frequency dynamic\nconvolution is superior in recognizing frequency-dependent patterns.", "published": "2022-03-29 07:27:27", "link": "http://arxiv.org/abs/2203.15296v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Training Speaker Embedding Extractors Using Multi-Speaker Audio with\n  Unknown Speaker Boundaries", "abstract": "In this paper, we demonstrate a method for training speaker embedding\nextractors using weak annotation. More specifically, we are using the full\nVoxCeleb recordings and the name of the celebrities appearing on each video\nwithout knowledge of the time intervals the celebrities appear in the video. We\nshow that by combining a baseline speaker diarization algorithm that requires\nno training or parameter tuning, a modified loss with aggregation over\nsegments, and a two-stage training approach, we are able to train a competitive\nResNet-based embedding extractor. Finally, we experiment with two different\naggregation functions and analyze their behaviour in terms of their gradients.", "published": "2022-03-29 11:06:08", "link": "http://arxiv.org/abs/2203.15436v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Frequency-Directional Attention Model for Multilingual Automatic Speech\n  Recognition", "abstract": "This paper proposes a model for transforming speech features using the\nfrequency-directional attention model for End-to-End (E2E) automatic speech\nrecognition. The idea is based on the hypothesis that in the phoneme system of\neach language, the characteristics of the frequency bands of speech when\nuttering them are different. By transforming the input Mel filter bank features\nwith an attention model that characterizes the frequency direction, a feature\ntransformation suitable for ASR in each language can be expected. This paper\nintroduces a Transformer-encoder as a frequency-directional attention model. We\nevaluated the proposed method on a multilingual E2E ASR system for six\ndifferent languages and found that the proposed method could achieve, on\naverage, 5.3 points higher accuracy than the ASR model for each language by\nintroducing the frequency-directional attention mechanism. Furthermore,\nvisualization of the attention weights based on the proposed method suggested\nthat it is possible to transform acoustic features considering the frequency\ncharacteristics of each language.", "published": "2022-03-29 12:21:20", "link": "http://arxiv.org/abs/2203.15473v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic\n  Speaker Verification", "abstract": "In this paper, we present Multi-scale Feature Aggregation Conformer\n(MFA-Conformer), an easy-to-implement, simple but effective backbone for\nautomatic speaker verification based on the Convolution-augmented Transformer\n(Conformer). The architecture of the MFA-Conformer is inspired by recent\nstateof-the-art models in speech recognition and speaker verification. Firstly,\nwe introduce a convolution subsampling layer to decrease the computational cost\nof the model. Secondly, we adopt Conformer blocks which combine Transformers\nand convolution neural networks (CNNs) to capture global and local features\neffectively. Finally, the output feature maps from all Conformer blocks are\nconcatenated to aggregate multi-scale representations before final pooling. We\nevaluate the MFA-Conformer on the widely used benchmarks. The best system\nobtains 0.64%, 1.29% and 1.63% EER on VoxCeleb1-O, SITW.Dev, and SITW.Eval set,\nrespectively. MFA-Conformer significantly outperforms the popular ECAPA-TDNN\nsystems in both recognition performance and inference speed. Last but not the\nleast, the ablation studies clearly demonstrate that the combination of global\nand local feature learning can lead to robust and accurate speaker embedding\nextraction. We have also released the code for future comparison.", "published": "2022-03-29 05:48:24", "link": "http://arxiv.org/abs/2203.15249v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Decomposed Temporal Dynamic CNN: Efficient Time-Adaptive Network for\n  Text-Independent Speaker Verification Explained with Speaker Activation Map", "abstract": "To extract accurate speaker information for text-independent speaker\nverification, temporal dynamic CNNs (TDY-CNNs) adapting kernels to each time\nbin was proposed. However, model size of TDY-CNN is too large and the adaptive\nkernel's degree of freedom is limited. To address these limitations, we propose\ndecomposed temporal dynamic CNNs (DTDY-CNNs) which forms time-adaptive kernel\nby combining static kernel with dynamic residual based on matrix decomposition.\nProposed DTDY-ResNet-34(x0.50) using attentive statistical pooling without data\naugmentation shows EER of 0.96%, which is better than other state-of-the-art\nmethods. DTDY-CNNs are successful upgrade of TDY-CNNs, reducing the model size\nby 64% and enhancing the performance. We showed that DTDY-CNNs extract more\naccurate frame-level speaker embeddings as well compared to TDY-CNNs. Detailed\nbehaviors of DTDY-ResNet-34(x0.50) on extraction of speaker information were\nanalyzed using speaker activation map (SAM) produced by modified\ngradient-weighted class activation mapping (Grad-CAM) for speaker verification.\nDTDY-ResNet-34(x0.50) effectively extracts speaker information from not only\nformant frequencies but also high frequency information of unvoiced phonemes,\nthus explaining its outstanding performance on text-independent speaker\nverification.", "published": "2022-03-29 06:46:53", "link": "http://arxiv.org/abs/2203.15277v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mel Frequency Spectral Domain Defenses against Adversarial Attacks on\n  Speech Recognition Systems", "abstract": "A variety of recent works have looked into defenses for deep neural networks\nagainst adversarial attacks particularly within the image processing domain.\nSpeech processing applications such as automatic speech recognition (ASR) are\nincreasingly relying on deep learning models, and so are also prone to\nadversarial attacks. However, many of the defenses explored for ASR simply\nadapt the image-domain defenses, which may not provide optimal robustness. This\npaper explores speech specific defenses using the mel spectral domain, and\nintroduces a novel defense method called 'mel domain noise flooding' (MDNF).\nMDNF applies additive noise to the mel spectrogram of a speech utterance prior\nto re-synthesising the audio signal. We test the defenses against strong\nwhite-box adversarial attacks such as projected gradient descent (PGD) and\nCarlini-Wagner (CW) attacks, and show better robustness compared to a\nrandomized smoothing baseline across strong threat models.", "published": "2022-03-29 06:58:26", "link": "http://arxiv.org/abs/2203.15283v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Automatic Detection of Speech Sound Disorder in Child Speech Using\n  Posterior-based Speaker Representations", "abstract": "This paper presents a macroscopic approach to automatic detection of speech\nsound disorder (SSD) in child speech. Typically, SSD is manifested by\npersistent articulation and phonological errors on specific phonemes in the\nlanguage. The disorder can be detected by focally analyzing the phonemes or the\nwords elicited by the child subject. In the present study, instead of\nattempting to detect individual phone- and word-level errors, we propose to\nextract a subject-level representation from a long utterance that is\nconstructed by concatenating multiple test words. The speaker verification\napproach, and posterior features generated by deep neural network models, are\napplied to derive various types of holistic representations. A linear\nclassifier is trained to differentiate disordered speech in normal one. On the\ntask of detecting SSD in Cantonese-speaking children, experimental results show\nthat the proposed approach achieves improved detection performance over\nprevious method that requires fusing phone-level detection results. Using\narticulatory posterior features to derive i-vectors from multiple-word\nutterances achieves an unweighted average recall of 78.2% and a macro F1 score\nof 78.0%.", "published": "2022-03-29 10:05:52", "link": "http://arxiv.org/abs/2203.15405v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating Self-supervised Pretraining Frameworks for Pathological\n  Speech Recognition", "abstract": "We investigate the performance of self-supervised pretraining frameworks on\npathological speech datasets used for automatic speech recognition (ASR).\nModern end-to-end models require thousands of hours of data to train well, but\nonly a small number of pathological speech datasets are publicly available. A\nproven solution to this problem is by first pretraining the model on a huge\nnumber of healthy speech datasets and then fine-tuning it on the pathological\nspeech datasets. One new pretraining framework called self-supervised learning\n(SSL) trains a network using only speech data, providing more flexibility in\ntraining data requirements and allowing more speech data to be used in\npretraining. We investigate SSL frameworks such as the wav2vec 2.0 and WavLM\nmodels using different setups and compare their performance with different\nsupervised pretraining setups, using two types of pathological speech, namely,\nJapanese electrolaryngeal and English dysarthric. Our results show that\nalthough SSL has shown success with minimally resourced healthy speech, we do\nnot find this to be the case with pathological speech. The best supervised\nsetup outperforms the best SSL setup by 13.9% character error rate in\nelectrolaryngeal speech and 16.8% word error rate in dysarthric speech.", "published": "2022-03-29 10:54:35", "link": "http://arxiv.org/abs/2203.15431v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transfer Learning Framework for Low-Resource Text-to-Speech using a\n  Large-Scale Unlabeled Speech Corpus", "abstract": "Training a text-to-speech (TTS) model requires a large scale text labeled\nspeech corpus, which is troublesome to collect. In this paper, we propose a\ntransfer learning framework for TTS that utilizes a large amount of unlabeled\nspeech dataset for pre-training. By leveraging wav2vec2.0 representation,\nunlabeled speech can highly improve performance, especially in the lack of\nlabeled speech. We also extend the proposed method to zero-shot multi-speaker\nTTS (ZS-TTS). The experimental results verify the effectiveness of the proposed\nmethod in terms of naturalness, intelligibility, and speaker generalization. We\nhighlight that the single speaker TTS model fine-tuned on the only 10 minutes\nof labeled dataset outperforms the other baselines, and the ZS-TTS model\nfine-tuned on the only 30 minutes of single speaker dataset can generate the\nvoice of the arbitrary speaker, by pre-training on unlabeled multi-speaker\nspeech corpus.", "published": "2022-03-29 11:26:56", "link": "http://arxiv.org/abs/2203.15447v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "On Metric Learning for Audio-Text Cross-Modal Retrieval", "abstract": "Audio-text retrieval aims at retrieving a target audio clip or caption from a\npool of candidates given a query in another modality. Solving such cross-modal\nretrieval task is challenging because it not only requires learning robust\nfeature representations for both modalities, but also requires capturing the\nfine-grained alignment between these two modalities. Existing cross-modal\nretrieval models are mostly optimized by metric learning objectives as both of\nthem attempt to map data to an embedding space, where similar data are close\ntogether and dissimilar data are far apart. Unlike other cross-modal retrieval\ntasks such as image-text and video-text retrievals, audio-text retrieval is\nstill an unexplored task. In this work, we aim to study the impact of different\nmetric learning objectives on the audio-text retrieval task. We present an\nextensive evaluation of popular metric learning objectives on the AudioCaps and\nClotho datasets. We demonstrate that NT-Xent loss adapted from self-supervised\nlearning shows stable performance across different datasets and training\nsettings, and outperforms the popular triplet-based losses. Our code is\navailable at https://github.com/XinhaoMei/audio-text_retrieval.", "published": "2022-03-29 13:18:50", "link": "http://arxiv.org/abs/2203.15537v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Locality Matters: A Locality-Biased Linear Attention for Automatic\n  Speech Recognition", "abstract": "Conformer has shown a great success in automatic speech recognition (ASR) on\nmany public benchmarks. One of its crucial drawbacks is the quadratic\ntime-space complexity with respect to the input sequence length, which\nprohibits the model to scale-up as well as process longer input audio\nsequences. To solve this issue, numerous linear attention methods have been\nproposed. However, these methods often have limited performance on ASR as they\ntreat tokens equally in modeling, neglecting the fact that the neighbouring\ntokens are often more connected than the distanced tokens. In this paper, we\ntake this fact into account and propose a new locality-biased linear attention\nfor Conformer. It not only achieves higher accuracy than the vanilla Conformer,\nbut also enjoys linear space-time computational complexity. To be specific, we\nreplace the softmax attention with a locality-biased linear attention (LBLA)\nmechanism in Conformer blocks. The LBLA contains a kernel function to ensure\nthe linear complexities and a cosine reweighing matrix to impose more weights\non neighbouring tokens. Extensive experiments on the LibriSpeech corpus show\nthat by introducing this locality bias to the Conformer, our method achieves a\nlower word error rate with more than 22% inference speed.", "published": "2022-03-29 14:20:00", "link": "http://arxiv.org/abs/2203.15609v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CycleGAN-Based Unpaired Speech Dereverberation", "abstract": "Typically, neural network-based speech dereverberation models are trained on\npaired data, composed of a dry utterance and its corresponding reverberant\nutterance. The main limitation of this approach is that such models can only be\ntrained on large amounts of data and a variety of room impulse responses when\nthe data is synthetically reverberated, since acquiring real paired data is\ncostly. In this paper we propose a CycleGAN-based approach that enables\ndereverberation models to be trained on unpaired data. We quantify the impact\nof using unpaired data by comparing the proposed unpaired model to a paired\nmodel with the same architecture and trained on the paired version of the same\ndataset. We show that the performance of the unpaired model is comparable to\nthe performance of the paired model on two different datasets, according to\nobjective evaluation metrics. Furthermore, we run two subjective evaluations\nand show that both models achieve comparable subjective quality on the AMI\ndataset, which was not seen during training.", "published": "2022-03-29 15:10:30", "link": "http://arxiv.org/abs/2203.15652v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level\n  and Utterance-Level Acoustic Representation Learning", "abstract": "Most text-to-speech (TTS) methods use high-quality speech corpora recorded in\na well-designed environment, incurring a high cost for data collection. To\nsolve this problem, existing noise-robust TTS methods are intended to use noisy\nspeech corpora as training data. However, they only address either\ntime-invariant or time-variant noises. We propose a degradation-robust TTS\nmethod, which can be trained on speech corpora that contain both additive\nnoises and environmental distortions. It jointly represents the time-variant\nadditive noises with a frame-level encoder and the time-invariant environmental\ndistortions with an utterance-level encoder. We also propose a regularization\nmethod to attain clean environmental embedding that is disentangled from the\nutterance-dependent information such as linguistic contents and speaker\ncharacteristics. Evaluation results show that our method achieved significantly\nhigher-quality synthetic speech than previous methods in the condition\nincluding both additive noise and reverberation.", "published": "2022-03-29 15:41:52", "link": "http://arxiv.org/abs/2203.15683v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Target Geometry Estimation Using Deep Neural Networks in Sonar Sensing", "abstract": "Accurate imaging of target shape is a crucial aspect of wideband FM biosonar\nin echolocating bats, for which we have developed new algorithms that provide a\nsolution for the shape of complicated targets in the computational domain. We\nuse recurrent neural networks and convolutional neural networks to determine\nthe number of glints (i.e., major reflecting surfaces) making up the target's\nstructure and the distances between the glints (target shape in sonar). Echoes\nare dechirped relative to broadcasts, and the dechirped spectrograms are\nscanned in short time segments to find local spectral ripple patterns arising\nfrom different interglint delay separations. By proceeding in successive\ntime-window slices, we mimic time-frequency neural processing in the bat's\nauditory system as a novel means of real-time target discrimination for sonar\nsensing in robotics.", "published": "2022-03-29 17:27:57", "link": "http://arxiv.org/abs/2203.15770v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech\n  Recognition", "abstract": "An unsupervised text-to-speech synthesis (TTS) system learns to generate\nspeech waveforms corresponding to any written sentence in a language by\nobserving: 1) a collection of untranscribed speech waveforms in that language;\n2) a collection of texts written in that language without access to any\ntranscribed speech. Developing such a system can significantly improve the\navailability of speech technology to languages without a large amount of\nparallel speech and text data. This paper proposes an unsupervised TTS system\nbased on an alignment module that outputs pseudo-text and another synthesis\nmodule that uses pseudo-text for training and real text for inference. Our\nunsupervised system can achieve comparable performance to the supervised system\nin seven languages with about 10-20 hours of speech each. A careful study on\nthe effect of text units and vocoders has also been conducted to better\nunderstand what factors may affect unsupervised TTS performance. The samples\ngenerated by our models can be found at\nhttps://cactuswiththoughts.github.io/UnsupTTS-Demo, and our code can be found\nat https://github.com/lwang114/UnsupTTS.", "published": "2022-03-29 17:57:53", "link": "http://arxiv.org/abs/2203.15796v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "4-bit Conformer with Native Quantization Aware Training for Speech\n  Recognition", "abstract": "Reducing the latency and model size has always been a significant research\nproblem for live Automatic Speech Recognition (ASR) application scenarios.\nAlong this direction, model quantization has become an increasingly popular\napproach to compress neural networks and reduce computation cost. Most of the\nexisting practical ASR systems apply post-training 8-bit quantization. To\nachieve a higher compression rate without introducing additional performance\nregression, in this study, we propose to develop 4-bit ASR models with native\nquantization aware training, which leverages native integer operations to\neffectively optimize both training and inference. We conducted two experiments\non state-of-the-art Conformer-based ASR models to evaluate our proposed\nquantization technique. First, we explored the impact of different precisions\nfor both weight and activation quantization on the LibriSpeech dataset, and\nobtained a lossless 4-bit Conformer model with 5.8x size reduction compared to\nthe float32 model. Following this, we for the first time investigated and\nrevealed the viability of 4-bit quantization on a practical ASR system that is\ntrained with large-scale datasets, and produced a lossless Conformer ASR model\nwith mixed 4-bit and 8-bit weights that has 5x size reduction compared to the\nfloat32 model.", "published": "2022-03-29 23:57:15", "link": "http://arxiv.org/abs/2203.15952v4", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "NeuraGen-A Low-Resource Neural Network based approach for Gender\n  Classification", "abstract": "Human voice is the source of several important information. This is in the\nform of features. These Features help in interpreting various features\nassociated with the speaker and speech. The speaker dependent work\nresearchersare targeted towards speaker identification, Speaker verification,\nspeaker biometric, forensics using feature, and cross-modal matching via speech\nand face images. In such context research, it is a very difficult task to come\nacross clean, and well annotated publicly available speech corpus as data set.\nAcquiring volunteers to generate such dataset is also very expensive, not to\nmention the enormous amount of effort and time researchers spend to gather such\ndata. The present paper work, a Neural Network proposal as NeuraGen focused\nwhich is a low-resource ANN architecture. The proposed tool used to classify\ngender of the speaker from the speech recordings. We have used speech\nrecordings collected from the ELSDSR and limited TIMIT datasets, from which we\nextracted 8 speech features, which were pre-processed and then fed into\nNeuraGen to identify the gender. NeuraGen has successfully achieved accuracy of\n90.7407% and F1 score of 91.227% in train and 20-fold cross validation dataset.", "published": "2022-03-29 05:57:24", "link": "http://arxiv.org/abs/2203.15253v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Emotion Recognition with Co-Attention based Multi-level Acoustic\n  Information", "abstract": "Speech Emotion Recognition (SER) aims to help the machine to understand\nhuman's subjective emotion from only audio information. However, extracting and\nutilizing comprehensive in-depth audio information is still a challenging task.\nIn this paper, we propose an end-to-end speech emotion recognition system using\nmulti-level acoustic information with a newly designed co-attention module. We\nfirstly extract multi-level acoustic information, including MFCC, spectrogram,\nand the embedded high-level acoustic information with CNN, BiLSTM and wav2vec2,\nrespectively. Then these extracted features are treated as multimodal inputs\nand fused by the proposed co-attention mechanism. Experiments are carried on\nthe IEMOCAP dataset, and our model achieves competitive performance with two\ndifferent speaker-independent cross-validation strategies. Our code is\navailable on GitHub.", "published": "2022-03-29 08:17:28", "link": "http://arxiv.org/abs/2203.15326v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Iranian Modal Music (Dastgah) detection using deep neural networks", "abstract": "Music classification and genre detection are topics in music information\nretrieval (MIR) that many articles have been published regarding their\nutilities in the modern world. However, this contribution is insufficient in\nnon-western music, such as Iranian modal music. In this work, we have\nimplemented several deep neural networks to recognize Iranian modal music in\nseven highly correlated categories. The best model, BiLGNet, which achieved 92\npercent overall accuracy, uses an architecture inspired by autoencoders,\nincluding bidirectional LSTM and GRU layers. We trained the models using the\nNava dataset, which includes 1786 records and up to 55 hours of music played\nsolo by Kamanche, Tar, Setar, Reed, and Santoor (Dulcimer). We considered\nMultiple features such as MFCC, Chroma CENS, and Mel spectrogram as input. The\nresults indicate that MFCC carries more valuable information for detecting\nIranian modal music (Dastgah) than other sound representations. Moreover, the\narchitecture inspired by autoencoders is robust in distinguishing highly\ncorrelated data like Dastgahs. It also shows that because of the precise order\nin Iranian Dastgah Music, Bidirectional Recurrent networks are more efficient\nthan any other networks that have been implemented in this study.", "published": "2022-03-29 08:27:41", "link": "http://arxiv.org/abs/2203.15335v3", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spoofing-Aware Speaker Verification by Multi-Level Fusion", "abstract": "Recently, many novel techniques have been introduced to deal with spoofing\nattacks, and achieve promising countermeasure (CM) performances. However, these\nworks only take the stand-alone CM models into account. Nowadays, a spoofing\naware speaker verification (SASV) challenge which aims to facilitate the\nresearch of integrated CM and ASV models, arguing that jointly optimizing CM\nand ASV models will lead to better performance, is taking place. In this paper,\nwe propose a novel multi-model and multi-level fusion strategy to tackle the\nSASV task. Compared with purely scoring fusion and embedding fusion methods,\nthis framework first utilizes embeddings from CM models, propagating CM\nembeddings into a CM block to obtain a CM score. In the second-level fusion,\nthe CM score and ASV scores directly from ASV systems will be concatenated into\na prediction block for the final decision. As a result, the best single fusion\nsystem has achieved the SASV-EER of 0.97% on the evaluation set. Then by\nensembling the top-5 fusion systems, the final SASV-EER reached 0.89%.", "published": "2022-03-29 09:16:38", "link": "http://arxiv.org/abs/2203.15377v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VoiceMe: Personalized voice generation in TTS", "abstract": "Novel text-to-speech systems can generate entirely new voices that were not\nseen during training. However, it remains a difficult task to efficiently\ncreate personalized voices from a high-dimensional speaker space. In this work,\nwe use speaker embeddings from a state-of-the-art speaker verification model\n(SpeakerNet) trained on thousands of speakers to condition a TTS model. We\nemploy a human sampling paradigm to explore this speaker latent space. We show\nthat users can create voices that fit well to photos of faces, art portraits,\nand cartoons. We recruit online participants to collectively manipulate the\nvoice of a speaking face. We show that (1) a separate group of human raters\nconfirms that the created voices match the faces, (2) speaker gender apparent\nfrom the face is well-recovered in the voice, and (3) people are consistently\nmoving towards the real voice prototype for the given face. Our results\ndemonstrate that this technology can be applied in a wide number of\napplications including character voice development in audiobooks and games,\npersonalized speech assistants, and individual voices for people with speech\nimpairment.", "published": "2022-03-29 09:19:43", "link": "http://arxiv.org/abs/2203.15379v2", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Machine Composition of Korean Music via Topological Data Analysis and\n  Artificial Neural Network", "abstract": "Common AI music composition algorithms based on artificial neural networks\nare to train a machine by feeding a large number of music pieces and create\nartificial neural networks that can produce music similar to the input music\ndata. This approach is a blackbox optimization, that is, the underlying\ncomposition algorithm is, in general, not known to users.\n  In this paper, we present a way of machine composition that trains a machine\nthe composition principle embedded in the given music data instead of directly\nfeeding music pieces. We propose this approach by using the concept of\n{\\color{black}{Overlap}} matrix proposed in \\cite{TPJ}. In \\cite{TPJ}, a type\nof Korean music, so-called the {\\it Dodeuri} music such as Suyeonjangjigok has\nbeen analyzed using topological data analysis (TDA), particularly using\npersistent homology. As the raw music data is not suitable for TDA analysis,\nthe music data is first reconstructed as a graph. The node of the graph is\ndefined as a two-dimensional vector composed of the pitch and duration of each\nmusic note. The edge between two nodes is created when those nodes appear\nconsecutively in the music flow. Distance is defined based on the frequency of\nsuch appearances. Through TDA on the constructed graph, a unique set of cycles\nis found for the given music. In \\cite{TPJ}, the new concept of the {\\it\n{\\color{black}{Overlap}} matrix} has been proposed, which visualizes how those\ncycles are interconnected over the music flow, in a matrix form.\n  In this paper, we explain how we use the {\\color{black}{Overlap}} matrix for\nmachine composition. The {\\color{black}{Overlap}} matrix makes it possible to\ncompose a new music piece algorithmically and also provide a seed music towards\nthe desired artificial neural network. In this paper, we use the {\\it Dodeuri}\nmusic and explain detailed steps.", "published": "2022-03-29 12:11:31", "link": "http://arxiv.org/abs/2203.15468v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "00A65, 55N31"], "primary_category": "cs.SD"}
{"title": "Learning neural audio features without supervision", "abstract": "Deep audio classification, traditionally cast as training a deep neural\nnetwork on top of mel-filterbanks in a supervised fashion, has recently\nbenefited from two independent lines of work. The first one explores \"learnable\nfrontends\", i.e., neural modules that produce a learnable time-frequency\nrepresentation, to overcome limitations of fixed features. The second one uses\nself-supervised learning to leverage unprecedented scales of pre-training data.\nIn this work, we study the feasibility of combining both approaches, i.e.,\npre-training learnable frontend jointly with the main architecture for\ndownstream classification. First, we show that pretraining two previously\nproposed frontends (SincNet and LEAF) on Audioset drastically improves\nlinear-probe performance over fixed mel-filterbanks, suggesting that learnable\ntime-frequency representations can benefit self-supervised pre-training even\nmore than supervised training. Surprisingly, randomly initialized learnable\nfilterbanks outperform mel-scaled initialization in the self-supervised\nsetting, a counter-intuitive result that questions the appropriateness of\nstrong priors when designing learnable filters. Through exploratory analysis of\nthe learned frontend components, we uncover crucial differences in properties\nof these frontends when used in a supervised and self-supervised setting,\nespecially the affinity of self-supervised filters to diverge significantly\nfrom the mel-scale to model a broader range of frequencies.", "published": "2022-03-29 12:59:08", "link": "http://arxiv.org/abs/2203.15519v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Disentangling speech from surroundings with neural embeddings", "abstract": "We present a method to separate speech signals from noisy environments in the\nembedding space of a neural audio codec. We introduce a new training procedure\nthat allows our model to produce structured encodings of audio waveforms given\nby embedding vectors, where one part of the embedding vector represents the\nspeech signal, and the rest represent the environment. We achieve this by\npartitioning the embeddings of different input waveforms and training the model\nto faithfully reconstruct audio from mixed partitions, thereby ensuring each\npartition encodes a separate audio attribute. As use cases, we demonstrate the\nseparation of speech from background noise or from reverberation\ncharacteristics. Our method also allows for targeted adjustments of the audio\noutput characteristics.", "published": "2022-03-29 13:58:33", "link": "http://arxiv.org/abs/2203.15578v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Passive Similarity based CNN Filter Pruning for Efficient Acoustic\n  Scene Classification", "abstract": "We present a method to develop low-complexity convolutional neural networks\n(CNNs) for acoustic scene classification (ASC). The large size and high\ncomputational complexity of typical CNNs is a bottleneck for their deployment\non resource-constrained devices. We propose a passive filter pruning framework,\nwhere a few convolutional filters from the CNNs are eliminated to yield\ncompressed CNNs. Our hypothesis is that similar filters produce similar\nresponses and give redundant information allowing such filters to be eliminated\nfrom the network. To identify similar filters, a cosine distance based greedy\nalgorithm is proposed. A fine-tuning process is then performed to regain much\nof the performance lost due to filter elimination. To perform efficient\nfine-tuning, we analyze how the performance varies as the number of fine-tuning\ntraining examples changes. An experimental evaluation of the proposed framework\nis performed on the publicly available DCASE 2021 Task 1A baseline network\ntrained for ASC. The proposed method is simple, reduces computations per\ninference by 27%, with 25% fewer parameters, with less than 1% drop in\naccuracy.", "published": "2022-03-29 17:00:06", "link": "http://arxiv.org/abs/2203.15751v1", "categories": ["eess.AS", "cs.AI", "cs.CC", "cs.LG"], "primary_category": "eess.AS"}
{"title": "A Sparsity-promoting Dictionary Model for Variational Autoencoders", "abstract": "Structuring the latent space in probabilistic deep generative models, e.g.,\nvariational autoencoders (VAEs), is important to yield more expressive models\nand interpretable representations, and to avoid overfitting. One way to achieve\nthis objective is to impose a sparsity constraint on the latent variables,\ne.g., via a Laplace prior. However, such approaches usually complicate the\ntraining phase, and they sacrifice the reconstruction quality to promote\nsparsity. In this paper, we propose a simple yet effective methodology to\nstructure the latent space via a sparsity-promoting dictionary model, which\nassumes that each latent code can be written as a sparse linear combination of\na dictionary's columns. In particular, we leverage a computationally efficient\nand tuning-free method, which relies on a zero-mean Gaussian latent prior with\nlearnable variances. We derive a variational inference scheme to train the\nmodel. Experiments on speech generative modeling demonstrate the advantage of\nthe proposed approach over competing techniques, since it promotes sparsity\nwhile not deteriorating the output speech quality.", "published": "2022-03-29 17:13:11", "link": "http://arxiv.org/abs/2203.15758v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Recent improvements of ASR models in the face of adversarial attacks", "abstract": "Like many other tasks involving neural networks, Speech Recognition models\nare vulnerable to adversarial attacks. However recent research has pointed out\ndifferences between attacks and defenses on ASR models compared to image\nmodels. Improving the robustness of ASR models requires a paradigm shift from\nevaluating attacks on one or a few models to a systemic approach in evaluation.\nWe lay the ground for such research by evaluating on various architectures a\nrepresentative set of adversarial attacks: targeted and untargeted,\noptimization and speech processing-based, white-box, black-box and targeted\nattacks. Our results show that the relative strengths of different attack\nalgorithms vary considerably when changing the model architecture, and that the\nresults of some attacks are not to be blindly trusted. They also indicate that\ntraining choices such as self-supervised pretraining can significantly impact\nrobustness by enabling transferable perturbations. We release our source code\nas a package that should help future research in evaluating their attacks and\ndefenses.", "published": "2022-03-29 22:40:37", "link": "http://arxiv.org/abs/2203.16536v2", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
