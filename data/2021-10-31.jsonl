{"title": "Visualization: the missing factor in Simultaneous Speech Translation", "abstract": "Simultaneous speech translation (SimulST) is the task in which output\ngeneration has to be performed on partial, incremental speech input. In recent\nyears, SimulST has become popular due to the spread of cross-lingual\napplication scenarios, like international live conferences and streaming\nlectures, in which on-the-fly speech translation can facilitate users' access\nto audio-visual content. In this paper, we analyze the characteristics of the\nSimulST systems developed so far, discussing their strengths and weaknesses. We\nthen concentrate on the evaluation framework required to properly assess\nsystems' effectiveness. To this end, we raise the need for a broader\nperformance analysis, also including the user experience standpoint. SimulST\nsystems, indeed, should be evaluated not only in terms of quality/latency\nmeasures, but also via task-oriented metrics accounting, for instance, for the\nvisualization strategy adopted. In light of this, we highlight which are the\ngoals achieved by the community and what is still missing.", "published": "2021-10-31 14:44:01", "link": "http://arxiv.org/abs/2111.00514v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minimum Description Length Recurrent Neural Networks", "abstract": "We train neural networks to optimize a Minimum Description Length score,\ni.e., to balance between the complexity of the network and its accuracy at a\ntask. We show that networks optimizing this objective function master tasks\ninvolving memory challenges and go beyond context-free languages. These\nlearners master languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^{2n}$,\n$a^nb^mc^{n+m}$, and they perform addition. Moreover, they often do so with\n100% accuracy. The networks are small, and their inner workings are\ntransparent. We thus provide formal proofs that their perfect accuracy holds\nnot only on a given test set, but for any input sequence. To our knowledge, no\nother connectionist model has been shown to capture the underlying grammars for\nthese languages in full generality.", "published": "2021-10-31 21:43:31", "link": "http://arxiv.org/abs/2111.00600v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Investigation of Commonsense Knowledge in Large Language\n  Models", "abstract": "Language models (LMs) trained on large amounts of data have shown impressive\nperformance on many NLP tasks under the zero-shot and few-shot setup. Here we\naim to better understand the extent to which such models learn commonsense\nknowledge -- a critical component of many NLP applications. We conduct a\nsystematic and rigorous zero-shot and few-shot commonsense evaluation of large\npre-trained LMs, where we: (i) carefully control for the LMs' ability to\nexploit potential surface cues and annotation artefacts, and (ii) account for\nvariations in performance that arise from factors that are not related to\ncommonsense knowledge. Our findings highlight the limitations of pre-trained\nLMs in acquiring commonsense knowledge without task-specific supervision;\nfurthermore, using larger models or few-shot evaluation are insufficient to\nachieve human-level commonsense performance.", "published": "2021-10-31 22:20:36", "link": "http://arxiv.org/abs/2111.00607v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DSC-IITISM at FinCausal 2021: Combining POS tagging with Attention-based\n  Contextual Representations for Identifying Causal Relationships in Financial\n  Documents", "abstract": "Causality detection draws plenty of attention in the field of Natural\nLanguage Processing and linguistics research. It has essential applications in\ninformation retrieval, event prediction, question answering, financial\nanalysis, and market research. In this study, we explore several methods to\nidentify and extract cause-effect pairs in financial documents using\ntransformers. For this purpose, we propose an approach that combines POS\ntagging with the BIO scheme, which can be integrated with modern transformer\nmodels to address this challenge of identifying causality in a given text. Our\nbest methodology achieves an F1-Score of 0.9551, and an Exact Match Score of\n0.8777 on the blind test in the FinCausal-2021 Shared Task at the FinCausal\n2021 Workshop.", "published": "2021-10-31 13:09:19", "link": "http://arxiv.org/abs/2111.00490v1", "categories": ["cs.CL", "cs.AI", "68T50 (Primary), 91F20 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Template Filling for Controllable Commonsense Reasoning", "abstract": "Large-scale sequence-to-sequence models have shown to be adept at both\nmultiple-choice and open-domain commonsense reasoning tasks. However, the\ncurrent systems do not provide the ability to control the various attributes of\nthe reasoning chain. To enable better controllability, we propose to study the\ncommonsense reasoning as a template filling task (TemplateCSR) -- where the\nlanguage models fills reasoning templates with the given constraints as control\nfactors. As an approach to TemplateCSR, we (i) propose a dataset of commonsense\nreasoning template-expansion pairs and (ii) introduce POTTER, a pretrained\nsequence-to-sequence model using prompts to perform commonsense reasoning\nacross concepts. Our experiments show that our approach outperforms baselines\nboth in generation metrics and factuality metrics. We also present a detailed\nerror analysis on our approach's ability to reliably perform commonsense\nreasoning.", "published": "2021-10-31 16:35:36", "link": "http://arxiv.org/abs/2111.00539v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quality Estimation Using Round-trip Translation with Sentence Embeddings", "abstract": "Estimating the quality of machine translation systems has been an ongoing\nchallenge for researchers in this field. Many previous attempts at using\nround-trip translation as a measure of quality have failed, and there is much\ndisagreement as to whether it can be a viable method of quality estimation. In\nthis paper, we revisit round-trip translation, proposing a system which aims to\nsolve the previous pitfalls found with the approach. Our method makes use of\nrecent advances in language representation learning to more accurately gauge\nthe similarity between the original and round-trip translated sentences.\nExperiments show that while our approach does not reach the performance of\ncurrent state of the art methods, it may still be an effective approach for\nsome language pairs.", "published": "2021-10-31 17:51:12", "link": "http://arxiv.org/abs/2111.00554v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Approach to Inference-Driven Dialogue Management within a Social\n  Chatbot", "abstract": "We present a chatbot implementing a novel dialogue management approach based\non logical inference. Instead of framing conversation a sequence of response\ngeneration tasks, we model conversation as a collaborative inference process in\nwhich speakers share information to synthesize new knowledge in real time. Our\nchatbot pipeline accomplishes this modelling in three broad stages. The first\nstage translates user utterances into a symbolic predicate representation. The\nsecond stage then uses this structured representation in conjunction with a\nlarger knowledge base to synthesize new predicates using efficient graph\nmatching. In the third and final stage, our bot selects a small subset of\npredicates and translates them into an English response. This approach lends\nitself to understanding latent semantics of user inputs, flexible initiative\ntaking, and responses that are novel and coherent with the dialogue context.", "published": "2021-10-31 19:01:07", "link": "http://arxiv.org/abs/2111.00570v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Went Wrong? Explaining Overall Dialogue Quality through\n  Utterance-Level Impacts", "abstract": "Improving user experience of a dialogue system often requires intensive\ndeveloper effort to read conversation logs, run statistical analyses, and\nintuit the relative importance of system shortcomings. This paper presents a\nnovel approach to automated analysis of conversation logs that learns the\nrelationship between user-system interactions and overall dialogue quality.\nUnlike prior work on utterance-level quality prediction, our approach learns\nthe impact of each interaction from the overall user rating without\nutterance-level annotation, allowing resultant model conclusions to be derived\non the basis of empirical evidence and at low cost. Our model identifies\ninteractions that have a strong correlation with the overall dialogue quality\nin a chatbot setting. Experiments show that the automated analysis from our\nmodel agrees with expert judgments, making this work the first to show that\nsuch weakly-supervised learning of utterance-level quality prediction is highly\nachievable.", "published": "2021-10-31 19:12:29", "link": "http://arxiv.org/abs/2111.00572v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EfficientWord-Net: An Open Source Hotword Detection Engine based on\n  One-shot Learning", "abstract": "Voice assistants like Siri, Google Assistant, Alexa etc. are used widely\nacross the globe for home automation, these require the use of special phrases\nalso known as hotwords to wake it up and perform an action like \"Hey Alexa!\",\n\"Ok Google!\" and \"Hey Siri!\" etc. These hotwords are detected with lightweight\nreal-time engines whose purpose is to detect the hotwords uttered by the user.\nThis paper presents the design and implementation of a hotword detection engine\nbased on one-shot learning which detects the hotword uttered by the user in\nreal-time with just one or few training samples of the hotword. This approach\nis efficient when compared to existing implementations because the process of\nadding a new hotword in the existing systems requires enormous amounts of\npositive and negative training samples and the model needs to retrain for every\nhotword. This makes the existing implementations inefficient in terms of\ncomputation and cost. The architecture proposed in this paper has achieved an\naccuracy of 94.51%.", "published": "2021-10-31 02:18:01", "link": "http://arxiv.org/abs/2111.00379v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.m"], "primary_category": "cs.CL"}
{"title": "FANS: Fusing ASR and NLU for on-device SLU", "abstract": "Spoken language understanding (SLU) systems translate voice input commands to\nsemantics which are encoded as an intent and pairs of slot tags and values.\nMost current SLU systems deploy a cascade of two neural models where the first\none maps the input audio to a transcript (ASR) and the second predicts the\nintent and slots from the transcript (NLU). In this paper, we introduce FANS, a\nnew end-to-end SLU model that fuses an ASR audio encoder to a multi-task NLU\ndecoder to infer the intent, slot tags, and slot values directly from a given\ninput audio, obviating the need for transcription. FANS consists of a shared\naudio encoder and three decoders, two of which are seq-to-seq decoders that\npredict non null slot tags and slot values in parallel and in an\nauto-regressive manner. FANS neural encoder and decoders architectures are\nflexible which allows us to leverage different combinations of LSTM,\nself-attention, and attenders. Our experiments show compared to the\nstate-of-the-art end-to-end SLU models, FANS reduces ICER and IRER errors\nrelatively by 30 % and 7 %, respectively, when tested on an in-house SLU\ndataset and by 0.86 % and 2 % absolute when tested on a public SLU dataset.", "published": "2021-10-31 03:50:19", "link": "http://arxiv.org/abs/2111.00400v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speech Emotion Recognition Using Quaternion Convolutional Neural\n  Networks", "abstract": "Although speech recognition has become a widespread technology, inferring\nemotion from speech signals still remains a challenge. To address this problem,\nthis paper proposes a quaternion convolutional neural network (QCNN) based\nspeech emotion recognition (SER) model in which Mel-spectrogram features of\nspeech signals are encoded in an RGB quaternion domain. We show that our QCNN\nbased SER model outperforms other real-valued methods in the Ryerson\nAudio-Visual Database of Emotional Speech and Song (RAVDESS, 8-classes)\ndataset, achieving, to the best of our knowledge, state-of-the-art results. The\nQCNN also achieves comparable results with the state-of-the-art methods in the\nInteractive Emotional Dyadic Motion Capture (IEMOCAP 4-classes) and Berlin\nEMO-DB (7-classes) datasets. Specifically, the model achieves an accuracy of\n77.87\\%, 70.46\\%, and 88.78\\% for the RAVDESS, IEMOCAP, and EMO-DB datasets,\nrespectively. In addition, our results show that the quaternion unit structure\nis better able to encode internal dependencies to reduce its model size\nsignificantly compared to other methods.", "published": "2021-10-31 04:06:07", "link": "http://arxiv.org/abs/2111.00404v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hierarchical Deep Residual Reasoning for Temporal Moment Localization", "abstract": "Temporal Moment Localization (TML) in untrimmed videos is a challenging task\nin the field of multimedia, which aims at localizing the start and end points\nof the activity in the video, described by a sentence query. Existing methods\nmainly focus on mining the correlation between video and sentence\nrepresentations or investigating the fusion manner of the two modalities. These\nworks mainly understand the video and sentence coarsely, ignoring the fact that\na sentence can be understood from various semantics, and the dominant words\naffecting the moment localization in the semantics are the action and object\nreference. Toward this end, we propose a Hierarchical Deep Residual Reasoning\n(HDRR) model, which decomposes the video and sentence into multi-level\nrepresentations with different semantics to achieve a finer-grained\nlocalization. Furthermore, considering that videos with different resolution\nand sentences with different length have different difficulty in understanding,\nwe design the simple yet effective Res-BiGRUs for feature fusion, which is able\nto grasp the useful information in a self-adapting manner. Extensive\nexperiments conducted on Charades-STA and ActivityNet-Captions datasets\ndemonstrate the superiority of our HDRR model compared with other\nstate-of-the-art methods.", "published": "2021-10-31 07:13:34", "link": "http://arxiv.org/abs/2111.00417v1", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.MM"}
{"title": "FinEAS: Financial Embedding Analysis of Sentiment", "abstract": "We introduce a new language representation model in finance called Financial\nEmbedding Analysis of Sentiment (FinEAS). In financial markets, news and\ninvestor sentiment are significant drivers of security prices. Thus, leveraging\nthe capabilities of modern NLP approaches for financial sentiment analysis is a\ncrucial component in identifying patterns and trends that are useful for market\nparticipants and regulators. In recent years, methods that use transfer\nlearning from large Transformer-based language models like BERT, have achieved\nstate-of-the-art results in text classification tasks, including sentiment\nanalysis using labelled datasets. Researchers have quickly adopted these\napproaches to financial texts, but best practices in this domain are not\nwell-established. In this work, we propose a new model for financial sentiment\nanalysis based on supervised fine-tuned sentence embeddings from a standard\nBERT model. We demonstrate our approach achieves significant improvements in\ncomparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific\nBERT.", "published": "2021-10-31 15:41:56", "link": "http://arxiv.org/abs/2111.00526v2", "categories": ["cs.CL", "q-fin.CP", "q-fin.PM"], "primary_category": "cs.CL"}
{"title": "Revealing and Protecting Labels in Distributed Training", "abstract": "Distributed learning paradigms such as federated learning often involve\ntransmission of model updates, or gradients, over a network, thereby avoiding\ntransmission of private data. However, it is possible for sensitive information\nabout the training data to be revealed from such gradients. Prior works have\ndemonstrated that labels can be revealed analytically from the last layer of\ncertain models (e.g., ResNet), or they can be reconstructed jointly with model\ninputs by using Gradients Matching [Zhu et al'19] with additional knowledge\nabout the current state of the model. In this work, we propose a method to\ndiscover the set of labels of training samples from only the gradient of the\nlast layer and the id to label mapping. Our method is applicable to a wide\nvariety of model architectures across multiple domains. We demonstrate the\neffectiveness of our method for model training in two domains - image\nclassification, and automatic speech recognition. Furthermore, we show that\nexisting reconstruction techniques improve their efficacy when used in\nconjunction with our method. Conversely, we demonstrate that gradient\nquantization and sparsification can significantly reduce the success of the\nattack.", "published": "2021-10-31 17:57:49", "link": "http://arxiv.org/abs/2111.00556v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Text Classification for Task-based Source Code Related Questions", "abstract": "There is a key demand to automatically generate code for small tasks for\ndevelopers. Websites such as StackOverflow provide a simplistic way by offering\nsolutions in small snippets which provide a complete answer to whatever task\nquestion the developer wants to code. Natural Language Processing and\nparticularly Question-Answering Systems are very helpful in resolving and\nworking on these tasks. In this paper, we develop a two-fold deep learning\nmodel: Seq2Seq and a binary classifier that takes in the intent (which is in\nnatural language) and code snippets in Python. We train both the intent and the\ncode utterances in the Seq2Seq model, where we decided to compare the effect of\nthe hidden layer embedding from the encoder for representing the intent and\nsimilarly, using the decoder's hidden layer embeddings for the code sequence.\nThen we combine both these embeddings and then train a simple binary neural\nnetwork classifier model for predicting if the intent is correctly answered by\nthe predicted code sequence from the seq2seq model. We find that the hidden\nstate layer's embeddings perform slightly better than regular standard\nembeddings from a constructed vocabulary. We experimented with our tests on the\nCoNaLa dataset in addition to the StaQC database consisting of simple task-code\nsnippet-based pairs. We empirically establish that using additional pre-trained\nembeddings for code snippets in Python is less context-based in comparison to\nusing hidden state context vectors from seq2seq models.", "published": "2021-10-31 20:10:21", "link": "http://arxiv.org/abs/2111.00580v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Towards Language Modelling in the Speech Domain Using Sub-word\n  Linguistic Units", "abstract": "Language models (LMs) for text data have been studied extensively for their\nusefulness in language generation and other downstream tasks. However, language\nmodelling purely in the speech domain is still a relatively unexplored topic,\nwith traditional speech LMs often depending on auxiliary text LMs for learning\ndistributional aspects of the language. For the English language, these LMs\ntreat words as atomic units, which presents inherent challenges to language\nmodelling in the speech domain. In this paper, we propose a novel LSTM-based\ngenerative speech LM that is inspired by the CBOW model and built on linguistic\nunits including syllables and phonemes. This offers better acoustic consistency\nacross utterances in the dataset, as opposed to single melspectrogram frames,\nor whole words. With a limited dataset, orders of magnitude smaller than that\nrequired by contemporary generative models, our model closely approximates\nbabbling speech. We show the effect of training with auxiliary text LMs,\nmultitask learning objectives, and auxiliary articulatory features. Through our\nexperiments, we also highlight some well known, but poorly documented\nchallenges in training generative speech LMs, including the mismatch between\nthe supervised learning objective with which these models are trained such as\nMean Squared Error (MSE), and the true objective, which is speech quality. Our\nexperiments provide an early indication that while validation loss and Mel\nCepstral Distortion (MCD) are not strongly correlated with generated speech\nquality, traditional text language modelling metrics like perplexity and\nnext-token-prediction accuracy might be.", "published": "2021-10-31 22:48:30", "link": "http://arxiv.org/abs/2111.00610v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "R-BERT-CNN: Drug-target interactions extraction from biomedical\n  literature", "abstract": "In this research, we present our work participation for the DrugProt task of\nBioCreative VII challenge. Drug-target interactions (DTIs) are critical for\ndrug discovery and repurposing, which are often manually extracted from the\nexperimental articles. There are >32M biomedical articles on PubMed and\nmanually extracting DTIs from such a huge knowledge base is challenging. To\nsolve this issue, we provide a solution for Track 1, which aims to extract 10\ntypes of interactions between drug and protein entities. We applied an Ensemble\nClassifier model that combines BioMed-RoBERTa, a state of art language model,\nwith Convolutional Neural Networks (CNN) to extract these relations. Despite\nthe class imbalances in the BioCreative VII DrugProt test corpus, our model\nachieves a good performance compared to the average of other submissions in the\nchallenge, with the micro F1 score of 55.67% (and 63% on BioCreative VI\nChemProt test corpus). The results show the potential of deep learning in\nextracting various types of DTIs.", "published": "2021-10-31 22:50:33", "link": "http://arxiv.org/abs/2111.00611v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Classifying YouTube Comments Based on Sentiment and Type of Sentence", "abstract": "As a YouTube channel grows, each video can potentially collect enormous\namounts of comments that provide direct feedback from the viewers. These\ncomments are a major means of understanding viewer expectations and improving\nchannel engagement. However, the comments only represent a general collection\nof user opinions about the channel and the content. Many comments are poorly\nconstructed, trivial, and have improper spellings and grammatical errors. As a\nresult, it is a tedious job to identify the comments that best interest the\ncontent creators. In this paper, we extract and classify the raw comments into\ndifferent categories based on both sentiment and sentence types that will help\nYouTubers find relevant comments for growing their viewership. Existing studies\nhave focused either on sentiment analysis (positive and negative) or\nclassification of sub-types within the same sentence types (e.g., types of\nquestions) on a text corpus. These have limited application on non-traditional\ntext corpus like YouTube comments. We address this challenge of text extraction\nand classification from YouTube comments using well-known statistical measures\nand machine learning models. We evaluate each combination of statistical\nmeasure and the machine learning model using cross validation and $F_1$ scores.\nThe results show that our approach that incorporates conventional methods\nperforms well on the classification task, validating its potential in assisting\ncontent creators increase viewer engagement on their channel.", "published": "2021-10-31 18:08:10", "link": "http://arxiv.org/abs/2111.01908v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Analysis of North Indian Classical Ragas Using Tonnetz", "abstract": "In North Indian Classical music, each raga has been traditionally associated\nwith a performance time, which supposedly maximizes its aesthetic and emotional\neffects on the listener. The objective of this work was to investigate the\nstructural basis, if any, for the association of ragas with different times of\nthe 24-hour span. The tonnetz framework has been used to analyze the pitch sets\nof 65 North Indian Classical ragas, and structural similarities have been\nobserved between ragas associated with (1) times of transition between day and\nnight, i.e., dawn and dusk, and (2) times between these transitions. These\nfindings could provide some insight into the scientific basis of the age-old\nraga-time relation, and their effects on the perception of the listener.", "published": "2021-10-31 09:05:48", "link": "http://arxiv.org/abs/2111.00436v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Revisiting joint decoding based multi-talker speech recognition with DNN\n  acoustic model", "abstract": "In typical multi-talker speech recognition systems, a neural network-based\nacoustic model predicts senone state posteriors for each speaker. These are\nlater used by a single-talker decoder which is applied on each speaker-specific\noutput stream separately. In this work, we argue that such a scheme is\nsub-optimal and propose a principled solution that decodes all speakers\njointly. We modify the acoustic model to predict joint state posteriors for all\nspeakers, enabling the network to express uncertainty about the attribution of\nparts of the speech signal to the speakers. We employ a joint decoder that can\nmake use of this uncertainty together with higher-level language information.\nFor this, we revisit decoding algorithms used in factorial generative models in\nearly multi-talker speech recognition systems. In contrast with these early\nworks, we replace the GMM acoustic model with DNN, which provides greater\nmodeling power and simplifies part of the inference. We demonstrate the\nadvantage of joint decoding in proof of concept experiments on a mixed-TIDIGITS\ndataset.", "published": "2021-10-31 09:28:04", "link": "http://arxiv.org/abs/2111.00009v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
