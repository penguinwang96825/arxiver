{"title": "LUNA: Language Understanding with Number Augmentations on Transformers\n  via Number Plugins and Pre-training", "abstract": "Transformers are widely used in NLP tasks. However, current approaches to\nleveraging transformers to understand language expose one weak spot: Number\nunderstanding. In some scenarios, numbers frequently occur, especially in\nsemi-structured data like tables. But current approaches to rich-number tasks\nwith transformer-based language models abandon or lose some of the numeracy\ninformation - e.g., breaking numbers into sub-word tokens - which leads to many\nnumber-related errors. In this paper, we propose the LUNA framework which\nimproves the numerical reasoning and calculation capabilities of\ntransformer-based language models. With the number plugin of NumTok and NumBed,\nLUNA represents each number as a whole to model input. With number\npre-training, including regression loss and model distillation, LUNA bridges\nthe gap between number and vocabulary embeddings. To the best of our knowledge,\nthis is the first work that explicitly injects numeracy capability into\nlanguage models using Number Plugins. Besides evaluating toy models on toy\ntasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT,\nTabBERT) over three different downstream tasks (TATQA, TabFact, CrediTrans),\nand observe the performances of language models are constantly improved by\nLUNA. The augmented models also improve the official baseline of TAT-QA (EM:\n50.15 -> 59.58) and achieve SOTA performance on CrediTrans (F1 = 86.17).", "published": "2022-12-06 01:31:37", "link": "http://arxiv.org/abs/2212.02691v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sources of Noise in Dialogue and How to Deal with Them", "abstract": "Training dialogue systems often entails dealing with noisy training examples\nand unexpected user inputs. Despite their prevalence, there currently lacks an\naccurate survey of dialogue noise, nor is there a clear sense of the impact of\neach noise type on task performance. This paper addresses this gap by first\nconstructing a taxonomy of noise encountered by dialogue systems. In addition,\nwe run a series of experiments to show how different models behave when\nsubjected to varying levels of noise and types of noise. Our results reveal\nthat models are quite robust to label errors commonly tackled by existing\ndenoising algorithms, but that performance suffers from dialogue-specific\nnoise. Driven by these observations, we design a data cleaning algorithm\nspecialized for conversational settings and apply it as a proof-of-concept for\ntargeted dialogue denoising.", "published": "2022-12-06 04:36:32", "link": "http://arxiv.org/abs/2212.02745v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Identification of Eviction Status from Electronic Health\n  Record Notes", "abstract": "Objective: Evictions are important social and behavioral determinants of\nhealth. Evictions are associated with a cascade of negative events that can\nlead to unemployment, housing insecurity/homelessness, long-term poverty, and\nmental health problems. In this study, we developed a natural language\nprocessing system to automatically detect eviction status from electronic\nhealth record (EHR) notes.\n  Materials and Methods: We first defined eviction status (eviction presence\nand eviction period) and then annotated eviction status in 5000 EHR notes from\nthe Veterans Health Administration (VHA). We developed a novel model, KIRESH,\nthat has shown to substantially outperform other state-of-the-art models such\nas fine-tuning pre-trained language models like BioBERT and BioClinicalBERT.\nMoreover, we designed a novel prompt to further improve the model performance\nby using the intrinsic connection between the two sub-tasks of eviction\npresence and period prediction. Finally, we used the Temperature Scaling-based\nCalibration on our KIRESH-Prompt method to avoid over-confidence issues arising\nfrom the imbalance dataset.\n  Results: KIRESH-Prompt substantially outperformed strong baseline models\nincluding fine-tuning the BioClinicalBERT model to achieve 0.74672 MCC, 0.71153\nMacro-F1, and 0.83396 Micro-F1 in predicting eviction period and 0.66827 MCC,\n0.62734 Macro-F1, and 0.7863 Micro-F1 in predicting eviction presence. We also\nconducted additional experiments on a benchmark social determinants of health\n(SBDH) dataset to demonstrate the generalizability of our methods.\n  Conclusion and Future Work: KIRESH-Prompt has substantially improved eviction\nstatus classification. We plan to deploy KIRESH-Prompt to the VHA EHRs as an\neviction surveillance system to help address the US Veterans' housing\ninsecurity.", "published": "2022-12-06 05:25:32", "link": "http://arxiv.org/abs/2212.02762v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Life-long Learning for Multilingual Neural Machine Translation with\n  Knowledge Distillation", "abstract": "A common scenario of Multilingual Neural Machine Translation (MNMT) is that\neach translation task arrives in a sequential manner, and the training data of\nprevious tasks is unavailable. In this scenario, the current methods suffer\nheavily from catastrophic forgetting (CF). To alleviate the CF, we investigate\nknowledge distillation based life-long learning methods. Specifically, in\none-tomany scenario, we propose a multilingual distillation method to make the\nnew model (student) jointly learn multilingual output from old model (teacher)\nand new task. In many-to one scenario, we find that direct distillation faces\nthe extreme partial distillation problem, and we propose two different methods\nto address it: pseudo input distillation and reverse teacher distillation. The\nexperimental results on twelve translation tasks show that the proposed methods\ncan better consolidate the previous knowledge and sharply alleviate the CF.", "published": "2022-12-06 07:36:16", "link": "http://arxiv.org/abs/2212.02800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context\n  Tuning", "abstract": "Dialogue State Tracking (DST), a key component of task-oriented conversation\nsystems, represents user intentions by determining the values of pre-defined\nslots in an ongoing dialogue. Existing approaches use hand-crafted templates\nand additional slot information to fine-tune and prompt large pre-trained\nlanguage models and elicit slot values from the dialogue context. Significant\nmanual effort and domain knowledge is required to design effective prompts,\nlimiting the generalizability of these approaches to new domains and tasks. In\nthis work, we propose DiSTRICT, a generalizable in-context tuning approach for\nDST that retrieves highly relevant training examples for a given dialogue to\nfine-tune the model without any hand-crafted templates. Experiments with the\nMultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches\nin various zero-shot and few-shot settings using a much smaller model, thereby\nproviding an important advantage for real-world deployments that often have\nlimited resource availability.", "published": "2022-12-06 09:40:15", "link": "http://arxiv.org/abs/2212.02851v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Template-based Recruitment Email Generation For Job Recommendation", "abstract": "Text generation has long been a popular research topic in NLP. However, the\ntask of generating recruitment emails from recruiters to candidates in the job\nrecommendation scenario has received little attention by the research\ncommunity. This work aims at defining the topic of automatic email generation\nfor job recommendation, identifying the challenges, and providing a baseline\ntemplate-based solution for Danish jobs. Evaluation by human experts shows that\nour method is effective. We wrap up by discussing the future research\ndirections for better solving this task.", "published": "2022-12-06 11:19:45", "link": "http://arxiv.org/abs/2212.02885v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Conditioned Creative Dialog Generation", "abstract": "We present a DialGPT based model for generating creative dialog responses\nthat are conditioned based on one of the following emotions: anger, disgust,\nfear, happiness, pain, sadness and surprise. Our model is capable of producing\na contextually apt response given an input sentence and a desired emotion\nlabel. Our model is capable of expressing the desired emotion with an accuracy\nof 0.6. The best performing emotions are neutral, fear and disgust. When\nmeasuring the strength of the expressed emotion, we find that anger, fear and\ndisgust are expressed in the most strong fashion by the model.", "published": "2022-12-06 12:05:43", "link": "http://arxiv.org/abs/2212.02907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modern French Poetry Generation with RoBERTa and GPT-2", "abstract": "We present a novel neural model for modern poetry generation in French. The\nmodel consists of two pretrained neural models that are fine-tuned for the poem\ngeneration task. The encoder of the model is a RoBERTa based one while the\ndecoder is based on GPT-2. This way the model can benefit from the superior\nnatural language understanding performance of RoBERTa and the good natural\nlanguage generation performance of GPT-2. Our evaluation shows that the model\ncan create French poetry successfully. On a 5 point scale, the lowest score of\n3.57 was given by human judges to typicality and emotionality of the output\npoetry while the best score of 3.79 was given to understandability.", "published": "2022-12-06 12:10:14", "link": "http://arxiv.org/abs/2212.02911v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Bridged Causal Interaction Network for Causal Emotion\n  Entailment", "abstract": "Causal Emotion Entailment aims to identify causal utterances that are\nresponsible for the target utterance with a non-neutral emotion in\nconversations. Previous works are limited in thorough understanding of the\nconversational context and accurate reasoning of the emotion cause. To this\nend, we propose Knowledge-Bridged Causal Interaction Network (KBCIN) with\ncommonsense knowledge (CSK) leveraged as three bridges. Specifically, we\nconstruct a conversational graph for each conversation and leverage the\nevent-centered CSK as the semantics-level bridge (S-bridge) to capture the deep\ninter-utterance dependencies in the conversational context via the CSK-Enhanced\nGraph Attention module. Moreover, social-interaction CSK serves as\nemotion-level bridge (E-bridge) and action-level bridge (A-bridge) to connect\ncandidate utterances with the target one, which provides explicit causal clues\nfor the Emotional Interaction module and Actional Interaction module to reason\nthe target emotion. Experimental results show that our model achieves better\nperformance over most baseline models. Our source code is publicly available at\nhttps://github.com/circle-hit/KBCIN.", "published": "2022-12-06 14:13:33", "link": "http://arxiv.org/abs/2212.02995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZeroKBC: A Comprehensive Benchmark for Zero-Shot Knowledge Base\n  Completion", "abstract": "Knowledge base completion (KBC) aims to predict the missing links in\nknowledge graphs. Previous KBC tasks and approaches mainly focus on the setting\nwhere all test entities and relations have appeared in the training set.\nHowever, there has been limited research on the zero-shot KBC settings, where\nwe need to deal with unseen entities and relations that emerge in a constantly\ngrowing knowledge base. In this work, we systematically examine different\npossible scenarios of zero-shot KBC and develop a comprehensive benchmark,\nZeroKBC, that covers these scenarios with diverse types of knowledge sources.\nOur systematic analysis reveals several missing yet important zero-shot KBC\nsettings. Experimental results show that canonical and state-of-the-art KBC\nsystems cannot achieve satisfactory performance on this challenging benchmark.\nBy analyzing the strength and weaknesses of these systems on solving ZeroKBC,\nwe further present several important observations and promising future\ndirections.", "published": "2022-12-06 16:02:09", "link": "http://arxiv.org/abs/2212.03091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LawngNLI: A Long-Premise Benchmark for In-Domain Generalization from\n  Short to Long Contexts and for Implication-Based Retrieval", "abstract": "Natural language inference has trended toward studying contexts beyond the\nsentence level. An important application area is law: past cases often do not\nforetell how they apply to new situations and implications must be inferred.\nThis paper introduces LawngNLI, constructed from U.S. legal opinions with\nautomatic labels with high human-validated accuracy. Premises are long and\nmultigranular. Experiments show two use cases. First, LawngNLI can benchmark\nfor in-domain generalization from short to long contexts. It has remained\nunclear if large-scale long-premise NLI datasets actually need to be\nconstructed: near-top performance on long premises could be achievable by\nfine-tuning using short premises. Without multigranularity, benchmarks cannot\ndistinguish lack of fine-tuning on long premises versus domain shift between\nshort and long datasets. In contrast, our long and short premises share the\nsame examples and domain. Models fine-tuned using several past NLI datasets\nand/or our short premises fall short of top performance on our long premises.\nSo for at least certain domains (such as ours), large-scale long-premise\ndatasets are needed. Second, LawngNLI can benchmark for implication-based\nretrieval. Queries are entailed or contradicted by target documents, allowing\nusers to move between arguments and evidence. Leading retrieval models perform\nreasonably zero shot on a LawngNLI-derived retrieval task. We compare different\nsystems for re-ranking, including lexical overlap and cross-encoders fine-tuned\nusing a modified LawngNLI or past NLI datasets. LawngNLI can train and test\nsystems for implication-based case retrieval and argumentation.", "published": "2022-12-06 18:42:39", "link": "http://arxiv.org/abs/2212.03222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counterfactual reasoning: Do language models need world knowledge for\n  causal understanding?", "abstract": "Current pre-trained language models have enabled remarkable improvements in\ndownstream tasks, but it remains difficult to distinguish effects of\nstatistical correlation from more systematic logical reasoning grounded on\nunderstanding of the real world. In this paper we tease these factors apart by\nleveraging counterfactual conditionals, which force language models to predict\nunusual consequences based on hypothetical propositions. We introduce a set of\ntests drawn from psycholinguistic experiments, as well as larger-scale\ncontrolled datasets, to probe counterfactual predictions from a variety of\npopular pre-trained language models. We find that models are consistently able\nto override real-world knowledge in counterfactual scenarios, and that this\neffect is more robust in case of stronger baseline world knowledge -- however,\nwe also find that for most models this effect appears largely to be driven by\nsimple lexical cues. When we mitigate effects of both world knowledge and\nlexical cues to test knowledge of linguistic nuances of counterfactuals, we\nfind that only GPT-3 shows sensitivity to these nuances, though this\nsensitivity is also non-trivially impacted by lexical associative factors.", "published": "2022-12-06 19:22:25", "link": "http://arxiv.org/abs/2212.03278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt\n  Tuning and Analysis of the Utility of Generated Text in AI", "abstract": "Controlled text generation is a very important task in the arena of natural\nlanguage processing due to its promising applications. In order to achieve this\ntask we mainly introduce the novel soft prompt tuning method of using soft\nprompts at both encoder and decoder levels together in a T5 model and\ninvestigate the performance as the behaviour of an additional soft prompt\nrelated to the decoder of a T5 model in controlled text generation remained\nunexplored. Then we also investigate the feasibility of steering the output of\nthis extended soft prompted T5 model at decoder level and finally analyse the\nutility of generated text to be used in AI related tasks such as training AI\nmodels with an interpretability analysis of the classifier trained with\nsynthetic text, as there is a lack of proper analysis of methodologies in\ngenerating properly labelled data to be utilized in AI tasks. Through the\nperformed in-depth intrinsic and extrinsic evaluations of this generation model\nalong with the artificially generated data, we found that this model produced\nbetter results compared to the T5 model with a single soft prompt at encoder\nlevel and the sentiment classifier trained using this artificially generated\ndata can produce comparable classification results to the results of a\nclassifier trained with real labelled data and also the classifier decision is\ninterpretable with respect to the input text content.", "published": "2022-12-06 12:31:53", "link": "http://arxiv.org/abs/2212.02924v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain", "abstract": "The field of cybersecurity is evolving fast. Experts need to be informed\nabout past, current and - in the best case - upcoming threats, because attacks\nare becoming more advanced, targets bigger and systems more complex. As this\ncannot be addressed manually, cybersecurity experts need to rely on machine\nlearning techniques. In the texutual domain, pre-trained language models like\nBERT have shown to be helpful, by providing a good baseline for further\nfine-tuning. However, due to the domain-knowledge and many technical terms in\ncybersecurity general language models might miss the gist of textual\ninformation, hence doing more harm than good. For this reason, we create a\nhigh-quality dataset and present a language model specifically tailored to the\ncybersecurity domain, which can serve as a basic building block for\ncybersecurity systems that deal with natural language. The model is compared\nwith other models based on 15 different domain-dependent extrinsic and\nintrinsic tasks as well as general tasks from the SuperGLUE benchmark. On the\none hand, the results of the intrinsic tasks show that our model improves the\ninternal representation space of words compared to the other models. On the\nother hand, the extrinsic, domain-dependent tasks, consisting of sequence\ntagging and classification, show that the model is best in specific application\nscenarios, in contrast to the others. Furthermore, we show that our approach\nagainst catastrophic forgetting works, as the model is able to retrieve the\npreviously trained domain-independent knowledge. The used dataset and trained\nmodel are made publicly available", "published": "2022-12-06 13:49:12", "link": "http://arxiv.org/abs/2212.02974v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Document-Level Abstractive Summarization", "abstract": "The task of automatic text summarization produces a concise and fluent text\nsummary while preserving key information and overall meaning. Recent approaches\nto document-level summarization have seen significant improvements in recent\nyears by using models based on the Transformer architecture. However, the\nquadratic memory and time complexities with respect to the sequence length make\nthem very expensive to use, especially with long sequences, as required by\ndocument-level summarization. Our work addresses the problem of document-level\nsummarization by studying how efficient Transformer techniques can be used to\nimprove the automatic summarization of very long texts. In particular, we will\nuse the arXiv dataset, consisting of several scientific papers and the\ncorresponding abstracts, as baselines for this work. Then, we propose a novel\nretrieval-enhanced approach based on the architecture which reduces the cost of\ngenerating a summary of the entire document by processing smaller chunks. The\nresults were below the baselines but suggest a more efficient memory a\nconsumption and truthfulness.", "published": "2022-12-06 14:39:09", "link": "http://arxiv.org/abs/2212.03013v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Style transfer and classification in hebrew news items", "abstract": "Hebrew is a Morphological rich language, making its modeling harder than\nsimpler language. Recent developments such as Transformers in general and Bert\nin particular opened a path for Hebrew models that reach SOTA results, not\nfalling short from other non-MRL languages. We explore the cutting edge in this\nfield performing style transfer, text generation and classification over news\narticles collected from online archives. Furthermore, the news portals that\nfeed our collective consciousness are an interesting corpus to study, as their\nanalysis and tracing might reveal insights about our society and discourse.", "published": "2022-12-06 14:47:29", "link": "http://arxiv.org/abs/2212.03019v1", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Contrastive Translation Memories", "abstract": "Retrieval-augmented Neural Machine Translation models have been successful in\nmany translation scenarios. Different from previous works that make use of\nmutually similar but redundant translation memories~(TMs), we propose a new\nretrieval-augmented NMT to model contrastively retrieved translation memories\nthat are holistically similar to the source sentence while individually\ncontrastive to each other providing maximal information gains in three phases.\nFirst, in TM retrieval phase, we adopt a contrastive retrieval algorithm to\navoid redundancy and uninformativeness of similar translation pieces. Second,\nin memory encoding stage, given a set of TMs we propose a novel Hierarchical\nGroup Attention module to gather both local context of each TM and global\ncontext of the whole TM set. Finally, in training phase, a Multi-TM contrastive\nlearning objective is introduced to learn salient feature of each TM with\nrespect to target sentence. Experimental results show that our framework\nobtains improvements over strong baselines on the benchmark datasets.", "published": "2022-12-06 17:10:17", "link": "http://arxiv.org/abs/2212.03140v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Switching to Discriminative Image Captioning by Relieving a Bottleneck\n  of Reinforcement Learning", "abstract": "Discriminativeness is a desirable feature of image captions: captions should\ndescribe the characteristic details of input images. However, recent\nhigh-performing captioning models, which are trained with reinforcement\nlearning (RL), tend to generate overly generic captions despite their high\nperformance in various other criteria. First, we investigate the cause of the\nunexpectedly low discriminativeness and show that RL has a deeply rooted side\neffect of limiting the output words to high-frequency words. The limited\nvocabulary is a severe bottleneck for discriminativeness as it is difficult for\na model to describe the details beyond its vocabulary. Then, based on this\nidentification of the bottleneck, we drastically recast discriminative image\ncaptioning as a much simpler task of encouraging low-frequency word generation.\nHinted by long-tail classification and debiasing methods, we propose methods\nthat easily switch off-the-shelf RL models to discriminativeness-aware models\nwith only a single-epoch fine-tuning on the part of the parameters. Extensive\nexperiments demonstrate that our methods significantly enhance the\ndiscriminativeness of off-the-shelf RL models and even outperform previous\ndiscriminativeness-aware methods with much smaller computational costs.\nDetailed analysis and human evaluation also verify that our methods boost the\ndiscriminativeness without sacrificing the overall quality of captions.", "published": "2022-12-06 18:55:20", "link": "http://arxiv.org/abs/2212.03230v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "KATSum: Knowledge-aware Abstractive Text Summarization", "abstract": "Text Summarization is recognised as one of the NLP downstream tasks and it\nhas been extensively investigated in recent years. It can assist people with\nperceiving the information rapidly from the Internet, including news articles,\nsocial posts, videos, etc. Most existing research works attempt to develop\nsummarization models to produce a better output. However, advent limitations of\nmost existing models emerge, including unfaithfulness and factual errors. In\nthis paper, we propose a novel model, named as Knowledge-aware Abstractive Text\nSummarization, which leverages the advantages offered by Knowledge Graph to\nenhance the standard Seq2Seq model. On top of that, the Knowledge Graph\ntriplets are extracted from the source text and utilised to provide keywords\nwith relational information, producing coherent and factually errorless\nsummaries. We conduct extensive experiments by using real-world data sets. The\nresults reveal that the proposed framework can effectively utilise the\ninformation from Knowledge Graph and significantly reduce the factual errors in\nthe summary.", "published": "2022-12-06 23:43:50", "link": "http://arxiv.org/abs/2212.03371v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improved Beam Search for Hallucination Mitigation in Abstractive\n  Summarization", "abstract": "Advancement in large pretrained language models has significantly improved\ntheir performance for conditional language generation tasks including\nsummarization albeit with hallucinations. To reduce hallucinations,\nconventional methods proposed improving beam search or using a fact checker as\na postprocessing step. In this paper, we investigate the use of the Natural\nLanguage Inference (NLI) entailment metric to detect and prevent hallucinations\nin summary generation. We propose an NLI-assisted beam re-ranking mechanism by\ncomputing entailment probability scores between the input context and\nsummarization model-generated beams during saliency-enhanced greedy decoding.\nMoreover, a diversity metric is introduced to compare its effectiveness against\nvanilla beam search. Our proposed algorithm significantly outperforms vanilla\nbeam decoding on XSum and CNN/DM datasets.", "published": "2022-12-06 02:33:47", "link": "http://arxiv.org/abs/2212.02712v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards human-compatible autonomous car: A study of non-verbal Turing\n  test in automated driving with affective transition modelling", "abstract": "Autonomous cars are indispensable when humans go further down the hands-free\nroute. Although existing literature highlights that the acceptance of the\nautonomous car will increase if it drives in a human-like manner, sparse\nresearch offers the naturalistic experience from a passenger's seat perspective\nto examine the humanness of current autonomous cars. The present study tested\nwhether the AI driver could create a human-like ride experience for passengers\nbased on 69 participants' feedback in a real-road scenario. We designed a ride\nexperience-based version of the non-verbal Turing test for automated driving.\nParticipants rode in autonomous cars (driven by either human or AI drivers) as\na passenger and judged whether the driver was human or AI. The AI driver failed\nto pass our test because passengers detected the AI driver above chance. In\ncontrast, when the human driver drove the car, the passengers' judgement was\naround chance. We further investigated how human passengers ascribe humanness\nin our test. Based on Lewin's field theory, we advanced a computational model\ncombining signal detection theory with pre-trained language models to predict\npassengers' humanness rating behaviour. We employed affective transition\nbetween pre-study baseline emotions and corresponding post-stage emotions as\nthe signal strength of our model. Results showed that the passengers'\nascription of humanness would increase with the greater affective transition.\nOur study suggested an important role of affective transition in passengers'\nascription of humanness, which might become a future direction for autonomous\ndriving.", "published": "2022-12-06 12:06:34", "link": "http://arxiv.org/abs/2212.02908v6", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "SODA: A Natural Language Processing Package to Extract Social\n  Determinants of Health for Cancer Studies", "abstract": "Objective: We aim to develop an open-source natural language processing (NLP)\npackage, SODA (i.e., SOcial DeterminAnts), with pre-trained transformer models\nto extract social determinants of health (SDoH) for cancer patients, examine\nthe generalizability of SODA to a new disease domain (i.e., opioid use), and\nevaluate the extraction rate of SDoH using cancer populations.\n  Methods: We identified SDoH categories and attributes and developed an SDoH\ncorpus using clinical notes from a general cancer cohort. We compared four\ntransformer-based NLP models to extract SDoH, examined the generalizability of\nNLP models to a cohort of patients prescribed with opioids, and explored\ncustomization strategies to improve performance. We applied the best NLP model\nto extract 19 categories of SDoH from the breast (n=7,971), lung (n=11,804),\nand colorectal cancer (n=6,240) cohorts.\n  Results and Conclusion: We developed a corpus of 629 cancer patients notes\nwith annotations of 13,193 SDoH concepts/attributes from 19 categories of SDoH.\nThe Bidirectional Encoder Representations from Transformers (BERT) model\nachieved the best strict/lenient F1 scores of 0.9216 and 0.9441 for SDoH\nconcept extraction, 0.9617 and 0.9626 for linking attributes to SDoH concepts.\nFine-tuning the NLP models using new annotations from opioid use patients\nimproved the strict/lenient F1 scores from 0.8172/0.8502 to 0.8312/0.8679. The\nextraction rates among 19 categories of SDoH varied greatly, where 10 SDoH\ncould be extracted from >70% of cancer patients, but 9 SDoH had a low\nextraction rate (<70% of cancer patients). The SODA package with pre-trained\ntransformer models is publicly available at\nhttps://github.com/uf-hobiinformatics-lab/SDoH_SODA.", "published": "2022-12-06 14:23:38", "link": "http://arxiv.org/abs/2212.03000v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Semantic-Conditional Diffusion Networks for Image Captioning", "abstract": "Recent advances on text-to-image generation have witnessed the rise of\ndiffusion models which act as powerful generative models. Nevertheless, it is\nnot trivial to exploit such latent variable models to capture the dependency\namong discrete words and meanwhile pursue complex visual-language alignment in\nimage captioning. In this paper, we break the deeply rooted conventions in\nlearning Transformer-based encoder-decoder, and propose a new diffusion model\nbased paradigm tailored for image captioning, namely Semantic-Conditional\nDiffusion Networks (SCD-Net). Technically, for each input image, we first\nsearch the semantically relevant sentences via cross-modal retrieval model to\nconvey the comprehensive semantic information. The rich semantics are further\nregarded as semantic prior to trigger the learning of Diffusion Transformer,\nwhich produces the output sentence in a diffusion process. In SCD-Net, multiple\nDiffusion Transformer structures are stacked to progressively strengthen the\noutput sentence with better visional-language alignment and linguistical\ncoherence in a cascaded manner. Furthermore, to stabilize the diffusion\nprocess, a new self-critical sequence training strategy is designed to guide\nthe learning of SCD-Net with the knowledge of a standard autoregressive\nTransformer model. Extensive experiments on COCO dataset demonstrate the\npromising potential of using diffusion models in the challenging image\ncaptioning task. Source code is available at\n\\url{https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/scdnet}.", "published": "2022-12-06 16:08:16", "link": "http://arxiv.org/abs/2212.03099v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Intent Recognition in Conversational Recommender Systems", "abstract": "Any organization needs to improve their products, services, and processes. In\nthis context, engaging with customers and understanding their journey is\nessential. Organizations have leveraged various techniques and technologies to\nsupport customer engagement, from call centres to chatbots and virtual agents.\nRecently, these systems have used Machine Learning (ML) and Natural Language\nProcessing (NLP) to analyze large volumes of customer feedback and engagement\ndata. The goal is to understand customers in context and provide meaningful\nanswers across various channels. Despite multiple advances in Conversational\nArtificial Intelligence (AI) and Recommender Systems (RS), it is still\nchallenging to understand the intent behind customer questions during the\ncustomer journey. To address this challenge, in this paper, we study and\nanalyze the recent work in Conversational Recommender Systems (CRS) in general\nand, more specifically, in chatbot-based CRS. We introduce a pipeline to\ncontextualize the input utterances in conversations. We then take the next step\ntowards leveraging reverse feature engineering to link the contextualized input\nand learning model to support intent recognition. Since performance evaluation\nis achieved based on different ML models, we use transformer base models to\nevaluate the proposed approach using a labelled dialogue dataset (MSDialogue)\nof question-answering interactions between information seekers and answer\nproviders.", "published": "2022-12-06 11:02:42", "link": "http://arxiv.org/abs/2212.03721v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust Speech Recognition via Large-Scale Weak Supervision", "abstract": "We study the capabilities of speech processing systems trained simply to\npredict large amounts of transcripts of audio on the internet. When scaled to\n680,000 hours of multilingual and multitask supervision, the resulting models\ngeneralize well to standard benchmarks and are often competitive with prior\nfully supervised results but in a zero-shot transfer setting without the need\nfor any fine-tuning. When compared to humans, the models approach their\naccuracy and robustness. We are releasing models and inference code to serve as\na foundation for further work on robust speech processing.", "published": "2022-12-06 18:46:04", "link": "http://arxiv.org/abs/2212.04356v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Audio-Visual Speech Representations Learning By\n  Multimodal Self-Distillation", "abstract": "In this work, we present a novel method, named AV2vec, for learning\naudio-visual speech representations by multimodal self-distillation. AV2vec has\na student and a teacher module, in which the student performs a masked latent\nfeature regression task using the multimodal target features generated online\nby the teacher. The parameters of the teacher model are a momentum update of\nthe student. Since our target features are generated online, AV2vec needs no\niteration step like AV-HuBERT and the total training time cost is reduced to\nless than one-fifth. We further propose AV2vec-MLM in this study, which\naugments AV2vec with a masked language model (MLM)-style loss using multitask\nlearning. Our experimental results show that AV2vec achieved comparable\nperformance to the AV-HuBERT baseline. When combined with an MLM-style loss,\nAV2vec-MLM outperformed baselines and achieved the best performance on the\ndownstream tasks.", "published": "2022-12-06 06:37:38", "link": "http://arxiv.org/abs/2212.02782v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BC-VAD: A Robust Bone Conduction Voice Activity Detection", "abstract": "Voice Activity Detection (VAD) is a fundamental module in many audio\napplications. Recent state-of-the-art VAD systems are often based on neural\nnetworks, but they require a computational budget that usually exceeds the\ncapabilities of a small battery-operated device when preserving the performance\nof larger models. In this work, we rely on the input from a bone conduction\nmicrophone (BCM) to design an efficient VAD (BC-VAD) robust against residual\nnon-stationary noises originating from the environment or speakers not wearing\nthe BCM.We first show that a larger VAD system (58k parameters) achieves\nstate-of-the-art results on a publicly available benchmark but fails when\nrunning on bone conduction signals. We then compare its variant BC-VAD (5k\nparameters and trained on BC data) with a baseline especially designed for a\nBCM and show that the proposed method achieves better performances under\nvarious metrics while keeping the realtime processing requirement for a\nmicrocontroller.", "published": "2022-12-06 14:14:00", "link": "http://arxiv.org/abs/2212.02996v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Covariance Regularization for Probabilistic Linear Discriminant Analysis", "abstract": "Probabilistic linear discriminant analysis (PLDA) is commonly used in speaker\nverification systems to score the similarity of speaker embeddings. Recent\nstudies improved the performance of PLDA in domain-matched conditions by\ndiagonalizing its covariance. We suspect such brutal pruning approach could\neliminate its capacity in modeling dimension correlation of speaker embeddings,\nleading to inadequate performance with domain adaptation. This paper explores\ntwo alternative covariance regularization approaches, namely, interpolated PLDA\nand sparse PLDA, to tackle the problem. The interpolated PLDA incorporates the\nprior knowledge from cosine scoring to interpolate the covariance of PLDA. The\nsparse PLDA introduces a sparsity penalty to update the covariance.\nExperimental results demonstrate that both approaches outperform diagonal\nregularization noticeably with domain adaptation. In addition, in-domain data\ncan be significantly reduced when training sparse PLDA for domain adaptation.", "published": "2022-12-06 15:12:58", "link": "http://arxiv.org/abs/2212.03039v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Label-free Knowledge Distillation with Contrastive Loss for Light-weight\n  Speaker Recognition", "abstract": "Very deep models for speaker recognition (SR) have demonstrated remarkable\nperformance improvement in recent research. However, it is impractical to\ndeploy these models for on-device applications with constrained computational\nresources. On the other hand, light-weight models are highly desired in\npractice despite their sub-optimal performance. This research aims to improve\nlight-weight SR models through large-scale label-free knowledge distillation\n(KD). Existing KD approaches for SR typically require speaker labels to learn\ntask-specific knowledge, due to the inefficiency of conventional loss for\ndistillation. To address the inefficiency problem and achieve label-free KD, we\npropose to employ the contrastive loss from self-supervised learning for\ndistillation. Extensive experiments are conducted on a collection of public\nspeech datasets from diverse sources. Results on light-weight SR models show\nthat the proposed approach of label-free KD with contrastive loss consistently\noutperforms both conventional distillation methods and self-supervised learning\nmethods by a significant margin.", "published": "2022-12-06 16:01:59", "link": "http://arxiv.org/abs/2212.03090v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Parameter Efficient Transfer Learning for Various Speech Processing\n  Tasks", "abstract": "Fine-tuning of self-supervised models is a powerful transfer learning method\nin a variety of fields, including speech processing, since it can utilize\ngeneric feature representations obtained from large amounts of unlabeled data.\nFine-tuning, however, requires a new parameter set for each downstream task,\nwhich is parameter inefficient. Adapter architecture is proposed to partially\nsolve this issue by inserting lightweight learnable modules into a frozen\npre-trained model. However, existing adapter architectures fail to adaptively\nleverage low- to high-level features stored in different layers, which is\nnecessary for solving various kinds of speech processing tasks. Thus, we\npropose a new adapter architecture to acquire feature representations more\nflexibly for various speech tasks. In experiments, we applied this adapter to\nWavLM on four speech tasks. It performed on par or better than naive\nfine-tuning, with only 11% of learnable parameters. It also outperformed an\nexisting adapter architecture.", "published": "2022-12-06 06:33:20", "link": "http://arxiv.org/abs/2212.02780v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "FretNet: Continuous-Valued Pitch Contour Streaming for Polyphonic Guitar\n  Tablature Transcription", "abstract": "In recent years, the task of Automatic Music Transcription (AMT), whereby\nvarious attributes of music notes are estimated from audio, has received\nincreasing attention. At the same time, the related task of Multi-Pitch\nEstimation (MPE) remains a challenging but necessary component of almost all\nAMT approaches, even if only implicitly. In the context of AMT, pitch\ninformation is typically quantized to the nominal pitches of the Western music\nscale. Even in more general contexts, MPE systems typically produce pitch\npredictions with some degree of quantization. In certain applications of AMT,\nsuch as Guitar Tablature Transcription (GTT), it is more meaningful to estimate\ncontinuous-valued pitch contours. Guitar tablature has the capacity to\nrepresent various playing techniques, some of which involve pitch modulation.\nContemporary approaches to AMT do not adequately address pitch modulation, and\noffer only less quantization at the expense of more model complexity. In this\npaper, we present a GTT formulation that estimates continuous-valued pitch\ncontours, grouping them according to their string and fret of origin. We\ndemonstrate that for this task, the proposed method significantly improves the\nresolution of MPE and simultaneously yields tablature estimation results\ncompetitive with baseline models.", "published": "2022-12-06 14:51:27", "link": "http://arxiv.org/abs/2212.03023v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
