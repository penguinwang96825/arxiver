{"title": "Improving Compositional Generalization in Math Word Problem Solving", "abstract": "Compositional generalization refers to a model's capability to generalize to\nnewly composed input data based on the data components observed during\ntraining. It has triggered a series of compositional generalization analysis on\ndifferent tasks as generalization is an important aspect of language and\nproblem solving skills. However, the similar discussion on math word problems\n(MWPs) is limited. In this manuscript, we study compositional generalization in\nMWP solving. Specifically, we first introduce a data splitting method to create\ncompositional splits from existing MWP datasets. Meanwhile, we synthesize data\nto isolate the effect of compositions. To improve the compositional\ngeneralization in MWP solving, we propose an iterative data augmentation method\nthat includes diverse compositional variation into training data and could\ncollaborate with MWP methods. During the evaluation, we examine a set of\nmethods and find all of them encounter severe performance loss on the evaluated\ndatasets. We also find our data augmentation method could significantly improve\nthe compositional generalization of general MWP methods. Code is available at\nhttps://github.com/demoleiwang/CGMWP.", "published": "2022-09-03 07:54:14", "link": "http://arxiv.org/abs/2209.01352v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CrossDial: An Entertaining Dialogue Dataset of Chinese Crosstalk", "abstract": "Crosstalk is a traditional Chinese theatrical performance art. It is commonly\nperformed by two performers in the form of a dialogue. With the typical\nfeatures of dialogues, crosstalks are also designed to be hilarious for the\npurpose of amusing the audience. In this study, we introduce CrossDial, the\nfirst open-source dataset containing most classic Chinese crosstalks crawled\nfrom the Web. Moreover, we define two new tasks, provide two benchmarks, and\ninvestigate the ability of current dialogue generation models in the field of\ncrosstalk generation. The experiment results and case studies demonstrate that\ncrosstalk generation is challenging for straightforward methods and remains an\ninteresting topic for future works.", "published": "2022-09-03 08:42:18", "link": "http://arxiv.org/abs/2209.01370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STAD: Self-Training with Ambiguous Data for Low-Resource Relation\n  Extraction", "abstract": "We present a simple yet effective self-training approach, named as STAD, for\nlow-resource relation extraction. The approach first classifies the\nauto-annotated instances into two groups: confident instances and uncertain\ninstances, according to the probabilities predicted by a teacher model. In\ncontrast to most previous studies, which mainly only use the confident\ninstances for self-training, we make use of the uncertain instances. To this\nend, we propose a method to identify ambiguous but useful instances from the\nuncertain instances and then divide the relations into candidate-label set and\nnegative-label set for each ambiguous instance. Next, we propose a set-negative\ntraining method on the negative-label sets for the ambiguous instances and a\npositive training method for the confident instances. Finally, a joint-training\nmethod is proposed to build the final relation extraction system on all data.\nExperimental results on two widely used datasets SemEval2010 Task-8 and\nRe-TACRED with low-resource settings demonstrate that this new self-training\napproach indeed achieves significant and consistent improvements when comparing\nto several competitive self-training systems. Code is publicly available at\nhttps://github.com/jjyunlp/STAD", "published": "2022-09-03 14:16:12", "link": "http://arxiv.org/abs/2209.01431v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Approaches to Multilingual Information Retrieval", "abstract": "Providing access to information across languages has been a goal of\nInformation Retrieval (IR) for decades. While progress has been made on Cross\nLanguage IR (CLIR) where queries are expressed in one language and documents in\nanother, the multilingual (MLIR) task to create a single ranked list of\ndocuments across many languages is considerably more challenging. This paper\ninvestigates whether advances in neural document translation and pretrained\nmultilingual neural language models enable improvements in the state of the art\nover earlier MLIR techniques. The results show that although combining neural\ndocument translation with neural ranking yields the best Mean Average Precision\n(MAP), 98% of that MAP score can be achieved with an 84% reduction in indexing\ntime by using a pretrained XLM-R multilingual language model to index documents\nin their native language, and that 2% difference in effectiveness is not\nstatistically significant. Key to achieving these results for MLIR is to\nfine-tune XLM-R using mixed-language batches from neural translations of MS\nMARCO passages.", "published": "2022-09-03 06:02:52", "link": "http://arxiv.org/abs/2209.01335v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "How to Prompt? Opportunities and Challenges of Zero- and Few-Shot\n  Learning for Human-AI Interaction in Creative Applications of Generative\n  Models", "abstract": "Deep generative models have the potential to fundamentally change the way we\ncreate high-fidelity digital content but are often hard to control. Prompting a\ngenerative model is a promising recent development that in principle enables\nend-users to creatively leverage zero-shot and few-shot learning to assign new\ntasks to an AI ad-hoc, simply by writing them down. However, for the majority\nof end-users writing effective prompts is currently largely a trial and error\nprocess. To address this, we discuss the key opportunities and challenges for\ninteractive creative applications that use prompting as a new paradigm for\nHuman-AI interaction. Based on our analysis, we propose four design goals for\nuser interfaces that support prompting. We illustrate these with concrete UI\ndesign sketches, focusing on the use case of creative writing. The research\ncommunity in HCI and AI can take these as starting points to develop adequate\nuser interfaces for models capable of zero- and few-shot learning.", "published": "2022-09-03 10:16:34", "link": "http://arxiv.org/abs/2209.01390v1", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "vieCap4H-VLSP 2021: Vietnamese Image Captioning for Healthcare Domain\n  using Swin Transformer and Attention-based LSTM", "abstract": "This study presents our approach on the automatic Vietnamese image captioning\nfor healthcare domain in text processing tasks of Vietnamese Language and\nSpeech Processing (VLSP) Challenge 2021, as shown in Figure 1. In recent years,\nimage captioning often employs a convolutional neural network-based\narchitecture as an encoder and a long short-term memory (LSTM) as a decoder to\ngenerate sentences. These models perform remarkably well in different datasets.\nOur proposed model also has an encoder and a decoder, but we instead use a Swin\nTransformer in the encoder, and a LSTM combined with an attention module in the\ndecoder. The study presents our training experiments and techniques used during\nthe competition. Our model achieves a BLEU4 score of 0.293 on the vietCap4H\ndataset, and the score is ranked the 3$^{rd}$ place on the private leaderboard.\nOur code can be found at \\url{https://git.io/JDdJm}.", "published": "2022-09-03 01:06:19", "link": "http://arxiv.org/abs/2209.01304v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Identify The Beehive Sound Using Deep Learning", "abstract": "Flowers play an essential role in removing the duller from the environment.\nThe life cycle of the flowering plants involves pollination, fertilization,\nflowering, seed-formation, dispersion, and germination. Honeybees pollinate\napproximately 75% of all flowering plants. Environmental pollution, climate\nchange, natural landscape demolition, and so on, threaten the natural habitats,\nthus continuously reducing the number of honeybees. As a result, several\nresearchers are attempting to resolve this issue. Applying acoustic\nclassification to recordings of beehive sounds may be a way of detecting\nchanges within them. In this research, we use deep learning techniques, namely\nSequential Neural Network, Convolutional Neural Network, and Recurrent Neural\nNetwork, on the recorded sounds to classify bee sounds from the nonbeehive\nnoises. In addition, we perform a comparative study among some popular non-deep\nlearning techniques, namely Support Vector Machine, Decision Tree, Random\nForest, and Na\\\"ive Bayes, with the deep learning techniques. The techniques\nare also verified on the combined recorded sounds (25-75% noises).", "published": "2022-09-03 09:07:39", "link": "http://arxiv.org/abs/2209.01374v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Equivariant Self-Supervision for Musical Tempo Estimation", "abstract": "Self-supervised methods have emerged as a promising avenue for representation\nlearning in the recent years since they alleviate the need for labeled\ndatasets, which are scarce and expensive to acquire. Contrastive methods are a\npopular choice for self-supervision in the audio domain, and typically provide\na learning signal by forcing the model to be invariant to some transformations\nof the input. These methods, however, require measures such as negative\nsampling or some form of regularisation to be taken to prevent the model from\ncollapsing on trivial solutions. In this work, instead of invariance, we\npropose to use equivariance as a self-supervision signal to learn audio tempo\nrepresentations from unlabelled data. We derive a simple loss function that\nprevents the network from collapsing on a trivial solution during training,\nwithout requiring any form of regularisation or negative sampling. Our\nexperiments show that it is possible to learn meaningful representations for\ntempo estimation by solely relying on equivariant self-supervision, achieving\nperformance comparable with supervised methods on several benchmarks. As an\nadded benefit, our method only requires moderate compute resources and\ntherefore remains accessible to a wide research community.", "published": "2022-09-03 18:43:39", "link": "http://arxiv.org/abs/2209.01478v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
