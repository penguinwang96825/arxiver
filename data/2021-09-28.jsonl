{"title": "\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken\n  Conversations", "abstract": "Most prior work in dialogue modeling has been on written conversations mostly\nbecause of existing data sets. However, written dialogues are not sufficient to\nfully capture the nature of spoken conversations as well as the potential\nspeech recognition errors in practical spoken dialogue systems. This work\npresents a new benchmark on spoken task-oriented conversations, which is\nintended to study multi-domain dialogue state tracking and knowledge-grounded\ndialogue modeling. We report that the existing state-of-the-art models trained\non written conversations are not performing well on our spoken data, as\nexpected. Furthermore, we observe improvements in task performances when\nleveraging n-best speech recognition hypotheses such as by combining\npredictions based on individual hypotheses. Our data set enables speech-based\nbenchmarking of task-oriented dialogue systems.", "published": "2021-09-28 04:51:04", "link": "http://arxiv.org/abs/2109.13489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided\n  MCTS Decoding", "abstract": "Large language models (LM) based on Transformers allow to generate plausible\nlong texts. In this paper, we explore how this generation can be further\ncontrolled at decoding time to satisfy certain constraints (e.g. being\nnon-toxic, conveying certain emotions, using a specific writing style, etc.)\nwithout fine-tuning the LM. Precisely, we formalize constrained generation as a\ntree exploration process guided by a discriminator that indicates how well the\nassociated sequence respects the constraint. This approach, in addition to\nbeing easier and cheaper to train than fine-tuning the LM, allows to apply the\nconstraint more finely and dynamically. We propose several original methods to\nsearch this generation tree, notably the Monte Carlo Tree Search (MCTS) which\nprovides theoretical guarantees on the search efficiency, but also simpler\nmethods based on re-ranking a pool of diverse sequences using the discriminator\nscores. These methods are evaluated, with automatic and human-based metrics, on\ntwo types of constraints and languages: review polarity and emotion control in\nFrench and English. We show that discriminator-guided MCTS decoding achieves\nstate-of-the-art results without having to tune the language model, in both\ntasks and languages. We also demonstrate that other proposed decoding methods\nbased on re-ranking can be really effective when diversity among the generated\npropositions is encouraged.", "published": "2021-09-28 09:29:15", "link": "http://arxiv.org/abs/2109.13582v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Active Learning for Argument Mining: A Practical Approach", "abstract": "Despite considerable recent progress, the creation of well-balanced and\ndiverse resources remains a time-consuming and costly challenge in Argument\nMining. Active Learning reduces the amount of data necessary for the training\nof machine learning models by querying the most informative samples for\nannotation and therefore is a promising method for resource creation. In a\nlarge scale comparison of several Active Learning methods, we show that Active\nLearning considerably decreases the effort necessary to get good deep learning\nperformance on the task of Argument Unit Recognition and Classification (AURC).", "published": "2021-09-28 10:58:47", "link": "http://arxiv.org/abs/2109.13611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking", "abstract": "Recent progress in task-oriented neural dialogue systems is largely focused\non a handful of languages, as annotation of training data is tedious and\nexpensive. Machine translation has been used to make systems multilingual, but\nthis can introduce a pipeline of errors. Another promising solution is using\ncross-lingual transfer learning through pretrained multilingual models.\nExisting methods train multilingual models with additional code-mixed task data\nor refine the cross-lingual representations through parallel ontologies. In\nthis work, we enhance the transfer learning process by intermediate fine-tuning\nof pretrained multilingual models, where the multilingual models are fine-tuned\nwith different but related data and/or tasks. Specifically, we use parallel and\nconversational movie subtitles datasets to design cross-lingual intermediate\ntasks suitable for downstream dialogue tasks. We use only 200K lines of\nparallel data for intermediate fine-tuning which is already available for 1782\nlanguage pairs. We test our approach on the cross-lingual dialogue state\ntracking task for the parallel MultiWoZ (English -> Chinese, Chinese ->\nEnglish) and Multilingual WoZ (English -> German, English -> Italian) datasets.\nWe achieve impressive improvements (> 20% on joint goal accuracy) on the\nparallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla\nbaseline with only 10% of the target language task data and zero-shot setup\nrespectively.", "published": "2021-09-28 11:22:38", "link": "http://arxiv.org/abs/2109.13620v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One to rule them all: Towards Joint Indic Language Hate Speech Detection", "abstract": "This paper is a contribution to the Hate Speech and Offensive Content\nIdentification in Indo-European Languages (HASOC) 2021 shared task. Social\nmedia today is a hotbed of toxic and hateful conversations, in various\nlanguages. Recent news reports have shown that current models struggle to\nautomatically identify hate posted in minority languages. Therefore,\nefficiently curbing hate speech is a critical challenge and problem of\ninterest. We present a multilingual architecture using state-of-the-art\ntransformer language models to jointly learn hate and offensive speech\ndetection across three languages namely, English, Hindi, and Marathi. On the\nprovided testing corpora, we achieve Macro F1 scores of 0.7996, 0.7748, 0.8651\nfor sub-task 1A and 0.6268, 0.5603 during the fine-grained classification of\nsub-task 1B. These results show the efficacy of exploiting a multilingual\ntraining scheme.", "published": "2021-09-28 13:30:00", "link": "http://arxiv.org/abs/2109.13711v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Homophony and R\u00e9nyi Entropy", "abstract": "Homophony's widespread presence in natural languages is a controversial\ntopic. Recent theories of language optimality have tried to justify its\nprevalence, despite its negative effects on cognitive processing time; e.g.,\nPiantadosi et al. (2012) argued homophony enables the reuse of efficient\nwordforms and is thus beneficial for languages. This hypothesis has recently\nbeen challenged by Trott and Bergen (2020), who posit that good wordforms are\nmore often homophonous simply because they are more phonotactically probable.\nIn this paper, we join in on the debate. We first propose a new\ninformation-theoretic quantification of a language's homophony: the sample\nR\\'enyi entropy. Then, we use this quantification to revisit Trott and Bergen's\nclaims. While their point is theoretically sound, a specific methodological\nissue in their experiments raises doubts about their results. After addressing\nthis issue, we find no clear pressure either towards or against homophony -- a\nmuch more nuanced result than either Piantadosi et al.'s or Trott and Bergen's\nfindings.", "published": "2021-09-28 14:41:19", "link": "http://arxiv.org/abs/2109.13766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Micromodels for Efficient, Explainable, and Reusable Systems: A Case\n  Study on Mental Health", "abstract": "Many statistical models have high accuracy on test benchmarks, but are not\nexplainable, struggle in low-resource scenarios, cannot be reused for multiple\ntasks, and cannot easily integrate domain expertise. These factors limit their\nuse, particularly in settings such as mental health, where it is difficult to\nannotate datasets and model outputs have significant impact. We introduce a\nmicromodel architecture to address these challenges. Our approach allows\nresearchers to build interpretable representations that embed domain knowledge\nand provide explanations throughout the model's decision process. We\ndemonstrate the idea on multiple mental health tasks: depression\nclassification, PTSD classification, and suicidal risk assessment. Our systems\nconsistently produce strong results, even in low-resource scenarios, and are\nmore interpretable than alternative methods.", "published": "2021-09-28 14:45:59", "link": "http://arxiv.org/abs/2109.13770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Single-dataset Experts for Multi-dataset Question Answering", "abstract": "Many datasets have been created for training reading comprehension models,\nand a natural question is whether we can combine them to build models that (1)\nperform better on all of the training datasets and (2) generalize and transfer\nbetter to new datasets. Prior work has addressed this goal by training one\nnetwork simultaneously on multiple datasets, which works well on average but is\nprone to over- or under-fitting different sub-distributions and might transfer\nworse compared to source models with more overlap with the target dataset. Our\napproach is to model multi-dataset question answering with a collection of\nsingle-dataset experts, by training a collection of lightweight,\ndataset-specific adapter modules (Houlsby et al., 2019) that share an\nunderlying Transformer model. We find that these Multi-Adapter Dataset Experts\n(MADE) outperform all our baselines in terms of in-distribution accuracy, and\nsimple methods based on parameter-averaging lead to better zero-shot\ngeneralization and few-shot transfer performance, offering a strong and\nversatile starting point for building new reading comprehension systems.", "published": "2021-09-28 17:08:22", "link": "http://arxiv.org/abs/2109.13880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Different Text-preprocessing Techniques Using The BERT Model Affect\n  The Gender Profiling of Authors", "abstract": "Forensic author profiling plays an important role in indicating possible\nprofiles for suspects. Among the many automated solutions recently proposed for\nauthor profiling, transfer learning outperforms many other state-of-the-art\ntechniques in natural language processing. Nevertheless, the sophisticated\ntechnique has yet to be fully exploited for author profiling. At the same time,\nwhereas current methods of author profiling, all largely based on features\nengineering, have spawned significant variation in each model used, transfer\nlearning usually requires a preprocessed text to be fed into the model. We\nreviewed multiple references in the literature and determined the most common\npreprocessing techniques associated with authors' genders profiling.\nConsidering the variations in potential preprocessing techniques, we conducted\nan experimental study that involved applying five such techniques to measure\neach technique's effect while using the BERT model, chosen for being one of the\nmost-used stock pretrained models. We used the Hugging face transformer library\nto implement the code for each preprocessing case. In our five experiments, we\nfound that BERT achieves the best accuracy in predicting the gender of the\nauthor when no preprocessing technique is applied. Our best case achieved\n86.67% accuracy in predicting the gender of authors.", "published": "2021-09-28 17:43:18", "link": "http://arxiv.org/abs/2109.13890v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with\n  Controllable Perturbations", "abstract": "Recent research has adopted a new experimental field centered around the\nconcept of text perturbations which has revealed that shuffled word order has\nlittle to no impact on the downstream performance of Transformer-based language\nmodels across many NLP tasks. These findings contradict the common\nunderstanding of how the models encode hierarchical and structural information\nand even question if the word order is modeled with position embeddings. To\nthis end, this paper proposes nine probing datasets organized by the type of\n\\emph{controllable} text perturbation for three Indo-European languages with a\nvarying degree of word order flexibility: English, Swedish and Russian. Based\non the probing analysis of the M-BERT and M-BART models, we report that the\nsyntactic sensitivity depends on the language and model pre-training\nobjectives. We also find that the sensitivity grows across layers together with\nthe increase of the perturbation granularity. Last but not least, we show that\nthe models barely use the positional information to induce syntactic trees from\ntheir intermediate self-attention and contextualized representations.", "published": "2021-09-28 20:15:29", "link": "http://arxiv.org/abs/2109.14017v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marked Attribute Bias in Natural Language Inference", "abstract": "Reporting and providing test sets for harmful bias in NLP applications is\nessential for building a robust understanding of the current problem. We\npresent a new observation of gender bias in a downstream NLP application:\nmarked attribute bias in natural language inference. Bias in downstream\napplications can stem from training data, word embeddings, or be amplified by\nthe model in use. However, focusing on biased word embeddings is potentially\nthe most impactful first step due to their universal nature. Here we seek to\nunderstand how the intrinsic properties of word embeddings contribute to this\nobserved marked attribute effect, and whether current post-processing methods\naddress the bias successfully. An investigation of the current debiasing\nlandscape reveals two open problems: none of the current debiased embeddings\nmitigate the marked attribute error, and none of the intrinsic bias measures\nare predictive of the marked attribute effect. By noticing that a new type of\nintrinsic bias measure correlates meaningfully with the marked attribute\neffect, we propose a new postprocessing debiasing scheme for static word\nembeddings. The proposed method applied to existing embeddings achieves new\nbest results on the marked attribute bias test set. See\nhttps://github.com/hillary-dawkins/MAB.", "published": "2021-09-28 20:45:02", "link": "http://arxiv.org/abs/2109.14039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias\n  Detection in Coreference Resolution", "abstract": "We observe an instance of gender-induced bias in a downstream application,\ndespite the absence of explicit gender words in the test cases. We provide a\ntest set, SoWinoBias, for the purpose of measuring such latent gender bias in\ncoreference resolution systems. We evaluate the performance of current\ndebiasing methods on the SoWinoBias test set, especially in reference to the\nmethod's design and altered embedding space properties. See\nhttps://github.com/hillarydawkins/SoWinoBias.", "published": "2021-09-28 21:03:32", "link": "http://arxiv.org/abs/2109.14047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hidden Markov Based Mathematical Model dedicated to Extract Ingredients\n  from Recipe Text", "abstract": "Natural Language Processing (NLP) is a branch of artificial intelligence that\ngives machines the ability to decode human languages. Partof-speech tagging\n(POS tagging) is a pre-processing task that requires an annotated corpus.\nRule-based and stochastic methods showed remarkable results for POS tag\nprediction. On this work, I performed a mathematical model based on Hidden\nMarkov structures and I obtained a high-level accuracy of ingredients extracted\nfrom text recipe with performances greater than what traditional methods could\nmake without unknown words consideration.", "published": "2021-09-28 14:38:11", "link": "http://arxiv.org/abs/2110.15707v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SYGMA: System for Generalizable Modular Question Answering OverKnowledge\n  Bases", "abstract": "Knowledge Base Question Answering (KBQA) tasks that in-volve complex\nreasoning are emerging as an important re-search direction. However, most KBQA\nsystems struggle withgeneralizability, particularly on two dimensions: (a)\nacrossmultiple reasoning types where both datasets and systems haveprimarily\nfocused on multi-hop reasoning, and (b) across mul-tiple knowledge bases, where\nKBQA approaches are specif-ically tuned to a single knowledge base. In this\npaper, wepresent SYGMA, a modular approach facilitating general-izability\nacross multiple knowledge bases and multiple rea-soning types. Specifically,\nSYGMA contains three high levelmodules: 1) KB-agnostic question understanding\nmodule thatis common across KBs 2) Rules to support additional reason-ing types\nand 3) KB-specific question mapping and answeringmodule to address the\nKB-specific aspects of the answer ex-traction. We demonstrate effectiveness of\nour system by evalu-ating on datasets belonging to two distinct knowledge\nbases,DBpedia and Wikidata. In addition, to demonstrate extensi-bility to\nadditional reasoning types we evaluate on multi-hopreasoning datasets and a new\nTemporal KBQA benchmarkdataset on Wikidata, namedTempQA-WD1, introduced in\nthispaper. We show that our generalizable approach has bettercompetetive\nperformance on multiple datasets on DBpediaand Wikidata that requires both\nmulti-hop and temporal rea-soning", "published": "2021-09-28 01:57:56", "link": "http://arxiv.org/abs/2109.13430v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Teacher-Student Learning Approach for Multi-lingual\n  Speech-to-Intent Classification", "abstract": "End-to-end speech-to-intent classification has shown its advantage in\nharvesting information from both text and speech. In this paper, we study a\ntechnique to develop such an end-to-end system that supports multiple\nlanguages. To overcome the scarcity of multi-lingual speech corpus, we exploit\nknowledge from a pre-trained multi-lingual natural language processing model.\nMulti-lingual bidirectional encoder representations from transformers (mBERT)\nmodels are trained on multiple languages and hence expected to perform well in\nthe multi-lingual scenario. In this work, we employ a teacher-student learning\napproach to sufficiently extract information from an mBERT model to train a\nmulti-lingual speech model. In particular, we use synthesized speech generated\nfrom an English-Mandarin text corpus for analysis and training of a\nmulti-lingual intent classification model. We also demonstrate that the\nteacher-student learning approach obtains an improved performance (91.02%) over\nthe traditional end-to-end (89.40%) intent classification approach in a\npractical multi-lingual scenario.", "published": "2021-09-28 04:43:11", "link": "http://arxiv.org/abs/2109.13486v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Instance-Based Neural Dependency Parsing", "abstract": "Interpretable rationales for model predictions are crucial in practical\napplications. We develop neural models that possess an interpretable inference\nprocess for dependency parsing. Our models adopt instance-based inference,\nwhere dependency edges are extracted and labeled by comparing them to edges in\na training set. The training edges are explicitly used for the predictions;\nthus, it is easy to grasp the contribution of each edge to the predictions. Our\nexperiments show that our instance-based models achieve competitive accuracy\nwith standard neural models and have the reasonable plausibility of\ninstance-based explanations.", "published": "2021-09-28 05:30:52", "link": "http://arxiv.org/abs/2109.13497v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Template-free Prompt Tuning for Few-shot NER", "abstract": "Prompt-based methods have been successfully applied in sentence-level\nfew-shot learning tasks, mostly owing to the sophisticated design of templates\nand label words. However, when applied to token-level labeling tasks such as\nNER, it would be time-consuming to enumerate the template queries over all\npotential entity spans. In this work, we propose a more elegant method to\nreformulate NER tasks as LM problems without any templates. Specifically, we\ndiscard the template construction process while maintaining the word prediction\nparadigm of pre-training models to predict a class-related pivot word (or label\nword) at the entity position. Meanwhile, we also explore principled ways to\nautomatically search for appropriate label words that the pre-trained models\ncan easily adapt to. While avoiding complicated template-based process, the\nproposed LM objective also reduces the gap between different objectives used in\npre-training and fine-tuning, thus it can better benefit the few-shot\nperformance. Experimental results demonstrate the effectiveness of the proposed\nmethod over bert-tagger and template-based method under few-shot setting.\nMoreover, the decoding speed of the proposed method is up to 1930.12 times\nfaster than the template-based method.", "published": "2021-09-28 07:19:24", "link": "http://arxiv.org/abs/2109.13532v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Counter Narrative Type Classification", "abstract": "The growing interest in employing counter narratives for hatred intervention\nbrings with it a focus on dataset creation and automation strategies. In this\nscenario, learning to recognize counter narrative types from natural text is\nexpected to be useful for applications such as hate speech countering, where\noperators from non-governmental organizations are supposed to answer to hate\nwith several and diverse arguments that can be mined from online sources. This\npaper presents the first multilingual work on counter narrative type\nclassification, evaluating SoTA pre-trained language models in monolingual,\nmultilingual and cross-lingual settings. When considering a fine-grained\nannotation of counter narrative classes, we report strong baseline\nclassification results for the majority of the counter narrative types,\nespecially if we translate every language to English before cross-lingual\nprediction. This suggests that knowledge about counter narratives can be\nsuccessfully transferred across languages.", "published": "2021-09-28 12:31:04", "link": "http://arxiv.org/abs/2109.13664v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CIDEr-R: Robust Consensus-based Image Description Evaluation", "abstract": "This paper shows that CIDEr-D, a traditional evaluation metric for image\ndescription, does not work properly on datasets where the number of words in\nthe sentence is significantly greater than those in the MS COCO Captions\ndataset. We also show that CIDEr-D has performance hampered by the lack of\nmultiple reference sentences and high variance of sentence length. To bypass\nthis problem, we introduce CIDEr-R, which improves CIDEr-D, making it more\nflexible in dealing with datasets with high sentence length variance. We\ndemonstrate that CIDEr-R is more accurate and closer to human judgment than\nCIDEr-D; CIDEr-R is more robust regarding the number of available references.\nOur results reveal that using Self-Critical Sequence Training to optimize\nCIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized,\nthe generated captions' length tends to be similar to the reference length.\nHowever, the models also repeat several times the same word to increase the\nsentence length.", "published": "2021-09-28 13:13:21", "link": "http://arxiv.org/abs/2109.13701v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Identifying and Mitigating Gender Bias in Hyperbolic Word Embeddings", "abstract": "Euclidean word embedding models such as GloVe and Word2Vec have been shown to\nreflect human-like gender biases. In this paper, we extend the study of gender\nbias to the recently popularized hyperbolic word embeddings. We propose\ngyrocosine bias, a novel measure for quantifying gender bias in hyperbolic word\nrepresentations and observe a significant presence of gender bias. To address\nthis problem, we propose Poincar\\'e Gender Debias (PGD), a novel debiasing\nprocedure for hyperbolic word representations. Experiments on a suit of\nevaluation tests show that PGD effectively reduces bias while adding a minimal\nsemantic offset.", "published": "2021-09-28 14:43:37", "link": "http://arxiv.org/abs/2109.13767v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Expectation-based Minimalist Grammars", "abstract": "Expectation-based Minimalist Grammars (e-MGs) are simplified versions of the\n(Conflated) Minimalist Grammars, (C)MGs, formalized by Stabler (Stabler, 2011,\n2013, 1997) and Phase-based Minimalist Grammars, PMGs (Chesi, 2005, 2007;\nStabler, 2011). The crucial simplification consists of driving structure\nbuilding only by relying on lexically encoded categorial top-down expectations.\nThe commitment on a top-down derivation (as in e-MGs and PMGs, as opposed to\n(C)MGs, Chomsky, 1995; Stabler, 2011) allows us to define a core derivation\nthat should be the same in both parsing and generation (Momma & Phillips,\n2018).", "published": "2021-09-28 17:00:10", "link": "http://arxiv.org/abs/2109.13871v3", "categories": ["cs.CL", "cs.CC"], "primary_category": "cs.CL"}
{"title": "Text Simplification for Comprehension-based Question-Answering", "abstract": "Text simplification is the process of splitting and rephrasing a sentence to\na sequence of sentences making it easier to read and understand while\npreserving the content and approximating the original meaning. Text\nsimplification has been exploited in NLP applications like machine translation,\nsummarization, semantic role labeling, and information extraction, opening a\nbroad avenue for its exploitation in comprehension-based question-answering\ndownstream tasks. In this work, we investigate the effect of text\nsimplification in the task of question-answering using a comprehension context.\nWe release Simple-SQuAD, a simplified version of the widely-used SQuAD dataset.\n  Firstly, we outline each step in the dataset creation pipeline, including\nstyle transfer, thresholding of sentences showing correct transfer, and offset\nfinding for each answer. Secondly, we verify the quality of the transferred\nsentences through various methodologies involving both automated and human\nevaluation. Thirdly, we benchmark the newly created corpus and perform an\nablation study for examining the effect of the simplification process in the\nSQuAD-based question answering task. Our experiments show that simplification\nleads to up to 2.04% and 1.74% increase in Exact Match and F1, respectively.\nFinally, we conclude with an analysis of the transfer process, investigating\nthe types of edits made by the model, and the effect of sentence length on the\ntransfer model.", "published": "2021-09-28 18:48:00", "link": "http://arxiv.org/abs/2109.13984v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text\n  Understanding", "abstract": "We present VideoCLIP, a contrastive approach to pre-train a unified model for\nzero-shot video and text understanding, without using any labels on downstream\ntasks. VideoCLIP trains a transformer for video and text by contrasting\ntemporally overlapping positive video-text pairs with hard negatives from\nnearest neighbor retrieval. Our experiments on a diverse series of downstream\ntasks, including sequence-level text-video retrieval, VideoQA, token-level\naction localization, and action segmentation reveal state-of-the-art\nperformance, surpassing prior work, and in some cases even outperforming\nsupervised approaches. Code is made available at\nhttps://github.com/pytorch/fairseq/tree/main/examples/MMPT.", "published": "2021-09-28 23:01:51", "link": "http://arxiv.org/abs/2109.14084v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Visually Grounded Reasoning across Languages and Cultures", "abstract": "The design of widespread vision-and-language datasets and pre-trained\nencoders directly adopts, or draws inspiration from, the concepts and images of\nImageNet. While one can hardly overestimate how much this benchmark contributed\nto progress in computer vision, it is mostly derived from lexical databases and\nimage queries in English, resulting in source material with a North American or\nWestern European bias. Therefore, we devise a new protocol to construct an\nImageNet-style hierarchy representative of more languages and cultures. In\nparticular, we let the selection of both concepts and images be entirely driven\nby native speakers, rather than scraping them automatically. Specifically, we\nfocus on a typologically diverse set of languages, namely, Indonesian, Mandarin\nChinese, Swahili, Tamil, and Turkish. On top of the concepts and images\nobtained through this new protocol, we create a multilingual dataset for\n{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting\nstatements from native speaker annotators about pairs of images. The task\nconsists of discriminating whether each grounded statement is true or false. We\nestablish a series of baselines using state-of-the-art models and find that\ntheir cross-lingual transfer performance lags dramatically behind supervised\nperformance in English. These results invite us to reassess the robustness and\naccuracy of current state-of-the-art models beyond a narrow domain, but also\nopen up new exciting challenges for the development of truly multilingual and\nmulticultural systems.", "published": "2021-09-28 16:51:38", "link": "http://arxiv.org/abs/2109.13238v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "When in Doubt: Improving Classification Performance with Alternating\n  Normalization", "abstract": "We introduce Classification with Alternating Normalization (CAN), a\nnon-parametric post-processing step for classification. CAN improves\nclassification accuracy for challenging examples by re-adjusting their\npredicted class probability distribution using the predicted class\ndistributions of high-confidence validation examples. CAN is easily applicable\nto any probabilistic classifier, with minimal computation overhead. We analyze\nthe properties of CAN using simulated experiments, and empirically demonstrate\nits effectiveness across a diverse set of classification tasks.", "published": "2021-09-28 02:55:42", "link": "http://arxiv.org/abs/2109.13449v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "VoxCeleb Enrichment for Age and Gender Recognition", "abstract": "VoxCeleb datasets are widely used in speaker recognition studies. Our work\nserves two purposes. First, we provide speaker age labels and (an alternative)\nannotation of speaker gender. Second, we demonstrate the use of this metadata\nby constructing age and gender recognition models with different features and\nclassifiers. We query different celebrity databases and apply consensus rules\nto derive age and gender labels. We also compare the original VoxCeleb gender\nlabels with our labels to identify records that might be mislabeled in the\noriginal VoxCeleb data. On modeling side, we design a comprehensive study of\nmultiple features and models for recognizing gender and age. Our best system,\nusing i-vector features, achieved an F1-score of 0.9829 for gender recognition\ntask using logistic regression, and the lowest mean absolute error (MAE) in age\nregression, 9.443 years, is obtained with ridge regression. This indicates\nchallenge in age estimation from in-the-wild style speech data.", "published": "2021-09-28 06:18:57", "link": "http://arxiv.org/abs/2109.13510v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Agreeing to Disagree: Annotating Offensive Language Datasets with\n  Annotators' Disagreement", "abstract": "Since state-of-the-art approaches to offensive language detection rely on\nsupervised learning, it is crucial to quickly adapt them to the continuously\nevolving scenario of social media. While several approaches have been proposed\nto tackle the problem from an algorithmic perspective, so to reduce the need\nfor annotated data, less attention has been paid to the quality of these data.\nFollowing a trend that has emerged recently, we focus on the level of agreement\namong annotators while selecting data to create offensive language datasets, a\ntask involving a high level of subjectivity. Our study comprises the creation\nof three novel datasets of English tweets covering different topics and having\nfive crowd-sourced judgments each. We also present an extensive set of\nexperiments showing that selecting training and test data according to\ndifferent levels of annotators' agreement has a strong effect on classifiers\nperformance and robustness. Our findings are further validated in cross-domain\nexperiments and studied using a popular benchmark dataset. We show that such\nhard cases, where low agreement is present, are not necessarily due to\npoor-quality annotation and we advocate for a higher presence of ambiguous\ncases in future datasets, particularly in test sets, to better account for the\ndifferent points of view expressed online.", "published": "2021-09-28 08:55:04", "link": "http://arxiv.org/abs/2109.13563v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Nana-HDR: A Non-attentive Non-autoregressive Hybrid Model for TTS", "abstract": "This paper presents Nana-HDR, a new non-attentive non-autoregressive model\nwith hybrid Transformer-based Dense-fuse encoder and RNN-based decoder for TTS.\nIt mainly consists of three parts: Firstly, a novel Dense-fuse encoder with\ndense connections between basic Transformer blocks for coarse feature fusion\nand a multi-head attention layer for fine feature fusion. Secondly, a\nsingle-layer non-autoregressive RNN-based decoder. Thirdly, a duration\npredictor instead of an attention model that connects the above hybrid encoder\nand decoder. Experiments indicate that Nana-HDR gives full play to the\nadvantages of each component, such as strong text encoding ability of\nTransformer-based encoder, stateful decoding without being bothered by exposure\nbias and local information preference, and stable alignment provided by\nduration predictor. Due to these advantages, Nana-HDR achieves competitive\nperformance in naturalness and robustness on two Mandarin corpora.", "published": "2021-09-28 12:45:14", "link": "http://arxiv.org/abs/2109.13673v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Actionable Entities Recognition Benchmark for Interactive Fiction", "abstract": "This paper presents a new natural language processing task - Actionable\nEntities Recognition (AER) - recognition of entities that protagonists could\ninteract with for further plot development. Though similar to classical Named\nEntity Recognition (NER), it has profound differences. In particular, it is\ncrucial for interactive fiction, where the agent needs to detect entities that\nmight be useful in the future. We also discuss if AER might be further helpful\nfor the systems dealing with narrative processing since actionable entities\nprofoundly impact the causal relationship in a story. We validate the proposed\ntask on two previously available datasets and present a new benchmark dataset\nfor the AER task that includes 5550 descriptions with one or more actionable\nentities.", "published": "2021-09-28 16:39:59", "link": "http://arxiv.org/abs/2109.13855v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "J.5; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Temporal Information and Event Markup Language: TIE-ML Markup Process\n  and Schema Version 1.0", "abstract": "Temporal Information and Event Markup Language (TIE-ML) is a markup strategy\nand annotation schema to improve the productivity and accuracy of temporal and\nevent related annotation of corpora to facilitate machine learning based model\ntraining. For the annotation of events, temporal sequencing, and durations, it\nis significantly simpler by providing an extremely reduced tag set for just\ntemporal relations and event enumeration. In comparison to other standards, as\nfor example the Time Markup Language (TimeML), it is much easier to use by\ndropping sophisticated formalisms, theoretical concepts, and annotation\napproaches. Annotations of corpora using TimeML can be mapped to TIE-ML with a\nloss, and TIE-ML annotations can be fully mapped to TimeML with certain\nunder-specification.", "published": "2021-09-28 17:43:54", "link": "http://arxiv.org/abs/2109.13892v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Unsolved Problems in ML Safety", "abstract": "Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), reducing inherent model hazards\n(\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout,\nwe clarify each problem's motivation and provide concrete research directions.", "published": "2021-09-28 17:59:36", "link": "http://arxiv.org/abs/2109.13916v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Generating Summaries for Scientific Paper Review", "abstract": "The review process is essential to ensure the quality of publications.\nRecently, the increase of submissions for top venues in machine learning and\nNLP has caused a problem of excessive burden on reviewers and has often caused\nconcerns regarding how this may not only overload reviewers, but also may\naffect the quality of the reviews. An automatic system for assisting with the\nreviewing process could be a solution for ameliorating the problem. In this\npaper, we explore automatic review summary generation for scientific papers. We\nposit that neural language models have the potential to be valuable candidates\nfor this task. In order to test this hypothesis, we release a new dataset of\nscientific papers and their reviews, collected from papers published in the\nNeurIPS conference from 2013 to 2020. We evaluate state of the art neural\nsummarization models, present initial results on the feasibility of automatic\nreview summary generation, and propose directions for the future.", "published": "2021-09-28 21:43:53", "link": "http://arxiv.org/abs/2109.14059v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAFT: A Real-World Few-Shot Text Classification Benchmark", "abstract": "Large pre-trained language models have shown promise for few-shot learning,\ncompleting text-based tasks given only a few task-specific examples. Will\nmodels soon solve classification tasks that have so far been reserved for human\nresearch assistants? Existing benchmarks are not designed to measure progress\nin applied settings, and so don't directly answer this question. The RAFT\nbenchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring\ntasks and uses an evaluation setup that mirrors deployment. Baseline\nevaluations on RAFT reveal areas current techniques struggle with: reasoning\nover long texts and tasks with many classes. Human baselines show that some\nclassification tasks are difficult for non-expert humans, reflecting that\nreal-world value sometimes depends on domain expertise. Yet even non-expert\nhuman baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets\nand leaderboard will track which model improvements translate into real-world\nbenefits at https://raft.elicit.org .", "published": "2021-09-28 22:35:31", "link": "http://arxiv.org/abs/2109.14076v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Private Language Model Adaptation for Speech Recognition", "abstract": "Speech model adaptation is crucial to handle the discrepancy between\nserver-side proxy training data and actual data received on local devices of\nusers. With the use of federated learning (FL), we introduce an efficient\napproach on continuously adapting neural network language models (NNLMs) on\nprivate devices with applications on automatic speech recognition (ASR). To\naddress the potential speech transcription errors in the on-device training\ncorpus, we perform empirical studies on comparing various strategies of\nleveraging token-level confidence scores to improve the NNLM quality in the FL\nsettings. Experiments show that compared with no model adaptation, the proposed\nmethod achieves relative 2.6% and 10.8% word error rate (WER) reductions on two\nspeech evaluation datasets, respectively. We also provide analysis in\nevaluating privacy guarantees of our presented procedure.", "published": "2021-09-28 00:15:43", "link": "http://arxiv.org/abs/2110.10026v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Word-level confidence estimation for RNN transducers", "abstract": "Confidence estimate is an often requested feature in applications such as\nmedical transcription where errors can impact patient care and the confidence\nestimate could be used to alert medical professionals to verify potential\nerrors in recognition.\n  In this paper, we present a lightweight neural confidence model tailored for\nAutomatic Speech Recognition (ASR) system with Recurrent Neural Network\nTransducers (RNN-T). Compared to other existing approaches, our model utilizes:\n(a) the time information associated with recognized words, which reduces the\ncomputational complexity, and (b) a simple and elegant trick for mapping\nbetween sub-word and word sequences. The mapping addresses the non-unique\ntokenization and token deletion problems while amplifying differences between\nconfusable words. Through extensive empirical evaluations on two different\nlong-form test sets, we demonstrate that the model achieves a performance of\n0.4 Normalized Cross Entropy (NCE) and 0.05 Expected Calibration Error (ECE).\nIt is robust across different ASR configurations, including target types\n(graphemes vs. morphemes), traffic conditions (streaming vs. non-streaming),\nand encoder types. We further discuss the importance of evaluation metrics to\nreflect practical applications and highlight the need for further work in\nimproving Area Under the Curve (AUC) for Negative Precision Rate (NPV) and True\nNegative Rate (TNR).", "published": "2021-09-28 18:38:00", "link": "http://arxiv.org/abs/2110.15222v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DeepPSL: End-to-end perception and reasoning", "abstract": "We introduce DeepPSL a variant of probabilistic soft logic (PSL) to produce\nan end-to-end trainable system that integrates reasoning and perception. PSL\nrepresents first-order logic in terms of a convex graphical model -- hinge-loss\nMarkov random fields (HL-MRFs). PSL stands out among probabilistic logic\nframeworks due to its tractability having been applied to systems of more than\n1 billion ground rules. The key to our approach is to represent predicates in\nfirst-order logic using deep neural networks and then to approximately\nback-propagate through the HL-MRF and thus train every aspect of the\nfirst-order system being represented. We believe that this approach represents\nan interesting direction for the integration of deep learning and reasoning\ntechniques with applications to knowledge base learning, multi-task learning,\nand explainability. Evaluation on three different tasks demonstrates that\nDeepPSL significantly outperforms state-of-the-art neuro-symbolic methods on\nscalability while achieving comparable or better accuracy.", "published": "2021-09-28 12:30:33", "link": "http://arxiv.org/abs/2109.13662v4", "categories": ["eess.SY", "cs.AI", "cs.CL", "cs.LG", "cs.SY"], "primary_category": "eess.SY"}
{"title": "North America Bixby Speaker Diarization System for the VoxCeleb Speaker\n  Recognition Challenge 2021", "abstract": "This paper describes the submission to the speaker diarization track of\nVoxCeleb Speaker Recognition Challenge 2021 done by North America Bixby Lab of\nSamsung Research America. Our speaker diarization system consists of four main\ncomponents such as overlap speech detection and speech separation, robust\nspeaker embedding extraction, spectral clustering with fused affinity matrix,\nand leakage filtering-based postprocessing. We evaluated our system on the\nVoxConverse dataset and the challenge evaluation set, which contain natural\nconversations of multiple talkers collected from YouTube. Our system obtained\n4.46%, 6.39%, and 6.16% of the diarization error rate on the VoxConverse\ndevelopment, test, and the challenge evaluation set, respectively.", "published": "2021-09-28 06:45:01", "link": "http://arxiv.org/abs/2109.13518v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "FastMVAE2: On improving and accelerating the fast variational\n  autoencoder-based source separation algorithm for determined mixtures", "abstract": "This paper proposes a new source model and training scheme to improve the\naccuracy and speed of the multichannel variational autoencoder (MVAE) method.\nThe MVAE method is a recently proposed powerful multichannel source separation\nmethod. It consists of pretraining a source model represented by a conditional\nVAE (CVAE) and then estimating separation matrices along with other unknown\nparameters so that the log-likelihood is non-decreasing given an observed\nmixture signal. Although the MVAE method has been shown to provide high source\nseparation performance, one drawback is the computational cost of the\nbackpropagation steps in the separation-matrix estimation algorithm. To\novercome this drawback, a method called \"FastMVAE\" was subsequently proposed,\nwhich uses an auxiliary classifier VAE (ACVAE) to train the source model. By\nusing the classifier and encoder trained in this way, the optimal parameters of\nthe source model can be inferred efficiently, albeit approximately, in each\nstep of the algorithm. However, the generalization capability of the trained\nACVAE source model was not satisfactory, which led to poor performance in\nsituations with unseen data. To improve the generalization capability, this\npaper proposes a new model architecture (called the \"ChimeraACVAE\" model) and a\ntraining scheme based on knowledge distillation. The experimental results\nrevealed that the proposed source model trained with the proposed loss function\nachieved better source separation performance with less computation time than\nFastMVAE. We also confirmed that our methods were able to separate 18 sources\nwith a reasonably good accuracy.", "published": "2021-09-28 05:30:03", "link": "http://arxiv.org/abs/2109.13496v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VoiceFixer: Toward General Speech Restoration with Neural Vocoder", "abstract": "Speech restoration aims to remove distortions in speech signals. Prior\nmethods mainly focus on single-task speech restoration (SSR), such as speech\ndenoising or speech declipping. However, SSR systems only focus on one task and\ndo not address the general speech restoration problem. In addition, previous\nSSR systems show limited performance in some speech restoration tasks such as\nspeech super-resolution. To overcome those limitations, we propose a general\nspeech restoration (GSR) task that attempts to remove multiple distortions\nsimultaneously. Furthermore, we propose VoiceFixer, a generative framework to\naddress the GSR task. VoiceFixer consists of an analysis stage and a synthesis\nstage to mimic the speech analysis and comprehension of the human auditory\nsystem. We employ a ResUNet to model the analysis stage and a neural vocoder to\nmodel the synthesis stage. We evaluate VoiceFixer with additive noise, room\nreverberation, low-resolution, and clipping distortions. Our baseline GSR model\nachieves a 0.499 higher mean opinion score (MOS) than the speech enhancement\nSSR model. VoiceFixer further surpasses the GSR baseline model on the MOS score\nby 0.256. Moreover, we observe that VoiceFixer generalizes well to severely\ndegraded real speech recordings, indicating its potential in restoring old\nmovies and historical speeches. The source code is available at\nhttps://github.com/haoheliu/voicefixer_main.", "published": "2021-09-28 13:51:16", "link": "http://arxiv.org/abs/2109.13731v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Articulatory Coordination for Speech Motor Tracking in Huntington\n  Disease", "abstract": "Huntington Disease (HD) is a progressive disorder which often manifests in\nmotor impairment. Motor severity (captured via motor score) is a key component\nin assessing overall HD severity. However, motor score evaluation involves\nin-clinic visits with a trained medical professional, which are expensive and\nnot always accessible. Speech analysis provides an attractive avenue for\ntracking HD severity because speech is easy to collect remotely and provides\ninsight into motor changes. HD speech is typically characterized as having\nirregular articulation. With this in mind, acoustic features that can capture\nvocal tract movement and articulatory coordination are particularly promising\nfor characterizing motor symptom progression in HD. In this paper, we present\nan experiment that uses Vocal Tract Coordination (VTC) features extracted from\nread speech to estimate a motor score. When using an elastic-net regression\nmodel, we find that VTC features significantly outperform other acoustic\nfeatures across varied-length audio segments, which highlights the\neffectiveness of these features for both short- and long-form reading tasks.\nLastly, we analyze the F-value scores of VTC features to visualize which\nchannels are most related to motor score. This work enables future research\nefforts to consider VTC features for acoustic analyses which target HD motor\nsymptomatology tracking.", "published": "2021-09-28 15:39:49", "link": "http://arxiv.org/abs/2109.13815v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The JHU submission to VoxSRC-21: Track 3", "abstract": "This technical report describes Johns Hopkins University speaker recognition\nsystem submitted to Voxceleb Speaker Recognition Challenge 2021 Track 3:\nSelf-supervised speaker verification (closed). Our overall training process is\nsimilar to the proposed one from the first place team in the last year's\nVoxSRC2020 challenge. The main difference is a recently proposed\nnon-contrastive self-supervised method in computer vision (CV), distillation\nwith no labels (DINO), is used to train our initial model, which outperformed\nthe last year's contrastive learning based on momentum contrast (MoCo). Also,\nthis requires only a few iterations in the iterative clustering stage, where\npseudo labels for supervised embedding learning are updated based on the\nclusters of the embeddings generated from a model that is continually\nfine-tuned over iterations. In the final stage, Res2Net50 is trained on the\nfinal pseudo labels from the iterative clustering stage. This is our best\nsubmitted model to the challenge, showing 1.89, 6.50, and 6.89 in EER(%) in\nvoxceleb1 test o, VoxSRC-21 validation, and test trials, respectively.", "published": "2021-09-28 01:30:10", "link": "http://arxiv.org/abs/2109.13425v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MSR-NV: Neural Vocoder Using Multiple Sampling Rates", "abstract": "The development of neural vocoders (NVs) has resulted in the high-quality and\nfast generation of waveforms. However, conventional NVs target a single\nsampling rate and require re-training when applied to different sampling rates.\nA suitable sampling rate varies from application to application due to the\ntrade-off between speech quality and generation speed. In this study, we\npropose a method to handle multiple sampling rates in a single NV, called the\nMSR-NV. By generating waveforms step-by-step starting from a low sampling rate,\nMSR-NV can efficiently learn the characteristics of each frequency band and\nsynthesize high-quality speech at multiple sampling rates. It can be regarded\nas an extension of the previously proposed NVs, and in this study, we extend\nthe structure of Parallel WaveGAN (PWG). Experimental evaluation results\ndemonstrate that the proposed method achieves remarkably higher subjective\nquality than the original PWG trained separately at 16, 24, and 48 kHz, without\nincreasing the inference time. We also show that MSR-NV can leverage speech\nwith lower sampling rates to further improve the quality of the synthetic\nspeech.", "published": "2021-09-28 13:31:20", "link": "http://arxiv.org/abs/2109.13714v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The impact of non-target events in synthetic soundscapes for sound event\n  detection", "abstract": "Detection and Classification Acoustic Scene and Events Challenge 2021 Task 4\nuses a heterogeneous dataset that includes both recorded and synthetic\nsoundscapes. Until recently only target sound events were considered when\nsynthesizing the soundscapes. However, recorded soundscapes often contain a\nsubstantial amount of non-target events that may affect the performance. In\nthis paper, we focus on the impact of these non-target events in the synthetic\nsoundscapes. Firstly, we investigate to what extent using non-target events\nalternatively during the training or validation phase (or none of them) helps\nthe system to correctly detect target events. Secondly, we analyze to what\nextend adjusting the signal-to-noise ratio between target and non-target events\nat training improves the sound event detection performance. The results show\nthat using both target and non-target events for only one of the phases\n(validation or training) helps the system to properly detect sound events,\noutperforming the baseline (which uses non-target events in both phases). The\npaper also reports the results of a preliminary study on evaluating the system\non clips that contain only non-target events. This opens questions for future\nwork on non-target subset and acoustic similarity between target and non-target\nevents which might confuse the system.", "published": "2021-09-28 21:46:19", "link": "http://arxiv.org/abs/2109.14061v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
