{"title": "3D-EX : A Unified Dataset of Definitions and Dictionary Examples", "abstract": "Definitions are a fundamental building block in lexicography, linguistics and\ncomputational semantics. In NLP, they have been used for retrofitting word\nembeddings or augmenting contextual representations in language models.\nHowever, lexical resources containing definitions exhibit a wide range of\nproperties, which has implications in the behaviour of models trained and\nevaluated on them. In this paper, we introduce 3D- EX , a dataset that aims to\nfill this gap by combining well-known English resources into one centralized\nknowledge repository in the form of <term, definition, example> triples. 3D- EX\nis a unified evaluation framework with carefully pre-computed\ntrain/validation/test splits to prevent memorization. We report experimental\nresults that suggest that this dataset could be effectively leveraged in\ndownstream NLP tasks. Code and data are available at\nhttps://github.com/F-Almeman/3D-EX .", "published": "2023-08-06 07:59:12", "link": "http://arxiv.org/abs/2308.03043v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "System-Initiated Transitions from Chit-Chat to Task-Oriented Dialogues\n  with Transition Info Extractor and Transition Sentence Generator", "abstract": "In this work, we study dialogue scenarios that start from chit-chat but\neventually switch to task-related services, and investigate how a unified\ndialogue model, which can engage in both chit-chat and task-oriented dialogues,\ntakes the initiative during the dialogue mode transition from chit-chat to\ntask-oriented in a coherent and cooperative manner. We firstly build a\n{transition info extractor} (TIE) that keeps track of the preceding chit-chat\ninteraction and detects the potential user intention to switch to a\ntask-oriented service. Meanwhile, in the unified model, a {transition sentence\ngenerator} (TSG) is extended through efficient Adapter tuning and transition\nprompt learning. When the TIE successfully finds task-related information from\nthe preceding chit-chat, such as a transition domain, then the TSG is activated\nautomatically in the unified model to initiate this transition by generating a\ntransition sentence under the guidance of transition information extracted by\nTIE. The experimental results show promising performance regarding the\nproactive transitions. We achieve an additional large improvement on TIE model\nby utilizing Conditional Random Fields (CRF). The TSG can flexibly generate\ntransition sentences while maintaining the unified capabilities of normal\nchit-chat and task-oriented response generation.", "published": "2023-08-06 12:25:22", "link": "http://arxiv.org/abs/2308.03098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PromptSum: Parameter-Efficient Controllable Abstractive Summarization", "abstract": "Prompt tuning (PT), a parameter-efficient technique that only tunes the\nadditional prompt embeddings while keeping the backbone pre-trained language\nmodel (PLM) frozen, has shown promising results in language understanding\ntasks, especially in low-resource scenarios. However, effective prompt design\nmethods suitable for generation tasks such as summarization are still lacking.\nAt the same time, summarization guided through instructions (discrete prompts)\ncan achieve a desirable double objective of high quality and controllability in\nsummary generation. Towards a goal of strong summarization performance under\nthe triple conditions of parameter-efficiency, data-efficiency, and\ncontrollability, we introduce PromptSum, a method combining PT with a\nmulti-task objective and discrete entity prompts for abstractive summarization.\nOur model achieves competitive ROUGE results on popular abstractive\nsummarization benchmarks coupled with a strong level of controllability through\nentities, all while only tuning several orders of magnitude less parameters.", "published": "2023-08-06 13:54:14", "link": "http://arxiv.org/abs/2308.03117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Kurosawa\": A Script Writer's Assistant", "abstract": "Storytelling is the lifeline of the entertainment industry -- movies, TV\nshows, and stand-up comedies, all need stories. A good and gripping script is\nthe lifeline of storytelling and demands creativity and resource investment.\nGood scriptwriters are rare to find and often work under severe time pressure.\nConsequently, entertainment media are actively looking for automation. In this\npaper, we present an AI-based script-writing workbench called KUROSAWA which\naddresses the tasks of plot generation and script generation. Plot generation\naims to generate a coherent and creative plot (600-800 words) given a prompt\n(15-40 words). Script generation, on the other hand, generates a scene (200-500\nwords) in a screenplay format from a brief description (15-40 words). Kurosawa\nneeds data to train. We use a 4-act structure of storytelling to annotate the\nplot dataset manually. We create a dataset of 1000 manually annotated plots and\ntheir corresponding prompts/storylines and a gold-standard dataset of 1000\nscenes with four main elements -- scene headings, action lines, dialogues, and\ncharacter names -- tagged individually. We fine-tune GPT-3 with the above\ndatasets to generate plots and scenes. These plots and scenes are first\nevaluated and then used by the scriptwriters of a large and famous media\nplatform ErosNow. We release the annotated datasets and the models trained on\nthese datasets as a working benchmark for automatic movie plot and script\ngeneration.", "published": "2023-08-06 14:09:02", "link": "http://arxiv.org/abs/2308.03122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Multiple References Era -- Addressing Data Leakage and Limited\n  Reference Diversity in NLG Evaluation", "abstract": "N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely\nutilized across a range of natural language generation (NLG) tasks. However,\nrecent studies have revealed a weak correlation between these matching-based\nmetrics and human evaluations, especially when compared with neural-based\nmetrics like BLEURT. In this paper, we conjecture that the performance\nbottleneck in matching-based metrics may be caused by the limited diversity of\nreferences. To address this issue, we propose to utilize \\textit{multiple\nreferences} to enhance the consistency between these metrics and human\nevaluations. Within the WMT Metrics benchmarks, we observe that the\nmulti-references F200spBLEU surpasses the conventional single-reference one by\nan accuracy improvement of 7.2\\%. Remarkably, it also exceeds the neural-based\nBERTscore by an accuracy enhancement of 3.9\\%. Moreover, we observe that the\ndata leakage issue in large language models (LLMs) can be mitigated to a large\nextent by our multi-reference metric. We release the code and data at\n\\url{https://github.com/SefaZeng/LLM-Ref}", "published": "2023-08-06 14:49:26", "link": "http://arxiv.org/abs/2308.03131v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten\n  Arabic Varieties", "abstract": "Despite the purported multilingual proficiency of instruction-finetuned large\nlanguage models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of\nthese models remains insufficiently explored. Considering this constraint, we\npresent a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5\nand GPT-4) regarding their machine translation proficiencies across ten\nvarieties of Arabic. Our evaluation covers diverse Arabic varieties such as\nClassical Arabic (CA), Modern Standard Arabic (MSA), and several country-level\ndialectal variants. Our analysis indicates that LLMs may encounter challenges\nwith dialects for which minimal public datasets exist, but on average are\nbetter translators of dialects than existing commercial systems. On CA and MSA,\ninstruction-tuned LLMs, however, trail behind commercial systems such as Google\nTranslate. Finally, we undertake a human-centric study to scrutinize the\nefficacy of the relatively recent model, Bard, in following human instructions\nduring translation tasks. Our analysis reveals a circumscribed capability of\nBard in aligning with human instructions in translation contexts. Collectively,\nour findings underscore that prevailing LLMs remain far from inclusive, with\nonly limited ability to cater for the linguistic and cultural intricacies of\ndiverse communities.", "published": "2023-08-06 08:29:16", "link": "http://arxiv.org/abs/2308.03051v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LARCH: Large Language Model-based Automatic Readme Creation with\n  Heuristics", "abstract": "Writing a readme is a crucial aspect of software development as it plays a\nvital role in managing and reusing program code. Though it is a pain point for\nmany developers, automatically creating one remains a challenge even with the\nrecent advancements in large language models (LLMs), because it requires\ngenerating an abstract description from thousands of lines of code. In this\ndemo paper, we show that LLMs are capable of generating a coherent and\nfactually correct readmes if we can identify a code fragment that is\nrepresentative of the repository. Building upon this finding, we developed\nLARCH (LLM-based Automatic Readme Creation with Heuristics) which leverages\nrepresentative code identification with heuristics and weak supervision.\nThrough human and automated evaluations, we illustrate that LARCH can generate\ncoherent and factually correct readmes in the majority of cases, outperforming\na baseline that does not rely on representative code identification. We have\nmade LARCH open-source and provided a cross-platform Visual Studio Code\ninterface and command-line interface, accessible at\nhttps://github.com/hitachi-nlp/larch. A demo video showcasing LARCH's\ncapabilities is available at https://youtu.be/ZUKkh5ED-O4.", "published": "2023-08-06 12:28:24", "link": "http://arxiv.org/abs/2308.03099v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Improving Domain-Specific Retrieval by NLI Fine-Tuning", "abstract": "The aim of this article is to investigate the fine-tuning potential of\nnatural language inference (NLI) data to improve information retrieval and\nranking. We demonstrate this for both English and Polish languages, using data\nfrom one of the largest Polish e-commerce sites and selected open-domain\ndatasets. We employ both monolingual and multilingual sentence encoders\nfine-tuned by a supervised method utilizing contrastive loss and NLI data. Our\nresults point to the fact that NLI fine-tuning increases the performance of the\nmodels in both tasks and both languages, with the potential to improve mono-\nand multilingual models. Finally, we investigate uniformity and alignment of\nthe embeddings to explain the effect of NLI-based fine-tuning for an\nout-of-domain use-case.", "published": "2023-08-06 12:40:58", "link": "http://arxiv.org/abs/2308.03103v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating\n  Vision-Language Models", "abstract": "Vision-language models (VLMs) have shown impressive performance in\nsubstantial downstream multi-modal tasks. However, only comparing the\nfine-tuned performance on downstream tasks leads to the poor interpretability\nof VLMs, which is adverse to their future improvement. Several prior works have\nidentified this issue and used various probing methods under a zero-shot\nsetting to detect VLMs' limitations, but they all examine VLMs using general\ndatasets instead of specialized ones. In practical applications, VLMs are\nusually applied to specific scenarios, such as e-commerce and news fields, so\nthe generalization of VLMs in specific domains should be given more attention.\nIn this paper, we comprehensively investigate the capabilities of popular VLMs\nin a specific field, the food domain. To this end, we build a food caption\ndataset, Food-500 Cap, which contains 24,700 food images with 494 categories.\nEach image is accompanied by a detailed caption, including fine-grained\nattributes of food, such as the ingredient, shape, and color. We also provide a\nculinary culture taxonomy that classifies each food category based on its\ngeographic origin in order to better analyze the performance differences of VLM\nin different regions. Experiments on our proposed datasets demonstrate that\npopular VLMs underperform in the food domain compared with their performance in\nthe general domain. Furthermore, our research reveals severe bias in VLMs'\nability to handle food items from different geographic regions. We adopt\ndiverse probing methods and evaluate nine VLMs belonging to different\narchitectures to verify the aforementioned observations. We hope that our study\nwill bring researchers' attention to VLM's limitations when applying them to\nthe domain of food or culinary cultures, and spur further investigations to\naddress this issue.", "published": "2023-08-06 15:56:31", "link": "http://arxiv.org/abs/2308.03151v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Spanish Pre-trained BERT Model and Evaluation Data", "abstract": "The Spanish language is one of the top 5 spoken languages in the world.\nNevertheless, finding resources to train or evaluate Spanish language models is\nnot an easy task. In this paper we help bridge this gap by presenting a\nBERT-based language model pre-trained exclusively on Spanish data. As a second\ncontribution, we also compiled several tasks specifically for the Spanish\nlanguage in a single repository much in the spirit of the GLUE benchmark. By\nfine-tuning our pre-trained Spanish model, we obtain better results compared to\nother BERT-based models pre-trained on multilingual corpora for most of the\ntasks, even achieving a new state-of-the-art on some of them. We have publicly\nreleased our model, the pre-training data, and the compilation of the Spanish\nbenchmarks.", "published": "2023-08-06 00:16:04", "link": "http://arxiv.org/abs/2308.02976v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Show Me the World in My Language: Establishing the First Baseline for\n  Scene-Text to Scene-Text Translation", "abstract": "In this work, we study the task of ``visually'' translating scene text from a\nsource language (e.g., Hindi) to a target language (e.g., English). Visual\ntranslation involves not just the recognition and translation of scene text but\nalso the generation of the translated image that preserves visual features of\nthe source scene text, such as font, size, and background. There are several\nchallenges associated with this task, such as translation with limited context,\ndeciding between translation and transliteration, accommodating varying text\nlengths within fixed spatial boundaries, and preserving the font and background\nstyles of the source scene text in the target language. To address this\nproblem, we make the following contributions: (i) We study visual translation\nas a standalone problem for the first time in the literature. (ii) We present a\ncascaded framework for visual translation that combines state-of-the-art\nmodules for scene text recognition, machine translation, and scene text\nsynthesis as a baseline for the task. (iii) We propose a set of task-specific\ndesign enhancements to design a variant of the baseline to obtain performance\nimprovements. (iv) Currently, the existing related literature lacks any\ncomprehensive performance evaluation for this novel task. To fill this gap, we\nintroduce several automatic and user-assisted evaluation metrics designed\nexplicitly for evaluating visual translation. Further, we evaluate presented\nbaselines for translating scene text between Hindi and English. Our experiments\ndemonstrate that although we can effectively perform visual translation over a\nlarge collection of scene text images, the presented baseline only partially\naddresses challenges posed by visual translation tasks. We firmly believe that\nthis new task and the limitations of existing models, as reported in this\npaper, should encourage further research in visual translation.", "published": "2023-08-06 05:23:25", "link": "http://arxiv.org/abs/2308.03024v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Automatically Correcting Large Language Models: Surveying the landscape\n  of diverse self-correction strategies", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across\na wide array of NLP tasks. However, their efficacy is undermined by undesired\nand inconsistent behaviors, including hallucination, unfaithful reasoning, and\ntoxic content. A promising approach to rectify these flaws is self-correction,\nwhere the LLM itself is prompted or guided to fix problems in its own output.\nTechniques leveraging automated feedback -- either produced by the LLM itself\nor some external system -- are of particular interest as they are a promising\nway to make LLM-based solutions more practical and deployable with minimal\nhuman feedback. This paper presents a comprehensive review of this emerging\nclass of techniques. We analyze and taxonomize a wide array of recent work\nutilizing these strategies, including training-time, generation-time, and\npost-hoc correction. We also summarize the major applications of this strategy\nand conclude by discussing future directions and challenges.", "published": "2023-08-06 18:38:52", "link": "http://arxiv.org/abs/2308.03188v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Average-Hard Attention Transformers are Constant-Depth Uniform Threshold\n  Circuits", "abstract": "Transformers have emerged as a widely used neural network model for various\nnatural language processing tasks. Previous research explored their\nrelationship with constant-depth threshold circuits, making two assumptions:\naverage-hard attention and logarithmic precision for internal computations\nrelative to input length. Merrill et al. (2022) prove that average-hard\nattention transformers recognize languages that fall within the complexity\nclass TC0, denoting the set of languages that can be recognized by\nconstant-depth polynomial-size threshold circuits. Likewise, Merrill and\nSabharwal (2023) show that log-precision transformers recognize languages\nwithin the class of uniform TC0. This shows that both transformer models can be\nsimulated by constant-depth threshold circuits, with the latter being more\nrobust due to generating a uniform circuit family. Our paper shows that the\nfirst result can be extended to yield uniform circuits as well.", "published": "2023-08-06 21:23:22", "link": "http://arxiv.org/abs/2308.03212v2", "categories": ["cs.CL", "cs.CC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi\n  (2023)", "abstract": "We present a critical assessment of Piantadosi's (2023) claim that \"Modern\nlanguage models refute Chomsky's approach to language,\" focusing on four main\npoints. First, despite the impressive performance and utility of large language\nmodels (LLMs), humans achieve their capacity for language after exposure to\nseveral orders of magnitude less data. The fact that young children become\ncompetent, fluent speakers of their native languages with relatively little\nexposure to them is the central mystery of language learning to which Chomsky\ninitially drew attention, and LLMs currently show little promise of solving\nthis mystery. Second, what can the artificial reveal about the natural? Put\nsimply, the implications of LLMs for our understanding of the cognitive\nstructures and mechanisms underlying language and its acquisition are like the\nimplications of airplanes for understanding how birds fly. Third, LLMs cannot\nconstitute scientific theories of language for several reasons, not least of\nwhich is that scientific theories must provide interpretable explanations, not\njust predictions. This leads to our final point: to even determine whether the\nlinguistic and cognitive capabilities of LLMs rival those of humans requires\nexplicating what humans' capacities actually are. In other words, it requires a\nseparate theory of language and cognition; generative linguistics provides\nprecisely such a theory. As such, we conclude that generative linguistics as a\nscientific discipline will remain indispensable throughout the 21st century and\nbeyond.", "published": "2023-08-06 23:41:14", "link": "http://arxiv.org/abs/2308.03228v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigation of Self-supervised Pre-trained Models for Classification\n  of Voice Quality from Speech and Neck Surface Accelerometer Signals", "abstract": "Prior studies in the automatic classification of voice quality have mainly\nstudied the use of the acoustic speech signal as input. Recently, a few studies\nhave been carried out by jointly using both speech and neck surface\naccelerometer (NSA) signals as inputs, and by extracting MFCCs and glottal\nsource features. This study examines simultaneously-recorded speech and NSA\nsignals in the classification of voice quality (breathy, modal, and pressed)\nusing features derived from three self-supervised pre-trained models\n(wav2vec2-BASE, wav2vec2-LARGE, and HuBERT) and using a SVM as well as CNNs as\nclassifiers. Furthermore, the effectiveness of the pre-trained models is\ncompared in feature extraction between glottal source waveforms and raw signal\nwaveforms for both speech and NSA inputs. Using two signal processing methods\n(quasi-closed phase (QCP) glottal inverse filtering and zero frequency\nfiltering (ZFF)), glottal source waveforms are estimated from both speech and\nNSA signals. The study has three main goals: (1) to study whether features\nderived from pre-trained models improve classification accuracy compared to\nconventional features (spectrogram, mel-spectrogram, MFCCs, i-vector, and\nx-vector), (2) to investigate which of the two modalities (speech vs. NSA) is\nmore effective in the classification task with pre-trained model-based\nfeatures, and (3) to evaluate whether the deep learning-based CNN classifier\ncan enhance the classification accuracy in comparison to the SVM classifier.\nThe results revealed that the use of the NSA input showed better classification\nperformance compared to the speech signal. Between the features, the\npre-trained model-based features showed better classification accuracies, both\nfor speech and NSA inputs compared to the conventional features. It was also\nfound that the HuBERT features performed better than the wav2vec2-BASE and\nwav2vec2-LARGE features.", "published": "2023-08-06 23:16:54", "link": "http://arxiv.org/abs/2308.03226v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Characterization of cough sounds using statistical analysis", "abstract": "Cough is a primary symptom of most respiratory diseases, and changes in cough\ncharacteristics provide valuable information for diagnosing respiratory\ndiseases. The characterization of cough sounds still lacks concrete evidence,\nwhich makes it difficult to accurately distinguish between different types of\ncoughs and other sounds. The objective of this research work is to characterize\ncough sounds with voiced content and cough sounds without voiced content.\nFurther, the cough sound characteristics are compared with the characteristics\nof speech. The proposed method to achieve this goal utilized spectral roll-off,\nspectral entropy, spectral flatness, spectral flux, zero crossing rate,\nspectral centroid, and spectral bandwidth attributes which describe the cough\nsounds related to the respiratory system, glottal information, and voice model.\nThese attributes are then subjected to statistical analysis using the measures\nof minimum, maximum, mean, median, and standard deviation. The experimental\nresults show that the mean and frequency distribution of spectral roll-off,\nspectral centroid, and spectral bandwidth are found to be higher for cough\nsounds than for speech signals. Spectral flatness levels in cough sounds will\nrise to 0.22, whereas spectral flux varies between 0.3 and 0.6. The Zero\nCrossing Rate (ZCR) of most frames of cough sounds is between 0.05 and 0.4.\nThese attributes contribute significant information while characterizing cough\nsounds.", "published": "2023-08-06 04:26:52", "link": "http://arxiv.org/abs/2308.03019v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "SoK: Acoustic Side Channels", "abstract": "We provide a state-of-the-art analysis of acoustic side channels, cover all\nthe significant academic research in the area, discuss their security\nimplications and countermeasures, and identify areas for future research. We\nalso make an attempt to bridge side channels and inverse problems, two fields\nthat appear to be completely isolated from each other but have deep\nconnections.", "published": "2023-08-06 14:36:33", "link": "http://arxiv.org/abs/2308.03806v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
