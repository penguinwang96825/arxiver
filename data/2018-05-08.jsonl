{"title": "Hierarchical Structured Model for Fine-to-coarse Manifesto Text Analysis", "abstract": "Election manifestos document the intentions, motives, and views of political\nparties. They are often used for analysing a party's fine-grained position on a\nparticular issue, as well as for coarse-grained positioning of a party on the\nleft--right spectrum. In this paper we propose a two-stage model for\nautomatically performing both levels of analysis over manifestos. In the first\nstep we employ a hierarchical multi-task structured deep model to predict fine-\nand coarse-grained positions, and in the second step we perform post-hoc\ncalibration of coarse-grained positions using probabilistic soft logic. We\nempirically show that the proposed model outperforms state-of-art approaches at\nboth granularities using manifestos from twelve countries, written in ten\ndifferent languages.", "published": "2018-05-08 04:05:54", "link": "http://arxiv.org/abs/1805.02823v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One \"Ruler\" for All Languages: Multi-Lingual Dialogue Evaluation with\n  Adversarial Multi-Task Learning", "abstract": "Automatic evaluating the performance of Open-domain dialogue system is a\nchallenging problem. Recent work in neural network-based metrics has shown\npromising opportunities for automatic dialogue evaluation. However, existing\nmethods mainly focus on monolingual evaluation, in which the trained metric is\nnot flexible enough to transfer across different languages. To address this\nissue, we propose an adversarial multi-task neural metric (ADVMT) for\nmulti-lingual dialogue evaluation, with shared feature extraction across\nlanguages. We evaluate the proposed model in two different languages.\nExperiments show that the adversarial multi-task neural metric achieves a high\ncorrelation with human annotation, which yields better performance than\nmonolingual ones and various existing metrics.", "published": "2018-05-08 09:24:32", "link": "http://arxiv.org/abs/1805.02914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Character-level Japanese-Chinese Neural Machine Translation\n  with Radicals as an Additional Input Feature", "abstract": "In recent years, Neural Machine Translation (NMT) has been proven to get\nimpressive results. While some additional linguistic features of input words\nimprove word-level NMT, any additional character features have not been used to\nimprove character-level NMT so far. In this paper, we show that the radicals of\nChinese characters (or kanji), as a character feature information, can be\neasily provide further improvements in the character-level NMT. In experiments\non WAT2016 Japanese-Chinese scientific paper excerpt corpus (ASPEC-JP), we find\nthat the proposed method improves the translation quality according to two\naspects: perplexity and BLEU. The character-level NMT with the radical input\nfeature's model got a state-of-the-art result of 40.61 BLEU points in the test\nset, which is an improvement of about 8.6 BLEU points over the best system on\nthe WAT2016 Japanese-to-Chinese translation subtask with ASPEC-JP. The\nimprovements over the character-level NMT with no additional input feature are\nup to about 1.5 and 1.4 BLEU points in the development-test set and the test\nset of the corpus, respectively.", "published": "2018-05-08 10:39:45", "link": "http://arxiv.org/abs/1805.02937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bleaching Text: Abstract Features for Cross-lingual Gender Prediction", "abstract": "Gender prediction has typically focused on lexical and social network\nfeatures, yielding good performance, but making systems highly language-,\ntopic-, and platform-dependent. Cross-lingual embeddings circumvent some of\nthese limitations, but capture gender-specific style less. We propose an\nalternative: bleaching text, i.e., transforming lexical strings into more\nabstract features. This study provides evidence that such features allow for\nbetter transfer across languages. Moreover, we present a first study on the\nability of humans to perform cross-lingual gender prediction. We find that\nhuman predictive power proves similar to that of our bleached models, and both\nperform better than lexical models.", "published": "2018-05-08 15:50:32", "link": "http://arxiv.org/abs/1805.03122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Post-Specialisation: Retrofitting Vectors of Words Unseen in Lexical\n  Resources", "abstract": "Word vector specialisation (also known as retrofitting) is a portable,\nlight-weight approach to fine-tuning arbitrary distributional word vector\nspaces by injecting external knowledge from rich lexical resources such as\nWordNet. By design, these post-processing methods only update the vectors of\nwords occurring in external lexicons, leaving the representations of all unseen\nwords intact. In this paper, we show that constraint-driven vector space\nspecialisation can be extended to unseen words. We propose a novel\npost-specialisation method that: a) preserves the useful linguistic knowledge\nfor seen words; while b) propagating this external signal to unseen words in\norder to improve their vector representations as well. Our post-specialisation\napproach explicits a non-linear specialisation function in the form of a deep\nneural network by learning to predict specialised vectors from their original\ndistributional counterparts. The learned function is then used to specialise\nvectors of unseen words. This approach, applicable to any post-processing\nmodel, yields considerable gains over the initial specialisation models both in\nintrinsic word similarity tasks, and in two downstream tasks: dialogue state\ntracking and lexical text simplification. The positive effects persist across\nthree languages, demonstrating the importance of specialising the full\nvocabulary of distributional word vector spaces.", "published": "2018-05-08 18:46:24", "link": "http://arxiv.org/abs/1805.03228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented\n  Visual Dialog", "abstract": "Creating an intelligent conversational system that understands vision and\nlanguage is one of the ultimate goals in Artificial Intelligence\n(AI)~\\cite{winograd1972understanding}. Extensive research has focused on\nvision-to-language generation, however, limited research has touched on\ncombining these two modalities in a goal-driven dialog context. We propose a\nmultimodal hierarchical reinforcement learning framework that dynamically\nintegrates vision and language for task-oriented visual dialog. The framework\njointly learns the multimodal dialog state representation and the hierarchical\ndialog policy to improve both dialog task success and efficiency. We also\npropose a new technique, state adaptation, to integrate context awareness in\nthe dialog state representation. We evaluate the proposed framework and the\nstate adaptation technique in an image guessing game and achieve promising\nresults.", "published": "2018-05-08 19:54:47", "link": "http://arxiv.org/abs/1805.03257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investor Reaction to Financial Disclosures Across Topics: An Application\n  of Latent Dirichlet Allocation", "abstract": "This paper provides a holistic study of how stock prices vary in their\nresponse to financial disclosures across different topics. Thereby, we\nspecifically shed light into the extensive amount of filings for which no a\npriori categorization of their content exists. For this purpose, we utilize an\napproach from data mining - namely, latent Dirichlet allocation - as a means of\ntopic modeling. This technique facilitates our task of automatically\ncategorizing, ex ante, the content of more than 70,000 regulatory 8-K filings\nfrom U.S. companies. We then evaluate the subsequent stock market reaction. Our\nempirical evidence suggests a considerable discrepancy among various types of\nnews stories in terms of their relevance and impact on financial markets. For\ninstance, we find a statistically significant abnormal return in response to\nearnings results and credit rating, but also for disclosures regarding business\nstrategy, the health sector, as well as mergers and acquisitions. Our results\nyield findings that benefit managers, investors and policy-makers by indicating\nhow regulatory filings should be structured and the topics most likely to\nprecede changes in stock valuations.", "published": "2018-05-08 22:22:26", "link": "http://arxiv.org/abs/1805.03308v1", "categories": ["cs.CL", "q-fin.GN"], "primary_category": "cs.CL"}
{"title": "Reasoning with Sarcasm by Reading In-between", "abstract": "Sarcasm is a sophisticated speech act which commonly manifests on social\ncommunities such as Twitter and Reddit. The prevalence of sarcasm on the social\nweb is highly disruptive to opinion mining systems due to not only its tendency\nof polarity flipping but also usage of figurative language. Sarcasm commonly\nmanifests with a contrastive theme either between positive-negative sentiments\nor between literal-figurative scenarios. In this paper, we revisit the notion\nof modeling contrast in order to reason with sarcasm. More specifically, we\npropose an attention-based neural model that looks in-between instead of\nacross, enabling it to explicitly model contrast and incongruity. We conduct\nextensive experiments on six benchmark datasets from Twitter, Reddit and the\nInternet Argument Corpus. Our proposed model not only achieves state-of-the-art\nperformance on all datasets but also enjoys improved interpretability.", "published": "2018-05-08 06:46:03", "link": "http://arxiv.org/abs/1805.02856v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Online normalizer calculation for softmax", "abstract": "The Softmax function is ubiquitous in machine learning, multiple previous\nworks suggested faster alternatives for it. In this paper we propose a way to\ncompute classical Softmax with fewer memory accesses and hypothesize that this\nreduction in memory accesses should improve Softmax performance on actual\nhardware. The benchmarks confirm this hypothesis: Softmax accelerates by up to\n1.3x and Softmax+TopK combined and fused by up to 5x.", "published": "2018-05-08 07:34:17", "link": "http://arxiv.org/abs/1805.02867v2", "categories": ["cs.PF", "cs.AI", "cs.CL"], "primary_category": "cs.PF"}
{"title": "Interpretable Adversarial Perturbation in Input Embedding Space for Text", "abstract": "Following great success in the image processing field, the idea of\nadversarial training has been applied to tasks in the natural language\nprocessing (NLP) field. One promising approach directly applies adversarial\ntraining developed in the image processing field to the input word embedding\nspace instead of the discrete input space of texts. However, this approach\nabandons such interpretability as generating adversarial texts to significantly\nimprove the performance of NLP tasks. This paper restores interpretability to\nsuch methods by restricting the directions of perturbations toward the existing\nwords in the input embedding space. As a result, we can straightforwardly\nreconstruct each input with perturbations to an actual text by considering the\nperturbations to be the replacement of words in the sentence while maintaining\nor even improving the task performance.", "published": "2018-05-08 09:27:46", "link": "http://arxiv.org/abs/1805.02917v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Regression Model of Recurrent Deep Neural Networks for Noise Robust\n  Estimation of the Fundamental Frequency Contour of Speech", "abstract": "The fundamental frequency (F0) contour of speech is a key aspect to represent\nspeech prosody that finds use in speech and spoken language analysis such as\nvoice conversion and speech synthesis as well as speaker and language\nidentification. This work proposes new methods to estimate the F0 contour of\nspeech using deep neural networks (DNNs) and recurrent neural networks (RNNs).\nThey are trained using supervised learning with the ground truth of F0\ncontours. The latest prior research addresses this problem first as a\nframe-by-frame-classification problem followed by sequence tracking using deep\nneural network hidden Markov model (DNN-HMM) hybrid architecture. This study,\nhowever, tackles the problem as a regression problem instead, in order to\nobtain F0 contours with higher frequency resolution from clean and noisy\nspeech. Experiments using PTDB-TUG corpus contaminated with additive noise\n(NOISEX-92) show the proposed method improves gross pitch error (GPE) by more\nthan 25 % at signal-to-noise ratios (SNRs) between -10 dB and +10 dB as\ncompared with one of the most noise-robust F0 trackers, PEFAC. Furthermore, the\nperformance on fine pitch error (FPE) is improved by approximately 20 % against\na state-of-the-art DNN-HMM-based approach.", "published": "2018-05-08 11:54:06", "link": "http://arxiv.org/abs/1805.02958v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Polite Dialogue Generation Without Parallel Data", "abstract": "Stylistic dialogue response generation, with valuable applications in\npersonality-based conversational agents, is a challenging task because the\nresponse needs to be fluent, contextually-relevant, as well as\nparalinguistically accurate. Moreover, parallel datasets for\nregular-to-stylistic pairs are usually unavailable. We present three\nweakly-supervised models that can generate diverse polite (or rude) dialogue\nresponses without parallel data. Our late fusion model (Fusion) merges the\ndecoder of an encoder-attention-decoder dialogue model with a language model\ntrained on stand-alone polite utterances. Our label-fine-tuning (LFT) model\nprepends to each source sequence a politeness-score scaled label (predicted by\nour state-of-the-art politeness classifier) during training, and at test time\nis able to generate polite, neutral, and rude responses by simply scaling the\nlabel embedding by the corresponding score. Our reinforcement learning model\n(Polite-RL) encourages politeness generation by assigning rewards proportional\nto the politeness classifier score of the sampled response. We also present two\nretrieval-based polite dialogue model baselines. Human evaluation validates\nthat while the Fusion and the retrieval-based models achieve politeness with\npoorer context-relevance, the LFT and Polite-RL models can produce\nsignificantly more polite responses without sacrificing dialogue quality.", "published": "2018-05-08 16:56:15", "link": "http://arxiv.org/abs/1805.03162v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved training of end-to-end attention models for speech recognition", "abstract": "Sequence-to-sequence attention-based models on subword units allow simple\nopen-vocabulary end-to-end speech recognition. In this work, we show that such\nmodels can achieve competitive results on the Switchboard 300h and LibriSpeech\n1000h tasks. In particular, we report the state-of-the-art word error rates\n(WER) of 3.54% on the dev-clean and 3.82% on the test-clean evaluation subsets\nof LibriSpeech. We introduce a new pretraining scheme by starting with a high\ntime reduction factor and lowering it during training, which is crucial both\nfor convergence and final performance. In some experiments, we also use an\nauxiliary CTC loss function to help the convergence. In addition, we train long\nshort-term memory (LSTM) language models on subword units. By shallow fusion,\nwe report up to 27% relative improvements in WER over the attention baseline\nwithout a language model.", "published": "2018-05-08 21:27:04", "link": "http://arxiv.org/abs/1805.03294v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Transfer Learning from Adult to Children for Speech Recognition:\n  Evaluation, Analysis and Recommendations", "abstract": "Children speech recognition is challenging mainly due to the inherent high\nvariability in children's physical and articulatory characteristics and\nexpressions. This variability manifests in both acoustic constructs and\nlinguistic usage due to the rapidly changing developmental stage in children's\nlife. Part of the challenge is due to the lack of large amounts of available\nchildren speech data for efficient modeling. This work attempts to address the\nkey challenges using transfer learning from adult's models to children's models\nin a Deep Neural Network (DNN) framework for children's Automatic Speech\nRecognition (ASR) task evaluating on multiple children's speech corpora with a\nlarge vocabulary. The paper presents a systematic and an extensive analysis of\nthe proposed transfer learning technique considering the key factors affecting\nchildren's speech recognition from prior literature. Evaluations are presented\non (i) comparisons of earlier GMM-HMM and the newer DNN Models, (ii)\neffectiveness of standard adaptation techniques versus transfer learning, (iii)\nvarious adaptation configurations in tackling the variabilities present in\nchildren speech, in terms of (a) acoustic spectral variability, and (b)\npronunciation variability and linguistic constraints. Our Analysis spans over\n(i) number of DNN model parameters (for adaptation), (ii) amount of adaptation\ndata, (iii) ages of children, (iv) age dependent-independent adaptation.\nFinally, we provide Recommendations on (i) the favorable strategies over\nvarious aforementioned - analyzed parameters, and (ii) potential future\nresearch directions and relevant challenges/problems persisting in DNN based\nASR for children's speech.", "published": "2018-05-08 23:59:04", "link": "http://arxiv.org/abs/1805.03322v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Statistical Analysis on E-Commerce Reviews, with Sentiment\n  Classification using Bidirectional Recurrent Neural Network (RNN)", "abstract": "Understanding customer sentiments is of paramount importance in marketing\nstrategies today. Not only will it give companies an insight as to how\ncustomers perceive their products and/or services, but it will also give them\nan idea on how to improve their offers. This paper attempts to understand the\ncorrelation of different variables in customer reviews on a women clothing\ne-commerce, and to classify each review whether it recommends the reviewed\nproduct or not and whether it consists of positive, negative, or neutral\nsentiment. To achieve these goals, we employed univariate and multivariate\nanalyses on dataset features except for review titles and review texts, and we\nimplemented a bidirectional recurrent neural network (RNN) with long-short term\nmemory unit (LSTM) for recommendation and sentiment classification. Results\nhave shown that a recommendation is a strong indicator of a positive sentiment\nscore, and vice-versa. On the other hand, ratings in product reviews are fuzzy\nindicators of sentiment scores. We also found out that the bidirectional LSTM\nwas able to reach an F1-score of 0.88 for recommendation classification, and\n0.93 for sentiment classification.", "published": "2018-05-08 11:58:20", "link": "http://arxiv.org/abs/1805.03687v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Comparing phonemes and visemes with DNN-based lipreading", "abstract": "There is debate if phoneme or viseme units are the most effective for a\nlipreading system. Some studies use phoneme units even though phonemes describe\nunique short sounds; other studies tried to improve lipreading accuracy by\nfocusing on visemes with varying results. We compare the performance of a\nlipreading system by modeling visual speech using either 13 viseme or 38\nphoneme units. We report the accuracy of our system at both word and unit\nlevels. The evaluation task is large vocabulary continuous speech using the\nTCD-TIMIT corpus. We complete our visual speech modeling via hybrid DNN-HMMs\nand our visual speech decoder is a Weighted Finite-State Transducer (WFST). We\nuse DCT and Eigenlips as a representation of mouth ROI image. The phoneme\nlipreading system word accuracy outperforms the viseme based system word\naccuracy. However, the phoneme system achieved lower accuracy at the unit level\nwhich shows the importance of the dictionary for decoding classification\noutputs into words.", "published": "2018-05-08 09:51:34", "link": "http://arxiv.org/abs/1805.02924v1", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Capsule Networks for Low Resource Spoken Language Understanding", "abstract": "Designing a spoken language understanding system for command-and-control\napplications can be challenging because of a wide variety of domains and users\nor because of a lack of training data. In this paper we discuss a system that\nlearns from scratch from user demonstrations. This method has the advantage\nthat the same system can be used for many domains and users without\nmodifications and that no training data is required prior to deployment. The\nuser is required to train the system, so for a user friendly experience it is\ncrucial to minimize the required amount of data. In this paper we investigate\nwhether a capsule network can make efficient use of the limited amount of\navailable training data. We compare the proposed model to an approach based on\nNon-negative Matrix Factorisation which is the state-of-the-art in this setting\nand another deep learning approach that was recently introduced for end-to-end\nspoken language understanding. We show that the proposed model outperforms the\nbaseline models for three command-and-control applications: controlling a small\nrobot, a vocally guided card game and a home automation task.", "published": "2018-05-08 09:45:56", "link": "http://arxiv.org/abs/1805.02922v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Phoneme-to-viseme mappings: the good, the bad, and the ugly", "abstract": "Visemes are the visual equivalent of phonemes. Although not precisely\ndefined, a working definition of a viseme is \"a set of phonemes which have\nidentical appearance on the lips\". Therefore a phoneme falls into one viseme\nclass but a viseme may represent many phonemes: a many to one mapping. This\nmapping introduces ambiguity between phonemes when using viseme classifiers.\nNot only is this ambiguity damaging to the performance of audio-visual\nclassifiers operating on real expressive speech, there is also considerable\nchoice between possible mappings. In this paper we explore the issue of this\nchoice of viseme-to-phoneme map. We show that there is definite difference in\nperformance between viseme-to-phoneme mappings and explore why some maps appear\nto work better than others. We also devise a new algorithm for constructing\nphoneme-to-viseme mappings from labeled speech data. These new visemes, `Bear'\nvisemes, are shown to perform better than previously known units.", "published": "2018-05-08 10:32:57", "link": "http://arxiv.org/abs/1805.02934v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Comparing heterogeneous visual gestures for measuring the diversity of\n  visual speech signals", "abstract": "Visual lip gestures observed whilst lipreading have a few working\ndefinitions, the most common two are; `the visual equivalent of a phoneme' and\n`phonemes which are indistinguishable on the lips'. To date there is no formal\ndefinition, in part because to date we have not established a two-way\nrelationship or mapping between visemes and phonemes. Some evidence suggests\nthat visual speech is highly dependent upon the speaker. So here, we use a\nphoneme-clustering method to form new phoneme-to-viseme maps for both\nindividual and multiple speakers. We test these phoneme to viseme maps to\nexamine how similarly speakers talk visually and we use signed rank tests to\nmeasure the distance between individuals. We conclude that broadly speaking,\nspeakers have the same repertoire of mouth gestures, where they differ is in\nthe use of the gestures.", "published": "2018-05-08 11:17:52", "link": "http://arxiv.org/abs/1805.02948v1", "categories": ["eess.IV", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "eess.IV"}
