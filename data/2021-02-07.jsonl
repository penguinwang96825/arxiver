{"title": "Representation Learning for Natural Language Processing", "abstract": "This book aims to review and present the recent advances of distributed\nrepresentation learning for NLP, including why representation learning can\nimprove NLP, how representation learning takes part in various important topics\nof NLP, and what challenges are still not well addressed by distributed\nrepresentation.", "published": "2021-02-07 07:37:07", "link": "http://arxiv.org/abs/2102.03732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memory Augmented Sequential Paragraph Retrieval for Multi-hop Question\n  Answering", "abstract": "Retrieving information from correlative paragraphs or documents to answer\nopen-domain multi-hop questions is very challenging. To deal with this\nchallenge, most of the existing works consider paragraphs as nodes in a graph\nand propose graph-based methods to retrieve them. However, in this paper, we\npoint out the intrinsic defect of such methods. Instead, we propose a new\narchitecture that models paragraphs as sequential data and considers multi-hop\ninformation retrieval as a kind of sequence labeling task. Specifically, we\ndesign a rewritable external memory to model the dependency among paragraphs.\nMoreover, a threshold gate mechanism is proposed to eliminate the distraction\nof noise paragraphs. We evaluate our method on both full wiki and distractor\nsubtask of HotpotQA, a public textual multi-hop QA dataset requiring multi-hop\ninformation retrieval. Experiments show that our method achieves significant\nimprovement over the published state-of-the-art method in retrieval and\ndownstream QA task performance.", "published": "2021-02-07 08:15:51", "link": "http://arxiv.org/abs/2102.03741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Sentence-embeddings by Manifold Approximation and\n  Projection", "abstract": "The concept of unsupervised universal sentence encoders has gained traction\nrecently, wherein pre-trained models generate effective task-agnostic\nfixed-dimensional representations for phrases, sentences and paragraphs. Such\nmethods are of varying complexity, from simple weighted-averages of word\nvectors to complex language-models based on bidirectional transformers. In this\nwork we propose a novel technique to generate sentence-embeddings in an\nunsupervised fashion by projecting the sentences onto a fixed-dimensional\nmanifold with the objective of preserving local neighbourhoods in the original\nspace. To delineate such neighbourhoods we experiment with several set-distance\nmetrics, including the recently proposed Word Mover's distance, while the\nfixed-dimensional projection is achieved by employing a scalable and efficient\nmanifold approximation method rooted in topological data analysis. We test our\napproach, which we term EMAP or Embeddings by Manifold Approximation and\nProjection, on six publicly available text-classification datasets of varying\nsize and complexity. Empirical results show that our method consistently\nperforms similar to or better than several alternative state-of-the-art\napproaches.", "published": "2021-02-07 13:27:58", "link": "http://arxiv.org/abs/2102.03795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spoiler Alert: Using Natural Language Processing to Detect Spoilers in\n  Book Reviews", "abstract": "This paper presents an NLP (Natural Language Processing) approach to\ndetecting spoilers in book reviews, using the University of California San\nDiego (UCSD) Goodreads Spoiler dataset. We explored the use of LSTM, BERT, and\nRoBERTa language models to perform spoiler detection at the sentence-level.\nThis was contrasted with a UCSD paper which performed the same task, but using\nhandcrafted features in its data preparation. Despite eschewing the use of\nhandcrafted features, our results from the LSTM model were able to slightly\nexceed the UCSD team's performance in spoiler detection.", "published": "2021-02-07 18:54:27", "link": "http://arxiv.org/abs/2102.03882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word frequency-rank relationship in tagged texts", "abstract": "We analyze the frequency-rank relationship in sub-vocabularies corresponding\nto three different grammatical classes (nouns, verbs, and others) in a\ncollection of literary works in English, whose words have been automatically\ntagged according to their grammatical role. Comparing with a null hypothesis\nwhich assumes that words belonging to each class are uniformly distributed\nacross the frequency-ranked vocabulary of the whole work, we disclose\nstatistically significant differences between the three classes. This results\npoint to the fact that frequency-rank relationships may reflect linguistic\nfeatures associated with grammatical function.", "published": "2021-02-07 15:17:51", "link": "http://arxiv.org/abs/2102.10992v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of\n  Pre-trained Language Models", "abstract": "Fine-tuning pre-trained language models (PLMs) has demonstrated its\neffectiveness on various downstream NLP tasks recently. However, in many\nlow-resource scenarios, the conventional fine-tuning strategies cannot\nsufficiently capture the important semantic features for downstream tasks. To\naddress this issue, we introduce a novel framework (named \"CSS-LM\") to improve\nthe fine-tuning phase of PLMs via contrastive semi-supervised learning.\nSpecifically, given a specific task, we retrieve positive and negative\ninstances from large-scale unlabeled corpora according to their domain-level\nand class-level semantic relatedness to the task. We then perform contrastive\nsemi-supervised learning on both the retrieved unlabeled and original labeled\ninstances to help PLMs capture crucial task-related semantic features. The\nexperimental results show that CSS-LM achieves better results than the\nconventional fine-tuning strategy on a series of downstream tasks with few-shot\nsettings, and outperforms the latest supervised contrastive fine-tuning\nstrategies. Our datasets and source code will be available to provide more\ndetails.", "published": "2021-02-07 09:27:26", "link": "http://arxiv.org/abs/2102.03752v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Note on Argumentative Topology: Circularity and Syllogisms as Unsolved\n  Problems", "abstract": "In the last couple of years there were a few attempts to apply topological\ndata analysis to text, and in particular to natural language inference. A\nrecent work by Tymochko et al. suggests the possibility of capturing `the\nnotion of logical shape in text,' using `topological delay embeddings,' a\ntechnique derived from dynamical systems, applied to word embeddings.\n  In this note we reconstruct their argument and show, using several old and\nnew examples, that the problem of connecting logic, topology and text is still\nvery much unsolved. We conclude that there is no clear answer to the question:\n``Can we find a circle in a circular argument?'' We point out some possible\navenues of exploration. The code used in our experiment is also shown.", "published": "2021-02-07 18:30:37", "link": "http://arxiv.org/abs/2102.03874v1", "categories": ["cs.AI", "cs.CL", "55N31", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating\n  Self-Attention", "abstract": "Transformers have emerged as a powerful tool for a broad range of natural\nlanguage processing tasks. A key component that drives the impressive\nperformance of Transformers is the self-attention mechanism that encodes the\ninfluence or dependence of other tokens on each specific token. While\nbeneficial, the quadratic complexity of self-attention on the input sequence\nlength has limited its application to longer sequences -- a topic being\nactively studied in the community. To address this limitation, we propose\nNystr\\\"{o}mformer -- a model that exhibits favorable scalability as a function\nof sequence length. Our idea is based on adapting the Nystr\\\"{o}m method to\napproximate standard self-attention with $O(n)$ complexity. The scalability of\nNystr\\\"{o}mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE\nbenchmark and IMDB reviews with standard sequence length, and find that our\nNystr\\\"{o}mformer performs comparably, or in a few cases, even slightly better,\nthan standard self-attention. On longer sequence tasks in the Long Range Arena\n(LRA) benchmark, Nystr\\\"{o}mformer performs favorably relative to other\nefficient self-attention methods. Our code is available at\nhttps://github.com/mlpen/Nystromformer.", "published": "2021-02-07 20:06:59", "link": "http://arxiv.org/abs/2102.03902v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An open access NLP dataset for Arabic dialects : Data collection,\n  labeling, and model construction", "abstract": "Natural Language Processing (NLP) is today a very active field of research\nand innovation. Many applications need however big sets of data for supervised\nlearning, suitably labelled for the training purpose. This includes\napplications for the Arabic language and its national dialects. However, such\nopen access labeled data sets in Arabic and its dialects are lacking in the\nData Science ecosystem and this lack can be a burden to innovation and research\nin this field. In this work, we present an open data set of social data content\nin several Arabic dialects. This data was collected from the Twitter social\nnetwork and consists on +50K twits in five (5) national dialects. Furthermore,\nthis data was labeled for several applications, namely dialect detection, topic\ndetection and sentiment analysis. We publish this data as an open access data\nto encourage innovation and encourage other works in the field of NLP for\nArabic dialects and social media. A selection of models were built using this\ndata set and are presented in this paper along with their performances.", "published": "2021-02-07 01:39:52", "link": "http://arxiv.org/abs/2102.11000v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Time-Domain Speech Extraction with Spatial Information and Multi Speaker\n  Conditioning Mechanism", "abstract": "In this paper, we present a novel multi-channel speech extraction system to\nsimultaneously extract multiple clean individual sources from a mixture in\nnoisy and reverberant environments. The proposed method is built on an improved\nmulti-channel time-domain speech separation network which employs speaker\nembeddings to identify and extract multiple targets without label permutation\nambiguity. To efficiently inform the speaker information to the extraction\nmodel, we propose a new speaker conditioning mechanism by designing an\nadditional speaker branch for receiving external speaker embeddings.\nExperiments on 2-channel WHAMR! data show that the proposed system improves by\n9% relative the source separation performance over a strong multi-channel\nbaseline, and it increases the speech recognition accuracy by more than 16%\nrelative over the same baseline.", "published": "2021-02-07 10:11:49", "link": "http://arxiv.org/abs/2102.03762v1", "categories": ["eess.AS", "cs.CL", "cs.SD", "68T10"], "primary_category": "eess.AS"}
{"title": "\"Short is the Road that Leads from Fear to Hate\": Fear Speech in Indian\n  WhatsApp Groups", "abstract": "WhatsApp is the most popular messaging app in the world. Due to its\npopularity, WhatsApp has become a powerful and cheap tool for political\ncampaigning being widely used during the 2019 Indian general election, where it\nwas used to connect to the voters on a large scale. Along with the campaigning,\nthere have been reports that WhatsApp has also become a breeding ground for\nharmful speech against various protected groups and religious minorities. Many\nsuch messages attempt to instil fear among the population about a specific\n(minority) community. According to research on inter-group conflict, such `fear\nspeech' messages could have a lasting impact and might lead to real offline\nviolence. In this paper, we perform the first large scale study on fear speech\nacross thousands of public WhatsApp groups discussing politics in India. We\ncurate a new dataset and try to characterize fear speech from this dataset. We\nobserve that users writing fear speech messages use various events and symbols\nto create the illusion of fear among the reader about a target community. We\nbuild models to classify fear speech and observe that current state-of-the-art\nNLP models do not perform well at this task. Fear speech messages tend to\nspread faster and could potentially go undetected by classifiers built to\ndetect traditional toxic speech due to their low toxic nature. Finally, using a\nnovel methodology to target users with Facebook ads, we conduct a survey among\nthe users of these WhatsApp groups to understand the types of users who consume\nand share fear speech. We believe that this work opens up new research\nquestions that are very different from tackling hate speech which the research\ncommunity has been traditionally involved in.", "published": "2021-02-07 18:14:16", "link": "http://arxiv.org/abs/2102.03870v1", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "EMA2S: An End-to-End Multimodal Articulatory-to-Speech System", "abstract": "Synthesized speech from articulatory movements can have real-world use for\npatients with vocal cord disorders, situations requiring silent speech, or in\nhigh-noise environments. In this work, we present EMA2S, an end-to-end\nmultimodal articulatory-to-speech system that directly converts articulatory\nmovements to speech signals. We use a neural-network-based vocoder combined\nwith multimodal joint-training, incorporating spectrogram, mel-spectrogram, and\ndeep features. The experimental results confirm that the multimodal approach of\nEMA2S outperforms the baseline system in terms of both objective evaluation and\nsubjective evaluation metrics. Moreover, results demonstrate that joint\nmel-spectrogram and deep feature loss training can effectively improve system\nperformance.", "published": "2021-02-07 12:14:14", "link": "http://arxiv.org/abs/2102.03786v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "U-vectors: Generating clusterable speaker embedding from unlabeled data", "abstract": "Speaker recognition deals with recognizing speakers by their speech. Most\nspeaker recognition systems are built upon two stages, the first stage extracts\nlow dimensional correlation embeddings from speech, and the second performs the\nclassification task. The robustness of a speaker recognition system mainly\ndepends on the extraction process of speech embeddings, which are primarily\npre-trained on a large-scale dataset. As the embedding systems are pre-trained,\nthe performance of speaker recognition models greatly depends on domain\nadaptation policy, which may reduce if trained using inadequate data. This\npaper introduces a speaker recognition strategy dealing with unlabeled data,\nwhich generates clusterable embedding vectors from small fixed-size speech\nframes. The unsupervised training strategy involves an assumption that a small\nspeech segment should include a single speaker. Depending on such a belief, a\npairwise constraint is constructed with noise augmentation policies, used to\ntrain AutoEmbedder architecture that generates speaker embeddings. Without\nrelying on domain adaption policy, the process unsupervisely produces\nclusterable speaker embeddings, termed unsupervised vectors (u-vectors). The\nevaluation is concluded in two popular speaker recognition datasets for English\nlanguage, TIMIT, and LibriSpeech. Also, a Bengali dataset is included to\nillustrate the diversity of the domain shifts for speaker recognition systems.\nFinally, we conclude that the proposed approach achieves satisfactory\nperformance using pairwise architectures.", "published": "2021-02-07 18:00:09", "link": "http://arxiv.org/abs/2102.03868v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
