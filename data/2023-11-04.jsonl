{"title": "Narrowing the Gap between Zero- and Few-shot Machine Translation by\n  Matching Styles", "abstract": "Large language models trained primarily in a monolingual setting have\ndemonstrated their ability to generalize to machine translation using zero- and\nfew-shot examples with in-context learning. However, even though zero-shot\ntranslations are relatively good, there remains a discernible gap comparing\ntheir performance with the few-shot setting. In this paper, we investigate the\nfactors contributing to this gap and find that this gap can largely be closed\n(for about 70%) by matching the writing styles of the target corpus.\nAdditionally, we explore potential approaches to enhance zero-shot baselines\nwithout the need for parallel demonstration examples, providing valuable\ninsights into how these methods contribute to improving translation metrics.", "published": "2023-11-04 03:18:45", "link": "http://arxiv.org/abs/2311.02310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Context-Dependent Translations for Evaluation Set Production", "abstract": "A major impediment to the transition to context-aware machine translation is\nthe absence of good evaluation metrics and test sets. Sentences that require\ncontext to be translated correctly are rare in test sets, reducing the utility\nof standard corpus-level metrics such as COMET or BLEU. On the other hand,\ndatasets that annotate such sentences are also rare, small in scale, and\navailable for only a few languages. To address this, we modernize, generalize,\nand extend previous annotation pipelines to produce CTXPRO, a tool that\nidentifies subsets of parallel documents containing sentences that require\ncontext to correctly translate five phenomena: gender, formality, and animacy\nfor pronouns, verb phrase ellipsis, and ambiguous noun inflections. The input\nto the pipeline is a set of hand-crafted, per-language, linguistically-informed\nrules that select contextual sentence pairs using coreference, part-of-speech,\nand morphological features provided by state-of-the-art tools. We apply this\npipeline to seven languages pairs (EN into and out-of DE, ES, FR, IT, PL, PT,\nand RU) and two datasets (OpenSubtitles and WMT test sets), and validate its\nperformance using both overlap with previous work and its ability to\ndiscriminate a contextual MT system from a sentence-based one. We release the\nCTXPRO pipeline and data as open source.", "published": "2023-11-04 04:29:08", "link": "http://arxiv.org/abs/2311.02321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing English Writing Proficiency in China's Polytechnic Students An\n  In-Depth Literature Review on the Application of the Input Hypothesis", "abstract": "Having good English writing skills is extremely important for students in\npolytechnic institutions. However, a lot of students in technical schools have\ndifficulties in reaching high levels of skill. The Input Hypothesis, created by\nStephen Krashen, suggests that people learn languages well when they receive\ninformation that's a little harder than what they already know but still\nunderstandable. This research paper wants to study how the Input Hypothesis can\nhelp polytechnic students improve their English writing skills. The study will\ninclude real-life observations and experiments from the previous research. We\nwill look at data from polytechnic students who are receiving special writing\ninstruction to see if the Input Hypothesis actually helps improve their writing\nskills. The paper can better inform polytechnic students, faculty members, and\nsupport staff and even members of the larger community about the attributions,\nthe processes, and the possible outcomes of second language development for\npolytechnic students.\n  Keywords: English writing skills, Polytechnic students, Input hypothesis,\nComprehensible input", "published": "2023-11-04 07:41:12", "link": "http://arxiv.org/abs/2311.02341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TreeSwap: Data Augmentation for Machine Translation via Dependency\n  Subtree Swapping", "abstract": "Data augmentation methods for neural machine translation are particularly\nuseful when limited amount of training data is available, which is often the\ncase when dealing with low-resource languages. We introduce a novel\naugmentation method, which generates new sentences by swapping objects and\nsubjects across bisentences. This is performed simultaneously based on the\ndependency parse trees of the source and target sentences. We name this method\nTreeSwap. Our results show that TreeSwap achieves consistent improvements over\nbaseline models in 4 language pairs in both directions on resource-constrained\ndatasets. We also explore domain-specific corpora, but find that our method\ndoes not make significant improvements on law, medical and IT data. We report\nthe scores of similar augmentation methods and find that TreeSwap performs\ncomparably. We also analyze the generated sentences qualitatively and find that\nthe augmentation produces a correct translation in most cases. Our code is\navailable on Github.", "published": "2023-11-04 09:27:40", "link": "http://arxiv.org/abs/2311.02355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Citance-Contextualized Summarization of Scientific Papers", "abstract": "Current approaches to automatic summarization of scientific papers generate\ninformative summaries in the form of abstracts. However, abstracts are not\nintended to show the relationship between a paper and the references cited in\nit. We propose a new contextualized summarization approach that can generate an\ninformative summary conditioned on a given sentence containing the citation of\na reference (a so-called \"citance\"). This summary outlines the content of the\ncited paper relevant to the citation location. Thus, our approach extracts and\nmodels the citances of a paper, retrieves relevant passages from cited papers,\nand generates abstractive summaries tailored to each citance. We evaluate our\napproach using $\\textbf{Webis-Context-SciSumm-2023}$, a new dataset containing\n540K~computer science papers and 4.6M~citances therein.", "published": "2023-11-04 14:08:15", "link": "http://arxiv.org/abs/2311.02408v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Chat GPT solve a Linguistics Exam?", "abstract": "The present study asks if ChatGPT4, the version of ChatGPT which uses the\nlanguage model GPT4, can successfully solve introductory linguistic exams.\nPrevious exam questions of an Introduction to Linguistics course at a German\nuniversity are used to test this. The exam questions were fed into ChatGPT4\nwith only minimal preprocessing. The results show that the language model is\nvery successful in the interpretation even of complex and nested tasks. It\nproved surprisingly successful in the task of broad phonetic transcription, but\nperformed less well in the analysis of morphemes and phrases. In simple cases\nit performs sufficiently well, but rarer cases, particularly with missing\none-to-one correspondence, are currently treated with mixed results. The model\nis not yet able to deal with visualisations, such as the analysis or generation\nof syntax trees. More extensive preprocessing, which translates these tasks\ninto text data, allow the model to also solve these tasks successfully.", "published": "2023-11-04 20:02:57", "link": "http://arxiv.org/abs/2311.02499v1", "categories": ["cs.CL", "J.5; K.3"], "primary_category": "cs.CL"}
{"title": "LLMs grasp morality in concept", "abstract": "Work in AI ethics and fairness has made much progress in regulating LLMs to\nreflect certain values, such as fairness, truth, and diversity. However, it has\ntaken the problem of how LLMs might 'mean' anything at all for granted. Without\naddressing this, it is not clear what imbuing LLMs with such values even means.\nIn response, we provide a general theory of meaning that extends beyond humans.\nWe use this theory to explicate the precise nature of LLMs as meaning-agents.\nWe suggest that the LLM, by virtue of its position as a meaning-agent, already\ngrasps the constructions of human society (e.g. morality, gender, and race) in\nconcept. Consequently, under certain ethical frameworks, currently popular\nmethods for model alignment are limited at best and counterproductive at worst.\nMoreover, unaligned models may help us better develop our moral and social\nphilosophy.", "published": "2023-11-04 01:37:41", "link": "http://arxiv.org/abs/2311.02294v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "You Only Forward Once: Prediction and Rationalization in A Single\n  Forward Pass", "abstract": "Unsupervised rationale extraction aims to extract concise and contiguous text\nsnippets to support model predictions without any annotated rationale. Previous\nstudies have used a two-phase framework known as the Rationalizing Neural\nPrediction (RNP) framework, which follows a generate-then-predict paradigm.\nThey assumed that the extracted explanation, called rationale, should be\nsufficient to predict the golden label. However, the assumption above deviates\nfrom the original definition and is too strict to perform well. Furthermore,\nthese two-phase models suffer from the interlocking problem and spurious\ncorrelations. To solve the above problems, we propose a novel single-phase\nframework called You Only Forward Once (YOFO), derived from a relaxed version\nof rationale where rationales aim to support model predictions rather than make\npredictions. In our framework, A pre-trained language model like BERT is\ndeployed to simultaneously perform prediction and rationalization with less\nimpact from interlocking or spurious correlations. Directly choosing the\nimportant tokens in an unsupervised manner is intractable. Instead of directly\nchoosing the important tokens, YOFO gradually removes unimportant tokens during\nforward propagation. Through experiments on the BeerAdvocate and Hotel Review\ndatasets, we demonstrate that our model is able to extract rationales and make\npredictions more accurately compared to RNP-based models. We observe an\nimprovement of up to 18.4\\% in token-level F1 compared to previous\nstate-of-the-art methods. We also conducted analyses and experiments to explore\nthe extracted rationales and token decay strategies. The results show that YOFO\ncan extract precise and important rationales while removing unimportant tokens\nin the middle part of the model.", "published": "2023-11-04 08:04:28", "link": "http://arxiv.org/abs/2311.02344v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Perturbation-based Active Learning for Question Answering", "abstract": "Building a question answering (QA) model with less annotation costs can be\nachieved by utilizing active learning (AL) training strategy. It selects the\nmost informative unlabeled training data to update the model effectively.\nAcquisition functions for AL are used to determine how informative each\ntraining example is, such as uncertainty or diversity based sampling. In this\nwork, we propose a perturbation-based active learning acquisition strategy and\ndemonstrate it is more effective than existing commonly used strategies.", "published": "2023-11-04 08:07:23", "link": "http://arxiv.org/abs/2311.02345v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OverHear: Headphone based Multi-sensor Keystroke Inference", "abstract": "Headphones, traditionally limited to audio playback, have evolved to\nintegrate sensors like high-definition microphones and accelerometers. While\nthese advancements enhance user experience, they also introduce potential\neavesdropping vulnerabilities, with keystroke inference being our concern in\nthis work. To validate this threat, we developed OverHear, a keystroke\ninference framework that leverages both acoustic and accelerometer data from\nheadphones. The accelerometer data, while not sufficiently detailed for\nindividual keystroke identification, aids in clustering key presses by hand\nposition. Concurrently, the acoustic data undergoes analysis to extract Mel\nFrequency Cepstral Coefficients (MFCC), aiding in distinguishing between\ndifferent keystrokes. These features feed into machine learning models for\nkeystroke prediction, with results further refined via dictionary-based word\nprediction methods. In our experimental setup, we tested various keyboard types\nunder different environmental conditions. We were able to achieve top-5 key\nprediction accuracy of around 80% for mechanical keyboards and around 60% for\nmembrane keyboards with top-100 word prediction accuracies over 70% for all\nkeyboard types. The results highlight the effectiveness and limitations of our\napproach in the context of real-world scenarios.", "published": "2023-11-04 00:48:20", "link": "http://arxiv.org/abs/2311.02288v1", "categories": ["cs.CR", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "TACNET: Temporal Audio Source Counting Network", "abstract": "In this paper, we introduce the Temporal Audio Source Counting Network\n(TaCNet), an innovative architecture that addresses limitations in audio source\ncounting tasks. TaCNet operates directly on raw audio inputs, eliminating\ncomplex preprocessing steps and simplifying the workflow. Notably, it excels in\nreal-time speaker counting, even with truncated input windows. Our extensive\nevaluation, conducted using the LibriCount dataset, underscores TaCNet's\nexceptional performance, positioning it as a state-of-the-art solution for\naudio source counting tasks. With an average accuracy of 74.18 percentage over\n11 classes, TaCNet demonstrates its effectiveness across diverse scenarios,\nincluding applications involving Chinese and Persian languages. This\ncross-lingual adaptability highlights its versatility and potential impact.", "published": "2023-11-04 10:48:14", "link": "http://arxiv.org/abs/2311.02369v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalized zero-shot audio-to-intent classification", "abstract": "Spoken language understanding systems using audio-only data are gaining\npopularity, yet their ability to handle unseen intents remains limited. In this\nstudy, we propose a generalized zero-shot audio-to-intent classification\nframework with only a few sample text sentences per intent. To achieve this, we\nfirst train a supervised audio-to-intent classifier by making use of a\nself-supervised pre-trained model. We then leverage a neural audio synthesizer\nto create audio embeddings for sample text utterances and perform generalized\nzero-shot classification on unseen intents using cosine similarity. We also\npropose a multimodal training strategy that incorporates lexical information\ninto the audio representation to improve zero-shot performance. Our multimodal\ntraining approach improves the accuracy of zero-shot intent classification on\nunseen intents of SLURP by 2.75% and 18.2% for the SLURP and internal\ngoal-oriented dialog datasets, respectively, compared to audio-only training.", "published": "2023-11-04 18:55:08", "link": "http://arxiv.org/abs/2311.02482v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Disentangled Speech Representations", "abstract": "Disentangled representation learning in speech processing has lagged behind\nother domains, largely due to the lack of datasets with annotated generative\nfactors for robust evaluation. To address this, we propose SynSpeech, a novel\nlarge-scale synthetic speech dataset specifically designed to enable research\non disentangled speech representations. SynSpeech includes controlled\nvariations in speaker identity, spoken text, and speaking style, with three\ndataset versions to support experimentation at different levels of complexity.\n  In this study, we present a comprehensive framework to evaluate disentangled\nrepresentation learning techniques, applying both linear probing and\nestablished supervised disentanglement metrics to assess the modularity,\ncompactness, and informativeness of the representations learned by a\nstate-of-the-art model. Using the RAVE model as a test case, we find that\nSynSpeech facilitates benchmarking across a range of factors, achieving\npromising disentanglement of simpler features like gender and speaking style,\nwhile highlighting challenges in isolating complex attributes like speaker\nidentity. This benchmark dataset and evaluation framework fills a critical gap,\nsupporting the development of more robust and interpretable speech\nrepresentation learning methods.", "published": "2023-11-04 04:54:17", "link": "http://arxiv.org/abs/2311.03389v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
