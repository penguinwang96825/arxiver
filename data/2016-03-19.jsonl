{"title": "A Fast Unified Model for Parsing and Sentence Understanding", "abstract": "Tree-structured neural networks exploit valuable syntactic parse information\nas they interpret the meanings of sentences. However, they suffer from two key\ntechnical problems that make them slow and unwieldy for large-scale NLP tasks:\nthey usually operate on parsed sentences and they do not directly support\nbatched computation. We address these issues by introducing the Stack-augmented\nParser-Interpreter Neural Network (SPINN), which combines parsing and\ninterpretation within a single tree-sequence hybrid model by integrating\ntree-structured sentence interpretation into the linear sequential structure of\na shift-reduce parser. Our model supports batched computation for a speedup of\nup to 25 times over other tree-structured models, and its integrated parser can\noperate on unparsed data with little loss in accuracy. We evaluate it on the\nStanford NLI entailment task and show that it significantly outperforms other\nsentence-encoding models.", "published": "2016-03-19 00:22:20", "link": "http://arxiv.org/abs/1603.06021v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Joint Learning of Compositional and Non-Compositional Phrase\n  Embeddings", "abstract": "We present a novel method for jointly learning compositional and\nnon-compositional phrase embeddings by adaptively weighting both types of\nembeddings using a compositionality scoring function. The scoring function is\nused to quantify the level of compositionality of each phrase, and the\nparameters of the function are jointly optimized with the objective for\nlearning phrase embeddings. In experiments, we apply the adaptive joint\nlearning method to the task of learning embeddings of transitive verb phrases,\nand show that the compositionality scores have strong correlation with human\nratings for verb-object compositionality, substantially outperforming the\nprevious state of the art. Moreover, our embeddings improve upon the previous\nbest model on a transitive verb disambiguation task. We also show that a simple\nensemble technique further improves the results for both tasks.", "published": "2016-03-19 08:53:29", "link": "http://arxiv.org/abs/1603.06067v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tree-to-Sequence Attentional Neural Machine Translation", "abstract": "Most of the existing Neural Machine Translation (NMT) models focus on the\nconversion of sequential data and do not directly use syntactic information. We\npropose a novel end-to-end syntactic NMT model, extending a\nsequence-to-sequence model with the source-side phrase structure. Our model has\nan attention mechanism that enables the decoder to generate a translated word\nwhile softly aligning it with phrases as well as words of the source sentence.\nExperimental results on the WAT'15 English-to-Japanese dataset demonstrate that\nour proposed model considerably outperforms sequence-to-sequence attentional\nNMT models and compares favorably with the state-of-the-art tree-to-string SMT\nsystem.", "published": "2016-03-19 10:08:40", "link": "http://arxiv.org/abs/1603.06075v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Hypernymy Detection with an Integrated Path-based and\n  Distributional Method", "abstract": "Detecting hypernymy relations is a key task in NLP, which is addressed in the\nliterature using two complementary approaches. Distributional methods, whose\nsupervised variants are the current best performers, and path-based methods,\nwhich received less research attention. We suggest an improved path-based\nalgorithm, in which the dependency paths are encoded using a recurrent neural\nnetwork, that achieves results comparable to distributional methods. We then\nextend the approach to integrate both path-based and distributional signals,\nsignificantly improving upon the state-of-the-art on this task.", "published": "2016-03-19 10:09:53", "link": "http://arxiv.org/abs/1603.06076v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Persona-Based Neural Conversation Model", "abstract": "We present persona-based models for handling the issue of speaker consistency\nin neural response generation. A speaker model encodes personas in distributed\nembeddings that capture individual characteristics such as background\ninformation and speaking style. A dyadic speaker-addressee model captures\nproperties of interactions between two interlocutors. Our models yield\nqualitative performance improvements in both perplexity and BLEU scores over\nbaseline sequence-to-sequence models, with similar gains in speaker consistency\nas measured by human judges.", "published": "2016-03-19 23:15:18", "link": "http://arxiv.org/abs/1603.06155v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Character-Level Decoder without Explicit Segmentation for Neural\n  Machine Translation", "abstract": "The existing machine translation systems, whether phrase-based or neural,\nhave relied almost exclusively on word-level modelling with explicit\nsegmentation. In this paper, we ask a fundamental question: can neural machine\ntranslation generate a character sequence without any explicit segmentation? To\nanswer this question, we evaluate an attention-based encoder-decoder with a\nsubword-level encoder and a character-level decoder on four language\npairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.\nOur experiments show that the models with a character-level decoder outperform\nthe ones with a subword-level decoder on all of the four language pairs.\nFurthermore, the ensembles of neural models with a character-level decoder\noutperform the state-of-the-art non-neural machine translation systems on\nEn-Cs, En-De and En-Fi and perform comparably on En-Ru.", "published": "2016-03-19 21:35:04", "link": "http://arxiv.org/abs/1603.06147v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Globally Normalized Transition-Based Neural Networks", "abstract": "We introduce a globally normalized transition-based neural network model that\nachieves state-of-the-art part-of-speech tagging, dependency parsing and\nsentence compression results. Our model is a simple feed-forward neural network\nthat operates on a task-specific transition system, yet achieves comparable or\nbetter accuracies than recurrent models. We discuss the importance of global as\nopposed to local normalization: a key insight is that the label bias problem\nimplies that globally normalized models can be strictly more expressive than\nlocally normalized models.", "published": "2016-03-19 03:56:03", "link": "http://arxiv.org/abs/1603.06042v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Generating Natural Questions About an Image", "abstract": "There has been an explosion of work in the vision & language community during\nthe past few years from image captioning to video transcription, and answering\nquestions about images. These tasks have focused on literal descriptions of the\nimage. To move beyond the literal, we choose to explore how questions about an\nimage are often directed at commonsense inference and the abstract events\nevoked by objects in the image. In this paper, we introduce the novel task of\nVisual Question Generation (VQG), where the system is tasked with asking a\nnatural and engaging question when shown an image. We provide three datasets\nwhich cover a variety of images from object-centric to event-centric, with\nconsiderably more abstract training data than provided to state-of-the-art\ncaptioning systems thus far. We train and test several generative and retrieval\nmodels to tackle the task of VQG. Evaluation results show that while such\nmodels ask reasonable questions for a variety of images, there is still a wide\ngap with human performance which motivates further work on connecting images\nwith commonsense knowledge and pragmatics. Our proposed task offers a new\nchallenge to the community which we hope furthers interest in exploring deeper\nconnections between vision & language.", "published": "2016-03-19 07:27:15", "link": "http://arxiv.org/abs/1603.06059v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "How Transferable are Neural Networks in NLP Applications?", "abstract": "Transfer learning is aimed to make use of valuable knowledge in a source\ndomain to help model performance in a target domain. It is particularly\nimportant to neural networks, which are very likely to be overfitting. In some\nfields like image processing, many studies have shown the effectiveness of\nneural network-based transfer learning. For neural NLP, however, existing\nstudies have only casually applied transfer learning, and conclusions are\ninconsistent. In this paper, we conduct systematic case studies and provide an\nilluminating picture on the transferability of neural networks in NLP.", "published": "2016-03-19 16:38:31", "link": "http://arxiv.org/abs/1603.06111v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension", "abstract": "We review the task of Sentence Pair Scoring, popular in the literature in\nvarious forms - viewed as Answer Sentence Selection, Semantic Text Scoring,\nNext Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a\ncomponent of Memory Networks.\n  We argue that all such tasks are similar from the model perspective and\npropose new baselines by comparing the performance of common IR metrics and\npopular convolutional, recurrent and attention-based neural models across many\nSentence Pair Scoring tasks and datasets. We discuss the problem of evaluating\nrandomized models, propose a statistically grounded methodology, and attempt to\nimprove comparisons by releasing new datasets that are much harder than some of\nthe currently used well explored benchmarks. We introduce a unified open source\nsoftware framework with easily pluggable models and tasks, which enables us to\nexperiment with multi-task reusability of trained sentence model. We set a new\nstate-of-art in performance on the Ubuntu Dialogue dataset.", "published": "2016-03-19 18:35:26", "link": "http://arxiv.org/abs/1603.06127v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
