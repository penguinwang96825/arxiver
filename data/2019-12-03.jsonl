{"title": "An Annotated Dataset of Coreference in English Literature", "abstract": "We present in this work a new dataset of coreference annotations for works of\nliterature in English, covering 29,103 mentions in 210,532 tokens from 100\nworks of fiction. This dataset differs from previous coreference datasets in\ncontaining documents whose average length (2,105.3 words) is four times longer\nthan other benchmark datasets (463.7 for OntoNotes), and contains examples of\ndifficult coreference problems common in literature. This dataset allows for an\nevaluation of cross-domain performance for the task of coreference resolution,\nand analysis into the characteristics of long-distance within-document\ncoreference.", "published": "2019-12-03 00:58:01", "link": "http://arxiv.org/abs/1912.01140v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Inflection Generation Using Neural Language Modeling", "abstract": "The use of Deep Neural Network architectures for Language Modeling has\nrecently seen a tremendous increase in interest in the field of NLP with the\nadvent of transfer learning and the shift in focus from rule-based and\npredictive models (supervised learning) to generative or unsupervised models to\nsolve the long-standing problems in NLP like Information Extraction or Question\nAnswering. While this shift has worked greatly for languages lacking in\ninflectional morphology, such as English, challenges still arise when trying to\nbuild similar systems for morphologically-rich languages, since their\nindividual words shift forms in context more often. In this paper we\ninvestigate the extent to which these new unsupervised or generative techniques\ncan serve to alleviate the type-token ratio disparity in morphologically rich\nlanguages. We apply an off-the-shelf neural language modeling library to the\nnewly introduced task of unsupervised inflection generation in the nominal\ndomain of three morphologically rich languages: Romanian, German, and Finnish.\nWe show that this neural language model architecture can successfully generate\nthe full inflection table of nouns without needing any pre-training on large,\nwikipedia-sized corpora, as long as the model is shown enough inflection\nexamples. In fact, our experiments show that pre-training hinders the\ngeneration performance.", "published": "2019-12-03 02:25:16", "link": "http://arxiv.org/abs/1912.01156v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine\n  Translation", "abstract": "Transfer learning between different language pairs has shown its\neffectiveness for Neural Machine Translation (NMT) in low-resource scenario.\nHowever, existing transfer methods involving a common target language are far\nfrom success in the extreme scenario of zero-shot translation, due to the\nlanguage space mismatch problem between transferor (the parent model) and\ntransferee (the child model) on the source side. To address this challenge, we\npropose an effective transfer learning approach based on cross-lingual\npre-training. Our key idea is to make all source languages share the same\nfeature space and thus enable a smooth transition for zero-shot translation. To\nthis end, we introduce one monolingual pre-training method and two bilingual\npre-training methods to obtain a universal encoder for different languages.\nOnce the universal encoder is constructed, the parent model built on such\nencoder is trained with large-scale annotated data and then directly applied in\nzero-shot translation scenario. Experiments on two public datasets show that\nour approach significantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.", "published": "2019-12-03 06:41:03", "link": "http://arxiv.org/abs/1912.01214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Enriched Visual Storytelling", "abstract": "Stories are diverse and highly personalized, resulting in a large possible\noutput space for story generation. Existing end-to-end approaches produce\nmonotonous stories because they are limited to the vocabulary and knowledge in\na single training dataset. This paper introduces KG-Story, a three-stage\nframework that allows the story generation model to take advantage of external\nKnowledge Graphs to produce interesting stories. KG-Story distills a set of\nrepresentative words from the input prompts, enriches the word set by using\nexternal knowledge graphs, and finally generates stories based on the enriched\nword set. This distill-enrich-generate framework allows the use of external\nresources not only for the enrichment phase, but also for the distillation and\ngeneration phases. In this paper, we show the superiority of KG-Story for\nvisual storytelling, where the input prompt is a sequence of five photos and\nthe output is a short story. Per the human ranking evaluation, stories\ngenerated by KG-Story are on average ranked better than that of the\nstate-of-the-art systems. Our code and output stories are available at\nhttps://github.com/zychen423/KE-VIST.", "published": "2019-12-03 16:16:13", "link": "http://arxiv.org/abs/1912.01496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reading the Manual: Event Extraction as Definition Comprehension", "abstract": "We ask whether text understanding has progressed to where we may extract\nevent information through incremental refinement of bleached statements derived\nfrom annotation manuals. Such a capability would allow for the trivial\nconstruction and extension of an extraction framework by intended end-users\nthrough declarations such as, \"Some person was born in some location at some\ntime.\" We introduce an example of a model that employs such statements, with\nexperiments illustrating we can extract events under closed ontologies and\ngeneralize to unseen event types simply by reading new definitions.", "published": "2019-12-03 18:31:42", "link": "http://arxiv.org/abs/1912.01586v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations", "abstract": "We present COSTRA 1.0, a dataset of complex sentence transformations. The\ndataset is intended for the study of sentence-level embeddings beyond simple\nword alternations or standard paraphrasing. This first version of the dataset\nis limited to sentences in Czech but the construction method is universal and\nwe plan to use it also for other languages. The dataset consist of 4,262 unique\nsentences with average length of 10 words, illustrating 15 types of\nmodifications such as simplification, generalization, or formal and informal\nlanguage variation. The hope is that with this dataset, we should be able to\ntest semantic properties of sentence embeddings and perhaps even to find some\ntopologically interesting 'skeleton' in the sentence embedding space. A\npreliminary analysis using LASER, multi-purpose multi-lingual sentence\nembeddings suggests that the LASER space does not exhibit the desired\nproperties.", "published": "2019-12-03 20:20:31", "link": "http://arxiv.org/abs/1912.01673v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMR-to-Text Generation with Cache Transition Systems", "abstract": "Text generation from AMR involves emitting sentences that reflect the meaning\nof their AMR annotations. Neural sequence-to-sequence models have successfully\nbeen used to decode strings from flattened graphs (e.g., using depth-first or\nrandom traversal). Such models often rely on attention-based decoders to map\nAMR node to English token sequences. Instead of linearizing AMR, we directly\nencode its graph structure and delegate traversal to the decoder. To enforce a\nsentence-aligned graph traversal and provide local graph context, we predict\ntransition-based parser actions in addition to English words. We present two\nmodel variants: one generates parser actions prior to words, while the other\ninterleaves actions with words.", "published": "2019-12-03 20:45:04", "link": "http://arxiv.org/abs/1912.01682v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HAMNER: Headword Amplified Multi-span Distantly Supervised Method for\n  Domain Specific Named Entity Recognition", "abstract": "To tackle Named Entity Recognition (NER) tasks, supervised methods need to\nobtain sufficient cleanly annotated data, which is labor and time consuming. On\nthe contrary, distantly supervised methods acquire automatically annotated data\nusing dictionaries to alleviate this requirement. Unfortunately, dictionaries\nhinder the effectiveness of distantly supervised methods for NER due to its\nlimited coverage, especially in specific domains. In this paper, we aim at the\nlimitations of the dictionary usage and mention boundary detection. We\ngeneralize the distant supervision by extending the dictionary with headword\nbased non-exact matching. We apply a function to better weight the matched\nentity mentions. We propose a span-level model, which classifies all the\npossible spans then infers the selected spans with a proposed dynamic\nprogramming algorithm. Experiments on all three benchmark datasets demonstrate\nthat our method outperforms previous state-of-the-art distantly supervised\nmethods.", "published": "2019-12-03 23:00:38", "link": "http://arxiv.org/abs/1912.01731v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "See and Read: Detecting Depression Symptoms in Higher Education Students\n  Using Multimodal Social Media Data", "abstract": "Mental disorders such as depression and anxiety have been increasing at\nalarming rates in the worldwide population. Notably, the major depressive\ndisorder has become a common problem among higher education students,\naggravated, and maybe even occasioned, by the academic pressures they must\nface. While the reasons for this alarming situation remain unclear (although\nwidely investigated), the student already facing this problem must receive\ntreatment. To that, it is first necessary to screen the symptoms. The\ntraditional way for that is relying on clinical consultations or answering\nquestionnaires. However, nowadays, the data shared at social media is a\nubiquitous source that can be used to detect the depression symptoms even when\nthe student is not able to afford or search for professional care. Previous\nworks have already relied on social media data to detect depression on the\ngeneral population, usually focusing on either posted images or texts or\nrelying on metadata. In this work, we focus on detecting the severity of the\ndepression symptoms in higher education students, by comparing deep learning to\nfeature engineering models induced from both the pictures and their captions\nposted on Instagram. The experimental results show that students presenting a\nBDI score higher or equal than 20 can be detected with 0.92 of recall and 0.69\nof precision in the best case, reached by a fusion model. Our findings show the\npotential of large-scale depression screening, which could shed light upon\nstudents at-risk.", "published": "2019-12-03 00:12:09", "link": "http://arxiv.org/abs/1912.01131v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Writing Across the World's Languages: Deep Internationalization for\n  Gboard, the Google Keyboard", "abstract": "This technical report describes our deep internationalization program for\nGboard, the Google Keyboard. Today, Gboard supports 900+ language varieties\nacross 70+ writing systems, and this report describes how and why we have been\nadding support for hundreds of language varieties from around the globe. Many\nlanguages of the world are increasingly used in writing on an everyday basis,\nand we describe the trends we see. We cover technological and logistical\nchallenges in scaling up a language technology product like Gboard to hundreds\nof language varieties, and describe how we built systems and processes to\noperate at scale. Finally, we summarize the key take-aways from user studies we\nran with speakers of hundreds of languages from around the world.", "published": "2019-12-03 06:56:15", "link": "http://arxiv.org/abs/1912.01218v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Modelling Semantic Categories using Conceptual Neighborhood", "abstract": "While many methods for learning vector space embeddings have been proposed in\nthe field of Natural Language Processing, these methods typically do not\ndistinguish between categories and individuals. Intuitively, if individuals are\nrepresented as vectors, we can think of categories as (soft) regions in the\nembedding space. Unfortunately, meaningful regions can be difficult to\nestimate, especially since we often have few examples of individuals that\nbelong to a given category. To address this issue, we rely on the fact that\ndifferent categories are often highly interdependent. In particular, categories\noften have conceptual neighbors, which are disjoint from but closely related to\nthe given category (e.g.\\ fruit and vegetable). Our hypothesis is that more\naccurate category representations can be learned by relying on the assumption\nthat the regions representing such conceptual neighbors should be adjacent in\nthe embedding space. We propose a simple method for identifying conceptual\nneighbors and then show that incorporating these conceptual neighbors indeed\nleads to more accurate region based representations.", "published": "2019-12-03 07:02:38", "link": "http://arxiv.org/abs/1912.01220v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TU Wien @ TREC Deep Learning '19 -- Simple Contextualization for\n  Re-ranking", "abstract": "The usage of neural network models puts multiple objectives in conflict with\neach other: Ideally we would like to create a neural model that is effective,\nefficient, and interpretable at the same time. However, in most instances we\nhave to choose which property is most important to us. We used the opportunity\nof the TREC 2019 Deep Learning track to evaluate the effectiveness of a\nbalanced neural re-ranking approach. We submitted results of the TK\n(Transformer-Kernel) model: a neural re-ranking model for ad-hoc search using\nan efficient contextualization mechanism. TK employs a very small number of\nlightweight Transformer layers to contextualize query and document word\nembeddings. To score individual term interactions, we use a document-length\nenhanced kernel-pooling, which enables users to gain insight into the model.\nOur best result for the passage ranking task is: 0.420 MAP, 0.671 nDCG, 0.598\nP@10 (TUW19-p3 full). Our best result for the document ranking task is: 0.271\nMAP, 0.465 nDCG, 0.730 P@10 (TUW19-d3 re-ranking).", "published": "2019-12-03 14:19:20", "link": "http://arxiv.org/abs/1912.01385v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SemEval-2016 Task 3: Community Question Answering", "abstract": "This paper describes the SemEval--2016 Task 3 on Community Question\nAnswering, which we offered in English and Arabic. For English, we had three\nsubtasks: Question--Comment Similarity (subtask A), Question--Question\nSimilarity (B), and Question--External Comment Similarity (C). For Arabic, we\nhad another subtask: Rerank the correct answers for a new question (D).\nEighteen teams participated in the task, submitting a total of 95 runs (38\nprimary and 57 contrastive) for the four subtasks. A variety of approaches and\nfeatures were used by the participating systems to address the different\nsubtasks, which are summarized in this paper. The best systems achieved an\nofficial score (MAP) of 79.19, 76.70, 55.41, and 45.83 in subtasks A, B, C, and\nD, respectively. These scores are significantly better than those for the\nbaselines that we provided. For subtask A, the best system improved over the\n2015 winner by 3 points absolute in terms of Accuracy.", "published": "2019-12-03 06:30:34", "link": "http://arxiv.org/abs/1912.01972v1", "categories": ["cs.CL", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SemEval-2016 Task 4: Sentiment Analysis in Twitter", "abstract": "This paper discusses the fourth year of the ``Sentiment Analysis in Twitter\nTask''. SemEval-2016 Task 4 comprises five subtasks, three of which represent a\nsignificant departure from previous editions. The first two subtasks are reruns\nfrom prior years and ask to predict the overall sentiment, and the sentiment\ntowards a topic in a tweet. The three new subtasks focus on two variants of the\nbasic ``sentiment classification in Twitter'' task. The first variant adopts a\nfive-point scale, which confers an ordinal character to the classification\ntask. The second variant focuses on the correct estimation of the prevalence of\neach class of interest, a task which has been called quantification in the\nsupervised learning literature. The task continues to be very popular,\nattracting a total of 43 teams.", "published": "2019-12-03 06:46:20", "link": "http://arxiv.org/abs/1912.01973v1", "categories": ["cs.CL", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "WaveFlow: A Compact Flow-based Model for Raw Audio", "abstract": "In this work, we propose WaveFlow, a small-footprint generative flow for raw\naudio, which is directly trained with maximum likelihood. It handles the\nlong-range structure of 1-D waveform with a dilated 2-D convolutional\narchitecture, while modeling the local variations using expressive\nautoregressive functions. WaveFlow provides a unified view of likelihood-based\nmodels for 1-D data, including WaveNet and WaveGlow as special cases. It\ngenerates high-fidelity speech as WaveNet, while synthesizing several orders of\nmagnitude faster as it only requires a few sequential steps to generate very\nlong waveforms with hundreds of thousands of time-steps. Furthermore, it can\nsignificantly reduce the likelihood gap that has existed between autoregressive\nmodels and flow-based models for efficient synthesis. Finally, our\nsmall-footprint WaveFlow has only 5.91M parameters, which is 15$\\times$ smaller\nthan WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6$\\times$\nfaster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered\ninference kernels.", "published": "2019-12-03 07:00:13", "link": "http://arxiv.org/abs/1912.01219v4", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Facilitating on-line opinion dynamics by mining expressions of\n  causation. The case of climate change debates on The Guardian", "abstract": "News website comment sections are spaces where potentially conflicting\nopinions and beliefs are voiced. Addressing questions of how to study such\ncultural and societal conflicts through technological means, the present\narticle critically examines possibilities and limitations of machine-guided\nexploration and potential facilitation of on-line opinion dynamics. These\ninvestigations are guided by a discussion of an experimental observatory for\nmining and analyzing opinions from climate change-related user comments on news\narticles from the TheGuardian.com. This observatory combines causal mapping\nmethods with computational text analysis in order to mine beliefs and visualize\nopinion landscapes based on expressions of causation. By (1) introducing\ndigital methods and open infrastructures for data exploration and analysis and\n(2) engaging in debates about the implications of such methods and\ninfrastructures, notably in terms of the leap from opinion observation to\ndebate facilitation, the article aims to make a practical and theoretical\ncontribution to the study of opinion dynamics and conflict in new media\nenvironments.", "published": "2019-12-03 09:20:41", "link": "http://arxiv.org/abs/1912.01252v1", "categories": ["cs.CY", "cs.CL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CY"}
{"title": "Multiscale Self Attentive Convolutions for Vision and Language Modeling", "abstract": "Self attention mechanisms have become a key building block in many\nstate-of-the-art language understanding models. In this paper, we show that the\nself attention operator can be formulated in terms of 1x1 convolution\noperations. Following this observation, we propose several novel operators:\nFirst, we introduce a 2D version of self attention that is applicable for 2D\nsignals such as images. Second, we present the 1D and 2D Self Attentive\nConvolutions (SAC) operator that generalizes self attention beyond 1x1\nconvolutions to 1xm and nxm convolutions, respectively. While 1D and 2D self\nattention operate on individual words and pixels, SAC operates on m-grams and\nimage patches, respectively. Third, we present a multiscale version of SAC\n(MSAC) which analyzes the input by employing multiple SAC operators that vary\nby filter size, in parallel. Finally, we explain how MSAC can be utilized for\nvision and language modeling, and further harness MSAC to form a cross\nattentive image similarity machinery.", "published": "2019-12-03 16:51:09", "link": "http://arxiv.org/abs/1912.01521v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Comparative Study of Pretrained Language Models on Thai Social Text\n  Categorization", "abstract": "The ever-growing volume of data of user-generated content on social media\nprovides a nearly unlimited corpus of unlabeled data even in languages where\nresources are scarce. In this paper, we demonstrate that state-of-the-art\nresults on two Thai social text categorization tasks can be realized by\npretraining a language model on a large noisy Thai social media corpus of over\n1.26 billion tokens and later fine-tuned on the downstream classification\ntasks. Due to the linguistically noisy and domain-specific nature of the\ncontent, our unique data preprocessing steps designed for Thai social media\nwere utilized to ease the training comprehension of the model. We compared four\nmodern language models: ULMFiT, ELMo with biLSTM, OpenAI GPT, and BERT. We\nsystematically compared the models across different dimensions including speed\nof pretraining and fine-tuning, perplexity, downstream classification\nbenchmarks, and performance in limited pretraining data.", "published": "2019-12-03 18:26:13", "link": "http://arxiv.org/abs/1912.01580v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep Contextualized Acoustic Representations For Semi-Supervised Speech\n  Recognition", "abstract": "We propose a novel approach to semi-supervised automatic speech recognition\n(ASR). We first exploit a large amount of unlabeled audio data via\nrepresentation learning, where we reconstruct a temporal slice of filterbank\nfeatures from past and future context frames. The resulting deep contextualized\nacoustic representations (DeCoAR) are then used to train a CTC-based end-to-end\nASR system using a smaller amount of labeled audio data. In our experiments, we\nshow that systems trained on DeCoAR consistently outperform ones trained on\nconventional filterbank features, giving 42% and 19% relative improvement over\nthe baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our\napproach can drastically reduce the amount of labeled data required;\nunsupervised training on LibriSpeech then supervision with 100 hours of labeled\ndata achieves performance on par with training on all 960 hours directly.\nPre-trained models and code will be released online.", "published": "2019-12-03 20:32:50", "link": "http://arxiv.org/abs/1912.01679v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual\n  Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well", "abstract": "In this paper, we reproduce the experiments of Artetxe et al. (2018b)\nregarding the robust self-learning method for fully unsupervised cross-lingual\nmappings of word embeddings. We show that the reproduction of their method is\nindeed feasible with some minor assumptions. We further investigate the\nrobustness of their model by introducing four new languages that are less\nsimilar to English than the ones proposed by the original paper. In order to\nassess the stability of their model, we also conduct a grid search over\nsensible hyperparameters. We then propose key recommendations applicable to any\nresearch project in order to deliver fully reproducible research.", "published": "2019-12-03 22:07:47", "link": "http://arxiv.org/abs/1912.01706v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Fast Intent Classification for Spoken Language Understanding", "abstract": "Spoken Language Understanding (SLU) systems consist of several machine\nlearning components operating together (e.g. intent classification, named\nentity recognition and resolution). Deep learning models have obtained state of\nthe art results on several of these tasks, largely attributed to their better\nmodeling capacity. However, an increase in modeling capacity comes with added\ncosts of higher latency and energy usage, particularly when operating on low\ncomplexity devices. To address the latency and computational complexity issues,\nwe explore a BranchyNet scheme on an intent classification scheme within SLU\nsystems. The BranchyNet scheme when applied to a high complexity model, adds\nexit points at various stages in the model allowing early decision making for a\nset of queries to the SLU model. We conduct experiments on the Facebook\nSemantic Parsing dataset with two candidate model architectures for intent\nclassification. Our experiments show that the BranchyNet scheme provides gains\nin terms of computational complexity without compromising model accuracy. We\nalso conduct analytical studies regarding the improvements in the computational\ncost, distribution of utterances that egress from various exit points and the\nimpact of adding more complexity to models with the BranchyNet scheme.", "published": "2019-12-03 22:46:59", "link": "http://arxiv.org/abs/1912.01728v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday\n  Tasks", "abstract": "We present ALFRED (Action Learning From Realistic Environments and\nDirectives), a benchmark for learning a mapping from natural language\ninstructions and egocentric vision to sequences of actions for household tasks.\nALFRED includes long, compositional tasks with non-reversible state changes to\nshrink the gap between research benchmarks and real-world applications. ALFRED\nconsists of expert demonstrations in interactive visual environments for 25k\nnatural language directives. These directives contain both high-level goals\nlike \"Rinse off a mug and place it in the coffee maker.\" and low-level language\ninstructions like \"Walk to the coffee maker on the right.\" ALFRED tasks are\nmore complex in terms of sequence length, action space, and language than\nexisting vision-and-language task datasets. We show that a baseline model based\non recent embodied vision-and-language tasks performs poorly on ALFRED,\nsuggesting that there is significant room for developing innovative grounded\nvisual language understanding models with this benchmark.", "published": "2019-12-03 23:18:59", "link": "http://arxiv.org/abs/1912.01734v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "High-quality Speech Synthesis Using Super-resolution Mel-Spectrogram", "abstract": "In speech synthesis and speech enhancement systems, melspectrograms need to\nbe precise in acoustic representations. However, the generated spectrograms are\nover-smooth, that could not produce high quality synthesized speech. Inspired\nby image-to-image translation, we address this problem by using a\nlearning-based post filter combining Pix2PixHD and ResUnet to reconstruct the\nmel-spectrograms together with super-resolution. From the resulting\nsuper-resolution spectrogram networks, we can generate enhanced spectrograms to\nproduce high quality synthesized speech. Our proposed model achieves improved\nmean opinion scores (MOS) of 3.71 and 4.01 over baseline results of 3.29 and\n3.84, while using vocoder Griffin-Lim and WaveNet, respectively.", "published": "2019-12-03 02:53:54", "link": "http://arxiv.org/abs/1912.01167v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HI-MIA : A Far-field Text-Dependent Speaker Verification Database and\n  the Baselines", "abstract": "This paper presents a far-field text-dependent speaker verification database\nnamed HI-MIA. We aim to meet the data requirement for far-field microphone\narray based speaker verification since most of the publicly available databases\nare single channel close-talking and text-independent. The database contains\nrecordings of 340 people in rooms designed for the far-field scenario.\nRecordings are captured by multiple microphone arrays located in different\ndirections and distance to the speaker and a high-fidelity close-talking\nmicrophone. Besides, we propose a set of end-to-end neural network based\nbaseline systems that adopt single-channel data for training. Moreover, we\npropose a testing background aware enrollment augmentation strategy to further\nenhance the performance. Results show that the fusion systems could achieve\n3.29% EER in the far-field enrollment far field testing task and 4.02% EER in\nthe close-talking enrollment and far-field testing task.", "published": "2019-12-03 07:47:25", "link": "http://arxiv.org/abs/1912.01231v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Style Classification with Compared Methods in XGB and BPNN", "abstract": "Scientists have used many different classification methods to solve the\nproblem of music classification. But the efficiency of each classification is\ndifferent. In this paper, we propose two compared methods on the task of music\nstyle classification. More specifically, feature extraction for representing\ntimbral texture, rhythmic content and pitch content are proposed. Comparative\nevaluations on performances of two classifiers were conducted for music\nclassification with different styles. The result shows that XGB is better\nsuited for small datasets than BPNN", "published": "2019-12-03 05:54:52", "link": "http://arxiv.org/abs/1912.01203v1", "categories": ["cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Design of an algorithm for acoustic signal detection of moving vehicles", "abstract": "The precise detection and counting of vehicles is fundamental to know the\nstate of agglomeration of the roads, as well as to anticipate possible actions\nof regulation of traffic or future changes that it is necessary to carry out in\nthe urban design in the management of the transport. It is in fact part of the\nprograms of many municipalities for statistical purposes, for the planning,\ndesign, extension and modeling of structures. The operator L.O.G. (Laplacian of\ngaussian) is commonly used for the detection of elements in images, by the\nanalysis of its border, and is a widely extended method in image filtering.\nHowever, as such is not an extended procedure in the analysis of\none-dimensional signals. We developed a procedure inspired by LOG filtering,\nand performed the analysis of the acoustic signal obtained by a portable\nrecorder located on the side of a single lane road, by evaluating the negative\nminima of the second discrete derivative of the smoothed acoustic signal,\nfiltering the selection with a threshold, in order to detect and count the\npassage of vehicles along the road. We obtain a simple procedure, scalable, and\nadequately validated by the results for that purpose.", "published": "2019-12-03 17:42:30", "link": "http://arxiv.org/abs/1912.01542v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Singing Voice Conversion with Disentangled Representations of Singer and\n  Vocal Technique Using Variational Autoencoders", "abstract": "We propose a flexible framework that deals with both singer conversion and\nsingers vocal technique conversion. The proposed model is trained on\nnon-parallel corpora, accommodates many-to-many conversion, and leverages\nrecent advances of variational autoencoders. It employs separate encoders to\nlearn disentangled latent representations of singer identity and vocal\ntechnique separately, with a joint decoder for reconstruction. Conversion is\ncarried out by simple vector arithmetic in the learned latent spaces. Both a\nquantitative analysis as well as a visualization of the converted spectrograms\nshow that our model is able to disentangle singer identity and vocal technique\nand successfully perform conversion of these attributes. To the best of our\nknowledge, this is the first work to jointly tackle conversion of singer\nidentity and vocal technique based on a deep learning approach.", "published": "2019-12-03 03:50:08", "link": "http://arxiv.org/abs/1912.02613v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
