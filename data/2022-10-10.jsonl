{"title": "Self-move and Other-move: Quantum Categorical Foundations of Japanese", "abstract": "The purpose of this work is to contribute toward the larger goal of creating\na Quantum Natural Language Processing (QNLP) translator program. This work\ncontributes original diagrammatic representations of the Japanese language\nbased on prior work that accomplished on the English language based on category\ntheory. The germane differences between the English and Japanese languages are\nemphasized to help address English language bias in the current body of\nresearch. Additionally, topological principles of these diagrams and many\npotential avenues for further research are proposed. Why is this endeavor\nimportant? Hundreds of languages have developed over the course of millennia\ncoinciding with the evolution of human interaction across time and geographic\nlocation. These languages are foundational to human survival, experience,\nflourishing, and living the good life. They are also, however, the strongest\nbarrier between people groups. Over the last several decades, advancements in\nNatural Language Processing (NLP) have made it easier to bridge the gap between\nindividuals who do not share a common language or culture. Tools like Google\nTranslate and DeepL make it easier than ever before to share our experiences\nwith people globally. Nevertheless, these tools are still inadequate as they\nfail to convey our ideas across the language barrier fluently, leaving people\nfeeling anxious and embarrassed. This is particularly true of languages born\nout of substantially different cultures, such as English and Japanese. Quantum\ncomputers offer the best chance to achieve translation fluency in that they are\nbetter suited to simulating the natural world and natural phenomenon such as\nnatural speech.\n  Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English\ngrammar, translation, topology, Quantum Natural Language Processing, Natural\nLanguage Processing", "published": "2022-10-10 06:26:59", "link": "http://arxiv.org/abs/2210.04451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distill the Image to Nowhere: Inversion Knowledge Distillation for\n  Multimodal Machine Translation", "abstract": "Past works on multimodal machine translation (MMT) elevate bilingual setup by\nincorporating additional aligned vision information. However, an image-must\nrequirement of the multimodal dataset largely hinders MMT's development --\nnamely that it demands an aligned form of [image, source text, target text].\nThis limitation is generally troublesome during the inference phase especially\nwhen the aligned image is not provided as in the normal NMT setup. Thus, in\nthis work, we introduce IKD-MMT, a novel MMT framework to support the\nimage-free inference phase via an inversion knowledge distillation scheme. In\nparticular, a multimodal feature generator is executed with a knowledge\ndistillation module, which directly generates the multimodal feature from\n(only) source texts as the input. While there have been a few prior works\nentertaining the possibility to support image-free inference for machine\ntranslation, their performances have yet to rival the image-must translation.\nIn our experiments, we identify our method as the first image-free approach to\ncomprehensively rival or even surpass (almost) all image-must frameworks, and\nachieved the state-of-the-art result on the often-used Multi30k benchmark. Our\ncode and data are available at: https://github.com/pengr/IKD-mmt/tree/master..", "published": "2022-10-10 07:36:59", "link": "http://arxiv.org/abs/2210.04468v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Key Information Modeling to Improve Less-Data Constrained\n  News Headline Generation via Duality Fine-Tuning", "abstract": "Recent language generative models are mostly trained on large-scale datasets,\nwhile in some real scenarios, the training datasets are often expensive to\nobtain and would be small-scale. In this paper we investigate the challenging\ntask of less-data constrained generation, especially when the generated news\nheadlines are short yet expected by readers to keep readable and informative\nsimultaneously. We highlight the key information modeling task and propose a\nnovel duality fine-tuning method by formally defining the probabilistic duality\nconstraints between key information prediction and headline generation tasks.\nThe proposed method can capture more information from limited data, build\nconnections between separate tasks, and is suitable for less-data constrained\ngeneration tasks. Furthermore, the method can leverage various pre-trained\ngenerative regimes, e.g., autoregressive and encoder-decoder models. We conduct\nextensive experiments to demonstrate that our method is effective and efficient\nto achieve improved performance in terms of language modeling metric and\ninformativeness correctness metric on two public datasets.", "published": "2022-10-10 07:59:36", "link": "http://arxiv.org/abs/2210.04473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Framework based Query Generation for Temporal Question\n  Answering over Knowledge Graphs", "abstract": "Answering factual questions with temporal intent over knowledge graphs\n(temporal KGQA) attracts rising attention in recent years. In the generation of\ntemporal queries, existing KGQA methods ignore the fact that some intrinsic\nconnections between events can make them temporally related, which may limit\ntheir capability. We systematically analyze the possible interpretation of\ntemporal constraints and conclude the interpretation structures as the Semantic\nFramework of Temporal Constraints, SF-TCons. Based on the semantic framework,\nwe propose a temporal question answering method, SF-TQA, which generates query\ngraphs by exploring the relevant facts of mentioned entities, where the\nexploring process is restricted by SF-TCons. Our evaluations show that SF-TQA\nsignificantly outperforms existing methods on two benchmarks over different\nknowledge graphs.", "published": "2022-10-10 08:40:28", "link": "http://arxiv.org/abs/2210.04490v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Robust Representations for Continual Relation Extraction via\n  Adversarial Class Augmentation", "abstract": "Continual relation extraction (CRE) aims to continually learn new relations\nfrom a class-incremental data stream. CRE model usually suffers from\ncatastrophic forgetting problem, i.e., the performance of old relations\nseriously degrades when the model learns new relations. Most previous work\nattributes catastrophic forgetting to the corruption of the learned\nrepresentations as new relations come, with an implicit assumption that the CRE\nmodels have adequately learned the old relations. In this paper, through\nempirical studies we argue that this assumption may not hold, and an important\nreason for catastrophic forgetting is that the learned representations do not\nhave good robustness against the appearance of analogous relations in the\nsubsequent learning process. To address this issue, we encourage the model to\nlearn more precise and robust representations through a simple yet effective\nadversarial class augmentation mechanism (ACA), which is easy to implement and\nmodel-agnostic. Experimental results show that ACA can consistently improve the\nperformance of state-of-the-art CRE models on two popular benchmarks.", "published": "2022-10-10 08:50:48", "link": "http://arxiv.org/abs/2210.04497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SelfMix: Robust Learning Against Textual Label Noise with Self-Mixup\n  Training", "abstract": "The conventional success of textual classification relies on annotated data,\nand the new paradigm of pre-trained language models (PLMs) still requires a few\nlabeled data for downstream tasks. However, in real-world applications, label\nnoise inevitably exists in training data, damaging the effectiveness,\nrobustness, and generalization of the models constructed on such data.\nRecently, remarkable achievements have been made to mitigate this dilemma in\nvisual data, while only a few explore textual data. To fill this gap, we\npresent SelfMix, a simple yet effective method, to handle label noise in text\nclassification tasks. SelfMix uses the Gaussian Mixture Model to separate\nsamples and leverages semi-supervised learning. Unlike previous works requiring\nmultiple models, our method utilizes the dropout mechanism on a single model to\nreduce the confirmation bias in self-training and introduces a textual-level\nmixup training strategy. Experimental results on three text classification\nbenchmarks with different types of text show that the performance of our\nproposed method outperforms these strong baselines designed for both textual\nand visual data under different noise ratios and noise types. Our code is\navailable at https://github.com/noise-learning/SelfMix.", "published": "2022-10-10 09:46:40", "link": "http://arxiv.org/abs/2210.04525v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Evaluation and Analysis of Idioms in Neural Machine\n  Translation", "abstract": "A major open problem in neural machine translation (NMT) is the translation\nof idiomatic expressions, such as \"under the weather\". The meaning of these\nexpressions is not composed by the meaning of their constituent words, and NMT\nmodels tend to translate them literally (i.e., word-by-word), which leads to\nconfusing and nonsensical translations. Research on idioms in NMT is limited\nand obstructed by the absence of automatic methods for quantifying these\nerrors. In this work, first, we propose a novel metric for automatically\nmeasuring the frequency of literal translation errors without human\ninvolvement. Equipped with this metric, we present controlled translation\nexperiments with models trained in different conditions (with/without the\ntest-set idioms) and across a wide range of (global and targeted) metrics and\ntest sets. We explore the role of monolingual pretraining and find that it\nyields substantial targeted improvements, even without observing any\ntranslation examples of the test-set idioms. In our analysis, we probe the role\nof idiom context. We find that the randomly initialized models are more local\nor \"myopic\" as they are relatively unaffected by variations of the idiom\ncontext, unlike the pretrained ones.", "published": "2022-10-10 10:30:09", "link": "http://arxiv.org/abs/2210.04545v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Are Poor Learners of Directional Inference", "abstract": "We examine LMs' competence of directional predicate entailments by supervised\nfine-tuning with prompts. Our analysis shows that contrary to their apparent\nsuccess on standard NLI, LMs show limited ability to learn such directional\ninference; moreover, existing datasets fail to test directionality, and/or are\ninfested by artefacts that can be learnt as proxy for entailments, yielding\nover-optimistic results. In response, we present BoOQA (Boolean Open QA), a\nrobust multi-lingual evaluation benchmark for directional predicate\nentailments, extrinsic to existing training sets. On BoOQA, we establish\nbaselines and show evidence of existing LM-prompting models being incompetent\ndirectional entailment learners, in contrast to entailment graphs, however\nlimited by sparsity.", "published": "2022-10-10 13:43:16", "link": "http://arxiv.org/abs/2210.04695v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Readability Controllable Biomedical Document Summarization", "abstract": "Different from general documents, it is recognised that the ease with which\npeople can understand a biomedical text is eminently varied, owing to the\nhighly technical nature of biomedical documents and the variance of readers'\ndomain knowledge. However, existing biomedical document summarization systems\nhave paid little attention to readability control, leaving users with summaries\nthat are incompatible with their levels of expertise. In recognition of this\nurgent demand, we introduce a new task of readability controllable\nsummarization for biomedical documents, which aims to recognise users'\nreadability demands and generate summaries that better suit their needs:\ntechnical summaries for experts and plain language summaries (PLS) for laymen.\nTo establish this task, we construct a corpus consisting of biomedical papers\nwith technical summaries and PLSs written by the authors, and benchmark\nmultiple advanced controllable abstractive and extractive summarization models\nbased on pre-trained language models (PLMs) with prevalent controlling and\ngeneration techniques. Moreover, we propose a novel masked language model (MLM)\nbased metric and its variant to effectively evaluate the readability\ndiscrepancy between lay and technical summaries. Experimental results from\nautomated and human evaluations show that though current control techniques\nallow for a certain degree of readability adjustment during generation, the\nperformance of existing controllable summarization methods is far from\ndesirable in this task.", "published": "2022-10-10 14:03:20", "link": "http://arxiv.org/abs/2210.04705v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empowering the Fact-checkers! Automatic Identification of Claim Spans on\n  Twitter", "abstract": "The widespread diffusion of medical and political claims in the wake of\nCOVID-19 has led to a voluminous rise in misinformation and fake news. The\ncurrent vogue is to employ manual fact-checkers to efficiently classify and\nverify such data to combat this avalanche of claim-ridden misinformation.\nHowever, the rate of information dissemination is such that it vastly outpaces\nthe fact-checkers' strength. Therefore, to aid manual fact-checkers in\neliminating the superfluous content, it becomes imperative to automatically\nidentify and extract the snippets of claim-worthy (mis)information present in a\npost. In this work, we introduce the novel task of Claim Span Identification\n(CSI). We propose CURT, a large-scale Twitter corpus with token-level claim\nspans on more than 7.5k tweets. Furthermore, along with the standard token\nclassification baselines, we benchmark our dataset with DABERTa, an\nadapter-based variation of RoBERTa. The experimental results attest that\nDABERTa outperforms the baseline systems across several evaluation metrics,\nimproving by about 1.5 points. We also report detailed error analysis to\nvalidate the model's performance along with the ablation studies. Lastly, we\nrelease our comprehensive span annotation guidelines for public use.", "published": "2022-10-10 14:08:46", "link": "http://arxiv.org/abs/2210.04710v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A dynamic programming algorithm for span-based nested named-entity\n  recognition in O(n^2)", "abstract": "Span-based nested named-entity recognition (NER) has a cubic-time complexity\nusing a variant of the CYK algorithm. We show that by adding a supplementary\nstructural constraint on the search space, nested NER has a quadratic-time\ncomplexity, that is the same asymptotic complexity than the non-nested case.\nThe proposed algorithm covers a large part of three standard English benchmarks\nand delivers comparable experimental results.", "published": "2022-10-10 14:47:36", "link": "http://arxiv.org/abs/2210.04738v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metaphorical Paraphrase Generation: Feeding Metaphorical Language Models\n  with Literal Texts", "abstract": "This study presents a new approach to metaphorical paraphrase generation by\nmasking literal tokens of literal sentences and unmasking them with\nmetaphorical language models. Unlike similar studies, the proposed algorithm\ndoes not only focus on verbs but also on nouns and adjectives. Despite the fact\nthat the transfer rate for the former is the highest (56%), the transfer of the\nlatter is feasible (24% and 31%). Human evaluation showed that our\nsystem-generated metaphors are considered more creative and metaphorical than\nhuman-generated ones while when using our transferred metaphors for data\naugmentation improves the state of the art in metaphorical sentence\nclassification by 3% in F1.", "published": "2022-10-10 15:11:27", "link": "http://arxiv.org/abs/2210.04756v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Neural Referential Form Selectors on a Realistic Multilingual\n  Dataset", "abstract": "Previous work on Neural Referring Expression Generation (REG) all uses\nWebNLG, an English dataset that has been shown to reflect a very limited range\nof referring expression (RE) use. To tackle this issue, we build a dataset\nbased on the OntoNotes corpus that contains a broader range of RE use in both\nEnglish and Chinese (a language that uses zero pronouns). We build neural\nReferential Form Selection (RFS) models accordingly, assess them on the dataset\nand conduct probing experiments. The experiments suggest that, compared to\nWebNLG, OntoNotes is better for assessing REG/RFS models. We compare English\nand Chinese RFS and confirm that, in line with linguistic theories, Chinese RFS\ndepends more on discourse context than English.", "published": "2022-10-10 16:42:25", "link": "http://arxiv.org/abs/2210.04828v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical3D Adapters for Long Video-to-text Summarization", "abstract": "In this paper, we focus on video-to-text summarization and investigate how to\nbest utilize multimodal information for summarizing long inputs (e.g., an\nhour-long TV show) into long outputs (e.g., a multi-sentence summary). We\nextend SummScreen (Chen et al., 2021), a dialogue summarization dataset\nconsisting of transcripts of TV episodes with reference summaries, and create a\nmultimodal variant by collecting corresponding full-length videos. We\nincorporate multimodal information into a pre-trained textual summarizer\nefficiently using adapter modules augmented with a hierarchical structure while\ntuning only 3.8\\% of model parameters. Our experiments demonstrate that\nmultimodal information offers superior performance over more memory-heavy and\nfully fine-tuned textual summarization methods.", "published": "2022-10-10 16:44:36", "link": "http://arxiv.org/abs/2210.04829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation", "abstract": "Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed\ninputs during training -- helps reduce model reliance on spurious correlations\nand improves generalization to out-of-distribution (OOD) data. Prior work on\ngenerating counterfactuals only considered restricted classes of perturbations,\nlimiting their effectiveness. We present COunterfactual Generation via\nRetrieval and Editing (CORE), a retrieval-augmented generation framework for\ncreating diverse counterfactual perturbations for CDA. For each training\nexample, CORE first performs a dense retrieval over a task-related unlabeled\ntext corpus using a learned bi-encoder and extracts relevant counterfactual\nexcerpts. CORE then incorporates these into prompts to a large language model\nwith few-shot learning capabilities, for counterfactual editing. Conditioning\nlanguage model edits on naturally occurring data results in diverse\nperturbations. Experiments on natural language inference and sentiment analysis\nbenchmarks show that CORE counterfactuals are more effective at improving\ngeneralization to OOD data compared to other DA approaches. We also show that\nthe CORE retrieval framework can be used to encourage diversity in manually\nauthored perturbations", "published": "2022-10-10 17:45:38", "link": "http://arxiv.org/abs/2210.04873v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "REV: Information-Theoretic Evaluation of Free-Text Rationales", "abstract": "Generating free-text rationales is a promising step towards explainable NLP,\nyet evaluating such rationales remains a challenge. Existing metrics have\nmostly focused on measuring the association between the rationale and a given\nlabel. We argue that an ideal metric should focus on the new information\nuniquely provided in the rationale that is otherwise not provided in the input\nor the label. We investigate this research problem from an\ninformation-theoretic perspective using conditional V-information (Hewitt et\nal., 2021). More concretely, we propose a metric called REV (Rationale\nEvaluation with conditional V-information), to quantify the amount of new,\nlabel-relevant information in a rationale beyond the information already\navailable in the input or the label. Experiments across four benchmarks with\nreasoning tasks, including chain-of-thought, demonstrate the effectiveness of\nREV in evaluating rationale-label pairs, compared to existing metrics. We\nfurther demonstrate REV is consistent with human judgments on rationale\nevaluations and provides more sensitive measurements of new information in\nfree-text rationales. When used alongside traditional performance metrics, REV\nprovides deeper insights into models' reasoning and prediction processes.", "published": "2022-10-10 19:31:30", "link": "http://arxiv.org/abs/2210.04982v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Representation Distillation with Contrastive Learning", "abstract": "Multilingual sentence representations from large models encode semantic\ninformation from two or more languages and can be used for different\ncross-lingual information retrieval and matching tasks. In this paper, we\nintegrate contrastive learning into multilingual representation distillation\nand use it for quality estimation of parallel sentences (i.e., find\nsemantically similar sentences that can be used as translations of each other).\nWe validate our approach with multilingual similarity search and corpus\nfiltering tasks. Experiments across different low-resource languages show that\nour method greatly outperforms previous sentence encoders such as LASER,\nLASER3, and LaBSE.", "published": "2022-10-10 22:27:04", "link": "http://arxiv.org/abs/2210.05033v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Retrieval Augmented Neural Machine Translation by Controlling\n  Source and Fuzzy-Match Interactions", "abstract": "We explore zero-shot adaptation, where a general-domain model has access to\ncustomer or domain specific parallel data at inference time, but not during\ntraining. We build on the idea of Retrieval Augmented Translation (RAT) where\ntop-k in-domain fuzzy matches are found for the source sentence, and\ntarget-language translations of those fuzzy-matched sentences are provided to\nthe translation model at inference time. We propose a novel architecture to\ncontrol interactions between a source sentence and the top-k fuzzy\ntarget-language matches, and compare it to architectures from prior work. We\nconduct experiments in two language pairs (En-De and En-Fr) by training models\non WMT data and testing them with five and seven multi-domain datasets,\nrespectively. Our approach consistently outperforms the alternative\narchitectures, improving BLEU across language pair, domain, and number k of\nfuzzy matches.", "published": "2022-10-10 23:33:15", "link": "http://arxiv.org/abs/2210.05047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DEPTWEET: A Typology for Social Media Texts to Detect Depression\n  Severities", "abstract": "Mental health research through data-driven methods has been hindered by a\nlack of standard typology and scarcity of adequate data. In this study, we\nleverage the clinical articulation of depression to build a typology for social\nmedia texts for detecting the severity of depression. It emulates the standard\nclinical assessment procedure Diagnostic and Statistical Manual of Mental\nDisorders (DSM-5) and Patient Health Questionnaire (PHQ-9) to encompass subtle\nindications of depressive disorders from tweets. Along with the typology, we\npresent a new dataset of 40191 tweets labeled by expert annotators. Each tweet\nis labeled as 'non-depressed' or 'depressed'. Moreover, three severity levels\nare considered for 'depressed' tweets: (1) mild, (2) moderate, and (3) severe.\nAn associated confidence score is provided with each label to validate the\nquality of annotation. We examine the quality of the dataset via representing\nsummary statistics while setting strong baseline results using attention-based\nmodels like BERT and DistilBERT. Finally, we extensively address the\nlimitations of the study to provide directions for further research.", "published": "2022-10-10 08:23:57", "link": "http://arxiv.org/abs/2210.05372v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Montague semantics and modifier consistency measurement in neural\n  language models", "abstract": "This work proposes a novel methodology for measuring compositional behavior\nin contemporary language embedding models. Specifically, we focus on adjectival\nmodifier phenomena in adjective-noun phrases. In recent years, distributional\nlanguage representation models have demonstrated great practical success. At\nthe same time, the need for interpretability has elicited questions on their\nintrinsic properties and capabilities. Crucially, distributional models are\noften inconsistent when dealing with compositional phenomena in natural\nlanguage, which has significant implications for their safety and fairness.\nDespite this, most current research on compositionality is directed towards\nimproving their performance on similarity tasks only. This work takes a\ndifferent approach, introducing three novel tests of compositional behavior\ninspired by Montague semantics. Our experimental results indicate that current\nneural language models do not behave according to the expected linguistic\ntheories. This indicates that current language models may lack the capability\nto capture the semantic properties we evaluated on limited context, or that\nlinguistic theories from Montagovian tradition may not match the expected\ncapabilities of distributional models.", "published": "2022-10-10 18:43:16", "link": "http://arxiv.org/abs/2212.04310v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XPrompt: Exploring the Extreme of Prompt Tuning", "abstract": "Prompt tuning learns soft prompts to condition frozen Pre-trained Language\nModels (PLMs) for performing downstream tasks in a parameter-efficient manner.\nWhile prompt tuning has gradually reached the performance level of fine-tuning\nas the model scale increases, there is still a large performance gap between\nprompt tuning and fine-tuning for models of moderate and small scales\n(typically less than 11B parameters). In this paper, we empirically show that\nthe trained prompt tokens can have a negative impact on a downstream task and\nthus degrade its performance. To bridge the gap, we propose a novel Prompt\ntuning model with an eXtremely small scale (XPrompt) under the regime of\nlottery tickets hypothesis. Specifically, XPrompt eliminates the negative\nprompt tokens at different granularity levels through a hierarchical structured\npruning, yielding a more parameter-efficient prompt yet with a competitive\nperformance. Comprehensive experiments are carried out on SuperGLUE tasks, and\nthe extensive results indicate that XPrompt is able to close the performance\ngap at smaller model scales.", "published": "2022-10-10 06:57:19", "link": "http://arxiv.org/abs/2210.04457v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating the Failure Modes of the AUC metric and Exploring\n  Alternatives for Evaluating Systems in Safety Critical Applications", "abstract": "With the increasing importance of safety requirements associated with the use\nof black box models, evaluation of selective answering capability of models has\nbeen critical. Area under the curve (AUC) is used as a metric for this purpose.\nWe find limitations in AUC; e.g., a model having higher AUC is not always\nbetter in performing selective answering. We propose three alternate metrics\nthat fix the identified limitations. On experimenting with ten models, our\nresults using the new metrics show that newer and larger pre-trained models do\nnot necessarily show better performance in selective answering. We hope our\ninsights will help develop better models tailored for safety-critical\napplications.", "published": "2022-10-10 07:22:31", "link": "http://arxiv.org/abs/2210.04466v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Do Children Texts Hold The Key To Commonsense Knowledge?", "abstract": "Compiling comprehensive repositories of commonsense knowledge is a\nlong-standing problem in AI. Many concerns revolve around the issue of\nreporting bias, i.e., that frequency in text sources is not a good proxy for\nrelevance or truth. This paper explores whether children's texts hold the key\nto commonsense knowledge compilation, based on the hypothesis that such content\nmakes fewer assumptions on the reader's knowledge, and therefore spells out\ncommonsense more explicitly. An analysis with several corpora shows that\nchildren's texts indeed contain much more, and more typical commonsense\nassertions. Moreover, experiments show that this advantage can be leveraged in\npopular language-model-based commonsense knowledge extraction settings, where\ntask-unspecific fine-tuning on small amounts of children texts (childBERT)\nalready yields significant improvements. This provides a refreshing perspective\ndifferent from the common trend of deriving progress from ever larger models\nand corpora.", "published": "2022-10-10 09:56:08", "link": "http://arxiv.org/abs/2210.04530v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HumSet: Dataset of Multilingual Information Extraction and\n  Classification for Humanitarian Crisis Response", "abstract": "Timely and effective response to humanitarian crises requires quick and\naccurate analysis of large amounts of text data - a process that can highly\nbenefit from expert-assisted NLP systems trained on validated and annotated\ndata in the humanitarian response domain. To enable creation of such NLP\nsystems, we introduce and release HumSet, a novel and rich multilingual dataset\nof humanitarian response documents annotated by experts in the humanitarian\nresponse community. The dataset provides documents in three languages (English,\nFrench, Spanish) and covers a variety of humanitarian crises from 2018 to 2021\nacross the globe. For each document, HUMSET provides selected snippets\n(entries) as well as assigned classes to each entry annotated using common\nhumanitarian information analysis frameworks. HUMSET also provides novel and\nchallenging entry extraction and multi-label entry classification tasks. In\nthis paper, we take a first step towards approaching these tasks and conduct a\nset of experiments on Pre-trained Language Models (PLM) to establish strong\nbaselines for future research in this domain. The dataset is available at\nhttps://blog.thedeep.io/humset/.", "published": "2022-10-10 11:28:07", "link": "http://arxiv.org/abs/2210.04573v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "YFACC: A Yor\u00f9b\u00e1 speech-image dataset for cross-lingual keyword\n  localisation through visual grounding", "abstract": "Visually grounded speech (VGS) models are trained on images paired with\nunlabelled spoken captions. Such models could be used to build speech systems\nin settings where it is impossible to get labelled data, e.g. for documenting\nunwritten languages. However, most VGS studies are in English or other\nhigh-resource languages. This paper attempts to address this shortcoming. We\ncollect and release a new single-speaker dataset of audio captions for 6k\nFlickr images in Yor\\`ub\\'a -- a real low-resource language spoken in Nigeria.\nWe train an attention-based VGS model where images are automatically tagged\nwith English visual labels and paired with Yor\\`ub\\'a utterances. This enables\ncross-lingual keyword localisation: a written English query is detected and\nlocated in Yor\\`ub\\'a speech. To quantify the effect of the smaller dataset, we\ncompare to English systems trained on similar and more data. We hope that this\nnew dataset will stimulate research in the use of VGS models for real\nlow-resource languages.", "published": "2022-10-10 11:58:10", "link": "http://arxiv.org/abs/2210.04600v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Survey of Methods for Addressing Class Imbalance in Deep-Learning\n  Based Natural Language Processing", "abstract": "Many natural language processing (NLP) tasks are naturally imbalanced, as\nsome target categories occur much more frequently than others in the real\nworld. In such scenarios, current NLP models still tend to perform poorly on\nless frequent classes. Addressing class imbalance in NLP is an active research\ntopic, yet, finding a good approach for a particular task and imbalance\nscenario is difficult.\n  With this survey, the first overview on class imbalance in deep-learning\nbased NLP, we provide guidance for NLP researchers and practitioners dealing\nwith imbalanced data. We first discuss various types of controlled and\nreal-world class imbalance. Our survey then covers approaches that have been\nexplicitly proposed for class-imbalanced NLP tasks or, originating in the\ncomputer vision community, have been evaluated on them. We organize the methods\nby whether they are based on sampling, data augmentation, choice of loss\nfunction, staged learning, or model design. Finally, we discuss open problems\nsuch as dealing with multi-label scenarios, and propose systematic benchmarking\nand reporting in order to move forward on this problem as a community.", "published": "2022-10-10 13:26:40", "link": "http://arxiv.org/abs/2210.04675v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning \"O\" Helps for Learning More: Handling the Concealed Entity\n  Problem for Class-incremental NER", "abstract": "As the categories of named entities rapidly increase, the deployed NER models\nare required to keep updating toward recognizing more entity types, creating a\ndemand for class-incremental learning for NER. Considering the privacy concerns\nand storage constraints, the standard paradigm for class-incremental NER\nupdates the models with training data only annotated with the new classes, yet\nthe entities from other entity classes are unlabeled, regarded as \"Non-entity\"\n(or \"O\"). In this work, we conduct an empirical study on the \"Unlabeled Entity\nProblem\" and find that it leads to severe confusion between \"O\" and entities,\ndecreasing class discrimination of old classes and declining the model's\nability to learn new classes. To solve the Unlabeled Entity Problem, we propose\na novel representation learning method to learn discriminative representations\nfor the entity classes and \"O\". Specifically, we propose an entity-aware\ncontrastive learning method that adaptively detects entity clusters in \"O\".\nFurthermore, we propose two effective distance-based relabeling strategies for\nbetter learning the old classes. We introduce a more realistic and challenging\nbenchmark for class-incremental NER, and the proposed method achieves up to\n10.62\\% improvement over the baseline methods.", "published": "2022-10-10 13:26:45", "link": "http://arxiv.org/abs/2210.04676v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating image captions with external encyclopedic knowledge", "abstract": "Accurately reporting what objects are depicted in an image is largely a\nsolved problem in automatic caption generation. The next big challenge on the\nway to truly humanlike captioning is being able to incorporate the context of\nthe image and related real world knowledge. We tackle this challenge by\ncreating an end-to-end caption generation system that makes extensive use of\nimage-specific encyclopedic data. Our approach includes a novel way of using\nimage location to identify relevant open-domain facts in an external knowledge\nbase, with their subsequent integration into the captioning pipeline at both\nthe encoding and decoding stages. Our system is trained and tested on a new\ndataset with naturally produced knowledge-rich captions, and achieves\nsignificant improvements over multiple baselines. We empirically demonstrate\nthat our approach is effective for generating contextualized captions with\nencyclopedic knowledge that is both factually accurate and relevant to the\nimage.", "published": "2022-10-10 16:09:21", "link": "http://arxiv.org/abs/2210.04806v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation Transfer Sets and their Impact on Downstream NLU\n  Tasks", "abstract": "Teacher-student knowledge distillation is a popular technique for compressing\ntoday's prevailing large language models into manageable sizes that fit\nlow-latency downstream applications. Both the teacher and the choice of\ntransfer set used for distillation are crucial ingredients in creating a high\nquality student. Yet, the generic corpora used to pretrain the teacher and the\ncorpora associated with the downstream target domain are often significantly\ndifferent, which raises a natural question: should the student be distilled\nover the generic corpora, so as to learn from high-quality teacher predictions,\nor over the downstream task corpora to align with finetuning? Our study\ninvestigates this trade-off using Domain Classification (DC) and Intent\nClassification/Named Entity Recognition (ICNER) as downstream tasks. We distill\nseveral multilingual students from a larger multilingual LM with varying\nproportions of generic and task-specific datasets, and report their performance\nafter finetuning on DC and ICNER. We observe significant improvements across\ntasks and test sets when only task-specific corpora is used. We also report on\nhow the impact of adding task-specific data to the transfer set correlates with\nthe similarity between generic and task-specific data. Our results clearly\nindicate that, while distillation from a generic LM benefits downstream tasks,\nstudents learn better using target domain data even if it comes at the price of\nnoisier teacher predictions. In other words, target domain data still trumps\nteacher knowledge.", "published": "2022-10-10 16:49:52", "link": "http://arxiv.org/abs/2210.04834v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SMiLE: Schema-augmented Multi-level Contrastive Learning for Knowledge\n  Graph Link Prediction", "abstract": "Link prediction is the task of inferring missing links between entities in\nknowledge graphs. Embedding-based methods have shown effectiveness in\naddressing this problem by modeling relational patterns in triples. However,\nthe link prediction task often requires contextual information in entity\nneighborhoods, while most existing embedding-based methods fail to capture it.\nAdditionally, little attention is paid to the diversity of entity\nrepresentations in different contexts, which often leads to false prediction\nresults. In this situation, we consider that the schema of knowledge graph\ncontains the specific contextual information, and it is beneficial for\npreserving the consistency of entities across contexts. In this paper, we\npropose a novel Schema-augmented Multi-level contrastive LEarning framework\n(SMiLE) to conduct knowledge graph link prediction. Specifically, we first\nexploit network schema as the prior constraint to sample negatives and\npre-train our model by employing a multi-level contrastive learning method to\nyield both prior schema and contextual information. Then we fine-tune our model\nunder the supervision of individual triples to learn subtler representations\nfor link prediction. Extensive experimental results on four knowledge graph\ndatasets with thorough analysis of each component demonstrate the effectiveness\nof our proposed framework against state-of-the-art baselines. The\nimplementation of SMiLE is available at https://github.com/GKNL/SMiLE.", "published": "2022-10-10 17:40:19", "link": "http://arxiv.org/abs/2210.04870v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What the DAAM: Interpreting Stable Diffusion Using Cross Attention", "abstract": "Large-scale diffusion neural networks represent a substantial milestone in\ntext-to-image generation, but they remain poorly understood, lacking\ninterpretability analyses. In this paper, we perform a text-image attribution\nanalysis on Stable Diffusion, a recently open-sourced model. To produce\npixel-level attribution maps, we upscale and aggregate cross-attention\nword-pixel scores in the denoising subnetwork, naming our method DAAM. We\nevaluate its correctness by testing its semantic segmentation ability on nouns,\nas well as its generalized attribution quality on all parts of speech, rated by\nhumans. We then apply DAAM to study the role of syntax in the pixel space,\ncharacterizing head--dependent heat map interaction patterns for ten common\ndependency relations. Finally, we study several semantic phenomena using DAAM,\nwith a focus on feature entanglement, where we find that cohyponyms worsen\ngeneration quality and descriptive adjectives attend too broadly. To our\nknowledge, we are the first to interpret large diffusion models from a\nvisuolinguistic perspective, which enables future lines of research. Our code\nis at https://github.com/castorini/daam.", "published": "2022-10-10 17:55:41", "link": "http://arxiv.org/abs/2210.04885v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Extracting or Guessing? Improving Faithfulness of Event Temporal\n  Relation Extraction", "abstract": "In this paper, we seek to improve the faithfulness of TempRel extraction\nmodels from two perspectives. The first perspective is to extract genuinely\nbased on contextual description. To achieve this, we propose to conduct\ncounterfactual analysis to attenuate the effects of two significant types of\ntraining biases: the event trigger bias and the frequent label bias. We also\nadd tense information into event representations to explicitly place an\nemphasis on the contextual description. The second perspective is to provide\nproper uncertainty estimation and abstain from extraction when no relation is\ndescribed in the text. By parameterization of Dirichlet Prior over the\nmodel-predicted categorical distribution, we improve the model estimates of the\ncorrectness likelihood and make TempRel predictions more selective. We also\nemploy temperature scaling to recalibrate the model confidence measure after\nbias mitigation. Through experimental analysis on MATRES, MATRES-DS, and\nTDDiscourse, we demonstrate that our model extracts TempRel and timelines more\nfaithfully compared to SOTA methods, especially under distribution shifts.", "published": "2022-10-10 19:53:13", "link": "http://arxiv.org/abs/2210.04992v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Not All Errors are Equal: Learning Text Generation Metrics using\n  Stratified Error Synthesis", "abstract": "Is it possible to build a general and automatic natural language generation\n(NLG) evaluation metric? Existing learned metrics either perform\nunsatisfactorily or are restricted to tasks where large human rating data is\nalready available. We introduce SESCORE, a model-based metric that is highly\ncorrelated with human judgements without requiring human annotation, by\nutilizing a novel, iterative error synthesis and severity scoring pipeline.\nThis pipeline applies a series of plausible errors to raw text and assigns\nseverity labels by simulating human judgements with entailment. We evaluate\nSESCORE against existing metrics by comparing how their scores correlate with\nhuman ratings. SESCORE outperforms all prior unsupervised metrics on multiple\ndiverse NLG tasks including machine translation, image captioning, and WebNLG\ntext generation. For WMT 20/21 En-De and Zh-En, SESCORE improve the average\nKendall correlation with human judgement from 0.154 to 0.195. SESCORE even\nachieves comparable performance to the best supervised metric COMET, despite\nreceiving no human-annotated training data.", "published": "2022-10-10 22:30:26", "link": "http://arxiv.org/abs/2210.05035v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video\n  Retrieval Benchmarks", "abstract": "Searching troves of videos with textual descriptions is a core multimodal\nretrieval task. Owing to the lack of a purpose-built dataset for text-to-video\nretrieval, video captioning datasets have been re-purposed to evaluate models\nby (1) treating captions as positive matches to their respective videos and (2)\nassuming all other videos to be negatives. However, this methodology leads to a\nfundamental flaw during evaluation: since captions are marked as relevant only\nto their original video, many alternate videos also match the caption, which\nintroduces false-negative caption-video pairs. We show that when these false\nnegatives are corrected, a recent state-of-the-art model gains 25\\% recall\npoints -- a difference that threatens the validity of the benchmark itself. To\ndiagnose and mitigate this issue, we annotate and release 683K additional\ncaption-video pairs. Using these, we recompute effectiveness scores for three\nmodels on two standard benchmarks (MSR-VTT and MSVD). We find that (1) the\nrecomputed metrics are up to 25\\% recall points higher for the best models, (2)\nthese benchmarks are nearing saturation for Recall@10, (3) caption length\n(generality) is related to the number of positives, and (4) annotation costs\ncan be mitigated through sampling. We recommend retiring these benchmarks in\ntheir current form, and we make recommendations for future text-to-video\nretrieval benchmarks.", "published": "2022-10-10 22:45:06", "link": "http://arxiv.org/abs/2210.05038v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling", "abstract": "Ensembling BERT models often significantly improves accuracy, but at the cost\nof significantly more computation and memory footprint. In this work, we\npropose Multi-CLS BERT, a novel ensembling method for CLS-based prediction\ntasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses\nmultiple CLS tokens with a parameterization and objective that encourages their\ndiversity. Thus instead of fine-tuning each BERT model in an ensemble (and\nrunning them all at test time), we need only fine-tune our single Multi-CLS\nBERT model (and run the one model at test time, ensembling just the multiple\nfinal CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on\ntop of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and\nRudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS\nBERT reliably improves both overall accuracy and confidence estimation. When\nonly 100 training samples are available in GLUE, the Multi-CLS BERT_Base model\ncan even outperform the corresponding BERT_Large model. We analyze the behavior\nof our Multi-CLS BERT, showing that it has many of the same characteristics and\nbehavior as a typical BERT 5-way ensemble, but with nearly 4-times less\ncomputation and memory.", "published": "2022-10-10 23:15:17", "link": "http://arxiv.org/abs/2210.05043v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Tuning with Special Token Adaptation", "abstract": "Parameter-efficient tuning aims at updating only a small subset of parameters\nwhen adapting a pretrained model to downstream tasks. In this work, we\nintroduce PASTA, in which we only modify the special token representations\n(e.g., [SEP] and [CLS] in BERT) before the self-attention module at each layer\nin Transformer-based models. PASTA achieves comparable performance to full\nfinetuning in natural language understanding tasks including text\nclassification and NER with up to only 0.029% of total parameters trained. Our\nwork not only provides a simple yet effective way of parameter-efficient\ntuning, which has a wide range of practical applications when deploying\nfinetuned models for multiple tasks, but also demonstrates the pivotal role of\nspecial tokens in pretrained language models", "published": "2022-10-10 01:02:51", "link": "http://arxiv.org/abs/2210.04382v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue", "abstract": "Embodied dialogue instruction following requires an agent to complete a\ncomplex sequence of tasks from a natural language exchange. The recent\nintroduction of benchmarks (Padmakumar et al., 2022) raises the question of how\nbest to train and evaluate models for this multi-turn, multi-agent,\nlong-horizon task. This paper contributes to that conversation, by arguing that\nimitation learning (IL) and related low-level metrics are actually misleading\nand do not align with the goals of embodied dialogue research and may hinder\nprogress. We provide empirical comparisons of metrics, analysis of three\nmodels, and make suggestions for how the field might best progress. First, we\nobserve that models trained with IL take spurious actions during evaluation.\nSecond, we find that existing models fail to ground query utterances, which are\nessential for task completion. Third, we argue evaluation should focus on\nhigher-level semantic goals.", "published": "2022-10-10 05:51:40", "link": "http://arxiv.org/abs/2210.04443v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CrowdChecked: Detecting Previously Fact-Checked Claims in Social Media", "abstract": "While there has been substantial progress in developing systems to automate\nfact-checking, they still lack credibility in the eyes of the users. Thus, an\ninteresting approach has emerged: to perform automatic fact-checking by\nverifying whether an input claim has been previously fact-checked by\nprofessional fact-checkers and to return back an article that explains their\ndecision. This is a sensible approach as people trust manual fact-checking, and\nas many claims are repeated multiple times. Yet, a major issue when building\nsuch systems is the small number of known tweet--verifying article pairs\navailable for training. Here, we aim to bridge this gap by making use of crowd\nfact-checking, i.e., mining claims in social media for which users have\nresponded with a link to a fact-checking article. In particular, we mine a\nlarge-scale collection of 330,000 tweets paired with a corresponding\nfact-checking article. We further propose an end-to-end framework to learn from\nthis noisy data based on modified self-adaptive training, in a distant\nsupervision scenario. Our experiments on the CLEF'21 CheckThat! test set show\nimprovements over the state of the art by two points absolute. Our code and\ndatasets are available at https://github.com/mhardalov/crowdchecked-claims", "published": "2022-10-10 06:05:52", "link": "http://arxiv.org/abs/2210.04447v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Using Both Demonstrations and Language Instructions to Efficiently Learn\n  Robotic Tasks", "abstract": "Demonstrations and natural language instructions are two common ways to\nspecify and teach robots novel tasks. However, for many complex tasks, a\ndemonstration or language instruction alone contains ambiguities, preventing\ntasks from being specified clearly. In such cases, a combination of both a\ndemonstration and an instruction more concisely and effectively conveys the\ntask to the robot than either modality alone. To instantiate this problem\nsetting, we train a single multi-task policy on a few hundred challenging\nrobotic pick-and-place tasks and propose DeL-TaCo (Joint Demo-Language Task\nConditioning), a method for conditioning a robotic policy on task embeddings\ncomprised of two components: a visual demonstration and a language instruction.\nBy allowing these two modalities to mutually disambiguate and clarify each\nother during novel task specification, DeL-TaCo (1) substantially decreases the\nteacher effort needed to specify a new task and (2) achieves better\ngeneralization performance on novel objects and instructions over previous\ntask-conditioning methods. To our knowledge, this is the first work to show\nthat simultaneously conditioning a multi-task robotic manipulation policy on\nboth demonstration and language embeddings improves sample efficiency and\ngeneralization over conditioning on either modality alone. See additional\nmaterials at https://deltaco-robot.github.io/", "published": "2022-10-10 08:06:58", "link": "http://arxiv.org/abs/2210.04476v2", "categories": ["cs.RO", "cs.CL", "cs.LG", "I.2.9; I.2.7; I.2.6"], "primary_category": "cs.RO"}
{"title": "Unified Detoxifying and Debiasing in Language Generation via\n  Inference-time Adaptive Optimization", "abstract": "Warning: this paper contains model outputs exhibiting offensiveness and\nbiases. Recently pre-trained language models (PLMs) have prospered in various\nnatural language generation (NLG) tasks due to their ability to generate fairly\nfluent text. Nevertheless, these models are observed to capture and reproduce\nharmful contents in training corpora, typically toxic language and social\nbiases, raising severe moral issues. Prior works on ethical NLG tackle\ndetoxifying and debiasing separately, which is problematic since we find\ndebiased models still exhibit toxicity while detoxified ones even exacerbate\nsocial biases. To address such a challenge, we propose the first unified\nframework of detoxifying and debiasing called UDDIA, which jointly formalizes\nthese two problems as rectifying the output space. We theoretically interpret\nour framework as learning a text distribution mixing weighted attributes.\nBesides, UDDIA conducts adaptive optimization of only a few parameters during\ndecoding based on a parameter-efficient tuning schema without any training\ndata. This leads to minimal generation quality loss and improved rectification\nperformance with acceptable computational cost. Experimental results\ndemonstrate that compared to several strong baselines, UDDIA achieves debiasing\nand detoxifying simultaneously and better balances efficiency and\neffectiveness, taking a further step towards practical ethical NLG.", "published": "2022-10-10 08:45:25", "link": "http://arxiv.org/abs/2210.04492v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Uncertainty Quantification with Pre-trained Language Models: A\n  Large-Scale Empirical Analysis", "abstract": "Pre-trained language models (PLMs) have gained increasing popularity due to\ntheir compelling prediction performance in diverse natural language processing\n(NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it\nis also crucial for the pipeline to minimize the calibration error, especially\nin safety-critical applications. That is, the pipeline should reliably indicate\nwhen we can trust its predictions. In particular, there are various\nconsiderations behind the pipeline: (1) the choice and (2) the size of PLM, (3)\nthe choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and\nmany more. Although prior work has looked into some of these considerations,\nthey usually draw conclusions based on a limited scope of empirical studies.\nThere still lacks a holistic analysis on how to compose a well-calibrated\nPLM-based prediction pipeline. To fill this void, we compare a wide range of\npopular options for each consideration based on three prevalent NLP\nclassification tasks and the setting of domain shift. In response, we recommend\nthe following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if\npossible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal\nLoss for fine-tuning.", "published": "2022-10-10 14:16:01", "link": "http://arxiv.org/abs/2210.04714v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Knowledge Prompts: Injecting World Knowledge into Language Models\n  through Soft Prompts", "abstract": "Soft prompts have been recently proposed as a tool for adapting large frozen\nlanguage models (LMs) to new tasks. In this work, we repurpose soft prompts to\nthe task of injecting world knowledge into LMs. We introduce a method to train\nsoft prompts via self-supervised learning on data from knowledge bases. The\nresulting soft knowledge prompts (KPs) are task independent and work as an\nexternal memory of the LMs. We perform qualitative and quantitative experiments\nand demonstrate that: (1) KPs can effectively model the structure of the\ntraining data; (2) KPs can be used to improve the performance of LMs in\ndifferent knowledge intensive tasks.", "published": "2022-10-10 14:31:16", "link": "http://arxiv.org/abs/2210.04726v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robustification of Multilingual Language Models to Real-world Noise in\n  Crosslingual Zero-shot Settings with Robust Contrastive Pretraining", "abstract": "Advances in neural modeling have achieved state-of-the-art (SOTA) results on\npublic natural language processing (NLP) benchmarks, at times surpassing human\nperformance. However, there is a gap between public benchmarks and real-world\napplications where noise, such as typographical or grammatical mistakes, is\nabundant and can result in degraded performance. Unfortunately, works which\nevaluate the robustness of neural models on noisy data and propose\nimprovements, are limited to the English language. Upon analyzing noise in\ndifferent languages, we observe that noise types vary greatly across languages.\nThus, existing investigations do not generalize trivially to multilingual\nsettings. To benchmark the performance of pretrained multilingual language\nmodels, we construct noisy datasets covering five languages and four NLP tasks\nand observe a clear gap in the performance between clean and noisy data in the\nzero-shot cross-lingual setting. After investigating several ways to boost the\nrobustness of multilingual models in this setting, we propose Robust\nContrastive Pretraining (RCP). RCP combines data augmentation with a\ncontrastive loss term at the pretraining stage and achieves large improvements\non noisy (and original test data) across two sentence-level (+3.2%) and two\nsequence-labeling (+10 F1-score) multilingual classification tasks.", "published": "2022-10-10 15:40:43", "link": "http://arxiv.org/abs/2210.04782v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer-based Localization from Embodied Dialog with Large-scale\n  Pre-training", "abstract": "We address the challenging task of Localization via Embodied Dialog (LED).\nGiven a dialog from two agents, an Observer navigating through an unknown\nenvironment and a Locator who is attempting to identify the Observer's\nlocation, the goal is to predict the Observer's final location in a map. We\ndevelop a novel LED-Bert architecture and present an effective pretraining\nstrategy. We show that a graph-based scene representation is more effective\nthan the top-down 2D maps used in prior works. Our approach outperforms\nprevious baselines.", "published": "2022-10-10 17:25:06", "link": "http://arxiv.org/abs/2210.04864v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Translate First Reorder Later: Leveraging Monotonicity in Semantic\n  Parsing", "abstract": "Prior work in semantic parsing has shown that conventional seq2seq models\nfail at compositional generalization tasks. This limitation led to a resurgence\nof methods that model alignments between sentences and their corresponding\nmeaning representations, either implicitly through latent variables or\nexplicitly by taking advantage of alignment annotations. We take the second\ndirection and propose TPOL, a two-step approach that first translates input\nsentences monotonically and then reorders them to obtain the correct output.\nThis is achieved with a modular framework comprising a Translator and a\nReorderer component. We test our approach on two popular semantic parsing\ndatasets. Our experiments show that by means of the monotonic translations,\nTPOL can learn reliable lexico-logical patterns from aligned data,\nsignificantly improving compositional generalization both over conventional\nseq2seq models, as well as over other approaches that exploit gold alignments.", "published": "2022-10-10 17:50:42", "link": "http://arxiv.org/abs/2210.04878v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating Executable Action Plans with Environmentally-Aware Language\n  Models", "abstract": "Large Language Models (LLMs) trained using massive text datasets have\nrecently shown promise in generating action plans for robotic agents from high\nlevel text queries. However, these models typically do not consider the robot's\nenvironment, resulting in generated plans that may not actually be executable,\ndue to ambiguities in the planned actions or environmental constraints. In this\npaper, we propose an approach to generate environmentally-aware action plans\nthat agents are better able to execute. Our approach involves integrating\nenvironmental objects and object relations as additional inputs into LLM action\nplan generation to provide the system with an awareness of its surroundings,\nresulting in plans where each generated action is mapped to objects present in\nthe scene. We also design a novel scoring function that, along with generating\nthe action steps and associating them with objects, helps the system\ndisambiguate among object instances and take into account their states. We\nevaluated our approach using the VirtualHome simulator and the ActivityPrograms\nknowledge base and found that action plans generated from our system had a 310%\nimprovement in executability and a 147% improvement in correctness over prior\nwork. The complete code and a demo of our method is publicly available at\nhttps://github.com/hri-ironlab/scene_aware_language_planner.", "published": "2022-10-10 18:56:57", "link": "http://arxiv.org/abs/2210.04964v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "DeepPerform: An Efficient Approach for Performance Testing of\n  Resource-Constrained Neural Networks", "abstract": "Today, an increasing number of Adaptive Deep Neural Networks (AdNNs) are\nbeing used on resource-constrained embedded devices. We observe that, similar\nto traditional software, redundant computation exists in AdNNs, resulting in\nconsiderable performance degradation. The performance degradation is dependent\non the input and is referred to as input-dependent performance bottlenecks\n(IDPBs). To ensure an AdNN satisfies the performance requirements of\nresource-constrained applications, it is essential to conduct performance\ntesting to detect IDPBs in the AdNN. Existing neural network testing methods\nare primarily concerned with correctness testing, which does not involve\nperformance testing. To fill this gap, we propose DeepPerform, a scalable\napproach to generate test samples to detect the IDPBs in AdNNs. We first\ndemonstrate how the problem of generating performance test samples detecting\nIDPBs can be formulated as an optimization problem. Following that, we\ndemonstrate how DeepPerform efficiently handles the optimization problem by\nlearning and estimating the distribution of AdNNs' computational consumption.\nWe evaluate DeepPerform on three widely used datasets against five popular AdNN\nmodels. The results show that DeepPerform generates test samples that cause\nmore severe performance degradation (FLOPs: increase up to 552\\%). Furthermore,\nDeepPerform is substantially more efficient than the baseline methods in\ngenerating test inputs(runtime overhead: only 6-10 milliseconds).", "published": "2022-10-10 03:50:16", "link": "http://arxiv.org/abs/2210.05370v2", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in\n  Fine-tuned Source Code Models", "abstract": "Large code datasets have become increasingly accessible for pre-training\nsource code models. However, for the fine-tuning phase, obtaining\nrepresentative training data that fully covers the code distribution for\nspecific downstream tasks remains challenging due to the task-specific nature\nand limited labeling resources. Moreover, fine-tuning pretrained models can\nresult in forgetting previously acquired pre-training knowledge. These lead to\nout-of-distribution (OOD) generalization issues with unexpected model inference\nbehaviors that have not been systematically studied yet. In this paper, we\ncontribute the first systematic approach that simulates various OOD scenarios\nalong different dimensions of source code data properties and study the\nfine-tuned model behaviors in such scenarios. We investigate the behaviors of\nmodels under different fine-tuning methodologies, including full fine-tuning\nand Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis,\nconducted on four state-of-the-art pretrained models and applied to two code\ngeneration tasks, exposes multiple failure modes attributed to OOD\ngeneralization issues. Additionally, our analysis uncovers that LoRA\nfine-tuning consistently exhibits significantly better OOD generalization\nperformance than full fine-tuning across various scenarios.", "published": "2022-10-10 16:07:24", "link": "http://arxiv.org/abs/2210.04802v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Automated Audio Captioning via Fusion of Low- and High- Dimensional\n  Features", "abstract": "Automated audio captioning (AAC) aims to describe the content of an audio\nclip using simple sentences. Existing AAC methods are developed based on an\nencoder-decoder architecture that success is attributed to the use of a\npre-trained CNN10 called PANNs as the encoder to learn rich audio\nrepresentations. AAC is a highly challenging task due to its high-dimensional\ntalent space involves audio of various scenarios. Existing methods only use the\nhigh-dimensional representation of the PANNs as the input of the decoder.\nHowever, the low-dimension representation may retain as much audio information\nas the high-dimensional representation may be neglected. In addition, although\nthe high-dimensional approach may predict the audio captions by learning from\nexisting audio captions, which lacks robustness and efficiency. To deal with\nthese challenges, a fusion model which integrates low- and high-dimensional\nfeatures AAC framework is proposed. In this paper, a new encoder-decoder\nframework is proposed called the Low- and High-Dimensional Feature Fusion\n(LHDFF) model for AAC. Moreover, in LHDFF, a new PANNs encoder is proposed\ncalled Residual PANNs (RPANNs) by fusing the low-dimensional feature from the\nintermediate convolution layer output and the high-dimensional feature from the\nfinal layer output of PANNs. To fully explore the information of the low- and\nhigh-dimensional fusion feature and high-dimensional feature respectively, we\nproposed dual transformer decoder structures to generate the captions in\nparallel. Especially, a probabilistic fusion approach is proposed that can\nensure the overall performance of the system is improved by concentrating on\nthe respective advantages of the two transformer decoders. Experimental results\nshow that LHDFF achieves the best performance on the Clotho and AudioCaps\ndatasets compared with other existing models", "published": "2022-10-10 22:39:41", "link": "http://arxiv.org/abs/2210.05037v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Efficient-tuning Methods in Self-supervised Speech Models", "abstract": "In this study, we aim to explore efficient tuning methods for speech\nself-supervised learning. Recent studies show that self-supervised learning\n(SSL) can learn powerful representations for different speech tasks. However,\nfine-tuning pre-trained models for each downstream task is\nparameter-inefficient since SSL models are notoriously large with millions of\nparameters. Adapters are lightweight modules commonly used in NLP to solve this\nproblem. In downstream tasks, the parameters of SSL models are frozen, and only\nthe adapters are trained. Given the lack of studies generally exploring the\neffectiveness of adapters for self-supervised speech tasks, we intend to fill\nthis gap by adding various adapter modules in pre-trained speech SSL models. We\nshow that the performance parity can be achieved with over 90% parameter\nreduction, and discussed the pros and cons of efficient tuning techniques. This\nis the first comprehensive investigation of various adapter types across speech\ntasks.", "published": "2022-10-10 11:08:12", "link": "http://arxiv.org/abs/2210.06175v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
