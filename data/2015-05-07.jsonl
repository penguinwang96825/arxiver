{"title": "Contextual Analysis for Middle Eastern Languages with Hidden Markov\n  Models", "abstract": "Displaying a document in Middle Eastern languages requires contextual\nanalysis due to different presentational forms for each character of the\nalphabet. The words of the document will be formed by the joining of the\ncorrect positional glyphs representing corresponding presentational forms of\nthe characters. A set of rules defines the joining of the glyphs. As usual,\nthese rules vary from language to language and are subject to interpretation by\nthe software developers.\n  In this paper, we propose a machine learning approach for contextual analysis\nbased on the first order Hidden Markov Model. We will design and build a model\nfor the Farsi language to exhibit this technology. The Farsi model achieves 94\n\\% accuracy with the training based on a short list of 89 Farsi vocabularies\nconsisting of 2780 Farsi characters.\n  The experiment can be easily extended to many languages including Arabic,\nUrdu, and Sindhi. Furthermore, the advantage of this approach is that the same\nsoftware can be used to perform contextual analysis without coding complex\nrules for each specific language. Of particular interest is that the languages\nwith fewer speakers can have greater representation on the web, since they are\ntypically ignored by software developers due to lack of financial incentives.", "published": "2015-05-07 16:03:02", "link": "http://arxiv.org/abs/1505.01757v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models for Image Captioning: The Quirks and What Works", "abstract": "Two recent approaches have achieved state-of-the-art results in image\ncaptioning. The first uses a pipelined process where a set of candidate words\nis generated by a convolutional neural network (CNN) trained on images, and\nthen a maximum entropy (ME) language model is used to arrange these words into\na coherent sentence. The second uses the penultimate activation layer of the\nCNN as input to a recurrent neural network (RNN) that then generates the\ncaption sequence. In this paper, we compare the merits of these different\nlanguage modeling approaches for the first time by using the same\nstate-of-the-art CNN as input. We examine issues in the different approaches,\nincluding linguistic irregularities, caption repetition, and data set overlap.\nBy combining key aspects of the ME and RNN methods, we achieve a new record\nperformance over previously published results on the benchmark COCO dataset.\nHowever, the gains we see in BLEU do not translate to human judgments.", "published": "2015-05-07 18:36:14", "link": "http://arxiv.org/abs/1505.01809v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
