{"title": "Logical Semantics, Dialogical Argumentation, and Textual Entailment", "abstract": "In this chapter, we introduce a new dialogical system for first order\nclassical logic which is close to natural language argumentation, and we prove\nits completeness with respect to usual classical validity. We combine our\ndialogical system with the Grail syntactic and semantic parser developed by the\nsecond author in order to address automated textual entailment, that is, we use\nit for deciding whether or not a sentence is a consequence of a short text.\nThis work-which connects natural language semantics and argumentation with\ndialogical logic-can be viewed as a step towards an inferentialist view of\nnatural language semantics.", "published": "2020-08-17 08:04:11", "link": "http://arxiv.org/abs/2008.07138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Syntactic Parsers on Biomedical Texts", "abstract": "Syntactic parsing is an important step in the automated text analysis which\naims at information extraction. Quality of the syntactic parsing determines to\na large extent the recall and precision of the text mining results. In this\npaper we evaluate the performance of several popular syntactic parsers in\napplication to the biomedical text mining.", "published": "2020-08-17 10:07:23", "link": "http://arxiv.org/abs/2008.07189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BUT-FIT at SemEval-2020 Task 4: Multilingual commonsense", "abstract": "This paper describes work of the BUT-FIT's team at SemEval 2020 Task 4 -\nCommonsense Validation and Explanation. We participated in all three subtasks.\nIn subtasks A and B, our submissions are based on pretrained language\nrepresentation models (namely ALBERT) and data augmentation. We experimented\nwith solving the task for another language, Czech, by means of multilingual\nmodels and machine translated dataset, or translated model inputs. We show that\nwith a strong machine translation system, our system can be used in another\nlanguage with a small accuracy loss. In subtask C, our submission, which is\nbased on pretrained sequence-to-sequence model (BART), ranked 1st in BLEU score\nranking, however, we show that the correlation between BLEU and human\nevaluation, in which our submission ended up 4th, is low. We analyse the\nmetrics used in the evaluation and we propose an additional score based on\nmodel from subtask B, which correlates well with our manual ranking, as well as\nreranking method based on the same principle. We performed an error and dataset\nanalysis for all subtasks and we present our findings.", "published": "2020-08-17 12:45:39", "link": "http://arxiv.org/abs/2008.07259v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HunFlair: An Easy-to-Use Tool for State-of-the-Art Biomedical Named\n  Entity Recognition", "abstract": "Summary: Named Entity Recognition (NER) is an important step in biomedical\ninformation extraction pipelines. Tools for NER should be easy to use, cover\nmultiple entity types, highly accurate, and robust towards variations in text\ngenre and style. To this end, we propose HunFlair, an NER tagger covering\nmultiple entity types integrated into the widely used NLP framework Flair.\nHunFlair outperforms other state-of-the-art standalone NER tools with an\naverage gain of 7.26 pp over the next best tool, can be installed with a single\ncommand and is applied with only four lines of code. Availability: HunFlair is\nfreely available through the Flair framework under an MIT license:\nhttps://github.com/flairNLP/flair and is compatible with all major operating\nsystems. Contact:{weberple,saengema,alan.akbik}@informatik.hu-berlin.de", "published": "2020-08-17 14:16:15", "link": "http://arxiv.org/abs/2008.07347v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Narrative Interpolation for Generating and Understanding Stories", "abstract": "We propose a method for controlled narrative/story generation where we are\nable to guide the model to produce coherent narratives with user-specified\ntarget endings by interpolation: for example, we are told that Jim went hiking\nand at the end Jim needed to be rescued, and we want the model to incrementally\ngenerate steps along the way. The core of our method is an interpolation model\nbased on GPT-2 which conditions on a previous sentence and a next sentence in a\nnarrative and fills in the gap. Additionally, a reranker helps control for\ncoherence of the generated text. With human evaluation, we show that\nending-guided generation results in narratives which are coherent, faithful to\nthe given ending guide, and require less manual effort on the part of the human\nguide writer than past approaches.", "published": "2020-08-17 16:45:50", "link": "http://arxiv.org/abs/2008.07466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Carrier Recognition from Personal Narratives", "abstract": "Personal Narratives (PN) - recollections of facts, events, and thoughts from\none's own experience - are often used in everyday conversations. So far, PNs\nhave mainly been explored for tasks such as valence prediction or emotion\nclassification (e.g. happy, sad). However, these tasks might overlook more\nfine-grained information that could prove to be relevant for understanding PNs.\nIn this work, we propose a novel task for Narrative Understanding: Emotion\nCarrier Recognition (ECR). Emotion carriers, the text fragments that carry the\nemotions of the narrator (e.g. loss of a grandpa, high school reunion), provide\na fine-grained description of the emotion state. We explore the task of ECR in\na corpus of PNs manually annotated with emotion carriers and investigate\ndifferent machine learning models for the task. We propose evaluation\nstrategies for ECR including metrics that can be appropriate for different\ntasks.", "published": "2020-08-17 17:16:08", "link": "http://arxiv.org/abs/2008.07481v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stock Index Prediction with Multi-task Learning and Word Polarity Over\n  Time", "abstract": "Sentiment-based stock prediction systems aim to explore sentiment or event\nsignals from online corpora and attempt to relate the signals to stock price\nvariations. Both the feature-based and neural-networks-based approaches have\ndelivered promising results. However, the frequently minor fluctuations of the\nstock prices restrict learning the sentiment of text from price patterns, and\nlearning market sentiment from text can be biased if the text is irrelevant to\nthe underlying market. In addition, when using discrete word features, the\npolarity of a certain term can change over time according to different events.\nTo address these issues, we propose a two-stage system that consists of a\nsentiment extractor to extract the opinion on the market trend and a summarizer\nthat predicts the direction of the index movement of following week given the\nopinions of the news over the current week. We adopt BERT with multitask\nlearning which additionally predicts the worthiness of the news and propose a\nmetric called Polarity-Over-Time to extract the word polarity among different\nevent periods. A Weekly-Monday prediction framework and a new dataset, the\n10-year Reuters financial news dataset, are also proposed.", "published": "2020-08-17 20:22:56", "link": "http://arxiv.org/abs/2008.07605v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Active Learning for Text Classification using Deep Neural\n  Networks", "abstract": "Natural language processing (NLP) and neural networks (NNs) have both\nundergone significant changes in recent years. For active learning (AL)\npurposes, NNs are, however, less commonly used -- despite their current\npopularity. By using the superior text classification performance of NNs for\nAL, we can either increase a model's performance using the same amount of data\nor reduce the data and therefore the required annotation efforts while keeping\nthe same performance. We review AL for text classification using deep neural\nnetworks (DNNs) and elaborate on two main causes which used to hinder the\nadoption: (a) the inability of NNs to provide reliable uncertainty estimates,\non which the most commonly used query strategies rely, and (b) the challenge of\ntraining DNNs on small data. To investigate the former, we construct a taxonomy\nof query strategies, which distinguishes between data-based, model-based, and\nprediction-based instance selection, and investigate the prevalence of these\nclasses in recent research. Moreover, we review recent NN-based advances in NLP\nlike word embeddings or language models in the context of (D)NNs, survey the\ncurrent state-of-the-art at the intersection of AL, text classification, and\nDNNs and relate recent advances in NLP to AL. Finally, we analyze recent work\nin AL for text classification, connect the respective query strategies to the\ntaxonomy, and outline commonalities and shortcomings. As a result, we highlight\ngaps in current research and present open research questions.", "published": "2020-08-17 12:53:20", "link": "http://arxiv.org/abs/2008.07267v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating for Diversity in Question Generation over Text", "abstract": "Generating diverse and relevant questions over text is a task with widespread\napplications. We argue that commonly-used evaluation metrics such as BLEU and\nMETEOR are not suitable for this task due to the inherent diversity of\nreference questions, and propose a scheme for extending conventional metrics to\nreflect diversity. We furthermore propose a variational encoder-decoder model\nfor this task. We show through automatic and human evaluation that our\nvariational model improves diversity without loss of quality, and demonstrate\nhow our evaluation scheme reflects this improvement.", "published": "2020-08-17 13:16:12", "link": "http://arxiv.org/abs/2008.07291v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "PIANOTREE VAE: Structured Representation Learning for Polyphonic Music", "abstract": "The dominant approach for music representation learning involves the deep\nunsupervised model family variational autoencoder (VAE). However, most, if not\nall, viable attempts on this problem have largely been limited to monophonic\nmusic. Normally composed of richer modality and more complex musical\nstructures, the polyphonic counterpart has yet to be addressed in the context\nof music representation learning. In this work, we propose the PianoTree VAE, a\nnovel tree-structure extension upon VAE aiming to fit the polyphonic music\nlearning. The experiments prove the validity of the PianoTree VAE via\n(i)-semantically meaningful latent code for polyphonic segments; (ii)-more\nsatisfiable reconstruction aside of decent geometry learned in the latent\nspace; (iii)-this model's benefits to the variety of the downstream music\ngeneration.", "published": "2020-08-17 06:48:59", "link": "http://arxiv.org/abs/2008.07118v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Interpretable Representation for Controllable Polyphonic Music\n  Generation", "abstract": "While deep generative models have become the leading methods for algorithmic\ncomposition, it remains a challenging problem to control the generation process\nbecause the latent variables of most deep-learning models lack good\ninterpretability. Inspired by the content-style disentanglement idea, we design\na novel architecture, under the VAE framework, that effectively learns two\ninterpretable latent factors of polyphonic music: chord and texture. The\ncurrent model focuses on learning 8-beat long piano composition segments. We\nshow that such chord-texture disentanglement provides a controllable generation\npathway leading to a wide spectrum of applications, including compositional\nstyle transfer, texture variation, and accompaniment arrangement. Both\nobjective and subjective evaluations show that our method achieves a successful\ndisentanglement and high quality controlled music generation.", "published": "2020-08-17 07:11:16", "link": "http://arxiv.org/abs/2008.07122v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad\n  Creative Refinement", "abstract": "In the online advertising industry, the process of designing an ad creative\n(i.e., ad text and image) requires manual labor. Typically, each advertiser\nlaunches multiple creatives via online A/B tests to infer effective creatives\nfor the target audience, that are then refined further in an iterative fashion.\nDue to the manual nature of this process, it is time-consuming to learn,\nrefine, and deploy the modified creatives. Since major ad platforms typically\nrun A/B tests for multiple advertisers in parallel, we explore the possibility\nof collaboratively learning ad creative refinement via A/B tests of multiple\nadvertisers. In particular, given an input ad creative, we study approaches to\nrefine the given ad text and image by: (i) generating new ad text, (ii)\nrecommending keyphrases for new ad text, and (iii) recommending image tags\n(objects in image) to select new ad image. Based on A/B tests conducted by\nmultiple advertisers, we form pairwise examples of inferior and superior ad\ncreatives, and use such pairs to train models for the above tasks. For\ngenerating new ad text, we demonstrate the efficacy of an encoder-decoder\narchitecture with copy mechanism, which allows some words from the (inferior)\ninput text to be copied to the output while incorporating new words associated\nwith higher click-through-rate. For the keyphrase and image tag recommendation\ntask, we demonstrate the efficacy of a deep relevance matching model, as well\nas the relative robustness of ranking approaches compared to ad text generation\nin cold-start scenarios with unseen advertisers. We also share broadly\napplicable insights from our experiments using data from the Yahoo Gemini ad\nplatform.", "published": "2020-08-17 16:46:28", "link": "http://arxiv.org/abs/2008.07467v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do face masks introduce bias in speech technologies? The case of\n  automated scoring of speaking proficiency", "abstract": "The COVID-19 pandemic has led to a dramatic increase in the use of face masks\nworldwide. Face coverings can affect both acoustic properties of the signal as\nwell as speech patterns and have unintended effects if the person wearing the\nmask attempts to use speech processing technologies. In this paper we explore\nthe impact of wearing face masks on the automated assessment of English\nlanguage proficiency. We use a dataset from a large-scale speaking test for\nwhich test-takers were required to wear face masks during the test\nadministration, and we compare it to a matched control sample of test-takers\nwho took the same test before the mask requirements were put in place. We find\nthat the two samples differ across a range of acoustic measures and also show a\nsmall but significant difference in speech patterns. However, these differences\ndo not lead to differences in human or automated scores of English language\nproficiency. Several measures of bias showed no differences in scores between\nthe two groups.", "published": "2020-08-17 17:58:29", "link": "http://arxiv.org/abs/2008.07520v2", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Resolving Intent Ambiguities by Retrieving Discriminative Clarifying\n  Questions", "abstract": "Task oriented Dialogue Systems generally employ intent detection systems in\norder to map user queries to a set of pre-defined intents. However, user\nqueries appearing in natural language can be easily ambiguous and hence such a\ndirect mapping might not be straightforward harming intent detection and\neventually the overall performance of a dialogue system. Moreover, acquiring\ndomain-specific clarification questions is costly. In order to disambiguate\nqueries which are ambiguous between two intents, we propose a novel method of\ngenerating discriminative questions using a simple rule based system which can\ntake advantage of any question generation system without requiring annotated\ndata of clarification questions. Our approach aims at discrimination between\ntwo intents but can be easily extended to clarification over multiple intents.\nSeeking clarification from the user to classify user intents not only helps\nunderstand the user intent effectively, but also reduces the roboticity of the\nconversation and makes the interaction considerably natural.", "published": "2020-08-17 18:11:13", "link": "http://arxiv.org/abs/2008.07559v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Generative Models are Unsupervised Predictors of Page Quality: A\n  Colossal-Scale Study", "abstract": "Large generative language models such as GPT-2 are well-known for their\nability to generate text as well as their utility in supervised downstream\ntasks via fine-tuning. Our work is twofold: firstly we demonstrate via human\nevaluation that classifiers trained to discriminate between human and\nmachine-generated text emerge as unsupervised predictors of \"page quality\",\nable to detect low quality content without any training. This enables fast\nbootstrapping of quality indicators in a low-resource setting. Secondly,\ncurious to understand the prevalence and nature of low quality pages in the\nwild, we conduct extensive qualitative and quantitative analysis over 500\nmillion web articles, making this the largest-scale study ever conducted on the\ntopic.", "published": "2020-08-17 07:13:24", "link": "http://arxiv.org/abs/2008.13533v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Predictive Model: From Prosody To Communicative\n  Gestures", "abstract": "Communicative gestures and speech acoustic are tightly linked. Our objective\nis to predict the timing of gestures according to the acoustic. That is, we\nwant to predict when a certain gesture occurs. We develop a model based on a\nrecurrent neural network with attention mechanism. The model is trained on a\ncorpus of natural dyadic interaction where the speech acoustic and the gesture\nphases and types have been annotated. The input of the model is a sequence of\nspeech acoustic and the output is a sequence of gesture classes. The classes we\nare using for the model output is based on a combination of gesture phases and\ngesture types. We use a sequence comparison technique to evaluate the model\nperformance. We find that the model can predict better certain gesture classes\nthan others. We also perform ablation studies which reveal that fundamental\nfrequency is a relevant feature for gesture prediction task. In another\nsub-experiment, we find that including eyebrow movements as acting as beat\ngesture improves the performance. Besides, we also find that a model trained on\nthe data of one given speaker also works for the other speaker of the same\nconversation. We also perform a subjective experiment to measure how\nrespondents judge the naturalness, the time consistency, and the semantic\nconsistency of the generated gesture timing of a virtual agent. Our respondents\nrate the output of our model favorably.", "published": "2020-08-17 21:55:22", "link": "http://arxiv.org/abs/2008.07643v2", "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Exploiting Fully Convolutional Network and Visualization Techniques on\n  Spontaneous Speech for Dementia Detection", "abstract": "In this paper, we exploit a Fully Convolutional Network (FCN) to analyze the\naudio data of spontaneous speech for dementia detection. A fully convolutional\nnetwork accommodates speech samples with varying lengths, thus enabling us to\nanalyze the speech sample without manual segmentation. Specifically, we first\nobtain the Mel Frequency Cepstral Coefficient (MFCC) feature map from each\nparticipant's audio data and convert the speech classification task on audio\ndata to an image classification task on MFCC feature maps. Then, to solve the\ndata insufficiency problem, we apply transfer learning by adopting a\npre-trained backbone Convolutional Neural Network (CNN) model from the\nMobileNet architecture and the ImageNet dataset. We further build a\nconvolutional layer to produce a heatmap using Otsu's method for visualization,\nenabling us to understand the impact of the time-series audio segments on the\nclassification results. We demonstrate that our classification model achieves\n66.7% over the testing dataset, 62.5% of the baseline model provided in the\nADReSS challenge. Through the visualization technique, we can evaluate the\nimpact of audio segments, such as filled pauses from the participants and\nrepeated questions from the investigator, on the classification results.", "published": "2020-08-17 01:37:39", "link": "http://arxiv.org/abs/2008.07052v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Task Learning for Interpretable Weakly Labelled Sound Event\n  Detection", "abstract": "Weakly Labelled learning has garnered lot of attention in recent years due to\nits potential to scale Sound Event Detection (SED) and is formulated as\nMultiple Instance Learning (MIL) problem. This paper proposes a Multi-Task\nLearning (MTL) framework for learning from Weakly Labelled Audio data which\nencompasses the traditional MIL setup. To show the utility of proposed\nframework, we use the input TimeFrequency representation (T-F) reconstruction\nas the auxiliary task. We show that the chosen auxiliary task de-noises\ninternal T-F representation and improves SED performance under noisy\nrecordings. Our second contribution is introducing two step Attention Pooling\nmechanism. By having 2-steps in attention mechanism, the network retains better\nT-F level information without compromising SED performance. The visualisation\nof first step and second step attention weights helps in localising the\naudio-event in T-F domain. For evaluating the proposed framework, we remix the\nDCASE 2019 task 1 acoustic scene data with DCASE 2018 Task 2 sounds event data\nunder 0, 10 and 20 db SNR resulting in a multi-class Weakly labelled SED\nproblem. The proposed total framework outperforms existing benchmark models\nover all SNRs, specifically 22.3 %, 12.8 %, 5.9 % improvement over benchmark\nmodel on 0, 10 and 20 dB SNR respectively. We carry out ablation study to\ndetermine the contribution of each auxiliary task and 2-step Attention Pooling\nto the SED performance improvement. The code is publicly released", "published": "2020-08-17 04:46:25", "link": "http://arxiv.org/abs/2008.07085v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "POP909: A Pop-song Dataset for Music Arrangement Generation", "abstract": "Music arrangement generation is a subtask of automatic music generation,\nwhich involves reconstructing and re-conceptualizing a piece with new\ncompositional techniques. Such a generation process inevitably requires\nreference from the original melody, chord progression, or other structural\ninformation. Despite some promising models for arrangement, they lack more\nrefined data to achieve better evaluations and more practical results. In this\npaper, we propose POP909, a dataset which contains multiple versions of the\npiano arrangements of 909 popular songs created by professional musicians. The\nmain body of the dataset contains the vocal melody, the lead instrument melody,\nand the piano accompaniment for each song in MIDI format, which are aligned to\nthe original audio files. Furthermore, we provide the annotations of tempo,\nbeat, key, and chords, where the tempo curves are hand-labeled and others are\ndone by MIR algorithms. Finally, we conduct several baseline experiments with\nthis dataset using standard deep music generation algorithms.", "published": "2020-08-17 08:08:14", "link": "http://arxiv.org/abs/2008.07142v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Variational Generative Models for Audio-visual Speech Separation", "abstract": "In this paper, we are interested in audio-visual speech separation given a\nsingle-channel audio recording as well as visual information (lips movements)\nassociated with each speaker. We propose an unsupervised technique based on\naudio-visual generative modeling of clean speech. More specifically, during\ntraining, a latent variable generative model is learned from clean speech\nspectrograms using a variational auto-encoder (VAE). To better utilize the\nvisual information, the posteriors of the latent variables are inferred from\nmixed speech (instead of clean speech) as well as the visual data. The visual\nmodality also serves as a prior for latent variables, through a visual network.\nAt test time, the learned generative model (both for speaker-independent and\nspeaker-dependent scenarios) is combined with an unsupervised non-negative\nmatrix factorization (NMF) variance model for background noise. All the latent\nvariables and noise parameters are then estimated by a Monte Carlo\nexpectation-maximization algorithm. Our experiments show that the proposed\nunsupervised VAE-based method yields better separation performance than\nNMF-based approaches as well as a supervised deep learning-based technique.", "published": "2020-08-17 10:12:33", "link": "http://arxiv.org/abs/2008.07191v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "StoRIR: Stochastic Room Impulse Response Generation for Audio Data\n  Augmentation", "abstract": "In this paper we introduce StoRIR - a stochastic room impulse response\ngeneration method dedicated to audio data augmentation in machine learning\napplications. This technique, in contrary to geometrical methods like\nimage-source or ray tracing, does not require prior definition of room\ngeometry, absorption coefficients or microphone and source placement and is\ndependent solely on the acoustic parameters of the room. The method is\nintuitive, easy to implement and allows to generate RIRs of very complicated\nenclosures. We show that StoRIR, when used for audio data augmentation in a\nspeech enhancement task, allows deep learning models to achieve better results\non a wide range of metrics than when using the conventional image-source\nmethod, effectively improving many of them by more than 5 %. We publish a\nPython implementation of StoRIR online", "published": "2020-08-17 11:56:47", "link": "http://arxiv.org/abs/2008.07231v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Low-Latency Speech Enhancement with Mobile Audio Streaming\n  Networks", "abstract": "We propose Mobile Audio Streaming Networks (MASnet) for efficient low-latency\nspeech enhancement, which is particularly suitable for mobile devices and other\napplications where computational capacity is a limitation. MASnet processes\nlinear-scale spectrograms, transforming successive noisy frames into\ncomplex-valued ratio masks which are then applied to the respective noisy\nframes. MASnet can operate in a low-latency incremental inference mode which\nmatches the complexity of layer-by-layer batch mode. Compared to a similar\nfully-convolutional architecture, MASnet incorporates depthwise and pointwise\nconvolutions for a large reduction in fused multiply-accumulate operations per\nsecond (FMA/s), at the cost of some reduction in SNR.", "published": "2020-08-17 12:18:34", "link": "http://arxiv.org/abs/2008.07244v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Deep Learning Based Open Set Acoustic Scene Classification", "abstract": "In this work, we compare the performance of three selected techniques in open\nset acoustic scenes classification (ASC). We test thresholding of the softmax\noutput of a deep network classifier, which is the most popular technique\nnowadays employed in ASC. Further we compare the results with the Openmax\nclassifier which is derived from the computer vision field. As the third model,\nwe use the Adapted Class-Conditioned Autoencoder (Adapted C2AE) which is our\nvariation of another computer vision related technique called C2AE. Adapted\nC2AE encompasses a more fair comparison of the given experiments and simplifies\nthe original inference procedure, making it more applicable in the real-life\nscenarios. We also analyse two training scenarios: without additional knowledge\nof unknown classes and another where a limited subset of examples from the\nunknown classes is available. We find that the C2AE based method outperforms\nthe thresholding and Openmax, obtaining $85.5\\%$ Area Under the Receiver\nOperating Characteristic curve (AUROC) and $66\\%$ of open set accuracy on data\nused in Detection and Classification of Acoustic Scenes and Events Challenge\n2019 Task 1C.", "published": "2020-08-17 12:23:27", "link": "http://arxiv.org/abs/2008.07247v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music Boundary Detection using Convolutional Neural Networks: A\n  comparative analysis of combined input features", "abstract": "The analysis of the structure of musical pieces is a task that remains a\nchallenge for Artificial Intelligence, especially in the field of Deep\nLearning. It requires prior identification of structural boundaries of the\nmusic pieces. This structural boundary analysis has recently been studied with\nunsupervised methods and \\textit{end-to-end} techniques such as Convolutional\nNeural Networks (CNN) using Mel-Scaled Log-magnitude Spectograms features\n(MLS), Self-Similarity Matrices (SSM) or Self-Similarity Lag Matrices (SSLM) as\ninputs and trained with human annotations. Several studies have been published\ndivided into unsupervised and \\textit{end-to-end} methods in which\npre-processing is done in different ways, using different distance metrics and\naudio characteristics, so a generalized pre-processing method to compute model\ninputs is missing. The objective of this work is to establish a general method\nof pre-processing these inputs by comparing the inputs calculated from\ndifferent pooling strategies, distance metrics and audio characteristics, also\ntaking into account the computing time to obtain them. We also establish the\nmost effective combination of inputs to be delivered to the CNN in order to\nestablish the most efficient way to extract the limits of the structure of the\nmusic pieces. With an adequate combination of input matrices and pooling\nstrategies we obtain a measurement accuracy $F_1$ of 0.411 that outperforms the\ncurrent one obtained under the same conditions.", "published": "2020-08-17 14:20:51", "link": "http://arxiv.org/abs/2008.07527v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Learning Based Source Separation Applied To Choir Ensembles", "abstract": "Choral singing is a widely practiced form of ensemble singing wherein a group\nof people sing simultaneously in polyphonic harmony. The most commonly\npracticed setting for choir ensembles consists of four parts; Soprano, Alto,\nTenor and Bass (SATB), each with its own range of fundamental frequencies\n(F$0$s). The task of source separation for this choral setting entails\nseparating the SATB mixture into the constituent parts. Source separation for\nmusical mixtures is well studied and many deep learning based methodologies\nhave been proposed for the same. However, most of the research has been focused\non a typical case which consists in separating vocal, percussion and bass\nsources from a mixture, each of which has a distinct spectral structure. In\ncontrast, the simultaneous and harmonic nature of ensemble singing leads to\nhigh structural similarity and overlap between the spectral components of the\nsources in a choral mixture, making source separation for choirs a harder task\nthan the typical case. This, along with the lack of an appropriate consolidated\ndataset has led to a dearth of research in the field so far. In this paper we\nfirst assess how well some of the recently developed methodologies for musical\nsource separation perform for the case of SATB choirs. We then propose a novel\ndomain-specific adaptation for conditioning the recently proposed U-Net\narchitecture for musical source separation using the fundamental frequency\ncontour of each of the singing groups and demonstrate that our proposed\napproach surpasses results from domain-agnostic architectures.", "published": "2020-08-17 22:07:44", "link": "http://arxiv.org/abs/2008.07645v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
