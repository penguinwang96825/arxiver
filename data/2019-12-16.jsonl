{"title": "Characterizing the dynamics of learning in repeated reference games", "abstract": "The language we use over the course of conversation changes as we establish\ncommon ground and learn what our partner finds meaningful. Here we draw upon\nrecent advances in natural language processing to provide a finer-grained\ncharacterization of the dynamics of this learning process. We release an open\ncorpus (>15,000 utterances) of extended dyadic interactions in a classic\nrepeated reference game task where pairs of participants had to coordinate on\nhow to refer to initially difficult-to-describe tangram stimuli. We find that\ndifferent pairs discover a wide variety of idiosyncratic but efficient and\nstable solutions to the problem of reference. Furthermore, these conventions\nare shaped by the communicative context: words that are more discriminative in\nthe initial context (i.e. that are used for one target more than others) are\nmore likely to persist through the final repetition. Finally, we find\nsystematic structure in how a speaker's referring expressions become more\nefficient over time: syntactic units drop out in clusters following positive\nfeedback from the listener, eventually leaving short labels containing\nopen-class parts of speech. These findings provide a higher resolution look at\nthe quantitative dynamics of ad hoc convention formation and support further\ndevelopment of computational models of learning in communication.", "published": "2019-12-16 05:25:07", "link": "http://arxiv.org/abs/1912.07199v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-based Neural Sentence Ordering", "abstract": "Sentence ordering is to restore the original paragraph from a set of\nsentences. It involves capturing global dependencies among sentences regardless\nof their input order. In this paper, we propose a novel and flexible\ngraph-based neural sentence ordering model, which adopts graph recurrent\nnetwork \\cite{Zhang:acl18} to accurately learn semantic representations of the\nsentences. Instead of assuming connections between all pairs of input\nsentences, we use entities that are shared among multiple sentences to make\nmore expressive graph representations with less noise. Experimental results\nshow that our proposed model outperforms the existing state-of-the-art systems\non several benchmark datasets, demonstrating the effectiveness of our model. We\nalso conduct a thorough analysis on how entities help the performance.", "published": "2019-12-16 07:23:21", "link": "http://arxiv.org/abs/1912.07225v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Dual Domain Adaptation for Neural Machine Translation", "abstract": "Previous studies on the domain adaptation for neural machine translation\n(NMT) mainly focus on the one-pass transferring out-of-domain translation\nknowledge to in-domain NMT model. In this paper, we argue that such a strategy\nfails to fully extract the domain-shared translation knowledge, and repeatedly\nutilizing corpora of different domains can lead to better distillation of\ndomain-shared translation knowledge. To this end, we propose an iterative dual\ndomain adaptation framework for NMT. Specifically, we first pre-train in-domain\nand out-of-domain NMT models using their own training corpora respectively, and\nthen iteratively perform bidirectional translation knowledge transfer (from\nin-domain to out-of-domain and then vice versa) based on knowledge distillation\nuntil the in-domain NMT model convergences. Furthermore, we extend the proposed\nframework to the scenario of multiple out-of-domain training corpora, where the\nabove-mentioned transfer is performed sequentially between the in-domain and\neach out-of-domain NMT models in the ascending order of their domain\nsimilarities. Empirical results on Chinese-English and English-German\ntranslation tasks demonstrate the effectiveness of our framework.", "published": "2019-12-16 08:21:28", "link": "http://arxiv.org/abs/1912.07239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Knowledge-aware Dialogue Generation via Knowledge Base\n  Question Answering", "abstract": "Neural network models usually suffer from the challenge of incorporating\ncommonsense knowledge into the open-domain dialogue systems. In this paper, we\npropose a novel knowledge-aware dialogue generation model (called TransDG),\nwhich transfers question representation and knowledge matching abilities from\nknowledge base question answering (KBQA) task to facilitate the utterance\nunderstanding and factual knowledge selection for dialogue generation. In\naddition, we propose a response guiding attention and a multi-step decoding\nstrategy to steer our model to focus on relevant features for response\ngeneration. Experiments on two benchmark datasets demonstrate that our model\nhas robust superiority over compared methods in generating informative and\nfluent dialogues. Our code is available at https://github.com/siat-nlp/TransDG.", "published": "2019-12-16 16:39:01", "link": "http://arxiv.org/abs/1912.07491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scale-dependent Relationships in Natural Language", "abstract": "Natural language exhibits statistical dependencies at a wide range of scales.\nFor instance, the mutual information between words in natural language decays\nlike a power law with the temporal lag between them. However, many statistical\nlearning models applied to language impose a sampling scale while extracting\nstatistical structure. For instance, Word2Vec constructs a vector embedding\nthat maximizes the prediction between a target word and the context words that\nappear nearby in the corpus. The size of the context is chosen by the user and\ndefines a strong scale; relationships over much larger temporal scales would be\ninvisible to the algorithm. This paper examines the family of Word2Vec\nembeddings generated while systematically manipulating the sampling scale used\nto define the context around each word. The primary result is that different\nlinguistic relationships are preferentially encoded at different scales.\nDifferent scales emphasize different syntactic and semantic relations between\nwords.Moreover, the neighborhoods of a given word in the embeddings change\nsignificantly depending on the scale. These results suggest that any individual\nscale can only identify a subset of the meaningful relationships a word might\nhave, and point toward the importance of developing scale-free models of\nsemantic meaning.", "published": "2019-12-16 17:12:00", "link": "http://arxiv.org/abs/1912.07506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synchronous Speech Recognition and Speech-to-Text Translation with\n  Interactive Decoding", "abstract": "Speech-to-text translation (ST), which translates source language speech into\ntarget language text, has attracted intensive attention in recent years.\nCompared to the traditional pipeline system, the end-to-end ST model has\npotential benefits of lower latency, smaller model size, and less error\npropagation. However, it is notoriously difficult to implement such a model\nwithout transcriptions as intermediate. Existing works generally apply\nmulti-task learning to improve translation quality by jointly training\nend-to-end ST along with automatic speech recognition (ASR). However, different\ntasks in this method cannot utilize information from each other, which limits\nthe improvement. Other works propose a two-stage model where the second model\ncan use the hidden state from the first one, but its cascade manner greatly\naffects the efficiency of training and inference process. In this paper, we\npropose a novel interactive attention mechanism which enables ASR and ST to\nperform synchronously and interactively in a single model. Specifically, the\ngeneration of transcriptions and translations not only relies on its previous\noutputs but also the outputs predicted in the other task. Experiments on TED\nspeech translation corpora have shown that our proposed model can outperform\nstrong baselines on the quality of speech translation and achieve better speech\nrecognition performances as well.", "published": "2019-12-16 08:22:43", "link": "http://arxiv.org/abs/1912.07240v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimized Tracking of Topic Evolution", "abstract": "Topic evolution modeling has been researched for a long time and has gained\nconsiderable interest. A state-of-the-art method has been recently using word\nmodeling algorithms in combination with community detection mechanisms to\nachieve better results in a more effective way. We analyse results of this\napproach and discuss the two major challenges that this approach still faces.\nAlthough the topics that have resulted from the recent algorithm are good in\ngeneral, they are very noisy due to many topics that are very unimportant\nbecause of their size, words, or ambiguity. Additionally, the number of words\ndefining each topic is too large, making it difficult to analyse them in their\nunsorted state. In this paper, we propose approaches to tackle these challenges\nby adding topic filtering and network analysis metrics to define the importance\nof a topic. We test different combinations of these metrics to see which\ncombination yields the best results. Furthermore, we add word filtering and\nranking to each topic to identify the words with the highest novelty\nautomatically. We evaluate our enhancement methods in two ways: human\nqualitative evaluation and automatic quantitative evaluation. Moreover, we\ncreated two case studies to test the quality of the clusters and words. In the\nquantitative evaluation, we use the pairwise mutual information score to test\nthe coherency of topics. The quantitative evaluation also includes an analysis\nof execution times for each part of the program. The results of the\nexperimental evaluations show that the two evaluation methods agree on the\npositive feasibility of the algorithm. We then show possible extensions in the\nform of usability and future improvements to the algorithm.", "published": "2019-12-16 14:43:11", "link": "http://arxiv.org/abs/1912.07419v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Predicting detection filters for small footprint open-vocabulary keyword\n  spotting", "abstract": "In this paper, we propose a fully-neural approach to open-vocabulary keyword\nspotting, that allows the users to include a customizable voice interface to\ntheir device and that does not require task-specific data. We present a keyword\ndetection neural network weighing less than 250KB, in which the topmost layer\nperforming keyword detection is predicted by an auxiliary network, that may be\nrun offline to generate a detector for any keyword. We show that the proposed\nmodel outperforms acoustic keyword spotting baselines by a large margin on two\ntasks of detecting keywords in utterances and three tasks of detecting isolated\nspeech commands. We also propose a method to fine-tune the model when specific\ntraining data is available for some keywords, which yields a performance\nsimilar to a standard speech command neural network while keeping the ability\nof the model to be applied to new keywords.", "published": "2019-12-16 18:41:49", "link": "http://arxiv.org/abs/1912.07575v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic Similarity To Improve Question Understanding in a Virtual\n  Patient", "abstract": "In medicine, a communicating virtual patient or doctor allows students to\ntrain in medical diagnosis and develop skills to conduct a medical\nconsultation. In this paper, we describe a conversational virtual standardized\npatient system to allow medical students to simulate a diagnosis strategy of an\nabdominal surgical emergency. We exploited the semantic properties captured by\ndistributed word representations to search for similar questions in the virtual\npatient dialogue system. We created two dialogue systems that were evaluated on\ndatasets collected during tests with students. The first system based on\nhand-crafted rules obtains $92.29\\%$ as $F1$-score on the studied clinical case\nwhile the second system that combines rules and semantic similarity achieves\n$94.88\\%$. It represents an error reduction of $9.70\\%$ as compared to the\nrules-only-based system.", "published": "2019-12-16 14:45:56", "link": "http://arxiv.org/abs/1912.07421v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Image Manipulation with Natural Language using Two-sidedAttentive\n  Conditional Generative Adversarial Network", "abstract": "Altering the content of an image with photo editing tools is a tedious task\nfor an inexperienced user. Especially, when modifying the visual attributes of\na specific object in an image without affecting other constituents such as\nbackground etc. To simplify the process of image manipulation and to provide\nmore control to users, it is better to utilize a simpler interface like natural\nlanguage. Therefore, in this paper, we address the challenge of manipulating\nimages using natural language description. We propose the Two-sidEd Attentive\nconditional Generative Adversarial Network (TEA-cGAN) to generate semantically\nmanipulated images while preserving other contents such as background intact.\nTEA-cGAN uses fine-grained attention both in the generator and discriminator of\nGenerative Adversarial Network (GAN) based framework at different scales.\nExperimental results show that TEA-cGAN which generates 128x128 and 256x256\nresolution images outperforms existing methods on CUB and Oxford-102 datasets\nboth quantitatively and qualitatively.", "published": "2019-12-16 16:21:13", "link": "http://arxiv.org/abs/1912.07478v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Towards Causal VQA: Revealing and Reducing Spurious Correlations by\n  Invariant and Covariant Semantic Editing", "abstract": "Despite significant success in Visual Question Answering (VQA), VQA models\nhave been shown to be notoriously brittle to linguistic variations in the\nquestions. Due to deficiencies in models and datasets, today's models often\nrely on correlations rather than predictions that are causal w.r.t. data. In\nthis paper, we propose a novel way to analyze and measure the robustness of the\nstate of the art models w.r.t semantic visual variations as well as propose\nways to make models more robust against spurious correlations. Our method\nperforms automated semantic image manipulations and tests for consistency in\nmodel predictions to quantify the model robustness as well as generate\nsynthetic data to counter these problems. We perform our analysis on three\ndiverse, state of the art VQA models and diverse question types with a\nparticular focus on challenging counting questions. In addition, we show that\nmodels can be made significantly more robust against inconsistent predictions\nusing our edited data. Finally, we show that results also translate to\nreal-world error cases of state of the art models, which results in improved\noverall performance.", "published": "2019-12-16 17:45:01", "link": "http://arxiv.org/abs/1912.07538v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Pipelines for Procedural Information Extraction from Scientific\n  Literature: Towards Recipes using Machine Learning and Data Science", "abstract": "This paper describes a machine learning and data science pipeline for\nstructured information extraction from documents, implemented as a suite of\nopen-source tools and extensions to existing tools. It centers around a\nmethodology for extracting procedural information in the form of recipes,\nstepwise procedures for creating an artifact (in this case synthesizing a\nnanomaterial), from published scientific literature. From our overall goal of\nproducing recipes from free text, we derive the technical objectives of a\nsystem consisting of pipeline stages: document acquisition and filtering,\npayload extraction, recipe step extraction as a relationship extraction task,\nrecipe assembly, and presentation through an information retrieval interface\nwith question answering (QA) functionality. This system meets computational\ninformation and knowledge management (CIKM) requirements of metadata-driven\npayload extraction, named entity extraction, and relationship extraction from\ntext. Functional contributions described in this paper include semi-supervised\nmachine learning methods for PDF filtering and payload extraction tasks,\nfollowed by structured extraction and data transformation tasks beginning with\nsection extraction, recipe steps as information tuples, and finally assembled\nrecipes. Measurable objective criteria for extraction quality include precision\nand recall of recipe steps, ordering constraints, and QA accuracy, precision,\nand recall. Results, key novel contributions, and significant open problems\nderived from this work center around the attribution of these holistic quality\nmeasures to specific machine learning and inference stages of the pipeline,\neach with their performance measures. The desired recipes contain identified\npreconditions, material inputs, and operations, and constitute the overall\noutput generated by our computational information and knowledge management\n(CIKM) system.", "published": "2019-12-16 23:04:03", "link": "http://arxiv.org/abs/1912.07747v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "I.2.7, I.2.6, H.3.3, H.3.4, I.2.10, I.5.4", "I.2.7; I.2.6; H.3.3; H.3.4; I.2.10; I.5.4"], "primary_category": "cs.IR"}
{"title": "Predicting the Outcome of Judicial Decisions made by the European Court\n  of Human Rights", "abstract": "In this study, machine learning models were constructed to predict whether\njudgments made by the European Court of Human Rights (ECHR) would lead to a\nviolation of an Article in the Convention on Human Rights. The problem is\nframed as a binary classification task where a judgment can lead to a\n\"violation\" or \"non-violation\" of a particular Article. Using auto-sklearn, an\nautomated algorithm selection package, models were constructed for 12 Articles\nin the Convention. To train these models, textual features were obtained from\nthe ECHR Judgment documents using N-grams, word embeddings and paragraph\nembeddings. Additional documents, from the ECHR, were incorporated into the\nmodels through the creation of a word embedding (echr2vec) and a doc2vec model.\nThe features obtained using the echr2vec embedding provided the highest\ncross-validation accuracy for 5 of the Articles. The overall test accuracy,\nacross the 12 Articles, was 68.83%. As far as we could tell, this is the first\nestimate of the accuracy of such machine learning models using a realistic test\nset. This provides an important benchmark for future work. As a baseline, a\nsimple heuristic of always predicting the most common outcome in the past was\nused. The heuristic achieved an overall test accuracy of 86.68% which is 29.7%\nhigher than the models. Again, this was seemingly the first study that included\nsuch a heuristic with which to compare model results. The higher accuracy\nachieved by the heuristic highlights the importance of including such a\nbaseline.", "published": "2019-12-16 15:42:30", "link": "http://arxiv.org/abs/1912.10819v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Continuous Speech Recognition using EEG and Video", "abstract": "In this paper we investigate whether electroencephalography (EEG) features\ncan be used to improve the performance of continuous visual speech recognition\nsystems. We implemented a connectionist temporal classification (CTC) based\nend-to-end automatic speech recognition (ASR) model for performing recognition.\nOur results demonstrate that EEG features are helpful in enhancing the\nperformance of continuous visual speech recognition systems.", "published": "2019-12-16 22:16:19", "link": "http://arxiv.org/abs/1912.07730v5", "categories": ["cs.LG", "eess.AS", "eess.IV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Data augmentation approaches for improving animal audio classification", "abstract": "In this paper we present ensembles of classifiers for automated animal audio\nclassification, exploiting different data augmentation techniques for training\nConvolutional Neural Networks (CNNs). The specific animal audio classification\nproblems are i) birds and ii) cat sounds, whose datasets are freely available.\nWe train five different CNNs on the original datasets and on their versions\naugmented by four augmentation protocols, working on the raw audio signals or\ntheir representations as spectrograms. We compared our best approaches with the\nstate of the art, showing that we obtain the best recognition rate on the same\ndatasets, without ad hoc parameter optimization. Our study shows that different\nCNNs can be trained for the purpose of animal audio classification and that\ntheir fusion works better than the stand-alone classifiers. To the best of our\nknowledge this is the largest study on data augmentation for CNNs in animal\naudio classification audio datasets using the same set of classifiers and\nparameters. Our MATLAB code is available at https://github.com/LorisNanni.", "published": "2019-12-16 23:30:42", "link": "http://arxiv.org/abs/1912.07756v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
