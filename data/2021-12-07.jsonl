{"title": "Improving Neural Cross-Lingual Summarization via Employing Optimal\n  Transport Distance for Knowledge Distillation", "abstract": "Current state-of-the-art cross-lingual summarization models employ multi-task\nlearning paradigm, which works on a shared vocabulary module and relies on the\nself-attention mechanism to attend among tokens in two languages. However,\ncorrelation learned by self-attention is often loose and implicit, inefficient\nin capturing crucial cross-lingual representations between languages. The\nmatter worsens when performing on languages with separate morphological or\nstructural features, making the cross-lingual alignment more challenging,\nresulting in the performance drop. To overcome this problem, we propose a novel\nKnowledge-Distillation-based framework for Cross-Lingual Summarization, seeking\nto explicitly construct cross-lingual correlation by distilling the knowledge\nof the monolingual summarization teacher into the cross-lingual summarization\nstudent. Since the representations of the teacher and the student lie on two\ndifferent vector spaces, we further propose a Knowledge Distillation loss using\nSinkhorn Divergence, an Optimal-Transport distance, to estimate the discrepancy\nbetween those teacher and student representations. Due to the intuitively\ngeometric nature of Sinkhorn Divergence, the student model can productively\nlearn to align its produced cross-lingual hidden states with monolingual hidden\nstates, hence leading to a strong correlation between distant languages.\nExperiments on cross-lingual summarization datasets in pairs of distant\nlanguages demonstrate that our method outperforms state-of-the-art models under\nboth high and low-resourced settings.", "published": "2021-12-07 03:45:02", "link": "http://arxiv.org/abs/2112.03473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dataset Geography: Mapping Language Data to Language Users", "abstract": "As language technologies become more ubiquitous, there are increasing efforts\ntowards expanding the language diversity and coverage of natural language\nprocessing (NLP) systems. Arguably, the most important factor influencing the\nquality of modern NLP systems is data availability. In this work, we study the\ngeographical representativeness of NLP datasets, aiming to quantify if and by\nhow much do NLP datasets match the expected needs of the language speakers. In\ndoing so, we use entity recognition and linking systems, also making important\nobservations about their cross-lingual consistency and giving suggestions for\nmore robust evaluation. Last, we explore some geographical and economic factors\nthat may explain the observed dataset distributions. Code and data are\navailable here: https://github.com/ffaisal93/dataset_geography. Additional\nvisualizations are available here: https://nlp.cs.gmu.edu/project/datasetmaps/.", "published": "2021-12-07 05:13:50", "link": "http://arxiv.org/abs/2112.03497v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ground-Truth, Whose Truth? -- Examining the Challenges with Annotating\n  Toxic Text Datasets", "abstract": "The use of machine learning (ML)-based language models (LMs) to monitor\ncontent online is on the rise. For toxic text identification, task-specific\nfine-tuning of these models are performed using datasets labeled by annotators\nwho provide ground-truth labels in an effort to distinguish between offensive\nand normal content. These projects have led to the development, improvement,\nand expansion of large datasets over time, and have contributed immensely to\nresearch on natural language. Despite the achievements, existing evidence\nsuggests that ML models built on these datasets do not always result in\ndesirable outcomes. Therefore, using a design science research (DSR) approach,\nthis study examines selected toxic text datasets with the goal of shedding\nlight on some of the inherent issues and contributing to discussions on\nnavigating these challenges for existing and future projects. To achieve the\ngoal of the study, we re-annotate samples from three toxic text datasets and\nfind that a multi-label approach to annotating toxic text samples can help to\nimprove dataset quality. While this approach may not improve the traditional\nmetric of inter-annotator agreement, it may better capture dependence on\ncontext and diversity in annotators. We discuss the implications of these\nresults for both theory and practice.", "published": "2021-12-07 06:58:22", "link": "http://arxiv.org/abs/2112.03529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parsing with Pretrained Language Models, Multiple Datasets, and Dataset\n  Embeddings", "abstract": "With an increase of dataset availability, the potential for learning from a\nvariety of data sources has increased. One particular method to improve\nlearning from multiple data sources is to embed the data source during\ntraining. This allows the model to learn generalizable features as well as\ndistinguishing features between datasets. However, these dataset embeddings\nhave mostly been used before contextualized transformer-based embeddings were\nintroduced in the field of Natural Language Processing. In this work, we\ncompare two methods to embed datasets in a transformer-based multilingual\ndependency parser, and perform an extensive evaluation. We show that: 1)\nembedding the dataset is still beneficial with these models 2) performance\nincreases are highest when embedding the dataset at the encoder level 3)\nunsurprisingly, we confirm that performance increases are highest for small\ndatasets and datasets with a low baseline score. 4) we show that training on\nthe combination of all datasets performs similarly to designing smaller\nclusters based on language-relatedness.", "published": "2021-12-07 10:47:07", "link": "http://arxiv.org/abs/2112.03625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GKS: Graph-based Knowledge Selector for Task-oriented Dialog System", "abstract": "In previous research, knowledge-selection tasks mostly rely on language\nmodel-based methods or knowledge ranking. However, while approaches that rely\non the language models take all knowledge as sequential input, knowledge does\nnot contain sequential information in most circumstances. On the other hand,\nthe knowledge-ranking methods leverage dialog history and each given knowledge\nsnippet separately, but they do not consider information between knowledge\nsnippets. In the Tenth Dialog System Technology Challenges (DSTC10), we\nparticipated in the second Knowledge-grounded Task-oriented Dialogue Modeling\non Spoken Conversations. To deal with the problems mentioned above, we modified\ntraining methods based on state-of-the-art (SOTA) models for the first and\nthird sub-tasks. As for the second sub-task of knowledge selection, we proposed\nGraph-Knowledge Selector (GKS), utilizing a graph-attention base model\nincorporated with the language model. GKS makes knowledge-selection decisions\nin the dialog by simultaneously considering each knowledge embedding generated\nfrom the language model without sequential features. Moreover, GKS leverages\nconsiderable knowledge in decision-making and takes relations across knowledge\nas part of the selection process. As a result, GKS outperforms several SOTA\nmodels proposed in the data-set on knowledge selection from the Ninth Dialog\nSystem Technology Challenges (DSTC9).", "published": "2021-12-07 14:16:26", "link": "http://arxiv.org/abs/2112.03719v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UCD-CS at TREC 2021 Incident Streams Track", "abstract": "In recent years, the task of mining important information from social media\nposts during crises has become a focus of research for the purposes of\nassisting emergency response (ES). The TREC Incident Streams (IS) track is a\nresearch challenge organised for this purpose. The track asks participating\nsystems to both classify a stream of crisis-related tweets into humanitarian\naid related information types and estimate their importance regarding\ncriticality. The former refers to a multi-label information type classification\ntask and the latter refers to a priority estimation task. In this paper, we\nreport on the participation of the University College Dublin School of Computer\nScience (UCD-CS) in TREC-IS 2021. We explored a variety of approaches,\nincluding simple machine learning algorithms, multi-task learning techniques,\ntext augmentation, and ensemble approaches. The official evaluation results\nindicate that our runs achieve the highest scores in many metrics. To aid\nreproducibility, our code is publicly available at\nhttps://github.com/wangcongcong123/crisis-mtl.", "published": "2021-12-07 14:47:27", "link": "http://arxiv.org/abs/2112.03737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A pragmatic account of the weak evidence effect", "abstract": "Language is not only used to transmit neutral information; we often seek to\npersuade by arguing in favor of a particular view. Persuasion raises a number\nof challenges for classical accounts of belief updating, as information cannot\nbe taken at face value. How should listeners account for a speaker's \"hidden\nagenda\" when incorporating new information? Here, we extend recent\nprobabilistic models of recursive social reasoning to allow for persuasive\ngoals and show that our model provides a pragmatic account for why weakly\nfavorable arguments may backfire, a phenomenon known as the weak evidence\neffect. Critically, this model predicts a systematic relationship between\nbelief updates and expectations about the information source: weak evidence\nshould only backfire when speakers are expected to act under persuasive goals\nand prefer the strongest evidence. We introduce a simple experimental paradigm\ncalled the Stick Contest to measure the extent to which the weak evidence\neffect depends on speaker expectations, and show that a pragmatic listener\nmodel accounts for the empirical data better than alternative models. Our\nfindings suggest further avenues for rational models of social reasoning to\nilluminate classical decision-making phenomena.", "published": "2021-12-07 16:16:01", "link": "http://arxiv.org/abs/2112.03799v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Story Generation as Question-Answering", "abstract": "Neural language model-based approaches to automated story generation suffer\nfrom two important limitations. First, language model-based story generators\ngenerally do not work toward a given goal or ending. Second, they often lose\ncoherence as the story gets longer. We propose a novel approach to automated\nstory generation that treats the problem as one of generative\nquestion-answering. Our proposed story generation system starts with sentences\nencapsulating the final event of the story. The system then iteratively (1)\nanalyzes the text describing the most recent event, (2) generates a question\nabout \"why\" a character is doing the thing they are doing in the event, and\nthen (3) attempts to generate another, preceding event that answers this\nquestion.", "published": "2021-12-07 16:32:30", "link": "http://arxiv.org/abs/2112.03808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Target Group Bias in Hate Speech Detectors", "abstract": "The ubiquity of offensive and hateful content on online fora necessitates the\nneed for automatic solutions that detect such content competently across target\ngroups. In this paper we show that text classification models trained on large\npublicly available datasets despite having a high overall performance, may\nsignificantly under-perform on several protected groups. On the\n\\citet{vidgen2020learning} dataset, we find the accuracy to be 37\\% lower on an\nunder annotated Black Women target group and 12\\% lower on Immigrants, where\nhate speech involves a distinct style. To address this, we propose to perform\ntoken-level hate sense disambiguation, and utilize tokens' hate sense\nrepresentations for detection, modeling more general signals. On two publicly\navailable datasets, we observe that the variance in model accuracy across\ntarget groups drops by at least 30\\%, improving the average target group\nperformance by 4\\% and worst case performance by 13\\%.", "published": "2021-12-07 17:49:34", "link": "http://arxiv.org/abs/2112.03858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNITER-Based Situated Coreference Resolution with Rich Multimodal Input", "abstract": "We present our work on the multimodal coreference resolution task of the\nSituated and Interactive Multimodal Conversation 2.0 (SIMMC 2.0) dataset as a\npart of the tenth Dialog System Technology Challenge (DSTC10). We propose a\nUNITER-based model utilizing rich multimodal context such as textual dialog\nhistory, object knowledge base and visual dialog scenes to determine whether\neach object in the current scene is mentioned in the current dialog turn.\nResults show that the proposed approach outperforms the official DSTC10\nbaseline substantially, with the object F1 score boosted from 36.6% to 77.3% on\nthe development set, demonstrating the effectiveness of the proposed object\nrepresentations from rich multimodal input. Our model ranks second in the\nofficial evaluation on the object coreference resolution task with an F1 score\nof 73.3% after model ensembling.", "published": "2021-12-07 06:31:18", "link": "http://arxiv.org/abs/2112.03521v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A deep language model to predict metabolic network equilibria", "abstract": "We show that deep learning models, and especially architectures like the\nTransformer, originally intended for natural language, can be trained on\nrandomly generated datasets to predict to very high accuracy both the\nqualitative and quantitative features of metabolic networks. Using standard\nmathematical techniques, we create large sets (40 million elements) of random\nnetworks that can be used to train our models. These trained models can predict\nnetwork equilibrium on random graphs in more than 99% of cases. They can also\ngeneralize to graphs with different structure than those encountered at\ntraining. Finally, they can predict almost perfectly the equilibria of a small\nset of known biological networks. Our approach is both very economical in\nexperimental data and uses only small and shallow deep-learning model, far from\nthe large architectures commonly used in machine translation. Such results pave\nthe way for larger use of deep learning models for problems related to\nbiological networks in key areas such as quantitative systems pharmacology,\nsystems biology, and synthetic biology.", "published": "2021-12-07 09:30:59", "link": "http://arxiv.org/abs/2112.03588v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Change Summarization of Diachronic Scholarly Paper Collections by\n  Semantic Evolution Analysis", "abstract": "The amount of scholarly data has been increasing dramatically over the last\nyears. For newcomers to a particular science domain (e.g., IR, physics, NLP) it\nis often difficult to spot larger trends and to position the latest research in\nthe context of prior scientific achievements and breakthroughs. Similarly,\nresearchers in the history of science are interested in tools that allow them\nto analyze and visualize changes in particular scientific domains. Temporal\nsummarization and related methods should be then useful for making sense of\nlarge volumes of scientific discourse data aggregated over time. We demonstrate\na novel approach to analyze the collections of research papers published over\nlonger time periods to provide a high-level overview of important semantic\nchanges that occurred over the progress of time. Our approach is based on\ncomparing word semantic representations over time and aims to support users in\na better understanding of large domain-focused archives of scholarly\npublications. As an example dataset we use the ACL Anthology Reference Corpus\nthat spans from 1979 to 2015 and contains 22,878 scholarly articles.", "published": "2021-12-07 11:15:19", "link": "http://arxiv.org/abs/2112.03634v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "raceBERT -- A Transformer-based Model for Predicting Race and Ethnicity\n  from Names", "abstract": "This paper presents raceBERT -- a transformer-based model for predicting race\nand ethnicity from character sequences in names, and an accompanying python\npackage. Using a transformer-based model trained on a U.S. Florida voter\nregistration dataset, the model predicts the likelihood of a name belonging to\n5 U.S. census race categories (White, Black, Hispanic, Asian & Pacific\nIslander, American Indian & Alaskan Native). I build on Sood and Laohaprapanon\n(2018) by replacing their LSTM model with transformer-based models (pre-trained\nBERT model, and a roBERTa model trained from scratch), and compare the results.\nTo the best of my knowledge, raceBERT achieves state-of-the-art results in race\nprediction using names, with an average f1-score of 0.86 -- a 4.1% improvement\nover the previous state-of-the-art, and improvements between 15-17% for\nnon-white names.", "published": "2021-12-07 16:30:40", "link": "http://arxiv.org/abs/2112.03807v3", "categories": ["cs.CL", "cs.LG", "68U01"], "primary_category": "cs.CL"}
{"title": "Natural Answer Generation: From Factoid Answer to Full-length Answer\n  using Grammar Correction", "abstract": "Question Answering systems these days typically use template-based language\ngeneration. Though adequate for a domain-specific task, these systems are too\nrestrictive and predefined for domain-independent systems. This paper proposes\na system that outputs a full-length answer given a question and the extracted\nfactoid answer (short spans such as named entities) as the input. Our system\nuses constituency and dependency parse trees of questions. A transformer-based\nGrammar Error Correction model GECToR (2020), is used as a post-processing step\nfor better fluency. We compare our system with (i) Modified Pointer Generator\n(SOTA) and (ii) Fine-tuned DialoGPT for factoid questions. We also test our\napproach on existential (yes-no) questions with better results. Our model\ngenerates accurate and fluent answers than the state-of-the-art (SOTA)\napproaches. The evaluation is done on NewsQA and SqUAD datasets with an\nincrement of 0.4 and 0.9 percentage points in ROUGE-1 score respectively. Also\nthe inference time is reduced by 85\\% as compared to the SOTA. The improved\ndatasets used for our evaluation will be released as part of the research\ncontribution.", "published": "2021-12-07 17:39:21", "link": "http://arxiv.org/abs/2112.03849v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EmTract: Extracting Emotions from Social Media", "abstract": "We develop an open-source tool (EmTract) that extracts emotions from social\nmedia text tailed for financial context. To do so, we annotate ten thousand\nshort messages from a financial social media platform (StockTwits) and combine\nit with open-source emotion data. We then use a pre-tuned NLP model,\nDistilBERT, augment its embedding space by including 4,861 tokens (emojis and\nemoticons), and then fit it first on the open-source emotion data, then\ntransfer it to our annotated financial social media data. Our model outperforms\ncompeting open-source state-of-the-art emotion classifiers, such as Emotion\nEnglish DistilRoBERTa-base on both human and chatGPT annotated data. Compared\nto dictionary based methods, our methodology has three main advantages for\nresearch in finance. First, our model is tailored to financial social media\ntext; second, it incorporates key aspects of social media data, such as\nnon-standard phrases, emojis, and emoticons; and third, it operates by\nsequentially learning a latent representation that includes features such as\nword order, word usage, and local context. Using EmTract, we explore the\nrelationship between investor emotions expressed on social media and asset\nprices. We show that firm-specific investor emotions are predictive of daily\nprice movements. Our findings show that emotions and market dynamics are\nclosely related, and we provide a tool to help study the role emotions play in\nfinancial markets.", "published": "2021-12-07 18:01:35", "link": "http://arxiv.org/abs/2112.03868v3", "categories": ["q-fin.PR", "cs.CL"], "primary_category": "q-fin.PR"}
{"title": "Emotion-Cause Pair Extraction in Customer Reviews", "abstract": "Emotion-Cause Pair Extraction (ECPE) is a complex yet popular area in Natural\nLanguage Processing due to its importance and potential applications in various\ndomains. In this report , we aim to present our work in ECPE in the domain of\nonline reviews. With a manually annotated dataset, we explore an algorithm to\nextract emotion cause pairs using a neural network. In addition, we propose a\nmodel using previous reference materials and combining emotion-cause pair\nextraction with research in the domain of emotion-aware word embeddings, where\nwe send these embeddings into a Bi-LSTM layer which gives us the emotionally\nrelevant clauses. With the constraint of a limited dataset, we achieved . The\noverall scope of our report comprises of a comprehensive literature review,\nimplementation of referenced methods for dataset construction and initial model\ntraining, and modifying previous work in ECPE by proposing an improvement to\nthe pipeline, as well as algorithm development and implementation for the\nspecific domain of reviews.", "published": "2021-12-07 20:56:20", "link": "http://arxiv.org/abs/2112.03984v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multinational Address Parsing: A Zero-Shot Evaluation", "abstract": "Address parsing consists of identifying the segments that make up an address,\nsuch as a street name or a postal code. Because of its importance for tasks\nlike record linkage, address parsing has been approached with many techniques,\nthe latest relying on neural networks. While these models yield notable\nresults, previous work on neural networks has only focused on parsing addresses\nfrom a single source country. This paper explores the possibility of\ntransferring the address parsing knowledge acquired by training deep learning\nmodels on some countries' addresses to others with no further training in a\nzero-shot transfer learning setting. We also experiment using an attention\nmechanism and a domain adversarial training algorithm in the same zero-shot\ntransfer setting to improve performance. Both methods yield state-of-the-art\nperformance for most of the tested countries while giving good results to the\nremaining countries. We also explore the effect of incomplete addresses on our\nbest model, and we evaluate the impact of using incomplete addresses during\ntraining. In addition, we propose an open-source Python implementation of some\nof our trained models.", "published": "2021-12-07 21:40:43", "link": "http://arxiv.org/abs/2112.04008v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Scoping Review of Publicly Available Language Tasks in Clinical\n  Natural Language Processing", "abstract": "Objective: to provide a scoping review of papers on clinical natural language\nprocessing (NLP) tasks that use publicly available electronic health record\ndata from a cohort of patients. Materials and Methods: We searched six\ndatabases, including biomedical research and computer science literature\ndatabase. A round of title/abstract screening and full-text screening were\nconducted by two reviewers. Our method followed the Preferred Reporting Items\nfor Systematic Reviews and Meta-Analysis (PRISMA) guidelines. Results: A total\nof 35 papers with 47 clinical NLP tasks met inclusion criteria between 2007 and\n2021. We categorized the tasks by the type of NLP problems, including name\nentity recognition, summarization, and other NLP tasks. Some tasks were\nintroduced with a topic of clinical decision support applications, such as\nsubstance abuse, phenotyping, cohort selection for clinical trial. We\nsummarized the tasks by publication and dataset information. Discussion: The\nbreadth of clinical NLP tasks keeps growing as the field of NLP evolves with\nadvancements in language systems. However, gaps exist in divergent interests\nbetween general domain NLP community and clinical informatics community, and in\ngeneralizability of the data sources. We also identified issues in data\nselection and preparation including the lack of time-sensitive data, and\ninvalidity of problem size and evaluation. Conclusions: The existing clinical\nNLP tasks cover a wide range of topics and the field will continue to grow and\nattract more attention from both general domain NLP and clinical informatics\ncommunity. We encourage future work to incorporate multi-disciplinary\ncollaboration, reporting transparency, and standardization in data preparation.", "published": "2021-12-07 22:49:58", "link": "http://arxiv.org/abs/2112.05780v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Answer Type and Relation Prediction Task (SMART 2021)", "abstract": "Each year the International Semantic Web Conference organizes a set of\nSemantic Web Challenges to establish competitions that will advance\nstate-of-the-art solutions in some problem domains. The Semantic Answer Type\nand Relation Prediction Task (SMART) task is one of the ISWC 2021 Semantic Web\nchallenges. This is the second year of the challenge after a successful SMART\n2020 at ISWC 2020. This year's version focuses on two sub-tasks that are very\nimportant to Knowledge Base Question Answering (KBQA): Answer Type Prediction\nand Relation Prediction. Question type and answer type prediction can play a\nkey role in knowledge base question answering systems providing insights about\nthe expected answer that are helpful to generate correct queries or rank the\nanswer candidates. More concretely, given a question in natural language, the\nfirst task is, to predict the answer type using a target ontology (e.g.,\nDBpedia or Wikidata. Similarly, the second task is to identify relations in the\nnatural language query and link them to the relations in a target ontology.\nThis paper discusses the task descriptions, benchmark datasets, and evaluation\nmetrics. For more information, please visit https://smart-task.github.io/2021/.", "published": "2021-12-07 16:17:41", "link": "http://arxiv.org/abs/2112.07606v2", "categories": ["cs.CL", "cs.AI", "F.4.1; I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Multi-speaker Emotional Text-to-speech Synthesizer", "abstract": "We present a methodology to train our multi-speaker emotional text-to-speech\nsynthesizer that can express speech for 10 speakers' 7 different emotions. All\nsilences from audio samples are removed prior to learning. This results in fast\nlearning by our model. Curriculum learning is applied to train our model\nefficiently. Our model is first trained with a large single-speaker neutral\ndataset, and then trained with neutral speech from all speakers. Finally, our\nmodel is trained using datasets of emotional speech from all speakers. In each\nstage, training samples of each speaker-emotion pair have equal probability to\nappear in mini-batches. Through this procedure, our model can synthesize speech\nfor all targeted speakers and emotions. Our synthesized audio sets are\navailable on our web page.", "published": "2021-12-07 08:12:41", "link": "http://arxiv.org/abs/2112.03557v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification", "abstract": "Modern Web systems such as social media and e-commerce contain rich contents\nexpressed in images and text. Leveraging information from multi-modalities can\nimprove the performance of machine learning tasks such as classification and\nrecommendation. In this paper, we propose the Cross-Modality Attention\nContrastive Language-Image Pre-training (CMA-CLIP), a new framework which\nunifies two types of cross-modality attentions, sequence-wise attention and\nmodality-wise attention, to effectively fuse information from image and text\npairs. The sequence-wise attention enables the framework to capture the\nfine-grained relationship between image patches and text tokens, while the\nmodality-wise attention weighs each modality by its relevance to the downstream\ntasks. In addition, by adding task specific modality-wise attentions and\nmultilayer perceptrons, our proposed framework is capable of performing\nmulti-task classification with multi-modalities.\n  We conduct experiments on a Major Retail Website Product Attribute (MRWPA)\ndataset and two public datasets, Food101 and Fashion-Gen. The results show that\nCMA-CLIP outperforms the pre-trained and fine-tuned CLIP by an average of 11.9%\nin recall at the same level of precision on the MRWPA dataset for multi-task\nclassification. It also surpasses the state-of-the-art method on Fashion-Gen\nDataset by 5.5% in accuracy and achieves competitive performance on Food101\nDataset. Through detailed ablation studies, we further demonstrate the\neffectiveness of both cross-modality attention modules and our method's\nrobustness against noise in image and text inputs, which is a common challenge\nin practice.", "published": "2021-12-07 08:23:42", "link": "http://arxiv.org/abs/2112.03562v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Question Answering Survey: Directions, Challenges, Datasets, Evaluation\n  Matrices", "abstract": "The usage and amount of information available on the internet increase over\nthe past decade. This digitization leads to the need for automated answering\nsystem to extract fruitful information from redundant and transitional\nknowledge sources. Such systems are designed to cater the most prominent answer\nfrom this giant knowledge source to the user query using natural language\nunderstanding (NLU) and thus eminently depends on the Question-answering(QA)\nfield.\n  Question answering involves but not limited to the steps like mapping of user\nquestion to pertinent query, retrieval of relevant information, finding the\nbest suitable answer from the retrieved information etc. The current\nimprovement of deep learning models evince compelling performance improvement\nin all these tasks.\n  In this review work, the research directions of QA field are analyzed based\non the type of question, answer type, source of evidence-answer, and modeling\napproach. This detailing followed by open challenges of the field like\nautomatic question generation, similarity detection and, low resource\navailability for a language. In the end, a survey of available datasets and\nevaluation measures is presented.", "published": "2021-12-07 08:53:40", "link": "http://arxiv.org/abs/2112.03572v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Structured Inference with Randomization", "abstract": "Deep discrete structured models have seen considerable progress recently, but\ntraditional inference using dynamic programming (DP) typically works with a\nsmall number of states (less than hundreds), which severely limits model\ncapacity. At the same time, across machine learning, there is a recent trend of\nusing randomized truncation techniques to accelerate computations involving\nlarge sums. Here, we propose a family of randomized dynamic programming (RDP)\nalgorithms for scaling structured models to tens of thousands of latent states.\nOur method is widely applicable to classical DP-based inference (partition,\nmarginal, reparameterization, entropy) and different graph structures (chains,\ntrees, and more general hypergraphs). It is also compatible with automatic\ndifferentiation: it can be integrated with neural networks seamlessly and\nlearned with gradient-based optimizers. Our core technique approximates the\nsum-product by restricting and reweighting DP on a small subset of nodes, which\nreduces computation by orders of magnitude. We further achieve low bias and\nvariance via Rao-Blackwellization and importance sampling. Experiments over\ndifferent graphs demonstrate the accuracy and efficiency of our approach.\nFurthermore, when using RDP for training a structured variational autoencoder\nwith a scaled inference network, we achieve better test likelihood than\nbaselines and successfully prevent posterior collapse. code at:\nhttps://github.com/FranxYao/RDP", "published": "2021-12-07 11:26:41", "link": "http://arxiv.org/abs/2112.03638v3", "categories": ["cs.LG", "cs.CL", "cs.DS", "stat.AP", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Grounded Language-Image Pre-training", "abstract": "This paper presents a grounded language-image pre-training (GLIP) model for\nlearning object-level, language-aware, and semantic-rich visual\nrepresentations. GLIP unifies object detection and phrase grounding for\npre-training. The unification brings two benefits: 1) it allows GLIP to learn\nfrom both detection and grounding data to improve both tasks and bootstrap a\ngood grounding model; 2) GLIP can leverage massive image-text pairs by\ngenerating grounding boxes in a self-training fashion, making the learned\nrepresentation semantic-rich. In our experiments, we pre-train GLIP on 27M\ngrounding data, including 3M human-annotated and 24M web-crawled image-text\npairs. The learned representations demonstrate strong zero-shot and few-shot\ntransferability to various object-level recognition tasks. 1) When directly\nevaluated on COCO and LVIS (without seeing any images in COCO during\npre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many\nsupervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val\nand 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13\ndownstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised\nDynamic Head. Code is released at https://github.com/microsoft/GLIP.", "published": "2021-12-07 17:47:50", "link": "http://arxiv.org/abs/2112.03857v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Robust Speech Representation Learning via Flow-based Embedding\n  Regularization", "abstract": "Over the recent years, various deep learning-based methods were proposed for\nextracting a fixed-dimensional embedding vector from speech signals. Although\nthe deep learning-based embedding extraction methods have shown good\nperformance in numerous tasks including speaker verification, language\nidentification and anti-spoofing, their performance is limited when it comes to\nmismatched conditions due to the variability within them unrelated to the main\ntask. In order to alleviate this problem, we propose a novel training strategy\nthat regularizes the embedding network to have minimum information about the\nnuisance attributes. To achieve this, our proposed method directly incorporates\nthe information bottleneck scheme into the training process, where the mutual\ninformation is estimated using the main task classifier and an auxiliary\nnormalizing flow network. The proposed method was evaluated on different speech\nprocessing tasks and showed improvement over the standard training strategy in\nall experimentation.", "published": "2021-12-07 02:30:37", "link": "http://arxiv.org/abs/2112.03454v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Danna-Sep: Unite to separate them all", "abstract": "Deep learning-based music source separation has gained a lot of interest in\nthe last decades. Most of the existing methods operate with either spectrograms\nor waveforms. Spectrogram based models learn suitable masks for separating\nmagnitude spectrogram into different sources, and waveform-based models\ndirectly generate waveforms of individual sources. The two types of models have\ncomplementary strengths; the former is superior given harmonic sources such as\nvocals, while the latter demonstrates better results for percussion and bass\ninstruments. In this work, we improved upon the state-of-the-art (SoTA) models\nand successfully combined the best of both worlds. The backbones of the\nproposed framework, dubbed Danna-Sep, are two spectrogram-based models\nincluding a modified X-UMX and U-Net, and an enhanced Demucs as the\nwaveform-based model. Given an input of mixture, we linearly combined\nrespective outputs from the three models to obtain the final result. We showed\nin the experiments that, despite its simplicity, Danna-Sep surpassed the SoTA\nmodels by a large margin in terms of Source-to-Distortion Ratio.", "published": "2021-12-07 15:07:07", "link": "http://arxiv.org/abs/2112.03752v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Training end-to-end speech-to-text models on mobile phones", "abstract": "Training the state-of-the-art speech-to-text (STT) models in mobile devices\nis challenging due to its limited resources relative to a server environment.\nIn addition, these models are trained on generic datasets that are not\nexhaustive in capturing user-specific characteristics. Recently, on-device\npersonalization techniques have been making strides in mitigating the problem.\nAlthough many current works have already explored the effectiveness of\non-device personalization, the majority of their findings are limited to\nsimulation settings or a specific smartphone. In this paper, we develop and\nprovide a detailed explanation of our framework to train end-to-end models in\nmobile phones. To make it simple, we considered a model based on connectionist\ntemporal classification (CTC) loss. We evaluated the framework on various\nmobile phones from different brands and reported the results. We provide enough\nevidence that fine-tuning the models and choosing the right hyperparameter\nvalues is a trade-off between the lowest WER achievable, training time\non-device, and memory consumption. Hence, this is vital for a successful\ndeployment of on-device training onto a resource-limited environment like\nmobile phones. We use training sets from speakers with different accents and\nrecord a 7.6% decrease in average word error rate (WER). We also report the\nassociated computational cost measurements with respect to time, memory usage,\nand cpu utilization in mobile phones in real-time.", "published": "2021-12-07 18:08:19", "link": "http://arxiv.org/abs/2112.03871v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Time-domain Real-valued Generalized Wiener Filter for Multi-channel\n  Neural Separation Systems", "abstract": "Frequency-domain beamformers have been successful in a wide range of\nmulti-channel neural separation systems in the past years. However, the\noperations in conventional frequency-domain beamformers are typically\nindependently-defined and complex-valued, which result in two drawbacks: the\nformer does not fully utilize the advantage of end-to-end optimization, and the\nlatter may introduce numerical instability during the training phase. Motivated\nby the recent success in end-to-end neural separation systems, in this paper we\npropose time-domain real-valued generalized Wiener filter (TD-GWF), a linear\nfilter defined on a 2-D learnable real-valued signal transform. TD-GWF splits\nthe transformed representation into groups and performs an minimum mean-square\nerror (MMSE) estimation on all available channels on each of the groups. We\nshow how TD-GWF can be connected to conventional filter-and-sum beamformers\nwhen certain signal transform and the number of groups are specified. Moreover,\ngiven the recent success in the sequential neural beamforming frameworks, we\nshow how TD-GWF can be applied in such frameworks to perform iterative\nbeamforming and separation to obtain an overall performance gain. Comprehensive\nexperiment results show that TD-GWF performs consistently better than\nconventional frequency-domain beamformers in the sequential neural beamforming\npipeline with various neural network architectures, microphone array scenarios,\nand task configurations.", "published": "2021-12-07 07:16:43", "link": "http://arxiv.org/abs/2112.03533v2", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
