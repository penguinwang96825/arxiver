{"title": "Acquisition of Translation Lexicons for Historically Unwritten Languages\n  via Bridging Loanwords", "abstract": "With the advent of informal electronic communications such as social media,\ncolloquial languages that were historically unwritten are being written for the\nfirst time in heavily code-switched environments. We present a method for\ninducing portions of translation lexicons through the use of expert knowledge\nin these settings where there are approximately zero resources available other\nthan a language informant, potentially not even large amounts of monolingual\ndata. We investigate inducing a Moroccan Darija-English translation lexicon via\nFrench loanwords bridging into English and find that a useful lexicon is\ninduced for human-assisted translation and statistical machine translation.", "published": "2017-06-06 00:55:25", "link": "http://arxiv.org/abs/1706.01570v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Text Summarization using Abstract Meaning Representation", "abstract": "With an ever increasing size of text present on the Internet, automatic\nsummary generation remains an important problem for natural language\nunderstanding. In this work we explore a novel full-fledged pipeline for text\nsummarization with an intermediate step of Abstract Meaning Representation\n(AMR). The pipeline proposed by us first generates an AMR graph of an input\nstory, through which it extracts a summary graph and finally, generate summary\nsentences from this summary graph. Our proposed method achieves\nstate-of-the-art results compared to the other text summarization routines\nbased on AMR. We also point out some significant problems in the existing\nevaluation methods, which make them unsuitable for evaluating summary quality.", "published": "2017-06-06 10:04:45", "link": "http://arxiv.org/abs/1706.01678v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Frame Tracking Model for Memory-Enhanced Dialogue Systems", "abstract": "Recently, resources and tasks were proposed to go beyond state tracking in\ndialogue systems. An example is the frame tracking task, which requires\nrecording multiple frames, one for each user goal set during the dialogue. This\nallows a user, for instance, to compare items corresponding to different goals.\nThis paper proposes a model which takes as input the list of frames created so\nfar during the dialogue, the current user utterance as well as the dialogue\nacts, slot types, and slot values associated with this utterance. The model\nthen outputs the frame being referenced by each triple of dialogue act, slot\ntype, and slot value. We show that on the recently published Frames dataset,\nthis model significantly outperforms a previously proposed rule-based baseline.\nIn addition, we propose an extensive analysis of the frame tracking task by\ndividing it into sub-tasks and assessing their difficulty with respect to our\nmodel.", "published": "2017-06-06 10:48:29", "link": "http://arxiv.org/abs/1706.01690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A General-Purpose Tagger with Convolutional Neural Networks", "abstract": "We present a general-purpose tagger based on convolutional neural networks\n(CNN), used for both composing word vectors and encoding context information.\nThe CNN tagger is robust across different tagging tasks: without task-specific\ntuning of hyper-parameters, it achieves state-of-the-art results in\npart-of-speech tagging, morphological tagging and supertagging. The CNN tagger\nis also robust against the out-of-vocabulary problem, it performs well on\nartificially unnormalized texts.", "published": "2017-06-06 12:11:50", "link": "http://arxiv.org/abs/1706.01723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label-Dependencies Aware Recurrent Neural Networks", "abstract": "In the last few years, Recurrent Neural Networks (RNNs) have proved effective\non several NLP tasks. Despite such great success, their ability to model\n\\emph{sequence labeling} is still limited. This lead research toward solutions\nwhere RNNs are combined with models which already proved effective in this\ndomain, such as CRFs. In this work we propose a solution far simpler but very\neffective: an evolution of the simple Jordan RNN, where labels are re-injected\nas input into the network, and converted into embeddings, in the same way as\nwords. We compare this RNN variant to all the other RNN models, Elman and\nJordan RNN, LSTM and GRU, on two well-known tasks of Spoken Language\nUnderstanding (SLU). Thanks to label embeddings and their combination at the\nhidden layer, the proposed variant, which uses more parameters than Elman and\nJordan RNNs, but far fewer than LSTM and GRU, is more effective than other\nRNNs, but also outperforms sophisticated CRF models.", "published": "2017-06-06 13:10:49", "link": "http://arxiv.org/abs/1706.01740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Linguistic Productivity of Unsupervised Deep Neural\n  Networks", "abstract": "Increasingly, cognitive scientists have demonstrated interest in applying\ntools from deep learning. One use for deep learning is in language acquisition\nwhere it is useful to know if a linguistic phenomenon can be learned through\ndomain-general means. To assess whether unsupervised deep learning is\nappropriate, we first pose a smaller question: Can unsupervised neural networks\napply linguistic rules productively, using them in novel situations? We draw\nfrom the literature on determiner/noun productivity by training an\nunsupervised, autoencoder network measuring its ability to combine nouns with\ndeterminers. Our simple autoencoder creates combinations it has not previously\nencountered and produces a degree of overlap matching adults. While this\npreliminary work does not provide conclusive evidence for productivity, it\nwarrants further investigation with more complex models. Further, this work\nhelps lay the foundations for future collaboration between the deep learning\nand cognitive science communities.", "published": "2017-06-06 16:16:51", "link": "http://arxiv.org/abs/1706.01839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext", "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence\nembeddings in the setting of Wieting et al. (2016b). We use neural machine\ntranslation to generate sentential paraphrases via back-translation of\nbilingual sentence pairs. We evaluate the paraphrase pairs by their ability to\nserve as training data for learning paraphrastic sentence embeddings. We find\nthat the data quality is stronger than prior work based on bitext and on par\nwith manually-written English paraphrase pairs, with the advantage that our\napproach can scale up to generate large training sets for many languages and\ndomains. We experiment with several language pairs and data sources, and\ndevelop a variety of data filtering techniques. In the process, we explore how\nneural machine translation output differs from human-written sentences, finding\nclear differences in length, the amount of repetition, and the use of rare\nwords.", "published": "2017-06-06 16:36:41", "link": "http://arxiv.org/abs/1706.01847v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Synergistic Union of Word2Vec and Lexicon for Domain Specific Semantic\n  Similarity", "abstract": "Semantic similarity measures are an important part in Natural Language\nProcessing tasks. However Semantic similarity measures built for general use do\nnot perform well within specific domains. Therefore in this study we introduce\na domain specific semantic similarity measure that was created by the\nsynergistic union of word2vec, a word embedding method that is used for\nsemantic similarity calculation and lexicon based (lexical) semantic similarity\nmethods. We prove that this proposed methodology out performs word embedding\nmethods trained on generic corpus and methods trained on domain specific corpus\nbut do not use lexical semantic similarity methods to augment the results.\nFurther, we prove that text lemmatization can improve the performance of word\nembedding methods.", "published": "2017-06-06 20:45:30", "link": "http://arxiv.org/abs/1706.01967v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marmara Turkish Coreference Corpus and Coreference Resolution Baseline", "abstract": "We describe the Marmara Turkish Coreference Corpus, which is an annotation of\nthe whole METU-Sabanci Turkish Treebank with mentions and coreference chains.\nCollecting eight or more independent annotations for each document allowed for\nfully automatic adjudication. We provide a baseline system for Turkish mention\ndetection and coreference resolution and evaluate it on the corpus.", "published": "2017-06-06 17:25:36", "link": "http://arxiv.org/abs/1706.01863v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring Offensive Speech in Online Political Discourse", "abstract": "The Internet and online forums such as Reddit have become an increasingly\npopular medium for citizens to engage in political conversations. However, the\nonline disinhibition effect resulting from the ability to use pseudonymous\nidentities may manifest in the form of offensive speech, consequently making\npolitical discussions more aggressive and polarizing than they already are.\nSuch environments may result in harassment and self-censorship from its\ntargets. In this paper, we present preliminary results from a large-scale\ntemporal measurement aimed at quantifying offensiveness in online political\ndiscussions.\n  To enable our measurements, we develop and evaluate an offensive speech\nclassifier. We then use this classifier to quantify and compare offensiveness\nin the political and general contexts. We perform our study using a database of\nover 168M Reddit comments made by over 7M pseudonyms between January 2015 and\nJanuary 2017 -- a period covering several divisive political events including\nthe 2016 US presidential elections.", "published": "2017-06-06 21:29:23", "link": "http://arxiv.org/abs/1706.01875v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
