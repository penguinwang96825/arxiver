{"title": "Simulating and analyzing a sparse order book: an application to intraday electricity markets", "abstract": "This paper presents a novel model for simulating and analyzing sparse limit\norder books (LOBs), with a specific application to the European intraday\nelectricity market. In illiquid markets, characterized by significant gaps\nbetween order levels due to sparse trading volumes, traditional LOB models\noften fall short. Our approach utilizes an inhomogeneous Poisson process to\naccurately capture the sporadic nature of order arrivals and cancellations on\nboth the bid and ask sides of the book. By applying this model to the intraday\nelectricity market, we gain insights into the unique microstructural behaviors\nand challenges of this dynamic trading environment. The results offer valuable\nimplications for market participants, enhancing their understanding of LOB\ndynamics in illiquid markets. This work contributes to the broader field of\nmarket microstructure by providing a robust framework adaptable to various\nilliquid market settings beyond electricity trading.", "published": "2024-10-09 12:55:05", "link": "http://arxiv.org/abs/2410.06839v1", "categories": ["q-fin.TR", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "First order Martingale model risk and semi-static hedging", "abstract": "We investigate model risk distributionally robust sensitivities for\nfunctionals on the Wasserstein space when the underlying model is constrained\nto the martingale class and/or is subject to constraints on the first marginal\nlaw. Our results extend the findings of Bartl, Drapeau, Obloj \\& Wiesel\n\\cite{bartl2021sensitivity} and Bartl \\& Wiesel \\cite{bartlsensitivityadapted}\nby introducing the minimization of the distributionally robust problem with\nrespect to semi-static hedging strategies. We provide explicit\ncharacterizations of the model risk (first order) optimal semi-static hedging\nstrategies. The distributional robustness is analyzed both in terms of the\nadapted Wasserstein metric and the more relevant standard Wasserstein metric.", "published": "2024-10-09 14:09:49", "link": "http://arxiv.org/abs/2410.06906v1", "categories": ["q-fin.MF", "math.OC", "math.PR", "49K45, 49Q22"], "primary_category": "q-fin.MF"}
{"title": "Statistical Arbitrage in Rank Space", "abstract": "Equity market dynamics are conventionally investigated in name space where\nstocks are indexed by company names. In contrast, by indexing stocks based on\ntheir ranks in capitalization, we gain a different perspective of market\ndynamics in rank space. Here, we demonstrate the superior performance of\nstatistical arbitrage in rank space over name space, driven by a robust market\nrepresentation and enhanced mean-reverting properties of residual returns in\nrank space. Our statistical arbitrage algorithm features an intraday\nrebalancing mechanism for effective conversion between portfolios in name and\nrank space. We explore statistical arbitrage with and without neural networks\nin both name and rank space and show that the portfolios obtained in rank space\nwith neural networks significantly outperform those in name space.", "published": "2024-10-09 06:05:11", "link": "http://arxiv.org/abs/2410.06568v1", "categories": ["q-fin.MF", "stat.ML"], "primary_category": "q-fin.MF"}
{"title": "Generating long-horizon stock \"buy\" signals with a neural language model", "abstract": "This paper describes experiments on fine-tuning a small language model to\ngenerate forecasts of long-horizon stock price movements. Inputs to the model\nare narrative text from 10-K reports of large market capitalization companies\nin the S&P 500 index; the output is a forward-looking buy or sell decision.\nPrice direction is predicted at discrete horizons up to 12 months after the\nreport filing date. The results reported here demonstrate good out-of-sample\nstatistical performance (F1-macro= 0.62) at medium to long investment horizons.\nIn particular, the buy signals generated from 10-K text are found most precise\nat 6 and 9 months in the future. As measured by the F1 score, the buy signal\nprovides between 4.8 and 9 percent improvement against a random stock selection\nmodel. In contrast, sell signals generated by the models do not perform well.\nThis may be attributed to the highly imbalanced out-of-sample data, or perhaps\ndue to management drafting annual reports with a bias toward positive language.\nCross-sectional analysis of performance by economic sector suggests that\nidiosyncratic reporting styles within industries are correlated with varying\ndegrees and time scales of price movement predictability.", "published": "2024-10-09 20:17:26", "link": "http://arxiv.org/abs/2410.18988v1", "categories": ["q-fin.ST", "econ.GN", "q-fin.EC"], "primary_category": "q-fin.ST"}
{"title": "Compressing Large Language Models with Automated Sub-Network Search", "abstract": "Large Language Models (LLMs) demonstrate exceptional reasoning abilities,\nenabling strong generalization across diverse tasks such as commonsense\nreasoning and instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. In this paper we consider model compression for LLMs to reduce model\nsize while improving downstream task performance. We phrase this as a neural\narchitecture search problem that automatically prunes structural components,\nsuch as attention heads, neurons, and layers by searching for the\nPareto-optimal set of sub-networks balancing between performance and on-device\nlatency. Compared to state-of-the-art structural pruning approaches and\nfine-tuned smaller sub-networks extracted from the pre-trained model, our\nmethod achieves upto 9.85% improvement on average on 11 diverse downstream\ntasks, while achieving up to 22% improvement of on-device latency.", "published": "2024-10-09 02:14:39", "link": "http://arxiv.org/abs/2410.06479v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Similarity of Circuits across Languages: a Case Study on the\n  Subject-verb Agreement Task", "abstract": "Several algorithms implemented by language models have recently been\nsuccessfully reversed-engineered. However, these findings have been\nconcentrated on specific tasks and models, leaving it unclear how universal\ncircuits are across different settings. In this paper, we study the circuits\nimplemented by Gemma 2B for solving the subject-verb agreement task across two\ndifferent languages, English and Spanish. We discover that both circuits are\nhighly consistent, being mainly driven by a particular attention head writing a\n`subject number' signal to the last residual stream, which is read by a small\nset of neurons in the final MLPs. Notably, this subject number signal is\nrepresented as a direction in the residual stream space, and is\nlanguage-independent. We demonstrate that this direction has a causal effect on\nthe model predictions, effectively flipping the Spanish predicted verb number\nby intervening with the direction found in English. Finally, we present\nevidence of similar behavior in other models within the Gemma 1 and Gemma 2\nfamilies.", "published": "2024-10-09 02:49:56", "link": "http://arxiv.org/abs/2410.06496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEGMENT+: Long Text Processing with Short-Context Language Models", "abstract": "There is a growing interest in expanding the input capacity of language\nmodels (LMs) across various domains. However, simply increasing the context\nwindow does not guarantee robust performance across diverse long-input\nprocessing tasks, such as understanding extensive documents and extracting\ndetailed information from lengthy and noisy data. In response, we introduce\nSEGMENT+, a general framework that enables LMs to handle extended inputs within\nlimited context windows efficiently. SEGMENT+ utilizes structured notes and a\nfiltering module to manage information flow, resulting in a system that is both\ncontrollable and interpretable. Our extensive experiments across various model\nsizes, focusing on long-document question-answering and Needle-in-a-Haystack\ntasks, demonstrate the effectiveness of SEGMENT+ in improving performance.", "published": "2024-10-09 03:40:22", "link": "http://arxiv.org/abs/2410.06519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel LLM-based Two-stage Summarization Approach for Long Dialogues", "abstract": "Long document summarization poses a significant challenge in natural language\nprocessing due to input lengths that exceed the capacity of most\nstate-of-the-art pre-trained language models. This study proposes a\nhierarchical framework that segments and condenses information from long\ndocuments, subsequently fine-tuning the processed text with an abstractive\nsummarization model. Unsupervised topic segmentation methods identify\nsemantically appropriate breakpoints. The condensation stage utilizes an\nunsupervised generation model to generate condensed data, and our current\nexperiments employ ChatGPT(v3.5). The summarization stage fine-tunes the\nabstractive summarization model on the condensed data to generate the final\nresults. This framework enables long documents to be processed on models even\nwhen the document length exceeds the model's maximum input size. The exclusion\nof the entire document from the summarization model reduces the time and\ncomputational resources required for training, making the framework suitable\nfor contexts with constrained local computational resources.", "published": "2024-10-09 03:42:40", "link": "http://arxiv.org/abs/2410.06520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet", "abstract": "As multimodal large language models (MLLMs) continue to demonstrate\nincreasingly competitive performance across a broad spectrum of tasks, more\nintricate and comprehensive benchmarks have been developed to assess these\ncutting-edge models. These benchmarks introduce new challenges to core\ncapabilities such as perception, reasoning, and planning. However, existing\nmultimodal benchmarks fall short in providing a focused evaluation of\nmulti-step planning based on spatial relationships in images. To bridge this\ngap, we present ING-VP, the first INteractive Game-based Vision Planning\nbenchmark, specifically designed to evaluate the spatial imagination and\nmulti-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,\nencompassing 300 levels, each with 6 unique configurations. A single model\nengages in over 60,000 rounds of interaction. The benchmark framework allows\nfor multiple comparison settings, including image-text vs. text-only inputs,\nsingle-step vs. multi-step reasoning, and with-history vs. without-history\nconditions, offering valuable insights into the model's capabilities. We\nevaluated numerous state-of-the-art MLLMs, with the highest-performing model,\nClaude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the\nanticipated standard. This work aims to provide a specialized evaluation\nframework to drive advancements in MLLMs' capacity for complex spatial\nreasoning and planning. The code is publicly available at\nhttps://github.com/Thisisus7/ING-VP.git.", "published": "2024-10-09 05:17:38", "link": "http://arxiv.org/abs/2410.06555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Bias and Enhancing Diagnostic Accuracy in Large Language\n  Models for Healthcare", "abstract": "Biased AI-generated medical advice and misdiagnoses can jeopardize patient\nsafety, making the integrity of AI in healthcare more critical than ever. As\nLarge Language Models (LLMs) take on a growing role in medical decision-making,\naddressing their biases and enhancing their accuracy is key to delivering safe,\nreliable care. This study addresses these challenges head-on by introducing new\nresources designed to promote ethical and precise AI in healthcare. We present\ntwo datasets: BiasMD, featuring 6,007 question-answer pairs crafted to evaluate\nand mitigate biases in health-related LLM outputs, and DiseaseMatcher, with\n32,000 clinical question-answer pairs spanning 700 diseases, aimed at assessing\nsymptom-based diagnostic accuracy. Using these datasets, we developed the\nEthiClinician, a fine-tuned model built on the ChatDoctor framework, which\noutperforms GPT-4 in both ethical reasoning and clinical judgment. By exposing\nand correcting hidden biases in existing models for healthcare, our work sets a\nnew benchmark for safer, more reliable patient outcomes.", "published": "2024-10-09 06:00:05", "link": "http://arxiv.org/abs/2410.06566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient\n  Attentions", "abstract": "Recent advancements in Transformer-based large language models (LLMs) have\nset new standards in natural language processing. However, the classical\nsoftmax attention incurs significant computational costs, leading to a $O(T)$\ncomplexity for per-token generation, where $T$ represents the context length.\nThis work explores reducing LLMs' complexity while maintaining performance by\nintroducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an\ninnovative data-dependent tempered selection (DDTS) mechanism within a linear\nattention-based, purely recurrent framework, achieving significant accuracy\nwhile drastically reducing the memory usage typically associated with recurrent\nmodels. This method exemplifies semantic compression by maintaining essential\ninput information with fixed-size hidden states. Building on this, Rodimus$+$\ncombines Rodimus with the innovative Sliding Window Shared-Key Attention\n(SW-SKA) in a hybrid approach, effectively leveraging the complementary\nsemantic, token, and head compression techniques. Our experiments demonstrate\nthat Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior\ndownstream performance against models trained on more tokens, including\nQwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the\naccuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints\nwill be available soon.", "published": "2024-10-09 06:22:36", "link": "http://arxiv.org/abs/2410.06577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tree of Problems: Improving structured problem solving with\n  compositionality", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nmultiple tasks through in-context learning. For complex reasoning tasks that\nrequire step-by-step thinking, Chain-of-Thought (CoT) prompting has given\nimpressive results, especially when combined with self-consistency.\nNonetheless, some tasks remain particularly difficult for LLMs to solve. Tree\nof Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing\nthe complex problem into paths of subproblems. In this paper, we propose Tree\nof Problems (ToP), a simpler version of ToT, which we hypothesise can work\nbetter for complex tasks that can be divided into identical subtasks. Our\nempirical results show that our approach outperforms ToT and GoT, and in\naddition performs better than CoT on complex reasoning tasks. All code for this\npaper is publicly available here:\nhttps://github.com/ArmelRandy/tree-of-problems.", "published": "2024-10-09 07:35:46", "link": "http://arxiv.org/abs/2410.06634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Universality: Studying Mechanistic Similarity Across Language\n  Model Architectures", "abstract": "The hypothesis of Universality in interpretability suggests that different\nneural networks may converge to implement similar algorithms on similar tasks.\nIn this work, we investigate two mainstream architectures for language\nmodeling, namely Transformers and Mambas, to explore the extent of their\nmechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate\ninterpretable features from these models and show that most features are\nsimilar in these two models. We also validate the correlation between feature\nsimilarity and Universality. We then delve into the circuit-level analysis of\nMamba models and find that the induction circuits in Mamba are structurally\nanalogous to those in Transformers. We also identify a nuanced difference we\ncall \\emph{Off-by-One motif}: The information of one token is written into the\nSSM state in its next position. Whilst interaction between tokens in\nTransformers does not exhibit such trend.", "published": "2024-10-09 08:28:53", "link": "http://arxiv.org/abs/2410.06672v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guaranteed Generation from Large Language Models", "abstract": "As large language models (LLMs) are increasingly used across various\napplications, there is a growing need to control text generation to satisfy\nspecific constraints or requirements. This raises a crucial question: Is it\npossible to guarantee strict constraint satisfaction in generated outputs while\npreserving the distribution of the original model as much as possible? We first\ndefine the ideal distribution - the one closest to the original model, which\nalso always satisfies the expressed constraint - as the ultimate goal of\nguaranteed generation. We then state a fundamental limitation, namely that it\nis impossible to reach that goal through autoregressive training alone. This\nmotivates the necessity of combining training-time and inference-time methods\nto enforce such guarantees. Based on this insight, we propose GUARD, a simple\nyet effective approach that combines an autoregressive proposal distribution\nwith rejection sampling. Through GUARD's theoretical properties, we show how\ncontrolling the KL divergence between a specific proposal and the target ideal\ndistribution simultaneously optimizes inference speed and distributional\ncloseness. To validate these theoretical concepts, we conduct extensive\nexperiments on two text generation settings with hard-to-satisfy constraints: a\nlexical constraint scenario and a sentiment reversal scenario. These\nexperiments show that GUARD achieves perfect constraint satisfaction while\nalmost preserving the ideal distribution with highly improved inference\nefficiency. GUARD provides a principled approach to enforcing strict guarantees\nfor LLMs without compromising their generative capabilities.", "published": "2024-10-09 09:39:55", "link": "http://arxiv.org/abs/2410.06716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seg2Act: Global Context-aware Action Generation for Document Logical\n  Structuring", "abstract": "Document logical structuring aims to extract the underlying hierarchical\nstructure of documents, which is crucial for document intelligence. Traditional\napproaches often fall short in handling the complexity and the variability of\nlengthy documents. To address these issues, we introduce Seg2Act, an\nend-to-end, generation-based method for document logical structuring,\nrevisiting logical structure extraction as an action generation task.\nSpecifically, given the text segments of a document, Seg2Act iteratively\ngenerates the action sequence via a global context-aware generative model, and\nsimultaneously updates its global context and current logical structure based\non the generated actions. Experiments on ChCatExt and HierDoc datasets\ndemonstrate the superior performance of Seg2Act in both supervised and transfer\nlearning settings.", "published": "2024-10-09 11:58:40", "link": "http://arxiv.org/abs/2410.06802v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FltLM: An Intergrated Long-Context Large Language Model for Effective\n  Context Filtering and Understanding", "abstract": "The development of Long-Context Large Language Models (LLMs) has markedly\nadvanced natural language processing by facilitating the process of textual\ndata across long documents and multiple corpora. However, Long-Context LLMs\nstill face two critical challenges: The lost in the middle phenomenon, where\ncrucial middle-context information is likely to be missed, and the distraction\nissue that the models lose focus due to overly extended contexts. To address\nthese challenges, we propose the Context Filtering Language Model (FltLM), a\nnovel integrated Long-Context LLM which enhances the ability of the model on\nmulti-document question-answering (QA) tasks. Specifically, FltLM innovatively\nincorporates a context filter with a soft mask mechanism, identifying and\ndynamically excluding irrelevant content to concentrate on pertinent\ninformation for better comprehension and reasoning. Our approach not only\nmitigates these two challenges, but also enables the model to operate\nconveniently in a single forward pass. Experimental results demonstrate that\nFltLM significantly outperforms supervised fine-tuning and retrieval-based\nmethods in complex QA scenarios, suggesting a promising solution for more\naccurate and reliable long-context natural language understanding applications.", "published": "2024-10-09 13:47:50", "link": "http://arxiv.org/abs/2410.06886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Model for Less-Resourced Language with 1 billion parameters", "abstract": "Large language models (LLMs) are a basic infrastructure for modern natural\nlanguage processing. Many commercial and open-source LLMs exist for English,\ne.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on\nmostly English texts, their fluency and knowledge of low-resource languages and\nsocieties are superficial. We present the development of large generative\nlanguage models for a less-resourced language. GaMS 1B - Generative Model for\nSlovene with 1 billion parameters was created by continuing pretraining of the\nexisting English OPT model. We developed a new tokenizer adapted to Slovene,\nCroatian, and English languages and used embedding initialization methods FOCUS\nand WECHSEL to transfer the embeddings from the English OPT model. We evaluate\nour models on several classification datasets from the Slovene suite of\nbenchmarks and generative sentence simplification task SENTA. We only used a\nfew-shot in-context learning of our models, which are not yet\ninstruction-tuned. For classification tasks, in this mode, the generative\nmodels lag behind the existing Slovene BERT-type models fine-tuned for specific\ntasks. On a sentence simplification task, the GaMS models achieve comparable or\nbetter performance than the GPT-3.5-Turbo model.", "published": "2024-10-09 13:59:34", "link": "http://arxiv.org/abs/2410.06898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Utilize the Flow before Stepping into the Same River Twice: Certainty\n  Represented Knowledge Flow for Refusal-Aware Instruction Tuning", "abstract": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict occurs when similar samples within the LLM's\nfeature space receive differing supervision signals (original vs. modified \"I\ndon't know\"). Dynamic conflict arises as the LLM's evolving knowledge during\nSFT enables it to answer previously unanswerable questions, but the\nnow-answerable training samples still retain the original \"I don't know\"\nsupervision signals from the initial LLM state, leading to inconsistencies.\nThese conflicts cause the trained LLM to misclassify known questions as\nunknown, resulting in over-refusal. To address this issue, we introduce\nCertainty Represented Knowledge Flow for Refusal-Aware Instructions Tuning\n(CRaFT). CRaFT centers on two main contributions: First, we additionally\nincorporate response certainty to selectively filter and modify data, reducing\nstatic conflicts. Second, we implement preliminary rehearsal training to\ncharacterize changes in the LLM's knowledge state, which helps mitigate dynamic\nconflicts during the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Code and data will be released at https://github.com/opendatalab/CRaFT .", "published": "2024-10-09 14:12:51", "link": "http://arxiv.org/abs/2410.06913v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference\n  Acceleration", "abstract": "Speculative decoding (SD) has emerged as a widely used paradigm to accelerate\nLLM inference without compromising quality. It works by first employing a\ncompact model to draft multiple tokens efficiently and then using the target\nLLM to verify them in parallel. While this technique has achieved notable\nspeedups, most existing approaches necessitate either additional parameters or\nextensive training to construct effective draft models, thereby restricting\ntheir applicability across different LLMs and tasks. To address this\nlimitation, we explore a novel plug-and-play SD solution with layer-skipping,\nwhich skips intermediate layers of the target LLM as the compact draft model.\nOur analysis reveals that LLMs exhibit great potential for self-acceleration\nthrough layer sparsity and the task-specific nature of this sparsity. Building\non these insights, we introduce SWIFT, an on-the-fly self-speculative decoding\nalgorithm that adaptively selects intermediate layers of LLMs to skip during\ninference. SWIFT does not require auxiliary models or additional training,\nmaking it a plug-and-play solution for accelerating LLM inference across\ndiverse input data streams. Our extensive experiments across a wide range of\nmodels and downstream tasks demonstrate that SWIFT can achieve over a 1.3x-1.6x\nspeedup while preserving the original distribution of the generated text. We\nrelease our code in https://github.com/hemingkx/SWIFT.", "published": "2024-10-09 14:15:30", "link": "http://arxiv.org/abs/2410.06916v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on\n  Relatively Free Word Ordered and Morphologically Rich Low Resource Languages", "abstract": "Neural dependency parsing has achieved remarkable performance for low\nresource morphologically rich languages. It has also been well-studied that\nmorphologically rich languages exhibit relatively free word order. This prompts\na fundamental investigation: Is there a way to enhance dependency parsing\nperformance, making the model robust to word order variations utilizing the\nrelatively free word order nature of morphologically rich languages? In this\nwork, we examine the robustness of graph-based parsing architectures on 7\nrelatively free word order languages. We focus on scrutinizing essential\nmodifications such as data augmentation and the removal of position encoding\nrequired to adapt these architectures accordingly. To this end, we propose a\ncontrastive self-supervised learning method to make the model robust to word\norder variations. Furthermore, our proposed modification demonstrates a\nsubstantial average gain of 3.03/2.95 points in 7 relatively free word order\nlanguages, as measured by the UAS/LAS Score metric when compared to the best\nperforming baseline.", "published": "2024-10-09 14:38:49", "link": "http://arxiv.org/abs/2410.06944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Selection via Optimal Control for Language Models", "abstract": "This work investigates the selection of high-quality pre-training data from\nmassive corpora to enhance LMs' capabilities for downstream usage. We formulate\ndata selection as a generalized Optimal Control problem, which can be solved\ntheoretically by Pontryagin's Maximum Principle (PMP), yielding a set of\nnecessary conditions that characterize the relationship between optimal data\nselection and LM training dynamics. Based on these theoretical results, we\nintroduce PMP-based Data Selection (PDS), a framework that approximates optimal\ndata selection by solving the PMP conditions. In our experiments, we adopt PDS\nto select data from CommmonCrawl and show that the PDS-selected corpus\naccelerates the learning of LMs and constantly boosts their performance on a\nwide range of downstream tasks across various model sizes. Moreover, the\nbenefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by\nthe extrapolation of the test loss curves according to the Scaling Laws. PDS\nalso improves data utilization when the pre-training data is limited, by\nreducing the data demand by 1.8 times, which helps mitigate the quick\nexhaustion of available web-crawled corpora. Our code, model, and data can be\nfound at https://github.com/microsoft/LMOps/tree/main/data_selection.", "published": "2024-10-09 17:06:57", "link": "http://arxiv.org/abs/2410.07064v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stanceformer: Target-Aware Transformer for Stance Detection", "abstract": "The task of Stance Detection involves discerning the stance expressed in a\ntext towards a specific subject or target. Prior works have relied on existing\ntransformer models that lack the capability to prioritize targets effectively.\nConsequently, these models yield similar performance regardless of whether we\nutilize or disregard target information, undermining the task's significance.\nTo address this challenge, we introduce Stanceformer, a target-aware\ntransformer model that incorporates enhanced attention towards the targets\nduring both training and inference. Specifically, we design a \\textit{Target\nAwareness} matrix that increases the self-attention scores assigned to the\ntargets. We demonstrate the efficacy of the Stanceformer with various\nBERT-based models, including state-of-the-art models and Large Language Models\n(LLMs), and evaluate its performance across three stance detection datasets,\nalongside a zero-shot dataset. Our approach Stanceformer not only provides\nsuperior performance but also generalizes even to other domains, such as\nAspect-based Sentiment Analysis. We make the code publicly\navailable.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}", "published": "2024-10-09 17:24:28", "link": "http://arxiv.org/abs/2410.07083v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning\n  Engineering", "abstract": "We introduce MLE-bench, a benchmark for measuring how well AI agents perform\nat machine learning engineering. To this end, we curate 75 ML\nengineering-related competitions from Kaggle, creating a diverse set of\nchallenging tasks that test real-world ML engineering skills such as training\nmodels, preparing datasets, and running experiments. We establish human\nbaselines for each competition using Kaggle's publicly available leaderboards.\nWe use open-source agent scaffolds to evaluate several frontier language models\non our benchmark, finding that the best-performing setup--OpenAI's o1-preview\nwith AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in\n16.9% of competitions. In addition to our main results, we investigate various\nforms of resource scaling for AI agents and the impact of contamination from\npre-training. We open-source our benchmark code (github.com/openai/mle-bench/)\nto facilitate future research in understanding the ML engineering capabilities\nof AI agents.", "published": "2024-10-09 17:34:27", "link": "http://arxiv.org/abs/2410.07095v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unleashing Multi-Hop Reasoning Potential in Large Language Models\n  through Repetition of Misordered Context", "abstract": "Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the position of\nsupporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order in which\nthe supporting documents are presented. We refer to this as the misordered\ncontext problem. To address this issue, we propose a simple yet effective\nmethod called context repetition (CoRe), which involves prompting the model by\nrepeatedly presenting the context to ensure the supporting documents are\npresented in the optimal order for the model. Using CoRe, we improve the F1\nscore by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p\non a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.", "published": "2024-10-09 17:41:53", "link": "http://arxiv.org/abs/2410.07103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Readiness of Prominent Small Language Models for the\n  Democratization of Financial Literacy", "abstract": "The use of small language models (SLMs), herein defined as models with less\nthan three billion parameters, is increasing across various domains and\napplications. Due to their ability to run on more accessible hardware and\npreserve user privacy, SLMs possess the potential to democratize access to\nlanguage models for individuals of different socioeconomic status and with\ndifferent privacy preferences. This study assesses several state-of-the-art\nSLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama\nproject) for use in the financial domain to support the development of\nfinancial literacy LMs. Democratizing access to quality financial information\nfor those who are financially under educated is greatly needed in society,\nparticularly as new financial markets and products emerge and participation in\nfinancial markets increases due to ease of access. We are the first to examine\nthe use of open-source SLMs to democratize access to financial question\nanswering capabilities for individuals and students. To this end, we provide an\nanalysis of the memory usage, inference time, similarity comparisons to\nground-truth answers, and output readability of prominent SLMs to determine\nwhich models are most accessible and capable of supporting access to financial\ninformation. We analyze zero-shot and few-shot learning variants of the models.\nThe results suggest that some off-the-shelf SLMs merit further exploration and\nfine-tuning to prepare them for individual use, while others may have limits to\ntheir democratization.", "published": "2024-10-09 17:48:40", "link": "http://arxiv.org/abs/2410.07118v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data\n  for LLM Pruning", "abstract": "Network pruning has emerged as a potential solution to make LLMs cheaper to\ndeploy. However, existing LLM pruning approaches universally rely on the C4\ndataset as the calibration data for calculating pruning scores, leaving its\noptimality unexplored. In this study, we evaluate the choice of calibration\ndata on LLM pruning, across a wide range of datasets that are most commonly\nused in LLM training and evaluation, including four pertaining datasets as well\nas three categories of downstream tasks encompassing nine datasets. Each\ndownstream dataset is prompted with In-Context Learning (ICL) and\nChain-of-Thought (CoT), respectively. Besides the already intriguing\nobservation that the choice of calibration data significantly impacts the\nperformance of pruned LLMs, our results also uncover several subtle and often\nunexpected findings, summarized as follows: (1) C4 is not the optimal choice\nfor LLM pruning, even among commonly used pre-training datasets; (2) arithmetic\ndatasets, when used as calibration data, performs on par or even better than\npre-training datasets; (3) pruning with downstream datasets does not\nnecessarily help the corresponding downstream task, compared to pre-training\ndata; (4) ICL is widely beneficial to all data categories, whereas CoT is only\nuseful on certain tasks. Our findings shed light on the importance of carefully\nselecting calibration data for LLM pruning and pave the way for more efficient\ndeployment of these powerful models in real-world applications. We release our\ncode at: https://github.com/abx393/llm-pruning-calibration-data.", "published": "2024-10-09 22:00:19", "link": "http://arxiv.org/abs/2410.07461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Localizing Factual Inconsistencies in Attributable Text Generation", "abstract": "There has been an increasing interest in detecting hallucinations in\nmodel-generated texts, both manually and automatically, at varying levels of\ngranularity. However, most existing methods fail to precisely pinpoint the\nerrors. In this work, we introduce QASemConsistency, a new formalism for\nlocalizing factual inconsistencies in attributable text generation, at a\nfine-grained level. Drawing inspiration from Neo-Davidsonian formal semantics,\nwe propose decomposing the generated text into minimal predicate-argument level\npropositions, expressed as simple question-answer (QA) pairs, and assess\nwhether each individual QA pair is supported by a trusted reference text. As\neach QA pair corresponds to a single semantic relation between a predicate and\nan argument, QASemConsistency effectively localizes the unsupported\ninformation. We first demonstrate the effectiveness of the QASemConsistency\nmethodology for human annotation, by collecting crowdsourced annotations of\ngranular consistency errors, while achieving a substantial inter-annotator\nagreement ($\\kappa > 0.7)$. Then, we implement several methods for\nautomatically detecting localized factual inconsistencies, with both supervised\nentailment models and open-source LLMs.", "published": "2024-10-09 22:53:48", "link": "http://arxiv.org/abs/2410.07473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MoDEM: Mixture of Domain Expert Models", "abstract": "We propose a novel approach to enhancing the performance and efficiency of\nlarge language models (LLMs) by combining domain prompt routing with\ndomain-specialized models. We introduce a system that utilizes a BERT-based\nrouter to direct incoming prompts to the most appropriate domain expert model.\nThese expert models are specifically tuned for domains such as health,\nmathematics and science. Our research demonstrates that this approach can\nsignificantly outperform general-purpose models of comparable size, leading to\na superior performance-to-cost ratio across various benchmarks. The\nimplications of this study suggest a potential paradigm shift in LLM\ndevelopment and deployment. Rather than focusing solely on creating\nincreasingly large, general-purpose models, the future of AI may lie in\ndeveloping ecosystems of smaller, highly specialized models coupled with\nsophisticated routing systems. This approach could lead to more efficient\nresource utilization, reduced computational costs, and superior overall\nperformance.", "published": "2024-10-09 23:52:54", "link": "http://arxiv.org/abs/2410.07490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DisasterQA: A Benchmark for Assessing the performance of LLMs in\n  Disaster Response", "abstract": "Disasters can result in the deaths of many, making quick response times\nvital. Large Language Models (LLMs) have emerged as valuable in the field. LLMs\ncan be used to process vast amounts of textual information quickly providing\nsituational context during a disaster. However, the question remains whether\nLLMs should be used for advice and decision making in a disaster. To evaluate\nthe capabilities of LLMs in disaster response knowledge, we introduce a\nbenchmark: DisasterQA created from six online sources. The benchmark covers a\nwide range of disaster response topics. We evaluated five LLMs each with four\ndifferent prompting methods on our benchmark, measuring both accuracy and\nconfidence levels through Logprobs. The results indicate that LLMs require\nimprovement on disaster response knowledge. We hope that this benchmark pushes\nforth further development of LLMs in disaster response, ultimately enabling\nthese models to work alongside. emergency managers in disasters.", "published": "2024-10-09 00:13:06", "link": "http://arxiv.org/abs/2410.20707v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and\n  Performance of SGD for Fine-Tuning Language Models", "abstract": "Fine-tuning language models (LMs) with the Adam optimizer often demands\nexcessive memory, limiting accessibility. The \"in-place\" version of Stochastic\nGradient Descent (IP-SGD) and Memory-Efficient Zeroth-order Optimizer (MeZO)\nhave been proposed to address this. However, IP-SGD still requires substantial\nmemory, and MeZO suffers from slow convergence and degraded final performance\ndue to its zeroth-order nature. This paper introduces Addax, a novel method\nthat improves both memory efficiency and performance of IP-SGD by integrating\nit with MeZO. Specifically, Addax computes zeroth- or first-order gradients of\ndata points in the minibatch based on their memory consumption, combining these\ngradient estimates to update directions. By computing zeroth-order gradients\nfor data points that require more memory and first-order gradients for others,\nAddax overcomes the slow convergence of MeZO and the excessive memory\nrequirement of IP-SGD. Additionally, the zeroth-order gradient acts as a\nregularizer for the first-order gradient, further enhancing the model's final\nperformance. Theoretically, we establish the convergence of Addax under mild\nassumptions, demonstrating faster convergence and less restrictive\nhyper-parameter choices than MeZO. Our experiments with diverse LMs and tasks\nshow that Addax consistently outperforms MeZO regarding accuracy and\nconvergence speed while having a comparable memory footprint. When fine-tuning\nOPT-13B with one A100 GPU, on average, Addax outperforms MeZO in accuracy/F1\nscore by 14% and runs 15x faster while using memory similar to MeZO. In our\nexperiments on the larger OPT-30B model, on average, Addax outperforms MeZO in\nterms of accuracy/F1 score by >16 and runs 30x faster on a single H100 GPU.\nMoreover, Addax surpasses the performance of standard fine-tuning approaches,\nsuch as IP-SGD and Adam, in most tasks with significantly less memory\nrequirement.", "published": "2024-10-09 00:49:08", "link": "http://arxiv.org/abs/2410.06441v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge\n  with Curriculum Preference Learning", "abstract": "Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique\nfor enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO\nhave enabled LLMs to distill high-quality behaviors from MCTS, improving their\nreasoning performance. However, existing distillation methods underutilize the\nrich trajectory information generated by MCTS, limiting the potential for\nimprovements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel\npairwise training framework that enables LLMs to self-improve through MCTS\nbehavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via\ntwo key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from\nchild nodes sharing the same parent in the search tree, providing step-level\ninformation for more effective MCTS behavior distillation. (2) AlphaLLM-CPL\nintroduces curriculum preference learning, dynamically adjusting the training\nsequence of trajectory pairs in each offline training epoch to prioritize\ncritical learning steps and mitigate overfitting. Experimental results on\nmathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly\noutperforms previous MCTS behavior distillation methods, substantially boosting\nthe reasoning capabilities of LLMs.", "published": "2024-10-09 03:20:02", "link": "http://arxiv.org/abs/2410.06508v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Chip-Tuning: Classify Before Language Models Say", "abstract": "The rapid development in the performance of large language models (LLMs) is\naccompanied by the escalation of model size, leading to the increasing cost of\nmodel training and inference. Previous research has discovered that certain\nlayers in LLMs exhibit redundancy, and removing these layers brings only\nmarginal loss in model performance. In this paper, we adopt the probing\ntechnique to explain the layer redundancy in LLMs and demonstrate that language\nmodels can be effectively pruned with probing classifiers. We propose\nchip-tuning, a simple and effective structured pruning framework specialized\nfor classification problems. Chip-tuning attaches tiny probing classifiers\nnamed chips to different layers of LLMs, and trains chips with the backbone\nmodel frozen. After selecting a chip for classification, all layers subsequent\nto the attached layer could be removed with marginal performance loss.\nExperimental results on various LLMs and datasets demonstrate that chip-tuning\nsignificantly outperforms previous state-of-the-art baselines in both accuracy\nand pruning ratio, achieving a pruning ratio of up to 50%. We also find that\nchip-tuning could be applied on multimodal models, and could be combined with\nmodel finetuning, proving its excellent compatibility.", "published": "2024-10-09 04:35:22", "link": "http://arxiv.org/abs/2410.06541v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "abstract": "We present TuringQ, the first benchmark designed to evaluate the reasoning\ncapabilities of large language models (LLMs) in the theory of computation.\nTuringQ consists of 4,006 undergraduate and graduate-level question-answer\npairs, categorized into four difficulty levels and covering seven core\ntheoretical areas. We evaluate several open-source LLMs, as well as GPT-4,\nusing Chain of Thought prompting and expert human assessment. Additionally, we\npropose an automated LLM-based evaluation system that demonstrates competitive\naccuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on\nTuringQ shows measurable improvements in reasoning ability and out-of-domain\ntasks such as algebra. TuringQ serves as both a benchmark and a resource for\nenhancing LLM performance in complex computational reasoning tasks. Our\nanalysis offers insights into LLM capabilities and advances in AI comprehension\nof theoretical computer science.", "published": "2024-10-09 04:53:38", "link": "http://arxiv.org/abs/2410.06547v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Investigating Cost-Efficiency of LLM-Generated Training Data for\n  Conversational Semantic Frame Analysis", "abstract": "Recent studies have demonstrated that few-shot learning allows LLMs to\ngenerate training data for supervised models at a low cost. However, the\nquality of LLM-generated data may not entirely match that of human-labeled\ndata. This raises a crucial question: how should one balance the trade-off\nbetween the higher quality but more expensive human data and the lower quality\nyet substantially cheaper LLM-generated data? In this paper, we synthesized\ntraining data for conversational semantic frame analysis using GPT-4 and\nexamined how to allocate budgets optimally to achieve the best performance. Our\nexperiments, conducted across various budget levels, reveal that optimal\ncost-efficiency is achieved by combining both human and LLM-generated data\nacross a wide range of budget levels. Notably, as the budget decreases, a\nhigher proportion of LLM-generated data becomes more preferable.", "published": "2024-10-09 05:15:13", "link": "http://arxiv.org/abs/2410.06550v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield\n  Better Language Models", "abstract": "Reinforcement Learning from Human Feedback significantly enhances Natural\nLanguage Processing by aligning language models with human expectations. A\ncritical factor in this alignment is the strength of reward models used during\ntraining. This study explores whether stronger reward models invariably lead to\nbetter language models. In this paper, through experiments on relevance,\nfactuality, and completeness tasks using the QA-FEEDBACK dataset and reward\nmodels based on Longformer, we uncover a surprising paradox: language models\ntrained with moderately accurate reward models outperform those guided by\nhighly accurate ones. This challenges the widely held belief that stronger\nreward models always lead to better language models, and opens up new avenues\nfor future research into the key factors driving model performance and how to\nchoose the most suitable reward models. Code and additional details are\navailable at https://github.com/EIT-NLP/AccuracyParadox-RLHF.", "published": "2024-10-09 05:17:08", "link": "http://arxiv.org/abs/2410.06554v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dissecting Fine-Tuning Unlearning in Large Language Models", "abstract": "Fine-tuning-based unlearning methods prevail for preventing targeted harmful,\nsensitive, or copyrighted information within large language models while\npreserving overall capabilities. However, the true effectiveness of these\nmethods is unclear. In this work, we delve into the limitations of\nfine-tuning-based unlearning through activation patching and parameter\nrestoration experiments. Our findings reveal that these methods alter the\nmodel's knowledge retrieval process, providing further evidence that they do\nnot genuinely erase the problematic knowledge embedded in the model parameters.\nInstead, the coefficients generated by the MLP components in the model's final\nlayer are the primary contributors to these seemingly positive unlearning\neffects, playing a crucial role in controlling the model's behaviors.\nFurthermore, behavioral tests demonstrate that this unlearning mechanism\ninevitably impacts the global behavior of the models, affecting unrelated\nknowledge or capabilities. The code is released at\nhttps://github.com/yihuaihong/Dissecting-FT-Unlearning.", "published": "2024-10-09 06:58:09", "link": "http://arxiv.org/abs/2410.06606v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QA-Calibration of Language Model Confidence Scores", "abstract": "To use generative question-and-answering (QA) systems for decision-making and\nin any critical application, these systems need to provide well-calibrated\nconfidence scores that reflect the correctness of their answers. Existing\ncalibration methods aim to ensure that the confidence score is, *on average*,\nindicative of the likelihood that the answer is correct. We argue, however,\nthat this standard (average-case) notion of calibration is difficult to\ninterpret for decision-making in generative QA. To address this, we generalize\nthe standard notion of average calibration and introduce QA-calibration, which\nensures calibration holds across different question-and-answer groups. We then\npropose discretized posthoc calibration schemes for achieving QA-calibration.\nWe establish distribution-free guarantees on the performance of this method and\nvalidate our method on confidence scores returned by elicitation prompts across\nmultiple QA benchmarks and large language models (LLMs).", "published": "2024-10-09 07:12:24", "link": "http://arxiv.org/abs/2410.06615v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Evolving Tools for Large Language Models", "abstract": "Tool learning enables large language models (LLMs) to interact with external\ntools and APIs, greatly expanding the application scope of LLMs. However, due\nto the dynamic nature of external environments, these tools and APIs may become\noutdated over time, preventing LLMs from correctly invoking tools. Existing\nresearch primarily focuses on static environments and overlooks this issue,\nlimiting the adaptability of LLMs in real-world applications. In this paper, we\npropose ToolEVO, a novel framework designed to enhance the adaptive and\nreflective capabilities of LLMs against tool variability. By leveraging Monte\nCarlo Tree Search, ToolEVO facilitates active exploration and interaction of\nLLMs within dynamic environments, allowing for autonomous self-reflection and\nself-updating of tool usage based on environmental feedback. Additionally, we\nintroduce ToolQA-D, a benchmark specifically designed to evaluate the impact of\ntool variability. Extensive experiments demonstrate the effectiveness and\nstability of our approach, highlighting the importance of adaptability to tool\nvariability for effective tool learning. Code:\nhttps://github.com/Chen-GX/ToolEVO", "published": "2024-10-09 07:14:45", "link": "http://arxiv.org/abs/2410.06617v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Subtle Errors Matter: Preference Learning via Error-injected\n  Self-editing", "abstract": "Large Language Models (LLMs) have exhibited strong mathematical reasoning\nprowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle yet critical\nerrors, such as miscalculations or incorrect substitutions, limit the LLMs'\nfull potential. Existing studies to improve mathematical ability typically\ninvolve applying preference learning to step-wise solution pairs. Although\nthese methods leverage samples of varying granularity to mitigate reasoning\nerrors, they overlook critical subtle errors. In this work, we propose a novel\npreference learning framework called eRror-Injected Self-Editing (RISE), which\ninjects predefined subtle errors into pivotal tokens in reasoning or\ncomputation steps to construct hard pairs for error mitigation. In detail, RISE\nuses the LLM itself to edit a small number of tokens in the solution, injecting\ndesigned subtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective without requiring fine-grained sampling or preference\nannotation. Extensive experiments validate the effectiveness of RISE, with\npreference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%\non GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect\nof error mitigation extends from mathematical reasoning to logical reasoning\nand code generation.", "published": "2024-10-09 07:43:38", "link": "http://arxiv.org/abs/2410.06638v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Code Executors: An Exploratory Study", "abstract": "The capabilities of Large Language Models (LLMs) have significantly evolved,\nextending from natural language processing to complex tasks like code\nunderstanding and generation. We expand the scope of LLMs' capabilities to a\nbroader context, using LLMs to execute code snippets to obtain the output. This\npaper pioneers the exploration of LLMs as code executors, where code snippets\nare directly fed to the models for execution, and outputs are returned. We are\nthe first to comprehensively examine this feasibility across various LLMs,\nincluding OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the\no1 model achieved over 90% accuracy in code execution, while others\ndemonstrated lower accuracy levels. Furthermore, we introduce an Iterative\nInstruction Prompting (IIP) technique that processes code snippets line by\nline, enhancing the accuracy of weaker models by an average of 7.22% (with the\nhighest improvement of 18.96%) and an absolute average improvement of 3.86%\nagainst CoT prompting (with the highest improvement of 19.46%). Our study not\nonly highlights the transformative potential of LLMs in coding but also lays\nthe groundwork for future advancements in automated programming and the\ncompletion of complex tasks.", "published": "2024-10-09 08:23:22", "link": "http://arxiv.org/abs/2410.06667v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Calibrating Verbalized Probabilities for Large Language Models", "abstract": "Calibrating verbalized probabilities presents a novel approach for reliably\nassessing and leveraging outputs from black-box Large Language Models (LLMs).\nRecent methods have demonstrated improved calibration by applying techniques\nlike Platt scaling or temperature scaling to the confidence scores generated by\nLLMs. In this paper, we explore the calibration of verbalized probability\ndistributions for discriminative tasks. First, we investigate the capability of\nLLMs to generate probability distributions over categorical labels. We\ntheoretically and empirically identify the issue of re-softmax arising from the\nscaling of verbalized probabilities, and propose using the invert softmax trick\nto approximate the \"logit\" by inverting verbalized probabilities. Through\nextensive evaluation on three public datasets, we demonstrate: (1) the robust\ncapability of LLMs in generating class distributions, and (2) the effectiveness\nof the invert softmax trick in estimating logits, which, in turn, facilitates\npost-calibration adjustments.", "published": "2024-10-09 09:20:24", "link": "http://arxiv.org/abs/2410.06707v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Mixed quantization in Large Language Models", "abstract": "Post-training quantization of Large Language Models (LLMs) has proven\neffective in reducing the computational requirements for running inference on\nthese models. In this study, we focus on a straightforward question: When\naiming for a specific accuracy or perplexity target for low-precision\nquantization, how many high-precision numbers or calculations are required to\npreserve as we scale LLMs to larger sizes? We first introduce a critical metric\nnamed the quantization ratio, which compares the number of parameters quantized\nto low-precision arithmetic against the total parameter count. Through\nextensive and carefully controlled experiments across different model families,\narithmetic types, and quantization granularities (e.g. layer-wise,\nmatmul-wise), we identify two central phenomenons. 1) The larger the models,\nthe better they can preserve performance with an increased quantization ratio,\nas measured by perplexity in pre-training tasks or accuracy in downstream\ntasks. 2) The finer the granularity of mixed-precision quantization (e.g.,\nmatmul-wise), the more the model can increase the quantization ratio. We\nbelieve these observed phenomena offer valuable insights for future AI hardware\ndesign and the development of advanced Efficient AI algorithms.", "published": "2024-10-09 09:45:01", "link": "http://arxiv.org/abs/2410.06722v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Which Programming Language and What Features at Pre-training Stage\n  Affect Downstream Logical Inference Performance?", "abstract": "Recent large language models (LLMs) have demonstrated remarkable\ngeneralization abilities in mathematics and logical reasoning tasks. Prior\nresearch indicates that LLMs pre-trained with programming language data exhibit\nhigh mathematical and reasoning abilities; however, this causal relationship\nhas not been rigorously tested. Our research aims to verify which programming\nlanguages and features during pre-training affect logical inference\nperformance. Specifically, we pre-trained decoder-based language models from\nscratch using datasets from ten programming languages (e.g., Python, C, Java)\nand three natural language datasets (Wikipedia, Fineweb, C4) under identical\nconditions. Thereafter, we evaluated the trained models in a few-shot\nin-context learning setting on logical reasoning tasks: FLD and bAbi, which do\nnot require commonsense or world knowledge. The results demonstrate that nearly\nall models trained with programming languages consistently outperform those\ntrained with natural languages, indicating that programming languages contain\nfactors that elicit logic inference performance. In addition, we found that\nmodels trained with programming languages exhibit a better ability to follow\ninstructions compared to those trained with natural languages. Further analysis\nreveals that the depth of Abstract Syntax Trees representing parsed results of\nprograms also affects logical reasoning performance. These findings will offer\ninsights into the essential elements of pre-training for acquiring the\nfoundational abilities of LLMs.", "published": "2024-10-09 10:13:13", "link": "http://arxiv.org/abs/2410.06735v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language\n  Models", "abstract": "Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task convergence but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder.", "published": "2024-10-09 10:20:32", "link": "http://arxiv.org/abs/2410.06741v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "To Preserve or To Compress: An In-Depth Study of Connector Selection in\n  Multimodal Large Language Models", "abstract": "In recent years, multimodal large language models (MLLMs) have garnered\nsignificant attention from both industry and academia. However, there is still\nconsiderable debate on constructing MLLM architectures, particularly regarding\nthe selection of appropriate connectors for perception tasks of varying\ngranularities. This paper systematically investigates the impact of connectors\non MLLM performance. Specifically, we classify connectors into\nfeature-preserving and feature-compressing types. Utilizing a unified\nclassification standard, we categorize sub-tasks from three comprehensive\nbenchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained\nperception, fine-grained perception, and reasoning, and evaluate the\nperformance. Our findings reveal that feature-preserving connectors excel in\n\\emph{fine-grained perception} tasks due to their ability to retain detailed\nvisual information. In contrast, feature-compressing connectors, while less\neffective in fine-grained perception tasks, offer significant speed advantages\nand perform comparably in \\emph{coarse-grained perception} and \\emph{reasoning}\ntasks. These insights are crucial for guiding MLLM architecture design and\nadvancing the optimization of MLLM architectures.", "published": "2024-10-09 10:53:18", "link": "http://arxiv.org/abs/2410.06765v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "From Pixels to Tokens: Revisiting Object Hallucinations in Large\n  Vision-Language Models", "abstract": "Hallucinations in large vision-language models (LVLMs) are a significant\nchallenge, i.e., generating objects that are not presented in the visual input,\nwhich impairs their reliability. Recent studies often attribute hallucinations\nto a lack of understanding of visual input, yet ignore a more fundamental\nissue: the model's inability to effectively extract or decouple visual\nfeatures. In this paper, we revisit the hallucinations in LVLMs from an\narchitectural perspective, investigating whether the primary cause lies in the\nvisual encoder (feature extraction) or the modal alignment module (feature\ndecoupling). Motivated by our findings on the preliminary investigation, we\npropose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.\nThis plug-and-play method can be integrated into various LVLMs, utilizing\nadaptive virtual tokens to extract object features from bounding boxes, thereby\naddressing hallucinations caused by insufficient decoupling of visual features.\nPATCH achieves state-of-the-art performance on multiple multi-modal\nhallucination datasets. We hope this approach provides researchers with deeper\ninsights into the underlying causes of hallucinations in LVLMs, fostering\nfurther advancements and innovation in this field.", "published": "2024-10-09 11:46:32", "link": "http://arxiv.org/abs/2410.06795v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level", "abstract": "Large language models (LLMs) have demonstrated immense utility across various\nindustries. However, as LLMs advance, the risk of harmful outputs increases due\nto incorrect or malicious instruction prompts. While current methods\neffectively address jailbreak risks, they share common limitations: 1) Judging\nharmful responses from the prefill-level lacks utilization of the model's\ndecoding outputs, leading to relatively lower effectiveness and robustness. 2)\nRejecting potentially harmful responses based on a single evaluation can\nsignificantly impair the model's helpfulness.This paper examines the LLMs'\ncapability to recognize harmful outputs, revealing and quantifying their\nproficiency in assessing the danger of previous tokens. Motivated by pilot\nexperiment results, we design a robust defense mechanism at the decoding level.\nOur novel decoder-oriented, step-by-step defense architecture corrects harmful\nqueries directly rather than rejecting them outright. We introduce speculative\ndecoding to enhance usability and facilitate deployment to boost secure\ndecoding speed. Extensive experiments demonstrate that our approach improves\nmodel security without compromising reasoning speed. Notably, our method\nleverages the model's ability to discern hazardous information, maintaining its\nhelpfulness compared to existing methods.", "published": "2024-10-09 12:09:30", "link": "http://arxiv.org/abs/2410.06809v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent\n  Approach", "abstract": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nTypes, and Distorted Handling Solutions. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a multi\nagent framework inspired by expert developer strategies for exception handling.\nSeeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist\nLLMs in detecting, capturing, and resolving exceptions more effectively. Our\nwork is the first systematic study on leveraging LLMs to enhance exception\nhandling practices, providing valuable insights for future improvements in code\nreliability.", "published": "2024-10-09 14:45:45", "link": "http://arxiv.org/abs/2410.06949v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Self-Boosting Large Language Models with Synthetic Preference Data", "abstract": "Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard.", "published": "2024-10-09 14:57:31", "link": "http://arxiv.org/abs/2410.06961v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment", "abstract": "Despite advancements in Large Language Model (LLM) alignment, understanding\nthe reasons behind LLM preferences remains crucial for bridging the gap between\ndesired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or\nproducing overly verbose outputs. However, current methods for evaluating\npreference alignment often lack explainability, relying on coarse-grained\ncomparisons. To address this, we introduce PROFILE (PRObing Factors of\nInfLuence for Explainability), a novel framework that uncovers and quantifies\nthe influence of specific factors driving preferences. PROFILE's factor level\nanalysis explains the 'why' behind human-model alignment and misalignment,\noffering insights into the direction of model improvement. We apply PROFILE to\nanalyze human and LLM preferences across three tasks: summarization, helpful\nresponse generation, and document-based question-answering. Our factor level\nanalysis reveals a substantial discrepancy between human and LLM preferences in\ngeneration tasks, whereas LLMs show strong alignment with human preferences in\nevaluation tasks. We demonstrate how leveraging factor level insights,\nincluding addressing misaligned factors or exploiting the generation-evaluation\ngap, can improve alignment with human preferences. This work underscores the\nimportance of explainable preference analysis and highlights PROFILE's\npotential to provide valuable training signals, driving further improvements in\nhuman-model alignment.", "published": "2024-10-09 15:02:34", "link": "http://arxiv.org/abs/2410.06965v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personal Intelligence System UniLM: Hybrid On-Device Small Language\n  Model and Server-Based Large Language Model for Malay Nusantara", "abstract": "In contexts with limited computational and data resources, high-resource\nlanguage models often prove inadequate, particularly when addressing the\nspecific needs of Malay languages. This paper introduces a Personal\nIntelligence System designed to efficiently integrate both on-device and\nserver-based models. The system incorporates SLiM-34M for on-device processing,\noptimized for low memory and power usage, and MANYAK-1.3B for server-based\ntasks, allowing for scalable, high-performance language processing. The models\nachieve significant results across various tasks, such as machine translation,\nquestion-answering, and translate IndoMMLU. Particularly noteworthy is\nSLiM-34M's ability to achieve a high improvement in accuracy compared to other\nLLMs while using 2 times fewer pre-training tokens. This work challenges the\nprevailing assumption that large-scale computational resources are necessary to\nbuild effective language models, contributing to the development of\nresource-efficient models for the Malay language with the unique orchestration\nbetween SLiM-34M and MANYAK-1.3B.", "published": "2024-10-09 15:11:13", "link": "http://arxiv.org/abs/2410.06973v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with\n  Patent-Paper Pairs", "abstract": "Dealing with long and highly complex technical text is a challenge for Large\nLanguage Models (LLMs), which still have to unfold their potential in\nsupporting expensive and timeintensive processes like patent drafting. Within\npatents, the description constitutes more than 90% of the document on average.\nYet, its automatic generation remains understudied. When drafting patent\napplications, patent attorneys typically receive invention reports (IRs), which\nare usually confidential, hindering research on LLM-supported patent drafting.\nOften, prepublication research papers serve as IRs. We leverage this duality to\nbuild PAP2PAT, an open and realistic benchmark for patent drafting consisting\nof 1.8k patent-paper pairs describing the same inventions. To address the\ncomplex longdocument patent generation task, we propose chunk-based\noutline-guided generation using the research paper as invention specification.\nOur extensive evaluation using PAP2PAT and a human case study show that LLMs\ncan effectively leverage information from the paper, but still struggle to\nprovide the necessary level of detail. Fine-tuning leads to more patent-style\nlanguage, but also to more hallucination. We release our data and code\nhttps://github.com/boschresearch/Pap2Pat.", "published": "2024-10-09 15:52:48", "link": "http://arxiv.org/abs/2410.07009v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CheXalign: Preference fine-tuning in chest X-ray interpretation models\n  without human feedback", "abstract": "Radiologists play a crucial role in translating medical images into\nactionable reports. However, the field faces staffing shortages and increasing\nworkloads. While automated approaches using vision-language models (VLMs) show\npromise as assistants, they require exceptionally high accuracy. Most current\nVLMs in radiology rely solely on supervised fine-tuning. Meanwhile, additional\npreference fine-tuning in the post-training pipeline has become standard\npractice in the general domain. The challenge in radiology lies in the\nprohibitive cost of obtaining radiologist feedback at scale. To address this\nchallenge, we propose an automated pipeline for preference feedback, focusing\non chest X-ray radiology report generation (RRG). Specifically, our method\nleverages publicly available datasets containing pairs of images and\nradiologist-written reference reports with reference-based metrics, or Judges,\neliminating the need for additional radiologist feedback. We investigate reward\noveroptimization via length exploitation in this setting and introduce a\nlength-controlled version of the GREEN score. Our best-performing setup\nachieves state-of-the-art CheXbert scores on the MIMIC-CXR dataset for the RRG\ntask while on average maintaining robust performance across six additional\nimage perception and reasoning tasks.", "published": "2024-10-09 16:07:11", "link": "http://arxiv.org/abs/2410.07025v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Clean Evaluations on Contaminated Visual Language Models", "abstract": "How to evaluate large language models (LLMs) cleanly has been established as\nan important research era to genuinely report the performance of possibly\ncontaminated LLMs. Yet, how to cleanly evaluate the visual language models\n(VLMs) is an under-studied problem. We propose a novel approach to achieve such\ngoals through data augmentation methods on the visual input information. We\nthen craft a new visual clean evaluation benchmark with thousands of data\ninstances. Through extensive experiments, we found that the traditional visual\ndata augmentation methods are useful, but they are at risk of being used as a\npart of the training data as a workaround. We further propose using BGR\naugmentation to switch the colour channel of the visual information. We found\nthat it is a simple yet effective method for reducing the effect of data\ncontamination and fortunately, it is also harmful to be used as a data\naugmentation method during training. It means that it is hard to integrate such\ndata augmentation into training by malicious trainers and it could be a\npromising technique to cleanly evaluate visual LLMs. Our code, data, and model\nweights will be released upon publication.", "published": "2024-10-09 16:13:19", "link": "http://arxiv.org/abs/2410.07030v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit\n  Positional Awareness", "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious domains, including role-playing, creative writing, mathematical\nreasoning, and coding. Despite these advancements, LLMs still encounter\nchallenges with length control, frequently failing to adhere to specific length\nconstraints due to their token-level operations and insufficient training on\ndata with strict length limitations. We identify this issue as stemming from a\nlack of positional awareness and propose novel approaches--PositionID Prompting\nand PositionID Fine-Tuning--to address it. These methods enhance the model's\nability to continuously monitor and manage text length during generation.\nAdditionally, we introduce PositionID CP Prompting to enable LLMs to perform\ncopy and paste operations accurately. Furthermore, we develop two benchmarks\nfor evaluating length control and copy-paste abilities. Our experiments\ndemonstrate that our methods significantly improve the model's adherence to\nlength constraints and copy-paste accuracy without compromising response\nquality.", "published": "2024-10-09 16:15:36", "link": "http://arxiv.org/abs/2410.07035v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robots in the Middle: Evaluating LLMs in Dispute Resolution", "abstract": "Mediation is a dispute resolution method featuring a neutral third-party\n(mediator) who intervenes to help the individuals resolve their dispute. In\nthis paper, we investigate to which extent large language models (LLMs) are\nable to act as mediators. We investigate whether LLMs are able to analyze\ndispute conversations, select suitable intervention types, and generate\nappropriate intervention messages. Using a novel, manually created dataset of\n50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human\nannotators across several key metrics. Overall, the LLMs showed strong\nperformance, even outperforming our human annotators across dimensions.\nSpecifically, in 62% of the cases, the LLMs chose intervention types that were\nrated as better than or equivalent to those chosen by humans. Moreover, in 84%\nof the cases, the intervention messages generated by the LLMs were rated as\nbetter than or equal to the intervention messages written by humans. LLMs\nlikewise performed favourably on metrics such as impartiality, understanding\nand contextualization. Our results demonstrate the potential of integrating AI\nin online dispute resolution (ODR) platforms.", "published": "2024-10-09 16:51:10", "link": "http://arxiv.org/abs/2410.07053v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Mitigating the Language Mismatch and Repetition Issues in LLM-based\n  Machine Translation via Model Editing", "abstract": "Large Language Models (LLMs) have recently revolutionized the NLP field,\nwhile they still fall short in some specific down-stream tasks. In the work, we\nfocus on utilizing LLMs to perform machine translation, where we observe that\ntwo patterns of errors frequently occur and drastically affect the translation\nquality: language mismatch and repetition. The work sets out to explore the\npotential for mitigating these two issues by leveraging model editing methods,\ne.g., by locating Feed-Forward Network (FFN) neurons or something that are\nresponsible for the errors and deactivating them in the inference time. We find\nthat directly applying such methods either limited effect on the targeted\nerrors or has significant negative side-effect on the general translation\nquality, indicating that the located components may also be crucial for\nensuring machine translation with LLMs on the rails. To this end, we propose to\nrefine the located components by fetching the intersection of the locating\nresults under different language settings, filtering out the aforementioned\ninformation that is irrelevant to targeted errors. The experiment results\nempirically demonstrate that our methods can effectively reduce the language\nmismatch and repetition ratios and meanwhile enhance or keep the general\ntranslation quality in most cases.", "published": "2024-10-09 16:51:21", "link": "http://arxiv.org/abs/2410.07054v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pixtral 12B", "abstract": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license.", "published": "2024-10-09 17:16:22", "link": "http://arxiv.org/abs/2410.07073v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mental Disorders Detection in the Era of Large Language Models", "abstract": "This paper compares the effectiveness of traditional machine learning\nmethods, encoder-based models, and large language models (LLMs) on the task of\ndetecting depression and anxiety. Five datasets were considered, each differing\nin format and the method used to define the target pathology class. We tested\nAutoML models based on linguistic features, several variations of encoder-based\nTransformers such as BERT, and state-of-the-art LLMs as pathology\nclassification models. The results demonstrated that LLMs outperform\ntraditional methods, particularly on noisy and small datasets where training\nexamples vary significantly in text length and genre. However, psycholinguistic\nfeatures and encoder-based models can achieve performance comparable to\nlanguage models when trained on texts from individuals with clinically\nconfirmed depression, highlighting their potential effectiveness in targeted\nclinical applications.", "published": "2024-10-09 17:51:55", "link": "http://arxiv.org/abs/2410.07129v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with\n  Modality Integration Rate", "abstract": "We present the Modality Integration Rate (MIR), an effective, robust, and\ngeneralized metric to indicate the multi-modal pre-training quality of Large\nVision Language Models (LVLMs). Large-scale pre-training plays a critical role\nin building capable LVLMs, while evaluating its training quality without the\ncostly supervised fine-tuning stage is under-explored. Loss, perplexity, and\nin-context evaluation results are commonly used pre-training metrics for Large\nLanguage Models (LLMs), while we observed that these metrics are less\nindicative when aligning a well-trained LLM with a new modality. Due to the\nlack of proper metrics, the research of LVLMs in the critical pre-training\nstage is hindered greatly, including the training data choice, efficient module\ndesign, etc. In this paper, we propose evaluating the pre-training quality from\nthe inter-modal distribution distance perspective and present MIR, the Modality\nIntegration Rate, which is 1) \\textbf{Effective} to represent the pre-training\nquality and show a positive relation with the benchmark performance after\nsupervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation\ndata. 3) \\textbf{Generalize} across training configurations and architecture\nchoices. We conduct a series of pre-training experiments to explore the\neffectiveness of MIR and observe satisfactory results that MIR is indicative\nabout training data selection, training strategy schedule, and model\narchitecture design to get better pre-training results. We hope MIR could be a\nhelpful metric for building capable LVLMs and inspire the following research\nabout modality alignment in different areas. Our code is at:\nhttps://github.com/shikiw/Modality-Integration-Rate.", "published": "2024-10-09 17:59:04", "link": "http://arxiv.org/abs/2410.07167v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language\n  Models", "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to\nassess LLMs on agent-based data science tasks. This benchmark features three\ncore elements: First, the tasks within DA-Code are inherently challenging,\nsetting them apart from traditional code generation tasks and demanding\nadvanced coding skills in grounding and planning. Second, examples in DA-Code\nare all based on real and diverse data, covering a wide range of complex data\nwrangling and analytics tasks. Third, to solve the tasks, the models must\nutilize complex data science programming languages, to perform intricate data\nprocessing and derive the answers. We set up the benchmark in a controllable\nand executable environment that aligns with real-world data analysis scenarios\nand is scalable. The annotators meticulously design the evaluation suite to\nensure the accuracy and robustness of the evaluation. We develop the DA-Agent\nbaseline. Experiments show that although the baseline performs better than\nother existing frameworks, using the current best LLMs achieves only 30.5%\naccuracy, leaving ample room for improvement. We release our benchmark at\nhttps://da-code-bench.github.io.", "published": "2024-10-09 18:00:05", "link": "http://arxiv.org/abs/2410.07331v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers", "abstract": "The performance of Transformer models has been enhanced by increasing the\nnumber of parameters and the length of the processed text. Consequently,\nfine-tuning the entire model becomes a memory-intensive process.\nHigh-performance methods for parameter-efficient fine-tuning (PEFT) typically\nwork with Attention blocks and often overlook MLP blocks, which contain about\nhalf of the model parameters. We propose a new selective PEFT method, namely\nSparseGrad, that performs well on MLP blocks. We transfer layer gradients to a\nspace where only about 1\\% of the layer's elements remain significant. By\nconverting gradients into a sparse structure, we reduce the number of updated\nparameters. We apply SparseGrad to fine-tune BERT and RoBERTa for the NLU task\nand LLaMa-2 for the Question-Answering task. In these experiments, with\nidentical memory requirements, our method outperforms LoRA and MeProp, robust\npopular state-of-the-art PEFT approaches.", "published": "2024-10-09 19:03:52", "link": "http://arxiv.org/abs/2410.07383v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transducer Consistency Regularization for Speech to Text Applications", "abstract": "Consistency regularization is a commonly used practice to encourage the model\nto generate consistent representation from distorted input features and improve\nmodel generalization. It shows significant improvement on various speech\napplications that are optimized with cross entropy criterion. However, it is\nnot straightforward to apply consistency regularization for the\ntransducer-based approaches, which are widely adopted for speech applications\ndue to the competitive performance and streaming characteristic. The main\nchallenge is from the vast alignment space of the transducer optimization\ncriterion and not all the alignments within the space contribute to the model\noptimization equally. In this study, we present Transducer Consistency\nRegularization (TCR), a consistency regularization method for transducer\nmodels. We apply distortions such as spec augmentation and dropout to create\ndifferent data views and minimize the distribution difference. We utilize\noccupational probabilities to give different weights on transducer output\ndistributions, thus only alignments close to oracle alignments would contribute\nto the model learning. Our experiments show the proposed method is superior to\nother consistency regularization implementations and could effectively reduce\nword error rate (WER) by 4.3\\% relatively comparing with a strong baseline on\nthe \\textsc{Librispeech} dataset.", "published": "2024-10-09 23:53:13", "link": "http://arxiv.org/abs/2410.07491v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Recent advancements in LLM Red-Teaming: Techniques, Defenses, and\n  Ethical Considerations", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks, but their vulnerability to jailbreak attacks\nposes significant security risks. This survey paper presents a comprehensive\nanalysis of recent advancements in attack strategies and defense mechanisms\nwithin the field of Large Language Model (LLM) red-teaming. We analyze various\nattack methods, including gradient-based optimization, reinforcement learning,\nand prompt engineering approaches. We discuss the implications of these attacks\non LLM safety and the need for improved defense mechanisms. This work aims to\nprovide a thorough understanding of the current landscape of red-teaming\nattacks and defenses on LLMs, enabling the development of more secure and\nreliable language models.", "published": "2024-10-09 01:35:38", "link": "http://arxiv.org/abs/2410.09097v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimizing Transformer based on high-performance optimizer for\n  predicting employment sentiment in American social media content", "abstract": "This article improves the Transformer model based on swarm intelligence\noptimization algorithm, aiming to predict the emotions of employment related\ntext content on American social media. Through text preprocessing, feature\nextraction, and vectorization, the text data was successfully converted into\nnumerical data and imported into the model for training. The experimental\nresults show that during the training process, the accuracy of the model\ngradually increased from 49.27% to 82.83%, while the loss value decreased from\n0.67 to 0.35, indicating a significant improvement in the performance of the\nmodel on the training set. According to the confusion matrix analysis of the\ntraining set, the accuracy of the training set is 86.15%. The confusion matrix\nof the test set also showed good performance, with an accuracy of 82.91%. The\naccuracy difference between the training set and the test set is only 3.24%,\nindicating that the model has strong generalization ability. In addition, the\nevaluation of polygon results shows that the model performs well in\nclassification accuracy, sensitivity, specificity, and area under the curve\n(AUC), with a Kappa coefficient of 0.66 and an F-measure of 0.80, further\nverifying the effectiveness of the model in social media sentiment analysis.\nThe improved model proposed in this article not only improves the accuracy of\nsentiment recognition in employment related texts on social media, but also has\nimportant practical significance. This social media based data analysis method\ncan not only capture social dynamics in a timely manner, but also promote\ndecision-makers to pay attention to public concerns and provide data support\nfor improving employment conditions.", "published": "2024-10-09 03:14:05", "link": "http://arxiv.org/abs/2410.10874v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Data Efficiency via Curating LLM-Driven Rating Systems", "abstract": "Instruction tuning is critical for adapting large language models (LLMs) to\ndownstream tasks, and recent studies have demonstrated that small amounts of\nhuman-curated data can outperform larger datasets, challenging traditional data\nscaling laws. While LLM-based data quality rating systems offer a\ncost-effective alternative to human annotation, they often suffer from\ninaccuracies and biases, even in powerful models like GPT-4. In this work, we\nintroduce DS2, a Diversity-aware Score curation method for Data Selection. By\nsystematically modeling error patterns through a score transition matrix, DS2\ncorrects LLM-based scores and promotes diversity in the selected data samples.\nOur approach shows that a curated subset (just 3.3% of the original dataset)\noutperforms full-scale datasets (300k samples) across various machine-alignment\nbenchmarks, and matches or surpasses human-aligned datasets such as LIMA with\nthe same sample size (1k samples). These findings challenge conventional data\nscaling assumptions, highlighting that redundant, low-quality samples can\ndegrade performance and reaffirming that \"more can be less.\"", "published": "2024-10-09 10:07:55", "link": "http://arxiv.org/abs/2410.10877v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Capturing Bias Diversity in LLMs", "abstract": "This paper presents research on enhancements to Large Language Models (LLMs)\nthrough the addition of diversity in its generated outputs. Our study\nintroduces a configuration of multiple LLMs which demonstrates the diversities\ncapable with a single LLM. By developing multiple customised instances of a GPT\nmodel, each reflecting biases in specific demographic characteristics including\ngender, age, and race, we propose, develop and evaluate a framework for a more\nnuanced and representative AI dialogue which we call BiasGPT. The customised\nGPT models will ultimately collaborate, merging their diverse perspectives on a\ntopic into an integrated response that captures a broad spectrum of human\nexperiences and viewpoints. In this paper, through experiments, we demonstrate\nthe capabilities of a GPT model to embed different biases which, when combined,\ncan open the possibilities of more inclusive AI technologies.", "published": "2024-10-09 17:07:50", "link": "http://arxiv.org/abs/2410.12839v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Two-Model Approach for Humour Style Recognition", "abstract": "Humour, a fundamental aspect of human communication, manifests itself in\nvarious styles that significantly impact social interactions and mental health.\nRecognising different humour styles poses challenges due to the lack of\nestablished datasets and machine learning (ML) models. To address this gap, we\npresent a new text dataset for humour style recognition, comprising 1463\ninstances across four styles (self-enhancing, self-deprecating, affiliative,\nand aggressive) and non-humorous text, with lengths ranging from 4 to 229\nwords. Our research employs various computational methods, including classic\nmachine learning classifiers, text embedding models, and DistilBERT, to\nestablish baseline performance. Additionally, we propose a two-model approach\nto enhance humour style recognition, particularly in distinguishing between\naffiliative and aggressive styles. Our method demonstrates an 11.61%\nimprovement in f1-score for affiliative humour classification, with consistent\nimprovements in the 14 models tested. Our findings contribute to the\ncomputational analysis of humour in text, offering new tools for studying\nhumour in literature, social media, and other textual sources.", "published": "2024-10-09 18:25:07", "link": "http://arxiv.org/abs/2410.12842v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Prompt Engineering: A Systematic Review with SWOT Analysis", "abstract": "In this paper, we conduct a comprehensive SWOT analysis of prompt engineering\ntechniques within the realm of Large Language Models (LLMs). Emphasizing\nlinguistic principles, we examine various techniques to identify their\nstrengths, weaknesses, opportunities, and threats. Our findings provide\ninsights into enhancing AI interactions and improving language model\ncomprehension of human prompts. The analysis covers techniques including\ntemplate-based approaches and fine-tuning, addressing the problems and\nchallenges associated with each. The conclusion offers future research\ndirections aimed at advancing the effectiveness of prompt engineering in\noptimizing human-machine communication.", "published": "2024-10-09 19:48:35", "link": "http://arxiv.org/abs/2410.12843v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TextLap: Customizing Language Models for Text-to-Layout Planning", "abstract": "Automatic generation of graphical layouts is crucial for many real-world\napplications, including designing posters, flyers, advertisements, and\ngraphical user interfaces. Given the incredible ability of Large language\nmodels (LLMs) in both natural language understanding and generation, we believe\nthat we could customize an LLM to help people create compelling graphical\nlayouts starting with only text instructions from the user. We call our method\nTextLap (text-based layout planning). It uses a curated instruction-based\nlayout planning dataset (InsLap) to customize LLMs as a graphic designer. We\ndemonstrate the effectiveness of TextLap and show that it outperforms strong\nbaselines, including GPT-4 based methods, for image generation and graphical\ndesign benchmarks.", "published": "2024-10-09 19:51:38", "link": "http://arxiv.org/abs/2410.12844v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QuAILoRA: Quantization-Aware Initialization for LoRA", "abstract": "QLoRA reduces the memory-cost of fine-tuning a large language model (LLM)\nwith LoRA by quantizing the base LLM. However, quantization introduces\nquantization errors that negatively impact model performance after fine-tuning.\nIn this paper we introduce QuAILoRA, a quantization-aware initialization for\nLoRA that mitigates this negative impact by decreasing quantization errors at\ninitialization. Our method spends a small amount of computational overhead to\ncompute this quantization-aware initialization, without increasing the\nmemory-cost of fine-tuning. We evaluate our method on several causal language\nmodeling and downstream evaluation tasks using several different model sizes\nand families. We observe that almost all LLMs fined-tuned with QuAILoRA achieve\nbetter validation perplexity. When evaluated on downstream tasks, we find that\nQuAILoRA yields improvements proportional to the negative effect of\nquantization error. On average, applying QuAILoRA to 4-bit QLoRA models yields\n75% of the validation perplexity decrease and 86% of the downstream task\naccuracy increase as doubling the quantization precision to 8-bit, without\nincreasing GPU memory utilization during fine-tuning.", "published": "2024-10-09 19:06:37", "link": "http://arxiv.org/abs/2410.14713v1", "categories": ["cs.LG", "cs.CL", "68T50"], "primary_category": "cs.LG"}
{"title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for\n  Enhanced Following of Instructions with Multiple Constraints", "abstract": "Instruction following is a key capability for LLMs. However, recent studies\nhave shown that LLMs often struggle with instructions containing multiple\nconstraints (e.g. a request to create a social media post \"in a funny tone\"\nwith \"no hashtag\"). Despite this, most evaluations focus solely on synthetic\ndata. To address this, we introduce RealInstruct, the first benchmark designed\nto evaluate LLMs' ability to follow real-world multi-constrained instructions\nby leveraging queries real users asked AI assistants. We also investigate\nmodel-based evaluation as a cost-effective alternative to human annotation for\nthis task. Our findings reveal that even the proprietary GPT-4 model fails to\nmeet at least one constraint on over 21% of instructions, highlighting the\nlimitations of state-of-the-art models. To address the performance gap between\nopen-source and proprietary models, we propose the Decompose, Critique and\nRefine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to\nfollow constraints. DeCRIM works by decomposing the original instruction into a\nlist of constraints and using a Critic model to decide when and where the LLM's\nresponse needs refinement. Our results show that DeCRIM improves Mistral's\nperformance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback.\nMoreover, we demonstrate that with strong feedback, open-source LLMs with\nDeCRIM can outperform GPT-4 on both benchmarks.", "published": "2024-10-09 01:25:10", "link": "http://arxiv.org/abs/2410.06458v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TorchTitan: One-stop PyTorch native solution for production ready LLM\n  pre-training", "abstract": "The development of large language models (LLMs) has been instrumental in\nadvancing state-of-the-art natural language processing applications. Training\nLLMs with billions of parameters and trillions of tokens require sophisticated\ndistributed systems that enable composing and comparing several\nstate-of-the-art techniques in order to efficiently scale across thousands of\naccelerators. However, existing solutions are complex, scattered across\nmultiple libraries/repositories, lack interoperability, and are cumbersome to\nmaintain. Thus, curating and empirically comparing training recipes require\nnon-trivial engineering effort.\n  This paper introduces TorchTitan, an open-source, PyTorch-native distributed\ntraining system that unifies state-of-the-art techniques, streamlining\nintegration and reducing overhead. TorchTitan enables 3D parallelism in a\nmodular manner with elastic scaling, providing comprehensive logging,\ncheckpointing, and debugging tools for production-ready training. It also\nincorporates hardware-software co-designed solutions, leveraging features like\nFloat8 training and SymmetricMemory. As a flexible test bed, TorchTitan\nfacilitates custom recipe curation and comparison, allowing us to develop\noptimized training recipes for Llama 3.1 and provide guidance on selecting\ntechniques for maximum efficiency based on our experiences.\n  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8\nbillion to 405 billion parameters, and showcase its exceptional performance,\nmodular composability, and elastic scalability. By stacking training\noptimizations, we demonstrate accelerations of 65.08% with 1D parallelism at\nthe 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at\nthe 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at\nthe 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized\nbaselines.", "published": "2024-10-09 03:26:11", "link": "http://arxiv.org/abs/2410.06511v2", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do great minds think alike? Investigating Human-AI Complementarity in\n  Question Answering with CAIMIRA", "abstract": "Recent advancements of large language models (LLMs) have led to claims of AI\nsurpassing humans in natural language processing (NLP) tasks such as textual\nunderstanding and reasoning. This work investigates these assertions by\nintroducing CAIMIRA, a novel framework rooted in item response theory (IRT)\nthat enables quantitative assessment and comparison of problem-solving\nabilities of question-answering (QA) agents: humans and AI systems. Through\nanalysis of over 300,000 responses from ~70 AI systems and 155 humans across\nthousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in\nknowledge domains and reasoning skills. Humans outperform AI systems in\nknowledge-grounded abductive and conceptual reasoning, while state-of-the-art\nLLMs like GPT-4 and LLaMA show superior performance on targeted information\nretrieval and fact-based reasoning, particularly when information gaps are\nwell-defined and addressable through pattern matching or data retrieval. These\nfindings highlight the need for future QA tasks to focus on questions that\nchallenge not only higher-order reasoning and scientific thinking, but also\ndemand nuanced linguistic interpretation and cross-contextual knowledge\napplication, helping advance AI developments that better emulate or complement\nhuman cognitive abilities in real-world problem-solving.", "published": "2024-10-09 03:53:26", "link": "http://arxiv.org/abs/2410.06524v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ETA: Evaluating Then Aligning Safety of Vision Language Models at\n  Inference Time", "abstract": "Vision Language Models (VLMs) have become essential backbones for multimodal\nintelligence, yet significant safety challenges limit their real-world\napplication. While textual inputs are often effectively safeguarded,\nadversarial visual inputs can easily bypass VLM defense mechanisms. Existing\ndefense methods are either resource-intensive, requiring substantial data and\ncompute, or fail to simultaneously ensure safety and usefulness in responses.\nTo address these limitations, we propose a novel two-phase inference-time\nalignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual\ncontents and output responses to establish a robust safety awareness in\nmultimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep\nlevels by conditioning the VLMs' generative distribution with an interference\nprefix and performing sentence-level best-of-N to search the most harmless and\nhelpful generation paths. Extensive experiments show that ETA outperforms\nbaseline methods in terms of harmlessness, helpfulness, and efficiency,\nreducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%\nwin-ties in GPT-4 helpfulness evaluation. The code is publicly available at\nhttps://github.com/DripNowhy/ETA.", "published": "2024-10-09 07:21:43", "link": "http://arxiv.org/abs/2410.06625v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enhancing Multimodal LLM for Detailed and Accurate Video Captioning\n  using Multi-Round Preference Optimization", "abstract": "Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimization (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimized using DPO. To further improve training, we\nintroduce a novel multi-round DPO (mrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initializing the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilize the\nprocess. To address potential catastrophic forgetting of non-captioning\nabilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO\nLLM by using the captions generated by the mrDPO-trained model as supervised\nlabels. Experiments show that mrDPO significantly enhances video-SALMONN 2's\ncaptioning accuracy, reducing global and local error rates by 40\\% and 20\\%,\nrespectively, while decreasing the repetition rate by 35\\%. The final\nvideo-SALMONN 2 model, with just 7 billion parameters, surpasses leading models\nsuch as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining\ncompetitive performance to the state-of-the-art on widely used video\nquestion-answering benchmark among models of similar size. Upon acceptance, we\nwill release the code, model checkpoints, and training and test data. Demos are\navailable at\n\\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.", "published": "2024-10-09 08:44:47", "link": "http://arxiv.org/abs/2410.06682v2", "categories": ["cs.CV", "cs.CL", "eess.IV"], "primary_category": "cs.CV"}
{"title": "PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs", "abstract": "In this work, we introduce PII-Scope, a comprehensive benchmark designed to\nevaluate state-of-the-art methodologies for PII extraction attacks targeting\nLLMs across diverse threat settings. Our study provides a deeper understanding\nof these attacks by uncovering several hyperparameters (e.g., demonstration\nselection) crucial to their effectiveness. Building on this understanding, we\nextend our study to more realistic attack scenarios, exploring PII attacks that\nemploy advanced adversarial strategies, including repeated and diverse\nquerying, and leveraging iterative learning for continual PII extraction.\nThrough extensive experimentation, our results reveal a notable underestimation\nof PII leakage in existing single-query attacks. In fact, we show that with\nsophisticated adversarial capabilities and a limited query budget, PII\nextraction rates can increase by up to fivefold when targeting the pretrained\nmodel. Moreover, we evaluate PII leakage on finetuned models, showing that they\nare more vulnerable to leakage than pretrained models. Overall, our work\nestablishes a rigorous empirical benchmark for PII extraction attacks in\nrealistic threat scenarios and provides a strong foundation for developing\neffective mitigation strategies.", "published": "2024-10-09 09:16:25", "link": "http://arxiv.org/abs/2410.06704v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MatMamba: A Matryoshka State Space Model", "abstract": "State Space Models (SSMs) like Mamba2 are a promising alternative to\nTransformers, with faster theoretical training and inference times --\nespecially for long context lengths. Recent work on Matryoshka Representation\nLearning -- and its application to Transformer backbones in works like\nMatFormer -- showed how to introduce nested granularities of smaller submodels\nin one universal elastic model. In this work, we present MatMamba: a state\nspace model which combines Matryoshka-style learning with Mamba2, by modifying\nthe block to contain nested dimensions to enable joint training and adaptive\ninference. MatMamba allows for efficient and adaptive deployment across various\nmodel sizes. We train a single large MatMamba model and are able to get a\nnumber of smaller nested models for free -- while maintaining or improving upon\nthe performance of a baseline smaller model trained from scratch. We train\nlanguage and image models at a variety of parameter sizes from 35M to 1.4B. Our\nresults on ImageNet and FineWeb show that MatMamba models scale comparably to\nTransformers, while having more efficient inference characteristics. This makes\nMatMamba a practically viable option for deploying large-scale models in an\nelastic way based on the available inference compute. Code and models are open\nsourced at \\url{https://github.com/ScaledFoundations/MatMamba}", "published": "2024-10-09 09:41:34", "link": "http://arxiv.org/abs/2410.06718v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with\n  Situation Puzzles", "abstract": "While advancements in NLP have significantly improved the performance of\nLarge Language Models (LLMs) on tasks requiring vertical thinking, their\nlateral thinking capabilities remain under-explored and challenging to measure\ndue to the complexity of assessing creative thought processes and the scarcity\nof relevant data. To address these challenges, we introduce SPLAT, a benchmark\nleveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.\nThis benchmark, containing 975 graded situation puzzles across three difficulty\nlevels, employs a new multi-turn player-judge framework instead of the\ntraditional model-based evaluation, which often necessitates a stronger\nevaluation model. This framework simulates an interactive game where the model\n(player) asks the evaluation model (judge) questions about an incomplete story\nto infer the full scenario. The judge answers based on a detailed reference\nscenario or evaluates if the player's predictions align with the reference one.\nThis approach lessens dependence on more robust evaluation models, enabling the\nassessment of state-of-the-art LLMs. The experiments demonstrate that a robust\nevaluation model, such as WizardLM-2, closely matches human judgements in both\nintermediate question-answering and final scenario accuracy, achieving over 80%\nagreement-similar to the agreement levels among humans. Furthermore, applying\ndata and reasoning processes from our benchmark to other lateral\nthinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to\nperformance enhancements. This suggests that our benchmark effectively\nevaluates and elicits the lateral thinking abilities of LLMs. Code is available\nat: https://github.com/chenqi008/LateralThinking.", "published": "2024-10-09 10:09:11", "link": "http://arxiv.org/abs/2410.06733v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MentalArena: Self-play Training of Language Models for Diagnosis and\n  Treatment of Mental Health Disorders", "abstract": "Mental health disorders are one of the most serious diseases in the world.\nMost people with such a disease lack access to adequate care, which highlights\nthe importance of training models for the diagnosis and treatment of mental\nhealth disorders. However, in the mental health domain, privacy concerns limit\nthe accessibility of personalized treatment data, making it challenging to\nbuild powerful models. In this paper, we introduce MentalArena, a self-play\nframework to train language models by generating domain-specific personalized\ndata, where we obtain a better model capable of making a personalized diagnosis\nand treatment (as a therapist) and providing information (as a patient). To\naccurately model human-like mental health patients, we devise Symptom Encoder,\nwhich simulates a real patient from both cognition and behavior perspectives.\nTo address intent bias during patient-therapist interactions, we propose\nSymptom Decoder to compare diagnosed symptoms with encoded symptoms, and\ndynamically manage the dialogue between patient and therapist according to the\nidentified deviations. We evaluated MentalArena against 6 benchmarks, including\nbiomedicalQA and mental health tasks, compared to 6 advanced models. Our\nmodels, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform\ntheir counterparts, including GPT-4o. We hope that our work can inspire future\nresearch on personalized care. Code is available in\nhttps://github.com/Scarelette/MentalArena/tree/main", "published": "2024-10-09 13:06:40", "link": "http://arxiv.org/abs/2410.06845v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models", "abstract": "We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality.", "published": "2024-10-09 15:18:57", "link": "http://arxiv.org/abs/2410.06981v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CursorCore: Assist Programming through Aligning Anything", "abstract": "Large language models have been successfully applied to programming\nassistance tasks, such as code completion, code insertion, and instructional\ncode editing. However, these applications remain insufficiently automated and\nstruggle to effectively integrate various types of information during the\nprogramming process, including coding history, current code, and user\ninstructions. In this work, we propose a new conversational framework that\ncomprehensively integrates these information sources, collect data to train our\nmodels and evaluate their performance. Firstly, to thoroughly evaluate how well\nmodels align with different types of information and the quality of their\noutputs, we introduce a new benchmark, APEval (Assist Programming Eval), to\ncomprehensively assess the performance of models in programming assistance\ntasks. Then, for data collection, we develop a data generation pipeline,\nProgramming-Instruct, which synthesizes training data from diverse sources,\nsuch as GitHub and online judge platforms. This pipeline can automatically\ngenerate various types of messages throughout the programming process. Finally,\nusing this pipeline, we generate 219K samples, fine-tune multiple models, and\ndevelop the CursorCore series. We show that CursorCore outperforms other models\nof comparable size. This framework unifies applications such as inline chat and\nautomated editing, contributes to the advancement of coding assistants. Code,\nmodels and data are freely available at\nhttps://github.com/TechxGenus/CursorCore.", "published": "2024-10-09 15:45:52", "link": "http://arxiv.org/abs/2410.07002v2", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "ReIFE: Re-evaluating Instruction-Following Evaluation", "abstract": "The automatic evaluation of instruction following typically involves using\nlarge language models (LLMs) to assess response quality. However, there is a\nlack of comprehensive evaluation of these LLM-based evaluators across two\ndimensions: the base LLMs and the evaluation protocols. Therefore, we present a\nthorough meta-evaluation of instruction following, including 25 base LLMs and\n15 recently proposed evaluation protocols, on 4 human-annotated datasets,\nassessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows\nus to identify the best-performing base LLMs and evaluation protocols with a\nhigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)\nBase LLM performance ranking remains largely consistent across evaluation\nprotocols, with less capable LLMs showing greater improvement from protocol\nenhancements; (2) Robust evaluation of evaluation protocols requires many base\nLLMs with varying capability levels, as protocol effectiveness can depend on\nthe base LLM used; (3) Evaluation results on different datasets are not always\nconsistent, so a rigorous evaluation requires multiple datasets with\ndistinctive features. We release our meta-evaluation suite ReIFE, which\nprovides the codebase and evaluation result collection for more than 500\nLLM-evaluator configurations, to support future research in\ninstruction-following evaluation.", "published": "2024-10-09 17:14:50", "link": "http://arxiv.org/abs/2410.07069v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses", "abstract": "Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.", "published": "2024-10-09 17:19:58", "link": "http://arxiv.org/abs/2410.07076v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Approach for Auto Generation of Labeling Functions for Software\n  Engineering Chatbots", "abstract": "Software engineering (SE) chatbots are increasingly gaining attention for\ntheir role in enhancing development processes. At the core of chatbots are the\nNatural Language Understanding platforms (NLUs), which enable them to\ncomprehend and respond to user queries. Before deploying NLUs, there is a need\nto train them with labeled data. However, acquiring such labeled data for SE\nchatbots is challenging due to the scarcity of high-quality datasets. This\nchallenge arises because training SE chatbots requires specialized vocabulary\nand phrases not found in typical language datasets. Consequently, chatbot\ndevelopers often resort to manually annotating user queries to gather the data\nnecessary for training effective chatbots, a process that is both\ntime-consuming and resource-intensive. Previous studies propose approaches to\nsupport chatbot practitioners in annotating users' posed queries. However,\nthese approaches require human intervention to generate rules, called labeling\nfunctions (LFs), that identify and categorize user queries based on specific\npatterns in the data. To address this issue, we propose an approach to\nautomatically generate LFs by extracting patterns from labeled user queries. We\nevaluate the effectiveness of our approach by applying it to the queries of\nfour diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)\nand measure the performance improvement gained from training the NLU on the\nqueries labeled by the generated LFs. We find that the generated LFs\neffectively label data with AUC scores of up to 85.3%, and NLU's performance\nimprovement of up to 27.2% across the studied datasets. Furthermore, our\nresults show that the number of LFs used to generate LFs affects the labeling\nperformance. We believe that our approach can save time and resources in\nlabeling users' queries, allowing practitioners to focus on core chatbot\nfunctionalities.", "published": "2024-10-09 17:34:14", "link": "http://arxiv.org/abs/2410.07094v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy", "abstract": "As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.", "published": "2024-10-09 17:45:47", "link": "http://arxiv.org/abs/2410.07109v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates", "abstract": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks.", "published": "2024-10-09 17:53:06", "link": "http://arxiv.org/abs/2410.07137v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stuffed Mamba: State Collapse and State Capacity of RNN-Based\n  Long-Context Modeling", "abstract": "One essential advantage of recurrent neural networks (RNNs) over\ntransformer-based language models is their linear computational complexity\nconcerning the sequence length, which makes them much faster in handling long\nsequences during inference. However, most publicly available RNNs (e.g., Mamba\nand RWKV) are trained on sequences with less than 10K tokens, and their\neffectiveness in longer contexts remains largely unsatisfying so far. In this\npaper, we study the cause of the inability to process long context for RNNs and\nsuggest critical mitigations. We examine two practical concerns when applying\nstate-of-the-art RNNs to long contexts: (1) the inability to extrapolate to\ninputs longer than the training length and (2) the upper bound of memory\ncapacity. Addressing the first concern, we first investigate *state collapse*\n(SC), a phenomenon that causes severe performance degradation on sequence\nlengths not encountered during training. With controlled experiments, we\nattribute this to overfitting due to the recurrent state being\noverparameterized for the training length. For the second concern, we train a\nseries of Mamba-2 models on long documents to empirically estimate the\nrecurrent state capacity in language modeling and passkey retrieval. Then,\nthree SC mitigation methods are proposed to improve Mamba-2's length\ngeneralizability, allowing the model to process more than 1M tokens without SC.\nWe also find that the recurrent state capacity in passkey retrieval scales\nexponentially to the state size, and we empirically train a Mamba-2 370M with\nnear-perfect passkey retrieval accuracy on 256K context length. This suggests a\npromising future for RNN-based long-context modeling.", "published": "2024-10-09 17:54:28", "link": "http://arxiv.org/abs/2410.07145v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Taking a turn for the better: Conversation redirection throughout the\n  course of mental-health therapy", "abstract": "Mental-health therapy involves a complex conversation flow in which patients\nand therapists continuously negotiate what should be talked about next. For\nexample, therapists might try to shift the conversation's direction to keep the\ntherapeutic process on track and avoid stagnation, or patients might push the\ndiscussion towards issues they want to focus on.\n  How do such patient and therapist redirections relate to the development and\nquality of their relationship? To answer this question, we introduce a\nprobabilistic measure of the extent to which a certain utterance immediately\nredirects the flow of the conversation, accounting for both the intention and\nthe actual realization of such a change. We apply this new measure to\ncharacterize the development of patient-therapist relationships over multiple\nsessions in a very large, widely-used online therapy platform. Our analysis\nreveals that (1) patient control of the conversation's direction generally\nincreases relative to that of the therapist as their relationship progresses;\nand (2) patients who have less control in the first few sessions are\nsignificantly more likely to eventually express dissatisfaction with their\ntherapist and terminate the relationship.", "published": "2024-10-09 17:54:41", "link": "http://arxiv.org/abs/2410.07147v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM\n  Unlearning", "abstract": "This work studies the problem of large language model (LLM) unlearning,\naiming to remove unwanted data influences (e.g., copyrighted or harmful\ncontent) while preserving model utility. Despite the increasing demand for\nunlearning, a technically-grounded optimization framework is lacking. Gradient\nascent (GA)-type methods, though widely used, are suboptimal as they reverse\nthe learning process without controlling optimization divergence (i.e.,\ndeviation from the pre-trained state), leading to risks of over-forgetting and\npotential model collapse. Negative preference optimization (NPO) has been\nproposed to address this issue and is considered one of the state-of-the-art\nLLM unlearning approaches. In this work, we revisit NPO and identify another\ncritical issue: reference model bias. This bias arises from using the reference\nmodel (i.e., the model prior to unlearning) to evaluate the unlearning success,\nwhich can compromise NPO's effectiveness. Specifically, it leads to (a) uneven\nallocation of optimization power across forget data with varying difficulty\nlevels and (b) ineffective gradient weight smoothing during the early stages of\nunlearning optimization. To overcome these challenges, we propose a simple yet\neffective unlearning optimization framework, called SimNPO, showing that\n`simplicity' in removing the reliance on a reference model (through the lens of\nsimple preference optimization) benefits unlearning. We provide deeper insights\ninto SimNPO's advantages through an analysis based on mixtures of Markov\nchains. Extensive experiments further validate SimNPO's efficacy on benchmarks\nlike TOFU and MUSE, as well as its robustness against relearning attacks. Codes\nare available at https://github.com/OPTML-Group/Unlearn-Simple.", "published": "2024-10-09 17:58:12", "link": "http://arxiv.org/abs/2410.07163v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making", "abstract": "We aim to evaluate Large Language Models (LLMs) for embodied decision making.\nWhile a significant body of work has been leveraging LLMs for decision making\nin embodied environments, we still lack a systematic understanding of their\nperformance because they are usually applied in different domains, for\ndifferent purposes, and built based on different inputs and outputs.\nFurthermore, existing evaluations tend to rely solely on a final success rate,\nmaking it difficult to pinpoint what ability is missing in LLMs and where the\nproblem lies, which in turn blocks embodied agents from leveraging LLMs\neffectively and selectively. To address these limitations, we propose a\ngeneralized interface (Embodied Agent Interface) that supports the\nformalization of various types of tasks and input-output specifications of\nLLM-based modules. Specifically, it allows us to unify 1) a broad set of\nembodied decision-making tasks involving both state and temporally extended\ngoals, 2) four commonly-used LLM-based modules for decision making: goal\ninterpretation, subgoal decomposition, action sequencing, and transition\nmodeling, and 3) a collection of fine-grained metrics which break down\nevaluation into various types of errors, such as hallucination errors,\naffordance errors, various types of planning errors, etc. Overall, our\nbenchmark offers a comprehensive assessment of LLMs' performance for different\nsubtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI\nsystems, and providing insights for effective and selective use of LLMs in\nembodied decision making.", "published": "2024-10-09 17:59:00", "link": "http://arxiv.org/abs/2410.07166v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio", "abstract": "Syllables are compositional units of spoken language that efficiently\nstructure human speech perception and production. However, current neural\nspeech representations lack such structure, resulting in dense token sequences\nthat are costly to process. To bridge this gap, we propose a new model, Sylber,\nthat produces speech representations with clean and robust syllabic structure.\nSpecifically, we propose a self-supervised learning (SSL) framework that\nbootstraps syllabic embeddings by distilling from its own initial unsupervised\nsyllabic segmentation. This results in a highly structured representation of\nspeech features, offering three key benefits: 1) a fast, linear-time syllable\nsegmentation algorithm, 2) efficient syllabic tokenization with an average of\n4.27 tokens per second, and 3) novel phonological units suited for efficient\nspoken language modeling. Our proposed segmentation method is highly robust and\ngeneralizes to out-of-domain data and unseen languages without any tuning. By\ntraining token-to-speech generative models, fully intelligible speech can be\nreconstructed from Sylber tokens with a significantly lower bitrate than\nbaseline SSL tokens. This suggests that our model effectively compresses speech\ninto a compact sequence of tokens with minimal information loss. Lastly, we\ndemonstrate that categorical perception-a linguistic phenomenon in speech\nperception-emerges naturally in Sylber, making the embedding space more\ncategorical and sparse than previous speech features and thus supporting the\nhigh efficiency of our tokenization. Together, we present a novel SSL approach\nfor representing speech as syllables, with significant potential for efficient\nspeech tokenization and spoken language modeling.", "published": "2024-10-09 17:59:04", "link": "http://arxiv.org/abs/2410.07168v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "One Initialization to Rule them All: Fine-tuning via Explained Variance\n  Adaptation", "abstract": "Foundation models (FMs) are pre-trained on large-scale datasets and then\nfine-tuned on a downstream task for a specific application. The most successful\nand most commonly used fine-tuning method is to update the pre-trained weights\nvia a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are\nusually initialized at random with a uniform rank distribution across the model\nweights. Recent works focus on different initialization schemes or the learning\nof adaptive ranks during fine-tuning. Both approaches have only been\ninvestigated in isolation, resulting in slow convergence or a uniform rank\ndistribution, in turn leading to suboptimal performance. We propose to improve\nLoRA by initializing the new weights in a data-driven manner by computing\nsingular value decomposition (SVD) on minibatches of activation vectors. Then,\nwe initialize the LoRA matrices with the obtained right-singular vectors and\nredistribute ranks among all weight matrices to provably store the maximum\namount of information of the downstream data in the newly introduced weights.\nIn this way, only what information to maintain or neglect during the\nfine-tuning process needs to be learned. We call our new method\n$\\textbf{E}$xplained $\\textbf{V}$ariance $\\textbf{A}$daptation (EVA). We apply\nEVA to a variety of fine-tuning tasks ranging from language generation and\nunderstanding to image classification and reinforcement learning. EVA exhibits\nfaster convergence than competitors and achieves the highest average score\nacross a multitude of tasks per domain while reducing the number of trainable\nparameters through rank redistribution.", "published": "2024-10-09 17:59:06", "link": "http://arxiv.org/abs/2410.07170v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Better Language Models Exhibit Higher Visual Alignment", "abstract": "How well do text-only Large Language Models (LLMs) naturally align with the\nvisual world? We provide the first direct analysis by utilizing frozen text\nrepresentations in a discriminative vision-language model framework and\nmeasuring zero-shot generalization on unseen classes. We find decoder-based\nLLMs exhibit high intrinsic visual alignment. In particular, more capable LLMs\nreliably demonstrate stronger generalization. Moreover, utilizing frozen LLMs\nleads to strong gains in cross-lingual settings, where our approach surpasses\nCLIP's accuracy of 1.4% with 38.7% for Chinese. Our proposed method improves\nboth robustness and generalization and also significantly reduces the need for\npaired data and compute, making vision-language models more accessible and\nadaptable.", "published": "2024-10-09 17:59:33", "link": "http://arxiv.org/abs/2410.07173v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge\n  Conflicts for Large Language Models", "abstract": "Retrieval-Augmented Generation (RAG), while effective in integrating external\nknowledge to address the limitations of large language models (LLMs), can be\nundermined by imperfect retrieval, which may introduce irrelevant, misleading,\nor even malicious information. Despite its importance, previous studies have\nrarely explored the behavior of RAG through joint analysis on how errors from\nimperfect retrieval attribute and propagate, and how potential conflicts arise\nbetween the LLMs' internal knowledge and external sources. We find that\nimperfect retrieval augmentation might be inevitable and quite harmful, through\ncontrolled analysis under realistic conditions. We identify the knowledge\nconflicts between LLM-internal and external knowledge from retrieval as a\nbottleneck to overcome in the post-retrieval stage of RAG. To render LLMs\nresilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach\nthat adaptively elicits essential information from LLMs' internal knowledge,\niteratively consolidates internal and external knowledge with source-awareness,\nand finalizes the answer according to information reliability. Our experiments\nusing Gemini and Claude demonstrate that Astute RAG significantly outperforms\nprevious robustness-enhanced RAG methods. Notably, Astute RAG is the only\napproach that matches or exceeds the performance of LLMs without RAG under\nworst-case scenarios. Further analysis reveals that Astute RAG effectively\nresolves knowledge conflicts, improving the reliability and trustworthiness of\nRAG systems.", "published": "2024-10-09 17:59:58", "link": "http://arxiv.org/abs/2410.07176v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Swin-BERT: A Feature Fusion System designed for Speech-based Alzheimer's\n  Dementia Detection", "abstract": "Speech is usually used for constructing an automatic Alzheimer's dementia\n(AD) detection system, as the acoustic and linguistic abilities show a decline\nin people living with AD at the early stages. However, speech includes not only\nAD-related local and global information but also other information unrelated to\ncognitive status, such as age and gender. In this paper, we propose a\nspeech-based system named Swin-BERT for automatic dementia detection. For the\nacoustic part, the shifted windows multi-head attention that proposed to\nextract local and global information from images, is used for designing our\nacoustic-based system. To decouple the effect of age and gender on acoustic\nfeature extraction, they are used as an extra input of the designed acoustic\nsystem. For the linguistic part, the rhythm-related information, which varies\nsignificantly between people living with and without AD, is removed while\ntranscribing the audio recordings into transcripts. To compensate for the\nremoved rhythm-related information, the character-level transcripts are\nproposed to be used as the extra input of a word-level BERT-style system.\nFinally, the Swin-BERT combines the acoustic features learned from our proposed\nacoustic-based system with our linguistic-based system. The experiments are\nbased on the two datasets provided by the international dementia detection\nchallenges: the ADReSS and ADReSSo. The results show that both the proposed\nacoustic and linguistic systems can be better or comparable with previous\nresearch on the two datasets. Superior results are achieved by the proposed\nSwin-BERT system on the ADReSS and ADReSSo datasets, which are 85.58\\% F-score\nand 87.32\\% F-score respectively.", "published": "2024-10-09 06:58:20", "link": "http://arxiv.org/abs/2410.07277v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Positive-Augmented Contrastive Learning for Vision-and-Language\n  Evaluation and Training", "abstract": "Despite significant advancements in caption generation, existing evaluation\nmetrics often fail to capture the full quality or fine-grained details of\ncaptions. This is mainly due to their reliance on non-specific human-written\nreferences or noisy pre-training data. Still, finding an effective metric is\ncrucial not only for captions evaluation but also for the generation phase.\nMetrics can indeed play a key role in the fine-tuning stage of captioning\nmodels, ultimately enhancing the quality of the generated captions. In this\npaper, we propose PAC-S++, a learnable metric that leverages the CLIP model,\npre-trained on both web-collected and cleaned data and regularized through\nadditional pairs of generated visual and textual positive samples. Exploiting\nthis stronger and curated pre-training, we also apply PAC-S++ as a reward in\nthe Self-Critical Sequence Training (SCST) stage typically employed to\nfine-tune captioning models. Extensive experiments on different image and video\ndatasets highlight the effectiveness of PAC-S++ compared to popular metrics for\nthe task, including its sensitivity to object hallucinations. Furthermore, we\nshow that integrating PAC-S++ into the fine-tuning stage of a captioning model\nresults in semantically richer captions with fewer repetitions and grammatical\nerrors. Evaluations on out-of-domain benchmarks further demonstrate the\nefficacy of our fine-tuning approach in enhancing model capabilities. Source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/pacscore.", "published": "2024-10-09 18:00:09", "link": "http://arxiv.org/abs/2410.07336v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Learn from Real: Reality Defender's Submission to ASVspoof5 Challenge", "abstract": "Audio deepfake detection is crucial to combat the malicious use of\nAI-synthesized speech. Among many efforts undertaken by the community, the\nASVspoof challenge has become one of the benchmarks to evaluate the\ngeneralizability and robustness of detection models. In this paper, we present\nReality Defender's submission to the ASVspoof5 challenge, highlighting a novel\npretraining strategy which significantly improves generalizability while\nmaintaining low computational cost during training. Our system SLIM learns the\nstyle-linguistics dependency embeddings from various types of bonafide speech\nusing self-supervised contrastive learning. The learned embeddings help to\ndiscriminate spoof from bonafide speech by focusing on the relationship between\nthe style and linguistics aspects. We evaluated our system on ASVspoof5,\nASV2019, and In-the-wild. Our submission achieved minDCF of 0.1499 and EER of\n5.5% on ASVspoof5 Track 1, and EER of 7.4% and 10.8% on ASV2019 and In-the-wild\nrespectively.", "published": "2024-10-09 18:55:28", "link": "http://arxiv.org/abs/2410.07379v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Advocating Character Error Rate for Multilingual ASR Evaluation", "abstract": "Automatic speech recognition (ASR) systems have traditionally been evaluated\nusing English datasets, with the word error rate (WER) serving as the\npredominant metric. WER's simplicity and ease of interpretation have\ncontributed to its widespread adoption, particularly for English. However, as\nASR systems expand to multilingual contexts, WER fails in various ways,\nparticularly with morphologically complex languages or those without clear word\nboundaries. Our work documents the limitations of WER as an evaluation metric\nand advocates for the character error rate (CER) as the primary metric in\nmultilingual ASR evaluation. We show that CER avoids many of the challenges WER\nfaces and exhibits greater consistency across writing systems. We support our\nproposition by conducting human evaluations of ASR transcriptions in three\nlanguages: Malayalam, English, and Arabic, which exhibit distinct morphological\ncharacteristics. We show that CER correlates more closely with human judgments\nthan WER, even for English. To facilitate further research, we release our\nhuman evaluation dataset for future benchmarking of ASR metrics. Our findings\nsuggest that CER should be prioritized, or at least supplemented, in\nmultilingual ASR evaluations to account for the varying linguistic\ncharacteristics of different languages.", "published": "2024-10-09 19:57:07", "link": "http://arxiv.org/abs/2410.07400v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The First VoicePrivacy Attacker Challenge Evaluation Plan", "abstract": "The First VoicePrivacy Attacker Challenge is a new kind of challenge\norganized as part of the VoicePrivacy initiative and supported by ICASSP 2025\nas the SP Grand Challenge It focuses on developing attacker systems against\nvoice anonymization, which will be evaluated against a set of anonymization\nsystems submitted to the VoicePrivacy 2024 Challenge. Training, development,\nand evaluation datasets are provided along with a baseline attacker system.\nParticipants shall develop their attacker systems in the form of automatic\nspeaker verification systems and submit their scores on the development and\nevaluation data to the organizers. To do so, they can use any additional\ntraining data and models, provided that they are openly available and declared\nbefore the specified deadline. The metric for evaluation is equal error rate\n(EER). Results will be presented at the ICASSP 2025 special session to which 5\nselected top-ranked participants will be invited to submit and present their\nchallenge systems.", "published": "2024-10-09 20:48:03", "link": "http://arxiv.org/abs/2410.07428v2", "categories": ["eess.AS", "cs.CL", "cs.CR"], "primary_category": "eess.AS"}
{"title": "SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection", "abstract": "Fine-tuning on task-specific data to boost downstream performance is a\ncrucial step for leveraging Large Language Models (LLMs). However, previous\nstudies have demonstrated that fine-tuning the models on several adversarial\nsamples or even benign data can greatly comprise the model's pre-equipped\nalignment and safety capabilities. In this work, we propose SEAL, a novel\nframework to enhance safety in LLM fine-tuning. SEAL learns a data ranker based\non the bilevel optimization to up rank the safe and high-quality fine-tuning\ndata and down rank the unsafe or low-quality ones. Models trained with SEAL\ndemonstrate superior quality over multiple baselines, with 8.5% and 9.7% win\nrate increase compared to random selection respectively on Llama-3-8b-Instruct\nand Merlinite-7b models. Our code is available on github\nhttps://github.com/hanshen95/SEAL.", "published": "2024-10-09 22:24:22", "link": "http://arxiv.org/abs/2410.07471v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Composite Learning Units: Generalized Learning Beyond Parameter Updates\n  to Transform LLMs into Adaptive Reasoners", "abstract": "Human learning thrives on the ability to learn from mistakes, adapt through\nfeedback, and refine understanding-processes often missing in static machine\nlearning models. In this work, we introduce Composite Learning Units (CLUs)\ndesigned to transform reasoners, such as Large Language Models (LLMs), into\nlearners capable of generalized, continuous learning without conventional\nparameter updates while enhancing their reasoning abilities through continual\ninteraction and feedback. CLUs are built on an architecture that allows a\nreasoning model to maintain and evolve a dynamic knowledge repository: a\nGeneral Knowledge Space for broad, reusable insights and a Prompt-Specific\nKnowledge Space for task-specific learning. Through goal-driven interactions,\nCLUs iteratively refine these knowledge spaces, enabling the system to adapt\ndynamically to complex tasks, extract nuanced insights, and build upon past\nexperiences autonomously. We demonstrate CLUs' effectiveness through a\ncryptographic reasoning task, where they continuously evolve their\nunderstanding through feedback to uncover hidden transformation rules. While\nconventional models struggle to grasp underlying logic, CLUs excel by engaging\nin an iterative, goal-oriented process. Specialized components-handling\nknowledge retrieval, prompt generation, and feedback analysis-work together\nwithin a reinforcing feedback loop. This approach allows CLUs to retain the\nmemory of past failures and successes, adapt autonomously, and apply\nsophisticated reasoning effectively, continually learning from mistakes while\nalso building on breakthroughs.", "published": "2024-10-09 02:27:58", "link": "http://arxiv.org/abs/2410.08037v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Instructional Segment Embedding: Improving LLM Safety with Instruction\n  Hierarchy", "abstract": "Large Language Models (LLMs) are susceptible to security and safety threats,\nsuch as prompt injection, prompt extraction, and harmful requests. One major\ncause of these vulnerabilities is the lack of an instruction hierarchy. Modern\nLLM architectures treat all inputs equally, failing to distinguish between and\nprioritize various types of instructions, such as system messages, user\nprompts, and data. As a result, lower-priority user prompts may override more\ncritical system instructions, including safety protocols. Existing approaches\nto achieving instruction hierarchy, such as delimiters and instruction-based\ntraining, do not address this issue at the architectural level. We introduce\nthe Instructional Segment Embedding (ISE) technique, inspired by BERT, to\nmodern large language models, which embeds instruction priority information\ndirectly into the model. This approach enables models to explicitly\ndifferentiate and prioritize various instruction types, significantly improving\nsafety against malicious prompts that attempt to override priority rules. Our\nexperiments on the Structured Query and Instruction Hierarchy benchmarks\ndemonstrate an average robust accuracy increase of up to 15.75% and 18.68%,\nrespectively. Furthermore, we observe an improvement in instruction-following\ncapability of up to 4.1% evaluated on AlpacaEval. Overall, our approach offers\na promising direction for enhancing the safety and effectiveness of LLM\narchitectures.", "published": "2024-10-09 12:52:41", "link": "http://arxiv.org/abs/2410.09102v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "AuditWen:An Open-Source Large Language Model for Audit", "abstract": "Intelligent auditing represents a crucial advancement in modern audit\npractices, enhancing both the quality and efficiency of audits within the realm\nof artificial intelligence. With the rise of large language model (LLM), there\nis enormous potential for intelligent models to contribute to audit domain.\nHowever, general LLMs applied in audit domain face the challenges of lacking\nspecialized knowledge and the presence of data biases. To overcome these\nchallenges, this study introduces AuditWen, an open-source audit LLM by\nfine-tuning Qwen with constructing instruction data from audit domain. We first\noutline the application scenarios for LLMs in the audit and extract\nrequirements that shape the development of LLMs tailored for audit purposes. We\nthen propose an audit LLM, called AuditWen, by fine-tuning Qwen with\nconstructing 28k instruction dataset from 15 audit tasks and 3 layers. In\nevaluation stage, we proposed a benchmark with 3k instructions that covers a\nset of critical audit tasks derived from the application scenarios. With the\nbenchmark, we compare AuditWen with other existing LLMs from information\nextraction, question answering and document generation. The experimental\nresults demonstrate superior performance of AuditWen both in question\nunderstanding and answer generation, making it an immediately valuable tool for\naudit.", "published": "2024-10-09 02:28:55", "link": "http://arxiv.org/abs/2410.10873v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "FreqMark: Frequency-Based Watermark for Sentence-Level Detection of\n  LLM-Generated Text", "abstract": "The increasing use of Large Language Models (LLMs) for generating highly\ncoherent and contextually relevant text introduces new risks, including misuse\nfor unethical purposes such as disinformation or academic dishonesty. To\naddress these challenges, we propose FreqMark, a novel watermarking technique\nthat embeds detectable frequency-based watermarks in LLM-generated text during\nthe token sampling process. The method leverages periodic signals to guide\ntoken selection, creating a watermark that can be detected with Short-Time\nFourier Transform (STFT) analysis. This approach enables accurate\nidentification of LLM-generated content, even in mixed-text scenarios with both\nhuman-authored and LLM-generated segments. Our experiments demonstrate the\nrobustness and precision of FreqMark, showing strong detection capabilities\nagainst various attack scenarios such as paraphrasing and token substitution.\nResults show that FreqMark achieves an AUC improvement of up to 0.98,\nsignificantly outperforming existing detection methods.", "published": "2024-10-09 05:01:48", "link": "http://arxiv.org/abs/2410.10876v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Herald: A Natural Language Annotated Lean 4 Dataset", "abstract": "Verifiable formal languages like Lean have profoundly impacted mathematical\nreasoning, particularly through the use of large language models (LLMs) for\nautomated reasoning. A significant challenge in training LLMs for these formal\nlanguages is the lack of parallel datasets that align natural language with\nformal language proofs. To address this challenge, this paper introduces a\nnovel framework for translating the Mathlib4 corpus (a unified library of\nmathematics in formal language Lean 4) into natural language. Building upon\nthis, we employ a dual augmentation strategy that combines tactic-based and\ninformal-based approaches, leveraging the Lean-jixia system, a Lean 4 analyzer.\nWe present the results of this pipeline on Mathlib4 as Herald (Hierarchy and\nRetrieval-based Translated Lean Dataset). We also propose the Herald\nTranslator, which is fine-tuned on Herald. Herald translator achieves a 93.2%\naccuracy (Pass@128) on formalizing statements in the miniF2F-test and a 22.5%\naccuracy on our internal graduate-level textbook dataset, outperforming\nInternLM2-Math-Plus-7B (74.0% and 7.5%) and TheoremLlama (50.1% and 4.0%).\nFurthermore, we propose a section-level translation framework for real-world\napplications. As a direct application of Herald translator, we have\nsuccessfully translated a template section in the Stack project, marking a\nnotable progress in the automatic formalization of graduate-level mathematical\nliterature. Our model, along with the datasets, are open-sourced to the public.", "published": "2024-10-09 10:11:24", "link": "http://arxiv.org/abs/2410.10878v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Enhancing Vision-Language Model Pre-training with Image-text Pair\n  Pruning Based on Word Frequency", "abstract": "We propose Word-Frequency-based Image-Text Pair Pruning (WFPP), a novel data\npruning method that improves the efficiency of VLMs. Unlike MetaCLIP, our\nmethod does not need metadata for pruning, but selects text-image pairs to\nprune based on the content of the text. Specifically, WFPP prunes text-image\npairs containing high-frequency words across the entire training dataset. The\neffect of WFPP is to reduce the dominance of frequent words. The result a\nbetter balanced word-frequency distribution in the dataset, which is known to\nimprove the training of word embedding models. After pre-training on the pruned\nsubset, we fine-tuned the model on the entire dataset for one additional epoch\nto achieve better performance. Our experiments demonstrate that applying WFPP\nwhen training a CLIP model improves performance on a wide range of downstream\ntasks. WFPP also provides the advantage of speeding up pre-training by using\nfewer samples. Additionally, we analyze the training data before and after\npruning to visualize how WFPP changes the balance of word frequencies. We hope\nour work encourages researchers to consider the distribution of words in the\ntraining data when pre-training VLMs, not limited to CLIP.", "published": "2024-10-09 11:54:41", "link": "http://arxiv.org/abs/2410.10879v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models", "abstract": "In the era of large language models (LLMs), detecting pretraining data has\nbeen increasingly important due to concerns about fair evaluation and ethical\nrisks. Current methods differentiate members and non-members by designing\nscoring functions, like Perplexity and Min-k%. However, the diversity and\ncomplexity of training data magnifies the difficulty of distinguishing, leading\nto suboptimal performance in detecting pretraining data. In this paper, we\nfirst explore the benefits of unseen data, which can be easily collected after\nthe release of the LLM. We find that the perplexities of LLMs shift differently\nfor members and non-members, after fine-tuning with a small amount of\npreviously unseen data. In light of this, we introduce a novel and effective\nmethod termed Fine-tuned Score Deviation(FSD), which improves the performance\nof current scoring functions for pretraining data detection. In particular, we\npropose to measure the deviation distance of current scores after fine-tuning\non a small amount of unseen data within the same domain. In effect, using a few\nunseen data can largely decrease the scores of all non-members, leading to a\nlarger deviation distance than members. Extensive experiments demonstrate the\neffectiveness of our method, significantly improving the AUC score on common\nbenchmark datasets across various models.", "published": "2024-10-09 15:36:42", "link": "http://arxiv.org/abs/2410.10880v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Answering Questions in Stages: Prompt Chaining for Contract QA", "abstract": "Finding answers to legal questions about clauses in contracts is an important\nform of analysis in many legal workflows (e.g., understanding market trends,\ndue diligence, risk mitigation) but more important is being able to do this at\nscale. Prior work showed that it is possible to use large language models with\nsimple zero-shot prompts to generate structured answers to questions, which can\nlater be incorporated into legal workflows. Such prompts, while effective on\nsimple and straightforward clauses, fail to perform when the clauses are long\nand contain information not relevant to the question. In this paper, we propose\ntwo-stage prompt chaining to produce structured answers to multiple-choice and\nmultiple-select questions and show that they are more effective than simple\nprompts on more nuanced legal text. We analyze situations where this technique\nworks well and areas where further refinement is needed, especially when the\nunderlying linguistic variations are more than can be captured by simply\nspecifying possible answers. Finally, we discuss future research that seeks to\nrefine this work by improving stage one results by making them more\nquestion-specific.", "published": "2024-10-09 17:14:13", "link": "http://arxiv.org/abs/2410.12840v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UniAutoML: A Human-Centered Framework for Unified Discriminative and\n  Generative AutoML with Large Language Models", "abstract": "Automated Machine Learning (AutoML) has simplified complex ML processes such\nas data pre-processing, model selection, and hyper-parameter searching.\nHowever, traditional AutoML frameworks focus solely on discriminative tasks,\noften falling short in tackling AutoML for generative models. Additionally,\nthese frameworks lack interpretability and user engagement during the training\nprocess, primarily due to the absence of human-centered design. It leads to a\nlack of transparency in final decision-making and limited user control,\npotentially reducing trust and adoption of AutoML methods. To address these\nlimitations, we introduce UniAutoML, a human-centered AutoML framework that\nleverages Large Language Models (LLMs) to unify AutoML for both discriminative\n(e.g., Transformers and CNNs for classification or regression tasks) and\ngenerative tasks (e.g., fine-tuning diffusion models or LLMs). The\nhuman-centered design of UniAutoML innovatively features a conversational user\ninterface (CUI) that facilitates natural language interactions, providing users\nwith real-time guidance, feedback, and progress updates for better\ninterpretability. This design enhances transparency and user control throughout\nthe AutoML training process, allowing users to seamlessly break down or modify\nthe model being trained. To mitigate potential risks associated with LLM\ngenerated content, UniAutoML incorporates a safety guardline that filters\ninputs and censors outputs. We evaluated UniAutoML's performance and usability\nthrough experiments on eight diverse datasets and user studies involving 25\nparticipants, demonstrating that UniAutoML not only enhances performance but\nalso improves user control and trust. Our human-centered design bridges the gap\nbetween AutoML capabilities and user understanding, making ML more accessible\nto a broader audience.", "published": "2024-10-09 17:33:15", "link": "http://arxiv.org/abs/2410.12841v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A two-stage transliteration approach to improve performance of a\n  multilingual ASR", "abstract": "End-to-end Automatic Speech Recognition (ASR) systems are rapidly claiming to\nbecome state-of-art over other modeling methods. Several techniques have been\nintroduced to improve their ability to handle multiple languages. However, due\nto variation in writing scripts for different languages, while decoding\nacoustically similar units, they do not always map to an appropriate grapheme\nin the target language. This restricts the scalability and adaptability of the\nmodel while dealing with multiple languages in code-mixing scenarios. This\npaper presents an approach to build a language-agnostic end-to-end model\ntrained on a grapheme set obtained by projecting the multilingual grapheme data\nto the script of a more generic target language. This approach saves the\nacoustic model from retraining to span over a larger space and can easily be\nextended to multiple languages. A two-stage transliteration process realizes\nthis approach and proves to minimize speech-class confusion. We performed\nexperiments with an end-to-end multilingual speech recognition system for two\nIndic Languages, namely Nepali and Telugu. The original grapheme space of these\nlanguages is projected to the Devanagari script. We achieved a relative\nreduction of 20% in the Word Error Rate (WER) and 24% in the Character Error\nRate (CER) in the transliterated space, over other language-dependent modeling\nmethods.", "published": "2024-10-09 05:30:33", "link": "http://arxiv.org/abs/2410.14709v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity", "abstract": "Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.", "published": "2024-10-09 13:06:43", "link": "http://arxiv.org/abs/2410.06846v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "InstructG2I: Synthesizing Images from Multimodal Attributed Graphs", "abstract": "In this paper, we approach an overlooked yet critical task Graph2Image:\ngenerating images from multimodal attributed graphs (MMAGs). This task poses\nsignificant challenges due to the explosion in graph size, dependencies among\ngraph entities, and the need for controllability in graph conditions. To\naddress these challenges, we propose a graph context-conditioned diffusion\nmodel called InstructG2I. InstructG2I first exploits the graph structure and\nmultimodal information to conduct informative neighbor sampling by combining\npersonalized page rank and re-ranking based on vision-language features. Then,\na Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary\nset of graph prompts to guide the denoising process of diffusion. Finally, we\npropose graph classifier-free guidance, enabling controllable generation by\nvarying the strength of graph guidance and multiple connected edges to a node.\nExtensive experiments conducted on three datasets from different domains\ndemonstrate the effectiveness and controllability of our approach. The code is\navailable at https://github.com/PeterGriffinJin/InstructG2I.", "published": "2024-10-09 17:56:15", "link": "http://arxiv.org/abs/2410.07157v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SI"], "primary_category": "cs.AI"}
{"title": "Efficient training strategies for natural sounding speech synthesis and\n  speaker adaptation based on FastPitch", "abstract": "This paper focuses on adapting the functionalities of the FastPitch model to\nthe Romanian language; extending the set of speakers from one to eighteen;\nsynthesising speech using an anonymous identity; and replicating the identities\nof new, unseen speakers. During this work, the effects of various\nconfigurations and training strategies were tested and discussed, along with\ntheir advantages and weaknesses. Finally, we settled on a new configuration,\nbuilt on top of the FastPitch architecture, capable of producing natural speech\nsynthesis, for both known (identities from the training dataset) and unknown\n(identities learnt through short reference samples) speakers. The anonymous\nspeaker can be used for text-to-speech synthesis, if one wants to cancel out\nthe identity information while keeping the semantic content whole and clear. At\nlast, we discussed possible limitations of our work, which will form the basis\nfor future investigations and advancements.", "published": "2024-10-09 11:37:21", "link": "http://arxiv.org/abs/2410.06787v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Mamba-based Segmentation Model for Speaker Diarization", "abstract": "Mamba is a newly proposed architecture which behaves like a recurrent neural\nnetwork (RNN) with attention-like capabilities. These properties are promising\nfor speaker diarization, as attention-based models have unsuitable memory\nrequirements for long-form audio, and traditional RNN capabilities are too\nlimited. In this paper, we propose to assess the potential of Mamba for\ndiarization by comparing the state-of-the-art neural segmentation of the\npyannote pipeline with our proposed Mamba-based variant. Mamba's stronger\nprocessing capabilities allow usage of longer local windows, which\nsignificantly improve diarization quality by making the speaker embedding\nextraction more reliable. We find Mamba to be a superior alternative to both\ntraditional RNN and the tested attention-based model. Our proposed Mamba-based\nsystem achieves state-of-the-art performance on three widely used diarization\ndatasets.", "published": "2024-10-09 01:30:12", "link": "http://arxiv.org/abs/2410.06459v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SRC-gAudio: Sampling-Rate-Controlled Audio Generation", "abstract": "We introduce SRC-gAudio, a novel audio generation model designed to\nfacilitate text-to-audio generation across a wide range of sampling rates\nwithin a single model architecture. SRC-gAudio incorporates the sampling rate\nas part of the generation condition to guide the diffusion-based audio\ngeneration process. Our model enables the generation of audio at multiple\nsampling rates with a single unified model. Furthermore, we explore the\npotential benefits of large-scale, low-sampling-rate data in enhancing the\ngeneration quality of high-sampling-rate audio. Through extensive experiments,\nwe demonstrate that SRC-gAudio effectively generates audio under controlled\nsampling rates. Additionally, our results indicate that pre-training on\nlow-sampling-rate data can lead to significant improvements in audio quality\nacross various metrics.", "published": "2024-10-09 04:48:32", "link": "http://arxiv.org/abs/2410.06544v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LS-EEND: Long-Form Streaming End-to-End Neural Diarization with Online\n  Attractor Extraction", "abstract": "This work proposes a frame-wise online/streaming end-to-end neural\ndiarization (EEND) method, which detects speaker activities in a\nframe-in-frame-out fashion. The proposed model mainly consists of a causal\nembedding encoder and an online attractor decoder. Speakers are modeled in the\nself-attention-based decoder along both the time and speaker dimensions, and\nframe-wise speaker attractors are automatically generated and updated for new\nspeakers and existing speakers, respectively. Retention mechanism is employed\nand especially adapted for long-form diarization with a linear temporal\ncomplexity. A multi-step progressive training strategy is proposed for\ngradually learning from easy tasks to hard tasks in terms of the number of\nspeakers and audio length. Finally, the proposed model (referred to as\nlong-form streaming EEND, LS-EEND) is able to perform streaming diarization for\na high (up to 8) and flexible number speakers and very long (say one hour)\naudio recordings. Experiments on various simulated and real-world datasets show\nthat: 1) when not using oracle speech activity information, the proposed model\nachieves new state-of-the-art online diarization error rate on all datasets,\nincluding CALLHOME (12.11%), DIHARD II (27.58%), DIHARD III (19.61%), and AMI\n(20.76%); 2) Due to the frame-in-frame-out processing fashion and the linear\ntemporal complexity, the proposed model achieves several times lower\nreal-time-factor than comparison online diarization models.", "published": "2024-10-09 08:26:13", "link": "http://arxiv.org/abs/2410.06670v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SCOREQ: Speech Quality Assessment with Contrastive Regression", "abstract": "In this paper, we present SCOREQ, a novel approach for speech quality\nprediction. SCOREQ is a triplet loss function for contrastive regression that\naddresses the domain generalisation shortcoming exhibited by state of the art\nno-reference speech quality metrics. In the paper we: (i) illustrate the\nproblem of L2 loss training failing at capturing the continuous nature of the\nmean opinion score (MOS) labels; (ii) demonstrate the lack of generalisation\nthrough a benchmarking evaluation across several speech domains; (iii) outline\nour approach and explore the impact of the architectural design decisions\nthrough incremental evaluation; (iv) evaluate the final model against state of\nthe art models for a wide variety of data and domains. The results show that\nthe lack of generalisation observed in state of the art speech quality metrics\nis addressed by SCOREQ. We conclude that using a triplet loss function for\ncontrastive regression improves generalisation for speech quality prediction\nmodels but also has potential utility across a wide range of applications using\nregression-based predictive models.", "published": "2024-10-09 08:30:59", "link": "http://arxiv.org/abs/2410.06675v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow\n  Matching", "abstract": "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech\nsystem based on flow matching with Diffusion Transformer (DiT). Without\nrequiring complex designs such as duration model, text encoder, and phoneme\nalignment, the text input is simply padded with filler tokens to the same\nlength as input speech, and then the denoising is performed for speech\ngeneration, which was originally proved feasible by E2 TTS. However, the\noriginal design of E2 TTS makes it hard to follow due to its slow convergence\nand low robustness. To address these issues, we first model the input with\nConvNeXt to refine the text representation, making it easy to align with the\nspeech. We further propose an inference-time Sway Sampling strategy, which\nsignificantly improves our model's performance and efficiency. This sampling\nstrategy for flow step can be easily applied to existing flow matching based\nmodels without retraining. Our design allows faster training and achieves an\ninference RTF of 0.15, which is greatly improved compared to state-of-the-art\ndiffusion-based TTS models. Trained on a public 100K hours multilingual\ndataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching\n(F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless\ncode-switching capability, and speed control efficiency. Demo samples can be\nfound at https://SWivid.github.io/F5-TTS. We release all code and checkpoints\nto promote community development.", "published": "2024-10-09 13:46:34", "link": "http://arxiv.org/abs/2410.06885v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Gumbel Rao Monte Carlo based Bi-Modal Neural Architecture Search for\n  Audio-Visual Deepfake Detection", "abstract": "Deepfakes pose a critical threat to biometric authentication systems by\ngenerating highly realistic synthetic media. Existing multimodal deepfake\ndetectors often struggle to adapt to diverse data and rely on simple fusion\nmethods. To address these challenges, we propose Gumbel-Rao Monte Carlo\nBi-modal Neural Architecture Search (GRMC-BMNAS), a novel architecture search\nframework that employs Gumbel-Rao Monte Carlo sampling to optimize multimodal\nfusion. It refines the Straight through Gumbel Softmax (STGS) method by\nreducing variance with Rao-Blackwellization, stabilizing network training.\nUsing a two-level search approach, the framework optimizes the network\narchitecture, parameters, and performance. Crucial features are efficiently\nidentified from backbone networks, while within the cell structure, a weighted\nfusion operation integrates information from various sources. By varying\nparameters such as temperature and number of Monte carlo samples yields an\narchitecture that maximizes classification performance and better\ngeneralisation capability. Experimental results on the FakeAVCeleb and SWAN-DF\ndatasets demonstrate an impressive AUC percentage of 95.4\\%, achieved with\nminimal model parameters.", "published": "2024-10-09 04:37:35", "link": "http://arxiv.org/abs/2410.06543v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech\n  Synthesis with Discrete Codec Modeling of EnGen-TTS", "abstract": "This research introduces a comprehensive Bahasa text-to-speech (TTS) dataset\nand a novel TTS model, EnGen-TTS, designed to enhance the quality and\nversatility of synthetic speech in the Bahasa language. The dataset, spanning\n\\textasciitilde55.0 hours and 52K audio recordings, integrates diverse textual\nsources, ensuring linguistic richness. A meticulous recording setup captures\nthe nuances of Bahasa phonetics, employing professional equipment to ensure\nhigh-fidelity audio samples. Statistical analysis reveals the dataset's scale\nand diversity, laying the foundation for model training and evaluation. The\nproposed EnGen-TTS model performs better than established baselines, achieving\na Mean Opinion Score (MOS) of 4.45 $\\pm$ 0.13. Additionally, our investigation\non real-time factor and model size highlights EnGen-TTS as a compelling choice,\nwith efficient performance. This research marks a significant advancement in\nBahasa TTS technology, with implications for diverse language applications.\nLink to Generated Samples: \\url{https://bahasa-harmony-comp.vercel.app/}", "published": "2024-10-09 07:01:05", "link": "http://arxiv.org/abs/2410.06608v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spectral and Rhythm Features for Audio Classification with Deep\n  Convolutional Neural Networks", "abstract": "Convolutional neural networks (CNNs) are widely used in computer vision. They\ncan be used not only for conventional digital image material to recognize\npatterns, but also for feature extraction from digital imagery representing\nspectral and rhythm features extracted from time-domain digital audio signals\nfor the acoustic classification of sounds. Different spectral and rhythm\nfeature representations like mel-scaled spectrograms, mel-frequency cepstral\ncoefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT)\nchromagrams, constant-Q transform (CQT) chromagrams and chroma energy\nnormalized statistics (CENS) chromagrams are investigated in terms of the audio\nclassification performance using a deep convolutional neural network. It can be\nclearly shown that the mel-scaled spectrograms and the mel-frequency cepstral\ncoefficients (MFCCs) perform significantly better than the other spectral and\nrhythm features investigated in this research for audio classification tasks\nusing deep CNNs. The experiments were carried out with the aid of the ESC-50\ndataset with 2,000 labeled environmental audio recordings.", "published": "2024-10-09 14:21:59", "link": "http://arxiv.org/abs/2410.06927v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Toward Robust Real-World Audio Deepfake Detection: Closing the\n  Explainability Gap", "abstract": "The rapid proliferation of AI-manipulated or generated audio deepfakes poses\nserious challenges to media integrity and election security. Current AI-driven\ndetection solutions lack explainability and underperform in real-world\nsettings. In this paper, we introduce novel explainability methods for\nstate-of-the-art transformer-based audio deepfake detectors and open-source a\nnovel benchmark for real-world generalizability. By narrowing the\nexplainability gap between transformer-based audio deepfake detectors and\ntraditional methods, our results not only build trust with human experts, but\nalso pave the way for unlocking the potential of citizen intelligence to\novercome the scalability issue in audio deepfake detection.", "published": "2024-10-09 21:08:28", "link": "http://arxiv.org/abs/2410.07436v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
