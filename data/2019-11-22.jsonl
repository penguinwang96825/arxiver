{"title": "Learning Multi-level Dependencies for Robust Word Recognition", "abstract": "Robust language processing systems are becoming increasingly important given\nthe recent awareness of dangerous situations where brittle machine learning\nmodels can be easily broken with the presence of noises. In this paper, we\nintroduce a robust word recognition framework that captures multi-level\nsequential dependencies in noised sentences. The proposed framework employs a\nsequence-to-sequence model over characters of each word, whose output is given\nto a word-level bi-directional recurrent neural network. We conduct extensive\nexperiments to verify the effectiveness of the framework. The results show that\nthe proposed framework outperforms state-of-the-art methods by a large margin\nand they also suggest that character-level dependencies can play an important\nrole in word recognition.", "published": "2019-11-22 00:04:07", "link": "http://arxiv.org/abs/1911.09789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Learning of Answer Selection and Answer Summary Generation in\n  Community Question Answering", "abstract": "Community question answering (CQA) gains increasing popularity in both\nacademy and industry recently. However, the redundancy and lengthiness issues\nof crowdsourced answers limit the performance of answer selection and lead to\nreading difficulties and misunderstandings for community users. To solve these\nproblems, we tackle the tasks of answer selection and answer summary generation\nin CQA with a novel joint learning model. Specifically, we design a\nquestion-driven pointer-generator network, which exploits the correlation\ninformation between question-answer pairs to aid in attending the essential\ninformation when generating answer summaries. Meanwhile, we leverage the answer\nsummaries to alleviate noise in original lengthy answers when ranking the\nrelevancy degrees of question-answer pairs. In addition, we construct a new\nlarge-scale CQA corpus, WikiHowQA, which contains long answers for answer\nselection as well as reference summaries for answer summarization. The\nexperimental results show that the joint learning method can effectively\naddress the answer redundancy issue in CQA and achieves state-of-the-art\nresults on both answer selection and text summarization tasks. Furthermore, the\nproposed model is shown to be of great transferring ability and applicability\nfor resource-poor CQA tasks, which lack of reference answer summaries.", "published": "2019-11-22 01:14:57", "link": "http://arxiv.org/abs/1911.09801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weakly-Supervised Opinion Summarization by Leveraging External\n  Information", "abstract": "Opinion summarization from online product reviews is a challenging task,\nwhich involves identifying opinions related to various aspects of the product\nbeing reviewed. While previous works require additional human effort to\nidentify relevant aspects, we instead apply domain knowledge from external\nsources to automatically achieve the same goal. This work proposes AspMem, a\ngenerative method that contains an array of memory cells to store\naspect-related knowledge. This explicit memory can help obtain a better opinion\nrepresentation and infer the aspect information more precisely. We evaluate\nthis method on both aspect identification and opinion summarization tasks. Our\nexperiments show that AspMem outperforms the state-of-the-art methods even\nthough, unlike the baselines, it does not rely on human supervision which is\ncarefully handcrafted for the given tasks.", "published": "2019-11-22 04:10:46", "link": "http://arxiv.org/abs/1911.09844v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Discrete CVAE for Response Generation on Short-Text Conversation", "abstract": "Neural conversation models such as encoder-decoder models are easy to\ngenerate bland and generic responses. Some researchers propose to use the\nconditional variational autoencoder(CVAE) which maximizes the lower bound on\nthe conditional log-likelihood on a continuous latent variable. With different\nsampled la-tent variables, the model is expected to generate diverse responses.\nAlthough the CVAE-based models have shown tremendous potential, their\nimprovement of generating high-quality responses is still unsatisfactory. In\nthis paper, we introduce a discrete latent variable with an explicit semantic\nmeaning to improve the CVAE on short-text conversation. A major advantage of\nour model is that we can exploit the semantic distance between the latent\nvariables to maintain good diversity between the sampled latent variables.\nAccordingly, we pro-pose a two-stage sampling approach to enable efficient\ndiverse variable selection from a large latent space assumed in the short-text\nconversation task. Experimental results indicate that our model outperforms\nvarious kinds of generation models under both automatic and human evaluations\nand generates more diverse and in-formative responses.", "published": "2019-11-22 04:14:31", "link": "http://arxiv.org/abs/1911.09845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifying Vietnamese Disease Outbreak Reports with Important Sentences\n  and Rich Features", "abstract": "Text classification is an important field of research from mid 90s up to now.\nIt has many applications, one of them is in Web-based biosurveillance systems\nwhich identify and summarize online disease outbreak reports. In this paper we\nfocus on classifying Vietnamese disease outbreak reports. We investigate\nimportant properties of disease outbreak reports, e.g., sentences containing\nnames of outbreak disease, locations. Evaluation on 10-time 10- fold\ncross-validation using the Support Vector Machine algorithm shows that using\nsentences containing disease outbreak names with its preceding/following\nsentences in combination with location features achieve the best F-score with\n86.67% - an improvement of 0.38% in comparison to using all raw text. Our\nresults suggest that using important sentences and rich feature can improve\nperformance of Vietnamese disease outbreak text classification.", "published": "2019-11-22 06:46:11", "link": "http://arxiv.org/abs/1911.09883v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continual adaptation for efficient machine communication", "abstract": "To communicate with new partners in new contexts, humans rapidly form new\nlinguistic conventions. Recent neural language models are able to comprehend\nand produce the existing conventions present in their training data, but are\nnot able to flexibly and interactively adapt those conventions on the fly as\nhumans do. We introduce an interactive repeated reference task as a benchmark\nfor models of adaptation in communication and propose a regularized continual\nlearning framework that allows an artificial agent initialized with a generic\nlanguage model to more accurately and efficiently communicate with a partner\nover time. We evaluate this framework through simulations on COCO and in\nreal-time reference game experiments with human partners.", "published": "2019-11-22 07:26:40", "link": "http://arxiv.org/abs/1911.09896v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Go From the General to the Particular: Multi-Domain Translation with\n  Domain Transformation Networks", "abstract": "The key challenge of multi-domain translation lies in simultaneously encoding\nboth the general knowledge shared across domains and the particular knowledge\ndistinctive to each domain in a unified model. Previous work shows that the\nstandard neural machine translation (NMT) model, trained on mixed-domain data,\ngenerally captures the general knowledge, but misses the domain-specific\nknowledge. In response to this problem, we augment NMT model with additional\ndomain transformation networks to transform the general representations to\ndomain-specific representations, which are subsequently fed to the NMT decoder.\nTo guarantee the knowledge transformation, we also propose two complementary\nsupervision signals by leveraging the power of knowledge distillation and\nadversarial learning. Experimental results on several language pairs, covering\nboth balanced and unbalanced multi-domain translation, demonstrate the\neffectiveness and universality of the proposed approach. Encouragingly, the\nproposed unified model achieves comparable results with the fine-tuning\napproach that requires multiple models to preserve the particular knowledge.\nFurther analyses reveal that the domain transformation networks successfully\ncapture the domain-specific knowledge as expected.", "published": "2019-11-22 08:15:52", "link": "http://arxiv.org/abs/1911.09912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Resource production of written forms of Sign Languages by a\n  user-centered editor, SWift (SignWriting improved fast transcriber)", "abstract": "The SignWriting improved fast transcriber (SWift), presented in this paper,\nis an advanced editor for computer-aided writing and transcribing of any Sign\nLanguage (SL) using SignWriting (SW). The application is an editor which allows\ncomposing and saving desired signs using the SW elementary components, called\n\"glyphs\". These make up a sort of alphabet, which does not depend on the\nnational Sign Language and which codes the basic components of any sign. The\nuser is guided through a fully-automated procedure, making the composition\nprocess fast and intuitive. SWift pursues the goal of helping to break down the\n\"electronic barriers\" that keep deaf people away from the web, and at the same\ntime to support linguistic research about Sign Languages features. For this\nreason it has been designed with a special attention to deaf user needs, and to\ngeneral usability issues. The editor has been developed in a modular way, so it\ncan be integrated everywhere the use of SW as an alternative to written\n\"verbal\" language may be advisable.", "published": "2019-11-22 08:36:42", "link": "http://arxiv.org/abs/1911.09919v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Culture-Independent Word Analogy Datasets", "abstract": "In text processing, deep neural networks mostly use word embeddings as an\ninput. Embeddings have to ensure that relations between words are reflected\nthrough distances in a high-dimensional numeric space. To compare the quality\nof different text embeddings, typically, we use benchmark datasets. We present\na collection of such datasets for the word analogy task in nine languages:\nCroatian, English, Estonian, Finnish, Latvian, Lithuanian, Russian, Slovenian,\nand Swedish. We redesigned the original monolingual analogy task to be much\nmore culturally independent and also constructed cross-lingual analogy datasets\nfor the involved languages. We present basic statistics of the created datasets\nand their initial evaluation using fastText embeddings.", "published": "2019-11-22 13:39:06", "link": "http://arxiv.org/abs/1911.10038v2", "categories": ["cs.CL", "J.5"], "primary_category": "cs.CL"}
{"title": "CRUR: Coupled-Recurrent Unit for Unification, Conceptualization and\n  Context Capture for Language Representation -- A Generalization of Bi\n  Directional LSTM", "abstract": "In this work we have analyzed a novel concept of sequential binding based\nlearning capable network based on the coupling of recurrent units with Bayesian\nprior definition. The coupling structure encodes to generate efficient tensor\nrepresentations that can be decoded to generate efficient sentences and can\ndescribe certain events. These descriptions are derived from structural\nrepresentations of visual features of images and media. An elaborated study of\nthe different types of coupling recurrent structures are studied and some\ninsights of their performance are provided. Supervised learning performance for\nnatural language processing is judged based on statistical evaluations,\nhowever, the truth is perspective, and in this case the qualitative evaluations\nreveal the real capability of the different architectural strengths and\nvariations. Bayesian prior definition of different embedding helps in better\ncharacterization of the sentences based on the natural language structure\nrelated to parts of speech and other semantic level categorization in a form\nwhich is machine interpret-able and inherits the characteristics of the Tensor\nRepresentation binding and unbinding based on the mutually orthogonality. Our\napproach has surpassed some of the existing basic works related to image\ncaptioning.", "published": "2019-11-22 16:48:10", "link": "http://arxiv.org/abs/1911.10132v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topical Phrase Extraction from Clinical Reports by Incorporating both\n  Local and Global Context", "abstract": "Making sense of words often requires to simultaneously examine the\nsurrounding context of a term as well as the global themes characterizing the\noverall corpus. Several topic models have already exploited word embeddings to\nrecognize local context, however, it has been weakly combined with the global\ncontext during the topic inference. This paper proposes to extract topical\nphrases corroborating the word embedding information with the global context\ndetected by Latent Semantic Analysis, and then combine them by means of the\nP\\'{o}lya urn model. To highlight the effectiveness of this combined approach\nthe model was assessed analyzing clinical reports, a challenging scenario\ncharacterized by technical jargon and a limited word statistics available.\nResults show it outperforms the state-of-the-art approaches in terms of both\ntopic coherence and computational cost.", "published": "2019-11-22 18:29:19", "link": "http://arxiv.org/abs/1911.10180v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Noisy Sentences Useless for Distant Supervised Relation Extraction?", "abstract": "The noisy labeling problem has been one of the major obstacles for distant\nsupervised relation extraction. Existing approaches usually consider that the\nnoisy sentences are useless and will harm the model's performance. Therefore,\nthey mainly alleviate this problem by reducing the influence of noisy\nsentences, such as applying bag-level selective attention or removing noisy\nsentences from sentence-bags. However, the underlying cause of the noisy\nlabeling problem is not the lack of useful information, but the missing\nrelation labels. Intuitively, if we can allocate credible labels for noisy\nsentences, they will be transformed into useful training data and benefit the\nmodel's performance. Thus, in this paper, we propose a novel method for distant\nsupervised relation extraction, which employs unsupervised deep clustering to\ngenerate reliable labels for noisy sentences. Specifically, our model contains\nthree modules: a sentence encoder, a noise detector and a label generator. The\nsentence encoder is used to obtain feature representations. The noise detector\ndetects noisy sentences from sentence-bags, and the label generator produces\nhigh-confidence relation labels for noisy sentences. Extensive experimental\nresults demonstrate that our model outperforms the state-of-the-art baselines\non a popular benchmark dataset, and can indeed alleviate the noisy labeling\nproblem.", "published": "2019-11-22 00:00:31", "link": "http://arxiv.org/abs/1911.09788v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Resource Cross-Lingual Named Entity Recognition", "abstract": "Recently, neural methods have achieved state-of-the-art (SOTA) results in\nNamed Entity Recognition (NER) tasks for many languages without the need for\nmanually crafted features. However, these models still require manually\nannotated training data, which is not available for many languages. In this\npaper, we propose an unsupervised cross-lingual NER model that can transfer NER\nknowledge from one language to another in a completely unsupervised way without\nrelying on any bilingual dictionary or parallel data. Our model achieves this\nthrough word-level adversarial learning and augmented fine-tuning with\nparameter sharing and feature augmentation. Experiments on five different\nlanguages demonstrate the effectiveness of our approach, outperforming existing\nmodels by a good margin and setting a new SOTA for each language pair.", "published": "2019-11-22 02:09:08", "link": "http://arxiv.org/abs/1911.09812v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neuron Interaction Based Representation Composition for Neural Machine\n  Translation", "abstract": "Recent NLP studies reveal that substantial linguistic information can be\nattributed to single neurons, i.e., individual dimensions of the representation\nvectors. We hypothesize that modeling strong interactions among neurons helps\nto better capture complex information by composing the linguistic properties\nembedded in individual neurons. Starting from this intuition, we propose a\nnovel approach to compose representations learned by different components in\nneural machine translation (e.g., multi-layer networks or multi-head\nattention), based on modeling strong interactions among neurons in the\nrepresentation vectors. Specifically, we leverage bilinear pooling to model\npairwise multiplicative interactions among individual neurons, and a low-rank\napproximation to make the model computationally feasible. We further propose\nextended bilinear pooling to incorporate first-order representations.\nExperiments on WMT14 English-German and English-French translation tasks show\nthat our model consistently improves performances over the SOTA Transformer\nbaseline. Further analyses demonstrate that our approach indeed captures more\nsyntactic and semantic information as expected.", "published": "2019-11-22 06:38:42", "link": "http://arxiv.org/abs/1911.09877v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Effective Modeling of Encoder-Decoder Architecture for Joint Entity and\n  Relation Extraction", "abstract": "A relation tuple consists of two entities and the relation between them, and\noften such tuples are found in unstructured text. There may be multiple\nrelation tuples present in a text and they may share one or both entities among\nthem. Extracting such relation tuples from a sentence is a difficult task and\nsharing of entities or overlapping entities among the tuples makes it more\nchallenging. Most prior work adopted a pipeline approach where entities were\nidentified first followed by finding the relations among them, thus missing the\ninteraction among the relation tuples in a sentence. In this paper, we propose\ntwo approaches to use encoder-decoder architecture for jointly extracting\nentities and relations. In the first approach, we propose a representation\nscheme for relation tuples which enables the decoder to generate one word at a\ntime like machine translation models and still finds all the tuples present in\na sentence with full entity names of different length and with overlapping\nentities. Next, we propose a pointer network-based decoding approach where an\nentire tuple is generated at every time step. Experiments on the publicly\navailable New York Times corpus show that our proposed approaches outperform\nprevious work and achieve significantly higher F1 scores.", "published": "2019-11-22 06:52:21", "link": "http://arxiv.org/abs/1911.09886v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Anaphora Resolution in Dialogue Systems for South Asian Languages", "abstract": "Anaphora resolution is a challenging task which has been the interest of NLP\nresearchers for a long time. Traditional resolution techniques like eliminative\nconstraints and weighted preferences were successful in many languages.\nHowever, they are ineffective in free word order languages like most SouthAsian\nlanguages.Heuristic and rule-based techniques were typical in these languages,\nwhich are constrained to context and domain.In this paper, we venture a new\nstrategy us-ing neural networks for resolving anaphora in human-human\ndialogues. The architecture chiefly consists of three components, a shallow\nparser for extracting features, a feature vector generator which produces the\nword embed-dings, and a neural network model which will predict the antecedent\nmention of an anaphora.The system has been trained and tested on Telugu\nconversation corpus we generated. Given the advantage of the semantic\ninformation in word embeddings and appending actor, gender, number, person and\npart of plural features the model has reached an F1-score of 86.", "published": "2019-11-22 12:20:44", "link": "http://arxiv.org/abs/1911.09994v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "High Quality ELMo Embeddings for Seven Less-Resourced Languages", "abstract": "Recent results show that deep neural networks using contextual embeddings\nsignificantly outperform non-contextual embeddings on a majority of text\nclassification task. We offer precomputed embeddings from popular contextual\nELMo model for seven languages: Croatian, Estonian, Finnish, Latvian,\nLithuanian, Slovenian, and Swedish. We demonstrate that the quality of\nembeddings strongly depends on the size of training set and show that existing\npublicly available ELMo embeddings for listed languages shall be improved. We\ntrain new ELMo embeddings on much larger training sets and show their advantage\nover baseline non-contextual FastText embeddings. In evaluation, we use two\nbenchmarks, the analogy task and the NER task.", "published": "2019-11-22 14:06:21", "link": "http://arxiv.org/abs/1911.10049v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Injecting Prior Knowledge into Image Caption Generation", "abstract": "Automatically generating natural language descriptions from an image is a\nchallenging problem in artificial intelligence that requires a good\nunderstanding of the visual and textual signals and the correlations between\nthem. The state-of-the-art methods in image captioning struggles to approach\nhuman level performance, especially when data is limited. In this paper, we\npropose to improve the performance of the state-of-the-art image captioning\nmodels by incorporating two sources of prior knowledge: (i) a conditional\nlatent topic attention, that uses a set of latent variables (topics) as an\nanchor to generate highly probable words and, (ii) a regularization technique\nthat exploits the inductive biases in syntactic and semantic structure of\ncaptions and improves the generalization of image captioning models. Our\nexperiments validate that our method produces more human interpretable captions\nand also leads to significant improvements on the MSCOCO dataset in both the\nfull and low data regimes.", "published": "2019-11-22 15:22:34", "link": "http://arxiv.org/abs/1911.10082v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Data Set of Internet Claims and Comparison of their Sentiments with\n  Credibility", "abstract": "In this modern era, communication has become faster and easier. This means\nfallacious information can spread as fast as reality. Considering the damage\nthat fake news kindles on the psychology of people and the fact that such news\nproliferates faster than truth, we need to study the phenomenon that helps\nspread fake news. An unbiased data set that depends on reality for rating news\nis necessary to construct predictive models for its classification. This paper\ndescribes the methodology to create such a data set. We collect our data from\nsnopes.com which is a fact-checking organization. Furthermore, we intend to\ncreate this data set not only for classification of the news but also to find\npatterns that reason the intent behind misinformation. We also formally define\nan Internet Claim, its credibility, and the sentiment behind such a claim. We\ntry to realize the relationship between the sentiment of a claim with its\ncredibility. This relationship pours light on the bigger picture behind the\npropagation of misinformation. We pave the way for further research based on\nthe methodology described in this paper to create the data set and usage of\npredictive modeling along with research-based on psychology/mentality of people\nto understand why fake news spreads much faster than reality.", "published": "2019-11-22 16:35:37", "link": "http://arxiv.org/abs/1911.10130v1", "categories": ["cs.IR", "cs.CL", "H.3.3, I.2.7", "H.3.3; I.2.7"], "primary_category": "cs.IR"}
{"title": "Moral Dilemmas for Artificial Intelligence: a position paper on an\n  application of Compositional Quantum Cognition", "abstract": "Traditionally, the way one evaluates the performance of an Artificial\nIntelligence (AI) system is via a comparison to human performance in specific\ntasks, treating humans as a reference for high-level cognition. However, these\ncomparisons leave out important features of human intelligence: the capability\nto transfer knowledge and make complex decisions based on emotional and\nrational reasoning. These decisions are influenced by current inferences as\nwell as prior experiences, making the decision process strongly subjective and\napparently biased. In this context, a definition of compositional intelligence\nis necessary to incorporate these features in future AI tests. Here, a concrete\nimplementation of this will be suggested, using recent developments in quantum\ncognition, natural language and compositional meaning of sentences, thanks to\ncategorical compositional models of meaning.", "published": "2019-11-22 17:25:32", "link": "http://arxiv.org/abs/1911.10154v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Interactive Text Ranking with Bayesian Optimisation: A Case Study on\n  Community QA and Summarisation", "abstract": "For many NLP applications, such as question answering and summarisation, the\ngoal is to select the best solution from a large space of candidates to meet a\nparticular user's needs. To address the lack of user-specific training data, we\npropose an interactive text ranking approach that actively selects pairs of\ncandidates, from which the user selects the best. Unlike previous strategies,\nwhich attempt to learn a ranking across the whole candidate space, our method\nemploys Bayesian optimisation to focus the user's labelling effort on high\nquality candidates and integrates prior knowledge in a Bayesian manner to cope\nbetter with small data scenarios. We apply our method to community question\nanswering (cQA) and extractive summarisation, finding that it significantly\noutperforms existing interactive approaches. We also show that the ranking\nfunction learned by our method is an effective reward function for\nreinforcement learning, which improves the state of the art for interactive\nsummarisation.", "published": "2019-11-22 18:31:53", "link": "http://arxiv.org/abs/1911.10183v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving N-gram Language Models with Pre-trained Deep Transformer", "abstract": "Although n-gram language models (LMs) have been outperformed by the\nstate-of-the-art neural LMs, they are still widely used in speech recognition\ndue to its high efficiency in inference. In this paper, we demonstrate that\nn-gram LM can be improved by neural LMs through a text generation based data\naugmentation method. In contrast to previous approaches, we employ a\nlarge-scale general domain pre-training followed by in-domain fine-tuning\nstrategy to construct deep Transformer based neural LMs. Large amount of\nin-domain text data is generated with the well trained deep Transformer to\nconstruct new n-gram LMs, which are then interpolated with baseline n-gram\nsystems. Empirical studies on different speech recognition tasks show that the\nproposed approach can effectively improve recognition accuracy. In particular,\nour proposed approach brings significant relative word error rate reduction up\nto 6.0% for domains with limited in-domain data.", "published": "2019-11-22 20:11:40", "link": "http://arxiv.org/abs/1911.10235v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Order Matters at Fanatics Recommending Sequentially Ordered Products by\n  LSTM Embedded with Word2Vec", "abstract": "A unique challenge for e-commerce recommendation is that customers are often\ninterested in products that are more advanced than their already purchased\nproducts, but not reversed. The few existing recommender systems modeling\nunidirectional sequence output a limited number of categories or continuous\nvariables. To model the ordered sequence, we design the first recommendation\nsystem that both embed purchased items with Word2Vec, and model the sequence\nwith stateless LSTM RNN. The click-through rate of this recommender system in\nproduction outperforms its solely Word2Vec based predecessor. Developed in\n2017, it was perhaps the first published real-world application that makes\ndistributed predictions of a single machine trained Keras model on Spark slave\nnodes at a scale of more than 0.4 million columns per row.", "published": "2019-11-22 02:39:41", "link": "http://arxiv.org/abs/1911.09818v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Factorized Multimodal Transformer for Multimodal Sequential Learning", "abstract": "The complex world around us is inherently multimodal and sequential\n(continuous). Information is scattered across different modalities and requires\nmultiple continuous sensors to be captured. As machine learning leaps towards\nbetter generalization to real world, multimodal sequential learning becomes a\nfundamental research area. Arguably, modeling arbitrarily distributed\nspatio-temporal dynamics within and across modalities is the biggest challenge\nin this research area. In this paper, we present a new transformer model,\ncalled the Factorized Multimodal Transformer (FMT) for multimodal sequential\nlearning. FMT inherently models the intramodal and intermodal (involving two or\nmore modalities) dynamics within its multimodal input in a factorized manner.\nThe proposed factorization allows for increasing the number of self-attentions\nto better model the multimodal phenomena at hand; without encountering\ndifficulties during training (e.g. overfitting) even on relatively low-resource\nsetups. All the attention mechanisms within FMT have a full time-domain\nreceptive field which allows them to asynchronously capture long-range\nmultimodal dynamics. In our experiments we focus on datasets that contain the\nthree commonly studied modalities of language, vision and acoustic. We perform\na wide range of experiments, spanning across 3 well-studied datasets and 21\ndistinct labels. FMT shows superior performance over previously proposed\nmodels, setting new state of the art in the studied datasets.", "published": "2019-11-22 03:14:32", "link": "http://arxiv.org/abs/1911.09826v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Data Programming using Continuous and Quality-Guided Labeling Functions", "abstract": "Scarcity of labeled data is a bottleneck for supervised learning models. A\nparadigm that has evolved for dealing with this problem is data programming. An\nexisting data programming paradigm allows human supervision to be provided as a\nset of discrete labeling functions (LF) that output possibly noisy labels to\ninput instances and a generative modelfor consolidating the weak labels. We\nenhance and generalize this paradigm by supporting functions that output a\ncontinuous score (instead of a hard label) that noisily correlates with labels.\nWe show across five applications that continuous LFs are more natural to\nprogram and lead to improved recall. We also show that accuracy of existing\ngenerative models is unstable with respect to initialization, training epochs,\nand learning rates. We give control to the data programmer to guide the\ntraining process by providing intuitive quality guides with each LF. We propose\nan elegant method of incorporating these guides into the generative model. Our\noverall method, called CAGE, makes the data programming paradigm more reliable\nthan other tricks based on initialization, sign-penalties, or soft-accuracy\nconstraints.", "published": "2019-11-22 05:05:42", "link": "http://arxiv.org/abs/1911.09860v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SWift -- A SignWriting editor to bridge between deaf world and\n  e-learning", "abstract": "SWift (SignWriting improved fast transcriber) is an advanced editor for\nSignWriting (SW). At present, SW is a promising alternative to provide\ndocuments in an easy-to-grasp written form of (any) Sign Language, the gestural\nway of communication which is widely adopted by the deaf community. SWift was\ndeveloped SW users, either deaf or not, to support collaboration and exchange\nof ideas. The application allows composing and saving desired signs using\nelementary components, called glyphs. The procedure that was devised guides and\nsimplifies the editing process. SWift aims at breaking the \"electronic\"\nbarriers that keep the deaf community away from ICT in general, and from\ne-learning in particular. The editor can be contained in a pluggable module;\ntherefore, it can be integrated everywhere the use of SW is an advisable\nalternative to written \"verbal\" language, which often hinders information\ngrasping by deaf users.", "published": "2019-11-22 08:44:23", "link": "http://arxiv.org/abs/1911.09923v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "The JDDC Corpus: A Large-Scale Multi-Turn Chinese Dialogue Dataset for\n  E-commerce Customer Service", "abstract": "Human conversations are complicated and building a human-like dialogue agent\nis an extremely challenging task. With the rapid development of deep learning\ntechniques, data-driven models become more and more prevalent which need a huge\namount of real conversation data. In this paper, we construct a large-scale\nreal scenario Chinese E-commerce conversation corpus, JDDC, with more than 1\nmillion multi-turn dialogues, 20 million utterances, and 150 million words. The\ndataset reflects several characteristics of human-human conversations, e.g.,\ngoal-driven, and long-term dependency among the context. It also covers various\ndialogue types including task-oriented, chitchat and question-answering. Extra\nintent information and three well-annotated challenge sets are also provided.\nThen, we evaluate several retrieval-based and generative models to provide\nbasic benchmark performance on the JDDC corpus. And we hope JDDC can serve as\nan effective testbed and benefit the development of fundamental research in\ndialogue task", "published": "2019-11-22 10:55:50", "link": "http://arxiv.org/abs/1911.09969v4", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Optimizing Data Usage via Differentiable Rewards", "abstract": "To acquire a new skill, humans learn better and faster if a tutor, based on\ntheir current knowledge level, informs them of how much attention they should\npay to particular content or practice problems. Similarly, a machine learning\nmodel could potentially be trained better with a scorer that \"adapts\" to its\ncurrent learning state and estimates the importance of each training data\ninstance. Training such an adaptive scorer efficiently is a challenging\nproblem; in order to precisely quantify the effect of a data instance at a\ngiven time during the training, it is typically necessary to first complete the\nentire training process. To efficiently optimize data usage, we propose a\nreinforcement learning approach called Differentiable Data Selection (DDS). In\nDDS, we formulate a scorer network as a learnable function of the training\ndata, which can be efficiently updated along with the main model being trained.\nSpecifically, DDS updates the scorer with an intuitive reward signal: it should\nup-weigh the data that has a similar gradient with a dev set upon which we\nwould finally like to perform well. Without significant computing overhead, DDS\ndelivers strong and consistent improvements over several strong baselines on\ntwo very different tasks of machine translation and image classification.", "published": "2019-11-22 15:38:01", "link": "http://arxiv.org/abs/1911.10088v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "HAL: Improved Text-Image Matching by Mitigating Visual Semantic Hubs", "abstract": "The hubness problem widely exists in high-dimensional embedding space and is\na fundamental source of error for cross-modal matching tasks. In this work, we\nstudy the emergence of hubs in Visual Semantic Embeddings (VSE) with\napplication to text-image matching. We analyze the pros and cons of two widely\nadopted optimization objectives for training VSE and propose a novel\nhubness-aware loss function (HAL) that addresses previous methods' defects.\nUnlike (Faghri et al.2018) which simply takes the hardest sample within a\nmini-batch, HAL takes all samples into account, using both local and global\nstatistics to scale up the weights of \"hubs\". We experiment our method with\nvarious configurations of model architectures and datasets. The method exhibits\nexceptionally good robustness and brings consistent improvement on the task of\ntext-image matching across all settings. Specifically, under the same model\narchitectures as (Faghri et al. 2018) and (Lee at al. 2018), by switching only\nthe learning objective, we report a maximum R@1improvement of 7.4% on MS-COCO\nand 8.3% on Flickr30k.", "published": "2019-11-22 15:51:08", "link": "http://arxiv.org/abs/1911.10097v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "TPsgtR: Neural-Symbolic Tensor Product Scene-Graph-Triplet\n  Representation for Image Captioning", "abstract": "Image captioning can be improved if the structure of the graphical\nrepresentations can be formulated with conceptual positional binding. In this\nwork, we have introduced a novel technique for caption generation using the\nneural-symbolic encoding of the scene-graphs, derived from regional visual\ninformation of the images and we call it Tensor Product Scene-Graph-Triplet\nRepresentation (TP$_{sgt}$R). While, most of the previous works concentrated on\nidentification of the object features in images, we introduce a neuro-symbolic\nembedding that can embed identified relationships among different regions of\nthe image into concrete forms, instead of relying on the model to compose for\nany/all combinations. These neural symbolic representation helps in better\ndefinition of the neural symbolic space for neuro-symbolic attention and can be\ntransformed to better captions. With this approach, we introduced two novel\narchitectures (TP$_{sgt}$R-TDBU and TP$_{sgt}$R-sTDBU) for comparison and\nexperiment result demonstrates that our approaches outperformed the other\nmodels, and generated captions are more comprehensive and natural.", "published": "2019-11-22 16:17:21", "link": "http://arxiv.org/abs/1911.10115v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Use of Artificial Intelligence to Analyse Risk in Legal Documents for a\n  Better Decision Support", "abstract": "Assessing risk for voluminous legal documents such as request for proposal;\ncontracts is tedious and error prone. We have developed \"risk-o-meter\", a\nframework, based on machine learning and natural language processing to review\nand assess risks of any legal document. Our framework uses Paragraph Vector, an\nunsupervised model to generate vector representation of text. This enables the\nframework to learn contextual relations of legal terms and generate sensible\ncontext aware embedding. The framework then feeds the vector space into a\nsupervised classification algorithm to predict whether a paragraph belongs to a\nper-defined risk category or not. The framework thus extracts risk prone\nparagraphs. This technique efficiently overcomes the limitations of\nkeyword-based search. We have achieved an accuracy of 91% for the risk category\nhaving the largest training dataset. This framework will help organizations\noptimize effort to identify risk from large document base with minimal human\nintervention and thus will help to have risk mitigated sustainable growth. Its\nmachine learning capability makes it scalable to uncover relevant information\nfrom any type of document apart from legal documents, provided the library is\nper-populated and rich.", "published": "2019-11-22 16:07:02", "link": "http://arxiv.org/abs/1912.01111v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Signal-Adaptive and Perceptually Optimized Sound Zones with Variable\n  Span Trade-Off Filters", "abstract": "Creating sound zones has been an active research field since the idea was\nfirst proposed. So far, most sound zone control methods rely on either an\noptimization of physical metrics such as acoustic contrast and signal\ndistortion or a mode decomposition of the desired sound field. By using these\ntypes of methods, approximately 15 dB of acoustic contrast between the\nreproduced sound field in the target zone and its leakage to other zone(s) has\nbeen reported in practical set-ups, but this is typically not high enough to\nsatisfy the people inside the zones. In this paper, we propose a sound zone\ncontrol method shaping the leakage errors so that they are as inaudible as\npossible for a given acoustic contrast. The shaping of the leakage errors is\nperformed by taking the time-varying input signal characteristics and the human\nauditory system into account when the loudspeaker control filters are\ncalculated. We show how this shaping can be performed using variable span\ntrade-off filters, and we show theoretically how these filters can be used for\ntrading signal distortion in the target zone for acoustic contrast. The\nproposed method is evaluated based on physical metrics such as acoustic\ncontrast and perceptual metrics such as STOI. The computational complexity and\nprocessing time of the proposed method for different system set-ups are also\ninvestigated. Lastly, the results of a MUSHRA listening test are reported. The\ntest results show that the proposed method provides more than 20% perceptual\nimprovement compared to existing sound zone control methods.", "published": "2019-11-22 13:04:26", "link": "http://arxiv.org/abs/1911.10016v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Time-Domain Multi-modal Bone/air Conducted Speech Enhancement", "abstract": "Previous studies have proven that integrating video signals, as a\ncomplementary modality, can facilitate improved performance for speech\nenhancement (SE). However, video clips usually contain large amounts of data\nand pose a high cost in terms of computational resources and thus may\ncomplicate the SE system. As an alternative source, a bone-conducted speech\nsignal has a moderate data size while manifesting speech-phoneme structures,\nand thus complements its air-conducted counterpart. In this study, we propose a\nnovel multi-modal SE structure in the time domain that leverages bone- and\nair-conducted signals. In addition, we examine two ensemble-learning-based\nstrategies, early fusion (EF) and late fusion (LF), to integrate the two types\nof speech signals, and adopt a deep learning-based fully convolutional network\nto conduct the enhancement. The experiment results on the Mandarin corpus\nindicate that this newly presented multi-modal (integrating bone- and\nair-conducted signals) SE structure significantly outperforms the single-source\nSE counterparts (with a bone- or air-conducted signal only) in various speech\nevaluation metrics. In addition, the adoption of an LF strategy other than an\nEF in this novel SE multi-modal structure achieves better results.", "published": "2019-11-22 04:17:17", "link": "http://arxiv.org/abs/1911.09847v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "GANkyoku: a Generative Adversarial Network for Shakuhachi Music", "abstract": "A common approach to generating symbolic music using neural networks involves\nrepeated sampling of an autoregressive model until the full output sequence is\nobtained. While such approaches have shown some promise in generating short\nsequences of music, this typically has not extended to cases where the final\ntarget sequence is significantly longer, for example an entire piece of music.\nIn this work we propose a network trained in an adversarial process to generate\nentire pieces of solo shakuhachi music, in the form of symbolic notation. The\npieces are intended to refer clearly to traditional shakuhachi music,\nmaintaining idiomaticity and key aesthetic qualities, while also adding novel\nfeatures, ultimately creating worthy additions to the contemporary shakuhachi\nrepertoire. A key subproblem is also addressed, namely the lack of relevant\ntraining data readily available, in two steps: firstly, we introduce the\nPH_Shaku dataset for symbolic traditional shakuhachi music; secondly, we build\non previous work using conditioning in generative adversarial networks to\nintroduce a technique for data augmentation.", "published": "2019-11-22 16:19:40", "link": "http://arxiv.org/abs/1911.10119v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T99"], "primary_category": "cs.SD"}
{"title": "Universal adversarial examples in speech command classification", "abstract": "Adversarial examples are inputs intentionally perturbed with the aim of\nforcing a machine learning model to produce a wrong prediction, while the\nchanges are not easily detectable by a human. Although this topic has been\nintensively studied in the image domain, classification tasks in the audio\ndomain have received less attention. In this paper we address the existence of\nuniversal perturbations for speech command classification. We provide evidence\nthat universal attacks can be generated for speech command classification\ntasks, which are able to generalize across different models to a significant\nextent. Additionally, a novel analytical framework is proposed for the\nevaluation of universal perturbations under different levels of universality,\ndemonstrating that the feasibility of generating effective perturbations\ndecreases as the universality level increases. Finally, we propose a more\ndetailed and rigorous framework to measure the amount of distortion introduced\nby the perturbations, demonstrating that the methods employed by convention are\nnot realistic in audio-based problems.", "published": "2019-11-22 18:31:52", "link": "http://arxiv.org/abs/1911.10182v4", "categories": ["cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Real-time Ultrasound-enhanced Multimodal Imaging of Tongue using 3D\n  Printable Stabilizer System: A Deep Learning Approach", "abstract": "Despite renewed awareness of the importance of articulation, it remains a\nchallenge for instructors to handle the pronunciation needs of language\nlearners. There are relatively scarce pedagogical tools for pronunciation\nteaching and learning. Unlike inefficient, traditional pronunciation\ninstructions like listening and repeating, electronic visual feedback (EVF)\nsystems such as ultrasound technology have been employed in new approaches.\nRecently, an ultrasound-enhanced multimodal method has been developed for\nvisualizing tongue movements of a language learner overlaid on the face-side of\nthe speaker's head. That system was evaluated for several language courses via\na blended learning paradigm at the university level. The result was asserted\nthat visualizing the articulator's system as biofeedback to language learners\nwill significantly improve articulation learning efficiency. In spite of the\nsuccessful usage of multimodal techniques for pronunciation training, it still\nrequires manual works and human manipulation. In this article, we aim to\ncontribute to this growing body of research by addressing difficulties of the\nprevious approaches by proposing a new comprehensive, automatic, real-time\nmultimodal pronunciation training system, benefits from powerful artificial\nintelligence techniques. The main objective of this research was to combine the\nadvantages of ultrasound technology, three-dimensional printing, and deep\nlearning algorithms to enhance the performance of previous systems. Our\npreliminary pedagogical evaluation of the proposed system revealed a\nsignificant improvement in flexibility, control, robustness, and autonomy.", "published": "2019-11-22 03:54:31", "link": "http://arxiv.org/abs/1911.09840v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
