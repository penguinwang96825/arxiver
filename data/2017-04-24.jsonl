{"title": "Fast and Accurate Neural Word Segmentation for Chinese", "abstract": "Neural models with minimal feature engineering have achieved competitive\nperformance against traditional methods for the task of Chinese word\nsegmentation. However, both training and working procedures of the current\nneural models are computationally inefficient. This paper presents a greedy\nneural word segmenter with balanced word and character embedding inputs to\nalleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of\nperforming segmentation much faster and even more accurate than\nstate-of-the-art neural models on Chinese benchmark datasets.", "published": "2017-04-24 05:50:29", "link": "http://arxiv.org/abs/1704.07047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selective Encoding for Abstractive Sentence Summarization", "abstract": "We propose a selective encoding model to extend the sequence-to-sequence\nframework for abstractive sentence summarization. It consists of a sentence\nencoder, a selective gate network, and an attention equipped decoder. The\nsentence encoder and decoder are built with recurrent neural networks. The\nselective gate network constructs a second level sentence representation by\ncontrolling the information flow from encoder to decoder. The second level\nrepresentation is tailored for sentence summarization task, which leads to\nbetter performance. We evaluate our model on the English Gigaword, DUC 2004 and\nMSR abstractive sentence summarization datasets. The experimental results show\nthat the proposed selective encoding model outperforms the state-of-the-art\nbaseline models.", "published": "2017-04-24 07:57:37", "link": "http://arxiv.org/abs/1704.07073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Incremental Neural Semantic Graph Parsing", "abstract": "Parsing sentences to linguistically-expressive semantic representations is a\nkey goal of Natural Language Processing. Yet statistical parsing has focused\nalmost exclusively on bilexical dependencies or domain-specific logical forms.\nWe propose a neural encoder-decoder transition-based parser which is the first\nfull-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The\nmodel architecture uses stack-based embedding features, predicting graphs\njointly with unlexicalized predicates and their token alignments. Our parser is\nmore accurate than attention-based baselines on MRS, and on an additional\nAbstract Meaning Representation (AMR) benchmark, and GPU batch processing makes\nit an order of magnitude faster than a high-precision grammar-based parser.\nFurther, the 86.69% Smatch score of our MRS parser is higher than the\nupper-bound on AMR parsing, making MRS an attractive choice as a semantic\nrepresentation.", "published": "2017-04-24 08:50:15", "link": "http://arxiv.org/abs/1704.07092v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge\n  Graph Embeddings", "abstract": "We study a symmetric collaborative dialogue setting in which two agents, each\nwith private knowledge, must strategically communicate to achieve a common\ngoal. The open-ended dialogue state in this setting poses new challenges for\nexisting dialogue systems. We collected a dataset of 11K human-human dialogues,\nwhich exhibits interesting lexical, semantic, and strategic elements. To model\nboth structured knowledge and unstructured language, we propose a neural model\nwith dynamic knowledge graph embeddings that evolve as the dialogue progresses.\nAutomatic and human evaluations show that our model is both more effective at\nachieving the goal and more human-like than baseline neural and rule-based\nmodels.", "published": "2017-04-24 10:38:24", "link": "http://arxiv.org/abs/1704.07130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam\n  Search", "abstract": "We present Grid Beam Search (GBS), an algorithm which extends beam search to\nallow the inclusion of pre-specified lexical constraints. The algorithm can be\nused with any model that generates a sequence $ \\mathbf{\\hat{y}} =\n\\{y_{0}\\ldots y_{T}\\} $, by maximizing $ p(\\mathbf{y} | \\mathbf{x}) =\n\\prod\\limits_{t}p(y_{t} | \\mathbf{x}; \\{y_{0} \\ldots y_{t-1}\\}) $. Lexical\nconstraints take the form of phrases or words that must be present in the\noutput sequence. This is a very general way to incorporate additional knowledge\ninto a model's output without requiring any modification of the model\nparameters or training data. We demonstrate the feasibility and flexibility of\nLexically Constrained Decoding by conducting experiments on Neural\nInteractive-Predictive Translation, as well as Domain Adaptation for Neural\nMachine Translation. Experiments show that GBS can provide large improvements\nin translation quality in interactive scenarios, and that, even without any\nuser input, GBS can be used to achieve significant gains in performance in\ndomain adaptation scenarios.", "published": "2017-04-24 10:55:20", "link": "http://arxiv.org/abs/1704.07138v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Found in Translation: Reconstructing Phylogenetic Language Trees from\n  Translations", "abstract": "Translation has played an important role in trade, law, commerce, politics,\nand literature for thousands of years. Translators have always tried to be\ninvisible; ideal translations should look as if they were written originally in\nthe target language. We show that traces of the source language remain in the\ntranslation product to the extent that it is possible to uncover the history of\nthe source language by looking only at the translation. Specifically, we\nautomatically reconstruct phylogenetic language trees from monolingual texts\n(translated from several source languages). The signal of the source language\nis so powerful that it is retained even after two phases of translation. This\nstrongly indicates that source language interference is the most dominant\ncharacteristic of translated texts, overshadowing the more subtle signals of\nuniversal properties of translation.", "published": "2017-04-24 11:14:20", "link": "http://arxiv.org/abs/1704.07146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Watset: Automatic Induction of Synsets from a Graph of Synonyms", "abstract": "This paper presents a new graph-based approach that induces synsets using\nsynonymy dictionaries and word embeddings. First, we build a weighted graph of\nsynonyms extracted from commonly available resources, such as Wiktionary.\nSecond, we apply word sense induction to deal with ambiguous words. Finally, we\ncluster the disambiguated version of the ambiguous input graph into synsets.\nOur meta-clustering approach lets us use an efficient hard clustering algorithm\nto perform a fuzzy clustering of the graph. Despite its simplicity, our\napproach shows excellent results, outperforming five competitive\nstate-of-the-art methods in terms of F-score on three gold standard datasets\nfor English and Russian derived from large-scale manually constructed lexical\nresources.", "published": "2017-04-24 11:49:08", "link": "http://arxiv.org/abs/1704.07157v1", "categories": ["cs.CL", "68T50", "I.2.6; I.5.3; I.2.4"], "primary_category": "cs.CL"}
{"title": "What is the Essence of a Claim? Cross-Domain Claim Identification", "abstract": "Argument mining has become a popular research area in NLP. It typically\nincludes the identification of argumentative components, e.g. claims, as the\ncentral component of an argument. We perform a qualitative analysis across six\ndifferent datasets and show that these appear to conceptualize claims quite\ndifferently. To learn about the consequences of such different\nconceptualizations of claim for practical applications, we carried out\nextensive experiments using state-of-the-art feature-rich and deep learning\nsystems, to identify claims in a cross-domain fashion. While the divergent\nperception of claims in different datasets is indeed harmful to cross-domain\nclassification, we show that there are shared properties on the lexical level\nas well as system configurations that can help to overcome these gaps.", "published": "2017-04-24 13:13:30", "link": "http://arxiv.org/abs/1704.07203v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Trie-Structured Bayesian Model for Unsupervised Morphological\n  Segmentation", "abstract": "In this paper, we introduce a trie-structured Bayesian model for unsupervised\nmorphological segmentation. We adopt prior information from different sources\nin the model. We use neural word embeddings to discover words that are\nmorphologically derived from each other and thereby that are semantically\nsimilar. We use letter successor variety counts obtained from tries that are\nbuilt by neural word embeddings. Our results show that using different\ninformation sources such as neural word embeddings and letter successor variety\nas prior information improves morphological segmentation in a Bayesian model.\nOur model outperforms other unsupervised morphological segmentation models on\nTurkish and gives promising results on English and German for scarce resources.", "published": "2017-04-24 17:07:26", "link": "http://arxiv.org/abs/1704.07329v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Native Language from Gaze", "abstract": "A fundamental question in language learning concerns the role of a speaker's\nfirst language in second language acquisition. We present a novel methodology\nfor studying this question: analysis of eye-movement patterns in second\nlanguage reading of free-form text. Using this methodology, we demonstrate for\nthe first time that the native language of English learners can be predicted\nfrom their gaze fixations when reading English. We provide analysis of\nclassifier uncertainty and learned features, which indicates that differences\nin English reading are likely to be rooted in linguistic divergences across\nnative languages. The presented framework complements production studies and\noffers new ground for advancing research on multilingualism.", "published": "2017-04-24 18:04:17", "link": "http://arxiv.org/abs/1704.07398v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention", "abstract": "To answer the question in machine comprehension (MC) task, the models need to\nestablish the interaction between the question and the context. To tackle the\nproblem that the single-pass model cannot reflect on and correct its answer, we\npresent Ruminating Reader. Ruminating Reader adds a second pass of attention\nand a novel information fusion component to the Bi-Directional Attention Flow\nmodel (BiDAF). We propose novel layer structures that construct an query-aware\ncontext vector representation and fuse encoding representation with\nintermediate representation on top of BiDAF model. We show that a multi-hop\nattention mechanism can be applied to a bi-directional attention structure. In\nexperiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by\na substantial margin, and matches or surpasses the performance of all other\npublished systems.", "published": "2017-04-24 18:49:38", "link": "http://arxiv.org/abs/1704.07415v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognizing Descriptive Wikipedia Categories for Historical Figures", "abstract": "Wikipedia is a useful knowledge source that benefits many applications in\nlanguage processing and knowledge representation. An important feature of\nWikipedia is that of categories. Wikipedia pages are assigned different\ncategories according to their contents as human-annotated labels which can be\nused in information retrieval, ad hoc search improvements, entity ranking and\ntag recommendations. However, important pages are usually assigned too many\ncategories, which makes it difficult to recognize the most important ones that\ngive the best descriptions.\n  In this paper, we propose an approach to recognize the most descriptive\nWikipedia categories. We observe that historical figures in a precise category\npresumably are mutually similar and such categorical coherence could be\nevaluated via texts or Wikipedia links of corresponding members in the\ncategory. We rank descriptive level of Wikipedia categories according to their\ncoherence and our ranking yield an overall agreement of 88.27% compared with\nhuman wisdom.", "published": "2017-04-24 19:28:52", "link": "http://arxiv.org/abs/1704.07427v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Challenge Set Approach to Evaluating Machine Translation", "abstract": "Neural machine translation represents an exciting leap forward in translation\nquality. But what longstanding weaknesses does it resolve, and which remain? We\naddress these questions with a challenge set approach to translation evaluation\nand error analysis. A challenge set consists of a small set of sentences, each\nhand-designed to probe a system's capacity to bridge a particular structural\ndivergence between languages. To exemplify this approach, we present an\nEnglish-French challenge set, and use it to analyze phrase-based and neural\nsystems. The resulting analysis provides not only a more fine-grained picture\nof the strengths of neural systems, but also insight into which linguistic\nphenomena remain out of reach.", "published": "2017-04-24 19:34:38", "link": "http://arxiv.org/abs/1704.07431v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting English Writing Styles For Non Native Speakers", "abstract": "This paper presents the first attempt, up to our knowledge, to classify\nEnglish writing styles on this scale with the challenge of classifying day to\nday language written by writers with different backgrounds covering various\nareas of topics.The paper proposes simple machine learning algorithms and\nsimple to generate features to solve hard problems. Relying on the scale of the\ndata available from large sources of knowledge like Wikipedia. We believe such\nsources of data are crucial to generate robust solutions for the web with high\naccuracy and easy to deploy in practice. The paper achieves 74\\% accuracy\nclassifying native versus non native speakers writing styles.\n  Moreover, the paper shows some interesting observations on the similarity\nbetween different languages measured by the similarity of their users English\nwriting styles. This technique could be used to show some well known facts\nabout languages as in grouping them into families, which our experiments\nsupport.", "published": "2017-04-24 20:04:25", "link": "http://arxiv.org/abs/1704.07441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Streaming Word Embeddings with the Space-Saving Algorithm", "abstract": "We develop a streaming (one-pass, bounded-memory) word embedding algorithm\nbased on the canonical skip-gram with negative sampling algorithm implemented\nin word2vec. We compare our streaming algorithm to word2vec empirically by\nmeasuring the cosine similarity between word pairs under each algorithm and by\napplying each algorithm in the downstream task of hashtag prediction on a\ntwo-month interval of the Twitter sample stream. We then discuss the results of\nthese experiments, concluding they provide partial validation of our approach\nas a streaming replacement for word2vec. Finally, we discuss potential failure\nmodes and suggest directions for future work.", "published": "2017-04-24 20:55:33", "link": "http://arxiv.org/abs/1704.07463v1", "categories": ["cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "An Analysis of Action Recognition Datasets for Language and Vision Tasks", "abstract": "A large amount of recent research has focused on tasks that combine language\nand vision, resulting in a proliferation of datasets and methods. One such task\nis action recognition, whose applications include image annotation, scene\nunder- standing and image retrieval. In this survey, we categorize the existing\nap- proaches based on how they conceptualize this problem and provide a\ndetailed review of existing datasets, highlighting their di- versity as well as\nadvantages and disad- vantages. We focus on recently devel- oped datasets which\nlink visual informa- tion with linguistic resources and provide a fine-grained\nsyntactic and semantic anal- ysis of actions in images.", "published": "2017-04-24 10:38:23", "link": "http://arxiv.org/abs/1704.07129v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance\n  Classification with Branch-LSTM", "abstract": "This paper describes team Turing's submission to SemEval 2017 RumourEval:\nDetermining rumour veracity and support for rumours (SemEval 2017 Task 8,\nSubtask A). Subtask A addresses the challenge of rumour stance classification,\nwhich involves identifying the attitude of Twitter users towards the\ntruthfulness of the rumour they are discussing. Stance classification is\nconsidered to be an important step towards rumour verification, therefore\nperforming well in this task is expected to be useful in debunking false\nrumours. In this work we classify a set of Twitter posts discussing rumours\ninto either supporting, denying, questioning or commenting on the underlying\nrumours. We propose a LSTM-based sequential model that, through modelling the\nconversational structure of tweets, which achieves an accuracy of 0.784 on the\nRumourEval test set outperforming all other systems in Subtask A.", "published": "2017-04-24 13:41:25", "link": "http://arxiv.org/abs/1704.07221v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Global Constraints and Reranking to Improve Cognates Detection", "abstract": "Global constraints and reranking have not been used in cognates detection\nresearch to date. We propose methods for using global constraints by performing\nrescoring of the score matrices produced by state of the art cognates detection\nsystems. Using global constraints to perform rescoring is complementary to\nstate of the art methods for performing cognates detection and results in\nsignificant performance improvements beyond current state of the art\nperformance on publicly available datasets with different language pairs and\nvarious conditions such as different levels of baseline state of the art\nperformance and different data size conditions, including with more realistic\nlarge data size conditions than have been evaluated with in the past.", "published": "2017-04-24 06:04:50", "link": "http://arxiv.org/abs/1704.07050v2", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.6; I.2.7; I.5.1; I.5.4"], "primary_category": "cs.CL"}
{"title": "Being Negative but Constructively: Lessons Learnt from Creating Better\n  Visual Question Answering Datasets", "abstract": "Visual question answering (Visual QA) has attracted a lot of attention\nlately, seen essentially as a form of (visual) Turing test that artificial\nintelligence should strive to achieve. In this paper, we study a crucial\ncomponent of this task: how can we design good datasets for the task? We focus\non the design of multiple-choice based datasets where the learner has to select\nthe right answer from a set of candidate ones including the target (\\ie the\ncorrect one) and the decoys (\\ie the incorrect ones). Through careful analysis\nof the results attained by state-of-the-art learning models and human\nannotators on existing datasets, we show that the design of the decoy answers\nhas a significant impact on how and what the learning models learn from the\ndatasets. In particular, the resulting learner can ignore the visual\ninformation, the question, or both while still doing well on the task. Inspired\nby this, we propose automatic procedures to remedy such design deficiencies. We\napply the procedures to re-construct decoy answers for two popular Visual QA\ndatasets as well as to create a new Visual QA dataset from the Visual Genome\nproject, resulting in the largest dataset for this task. Extensive empirical\nstudies show that the design deficiencies have been alleviated in the remedied\ndatasets and the performance on them is likely a more faithful indicator of the\ndifference among learning models. The datasets are released and publicly\navailable via http://www.teds.usc.edu/website_vqa/.", "published": "2017-04-24 10:05:19", "link": "http://arxiv.org/abs/1704.07121v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Multitask Learning for Sequence Labeling", "abstract": "We propose a sequence labeling framework with a secondary training objective,\nlearning to predict surrounding words for every word in the dataset. This\nlanguage modeling objective incentivises the system to learn general-purpose\npatterns of semantic and syntactic composition, which are also useful for\nimproving accuracy on different sequence labeling tasks. The architecture was\nevaluated on a range of datasets, covering the tasks of error detection in\nlearner texts, named entity recognition, chunking and POS-tagging. The novel\nlanguage modeling objective provided consistent performance improvements on\nevery benchmark, without requiring any additional annotated or unannotated\ndata.", "published": "2017-04-24 11:47:06", "link": "http://arxiv.org/abs/1704.07156v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.5.1; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Parsing Speech: A Neural Approach to Integrating Lexical and\n  Acoustic-Prosodic Information", "abstract": "In conversational speech, the acoustic signal provides cues that help\nlisteners disambiguate difficult parses. For automatically parsing spoken\nutterances, we introduce a model that integrates transcribed text and\nacoustic-prosodic features using a convolutional neural network over energy and\npitch trajectories coupled with an attention-based recurrent neural network\nthat accepts text and prosodic features. We find that different types of\nacoustic-prosodic features are individually helpful, and together give\nstatistically significant improvements in parse and disfluency detection F1\nscores over a strong text-only baseline. For this study with known sentence\nboundaries, error analyses show that the main benefit of acoustic-prosodic\nfeatures is in sentences with disfluencies, attachment decisions are most\nimproved, and transcription errors obscure gains from prosody.", "published": "2017-04-24 15:33:26", "link": "http://arxiv.org/abs/1704.07287v2", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Multi-Task Video Captioning with Video and Entailment Generation", "abstract": "Video captioning, the task of describing the content of a video, has seen\nsome promising improvements in recent years with sequence-to-sequence models,\nbut accurately learning the temporal and logical dynamics involved in the task\nstill remains a challenge, especially given the lack of sufficient annotated\ndata. We improve video captioning by sharing knowledge with two related\ndirected-generation tasks: a temporally-directed unsupervised video prediction\ntask to learn richer context-aware video encoder representations, and a\nlogically-directed language entailment generation task to learn better\nvideo-entailed caption decoder representations. For this, we present a\nmany-to-many multi-task learning model that shares parameters across the\nencoders and decoders of the three tasks. We achieve significant improvements\nand the new state-of-the-art on several standard video captioning datasets\nusing diverse automatic and human evaluations. We also show mutual multi-task\nimprovements on the entailment generation task.", "published": "2017-04-24 23:07:32", "link": "http://arxiv.org/abs/1704.07489v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "GaKCo: a Fast GApped k-mer string Kernel using COunting", "abstract": "String Kernel (SK) techniques, especially those using gapped $k$-mers as\nfeatures (gk), have obtained great success in classifying sequences like DNA,\nprotein, and text. However, the state-of-the-art gk-SK runs extremely slow when\nwe increase the dictionary size ($\\Sigma$) or allow more mismatches ($M$). This\nis because current gk-SK uses a trie-based algorithm to calculate co-occurrence\nof mismatched substrings resulting in a time cost proportional to\n$O(\\Sigma^{M})$. We propose a \\textbf{fast} algorithm for calculating\n\\underline{Ga}pped $k$-mer \\underline{K}ernel using \\underline{Co}unting\n(GaKCo). GaKCo uses associative arrays to calculate the co-occurrence of\nsubstrings using cumulative counting. This algorithm is fast, scalable to\nlarger $\\Sigma$ and $M$, and naturally parallelizable. We provide a rigorous\nasymptotic analysis that compares GaKCo with the state-of-the-art gk-SK.\nTheoretically, the time cost of GaKCo is independent of the $\\Sigma^{M}$ term\nthat slows down the trie-based approach. Experimentally, we observe that GaKCo\nachieves the same accuracy as the state-of-the-art and outperforms its speed by\nfactors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein\n(12 datasets), and character-based English text (2 datasets), respectively.\n  GaKCo is shared as an open source tool at\n\\url{https://github.com/QData/GaKCo-SVM}", "published": "2017-04-24 21:43:21", "link": "http://arxiv.org/abs/1704.07468v3", "categories": ["cs.LG", "cs.AI", "cs.CC", "cs.CL", "cs.DS"], "primary_category": "cs.LG"}
