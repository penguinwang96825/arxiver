{"title": "Investigating the Effect of Segmentation Methods on Neural Model based\n  Sentiment Analysis on Informal Short Texts in Turkish", "abstract": "This work investigates segmentation approaches for sentiment analysis on\ninformal short texts in Turkish. The two building blocks of the proposed work\nare segmentation and deep neural network model. Segmentation focuses on\npreprocessing of text with different methods. These methods are grouped in\nfour: morphological, sub-word, tokenization, and hybrid approaches. We analyzed\nseveral variants for each of these four methods. The second stage focuses on\nevaluation of the neural model for sentiment analysis. The performance of each\nsegmentation method is evaluated under Convolutional Neural Network (CNN) and\nRecurrent Neural Network (RNN) model proposed in the literature for sentiment\nclassification.", "published": "2019-02-18 16:26:01", "link": "http://arxiv.org/abs/1902.06635v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCEF: A Support-Confidence-aware Embedding Framework for Knowledge Graph\n  Refinement", "abstract": "Knowledge graph (KG) refinement mainly aims at KG completion and correction\n(i.e., error detection). However, most conventional KG embedding models only\nfocus on KG completion with an unreasonable assumption that all facts in KG\nhold without noises, ignoring error detection which also should be significant\nand essential for KG refinement.In this paper, we propose a novel\nsupport-confidence-aware KG embedding framework (SCEF), which implements KG\ncompletion and correction simultaneously by learning knowledge representations\nwith both triple support and triple confidence. Specifically, we build model\nenergy function by incorporating conventional translation-based model with\nsupport and confidence. To make our triple support-confidence more sufficient\nand robust, we not only consider the internal structural information in KG,\nstudying the approximate relation entailment as triple confidence constraints,\nbut also the external textual evidence, proposing two kinds of triple supports\nwith entity types and descriptions respectively.Through extensive experiments\non real-world datasets, we demonstrate SCEF's effectiveness.", "published": "2019-02-18 02:00:23", "link": "http://arxiv.org/abs/1902.06377v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix\n  Space Model", "abstract": "Continuous Bag of Words (CBOW) is a powerful text embedding method. Due to\nits strong capabilities to encode word content, CBOW embeddings perform well on\na wide range of downstream tasks while being efficient to compute. However,\nCBOW is not capable of capturing the word order. The reason is that the\ncomputation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ\nand ZYX are the same. In order to address this shortcoming, we propose a\nlearning algorithm for the Continuous Matrix Space Model, which we call\nContinual Multiplication of Words (CMOW). Our algorithm is an adaptation of\nword2vec, so that it can be trained on large quantities of unlabeled text. We\nempirically show that CMOW better captures linguistic properties, but it is\ninferior to CBOW in memorizing word content. Motivated by these findings, we\npropose a hybrid model that combines the strengths of CBOW and CMOW. Our\nresults show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to\nmemorize word content while at the same time substantially improving its\nability to encode other linguistic information by 8%. As a result, the hybrid\nalso performs better on 8 out of 11 supervised downstream tasks with an average\nimprovement of 1.2%.", "published": "2019-02-18 06:54:14", "link": "http://arxiv.org/abs/1902.06423v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"The Michael Jordan of Greatness\": Extracting Vossian Antonomasia from\n  Two Decades of the New York Times, 1987-2007", "abstract": "Vossian Antonomasia is a prolific stylistic device, in use since antiquity.\nIt can compress the introduction or description of a person or another named\nentity into a terse, poignant formulation and can best be explained by an\nexample: When Norwegian world champion Magnus Carlsen is described as \"the\nMozart of chess\", it is Vossian Antonomasia we are dealing with. The pattern is\nsimple: A source (Mozart) is used to describe a target (Magnus Carlsen), the\ntransfer of meaning is reached via a modifier (\"of chess\"). This phenomenon has\nbeen discussed before (as 'metaphorical antonomasia' or, with special focus on\nthe source object, as 'paragons'), but no corpus-based approach has been\nundertaken as yet to explore its breadth and variety. We are looking into a\nfull-text newspaper corpus (The New York Times, 1987-2007) and describe a new\nmethod for the automatic extraction of Vossian Antonomasia based on Wikidata\nentities. Our analysis offers new insights into the occurrence of popular\nparagons and their distribution.", "published": "2019-02-18 07:21:21", "link": "http://arxiv.org/abs/1902.06428v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Self-Attention Aligner: A Latency-Control End-to-End Model for ASR Using\n  Self-Attention Network and Chunk-Hopping", "abstract": "Self-attention network, an attention-based feedforward neural network, has\nrecently shown the potential to replace recurrent neural networks (RNNs) in a\nvariety of NLP tasks. However, it is not clear if the self-attention network\ncould be a good alternative of RNNs in automatic speech recognition (ASR),\nwhich processes the longer speech sequences and may have online recognition\nrequirements. In this paper, we present a RNN-free end-to-end model:\nself-attention aligner (SAA), which applies the self-attention networks to a\nsimplified recurrent neural aligner (RNA) framework. We also propose a\nchunk-hopping mechanism, which enables the SAA model to encode on segmented\nframe chunks one after another to support online recognition. Experiments on\ntwo Mandarin ASR datasets show the replacement of RNNs by the self-attention\nnetworks yields a 8.4%-10.2% relative character error rate (CER) reduction. In\naddition, the chunk-hopping mechanism allows the SAA to have only a 2.5%\nrelative CER degradation with a 320ms latency. After jointly training with a\nself-attention network language model, our SAA model obtains further error rate\nreduction on multiple datasets. Especially, it achieves 24.12% CER on the\nMandarin ASR benchmark (HKUST), exceeding the best end-to-end model by over 2%\nabsolute CER.", "published": "2019-02-18 08:17:24", "link": "http://arxiv.org/abs/1902.06450v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learned In Speech Recognition: Contextual Acoustic Word Embeddings", "abstract": "End-to-end acoustic-to-word speech recognition models have recently gained\npopularity because they are easy to train, scale well to large amounts of\ntraining data, and do not require a lexicon. In addition, word models may also\nbe easier to integrate with downstream tasks such as spoken language\nunderstanding, because inference (search) is much simplified compared to\nphoneme, character or any other sort of sub-word units. In this paper, we\ndescribe methods to construct contextual acoustic word embeddings directly from\na supervised sequence-to-sequence acoustic-to-word speech recognition model\nusing the learned attention distribution. On a suite of 16 standard sentence\nevaluation tasks, our embeddings show competitive performance against a\nword2vec model trained on the speech transcriptions. In addition, we evaluate\nthese embeddings on a spoken language understanding task, and observe that our\nembeddings match the performance of text-based embeddings in a pipeline of\nfirst performing speech recognition and then constructing word embeddings from\ntranscriptions.", "published": "2019-02-18 23:06:56", "link": "http://arxiv.org/abs/1902.06833v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Classifying textual data: shallow, deep and ensemble methods", "abstract": "This paper focuses on a comparative evaluation of the most common and modern\nmethods for text classification, including the recent deep learning strategies\nand ensemble methods. The study is motivated by a challenging real data\nproblem, characterized by high-dimensional and extremely sparse data, deriving\nfrom incoming calls to the customer care of an Italian phone company. We will\nshow that deep learning outperforms many classical (shallow) strategies but the\ncombination of shallow and deep learning methods in a unique ensemble\nclassifier may improve the robustness and the accuracy of \"single\"\nclassification methods.", "published": "2019-02-18 15:47:35", "link": "http://arxiv.org/abs/1902.07068v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Discovery of Natural Language Concepts in Individual Units of CNNs", "abstract": "Although deep convolutional networks have achieved improved performance in\nmany natural language tasks, they have been treated as black boxes because they\nare difficult to interpret. Especially, little is known about how they\nrepresent language in their intermediate layers. In an attempt to understand\nthe representations of deep convolutional networks trained on language tasks,\nwe show that individual units are selectively responsive to specific morphemes,\nwords, and phrases, rather than responding to arbitrary and uninterpretable\npatterns. In order to quantitatively analyze such an intriguing phenomenon, we\npropose a concept alignment method based on how units respond to the replicated\ntext. We conduct analyses with different architectures on multiple datasets for\nclassification and translation tasks and provide new insights into how deep\nmodels understand natural language.", "published": "2019-02-18 06:19:14", "link": "http://arxiv.org/abs/1902.07249v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Securing Voice-driven Interfaces against Fake (Cloned) Audio Attacks", "abstract": "Voice cloning technologies have found applications in a variety of areas\nranging from personalized speech interfaces to advertisement, robotics, and so\non. Existing voice cloning systems are capable of learning speaker\ncharacteristics and use trained models to synthesize a person's voice from only\na few audio samples. Advances in cloned speech generation technologies are\ncapable of generating perceptually indistinguishable speech from a bona-fide\nspeech. These advances pose new security and privacy threats to voice-driven\ninterfaces and speech-based access control systems. The state-of-the-art speech\nsynthesis technologies use trained or tuned generative models for cloned speech\ngeneration. Trained generative models rely on linear operations, learned\nweights, and excitation source for cloned speech synthesis. These systems leave\ncharacteristic artifacts in the synthesized speech. Higher-order spectral\nanalysis is used to capture differentiating attributes between bona-fide and\ncloned audios. Specifically, quadrature phase coupling (QPC) in the estimated\nbicoherence, Gaussianity test statistics, and linearity test statistics are\nused to capture generative model artifacts. Performance of the proposed method\nis evaluated on cloned audios generated using speaker adaptation- and speaker\nencoding-based approaches. Experimental results for a dataset consisting of 126\ncloned speech and 8 bona-fide speech samples indicate that the proposed method\nis capable of detecting bona-fide and cloned audios with close to a perfect\ndetection rate.", "published": "2019-02-18 20:10:35", "link": "http://arxiv.org/abs/1902.06782v1", "categories": ["eess.AS", "cs.SD", "92C55", "I.2.1; I.5.4"], "primary_category": "eess.AS"}
{"title": "End-to-end Lyrics Alignment for Polyphonic Music Using an\n  Audio-to-Character Recognition Model", "abstract": "Time-aligned lyrics can enrich the music listening experience by enabling\nkaraoke, text-based song retrieval and intra-song navigation, and other\napplications. Compared to text-to-speech alignment, lyrics alignment remains\nhighly challenging, despite many attempts to combine numerous sub-modules\nincluding vocal separation and detection in an effort to break down the\nproblem. Furthermore, training required fine-grained annotations to be\navailable in some form. Here, we present a novel system based on a modified\nWave-U-Net architecture, which predicts character probabilities directly from\nraw audio using learnt multi-scale representations of the various signal\ncomponents. There are no sub-modules whose interdependencies need to be\noptimized. Our training procedure is designed to work with weak, line-level\nannotations available in the real world. With a mean alignment error of 0.35s\non a standard dataset our system outperforms the state-of-the-art by an order\nof magnitude.", "published": "2019-02-18 20:53:55", "link": "http://arxiv.org/abs/1902.06797v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
