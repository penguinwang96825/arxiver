{"title": "Mind The Facts: Knowledge-Boosted Coherent Abstractive Text\n  Summarization", "abstract": "Neural models have become successful at producing abstractive summaries that\nare human-readable and fluent. However, these models have two critical\nshortcomings: they often don't respect the facts that are either included in\nthe source article or are known to humans as commonsense knowledge, and they\ndon't produce coherent summaries when the source article is long. In this work,\nwe propose a novel architecture that extends Transformer encoder-decoder\narchitecture in order to improve on these shortcomings. First, we incorporate\nentity-level knowledge from the Wikidata knowledge graph into the\nencoder-decoder architecture. Injecting structural world knowledge from\nWikidata helps our abstractive summarization model to be more fact-aware.\nSecond, we utilize the ideas used in Transformer-XL language model in our\nproposed encoder-decoder architecture. This helps our model with producing\ncoherent summaries even when the source article is long. We test our model on\nCNN/Daily Mail summarization dataset and show improvements on ROUGE scores over\nthe baseline Transformer model. We also include model predictions for which our\nmodel accurately conveys the facts, while the baseline Transformer model\ndoesn't.", "published": "2020-06-27 20:06:15", "link": "http://arxiv.org/abs/2006.15435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with\n  Bilingual Semantic Similarity Rewards", "abstract": "Cross-lingual text summarization aims at generating a document summary in one\nlanguage given input in another language. It is a practically important but\nunder-explored task, primarily due to the dearth of available data. Existing\nmethods resort to machine translation to synthesize training data, but such\npipeline approaches suffer from error propagation. In this work, we propose an\nend-to-end cross-lingual text summarization model. The model uses reinforcement\nlearning to directly optimize a bilingual semantic similarity metric between\nthe summaries generated in a target language and gold summaries in a source\nlanguage. We also introduce techniques to pre-train the model leveraging\nmonolingual summarization and machine translation objectives. Experimental\nresults in both English--Chinese and English--German cross-lingual\nsummarization settings demonstrate the effectiveness of our methods. In\naddition, we find that reinforcement learning models with bilingual semantic\nsimilarity as rewards generate more fluent sentences than strong baselines.", "published": "2020-06-27 21:51:38", "link": "http://arxiv.org/abs/2006.15454v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Domain Suggestion Mining Leveraging Fine-Grained Analysis", "abstract": "Suggestion mining tasks are often semantically complex and lack sophisticated\nmethodologies that can be applied to real-world data. The presence of\nsuggestions across a large diversity of domains and the absence of large\nlabelled and balanced datasets render this task particularly challenging to\ndeal with. In an attempt to overcome these challenges, we propose a two-tier\npipeline that leverages Discourse Marker based oversampling and fine-grained\nsuggestion mining techniques to retrieve suggestions from online forums.\nThrough extensive comparison on a real-world open-domain suggestion dataset, we\ndemonstrate how the oversampling technique combined with transformer based\nfine-grained analysis can beat the state of the art. Additionally, we perform\nextensive qualitative and qualitative analysis to give construct validity to\nour proposed pipeline. Finally, we discuss the practical, computational and\nreproducibility aspects of the deployment of our pipeline across the web.", "published": "2020-06-27 21:01:52", "link": "http://arxiv.org/abs/2007.04297v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Normalizador Neural de Datas e Endere\u00e7os", "abstract": "Documents of any kind present a wide variety of date and address formats, in\nsome cases dates can be written entirely in full or even have different types\nof separators. The pattern disorder in addresses is even greater due to the\ngreater possibility of interchanging between streets, neighborhoods, cities and\nstates. In the context of natural language processing, problems of this nature\nare handled by rigid tools such as ReGex or DateParser, which are efficient as\nlong as the expected input is pre-configured. When these algorithms are given\nan unexpected format, errors and unwanted outputs happen. To circumvent this\nchallenge, we present a solution with deep neural networks state of art T5 that\ntreats non-preconfigured formats of dates and addresses with accuracy above 90%\nin some cases. With this model, our proposal brings generalization to the task\nof normalizing dates and addresses. We also deal with this problem with noisy\ndata that simulates possible errors in the text.", "published": "2020-06-27 20:24:35", "link": "http://arxiv.org/abs/2007.04300v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty-aware Self-training for Text Classification with Few Labels", "abstract": "Recent success of large-scale pre-trained language models crucially hinge on\nfine-tuning them on large amounts of labeled data for the downstream task, that\nare typically expensive to acquire. In this work, we study self-training as one\nof the earliest semi-supervised learning approaches to reduce the annotation\nbottleneck by making use of large-scale unlabeled data for the target task.\nStandard self-training mechanism randomly samples instances from the unlabeled\npool to pseudo-label and augment labeled data. In this work, we propose an\napproach to improve self-training by incorporating uncertainty estimates of the\nunderlying neural network leveraging recent advances in Bayesian deep learning.\nSpecifically, we propose (i) acquisition functions to select instances from the\nunlabeled pool leveraging Monte Carlo (MC) Dropout, and (ii) learning mechanism\nleveraging model confidence for self-training. As an application, we focus on\ntext classification on five benchmark datasets. We show our methods leveraging\nonly 20-30 labeled samples per class for each task for training and for\nvalidation can perform within 3% of fully supervised pre-trained language\nmodels fine-tuned on thousands of labeled instances with an aggregate accuracy\nof 91% and improving by upto 12% over baselines.", "published": "2020-06-27 08:13:58", "link": "http://arxiv.org/abs/2006.15315v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "String-based methods for tonal harmony: A corpus study of Haydn's string\n  quartets", "abstract": "This chapter considers how string-based methods might be adapted to address\nmusic-analytic questions related to the discovery of musical organization, with\nparticular attention devoted to the analysis of tonal harmony. I begin by\napplying the taxonomy of mental organization proposed by Mandler (1979) to the\nconcept of musical organization. Using this taxonomy as a guide, I then present\nevidence for three principles of tonal harmony -- recurrence, syntax, and\nrecursion -- using a corpus of Haydn string quartets.", "published": "2020-06-27 17:42:15", "link": "http://arxiv.org/abs/2006.15411v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Evolution of Programming Languages", "abstract": "This paper attempts to connects the evolution of computer languages with the\nevolution of life, where the later has been dictated by \\emph{theory of\nevolution of species}, and tries to give supportive evidence that the new\nlanguages are more robust than the previous, carry-over the mixed features of\nolder languages, such that strong features gets added into them and weak\nfeatures of older languages gets removed. In addition, an analysis of most\nprominent programming languages is presented, emphasizing on how the features\nof existing languages have influenced the development of new programming\nlanguages. At the end, it suggests a set of experimental languages, which may\nrule the world of programming languages in the time of new multi-core\narchitectures.\n  Index terms- Programming languages' evolution, classifications of languages,\nfuture languages, scripting-languages.", "published": "2020-06-27 10:18:14", "link": "http://arxiv.org/abs/2007.02699v1", "categories": ["cs.PL", "cs.CL", "D.3"], "primary_category": "cs.PL"}
{"title": "Video-Grounded Dialogues with Pretrained Generation Language Models", "abstract": "Pre-trained language models have shown remarkable success in improving\nvarious downstream NLP tasks due to their ability to capture dependencies in\ntextual data and generate natural responses. In this paper, we leverage the\npower of pre-trained language models for improving video-grounded dialogue,\nwhich is very challenging and involves complex features of different dynamics:\n(1) Video features which can extend across both spatial and temporal\ndimensions; and (2) Dialogue features which involve semantic dependencies over\nmultiple dialogue turns. We propose a framework by extending GPT-2 models to\ntackle these challenges by formulating video-grounded dialogue tasks as a\nsequence-to-sequence task, combining both visual and textual representation\ninto a structured sequence, and fine-tuning a large pre-trained GPT-2 network.\nOur framework allows fine-tuning language models to capture dependencies across\nmultiple modalities over different levels of information: spatio-temporal level\nin video and token-sentence level in dialogue context. We achieve promising\nimprovement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from\nDSTC7, which supports a potential direction in this line of research.", "published": "2020-06-27 08:24:26", "link": "http://arxiv.org/abs/2006.15319v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sound Event Detection Using Duration Robust Loss Function", "abstract": "Many methods of sound event detection (SED) based on machine learning regard\na segmented time frame as one data sample to model training. However, the sound\ndurations of sound events vary greatly depending on the sound event class,\ne.g., the sound event ``fan'' has a long time duration, while the sound event\n``mouse clicking'' is instantaneous. The difference in the time duration\nbetween sound event classes thus causes a serious data imbalance problem in\nSED. In this paper, we propose a method for SED using a duration robust loss\nfunction, which can focus model training on sound events of short duration. In\nthe proposed method, we focus on a relationship between the duration of the\nsound event and the ease/difficulty of model training. In particular, many\nsound events of long duration (e.g., sound event ``fan'') are stationary\nsounds, which have less variation in their acoustic features and their model\ntraining is easy. Meanwhile, some sound events of short duration (e.g., sound\nevent ``object impact'') have more than one audio pattern, such as attack,\ndecay, and release parts. We thus apply a class-wise reweighting to the\nbinary-cross entropy loss function depending on the ease/difficulty of model\ntraining. Evaluation experiments conducted using TUT Sound Events 2016/2017 and\nTUT Acoustic Scenes 2016 datasets show that the proposed method respectively\nimproves the detection performance of sound events by 3.15 and 4.37 percentage\npoints in macro- and micro-Fscores compared with a conventional method using\nthe binary-cross entropy loss function.", "published": "2020-06-27 01:49:25", "link": "http://arxiv.org/abs/2006.15253v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Anomalous Sound Detection using unsupervised and semi-supervised\n  autoencoders and gammatone audio representation", "abstract": "Anomalous sound detection (ASD) is, nowadays, one of the topical subjects in\nmachine listening discipline. Unsupervised detection is attracting a lot of\ninterest due to its immediate applicability in many fields. For example,\nrelated to industrial processes, the early detection of malfunctions or damage\nin machines can mean great savings and an improvement in the efficiency of\nindustrial processes. This problem can be solved with an unsupervised ASD\nsolution since industrial machines will not be damaged simply by having this\naudio data in the training stage. This paper proposes a novel framework based\non convolutional autoencoders (both unsupervised and semi-supervised) and a\nGammatone-based representation of the audio. The results obtained by these\narchitectures substantially exceed the results presented as a baseline.", "published": "2020-06-27 08:25:47", "link": "http://arxiv.org/abs/2006.15321v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Beneath (or beyond) the surface: Discovering voice-leading patterns with\n  skip-grams", "abstract": "Recurrent voice-leading patterns like the Mi-Re-Do compound cadence (MRDCC)\nrarely appear on the musical surface in complex polyphonic textures, so finding\nthese patterns using computational methods remains a tremendous challenge. The\npresent study extends the canonical n-gram approach by using skip-grams, which\ninclude sub-sequences in an n-gram list if their constituent members occur\nwithin a certain number of skips. We compiled four data sets of Western tonal\nmusic consisting of symbolic encodings of the notated score and a recorded\nperformance, created a model pipeline for defining, counting, filtering, and\nranking skip-grams, and ranked the position of the MRDCC in every possible\nmodel configuration. We found that the MRDCC receives a higher rank in the list\nwhen the pipeline employs 5 skips, filters the list by excluding n-gram types\nthat do not reflect a genuine harmonic change between adjacent members, and\nranks the remaining types using a statistical association measure.", "published": "2020-06-27 16:21:18", "link": "http://arxiv.org/abs/2006.15399v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Listen carefully and tell: an audio captioning system based on residual\n  learning and gammatone audio representation", "abstract": "Automated audio captioning is machine listening task whose goal is to\ndescribe an audio using free text. An automated audio captioning system has to\nbe implemented as it accepts an audio as input and outputs as textual\ndescription, that is, the caption of the signal. This task can be useful in\nmany applications such as automatic content description or machine-to-machine\ninteraction. In this work, an automatic audio captioning based on residual\nlearning on the encoder phase is proposed. The encoder phase is implemented via\ndifferent Residual Networks configurations. The decoder phase (create the\ncaption) is run using recurrent layers plus attention mechanism. The audio\nrepresentation chosen has been Gammatone. Results show that the framework\nproposed in this work surpass the baseline system in challenge results.", "published": "2020-06-27 17:16:49", "link": "http://arxiv.org/abs/2006.15406v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
