{"title": "MEE: A Novel Multilingual Event Extraction Dataset", "abstract": "Event Extraction (EE) is one of the fundamental tasks in Information\nExtraction (IE) that aims to recognize event mentions and their arguments\n(i.e., participants) from text. Due to its importance, extensive methods and\nresources have been developed for Event Extraction. However, one limitation of\ncurrent research for EE involves the under-exploration for non-English\nlanguages in which the lack of high-quality multilingual EE datasets for model\ntraining and evaluation has been the main hindrance. To address this\nlimitation, we propose a novel Multilingual Event Extraction dataset (MEE) that\nprovides annotation for more than 50K event mentions in 8 typologically\ndifferent languages. MEE comprehensively annotates data for entity mentions,\nevent triggers and event arguments. We conduct extensive experiments on the\nproposed dataset to reveal challenges and opportunities for multilingual EE.", "published": "2022-11-11 02:01:41", "link": "http://arxiv.org/abs/2211.05955v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MINION: a Large-Scale and Diverse Dataset for Multilingual Event\n  Detection", "abstract": "Event Detection (ED) is the task of identifying and classifying trigger words\nof event mentions in text. Despite considerable research efforts in recent\nyears for English text, the task of ED in other languages has been\nsignificantly less explored. Switching to non-English languages, important\nresearch questions for ED include how well existing ED models perform on\ndifferent languages, how challenging ED is in other languages, and how well ED\nknowledge and annotation can be transferred across languages. To answer those\nquestions, it is crucial to obtain multilingual ED datasets that provide\nconsistent event annotation for multiple languages. There exist some\nmultilingual ED datasets; however, they tend to cover a handful of languages\nand mainly focus on popular ones. Many languages are not covered in existing\nmultilingual ED datasets. In addition, the current datasets are often small and\nnot accessible to the public. To overcome those shortcomings, we introduce a\nnew large-scale multilingual dataset for ED (called MINION) that consistently\nannotates events for 8 different languages; 5 of them have not been supported\nby existing multilingual datasets. We also perform extensive experiments and\nanalysis to demonstrate the challenges and transferability of ED across\nlanguages in MINION that in all call for more research effort in this area.", "published": "2022-11-11 02:09:51", "link": "http://arxiv.org/abs/2211.05958v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Getting the Most out of Simile Recognition", "abstract": "Simile recognition involves two subtasks: simile sentence classification that\ndiscriminates whether a sentence contains simile, and simile component\nextraction that locates the corresponding objects (i.e., tenors and vehicles).\nRecent work ignores features other than surface strings. In this paper, we\nexplore expressive features for this task to achieve more effective data\nutilization. Particularly, we study two types of features: 1) input-side\nfeatures that include POS tags, dependency trees and word definitions, and 2)\ndecoding features that capture the interdependence among various decoding\ndecisions. We further construct a model named HGSR, which merges the input-side\nfeatures as a heterogeneous graph and leverages decoding features via\ndistillation. Experiments show that HGSR significantly outperforms the current\nstate-of-the-art systems and carefully designed baselines, verifying the\neffectiveness of introduced features. Our code is available at\nhttps://github.com/DeepLearnXMU/HGSR.", "published": "2022-11-11 03:22:45", "link": "http://arxiv.org/abs/2211.05984v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CCPrefix: Counterfactual Contrastive Prefix-Tuning for Many-Class\n  Classification", "abstract": "Recently, prefix-tuning was proposed to efficiently adapt pre-trained\nlanguage models to a broad spectrum of natural language classification tasks.\nIt leverages soft prefix as task-specific indicators and language verbalizers\nas categorical-label mentions to narrow the formulation gap from pre-training\nlanguage models. However, when the label space increases considerably (i.e.,\nmany-class classification), such a tuning technique suffers from a verbalizer\nambiguity problem since the many-class labels are represented by\nsemantic-similar verbalizers in short language phrases. To overcome this,\ninspired by the human-decision process that the most ambiguous classes would be\nmulled over for each instance, we propose a brand-new prefix-tuning method,\nCounterfactual Contrastive Prefix-tuning (CCPrefix), for many-class\nclassification. Basically, an instance-dependent soft prefix, derived from\nfact-counterfactual pairs in the label space, is leveraged to complement the\nlanguage verbalizers in many-class classification. We conduct experiments on\nmany-class benchmark datasets in both the fully supervised setting and the\nfew-shot setting, which indicates that our model outperforms former baselines.", "published": "2022-11-11 03:45:59", "link": "http://arxiv.org/abs/2211.05987v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Knowledge Enhanced Pre-trained Language Models", "abstract": "Pre-trained Language Models (PLMs) which are trained on large text corpus via\nself-supervised learning method, have yielded promising performance on various\ntasks in Natural Language Processing (NLP). However, though PLMs with huge\nparameters can effectively possess rich knowledge learned from massive training\ntext and benefit downstream tasks at the fine-tuning stage, they still have\nsome limitations such as poor reasoning ability due to the lack of external\nknowledge. Research has been dedicated to incorporating knowledge into PLMs to\ntackle these issues. In this paper, we present a comprehensive review of\nKnowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear\ninsight into this thriving field. We introduce appropriate taxonomies\nrespectively for Natural Language Understanding (NLU) and Natural Language\nGeneration (NLG) to highlight these two main tasks of NLP. For NLU, we divide\nthe types of knowledge into four categories: linguistic knowledge, text\nknowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are\ncategorized into KG-based and retrieval-based methods. Finally, we point out\nsome promising future directions of KE-PLMs.", "published": "2022-11-11 04:29:02", "link": "http://arxiv.org/abs/2211.05994v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-game Toxic Language Detection: Shared Task and Attention Residuals", "abstract": "In-game toxic language becomes the hot potato in the gaming industry and\ncommunity. There have been several online game toxicity analysis frameworks and\nmodels proposed. However, it is still challenging to detect toxicity due to the\nnature of in-game chat, which has extremely short length. In this paper, we\ndescribe how the in-game toxic language shared task has been established using\nthe real-world in-game chat data. In addition, we propose and introduce the\nmodel/framework for toxic language token tagging (slot filling) from the\nin-game chat. The relevant code is publicly available on GitHub:\nhttps://github.com/Yuanzhe-Jia/In-Game-Toxic-Detection", "published": "2022-11-11 04:33:45", "link": "http://arxiv.org/abs/2211.05995v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoRAL: a Context-aware Croatian Abusive Language Dataset", "abstract": "In light of unprecedented increases in the popularity of the internet and\nsocial media, comment moderation has never been a more relevant task.\nSemi-automated comment moderation systems greatly aid human moderators by\neither automatically classifying the examples or allowing the moderators to\nprioritize which comments to consider first. However, the concept of\ninappropriate content is often subjective, and such content can be conveyed in\nmany subtle and indirect ways. In this work, we propose CoRAL -- a language and\nculturally aware Croatian Abusive dataset covering phenomena of implicitness\nand reliance on local and global context. We show experimentally that current\nmodels degrade when comments are not explicit and further degrade when language\nskill and context knowledge are required to interpret the comment.", "published": "2022-11-11 08:10:13", "link": "http://arxiv.org/abs/2211.06053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Helping the Weak Makes You Strong: Simple Multi-Task Learning Improves\n  Non-Autoregressive Translators", "abstract": "Recently, non-autoregressive (NAR) neural machine translation models have\nreceived increasing attention due to their efficient parallel decoding.\nHowever, the probabilistic framework of NAR models necessitates conditional\nindependence assumption on target sequences, falling short of characterizing\nhuman language data. This drawback results in less informative learning signals\nfor NAR models under conventional MLE training, thereby yielding unsatisfactory\naccuracy compared to their autoregressive (AR) counterparts. In this paper, we\npropose a simple and model-agnostic multi-task learning framework to provide\nmore informative learning signals. During training stage, we introduce a set of\nsufficiently weak AR decoders that solely rely on the information provided by\nNAR decoder to make prediction, forcing the NAR decoder to become stronger or\nelse it will be unable to support its weak AR partners. Experiments on WMT and\nIWSLT datasets show that our approach can consistently improve accuracy of\nmultiple NAR baselines without adding any additional decoding overhead.", "published": "2022-11-11 09:10:14", "link": "http://arxiv.org/abs/2211.06075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unimodal and Multimodal Representation Training for Relation Extraction", "abstract": "Multimodal integration of text, layout and visual information has achieved\nSOTA results in visually rich document understanding (VrDU) tasks, including\nrelation extraction (RE). However, despite its importance, evaluation of the\nrelative predictive capacity of these modalities is less prevalent. Here, we\ndemonstrate the value of shared representations for RE tasks by conducting\nexperiments in which each data type is iteratively excluded during training. In\naddition, text and layout data are evaluated in isolation. While a bimodal text\nand layout approach performs best (F1=0.684), we show that text is the most\nimportant single predictor of entity relations. Additionally, layout geometry\nis highly predictive and may even be a feasible unimodal approach. Despite\nbeing less effective, we highlight circumstances where visual information can\nbolster performance. In total, our results demonstrate the efficacy of training\njoint representations for RE.", "published": "2022-11-11 12:39:35", "link": "http://arxiv.org/abs/2211.06168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocuT5: Seq2seq SQL Generation with Table Documentation", "abstract": "Current SQL generators based on pre-trained language models struggle to\nanswer complex questions requiring domain context or understanding fine-grained\ntable structure. Humans would deal with these unknowns by reasoning over the\ndocumentation of the tables. Based on this hypothesis, we propose DocuT5, which\nuses off-the-shelf language model architecture and injects knowledge from\nexternal `documentation' to improve domain generalization. We perform\nexperiments on the Spider family of datasets that contain complex questions\nthat are cross-domain and multi-table. Specifically, we develop a new\ntext-to-SQL failure taxonomy and find that 19.6% of errors are due to foreign\nkey mistakes, and 49.2% are due to a lack of domain knowledge. We proposed\nDocuT5, a method that captures knowledge from (1) table structure context of\nforeign keys and (2) domain knowledge through contextualizing tables and\ncolumns. Both types of knowledge improve over state-of-the-art T5 with\nconstrained decoding on Spider, and domain knowledge produces state-of-the-art\ncomparable effectiveness on Spider-DK and Spider-SYN datasets.", "published": "2022-11-11 13:31:55", "link": "http://arxiv.org/abs/2211.06193v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Factual Consistency in Summarization with Compression-Based\n  Post-Editing", "abstract": "State-of-the-art summarization models still struggle to be factually\nconsistent with the input text. A model-agnostic way to address this problem is\npost-editing the generated summaries. However, existing approaches typically\nfail to remove entity errors if a suitable input entity replacement is not\navailable or may insert erroneous content. In our work, we focus on removing\nextrinsic entity errors, or entities not in the source, to improve consistency\nwhile retaining the summary's essential information and form. We propose to use\nsentence-compression data to train the post-editing model to take a summary\nwith extrinsic entity errors marked with special tokens and output a\ncompressed, well-formed summary with those errors removed. We show that this\nmodel improves factual consistency while maintaining ROUGE, improving entity\nprecision by up to 30% on XSum, and that this model can be applied on top of\nanother post-editor, improving entity precision by up to a total of 38%. We\nperform an extensive comparison of post-editing approaches that demonstrate\ntrade-offs between factual consistency, informativeness, and grammaticality,\nand we analyze settings where post-editors show the largest improvements.", "published": "2022-11-11 13:35:38", "link": "http://arxiv.org/abs/2211.06196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving word mover's distance by leveraging self-attention matrix", "abstract": "Measuring the semantic similarity between two sentences is still an important\ntask. The word mover's distance (WMD) computes the similarity via the optimal\nalignment between the sets of word embeddings. However, WMD does not utilize\nword order, making it challenging to distinguish sentences with significant\noverlaps of similar words, even if they are semantically very different. Here,\nwe attempt to improve WMD by incorporating the sentence structure represented\nby BERT's self-attention matrix (SAM). The proposed method is based on the\nFused Gromov-Wasserstein distance, which simultaneously considers the\nsimilarity of the word embedding and the SAM for calculating the optimal\ntransport between two sentences. Experiments demonstrate the proposed method\nenhances WMD and its variants in paraphrase identification with near-equivalent\nperformance in semantic textual similarity. Our code is available at\n\\url{https://github.com/ymgw55/WSMD}.", "published": "2022-11-11 14:25:08", "link": "http://arxiv.org/abs/2211.06229v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Align, Write, Re-order: Explainable End-to-End Speech Translation via\n  Operation Sequence Generation", "abstract": "The black-box nature of end-to-end speech translation (E2E ST) systems makes\nit difficult to understand how source language inputs are being mapped to the\ntarget language. To solve this problem, we would like to simultaneously\ngenerate automatic speech recognition (ASR) and ST predictions such that each\nsource language word is explicitly mapped to a target language word. A major\nchallenge arises from the fact that translation is a non-monotonic sequence\ntransduction task due to word ordering differences between languages -- this\nclashes with the monotonic nature of ASR. Therefore, we propose to generate ST\ntokens out-of-order while remembering how to re-order them later. We achieve\nthis by predicting a sequence of tuples consisting of a source word, the\ncorresponding target words, and post-editing operations dictating the correct\ninsertion points for the target word. We examine two variants of such operation\nsequences which enable generation of monotonic transcriptions and non-monotonic\ntranslations from the same speech input simultaneously. We apply our approach\nto offline and real-time streaming models, demonstrating that we can provide\nexplainable translations without sacrificing quality or latency. In fact, the\ndelayed re-ordering ability of our approach improves performance during\nstreaming. As an added benefit, our method performs ASR and ST simultaneously,\nmaking it faster than using two separate systems to perform these tasks.", "published": "2022-11-11 02:29:28", "link": "http://arxiv.org/abs/2211.05967v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Hardness-guided domain adaptation to recognise biomedical named entities\n  under low-resource scenarios", "abstract": "Domain adaptation is an effective solution to data scarcity in low-resource\nscenarios. However, when applied to token-level tasks such as bioNER, domain\nadaptation methods often suffer from the challenging linguistic characteristics\nthat clinical narratives possess, which leads to unsatisfactory performance. In\nthis paper, we present a simple yet effective hardness-guided domain adaptation\n(HGDA) framework for bioNER tasks that can effectively leverage the domain\nhardness information to improve the adaptability of the learnt model in\nlow-resource scenarios. Experimental results on biomedical datasets show that\nour model can achieve significant performance improvement over the recently\npublished state-of-the-art (SOTA) MetaNER model", "published": "2022-11-11 03:11:21", "link": "http://arxiv.org/abs/2211.05980v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gradient Imitation Reinforcement Learning for General Low-Resource\n  Information Extraction", "abstract": "Information Extraction (IE) aims to extract structured information from\nheterogeneous sources. IE from natural language texts include sub-tasks such as\nNamed Entity Recognition (NER), Relation Extraction (RE), and Event Extraction\n(EE). Most IE systems require comprehensive understandings of sentence\nstructure, implied semantics, and domain knowledge to perform well; thus, IE\ntasks always need adequate external resources and annotations. However, it\ntakes time and effort to obtain more human annotations. Low-Resource\nInformation Extraction (LRIE) strives to use unsupervised data, reducing the\nrequired resources and human annotation. In practice, existing systems either\nutilize self-training schemes to generate pseudo labels that will cause the\ngradual drift problem, or leverage consistency regularization methods which\ninevitably possess confirmation bias. To alleviate confirmation bias due to the\nlack of feedback loops in existing LRIE learning paradigms, we develop a\nGradient Imitation Reinforcement Learning (GIRL) method to encourage\npseudo-labeled data to imitate the gradient descent direction on labeled data,\nwhich can force pseudo-labeled data to achieve better optimization capabilities\nsimilar to labeled data. Based on how well the pseudo-labeled data imitates the\ninstructive gradient descent direction obtained from labeled data, we design a\nreward to quantify the imitation process and bootstrap the optimization\ncapability of pseudo-labeled data through trial and error. In addition to\nlearning paradigms, GIRL is not limited to specific sub-tasks, and we leverage\nGIRL to solve all IE sub-tasks (named entity recognition, relation extraction,\nand event extraction) in low-resource settings (semi-supervised IE and few-shot\nIE).", "published": "2022-11-11 05:37:19", "link": "http://arxiv.org/abs/2211.06014v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "English Contrastive Learning Can Learn Universal Cross-lingual Sentence\n  Embeddings", "abstract": "Universal cross-lingual sentence embeddings map semantically similar\ncross-lingual sentences into a shared embedding space. Aligning cross-lingual\nsentence embeddings usually requires supervised cross-lingual parallel\nsentences. In this work, we propose mSimCSE, which extends SimCSE to\nmultilingual settings and reveal that contrastive learning on English data can\nsurprisingly learn high-quality universal cross-lingual sentence embeddings\nwithout any parallel data. In unsupervised and weakly supervised settings,\nmSimCSE significantly improves previous sentence embedding methods on\ncross-lingual retrieval and multilingual STS tasks. The performance of\nunsupervised mSimCSE is comparable to fully supervised methods in retrieving\nlow-resource languages and multilingual STS. The performance can be further\nenhanced when cross-lingual NLI data is available. Our code is publicly\navailable at https://github.com/yaushian/mSimCSE.", "published": "2022-11-11 11:17:56", "link": "http://arxiv.org/abs/2211.06127v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Developer Discussions to Guide Fixing Bugs in Software", "abstract": "Automatically fixing software bugs is a challenging task. While recent work\nshowed that natural language context is useful in guiding bug-fixing models,\nthe approach required prompting developers to provide this context, which was\nsimulated through commit messages written after the bug-fixing code changes\nwere made. We instead propose using bug report discussions, which are available\nbefore the task is performed and are also naturally occurring, avoiding the\nneed for any additional information from developers. For this, we augment\nstandard bug-fixing datasets with bug report discussions. Using these newly\ncompiled datasets, we demonstrate that various forms of natural language\ncontext derived from such discussions can aid bug-fixing, even leading to\nimproved performance over using commit messages corresponding to the oracle\nbug-fixing commits.", "published": "2022-11-11 16:37:33", "link": "http://arxiv.org/abs/2211.06335v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Analysis of Male and Female Speakers' Word Choices in Public Speeches", "abstract": "The extent to which men and women use language differently has been\nquestioned previously. Finding clear and consistent gender differences in\nlanguage is not conclusive in general, and the research is heavily influenced\nby the context and method employed to identify the difference. In addition, the\nmajority of the research was conducted in written form, and the sample was\ncollected in writing. Therefore, we compared the word choices of male and\nfemale presenters in public addresses such as TED lectures. The frequency of\nnumerous types of words, such as parts of speech (POS), linguistic,\npsychological, and cognitive terms were analyzed statistically to determine how\nmale and female speakers use words differently. Based on our data, we\ndetermined that male speakers use specific types of linguistic, psychological,\ncognitive, and social words in considerably greater frequency than female\nspeakers.", "published": "2022-11-11 17:30:28", "link": "http://arxiv.org/abs/2211.06366v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "A Federated Approach to Predicting Emojis in Hindi Tweets", "abstract": "The use of emojis affords a visual modality to, often private, textual\ncommunication. The task of predicting emojis however provides a challenge for\nmachine learning as emoji use tends to cluster into the frequently used and the\nrarely used emojis. Much of the machine learning research on emoji use has\nfocused on high resource languages and has conceptualised the task of\npredicting emojis around traditional server-side machine learning approaches.\nHowever, traditional machine learning approaches for private communication can\nintroduce privacy concerns, as these approaches require all data to be\ntransmitted to a central storage. In this paper, we seek to address the dual\nconcerns of emphasising high resource languages for emoji prediction and\nrisking the privacy of people's data. We introduce a new dataset of $118$k\ntweets (augmented from $25$k unique tweets) for emoji prediction in Hindi, and\npropose a modification to the federated learning algorithm, CausalFedGSD, which\naims to strike a balance between model performance and user privacy. We show\nthat our approach obtains comparative scores with more complex centralised\nmodels while reducing the amount of data required to optimise the models and\nminimising risks to user privacy.", "published": "2022-11-11 18:37:33", "link": "http://arxiv.org/abs/2211.06401v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Architectural Bottleneck Principle", "abstract": "In this paper, we seek to measure how much information a component in a\nneural network could extract from the representations fed into it. Our work\nstands in contrast to prior probing work, most of which investigates how much\ninformation a model's representations contain. This shift in perspective leads\nus to propose a new principle for probing, the architectural bottleneck\nprinciple: In order to estimate how much information a given component could\nextract, a probe should look exactly like the component. Relying on this\nprinciple, we estimate how much syntactic information is available to\ntransformers through our attentional probe, a probe that exactly resembles a\ntransformer's self-attention head. Experimentally, we find that, in three\nmodels (BERT, ALBERT, and RoBERTa), a sentence's syntax tree is mostly\nextractable by our probe, suggesting these models have access to syntactic\ninformation while composing their contextual representations. Whether this\ninformation is actually used by these models, however, remains an open\nquestion.", "published": "2022-11-11 18:58:08", "link": "http://arxiv.org/abs/2211.06420v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Platform and Cross-Domain Abusive Language Detection with\n  Supervised Contrastive Learning", "abstract": "The prevalence of abusive language on different online platforms has been a\nmajor concern that raises the need for automated cross-platform abusive\nlanguage detection. However, prior works focus on concatenating data from\nmultiple platforms, inherently adopting Empirical Risk Minimization (ERM)\nmethod. In this work, we address this challenge from the perspective of domain\ngeneralization objective. We design SCL-Fish, a supervised contrastive learning\nintegrated meta-learning algorithm to detect abusive language on unseen\nplatforms. Our experimental analysis shows that SCL-Fish achieves better\nperformance over ERM and the existing state-of-the-art models. We also show\nthat SCL-Fish is data-efficient and achieves comparable performance with the\nlarge-scale pre-trained models upon finetuning for the abusive language\ndetection task.", "published": "2022-11-11 19:22:36", "link": "http://arxiv.org/abs/2211.06452v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Breadth-First Pipeline Parallelism", "abstract": "We introduce Breadth-First Pipeline Parallelism, a novel training schedule\nwhich optimizes the combination of pipeline and data parallelism. Breadth-First\nPipeline Parallelism lowers training time, cost and memory usage by combining a\nhigh GPU utilization with a small batch size per GPU, and by making use of\nfully sharded data parallelism. Experimentally, we observed an increase of up\nto 43% in training throughput for a 52 billion-parameter model using a small\nbatch size per GPU compared to Megatron-LM, which would reduce the training\ntime and cost by the same amount on a large GPU cluster.", "published": "2022-11-11 02:00:32", "link": "http://arxiv.org/abs/2211.05953v2", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "Using Persuasive Writing Strategies to Explain and Detect Health\n  Misinformation", "abstract": "Nowadays, the spread of misinformation is a prominent problem in society. Our\nresearch focuses on aiding the automatic identification of misinformation by\nanalyzing the persuasive strategies employed in textual documents. We introduce\na novel annotation scheme encompassing common persuasive writing tactics to\nachieve our objective. Additionally, we provide a dataset on health\nmisinformation, thoroughly annotated by experts utilizing our proposed scheme.\nOur contribution includes proposing a new task of annotating pieces of text\nwith their persuasive writing strategy types. We evaluate fine-tuning and\nprompt-engineering techniques with pre-trained language models of the BERT\nfamily and the generative large language models of the GPT family using\npersuasive strategies as an additional source of information. We evaluate the\neffects of employing persuasive strategies as intermediate labels in the\ncontext of misinformation detection. Our results show that those strategies\nenhance accuracy and improve the explainability of misinformation detection\nmodels. The persuasive strategies can serve as valuable insights and\nexplanations, enabling other models or even humans to make more informed\ndecisions regarding the trustworthiness of the information.", "published": "2022-11-11 03:26:37", "link": "http://arxiv.org/abs/2211.05985v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SceneFake: An Initial Dataset and Benchmarks for Scene Fake Audio\n  Detection", "abstract": "Many datasets have been designed to further the development of fake audio\ndetection. However, fake utterances in previous datasets are mostly generated\nby altering timbre, prosody, linguistic content or channel noise of original\naudio. These datasets leave out a scenario, in which the acoustic scene of an\noriginal audio is manipulated with a forged one. It will pose a major threat to\nour society if some people misuse the manipulated audio with malicious purpose.\nTherefore, this motivates us to fill in the gap. This paper proposes such a\ndataset for scene fake audio detection named SceneFake, where a manipulated\naudio is generated by only tampering with the acoustic scene of an real\nutterance by using speech enhancement technologies. Some scene fake audio\ndetection benchmark results on the SceneFake dataset are reported in this\npaper. In addition, an analysis of fake attacks with different speech\nenhancement technologies and signal-to-noise ratios are presented in this\npaper. The results indicate that scene fake utterances cannot be reliably\ndetected by baseline models trained on the ASVspoof 2019 dataset. Although\nthese models perform well on the SceneFake training set and seen testing set,\ntheir performance is poor on the unseen test set. The dataset\n(https://zenodo.org/record/7663324#.Y_XKMuPYuUk) and benchmark source codes\n(https://github.com/ADDchallenge/SceneFake) are publicly available.", "published": "2022-11-11 09:05:50", "link": "http://arxiv.org/abs/2211.06073v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards automating Numerical Consistency Checks in Financial Reports", "abstract": "We introduce KPI-Check, a novel system that automatically identifies and\ncross-checks semantically equivalent key performance indicators (KPIs), e.g.\n\"revenue\" or \"total costs\", in real-world German financial reports. It combines\na financial named entity and relation extraction module with a BERT-based\nfiltering and text pair classification component to extract KPIs from\nunstructured sentences before linking them to synonymous occurrences in the\nbalance sheet and profit & loss statement. The tool achieves a high matching\nperformance of $73.00$% micro F$_1$ on a hold out test set and is currently\nbeing deployed for a globally operating major auditing firm to assist the\nauditing procedure of financial statements.", "published": "2022-11-11 10:35:07", "link": "http://arxiv.org/abs/2211.06112v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Much Hate with #china? A Preliminary Analysis on China-related\n  Hateful Tweets Two Years After the Covid Pandemic Began", "abstract": "Following the outbreak of a global pandemic, online content is filled with\nhate speech. Donald Trump's ''Chinese Virus'' tweet shifted the blame for the\nspread of the Covid-19 virus to China and the Chinese people, which triggered a\nnew round of anti-China hate both online and offline. This research intends to\nexamine China-related hate speech on Twitter during the two years following the\nburst of the pandemic (2020 and 2021). Through Twitter's API, in total\n2,172,333 tweets hashtagged #china posted during the time were collected. By\nemploying multiple state-of-the-art pretrained language models for hate speech\ndetection, we identify a wide range of hate of various types, resulting in an\nautomatically labeled anti-China hate speech dataset. We identify a hateful\nrate in #china tweets of 2.5% in 2020 and 1.9% in 2021. This is well above the\naverage rate of online hate speech on Twitter at 0.6% identified in Gao et al.,\n2017. We further analyzed the longitudinal development of #china tweets and\nthose identified as hateful in 2020 and 2021 through visualizing the daily\nnumber and hate rate over the two years. Our keyword analysis of hate speech in\n#china tweets reveals the most frequently mentioned terms in the hateful #china\ntweets, which can be used for further social science studies.", "published": "2022-11-11 10:48:00", "link": "http://arxiv.org/abs/2211.06116v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A hybrid entity-centric approach to Persian pronoun resolution", "abstract": "Pronoun resolution is a challenging subset of an essential field in natural\nlanguage processing called coreference resolution. Coreference resolution is\nabout finding all entities in the text that refers to the same real-world\nentity. This paper presents a hybrid model combining multiple rulebased sieves\nwith a machine-learning sieve for pronouns. For this purpose, seven\nhigh-precision rule-based sieves are designed for the Persian language. Then, a\nrandom forest classifier links pronouns to the previous partial clusters. The\npresented method demonstrates exemplary performance using pipeline design and\ncombining the advantages of machine learning and rulebased methods. This method\nhas solved some challenges in end-to-end models. In this paper, the authors\ndevelop a Persian coreference corpus called Mehr in the form of 400 documents.\nThis corpus fixes some weaknesses of the previous corpora in the Persian\nlanguage. Finally, the efficiency of the presented system compared to the\nearlier model in Persian is reported by evaluating the proposed method on the\nMehr and Uppsala test sets.", "published": "2022-11-11 14:59:58", "link": "http://arxiv.org/abs/2211.06257v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing and Adversarial: Improve ASR with Speaker Labels", "abstract": "ASR can be improved by multi-task learning (MTL) with domain enhancing or\ndomain adversarial training, which are two opposite objectives with the aim to\nincrease/decrease domain variance towards domain-aware/agnostic ASR,\nrespectively. In this work, we study how to best apply these two opposite\nobjectives with speaker labels to improve conformer-based ASR. We also propose\na novel adaptive gradient reversal layer for stable and effective adversarial\ntraining without tuning effort. Detailed analysis and experimental verification\nare conducted to show the optimal positions in the ASR neural network (NN) to\napply speaker enhancing and adversarial training. We also explore their\ncombination for further improvement, achieving the same performance as\ni-vectors plus adversarial training. Our best speaker-based MTL achieves 7\\%\nrelative improvement on the Switchboard Hub5'00 set. We also investigate the\neffect of such speaker-based MTL w.r.t. cleaner dataset and weaker ASR NN.", "published": "2022-11-11 17:40:08", "link": "http://arxiv.org/abs/2211.06369v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech-to-Speech Translation For A Real-world Unwritten Language", "abstract": "We study speech-to-speech translation (S2ST) that translates speech from one\nlanguage into another language and focuses on building systems to support\nlanguages without standard text writing systems. We use English-Taiwanese\nHokkien as a case study, and present an end-to-end solution from training data\ncollection, modeling choices to benchmark dataset release. First, we present\nefforts on creating human annotated data, automatically mining data from large\nunlabeled speech datasets, and adopting pseudo-labeling to produce weakly\nsupervised data. On the modeling, we take advantage of recent advances in\napplying self-supervised discrete representations as target for prediction in\nS2ST and show the effectiveness of leveraging additional text supervision from\nMandarin, a language similar to Hokkien, in model training. Finally, we release\nan S2ST benchmark set to facilitate future research in this field. The demo can\nbe found at https://huggingface.co/spaces/facebook/Hokkien_Translation .", "published": "2022-11-11 20:21:38", "link": "http://arxiv.org/abs/2211.06474v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Far Side of Failure: Investigating the Impact of Speech Recognition\n  Errors on Subsequent Dementia Classification", "abstract": "Linguistic anomalies detectable in spontaneous speech have shown promise for\nvarious clinical applications including screening for dementia and other forms\nof cognitive impairment. The feasibility of deploying automated tools that can\nclassify language samples obtained from speech in large-scale clinical settings\ndepends on the ability to capture and automatically transcribe the speech for\nsubsequent analysis. However, the impressive performance of self-supervised\nlearning (SSL) automatic speech recognition (ASR) models with curated speech\ndata is not apparent with challenging speech samples from clinical settings.\nOne of the key questions for successfully applying ASR models for clinical\napplications is whether imperfect transcripts they generate provide sufficient\ninformation for downstream tasks to operate at an acceptable level of accuracy.\nIn this study, we examine the relationship between the errors produced by\nseveral deep learning ASR systems and their impact on the downstream task of\ndementia classification. One of our key findings is that, paradoxically, ASR\nsystems with relatively high error rates can produce transcripts that result in\nbetter downstream classification accuracy than classification based on verbatim\ntranscripts.", "published": "2022-11-11 17:06:45", "link": "http://arxiv.org/abs/2211.07430v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "An Adapter based Multi-label Pre-training for Speech Separation and\n  Enhancement", "abstract": "In recent years, self-supervised learning (SSL) has achieved tremendous\nsuccess in various speech tasks due to its power to extract representations\nfrom massive unlabeled data. However, compared with tasks such as speech\nrecognition (ASR), the improvements from SSL representation in speech\nseparation (SS) and enhancement (SE) are considerably smaller. Based on HuBERT,\nthis work investigates improving the SSL model for SS and SE. We first update\nHuBERT's masked speech prediction (MSP) objective by integrating the separation\nand denoising terms, resulting in a multiple pseudo label pre-training scheme,\nwhich significantly improves HuBERT's performance on SS and SE but degrades the\nperformance on ASR. To maintain its performance gain on ASR, we further propose\nan adapter-based architecture for HuBERT's Transformer encoder, where only a\nfew parameters of each layer are adjusted to the multiple pseudo label MSP\nwhile other parameters remain frozen as default HuBERT. Experimental results\nshow that our proposed adapter-based multiple pseudo label HuBERT yield\nconsistent and significant performance improvements on SE, SS, and ASR tasks,\nwith a faster pre-training speed, at only marginal parameters increase.", "published": "2022-11-11 07:34:32", "link": "http://arxiv.org/abs/2211.06041v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Vocal Breath Sound Based Gender Classification", "abstract": "Voiced speech signals such as continuous speech are known to have acoustic\nfeatures such as pitch(F0), and formant frequencies(F1, F2, F3) which can be\nused for gender classification. However, gender classification studies using\nnon-speech signals such as vocal breath sounds have not been explored as they\nlack typical gender-specific acoustic features. In this work, we explore\nwhether vocal breath sounds encode gender information and if so, to what extent\nit can be used for automatic gender classification. In this study, we explore\nthe use of data-driven and knowledge-based features from vocal breath sounds as\nwell as the classifier complexity for gender classification. We also explore\nthe importance of the location and duration of breath signal segments to be\nused for automatic classification. Experiments with 54.23 minutes of male and\n51.83 minutes of female breath sounds reveal that knowledge-based features,\nnamely MFCC statistics, with low-complexity classifier perform comparably to\nthe data-driven features with classifiers of higher complexity. Breath segments\nwith an average duration of 3 seconds are found to be the best choice\nirrespective of the location which avoids the need for breath cycle boundary\nannotation.", "published": "2022-11-11 17:43:26", "link": "http://arxiv.org/abs/2211.06371v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Gait Triaging Toolkit for Overlapping Acoustic Events in Indoor\n  Environments", "abstract": "Gait has been used in clinical and healthcare applications to assess the\nphysical and cognitive health of older adults. Acoustic based gait detection is\na promising approach to collect gait data of older adults passively and\nnon-intrusively. However, there has been limited work in developing acoustic\nbased gait detectors that can operate in noisy polyphonic acoustic scenes of\nhomes and care homes. We attribute this to the lack of good quality gait\ndatasets from the real-world to train a gait detector on. In this paper, we put\nforward a novel machine learning based filter which can triage gait audio\nsamples suitable for training machine learning models for gait detection. The\nfilter achieves this by eliminating noisy samples at an f(1) score of 0.85 and\nprioritising gait samples with distinct spectral features and minimal noise. To\ndemonstrate the effectiveness of the filter, we train and evaluate a deep\nlearning model on gait datasets collected from older adults with and without\napplying the filter. The model registers an increase of 25 points in its f(1)\nscore on unseen real-word gait data when trained with the filtered gait\nsamples. The proposed filter will help automate the task of manual annotation\nof gait samples for training acoustic based gait detection models for older\nadults in indoor environments.", "published": "2022-11-11 01:33:14", "link": "http://arxiv.org/abs/2211.05944v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Pornography Recognition Using Convolutional Neural Networks and\n  Bag of Refinements", "abstract": "A large number of pornographic audios publicly available on the Internet\nseriously threaten the mental and physical health of children, but these audios\nare rarely detected and filtered. In this paper, we firstly propose a\nconvolutional neural networks (CNN) based model for acoustic pornography\nrecognition. Then, we research a collection of refinements and verify their\neffectiveness through ablation studies. Finally, we stack all refinements\ntogether to verify whether they can further improve the accuracy of the model.\nExperimental results on our newly-collected large dataset consisting of 224127\npornographic audios and 274206 normal samples demonstrate the effectiveness of\nour proposed model and these refinements. Specifically, the proposed model\nachieves an accuracy of 92.46% and the accuracy is further improved to 97.19%\nwhen all refinements are combined.", "published": "2022-11-11 03:21:32", "link": "http://arxiv.org/abs/2211.05983v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MaskedSpeech: Context-aware Speech Synthesis with Masking Strategy", "abstract": "Humans often speak in a continuous manner which leads to coherent and\nconsistent prosody properties across neighboring utterances. However, most\nstate-of-the-art speech synthesis systems only consider the information within\neach sentence and ignore the contextual semantic and acoustic features. This\nmakes it inadequate to generate high-quality paragraph-level speech which\nrequires high expressiveness and naturalness. To synthesize natural and\nexpressive speech for a paragraph, a context-aware speech synthesis system\nnamed MaskedSpeech is proposed in this paper, which considers both contextual\nsemantic and acoustic features. Inspired by the masking strategy in the speech\nediting research, the acoustic features of the current sentence are masked out\nand concatenated with those of contextual speech, and further used as\nadditional model input. The phoneme encoder takes the concatenated phoneme\nsequence from neighboring sentences as input and learns fine-grained semantic\ninformation from contextual text. Furthermore, cross-utterance coarse-grained\nsemantic features are employed to improve the prosody generation. The model is\ntrained to reconstruct the masked acoustic features with the augmentation of\nboth the contextual semantic and acoustic features. Experimental results\ndemonstrate that the proposed MaskedSpeech outperformed the baseline system\nsignificantly in terms of naturalness and expressiveness.", "published": "2022-11-11 12:48:27", "link": "http://arxiv.org/abs/2211.06170v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optimal Condition Training for Target Source Separation", "abstract": "Recent research has shown remarkable performance in leveraging multiple\nextraneous conditional and non-mutually exclusive semantic concepts for sound\nsource separation, allowing the flexibility to extract a given target source\nbased on multiple different queries. In this work, we propose a new optimal\ncondition training (OCT) method for single-channel target source separation,\nbased on greedy parameter updates using the highest performing condition among\nequivalent conditions associated with a given target source. Our experiments\nshow that the complementary information carried by the diverse semantic\nconcepts significantly helps to disentangle and isolate sources of interest\nmuch more efficiently compared to single-conditioned models. Moreover, we\npropose a variation of OCT with condition refinement, in which an initial\nconditional vector is adapted to the given mixture and transformed to a more\namenable representation for target source extraction. We showcase the\neffectiveness of OCT on diverse source separation experiments where it improves\nupon permutation invariant models with oracle assignment and obtains\nstate-of-the-art performance in the more challenging task of text-based source\nseparation, outperforming even dedicated text-only conditioned models.", "published": "2022-11-11 00:04:55", "link": "http://arxiv.org/abs/2211.05927v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continuous Soft Pseudo-Labeling in ASR", "abstract": "Continuous pseudo-labeling (PL) algorithms such as slimIPL have recently\nemerged as a powerful strategy for semi-supervised learning in speech\nrecognition. In contrast with earlier strategies that alternated between\ntraining a model and generating pseudo-labels (PLs) with it, here PLs are\ngenerated in end-to-end manner as training proceeds, improving training speed\nand the accuracy of the final model. PL shares a common theme with\nteacher-student models such as distillation in that a teacher model generates\ntargets that need to be mimicked by the student model being trained. However,\ninterestingly, PL strategies in general use hard-labels, whereas distillation\nuses the distribution over labels as the target to mimic. Inspired by\ndistillation we expect that specifying the whole distribution (aka soft-labels)\nover sequences as the target for unlabeled data, instead of a single best pass\npseudo-labeled transcript (hard-labels) should improve PL performance and\nconvergence. Surprisingly and unexpectedly, we find that soft-labels targets\ncan lead to training divergence, with the model collapsing to a degenerate\ntoken distribution per frame. We hypothesize that the reason this does not\nhappen with hard-labels is that training loss on hard-labels imposes\nsequence-level consistency that keeps the model from collapsing to the\ndegenerate solution. In this paper, we show several experiments that support\nthis hypothesis, and experiment with several regularization approaches that can\nameliorate the degenerate collapse when using soft-labels. These approaches can\nbring the accuracy of soft-labels closer to that of hard-labels, and while they\nare unable to outperform them yet, they serve as a useful framework for further\nimprovements.", "published": "2022-11-11 05:16:18", "link": "http://arxiv.org/abs/2211.06007v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Semi-supervised learning for continuous emotional intensity controllable\n  speech synthesis with disentangled representations", "abstract": "Recent text-to-speech models have reached the level of generating natural\nspeech similar to what humans say. But there still have limitations in terms of\nexpressiveness. The existing emotional speech synthesis models have shown\ncontrollability using interpolated features with scaling parameters in\nemotional latent space. However, the emotional latent space generated from the\nexisting models is difficult to control the continuous emotional intensity\nbecause of the entanglement of features like emotions, speakers, etc. In this\npaper, we propose a novel method to control the continuous intensity of\nemotions using semi-supervised learning. The model learns emotions of\nintermediate intensity using pseudo-labels generated from phoneme-level\nsequences of speech information. An embedding space built from the proposed\nmodel satisfies the uniform grid geometry with an emotional basis. The\nexperimental results showed that the proposed method was superior in\ncontrollability and naturalness.", "published": "2022-11-11 12:28:07", "link": "http://arxiv.org/abs/2211.06160v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Sequence-to-Sequence Transformer-Transducer Models for Keyword\n  Spotting", "abstract": "In this paper, we present a novel approach to adapt a sequence-to-sequence\nTransformer-Transducer ASR system to the keyword spotting (KWS) task. We\nachieve this by replacing the keyword in the text transcription with a special\ntoken <kw> and training the system to detect the <kw> token in an audio stream.\nAt inference time, we create a decision function inspired by conventional KWS\napproaches, to make our approach more suitable for the KWS task. Furthermore,\nwe introduce a specific keyword spotting loss by adapting the\nsequence-discriminative Minimum Bayes-Risk training technique. We find that our\napproach significantly outperforms ASR based KWS systems. When compared with a\nconventional keyword spotting system, our proposal has similar performance\nwhile bringing the advantages and flexibility of sequence-to-sequence training.\nAdditionally, when combined with the conventional KWS system, our approach can\nimprove the performance at any operation point.", "published": "2022-11-11 20:41:46", "link": "http://arxiv.org/abs/2211.06478v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Augmenting Transformer-Transducer Based Speaker Change Detection With\n  Token-Level Training Loss", "abstract": "In this work we propose a novel token-based training strategy that improves\nTransformer-Transducer (T-T) based speaker change detection (SCD) performance.\nThe conventional T-T based SCD model loss optimizes all output tokens equally.\nDue to the sparsity of the speaker changes in the training data, the\nconventional T-T based SCD model loss leads to sub-optimal detection accuracy.\nTo mitigate this issue, we use a customized edit-distance algorithm to estimate\nthe token-level SCD false accept (FA) and false reject (FR) rates during\ntraining and optimize model parameters to minimize a weighted combination of\nthe FA and FR, focusing the model on accurately predicting speaker changes. We\nalso propose a set of evaluation metrics that align better with commercial use\ncases. Experiments on a group of challenging real-world datasets show that the\nproposed training method can significantly improve the overall performance of\nthe SCD model with the same number of parameters.", "published": "2022-11-11 21:09:58", "link": "http://arxiv.org/abs/2211.06482v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Handling Trade-Offs in Speech Separation with Sparsely-Gated Mixture of\n  Experts", "abstract": "Employing a monaural speech separation (SS) model as a front-end for\nautomatic speech recognition (ASR) involves balancing two kinds of trade-offs.\nFirst, while a larger model improves the SS performance, it also requires a\nhigher computational cost. Second, an SS model that is more optimized for\nhandling overlapped speech is likely to introduce more processing artifacts in\nnon-overlapped-speech regions. In this paper, we address these trade-offs with\na sparsely-gated mixture-of-experts (MoE) architecture. Comprehensive\nevaluation results obtained using both simulated and real meeting recordings\nshow that our proposed sparsely-gated MoE SS model achieves superior separation\ncapabilities with less speech distortion, while involving only a marginal\nrun-time cost increase.", "published": "2022-11-11 22:07:43", "link": "http://arxiv.org/abs/2211.06493v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "On the robustness of non-intrusive speech quality model by adversarial\n  examples", "abstract": "It has been shown recently that deep learning based models are effective on\nspeech quality prediction and could outperform traditional metrics in various\nperspectives. Although network models have potential to be a surrogate for\ncomplex human hearing perception, they may contain instabilities in\npredictions. This work shows that deep speech quality predictors can be\nvulnerable to adversarial perturbations, where the prediction can be changed\ndrastically by unnoticeable perturbations as small as $-30$ dB compared with\nspeech inputs. In addition to exposing the vulnerability of deep speech quality\npredictors, we further explore and confirm the viability of adversarial\ntraining for strengthening robustness of models.", "published": "2022-11-11 23:06:24", "link": "http://arxiv.org/abs/2211.06508v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
