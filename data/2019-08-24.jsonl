{"title": "Multi-view Story Characterization from Movie Plot Synopses and Reviews", "abstract": "This paper considers the problem of characterizing stories by inferring\nproperties such as theme and style using written synopses and reviews of\nmovies. We experiment with a multi-label dataset of movie synopses and a tagset\nrepresenting various attributes of stories (e.g., genre, type of events). Our\nproposed multi-view model encodes the synopses and reviews using hierarchical\nattention and shows improvement over methods that only use synopses. Finally,\nwe demonstrate how can we take advantage of such a model to extract a\ncomplementary set of story-attributes from reviews without direct supervision.\nWe have made our dataset and source code publicly available at\nhttps://ritual.uh.edu/ multiview-tag-2020.", "published": "2019-08-24 03:27:43", "link": "http://arxiv.org/abs/1908.09083v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT for Coreference Resolution: Baselines and Analysis", "abstract": "We apply BERT to coreference resolution, achieving strong improvements on the\nOntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of\nmodel predictions indicates that, compared to ELMo and BERT-base, BERT-large is\nparticularly better at distinguishing between related but distinct entities\n(e.g., President and CEO). However, there is still room for improvement in\nmodeling document-level context, conversations, and mention paraphrasing. Our\ncode and models are publicly available.", "published": "2019-08-24 05:07:36", "link": "http://arxiv.org/abs/1908.09091v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Invariant Feature Distillation for Cross-Domain Sentiment\n  Classification", "abstract": "Cross-domain sentiment classification has drawn much attention in recent\nyears. Most existing approaches focus on learning domain-invariant\nrepresentations in both the source and target domains, while few of them pay\nattention to the domain-specific information. Despite the non-transferability\nof the domain-specific information, simultaneously learning domain-dependent\nrepresentations can facilitate the learning of domain-invariant\nrepresentations. In this paper, we focus on aspect-level cross-domain sentiment\nclassification, and propose to distill the domain-invariant sentiment features\nwith the help of an orthogonal domain-dependent task, i.e. aspect detection,\nwhich is built on the aspects varying widely in different domains. We conduct\nextensive experiments on three public datasets and the experimental results\ndemonstrate the effectiveness of our method.", "published": "2019-08-24 10:50:23", "link": "http://arxiv.org/abs/1908.09122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-Based Named Entity Recognition", "abstract": "In this paper, we propose a new strategy for the task of named entity\nrecognition (NER). We cast the task as a query-based machine reading\ncomprehension task: e.g., the task of extracting entities with PER is\nformalized as answering the question of \"which person is mentioned in the text\n?\". Such a strategy comes with the advantage that it solves the long-standing\nissue of handling overlapping or nested entities (the same token that\nparticipates in more than one entity categories) with sequence-labeling\ntechniques for NER. Additionally, since the query encodes informative prior\nknowledge, this strategy facilitates the process of entity extraction, leading\nto better performances. We experiment the proposed model on five widely used\nNER datasets on English and Chinese, including MSRA, Resume, OntoNotes, ACE04\nand ACE05. The proposed model sets new SOTA results on all of these datasets.", "published": "2019-08-24 13:42:57", "link": "http://arxiv.org/abs/1908.09138v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Domain Adaptation for Machine Reading Comprehension", "abstract": "In this paper, we focus on unsupervised domain adaptation for Machine Reading\nComprehension (MRC), where the source domain has a large amount of labeled\ndata, while only unlabeled passages are available in the target domain. To this\nend, we propose an Adversarial Domain Adaptation framework (AdaMRC), where\n($i$) pseudo questions are first generated for unlabeled passages in the target\ndomain, and then ($ii$) a domain classifier is incorporated into an MRC model\nto predict which domain a given passage-question pair comes from. The\nclassifier and the passage-question encoder are jointly trained using\nadversarial learning to enforce domain-invariant representation learning.\nComprehensive evaluations demonstrate that our approach ($i$) is generalizable\nto different MRC models and datasets, ($ii$) can be combined with pre-trained\nlarge-scale language models (such as ELMo and BERT), and ($iii$) can be\nextended to semi-supervised learning.", "published": "2019-08-24 21:08:26", "link": "http://arxiv.org/abs/1908.09209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DAST Model: Deciding About Semantic Complexity of a Text", "abstract": "Measuring text complexity is an essential task in several fields and\napplications (such as NLP, semantic web, smart education, etc.). The semantic\nlayer of text is more tacit than its syntactic structure and, as a result,\ncalculation of semantic complexity is more difficult than syntactic complexity.\nWhile there are famous and powerful academic and commercial syntactic\ncomplexity measures, the problem of measuring semantic complexity is still a\nchallenging one. In this paper, we introduce the DAST model, which stands for\nDeciding About Semantic Complexity of a Text. DAST proposes an intuitionistic\napproach to semantics that lets us have a well-defined model for the semantics\nof a text and its complexity: semantic is considered as a lattice of intuitions\nand, as a result, semantic complexity is defined as the result of a calculation\non this lattice. A set theoretic formal definition of semantic complexity, as a\n6-tuple formal system, is provided. By using this formal system, a method for\nmeasuring semantic complexity is presented. The evaluation of the proposed\napproach is done by a set of three human-judgment experiments. The results show\nthat DAST model is capable of deciding about semantic complexity of text.\nFurthermore, the analysis of the results leads us to introduce a Markovian\nmodel for the process of common-sense, multiple-steps and semantic-complexity\nreasoning in people. The results of Experiments demonstrate that our method\noutperforms the random baseline with improvement in better precision and\ncompetes with other methods by less error percentage.", "published": "2019-08-24 03:10:38", "link": "http://arxiv.org/abs/1908.09080v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Text Summarization of Legal Cases: A Hybrid Approach", "abstract": "Manual Summarization of large bodies of text involves a lot of human effort\nand time, especially in the legal domain. Lawyers spend a lot of time preparing\nlegal briefs of their clients' case files. Automatic Text summarization is a\nconstantly evolving field of Natural Language Processing(NLP), which is a\nsubdiscipline of the Artificial Intelligence Field. In this paper a hybrid\nmethod for automatic text summarization of legal cases using k-means clustering\ntechnique and tf-idf(term frequency-inverse document frequency) word vectorizer\nis proposed. The summary generated by the proposed method is compared using\nROGUE evaluation parameters with the case summary as prepared by the lawyer for\nappeal in court. Further, suggestions for improving the proposed method are\nalso presented.", "published": "2019-08-24 10:05:40", "link": "http://arxiv.org/abs/1908.09119v1", "categories": ["cs.CL", "cs.IR", "68T50"], "primary_category": "cs.CL"}
{"title": "Position-Aware Self-Attention based Neural Sequence Labeling", "abstract": "Sequence labeling is a fundamental task in natural language processing and\nhas been widely studied. Recently, RNN-based sequence labeling models have\nincreasingly gained attentions. Despite superior performance achieved by\nlearning the long short-term (i.e., successive) dependencies, the way of\nsequentially processing inputs might limit the ability to capture the\nnon-continuous relations over tokens within a sentence. To tackle the problem,\nwe focus on how to effectively model successive and discrete dependencies of\neach token for enhancing the sequence labeling performance. Specifically, we\npropose an innovative attention-based model (called position-aware\nselfattention, i.e., PSA) as well as a well-designed self-attentional context\nfusion layer within a neural network architecture, to explore the positional\ninformation of an input sequence for capturing the latent relations among\ntokens. Extensive experiments on three classical tasks in sequence labeling\ndomain, i.e., partof-speech (POS) tagging, named entity recognition (NER) and\nphrase chunking, demonstrate our proposed model outperforms the\nstate-of-the-arts without any external knowledge, in terms of various metrics.", "published": "2019-08-24 11:40:08", "link": "http://arxiv.org/abs/1908.09128v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A framework for anomaly detection using language modeling, and its\n  applications to finance", "abstract": "In the finance sector, studies focused on anomaly detection are often\nassociated with time-series and transactional data analytics. In this paper, we\nlay out the opportunities for applying anomaly and deviation detection methods\nto text corpora and challenges associated with them. We argue that language\nmodels that use distributional semantics can play a significant role in\nadvancing these studies in novel directions, with new applications in risk\nidentification, predictive modeling, and trend analysis.", "published": "2019-08-24 15:52:57", "link": "http://arxiv.org/abs/1908.09156v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Propagate-Selector: Detecting Supporting Sentences for Question\n  Answering via Graph Neural Networks", "abstract": "In this study, we propose a novel graph neural network called\npropagate-selector (PS), which propagates information over sentences to\nunderstand information that cannot be inferred when considering sentences in\nisolation. First, we design a graph structure in which each node represents an\nindividual sentence, and some pairs of nodes are selectively connected based on\nthe text structure. Then, we develop an iterative attentive aggregation and a\nskip-combine method in which a node interacts with its neighborhood nodes to\naccumulate the necessary information. To evaluate the performance of the\nproposed approaches, we conduct experiments with the standard HotpotQA dataset.\nThe empirical results demonstrate the superiority of our proposed approach,\nwhich obtains the best performances, compared to the widely used\nanswer-selection models that do not consider the intersentential relationship.", "published": "2019-08-24 13:37:35", "link": "http://arxiv.org/abs/1908.09137v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Representation Learning with Autoencoders for Electronic Health Records:\n  A Comparative Study", "abstract": "Increasing volume of Electronic Health Records (EHR) in recent years provides\ngreat opportunities for data scientists to collaborate on different aspects of\nhealthcare research by applying advanced analytics to these EHR clinical data.\nA key requirement however is obtaining meaningful insights from high\ndimensional, sparse and complex clinical data. Data science approaches\ntypically address this challenge by performing feature learning in order to\nbuild more reliable and informative feature representations from clinical data\nfollowed by supervised learning. In this paper, we propose a predictive\nmodeling approach based on deep learning based feature representations and word\nembedding techniques. Our method uses different deep architectures (stacked\nsparse autoencoders, deep belief network, adversarial autoencoders and\nvariational autoencoders) for feature representation in higher-level\nabstraction to obtain effective and robust features from EHRs, and then build\nprediction models on top of them. Our approach is particularly useful when the\nunlabeled data is abundant whereas labeled data is scarce. We investigate the\nperformance of representation learning through a supervised learning approach.\nOur focus is to present a comparative study to evaluate the performance of\ndifferent deep architectures through supervised learning and provide insights\nin the choice of deep feature representation techniques. Our experiments\ndemonstrate that for small data sets, stacked sparse autoencoder demonstrates a\nsuperior generality performance in prediction due to sparsity regularization\nwhereas variational autoencoders outperform the competing approaches for large\ndata sets due to its capability of learning the representation distribution", "published": "2019-08-24 17:38:30", "link": "http://arxiv.org/abs/1908.09174v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Release Strategies and the Social Impacts of Language Models", "abstract": "Large language models have a range of beneficial uses: they can assist in\nprose, poetry, and programming; analyze dataset biases; and more. However,\ntheir flexibility and generative capabilities also raise misuse concerns. This\nreport discusses OpenAI's work related to the release of its GPT-2 language\nmodel. It discusses staged release, which allows time between model releases to\nconduct risk and benefit analyses as model sizes increased. It also discusses\nongoing partnership-based research and provides recommendations for better\ncoordination and responsible publication in AI.", "published": "2019-08-24 20:41:40", "link": "http://arxiv.org/abs/1908.09203v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2; I.2.7; K.4"], "primary_category": "cs.CL"}
