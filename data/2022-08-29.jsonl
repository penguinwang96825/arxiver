{"title": "Reweighting Strategy based on Synthetic Data Identification for Sentence\n  Similarity", "abstract": "Semantically meaningful sentence embeddings are important for numerous tasks\nin natural language processing. To obtain such embeddings, recent studies\nexplored the idea of utilizing synthetically generated data from pretrained\nlanguage models (PLMs) as a training corpus. However, PLMs often generate\nsentences much different from the ones written by human. We hypothesize that\ntreating all these synthetic examples equally for training deep neural networks\ncan have an adverse effect on learning semantically meaningful embeddings. To\nanalyze this, we first train a classifier that identifies machine-written\nsentences, and observe that the linguistic features of the sentences identified\nas written by a machine are significantly different from those of human-written\nsentences. Based on this, we propose a novel approach that first trains the\nclassifier to measure the importance of each sentence. The distilled\ninformation from the classifier is then used to train a reliable sentence\nembedding model. Through extensive evaluation on four real-world datasets, we\ndemonstrate that our model trained on synthetic data generalizes well and\noutperforms the existing baselines. Our implementation is publicly available at\nhttps://github.com/ddehun/coling2022_reweighting_sts.", "published": "2022-08-29 05:42:22", "link": "http://arxiv.org/abs/2208.13376v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse\n  Representations and Content Enhancing", "abstract": "Non-parallel text style transfer is an important task in natural language\ngeneration. However, previous studies concentrate on the token or sentence\nlevel, such as sentence sentiment and formality transfer, but neglect long\nstyle transfer at the discourse level. Long texts usually involve more\ncomplicated author linguistic preferences such as discourse structures than\nsentences. In this paper, we formulate the task of non-parallel story\nauthor-style transfer, which requires transferring an input story into a\nspecified author style while maintaining source semantics. To tackle this\nproblem, we propose a generation model, named StoryTrans, which leverages\ndiscourse representations to capture source content information and transfer\nthem to target styles with learnable style embeddings. We use an additional\ntraining objective to disentangle stylistic features from the learned discourse\nrepresentation to prevent the model from degenerating to an auto-encoder.\nMoreover, to enhance content preservation, we design a mask-and-fill framework\nto explicitly fuse style-specific keywords of source texts into generation.\nFurthermore, we constructed new datasets for this task in Chinese and English,\nrespectively. Extensive experiments show that our model outperforms strong\nbaselines in overall performance of style transfer and content preservation.", "published": "2022-08-29 08:47:49", "link": "http://arxiv.org/abs/2208.13423v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supporting Medical Relation Extraction via Causality-Pruned Semantic\n  Dependency Forest", "abstract": "Medical Relation Extraction (MRE) task aims to extract relations between\nentities in medical texts. Traditional relation extraction methods achieve\nimpressive success by exploring the syntactic information, e.g., dependency\ntree. However, the quality of the 1-best dependency tree for medical texts\nproduced by an out-of-domain parser is relatively limited so that the\nperformance of medical relation extraction method may degenerate. To this end,\nwe propose a method to jointly model semantic and syntactic information from\nmedical texts based on causal explanation theory. We generate dependency\nforests consisting of the semantic-embedded 1-best dependency tree. Then, a\ntask-specific causal explainer is adopted to prune the dependency forests,\nwhich are further fed into a designed graph convolutional network to learn the\ncorresponding representation for downstream task. Empirically, the various\ncomparisons on benchmark medical datasets demonstrate the effectiveness of our\nmodel.", "published": "2022-08-29 10:17:47", "link": "http://arxiv.org/abs/2208.13472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "naab: A ready-to-use plug-and-play corpus for Farsi", "abstract": "The rise of large language models (LLMs) has transformed numerous natural\nlanguage processing (NLP) tasks, yet their performance in low and mid-resource\nlanguages, such as Farsi, still lags behind resource-rich languages like\nEnglish. To address this gap, we introduce naab, the largest publicly\navailable, cleaned, and ready-to-use Farsi textual corpus. naab consists of\n130GB of data, comprising over 250 million paragraphs and 15 billion words.\nNamed after the Farsi word NAAB (meaning \"pure\" or \"high-grade\"), this corpus\nis openly accessible via Hugging Face, offering researchers a valuable resource\nfor Farsi NLP tasks. In addition to naab, we provide naab-raw, an unprocessed\nversion of the dataset, along with a pre-processing toolkit that allows users\nto clean their custom corpora. These resources empower NLP researchers and\npractitioners, particularly those focusing on low-resource languages, to\nimprove the performance of LLMs in their respective domains and bridge the gap\nbetween resource-rich and resource-poor languages.", "published": "2022-08-29 10:40:58", "link": "http://arxiv.org/abs/2208.13486v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning a General Clause-to-Clause Relationships for Enhancing\n  Emotion-Cause Pair Extraction", "abstract": "Emotion-cause pair extraction (ECPE) is an emerging task aiming to extract\npotential pairs of emotions and corresponding causes from documents. Previous\napproaches have focused on modeling the pair-to-pair relationship and achieved\npromising results. However, the clause-to-clause relationship, which\nfundamentally symbolizes the underlying structure of a document, has still been\nin its research infancy. In this paper, we define a novel clause-to-clause\nrelationship. To learn it applicably, we propose a general clause-level\nencoding model named EA-GAT comprising E-GAT and Activation Sort. E-GAT is\ndesigned to aggregate information from different types of clauses; Activation\nSort leverages the individual emotion/cause prediction and the sort-based\nmapping to propel the clause to a more favorable representation. Since EA-GAT\nis a clause-level encoding model, it can be broadly integrated with any\nprevious approach. Experimental results show that our approach has a\nsignificant advantage over all current approaches on the Chinese and English\nbenchmark corpus, with an average of $2.1\\%$ and $1.03\\%$.", "published": "2022-08-29 12:39:39", "link": "http://arxiv.org/abs/2208.13549v2", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future\n  Directions", "abstract": "Text-to-SQL parsing is an essential and challenging task. The goal of\ntext-to-SQL parsing is to convert a natural language (NL) question to its\ncorresponding structured query language (SQL) based on the evidences provided\nby relational databases. Early text-to-SQL parsing systems from the database\ncommunity achieved a noticeable progress with the cost of heavy human\nengineering and user interactions with the systems. In recent years, deep\nneural networks have significantly advanced this task by neural generation\nmodels, which automatically learn a mapping function from an input NL question\nto an output SQL query. Subsequently, the large pre-trained language models\nhave taken the state-of-the-art of the text-to-SQL parsing task to a new level.\nIn this survey, we present a comprehensive review on deep learning approaches\nfor text-to-SQL parsing. First, we introduce the text-to-SQL parsing corpora\nwhich can be categorized as single-turn and multi-turn. Second, we provide a\nsystematical overview of pre-trained language models and existing methods for\ntext-to-SQL parsing. Third, we present readers with the challenges faced by\ntext-to-SQL parsing and explore some potential future directions in this field.", "published": "2022-08-29 14:24:13", "link": "http://arxiv.org/abs/2208.13629v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval", "abstract": "Retrieval models based on dense representations in semantic space have become\nan indispensable branch for first-stage retrieval. These retrievers benefit\nfrom surging advances in representation learning towards compressive global\nsequence-level embeddings. However, they are prone to overlook local salient\nphrases and entity mentions in texts, which usually play pivot roles in\nfirst-stage retrieval. To mitigate this weakness, we propose to make a dense\nretriever align a well-performing lexicon-aware representation model. The\nalignment is achieved by weakened knowledge distillations to enlighten the\nretriever via two aspects -- 1) a lexicon-augmented contrastive objective to\nchallenge the dense encoder and 2) a pair-wise rank-consistent regularization\nto make dense model's behavior incline to the other. We evaluate our model on\nthree public benchmarks, which shows that with a comparable lexicon-aware\nretriever as the teacher, our proposed dense one can bring consistent and\nsignificant improvements, and even outdo its teacher. In addition, we found our\nimprovement on the dense retriever is complementary to the standard ranker\ndistillation, which can further lift state-of-the-art performance.", "published": "2022-08-29 15:09:28", "link": "http://arxiv.org/abs/2208.13661v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Mathematical Concepts from Text", "abstract": "We investigate different systems for extracting mathematical entities from\nEnglish texts in the mathematical field of category theory as a first step for\nconstructing a mathematical knowledge graph. We consider four different term\nextractors and compare their results. This small experiment showcases some of\nthe issues with the construction and evaluation of terms extracted from noisy\ndomain text. We also make available two open corpora in research mathematics,\nin particular in category theory: a small corpus of 755 abstracts from the\njournal TAC (3188 sentences), and a larger corpus from the nLab community wiki\n(15,000 sentences).", "published": "2022-08-29 18:45:05", "link": "http://arxiv.org/abs/2208.13830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evolving Label Usage within Generation Z when Self-Describing Sexual\n  Orientation", "abstract": "Evaluating change in ranked term importance in a growing corpus is a powerful\ntool for understanding changes in vocabulary usage. In this paper, we analyze a\ncorpus of free-response answers where 33,993 LGBTQ Generation Z respondents\nfrom age 13 to 24 in the United States are asked to self-describe their sexual\norientation. We observe that certain labels, such as bisexual, pansexual, and\nlesbian, remain equally important across age groups. The importance of other\nlabels, such as homosexual, demisexual, and omnisexual, evolve across age\ngroups. Although Generation Z is often stereotyped as homogenous, we observe\nnoticeably different label usage when self-describing sexual orientation within\nit. We urge that interested parties must routinely survey the most important\nsexual orientation labels to their target audience and refresh their materials\n(such as demographic surveys) to reflect the constantly evolving LGBTQ\ncommunity and create an inclusive environment.", "published": "2022-08-29 18:52:58", "link": "http://arxiv.org/abs/2208.13833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Impact of Sentiments of Scientific Articles on COVID-19\n  Vaccination Rates", "abstract": "At the peak of the COVID-19 pandemic, numerous countries worldwide sought to\nmobilize vaccination campaigns in an attempt to curb the spread and number of\ndeaths caused by the virus. One avenue in which information regarding COVID\nvaccinations is propagated is that of scientific articles, which provide a\ncertain level of credibility regarding this. Hence, this increases the\nprobability that people who view these articles would get vaccinated if the\narticles convey a positive message on vaccinations and conversely decreases the\nprobability of vaccinations if the articles convey a negative message. This\nbeing said, this study aims to investigate the correlation between article\nsentiments and the corresponding increase or decrease in vaccinations in the\nUnited States. To do this, a lexicon-based sentiment analysis was performed in\ntwo steps: first, article content was scraped via a Python library called\nBeautifulSoup, and second, VADER was used to obtain the sentiment analysis\nscores for each article based on the scraped text content. Results suggest that\nthere was a relatively weak correlation between the average sentiment score of\narticles and the corresponding increase or decrease in COVID vaccination rates\nin the US.", "published": "2022-08-29 05:11:23", "link": "http://arxiv.org/abs/2209.08154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NL2GDPR: Automatically Develop GDPR Compliant Android Application\n  Features from Natural Language", "abstract": "The recent privacy leakage incidences and the more strict policy regulations\ndemand a much higher standard of compliance for companies and mobile apps.\nHowever, such obligations also impose significant challenges on app developers\nfor complying with these regulations that contain various perspectives,\nactivities, and roles, especially for small companies and developers who are\nless experienced in this matter or with limited resources. To address these\nhurdles, we develop an automatic tool, NL2GDPR, which can generate policies\nfrom natural language descriptions from the developer while also ensuring the\napp's functionalities are compliant with General Data Protection Regulation\n(GDPR). NL2GDPR is developed by leveraging an information extraction tool, OIA\n(Open Information Annotation), developed by Baidu Cognitive Computing Lab.\n  At the core, NL2GDPR is a privacy-centric information extraction model,\nappended with a GDPR policy finder and a policy generator. We perform a\ncomprehensive study to grasp the challenges in extracting privacy-centric\ninformation and generating privacy policies, while exploiting optimizations for\nthis specific task. With NL2GDPR, we can achieve 92.9%, 95.2%, and 98.4%\naccuracy in correctly identifying GDPR policies related to personal data\nstorage, process, and share types, respectively. To the best of our knowledge,\nNL2GDPR is the first tool that allows a developer to automatically generate\nGDPR compliant policies, with only the need of entering the natural language\nfor describing the app features. Note that other non-GDPR-related features\nmight be integrated with the generated features to build a complex app.", "published": "2022-08-29 04:16:50", "link": "http://arxiv.org/abs/2208.13361v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Demystifying the COVID-19 vaccine discourse on Twitter", "abstract": "Developing an understanding of the public discourse on COVID-19 vaccination\non social media is important not only for addressing the current COVID-19\npandemic, but also for future pathogen outbreaks. We examine a Twitter dataset\ncontaining 75 million English tweets discussing COVID-19 vaccination from March\n2020 to March 2021. We train a stance detection algorithm using natural\nlanguage processing (NLP) techniques to classify tweets as `anti-vax' or\n`pro-vax', and examine the main topics of discourse using topic modelling\ntechniques. While pro-vax tweets (37 million) far outnumbered anti-vax tweets\n(10 million), a majority of tweets from both stances (63% anti-vax and 53%\npro-vax tweets) came from dual-stance users who posted both pro- and anti-vax\ntweets during the observation period. Pro-vax tweets focused mostly on vaccine\ndevelopment, while anti-vax tweets covered a wide range of topics, some of\nwhich included genuine concerns, though there was a large dose of falsehoods. A\nnumber of topics were common to both stances, though pro- and anti-vax tweets\ndiscussed them from opposite viewpoints. Memes and jokes were amongst the most\nretweeted messages. Whereas concerns about polarisation and online prevalence\nof anti-vax discourse are unfounded, targeted countering of falsehoods is\nimportant.", "published": "2022-08-29 11:56:21", "link": "http://arxiv.org/abs/2208.13523v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Combating high variance in Data-Scarce Implicit Hate Speech\n  Classification", "abstract": "Hate speech classification has been a long-standing problem in natural\nlanguage processing. However, even though there are numerous hate speech\ndetection methods, they usually overlook a lot of hateful statements due to\nthem being implicit in nature. Developing datasets to aid in the task of\nimplicit hate speech classification comes with its own challenges; difficulties\nare nuances in language, varying definitions of what constitutes hate speech,\nand the labor-intensive process of annotating such data. This had led to a\nscarcity of data available to train and test such systems, which gives rise to\nhigh variance problems when parameter-heavy transformer-based models are used\nto address the problem. In this paper, we explore various optimization and\nregularization techniques and develop a novel RoBERTa-based model that achieves\nstate-of-the-art performance.", "published": "2022-08-29 13:45:21", "link": "http://arxiv.org/abs/2208.13595v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Debiasing Word Embeddings with Nonlinear Geometry", "abstract": "Debiasing word embeddings has been largely limited to individual and\nindependent social categories. However, real-world corpora typically present\nmultiple social categories that possibly correlate or intersect with each\nother. For instance, \"hair weaves\" is stereotypically associated with African\nAmerican females, but neither African American nor females alone. Therefore,\nthis work studies biases associated with multiple social categories: joint\nbiases induced by the union of different categories and intersectional biases\nthat do not overlap with the biases of the constituent categories. We first\nempirically observe that individual biases intersect non-trivially (i.e., over\na one-dimensional subspace). Drawing from the intersectional theory in social\nscience and the linguistic theory, we then construct an intersectional subspace\nto debias for multiple social categories using the nonlinear geometry of\nindividual biases. Empirical evaluations corroborate the efficacy of our\napproach. Data and implementation code can be downloaded at\nhttps://github.com/GitHubLuCheng/Implementation-of-JoSEC-COLING-22.", "published": "2022-08-29 21:40:27", "link": "http://arxiv.org/abs/2208.13899v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personal Attribute Prediction from Conversations", "abstract": "Personal knowledge bases (PKBs) are critical to many applications, such as\nWeb-based chatbots and personalized recommendation. Conversations containing\nrich personal knowledge can be regarded as a main source to populate the PKB.\nGiven a user, a user attribute, and user utterances from a conversational\nsystem, we aim to predict the personal attribute value for the user, which is\nhelpful for the enrichment of PKBs. However, there are three issues existing in\nprevious studies: (1) manually labeled utterances are required for model\ntraining; (2) personal attribute knowledge embedded in both utterances and\nexternal resources is underutilized; (3) the performance on predicting some\ndifficult personal attributes is unsatisfactory. In this paper, we propose a\nframework DSCGN based on the pre-trained language model with a noise-robust\nloss function to predict personal attributes from conversations without\nrequiring any labeled utterances. We yield two categories of supervision, i.e.,\ndocument-level supervision via a distant supervision strategy and\ncontextualized word-level supervision via a label guessing method, by mining\nthe personal attribute knowledge embedded in both unlabeled utterances and\nexternal resources to fine-tune the language model. Extensive experiments over\ntwo real-world data sets (i.e., a profession data set and a hobby data set)\nshow our framework obtains the best performance compared with all the twelve\nbaselines in terms of nDCG and MRR.", "published": "2022-08-29 15:21:53", "link": "http://arxiv.org/abs/2209.09619v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Turn-Taking Prediction for Natural Conversational Speech", "abstract": "While a streaming voice assistant system has been used in many applications,\nthis system typically focuses on unnatural, one-shot interactions assuming\ninput from a single voice query without hesitation or disfluency. However, a\ncommon conversational utterance often involves multiple queries with\nturn-taking, in addition to disfluencies. These disfluencies include pausing to\nthink, hesitations, word lengthening, filled pauses and repeated phrases. This\nmakes doing speech recognition with conversational speech, including one with\nmultiple queries, a challenging task. To better model the conversational\ninteraction, it is critical to discriminate disfluencies and end of query in\norder to allow the user to hold the floor for disfluencies while having the\nsystem respond as quickly as possible when the user has finished speaking. In\nthis paper, we present a turntaking predictor built on top of the end-to-end\n(E2E) speech recognizer. Our best system is obtained by jointly optimizing for\nASR task and detecting when the user is paused to think or finished speaking.\nThe proposed approach demonstrates over 97% recall rate and 85% precision rate\non predicting true turn-taking with only 100 ms latency on a test set designed\nwith 4 types of disfluencies inserted in conversational utterances.", "published": "2022-08-29 01:09:23", "link": "http://arxiv.org/abs/2208.13321v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Streaming Intended Query Detection using E2E Modeling for Continued\n  Conversation", "abstract": "In voice-enabled applications, a predetermined hotword isusually used to\nactivate a device in order to attend to the query.However, speaking queries\nfollowed by a hotword each timeintroduces a cognitive burden in continued\nconversations. Toavoid repeating a hotword, we propose a streaming\nend-to-end(E2E) intended query detector that identifies the utterancesdirected\ntowards the device and filters out other utterancesnot directed towards device.\nThe proposed approach incor-porates the intended query detector into the E2E\nmodel thatalready folds different components of the speech recognitionpipeline\ninto one neural network.The E2E modeling onspeech decoding and intended query\ndetection also allows us todeclare a quick intended query detection based on\nearly partialrecognition result, which is important to decrease latencyand make\nthe system responsive. We demonstrate that theproposed E2E approach yields a\n22% relative improvement onequal error rate (EER) for the detection accuracy\nand 600 mslatency improvement compared with an independent intendedquery\ndetector. In our experiment, the proposed model detectswhether the user is\ntalking to the device with a 8.7% EERwithin 1.4 seconds of median latency after\nuser starts speaking.", "published": "2022-08-29 01:15:50", "link": "http://arxiv.org/abs/2208.13322v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Language Agnostic Multilingual Streaming On-Device ASR System", "abstract": "On-device end-to-end (E2E) models have shown improvements over a conventional\nmodel on English Voice Search tasks in both quality and latency. E2E models\nhave also shown promising results for multilingual automatic speech recognition\n(ASR). In this paper, we extend our previous capacity solution to streaming\napplications and present a streaming multilingual E2E ASR system that runs\nfully on device with comparable quality and latency to individual monolingual\nmodels. To achieve that, we propose an Encoder Endpointer model and an\nEnd-of-Utterance (EOU) Joint Layer for a better quality and latency trade-off.\nOur system is built in a language agnostic manner allowing it to natively\nsupport intersentential code switching in real time. To address the feasibility\nconcerns on large models, we conducted on-device profiling and replaced the\ntime consuming LSTM decoder with the recently developed Embedding decoder. With\nthese changes, we managed to run such a system on a mobile device in less than\nreal time.", "published": "2022-08-29 22:34:59", "link": "http://arxiv.org/abs/2208.13916v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring and Evaluating Personalized Models for Code Generation", "abstract": "Large Transformer models achieved the state-of-the-art status for Natural\nLanguage Understanding tasks and are increasingly becoming the baseline model\narchitecture for modeling source code. Transformers are usually pre-trained on\nlarge unsupervised corpora, learning token representations and transformations\nrelevant to modeling generally available text, and are then fine-tuned on a\nparticular downstream task of interest. While fine-tuning is a tried-and-true\nmethod for adapting a model to a new domain -- for example, question-answering\non a given topic -- generalization remains an on-going challenge. In this\npaper, we explore and evaluate transformer model fine-tuning for\npersonalization. In the context of generating unit tests for Java methods, we\nevaluate learning to personalize to a specific software project using several\npersonalization techniques. We consider three key approaches: (i) custom\nfine-tuning, which allows all the model parameters to be tuned; (ii)\nlightweight fine-tuning, which freezes most of the model's parameters, allowing\ntuning of the token embeddings and softmax layer only or the final layer alone;\n(iii) prefix tuning, which keeps model parameters frozen, but optimizes a small\nproject-specific prefix vector. Each of these techniques offers a trade-off in\ntotal compute cost and predictive performance, which we evaluate by code and\ntask-specific metrics, training time, and total computational operations. We\ncompare these fine-tuning strategies for code generation and discuss the\npotential generalization and cost benefits of each in various deployment\nscenarios.", "published": "2022-08-29 23:28:46", "link": "http://arxiv.org/abs/2208.13928v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "On Grounded Planning for Embodied Tasks with Language Models", "abstract": "Language models (LMs) have demonstrated their capability in possessing\ncommonsense knowledge of the physical world, a crucial aspect of performing\ntasks in everyday life. However, it remains unclear **whether LMs have the\ncapacity to generate grounded, executable plans for embodied tasks.** This is a\nchallenging task as LMs lack the ability to perceive the environment through\nvision and feedback from the physical environment. In this paper, we address\nthis important research question and present the first investigation into the\ntopic. Our novel problem formulation, named **G-PlanET**, inputs a high-level\ngoal and a data table about objects in a specific environment, and then outputs\na step-by-step actionable plan for a robotic agent to follow. To facilitate the\nstudy, we establish an **evaluation protocol** and design a dedicated metric to\nassess the quality of the plans. Our experiments demonstrate that the use of\ntables for encoding the environment and an iterative decoding strategy can\nsignificantly enhance the LMs' ability in grounded planning. Our analysis also\nreveals interesting and non-trivial findings.", "published": "2022-08-29 16:37:18", "link": "http://arxiv.org/abs/2209.00465v3", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "LogicRank: Logic Induced Reranking for Generative Text-to-Image Systems", "abstract": "Text-to-image models have recently achieved remarkable success with seemingly\naccurate samples in photo-realistic quality. However as state-of-the-art\nlanguage models still struggle evaluating precise statements consistently, so\ndo language model based image generation processes. In this work we showcase\nproblems of state-of-the-art text-to-image models like DALL-E with generating\naccurate samples from statements related to the draw bench benchmark.\nFurthermore we show that CLIP is not able to rerank those generated samples\nconsistently. To this end we propose LogicRank, a neuro-symbolic reasoning\nframework that can result in a more accurate ranking-system for such\nprecision-demanding settings. LogicRank integrates smoothly into the generation\nprocess of text-to-image models and moreover can be used to further fine-tune\ntowards a more logical precise model.", "published": "2022-08-29 11:40:36", "link": "http://arxiv.org/abs/2208.13518v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LO", "cs.SC"], "primary_category": "cs.AI"}
