{"title": "$C^3$: Confidence Calibration Model Cascade for Inference-Efficient\n  Cross-Lingual Natural Language Understanding", "abstract": "Cross-lingual natural language understanding (NLU) is a critical task in\nnatural language processing (NLP). Recent advancements have seen multilingual\npre-trained language models (mPLMs) significantly enhance the performance of\nthese tasks. However, mPLMs necessitate substantial resources and incur high\ncomputational costs during inference, posing challenges for deployment in\nreal-world and real-time systems. Existing model cascade methods seek to\nenhance inference efficiency by greedily selecting the lightest model capable\nof processing the current input from a variety of models, based on model\nconfidence scores. Nonetheless, deep models tend to exhibit overconfidence, and\nconfidence distributions vary across languages. This leads to the emission of\nconfident but incorrect predictions by smaller models, hindering their ability\nto generalize effectively across test languages. In this study, we introduce a\nconfidence calibration model cascade ($C^3$) method. This approach, simple yet\neffective, involves calibration prior to cascade inference, thereby enhancing\ncascade accuracy through more reliable predictions. Extensive experiments\nconducted on three cross-lingual benchmarks demonstrate that $C^3$\nsignificantly outperforms all state-of-the-art baselines.", "published": "2024-02-25 05:07:56", "link": "http://arxiv.org/abs/2402.15991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate\n  Suffix Embeddings", "abstract": "The safety defense methods of Large language models(LLMs) stays limited\nbecause the dangerous prompts are manually curated to just few known attack\ntypes, which fails to keep pace with emerging varieties. Recent studies found\nthat attaching suffixes to harmful instructions can hack the defense of LLMs\nand lead to dangerous outputs. However, similar to traditional text adversarial\nattacks, this approach, while effective, is limited by the challenge of the\ndiscrete tokens. This gradient based discrete optimization attack requires over\n100,000 LLM calls, and due to the unreadable of adversarial suffixes, it can be\nrelatively easily penetrated by common defense methods such as perplexity\nfilters. To cope with this challenge, in this paper, we proposes an Adversarial\nSuffix Embedding Translation Framework (ASETF), aimed at transforming\ncontinuous adversarial suffix embeddings into coherent and understandable text.\nThis method greatly reduces the computational overhead during the attack\nprocess and helps to automatically generate multiple adversarial samples, which\ncan be used as data to strengthen LLMs security defense. Experimental\nevaluations were conducted on Llama2, Vicuna, and other prominent LLMs,\nemploying harmful directives sourced from the Advbench dataset. The results\nindicate that our method significantly reduces the computation time of\nadversarial suffixes and achieves a much better attack success rate to existing\ntechniques, while significantly enhancing the textual fluency of the prompts.\nIn addition, our approach can be generalized into a broader method for\ngenerating transferable adversarial suffixes that can successfully attack\nmultiple LLMs, even black-box LLMs, such as ChatGPT and Gemini.", "published": "2024-02-25 06:46:27", "link": "http://arxiv.org/abs/2402.16006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GraphWiz: An Instruction-Following Language Model for Graph Problems", "abstract": "Large language models (LLMs) have achieved impressive success across several\nfields, but their proficiency in understanding and resolving complex graph\nproblems is less explored. To bridge this gap, we introduce GraphInstruct, a\nnovel and comprehensive instruction-tuning dataset designed to equip language\nmodels with the ability to tackle a broad spectrum of graph problems using\nexplicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an\nopen-source language model capable of resolving various graph problem types\nwhile generating clear reasoning processes. To enhance the model's capability\nand reliability, we incorporate the Direct Preference Optimization (DPO)\nframework into the graph problem-solving context. The enhanced model,\nGraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with\ndifferent complexity levels, surpassing GPT-4 which has an average accuracy of\n43.8%. Moreover, our research delves into the delicate balance between training\ndata volume and model performance, highlighting the potential for overfitting\nwith increased data. We also explore the transferability of the model's\nreasoning ability across different graph tasks, indicating the model's\nadaptability and practical application potential. Our investigation offers a\nnew blueprint and valuable insights for developing LLMs specialized in graph\nreasoning and problem-solving.", "published": "2024-02-25 08:41:32", "link": "http://arxiv.org/abs/2402.16029v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using\n  Discharge Summaries", "abstract": "Discharge summaries in Electronic Health Records (EHRs) are crucial for\nclinical decision-making, but their length and complexity make information\nextraction challenging, especially when dealing with accumulated summaries\nacross multiple patient admissions. Large Language Models (LLMs) show promise\nin addressing this challenge by efficiently analyzing vast and complex data.\nExisting benchmarks, however, fall short in properly evaluating LLMs'\ncapabilities in this context, as they typically focus on single-note\ninformation or limited topics, failing to reflect the real-world inquiries\nrequired by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel\nbenchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each\nlinked to distinct patients' discharge summaries. Every QA pair is initially\ngenerated using GPT-4 and then manually reviewed and refined by three\nclinicians to ensure clinical relevance. EHRNoteQA includes questions that\nrequire information across multiple discharge summaries and covers eight\ndiverse topics, mirroring the complexity and diversity of real clinical\ninquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice\nquestion answering, and propose a reliable evaluation method for each. We\nevaluate 27 LLMs using EHRNoteQA and examine various factors affecting the\nmodel performance (e.g., the length and number of discharge summaries).\nFurthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations\nin clinical practice, we measure the correlation between the LLM performance on\nEHRNoteQA, and the LLM performance manually evaluated by clinicians. Results\nshow that LLM performance on EHRNoteQA have higher correlation with\nclinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to\nother benchmarks, demonstrating its practical relevance in evaluating LLMs in\nclinical settings.", "published": "2024-02-25 09:41:50", "link": "http://arxiv.org/abs/2402.16040v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Say More with Less: Understanding Prompt Learning Behaviors through Gist\n  Compression", "abstract": "Large language models (LLMs) require lengthy prompts as the input context to\nproduce output aligned with user intentions, a process that incurs extra costs\nduring inference. In this paper, we propose the Gist COnditioned deCOding\n(Gist-COCO) model, introducing a novel method for compressing prompts which\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\nencoder-decoder based language model and then incorporates an additional\nencoder as a plugin module to compress prompts with inputs using gist tokens.\nIt finetunes the compression plugin module and uses the representations of gist\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\nthe representations of gist tokens into gist prompts, the compression ability\nof Gist-COCO can be generalized to different LLMs with high compression rates.\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\ncompression models in both passage and instruction compression tasks. Further\nanalysis on gist verbalization results suggests that our gist prompts serve\ndifferent functions in aiding language models. They may directly provide\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .", "published": "2024-02-25 11:07:08", "link": "http://arxiv.org/abs/2402.16058v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Large Language Models Encode Context Knowledge? A Layer-Wise Probing\n  Study", "abstract": "Previous work has showcased the intriguing capability of large language\nmodels (LLMs) in retrieving facts and processing context knowledge. However,\nonly limited research exists on the layer-wise capability of LLMs to encode\nknowledge, which challenges our understanding of their internal mechanisms. In\nthis paper, we devote the first attempt to investigate the layer-wise\ncapability of LLMs through probing tasks. We leverage the powerful generative\ncapability of ChatGPT to construct probing datasets, providing diverse and\ncoherent evidence corresponding to various facts. We employ $\\mathcal V$-usable\ninformation as the validation metric to better reflect the capability in\nencoding context knowledge across different layers. Our experiments on\nconflicting and newly acquired knowledge show that LLMs: (1) prefer to encode\nmore context knowledge in the upper layers; (2) primarily encode context\nknowledge within knowledge-related entity tokens at lower layers while\nprogressively expanding more knowledge within other tokens at upper layers; and\n(3) gradually forget the earlier context knowledge retained within the\nintermediate layers when provided with irrelevant evidence. Code is publicly\navailable at https://github.com/Jometeorie/probing_llama.", "published": "2024-02-25 11:15:42", "link": "http://arxiv.org/abs/2402.16061v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting Predictive Probabilities: Model Confidence or Human Label\n  Variation?", "abstract": "With the rise of increasingly powerful and user-facing NLP systems, there is\ngrowing interest in assessing whether they have a good representation of\nuncertainty by evaluating the quality of their predictive distribution over\noutcomes. We identify two main perspectives that drive starkly different\nevaluation protocols. The first treats predictive probability as an indication\nof model confidence; the second as an indication of human label variation. We\ndiscuss their merits and limitations, and take the position that both are\ncrucial for trustworthy and fair NLP systems, but that exploiting a single\npredictive distribution is limiting. We recommend tools and highlight exciting\ndirections towards models with disentangled representations of uncertainty\nabout predictions and uncertainty about human labels.", "published": "2024-02-25 15:00:13", "link": "http://arxiv.org/abs/2402.16102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Fusion of Chat LLMs: A Preliminary Technical Report", "abstract": "Recently, FuseLLM introduced the concept of knowledge fusion to transfer the\ncollective knowledge of multiple structurally varied LLMs into a target LLM\nthrough lightweight continual training. In this report, we extend the\nscalability and flexibility of the FuseLLM framework to realize the fusion of\nchat LLMs, resulting in FusionChat. FusionChat comprises two main stages.\nFirstly, we undertake knowledge fusion for structurally and scale-varied source\nLLMs to derive multiple target LLMs of identical structure and size via\nlightweight fine-tuning. Then, these target LLMs are merged within the\nparameter space, wherein we propose a novel method for determining the merging\nweights based on the variation ratio of parameter matrices before and after\nfine-tuning. We validate our approach using three prominent chat LLMs with\ndiverse architectures and scales, namely NH2-Mixtral-8x7B, NH2-Solar-10.7B, and\nOpenChat-3.5-7B. Experimental results spanning various chat domains demonstrate\nthe superiority of FusionChat-7B across a broad spectrum of chat LLMs at 7B and\n34B scales, even surpassing GPT-3.5 (March) and approaching\nMixtral-8x7B-Instruct.", "published": "2024-02-25 15:11:58", "link": "http://arxiv.org/abs/2402.16107v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization", "abstract": "Supervised fine-tuning is the most common method to adapt large language\nmodels (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive\ncomputational resources. Recently, parameter-efficient fine-tuning (PEFT)\nmethods have been widely studied due to its cost-effectiveness. LoRA is one of\nthe most widely used methods, which assumes that the optimization process is\nessentially low-dimensional. Although LoRA fine-tuning is effective, there is\nstill a performance gap compared to full fine-tuning, since its weight update\nis limited to low-rank matrices. In order to break the low-rank bottleneck in\nLoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank\nupdate matrices multiple times to achieve a higher update rank. PLoRA has\nmultiple training stages. During each stage, we still update only the LoRA\nweights. However, at the end of each stage, we unload the LoRA weights into the\nbackbone parameters and then reinitialize the LoRA states. Experimental results\nshow that PLoRA has stronger learning ability, approximately 1.8 times that of\nLoRA's learning ability at most, but it does not increase memory usage.\nFurther, we introduce a momentum-based unloading strategy for PLoRA to mitigate\nthe training instability.", "published": "2024-02-25 16:43:41", "link": "http://arxiv.org/abs/2402.16141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DistALANER: Distantly Supervised Active Learning Augmented Named Entity\n  Recognition in the Open Source Software Ecosystem", "abstract": "With the AI revolution in place, the trend for building automated systems to\nsupport professionals in different domains such as the open source software\nsystems, healthcare systems, banking systems, transportation systems and many\nothers have become increasingly prominent. A crucial requirement in the\nautomation of support tools for such systems is the early identification of\nnamed entities, which serves as a foundation for developing specialized\nfunctionalities. However, due to the specific nature of each domain, different\ntechnical terminologies and specialized languages, expert annotation of\navailable data becomes expensive and challenging. In light of these challenges,\nthis paper proposes a novel named entity recognition (NER) technique\nspecifically tailored for the open-source software systems. Our approach aims\nto address the scarcity of annotated software data by employing a comprehensive\ntwo-step distantly supervised annotation process. This process strategically\nleverages language heuristics, unique lookup tables, external knowledge\nsources, and an active learning approach. By harnessing these powerful\ntechniques, we not only enhance model performance but also effectively mitigate\nthe limitations associated with cost and the scarcity of expert annotators. It\nis noteworthy that our model significantly outperforms the state-of-the-art\nLLMs by a substantial margin. We also show the effectiveness of NER in the\ndownstream task of relation extraction.", "published": "2024-02-25 17:40:49", "link": "http://arxiv.org/abs/2402.16159v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defending Large Language Models against Jailbreak Attacks via Semantic\n  Smoothing", "abstract": "Aligned large language models (LLMs) are vulnerable to jailbreaking attacks,\nwhich bypass the safeguards of targeted LLMs and fool them into generating\nobjectionable content. While initial defenses show promise against token-based\nthreat models, there do not exist defenses that provide robustness against\nsemantic attacks and avoid unfavorable trade-offs between robustness and\nnominal performance. To meet this need, we propose SEMANTICSMOOTH, a\nsmoothing-based defense that aggregates the predictions of multiple\nsemantically transformed copies of a given input prompt. Experimental results\ndemonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against\nGCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on\ninstruction following benchmarks such as InstructionFollowing and AlpacaEval.\nThe codes will be publicly available at\nhttps://github.com/UCSB-NLP-Chang/SemanticSmooth.", "published": "2024-02-25 20:36:03", "link": "http://arxiv.org/abs/2402.16192v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and\n  Emotion Modeling", "abstract": "Effective feature representations play a critical role in enhancing the\nperformance of text generation models that rely on deep neural networks.\nHowever, current approaches suffer from several drawbacks, such as the\ninability to capture the deep semantics of language and sensitivity to minor\ninput variations, resulting in significant changes in the generated text. In\nthis paper, we present a novel solution to these challenges by employing a\nmixture of experts, multiple encoders, to offer distinct perspectives on the\nemotional state of the user's utterance while simultaneously enhancing\nperformance. We propose an end-to-end model architecture called ASEM that\nperforms emotion analysis on top of sentiment analysis for open-domain\nchatbots, enabling the generation of empathetic responses that are fluent and\nrelevant. In contrast to traditional attention mechanisms, the proposed model\nemploys a specialized attention strategy that uniquely zeroes in on sentiment\nand emotion nuances within the user's utterance. This ensures the generation of\ncontext-rich representations tailored to the underlying emotional tone and\nsentiment intricacies of the text. Our approach outperforms existing methods\nfor generating empathetic embeddings, providing empathetic and diverse\nresponses. The performance of our proposed model significantly exceeds that of\nexisting models, enhancing emotion detection accuracy by 6.2% and lexical\ndiversity by 1.4%.", "published": "2024-02-25 20:36:51", "link": "http://arxiv.org/abs/2402.16194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination\n  Tendency of LLMs", "abstract": "Hallucinations pose a significant challenge to the reliability and alignment\nof Large Language Models (LLMs), limiting their widespread acceptance beyond\nchatbot applications. Despite ongoing efforts, hallucinations remain a\nprevalent challenge in LLMs. The detection of hallucinations itself is also a\nformidable task, frequently requiring manual labeling or constrained\nevaluations. This paper introduces an automated scalable framework that\ncombines benchmarking LLMs' hallucination tendencies with efficient\nhallucination detection. We leverage LLMs to generate challenging tasks related\nto hypothetical phenomena, subsequently employing them as agents for efficient\nhallucination detection. The framework is domain-agnostic, allowing the use of\nany language model for benchmark creation or evaluation in any domain. We\nintroduce the publicly available HypoTermQA Benchmarking Dataset, on which\nstate-of-the-art models' performance ranged between 3% and 11%, and evaluator\nagents demonstrated a 6% error rate in hallucination prediction. The proposed\nframework provides opportunities to test and improve LLMs. Additionally, it has\nthe potential to generate benchmarking datasets tailored to specific domains,\nsuch as law, health, and finance.", "published": "2024-02-25 22:23:37", "link": "http://arxiv.org/abs/2402.16211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Likelihood-based Mitigation of Evaluation Bias in Large Language Models", "abstract": "Large Language Models (LLMs) are widely used to evaluate natural language\ngeneration tasks as automated metrics. However, the likelihood, a measure of\nLLM's plausibility for a sentence, can vary due to superficial differences in\nsentences, such as word order and sentence structure. It is therefore possible\nthat there might be a likelihood bias if LLMs are used for evaluation: they\nmight overrate sentences with higher likelihoods while underrating those with\nlower likelihoods. In this paper, we investigate the presence and impact of\nlikelihood bias in LLM-based evaluators. We also propose a method to mitigate\nthe likelihood bias. Our method utilizes highly biased instances as few-shot\nexamples for in-context learning. Our experiments in evaluating the\ndata-to-text and grammatical error correction tasks reveal that several LLMs we\ntest display a likelihood bias. Furthermore, our proposed method successfully\nmitigates this bias, also improving evaluation performance (in terms of\ncorrelation of models with human scores) significantly.", "published": "2024-02-25 04:52:02", "link": "http://arxiv.org/abs/2402.15987v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Machine Learning Approach to Detect Customer Satisfaction From\n  Multiple Tweet Parameters", "abstract": "Since internet technologies have advanced, one of the primary factors in\ncompany development is customer happiness. Online platforms have become\nprominent places for sharing reviews. Twitter is one of these platforms where\ncustomers frequently post their thoughts. Reviews of flights on these platforms\nhave become a concern for the airline business. A positive review can help the\ncompany grow, while a negative one can quickly ruin its revenue and reputation.\nSo it's vital for airline businesses to examine the feedback and experiences of\ntheir customers and enhance their services to remain competitive. But studying\nthousands of tweets and analyzing them to find the satisfaction of the customer\nis quite a difficult task. This tedious process can be made easier by using a\nmachine learning approach to analyze tweets to determine client satisfaction\nlevels. Some work has already been done on this strategy to automate the\nprocedure using machine learning and deep learning techniques. However, they\nare all purely concerned with assessing the text's sentiment. In addition to\nthe text, the tweet also includes the time, location, username, airline name,\nand so on. This additional information can be crucial for improving the model's\noutcome. To provide a machine learning based solution, this work has broadened\nits perspective to include these qualities. And it has come as no surprise that\nthe additional features beyond text sentiment analysis produce better outcomes\nin machine learning based models.", "published": "2024-02-25 05:16:43", "link": "http://arxiv.org/abs/2402.15992v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PST-Bench: Tracing and Benchmarking the Source of Publications", "abstract": "Tracing the source of research papers is a fundamental yet challenging task\nfor researchers. The billion-scale citation relations between papers hinder\nresearchers from understanding the evolution of science efficiently. To date,\nthere is still a lack of an accurate and scalable dataset constructed by\nprofessional researchers to identify the direct source of their studied papers,\nbased on which automatic algorithms can be developed to expand the evolutionary\nknowledge of science. In this paper, we study the problem of paper source\ntracing (PST) and construct a high-quality and ever-increasing dataset\nPST-Bench in computer science. Based on PST-Bench, we reveal several intriguing\ndiscoveries, such as the differing evolution patterns across various topics. An\nexploration of various methods underscores the hardness of PST-Bench,\npinpointing potential directions on this topic. The dataset and codes have been\navailable at https://github.com/THUDM/paper-source-trace.", "published": "2024-02-25 06:56:43", "link": "http://arxiv.org/abs/2402.16009v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "HiGPT: Heterogeneous Graph Language Model", "abstract": "Heterogeneous graph learning aims to capture complex relationships and\ndiverse relational semantics among entities in a heterogeneous graph to obtain\nmeaningful representations for nodes and edges. Recent advancements in\nheterogeneous graph neural networks (HGNNs) have achieved state-of-the-art\nperformance by considering relation heterogeneity and using specialized message\nfunctions and aggregation rules. However, existing frameworks for heterogeneous\ngraph learning have limitations in generalizing across diverse heterogeneous\ngraph datasets. Most of these frameworks follow the \"pre-train\" and \"fine-tune\"\nparadigm on the same dataset, which restricts their capacity to adapt to new\nand unseen data. This raises the question: \"Can we generalize heterogeneous\ngraph models to be well-adapted to diverse downstream learning tasks with\ndistribution shifts in both node token sets and relation type heterogeneity?''\nTo tackle those challenges, we propose HiGPT, a general large graph model with\nHeterogeneous graph instruction-tuning paradigm. Our framework enables learning\nfrom arbitrary heterogeneous graphs without the need for any fine-tuning\nprocess from downstream datasets. To handle distribution shifts in\nheterogeneity, we introduce an in-context heterogeneous graph tokenizer that\ncaptures semantic relationships in different heterogeneous graphs, facilitating\nmodel adaptation. We incorporate a large corpus of heterogeneity-aware graph\ninstructions into our HiGPT, enabling the model to effectively comprehend\ncomplex relation heterogeneity and distinguish between various types of graph\ntokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction\naugmentation paradigm to mitigate data scarcity by generating diverse and\ninformative instructions. Through comprehensive evaluations, our proposed\nframework demonstrates exceptional performance in terms of generalization\nperformance.", "published": "2024-02-25 08:07:22", "link": "http://arxiv.org/abs/2402.16024v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Don't Forget Your Reward Values: Language Model Alignment via\n  Value-based Calibration", "abstract": "While Reinforcement Learning from Human Feedback (RLHF) significantly\nenhances the generation quality of Large Language Models (LLMs), recent studies\nhave raised concerns regarding the complexity and instability associated with\nthe Proximal Policy Optimization (PPO) algorithm, proposing a series of\norder-based calibration methods as viable alternatives. This paper delves\nfurther into current order-based methods, examining their inefficiencies in\nutilizing reward values and addressing misalignment issues. Building upon these\nfindings, we propose a novel \\textbf{V}alue-based \\textbf{C}ali\\textbf{B}ration\n(VCB) method to better align LLMs with human preferences. Experimental results\ndemonstrate that VCB surpasses existing alignment methods on AI assistant and\nsummarization datasets, providing impressive generalizability, robustness, and\nstability in diverse settings.", "published": "2024-02-25 08:45:10", "link": "http://arxiv.org/abs/2402.16030v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emotion Classification in Short English Texts using Deep Learning\n  Techniques", "abstract": "Detecting emotions in limited text datasets from under-resourced languages\npresents a formidable obstacle, demanding specialized frameworks and\ncomputational strategies. This study conducts a thorough examination of deep\nlearning techniques for discerning emotions in short English texts. Deep\nlearning approaches employ transfer learning and word embedding, notably BERT,\nto attain superior accuracy. To evaluate these methods, we introduce the\n\"SmallEnglishEmotions\" dataset, comprising 6372 varied short English texts\nannotated with five primary emotion categories. Our experiments reveal that\ntransfer learning and BERT-based text embedding outperform alternative methods\nin accurately categorizing the text in the dataset.", "published": "2024-02-25 09:17:22", "link": "http://arxiv.org/abs/2402.16034v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text Understanding and Generation Using Transformer Models for\n  Intelligent E-commerce Recommendations", "abstract": "With the rapid development of artificial intelligence technology, Transformer\nstructural pre-training model has become an important tool for large language\nmodel (LLM) tasks. In the field of e-commerce, these models are especially\nwidely used, from text understanding to generating recommendation systems,\nwhich provide powerful technical support for improving user experience and\noptimizing service processes. This paper reviews the core application scenarios\nof Transformer pre-training model in e-commerce text understanding and\nrecommendation generation, including but not limited to automatic generation of\nproduct descriptions, sentiment analysis of user comments, construction of\npersonalized recommendation system and automated processing of customer service\nconversations. Through a detailed analysis of the model's working principle,\nimplementation process, and application effects in specific cases, this paper\nemphasizes the unique advantages of pre-trained models in understanding complex\nuser intentions and improving the quality of recommendations. In addition, the\nchallenges and improvement directions for the future are also discussed, such\nas how to further improve the generalization ability of the model, the ability\nto handle large-scale data sets, and technical strategies to protect user\nprivacy. Ultimately, the paper points out that the application of Transformer\nstructural pre-training models in e-commerce has not only driven technological\ninnovation, but also brought substantial benefits to merchants and consumers,\nand looking forward, these models will continue to play a key role in\ne-commerce and beyond.", "published": "2024-02-25 09:19:11", "link": "http://arxiv.org/abs/2402.16035v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Public Perceptions of AI Conversational Agents: A\n  Cross-Cultural Analysis", "abstract": "Conversational Agents (CAs) have increasingly been integrated into everyday\nlife, sparking significant discussions on social media. While previous research\nhas examined public perceptions of AI in general, there is a notable lack in\nresearch focused on CAs, with fewer investigations into cultural variations in\nCA perceptions. To address this gap, this study used computational methods to\nanalyze about one million social media discussions surrounding CAs and compared\npeople's discourses and perceptions of CAs in the US and China. We find Chinese\nparticipants tended to view CAs hedonically, perceived voice-based and\nphysically embodied CAs as warmer and more competent, and generally expressed\npositive emotions. In contrast, US participants saw CAs more functionally, with\nan ambivalent attitude. Warm perception was a key driver of positive emotions\ntoward CAs in both countries. We discussed practical implications for designing\ncontextually sensitive and user-centric CAs to resonate with various users'\npreferences and needs.", "published": "2024-02-25 09:34:22", "link": "http://arxiv.org/abs/2402.16039v1", "categories": ["cs.HC", "cs.CL", "H.5.2"], "primary_category": "cs.HC"}
{"title": "Detecting Machine-Generated Texts by Multi-Population Aware Optimization\n  for Maximum Mean Discrepancy", "abstract": "Large language models (LLMs) such as ChatGPT have exhibited remarkable\nperformance in generating human-like texts. However, machine-generated texts\n(MGTs) may carry critical risks, such as plagiarism issues, misleading\ninformation, or hallucination issues. Therefore, it is very urgent and\nimportant to detect MGTs in many situations. Unfortunately, it is challenging\nto distinguish MGTs and human-written texts because the distributional\ndiscrepancy between them is often very subtle due to the remarkable performance\nof LLMs. In this paper, we seek to exploit \\textit{maximum mean discrepancy}\n(MMD) to address this issue in the sense that MMD can well identify\ndistributional discrepancies. However, directly training a detector with MMD\nusing diverse MGTs will incur a significantly increased variance of MMD since\nMGTs may contain \\textit{multiple text populations} due to various LLMs. This\nwill severely impair MMD's ability to measure the difference between two\nsamples. To tackle this, we propose a novel \\textit{multi-population} aware\noptimization method for MMD called MMD-MP, which can \\textit{avoid variance\nincreases} and thus improve the stability to measure the distributional\ndiscrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and\nsentence-based detection, respectively. Extensive experiments on various LLMs,\n\\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP. The\nsource code is available at \\url{https://github.com/ZSHsh98/MMD-MP}.", "published": "2024-02-25 09:44:56", "link": "http://arxiv.org/abs/2402.16041v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Temporal Extrapolation of Multimodal Large Language Models\n  with Temporal Grounding Bridge", "abstract": "Despite progress in multimodal large language models (MLLMs), the challenge\nof interpreting long-form videos in response to linguistic queries persists,\nlargely due to the inefficiency in temporal grounding and limited pre-trained\ncontext window size. In this work, we introduce Temporal Grounding Bridge\n(TGB), a novel framework that bootstraps MLLMs with advanced temporal grounding\ncapabilities and broadens their contextual scope. Our framework significantly\nenhances the temporal capabilities of current MLLMs through three key\ninnovations: an efficient multi-span temporal grounding algorithm applied to\nlow-dimension temporal features projected from flow; a multimodal length\nextrapolation training paradigm that utilizes low-dimension temporal features\nto extend the training context window size; and a bootstrapping framework that\nbridges our model with pluggable MLLMs without requiring annotation. We\nvalidate TGB across seven video benchmarks and demonstrate substantial\nperformance improvements compared with prior MLLMs. Notably, our model,\ninitially trained on sequences of four frames, effectively handles sequences up\nto 16 longer without sacrificing performance, highlighting its scalability and\neffectiveness in real-world applications. Our code is publicly available at\nhttps://github.com/bigai-nlco/VideoTGB", "published": "2024-02-25 10:27:46", "link": "http://arxiv.org/abs/2402.16050v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Citation-Enhanced Generation for LLM-based Chatbots", "abstract": "Large language models (LLMs) exhibit powerful general intelligence across\ndiverse scenarios, including their integration into chatbots. However, a vital\nchallenge of LLM-based chatbots is that they may produce hallucinated content\nin responses, which significantly limits their applicability. Various efforts\nhave been made to alleviate hallucination, such as retrieval augmented\ngeneration and reinforcement learning with human feedback, but most of them\nrequire additional training and data annotation. In this paper, we propose a\nnovel post-hoc Citation-Enhanced Generation (CEG) approach combined with\nretrieval argumentation. Unlike previous studies that focus on preventing\nhallucinations during generation, our method addresses this issue in a post-hoc\nway. It incorporates a retrieval module to search for supporting documents\nrelevant to the generated content, and employs a natural language\ninference-based citation generation module. Once the statements in the\ngenerated content lack of reference, our model can regenerate responses until\nall statements are supported by citations. Note that our method is a\ntraining-free plug-and-play plugin that is capable of various LLMs. Experiments\non various hallucination-related datasets show our framework outperforms\nstate-of-the-art methods in both hallucination detection and response\nregeneration on three benchmarks. Our codes and dataset will be publicly\navailable.", "published": "2024-02-25 11:24:41", "link": "http://arxiv.org/abs/2402.16063v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Training a Bilingual Language Model by Mapping Tokens onto a Shared\n  Character Space", "abstract": "We train a bilingual Arabic-Hebrew language model using a transliterated\nversion of Arabic texts in Hebrew, to ensure both languages are represented in\nthe same script. Given the morphological, structural similarities, and the\nextensive number of cognates shared among Arabic and Hebrew, we assess the\nperformance of a language model that employs a unified script for both\nlanguages, on machine translation which requires cross-lingual knowledge. The\nresults are promising: our model outperforms a contrasting model which keeps\nthe Arabic texts in the Arabic script, demonstrating the efficacy of the\ntransliteration step. Despite being trained on a dataset approximately 60%\nsmaller than that of other existing language models, our model appears to\ndeliver comparable performance in machine translation across both translation\ndirections.", "published": "2024-02-25 11:26:39", "link": "http://arxiv.org/abs/2402.16065v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by\n  Long-Short-Term Prompting", "abstract": "Time-series forecasting (TSF) finds broad applications in real-world\nscenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates\nstrong zero-shot TSF capabilities while preserving computational efficiency.\nHowever, existing prompting methods oversimplify TSF as language next-token\npredictions, overlooking its dynamic nature and lack of integration with\nstate-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose\nLSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks.\nLSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks,\ntailoring prompts to each. LSTPrompt guides LLMs to regularly reassess\nforecasting mechanisms to enhance adaptability. Extensive evaluations\ndemonstrate consistently better performance of LSTPrompt than existing\nprompting methods, and competitive results compared to foundation TSF models.", "published": "2024-02-25 16:14:26", "link": "http://arxiv.org/abs/2402.16132v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Generative Artificial Intelligence Means for Terminological\n  Definitions", "abstract": "This paper examines the impact of Generative Artificial Intelligence (GenAI)\ntools like ChatGPT on the creation and consumption of terminological\ndefinitions. From the terminologist's point of view, the strategic use of GenAI\ntools can streamline the process of crafting definitions, reducing both time\nand effort, while potentially enhancing quality. GenAI tools enable AI-assisted\nterminography, notably post-editing terminography, where the machine produces a\ndefinition that the terminologist then corrects or refines. However, the\npotential of GenAI tools to fulfill all the terminological needs of a user,\nincluding term definitions, challenges the very existence of terminological\ndefinitions and resources as we know them. Unlike terminological definitions,\nGenAI tools can describe the knowledge activated by a term in a specific\ncontext. However, a main drawback of these tools is that their output can\ncontain errors. For this reason, users requiring reliability will likely still\nresort to terminological resources for definitions. Nevertheless, with the\ninevitable integration of AI into terminology work, the distinction between\nhuman-created and AI-created content will become increasingly blurred.", "published": "2024-02-25 16:36:51", "link": "http://arxiv.org/abs/2402.16139v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Text to Transformation: A Comprehensive Review of Large Language\n  Models' Versatility", "abstract": "This groundbreaking study explores the expanse of Large Language Models\n(LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional\nEncoder Representations from Transformers (BERT) across varied domains ranging\nfrom technology, finance, healthcare to education. Despite their established\nprowess in Natural Language Processing (NLP), these LLMs have not been\nsystematically examined for their impact on domains such as fitness, and\nholistic well-being, urban planning, climate modelling as well as disaster\nmanagement. This review paper, in addition to furnishing a comprehensive\nanalysis of the vast expanse and extent of LLMs' utility in diverse domains,\nrecognizes the research gaps and realms where the potential of LLMs is yet to\nbe harnessed. This study uncovers innovative ways in which LLMs can leave a\nmark in the fields like fitness and wellbeing, urban planning, climate\nmodelling and disaster response which could inspire future researches and\napplications in the said avenues.", "published": "2024-02-25 16:47:59", "link": "http://arxiv.org/abs/2402.16142v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hitting \"Probe\"rty with Non-Linearity, and More", "abstract": "Structural probes learn a linear transformation to find how dependency trees\nare embedded in the hidden states of language models. This simple design may\nnot allow for full exploitation of the structure of the encoded information.\nHence, to investigate the structure of the encoded information to its full\nextent, we incorporate non-linear structural probes. We reformulate the design\nof non-linear structural probes introduced by White et al. making its design\nsimpler yet effective. We also design a visualization framework that lets us\nqualitatively assess how strongly two words in a sentence are connected in the\npredicted dependency tree. We use this technique to understand which non-linear\nprobe variant is good at encoding syntactical information. Additionally, we\nalso use it to qualitatively investigate the structure of dependency trees that\nBERT encodes in each of its layers. We find that the radial basis function\n(RBF) is an effective non-linear probe for the BERT model than the linear\nprobe.", "published": "2024-02-25 18:33:25", "link": "http://arxiv.org/abs/2402.16168v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bootstrapping Cognitive Agents with a Large Language Model", "abstract": "Large language models contain noisy general knowledge of the world, yet are\nhard to train or fine-tune. On the other hand cognitive architectures have\nexcellent interpretability and are flexible to update but require a lot of\nmanual work to instantiate. In this work, we combine the best of both worlds:\nbootstrapping a cognitive-based model with the noisy knowledge encoded in large\nlanguage models. Through an embodied agent doing kitchen tasks, we show that\nour proposed framework yields better efficiency compared to an agent based\nentirely on large language models. Our experiments indicate that large language\nmodels are a good source of information for cognitive architectures, and the\ncognitive architecture in turn can verify and update the knowledge of large\nlanguage models to a specific domain.", "published": "2024-02-25 01:40:30", "link": "http://arxiv.org/abs/2403.00810v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Cognitive Bias in Decision-Making with LLMs", "abstract": "Large language models (LLMs) offer significant potential as tools to support\nan expanding range of decision-making tasks. Given their training on human\n(created) data, LLMs have been shown to inherit societal biases against\nprotected groups, as well as be subject to bias functionally resembling\ncognitive bias. Human-like bias can impede fair and explainable decisions made\nwith LLM assistance. Our work introduces BiasBuster, a framework designed to\nuncover, evaluate, and mitigate cognitive bias in LLMs, particularly in\nhigh-stakes decision-making tasks. Inspired by prior research in psychology and\ncognitive science, we develop a dataset containing 13,465 prompts to evaluate\nLLM decisions on different cognitive biases (e.g., prompt-induced, sequential,\ninherent). We test various bias mitigation strategies, while proposing a novel\nmethod utilizing LLMs to debias their own human-like cognitive bias within\nprompts. Our analysis provides a comprehensive picture of the presence and\neffects of cognitive bias across commercial and open-source models. We\ndemonstrate that our selfhelp debiasing effectively mitigates model answers\nthat display patterns akin to human cognitive bias without having to manually\ncraft examples for each bias.", "published": "2024-02-25 02:35:56", "link": "http://arxiv.org/abs/2403.00811v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LoRA Meets Dropout under a Unified Framework", "abstract": "With the remarkable capabilities, large language models (LLMs) have emerged\nas essential elements in numerous NLP applications, while parameter-efficient\nfinetuning, especially LoRA, has gained popularity as a lightweight approach\nfor model customization. Meanwhile, various dropout methods, initially designed\nfor full finetuning with all the parameters updated, alleviates overfitting\nassociated with excessive parameter redundancy. Hence, a possible contradiction\narises from negligible trainable parameters of LoRA and the effectiveness of\nprevious dropout methods, which has been largely overlooked. To fill this gap,\nwe first confirm that parameter-efficient LoRA is also overfitting-prone. We\nthen revisit transformer-specific dropout methods, and establish their\nequivalence and distinctions mathematically and empirically. Building upon this\ncomparative analysis, we introduce a unified framework for a comprehensive\ninvestigation, which instantiates these methods based on dropping position,\nstructural pattern and compensation measure. Through this framework, we reveal\nthe new preferences and performance comparisons of them when involved with\nlimited trainable parameters. This framework also allows us to amalgamate the\nmost favorable aspects into a novel dropout method named HiddenKey. Extensive\nexperiments verify the remarkable superiority and sufficiency of HiddenKey\nacross multiple models and tasks, which highlights it as the preferred approach\nfor high-performance and parameter-efficient finetuning of LLMs.", "published": "2024-02-25 07:09:10", "link": "http://arxiv.org/abs/2403.00812v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DetoxLLM: A Framework for Detoxification with Explanations", "abstract": "Prior works on detoxification are scattered in the sense that they do not\ncover all aspects of detoxification needed in a real-world scenario. Notably,\nprior works restrict the task of developing detoxification models to only a\nseen subset of platforms, leaving the question of how the models would perform\non unseen platforms unexplored. Additionally, these works do not address\nnon-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified\nwithout altering the meaning. We propose DetoxLLM, the first comprehensive\nend-to-end detoxification framework, which attempts to alleviate the\naforementioned limitations. We first introduce a cross-platform pseudo-parallel\ncorpus applying multi-step data processing and generation strategies leveraging\nChatGPT. We then train a suite of detoxification models with our cross-platform\ncorpus. We show that our detoxification models outperform the SoTA model\ntrained with human-annotated parallel corpus. We further introduce explanation\nto promote transparency and trustworthiness. DetoxLLM additionally offers a\nunique paraphrase detector especially dedicated for the detoxification task to\ntackle the non-detoxifiable cases. Through experimental analysis, we\ndemonstrate the effectiveness of our cross-platform corpus and the robustness\nof DetoxLLM against adversarial toxicity.", "published": "2024-02-25 01:56:47", "link": "http://arxiv.org/abs/2402.15951v2", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Direct Punjabi to English speech translation using discrete units", "abstract": "Speech-to-speech translation is yet to reach the same level of coverage as\ntext-to-text translation systems. The current speech technology is highly\nlimited in its coverage of over 7000 languages spoken worldwide, leaving more\nthan half of the population deprived of such technology and shared experiences.\nWith voice-assisted technology (such as social robots and speech-to-text apps)\nand auditory content (such as podcasts and lectures) on the rise, ensuring that\nthe technology is available for all is more important than ever. Speech\ntranslation can play a vital role in mitigating technological disparity and\ncreating a more inclusive society. With a motive to contribute towards speech\ntranslation research for low-resource languages, our work presents a direct\nspeech-to-speech translation model for one of the Indic languages called\nPunjabi to English. Additionally, we explore the performance of using a\ndiscrete representation of speech called discrete acoustic units as input to\nthe Transformer-based translation model. The model, abbreviated as Unit-to-Unit\nTranslation (U2UT), takes a sequence of discrete units of the source language\n(the language being translated from) and outputs a sequence of discrete units\nof the target language (the language being translated to). Our results show\nthat the U2UT model performs better than the Speech-to-Unit Translation (S2UT)\nmodel by a 3.69 BLEU score.", "published": "2024-02-25 03:03:34", "link": "http://arxiv.org/abs/2402.15967v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Phonetic and Lexical Discovery of a Canine Language using HuBERT", "abstract": "This paper delves into the pioneering exploration of potential communication\npatterns within dog vocalizations and transcends traditional linguistic\nanalysis barriers, which heavily relies on human priori knowledge on limited\ndatasets to find sound units in dog vocalization. We present a self-supervised\napproach with HuBERT, enabling the accurate classification of phoneme labels\nand the identification of vocal patterns that suggest a rudimentary vocabulary\nwithin dog vocalizations. Our findings indicate a significant acoustic\nconsistency in these identified canine vocabulary, covering the entirety of\nobserved dog vocalization sequences. We further develop a web-based dog\nvocalization labeling system. This system can highlight phoneme n-grams,\npresent in the vocabulary, in the dog audio uploaded by users.", "published": "2024-02-25 04:35:45", "link": "http://arxiv.org/abs/2402.15985v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TMT: Tri-Modal Translation between Speech, Image, and Text by Processing\n  Different Modalities as Different Languages", "abstract": "The capability to jointly process multi-modal information is becoming an\nessential task. However, the limited number of paired multi-modal data and the\nlarge computational requirements in multi-modal learning hinder the\ndevelopment. We propose a novel Tri-Modal Translation (TMT) model that\ntranslates between arbitrary modalities spanning speech, image, and text. We\nintroduce a novel viewpoint, where we interpret different modalities as\ndifferent languages, and treat multi-modal translation as a well-established\nmachine translation problem. To this end, we tokenize speech and image data\ninto discrete tokens, which provide a unified interface across modalities and\nsignificantly decrease the computational cost. In the proposed TMT, a\nmulti-modal encoder-decoder conducts the core translation, whereas\nmodality-specific processing is conducted only within the tokenization and\ndetokenization stages. We evaluate the proposed TMT on all six modality\ntranslation tasks. TMT outperforms single model counterparts consistently,\ndemonstrating that unifying tasks is beneficial not only for practicality but\nalso for performance.", "published": "2024-02-25 07:46:57", "link": "http://arxiv.org/abs/2402.16021v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Deep Learning Approaches for Improving Question Answering Systems in\n  Hepatocellular Carcinoma Research", "abstract": "In recent years, advancements in natural language processing (NLP) have been\nfueled by deep learning techniques, particularly through the utilization of\npowerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3,\ntrained on vast amounts of data, have revolutionized language understanding and\ngeneration. These pre-trained models serve as robust bases for various tasks\nincluding semantic understanding, intelligent writing, and reasoning, paving\nthe way for a more generalized form of artificial intelligence. NLP, as a vital\napplication of AI, aims to bridge the gap between humans and computers through\nnatural language interaction. This paper delves into the current landscape and\nfuture prospects of large-scale model-based NLP, focusing on the\nquestion-answering systems within this domain. Practical cases and developments\nin artificial intelligence-driven question-answering systems are analyzed to\nfoster further exploration and research in the realm of large-scale NLP.", "published": "2024-02-25 09:32:17", "link": "http://arxiv.org/abs/2402.16038v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Likely Do LLMs with CoT Mimic Human Reasoning?", "abstract": "Chain-of-thought emerges as a promising technique for eliciting reasoning\ncapabilities from Large Language Models (LLMs). However, it does not always\nimprove task performance or accurately represent reasoning processes, leaving\nunresolved questions about its usage. In this paper, we diagnose the underlying\nmechanism by comparing the reasoning process of LLMs with humans, using causal\nanalysis to understand the relationships between the problem instruction,\nreasoning, and the answer in LLMs. Our empirical study reveals that LLMs often\ndeviate from the ideal causal chain, resulting in spurious correlations and\npotential consistency errors (inconsistent reasoning and answers). We also\nexamine various factors influencing the causal structure, finding that\nin-context learning with examples strengthens it, while post-training\ntechniques like supervised fine-tuning and reinforcement learning on human\nfeedback weaken it. To our surprise, the causal structure cannot be\nstrengthened by enlarging the model size only, urging research on new\ntechniques. We hope that this preliminary study will shed light on\nunderstanding and improving the reasoning process in LLM.", "published": "2024-02-25 10:13:04", "link": "http://arxiv.org/abs/2402.16048v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design\n  Choices", "abstract": "Advances in generative models have made it possible for AI-generated text,\ncode, and images to mirror human-generated content in many applications.\nWatermarking, a technique that aims to embed information in the output of a\nmodel to verify its source, is useful for mitigating the misuse of such\nAI-generated content. However, we show that common design choices in LLM\nwatermarking schemes make the resulting systems surprisingly susceptible to\nattack -- leading to fundamental trade-offs in robustness, utility, and\nusability. To navigate these trade-offs, we rigorously study a set of simple\nyet effective attacks on common watermarking systems, and propose guidelines\nand defenses for LLM watermarking in practice.", "published": "2024-02-25 20:24:07", "link": "http://arxiv.org/abs/2402.16187v3", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "IR2: Information Regularization for Information Retrieval", "abstract": "Effective information retrieval (IR) in settings with limited training data,\nparticularly for complex queries, remains a challenging task. This paper\nintroduces IR2, Information Regularization for Information Retrieval, a\ntechnique for reducing overfitting during synthetic data generation. This\napproach, representing a novel application of regularization techniques in\nsynthetic data creation for IR, is tested on three recent IR tasks\ncharacterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook.\nExperimental results indicate that our regularization techniques not only\noutperform previous synthetic query generation methods on the tasks considered\nbut also reduce cost by up to 50%. Furthermore, this paper categorizes and\nexplores three regularization methods at different stages of the query\nsynthesis pipeline-input, prompt, and output-each offering varying degrees of\nperformance improvement compared to models where no regularization is applied.\nThis provides a systematic approach for optimizing synthetic data generation in\ndata-limited, complex-query IR scenarios. All code, prompts and synthetic data\nare available at\nhttps://github.com/Info-Regularization/Information-Regularization.", "published": "2024-02-25 21:25:06", "link": "http://arxiv.org/abs/2402.16200v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Debug like a Human: A Large Language Model Debugger via Verifying\n  Runtime Execution Step-by-step", "abstract": "Large language models (LLMs) are leading significant progress in code\ngeneration. Beyond one-pass code generation, recent works further integrate\nunit tests and program verifiers into LLMs to iteratively refine the generated\nprograms. However, these works consider the generated programs as an\nindivisible entity, which falls short for LLMs in debugging the programs,\nespecially when the programs contain complex logic flows and data operations.\nIn contrast, when human developers debug programs, they typically set\nbreakpoints and selectively examine runtime execution information. The\nexecution flow and the intermediate variables play a crucial role in the\ndebugging process, yet they are underutilized in the existing literature on\ncode generation. In this study, we introduce Large Language Model Debugger\n(LDB), a novel debugging framework that enables LLMs to refine their generated\nprograms with the runtime execution information. Specifically, LDB segments the\nprograms into basic blocks and tracks the values of intermediate variables\nafter each block throughout the runtime execution. This allows LLMs to\nconcentrate on simpler code units within the overall execution flow, verify\ntheir correctness against the task description block by block, and efficiently\npinpoint any potential errors. Experiments demonstrate that LDB consistently\nenhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and\nTransCoder benchmarks, archiving new state-of-the-art performance in code\ndebugging for various LLM selections.", "published": "2024-02-25 00:56:27", "link": "http://arxiv.org/abs/2402.16906v6", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM\n  Jailbreakers", "abstract": "The safety alignment of Large Language Models (LLMs) is vulnerable to both\nmanual and automated jailbreak attacks, which adversarially trigger LLMs to\noutput harmful content. However, current methods for jailbreaking LLMs, which\nnest entire harmful prompts, are not effective at concealing malicious intent\nand can be easily identified and rejected by well-aligned LLMs. This paper\ndiscovers that decomposing a malicious prompt into separated sub-prompts can\neffectively obscure its underlying malicious intent by presenting it in a\nfragmented, less detectable form, thereby addressing these limitations. We\nintroduce an automatic prompt \\textbf{D}ecomposition and\n\\textbf{R}econstruction framework for jailbreak \\textbf{Attack} (DrAttack).\nDrAttack includes three key components: (a) `Decomposition' of the original\nprompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly\nby in-context learning with semantically similar but harmless reassembling\ndemo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts'\nsynonyms that maintain the original intent while jailbreaking LLMs. An\nextensive empirical study across multiple open-source and closed-source LLMs\ndemonstrates that, with a significantly reduced number of queries, DrAttack\nobtains a substantial gain of success rate over prior SOTA prompt-only\nattackers. Notably, the success rate of 78.0\\% on GPT-4 with merely 15 queries\nsurpassed previous art by 33.1\\%. The project is available at\nhttps://github.com/xirui-li/DrAttack.", "published": "2024-02-25 17:43:29", "link": "http://arxiv.org/abs/2402.16914v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "UrbanGPT: Spatio-Temporal Large Language Models", "abstract": "Spatio-temporal prediction aims to forecast and gain insights into the\never-changing dynamics of urban environments across both time and space. Its\npurpose is to anticipate future patterns, trends, and events in diverse facets\nof urban life, including transportation, population movement, and crime rates.\nAlthough numerous efforts have been dedicated to developing neural network\ntechniques for accurate predictions on spatio-temporal data, it is important to\nnote that many of these methods heavily depend on having sufficient labeled\ndata to generate precise spatio-temporal representations. Unfortunately, the\nissue of data scarcity is pervasive in practical urban sensing scenarios.\nConsequently, it becomes necessary to build a spatio-temporal model with strong\ngeneralization capabilities across diverse spatio-temporal learning scenarios.\nTaking inspiration from the remarkable achievements of large language models\n(LLMs), our objective is to create a spatio-temporal LLM that can exhibit\nexceptional generalization capabilities across a wide range of downstream urban\ntasks. To achieve this objective, we present the UrbanGPT, which seamlessly\nintegrates a spatio-temporal dependency encoder with the instruction-tuning\nparadigm. This integration enables LLMs to comprehend the complex\ninter-dependencies across time and space, facilitating more comprehensive and\naccurate predictions under data scarcity. To validate the effectiveness of our\napproach, we conduct extensive experiments on various public datasets, covering\ndifferent spatio-temporal prediction tasks. The results consistently\ndemonstrate that our UrbanGPT, with its carefully designed architecture,\nconsistently outperforms state-of-the-art baselines. These findings highlight\nthe potential of building large language models for spatio-temporal learning,\nparticularly in zero-shot scenarios where labeled data is scarce.", "published": "2024-02-25 12:37:29", "link": "http://arxiv.org/abs/2403.00813v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic\n  Health Records", "abstract": "We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical\npredictions on Electronic Health Records (EHRs). RAM-EHR first collects\nmultiple knowledge sources, converts them into text format, and uses dense\nretrieval to obtain information related to medical concepts. This strategy\naddresses the difficulties associated with complex names for the concepts.\nRAM-EHR then augments the local EHR predictive model co-trained with\nconsistency regularization to capture complementary information from patient\nvisits and summarized knowledge. Experiments on two EHR datasets show the\nefficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in\nAUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized\nknowledge from RAM-EHR for clinical prediction tasks. The code will be\npublished at \\url{https://github.com/ritaranx/RAM-EHR}.", "published": "2024-02-25 23:10:20", "link": "http://arxiv.org/abs/2403.00815v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "q-bio.OT"], "primary_category": "cs.CL"}
{"title": "Evaluating Robustness of Generative Search Engine on Adversarial Factual\n  Questions", "abstract": "Generative search engines have the potential to transform how people seek\ninformation online, but generated responses from existing large language models\n(LLMs)-backed generative search engines may not always be accurate.\nNonetheless, retrieval-augmented generation exacerbates safety concerns, since\nadversaries may successfully evade the entire system by subtly manipulating the\nmost vulnerable part of a claim. To this end, we propose evaluating the\nrobustness of generative search engines in the realistic and high-risk setting,\nwhere adversaries have only black-box system access and seek to deceive the\nmodel into returning incorrect responses. Through a comprehensive human\nevaluation of various generative search engines, such as Bing Chat,\nPerplexityAI, and YouChat across diverse queries, we demonstrate the\neffectiveness of adversarial factual questions in inducing incorrect responses.\nMoreover, retrieval-augmented generation exhibits a higher susceptibility to\nfactual errors compared to LLMs without retrieval. These findings highlight the\npotential security risks of these systems and emphasize the need for rigorous\nevaluation before deployment.", "published": "2024-02-25 11:22:19", "link": "http://arxiv.org/abs/2403.12077v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "InstructEdit: Instruction-based Knowledge Editing for Large Language\n  Models", "abstract": "Knowledge editing for large language models can offer an efficient solution\nto alter a model's behavior without negatively impacting the overall\nperformance. However, the current approaches encounter issues with limited\ngeneralizability across tasks, necessitating one distinct editor for each task,\nsignificantly hindering the broader applications. To address this, we take the\nfirst step to analyze the multi-task generalization issue in knowledge editing.\nSpecifically, we develop an instruction-based editing technique, termed\nInstructEdit, which facilitates the editor's adaptation to various task\nperformances simultaneously using simple instructions. With only one unified\neditor for each LLM, we empirically demonstrate that InstructEdit can improve\nthe editor's control, leading to an average 14.86% increase in Reliability in\nmulti-task editing setting. Furthermore, experiments involving holdout unseen\ntask illustrate that InstructEdit consistently surpass previous strong\nbaselines. To further investigate the underlying mechanisms of\ninstruction-based knowledge editing, we analyze the principal components of the\nediting gradient directions, which unveils that instructions can help control\noptimization direction with stronger OOD generalization. Code and datasets are\navailable in https://github.com/zjunlp/EasyEdit.", "published": "2024-02-25 15:46:33", "link": "http://arxiv.org/abs/2402.16123v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatMusician: Understanding and Generating Music Intrinsically with LLM", "abstract": "While Large Language Models (LLMs) demonstrate impressive capabilities in\ntext generation, we find that their ability has yet to be generalized to music,\nhumanity's creative language. We introduce ChatMusician, an open-source LLM\nthat integrates intrinsic musical abilities. It is based on continual\npre-training and finetuning LLaMA2 on a text-compatible music representation,\nABC notation, and the music is treated as a second language. ChatMusician can\nunderstand and generate music with a pure text tokenizer without any external\nmulti-modal neural structures or tokenizers. Interestingly, endowing musical\nabilities does not harm language abilities, even achieving a slightly higher\nMMLU score. Our model is capable of composing well-structured, full-length\nmusic, conditioned on texts, chords, melodies, motifs, musical forms, etc,\nsurpassing GPT-4 baseline. On our meticulously curated college-level music\nunderstanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and\nGPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs\ncan be an excellent compressor for music, but there remains significant\nterritory to be conquered. We release our 4B token music-language corpora\nMusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.", "published": "2024-02-25 17:19:41", "link": "http://arxiv.org/abs/2402.16153v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring the Power of Pure Attention Mechanisms in Blind Room Parameter\n  Estimation", "abstract": "Dynamic parameterization of acoustic environments has drawn widespread\nattention in the field of audio processing. Precise representation of local\nroom acoustic characteristics is crucial when designing audio filters for\nvarious audio rendering applications. Key parameters in this context include\nreverberation time (RT60) and geometric room volume. In recent years, neural\nnetworks have been extensively applied in the task of blind room parameter\nestimation. However, there remains a question of whether pure attention\nmechanisms can achieve superior performance in this task. To address this\nissue, this study employs blind room parameter estimation based on monaural\nnoisy speech signals. Various model architectures are investigated, including a\nproposed attention-based model. This model is a convolution-free Audio\nSpectrogram Transformer, utilizing patch splitting, attention mechanisms, and\ncross-modality transfer learning from a pretrained Vision Transformer.\nExperimental results suggest that the proposed attention mechanism-based model,\nrelying purely on attention mechanisms without using convolution, exhibits\nsignificantly improved performance across various room parameter estimation\ntasks, especially with the help of dedicated pretraining and data augmentation\nschemes. Additionally, the model demonstrates more advantageous adaptability\nand robustness when handling variable-length audio inputs compared to existing\nmethods.", "published": "2024-02-25 06:32:21", "link": "http://arxiv.org/abs/2402.16003v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory", "abstract": "Live performances of music are always charming, with the unpredictability of\nimprovisation due to the dynamic between musicians and interactions with the\naudience. Jazz improvisation is a particularly noteworthy example for further\ninvestigation from a theoretical perspective. Here, we introduce a novel\nmathematical game theory model for jazz improvisation, providing a framework\nfor studying music theory and improvisational methodologies. We use\ncomputational modeling, mainly reinforcement learning, to explore diverse\nstochastic improvisational strategies and their paired performance on\nimprovisation. We find that the most effective strategy pair is a strategy that\nreacts to the most recent payoff (Stepwise Changes) with a reinforcement\nlearning strategy limited to notes in the given chord (Chord-Following\nReinforcement Learning). Conversely, a strategy that reacts to the partner's\nlast note and attempts to harmonize with it (Harmony Prediction) strategy pair\nyields the lowest non-control payoff and highest standard deviation, indicating\nthat picking notes based on immediate reactions to the partner player can yield\ninconsistent outcomes. On average, the Chord-Following Reinforcement Learning\nstrategy demonstrates the highest mean payoff, while Harmony Prediction\nexhibits the lowest. Our work lays the foundation for promising applications\nbeyond jazz: including the use of artificial intelligence (AI) models to\nextract data from audio clips to refine musical reward systems, and training\nmachine learning (ML) models on existing jazz solos to further refine\nstrategies within the game.", "published": "2024-02-25 16:46:15", "link": "http://arxiv.org/abs/2403.03224v1", "categories": ["physics.soc-ph", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "physics.soc-ph"}
