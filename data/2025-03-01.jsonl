{"title": "Abel's Functional Equation and Interrelations", "abstract": "Convex solutions $A,B,I,J$ of four Abel equations are numerically studied. We\ndo not know exact formulas for any of these functions, but conjecture that\n$A,B$ and $I,J$ are closely related. [Corrigendum at end.]", "published": "2025-03-01 18:11:18", "link": "http://arxiv.org/abs/2503.00579v3", "categories": ["math.CA", "cs.DM", "math.CO", "math.NT", "39B22 (Primary) 11B37, 26A18, 39-08, 41A60, 65D20 (Secondary)"], "primary_category": "math.CA"}
{"title": "On the time complexity of finding a well-spread perfect matching in bridgeless cubic graphs", "abstract": "We present an algorithm for finding a perfect matching in a\n$3$-edge-connected cubic graph that intersects every $3$-edge cut in exactly\none edge. Specifically, we propose an algorithm with a time complexity of $O(n\n\\log^4 n)$, which significantly improves upon the previously known\n$O(n^3)$-time algorithms for the same problem. The technique we use for the\nimprovement is efficient use of cactus model of 3-edge cuts. As an application,\nwe use our algorithm to compute embeddings of $3$-edge-connected cubic graphs\nwith limited number of singular edges (i.e., edges that are twice in the\nboundary of one face) in $O(n \\log^4 n)$ time; this application contributes to\nthe study of the well-known Cycle Double Cover conjecture.", "published": "2025-03-01 00:44:59", "link": "http://arxiv.org/abs/2503.00263v1", "categories": ["cs.DS", "cs.DM", "math.CO"], "primary_category": "cs.DS"}
{"title": "Understanding the Commodity Futures Term Structure Through Signatures", "abstract": "Signature methods have been widely and effectively used as a tool for feature\nextraction in statistical learning methods, notably in mathematical finance.\nThey lack, however, interpretability: in the general case, it is unclear why\nsignatures actually work. The present article aims to address this issue\ndirectly, by introducing and developing the concept of signature perturbations.\nIn particular, we construct a regular perturbation of the signature of the term\nstructure of log prices for various commodities, in terms of the convenience\nyield. Our perturbation expansion and rigorous convergence estimates help\nexplain the success of signature-based classification of commodities markets\naccording to their term structure, with the volatility of the convenience yield\nas the major discriminant.", "published": "2025-03-01 20:04:44", "link": "http://arxiv.org/abs/2503.00603v1", "categories": ["q-fin.MF", "math.PR", "q-fin.ST", "60L10, 62H30, 91G15, 91G20"], "primary_category": "q-fin.MF"}
{"title": "Ornstein-Uhlenbeck Process for Horse Race Betting: A Micro-Macro Analysis of Herding and Informed Bettors", "abstract": "We model the time evolution of single win odds in Japanese horse racing as a\nstochastic process, deriving an Ornstein--Uhlenbeck process by analyzing the\nprobability dynamics of vote shares and the empirical time series of odds\nmovements. Our framework incorporates two types of bettors: herders, who adjust\ntheir bets based on current odds, and fundamentalists, who wager based on a\nhorse's true winning probability. Using data from 3450 Japan Racing Association\nraces in 2008, we identify a microscopic probability rule governing individual\nbets and a mean-reverting macroscopic pattern in odds convergence. This\nstructure parallels financial markets, where traders' decisions are influenced\nby market fluctuations, and the interplay between herding and fundamentalist\nstrategies shapes price dynamics. These results highlight the broader\napplicability of our approach to non-equilibrium financial and betting markets,\nwhere mean-reverting dynamics emerge from simple behavioral interactions.", "published": "2025-03-01 07:24:53", "link": "http://arxiv.org/abs/2503.16470v1", "categories": ["physics.soc-ph", "q-fin.ST"], "primary_category": "physics.soc-ph"}
{"title": "Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of Bilateral Financial Exchanges, A bond market study", "abstract": "Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies.", "published": "2025-03-01 03:15:13", "link": "http://arxiv.org/abs/2503.00320v2", "categories": ["q-fin.TR", "cs.AI", "cs.MA"], "primary_category": "q-fin.TR"}
{"title": "How Deep is Love in LLMs' Hearts? Exploring Semantic Size in Human-like\n  Cognition", "abstract": "How human cognitive abilities are formed has long captivated researchers.\nHowever, a significant challenge lies in developing meaningful methods to\nmeasure these complex processes. With the advent of large language models\n(LLMs), which now rival human capabilities in various domains, we are presented\nwith a unique testbed to investigate human cognition through a new lens. Among\nthe many facets of cognition, one particularly crucial aspect is the concept of\nsemantic size, the perceived magnitude of both abstract and concrete words or\nconcepts. This study seeks to investigate whether LLMs exhibit similar\ntendencies in understanding semantic size, thereby providing insights into the\nunderlying mechanisms of human cognition. We begin by exploring metaphorical\nreasoning, comparing how LLMs and humans associate abstract words with concrete\nobjects of varying sizes. Next, we examine LLMs' internal representations to\nevaluate their alignment with human cognitive processes. Our findings reveal\nthat multi-modal training is crucial for LLMs to achieve more human-like\nunderstanding, suggesting that real-world, multi-modal experiences are\nsimilarly vital for human cognitive development. Lastly, we examine whether\nLLMs are influenced by attention-grabbing headlines with larger semantic sizes\nin a real-world web shopping scenario. The results show that multi-modal LLMs\nare more emotionally engaged in decision-making, but this also introduces\npotential biases, such as the risk of manipulation through clickbait headlines.\nUltimately, this study offers a novel perspective on how LLMs interpret and\ninternalize language, from the smallest concrete objects to the most profound\nabstract concepts like love. The insights gained not only improve our\nunderstanding of LLMs but also provide new avenues for exploring the cognitive\nabilities that define human intelligence.", "published": "2025-03-01 03:35:56", "link": "http://arxiv.org/abs/2503.00330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Multi-Stage BERT Fusion Framework with Dual Attention for\n  Enhanced Cyberbullying Detection in Social Media", "abstract": "Detecting and classifying cyberbullying in social media is hard because of\nthe complex nature of online language and the changing nature of content. This\nstudy presents a multi-stage BERT fusion framework. It uses hierarchical\nembeddings, dual attention mechanisms, and extra features to improve detection\nof cyberbullying content. The framework combines BERT embeddings with features\nlike sentiment and topic information. It uses self-attention and\ncross-attention to align features and has a hierarchical classification head\nfor multi-category classification. A dynamic loss balancing strategy helps\noptimize learning and improves accuracy, precision, recall, and F1-score. These\nresults show the model's strong performance and potential for broader use in\nanalyzing social media content.", "published": "2025-03-01 04:12:31", "link": "http://arxiv.org/abs/2503.00342v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Approaching the Limits to EFL Writing Enhancement with AI-generated Text\n  and Diverse Learners", "abstract": "Generative artificial intelligence (AI) chatbots, such as ChatGPT, are\nreshaping how English as a foreign language (EFL) students write since students\ncan compose texts by integrating their own words with AI-generated text. This\nstudy investigated how 59 Hong Kong secondary school students with varying\nlevels of academic achievement interacted with AI-generated text to compose a\nfeature article, exploring whether any interaction patterns benefited the\noverall quality of the article. Through content analysis, multiple linear\nregression and cluster analysis, we found the overall number of words --\nwhether AI- or human-generated -- is the main predictor of writing quality.\nHowever, the impact varies by students' competence to write independently, for\ninstance, by using their own words accurately and coherently to compose a text,\nand to follow specific interaction patterns with AI-generated text. Therefore,\nalthough composing texts with human words and AI-generated text may become\nprevalent in EFL writing classrooms, without educators' careful attention to\nEFL writing pedagogy and AI literacy, high-achieving students stand to benefit\nmore from using AI-generated text than low-achieving students.", "published": "2025-03-01 06:29:00", "link": "http://arxiv.org/abs/2503.00367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-Labeled Dataset for Indonesian Discourse: Examining Toxicity,\n  Polarization, and Demographics Information", "abstract": "Polarization is defined as divisive opinions held by two or more groups on\nsubstantive issues. As the world's third-largest democracy, Indonesia faces\ngrowing concerns about the interplay between political polarization and online\ntoxicity, which is often directed at vulnerable minority groups. Despite the\nimportance of this issue, previous NLP research has not fully explored the\nrelationship between toxicity and polarization. To bridge this gap, we present\na novel multi-label Indonesian dataset that incorporates toxicity,\npolarization, and annotator demographic information. Benchmarking this dataset\nusing BERT-base models and large language models (LLMs) shows that polarization\ninformation enhances toxicity classification, and vice versa. Furthermore,\nproviding demographic information significantly improves the performance of\npolarization classification.", "published": "2025-03-01 09:33:10", "link": "http://arxiv.org/abs/2503.00417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AILS-NTUA at SemEval-2025 Task 8: Language-to-Code prompting and Error\n  Fixing for Tabular Question Answering", "abstract": "In this paper, we present our submission to SemEval-2025 Task 8: Question\nAnswering over Tabular Data. This task, evaluated on the DataBench dataset,\nassesses Large Language Models' (LLMs) ability to answer natural language\nquestions over structured data while addressing topic diversity and table size\nlimitations in previous benchmarks. We propose a system that employs effective\nLLM prompting to translate natural language queries into executable code,\nenabling accurate responses, error correction, and interpretability. Our\napproach ranks first in both subtasks of the competition in the proprietary\nmodel category, significantly outperforming the organizer's baseline.", "published": "2025-03-01 10:24:42", "link": "http://arxiv.org/abs/2503.00435v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Figurative Archive: an open dataset and web-based application for the\n  study of metaphor", "abstract": "Research on metaphor has steadily increased over the last decades, as this\nphenomenon opens a window into a range of processes in language and cognition,\nfrom pragmatic inference to abstraction and embodied simulation. At the same\ntime, the demand for rigorously constructed and extensively normed experimental\nmaterials increased as well. Here, we present the Figurative Archive, an open\ndatabase of 997 metaphors in Italian enriched with rating and corpus-based\nmeasures (from familiarity to lexical frequency), derived by collecting stimuli\nused across 11 studies. It includes both everyday and literary metaphors,\nvarying in structure and semantic domains. Dataset validation comprised\ncorrelations between familiarity and other measures. The Figurative Archive has\nseveral aspects of novelty: it is increased in size compared to previous\nresources; it includes a novel measure of inclusiveness, to comply with current\nrecommendations for non-discriminatory language use; it is displayed in a\nweb-based interface, with features for a flexible and customized consultation.\nWe provide guidelines for using the Archive in future metaphor studies, in the\nspirit of open science.", "published": "2025-03-01 10:47:45", "link": "http://arxiv.org/abs/2503.00444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unstable Grounds for Beautiful Trees? Testing the Robustness of Concept\n  Translations in the Compilation of Multilingual Wordlists", "abstract": "Multilingual wordlists play a crucial role in comparative linguistics. While\nmany studies have been carried out to test the power of computational methods\nfor language subgrouping or divergence time estimation, few studies have put\nthe data upon which these studies are based to a rigorous test. Here, we\nconduct a first experiment that tests the robustness of concept translation as\nan integral part of the compilation of multilingual wordlists. Investigating\nthe variation in concept translations in independently compiled wordlists from\n10 dataset pairs covering 9 different language families, we find that on\naverage, only 83% of all translations yield the same word form, while identical\nforms in terms of phonetic transcriptions can only be found in 23% of all\ncases. Our findings can prove important when trying to assess the uncertainty\nof phylogenetic studies and the conclusions derived from them.", "published": "2025-03-01 12:16:45", "link": "http://arxiv.org/abs/2503.00464v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tutorial Proposal: Speculative Decoding for Efficient LLM Inference", "abstract": "This tutorial presents a comprehensive introduction to Speculative Decoding\n(SD), an advanced technique for LLM inference acceleration that has garnered\nsignificant research interest in recent years. SD is introduced as an\ninnovative decoding paradigm to mitigate the high inference latency stemming\nfrom autoregressive decoding in LLMs. At each decoding step, SD efficiently\ndrafts several future tokens and then verifies them in parallel. This approach,\nunlike traditional autoregressive decoding, facilitates the simultaneous\ndecoding of multiple tokens per step, thereby achieving promising 2x-4x\nspeedups in LLM inference while maintaining original distributions. This\ntutorial delves into the latest techniques in SD, including draft model\narchitectures and verification strategies. Additionally, it explores the\nacceleration potential and future research directions in this promising field.\nWe aim for this tutorial to elucidate the current research landscape and offer\ninsights for researchers interested in Speculative Decoding, ultimately\ncontributing to more efficient LLM inference.", "published": "2025-03-01 13:34:42", "link": "http://arxiv.org/abs/2503.00491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToolDial: Multi-turn Dialogue Generation Method for Tool-Augmented\n  Language Models", "abstract": "Tool-Augmented Language Models (TALMs) leverage external APIs to answer user\nqueries across various domains. However, existing benchmark datasets for TALM\nresearch often feature simplistic dialogues that do not reflect real-world\nscenarios, such as the need for models to ask clarifying questions or\nproactively call additional APIs when essential information is missing. To\naddress these limitations, we construct and release ToolDial, a dataset\ncomprising 11,111 multi-turn dialogues, with an average of 8.95 turns per\ndialogue, based on APIs from RapidAPI. ToolDial has two key characteristics.\nFirst, the dialogues incorporate 16 user and system actions (e.g., \"Request\",\n\"Clarify\", \"Fail inform\") to capture the rich dynamics of real-world\ninteractions. Second, we simulate dialogues where the system requests necessary\ninformation from the user based on API documentation and seeks additional APIs\nif the user fails to provide the required information. To facilitate this\nprocess, we introduce a method for generating an API graph that represents\ninput and output compatibility between APIs. Using ToolDial, we evaluate a\nsuite of language models on their ability to predict correct actions and\nextract input parameter values for API calls from the dialogue history. Modern\nlanguage models achieve accuracy scores below 70%, indicating substantial room\nfor improvement. We release our dataset and code at\nhttps://github.com/holi-lab/ToolDial.", "published": "2025-03-01 17:23:51", "link": "http://arxiv.org/abs/2503.00564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Multi-Objective Preference Alignment with Online DPO", "abstract": "Multi-objective preference alignment of large language models (LLMs) is\ncritical for developing AI systems that are more configurable, personalizable,\nhelpful, and safe. However, optimizing model outputs to satisfy diverse\nobjectives with variable weights at inference time for truly personalized\nmodels presents a significant challenge. Existing approaches are either\ncomputationally expensive to train or do not sufficiently steer model\nbehaviors. This paper introduces the Multi-Objective Online DPO (MO-ODPO)\nalgorithm, designed to robustly and efficiently align model behaviors with\nmultiple, potentially conflicting human preferences. Our approach incorporates\na prompt conditioning mechanism, allowing us to train a single\npreference-conditional policy, that can adapt to new preference combinations at\ninference. Experiments on two popular benchmarks show that MO-ODPO\nPareto-dominates existing baselines while providing excellent inference-time\nsteerability between diverse objectives.", "published": "2025-03-01 02:01:49", "link": "http://arxiv.org/abs/2503.00295v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unlocking Efficient, Scalable, and Continual Knowledge Editing with\n  Basis-Level Representation Fine-Tuning", "abstract": "Large language models (LLMs) have achieved remarkable performance on various\nnatural language tasks. However, they are trained on static corpora and their\nknowledge can become outdated quickly in the fast-changing world. This\nmotivates the development of knowledge editing methods designed to update\ncertain knowledge in LLMs without changing unrelated others. To make selective\nedits, previous efforts often sought to update a small amount of parameters in\nsome specific layer(s) of a LLM. Nonetheless, in challenging scenarios, they\nstill fall short in making successful edits while preserving knowledge\nirrelevant to the updates simultaneously, resulting in a notable\nediting-locality trade-off. In this work, we question if the trade-offs are\ncaused by the fact that parameter-based updates have a global effect, i.e.,\nedited parameters affect all inputs indiscriminately. In light of this, we\nexplore the feasibility of representation fine-tuning, which applied some\nlinear update to a few representations in a learned subspace, for knowledge\nediting. While being effective to enhance an LLM's general ability as\ndemonstrated in the previous work, we theoretically show that this linear\nupdate imposes a tension in editing-locality trade-off. Subsequently, BaFT is\nproposed to break the linearity. BaFT computes a weight for each basis that\nspans a dimension of the subspace based on the input representation. This\ninput-dependent weighting mechanism allows BaFT to manage different types of\nknowledge in an adaptive way, thereby achieving a better editing-locality\ntrade-off. Experiments on three LLMs with five editing benchmarks in diverse\nscenarios show the superiority of our method.", "published": "2025-03-01 02:34:44", "link": "http://arxiv.org/abs/2503.00306v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "More of the Same: Persistent Representational Harms Under Increased\n  Representation", "abstract": "To recognize and mitigate the harms of generative AI systems, it is crucial\nto consider who is represented in the outputs of generative AI systems and how\npeople are represented. A critical gap emerges when naively improving who is\nrepresented, as this does not imply bias mitigation efforts have been applied\nto address how people are represented. We critically examined this by\ninvestigating gender representation in occupation across state-of-the-art large\nlanguage models. We first show evidence suggesting that over time there have\nbeen interventions to models altering the resulting gender distribution, and we\nfind that women are more represented than men when models are prompted to\ngenerate biographies or personas. We then demonstrate that representational\nbiases persist in how different genders are represented by examining\nstatistically significant word differences across genders. This results in a\nproliferation of representational harms, stereotypes, and neoliberalism ideals\nthat, despite existing interventions to increase female representation,\nreinforce existing systems of oppression.", "published": "2025-03-01 03:45:35", "link": "http://arxiv.org/abs/2503.00333v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "U-NIAH: Unified RAG and LLM Evaluation for Long Context\n  Needle-In-A-Haystack", "abstract": "Recent advancements in Large Language Models (LLMs) have expanded their\ncontext windows to unprecedented lengths, sparking debates about the necessity\nof Retrieval-Augmented Generation (RAG). To address the fragmented evaluation\nparadigms and limited cases in existing Needle-in-a-Haystack (NIAH), this paper\nintroduces U-NIAH, a unified framework that systematically compares LLMs and\nRAG methods in controlled long context settings. Our framework extends beyond\ntraditional NIAH by incorporating multi-needle, long-needle, and\nneedle-in-needle configurations, along with different retrieval settings, while\nleveraging the synthetic Starlight Academy dataset-a fictional magical\nuniverse-to eliminate biases from pre-trained knowledge. Through extensive\nexperiments, we investigate three research questions: (1) performance\ntrade-offs between LLMs and RAG, (2) error patterns in RAG, and (3) RAG's\nlimitations in complex settings. Our findings show that RAG significantly\nenhances smaller LLMs by mitigating the \"lost-in-the-middle\" effect and\nimproving robustness, achieving an 82.58% win-rate over LLMs. However, we\nobserve that retrieval noise and reverse chunk ordering degrade performance,\nwhile surprisingly, advanced reasoning LLMs exhibit reduced RAG compatibility\ndue to sensitivity to semantic distractors. We identify typical error patterns\nincluding omission due to noise, hallucination under high noise critical\ncondition, and self-doubt behaviors. Our work not only highlights the\ncomplementary roles of RAG and LLMs, but also provides actionable insights for\noptimizing deployments. Code: https://github.com/Tongji-KGLLM/U-NIAH.", "published": "2025-03-01 05:05:24", "link": "http://arxiv.org/abs/2503.00353v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Structured Reasoning for Fairness: A Multi-Agent Approach to Bias\n  Detection in Textual Data", "abstract": "From disinformation spread by AI chatbots to AI recommendations that\ninadvertently reinforce stereotypes, textual bias poses a significant challenge\nto the trustworthiness of large language models (LLMs). In this paper, we\npropose a multi-agent framework that systematically identifies biases by\ndisentangling each statement as fact or opinion, assigning a bias intensity\nscore, and providing concise, factual justifications. Evaluated on 1,500\nsamples from the WikiNPOV dataset, the framework achieves 84.9%\naccuracy$\\unicode{x2014}$an improvement of 13.0% over the zero-shot\nbaseline$\\unicode{x2014}$demonstrating the efficacy of explicitly modeling fact\nversus opinion prior to quantifying bias intensity. By combining enhanced\ndetection accuracy with interpretable explanations, this approach sets a\nfoundation for promoting fairness and accountability in modern language models.", "published": "2025-03-01 05:27:54", "link": "http://arxiv.org/abs/2503.00355v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BERT-based model for Vietnamese Fact Verification Dataset", "abstract": "The rapid advancement of information and communication technology has\nfacilitated easier access to information. However, this progress has also\nnecessitated more stringent verification measures to ensure the accuracy of\ninformation, particularly within the context of Vietnam. This paper introduces\nan approach to address the challenges of Fact Verification using the Vietnamese\ndataset by integrating both sentence selection and classification modules into\na unified network architecture. The proposed approach leverages the power of\nlarge language models by utilizing pre-trained PhoBERT and XLM-RoBERTa as the\nbackbone of the network. The proposed model was trained on a Vietnamese\ndataset, named ISE-DSC01, and demonstrated superior performance compared to the\nbaseline model across all three metrics. Notably, we achieved a Strict Accuracy\nlevel of 75.11\\%, indicating a remarkable 28.83\\% improvement over the baseline\nmodel.", "published": "2025-03-01 05:31:04", "link": "http://arxiv.org/abs/2503.00356v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rehearse With User: Personalized Opinion Summarization via Role-Playing\n  based on Large Language Models", "abstract": "Personalized opinion summarization is crucial as it considers individual user\ninterests while generating product summaries. Recent studies show that although\nlarge language models demonstrate powerful text summarization and evaluation\ncapabilities without the need for training data, they face difficulties in\npersonalized tasks involving long texts. To address this, \\textbf{Rehearsal}, a\npersonalized opinion summarization framework via LLMs-based role-playing is\nproposed. Having the model act as the user, the model can better understand the\nuser's personalized needs. Additionally, a role-playing supervisor and practice\nprocess are introduced to improve the role-playing ability of the LLMs, leading\nto a better expression of user needs. Furthermore, through suggestions from\nvirtual users, the summary generation is intervened, ensuring that the\ngenerated summary includes information of interest to the user, thus achieving\npersonalized summary generation. Experiment results demonstrate that our method\ncan effectively improve the level of personalization in large model-generated\nsummaries.", "published": "2025-03-01 11:05:01", "link": "http://arxiv.org/abs/2503.00449v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Qilin: A Multimodal Information Retrieval Dataset with APP-level User\n  Sessions", "abstract": "User-generated content (UGC) communities, especially those featuring\nmultimodal content, improve user experiences by integrating visual and textual\ninformation into results (or items). The challenge of improving user\nexperiences in complex systems with search and recommendation (S\\&R) services\nhas drawn significant attention from both academia and industry these years.\nHowever, the lack of high-quality datasets has limited the research progress on\nmultimodal S\\&R. To address the growing need for developing better S\\&R\nservices, we present a novel multimodal information retrieval dataset in this\npaper, namely Qilin. The dataset is collected from Xiaohongshu, a popular\nsocial platform with over 300 million monthly active users and an average\nsearch penetration rate of over 70\\%. In contrast to existing datasets,\n\\textsf{Qilin} offers a comprehensive collection of user sessions with\nheterogeneous results like image-text notes, video notes, commercial notes, and\ndirect answers, facilitating the development of advanced multimodal neural\nretrieval models across diverse task settings. To better model user\nsatisfaction and support the analysis of heterogeneous user behaviors, we also\ncollect extensive APP-level contextual signals and genuine user feedback.\nNotably, Qilin contains user-favored answers and their referred results for\nsearch requests triggering the Deep Query Answering (DQA) module. This allows\nnot only the training \\& evaluation of a Retrieval-augmented Generation (RAG)\npipeline, but also the exploration of how such a module would affect users'\nsearch behavior. Through comprehensive analysis and experiments, we provide\ninteresting findings and insights for further improving S\\&R systems. We hope\nthat \\textsf{Qilin} will significantly contribute to the advancement of\nmultimodal content platforms with S\\&R services in the future.", "published": "2025-03-01 14:15:00", "link": "http://arxiv.org/abs/2503.00501v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Instructor-Worker Large Language Model System for Policy Recommendation:\n  a Case Study on Air Quality Analysis of the January 2025 Los Angeles\n  Wildfires", "abstract": "The Los Angeles wildfires of January 2025 caused more than 250 billion\ndollars in damage and lasted for nearly an entire month before containment.\nFollowing our previous work, the Digital Twin Building, we modify and leverage\nthe multi-agent large language model framework as well as the cloud-mapping\nintegration to study the air quality during the Los Angeles wildfires. Recent\nadvances in large language models have allowed for out-of-the-box automated\nlarge-scale data analysis. We use a multi-agent large language system comprised\nof an Instructor agent and Worker agents. Upon receiving the users'\ninstructions, the Instructor agent retrieves the data from the cloud platform\nand produces instruction prompts to the Worker agents. The Worker agents then\nanalyze the data and provide summaries. The summaries are finally input back\ninto the Instructor agent, which then provides the final data analysis. We test\nthis system's capability for data-based policy recommendation by assessing our\nInstructor-Worker LLM system's health recommendations based on air quality\nduring the Los Angeles wildfires.", "published": "2025-03-01 17:29:26", "link": "http://arxiv.org/abs/2503.00566v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LoR2C : Low-Rank Residual Connection Adaptation for Parameter-Efficient\n  Fine-Tuning", "abstract": "In recent years, pretrained large language models have demonstrated\noutstanding performance across various natural language processing tasks.\nHowever, full-parameter fine-tuning methods require adjusting all model\nparameters, leading to immense computational resource demands. Although\nparameter-efficient fine-tuning methods like LoRA have significantly reduced\nthe number of parameters, they still face challenges such as gradient vanishing\nand the potential for further parameter reduction. To address these issues,\nthis paper proposes a novel parameter-efficient fine-tuning method called LoR2C\n(Low-Rank Residual Connection Adaptation). LoR2C introduces residual\nconnections with low-rank matrices within the model layers, which not only\nreduces the number of fine-tuning parameters but also effectively alleviates\nthe gradient vanishing problem. Additionally, this paper presents three\noptimization variants of LoR2C: ShareLoR2C, MergeLoR2C, and InjectLoR2C. These\nvariants further improve parameter efficiency and model performance through\nparameter sharing, module merging, and injection mechanisms, respectively.\nExperimental results on multiple natural language understanding and natural\nlanguage generation tasks demonstrate that LoR2C and its optimized variants\nsignificantly reduce parameter overhead while maintaining or even improving\nperformance, outperforming existing mainstream parameter-efficient fine-tuning\nmethods.Our code is publicly available at https://github.com/Oblivioniss/LoR2C.", "published": "2025-03-01 17:42:57", "link": "http://arxiv.org/abs/2503.00572v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Keyphrase Generation: Investigating Specialized Instructions\n  and Multi-Sample Aggregation on Large Language Models", "abstract": "Keyphrases are the essential topical phrases that summarize a document.\nKeyphrase generation is a long-standing NLP task for automatically generating\nkeyphrases for a given document. While the task has been comprehensively\nexplored in the past via various models, only a few works perform some\npreliminary analysis of Large Language Models (LLMs) for the task. Given the\nimpact of LLMs in the field of NLP, it is important to conduct a more thorough\nexamination of their potential for keyphrase generation. In this paper, we\nattempt to meet this demand with our research agenda. Specifically, we focus on\nthe zero-shot capabilities of open-source instruction-tuned LLMs (Phi-3,\nLlama-3) and the closed-source GPT-4o for this task. We systematically\ninvestigate the effect of providing task-relevant specialized instructions in\nthe prompt. Moreover, we design task-specific counterparts to\nself-consistency-style strategies for LLMs and show significant benefits from\nour proposals over the baselines.", "published": "2025-03-01 19:38:57", "link": "http://arxiv.org/abs/2503.00597v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An evaluation of DeepSeek Models in Biomedical Natural Language\n  Processing", "abstract": "The advancement of Large Language Models (LLMs) has significantly impacted\nbiomedical Natural Language Processing (NLP), enhancing tasks such as named\nentity recognition, relation extraction, event extraction, and text\nclassification. In this context, the DeepSeek series of models have shown\npromising potential in general NLP tasks, yet their capabilities in the\nbiomedical domain remain underexplored. This study evaluates multiple DeepSeek\nmodels (Distilled-DeepSeek-R1 series and Deepseek-LLMs) across four key\nbiomedical NLP tasks using 12 datasets, benchmarking them against\nstate-of-the-art alternatives (Llama3-8B, Qwen2.5-7B, Mistral-7B, Phi-4-14B,\nGemma-2-9B). Our results reveal that while DeepSeek models perform\ncompetitively in named entity recognition and text classification, challenges\npersist in event and relation extraction due to precision-recall trade-offs. We\nprovide task-specific model recommendations and highlight future research\ndirections. This evaluation underscores the strengths and limitations of\nDeepSeek models in biomedical NLP, guiding their future deployment and\noptimization.", "published": "2025-03-01 21:26:29", "link": "http://arxiv.org/abs/2503.00624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conceptual Contrastive Edits in Textual and Vision-Language Retrieval", "abstract": "As deep learning models grow in complexity, achieving model-agnostic\ninterpretability becomes increasingly vital. In this work, we employ post-hoc\nconceptual contrastive edits to expose noteworthy patterns and biases imprinted\nin representations of retrieval models. We systematically design optimal and\ncontrollable contrastive interventions targeting various parts of speech, and\neffectively apply them to explain both linguistic and visiolinguistic\npre-trained models in a black-box manner. Additionally, we introduce a novel\nmetric to assess the per-word impact of contrastive interventions on model\noutcomes, providing a comprehensive evaluation of each intervention's\neffectiveness.", "published": "2025-03-01 10:14:28", "link": "http://arxiv.org/abs/2503.01914v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-linguistic disagreement as a conflict of semantic alignment norms\n  in multilingual AI~Linguistic Diversity as a Problem for Philosophy,\n  Cognitive Science, and AI~", "abstract": "Multilingual large language models (LLMs) face an often-overlooked challenge\nstemming from intrinsic semantic differences across languages. Linguistic\ndivergence can sometimes lead to cross-linguistic disagreements--disagreements\npurely due to semantic differences about a relevant concept. This paper\nidentifies such disagreements as conflicts between two fundamental alignment\nnorms in multilingual LLMs: cross-linguistic consistency (CL-consistency),\nwhich seeks universal concepts across languages, and consistency with folk\njudgments (Folk-consistency), which respects language-specific semantic norms.\nThrough examining responses of conversational multilingual AIs in English and\nJapanese with the cases used in philosophy (cases of knowledge-how\nattributions), this study demonstrates that even state-of-the-art LLMs provide\ndivergent and internally inconsistent responses. Such findings reveal a novel\nqualitative limitation in crosslingual knowledge transfer, or conceptual\ncrosslingual knowledge barriers, challenging the assumption that universal\nrepresentations and cross-linguistic transfer capabilities are inherently\ndesirable. Moreover, they reveal conflicts of alignment policies of their\ndevelopers, highlighting critical normative questions for LLM researchers and\ndevelopers. The implications extend beyond technical alignment challenges,\nraising normative, moral-political, and metaphysical questions about the ideals\nunderlying AI development--questions that are shared with philosophers and\ncognitive scientists but for which no one yet has definitive answers, inviting\na multidisciplinary approach to balance the practical benefits of\ncross-linguistic consistency and respect for linguistic diversity.", "published": "2025-03-01 03:31:40", "link": "http://arxiv.org/abs/2503.04792v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentence-level Reward Model can Generalize Better for Aligning LLM from\n  Human Preference", "abstract": "Learning reward models from human preference datasets and subsequently\noptimizing language models via reinforcement learning has emerged as a\nfundamental paradigm for aligning LLMs with human preferences. The performance\nof the reward model plays a crucial role in the effectiveness of alignment.\nPrevious reward models operate at a coarse-grained level, requiring the\ngeneration of a complete response to obtain a reward value. The sparse reward\nmay present challenges for downstream reinforcement learning. While recent\nefforts have attempted to learn token-level reward models, the lack of explicit\nsemantic information makes it difficult to model the credit of every individual\ntoken. In this paper, we propose assigning scores to every sentence,\nintroducing an intermediate-grained reward model. By segmenting the complete\nresponse into sentences and applying differential operations to reward output\nat the start and end positions of each sentence, we can effectively model the\nrewards of sentences. Moreover, a novel attention mechanism is introduced to\naggregate the scores of all sentences into a response-level score, which allows\nit to be trained using the Bradley-Terry model. On common benchmarks, our\nmethod outperforms the response-level reward model by 2.7% on RewardBench (for\nreward modeling evaluation) and surpasses all baselines on AlpacaEval (for\nalignment evaluation).", "published": "2025-03-01 14:11:04", "link": "http://arxiv.org/abs/2503.04793v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Decoupling Content and Expression: Two-Dimensional Detection of\n  AI-Generated Text", "abstract": "The wide usage of LLMs raises critical requirements on detecting AI\nparticipation in texts. Existing studies investigate these detections in\nscattered contexts, leaving a systematic and unified approach unexplored. In\nthis paper, we present HART, a hierarchical framework of AI risk levels, each\ncorresponding to a detection task. To address these tasks, we propose a novel\n2D Detection Method, decoupling a text into content and language expression.\nOur findings show that content is resistant to surface-level changes, which can\nserve as a key feature for detection. Experiments demonstrate that 2D method\nsignificantly outperforms existing detectors, achieving an AUROC improvement\nfrom 0.705 to 0.849 for level-2 detection and from 0.807 to 0.886 for RAID. We\nrelease our data and code at https://github.com/baoguangsheng/truth-mirror.", "published": "2025-03-01 00:19:13", "link": "http://arxiv.org/abs/2503.00258v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Reducing Large Language Model Safety Risks in Women's Health using\n  Semantic Entropy", "abstract": "Large language models (LLMs) hold substantial promise for clinical decision\nsupport. However, their widespread adoption in medicine, particularly in\nhealthcare, is hindered by their propensity to generate false or misleading\noutputs, known as hallucinations. In high-stakes domains such as women's health\n(obstetrics & gynaecology), where errors in clinical reasoning can have\nprofound consequences for maternal and neonatal outcomes, ensuring the\nreliability of AI-generated responses is critical. Traditional methods for\nquantifying uncertainty, such as perplexity, fail to capture meaning-level\ninconsistencies that lead to misinformation. Here, we evaluate semantic entropy\n(SE), a novel uncertainty metric that assesses meaning-level variation, to\ndetect hallucinations in AI-generated medical content. Using a clinically\nvalidated dataset derived from UK RCOG MRCOG examinations, we compared SE with\nperplexity in identifying uncertain responses. SE demonstrated superior\nperformance, achieving an AUROC of 0.76 (95% CI: 0.75-0.78), compared to 0.62\n(0.60-0.65) for perplexity. Clinical expert validation further confirmed its\neffectiveness, with SE achieving near-perfect uncertainty discrimination\n(AUROC: 0.97). While semantic clustering was successful in only 30% of cases,\nSE remains a valuable tool for improving AI safety in women's health. These\nfindings suggest that SE could enable more reliable AI integration into\nclinical practice, particularly in resource-limited settings where LLMs could\naugment care. This study highlights the potential of SE as a key safeguard in\nthe responsible deployment of AI-driven tools in women's health, leading to\nsafer and more effective digital health interventions.", "published": "2025-03-01 00:57:52", "link": "http://arxiv.org/abs/2503.00269v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "NeuroLit Navigator: A Neurosymbolic Approach to Scholarly Article\n  Searches for Systematic Reviews", "abstract": "The introduction of Large Language Models (LLMs) has significantly impacted\nvarious fields, including education, for example, by enabling the creation of\npersonalized learning materials. However, their use in Systematic Reviews (SRs)\nreveals limitations such as restricted access to specialized vocabularies, lack\nof domain-specific reasoning, and a tendency to generate inaccurate\ninformation. Existing SR tools often rely on traditional NLP methods and fail\nto address these issues adequately. To overcome these challenges, we developed\nthe ``NeuroLit Navigator,'' a system that combines domain-specific LLMs with\nstructured knowledge sources like Medical Subject Headings (MeSH) and the\nUnified Medical Language System (UMLS). This integration enhances query\nformulation, expands search vocabularies, and deepens search scopes, enabling\nmore precise searches. Deployed in multiple universities and tested by over a\ndozen librarians, the NeuroLit Navigator has reduced the time required for\ninitial literature searches by 90\\%. Despite this efficiency, the initial set\nof articles retrieved can vary in relevance and quality. Nonetheless, the\nsystem has greatly improved the reproducibility of search results,\ndemonstrating its potential to support librarians in the SR process.", "published": "2025-03-01 01:11:24", "link": "http://arxiv.org/abs/2503.00278v1", "categories": ["cs.CY", "cs.CL", "cs.IR"], "primary_category": "cs.CY"}
{"title": "Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with\n  Query-Oriented Pivot Tasks", "abstract": "Perception-enhanced pre-training, particularly through grounding techniques,\nis widely adopted to enhance the performance of graphical user interface (GUI)\nagents. However, in resource-constrained scenarios, the format discrepancy\nbetween coordinate-oriented grounding and action-oriented reasoning limits the\neffectiveness of grounding for reasoning tasks. To address this challenge, we\npropose a query-oriented pivot approach called query inference, which serves as\na bridge between GUI grounding and reasoning. By inferring potential user\nqueries from a screenshot and its associated element coordinates, query\ninference improves the understanding of coordinates while aligning more closely\nwith reasoning tasks. Experimental results show that query inference\noutperforms previous grounding techniques under the same training data scale.\nNotably, query inference achieves comparable or even better performance to\nlarge-scale grounding-enhanced OS-Atlas with less than 0.1% of training data.\nFurthermore, we explore the impact of reasoning formats and demonstrate that\nintegrating additional semantic information into the input further boosts\nreasoning performance. The code is publicly available at\nhttps://github.com/ZrW00/GUIPivot.", "published": "2025-03-01 08:29:59", "link": "http://arxiv.org/abs/2503.00401v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Embracing Diversity: A Multi-Perspective Approach with Soft Labels", "abstract": "Prior studies show that adopting the annotation diversity shaped by different\nbackgrounds and life experiences and incorporating them into the model\nlearning, i.e. multi-perspective approach, contribute to the development of\nmore responsible models. Thus, in this paper we propose a new framework for\ndesigning and further evaluating perspective-aware models on stance detection\ntask,in which multiple annotators assign stances based on a controversial\ntopic. We also share a new dataset established through obtaining both human and\nLLM annotations. Results show that the multi-perspective approach yields better\nclassification performance (higher F1-scores), outperforming the traditional\napproaches that use a single ground-truth, while displaying lower model\nconfidence scores, probably due to the high level of subjectivity of the stance\ndetection task.", "published": "2025-03-01 13:33:38", "link": "http://arxiv.org/abs/2503.00489v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech\n  Enhancement", "abstract": "Recent advancements in language models (LMs) have demonstrated strong\ncapabilities in semantic understanding and contextual modeling, which have\nflourished in generative speech enhancement (SE). However, many LM-based SE\napproaches primarily focus on semantic information, often neglecting the\ncritical role of acoustic information, which leads to acoustic inconsistency\nafter enhancement and limited generalization across diverse SE tasks. In this\npaper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes\ngeneralization capabilities for speech enhancement. LLaSE-G1 offers the\nfollowing key contributions: First, to mitigate acoustic inconsistency,\nLLaSE-G1 employs continuous representations from WavLM as input and predicts\nspeech tokens from X-Codec2, maximizing acoustic preservation. Second, to\npromote generalization capability, LLaSE-G1 introduces dual-channel inputs and\noutputs, unifying multiple SE tasks without requiring task-specific IDs. Third,\nLLaSE-G1 outperforms prior task-specific discriminative and generative SE\nmodels, demonstrating scaling effects at test time and emerging capabilities\nfor unseen SE tasks. Additionally, we release our code and models to support\nfurther research in this area.", "published": "2025-03-01 13:44:50", "link": "http://arxiv.org/abs/2503.00493v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Distributionally Robust Reinforcement Learning with Human Feedback", "abstract": "Reinforcement learning from human feedback (RLHF) has evolved to be one of\nthe main methods for fine-tuning large language models (LLMs). However,\nexisting RLHF methods are non-robust, and their performance deteriorates if the\ndownstream task differs significantly from the preference dataset used in\nfine-tuning. In order to mitigate this problem, we introduce a distributionally\nrobust RLHF for fine-tuning LLMs. In particular, our goal is to ensure that a\nfine-tuned model retains its performance even when the distribution of prompts\nsignificantly differs from the distribution encountered during fine-tuning. We\nformulate distributionally robust optimization (DRO) version of two popular\nfine-tuning methods -- (1) reward-based RLHF and (2) reward-free DPO (direct\npreference optimization). We propose a minibatch gradient descent based\nalgorithms for both of them, and theoretically prove convergence guarantees for\nthe algorithms. Subsequently, we evaluate our algorithms on an\nout-of-distribution (OOD) task by first training the model on the\nUnified-Feedback dataset and evaluating its performance on two different\ndatasets. The experimental results show that our robust training improves the\naccuracy of the learned reward models on average, and markedly on some tasks,\nsuch as reasoning. Furthermore, we show that the robust versions of policy\noptimization methods, similarly improve performance on OOD tasks.", "published": "2025-03-01 15:43:39", "link": "http://arxiv.org/abs/2503.00539v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge", "abstract": "This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge\nevaluation regime, where the adversary controls both the candidate and\nevaluator model. The backdoored evaluator victimizes benign users by unfairly\nassigning inflated scores to adversary. A trivial single token backdoor\npoisoning 1% of the evaluator training data triples the adversary's score with\nrespect to their legitimate score. We systematically categorize levels of data\naccess corresponding to three real-world settings, (1) web poisoning, (2)\nmalicious annotator, and (3) weight poisoning. These regimes reflect a weak to\nstrong escalation of data access that highly correlates with attack severity.\nUnder the weakest assumptions - web poisoning (1), the adversary still induces\na 20% score inflation. Likewise, in the (3) weight poisoning regime, the\nstronger assumptions enable the adversary to inflate their scores from 1.5/5 to\n4.9/5. The backdoor threat generalizes across different evaluator\narchitectures, trigger designs, evaluation tasks, and poisoning rates. By\npoisoning 10% of the evaluator training data, we control toxicity judges\n(Guardrails) to misclassify toxic prompts as non-toxic 89% of the time, and\ndocument reranker judges in RAG to rank the poisoned document first 97% of the\ntime. LLM-as-a-Judge is uniquely positioned at the intersection of ethics and\ntechnology, where social implications of mislead model selection and evaluation\nconstrain the available defensive tools. Amidst these challenges, model merging\nemerges as a principled tool to offset the backdoor, reducing ASR to near 0%\nwhilst maintaining SOTA performance. Model merging's low computational cost and\nconvenient integration into the current LLM Judge training pipeline position it\nas a promising avenue for backdoor mitigation in the LLM-as-a-Judge setting.", "published": "2025-03-01 19:35:01", "link": "http://arxiv.org/abs/2503.00596v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented\n  Data Processing Systems", "abstract": "The emergence of AI-augmented Data Processing Systems (DPSs) has introduced\npowerful semantic operators that extend traditional data management\ncapabilities with LLM-based processing. However, these systems face fundamental\nreliability (a.k.a. trust) challenges, as LLMs can generate erroneous outputs,\nlimiting their adoption in critical domains. Existing approaches to LLM\nconstraints--ranging from user-defined functions to constrained decoding--are\nfragmented, imperative, and lack semantics-aware integration into query\nexecution. To address this gap, we introduce Semantic Integrity Constraints\n(SICs), a novel declarative abstraction that extends traditional database\nintegrity constraints to govern and optimize semantic operators within DPSs.\nSICs integrate seamlessly into the relational model, allowing users to specify\ncommon classes of constraints (e.g., grounding and soundness) while enabling\nquery-aware enforcement and optimization strategies.\n  In this paper, we present the core design of SICs, describe their formal\nintegration into query execution, and detail our conception of grounding\nconstraints, a key SIC class that ensures factual consistency of generated\noutputs. In addition, we explore novel enforcement mechanisms, combining\nproactive (constrained decoding) and reactive (validation and recovery)\ntechniques to optimize efficiency and reliability. Our work establishes SICs as\na foundational framework for trustworthy, high-performance AI-augmented data\nprocessing, paving the way for future research in constraint-driven\noptimizations, adaptive enforcement, and enterprise-scale deployments.", "published": "2025-03-01 19:59:25", "link": "http://arxiv.org/abs/2503.00600v1", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Statistical Mechanics of Semantic Compression", "abstract": "The basic problem of semantic compression is to minimize the length of a\nmessage while preserving its meaning. This differs from classical notions of\ncompression in that the distortion is not measured directly at the level of\nbits, but rather in an abstract semantic space. In order to make this precise,\nwe take inspiration from cognitive neuroscience and machine learning and model\nsemantic space as a continuous Euclidean vector space. In such a space, stimuli\nlike speech, images, or even ideas, are mapped to high-dimensional real\nvectors, and the location of these embeddings determines their meaning relative\nto other embeddings. This suggests that a natural metric for semantic\nsimilarity is just the Euclidean distance, which is what we use in this work.\nWe map the optimization problem of determining the minimal-length,\nmeaning-preserving message to a spin glass Hamiltonian and solve the resulting\nstatistical mechanics problem using replica theory. We map out the replica\nsymmetric phase diagram, identifying distinct phases of semantic compression: a\nfirst-order transition occurs between lossy and lossless compression, whereas a\ncontinuous crossover is seen from extractive to abstractive compression. We\nconclude by showing numerical simulations of compressions obtained by simulated\nannealing and greedy algorithms, and argue that while the problem of finding a\nmeaning-preserving compression is computationally hard in the worst case, there\nexist efficient algorithms which achieve near optimal performance in the\ntypical case.", "published": "2025-03-01 20:38:16", "link": "http://arxiv.org/abs/2503.00612v1", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.CL", "q-bio.NC"], "primary_category": "cond-mat.dis-nn"}
{"title": "How to Steer LLM Latents for Hallucination Detection?", "abstract": "Hallucinations in LLMs pose a significant concern to their safe deployment in\nreal-world applications. Recent approaches have leveraged the latent space of\nLLMs for hallucination detection, but their embeddings, optimized for\nlinguistic coherence rather than factual accuracy, often fail to clearly\nseparate truthful and hallucinated content. To this end, we propose the\nTruthfulness Separator Vector (TSV), a lightweight and flexible steering vector\nthat reshapes the LLM's representation space during inference to enhance the\nseparation between truthful and hallucinated outputs, without altering model\nparameters. Our two-stage framework first trains TSV on a small set of labeled\nexemplars to form compact and well-separated clusters. It then augments the\nexemplar set with unlabeled LLM generations, employing an optimal\ntransport-based algorithm for pseudo-labeling combined with a confidence-based\nfiltering process. Extensive experiments demonstrate that TSV achieves\nstate-of-the-art performance with minimal labeled data, exhibiting strong\ngeneralization across datasets and providing a practical solution for\nreal-world LLM applications.", "published": "2025-03-01 19:19:34", "link": "http://arxiv.org/abs/2503.01917v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MedSimAI: Simulation and Formative Feedback Generation to Enhance\n  Deliberate Practice in Medical Education", "abstract": "Medical education faces challenges in scalability, accessibility, and\nconsistency, particularly in clinical skills training for physician-patient\ncommunication. Traditional simulation-based learning, while effective, is\nresource-intensive, difficult to schedule, and often highly variable in\nfeedback quality. Through a collaboration between AI, learning science, and\nmedical education experts, we co-developed MedSimAI, an AI-powered simulation\nplatform that enables deliberate practice, self-regulated learning (SRL), and\nautomated assessment through interactive patient encounters. Leveraging large\nlanguage models (LLMs), MedSimAI generates realistic clinical interactions and\nprovides immediate, structured feedback using established medical evaluation\nframeworks such as the Master Interview Rating Scale (MIRS). In a pilot study\nwith 104 first-year medical students, we examined engagement, conversation\npatterns, and user perceptions. Students found MedSimAI beneficial for\nrepeated, realistic patient-history practice. Conversation analysis revealed\nthat certain higher-order skills were often overlooked, though students\ngenerally performed systematic histories and empathic listening. By integrating\nunlimited practice opportunities, real-time AI assessment, and SRL principles,\nMedSimAI addresses key limitations of traditional simulation-based training,\nmaking high-quality clinical education more accessible and scalable.", "published": "2025-03-01 00:51:55", "link": "http://arxiv.org/abs/2503.05793v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Psychological Counseling Ability of Large Language Models", "abstract": "With the development of science and the continuous progress of artificial\nintelligence technology, Large Language Models (LLMs) have begun to be widely\nutilized across various fields. However, in the field of psychological\ncounseling, the ability of LLMs have not been systematically assessed. In this\nstudy, we assessed the psychological counseling ability of mainstream LLMs\nusing 1096 psychological counseling skill questions which were selected from\nthe Chinese National Counselor Level 3 Examination, including Knowledge-based,\nAnalytical-based, and Application-based question types. The analysis showed\nthat the correctness rates of the LLMs for Chinese questions, in descending\norder, were GLM-3 (46.5%), GPT-4 (46.1%), Gemini (45.0%), ERNIE-3.5 (45.7%) and\nGPT-3.5 (32.9%). The correctness rates of the LLMs for English questions, in\ndescending order, were ERNIE-3.5 (43.9%), GPT-4 (40.6%), Gemini (36.6%), GLM-3\n(29.9%) and GPT-3.5 (29.5%). A chi-square test indicated significant\ndifferences in the LLMs' performance on Chinese and English questions.\nFurthermore, we subsequently utilized the Counselor's Guidebook (Level 3) as a\nreference for ERNIE-3.5, resulting in a new correctness rate of 59.6%, a 13.8%\nimprovement over its initial rate of 45.8%. In conclusion, the study assessed\nthe psychological counseling ability of LLMs for the first time, which may\nprovide insights for future enhancement and improvement of psychological\ncounseling ability of LLMs.", "published": "2025-03-01 08:01:25", "link": "http://arxiv.org/abs/2503.07627v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "EXCLAIM: An Explainable Cross-Modal Agentic System for Misinformation\n  Detection with Hierarchical Retrieval", "abstract": "Misinformation continues to pose a significant challenge in today's\ninformation ecosystem, profoundly shaping public perception and behavior. Among\nits various manifestations, Out-of-Context (OOC) misinformation is particularly\nobscure, as it distorts meaning by pairing authentic images with misleading\ntextual narratives. Existing methods for detecting OOC misinformation\npredominantly rely on coarse-grained similarity metrics between image-text\npairs, which often fail to capture subtle inconsistencies or provide meaningful\nexplainability. While multi-modal large language models (MLLMs) demonstrate\nremarkable capabilities in visual reasoning and explanation generation, they\nhave not yet demonstrated the capacity to address complex, fine-grained, and\ncross-modal distinctions necessary for robust OOC detection. To overcome these\nlimitations, we introduce EXCLAIM, a retrieval-based framework designed to\nleverage external knowledge through multi-granularity index of multi-modal\nevents and entities. Our approach integrates multi-granularity contextual\nanalysis with a multi-agent reasoning architecture to systematically evaluate\nthe consistency and integrity of multi-modal news content. Comprehensive\nexperiments validate the effectiveness and resilience of EXCLAIM, demonstrating\nits ability to detect OOC misinformation with 4.3% higher accuracy compared to\nstate-of-the-art approaches, while offering explainable and actionable\ninsights.", "published": "2025-03-01 06:32:27", "link": "http://arxiv.org/abs/2504.06269v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via\n  Network Architecture Search", "abstract": "Lightweight models are essential for real-time speech enhancement\napplications. In recent years, there has been a growing trend toward developing\nincreasingly compact models for speech enhancement. In this paper, we propose\nan Ultra-Lightweight U-net optimized by Network Architecture Search (UL-UNAS),\nwhich is suitable for implementation in low-footprint devices. Firstly, we\nexplore the application of various efficient convolutional blocks within the\nU-Net framework to identify the most promising candidates. Secondly, we\nintroduce two boosting components to enhance the capacity of these\nconvolutional blocks: a novel activation function named affine PReLU and a\ncausal time-frequency attention module. Furthermore, we leverage neural\narchitecture search to discover an optimal architecture within our carefully\ndesigned search space. By integrating the above strategies, UL-UNAS not only\nsignificantly outperforms the latest ultra-lightweight models with the same or\nlower computational complexity, but also delivers competitive performance\ncompared to recent baseline models that require substantially higher\ncomputational resources.", "published": "2025-03-01 04:08:55", "link": "http://arxiv.org/abs/2503.00340v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Synthetic data enables context-aware bioacoustic sound event detection", "abstract": "We propose a methodology for training foundation models that enhances their\nin-context learning capabilities within the domain of bioacoustic signal\nprocessing. We use synthetically generated training data, introducing a\ndomain-randomization-based pipeline that constructs diverse acoustic scenes\nwith temporally strong labels. We generate over 8.8 thousand hours of\nstrongly-labeled audio and train a query-by-example, transformer-based model to\nperform few-shot bioacoustic sound event detection. Our second contribution is\na public benchmark of 13 diverse few-shot bioacoustics tasks. Our model\noutperforms previously published methods by 49%, and we demonstrate that this\nis due to both model design and data scale. We make our trained model available\nvia an API, to provide ecologists and ethologists with a training-free tool for\nbioacoustic sound event detection.", "published": "2025-03-01 02:03:22", "link": "http://arxiv.org/abs/2503.00296v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds", "abstract": "We propose BGM2Pose, a non-invasive 3D human pose estimation method using\narbitrary music (e.g., background music) as active sensing signals. Unlike\nexisting approaches that significantly limit practicality by employing\nintrusive chirp signals within the audible range, our method utilizes natural\nmusic that causes minimal discomfort to humans. Estimating human poses from\nstandard music presents significant challenges. In contrast to sound sources\nspecifically designed for measurement, regular music varies in both volume and\npitch. These dynamic changes in signals caused by music are inevitably mixed\nwith alterations in the sound field resulting from human motion, making it hard\nto extract reliable cues for pose estimation. To address these challenges,\nBGM2Pose introduces a Contrastive Pose Extraction Module that employs\ncontrastive learning and hard negative sampling to eliminate musical components\nfrom the recorded data, isolating the pose information. Additionally, we\npropose a Frequency-wise Attention Module that enables the model to focus on\nsubtle acoustic variations attributable to human movement by dynamically\ncomputing attention across frequency bands. Experiments suggest that our method\noutperforms the existing methods, demonstrating substantial potential for\nreal-world applications. Our datasets and code will be made publicly available.", "published": "2025-03-01 07:32:19", "link": "http://arxiv.org/abs/2503.00389v1", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Language Model Mapping in Multimodal Music Learning: A Grand Challenge\n  Proposal", "abstract": "We have seen remarkable success in representation learning and language\nmodels (LMs) using deep neural networks. Many studies aim to build the\nunderlying connections among different modalities via the alignment and\nmappings at the token or embedding level, but so far, most methods are very\ndata-hungry, limiting their performance in domains such as music where paired\ndata are less abundant. We argue that the embedding alignment is only at the\nsurface level of multimodal alignment. In this paper, we propose a grand\nchallenge of \\textit{language model mapping} (LMM), i.e., how to map the\nessence implied in the LM of one domain to the LM of another domain under the\nassumption that LMs of different modalities are tracking the same underlying\nphenomena. We first introduce a basic setup of LMM, highlighting the goal to\nunveil a deeper aspect of cross-modal alignment as well as to achieve more\nsample-efficiency learning. We then discuss why music is an ideal domain in\nwhich to conduct LMM research. After that, we connect LMM in music with a more\ngeneral and challenging scientific problem of \\textit{learning to take actions\nbased on both sensory input and abstract symbols}, and in the end, present an\nadvanced version of the challenge problem setup.", "published": "2025-03-01 10:04:36", "link": "http://arxiv.org/abs/2503.00427v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PodAgent: A Comprehensive Framework for Podcast Generation", "abstract": "Existing Existing automatic audio generation methods struggle to generate\npodcast-like audio programs effectively. The key challenges lie in in-depth\ncontent generation, appropriate and expressive voice production. This paper\nproposed PodAgent, a comprehensive framework for creating audio programs.\nPodAgent 1) generates informative topic-discussion content by designing a\nHost-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for\nsuitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis\nmethod to generate expressive conversational speech. Given the absence of\nstandardized evaluation criteria for podcast-like audio generation, we\ndeveloped comprehensive assessment guidelines to effectively evaluate the\nmodel's performance. Experimental results demonstrate PodAgent's effectiveness,\nsignificantly surpassing direct GPT-4 generation in topic-discussion dialogue\ncontent, achieving an 87.4% voice-matching accuracy, and producing more\nexpressive speech through LLM-guided synthesis. Demo page:\nhttps://podcast-agent.github.io/demo/. Source code:\nhttps://github.com/yujxx/PodAgent.", "published": "2025-03-01 11:35:17", "link": "http://arxiv.org/abs/2503.00455v1", "categories": ["cs.SD", "cs.AI", "cs.MA", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
