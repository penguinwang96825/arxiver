{"title": "Frame Stacking and Retaining for Recurrent Neural Network Acoustic Model", "abstract": "Frame stacking is broadly applied in end-to-end neural network training like\nconnectionist temporal classification (CTC), and it leads to more accurate\nmodels and faster decoding. However, it is not well-suited to conventional\nneural network based on context-dependent state acoustic model, if the decoder\nis unchanged. In this paper, we propose a novel frame retaining method which is\napplied in decoding. The system which combined frame retaining with frame\nstacking could reduces the time consumption of both training and decoding. Long\nshort-term memory (LSTM) recurrent neural networks (RNNs) using it achieve\nalmost linear training speedup and reduces relative 41\\% real time factor\n(RTF). At the same time, recognition performance is no degradation or improves\nsightly on Shenma voice search dataset in Mandarin.", "published": "2017-05-17 02:34:27", "link": "http://arxiv.org/abs/1705.05992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlabeled Data for Morphological Generation With Character-Based\n  Sequence-to-Sequence Models", "abstract": "We present a semi-supervised way of training a character-based\nencoder-decoder recurrent neural network for morphological reinflection, the\ntask of generating one inflected word form from another. This is achieved by\nusing unlabeled tokens or random strings as training data for an autoencoding\ntask, adapting a network for morphological reinflection, and performing\nmulti-task training. We thus use limited labeled data more effectively,\nobtaining up to 9.9% improvement over state-of-the-art baselines for 8\ndifferent languages.", "published": "2017-05-17 11:48:15", "link": "http://arxiv.org/abs/1705.06106v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Political Footprints: Political Discourse Analysis using Pre-Trained\n  Word Vectors", "abstract": "In this paper, we discuss how machine learning could be used to produce a\nsystematic and more objective political discourse analysis. Political\nfootprints are vector space models (VSMs) applied to political discourse. Each\nof their vectors represents a word, and is produced by training the English\nlexicon on large text corpora. This paper presents a simple implementation of\npolitical footprints, some heuristics on how to use them, and their application\nto four cases: the U.N. Kyoto Protocol and Paris Agreement, and two U.S.\npresidential elections. The reader will be offered a number of reasons to\nbelieve that political footprints produce meaningful results, along with some\nsuggestions on how to improve their implementation.", "published": "2017-05-17 21:29:08", "link": "http://arxiv.org/abs/1705.06353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoding Sentiment from Distributed Representations of Sentences", "abstract": "Distributed representations of sentences have been developed recently to\nrepresent their meaning as real-valued vectors. However, it is not clear how\nmuch information such representations retain about the polarity of sentences.\nTo study this question, we decode sentiment from unsupervised sentence\nrepresentations learned with different architectures (sensitive to the order of\nwords, the order of sentences, or none) in 9 typologically diverse languages.\nSentiment results from the (recursive) composition of lexical items and\ngrammatical strategies such as negation and concession. The results are\nmanifold: we show that there is no `one-size-fits-all' representation\narchitecture outperforming the others across the board. Rather, the top-ranking\narchitectures depend on the language and data at hand. Moreover, we find that\nin several cases the additive composition model based on skip-gram word vectors\nmay surpass supervised state-of-art architectures such as bidirectional LSTMs.\nFinally, we provide a possible explanation of the observed variation based on\nthe type of negative constructions in each language.", "published": "2017-05-17 23:32:12", "link": "http://arxiv.org/abs/1705.06369v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Identify Ambiguous and Misleading News Headlines", "abstract": "Accuracy is one of the basic principles of journalism. However, it is\nincreasingly hard to manage due to the diversity of news media. Some editors of\nonline news tend to use catchy headlines which trick readers into clicking.\nThese headlines are either ambiguous or misleading, degrading the reading\nexperience of the audience. Thus, identifying inaccurate news headlines is a\ntask worth studying. Previous work names these headlines \"clickbaits\" and\nmainly focus on the features extracted from the headlines, which limits the\nperformance since the consistency between headlines and news bodies is\nunderappreciated. In this paper, we clearly redefine the problem and identify\nambiguous and misleading headlines separately. We utilize class sequential\nrules to exploit structure information when detecting ambiguous headlines. For\nthe identification of misleading headlines, we extract features based on the\ncongruence between headlines and bodies. To make use of the large unlabeled\ndata set, we apply a co-training method and gain an increase in performance.\nThe experiment results show the effectiveness of our methods. Then we use our\nclassifiers to detect inaccurate headlines crawled from different sources and\nconduct a data analysis.", "published": "2017-05-17 07:24:02", "link": "http://arxiv.org/abs/1705.06031v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Utility of General and Specific Word Embeddings for Classifying\n  Translational Stages of Research", "abstract": "Conventional text classification models make a bag-of-words assumption\nreducing text into word occurrence counts per document. Recent algorithms such\nas word2vec are capable of learning semantic meaning and similarity between\nwords in an entirely unsupervised manner using a contextual window and doing so\nmuch faster than previous methods. Each word is projected into vector space\nsuch that similar meaning words such as \"strong\" and \"powerful\" are projected\ninto the same general Euclidean space. Open questions about these embeddings\ninclude their utility across classification tasks and the optimal properties\nand source of documents to construct broadly functional embeddings. In this\nwork, we demonstrate the usefulness of pre-trained embeddings for\nclassification in our task and demonstrate that custom word embeddings, built\nin the domain and for the tasks, can improve performance over word embeddings\nlearnt on more general data including news articles or Wikipedia.", "published": "2017-05-17 17:08:11", "link": "http://arxiv.org/abs/1705.06262v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Named-Entity Recognition with Neural Networks", "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for named-entity recognition (NER). In order to achieve high\nperformances, ANNs need to be trained on a large labeled dataset. However,\nlabels might be difficult to obtain for the dataset on which the user wants to\nperform NER: label scarcity is particularly pronounced for patient note\nde-identification, which is an instance of NER. In this work, we analyze to\nwhat extent transfer learning may address this issue. In particular, we\ndemonstrate that transferring an ANN model trained on a large labeled dataset\nto another dataset with a limited number of labels improves upon the\nstate-of-the-art results on two different datasets for patient note\nde-identification.", "published": "2017-05-17 17:45:15", "link": "http://arxiv.org/abs/1705.06273v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
