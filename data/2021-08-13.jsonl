{"title": "FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning", "abstract": "Most previous methods for text data augmentation are limited to simple tasks\nand weak baselines. We explore data augmentation on hard tasks (i.e., few-shot\nnatural language understanding) and strong baselines (i.e., pretrained models\nwith over one billion parameters). Under this setting, we reproduced a large\nnumber of previous augmentation methods and found that these methods bring\nmarginal gains at best and sometimes degrade the performance much. To address\nthis challenge, we propose a novel data augmentation method FlipDA that jointly\nuses a generative model and a classifier to generate label-flipped data.\nCentral to the idea of FlipDA is the discovery that generating label-flipped\ndata is more crucial to the performance than generating label-preserved data.\nExperiments show that FlipDA achieves a good tradeoff between effectiveness and\nrobustness -- it substantially improves many tasks while not negatively\naffecting the others.", "published": "2021-08-13 17:51:31", "link": "http://arxiv.org/abs/2108.06332v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MTG: A Benchmark Suite for Multilingual Text Generation", "abstract": "We introduce MTG, a new benchmark suite for training and evaluating\nmultilingual text generation. It is the first-proposed multilingual multiway\ntext generation dataset with the largest human-annotated data (400k). It\nincludes four generation tasks (story generation, question generation, title\ngeneration and text summarization) across five languages (English, German,\nFrench, Spanish and Chinese). The multiway setup enables testing knowledge\ntransfer capabilities for a model across languages and tasks. Using MTG, we\ntrain and analyze several popular multilingual generation models from different\naspects. Our benchmark suite fosters model performance enhancement with more\nhuman-annotated parallel data. It provides comprehensive evaluations with\ndiverse generation scenarios. Code and data are available at\n\\url{https://github.com/zide05/MTG}.", "published": "2021-08-13 13:25:08", "link": "http://arxiv.org/abs/2108.07140v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Reasoning with Relational Digraph", "abstract": "Reasoning on the knowledge graph (KG) aims to infer new facts from existing\nones. Methods based on the relational path have shown strong, interpretable,\nand transferable reasoning ability. However, paths are naturally limited in\ncapturing local evidence in graphs. In this paper, we introduce a novel\nrelational structure, i.e., relational directed graph (r-digraph), which is\ncomposed of overlapped relational paths, to capture the KG's local evidence.\nSince the r- digraphs are more complex than paths, how to efficiently construct\nand effectively learn from them are challenging. Directly encoding the\nr-digraphs cannot scale well and capturing query-dependent information is hard\nin r-digraphs. We propose a variant of graph neural network, i.e., RED-GNN, to\naddress the above challenges. Specifically, RED-GNN makes use of dynamic\nprogramming to recursively encodes multiple r-digraphs with shared edges, and\nutilizes a query-dependent attention mechanism to select the strongly\ncorrelated edges. We demonstrate that RED-GNN is not only efficient but also\ncan achieve significant performance gains in both inductive and transductive\nreasoning tasks over existing methods. Besides, the learned attention weights\nin RED-GNN can exhibit interpretable evidence for KG reasoning.", "published": "2021-08-13 03:27:01", "link": "http://arxiv.org/abs/2108.06040v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Aspect Sentiment Triplet Extraction Using Reinforcement Learning", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting triplets\nof aspect terms, their associated sentiments, and the opinion terms that\nprovide evidence for the expressed sentiments. Previous approaches to ASTE\nusually simultaneously extract all three components or first identify the\naspect and opinion terms, then pair them up to predict their sentiment\npolarities. In this work, we present a novel paradigm, ASTE-RL, by regarding\nthe aspect and opinion terms as arguments of the expressed sentiment in a\nhierarchical reinforcement learning (RL) framework. We first focus on\nsentiments expressed in a sentence, then identify the target aspect and opinion\nterms for that sentiment. This takes into account the mutual interactions among\nthe triplet's components while improving exploration and sample efficiency.\nFurthermore, this hierarchical RLsetup enables us to deal with multiple and\noverlapping triplets. In our experiments, we evaluate our model on existing\ndatasets from laptop and restaurant domains and show that it achieves\nstate-of-the-art performance. The implementation of this work is publicly\navailable at https://github.com/declare-lab/ASTE-RL.", "published": "2021-08-13 07:38:48", "link": "http://arxiv.org/abs/2108.06107v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Answer Similarity for Evaluating Question Answering Models", "abstract": "The evaluation of question answering models compares ground-truth annotations\nwith model predictions. However, as of today, this comparison is mostly\nlexical-based and therefore misses out on answers that have no lexical overlap\nbut are still semantically similar, thus treating correct answers as false.\nThis underestimation of the true performance of models hinders user acceptance\nin applications and complicates a fair comparison of different models.\nTherefore, there is a need for an evaluation metric that is based on semantics\ninstead of pure string similarity. In this short paper, we present SAS, a\ncross-encoder-based metric for the estimation of semantic answer similarity,\nand compare it to seven existing metrics. To this end, we create an English and\na German three-way annotated evaluation dataset containing pairs of answers\nalong with human judgment of their semantic similarity, which we release along\nwith an implementation of the SAS metric and the experiments. We find that\nsemantic similarity metrics based on recent transformer models correlate much\nbetter with human judgment than traditional lexical similarity metrics on our\ntwo newly created datasets and one dataset from related work.", "published": "2021-08-13 09:12:27", "link": "http://arxiv.org/abs/2108.06130v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Structured Dynamic Sparse Pre-Training of BERT", "abstract": "Identifying algorithms for computational efficient unsupervised training of\nlarge language models is an important and active area of research. In this\nwork, we develop and study a straightforward, dynamic always-sparse\npre-training approach for BERT language modeling task, which leverages periodic\ncompression steps based on magnitude pruning followed by random parameter\nre-allocation. This approach enables us to achieve Pareto improvements in terms\nof the number of floating-point operations (FLOPs) over statically sparse and\ndense models across a broad spectrum of network sizes. Furthermore, we\ndemonstrate that training remains FLOP-efficient when using coarse-grained\nblock sparsity, making it particularly promising for efficient execution on\nmodern hardware accelerators.", "published": "2021-08-13 14:54:26", "link": "http://arxiv.org/abs/2108.06277v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Single and Multiple Representations in Dense Passage Retrieval", "abstract": "The advent of contextualised language models has brought gains in search\neffectiveness, not just when applied for re-ranking the output of classical\nweighting models such as BM25, but also when used directly for passage indexing\nand retrieval, a technique which is called dense retrieval. In the existing\nliterature in neural ranking, two dense retrieval families have become\napparent: single representation, where entire passages are represented by a\nsingle embedding (usually BERT's [CLS] token, as exemplified by the recent ANCE\napproach), or multiple representations, where each token in a passage is\nrepresented by its own embedding (as exemplified by the recent ColBERT\napproach). These two families have not been directly compared. However, because\nof the likely importance of dense retrieval moving forward, a clear\nunderstanding of their advantages and disadvantages is paramount. To this end,\nthis paper contributes a direct study on their comparative effectiveness,\nnoting situations where each method under/over performs w.r.t. each other, and\nw.r.t. a BM25 baseline. We observe that, while ANCE is more efficient than\nColBERT in terms of response time and memory usage, multiple representations\nare statistically more effective than the single representations for MAP and\nMRR@10. We also show that multiple representations obtain better improvements\nthan single representations for queries that are the hardest for BM25, as well\nas for definitional queries, and those with complex information needs.", "published": "2021-08-13 15:01:53", "link": "http://arxiv.org/abs/2108.06279v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Diachronic Analysis of German Parliamentary Proceedings: Ideological\n  Shifts through the Lens of Political Biases", "abstract": "We analyze bias in historical corpora as encoded in diachronic distributional\nsemantic models by focusing on two specific forms of bias, namely a political\n(i.e., anti-communism) and racist (i.e., antisemitism) one. For this, we use a\nnew corpus of German parliamentary proceedings, DeuPARL, spanning the period\n1867--2020. We complement this analysis of historical biases in diachronic word\nembeddings with a novel measure of bias on the basis of term co-occurrences and\ngraph-based label propagation. The results of our bias measurements align with\ncommonly perceived historical trends of antisemitic and anti-communist biases\nin German politics in different time periods, thus indicating the viability of\nanalyzing historical bias trends using semantic spaces induced from historical\ncorpora.", "published": "2021-08-13 15:58:07", "link": "http://arxiv.org/abs/2108.06295v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "MeetSum: Transforming Meeting Transcript Summarization using\n  Transformers!", "abstract": "Creating abstractive summaries from meeting transcripts has proven to be\nchallenging due to the limited amount of labeled data available for training\nneural network models. Moreover, Transformer-based architectures have proven to\nbeat state-of-the-art models in summarizing news data. In this paper, we\nutilize a Transformer-based Pointer Generator Network to generate abstract\nsummaries for meeting transcripts. This model uses 2 LSTMs as an encoder and a\ndecoder, a Pointer network which copies words from the inputted text, and a\nGenerator network to produce out-of-vocabulary words (hence making the summary\nabstractive). Moreover, a coverage mechanism is used to avoid repetition of\nwords in the generated summary. First, we show that training the model on a\nnews summary dataset and using zero-shot learning to test it on the meeting\ndataset proves to produce better results than training it on the AMI meeting\ndataset. Second, we show that training this model first on out-of-domain data,\nsuch as the CNN-Dailymail dataset, followed by a fine-tuning stage on the AMI\nmeeting dataset is able to improve the performance of the model significantly.\nWe test our model on a testing set from the AMI dataset and report the ROUGE-2\nscore of the generated summary to compare with previous literature. We also\nreport the Factual score of our summaries since it is a better benchmark for\nabstractive summaries since the ROUGE-2 score is limited to measuring\nword-overlaps. We show that our improved model is able to improve on previous\nmodels by at least 5 ROUGE-2 scores, which is a substantial improvement. Also,\na qualitative analysis of the summaries generated by our model shows that these\nsummaries and human-readable and indeed capture most of the important\ninformation from the transcripts.", "published": "2021-08-13 16:34:09", "link": "http://arxiv.org/abs/2108.06310v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Dataset for Answering Time-Sensitive Questions", "abstract": "Time is an important dimension in our physical world. Lots of facts can\nevolve with respect to time. For example, the U.S. President might change every\nfour years. Therefore, it is important to consider the time dimension and\nempower the existing QA models to reason over time. However, the existing QA\ndatasets contain rather few time-sensitive questions, hence not suitable for\ndiagnosing or benchmarking the model's temporal reasoning capability. In order\nto promote research in this direction, we propose to construct a time-sensitive\nQA dataset. The dataset is constructed by 1) mining time-evolving facts from\nWikiData and aligning them to their corresponding Wikipedia page, 2) employing\ncrowd workers to verify and calibrate these noisy facts, 3) generating\nquestion-answer pairs based on the annotated time-sensitive facts. Our dataset\nposes challenges in the aspect of both temporal understanding and temporal\nreasoning. We evaluate different SoTA long-document QA systems like BigBird and\nFiD on our dataset. The best-performing model FiD can only achieve 46\\%\naccuracy, still far behind the human performance of 87\\%. We demonstrate that\nthese models are still lacking the ability to perform consistent temporal\nreasoning. Therefore, we believe that our dataset could serve as a benchmark to\ndevelop NLP models more sensitive to temporal shifts. The dataset and code are\nreleased in~\\url{https://github.com/wenhuchen/Time-Sensitive-QA}.", "published": "2021-08-13 16:42:25", "link": "http://arxiv.org/abs/2108.06314v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Low-Resource Adaptation of Open-Domain Generative Chatbots", "abstract": "Recent work building open-domain chatbots has demonstrated that increasing\nmodel size improves performance. On the other hand, latency and connectivity\nconsiderations dictate the move of digital assistants on the device. Giving a\ndigital assistant like Siri, Alexa, or Google Assistant the ability to discuss\njust about anything leads to the need for reducing the chatbot model size such\nthat it fits on the user's device. We demonstrate that low parameter models can\nsimultaneously retain their general knowledge conversational abilities while\nimproving in a specific domain. Additionally, we propose a generic framework\nthat accounts for variety in question types, tracks reference throughout\nmulti-turn conversations, and removes inconsistent and potentially toxic\nresponses. Our framework seamlessly transitions between chatting and performing\ntransactional tasks, which will ultimately make interactions with digital\nassistants more human-like. We evaluate our framework on 1 internal and 4\npublic benchmark datasets using both automatic (Perplexity) and human (SSA -\nSensibleness and Specificity Average) evaluation metrics and establish\ncomparable performance while reducing model parameters by 90%.", "published": "2021-08-13 17:40:30", "link": "http://arxiv.org/abs/2108.06329v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalized Optimal Linear Orders", "abstract": "The sequential structure of language, and the order of words in a sentence\nspecifically, plays a central role in human language processing. Consequently,\nin designing computational models of language, the de facto approach is to\npresent sentences to machines with the words ordered in the same order as in\nthe original human-authored sentence. The very essence of this work is to\nquestion the implicit assumption that this is desirable and inject theoretical\nsoundness into the consideration of word order in natural language processing.\nIn this thesis, we begin by uniting the disparate treatments of word order in\ncognitive science, psycholinguistics, computational linguistics, and natural\nlanguage processing under a flexible algorithmic framework. We proceed to use\nthis heterogeneous theoretical foundation as the basis for exploring new word\norders with an undercurrent of psycholinguistic optimality. In particular, we\nfocus on notions of dependency length minimization given the difficulties in\nhuman and computational language processing in handling long-distance\ndependencies. We then discuss algorithms for finding optimal word orders\nefficiently in spite of the combinatorial space of possibilities. We conclude\nby addressing the implications of these word orders on human language and their\ndownstream impacts when integrated in computational models.", "published": "2021-08-13 13:10:15", "link": "http://arxiv.org/abs/2108.10692v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GQE-PRF: Generative Query Expansion with Pseudo-Relevance Feedback", "abstract": "Query expansion with pseudo-relevance feedback (PRF) is a powerful approach\nto enhance the effectiveness in information retrieval. Recently, with the rapid\nadvance of deep learning techniques, neural text generation has achieved\npromising success in many natural language tasks. To leverage the strength of\ntext generation for information retrieval, in this article, we propose a novel\napproach which effectively integrates text generation models into PRF-based\nquery expansion. In particular, our approach generates augmented query terms\nvia neural text generation models conditioned on both the initial query and\npseudo-relevance feedback. Moreover, in order to train the generative model, we\nadopt the conditional generative adversarial nets (CGANs) and propose the\nPRF-CGAN method in which both the generator and the discriminator are\nconditioned on the pseudo-relevance feedback. We evaluate the performance of\nour approach on information retrieval tasks using two benchmark datasets. The\nexperimental results show that our approach achieves comparable performance or\noutperforms traditional query expansion methods on both the retrieval and\nreranking tasks.", "published": "2021-08-13 01:09:02", "link": "http://arxiv.org/abs/2108.06010v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TPRM: A Topic-based Personalized Ranking Model for Web Search", "abstract": "Ranking models have achieved promising results, but it remains challenging to\ndesign personalized ranking systems to leverage user profiles and semantic\nrepresentations between queries and documents. In this paper, we propose a\ntopic-based personalized ranking model (TPRM) that integrates user topical\nprofile with pretrained contextualized term representations to tailor the\ngeneral document ranking list. Experiments on the real-world dataset\ndemonstrate that TPRM outperforms state-of-the-art ad-hoc ranking models and\npersonalized ranking models significantly.", "published": "2021-08-13 01:16:55", "link": "http://arxiv.org/abs/2108.06014v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense\n  Passage Retrieval", "abstract": "Recently, dense passage retrieval has become a mainstream approach to finding\nrelevant information in various natural language processing tasks. A number of\nstudies have been devoted to improving the widely adopted dual-encoder\narchitecture. However, most of the previous studies only consider query-centric\nsimilarity relation when learning the dual-encoder retriever. In order to\ncapture more comprehensive similarity relations, we propose a novel approach\nthat leverages both query-centric and PAssage-centric sImilarity Relations\n(called PAIR) for dense passage retrieval. To implement our approach, we make\nthree major technical contributions by introducing formal formulations of the\ntwo kinds of similarity relations, generating high-quality pseudo labeled data\nvia knowledge distillation, and designing an effective two-stage training\nprocedure that incorporates passage-centric similarity relation constraint.\nExtensive experiments show that our approach significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions datasets.", "published": "2021-08-13 02:07:43", "link": "http://arxiv.org/abs/2108.06027v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Zero-shot Task Transfer for Invoice Extraction via Class-aware QA\n  Ensemble", "abstract": "We present VESPA, an intentionally simple yet novel zero-shot system for\nlayout, locale, and domain agnostic document extraction. In spite of the\navailability of large corpora of documents, the lack of labeled and validated\ndatasets makes it a challenge to discriminatively train document extraction\nmodels for enterprises. We show that this problem can be addressed by simply\ntransferring the information extraction (IE) task to a natural language\nQuestion-Answering (QA) task without engineering task-specific architectures.\nWe demonstrate the effectiveness of our system by evaluating on a closed corpus\nof real-world retail and tax invoices with multiple complex layouts, domains,\nand geographies. The empirical evaluation shows that our system outperforms 4\nprominent commercial invoice solutions that use discriminatively trained models\nwith architectures specifically crafted for invoice extraction. We extracted 6\nfields with zero upfront human annotation or training with an Avg. F1 of 87.50.", "published": "2021-08-13 05:36:07", "link": "http://arxiv.org/abs/2108.06069v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Multilingual training set selection for ASR in under-resourced Malian\n  languages", "abstract": "We present first speech recognition systems for the two severely\nunder-resourced Malian languages Bambara and Maasina Fulfulde. These systems\nwill be used by the United Nations as part of a monitoring system to inform and\nsupport humanitarian programmes in rural Africa. We have compiled datasets in\nBambara and Maasina Fulfulde, but since these are very small, we take advantage\nof six similarly under-resourced datasets in other languages for multilingual\ntraining. We focus specifically on the best composition of the multilingual\npool of speech data for multilingual training. We find that, although\nmaximising the training pool by including all six additional languages provides\nimproved speech recognition in both target languages, substantially better\nperformance can be achieved by a more judicious choice. Our experiments show\nthat the addition of just one language provides best performance. For Bambara,\nthis additional language is Maasina Fulfulde, and its introduction leads to a\nrelative word error rate reduction of 6.7%, as opposed to a 2.4% relative\nreduction achieved when pooling all six additional languages. For the case of\nMaasina Fulfulde, best performance was achieved when adding only Luganda,\nleading to a relative word error rate improvement of 9.4% as opposed to a 3.9%\nrelative improvement when pooling all six languages. We conclude that careful\nselection of the out-of-language data is worthwhile for multilingual training\neven in highly under-resourced settings, and that the general assumption that\nmore data is better does not always hold.", "published": "2021-08-13 10:36:06", "link": "http://arxiv.org/abs/2108.06164v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Feature learning for efficient ASR-free keyword spotting in low-resource\n  languages", "abstract": "We consider feature learning for efficient keyword spotting that can be\napplied in severely under-resourced settings. The objective is to support\nhumanitarian relief programmes by the United Nations in parts of Africa in\nwhich almost no language resources are available. For rapid development in such\nlanguages, we rely on a small, easily-compiled set of isolated keywords. These\nkeyword templates are applied to a large corpus of in-domain but untranscribed\nspeech using dynamic time warping (DTW). The resulting DTW alignment scores are\nused to train a convolutional neural network (CNN) which is orders of magnitude\nmore computationally efficient and suitable for real-time application. We\noptimise this neural network keyword spotter by identifying robust acoustic\nfeatures in this almost zero-resource setting. First, we incorporate\ninformation from well-resourced but unrelated languages using a multilingual\nbottleneck feature (BNF) extractor. Next, we consider features extracted from\nan autoencoder (AE) trained on in-domain but untranscribed data. Finally, we\nconsider correspondence autoencoder (CAE) features which are fine-tuned on the\nsmall set of in-domain labelled data. Experiments in South African English and\nLuganda, a low-resource language, show that BNF and CAE features achieve a 5%\nrelative performance improvement over baseline MFCCs. However, using BNFs as\ninput to the CAE results in a more than 27% relative improvement over MFCCs in\nROC area-under-the-curve (AUC) and more than twice as many top-10 retrievals.\nWe show that, using these features, the CNN-DTW keyword spotter performs almost\nas well as the DTW keyword spotter while outperforming a baseline CNN trained\nonly on the keyword templates. The CNN-DTW keyword spotter using BNF-derived\nCAE features represents an efficient approach with competitive performance\nsuited to rapid deployment in a severely under-resourced scenario.", "published": "2021-08-13 11:39:50", "link": "http://arxiv.org/abs/2108.06174v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Enhancing audio quality for expressive Neural Text-to-Speech", "abstract": "Artificial speech synthesis has made a great leap in terms of naturalness as\nrecent Text-to-Speech (TTS) systems are capable of producing speech with\nsimilar quality to human recordings. However, not all speaking styles are easy\nto model: highly expressive voices are still challenging even to recent TTS\narchitectures since there seems to be a trade-off between expressiveness in a\ngenerated audio and its signal quality. In this paper, we present a set of\ntechniques that can be leveraged to enhance the signal quality of a\nhighly-expressive voice without the use of additional data. The proposed\ntechniques include: tuning the autoregressive loop's granularity during\ntraining; using Generative Adversarial Networks in acoustic modelling; and the\nuse of Variational Auto-Encoders in both the acoustic model and the neural\nvocoder. We show that, when combined, these techniques greatly closed the gap\nin perceived naturalness between the baseline system and recordings by 39% in\nterms of MUSHRA scores for an expressive celebrity voice.", "published": "2021-08-13 14:32:39", "link": "http://arxiv.org/abs/2108.06270v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Cross-modal Spectrum Transformation Network For Acoustic Scene\n  classification", "abstract": "Convolutional neural networks (CNNs) with log-mel spectrum features have\nshown promising results for acoustic scene classification tasks. However, the\nperformance of these CNN based classifiers is still lacking as they do not\ngeneralise well for unknown environments. To address this issue, we introduce\nan acoustic spectrum transformation network where traditional log-mel spectrums\nare transformed into imagined visual features (IVF). The imagined visual\nfeatures are learned by exploiting the relationship between audio and visual\nfeatures present in video recordings. An auto-encoder is used to encode images\nas visual features and a transformation network learns how to generate imagined\nvisual features from log-mel. Our model is trained on a large dataset of\nYoutube videos. We test our proposed method on the scene classification task of\nDCASE and ESC-50, where our method outperforms other spectrum features,\nespecially for unseen environments.", "published": "2021-08-13 21:05:14", "link": "http://arxiv.org/abs/2108.06401v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
