{"title": "MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue\n  Evaluation", "abstract": "Chatbots are designed to carry out human-like conversations across different\ndomains, such as general chit-chat, knowledge exchange, and persona-grounded\nconversations. To measure the quality of such conversational agents, a dialogue\nevaluator is expected to conduct assessment across domains as well. However,\nmost of the state-of-the-art automatic dialogue evaluation metrics (ADMs) are\nnot designed for multi-domain evaluation. We are motivated to design a general\nand robust framework, MDD-Eval, to address the problem. Specifically, we first\ntrain a teacher evaluator with human-annotated data to acquire a rating skill\nto tell good dialogue responses from bad ones in a particular domain and then,\nadopt a self-training strategy to train a new evaluator with teacher-annotated\nmulti-domain data, that helps the new evaluator to generalize across multiple\ndomains. MDD-Eval is extensively assessed on six dialogue evaluation\nbenchmarks. Empirical results show that the MDD-Eval framework achieves a\nstrong performance with an absolute improvement of 7% over the state-of-the-art\nADMs in terms of mean Spearman correlation scores across all the evaluation\nbenchmarks.", "published": "2021-12-14 07:01:20", "link": "http://arxiv.org/abs/2112.07194v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple Local Attentions Remain Competitive for Long-Context Tasks", "abstract": "Many NLP tasks require processing long contexts beyond the length limit of\npretrained models. In order to scale these models to longer text sequences,\nmany efficient long-range attention variants have been proposed. Despite the\nabundance of research along this direction, it is still difficult to gauge the\nrelative effectiveness of these models in practical use cases, e.g., if we\napply these models following the pretrain-and-finetune paradigm. In this work,\nwe aim to conduct a thorough analysis of these emerging models with large-scale\nand controlled experiments. For each attention variant, we pretrain large-size\nmodels using the same long-doc corpus and then finetune these models for\nreal-world long-context tasks. Our findings reveal pitfalls of an existing\nwidely-used long-range benchmark and show none of the tested efficient\nattentions can beat a simple local window attention under standard pretraining\nparadigms. Further analysis on local attention variants suggests that even the\ncommonly used attention-window overlap is not necessary to achieve good\ndownstream results -- using disjoint local attentions, we are able to build a\nsimpler and more efficient long-doc QA model that matches the performance of\nLongformer~\\citep{longformer} with half of its pretraining compute.\n  The code to replicate our experiments can be found at\nhttps://github.com/pytorch/fairseq/tree/main/examples/xformers", "published": "2021-12-14 07:37:58", "link": "http://arxiv.org/abs/2112.07210v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Search with Mixed-Initiative -- Asking Good Clarification\n  Questions backed-up by Passage Retrieval", "abstract": "We deal with the scenario of conversational search, where user queries are\nunder-specified or ambiguous. This calls for a mixed-initiative setup.\nUser-asks (queries) and system-answers, as well as system-asks (clarification\nquestions) and user response, in order to clarify her information needs. We\nfocus on the task of selecting the next clarification question, given the\nconversation context. Our method leverages passage retrieval from a background\ncontent to fine-tune two deep-learning models for ranking candidate\nclarification questions. We evaluated our method on two different use-cases.\nThe first is an open domain conversational search in a large web collection.\nThe second is a task-oriented customer-support setup. We show that our method\nperforms well on both use-cases.", "published": "2021-12-14 11:27:16", "link": "http://arxiv.org/abs/2112.07308v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TASSY -- A Text Annotation Survey System", "abstract": "We present a free and open-source tool for creating web-based surveys that\ninclude text annotation tasks. Existing tools offer either text annotation or\nsurvey functionality but not both. Combining the two input types is\nparticularly relevant for investigating a reader's perception of a text which\nalso depends on the reader's background, such as age, gender, and education.\nOur tool caters primarily to the needs of researchers in the Library and\nInformation Sciences, the Social Sciences, and the Humanities who apply Content\nAnalysis to investigate, e.g., media bias, political communication, or fake\nnews.", "published": "2021-12-14 13:32:13", "link": "http://arxiv.org/abs/2112.07391v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do You Think It's Biased? How To Ask For The Perception Of Media Bias", "abstract": "Media coverage possesses a substantial effect on the public perception of\nevents. The way media frames events can significantly alter the beliefs and\nperceptions of our society. Nevertheless, nearly all media outlets are known to\nreport news in a biased way. While such bias can be introduced by altering the\nword choice or omitting information, the perception of bias also varies largely\ndepending on a reader's personal background. Therefore, media bias is a very\ncomplex construct to identify and analyze. Even though media bias has been the\nsubject of many studies, previous assessment strategies are oversimplified,\nlack overlap and empirical evaluation. Thus, this study aims to develop a scale\nthat can be used as a reliable standard to evaluate article bias. To name an\nexample: Intending to measure bias in a news article, should we ask, \"How\nbiased is the article?\" or should we instead ask, \"How did the article treat\nthe American president?\". We conducted a literature search to find 824 relevant\nquestions about text perception in previous research on the topic. In a\nmulti-iterative process, we summarized and condensed these questions\nsemantically to conclude a complete and representative set of possible question\ntypes about bias. The final set consisted of 25 questions with varying\nanswering formats, 17 questions using semantic differentials, and six ratings\nof feelings. We tested each of the questions on 190 articles with overall 663\nparticipants to identify how well the questions measure an article's perceived\nbias. Our results show that 21 final items are suitable and reliable for\nmeasuring the perception of media bias. We publish the final set of questions\non http://bias-question-tree.gipplab.org/.", "published": "2021-12-14 13:33:57", "link": "http://arxiv.org/abs/2112.07392v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards A Reliable Ground-Truth For Biased Language Detection", "abstract": "Reference texts such as encyclopedias and news articles can manifest biased\nlanguage when objective reporting is substituted by subjective writing.\nExisting methods to detect bias mostly rely on annotated data to train machine\nlearning models. However, low annotator agreement and comparability is a\nsubstantial drawback in available media bias corpora. To evaluate data\ncollection options, we collect and compare labels obtained from two popular\ncrowdsourcing platforms. Our results demonstrate the existing crowdsourcing\napproaches' lack of data quality, underlining the need for a trained expert\nframework to gather a more reliable dataset. By creating such a framework and\ngathering a first dataset, we are able to improve Krippendorff's $\\alpha$ =\n0.144 (crowdsourcing labels) to $\\alpha$ = 0.419 (expert labels). We conclude\nthat detailed annotator training increases data quality, improving the\nperformance of existing bias detection systems. We will continue to extend our\ndataset in the future.", "published": "2021-12-14 14:13:05", "link": "http://arxiv.org/abs/2112.07421v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks", "abstract": "Labelled data is the foundation of most natural language processing tasks.\nHowever, labelling data is difficult and there often are diverse valid beliefs\nabout what the correct data labels should be. So far, dataset creators have\nacknowledged annotator subjectivity, but rarely actively managed it in the\nannotation process. This has led to partly-subjective datasets that fail to\nserve a clear downstream use. To address this issue, we propose two contrasting\nparadigms for data annotation. The descriptive paradigm encourages annotator\nsubjectivity, whereas the prescriptive paradigm discourages it. Descriptive\nannotation allows for the surveying and modelling of different beliefs, whereas\nprescriptive annotation enables the training of models that consistently apply\none belief. We discuss benefits and challenges in implementing both paradigms,\nand argue that dataset creators should explicitly aim for one or the other to\nfacilitate the intended use of their dataset. Lastly, we conduct an annotation\nexperiment using hate speech data that illustrates the contrast between the two\nparadigms.", "published": "2021-12-14 15:38:22", "link": "http://arxiv.org/abs/2112.07475v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Dynamics of Success: Fractal Scaling of Story Arcs Predicts\n  Reader Preferences", "abstract": "We explore the correlation between the sentiment arcs of H. C. Andersen's\nfairy tales and their popularity, measured as their average score on the\nplatform GoodReads. Specifically, we do not conceive a story's overall\nsentimental trend as predictive \\textit{per se}, but we focus on its coherence\nand predictability over time as represented by the arc's Hurst exponent. We\nfind that degrading Hurst values tend to imply degrading quality scores, while\na Hurst exponent between .55 and .65 might indicate a \"sweet spot\" for literary\nappreciation.", "published": "2021-12-14 16:00:51", "link": "http://arxiv.org/abs/2112.07497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LMTurk: Few-Shot Learners as Crowdsourcing Workers in a\n  Language-Model-as-a-Service Framework", "abstract": "Vast efforts have been devoted to creating high-performance few-shot\nlearners, i.e., large-scale pretrained language models (PLMs) that perform well\nwith little downstream task training data. Training PLMs has incurred\nsignificant cost, but utilizing the few-shot learners is still challenging due\nto their enormous size. This work focuses on a crucial question: How to make\neffective use of these few-shot learners? We propose LMTurk, a novel approach\nthat treats few-shot learners as crowdsourcing workers. The rationale is that\ncrowdsourcing workers are in fact few-shot learners: They are shown a few\nillustrative examples to learn about a task and then start annotating. LMTurk\nemploys few-shot learners built upon PLMs as workers. We show that the\nresulting annotations can be utilized to train models that solve the task well\nand are small enough to be deployable in practical scenarios. Active learning\nis integrated into LMTurk to reduce the amount of queries made to PLMs,\nminimizing the computational cost of running PLM inference passes. Altogether,\nLMTurk is an important step towards making effective use of current PLMs.", "published": "2021-12-14 16:34:22", "link": "http://arxiv.org/abs/2112.07522v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforced Abstractive Summarization with Adaptive Length Controlling", "abstract": "Document summarization, as a fundamental task in natural language generation,\naims to generate a short and coherent summary for a given document.\nControllable summarization, especially of the length, is an important issue for\nsome practical applications, especially how to trade-off the length constraint\nand information integrity. In this paper, we propose an \\textbf{A}daptive\n\\textbf{L}ength \\textbf{C}ontrolling \\textbf{O}ptimization (\\textbf{ALCO})\nmethod to leverage two-stage abstractive summarization model via reinforcement\nlearning. ALCO incorporates length constraint into the stage of sentence\nextraction to penalize the overlength extracted sentences. Meanwhile, a\nsaliency estimation mechanism is designed to preserve the salient information\nin the generated sentences. A series of experiments have been conducted on a\nwildly-used benchmark dataset \\textit{CNN/Daily Mail}. The results have shown\nthat ALCO performs better than the popular baselines in terms of length\ncontrollability and content preservation.", "published": "2021-12-14 16:48:47", "link": "http://arxiv.org/abs/2112.07534v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforcing Semantic-Symmetry for Document Summarization", "abstract": "Document summarization condenses a long document into a short version with\nsalient information and accurate semantic descriptions. The main issue is how\nto make the output summary semantically consistent with the input document. To\nreach this goal, recently, researchers have focused on supervised end-to-end\nhybrid approaches, which contain an extractor module and abstractor module.\nAmong them, the extractor identifies the salient sentences from the input\ndocument, and the abstractor generates a summary from the salient sentences.\nThis model successfully keeps the consistency between the generated summary and\nthe reference summary via various strategies (e.g., reinforcement learning).\nThere are two semantic gaps when training the hybrid model (one is between\ndocument and extracted sentences, and the other is between extracted sentences\nand summary). However, they are not explicitly considered in the existing\nmethods, which usually results in a semantic bias of summary. To mitigate the\nabove issue, in this paper, a new \\textbf{r}einforcing\ns\\textbf{e}mantic-\\textbf{sy}mmetry learning \\textbf{m}odel is proposed for\ndocument summarization (\\textbf{ReSyM}). ReSyM introduces a\nsemantic-consistency reward in the extractor to bridge the first gap. A\nsemantic dual-reward is designed to bridge the second gap in the abstractor.\nThe whole document summarization process is implemented via reinforcement\nlearning with a hybrid reward mechanism (combining the above two rewards).\nMoreover, a comprehensive sentence representation learning method is presented\nto sufficiently capture the information from the original document. A series of\nexperiments have been conducted on two wildly used benchmark datasets CNN/Daily\nMail and BigPatent. The results have shown the superiority of ReSyM by\ncomparing it with the state-of-the-art baselines in terms of various evaluation\nmetrics.", "published": "2021-12-14 17:41:37", "link": "http://arxiv.org/abs/2112.07583v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Compositional Generalization with Latent Structure and Data\n  Augmentation", "abstract": "Generic unstructured neural networks have been shown to struggle on\nout-of-distribution compositional generalization. Compositional data\naugmentation via example recombination has transferred some prior knowledge\nabout compositionality to such black-box neural models for several semantic\nparsing tasks, but this often required task-specific engineering or provided\nlimited gains.\n  We present a more powerful data recombination method using a model called\nCompositional Structure Learner (CSL). CSL is a generative model with a\nquasi-synchronous context-free grammar backbone, which we induce from the\ntraining data. We sample recombined examples from CSL and add them to the\nfine-tuning data of a pre-trained sequence-to-sequence model (T5). This\nprocedure effectively transfers most of CSL's compositional bias to T5 for\ndiagnostic tasks, and results in a model even stronger than a T5-CSL ensemble\non two real world compositional generalization tasks. This results in new\nstate-of-the-art performance for these challenging semantic parsing tasks\nrequiring generalization to both natural language variation and novel\ncompositions of elements.", "published": "2021-12-14 18:03:28", "link": "http://arxiv.org/abs/2112.07610v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Neural Models for Query-Focused Summarization", "abstract": "Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. While recently released datasets, such as QMSum or AQuaMuSe,\nfacilitate research efforts in QFS, the field lacks a comprehensive study of\nthe broad space of applicable modeling methods. In this paper we conduct a\nsystematic exploration of neural approaches to QFS, considering two general\nclasses of methods: two-stage extractive-abstractive solutions and end-to-end\nmodels. Within those categories, we investigate existing models and explore\nstrategies for transfer learning. We also present two modeling extensions that\nachieve state-of-the-art performance on the QMSum dataset, up to a margin of\n3.38 ROUGE-1, 3.72 ROUGE2, and 3.28 ROUGE-L when combined with transfer\nlearning strategies. Results from human evaluation suggest that the best models\nproduce more comprehensive and factually consistent summaries compared to a\nbaseline model. Code and checkpoints are made publicly available:\nhttps://github.com/salesforce/query-focused-sum.", "published": "2021-12-14 18:33:29", "link": "http://arxiv.org/abs/2112.07637v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massive-scale Decoding for Text Generation using Lattices", "abstract": "Conditional neural text generation models generate high-quality outputs, but\noften concentrate around a mode when what we really want is a diverse set of\noptions. We present a search algorithm to construct lattices encoding a massive\nnumber of generation options. First, we restructure decoding as a best-first\nsearch, which explores the space differently than beam search and improves\nefficiency by avoiding pruning paths. Second, we revisit the idea of hypothesis\nrecombination: we can identify pairs of similar generation candidates during\nsearch and merge them as an approximation. On both summarization and machine\ntranslation, we show that our algorithm encodes thousands of diverse options\nthat remain grammatical and high-quality into one lattice. This algorithm\nprovides a foundation for building downstream generation applications on top of\nmassive-scale diverse outputs.", "published": "2021-12-14 18:56:11", "link": "http://arxiv.org/abs/2112.07660v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representing Inferences and their Lexicalization", "abstract": "We have recently begun a project to develop a more effective and efficient\nway to marshal inferences from background knowledge to facilitate deep natural\nlanguage understanding. The meaning of a word is taken to be the entities,\npredications, presuppositions, and potential inferences that it adds to an\nongoing situation. As words compose, the minimal model in the situation evolves\nto limit and direct inference. At this point we have developed our\ncomputational architecture and implemented it on real text. Our focus has been\non proving the feasibility of our design.", "published": "2021-12-14 19:23:43", "link": "http://arxiv.org/abs/2112.07711v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Do Answers to Boolean Questions Need Explanations? Yes", "abstract": "Existing datasets that contain boolean questions, such as BoolQ and TYDI QA ,\nprovide the user with a YES/NO response to the question. However, a one word\nresponse is not sufficient for an explainable system. We promote explainability\nby releasing a new set of annotations marking the evidence in existing TyDi QA\nand BoolQ datasets. We show that our annotations can be used to train a model\nthat extracts improved evidence spans compared to models that rely on existing\nresources. We confirm our findings with a user study which shows that our\nextracted evidence spans enhance the user experience. We also provide further\ninsight into the challenges of answering boolean questions, such as passages\ncontaining conflicting YES and NO answers, and varying degrees of relevance of\nthe predicted evidence.", "published": "2021-12-14 22:40:28", "link": "http://arxiv.org/abs/2112.07772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Online antisemitism across platforms", "abstract": "We created a fine-grained AI system for the detection of antisemitism. This\nExplainable AI will identify English and German anti-Semitic expressions of\ndehumanization, verbal aggression and conspiracies in online social media\nmessages across platforms, to support high-level decision making.", "published": "2021-12-14 23:06:21", "link": "http://arxiv.org/abs/2112.07783v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building on Huang et al. GlossBERT for Word Sense Disambiguation", "abstract": "We propose to take on the problem ofWord Sense Disambiguation (WSD). In\nlanguage, words of the same form can take different meanings depending on\ncontext. While humans easily infer the meaning or gloss of such words by their\ncontext, machines stumble on this task.As such, we intend to replicated and\nexpand upon the results of Huang et al.GlossBERT, a model which they design to\ndisambiguate these words (Huang et al.,2019). Specifically, we propose the\nfollowing augmentations: data-set tweaking(alpha hyper-parameter), ensemble\nmethods, and replacement of BERT with BART andALBERT. The following GitHub\nrepository contains all code used in this report, which extends on the code\nmade available by Huang et al.", "published": "2021-12-14 01:14:20", "link": "http://arxiv.org/abs/2112.07089v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discovering Explanatory Sentences in Legal Case Decisions Using\n  Pre-trained Language Models", "abstract": "Legal texts routinely use concepts that are difficult to understand. Lawyers\nelaborate on the meaning of such concepts by, among other things, carefully\ninvestigating how have they been used in past. Finding text snippets that\nmention a particular concept in a useful way is tedious, time-consuming, and,\nhence, expensive. We assembled a data set of 26,959 sentences, coming from\nlegal case decisions, and labeled them in terms of their usefulness for\nexplaining selected legal concepts. Using the dataset we study the\neffectiveness of transformer-based models pre-trained on large language corpora\nto detect which of the sentences are useful. In light of models' predictions,\nwe analyze various linguistic properties of the explanatory sentences as well\nas their relationship to the legal concept that needs to be explained. We show\nthat the transformer-based models are capable of learning surprisingly\nsophisticated features and outperform the prior approaches to the task.", "published": "2021-12-14 04:56:39", "link": "http://arxiv.org/abs/2112.07165v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained\n  Language Model Compression", "abstract": "Pre-trained Language Models (PLMs) have achieved great success in various\nNatural Language Processing (NLP) tasks under the pre-training and fine-tuning\nparadigm. With large quantities of parameters, PLMs are computation-intensive\nand resource-hungry. Hence, model pruning has been introduced to compress\nlarge-scale PLMs. However, most prior approaches only consider task-specific\nknowledge towards downstream tasks, but ignore the essential task-agnostic\nknowledge during pruning, which may cause catastrophic forgetting problem and\nlead to poor generalization ability. To maintain both task-agnostic and\ntask-specific knowledge in our pruned model, we propose ContrAstive Pruning\n(CAP) under the paradigm of pre-training and fine-tuning. It is designed as a\ngeneral framework, compatible with both structured and unstructured pruning.\nUnified in contrastive learning, CAP enables the pruned model to learn from the\npre-trained model for task-agnostic knowledge, and fine-tuned model for\ntask-specific knowledge. Besides, to better retain the performance of the\npruned model, the snapshots (i.e., the intermediate models at each pruning\niteration) also serve as effective supervisions for pruning. Our extensive\nexperiments show that adopting CAP consistently yields significant\nimprovements, especially in extremely high sparsity scenarios. With only 3%\nmodel parameters reserved (i.e., 97% sparsity), CAP successfully achieves 99.2%\nand 96.3% of the original BERT performance in QQP and MNLI tasks. In addition,\nour probing experiments demonstrate that the model pruned by CAP tends to\nachieve better generalization ability.", "published": "2021-12-14 07:14:09", "link": "http://arxiv.org/abs/2112.07198v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TopNet: Learning from Neural Topic Model to Generate Long Stories", "abstract": "Long story generation (LSG) is one of the coveted goals in natural language\nprocessing. Different from most text generation tasks, LSG requires to output a\nlong story of rich content based on a much shorter text input, and often\nsuffers from information sparsity. In this paper, we propose \\emph{TopNet} to\nalleviate this problem, by leveraging the recent advances in neural topic\nmodeling to obtain high-quality skeleton words to complement the short input.\nIn particular, instead of directly generating a story, we first learn to map\nthe short text input to a low-dimensional topic distribution (which is\npre-assigned by a topic model). Based on this latent topic distribution, we can\nuse the reconstruction decoder of the topic model to sample a sequence of\ninter-related words as a skeleton for the story. Experiments on two benchmark\ndatasets show that our proposed framework is highly effective in skeleton word\nselection and significantly outperforms the state-of-the-art models in both\nautomatic evaluation and human evaluation.", "published": "2021-12-14 09:47:53", "link": "http://arxiv.org/abs/2112.07259v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Model Uncertainty-Aware Knowledge Amalgamation for Pre-Trained Language\n  Models", "abstract": "As many fine-tuned pre-trained language models~(PLMs) with promising\nperformance are generously released, investigating better ways to reuse these\nmodels is vital as it can greatly reduce the retraining computational cost and\nthe potential environmental side-effects. In this paper, we explore a novel\nmodel reuse paradigm, Knowledge Amalgamation~(KA) for PLMs. Without human\nannotations available, KA aims to merge the knowledge from different\nteacher-PLMs, each of which specializes in a different classification problem,\ninto a versatile student model. The achieve this, we design a Model\nUncertainty--aware Knowledge Amalgamation~(MUKA) framework, which identifies\nthe potential adequate teacher using Monte-Carlo Dropout for approximating the\ngolden supervision to guide the student. Experimental results demonstrate that\nMUKA achieves substantial improvements over baselines on benchmark datasets.\nFurther analysis shows that MUKA can generalize well under several complicate\nsettings with multiple teacher models, heterogeneous teachers, and even\ncross-dataset teachers.", "published": "2021-12-14 12:26:24", "link": "http://arxiv.org/abs/2112.07327v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Row, Multi-Span Distant Supervision For Table+Text Question", "abstract": "Question answering (QA) over tables and linked text, also called TextTableQA,\nhas witnessed significant research in recent years, as tables are often found\nembedded in documents along with related text. HybridQA and OTT-QA are the two\nbest-known TextTableQA datasets, with questions that are best answered by\ncombining information from both table cells and linked text passages. A common\nchallenge in both datasets, and TextTableQA in general, is that the training\ninstances include just the question and answer, where the gold answer may match\nnot only multiple table cells across table rows but also multiple text spans\nwithin the scope of a table row and its associated text. This leads to a noisy\nmulti instance training regime. We present MITQA, a transformer-based\nTextTableQA system that is explicitly designed to cope with distant supervision\nalong both these axes, through a multi-instance loss objective, together with\ncareful curriculum design. Our experiments show that the proposed\nmulti-instance distant supervision approach helps MITQA get state-of-the-art\nresults beating the existing baselines for both HybridQA and OTT-QA, putting\nMITQA at the top of HybridQA leaderboard with best EM and F1 scores on a held\nout test set.", "published": "2021-12-14 12:48:19", "link": "http://arxiv.org/abs/2112.07337v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "You Only Need One Model for Open-domain Question Answering", "abstract": "Recent approaches to Open-domain Question Answering refer to an external\nknowledge base using a retriever model, optionally rerank passages with a\nseparate reranker model and generate an answer using another reader model.\nDespite performing related tasks, the models have separate parameters and are\nweakly-coupled during training. We propose casting the retriever and the\nreranker as internal passage-wise attention mechanisms applied sequentially\nwithin the transformer architecture and feeding computed representations to the\nreader, with the hidden representations progressively refined at each stage.\nThis allows us to use a single question answering model trained end-to-end,\nwhich is a more efficient use of model capacity and also leads to better\ngradient flow. We present a pre-training method to effectively train this\narchitecture and evaluate our model on the Natural Questions and TriviaQA open\ndatasets. For a fixed parameter budget, our model outperforms the previous\nstate-of-the-art model by 1.0 and 0.7 exact match scores.", "published": "2021-12-14 13:21:11", "link": "http://arxiv.org/abs/2112.07381v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identification of Biased Terms in News Articles by Comparison of\n  Outlet-specific Word Embeddings", "abstract": "Slanted news coverage, also called media bias, can heavily influence how news\nconsumers interpret and react to the news. To automatically identify biased\nlanguage, we present an exploratory approach that compares the context of\nrelated words. We train two word embedding models, one on texts of left-wing,\nthe other on right-wing news outlets. Our hypothesis is that a word's\nrepresentations in both word embedding spaces are more similar for non-biased\nwords than biased words. The underlying idea is that the context of biased\nwords in different news outlets varies more strongly than the one of non-biased\nwords, since the perception of a word as being biased differs depending on its\ncontext. While we do not find statistical significance to accept the\nhypothesis, the results show the effectiveness of the approach. For example,\nafter a linear mapping of both word embeddings spaces, 31% of the words with\nthe largest distances potentially induce bias. To improve the results, we find\nthat the dataset needs to be significantly larger, and we derive further\nmethodology as future research direction. To our knowledge, this paper presents\nthe first in-depth look at the context of bias words measured by word\nembeddings.", "published": "2021-12-14 13:23:49", "link": "http://arxiv.org/abs/2112.07384v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploring the Limits of Natural Language Inference Based Setup for\n  Few-Shot Intent Detection", "abstract": "Intent Detection is one of the core tasks of dialog systems. Few-shot Intent\nDetection is challenging due to limited number of annotated utterances for\nnovel classes. Generalized Few-shot intent detection is more realistic but\nchallenging setup which aims to discriminate the joint label space of both\nnovel intents which have few examples each and existing intents consisting of\nenough labeled data. Large label spaces and fewer number of shots increase the\ncomplexity of the task. In this work, we employ a simple and effective method\nbased on Natural Language Inference that leverages the semantics in the\nclass-label names to learn and predict the novel classes. Our method achieves\nstate-of-the-art results on 1-shot and 5-shot intent detection task with gains\nranging from 2-8\\% points in F1 score on four benchmark datasets. Our method\nalso outperforms existing approaches on a more practical setting of generalized\nfew-shot intent detection with gains up to 20% F1 score. We show that the\nsuggested approach performs well across single and multi domain datasets with\nthe number of class labels from as few as 7 to as high as 150.", "published": "2021-12-14 14:47:23", "link": "http://arxiv.org/abs/2112.07434v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text Classification Models for Form Entity Linking", "abstract": "Forms are a widespread type of template-based document used in a great\nvariety of fields including, among others, administration, medicine, finance,\nor insurance. The automatic extraction of the information included in these\ndocuments is greatly demanded due to the increasing volume of forms that are\ngenerated in a daily basis. However, this is not a straightforward task when\nworking with scanned forms because of the great diversity of templates with\ndifferent location of form entities, and the quality of the scanned documents.\nIn this context, there is a feature that is shared by all forms: they contain a\ncollection of interlinked entities built as key-value (or label-value) pairs,\ntogether with other entities such as headers or images. In this work, we have\ntacked the problem of entity linking in forms by combining image processing\ntechniques and a text classification model based on the BERT architecture. This\napproach achieves state-of-the-art results with a F1-score of 0.80 on the FUNSD\ndataset, a 5% improvement regarding the best previous method. The code of this\nproject is available at https://github.com/mavillot/FUNSD-Entity-Linking.", "published": "2021-12-14 14:59:44", "link": "http://arxiv.org/abs/2112.07443v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Tackling Query-Focused Summarization as A Knowledge-Intensive Task: A\n  Pilot Study", "abstract": "Query-focused summarization (QFS) requires generating a summary given a query\nusing a set of relevant documents. However, such relevant documents should be\nannotated manually and thus are not readily available in realistic scenarios.\nTo address this limitation, we tackle the QFS task as a knowledge-intensive\n(KI) task without access to any relevant documents. Instead, we assume that\nthese documents are present in a large-scale knowledge corpus and should be\nretrieved first. To explore this new setting, we build a new dataset (KI-QFS)\nby adapting existing QFS datasets. In this dataset, answering the query\nrequires document retrieval from a knowledge corpus. We construct three\ndifferent knowledge corpora, and we further provide relevance annotations to\nenable retrieval evaluation. Finally, we benchmark the dataset with\nstate-of-the-art QFS models and retrieval-enhanced models. The experimental\nresults demonstrate that QFS models perform significantly worse on KI-QFS\ncompared to the original QFS task, indicating that the knowledge-intensive\nsetting is much more challenging and offers substantial room for improvement.\nWe believe that our investigation will inspire further research into addressing\nQFS in more realistic scenarios.", "published": "2021-12-14 16:49:47", "link": "http://arxiv.org/abs/2112.07536v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "VALSE: A Task-Independent Benchmark for Vision and Language Models\n  Centered on Linguistic Phenomena", "abstract": "We propose VALSE (Vision And Language Structured Evaluation), a novel\nbenchmark designed for testing general-purpose pretrained vision and language\n(V&L) models for their visio-linguistic grounding capabilities on specific\nlinguistic phenomena. VALSE offers a suite of six tests covering various\nlinguistic constructs. Solving these requires models to ground linguistic\nphenomena in the visual modality, allowing more fine-grained evaluations than\nhitherto possible. We build VALSE using methods that support the construction\nof valid foils, and report results from evaluating five widely-used V&L models.\nOur experiments suggest that current models have considerable difficulty\naddressing most phenomena. Hence, we expect VALSE to serve as an important\nbenchmark to measure future progress of pretrained V&L models from a linguistic\nperspective, complementing the canonical task-centred V&L evaluations.", "published": "2021-12-14 17:15:04", "link": "http://arxiv.org/abs/2112.07566v2", "categories": ["cs.CL", "cs.CV", "68Txx", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of\n  Dense Retrieval", "abstract": "Dense retrieval approaches can overcome the lexical gap and lead to\nsignificantly improved search results. However, they require large amounts of\ntraining data which is not available for most domains. As shown in previous\nwork (Thakur et al., 2021b), the performance of dense retrievers severely\ndegrades under a domain shift. This limits the usage of dense retrieval\napproaches to only a few domains with large training datasets.\n  In this paper, we propose the novel unsupervised domain adaptation method\nGenerative Pseudo Labeling (GPL), which combines a query generator with pseudo\nlabeling from a cross-encoder. On six representative domain-specialized\ndatasets, we find the proposed GPL can outperform an out-of-the-box\nstate-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL\nrequires less (unlabeled) data from the target domain and is more robust in its\ntraining than previous methods.\n  We further investigate the role of six recent pre-training methods in the\nscenario of domain adaptation for retrieval tasks, where only three could yield\nimproved results. The best approach, TSDAE (Wang et al., 2021) can be combined\nwith GPL, yielding another average improvement of 1.4 points nDCG@10 across the\nsix tasks. The code and the models are available at\nhttps://github.com/UKPLab/gpl.", "published": "2021-12-14 17:34:43", "link": "http://arxiv.org/abs/2112.07577v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Dual-Key Multimodal Backdoors for Visual Question Answering", "abstract": "The success of deep learning has enabled advances in multimodal tasks that\nrequire non-trivial fusion of multiple input domains. Although multimodal\nmodels have shown potential in many problems, their increased complexity makes\nthem more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of\nsecurity vulnerability wherein an attacker embeds a malicious secret behavior\ninto a network (e.g. targeted misclassification) that is activated when an\nattacker-specified trigger is added to an input. In this work, we show that\nmultimodal networks are vulnerable to a novel type of attack that we refer to\nas Dual-Key Multimodal Backdoors. This attack exploits the complex fusion\nmechanisms used by state-of-the-art networks to embed backdoors that are both\neffective and stealthy. Instead of using a single trigger, the proposed attack\nembeds a trigger in each of the input modalities and activates the malicious\nbehavior only when both the triggers are present. We present an extensive study\nof multimodal backdoors on the Visual Question Answering (VQA) task with\nmultiple architectures and visual feature backbones. A major challenge in\nembedding backdoors in VQA models is that most models use visual features\nextracted from a fixed pretrained object detector. This is challenging for the\nattacker as the detector can distort or ignore the visual trigger entirely,\nwhich leads to models where backdoors are over-reliant on the language trigger.\nWe tackle this problem by proposing a visual trigger optimization strategy\ndesigned for pretrained object detectors. Through this method, we create\nDual-Key Backdoors with over a 98% attack success rate while only poisoning 1%\nof the training data. Finally, we release TrojVQA, a large collection of clean\nand trojan VQA models to enable research in defending against multimodal\nbackdoors.", "published": "2021-12-14 18:59:52", "link": "http://arxiv.org/abs/2112.07668v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning to Retrieve Passages without Supervision", "abstract": "Dense retrievers for open-domain question answering (ODQA) have been shown to\nachieve impressive performance by training on large datasets of\nquestion-passage pairs. In this work we ask whether this dependence on labeled\ndata can be reduced via unsupervised pretraining that is geared towards ODQA.\nWe show this is in fact possible, via a novel pretraining scheme designed for\nretrieval. Our \"recurring span retrieval\" approach uses recurring spans across\npassages in a document to create pseudo examples for contrastive learning. Our\npretraining scheme directly controls for term overlap across pseudo queries and\nrelevant passages, thus allowing to model both lexical and semantic relations\nbetween them. The resulting model, named Spider, performs surprisingly well\nwithout any labeled training examples on a wide range of ODQA datasets.\nSpecifically, it significantly outperforms all other pretrained baselines in a\nzero-shot setting, and is competitive with BM25, a strong sparse baseline.\nMoreover, a hybrid retriever over Spider and BM25 improves over both, and is\noften competitive with DPR models, which are trained on tens of thousands of\nexamples. Last, notable gains are observed when using Spider as an\ninitialization for supervised training.", "published": "2021-12-14 19:18:08", "link": "http://arxiv.org/abs/2112.07708v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Classifying Emails into Human vs Machine Category", "abstract": "It is an essential product requirement of Yahoo Mail to distinguish between\npersonal and machine-generated emails. The old production classifier in Yahoo\nMail was based on a simple logistic regression model. That model was trained by\naggregating features at the SMTP address level. We propose building deep\nlearning models at the message level. We built and trained four individual CNN\nmodels: (1) a content model with subject and content as input; (2) a sender\nmodel with sender email address and name as input; (3) an action model by\nanalyzing email recipients' action patterns and correspondingly generating\ntarget labels based on senders' opening/deleting behaviors; (4) a salutation\nmodel by utilizing senders' \"explicit salutation\" signal as positive labels.\nNext, we built a final full model after exploring different combinations of the\nabove four models. Experimental results on editorial data show that our full\nmodel improves the adjusted-recall from 70.5% to 78.8% compared to the old\nproduction model, while at the same time lifts the precision from 94.7% to\n96.0%. Our full model also significantly beats the state-of-the-art Bert model\nat this task. This full model has been deployed into the current production\nsystem (Yahoo Mail 6).", "published": "2021-12-14 20:55:35", "link": "http://arxiv.org/abs/2112.07742v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Boosted Dense Retriever", "abstract": "We propose DrBoost, a dense retrieval ensemble inspired by boosting. DrBoost\nis trained in stages: each component model is learned sequentially and\nspecialized by focusing only on retrieval mistakes made by the current\nensemble. The final representation is the concatenation of the output vectors\nof all the component models, making it a drop-in replacement for standard dense\nretrievers at test time. DrBoost enjoys several advantages compared to standard\ndense retrieval models. It produces representations which are 4x more compact,\nwhile delivering comparable retrieval results. It also performs surprisingly\nwell under approximate search with coarse quantization, reducing latency and\nbandwidth needs by another 4x. In practice, this can make the difference\nbetween serving indices from disk versus from memory, paving the way for much\ncheaper deployments.", "published": "2021-12-14 22:39:57", "link": "http://arxiv.org/abs/2112.07771v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Maximum Bayes Smatch Ensemble Distillation for AMR Parsing", "abstract": "AMR parsing has experienced an unprecendented increase in performance in the\nlast three years, due to a mixture of effects including architecture\nimprovements and transfer learning. Self-learning techniques have also played a\nrole in pushing performance forward. However, for most recent high performant\nparsers, the effect of self-learning and silver data augmentation seems to be\nfading. In this paper we propose to overcome this diminishing returns of silver\ndata by combining Smatch-based ensembling techniques with ensemble\ndistillation. In an extensive experimental setup, we push single model English\nparser performance to a new state-of-the-art, 85.9 (AMR2.0) and 84.3 (AMR3.0),\nand return to substantial gains from silver data augmentation. We also attain a\nnew state-of-the-art for cross-lingual AMR parsing for Chinese, German, Italian\nand Spanish. Finally we explore the impact of the proposed technique on domain\nadaptation, and show that it can produce gains rivaling those of human\nannotated data for QALD-9 and achieve a new state-of-the-art for BioAMR.", "published": "2021-12-14 23:29:37", "link": "http://arxiv.org/abs/2112.07790v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BACON: Deep-Learning Powered AI for Poetry Generation with Author\n  Linguistic Style Transfer", "abstract": "This paper describes BACON, a basic prototype of an automatic poetry\ngenerator with author linguistic style transfer. It combines concepts and\ntechniques from finite state machinery, probabilistic models, artificial neural\nnetworks and deep learning, to write original poetry with rich\naesthetic-qualities in the style of any given author. Extrinsic evaluation of\nthe output generated by BACON shows that participants were unable to tell the\ndifference between human and AI-generated poems in any statistically\nsignificant way.", "published": "2021-12-14 00:08:36", "link": "http://arxiv.org/abs/2112.11483v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-shot Multi-hop Question Answering over Knowledge Base", "abstract": "KBQA is a task that requires to answer questions by using semantic structured\ninformation in knowledge base. Previous work in this area has been restricted\ndue to the lack of large semantic parsing dataset and the exponential growth of\nsearching space with the increasing hops of relation paths. In this paper, we\npropose an efficient pipeline method equipped with a pre-trained language\nmodel. By adopting Beam Search algorithm, the searching space will not be\nrestricted in subgraph of 3 hops. Besides, we propose a data generation\nstrategy, which enables our model to generalize well from few training samples.\nWe evaluate our model on an open-domain complex Chinese Question Answering task\nCCKS2019 and achieve F1-score of 62.55% on the test dataset. In addition, in\norder to test the few-shot learning capability of our model, we ramdomly select\n10% of the primary data to train our model, the result shows that our model can\nstill achieves F1-score of 58.54%, which verifies the capability of our model\nto process KBQA task and the advantage in few-shot Learning.", "published": "2021-12-14 00:56:54", "link": "http://arxiv.org/abs/2112.11909v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Interactive Language Modeling", "abstract": "Interaction between caregivers and children plays a critical role in human\nlanguage acquisition and development. Given this observation, it is remarkable\nthat explicit interaction plays little to no role in artificial language\nmodeling -- which also targets the acquisition of human language, yet by\nartificial models. Moreover, an interactive approach to language modeling has\nthe potential to make language models substantially more versatile and to\nconsiderably impact downstream applications. Motivated by these considerations,\nwe pioneer the space of interactive language modeling. As a first contribution\nwe present a road map in which we detail the steps that need to be taken\ntowards interactive language modeling. We then lead by example and take the\nfirst steps on this road map, showing the initial feasibility of our approach.\nAs such, this work aims to be the start of a larger research agenda on\ninteractive language modeling.", "published": "2021-12-14 18:35:02", "link": "http://arxiv.org/abs/2112.11911v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Hybrid CTC/Attention End-to-end Speech Recognition with\n  Pretrained Acoustic and Language Model", "abstract": "Recently, self-supervised pretraining has achieved impressive results in\nend-to-end (E2E) automatic speech recognition (ASR). However, the dominant\nsequence-to-sequence (S2S) E2E model is still hard to fully utilize the\nself-supervised pre-training methods because its decoder is conditioned on\nacoustic representation thus cannot be pretrained separately. In this paper, we\npropose a pretrained Transformer (Preformer) S2S ASR architecture based on\nhybrid CTC/attention E2E models to fully utilize the pretrained acoustic models\n(AMs) and language models (LMs). In our framework, the encoder is initialized\nwith a pretrained AM (wav2vec2.0). The Preformer leverages CTC as an auxiliary\ntask during training and inference. Furthermore, we design a one-cross decoder\n(OCD), which relaxes the dependence on acoustic representations so that it can\nbe initialized with pretrained LM (DistilGPT2). Experiments are conducted on\nthe AISHELL-1 corpus and achieve a $4.6\\%$ character error rate (CER) on the\ntest set. Compared with our vanilla hybrid CTC/attention Transformer baseline,\nour proposed CTC/attention-based Preformer yields $27\\%$ relative CER\nreduction. To the best of our knowledge, this is the first work to utilize both\npretrained AM and LM in a S2S ASR system.", "published": "2021-12-14 09:38:31", "link": "http://arxiv.org/abs/2112.07254v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in\n  Pretrained Language Models", "abstract": "An increasing awareness of biased patterns in natural language processing\nresources, like BERT, has motivated many metrics to quantify `bias' and\n`fairness'. But comparing the results of different metrics and the works that\nevaluate with such metrics remains difficult, if not outright impossible. We\nsurvey the existing literature on fairness metrics for pretrained language\nmodels and experimentally evaluate compatibility, including both biases in\nlanguage models as in their downstream tasks. We do this by a mixture of\ntraditional literature survey and correlation analysis, as well as by running\nempirical evaluations. We find that many metrics are not compatible and highly\ndepend on (i) templates, (ii) attribute and target seeds and (iii) the choice\nof embeddings. These results indicate that fairness or bias evaluation remains\nchallenging for contextualized language models, if not at least highly\nsubjective. To improve future comparisons and fairness evaluations, we\nrecommend avoiding embedding-based metrics and focusing on fairness evaluations\nin downstream tasks.", "published": "2021-12-14 15:04:56", "link": "http://arxiv.org/abs/2112.07447v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoCo-BERT: Improving Video-Language Pre-training with Contrastive\n  Cross-modal Matching and Denoising", "abstract": "BERT-type structure has led to the revolution of vision-language pre-training\nand the achievement of state-of-the-art results on numerous vision-language\ndownstream tasks. Existing solutions dominantly capitalize on the multi-modal\ninputs with mask tokens to trigger mask-based proxy pre-training tasks (e.g.,\nmasked language modeling and masked object/frame prediction). In this work, we\nargue that such masked inputs would inevitably introduce noise for cross-modal\nmatching proxy task, and thus leave the inherent vision-language association\nunder-explored. As an alternative, we derive a particular form of cross-modal\nproxy objective for video-language pre-training, i.e., Contrastive Cross-modal\nmatching and denoising (CoCo). By viewing the masked frame/word sequences as\nthe noisy augmentation of primary unmasked ones, CoCo strengthens\nvideo-language association by simultaneously pursuing inter-modal matching and\nintra-modal denoising between masked and unmasked inputs in a contrastive\nmanner. Our CoCo proxy objective can be further integrated into any BERT-type\nencoder-decoder structure for video-language pre-training, named as Contrastive\nCross-modal BERT (CoCo-BERT). We pre-train CoCo-BERT on TV dataset and a newly\ncollected large-scale GIF video dataset (ACTION). Through extensive experiments\nover a wide range of downstream tasks (e.g., cross-modal retrieval, video\nquestion answering, and video captioning), we demonstrate the superiority of\nCoCo-BERT as a pre-trained structure.", "published": "2021-12-14 16:22:44", "link": "http://arxiv.org/abs/2112.07515v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "On the Use of External Data for Spoken Named Entity Recognition", "abstract": "Spoken language understanding (SLU) tasks involve mapping from speech audio\nsignals to semantic labels. Given the complexity of such tasks, good\nperformance might be expected to require large labeled datasets, which are\ndifficult to collect for each new task and domain. However, recent advances in\nself-supervised speech representations have made it feasible to consider\nlearning SLU models with limited labeled data. In this work we focus on\nlow-resource spoken named entity recognition (NER) and address the question:\nBeyond self-supervised pre-training, how can we use external speech and/or text\ndata that are not annotated for the task? We draw on a variety of approaches,\nincluding self-training, knowledge distillation, and transfer learning, and\nconsider their applicability to both end-to-end models and pipeline (speech\nrecognition followed by text NER model) approaches. We find that several of\nthese approaches improve performance in resource-constrained settings beyond\nthe benefits from pre-trained representations alone. Compared to prior work, we\nfind improved F1 scores of up to 16%. While the best baseline model is a\npipeline approach, the best performance when using external data is ultimately\nachieved by an end-to-end model. We provide detailed comparisons and analyses,\nshowing for example that end-to-end models are able to focus on the more\nNER-specific words.", "published": "2021-12-14 18:49:26", "link": "http://arxiv.org/abs/2112.07648v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "How are cities pledging net zero? A computational approach to analyzing\n  subnational climate strategies", "abstract": "Cities have become primary actors on climate change and are increasingly\nsetting goals aimed at net-zero emissions. The rapid proliferation of\nsubnational governments \"racing to zero\" emissions and articulating their own\nclimate mitigation plans warrants closer examination to understand how these\nactors intend to meet these goals. The scattered, incomplete and heterogeneous\nnature of city climate policy documents, however, has made their systemic\nanalysis challenging. We analyze 318 climate action documents from cities that\nhave pledged net-zero targets or joined a transnational climate initiative with\nthis goal using machine learning-based natural language processing (NLP)\ntechniques. We use these approaches to accomplish two primary goals: 1)\ndetermine text patterns that predict \"ambitious\" net-zero targets, where we\ndefine an ambitious target as one that encompasses a subnational government's\neconomy-wide emissions; and 2) perform a sectoral analysis to identify patterns\nand trade-offs in climate action themes (i.e., land-use, industry, buildings,\netc.). We find that cities that have defined ambitious climate actions tend to\nemphasize quantitative metrics and specific high-emitting sectors in their\nplans, supported by mentions of governance and citizen participation. Cities\npredominantly emphasize energy-related actions in their plans, particularly in\nthe buildings, transport and heating sectors, but often at the expense of other\nsectors, including land-use and climate impacts. The method presented in this\npaper provides a replicable, scalable approach to analyzing climate action\nplans and a first step towards facilitating cross-city learning.", "published": "2021-12-14 21:33:39", "link": "http://arxiv.org/abs/2112.11207v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "stat.AP"], "primary_category": "cs.CY"}
{"title": "Explore Long-Range Context feature for Speaker Verification", "abstract": "Capturing long-range dependency and modeling long temporal contexts is proven\nto benefit speaker verification tasks. In this paper, we propose the\ncombination of the Hierarchical-Split block(HS-block) and the Depthwise\nSeparable Self-Attention(DSSA) module to capture richer multi-range context\nspeaker features from a local and global perspective respectively.\nSpecifically, the HS-block splits the feature map and filters into several\ngroups and stacks them in one block, which enlarges the receptive fields(RFs)\nlocally. The DSSA module improves the multi-head self-attention mechanism by\nthe depthwise-separable strategy and explicit sparse attention strategy to\nmodel the pairwise relations globally and captures effective long-range\ndependencies in each channel. Experiments are conducted on the Voxceleb and\nSITW. Our best system achieves 1.27% EER on the Voxceleb1 test set and 1.56% on\nSITW by applying the combination of HS-block and DSSA module.", "published": "2021-12-14 03:08:39", "link": "http://arxiv.org/abs/2112.07134v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spatiogram: A phase based directional angular measure and perceptual\n  weighting for ensemble source width", "abstract": "In concert hall studies, inter-aural cross-correlation (IACC), which is\nsignal dependent, is used as a measure of perceptual source width. The same\nmeasure is used for perceptual source width in the case of distributed sources\nalso. In this work, we examine the validity of IACC for both the cases and\ndevelop an improved measure for ensemble-like distributed sources. We decompose\nthe new objective measure for perceptual ensemble source width (ESW) into two\ncomponents (i) phase based directional angular measure, which is timbre\nindependent (spatial measure) and (ii) mean time-bandwidth energy (MTBE), a\nperceptual weight, (timbre measure). This combination of spatial and timbral\nmeasures can be extended as an alternate measure for determining auditory\nsource width (ASW) and listener envelopment (LEV) of arbitrary signals in\nconcert-hall and room acoustics.", "published": "2021-12-14 08:00:36", "link": "http://arxiv.org/abs/2112.07216v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-end speaker diarization with transformer", "abstract": "Speaker diarization is connected to semantic segmentation in computer vision.\nInspired from MaskFormer \\cite{cheng2021per} which treats semantic segmentation\nas a set-prediction problem, we propose an end-to-end approach to predict a set\nof targets consisting of binary masks, vocal activities and speaker vectors.\nOur model, which we coin \\textit{DiFormer}, is mainly based on a speaker\nencoder and a feature pyramid network (FPN) module to extract multi-scale\nspeaker features which are then fed into a transformer encoder-decoder to\npredict a set of diarization targets from learned query embedding. To account\nfor temporal characteristics of speech signal, bidirectional LSTMs are inserted\ninto the mask prediction module to improve temporal consistency. Our model\nhandles unknown number of speakers, speech overlaps, as well as vocal activity\ndetection in a unified way. Experiments on multimedia and meeting datasets\ndemonstrate the effectiveness of our approach.", "published": "2021-12-14 15:23:46", "link": "http://arxiv.org/abs/2112.07463v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A literature review on COVID-19 disease diagnosis from respiratory sound\n  data", "abstract": "The World Health Organization (WHO) has announced a COVID-19 was a global\npandemic in March 2020. It was initially started in china in the year 2019\nDecember and affected an expanding number of nations in various countries in\nthe last few months. In this particular situation, many techniques, methods,\nand AI-based classification algorithms are put in the spotlight in reacting to\nfight against it and reduce the rate of such a global health crisis. COVID-19's\nmain signs are heavy temperature, different cough, cold, breathing shortness,\nand a combination of loss of sense of smell and chest tightness. The digital\nworld is growing day by day, in this context digital stethoscope can read all\nof these symptoms and diagnose respiratory disease. In this study, we majorly\nfocus on literature reviews of how SARS-CoV-2 is spreading and in-depth\nanalysis of the diagnosis of COVID-19 disease from human respiratory sounds\nlike cough, voice, and breath by analyzing the respiratory sound parameters. We\nhope this review will provide an initiative for the clinical scientists and\nresearcher's community to initiate open access, scalable, and accessible work\nin the collective battle against COVID-19.", "published": "2021-12-14 10:30:21", "link": "http://arxiv.org/abs/2112.07670v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real-Time Neural Voice Camouflage", "abstract": "Automatic speech recognition systems have created exciting possibilities for\napplications, however they also enable opportunities for systematic\neavesdropping. We propose a method to camouflage a person's voice over-the-air\nfrom these systems without inconveniencing the conversation between people in\nthe room. Standard adversarial attacks are not effective in real-time streaming\nsituations because the characteristics of the signal will have changed by the\ntime the attack is executed. We introduce predictive attacks, which achieve\nreal-time performance by forecasting the attack that will be the most effective\nin the future. Under real-time constraints, our method jams the established\nspeech recognition system DeepSpeech 3.9x more than baselines as measured\nthrough word error rate, and 6.6x more as measured through character error\nrate. We furthermore demonstrate our approach is practically effective in\nrealistic environments over physical distances.", "published": "2021-12-14 00:27:44", "link": "http://arxiv.org/abs/2112.07076v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ImportantAug: a data augmentation agent for speech", "abstract": "We introduce ImportantAug, a technique to augment training data for speech\nclassification and recognition models by adding noise to unimportant regions of\nthe speech and not to important regions. Importance is predicted for each\nutterance by a data augmentation agent that is trained to maximize the amount\nof noise it adds while minimizing its impact on recognition performance. The\neffectiveness of our method is illustrated on version two of the Google Speech\nCommands (GSC) dataset. On the standard GSC test set, it achieves a 23.3%\nrelative error rate reduction compared to conventional noise augmentation which\napplies noise to speech without regard to where it might be most effective. It\nalso provides a 25.4% error rate reduction compared to a baseline without data\naugmentation. Additionally, the proposed ImportantAug outperforms the\nconventional noise augmentation and the baseline on two test sets with\nadditional noise added.", "published": "2021-12-14 04:37:04", "link": "http://arxiv.org/abs/2112.07156v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Embedding-based Music Emotion Recognition Using Composite Loss", "abstract": "Most music emotion recognition approaches perform classification or\nregression that estimates a general emotional category from a distribution of\nmusic samples, but without considering emotional variations (e.g., happiness\ncan be further categorised into much, moderate or little happiness). We propose\nan embedding-based music emotion recognition approach that associates music\nsamples with emotions in a common embedding space by considering both general\nemotional categories and fine-grained discrimination within each category.\nSince the association of music samples with emotions is uncertain due to\nsubjective human perceptions, we compute composite loss-based embeddings\nobtained to maximise two statistical characteristics, one being the correlation\nbetween music samples and emotions based on canonical correlation analysis, and\nthe other being a probabilistic similarity between a music sample and an\nemotion with KL-divergence. The experiments on two benchmark datasets\ndemonstrate the effectiveness of our embedding-based approach, the composite\nloss and learned acoustic features. In addition, detailed analysis shows that\nour approach can accomplish robust bidirectional music emotion recognition that\nnot only identifies music samples matching with a specific emotion but also\ndetects emotions expressed in a certain music sample.", "published": "2021-12-14 06:54:08", "link": "http://arxiv.org/abs/2112.07192v5", "categories": ["cs.SD", "cs.HC", "cs.MM", "eess.AS", "H.5.5; H.5.1; H.3.1"], "primary_category": "cs.SD"}
{"title": "Noise Reduction and Driving Event Extraction Method for Performance\n  Improvement on Driving Noise-based Surface Anomaly Detection", "abstract": "Foreign substances on the road surface, such as rainwater or black ice,\nreduce the friction between the tire and the surface. The above situation will\nreduce the braking performance and make difficult to control the vehicle body\nposture. In that case, there is a possibility of property damage at least. In\nthe worst case, personal damage will be occured. To avoid this problem, a road\nanomaly detection model is proposed based on vehicle driving noise. However,\nthe prior proposal does not consider the extra noise, mixed with driving noise,\nand skipping calculations for moments without vehicle driving. In this paper,\nwe propose a simple driving event extraction method and noise reduction method\nfor improving computational efficiency and anomaly detection performance.", "published": "2021-12-14 07:50:30", "link": "http://arxiv.org/abs/2112.07214v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic COVID-19 disease diagnosis using 1D convolutional neural\n  network and augmentation with human respiratory sound based on parameters:\n  cough, breath, and voice", "abstract": "The issue in respiratory sound classification has attained good attention\nfrom the clinical scientists and medical researcher's group in the last year to\ndiagnosing COVID-19 disease. To date, various models of Artificial Intelligence\n(AI) entered into the real-world to detect the COVID-19 disease from\nhuman-generated sounds such as voice/speech, cough, and breath. The\nConvolutional Neural Network (CNN) model is implemented for solving a lot of\nreal-world problems on machines based on Artificial Intelligence (AI). In this\ncontext, one dimension (1D) CNN is suggested and implemented to diagnose\nrespiratory diseases of COVID-19 from human respiratory sounds such as a voice,\ncough, and breath. An augmentation-based mechanism is applied to improve the\npreprocessing performance of the COVID-19 sounds dataset and to automate\nCOVID-19 disease diagnosis using the 1D convolutional network. Furthermore, a\nDDAE (Data De-noising Auto Encoder) technique is used to generate deep sound\nfeatures such as the input function to the 1D CNN instead of adopting the\nstandard input of MFCC (Mel-frequency cepstral coefficient), and it is\nperformed better accuracy and performance than previous models.", "published": "2021-12-14 10:41:04", "link": "http://arxiv.org/abs/2112.07285v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "Supervised Learning for Multi Zone Sound Field Reproduction under Harsh\n  Environmental Conditions", "abstract": "This manuscript presents an approach for multi zone sound field reproduction\nusing supervised learning. Traditional multi zone sound field reproduction\nmethods assume constant speed of sound, neglecting nonlinear effects like wind\nand temperature stratification. We show how to overcome these restrictions\nusing supervised learning of transfer functions. The quality of the solution is\nmeasured by the acoustic contrast and the reproduction error. Our results show\nthat for the chosen setup, even with relatively small wind speeds, the acoustic\ncontrast and reproduction error can be improved by up to 16 dB, when wind is\nconsidered in the trained model.", "published": "2021-12-14 13:07:53", "link": "http://arxiv.org/abs/2112.07349v1", "categories": ["cs.SD", "eess.AS", "physics.flu-dyn"], "primary_category": "cs.SD"}
{"title": "Robustifying automatic speech recognition by extracting slowly varying\n  features", "abstract": "In the past few years, it has been shown that deep learning systems are\nhighly vulnerable under attacks with adversarial examples. Neural-network-based\nautomatic speech recognition (ASR) systems are no exception. Targeted and\nuntargeted attacks can modify an audio input signal in such a way that humans\nstill recognise the same words, while ASR systems are steered to predict a\ndifferent transcription. In this paper, we propose a defense mechanism against\ntargeted adversarial attacks consisting in removing fast-changing features from\nthe audio signals, either by applying slow feature analysis, a low-pass filter,\nor both, before feeding the input to the ASR system. We perform an empirical\nanalysis of hybrid ASR models trained on data pre-processed in such a way.\nWhile the resulting models perform quite well on benign data, they are\nsignificantly more robust against targeted adversarial attacks: Our final,\nproposed model shows a performance on clean data similar to the baseline model,\nwhile being more than four times more robust.", "published": "2021-12-14 13:50:23", "link": "http://arxiv.org/abs/2112.07400v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Visualizing Ensemble Predictions of Music Mood", "abstract": "Music mood classification has been a challenging problem in comparison with\nother music classification problems (e.g., genre, composer, or period). One\nsolution for addressing this challenge is to use an ensemble of machine\nlearning models. In this paper, we show that visualization techniques can\neffectively convey the popular prediction as well as uncertainty at different\nmusic sections along the temporal axis while enabling the analysis of\nindividual ML models in conjunction with their application to different musical\ndata. In addition to the traditional visual designs, such as stacked line\ngraph, ThemeRiver, and pixel-based visualization, we introduce a new variant of\nThemeRiver, called \"dual-flux ThemeRiver\", which allows viewers to observe and\nmeasure the most popular prediction more easily than stacked line graph and\nThemeRiver. Together with pixel-based visualization, dual-flux ThemeRiver plots\ncan also assist in model-development workflows, in addition to annotating music\nusing ensemble model predictions.", "published": "2021-12-14 18:13:21", "link": "http://arxiv.org/abs/2112.07627v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
