{"title": "A Survey of Code-switching: Linguistic and Social Perspectives for\n  Language Technologies", "abstract": "The analysis of data in which multiple languages are represented has gained\npopularity among computational linguists in recent years. So far, much of this\nresearch focuses mainly on the improvement of computational methods and largely\nignores linguistic and social aspects of C-S discussed across a wide range of\nlanguages within the long-established literature in linguistics. To fill this\ngap, we offer a survey of code-switching (C-S) covering the literature in\nlinguistics with a reflection on the key issues in language technologies. From\nthe linguistic perspective, we provide an overview of structural and functional\npatterns of C-S focusing on the literature from European and Indian contexts as\nhighly multilingual areas. From the language technologies perspective, we\ndiscuss how massive language models fail to represent diverse C-S types due to\nlack of appropriate training data, lack of robust evaluation benchmarks for C-S\n(across multilingual situations and types of C-S) and lack of end-to-end\nsystems that cover sociolinguistic aspects of C-S as well. Our survey will be a\nstep towards an outcome of mutual benefit for computational scientists and\nlinguists with a shared interest in multilingualism and C-S.", "published": "2023-01-05 09:08:04", "link": "http://arxiv.org/abs/2301.01967v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion-Cause Pair Extraction as Question Answering", "abstract": "The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all\npotential emotion-cause pairs of a document without any annotation of emotion\nor cause clauses. Previous approaches on ECPE have tried to improve\nconventional two-step processing schemes by using complex architectures for\nmodeling emotion-cause interaction. In this paper, we cast the ECPE task to the\nquestion answering (QA) problem and propose simple yet effective BERT-based\nsolutions to tackle it. Given a document, our Guided-QA model first predicts\nthe best emotion clause using a fixed question. Then the predicted emotion is\nused as a question to predict the most potential cause for the emotion. We\nevaluate our model on a standard ECPE corpus. The experimental results show\nthat despite its simplicity, our Guided-QA achieves promising results and is\neasy to reproduce. The code of Guided-QA is also provided.", "published": "2023-01-05 09:33:41", "link": "http://arxiv.org/abs/2301.01982v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HIT-SCIR at MMNLU-22: Consistency Regularization for Multilingual Spoken\n  Language Understanding", "abstract": "Multilingual spoken language understanding (SLU) consists of two sub-tasks,\nnamely intent detection and slot filling. To improve the performance of these\ntwo sub-tasks, we propose to use consistency regularization based on a hybrid\ndata augmentation strategy. The consistency regularization enforces the\npredicted distributions for an example and its semantically equivalent\naugmentation to be consistent. We conduct experiments on the MASSIVE dataset\nunder both full-dataset and zero-shot settings. Experimental results\ndemonstrate that our proposed method improves the performance on both intent\ndetection and slot filling tasks. Our system\\footnote{The code will be\navailable at \\url{https://github.com/bozheng-hit/MMNLU-22-HIT-SCIR}.} ranked\n1st in the MMNLU-22 competition under the full-dataset setting.", "published": "2023-01-05 11:21:15", "link": "http://arxiv.org/abs/2301.02010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TextDescriptives: A Python package for calculating a large variety of\n  metrics from text", "abstract": "TextDescriptives is a Python package for calculating a large variety of\nmetrics from text. It is built on top of spaCy and can be easily integrated\ninto existing workflows. The package has already been used for analysing the\nlinguistic stability of clinical texts, creating features for predicting\nneuropsychiatric conditions, and analysing linguistic goals of primary school\nstudents. This paper describes the package and its features.", "published": "2023-01-05 13:19:17", "link": "http://arxiv.org/abs/2301.02057v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Broadcast News Summarization; a comparative study on\n  Maximal Marginal Relevance (MMR) and Latent Semantic Analysis (LSA)", "abstract": "The methods of automatic speech summarization are classified into two groups:\nsupervised and unsupervised methods. Supervised methods are based on a set of\nfeatures, while unsupervised methods perform summarization based on a set of\nrules. Latent Semantic Analysis (LSA) and Maximal Marginal Relevance (MMR) are\nconsidered the most important and well-known unsupervised methods in automatic\nspeech summarization. This study set out to investigate the performance of two\naforementioned unsupervised methods in transcriptions of Persian broadcast news\nsummarization. The results show that in generic summarization, LSA outperforms\nMMR, and in query-based summarization, MMR outperforms LSA in broadcast news\nsummarization.", "published": "2023-01-05 20:13:44", "link": "http://arxiv.org/abs/2301.02284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The political ideology of conversational AI: Converging evidence on\n  ChatGPT's pro-environmental, left-libertarian orientation", "abstract": "Conversational artificial intelligence (AI) disrupts how humans interact with\ntechnology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue\nmodel that can converse with its human counterparts with unprecedented\ncapabilities. ChatGPT has witnessed tremendous attention from the media,\nacademia, industry, and the general public, attracting more than a million\nusers within days of its release. However, its explosive adoption for\ninformation search and as an automated decision aid underscores the importance\nto understand its limitations and biases. This paper focuses on one of\ndemocratic society's most important decision-making processes: political\nelections. Prompting ChatGPT with 630 political statements from two leading\nvoting advice applications and the nation-agnostic political compass test in\nthree pre-registered experiments, we uncover ChatGPT's pro-environmental,\nleft-libertarian ideology. For example, ChatGPT would impose taxes on flights,\nrestrict rent increases, and legalize abortion. In the 2021 elections, it would\nhave voted most likely for the Greens both in Germany (B\\\"undnis 90/Die\nGr\\\"unen) and in the Netherlands (GroenLinks). Our findings are robust when\nnegating the prompts, reversing the order of the statements, varying prompt\nformality, and across languages (English, German, Dutch, and Spanish). We\nconclude by discussing the implications of politically biased conversational AI\non society.", "published": "2023-01-05 07:13:13", "link": "http://arxiv.org/abs/2301.01768v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Topic Segmentation Model Focusing on Local Context", "abstract": "Topic segmentation is important in understanding scientific documents since\nit can not only provide better readability but also facilitate downstream tasks\nsuch as information retrieval and question answering by creating appropriate\nsections or paragraphs. In the topic segmentation task, topic coherence is\ncritical in predicting segmentation boundaries. Most of the existing models\nhave tried to exploit as many contexts as possible to extract useful\ntopic-related information. However, additional context does not always bring\npromising results, because the local context between sentences becomes\nincoherent despite more sentences being supplemented. To alleviate this issue,\nwe propose siamese sentence embedding layers which process two input sentences\nindependently to get appropriate amount of information without being hampered\nby excessive information. Also, we adopt multi-task learning techniques\nincluding Same Topic Prediction (STP), Topic Classification (TC) and Next\nSentence Prediction (NSP). When these three classification layers are combined\nin a multi-task manner, they can make up for each other's limitations,\nimproving performance in all three tasks. We experiment different combinations\nof the three layers and report how each layer affects other layers in the same\ncombination as well as the overall segmentation performance. The model we\nproposed achieves the state-of-the-art result in the WikiSection dataset.", "published": "2023-01-05 06:57:42", "link": "http://arxiv.org/abs/2301.01935v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Table-to-Text Generation with Pretrained Language Model: A Table\n  Structure Understanding and Text Deliberating Approach", "abstract": "Although remarkable progress on the neural table-to-text methods has been\nmade, the generalization issues hinder the applicability of these models due to\nthe limited source tables. Large-scale pretrained language models sound like a\npromising solution to tackle such issues. However, how to effectively bridge\nthe gap between the structured table and the text input by fully leveraging\ntable information to fuel the pretrained model is still not well explored.\nBesides, another challenge of integrating the deliberation mechanism into the\ntext-to-text pretrained model for solving the table-to-text task remains seldom\nstudied. In this paper, to implement the table-to-text generation with\npretrained language model, we propose a table structure understanding and text\ndeliberating approach, namely TASD. Specifically, we devise a three-layered\nmulti-head attention network to realize the table-structure-aware text\ngeneration model with the help of the pretrained language model. Furthermore, a\nmulti-pass decoder framework is adopted to enhance the capability of polishing\ngenerated text for table descriptions. The empirical studies, as well as human\nevaluation, on two public datasets, validate that our approach can generate\nfaithful and fluent descriptive texts for different types of tables.", "published": "2023-01-05 14:03:26", "link": "http://arxiv.org/abs/2301.02071v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Anaphora Resolution in Dialogue: System Description (CODI-CRAC 2022\n  Shared Task)", "abstract": "We describe three models submitted for the CODI-CRAC 2022 shared task. To\nperform identity anaphora resolution, we test several combinations of the\nincremental clustering approach based on the Workspace Coreference System (WCS)\nwith other coreference models. The best result is achieved by adding the\n''cluster merging'' version of the coref-hoi model, which brings up to 10.33%\nimprovement 1 over vanilla WCS clustering. Discourse deixis resolution is\nimplemented as multi-task learning: we combine the learning objective of\ncorefhoi with anaphor type classification. We adapt the higher-order resolution\nmodel introduced in Joshi et al. (2019) for bridging resolution given gold\nmentions and anaphors.", "published": "2023-01-05 15:42:17", "link": "http://arxiv.org/abs/2301.02113v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ANNA: Abstractive Text-to-Image Synthesis with Filtered News Captions", "abstract": "Advancements in Text-to-Image synthesis over recent years have focused more\non improving the quality of generated samples using datasets with descriptive\nprompts. However, real-world image-caption pairs present in domains such as\nnews data do not use simple and directly descriptive captions. With captions\ncontaining information on both the image content and underlying contextual\ncues, they become abstractive in nature. In this paper, we launch ANNA, an\nAbstractive News captioNs dAtaset extracted from online news articles in a\nvariety of different contexts. We explore the capabilities of current\nText-to-Image synthesis models to generate news domain-specific images using\nabstractive captions by benchmarking them on ANNA, in both standard training\nand transfer learning settings. The generated images are judged on the basis of\ncontextual relevance, visual quality, and perceptual similarity to ground-truth\nimage-caption pairs. Through our experiments, we show that techniques such as\ntransfer learning achieve limited success in understanding abstractive captions\nbut still fail to consistently learn the relationships between content and\ncontext features. The Dataset is available at\nhttps://github.com/aashish2000/ANNA .", "published": "2023-01-05 17:19:01", "link": "http://arxiv.org/abs/2301.02160v2", "categories": ["cs.CV", "cs.CL", "65D19"], "primary_category": "cs.CV"}
{"title": "CiT: Curation in Training for Effective Vision-Language Data", "abstract": "Large vision-language models are generally applicable to many downstream\ntasks, but come at an exorbitant training cost that only large institutions can\nafford. This paper trades generality for efficiency and presents Curation in\nTraining (CiT), a simple and efficient vision-text learning algorithm that\ncouples a data objective into training. CiT automatically yields quality data\nto speed-up contrastive image-text training and alleviates the need for an\noffline data filtering pipeline, allowing broad data sources (including raw\nimage-text pairs from the web). CiT contains two loops: an outer loop curating\nthe training data and an inner loop consuming the curated training data. The\ntext encoder connects the two loops. Given metadata for tasks of interest,\ne.g., class names, and a large pool of image-text pairs, CiT alternatively\nselects relevant training data from the pool by measuring the similarity of\ntheir text embeddings and embeddings of the metadata. In our experiments, we\nobserve that CiT can speed up training by over an order of magnitude,\nespecially if the raw data size is large.", "published": "2023-01-05 18:59:57", "link": "http://arxiv.org/abs/2301.02241v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Language as a Latent Sequence: deep latent variable models for\n  semi-supervised paraphrase generation", "abstract": "This paper explores deep latent variable models for semi-supervised\nparaphrase generation, where the missing target pair for unlabelled data is\nmodelled as a latent paraphrase sequence. We present a novel unsupervised model\nnamed variational sequence auto-encoding reconstruction (VSAR), which performs\nlatent sequence inference given an observed text. To leverage information from\ntext pairs, we additionally introduce a novel supervised model we call dual\ndirectional learning (DDL), which is designed to integrate with our proposed\nVSAR model. Combining VSAR with DDL (DDL+VSAR) enables us to conduct\nsemi-supervised learning. Still, the combined model suffers from a cold-start\nproblem. To further combat this issue, we propose an improved weight\ninitialisation solution, leading to a novel two-stage training scheme we call\nknowledge-reinforced-learning (KRL). Our empirical evaluations suggest that the\ncombined model yields competitive performance against the state-of-the-art\nsupervised baselines on complete data. Furthermore, in scenarios where only a\nfraction of the labelled pairs are available, our combined model consistently\noutperforms the strong supervised model baseline (DDL) by a significant margin\n(p <.05; Wilcoxon test). Our code is publicly available at\n\"https://github.com/jialin-yu/latent-sequence-paraphrase\".", "published": "2023-01-05 19:35:30", "link": "http://arxiv.org/abs/2301.02275v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Change User Preference Adversarially?", "abstract": "Pretrained large language models (LLMs) are becoming increasingly powerful\nand ubiquitous in mainstream applications such as being a personal assistant, a\ndialogue model, etc. As these models become proficient in deducing user\npreferences and offering tailored assistance, there is an increasing concern\nabout the ability of these models to influence, modify and in the extreme case\nmanipulate user preference adversarially. The issue of lack of interpretability\nin these models in adversarial settings remains largely unsolved. This work\ntries to study adversarial behavior in user preferences from the lens of\nattention probing, red teaming and white-box analysis. Specifically, it\nprovides a bird's eye view of existing literature, offers red teaming samples\nfor dialogue models like ChatGPT and GODEL and probes the attention mechanism\nin the latter for non-adversarial and adversarial settings.", "published": "2023-01-05 18:49:21", "link": "http://arxiv.org/abs/2302.10291v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Critical Perspectives: A Benchmark Revealing Pitfalls in PerspectiveAPI", "abstract": "Detecting \"toxic\" language in internet content is a pressing social and\ntechnical challenge. In this work, we focus on PERSPECTIVE from Jigsaw, a\nstate-of-the-art tool that promises to score the \"toxicity\" of text, with a\nrecent model update that claims impressive results (Lees et al., 2022). We seek\nto challenge certain normative claims about toxic language by proposing a new\nbenchmark, Selected Adversarial SemanticS, or SASS. We evaluate PERSPECTIVE on\nSASS, and compare to low-effort alternatives, like zero-shot and few-shot GPT-3\nprompt models, in binary classification settings. We find that PERSPECTIVE\nexhibits troubling shortcomings across a number of our toxicity categories.\nSASS provides a new tool for evaluating performance on previously undetected\ntoxic language that avoids common normative pitfalls. Our work leads us to\nemphasize the importance of questioning assumptions made by tools already in\ndeployment for toxicity detection in order to anticipate and prevent disparate\nharms.", "published": "2023-01-05 02:12:47", "link": "http://arxiv.org/abs/2301.01874v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "GIVL: Improving Geographical Inclusivity of Vision-Language Models with\n  Pre-Training Methods", "abstract": "A key goal for the advancement of AI is to develop technologies that serve\nthe needs not just of one group but of all communities regardless of their\ngeographical region. In fact, a significant proportion of knowledge is locally\nshared by people from certain regions but may not apply equally in other\nregions because of cultural differences. If a model is unaware of regional\ncharacteristics, it may lead to performance disparity across regions and result\nin bias against underrepresented groups. We propose GIVL, a Geographically\nInclusive Vision-and-Language Pre-trained model. There are two attributes of\ngeo-diverse visual concepts which can help to learn geo-diverse knowledge: 1)\nconcepts under similar categories have unique knowledge and visual\ncharacteristics, 2) concepts with similar visual features may fall in\ncompletely different categories. Motivated by the attributes, we design new\npre-training objectives Image Knowledge Matching (IKM) and Image Edit Checking\n(IEC) to pre-train GIVL. Compared with similar-size models pre-trained with\nsimilar scale of data, GIVL achieves state-of-the-art (SOTA) and more balanced\nperformance on geo-diverse V&L tasks.", "published": "2023-01-05 03:43:45", "link": "http://arxiv.org/abs/2301.01893v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS).\nSpecifically, we train a neural codec language model (called Vall-E) using\ndiscrete codes derived from an off-the-shelf neural audio codec model, and\nregard TTS as a conditional language modeling task rather than continuous\nsignal regression as in previous work. During the pre-training stage, we scale\nup the TTS training data to 60K hours of English speech which is hundreds of\ntimes larger than existing systems. Vall-E emerges in-context learning\ncapabilities and can be used to synthesize high-quality personalized speech\nwith only a 3-second enrolled recording of an unseen speaker as an acoustic\nprompt. Experiment results show that Vall-E significantly outperforms the\nstate-of-the-art zero-shot TTS system in terms of speech naturalness and\nspeaker similarity. In addition, we find Vall-E could preserve the speaker's\nemotion and acoustic environment of the acoustic prompt in synthesis. See\nhttps://aka.ms/valle for demos of our work.", "published": "2023-01-05 15:37:15", "link": "http://arxiv.org/abs/2301.02111v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Reprogramming Pretrained Language Models for Protein Sequence\n  Representation Learning", "abstract": "Machine Learning-guided solutions for protein learning tasks have made\nsignificant headway in recent years. However, success in scientific discovery\ntasks is limited by the accessibility of well-defined and labeled in-domain\ndata. To tackle the low-data constraint, recent adaptions of deep learning\nmodels pretrained on millions of protein sequences have shown promise; however,\nthe construction of such domain-specific large-scale model is computationally\nexpensive. Here, we propose Representation Learning via Dictionary Learning\n(R2DL), an end-to-end representation learning framework in which we reprogram\ndeep models for alternate-domain tasks that can perform well on protein\nproperty prediction with significantly fewer training samples. R2DL reprograms\na pretrained English language model to learn the embeddings of protein\nsequences, by learning a sparse linear mapping between English and protein\nsequence vocabulary embeddings. Our model can attain better accuracy and\nsignificantly improve the data efficiency by up to $10^5$ times over the\nbaselines set by pretrained and standard supervised methods. To this end, we\nreprogram an off-the-shelf pre-trained English language transformer and\nbenchmark it on a set of protein physicochemical prediction tasks (secondary\nstructure, stability, homology, stability) as well as on a biomedically\nrelevant set of protein function prediction tasks (antimicrobial, toxicity,\nantibody affinity).", "published": "2023-01-05 15:55:18", "link": "http://arxiv.org/abs/2301.02120v1", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Towards Autoformalization of Mathematics and Code Correctness:\n  Experiments with Elementary Proofs", "abstract": "The ever-growing complexity of mathematical proofs makes their manual\nverification by mathematicians very cognitively demanding. Autoformalization\nseeks to address this by translating proofs written in natural language into a\nformal representation that is computer-verifiable via interactive theorem\nprovers. In this paper, we introduce a semantic parsing approach, based on the\nUniversal Transformer architecture, that translates elementary mathematical\nproofs into an equivalent formalization in the language of the Coq interactive\ntheorem prover. The same architecture is also trained to translate simple\nimperative code decorated with Hoare triples into formally verifiable proofs of\ncorrectness in Coq. Experiments on a limited domain of artificial and\nhuman-written proofs show that the models generalize well to intermediate\nlengths not seen during training and variations in natural language.", "published": "2023-01-05 17:56:00", "link": "http://arxiv.org/abs/2301.02195v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in\n  Radiology", "abstract": "In this paper, we consider enhancing medical visual-language pre-training\n(VLP) with domain-specific knowledge, by exploiting the paired image-text\nreports from the radiological daily practice. In particular, we make the\nfollowing contributions: First, unlike existing works that directly process the\nraw reports, we adopt a novel triplet extraction module to extract the\nmedical-related information, avoiding unnecessary complexity from language\ngrammar and enhancing the supervision signals; Second, we propose a novel\ntriplet encoding module with entity translation by querying a knowledge base,\nto exploit the rich domain knowledge in medical field, and implicitly build\nrelationships between medical entities in the language embedding space; Third,\nwe propose to use a Transformer-based fusion model for spatially aligning the\nentity description with visual signals at the image patch level, enabling the\nability for medical diagnosis; Fourth, we conduct thorough experiments to\nvalidate the effectiveness of our architecture, and benchmark on numerous\npublic benchmarks, e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax,\nCOVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning\nsettings, our model has demonstrated strong performance compared with the\nformer methods on disease classification and grounding.", "published": "2023-01-05 18:55:09", "link": "http://arxiv.org/abs/2301.02228v3", "categories": ["eess.IV", "cs.CL", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Sequentially Controlled Text Generation", "abstract": "While GPT-2 generates sentences that are remarkably human-like, longer\ndocuments can ramble and do not follow human-like writing structure. We study\nthe problem of imposing structure on long-range text. We propose a novel\ncontrolled text generation task, sequentially controlled text generation, and\nidentify a dataset, NewsDiscourse as a starting point for this task. We develop\na sequential controlled text generation pipeline with generation and editing.\nWe test different degrees of structural awareness and show that, in general,\nmore structural awareness results in higher control-accuracy, grammaticality,\ncoherency and topicality, approaching human-level writing performance.", "published": "2023-01-05 21:23:51", "link": "http://arxiv.org/abs/2301.02299v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets", "abstract": "The research applies AI-driven code assistants to analyze a selection of\ninfluential computer code that has shaped modern technology, including email,\ninternet browsing, robotics, and malicious software. The original contribution\nof this study was to examine half of the most significant code advances in the\nlast 50 years and, in some cases, to provide notable improvements in clarity or\nperformance. The AI-driven code assistant could provide insights into\nobfuscated code or software lacking explanatory commentary in all cases\nexamined. We generated additional sample problems based on bug corrections and\ncode optimizations requiring much deeper reasoning than a traditional Google\nsearch might provide. Future work focuses on adding automated documentation and\ncode commentary and translating select large code bases into more modern\nversions with multiple new application programming interfaces (APIs) and\nchained multi-tasks. The AI-driven code assistant offers a valuable tool for\nsoftware engineering, particularly in its ability to provide human-level\nexpertise and assist in refactoring legacy code or simplifying the explanation\nor functionality of high-value repositories.", "published": "2023-01-05 23:17:17", "link": "http://arxiv.org/abs/2301.03373v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "SPRING: Situated Conversation Agent Pretrained with Multimodal Questions\n  from Incremental Layout Graph", "abstract": "Existing multimodal conversation agents have shown impressive abilities to\nlocate absolute positions or retrieve attributes in simple scenarios, but they\nfail to perform well when complex relative positions and information alignments\nare involved, which poses a bottleneck in response quality. In this paper, we\npropose a Situated Conversation Agent Petrained with Multimodal Questions from\nINcremental Layout Graph (SPRING) with abilities of reasoning multi-hops\nspatial relations and connecting them with visual attributes in crowded\nsituated scenarios. Specifically, we design two types of Multimodal Question\nAnswering (MQA) tasks to pretrain the agent. All QA pairs utilized during\npretraining are generated from novel Incremental Layout Graphs (ILG). QA pair\ndifficulty labels automatically annotated by ILG are used to promote MQA-based\nCurriculum Learning. Experimental results verify the SPRING's effectiveness,\nshowing that it significantly outperforms state-of-the-art approaches on both\nSIMMC 1.0 and SIMMC 2.0 datasets.", "published": "2023-01-05 08:03:47", "link": "http://arxiv.org/abs/2301.01949v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Automatic Sound Event Detection and Classification of Great Ape Calls\n  Using Neural Networks", "abstract": "We present a novel approach to automatically detect and classify great ape\ncalls from continuous raw audio recordings collected during field research. Our\nmethod leverages deep pretrained and sequential neural networks, including\nwav2vec 2.0 and LSTM, and is validated on three data sets from three different\ngreat ape lineages (orangutans, chimpanzees, and bonobos). The recordings were\ncollected by different researchers and include different annotation schemes,\nwhich our pipeline preprocesses and trains in a uniform fashion. Our results\nfor call detection and classification attain high accuracy. Our method is aimed\nto be generalizable to other animal species, and more generally, sound event\ndetection tasks. To foster future research, we make our pipeline and methods\npublicly available.", "published": "2023-01-05 18:33:40", "link": "http://arxiv.org/abs/2301.02214v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Singing voice synthesis based on frame-level sequence-to-sequence models\n  considering vocal timing deviation", "abstract": "This paper proposes singing voice synthesis (SVS) based on frame-level\nsequence-to-sequence models considering vocal timing deviation. In SVS, it is\nessential to synchronize the timing of singing with temporal structures\nrepresented by scores, taking into account that there are differences between\nactual vocal timing and note start timing. In many SVS systems including our\nprevious work, phoneme-level score features are converted into frame-level ones\non the basis of phoneme boundaries obtained by external aligners to take into\naccount vocal timing deviations. Therefore, the sound quality is affected by\nthe aligner accuracy in this system. To alleviate this problem, we introduce an\nattention mechanism with frame-level features. In the proposed system, the\nattention mechanism absorbs alignment errors in phoneme boundaries.\nAdditionally, we evaluate the system with pseudo-phoneme-boundaries defined by\nheuristic rules based on musical scores when there is no aligner. The\nexperimental results show the effectiveness of the proposed system.", "published": "2023-01-05 19:00:10", "link": "http://arxiv.org/abs/2301.02262v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
