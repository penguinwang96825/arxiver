{"title": "Training an NLP Scholar at a Small Liberal Arts College: A Backwards\n  Designed Course Proposal", "abstract": "The rapid growth in natural language processing (NLP) over the last couple\nyears has generated student interest and excitement in learning more about the\nfield. In this paper, we present two types of students that NLP courses might\nwant to train. First, an \"NLP engineer\" who is able to flexibly design, build\nand apply new technologies in NLP for a wide range of tasks. Second, an \"NLP\nscholar\" who is able to pose, refine and answer questions in NLP and how it\nrelates to the society, while also learning to effectively communicate these\nanswers to a broader audience. While these two types of skills are not mutually\nexclusive -- NLP engineers should be able to think critically, and NLP scholars\nshould be able to build systems -- we think that courses can differ in the\nbalance of these skills. As educators at Small Liberal Arts Colleges, the\nstrengths of our students and our institution favors an approach that is better\nsuited to train NLP scholars. In this paper we articulate what kinds of skills\nan NLP scholar should have, and then adopt a backwards design to propose course\ncomponents that can aid the acquisition of these skills.", "published": "2024-08-11 00:50:59", "link": "http://arxiv.org/abs/2408.05664v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language-Informed Beam Search Decoding for Multilingual Machine\n  Translation", "abstract": "Beam search decoding is the de-facto method for decoding auto-regressive\nNeural Machine Translation (NMT) models, including multilingual NMT where the\ntarget language is specified as an input. However, decoding multilingual NMT\nmodels commonly produces ``off-target'' translations -- yielding translation\noutputs not in the intended language. In this paper, we first conduct an error\nanalysis of off-target translations for a strong multilingual NMT model and\nidentify how these decodings are produced during beam search. We then propose\nLanguage-informed Beam Search (LiBS), a general decoding algorithm\nincorporating an off-the-shelf Language Identification (LiD) model into beam\nsearch decoding to reduce off-target translations. LiBS is an inference-time\nprocedure that is NMT-model agnostic and does not require any additional\nparallel data. Results show that our proposed LiBS algorithm on average\nimproves +1.1 BLEU and +0.9 BLEU on WMT and OPUS datasets, and reduces\noff-target rates from 22.9\\% to 7.7\\% and 65.8\\% to 25.3\\% respectively.", "published": "2024-08-11 09:57:46", "link": "http://arxiv.org/abs/2408.05738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HiLight: A Hierarchy-aware Light Global Model with Hierarchical Local\n  ConTrastive Learning", "abstract": "Hierarchical text classification (HTC) is a special sub-task of multi-label\nclassification (MLC) whose taxonomy is constructed as a tree and each sample is\nassigned with at least one path in the tree. Latest HTC models contain three\nmodules: a text encoder, a structure encoder and a multi-label classification\nhead. Specially, the structure encoder is designed to encode the hierarchy of\ntaxonomy. However, the structure encoder has scale problem. As the taxonomy\nsize increases, the learnable parameters of recent HTC works grow rapidly.\nRecursive regularization is another widely-used method to introduce\nhierarchical information but it has collapse problem and generally relaxed by\nassigning with a small weight (ie. 1e-6). In this paper, we propose a\nHierarchy-aware Light Global model with Hierarchical local conTrastive learning\n(HiLight), a lightweight and efficient global model only consisting of a text\nencoder and a multi-label classification head. We propose a new learning task\nto introduce the hierarchical information, called Hierarchical Local\nContrastive Learning (HiLCL). Extensive experiments are conducted on two\nbenchmark datasets to demonstrate the effectiveness of our model.", "published": "2024-08-11 14:26:58", "link": "http://arxiv.org/abs/2408.05786v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SAGA: A Participant-specific Examination of Story Alternatives and Goal\n  Applicability for a Deeper Understanding of Complex Events", "abstract": "Interpreting and assessing goal driven actions is vital to understanding and\nreasoning over complex events. It is important to be able to acquire the\nknowledge needed for this understanding, though doing so is challenging. We\nargue that such knowledge can be elicited through a participant achievement\nlens. We analyze a complex event in a narrative according to the intended\nachievements of the participants in that narrative, the likely future actions\nof the participants, and the likelihood of goal success. We collect 6.3K high\nquality goal and action annotations reflecting our proposed participant\nachievement lens, with an average weighted Fleiss-Kappa IAA of 80%. Our\ncollection contains annotated alternate versions of each narrative. These\nalternate versions vary minimally from the \"original\" story, but can license\ndrastically different inferences. Our findings suggest that while modern large\nlanguage models can reflect some of the goal-based knowledge we study, they\nfind it challenging to fully capture the design and intent behind concerted\nactions, even when the model pretraining included the data from which we\nextracted the goal knowledge. We show that smaller models fine-tuned on our\ndataset can achieve performance surpassing larger models.", "published": "2024-08-11 14:52:40", "link": "http://arxiv.org/abs/2408.05793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defining Boundaries: A Spectrum of Task Feasibility for Large Language\n  Models", "abstract": "Large language models (LLMs) have shown remarkable performance in various\ntasks but often fail to handle queries that exceed their knowledge and\ncapabilities, leading to incorrect or fabricated responses. This paper\naddresses the need for LLMs to recognize and refuse infeasible tasks due to the\nrequired skills surpassing their capabilities. We first conceptualize\ninfeasible tasks for LLMs and provide categorizations that cover a spectrum of\nrelated hallucinations over existing literature. We develop and benchmark a new\ndataset comprising diverse infeasible and feasible tasks to evaluate multiple\nLLMs' abilities to reject infeasible tasks. Furthermore, we explore the\npotential of increasing LLMs' refusal capabilities with fine-tuning.\nExperiments validate the effectiveness of our trained models, offering\npromising directions for refining the operational boundaries of LLMs in real\napplications.", "published": "2024-08-11 22:58:23", "link": "http://arxiv.org/abs/2408.05873v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask Fine-Tuning and Generative Adversarial Learning for Improved\n  Auxiliary Classification", "abstract": "In this study, we implement a novel BERT architecture for multitask\nfine-tuning on three downstream tasks: sentiment classification, paraphrase\ndetection, and semantic textual similarity prediction. Our model, Multitask\nBERT, incorporates layer sharing and a triplet architecture, custom sentence\npair tokenization, loss pairing, and gradient surgery. Such optimizations yield\na 0.516 sentiment classification accuracy, 0.886 paraphase detection accuracy,\nand 0.864 semantic textual similarity correlation on test data. We also apply\ngenerative adversarial learning to BERT, constructing a conditional generator\nmodel that maps from latent space to create fake embeddings in\n$\\mathbb{R}^{768}$. These fake embeddings are concatenated with real BERT\nembeddings and passed into a discriminator model for auxiliary classification.\nUsing this framework, which we refer to as AC-GAN-BERT, we conduct\nsemi-supervised sensitivity analyses to investigate the effect of increasing\namounts of unlabeled training data on AC-GAN-BERT's test accuracy. Overall,\naside from implementing a high-performing multitask classification system, our\nnovelty lies in the application of adversarial learning to construct a\ngenerator that mimics BERT. We find that the conditional generator successfully\nproduces rich embeddings with clear spatial correlation with class labels,\ndemonstrating avoidance of mode collapse. Our findings validate the GAN-BERT\napproach and point to future directions of generator-aided knowledge\ndistillation.", "published": "2024-08-11 20:05:54", "link": "http://arxiv.org/abs/2408.15265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reference-free Hallucination Detection for Large Vision-Language Models", "abstract": "Large vision-language models (LVLMs) have made significant progress in recent\nyears. While LVLMs exhibit excellent ability in language understanding,\nquestion answering, and conversations of visual inputs, they are prone to\nproducing hallucinations. While several methods are proposed to evaluate the\nhallucinations in LVLMs, most are reference-based and depend on external tools,\nwhich complicates their practical application. To assess the viability of\nalternative methods, it is critical to understand whether the reference-free\napproaches, which do not rely on any external tools, can efficiently detect\nhallucinations. Therefore, we initiate an exploratory study to demonstrate the\neffectiveness of different reference-free solutions in detecting hallucinations\nin LVLMs. In particular, we conduct an extensive study on three kinds of\ntechniques: uncertainty-based, consistency-based, and supervised uncertainty\nquantification methods on four representative LVLMs across two different tasks.\nThe empirical results show that the reference-free approaches are capable of\neffectively detecting non-factual responses in LVLMs, with the supervised\nuncertainty quantification method outperforming the others, achieving the best\nperformance across different settings.", "published": "2024-08-11 13:17:14", "link": "http://arxiv.org/abs/2408.05767v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for\n  Speech Processing", "abstract": "Deep learning has brought significant improvements to the field of\ncross-modal representation learning. For tasks such as text-to-speech (TTS),\nvoice conversion (VC), and automatic speech recognition (ASR), a cross-modal\nfine-grained (frame-level) sequence representation is desired, emphasizing the\nsemantic content of the text modality while de-emphasizing the paralinguistic\ninformation of the speech modality. We propose a method called \"Vector\nQuantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the\ncross-modal aligned sequence transcoder to bring text and speech into a joint\nmultimodal space, learning how to connect text and speech at the frame level.\nThe proposed VQ-CTAP is a paradigm for cross-modal sequence representation\nlearning, offering a promising solution for fine-grained generation and\nrecognition tasks in speech processing. The VQ-CTAP can be directly applied to\nVC and ASR tasks without fine-tuning or additional structures. We propose a\nsequence-aware semantic connector, which connects multiple frozen pre-trained\nmodules for the TTS task, exhibiting a plug-and-play capability. We design a\nstepping optimization strategy to ensure effective model convergence by\ngradually injecting and adjusting the influence of various loss components.\nFurthermore, we propose a semantic-transfer-wise paralinguistic consistency\nloss to enhance representational capabilities, allowing the model to better\ngeneralize to unseen data and capture the nuances of paralinguistic\ninformation. In addition, VQ-CTAP achieves high-compression speech coding at a\nrate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the\nsampling rate. The audio demo is available at\nhttps://qiangchunyu.github.io/VQCTAP/", "published": "2024-08-11 12:24:23", "link": "http://arxiv.org/abs/2408.05758v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LI-TTA: Language Informed Test-Time Adaptation for Automatic Speech\n  Recognition", "abstract": "Test-Time Adaptation (TTA) has emerged as a crucial solution to the domain\nshift challenge, wherein the target environment diverges from the original\ntraining environment. A prime exemplification is TTA for Automatic Speech\nRecognition (ASR), which enhances model performance by leveraging output\nprediction entropy minimization as a self-supervision signal. However, a key\nlimitation of this self-supervision lies in its primary focus on acoustic\nfeatures, with minimal attention to the linguistic properties of the input. To\naddress this gap, we propose Language Informed Test-Time Adaptation (LI-TTA),\nwhich incorporates linguistic insights during TTA for ASR. LI-TTA integrates\ncorrections from an external language model to merge linguistic with acoustic\ninformation by minimizing the CTC loss from the correction alongside the\nstandard TTA loss. With extensive experiments, we show that LI-TTA effectively\nimproves the performance of TTA for ASR in various distribution shift\nsituations.", "published": "2024-08-11 13:19:27", "link": "http://arxiv.org/abs/2408.05769v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "HateSieve: A Contrastive Learning Framework for Detecting and Segmenting\n  Hateful Content in Multimodal Memes", "abstract": "Amidst the rise of Large Multimodal Models (LMMs) and their widespread\napplication in generating and interpreting complex content, the risk of\npropagating biased and harmful memes remains significant. Current safety\nmeasures often fail to detect subtly integrated hateful content within\n``Confounder Memes''. To address this, we introduce \\textsc{HateSieve}, a new\nframework designed to enhance the detection and segmentation of hateful\nelements in memes. \\textsc{HateSieve} features a novel Contrastive Meme\nGenerator that creates semantically paired memes, a customized triplet dataset\nfor contrastive learning, and an Image-Text Alignment module that produces\ncontext-aware embeddings for accurate meme segmentation. Empirical experiments\non the Hateful Meme Dataset show that \\textsc{HateSieve} not only surpasses\nexisting LMMs in performance with fewer trainable parameters but also offers a\nrobust mechanism for precisely identifying and isolating hateful content.\n\\textcolor{red}{Caution: Contains academic discussions of hate speech; viewer\ndiscretion advised.}", "published": "2024-08-11 14:56:06", "link": "http://arxiv.org/abs/2408.05794v1", "categories": ["cs.AI", "cs.CL", "cs.MM", "cs.SI"], "primary_category": "cs.AI"}
{"title": "Iterative Improvement of an Additively Regularized Topic Model", "abstract": "Topic modelling is fundamentally a soft clustering problem (of known objects\n-- documents, over unknown clusters -- topics). That is, the task is\nincorrectly posed. In particular, the topic models are unstable and incomplete.\nAll this leads to the fact that the process of finding a good topic model\n(repeated hyperparameter selection, model training, and topic quality\nassessment) can be particularly long and labor-intensive. We aim to simplify\nthe process, to make it more deterministic and provable. To this end, we\npresent a method for iterative training of a topic model. The essence of the\nmethod is that a series of related topic models are trained so that each\nsubsequent model is at least as good as the previous one, i.e., that it retains\nall the good topics found earlier. The connection between the models is\nachieved by additive regularization. The result of this iterative training is\nthe last topic model in the series, which we call the iteratively updated\nadditively regularized topic model (ITAR). Experiments conducted on several\ncollections of natural language texts show that the proposed ITAR model\nperforms better than other popular topic models (LDA, ARTM, BERTopic), its\ntopics are diverse, and its perplexity (ability to \"explain\" the underlying\ndata) is moderate.", "published": "2024-08-11 18:22:12", "link": "http://arxiv.org/abs/2408.05840v3", "categories": ["cs.CL", "cs.IR", "math.PR"], "primary_category": "cs.CL"}
{"title": "LLM-Based Robust Product Classification in Commerce and Compliance", "abstract": "Product classification is a crucial task in international trade, as\ncompliance regulations are verified and taxes and duties are applied based on\nproduct categories. Manual classification of products is time-consuming and\nerror-prone, and the sheer volume of products imported and exported renders the\nmanual process infeasible. Consequently, e-commerce platforms and enterprises\ninvolved in international trade have turned to automatic product classification\nusing machine learning. However, current approaches do not consider the\nreal-world challenges associated with product classification, such as very\nabbreviated and incomplete product descriptions. In addition, recent\nadvancements in generative Large Language Models (LLMs) and their reasoning\ncapabilities are mainly untapped in product classification and e-commerce. In\nthis research, we explore the real-life challenges of industrial classification\nand we propose data perturbations that allow for realistic data simulation.\nFurthermore, we employ LLM-based product classification to improve the\nrobustness of the prediction in presence of incomplete data. Our research shows\nthat LLMs with in-context learning outperform the supervised approaches in the\nclean-data scenario. Additionally, we illustrate that LLMs are significantly\nmore robust than the supervised approaches when data attacks are present.", "published": "2024-08-11 22:59:32", "link": "http://arxiv.org/abs/2408.05874v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Post-Training Sparse Attention with Double Sparsity", "abstract": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.", "published": "2024-08-11 18:40:36", "link": "http://arxiv.org/abs/2408.07092v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Kov: Transferable and Naturalistic Black-Box LLM Attacks using Markov\n  Decision Processes and Tree Search", "abstract": "Eliciting harmful behavior from large language models (LLMs) is an important\ntask to ensure the proper alignment and safety of the models. Often when\ntraining LLMs, ethical guidelines are followed yet alignment failures may still\nbe uncovered through red teaming adversarial attacks. This work frames the\nred-teaming problem as a Markov decision process (MDP) and uses Monte Carlo\ntree search to find harmful behaviors of black-box, closed-source LLMs. We\noptimize token-level prompt suffixes towards targeted harmful behaviors on\nwhite-box LLMs and include a naturalistic loss term, log-perplexity, to\ngenerate more natural language attacks for better interpretability. The\nproposed algorithm, Kov, trains on white-box LLMs to optimize the adversarial\nattacks and periodically evaluates responses from the black-box LLM to guide\nthe search towards more harmful black-box behaviors. In our preliminary study,\nresults indicate that we can jailbreak black-box models, such as GPT-3.5, in\nonly 10 queries, yet fail on GPT-4$-$which may indicate that newer models are\nmore robust to token-level attacks. All work to reproduce these results is open\nsourced (https://github.com/sisl/Kov.jl).", "published": "2024-08-11 20:31:52", "link": "http://arxiv.org/abs/2408.08899v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "GPT-4 Emulates Average-Human Emotional Cognition from a Third-Person\n  Perspective", "abstract": "This paper extends recent investigations on the emotional reasoning abilities\nof Large Language Models (LLMs). Current research on LLMs has not directly\nevaluated the distinction between how LLMs predict the self-attribution of\nemotions and the perception of others' emotions. We first look at carefully\ncrafted emotion-evoking stimuli, originally designed to find patterns of brain\nneural activity representing fine-grained inferred emotional attributions of\nothers. We show that GPT-4 is especially accurate in reasoning about such\nstimuli. This suggests LLMs agree with humans' attributions of others' emotions\nin stereotypical scenarios remarkably more than self-attributions of emotions\nin idiosyncratic situations. To further explore this, our second study utilizes\na dataset containing annotations from both the author and a third-person\nperspective. We find that GPT-4's interpretations align more closely with human\njudgments about the emotions of others than with self-assessments. Notably,\nconventional computational models of emotion primarily rely on self-reported\nground truth as the gold standard. However, an average observer's standpoint,\nwhich LLMs appear to have adopted, might be more relevant for many downstream\napplications, at least in the absence of individual information and adequate\nsafety considerations.", "published": "2024-08-11 01:22:09", "link": "http://arxiv.org/abs/2408.13718v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Extracting Urban Sound Information for Residential Areas in Smart Cities\n  Using an End-to-End IoT System", "abstract": "With rapid urbanization comes the increase of community, construction, and\ntransportation noise in residential areas. The conventional approach of solely\nrelying on sound pressure level (SPL) information to decide on the noise\nenvironment and to plan out noise control and mitigation strategies is\ninadequate. This paper presents an end-to-end IoT system that extracts\nreal-time urban sound metadata using edge devices, providing information on the\nsound type, location and duration, rate of occurrence, loudness, and azimuth of\na dominant noise in nine residential areas. The collected metadata on\nenvironmental sound is transmitted to and aggregated in a cloud-based platform\nto produce detailed descriptive analytics and visualization. Our approach to\nintegrating different building blocks, namely, hardware, software, cloud\ntechnologies, and signal processing algorithms to form our real-time IoT system\nis outlined. We demonstrate how some of the sound metadata extracted by our\nsystem are used to provide insights into the noise in residential areas. A\nscalable workflow to collect and prepare audio recordings from nine residential\nareas to construct our urban sound dataset for training and evaluating a\nlocation-agnostic model is discussed. Some practical challenges of managing and\nmaintaining a sensor network deployed at numerous locations are also addressed.", "published": "2024-08-11 08:22:44", "link": "http://arxiv.org/abs/2408.05721v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
