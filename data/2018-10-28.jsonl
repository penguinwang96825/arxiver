{"title": "Middle-Out Decoding", "abstract": "Despite being virtually ubiquitous, sequence-to-sequence models are\nchallenged by their lack of diversity and inability to be externally\ncontrolled. In this paper, we speculate that a fundamental shortcoming of\nsequence generation models is that the decoding is done strictly from\nleft-to-right, meaning that outputs values generated earlier have a profound\neffect on those generated later. To address this issue, we propose a novel\nmiddle-out decoder architecture that begins from an initial middle-word and\nsimultaneously expands the sequence in both directions. To facilitate\ninformation flow and maintain consistent decoding, we introduce a dual\nself-attention mechanism that allows us to model complex dependencies between\nthe outputs. We illustrate the performance of our model on the task of video\ncaptioning, as well as a synthetic sequence de-noising task. Our middle-out\ndecoder achieves significant improvements on de-noising and competitive\nperformance in the task of video captioning, while quantifiably improving the\ncaption diversity. Furthermore, we perform a qualitative analysis that\ndemonstrates our ability to effectively control the generation process of our\ndecoder.", "published": "2018-10-28 00:19:26", "link": "http://arxiv.org/abs/1810.11735v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Modeling for Code-Switching: Evaluation, Integration of\n  Monolingual Data, and Discriminative Training", "abstract": "We focus on the problem of language modeling for code-switched language, in\nthe context of automatic speech recognition (ASR). Language modeling for\ncode-switched language is challenging for (at least) three reasons: (1) lack of\navailable large-scale code-switched data for training; (2) lack of a replicable\nevaluation setup that is ASR directed yet isolates language modeling\nperformance from the other intricacies of the ASR system; and (3) the reliance\non generative modeling. We tackle these three issues: we propose an\nASR-motivated evaluation setup which is decoupled from an ASR system and the\nchoice of vocabulary, and provide an evaluation dataset for English-Spanish\ncode-switching. This setup lends itself to a discriminative training approach,\nwhich we demonstrate to work better than generative language modeling. Finally,\nwe explore a variety of training protocols and verify the effectiveness of\ntraining with large amounts of monolingual data followed by fine-tuning with\nsmall amounts of code-switched data, for both the generative and discriminative\ncases.", "published": "2018-10-28 22:15:32", "link": "http://arxiv.org/abs/1810.11895v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel\n  Textual Transfer", "abstract": "We consider the problem of automatically generating textual paraphrases with\nmodified attributes or properties, focusing on the setting without parallel\ndata (Hu et al., 2017; Shen et al., 2017). This setting poses challenges for\nevaluation. We show that the metric of post-transfer classification accuracy is\ninsufficient on its own, and propose additional metrics based on semantic\npreservation and fluency as well as a way to combine them into a single overall\nscore. We contribute new loss functions and training strategies to address the\ndifferent metrics. Semantic preservation is addressed by adding a cyclic\nconsistency loss and a loss based on paraphrase pairs, while fluency is\nimproved by integrating losses based on style-specific language models. We\nexperiment with a Yelp sentiment dataset and a new literature dataset that we\npropose, using multiple models that extend prior work (Shen et al., 2017). We\ndemonstrate that our metrics correlate well with human judgments, at both the\nsentence-level and system-level. Automatic and manual evaluation also show\nlarge improvements over the baseline method of Shen et al. (2017). We hope that\nour proposed metrics can speed up system development for new textual transfer\ntasks while also encouraging the community to address our three complementary\naspects of transfer quality.", "published": "2018-10-28 20:40:16", "link": "http://arxiv.org/abs/1810.11878v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robots Learning to Say `No': Prohibition and Rejective Mechanisms in\n  Acquisition of Linguistic Negation", "abstract": "`No' belongs to the first ten words used by children and embodies the first\nactive form of linguistic negation. Despite its early occurrence the details of\nits acquisition process remain largely unknown. The circumstance that `no'\ncannot be construed as a label for perceptible objects or events puts it\noutside of the scope of most modern accounts of language acquisition. Moreover,\nmost symbol grounding architectures will struggle to ground the word due to its\nnon-referential character. In an experimental study involving the child-like\nhumanoid robot iCub that was designed to illuminate the acquisition process of\nnegation words, the robot is deployed in several rounds of speech-wise\nunconstrained interaction with na\\\"ive participants acting as its language\nteachers. The results corroborate the hypothesis that affect or volition plays\na pivotal role in the socially distributed acquisition process. Negation words\nare prosodically salient within prohibitive utterances and negative intent\ninterpretations such that they can be easily isolated from the teacher's speech\nsignal. These words subsequently may be grounded in negative affective states.\nHowever, observations of the nature of prohibitive acts and the temporal\nrelationships between its linguistic and extra-linguistic components raise\nserious questions over the suitability of Hebbian-type algorithms for language\ngrounding.", "published": "2018-10-28 12:17:27", "link": "http://arxiv.org/abs/1810.11804v1", "categories": ["cs.CL", "cs.AI", "cs.RO", "I.2.6; I.2.7; I.5.5; H.5.2"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Translation with MMD Networks", "abstract": "This work aims to improve semi-supervised learning in a neural network\narchitecture by introducing a hybrid supervised and unsupervised cost function.\nThe unsupervised component is trained using a differentiable estimator of the\nMaximum Mean Discrepancy (MMD) distance between the network output and the\ntarget dataset. We introduce the notion of an $n$-channel network and several\nmethods to improve performance of these nets based on supervised\npre-initialization, and multi-scale kernels. This work investigates the\neffectiveness of these methods on language translation where very few quality\ntranslations are known \\textit{a priori}. We also present a thorough\ninvestigation of the hyper-parameter space of this method on both synthetic\ndata.", "published": "2018-10-28 23:40:54", "link": "http://arxiv.org/abs/1810.11906v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "LPCNet: Improving Neural Speech Synthesis Through Linear Prediction", "abstract": "Neural speech synthesis models have recently demonstrated the ability to\nsynthesize high quality speech for text-to-speech and compression applications.\nThese new models often require powerful GPUs to achieve real-time operation, so\nbeing able to reduce their complexity would open the way for many new\napplications. We propose LPCNet, a WaveRNN variant that combines linear\nprediction with recurrent neural networks to significantly improve the\nefficiency of speech synthesis. We demonstrate that LPCNet can achieve\nsignificantly higher quality than WaveRNN for the same network size and that\nhigh quality LPCNet speech synthesis is achievable with a complexity under 3\nGFLOPS. This makes it easier to deploy neural synthesis applications on\nlower-power devices, such as embedded systems and mobile phones.", "published": "2018-10-28 17:59:57", "link": "http://arxiv.org/abs/1810.11846v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hypergraph based semi-supervised learning algorithms applied to speech\n  recognition problem: a novel approach", "abstract": "Most network-based speech recognition methods are based on the assumption\nthat the labels of two adjacent speech samples in the network are likely to be\nthe same. However, assuming the pairwise relationship between speech samples is\nnot complete. The information a group of speech samples that show very similar\npatterns and tend to have similar labels is missed. The natural way overcoming\nthe information loss of the above assumption is to represent the feature data\nof speech samples as the hypergraph. Thus, in this paper, the three\nun-normalized, random walk, and symmetric normalized hypergraph Laplacian based\nsemi-supervised learning methods applied to hypergraph constructed from the\nfeature data of speech samples in order to predict the labels of speech samples\nare introduced. Experiment results show that the sensitivity performance\nmeasures of these three hypergraph Laplacian based semi-supervised learning\nmethods are greater than the sensitivity performance measures of the Hidden\nMarkov Model method (the current state of the art method applied to speech\nrecognition problem) and graph based semi-supervised learning methods (i.e. the\ncurrent state of the art network-based method for classification problems)\napplied to network created from the feature data of speech samples.", "published": "2018-10-28 13:37:14", "link": "http://arxiv.org/abs/1810.12743v1", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS", "05C85"], "primary_category": "stat.ML"}
{"title": "Robust Audio Adversarial Example for a Physical Attack", "abstract": "We propose a method to generate audio adversarial examples that can attack a\nstate-of-the-art speech recognition model in the physical world. Previous work\nassumes that generated adversarial examples are directly fed to the recognition\nmodel, and is not able to perform such a physical attack because of\nreverberation and noise from playback environments. In contrast, our method\nobtains robust adversarial examples by simulating transformations caused by\nplayback or recording in the physical world and incorporating the\ntransformations into the generation process. Evaluation and a listening\nexperiment demonstrated that our adversarial examples are able to attack\nwithout being noticed by humans. This result suggests that audio adversarial\nexamples generated by the proposed method may become a real threat.", "published": "2018-10-28 10:50:24", "link": "http://arxiv.org/abs/1810.11793v4", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
