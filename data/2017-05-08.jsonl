{"title": "Density Estimation for Geolocation via Convolutional Mixture Density\n  Network", "abstract": "Nowadays, geographic information related to Twitter is crucially important\nfor fine-grained applications. However, the amount of geographic information\navail- able on Twitter is low, which makes the pursuit of many applications\nchallenging. Under such circumstances, estimating the location of a tweet is an\nimportant goal of the study. Unlike most previous studies that estimate the\npre-defined district as the classification task, this study employs a\nprobability distribution to represent richer information of the tweet, not only\nthe location but also its ambiguity. To realize this modeling, we propose the\nconvolutional mixture density network (CMDN), which uses text data to estimate\nthe mixture model parameters. Experimentally obtained results reveal that CMDN\nachieved the highest prediction performance among the method for predicting the\nexact coordinates. It also provides a quantitative representation of the\nlocation ambiguity for each tweet that properly works for extracting the\nreliable location estimations.", "published": "2017-05-08 05:50:47", "link": "http://arxiv.org/abs/1705.02750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforced Mnemonic Reader for Machine Reading Comprehension", "abstract": "In this paper, we introduce the Reinforced Mnemonic Reader for machine\nreading comprehension tasks, which enhances previous attentive readers in two\naspects. First, a reattention mechanism is proposed to refine current\nattentions by directly accessing to past attentions that are temporally\nmemorized in a multi-round alignment architecture, so as to avoid the problems\nof attention redundancy and attention deficiency. Second, a new optimization\napproach, called dynamic-critical reinforcement learning, is introduced to\nextend the standard supervised method. It always encourages to predict a more\nacceptable answer so as to address the convergence suppression problem occurred\nin traditional reinforcement learning algorithms. Extensive experiments on the\nStanford Question Answering Dataset (SQuAD) show that our model achieves\nstate-of-the-art results. Meanwhile, our model outperforms previous systems by\nover 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD\ndatasets.", "published": "2017-05-08 09:43:05", "link": "http://arxiv.org/abs/1705.02798v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment", "abstract": "Type-level word embeddings use the same set of parameters to represent all\ninstances of a word regardless of its context, ignoring the inherent lexical\nambiguity in language. Instead, we embed semantic concepts (or synsets) as\ndefined in WordNet and represent a word token in a particular context by\nestimating a distribution over relevant semantic concepts. We use the new,\ncontext-sensitive embeddings in a model for predicting prepositional phrase(PP)\nattachments and jointly learn the concept embeddings and model parameters. We\nshow that using context-sensitive embeddings improves the accuracy of the PP\nattachment model by 5.4% absolute points, which amounts to a 34.4% relative\nreduction in errors.", "published": "2017-05-08 15:40:51", "link": "http://arxiv.org/abs/1705.02925v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Convolutional Sequence to Sequence Learning", "abstract": "The prevalent approach to sequence to sequence learning maps an input\nsequence to a variable length output sequence via recurrent neural networks. We\nintroduce an architecture based entirely on convolutional neural networks.\nCompared to recurrent models, computations over all elements can be fully\nparallelized during training and optimization is easier since the number of\nnon-linearities is fixed and independent of the input length. Our use of gated\nlinear units eases gradient propagation and we equip each decoder layer with a\nseparate attention module. We outperform the accuracy of the deep LSTM setup of\nWu et al. (2016) on both WMT'14 English-German and WMT'14 English-French\ntranslation at an order of magnitude faster speed, both on GPU and CPU.", "published": "2017-05-08 23:25:30", "link": "http://arxiv.org/abs/1705.03122v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combating Human Trafficking with Deep Multimodal Models", "abstract": "Human trafficking is a global epidemic affecting millions of people across\nthe planet. Sex trafficking, the dominant form of human trafficking, has seen a\nsignificant rise mostly due to the abundance of escort websites, where human\ntraffickers can openly advertise among at-will escort advertisements. In this\npaper, we take a major step in the automatic detection of advertisements\nsuspected to pertain to human trafficking. We present a novel dataset called\nTrafficking-10k, with more than 10,000 advertisements annotated for this task.\nThe dataset contains two sources of information per advertisement: text and\nimages. For the accurate detection of trafficking advertisements, we designed\nand trained a deep multimodal model called the Human Trafficking Deep Network\n(HTDN).", "published": "2017-05-08 03:48:01", "link": "http://arxiv.org/abs/1705.02735v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "The Pragmatics of Indirect Commands in Collaborative Discourse", "abstract": "Today's artificial assistants are typically prompted to perform tasks through\ndirect, imperative commands such as \\emph{Set a timer} or \\emph{Pick up the\nbox}. However, to progress toward more natural exchanges between humans and\nthese assistants, it is important to understand the way non-imperative\nutterances can indirectly elicit action of an addressee. In this paper, we\ninvestigate command types in the setting of a grounded, collaborative game. We\nfocus on a less understood family of utterances for eliciting agent action,\nlocatives like \\emph{The chair is in the other room}, and demonstrate how these\nutterances indirectly command in specific game state contexts. Our work shows\nthat models with domain-specific grounding can effectively realize the\npragmatic reasoning that is necessary for more robust natural language\ninteraction.", "published": "2017-05-08 19:34:23", "link": "http://arxiv.org/abs/1705.03454v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sequential Dialogue Context Modeling for Spoken Language Understanding", "abstract": "Spoken Language Understanding (SLU) is a key component of goal oriented\ndialogue systems that would parse user utterances into semantic frame\nrepresentations. Traditionally SLU does not utilize the dialogue history beyond\nthe previous system turn and contextual ambiguities are resolved by the\ndownstream components. In this paper, we explore novel approaches for modeling\ndialogue context in a recurrent neural network (RNN) based language\nunderstanding system. We propose the Sequential Dialogue Encoder Network, that\nallows encoding context from the dialogue history in chronological order. We\ncompare the performance of our proposed architecture with two context models,\none that uses just the previous turn context and another that encodes dialogue\ncontext in a memory network, but loses the order of utterances in the dialogue\nhistory. Experiments with a multi-domain dialogue dataset demonstrate that the\nproposed architecture results in reduced semantic frame error rates.", "published": "2017-05-08 20:57:30", "link": "http://arxiv.org/abs/1705.03455v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
