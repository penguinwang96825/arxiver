{"title": "Social media mining for identification and exploration of health-related\n  information from pregnant women", "abstract": "Widespread use of social media has led to the generation of substantial\namounts of information about individuals, including health-related information.\nSocial media provides the opportunity to study health-related information about\nselected population groups who may be of interest for a particular study. In\nthis paper, we explore the possibility of utilizing social media to perform\ntargeted data collection and analysis from a particular population group --\npregnant women. We hypothesize that we can use social media to identify cohorts\nof pregnant women and follow them over time to analyze crucial health-related\ninformation. To identify potentially pregnant women, we employ simple\nrule-based searches that attempt to detect pregnancy announcements with\nmoderate precision. To further filter out false positives and noise, we employ\na supervised classifier using a small number of hand-annotated data. We then\ncollect their posts over time to create longitudinal health timelines and\nattempt to divide the timelines into different pregnancy trimesters. Finally,\nwe assess the usefulness of the timelines by performing a preliminary analysis\nto estimate drug intake patterns of our cohort at different trimesters. Our\nrule-based cohort identification technique collected 53,820 users over thirty\nmonths from Twitter. Our pregnancy announcement classification technique\nachieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user\ntimelines. Analysis of the timelines revealed that pertinent health-related\ninformation, such as drug-intake and adverse reactions can be mined from the\ndata. Our approach to using user timelines in this fashion has produced very\nencouraging results and can be employed for other important tasks where\ncohorts, for which health-related information may not be available from other\nsources, are required to be followed over time to derive population-based\nestimates.", "published": "2017-02-08 03:19:57", "link": "http://arxiv.org/abs/1702.02261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Source-Side Latent Graph Parsing", "abstract": "This paper presents a novel neural machine translation model which jointly\nlearns translation and source-side latent graph representations of sentences.\nUnlike existing pipelined approaches using syntactic parsers, our end-to-end\nmodel learns a latent graph parser as part of the encoder of an attention-based\nneural machine translation model, and thus the parser is optimized according to\nthe translation objective. In experiments, we first show that our model\ncompares favorably with state-of-the-art sequential and pipelined syntax-based\nNMT models. We also show that the performance of our model can be further\nimproved by pre-training it with a small amount of treebank annotations. Our\nfinal ensemble model significantly outperforms the previous best models on the\nstandard English-to-Japanese translation dataset.", "published": "2017-02-08 03:32:23", "link": "http://arxiv.org/abs/1702.02265v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Annotated Turkish Corpus for Named Entity Recognition and\n  Text Categorization using Large-Scale Gazetteers", "abstract": "Turkish Wikipedia Named-Entity Recognition and Text Categorization (TWNERTC)\ndataset is a collection of automatically categorized and annotated sentences\nobtained from Wikipedia. We constructed large-scale gazetteers by using a graph\ncrawler algorithm to extract relevant entity and domain information from a\nsemantic knowledge base, Freebase. The constructed gazetteers contains\napproximately 300K entities with thousands of fine-grained entity types under\n77 different domains. Since automated processes are prone to ambiguity, we also\nintroduce two new content specific noise reduction methodologies. Moreover, we\nmap fine-grained entity types to the equivalent four coarse-grained types:\nperson, loc, org, misc. Eventually, we construct six different dataset versions\nand evaluate the quality of annotations by comparing ground truths from human\nannotators. We make these datasets publicly available to support studies on\nTurkish named-entity recognition (NER) and text categorization (TC).", "published": "2017-02-08 10:45:23", "link": "http://arxiv.org/abs/1702.02363v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Multi-document Neural Attention for Multiple Answer Prediction", "abstract": "People have information needs of varying complexity, which can be solved by\nan intelligent agent able to answer questions formulated in a proper way,\neventually considering user context and preferences. In a scenario in which the\nuser profile can be considered as a question, intelligent agents able to answer\nquestions can be used to find the most relevant answers for a given user. In\nthis work we propose a novel model based on Artificial Neural Networks to\nanswer questions with multiple answers by exploiting multiple facts retrieved\nfrom a knowledge base. The model is evaluated on the factoid Question Answering\nand top-n recommendation tasks of the bAbI Movie Dialog dataset. After\nassessing the performance of the model on both tasks, we try to define the\nlong-term goal of a conversational recommender system able to interact using\nnatural language and to support users in their information seeking processes in\na personalized way.", "published": "2017-02-08 10:58:02", "link": "http://arxiv.org/abs/1702.02367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "abstract": "In this paper we explore the effect of architectural choices on learning a\nVariational Autoencoder (VAE) for text generation. In contrast to the\npreviously introduced VAE model for text where both the encoder and decoder are\nRNNs, we propose a novel hybrid architecture that blends fully feed-forward\nconvolutional and deconvolutional components with a recurrent language model.\nOur architecture exhibits several attractive properties such as faster run time\nand convergence, ability to better handle long sequences and, more importantly,\nit helps to avoid some of the major difficulties posed by training VAE models\non textual data.", "published": "2017-02-08 12:11:41", "link": "http://arxiv.org/abs/1702.02390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Domain Knowledge via Grouped Weight Sharing with Application\n  to Text Categorization", "abstract": "A fundamental advantage of neural models for NLP is their ability to learn\nrepresentations from scratch. However, in practice this often means ignoring\nexisting external linguistic resources, e.g., WordNet or domain specific\nontologies such as the Unified Medical Language System (UMLS). We propose a\ngeneral, novel method for exploiting such resources via weight sharing. Prior\nwork on weight sharing in neural networks has considered it largely as a means\nof model compression. In contrast, we treat weight sharing as a flexible\nmechanism for incorporating prior knowledge into neural models. We show that\nthis approach consistently yields improved performance on classification tasks\ncompared to baseline strategies that do not exploit weight sharing.", "published": "2017-02-08 17:30:51", "link": "http://arxiv.org/abs/1702.02535v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Audience's Laughter Using Convolutional Neural Network", "abstract": "For the purpose of automatically evaluating speakers' humor usage, we build a\npresentation corpus containing humorous utterances based on TED talks. Compared\nto previous data resources supporting humor recognition research, ours has\nseveral advantages, including (a) both positive and negative instances coming\nfrom a homogeneous data set, (b) containing a large number of speakers, and (c)\nbeing open. Focusing on using lexical cues for humor recognition, we\nsystematically compare a newly emerging text classification method based on\nConvolutional Neural Networks (CNNs) with a well-established conventional\nmethod using linguistic knowledge. The advantages of the CNN method are both\ngetting higher detection accuracies and being able to learn essential features\nautomatically.", "published": "2017-02-08 19:10:53", "link": "http://arxiv.org/abs/1702.02584v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Selection Strategies for Multi-Domain Sentiment Analysis", "abstract": "Domain adaptation is important in sentiment analysis as sentiment-indicating\nwords vary between domains. Recently, multi-domain adaptation has become more\npervasive, but existing approaches train on all available source domains\nincluding dissimilar ones. However, the selection of appropriate training data\nis as important as the choice of algorithm. We undertake -- to our knowledge\nfor the first time -- an extensive study of domain similarity metrics in the\ncontext of sentiment analysis and propose novel representations, metrics, and a\nnew scope for data selection. We evaluate the proposed methods on two\nlarge-scale multi-domain adaptation settings on tweets and reviews and\ndemonstrate that they consistently outperform strong random and balanced\nbaselines, while our proposed selection strategy outperforms instance-level\nselection and yields the best score on a large reviews corpus.", "published": "2017-02-08 13:49:59", "link": "http://arxiv.org/abs/1702.02426v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Trainable Greedy Decoding for Neural Machine Translation", "abstract": "Recent research in neural machine translation has largely focused on two\naspects; neural network architectures and end-to-end learning algorithms. The\nproblem of decoding, however, has received relatively little attention from the\nresearch community. In this paper, we solely focus on the problem of decoding\ngiven a trained neural machine translation model. Instead of trying to build a\nnew decoding algorithm for any specific decoding objective, we propose the idea\nof trainable decoding algorithm in which we train a decoding algorithm to find\na translation that maximizes an arbitrary decoding objective. More\nspecifically, we design an actor that observes and manipulates the hidden state\nof the neural machine translation decoder and propose to train it using a\nvariant of deterministic policy gradient. We extensively evaluate the proposed\nalgorithm using four language pairs and two decoding objectives and show that\nwe can indeed train a trainable greedy decoder that generates a better\ntranslation (in terms of a target decoding objective) with minimal\ncomputational overhead.", "published": "2017-02-08 13:56:16", "link": "http://arxiv.org/abs/1702.02429v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Character-level Deep Conflation for Business Data Analytics", "abstract": "Connecting different text attributes associated with the same entity\n(conflation) is important in business data analytics since it could help merge\ntwo different tables in a database to provide a more comprehensive profile of\nan entity. However, the conflation task is challenging because two text strings\nthat describe the same entity could be quite different from each other for\nreasons such as misspelling. It is therefore critical to develop a conflation\nmodel that is able to truly understand the semantic meaning of the strings and\nmatch them at the semantic level. To this end, we develop a character-level\ndeep conflation model that encodes the input text strings from character level\ninto finite dimension feature vectors, which are then used to compute the\ncosine similarity between the text strings. The model is trained in an\nend-to-end manner using back propagation and stochastic gradient descent to\nmaximize the likelihood of the correct association. Specifically, we propose\ntwo variants of the deep conflation model, based on long-short-term memory\n(LSTM) recurrent neural network (RNN) and convolutional neural network (CNN),\nrespectively. Both models perform well on a real-world business analytics\ndataset and significantly outperform the baseline bag-of-character (BoC) model.", "published": "2017-02-08 22:24:14", "link": "http://arxiv.org/abs/1702.02640v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Name Disambiguation in Anonymized Graphs using Network Embedding", "abstract": "In real-world, our DNA is unique but many people share names. This phenomenon\noften causes erroneous aggregation of documents of multiple persons who are\nnamesake of one another. Such mistakes deteriorate the performance of document\nretrieval, web search, and more seriously, cause improper attribution of credit\nor blame in digital forensic. To resolve this issue, the name disambiguation\ntask is designed which aims to partition the documents associated with a name\nreference such that each partition contains documents pertaining to a unique\nreal-life person. Existing solutions to this task substantially rely on feature\nengineering, such as biographical feature extraction, or construction of\nauxiliary features from Wikipedia. However, for many scenarios, such features\nmay be costly to obtain or unavailable due to the risk of privacy violation. In\nthis work, we propose a novel name disambiguation method. Our proposed method\nis non-intrusive of privacy because instead of using attributes pertaining to a\nreal-life person, our method leverages only relational data in the form of\nanonymized graphs. In the methodological aspect, the proposed method uses a\nnovel representation learning model to embed each document in a low dimensional\nvector space where name disambiguation can be solved by a hierarchical\nagglomerative clustering algorithm. Our experimental results demonstrate that\nthe proposed method is significantly better than the existing name\ndisambiguation methods working in a similar setting.", "published": "2017-02-08 04:54:09", "link": "http://arxiv.org/abs/1702.02287v4", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Automatic Rule Extraction from Long Short Term Memory Networks", "abstract": "Although deep learning models have proven effective at solving problems in\nnatural language processing, the mechanism by which they come to their\nconclusions is often unclear. As a result, these models are generally treated\nas black boxes, yielding no insight of the underlying learned patterns. In this\npaper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new\napproach for tracking the importance of a given input to the LSTM for a given\noutput. By identifying consistently important patterns of words, we are able to\ndistill state of the art LSTMs on sentiment analysis and question answering\ninto a set of representative phrases. This representation is then\nquantitatively validated by using the extracted phrases to construct a simple,\nrule-based classifier which approximates the output of the LSTM.", "published": "2017-02-08 17:46:37", "link": "http://arxiv.org/abs/1702.02540v2", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
