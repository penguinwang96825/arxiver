{"title": "Dense Information Flow for Neural Machine Translation", "abstract": "Recently, neural machine translation has achieved remarkable progress by\nintroducing well-designed deep neural networks into its encoder-decoder\nframework. From the optimization perspective, residual connections are adopted\nto improve learning performance for both encoder and decoder in most of these\ndeep architectures, and advanced attention connections are applied as well.\nInspired by the success of the DenseNet model in computer vision problems, in\nthis paper, we propose a densely connected NMT architecture (DenseNMT) that is\nable to train more efficiently for NMT. The proposed DenseNMT not only allows\ndense connection in creating new features for both encoder and decoder, but\nalso uses the dense attention structure to improve attention quality. Our\nexperiments on multiple datasets show that DenseNMT structure is more\ncompetitive and efficient.", "published": "2018-06-03 01:29:27", "link": "http://arxiv.org/abs/1806.00722v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Advanced Dialogue Managers for Goal-Oriented Dialogue Systems", "abstract": "Goal-Oriented (GO) Dialogue Systems, colloquially known as goal oriented\nchatbots, help users achieve a predefined goal (e.g. book a movie ticket)\nwithin a closed domain. A first step is to understand the user's goal by using\nnatural language understanding techniques. Once the goal is known, the bot must\nmanage a dialogue to achieve that goal, which is conducted with respect to a\nlearnt policy. The success of the dialogue system depends on the quality of the\npolicy, which is in turn reliant on the availability of high-quality training\ndata for the policy learning method, for instance Deep Reinforcement Learning.\n  Due to the domain specificity, the amount of available data is typically too\nlow to allow the training of good dialogue policies. In this master thesis we\nintroduce a transfer learning method to mitigate the effects of the low\nin-domain data availability. Our transfer learning based approach improves the\nbot's success rate by $20\\%$ in relative terms for distant domains and we more\nthan double it for close domains, compared to the model without transfer\nlearning. Moreover, the transfer learning chatbots learn the policy up to 5 to\n10 times faster. Finally, as the transfer learning approach is complementary to\nadditional processing such as warm-starting, we show that their joint\napplication gives the best outcomes.", "published": "2018-06-03 12:36:06", "link": "http://arxiv.org/abs/1806.00780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Tree Learning with Differentiable Parsers: Shift-Reduce Parsing\n  and Chart Parsing", "abstract": "Latent tree learning models represent sentences by composing their words\naccording to an induced parse tree, all based on a downstream task. These\nmodels often outperform baselines which use (externally provided) syntax trees\nto drive the composition order. This work contributes (a) a new latent tree\nlearning model based on shift-reduce parsing, with competitive downstream\nperformance and non-trivial induced trees, and (b) an analysis of the trees\nlearned by our shift-reduce model and by a chart-based model.", "published": "2018-06-03 17:34:41", "link": "http://arxiv.org/abs/1806.00840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TI-CNN: Convolutional Neural Networks for Fake News Detection", "abstract": "With the development of social networks, fake news for various commercial and\npolitical purposes has been appearing in large numbers and gotten widespread in\nthe online world. With deceptive words, people can get infected by the fake\nnews very easily and will share them without any fact-checking. For instance,\nduring the 2016 US president election, various kinds of fake news about the\ncandidates widely spread through both official news media and the online social\nnetworks. These fake news is usually released to either smear the opponents or\nsupport the candidate on their side. The erroneous information in the fake news\nis usually written to motivate the voters' irrational emotion and enthusiasm.\nSuch kinds of fake news sometimes can bring about devastating effects, and an\nimportant goal in improving the credibility of online social networks is to\nidentify the fake news timely. In this paper, we propose to study the fake news\ndetection problem. Automatic fake news identification is extremely hard, since\npure model based fact-checking for news is still an open problem, and few\nexisting models can be applied to solve the problem. With a thorough\ninvestigation of a fake news data, lots of useful explicit features are\nidentified from both the text words and images used in the fake news. Besides\nthe explicit features, there also exist some hidden patterns in the words and\nimages used in fake news, which can be captured with a set of latent features\nextracted via the multiple convolutional layers in our model. A model named as\nTI-CNN (Text and Image information based Convolutinal Neural Network) is\nproposed in this paper. By projecting the explicit and latent features into a\nunified feature space, TI-CNN is trained with both the text and image\ninformation simultaneously. Extensive experiments carried on the real-world\nfake news datasets have demonstrate the effectiveness of TI-CNN.", "published": "2018-06-03 08:09:58", "link": "http://arxiv.org/abs/1806.00749v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Psychological State in Text: A Limitation of Sentiment Analysis", "abstract": "Starting with the idea that sentiment analysis models should be able to\npredict not only positive or negative but also other psychological states of a\nperson, we implement a sentiment analysis model to investigate the relationship\nbetween the model and emotional state. We first examine psychological\nmeasurements of 64 participants and ask them to write a book report about a\nstory. After that, we train our sentiment analysis model using crawled movie\nreview data. We finally evaluate participants' writings, using the pretrained\nmodel as a concept of transfer learning. The result shows that sentiment\nanalysis model performs good at predicting a score, but the score does not have\nany correlation with human's self-checked sentiment.", "published": "2018-06-03 08:52:23", "link": "http://arxiv.org/abs/1806.00754v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transfer Topic Labeling with Domain-Specific Knowledge Base: An Analysis\n  of UK House of Commons Speeches 1935-2014", "abstract": "Topic models are widely used in natural language processing, allowing\nresearchers to estimate the underlying themes in a collection of documents.\nMost topic models use unsupervised methods and hence require the additional\nstep of attaching meaningful labels to estimated topics. This process of manual\nlabeling is not scalable and suffers from human bias. We present a\nsemi-automatic transfer topic labeling method that seeks to remedy these\nproblems. Domain-specific codebooks form the knowledge-base for automated topic\nlabeling. We demonstrate our approach with a dynamic topic model analysis of\nthe complete corpus of UK House of Commons speeches 1935-2014, using the coding\ninstructions of the Comparative Agendas Project to label topics. We show that\nour method works well for a majority of the topics we estimate; but we also\nfind that institution-specific topics, in particular on subnational governance,\nrequire manual input. We validate our results using human expert coding.", "published": "2018-06-03 13:22:10", "link": "http://arxiv.org/abs/1806.00793v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Learning Semantic Sentence Embeddings using Sequential Pair-wise\n  Discriminator", "abstract": "In this paper, we propose a method for obtaining sentence-level embeddings.\nWhile the problem of securing word-level embeddings is very well studied, we\npropose a novel method for obtaining sentence-level embeddings. This is\nobtained by a simple method in the context of solving the paraphrase generation\ntask. If we use a sequential encoder-decoder model for generating paraphrase,\nwe would like the generated paraphrase to be semantically close to the original\nsentence. One way to ensure this is by adding constraints for true paraphrase\nembeddings to be close and unrelated paraphrase candidate sentence embeddings\nto be far. This is ensured by using a sequential pair-wise discriminator that\nshares weights with the encoder that is trained with a suitable loss function.\nOur loss function penalizes paraphrase sentence embedding distances from being\ntoo large. This loss is used in combination with a sequential encoder-decoder\nnetwork. We also validated our method by evaluating the obtained embeddings for\na sentiment analysis task. The proposed method results in semantic embeddings\nand outperforms the state-of-the-art on the paraphrase generation and sentiment\nanalysis task on standard datasets. These results are also shown to be\nstatistically significant.", "published": "2018-06-03 15:00:05", "link": "http://arxiv.org/abs/1806.00807v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Contextualize, Show and Tell: A Neural Visual Storyteller", "abstract": "We present a neural model for generating short stories from image sequences,\nwhich extends the image description model by Vinyals et al. (Vinyals et al.,\n2015). This extension relies on an encoder LSTM to compute a context vector of\neach story from the image sequence. This context vector is used as the first\nstate of multiple independent decoder LSTMs, each of which generates the\nportion of the story corresponding to each image in the sequence by taking the\nimage embedding as the first input. Our model showed competitive results with\nthe METEOR metric and human ratings in the internal track of the Visual\nStorytelling Challenge 2018.", "published": "2018-06-03 05:09:54", "link": "http://arxiv.org/abs/1806.00738v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Cast Attention Networks for Retrieval-based Question Answering and\n  Response Prediction", "abstract": "Attention is typically used to select informative sub-phrases that are used\nfor prediction. This paper investigates the novel use of attention as a form of\nfeature augmentation, i.e, casted attention. We propose Multi-Cast Attention\nNetworks (MCAN), a new attention mechanism and general model architecture for a\npotpourri of ranking tasks in the conversational modeling and question\nanswering domains. Our approach performs a series of soft attention operations,\neach time casting a scalar feature upon the inner word embeddings. The key idea\nis to provide a real-valued hint (feature) to a subsequent encoder layer and is\ntargeted at improving the representation learning process. There are several\nadvantages to this design, e.g., it allows an arbitrary number of attention\nmechanisms to be casted, allowing for multiple attention types (e.g.,\nco-attention, intra-attention) and attention variants (e.g., alignment-pooling,\nmax-pooling, mean-pooling) to be executed simultaneously. This not only\neliminates the costly need to tune the nature of the co-attention layer, but\nalso provides greater extents of explainability to practitioners. Via extensive\nexperiments on four well-known benchmark datasets, we show that MCAN achieves\nstate-of-the-art performance. On the Ubuntu Dialogue Corpus, MCAN outperforms\nexisting state-of-the-art models by $9\\%$. MCAN also achieves the best\nperforming score to date on the well-studied TrecQA dataset.", "published": "2018-06-03 12:22:28", "link": "http://arxiv.org/abs/1806.00778v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
