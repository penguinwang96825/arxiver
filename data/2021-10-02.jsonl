{"title": "Clustering and Network Analysis for the Embedding Spaces of Sentences\n  and Sub-Sentences", "abstract": "Sentence embedding methods offer a powerful approach for working with short\ntextual constructs or sequences of words. By representing sentences as dense\nnumerical vectors, many natural language processing (NLP) applications have\nimproved their performance. However, relatively little is understood about the\nlatent structure of sentence embeddings. Specifically, research has not\naddressed whether the length and structure of sentences impact the sentence\nembedding space and topology. This paper reports research on a set of\ncomprehensive clustering and network analyses targeting sentence and\nsub-sentence embedding spaces. Results show that one method generates the most\nclusterable embeddings. In general, the embeddings of span sub-sentences have\nbetter clustering properties than the original sentences. The results have\nimplications for future sentence embedding models and applications.", "published": "2021-10-02 00:47:35", "link": "http://arxiv.org/abs/2110.00697v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TopiOCQA: Open-domain Conversational Question Answering with Topic\n  Switching", "abstract": "In a conversational question answering scenario, a questioner seeks to\nextract information about a topic through a series of interdependent questions\nand answers. As the conversation progresses, they may switch to related topics,\na phenomenon commonly observed in information-seeking search sessions. However,\ncurrent datasets for conversational question answering are limiting in two\nways: 1) they do not contain topic switches; and 2) they assume the reference\ntext for the conversation is given, i.e., the setting is not open-domain. We\nintroduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset\nwith topic switches on Wikipedia. TopiOCQA contains 3,920 conversations with\ninformation-seeking questions and free-form answers. On average, a conversation\nin our dataset spans 13 question-answer turns and involves four topics\n(documents). TopiOCQA poses a challenging test-bed for models, where efficient\nretrieval is required on multiple turns of the same conversation, in\nconjunction with constructing valid responses using conversational history. We\nevaluate several baselines, by combining state-of-the-art document retrieval\nmethods with neural reader models. Our best model achieves F1 of 55.8, falling\nshort of human performance by 14.2 points, indicating the difficulty of our\ndataset. Our dataset and code is available at\nhttps://mcgill-nlp.github.io/topiocqa", "published": "2021-10-02 09:53:48", "link": "http://arxiv.org/abs/2110.00768v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect Sentiment Quad Prediction as Paraphrase Generation", "abstract": "Aspect-based sentiment analysis (ABSA) has been extensively studied in recent\nyears, which typically involves four fundamental sentiment elements, including\nthe aspect category, aspect term, opinion term, and sentiment polarity.\nExisting studies usually consider the detection of partial sentiment elements,\ninstead of predicting the four elements in one shot. In this work, we introduce\nthe Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all\nsentiment elements in quads for a given opinionated sentence, which can reveal\na more comprehensive and complete aspect-level sentiment structure. We further\npropose a novel \\textsc{Paraphrase} modeling paradigm to cast the ASQP task to\na paraphrase generation process. On one hand, the generation formulation allows\nsolving ASQP in an end-to-end manner, alleviating the potential error\npropagation in the pipeline solution. On the other hand, the semantics of the\nsentiment elements can be fully exploited by learning to generate them in the\nnatural language form. Extensive experiments on benchmark datasets show the\nsuperiority of our proposed method and the capacity of cross-task transfer with\nthe proposed unified \\textsc{Paraphrase} modeling framework.", "published": "2021-10-02 12:57:27", "link": "http://arxiv.org/abs/2110.00796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Swiss-Judgment-Prediction: A Multilingual Legal Judgment Prediction\n  Benchmark", "abstract": "In many jurisdictions, the excessive workload of courts leads to high delays.\nSuitable predictive AI models can assist legal professionals in their work, and\nthus enhance and speed up the process. So far, Legal Judgment Prediction (LJP)\ndatasets have been released in English, French, and Chinese. We publicly\nrelease a multilingual (German, French, and Italian), diachronic (2000-2020)\ncorpus of 85K cases from the Federal Supreme Court of Switzerland (FSCS). We\nevaluate state-of-the-art BERT-based methods including two variants of BERT\nthat overcome the BERT input (text) length limitation (up to 512 tokens).\nHierarchical BERT has the best performance (approx. 68-70% Macro-F1-Score in\nGerman and French). Furthermore, we study how several factors (canton of\norigin, year of publication, text length, legal area) affect performance. We\nrelease both the benchmark dataset and our code to accelerate future research\nand ensure reproducibility.", "published": "2021-10-02 13:50:21", "link": "http://arxiv.org/abs/2110.00806v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping Language to Programs using Multiple Reward Components with\n  Inverse Reinforcement Learning", "abstract": "Mapping natural language instructions to programs that computers can process\nis a fundamental challenge. Existing approaches focus on likelihood-based\ntraining or using reinforcement learning to fine-tune models based on a single\nreward. In this paper, we pose program generation from language as Inverse\nReinforcement Learning. We introduce several interpretable reward components\nand jointly learn (1) a reward function that linearly combines them, and (2) a\npolicy for program generation. Fine-tuning with our approach achieves\nsignificantly better performance than competitive methods using Reinforcement\nLearning (RL). On the VirtualHome framework, we get improvements of up to 9.0%\non the Longest Common Subsequence metric and 14.7% on recall-based metrics over\nprevious work on this framework (Puig et al., 2018). The approach is\ndata-efficient, showing larger gains in performance in the low-data regime.\nGenerated programs are also preferred by human evaluators over an RL-based\napproach, and rated higher on relevance, completeness, and human-likeness.", "published": "2021-10-02 16:58:26", "link": "http://arxiv.org/abs/2110.00842v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-shot Multilingual Neural Machine Translation for\n  Low-Resource Languages", "abstract": "Although the multilingual Neural Machine Translation(NMT), which extends\nGoogle's multilingual NMT, has ability to perform zero-shot translation and the\niterative self-learning algorithm can improve the quality of zero-shot\ntranslation, it confronts with two problems: the multilingual NMT model is\nprone to generate wrong target language when implementing zero-shot\ntranslation; the self-learning algorithm, which uses beam search to generate\nsynthetic parallel data, demolishes the diversity of the generated source\nlanguage and amplifies the impact of the same noise during the iterative\nlearning process. In this paper, we propose the tagged-multilingual NMT model\nand improve the self-learning algorithm to handle these two problems. Firstly,\nwe extend the Google's multilingual NMT model and add target tokens to the\ntarget languages, which associates the start tag with the target language to\nensure that the source language can be translated to the required target\nlanguage. Secondly, we improve the self-learning algorithm by replacing beam\nsearch with random sample to increases the diversity of the generated data and\nmakes it properly cover the true data distribution. Experimental results on\nIWSLT show that the adjusted tagged-multilingual NMT separately obtains 9.41\nand 7.85 BLEU scores over the multilingual NMT on 2010 and 2017\nRomanian-Italian test sets. Similarly, it obtains 9.08 and 7.99 BLEU scores on\nItalian-Romanian zero-shot translation. Furthermore, the improved self-learning\nalgorithm shows its superiorities over the conventional self-learning algorithm\non zero-shot translations.", "published": "2021-10-02 02:50:53", "link": "http://arxiv.org/abs/2110.00712v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Case Study to Reveal if an Area of Interest has a Trend in Ongoing\n  Tweets Using Word and Sentence Embeddings", "abstract": "In the field of Natural Language Processing, information extraction from\ntexts has been the objective of many researchers for years. Many different\ntechniques have been applied in order to reveal the opinion that a tweet might\nhave, thus understanding the sentiment of the small writing up to 280\ncharacters. Other than figuring out the sentiment of a tweet, a study can also\nfocus on finding the correlation of the tweets with a certain area of interest,\nwhich constitutes the purpose of this study. In order to reveal if an area of\ninterest has a trend in ongoing tweets, we have proposed an easily applicable\nautomated methodology in which the Daily Mean Similarity Scores that show the\nsimilarity between the daily tweet corpus and the target words representing our\narea of interest is calculated by using a na\\\"ive correlation-based technique\nwithout training any Machine Learning Model. The Daily Mean Similarity Scores\nhave mainly based on cosine similarity and word/sentence embeddings computed by\nMultilanguage Universal Sentence Encoder and showed main opinion stream of the\ntweets with respect to a certain area of interest, which proves that an ongoing\ntrend of a specific subject on Twitter can easily be captured in almost real\ntime by using the proposed methodology in this study. We have also compared the\neffectiveness of using word versus sentence embeddings while applying our\nmethodology and realized that both give almost the same results, whereas using\nword embeddings requires less computational time than sentence embeddings, thus\nbeing more effective. This paper will start with an introduction followed by\nthe background information about the basics, then continue with the explanation\nof the proposed methodology and later on finish by interpreting the results and\nconcluding the findings.", "published": "2021-10-02 18:44:55", "link": "http://arxiv.org/abs/2110.00866v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.5.4; J.4"], "primary_category": "cs.CL"}
{"title": "Is There More Pattern in Knowledge Graph? Exploring Proximity Pattern\n  for Knowledge Graph Embedding", "abstract": "Modeling of relation pattern is the core focus of previous Knowledge Graph\nEmbedding works, which represents how one entity is related to another\nsemantically by some explicit relation. However, there is a more natural and\nintuitive relevancy among entities being always ignored, which is that how one\nentity is close to another semantically, without the consideration of any\nexplicit relation. We name such semantic phenomenon in knowledge graph as\nproximity pattern. In this work, we explore the problem of how to define and\nrepresent proximity pattern, and how it can be utilized to help knowledge graph\nembedding. Firstly, we define the proximity of any two entities according to\ntheir statistically shared queries, then we construct a derived graph structure\nand represent the proximity pattern from global view. Moreover, with the\noriginal knowledge graph, we design a Chained couPle-GNN (CP-GNN) architecture\nto deeply merge the two patterns (graphs) together, which can encode a more\ncomprehensive knowledge embedding. Being evaluated on FB15k-237 and WN18RR\ndatasets, CP-GNN achieves state-of-the-art results for Knowledge Graph\nCompletion task, and can especially boost the modeling capacity for complex\nqueries that contain multiple answer entities, proving the effectiveness of\nintroduced proximity pattern.", "published": "2021-10-02 03:50:42", "link": "http://arxiv.org/abs/2110.00720v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Minimizing LR(1) State Machines is NP-Hard", "abstract": "LR(1) parsing was a focus of extensive research in the past 50 years. Though\nmost fundamental mysteries have been resolved, a few remain hidden in the dark\ncorners. The one we bumped into is the minimization of the LR(1) state\nmachines, which we prove is NP-hard. It is the node-coloring problem that is\nreduced to the minimization puzzle. The reduction makes use of two technique:\nindirect reduction and incremental construction. Indirect reduction means the\ngraph to be colored is not reduced to an LR(1) state machine directly. Instead,\nit is reduced to a context-free grammar from which an LR(1) state machine is\nderived. Furthermore, by considering the nodes in the graph to be colored one\nat a time, the context-free grammar is incrementally extended from a template\ncontext-free grammar that is for a two-node graph. The extension is done by\nadding new grammar symbols and rules. A minimized LR(1) machine can be used to\nrecover a minimum coloring of the original graph.", "published": "2021-10-02 10:21:34", "link": "http://arxiv.org/abs/2110.00776v1", "categories": ["cs.CL", "cs.CC", "cs.FL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Sentiment Analysis Using NLP and Different\n  Machine Learning Techniques on US Airline Twitter Data", "abstract": "Today's business ecosystem has become very competitive. Customer satisfaction\nhas become a major focus for business growth. Business organizations are\nspending a lot of money and human resources on various strategies to understand\nand fulfill their customer's needs. But, because of defective manual analysis\non multifarious needs of customers, many organizations are failing to achieve\ncustomer satisfaction. As a result, they are losing customer's loyalty and\nspending extra money on marketing. We can solve the problems by implementing\nSentiment Analysis. It is a combined technique of Natural Language Processing\n(NLP) and Machine Learning (ML). Sentiment Analysis is broadly used to extract\ninsights from wider public opinion behind certain topics, products, and\nservices. We can do it from any online available data. In this paper, we have\nintroduced two NLP techniques (Bag-of-Words and TF-IDF) and various ML\nclassification algorithms (Support Vector Machine, Logistic Regression,\nMultinomial Naive Bayes, Random Forest) to find an effective approach for\nSentiment Analysis on a large, imbalanced, and multi-classed dataset. Our best\napproaches provide 77% accuracy using Support Vector Machine and Logistic\nRegression with Bag-of-Words technique.", "published": "2021-10-02 18:05:00", "link": "http://arxiv.org/abs/2110.00859v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simplify Your Law: Using Information Theory to Deduplicate Legal\n  Documents", "abstract": "Textual redundancy is one of the main challenges to ensuring that legal texts\nremain comprehensible and maintainable. Drawing inspiration from the\nrefactoring literature in software engineering, which has developed methods to\nexpose and eliminate duplicated code, we introduce the duplicated phrase\ndetection problem for legal texts and propose the Dupex algorithm to solve it.\nLeveraging the Minimum Description Length principle from information theory,\nDupex identifies a set of duplicated phrases, called patterns, that together\nbest compress a given input text. Through an extensive set of experiments on\nthe Titles of the United States Code, we confirm that our algorithm works well\nin practice: Dupex will help you simplify your law.", "published": "2021-10-02 06:19:14", "link": "http://arxiv.org/abs/2110.00735v1", "categories": ["cs.CL", "cs.CY", "cs.IT", "cs.SE", "math.IT"], "primary_category": "cs.CL"}
{"title": "Significance of Data Augmentation for Improving Cleft Lip and Palate\n  Speech Recognition", "abstract": "The automatic recognition of pathological speech, particularly from children\nwith any articulatory impairment, is a challenging task due to various reasons.\nThe lack of available domain specific data is one such obstacle that hinders\nits usage for different speech-based applications targeting pathological\nspeakers. In line with the challenge, in this work, we investigate a few data\naugmentation techniques to simulate training data for improving the children\nspeech recognition considering the case of cleft lip and palate (CLP) speech.\nThe augmentation techniques explored in this study, include vocal tract length\nperturbation (VTLP), reverberation, speaking rate, pitch modification, and\nspeech feature modification using cycle consistent adversarial networks\n(CycleGAN). Our study finds that the data augmentation methods significantly\nimprove the CLP speech recognition performance, which is more evident when we\nused feature modification using CycleGAN, VTLP and reverberation based methods.\nMore specifically, the results from this study show that our systems produce an\nimproved phone error rate compared to the systems without data augmentation.", "published": "2021-10-02 13:03:44", "link": "http://arxiv.org/abs/2110.00797v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Processing Phoneme Specific Segments for Cleft Lip and Palate Speech\n  Enhancement", "abstract": "The cleft lip and palate (CLP) speech intelligibility is distorted due to the\ndeformation in their articulatory system. For addressing the same, a few\nprevious works perform phoneme specific modification in CLP speech. In CLP\nspeech, both the articulation error and the nasalization distorts the\nintelligibility of a word. Consequently, modification of a specific phoneme may\nnot always yield in enhanced entire word-level intelligibility. For such cases,\nit is important to identify and isolate the phoneme specific error based on the\nknowledge of acoustic events. Accordingly, the phoneme specific error\nmodification algorithms can be exploited for transforming the specified errors\nand enhance the word-level intelligibility. Motivated by that, in this work, we\ncombine some of salient phoneme specific enhancement approaches and demonstrate\ntheir effectiveness in improving the word-level intelligibility of CLP speech.\nThe enhanced speech samples are evaluated using subjective and objective\nevaluation metrics.", "published": "2021-10-02 12:51:06", "link": "http://arxiv.org/abs/2110.00794v1", "categories": ["cs.SD", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "End-to-End Complex-Valued Multidilated Convolutional Neural Network for\n  Joint Acoustic Echo Cancellation and Noise Suppression", "abstract": "Echo and noise suppression is an integral part of a full-duplex communication\nsystem. Many recent acoustic echo cancellation (AEC) systems rely on a separate\nadaptive filtering module for linear echo suppression and a neural module for\nresidual echo suppression. However, not only do adaptive filtering modules\nrequire convergence and remain susceptible to changes in acoustic environments,\nbut this two-stage framework also often introduces unnecessary delays to the\nAEC system when neural modules are already capable of both linear and nonlinear\necho suppression. In this paper, we exploit the offset-compensating ability of\ncomplex time-frequency masks and propose an end-to-end complex-valued neural\nnetwork architecture. The building block of the proposed model is a\npseudocomplex extension based on the densely-connected multidilated DenseNet\n(D3Net) building block, resulting in a very small network of only 354K\nparameters. The architecture utilized the multi-resolution nature of the D3Net\nbuilding blocks to eliminate the need for pooling, allowing the network to\nextract features using large receptive fields without any loss of output\nresolution. We also propose a dual-mask technique for joint echo and noise\nsuppression with simultaneous speech enhancement. Evaluation on both synthetic\nand real test sets demonstrated promising results across multiple energy-based\nmetrics and perceptual proxies.", "published": "2021-10-02 07:41:41", "link": "http://arxiv.org/abs/2110.00745v3", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
