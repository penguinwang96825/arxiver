{"title": "Improving Early Sepsis Prediction with Multi Modal Learning", "abstract": "Sepsis is a life-threatening disease with high morbidity, mortality and\nhealthcare costs. The early prediction and administration of antibiotics and\nintravenous fluids is considered crucial for the treatment of sepsis and can\nsave potentially millions of lives and billions in health care costs.\nProfessional clinical care practitioners have proposed clinical criterion which\naid in early detection of sepsis; however, performance of these criterion is\noften limited. Clinical text provides essential information to estimate the\nseverity of the sepsis in addition to structured clinical data. In this study,\nwe explore how clinical text can complement structured data towards early\nsepsis prediction task. In this paper, we propose multi modal model which\nincorporates both structured data in the form of patient measurements as well\nas textual notes on the patient. We employ state-of-the-art NLP models such as\nBERT and a highly specialized NLP model in Amazon Comprehend Medical to\nrepresent the text. On the MIMIC-III dataset containing records of ICU\nadmissions, we show that by using these notes, one achieves an improvement of\n6.07 points in a standard utility score for Sepsis prediction and 2.89% in\nAUROC score. Our methods significantly outperforms a clinical criteria\nsuggested by experts, qSOFA, as well as the winning model of the PhysioNet\nComputing in Cardiology Challenge for predicting Sepsis.", "published": "2021-07-23 09:25:31", "link": "http://arxiv.org/abs/2107.11094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Bilingual Conversational Characteristics for Neural Chat\n  Translation", "abstract": "Neural chat translation aims to translate bilingual conversational text,\nwhich has a broad application in international exchanges and cooperation.\nDespite the impressive performance of sentence-level and context-aware Neural\nMachine Translation (NMT), there still remain challenges to translate bilingual\nconversational text due to its inherent characteristics such as role\npreference, dialogue coherence, and translation consistency. In this paper, we\naim to promote the translation quality of conversational text by modeling the\nabove properties. Specifically, we design three latent variational modules to\nlearn the distributions of bilingual conversational characteristics. Through\nsampling from these learned distributions, the latent variables, tailored for\nrole preference, dialogue coherence, and translation consistency, are\nincorporated into the NMT model for better translation. We evaluate our\napproach on the benchmark dataset BConTrasT (English-German) and a\nself-collected bilingual dialogue corpus, named BMELD (English-Chinese).\nExtensive experiments show that our approach notably boosts the performance\nover strong baselines by a large margin and significantly surpasses some\nstate-of-the-art context-aware NMT models in terms of BLEU and TER.\nAdditionally, we make the BMELD dataset publicly available for the research\ncommunity.", "published": "2021-07-23 12:23:34", "link": "http://arxiv.org/abs/2107.11164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Powering Effective Climate Communication with a Climate Knowledge Base", "abstract": "While many accept climate change and its growing impacts, few converse about\nit well, limiting the adoption speed of societal changes necessary to address\nit. In order to make effective climate communication easier, we aim to build a\nsystem that presents to any individual the climate information predicted to\nbest motivate and inspire them to take action given their unique set of\npersonal values. To alleviate the cold-start problem, the system relies on a\nknowledge base (ClimateKB) of causes and effects of climate change, and their\nassociations to personal values. Since no such comprehensive ClimateKB exists,\nwe revisit knowledge base construction techniques and build a ClimateKB from\nfree text. We plan to open source the ClimateKB and associated code to\nencourage future research and applications.", "published": "2021-07-23 17:02:06", "link": "http://arxiv.org/abs/2107.11351v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling Latent Translations for Cross-Lingual Transfer", "abstract": "While achieving state-of-the-art results in multiple tasks and languages,\ntranslation-based cross-lingual transfer is often overlooked in favour of\nmassively multilingual pre-trained encoders. Arguably, this is due to its main\nlimitations: 1) translation errors percolating to the classification phase and\n2) the insufficient expressiveness of the maximum-likelihood translation. To\nremedy this, we propose a new technique that integrates both steps of the\ntraditional pipeline (translation and classification) into a single model, by\ntreating the intermediate translations as a latent random variable. As a\nresult, 1) the neural machine translation system can be fine-tuned with a\nvariant of Minimum Risk Training where the reward is the accuracy of the\ndownstream task classifier. Moreover, 2) multiple samples can be drawn to\napproximate the expected loss across all possible translations during\ninference. We evaluate our novel latent translation-based model on a series of\nmultilingual NLU tasks, including commonsense reasoning, paraphrase\nidentification, and natural language inference. We report gains for both\nzero-shot and few-shot learning setups, up to 2.7 accuracy points on average,\nwhich are even more prominent for low-resource languages (e.g., Haitian\nCreole). Finally, we carry out in-depth analyses comparing different underlying\nNMT models and assessing the impact of alternative translations on the\ndownstream performance.", "published": "2021-07-23 17:11:27", "link": "http://arxiv.org/abs/2107.11353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Brazilian Portuguese Speech Recognition Using Wav2vec 2.0", "abstract": "Deep learning techniques have been shown to be efficient in various tasks,\nespecially in the development of speech recognition systems, that is, systems\nthat aim to transcribe an audio sentence in a sequence of written words.\nDespite the progress in the area, speech recognition can still be considered\ndifficult, especially for languages lacking available data, such as Brazilian\nPortuguese (BP). In this sense, this work presents the development of an public\nAutomatic Speech Recognition (ASR) system using only open available audio data,\nfrom the fine-tuning of the Wav2vec 2.0 XLSR-53 model pre-trained in many\nlanguages, over BP data. The final model presents an average word error rate of\n12.4% over 7 different datasets (10.5% when applying a language model).\nAccording to our knowledge, the obtained error is the lowest among open\nend-to-end (E2E) ASR models for BP.", "published": "2021-07-23 18:54:39", "link": "http://arxiv.org/abs/2107.11414v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Medical Literature Mining and Retrieval in a Conversational Setting", "abstract": "The Covid-19 pandemic has caused a spur in the medical research literature.\nWith new research advances in understanding the virus, there is a need for\nrobust text mining tools which can process, extract and present answers from\nthe literature in a concise and consumable way. With a DialoGPT based\nmulti-turn conversation generation module, and BM-25 \\& neural embeddings based\nensemble information retrieval module, in this paper we present a\nconversational system, which can retrieve and answer coronavirus-related\nqueries from the rich medical literature, and present it in a conversational\nsetting with the user. We further perform experiments to compare neural\nembedding-based document retrieval and the traditional BM25 retrieval algorithm\nand report the results.", "published": "2021-07-23 23:02:59", "link": "http://arxiv.org/abs/2108.01436v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Emotion analysis and detection during COVID-19", "abstract": "Crises such as natural disasters, global pandemics, and social unrest\ncontinuously threaten our world and emotionally affect millions of people\nworldwide in distinct ways. Understanding emotions that people express during\nlarge-scale crises helps inform policy makers and first responders about the\nemotional states of the population as well as provide emotional support to\nthose who need such support. We present CovidEmo, ~3K English tweets labeled\nwith emotions and temporally distributed across 18 months. Our analyses reveal\nthe emotional toll caused by COVID-19, and changes of the social narrative and\nassociated emotions over time. Motivated by the time-sensitive nature of crises\nand the cost of large-scale annotation efforts, we examine how well large\npre-trained language models generalize across domains and timeline in the task\nof perceived emotion prediction in the context of COVID-19. Our analyses\nsuggest that cross-domain information transfers occur, yet there are still\nsignificant gaps. We propose semi-supervised learning as a way to bridge this\ngap, obtaining significantly better performance using unlabeled data from the\ntarget domain.", "published": "2021-07-23 04:07:14", "link": "http://arxiv.org/abs/2107.11020v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Differentiable Language Model Adversarial Attack on Text Classifiers", "abstract": "Robustness of huge Transformer-based models for natural language processing\nis an important issue due to their capabilities and wide adoption. One way to\nunderstand and improve robustness of these models is an exploration of an\nadversarial attack scenario: check if a small perturbation of an input can fool\na model.\n  Due to the discrete nature of textual data, gradient-based adversarial\nmethods, widely used in computer vision, are not applicable per~se. The\nstandard strategy to overcome this issue is to develop token-level\ntransformations, which do not take the whole sentence into account.\n  In this paper, we propose a new black-box sentence-level attack. Our method\nfine-tunes a pre-trained language model to generate adversarial examples. A\nproposed differentiable loss function depends on a substitute classifier score\nand an approximate edit distance computed via a deep learning model.\n  We show that the proposed attack outperforms competitors on a diverse set of\nNLP problems for both computed metrics and human evaluation. Moreover, due to\nthe usage of the fine-tuned language model, the generated adversarial examples\nare hard to detect, thus current models are not robust. Hence, it is difficult\nto defend from the proposed attack, which is not the case for other attacks.", "published": "2021-07-23 14:43:13", "link": "http://arxiv.org/abs/2107.11275v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Similarity Based Label Smoothing For Dialogue Generation", "abstract": "Generative neural conversational systems are generally trained with the\nobjective of minimizing the entropy loss between the training \"hard\" targets\nand the predicted logits. Often, performance gains and improved generalization\ncan be achieved by using regularization techniques like label smoothing, which\nconverts the training \"hard\" targets to \"soft\" targets. However, label\nsmoothing enforces a data independent uniform distribution on the incorrect\ntraining targets, which leads to an incorrect assumption of equi-probable\nincorrect targets for each correct target. In this paper we propose and\nexperiment with incorporating data dependent word similarity based weighing\nmethods to transforms the uniform distribution of the incorrect target\nprobabilities in label smoothing, to a more natural distribution based on\nsemantics. We introduce hyperparameters to control the incorrect target\ndistribution, and report significant performance gains over networks trained\nusing standard label smoothing based loss, on two standard open domain dialogue\ncorpora.", "published": "2021-07-23 23:25:19", "link": "http://arxiv.org/abs/2107.11481v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OLR 2021 Challenge: Datasets, Rules and Baselines", "abstract": "This paper introduces the sixth Oriental Language Recognition (OLR) 2021\nChallenge, which intends to improve the performance of language recognition\nsystems and speech recognition systems within multilingual scenarios. The data\nprofile, four tasks, two baselines, and the evaluation principles are\nintroduced in this paper. In addition to the Language Identification (LID)\ntasks, multilingual Automatic Speech Recognition (ASR) tasks are introduced to\nOLR 2021 Challenge for the first time. The challenge this year focuses on more\npractical and challenging problems, with four tasks: (1) constrained LID, (2)\nunconstrained LID, (3) constrained multilingual ASR, (4) unconstrained\nmultilingual ASR. Baselines for LID tasks and multilingual ASR tasks are\nprovided, respectively. The LID baseline system is an extended TDNN x-vector\nmodel constructed with Pytorch. A transformer-based end-to-end model is\nprovided as the multilingual ASR baseline system. These recipes will be online\npublished, and available for participants to construct their own LID or ASR\nsystems. The baseline results demonstrate that those tasks are rather\nchallenging and deserve more effort to achieve better performance.", "published": "2021-07-23 09:57:29", "link": "http://arxiv.org/abs/2107.11113v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Adversarial Reinforced Instruction Attacker for Robust Vision-Language\n  Navigation", "abstract": "Language instruction plays an essential role in the natural language grounded\nnavigation tasks. However, navigators trained with limited human-annotated\ninstructions may have difficulties in accurately capturing key information from\nthe complicated instruction at different timesteps, leading to poor navigation\nperformance. In this paper, we exploit to train a more robust navigator which\nis capable of dynamically extracting crucial factors from the long instruction,\nby using an adversarial attacking paradigm. Specifically, we propose a Dynamic\nReinforced Instruction Attacker (DR-Attacker), which learns to mislead the\nnavigator to move to the wrong target by destroying the most instructive\ninformation in instructions at different timesteps. By formulating the\nperturbation generation as a Markov Decision Process, DR-Attacker is optimized\nby the reinforcement learning algorithm to generate perturbed instructions\nsequentially during the navigation, according to a learnable attack score.\nThen, the perturbed instructions, which serve as hard samples, are used for\nimproving the robustness of the navigator with an effective adversarial\ntraining strategy and an auxiliary self-supervised reasoning task. Experimental\nresults on both Vision-and-Language Navigation (VLN) and Navigation from Dialog\nHistory (NDH) tasks show the superiority of our proposed method over\nstate-of-the-art methods. Moreover, the visualization analysis shows the\neffectiveness of the proposed DR-Attacker, which can successfully attack\ncrucial information in the instructions at different timesteps. Code is\navailable at https://github.com/expectorlin/DR-Attacker.", "published": "2021-07-23 14:11:31", "link": "http://arxiv.org/abs/2107.11252v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Text Classification and Clustering with Annealing Soft Nearest Neighbor\n  Loss", "abstract": "We define disentanglement as how far class-different data points from each\nother are, relative to the distances among class-similar data points. When\nmaximizing disentanglement during representation learning, we obtain a\ntransformed feature representation where the class memberships of the data\npoints are preserved. If the class memberships of the data points are\npreserved, we would have a feature representation space in which a nearest\nneighbour classifier or a clustering algorithm would perform well. We take\nadvantage of this method to learn better natural language representation, and\nemploy it on text classification and text clustering tasks. Through\ndisentanglement, we obtain text representations with better-defined clusters\nand improve text classification performance. Our approach had a test\nclassification accuracy of as high as 90.11% and test clustering accuracy of\n88% on the AG News dataset, outperforming our baseline models -- without any\nother training tricks or regularization.", "published": "2021-07-23 09:05:39", "link": "http://arxiv.org/abs/2107.14597v1", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "SALADnet: Self-Attentive multisource Localization in the Ambisonics\n  Domain", "abstract": "In this work, we propose a novel self-attention based neural network for\nrobust multi-speaker localization from Ambisonics recordings. Starting from a\nstate-of-the-art convolutional recurrent neural network, we investigate the\nbenefit of replacing the recurrent layers by self-attention encoders, inherited\nfrom the Transformer architecture. We evaluate these models on synthetic and\nreal-world data, with up to 3 simultaneous speakers. The obtained results\nindicate that the majority of the proposed architectures either perform on par,\nor outperform the CRNN baseline, especially in the multisource scenario.\nMoreover, by avoiding the recurrent layers, the proposed models lend themselves\nto parallel computing, which is shown to produce considerable savings in\nexecution time.", "published": "2021-07-23 08:14:09", "link": "http://arxiv.org/abs/2107.11066v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-channel Speech Enhancement with 2-D Convolutional Time-frequency\n  Domain Features and a Pre-trained Acoustic Model", "abstract": "We propose a multi-channel speech enhancement approach with a novel two-stage\nfeature fusion method and a pre-trained acoustic model in a multi-task learning\nparadigm. In the first fusion stage, the time-domain and frequency-domain\nfeatures are extracted separately. In the time domain, the multi-channel\nconvolution sum (MCS) and the inter-channel convolution differences (ICDs)\nfeatures are computed and then integrated with the first 2-D convolutional\nlayer, while in the frequency domain, the log-power spectra (LPS) features from\nboth original channels and super-directive beamforming outputs are combined\nwith a second 2-D convolutional layer. To fully integrate the rich information\nof multi-channel speech, i.e. time-frequency domain features and the array\ngeometry, we apply a third 2-D convolutional layer in the second fusion stage\nto obtain the final convolutional features. Furthermore, we propose to use a\nfixed clean acoustic model trained with the end-to-end lattice-free maximum\nmutual information criterion to enforce the enhanced output to have the same\ndistribution as the clean waveform to alleviate the over-estimation problem of\nthe enhancement task and constrain distortion. On the Task1 development dataset\nof ConferencingSpeech 2021 challenge, a PESQ improvement of 0.24 and 0.19 is\nattained compared to the official baseline and a recently proposed\nmulti-channel separation method.", "published": "2021-07-23 13:32:43", "link": "http://arxiv.org/abs/2107.11222v3", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Multi-Channel Automatic Music Transcription Using Tensor Algebra", "abstract": "Music is an art, perceived in unique ways by every listener, coming from\nacoustic signals. In the meantime, standards as musical scores exist to\ndescribe it. Even if humans can make this transcription, it is costly in terms\nof time and efforts, even more with the explosion of information consecutively\nto the rise of the Internet. In that sense, researches are driven in the\ndirection of Automatic Music Transcription. While this task is considered\nsolved in the case of single notes, it is still open when notes superpose\nthemselves, forming chords. This report aims at developing some of the existing\ntechniques towards Music Transcription, particularly matrix factorization, and\nintroducing the concept of multi-channel automatic music transcription. This\nconcept will be explored with mathematical objects called tensors.", "published": "2021-07-23 14:07:40", "link": "http://arxiv.org/abs/2107.11250v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS", "H.5.5"], "primary_category": "cs.SD"}
{"title": "Using Deep Learning Techniques and Inferential Speech Statistics for AI\n  Synthesised Speech Recognition", "abstract": "The recent developments in technology have re-warded us with amazing audio\nsynthesis models like TACOTRON and WAVENETS. On the other side, it poses\ngreater threats such as speech clones and deep fakes, that may go undetected.\nTo tackle these alarming situations, there is an urgent need to propose models\nthat can help discriminate a synthesized speech from an actual human speech and\nalso identify the source of such a synthesis. Here, we propose a model based on\nConvolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network\n(BiRNN) that helps to achieve both the aforementioned objectives. The temporal\ndependencies present in AI synthesized speech are exploited using Bidirectional\nRNN and CNN. The model outperforms the state-of-the-art approaches by\nclassifying the AI synthesized audio from real human speech with an error rate\nof 1.9% and detecting the underlying architecture with an accuracy of 97%.", "published": "2021-07-23 18:43:10", "link": "http://arxiv.org/abs/2107.11412v1", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Automatic Detection Of Noise Events at Shooting Range Using Machine\n  Learning", "abstract": "Outdoor shooting ranges are subject to noise regulations from local and\nnational authorities. Restrictions found in these regulations may include\nlimits on times of activities, the overall number of noise events, as well as\nlimits on number of events depending on the class of noise or activity. A noise\nmonitoring system may be used to track overall sound levels, but rarely provide\nthe ability to detect activity or count the number of events, required to\ncompare directly with such regulations. This work investigates the feasibility\nand performance of an automatic detection system to count noise events. An\nempirical evaluation was done by collecting data at a newly constructed\nshooting range and training facility. The data includes tests of multiple\nweapon configurations from small firearms to high caliber rifles and\nexplosives, at multiple source positions, and collected on multiple different\ndays. Several alternative machine learning models are tested, using as inputs\ntime-series of standard acoustic indicators such as A-weighted sound levels and\n1/3 octave spectrogram, and classifiers such as Logistic Regression and\nConvolutional Neural Networks. Performance for the various alternatives are\nreported in terms of the False Positive Rate and False Negative Rate. The\ndetection performance was found to be satisfactory for use in automatic logging\nof time-periods with training activity.", "published": "2021-07-23 20:36:43", "link": "http://arxiv.org/abs/2107.11453v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
