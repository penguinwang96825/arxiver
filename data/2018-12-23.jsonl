{"title": "Supervised Sentiment Classification with CNNs for Diverse SE Datasets", "abstract": "Sentiment analysis, a popular technique for opinion mining, has been used by\nthe software engineering research community for tasks such as assessing app\nreviews, developer emotions in issue trackers and developer opinions on APIs.\nPast research indicates that state-of-the-art sentiment analysis techniques\nhave poor performance on SE data. This is because sentiment analysis tools are\noften designed to work on non-technical documents such as movie reviews. In\nthis study, we attempt to solve the issues with existing sentiment analysis\ntechniques for SE texts by proposing a hierarchical model based on\nconvolutional neural networks (CNN) and long short-term memory (LSTM) trained\non top of pre-trained word vectors. We assessed our model's performance and\nreliability by comparing it with a number of frequently used sentiment analysis\ntools on five gold standard datasets. Our results show that our model pushes\nthe state of the art further on all datasets in terms of accuracy. We also show\nthat it is possible to get better accuracy after labelling a small sample of\nthe dataset and re-training our model rather than using an unsupervised\nclassifier.", "published": "2018-12-23 03:47:29", "link": "http://arxiv.org/abs/1812.09653v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Non-Autoregressive Neural Machine Translation with Enhanced Decoder\n  Input", "abstract": "Non-autoregressive translation (NAT) models, which remove the dependence on\nprevious target tokens from the inputs of the decoder, achieve significantly\ninference speedup but at the cost of inferior accuracy compared to\nautoregressive translation (AT) models. Previous work shows that the quality of\nthe inputs of the decoder is important and largely impacts the model accuracy.\nIn this paper, we propose two methods to enhance the decoder inputs so as to\nimprove NAT models. The first one directly leverages a phrase table generated\nby conventional SMT approaches to translate source tokens to target tokens,\nwhich are then fed into the decoder as inputs. The second one transforms\nsource-side word embeddings to target-side word embeddings through\nsentence-level alignment and word-level adversary learning, and then feeds the\ntransformed word embeddings into the decoder as inputs. Experimental results\nshow our method largely outperforms the NAT baseline~\\citep{gu2017non} by\n$5.11$ BLEU scores on WMT14 English-German task and $4.72$ BLEU scores on WMT16\nEnglish-Romanian task.", "published": "2018-12-23 06:28:30", "link": "http://arxiv.org/abs/1812.09664v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Context-Aware Semantic Relationships in Sparse Mobile Datasets", "abstract": "Traditional semantic similarity models often fail to encapsulate the external\ncontext in which texts are situated. However, textual datasets generated on\nmobile platforms can help us build a truer representation of semantic\nsimilarity by introducing multimodal data. This is especially important in\nsparse datasets, making solely text-driven interpretation of context more\ndifficult. In this paper, we develop new algorithms for building external\nfeatures into sentence embeddings and semantic similarity scores. Then, we test\nthem on embedding spaces on data from Twitter, using each tweet's time and\ngeolocation to better understand its context. Ultimately, we show that applying\nPCA with eight components to the embedding space and appending multimodal\nfeatures yields the best outcomes. This yields a considerable improvement over\npure text-based approaches for discovering similar tweets. Our results suggest\nthat our new algorithm can help improve semantic understanding in various\nsettings.", "published": "2018-12-23 03:35:56", "link": "http://arxiv.org/abs/1812.09650v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Cross-Architecture Instruction Embedding Model for Natural Language\n  Processing-Inspired Binary Code Analysis", "abstract": "Given a closed-source program, such as most of proprietary software and\nviruses, binary code analysis is indispensable for many tasks, such as code\nplagiarism detection and malware analysis. Today, source code is very often\ncompiled for various architectures, making cross-architecture binary code\nanalysis increasingly important. A binary, after being disassembled, is\nexpressed in an assembly languages. Thus, recent work starts exploring Natural\nLanguage Processing (NLP) inspired binary code analysis. In NLP, words are\nusually represented in high-dimensional vectors (i.e., embeddings) to\nfacilitate further processing, which is one of the most common and critical\nsteps in many NLP tasks. We regard instructions as words in NLP-inspired binary\ncode analysis, and aim to represent instructions as embeddings as well.\n  To facilitate cross-architecture binary code analysis, our goal is that\nsimilar instructions, regardless of their architectures, have embeddings close\nto each other. To this end, we propose a joint learning approach to generating\ninstruction embeddings that capture not only the semantics of instructions\nwithin an architecture, but also their semantic relationships across\narchitectures. To the best of our knowledge, this is the first work on building\ncross-architecture instruction embedding model. As a showcase, we apply the\nmodel to resolving one of the most fundamental problems for binary code\nsimilarity comparison---semantics-based basic block comparison, and the\nsolution outperforms the code statistics based approach. It demonstrates that\nit is promising to apply the model to other cross-architecture binary code\nanalysis tasks.", "published": "2018-12-23 03:44:03", "link": "http://arxiv.org/abs/1812.09652v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Optimizing Answer Set Computation via Heuristic-Based Decomposition", "abstract": "Answer Set Programming (ASP) is a purely declarative formalism developed in\nthe field of logic programming and nonmonotonic reasoning: computational\nproblems are encoded by logic programs whose answer sets, corresponding to\nsolutions, are computed by an ASP system. Different, semantically equivalent,\nprograms can be defined for the same problem; however, performance of systems\nevaluating them might significantly vary. We propose an approach for\nautomatically transforming an input logic program into an equivalent one that\ncan be evaluated more efficiently. One can make use of existing\ntree-decomposition techniques for rewriting selected rules into a set of\nmultiple ones; the idea is to guide and adaptively apply them on the basis of\nproper new heuristics, to obtain a smart rewriting algorithm to be integrated\ninto an ASP system. The method is rather general: it can be adapted to any\nsystem and implement different preference policies. Furthermore, we define a\nset of new heuristics tailored at optimizing grounding, one of the main phases\nof the ASP computation; we use them in order to implement the approach into the\nASP system DLV, in particular into its grounding subsystem I-DLV, and carry out\nan extensive experimental activity for assessing the impact of the proposal.\nUnder consideration in Theory and Practice of Logic Programming (TPLP).", "published": "2018-12-23 14:35:58", "link": "http://arxiv.org/abs/1812.09718v2", "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Pansori: ASR Corpus Generation from Open Online Video Contents", "abstract": "This paper introduces Pansori, a program used to create ASR (automatic speech\nrecognition) corpora from online video contents. It utilizes a cloud-based\nspeech API to easily create a corpus in different languages. Using this\nprogram, we semi-automatically generated the Pansori-TEDxKR dataset from Korean\nTED conference talks with community-transcribed subtitles. It is the first\nhigh-quality corpus for the Korean language freely available for independent\nresearch. Pansori is released as an open-source software and the generated\ncorpus is released under a permissive public license for community use and\nparticipation.", "published": "2018-12-23 23:57:07", "link": "http://arxiv.org/abs/1812.09798v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Speech Recognition via Segmental Empirical Output\n  Distribution Matching", "abstract": "We consider the problem of training speech recognition systems without using\nany labeled data, under the assumption that the learner can only access to the\ninput utterances and a phoneme language model estimated from a non-overlapping\ncorpus. We propose a fully unsupervised learning algorithm that alternates\nbetween solving two sub-problems: (i) learn a phoneme classifier for a given\nset of phoneme segmentation boundaries, and (ii) refining the phoneme\nboundaries based on a given classifier. To solve the first sub-problem, we\nintroduce a novel unsupervised cost function named Segmental Empirical Output\nDistribution Matching, which generalizes the work in (Liu et al., 2017) to\nsegmental structures. For the second sub-problem, we develop an approximate MAP\napproach to refining the boundaries obtained from Wang et al. (2017).\nExperimental results on TIMIT dataset demonstrate the success of this fully\nunsupervised phoneme recognition system, which achieves a phone error rate\n(PER) of 41.6%. Although it is still far away from the state-of-the-art\nsupervised systems, we show that with oracle boundaries and matching language\nmodel, the PER could be improved to 32.5%.This performance approaches the\nsupervised system of the same model architecture, demonstrating the great\npotential of the proposed method.", "published": "2018-12-23 01:58:39", "link": "http://arxiv.org/abs/1812.09323v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
