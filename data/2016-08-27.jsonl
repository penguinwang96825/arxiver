{"title": "A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence\n  Features", "abstract": "Relation classification is associated with many potential applications in the\nartificial intelligence area. Recent approaches usually leverage neural\nnetworks based on structure features such as syntactic or dependency features\nto solve this problem. However, high-cost structure features make such\napproaches inconvenient to be directly used. In addition, structure features\nare probably domain-dependent. Therefore, this paper proposes a bi-directional\nlong-short-term-memory recurrent-neural-network (Bi-LSTM-RNN) model based on\nlow-cost sequence features to address relation classification. This model\ndivides a sentence or text segment into five parts, namely two target entities\nand their three contexts. It learns the representations of entities and their\ncontexts, and uses them to classify relations. We evaluate our model on two\nstandard benchmark datasets in different domains, namely SemEval-2010 Task 8\nand BioNLP-ST 2016 Task BB3. In the former dataset, our model achieves\ncomparable performance compared with other models using sequence features. In\nthe latter dataset, our model obtains the third best results compared with\nother models in the official evaluation. Moreover, we find that the context\nbetween two target entities plays the most important role in relation\nclassification. Furthermore, statistic experiments show that the context\nbetween two target entities can be used as an approximate replacement of the\nshortest dependency path when dependency parsing is not used.", "published": "2016-08-27 15:41:22", "link": "http://arxiv.org/abs/1608.07720v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Testing APSyn against Vector Cosine on Similarity Estimation", "abstract": "In Distributional Semantic Models (DSMs), Vector Cosine is widely used to\nestimate similarity between word vectors, although this measure was noticed to\nsuffer from several shortcomings. The recent literature has proposed other\nmethods which attempt to mitigate such biases. In this paper, we intend to\ninvestigate APSyn, a measure that computes the extent of the intersection\nbetween the most associated contexts of two target words, weighting it by\ncontext relevance. We evaluated this metric in a similarity estimation task on\nseveral popular test sets, and our results show that APSyn is in fact highly\ncompetitive, even with respect to the results reported in the literature for\nword embeddings. On top of it, APSyn addresses some of the weaknesses of Vector\nCosine, performing well also on genuine similarity estimation.", "published": "2016-08-27 19:57:55", "link": "http://arxiv.org/abs/1608.07738v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to generalize to new compositions in image understanding", "abstract": "Recurrent neural networks have recently been used for learning to describe\nimages using natural language. However, it has been observed that these models\ngeneralize poorly to scenes that were not observed during training, possibly\ndepending too strongly on the statistics of the text in the training data. Here\nwe propose to describe images using short structured representations, aiming to\ncapture the crux of a description. These structured representations allow us to\ntease-out and evaluate separately two types of generalization: standard\ngeneralization to new images with similar scenes, and generalization to new\ncombinations of known entities. We compare two learning approaches on the\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\nAttend and Tell), and a simple structured prediction model on top of a deep\nnetwork. We find that the structured model generalizes to new compositions\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\nstructured representations. By providing a concrete method to quantify\ngeneralization for unseen combinations, we argue that structured\nrepresentations and compositional splits are a useful benchmark for image\ncaptioning, and advocate compositional models that capture linguistic and\nvisual structure.", "published": "2016-08-27 00:34:00", "link": "http://arxiv.org/abs/1608.07639v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
