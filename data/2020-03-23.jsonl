{"title": "E2EET: From Pipeline to End-to-end Entity Typing via Transformer-Based\n  Embeddings", "abstract": "Entity Typing (ET) is the process of identifying the semantic types of every\nentity within a corpus. In contrast to Named Entity Recognition, where each\ntoken in a sentence is labelled with zero or one class label, ET involves\nlabelling each entity mention with one or more class labels. Existing entity\ntyping models, which operate at the mention level, are limited by two key\nfactors: they do not make use of recently-proposed context-dependent\nembeddings, and are trained on fixed context windows. They are therefore\nsensitive to window size selection and are unable to incorporate the context of\nthe entire document. In light of these drawbacks we propose to incorporate\ncontext using transformer-based embeddings for a mention-level model, and an\nend-to-end model using a Bi-GRU to remove the dependency on window size. An\nextensive ablative study demonstrates the effectiveness of contextualised\nembeddings for mention-level models and the competitiveness of our end-to-end\nmodel for entity typing.", "published": "2020-03-23 06:46:28", "link": "http://arxiv.org/abs/2003.10097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Word Polysemy Quantification with Multiresolution Grids of\n  Contextual Embeddings", "abstract": "The number of senses of a given word, or polysemy, is a very subjective\nnotion, which varies widely across annotators and resources. We propose a novel\nmethod to estimate polysemy, based on simple geometry in the contextual\nembedding space. Our approach is fully unsupervised and purely data-driven. We\nshow through rigorous experiments that our rankings are well correlated (with\nstrong statistical significance) with 6 different rankings derived from famous\nhuman-constructed resources such as WordNet, OntoNotes, Oxford, Wikipedia etc.,\nfor 6 different standard metrics. We also visualize and analyze the correlation\nbetween the human rankings. A valuable by-product of our method is the ability\nto sample, at no extra cost, sentences containing different senses of a given\nword. Finally, the fully unsupervised nature of our method makes it applicable\nto any language.\n  Code and data are publicly available at\nhttps://github.com/ksipos/polysemy-assessment .\n  The paper was accepted as a long paper at EACL 2021.", "published": "2020-03-23 12:38:40", "link": "http://arxiv.org/abs/2003.10224v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast Cross-domain Data Augmentation through Neural Sentence Editing", "abstract": "Data augmentation promises to alleviate data scarcity. This is most important\nin cases where the initial data is in short supply. This is, for existing\nmethods, also where augmenting is the most difficult, as learning the full data\ndistribution is impossible. For natural language, sentence editing offers a\nsolution - relying on small but meaningful changes to the original ones.\nLearning which changes are meaningful also requires large amounts of training\ndata. We thus aim to learn this in a source domain where data is abundant and\napply it in a different, target domain, where data is scarce - cross-domain\naugmentation.\n  We create the Edit-transformer, a Transformer-based sentence editor that is\nsignificantly faster than the state of the art and also works cross-domain. We\nargue that, due to its structure, the Edit-transformer is better suited for\ncross-domain environments than its edit-based predecessors. We show this\nperformance gap on the Yelp-Wikipedia domain pairs. Finally, we show that due\nto this cross-domain performance advantage, the Edit-transformer leads to\nmeaningful performance gains in several downstream tasks.", "published": "2020-03-23 13:05:27", "link": "http://arxiv.org/abs/2003.10254v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than\n  Generators", "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the\ninput by replacing some tokens with [MASK] and then train a model to\nreconstruct the original tokens. While they produce good results when\ntransferred to downstream NLP tasks, they generally require large amounts of\ncompute to be effective. As an alternative, we propose a more sample-efficient\npre-training task called replaced token detection. Instead of masking the\ninput, our approach corrupts it by replacing some tokens with plausible\nalternatives sampled from a small generator network. Then, instead of training\na model that predicts the original identities of the corrupted tokens, we train\na discriminative model that predicts whether each token in the corrupted input\nwas replaced by a generator sample or not. Thorough experiments demonstrate\nthis new pre-training task is more efficient than MLM because the task is\ndefined over all input tokens rather than just the small subset that was masked\nout. As a result, the contextual representations learned by our approach\nsubstantially outperform the ones learned by BERT given the same model size,\ndata, and compute. The gains are particularly strong for small models; for\nexample, we train a model on one GPU for 4 days that outperforms GPT (trained\nusing 30x more compute) on the GLUE natural language understanding benchmark.\nOur approach also works well at scale, where it performs comparably to RoBERTa\nand XLNet while using less than 1/4 of their compute and outperforms them when\nusing the same amount of compute.", "published": "2020-03-23 21:17:42", "link": "http://arxiv.org/abs/2003.10555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Yor\u00f9b\u00e1 Diacritic Restoration", "abstract": "Yor\\`ub\\'a is a widely spoken West African language with a writing system\nrich in orthographic and tonal diacritics. They provide morphological\ninformation, are crucial for lexical disambiguation, pronunciation and are\nvital for any computational Speech or Natural Language Processing tasks.\nHowever diacritic marks are commonly excluded from electronic texts due to\nlimited device and application support as well as general education on proper\nusage. We report on recent efforts at dataset cultivation. By aggregating and\nimproving disparate texts from the web and various personal libraries, we were\nable to significantly grow our clean Yor\\`ub\\'a dataset from a majority\nBibilical text corpora with three sources to millions of tokens from over a\ndozen sources. We evaluate updated diacritic restoration models on a new,\ngeneral purpose, public-domain Yor\\`ub\\'a evaluation dataset of modern\njournalistic news text, selected to be multi-purpose and reflecting\ncontemporary usage. All pre-trained models, datasets and source-code have been\nreleased as an open-source project to advance efforts on Yor\\`ub\\'a language\ntechnology.", "published": "2020-03-23 22:07:15", "link": "http://arxiv.org/abs/2003.10564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Caption Generation of Robot Behaviors based on Unsupervised Learning of\n  Action Segments", "abstract": "Bridging robot action sequences and their natural language captions is an\nimportant task to increase explainability of human assisting robots in their\nrecently evolving field. In this paper, we propose a system for generating\nnatural language captions that describe behaviors of human assisting robots.\nThe system describes robot actions by using robot observations; histories from\nactuator systems and cameras, toward end-to-end bridging between robot actions\nand natural language captions. Two reasons make it challenging to apply\nexisting sequence-to-sequence models to this mapping: 1) it is hard to prepare\na large-scale dataset for any kind of robots and their environment, and 2)\nthere is a gap between the number of samples obtained from robot action\nobservations and generated word sequences of captions. We introduced\nunsupervised segmentation based on K-means clustering to unify typical robot\nobservation patterns into a class. This method makes it possible for the\nnetwork to learn the relationship from a small amount of data. Moreover, we\nutilized a chunking method based on byte-pair encoding (BPE) to fill in the gap\nbetween the number of samples of robot action observations and words in a\ncaption. We also applied an attention mechanism to the segmentation task.\nExperimental results show that the proposed model based on unsupervised\nlearning can generate better descriptions than other methods. We also show that\nthe attention mechanism did not work well in our low-resource setting.", "published": "2020-03-23 03:44:56", "link": "http://arxiv.org/abs/2003.10066v1", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Multimodal Analytics for Real-world News using Measures of Cross-modal\n  Entity Consistency", "abstract": "The World Wide Web has become a popular source for gathering information and\nnews. Multimodal information, e.g., enriching text with photos, is typically\nused to convey the news more effectively or to attract attention. Photo content\ncan range from decorative, depict additional important information, or can even\ncontain misleading information. Therefore, automatic approaches to quantify\ncross-modal consistency of entity representation can support human assessors to\nevaluate the overall multimodal message, for instance, with regard to bias or\nsentiment. In some cases such measures could give hints to detect fake news,\nwhich is an increasingly important topic in today's society. In this paper, we\nintroduce a novel task of cross-modal consistency verification in real-world\nnews and present a multimodal approach to quantify the entity coherence between\nimage and text. Named entity linking is applied to extract persons, locations,\nand events from news texts. Several measures are suggested to calculate\ncross-modal similarity for these entities using state of the art approaches. In\ncontrast to previous work, our system automatically gathers example data from\nthe Web and is applicable to real-world news. Results on two novel datasets\nthat cover different languages, topics, and domains demonstrate the feasibility\nof our approach. Datasets and code are publicly available to foster research\ntowards this new direction.", "published": "2020-03-23 17:49:06", "link": "http://arxiv.org/abs/2003.10421v2", "categories": ["cs.CL", "cs.IR", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Data-driven models and computational tools for neurolinguistics: a\n  language technology perspective", "abstract": "In this paper, our focus is the connection and influence of language\ntechnologies on the research in neurolinguistics. We present a review of brain\nimaging-based neurolinguistic studies with a focus on the natural language\nrepresentations, such as word embeddings and pre-trained language models.\nMutual enrichment of neurolinguistics and language technologies leads to\ndevelopment of brain-aware natural language representations. The importance of\nthis research area is emphasized by medical applications.", "published": "2020-03-23 20:41:51", "link": "http://arxiv.org/abs/2003.10540v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation", "abstract": "Optical character recognition (OCR) systems performance have improved\nsignificantly in the deep learning era. This is especially true for handwritten\ntext recognition (HTR), where each author has a unique style, unlike printed\ntext, where the variation is smaller by design. That said, deep learning based\nHTR is limited, as in every other task, by the number of training examples.\nGathering data is a challenging and costly task, and even more so, the labeling\ntask that follows, of which we focus here. One possible approach to reduce the\nburden of data annotation is semi-supervised learning. Semi supervised methods\nuse, in addition to labeled data, some unlabeled samples to improve\nperformance, compared to fully supervised ones. Consequently, such methods may\nadapt to unseen images during test time.\n  We present ScrabbleGAN, a semi-supervised approach to synthesize handwritten\ntext images that are versatile both in style and lexicon. ScrabbleGAN relies on\na novel generative model which can generate images of words with an arbitrary\nlength. We show how to operate our approach in a semi-supervised manner,\nenjoying the aforementioned benefits such as performance boost over state of\nthe art supervised HTR. Furthermore, our generator can manipulate the resulting\ntext style. This allows us to change, for instance, whether the text is\ncursive, or how thin is the pen stroke.", "published": "2020-03-23 21:41:19", "link": "http://arxiv.org/abs/2003.10557v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BaitWatcher: A lightweight web interface for the detection of\n  incongruent news headlines", "abstract": "In digital environments where substantial amounts of information are shared\nonline, news headlines play essential roles in the selection and diffusion of\nnews articles. Some news articles attract audience attention by showing\nexaggerated or misleading headlines. This study addresses the \\textit{headline\nincongruity} problem, in which a news headline makes claims that are either\nunrelated or opposite to the contents of the corresponding article. We present\n\\textit{BaitWatcher}, which is a lightweight web interface that guides readers\nin estimating the likelihood of incongruence in news articles before clicking\non the headlines. BaitWatcher utilizes a hierarchical recurrent encoder that\nefficiently learns complex textual representations of a news headline and its\nassociated body text. For training the model, we construct a million scale\ndataset of news articles, which we also release for broader research use. Based\non the results of a focus group interview, we discuss the importance of\ndeveloping an interpretable AI agent for the design of a better interface for\nmitigating the effects of online misinformation.", "published": "2020-03-23 23:43:02", "link": "http://arxiv.org/abs/2003.11459v1", "categories": ["cs.CL", "cs.IR", "cs.SI", "68U15"], "primary_category": "cs.CL"}
{"title": "Word2Vec: Optimal Hyper-Parameters and Their Impact on NLP Downstream\n  Tasks", "abstract": "Word2Vec is a prominent model for natural language processing (NLP) tasks.\nSimilar inspiration is found in distributed embeddings for new state-of-the-art\n(SotA) deep neural networks. However, wrong combination of hyper-parameters can\nproduce poor quality vectors. The objective of this work is to empirically show\noptimal combination of hyper-parameters exists and evaluate various\ncombinations. We compare them with the released, pre-trained original word2vec\nmodel. Both intrinsic and extrinsic (downstream) evaluations, including named\nentity recognition (NER) and sentiment analysis (SA) were carried out. The\ndownstream tasks reveal that the best model is usually task-specific, high\nanalogy scores don't necessarily correlate positively with F1 scores and the\nsame applies to focus on data alone. Increasing vector dimension size after a\npoint leads to poor quality or performance. If ethical considerations to save\ntime, energy and the environment are made, then reasonably smaller corpora may\ndo just as well or even better in some cases. Besides, using a small corpus, we\nobtain better human-assigned WordSim scores, corresponding Spearman correlation\nand better downstream performances (with significance tests) compared to the\noriginal model, trained on 100 billion-word corpus.", "published": "2020-03-23 07:38:17", "link": "http://arxiv.org/abs/2003.11645v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Dialect Identification of Spoken North S\u00e1mi Language Varieties Using\n  Prosodic Features", "abstract": "This work explores the application of various supervised classification\napproaches using prosodic information for the identification of spoken North\nS\\'ami language varieties. Dialects are language varieties that enclose\ncharacteristics specific for a given region or community. These characteristics\nreflect segmental and suprasegmental (prosodic) differences but also high-level\nproperties such as lexical and morphosyntactic. One aspect that is of\nparticular interest and that has not been studied extensively is how the\ndifferences in prosody may underpin the potential differences among different\ndialects. To address this, this work focuses on investigating the standard\nacoustic prosodic features of energy, fundamental frequency, spectral tilt,\nduration, and their combinations, using sequential and context-independent\nsupervised classification methods, and evaluated separately over two different\nunits in speech: words and syllables. The primary aim of this work is to gain a\nbetter understanding on the role of prosody in identifying among the different\nlanguage varieties. Our results show that prosodic information holds an\nimportant role in distinguishing between the five areal varieties of North\nS\\'ami where the inclusion of contextual information for all acoustic prosodic\nfeatures is critical for the identification of dialects for words and\nsyllables.", "published": "2020-03-23 11:16:45", "link": "http://arxiv.org/abs/2003.10183v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Low Latency End-to-End Streaming Speech Recognition with a Scout Network", "abstract": "The attention-based Transformer model has achieved promising results for\nspeech recognition (SR) in the offline mode. However, in the streaming mode,\nthe Transformer model usually incurs significant latency to maintain its\nrecognition accuracy when applying a fixed-length look-ahead window in each\nencoder layer. In this paper, we propose a novel low-latency streaming approach\nfor Transformer models, which consists of a scout network and a recognition\nnetwork. The scout network detects the whole word boundary without seeing any\nfuture frames, while the recognition network predicts the next subword by\nutilizing the information from all the frames before the predicted boundary.\nOur model achieves the best performance (2.7/6.4 WER) with only 639 ms latency\non the test-clean and test-other data sets of Librispeech.", "published": "2020-03-23 16:34:19", "link": "http://arxiv.org/abs/2003.10369v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
