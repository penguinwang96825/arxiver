{"title": "Tree-structured multi-stage principal component analysis (TMPCA): theory\n  and applications", "abstract": "A PCA based sequence-to-vector (seq2vec) dimension reduction method for the\ntext classification problem, called the tree-structured multi-stage principal\ncomponent analysis (TMPCA) is presented in this paper. Theoretical analysis and\napplicability of TMPCA are demonstrated as an extension to our previous work\n(Su, Huang & Kuo). Unlike conventional word-to-vector embedding methods, the\nTMPCA method conducts dimension reduction at the sequence level without labeled\ntraining data. Furthermore, it can preserve the sequential structure of input\nsequences. We show that TMPCA is computationally efficient and able to\nfacilitate sequence-based text classification tasks by preserving strong mutual\ninformation between its input and output mathematically. It is also\ndemonstrated by experimental results that a dense (fully connected) network\ntrained on the TMPCA preprocessed data achieves better performance than\nstate-of-the-art fastText and other neural-network-based solutions.", "published": "2018-07-22 03:15:44", "link": "http://arxiv.org/abs/1807.08228v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "German Dialect Identification Using Classifier Ensembles", "abstract": "In this paper we present the GDI_classification entry to the second German\nDialect Identification (GDI) shared task organized within the scope of the\nVarDial Evaluation Campaign 2018. We present a system based on SVM classifier\nensembles trained on characters and words. The system was trained on a\ncollection of speech transcripts of five Swiss-German dialects provided by the\norganizers. The transcripts included in the dataset contained speakers from\nBasel, Bern, Lucerne, and Zurich. Our entry in the challenge reached 62.03%\nF1-score and was ranked third out of eight teams.", "published": "2018-07-22 03:20:19", "link": "http://arxiv.org/abs/1807.08230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Scientific Writing Styles from the Perspective of Linguistic\n  Complexity", "abstract": "Publishing articles in high-impact English journals is difficult for scholars\naround the world, especially for non-native English-speaking scholars (NNESs),\nmost of whom struggle with proficiency in English. In order to uncover the\ndifferences in English scientific writing between native English-speaking\nscholars (NESs) and NNESs, we collected a large-scale data set containing more\nthan 150,000 full-text articles published in PLoS between 2006 and 2015. We\ndivided these articles into three groups according to the ethnic backgrounds of\nthe first and corresponding authors, obtained by Ethnea, and examined the\nscientific writing styles in English from a two-fold perspective of linguistic\ncomplexity: (1) syntactic complexity, including measurements of sentence length\nand sentence complexity; and (2) lexical complexity, including measurements of\nlexical diversity, lexical density, and lexical sophistication. The\nobservations suggest marginal differences between groups in syntactical and\nlexical complexity.", "published": "2018-07-22 21:35:01", "link": "http://arxiv.org/abs/1807.08374v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-scale Alignment and Contextual History for Attention Mechanism in\n  Sequence-to-sequence Model", "abstract": "A sequence-to-sequence model is a neural network module for mapping two\nsequences of different lengths. The sequence-to-sequence model has three core\nmodules: encoder, decoder, and attention. Attention is the bridge that connects\nthe encoder and decoder modules and improves model performance in many tasks.\nIn this paper, we propose two ideas to improve sequence-to-sequence model\nperformance by enhancing the attention module. First, we maintain the history\nof the location and the expected context from several previous time-steps.\nSecond, we apply multiscale convolution from several previous attention vectors\nto the current decoder state. We utilized our proposed framework for\nsequence-to-sequence speech recognition and text-to-speech systems. The results\nreveal that our proposed extension could improve performance significantly\ncompared to a standard attention baseline.", "published": "2018-07-22 13:10:30", "link": "http://arxiv.org/abs/1807.08280v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unified Hypersphere Embedding for Speaker Recognition", "abstract": "Incremental improvements in accuracy of Convolutional Neural Networks are\nusually achieved through use of deeper and more complex models trained on\nlarger datasets. However, enlarging dataset and models increases the\ncomputation and storage costs and cannot be done indefinitely. In this work, we\nseek to improve the identification and verification accuracy of a\ntext-independent speaker recognition system without use of extra data or deeper\nand more complex models by augmenting the training and testing data, finding\nthe optimal dimensionality of embedding space and use of more discriminative\nloss functions. Results of experiments on VoxCeleb dataset suggest that: (i)\nSimple repetition and random time-reversion of utterances can reduce prediction\nerrors by up to 18%. (ii) Lower dimensional embeddings are more suitable for\nverification. (iii) Use of proposed logistic margin loss function leads to\nunified embeddings with state-of-the-art identification and competitive\nverification accuracies.", "published": "2018-07-22 16:26:31", "link": "http://arxiv.org/abs/1807.08312v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
