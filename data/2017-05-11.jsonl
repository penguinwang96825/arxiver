{"title": "Learning with Noise: Enhance Distantly Supervised Relation Extraction\n  with Dynamic Transition Matrix", "abstract": "Distant supervision significantly reduces human efforts in building training\ndata for many classification tasks. While promising, this technique often\nintroduces noise to the generated training data, which can severely affect the\nmodel performance. In this paper, we take a deep look at the application of\ndistant supervision in relation extraction. We show that the dynamic transition\nmatrix can effectively characterize the noise in the training data built by\ndistant supervision. The transition matrix can be effectively trained using a\nnovel curriculum learning based method without any direct supervision about the\nnoise. We thoroughly evaluate our approach under a wide range of extraction\nscenarios. Experimental results show that our approach consistently improves\nthe extraction results and outperforms the state-of-the-art in various\nevaluation scenarios.", "published": "2017-05-11 02:56:29", "link": "http://arxiv.org/abs/1705.03995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Content-based Approach for Vietnamese Spam SMS Filtering", "abstract": "Short Message Service (SMS) spam is a serious problem in Vietnam because of\nthe availability of very cheap pre-paid SMS packages. There are some systems to\ndetect and filter spam messages for English, most of which use machine learning\ntechniques to analyze the content of messages and classify them. For\nVietnamese, there is some research on spam email filtering but none focused on\nSMS. In this work, we propose the first system for filtering Vietnamese spam\nSMS. We first propose an appropriate preprocessing method since existing tools\nfor Vietnamese preprocessing cannot give good accuracy on our dataset. We then\nexperiment with vector representations and classifiers to find the best model\nfor this problem. Our system achieves an accuracy of 94% when labelling spam\nmessages while the misclassification rate of legitimate messages is relatively\nsmall, about only 0.4%. This is an encouraging result compared to that of\nEnglish and can be served as a strong baseline for future development of\nVietnamese SMS spam prevention systems.", "published": "2017-05-11 04:04:33", "link": "http://arxiv.org/abs/1705.04003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Semantic Role Labelling System for Vietnamese", "abstract": "Semantic role labelling (SRL) is a task in natural language processing which\ndetects and classifies the semantic arguments associated with the predicates of\na sentence. It is an important step towards understanding the meaning of a\nnatural language. There exists SRL systems for well-studied languages like\nEnglish, Chinese or Japanese but there is not any such system for the\nVietnamese language. In this paper, we present the first SRL system for\nVietnamese with encouraging accuracy. We first demonstrate that a simple\napplication of SRL techniques developed for English could not give a good\naccuracy for Vietnamese. We then introduce a new algorithm for extracting\ncandidate syntactic constituents, which is much more accurate than the common\nnode-mapping algorithm usually used in the identification step. Finally, in the\nclassification step, in addition to the common linguistic features, we propose\nnovel and useful features for use in SRL. Our SRL system achieves an $F_1$\nscore of 73.53\\% on the Vietnamese PropBank corpus. This system, including\nsoftware and corpus, is available as an open source project and we believe that\nit is a good baseline for the development of future Vietnamese SRL systems.", "published": "2017-05-11 07:08:30", "link": "http://arxiv.org/abs/1705.04038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Recurrent Neural Network Models for Vietnamese Named Entity\n  Recognition: Word-level vs. Character-level", "abstract": "This paper demonstrates end-to-end neural network architectures for\nVietnamese named entity recognition. Our best model is a combination of\nbidirectional Long Short-Term Memory (Bi-LSTM), Convolutional Neural Network\n(CNN), Conditional Random Field (CRF), using pre-trained word embeddings as\ninput, which achieves an F1 score of 88.59% on a standard test set. Our system\nis able to achieve a comparable performance to the first-rank system of the\nVLSP campaign without using any syntactic or hand-crafted features. We also\ngive an extensive empirical study on using common deep learning models for\nVietnamese NER, at both word and character level.", "published": "2017-05-11 07:31:39", "link": "http://arxiv.org/abs/1705.04044v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Compositional Neural Networks over Tree Structure", "abstract": "Tree-structured neural networks have proven to be effective in learning\nsemantic representations by exploiting syntactic information. In spite of their\nsuccess, most existing models suffer from the underfitting problem: they\nrecursively use the same shared compositional function throughout the whole\ncompositional process and lack expressive power due to inability to capture the\nrichness of compositionality. In this paper, we address this issue by\nintroducing the dynamic compositional neural networks over tree structure\n(DC-TreeNN), in which the compositional function is dynamically generated by a\nmeta network. The role of meta-network is to capture the metaknowledge across\nthe different compositional rules and formulate them. Experimental results on\ntwo typical tasks show the effectiveness of the proposed models.", "published": "2017-05-11 13:11:23", "link": "http://arxiv.org/abs/1705.04153v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sketching Word Vectors Through Hashing", "abstract": "We propose a new fast word embedding technique using hash functions. The\nmethod is a derandomization of a new type of random projections: By\ndisregarding the classic constraint used in designing random projections (i.e.,\npreserving pairwise distances in a particular normed space), our solution\nexploits extremely sparse non-negative random projections. Our experiments show\nthat the proposed method can achieve competitive results, comparable to neural\nembedding learning techniques, however, with only a fraction of the\ncomputational complexity of these methods. While the proposed derandomization\nenhances the computational and space complexity of our method, the possibility\nof applying weighting methods such as positive pointwise mutual information\n(PPMI) to our models after their construction (and at a reduced dimensionality)\nimparts a high discriminatory power to the resulting embeddings. Obviously,\nthis method comes with other known benefits of random projection-based\ntechniques such as ease of update.", "published": "2017-05-11 15:53:00", "link": "http://arxiv.org/abs/1705.04253v2", "categories": ["cs.CL", "68W20, 68W01, 68T01, 68T20, 68T50", "I.2.7; I.2.4; I.5.2"], "primary_category": "cs.CL"}
{"title": "A Deep Reinforced Model for Abstractive Summarization", "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization\nhave achieved good performance on short input and output sequences. For longer\ndocuments and summaries however these models often include repetitive and\nincoherent phrases. We introduce a neural network model with a novel\nintra-attention that attends over the input and continuously generated output\nseparately, and a new training method that combines standard supervised word\nprediction and reinforcement learning (RL). Models trained only with supervised\nlearning often exhibit \"exposure bias\" - they assume ground truth is provided\nat each step during training. However, when standard word prediction is\ncombined with the global sequence prediction training of RL the resulting\nsummaries become more readable. We evaluate this model on the CNN/Daily Mail\nand New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the\nCNN/Daily Mail dataset, an improvement over previous state-of-the-art models.\nHuman evaluation also shows that our model produces higher quality summaries.", "published": "2017-05-11 17:39:35", "link": "http://arxiv.org/abs/1705.04304v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Bias in Production Speech Models", "abstract": "Replacing hand-engineered pipelines with end-to-end deep learning systems has\nenabled strong results in applications like speech and object recognition.\nHowever, the causality and latency constraints of production systems put\nend-to-end speech models back into the underfitting regime and expose biases in\nthe model that we show cannot be overcome by \"scaling up\", i.e., training\nbigger models on more data. In this work we systematically identify and address\nsources of bias, reducing error rates by up to 20% while remaining practical\nfor deployment. We achieve this by utilizing improved neural architectures for\nstreaming inference, solving optimization issues, and employing strategies that\nincrease audio and label modelling versatility.", "published": "2017-05-11 23:34:42", "link": "http://arxiv.org/abs/1705.04400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the role of words in the network structure of texts: application to\n  authorship attribution", "abstract": "Well-established automatic analyses of texts mainly consider frequencies of\nlinguistic units, e.g. letters, words and bigrams, while methods based on\nco-occurrence networks consider the structure of texts regardless of the nodes\nlabel (i.e. the words semantics). In this paper, we reconcile these distinct\nviewpoints by introducing a generalized similarity measure to compare texts\nwhich accounts for both the network structure of texts and the role of\nindividual words in the networks. We use the similarity measure for authorship\nattribution of three collections of books, each composed of 8 authors and 10\nbooks per author. High accuracy rates were obtained with typical values from\n90% to 98.75%, much higher than with the traditional the TF-IDF approach for\nthe same collections. These accuracies are also higher than taking only the\ntopology of networks into account. We conclude that the different properties of\nspecific words on the macroscopic scale structure of a whole text are as\nrelevant as their frequency of appearance; conversely, considering the identity\nof nodes brings further knowledge about a piece of text represented as a\nnetwork.", "published": "2017-05-11 14:00:10", "link": "http://arxiv.org/abs/1705.04187v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Imagination improves Multimodal Translation", "abstract": "We decompose multimodal translation into two sub-tasks: learning to translate\nand learning visually grounded representations. In a multitask learning\nframework, translations are learned in an attention-based encoder-decoder, and\ngrounded representations are learned through image representation prediction.\nOur approach improves translation performance compared to the state of the art\non the Multi30K dataset. Furthermore, it is equally effective if we train the\nimage prediction task on the external MS COCO dataset, and we find improvements\nif we train the translation model on the external News Commentary parallel\ntext.", "published": "2017-05-11 18:49:17", "link": "http://arxiv.org/abs/1705.04350v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Program Induction by Rationale Generation : Learning to Solve and\n  Explain Algebraic Word Problems", "abstract": "Solving algebraic word problems requires executing a series of arithmetic\noperations---a program---to obtain a final answer. However, since programs can\nbe arbitrarily complicated, inducing them directly from question-answer pairs\nis a formidable challenge. To make this task more feasible, we solve these\nproblems by generating answer rationales, sequences of natural language and\nhuman-readable mathematical expressions that derive the final answer through a\nseries of small steps. Although rationales do not explicitly specify programs,\nthey provide a scaffolding for their structure via intermediate milestones. To\nevaluate our approach, we have created a new 100,000-sample dataset of\nquestions, answers and rationales. Experimental results show that indirect\nsupervision of program learning via answer rationales is a promising strategy\nfor inducing arithmetic programs.", "published": "2017-05-11 13:04:47", "link": "http://arxiv.org/abs/1705.04146v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
