{"title": "Sense Embedding Learning for Word Sense Induction", "abstract": "Conventional word sense induction (WSI) methods usually represent each\ninstance with discrete linguistic features or cooccurrence features, and train\na model for each polysemous word individually. In this work, we propose to\nlearn sense embeddings for the WSI task. In the training stage, our method\ninduces several sense centroids (embedding) for each polysemous word. In the\ntesting stage, our method represents each instance as a contextual vector, and\ninduces its sense by finding the nearest sense centroid in the embedding space.\nThe advantages of our method are (1) distributed sense vectors are taken as the\nknowledge representations which are trained discriminatively, and usually have\nbetter performance than traditional count-based distributional models, and (2)\na general model for the whole vocabulary is jointly trained to induce sense\ncentroids under the mutlitask learning framework. Evaluated on SemEval-2010 WSI\ndataset, our method outperforms all participants and most of the recent\nstate-of-the-art methods. We further verify the two advantages by comparing\nwith carefully designed baselines.", "published": "2016-06-17 02:49:52", "link": "http://arxiv.org/abs/1606.05409v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax\n  Trees and Strings", "abstract": "We present a natural language generator based on the sequence-to-sequence\napproach that can be trained to produce natural language strings as well as\ndeep syntax dependency trees from input dialogue acts, and we use it to\ndirectly compare two-step generation with separate sentence planning and\nsurface realization stages to a joint, one-step approach. We were able to train\nboth setups successfully using very little training data. The joint setup\noffers better performance, surpassing state-of-the-art with regards to\nn-gram-based scores while providing more relevant outputs.", "published": "2016-06-17 11:51:25", "link": "http://arxiv.org/abs/1606.05491v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Universal, Unsupervised (Rule-Based), Uncovered Sentiment Analysis", "abstract": "We present a novel unsupervised approach for multilingual sentiment analysis\ndriven by compositional syntax-based rules. On the one hand, we exploit some of\nthe main advantages of unsupervised algorithms: (1) the interpretability of\ntheir output, in contrast with most supervised models, which behave as a black\nbox and (2) their robustness across different corpora and domains. On the other\nhand, by introducing the concept of compositional operations and exploiting\nsyntactic information in the form of universal dependencies, we tackle one of\ntheir main drawbacks: their rigidity on data that are structured differently\ndepending on the language concerned. Experiments show an improvement both over\nexisting unsupervised methods, and over state-of-the-art supervised models when\nevaluating outside their corpus of origin. Experiments also show how the same\ncompositional operations can be shared across languages. The system is\navailable at http://www.grupolys.org/software/UUUSA/", "published": "2016-06-17 14:53:02", "link": "http://arxiv.org/abs/1606.05545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two Discourse Driven Language Models for Semantics", "abstract": "Natural language understanding often requires deep semantic knowledge.\nExpanding on previous proposals, we suggest that some important aspects of\nsemantic knowledge can be modeled as a language model if done at an appropriate\nlevel of abstraction. We develop two distinct models that capture semantic\nframe chains and discourse information while abstracting over the specific\nmentions of predicates and entities. For each model, we investigate four\nimplementations: a \"standard\" N-gram language model and three discriminatively\ntrained \"neural\" language models that generate embeddings for semantic frames.\nThe quality of the semantic language models (SemLM) is evaluated both\nintrinsically, using perplexity and a narrative cloze test and extrinsically -\nwe show that our SemLM helps improve performance on semantic natural language\nprocessing tasks such as co-reference resolution and discourse parsing.", "published": "2016-06-17 21:19:35", "link": "http://arxiv.org/abs/1606.05679v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Socially-Informed Timeline Generation for Complex Events", "abstract": "Existing timeline generation systems for complex events consider only\ninformation from traditional media, ignoring the rich social context provided\nby user-generated content that reveals representative public interests or\ninsightful opinions. We instead aim to generate socially-informed timelines\nthat contain both news article summaries and selected user comments. We present\nan optimization framework designed to balance topical cohesion between the\narticle and comment summaries along with their informativeness and coverage of\nthe event. Automatic evaluations on real-world datasets that cover four complex\nevents show that our system produces more informative timelines than\nstate-of-the-art systems. In human evaluation, the associated comment summaries\nare furthermore rated more insightful than editor's picks and comments ranked\nhighly by users.", "published": "2016-06-17 22:52:09", "link": "http://arxiv.org/abs/1606.05699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-Focused Opinion Summarization for User-Generated Content", "abstract": "We present a submodular function-based framework for query-focused opinion\nsummarization. Within our framework, relevance ordering produced by a\nstatistical ranker, and information coverage with respect to topic distribution\nand diverse viewpoints are both encoded as submodular functions. Dispersion\nfunctions are utilized to minimize the redundancy. We are the first to evaluate\ndifferent metrics of text similarity for submodularity-based summarization\nmethods. By experimenting on community QA and blog summarization, we show that\nour system outperforms state-of-the-art approaches in both automatic evaluation\nand human evaluation. A human evaluation task is conducted on Amazon Mechanical\nTurk with scale, and shows that our systems are able to generate summaries of\nhigh overall quality and information diversity.", "published": "2016-06-17 23:05:41", "link": "http://arxiv.org/abs/1606.05702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Piece of My Mind: A Sentiment Analysis Approach for Online Dispute\n  Detection", "abstract": "We investigate the novel task of online dispute detection and propose a\nsentiment analysis solution to the problem: we aim to identify the sequence of\nsentence-level sentiments expressed during a discussion and to use them as\nfeatures in a classifier that predicts the DISPUTE/NON-DISPUTE label for the\ndiscussion as a whole. We evaluate dispute detection approaches on a newly\ncreated corpus of Wikipedia Talk page disputes and find that classifiers that\nrely on our sentiment tagging features outperform those that do not. The best\nmodel achieves a very promising F1 score of 0.78 and an accuracy of 0.80.", "published": "2016-06-17 23:22:39", "link": "http://arxiv.org/abs/1606.05704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Agreement and Disagreement Identification in Online\n  Discussions with A Socially-Tuned Sentiment Lexicon", "abstract": "We study the problem of agreement and disagreement detection in online\ndiscussions. An isotonic Conditional Random Fields (isotonic CRF) based\nsequential model is proposed to make predictions on sentence- or segment-level.\nWe automatically construct a socially-tuned lexicon that is bootstrapped from\nexisting general-purpose sentiment lexicons to further improve the performance.\nWe evaluate our agreement and disagreement tagging model on two disparate\nonline discussion corpora -- Wikipedia Talk pages and online debates. Our model\nis shown to outperform the state-of-the-art approaches in both datasets. For\nexample, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for\nagreement and disagreement detection, when a linear chain CRF obtains 0.58 and\n0.56 for the discussions on Wikipedia Talk pages.", "published": "2016-06-17 23:29:11", "link": "http://arxiv.org/abs/1606.05706v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Inference using Statistical Name Characteristics in Twitter", "abstract": "Much attention has been given to the task of gender inference of Twitter\nusers. Although names are strong gender indicators, the names of Twitter users\nare rarely used as a feature; probably due to the high number of ill-formed\nnames, which cannot be found in any name dictionary. Instead of relying solely\non a name database, we propose a novel name classifier. Our approach extracts\ncharacteristics from the user names and uses those in order to assign the names\nto a gender. This enables us to classify international first names as well as\nill-formed names.", "published": "2016-06-17 10:24:29", "link": "http://arxiv.org/abs/1606.05467v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Data-driven HR - R\u00e9sum\u00e9 Analysis Based on Natural Language\n  Processing and Machine Learning", "abstract": "Recruiters usually spend less than a minute looking at each r\\'esum\\'e when\ndeciding whether it's worth continuing the recruitment process with the\ncandidate. Recruiters focus on keywords, and it's almost impossible to\nguarantee a fair process of candidate selection. The main scope of this paper\nis to tackle this issue by introducing a data-driven approach that shows how to\nprocess r\\'esum\\'es automatically and give recruiters more time to only examine\npromising candidates. Furthermore, we show how to leverage Machine Learning and\nNatural Language Processing in order to extract all required information from\nthe r\\'esum\\'es. Once the information is extracted, a ranking score is\ncalculated. The score describes how well the candidates fit based on their\neducation, work experience and skills. Later this paper illustrates a prototype\napplication that shows how this novel approach can increase the productivity of\nrecruiters. The application enables them to filter and rank candidates based on\npredefined job descriptions. Guided by the ranking, recruiters can get deeper\ninsights from candidate profiles and validate why and how the application\nranked them. This application shows how to improve the hiring process by giving\nan unbiased hiring decision support.", "published": "2016-06-17 17:52:31", "link": "http://arxiv.org/abs/1606.05611v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using\n  Character and Word-Level CNNs", "abstract": "This paper describes our approach for the Detecting Stance in Tweets task\n(SemEval-2016 Task 6). We utilized recent advances in short text categorization\nusing deep learning to create word-level and character-level models. The choice\nbetween word-level and character-level models in each particular case was\ninformed through validation performance. Our final system is a combination of\nclassifiers using word-level or character-level models. We also employed novel\ndata augmentation techniques to expand and diversify our training dataset, thus\nmaking our system more robust. Our system achieved a macro-average precision,\nrecall and F1-scores of 0.67, 0.61 and 0.635 respectively.", "published": "2016-06-17 22:32:50", "link": "http://arxiv.org/abs/1606.05694v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Stance Detection with Bidirectional Conditional Encoding", "abstract": "Stance detection is the task of classifying the attitude expressed in a text\ntowards a target such as Hillary Clinton to be \"positive\", negative\" or\n\"neutral\". Previous work has assumed that either the target is mentioned in the\ntext or that training data for every target is given. This paper considers the\nmore challenging version of this task, where targets are not always mentioned\nand no training data is available for the test targets. We experiment with\nconditional LSTM encoding, which builds a representation of the tweet that is\ndependent on the target, and demonstrate that it outperforms encoding the tweet\nand the target independently. Performance is improved further when the\nconditional model is augmented with bidirectional encoding. We evaluate our\napproach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving\nperformance second best only to a system trained on semi-automatically labelled\ntweets for the test target. When such weak supervision is added, our approach\nachieves state-of-the-art results.", "published": "2016-06-17 09:39:47", "link": "http://arxiv.org/abs/1606.05464v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SMS Spam Filtering using Probabilistic Topic Modelling and Stacked\n  Denoising Autoencoder", "abstract": "In This paper we present a novel approach to spam filtering and demonstrate\nits applicability with respect to SMS messages. Our approach requires minimum\nfeatures engineering and a small set of la- belled data samples. Features are\nextracted using topic modelling based on latent Dirichlet allocation, and then\na comprehensive data model is created using a Stacked Denoising Autoencoder\n(SDA). Topic modelling summarises the data providing ease of use and high\ninterpretability by visualising the topics using word clouds. Given that the\nSMS messages can be regarded as either spam (unwanted) or ham (wanted), the SDA\nis able to model the messages and accurately discriminate between the two\nclasses without the need for a pre-labelled training set. The results are\ncompared against the state-of-the-art spam detection algorithms with our\nproposed approach achieving over 97% accuracy which compares favourably to the\nbest reported algorithms presented in the literature.", "published": "2016-06-17 15:15:18", "link": "http://arxiv.org/abs/1606.05554v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
