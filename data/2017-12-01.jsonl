{"title": "Text Generation Based on Generative Adversarial Nets with Latent\n  Variable", "abstract": "In this paper, we propose a model using generative adversarial net (GAN) to\ngenerate realistic text. Instead of using standard GAN, we combine variational\nautoencoder (VAE) with generative adversarial net. The use of high-level latent\nrandom variables is helpful to learn the data distribution and solve the\nproblem that generative adversarial net always emits the similar data. We\npropose the VGAN model where the generative model is composed of recurrent\nneural network and VAE. The discriminative model is a convolutional neural\nnetwork. We train the model via policy gradient. We apply the proposed model to\nthe task of text generation and compare it to other recent neural network based\nmodels, such as recurrent neural network language model and SeqGAN. We evaluate\nthe performance of the model by calculating negative log-likelihood and the\nBLEU score. We conduct experiments on three benchmark datasets, and results\nshow that our model outperforms other previous models.", "published": "2017-12-01 03:14:51", "link": "http://arxiv.org/abs/1712.00170v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual\n  Question Answering", "abstract": "A number of studies have found that today's Visual Question Answering (VQA)\nmodels are heavily driven by superficial correlations in the training data and\nlack sufficient image grounding. To encourage development of models geared\ntowards the latter, we propose a new setting for VQA where for every question\ntype, train and test sets have different prior distributions of answers.\nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we\ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2\nrespectively). First, we evaluate several existing VQA models under this new\nsetting and show that their performance degrades significantly compared to the\noriginal VQA setting. Second, we propose a novel Grounded Visual Question\nAnswering model (GVQA) that contains inductive biases and restrictions in the\narchitecture specifically designed to prevent the model from 'cheating' by\nprimarily relying on priors in the training data. Specifically, GVQA explicitly\ndisentangles the recognition of visual concepts present in the image from the\nidentification of plausible answer space for a given question, enabling the\nmodel to more robustly generalize across different distributions of answers.\nGVQA is built off an existing VQA model -- Stacked Attention Networks (SAN).\nOur experiments demonstrate that GVQA significantly outperforms SAN on both\nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more\npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in\nseveral cases. GVQA offers strengths complementary to SAN when trained and\nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more\ntransparent and interpretable than existing VQA models.", "published": "2017-12-01 15:48:50", "link": "http://arxiv.org/abs/1712.00377v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Visual Features for Context-Aware Speech Recognition", "abstract": "Automatic transcriptions of consumer-generated multi-media content such as\n\"Youtube\" videos still exhibit high word error rates. Such data typically\noccupies a very broad domain, has been recorded in challenging conditions, with\ncheap hardware and a focus on the visual modality, and may have been\npost-processed or edited. In this paper, we extend our earlier work on adapting\nthe acoustic model of a DNN-based speech recognition system to an RNN language\nmodel and show how both can be adapted to the objects and scenes that can be\nautomatically detected in the video. We are working on a corpus of \"how-to\"\nvideos from the web, and the idea is that an object that can be seen (\"car\"),\nor a scene that is being detected (\"kitchen\") can be used to condition both\nmodels on the \"context\" of the recording, thereby reducing perplexity and\nimproving transcription. We achieve good improvements in both cases and compare\nand analyze the respective reductions in word error rate. We expect that our\nresults can be used for any type of speech processing in which \"context\"\ninformation is available, for example in robotics, man-machine interaction, or\nwhen indexing large audio-visual archives, and should ultimately help to bring\ntogether the \"video-to-text\" and \"speech-to-text\" communities.", "published": "2017-12-01 20:56:31", "link": "http://arxiv.org/abs/1712.00489v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Audio Cover Song Identification using Convolutional Neural Network", "abstract": "In this paper, we propose a new approach to cover song identification using a\nCNN (convolutional neural network). Most previous studies extract the feature\nvectors that characterize the cover song relation from a pair of songs and used\nit to compute the (dis)similarity between the two songs. Based on the\nobservation that there is a meaningful pattern between cover songs and that\nthis can be learned, we have reformulated the cover song identification problem\nin a machine learning framework. To do this, we first build the CNN using as an\ninput a cross-similarity matrix generated from a pair of songs. We then\nconstruct the data set composed of cover song pairs and non-cover song pairs,\nwhich are used as positive and negative training samples, respectively. The\ntrained CNN outputs the probability of being in the cover song relation given a\ncross-similarity matrix generated from any two pieces of music and identifies\nthe cover song by ranking on the probability. Experimental results show that\nthe proposed algorithm achieves performance better than or comparable to the\nstate-of-the-art.", "published": "2017-12-01 02:45:46", "link": "http://arxiv.org/abs/1712.00166v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker identification from the sound of the human breath", "abstract": "This paper examines the speaker identification potential of breath sounds in\ncontinuous speech. Speech is largely produced during exhalation. In order to\nreplenish air in the lungs, speakers must periodically inhale. When inhalation\noccurs in the midst of continuous speech, it is generally through the mouth.\nIntra-speech breathing behavior has been the subject of much study, including\nthe patterns, cadence, and variations in energy levels. However, an often\nignored characteristic is the {\\em sound} produced during the inhalation phase\nof this cycle. Intra-speech inhalation is rapid and energetic, performed with\nopen mouth and glottis, effectively exposing the entire vocal tract to enable\nmaximum intake of air. This results in vocal tract resonances evoked by\nturbulence that are characteristic of the speaker's speech-producing apparatus.\nConsequently, the sounds of inhalation are expected to carry information about\nthe speaker's identity. Moreover, unlike other spoken sounds which are subject\nto active control, inhalation sounds are generally more natural and less\naffected by voluntary influences. The goal of this paper is to demonstrate that\nbreath sounds are indeed bio-signatures that can be used to identify speakers.\nWe show that these sounds by themselves can yield remarkably accurate speaker\nrecognition with appropriate feature representations and classification\nframeworks.", "published": "2017-12-01 03:16:23", "link": "http://arxiv.org/abs/1712.00171v2", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Utilizing Domain Knowledge in End-to-End Audio Processing", "abstract": "End-to-end neural network based approaches to audio modelling are generally\noutperformed by models trained on high-level data representations. In this\npaper we present preliminary work that shows the feasibility of training the\nfirst layers of a deep convolutional neural network (CNN) model to learn the\ncommonly-used log-scaled mel-spectrogram transformation. Secondly, we\ndemonstrate that upon initializing the first layers of an end-to-end CNN\nclassifier with the learned transformation, convergence and performance on the\nESC-50 environmental sound classification dataset are similar to a CNN-based\nmodel trained on the highly pre-processed log-scaled mel-spectrogram features.", "published": "2017-12-01 09:49:21", "link": "http://arxiv.org/abs/1712.00254v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Wavenet based low rate speech coding", "abstract": "Traditional parametric coding of speech facilitates low rate but provides\npoor reconstruction quality because of the inadequacy of the model used. We\ndescribe how a WaveNet generative speech model can be used to generate high\nquality speech from the bit stream of a standard parametric coder operating at\n2.4 kb/s. We compare this parametric coder with a waveform coder based on the\nsame generative model and show that approximating the signal waveform incurs a\nlarge rate penalty. Our experiments confirm the high performance of the WaveNet\nbased coder and show that the speech produced by the system is able to\nadditionally perform implicit bandwidth extension and does not significantly\nimpair recognition of the original speaker for the human listener, even when\nthat speaker has not been used during the training of the generative model.", "published": "2017-12-01 09:45:30", "link": "http://arxiv.org/abs/1712.01120v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
