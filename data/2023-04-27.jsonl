{"title": "Controllable Data Augmentation for Context-Dependent Text-to-SQL", "abstract": "The limited scale of annotated data constraints existing context-dependent\ntext-to-SQL models because of the complexity of labeling. The data augmentation\nmethod is a commonly used method to solve this problem. However, the data\ngenerated by current augmentation methods often lack diversity. In this paper,\nwe introduce ConDA, which generates interactive questions and corresponding SQL\nresults. We designed the SQL dialogue state to enhance the data diversity\nthrough the state transition. Meanwhile, we also present a filter method to\nensure the data quality by a grounding model. Additionally, we utilize a\ngrounding model to identify and filter low-quality questions that mismatch the\nstate information. Experimental results on the SParC and CoSQL datasets show\nthat ConDA boosts the baseline model to achieve an average improvement of\n$3.3\\%$ on complex questions. Moreover, we analyze the augmented data, which\nreveals that the data generated by ConDA are of high quality in both SQL\ntemplate hardness and types, turns, and question consistency.", "published": "2023-04-27 01:00:10", "link": "http://arxiv.org/abs/2304.13902v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Evaluation of POS Taggers: From Wall Street Journal to\n  Fandom Wiki", "abstract": "The Wall Street Journal section of the Penn Treebank has been the de-facto\nstandard for evaluating POS taggers for a long time, and accuracies over 97\\%\nhave been reported. However, less is known about out-of-domain tagger\nperformance, especially with fine-grained label sets. Using data from Elder\nScrolls Fandom, a wiki about the \\textit{Elder Scrolls} video game universe, we\ncreate a modest dataset for qualitatively evaluating the cross-domain\nperformance of two POS taggers: the Stanford tagger (Toutanova et al. 2003) and\nBilty (Plank et al. 2016), both trained on WSJ. Our analyses show that\nperformance on tokens seen during training is almost as good as in-domain\nperformance, but accuracy on unknown tokens decreases from 90.37% to 78.37%\n(Stanford) and 87.84\\% to 80.41\\% (Bilty) across domains. Both taggers struggle\nwith proper nouns and inconsistent capitalization.", "published": "2023-04-27 07:24:29", "link": "http://arxiv.org/abs/2304.13989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SweCTRL-Mini: a data-transparent Transformer-based large language model\n  for controllable text generation in Swedish", "abstract": "We present SweCTRL-Mini, a large Swedish language model that can be used for\ninference and fine-tuning on a single consumer-grade GPU. The model is based on\nthe CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019),\nwhich means that users of the SweCTRL-Mini model can control the genre of the\ngenerated text by inserting special tokens in the generation prompts.\nSweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a\nset of Swedish novels. In this article, we provide (1) a detailed account of\nthe utilized training data and text pre-processing steps, to the extent that it\nis possible to check whether a specific phrase/source was a part of the\ntraining data, and (2) an evaluation of the model on both discriminative tasks,\nusing automatic evaluation methods, and generative tasks, using human referees.\nWe also compare the generative capabilities of the model with those of GPT-3.\nSweCTRL-Mini is fully open and available for download.", "published": "2023-04-27 07:32:37", "link": "http://arxiv.org/abs/2304.13994v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Origin Tracing and Detecting of LLMs", "abstract": "The extraordinary performance of large language models (LLMs) heightens the\nimportance of detecting whether the context is generated by an AI system. More\nimportantly, while more and more companies and institutions release their LLMs,\nthe origin can be hard to trace. Since LLMs are heading towards the time of\nAGI, similar to the origin tracing in anthropology, it is of great importance\nto trace the origin of LLMs. In this paper, we first raise the concern of the\norigin tracing of LLMs and propose an effective method to trace and detect\nAI-generated contexts. We introduce a novel algorithm that leverages the\ncontrastive features between LLMs and extracts model-wise features to trace the\ntext origins. Our proposed method works under both white-box and black-box\nsettings therefore can be widely generalized to detect various LLMs.(e.g. can\nbe generalized to detect GPT-3 models without the GPT-3 models). Also, our\nproposed method requires only limited data compared with the supervised\nlearning methods and can be extended to trace new-coming model origins. We\nconstruct extensive experiments to examine whether we can trace the origins of\ngiven texts. We provide valuable observations based on the experimental\nresults, such as the difficulty level of AI origin tracing, and the AI origin\nsimilarities, and call for ethical concerns of LLM providers. We are releasing\nall codes and data as a toolkit and benchmark for future AI origin tracing and\ndetecting studies. \\footnote{We are releasing all available resource at\n\\url{https://github.com/OpenLMLab/}.}", "published": "2023-04-27 10:05:57", "link": "http://arxiv.org/abs/2304.14072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NAP at SemEval-2023 Task 3: Is Less Really More? (Back-)Translation as\n  Data Augmentation Strategies for Detecting Persuasion Techniques", "abstract": "Persuasion techniques detection in news in a multi-lingual setup is\nnon-trivial and comes with challenges, including little training data. Our\nsystem successfully leverages (back-)translation as data augmentation\nstrategies with multi-lingual transformer models for the task of detecting\npersuasion techniques. The automatic and human evaluation of our augmented data\nallows us to explore whether (back-)translation aid or hinder performance. Our\nin-depth analyses indicate that both data augmentation strategies boost\nperformance; however, balancing human-produced and machine-generated data seems\nto be crucial.", "published": "2023-04-27 13:33:08", "link": "http://arxiv.org/abs/2304.14179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UIO at SemEval-2023 Task 12: Multilingual fine-tuning for sentiment\n  classification in low-resource languages", "abstract": "Our contribution to the 2023 AfriSenti-SemEval shared task 12: Sentiment\nAnalysis for African Languages, provides insight into how a multilingual large\nlanguage model can be a resource for sentiment analysis in languages not seen\nduring pretraining. The shared task provides datasets of a variety of African\nlanguages from different language families. The languages are to various\ndegrees related to languages used during pretraining, and the language data\ncontain various degrees of code-switching. We experiment with both monolingual\nand multilingual datasets for the final fine-tuning, and find that with the\nprovided datasets that contain samples in the thousands, monolingual\nfine-tuning yields the best results.", "published": "2023-04-27 13:51:18", "link": "http://arxiv.org/abs/2304.14189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Modular Approach for Multilingual Timex Detection and Normalization\n  using Deep Learning and Grammar-based methods", "abstract": "Detecting and normalizing temporal expressions is an essential step for many\nNLP tasks. While a variety of methods have been proposed for detection, best\nnormalization approaches rely on hand-crafted rules. Furthermore, most of them\nhave been designed only for English. In this paper we present a modular\nmultilingual temporal processing system combining a fine-tuned Masked Language\nModel for detection, and a grammar-based normalizer. We experiment in Spanish\nand English and compare with HeidelTime, the state-of-the-art in multilingual\ntemporal processing. We obtain best results in gold timex normalization, timex\ndetection and type recognition, and competitive performance in the combined\nTempEval-3 relaxed value metric. A detailed error analysis shows that detecting\nonly those timexes for which it is feasible to provide a normalization is\nhighly beneficial in this last metric. This raises the question of which is the\nbest strategy for timex processing, namely, leaving undetected those timexes\nfor which is not easy to provide normalization rules or aiming for high\ncoverage.", "published": "2023-04-27 14:32:23", "link": "http://arxiv.org/abs/2304.14221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who", "abstract": "Automated fact-checking is often presented as an epistemic tool that\nfact-checkers, social media consumers, and other stakeholders can use to fight\nmisinformation. Nevertheless, few papers thoroughly discuss how. We document\nthis by analysing 100 highly-cited papers, and annotating epistemic elements\nrelated to intended use, i.e., means, ends, and stakeholders. We find that\nnarratives leaving out some of these aspects are common, that many papers\npropose inconsistent means and ends, and that the feasibility of suggested\nstrategies rarely has empirical backing. We argue that this vagueness actively\nhinders the technology from reaching its goals, as it encourages overclaiming,\nlimits criticism, and prevents stakeholder feedback. Accordingly, we provide\nseveral recommendations for thinking and writing about the use of fact-checking\nartefacts.", "published": "2023-04-27 14:55:23", "link": "http://arxiv.org/abs/2304.14238v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-Level Sentiment Analysis (ELSA): An exploratory task survey", "abstract": "This paper explores the task of identifying the overall sentiment expressed\ntowards volitional entities (persons and organizations) in a document -- what\nwe refer to as Entity-Level Sentiment Analysis (ELSA). While identifying\nsentiment conveyed towards an entity is well researched for shorter texts like\ntweets, we find little to no research on this specific task for longer texts\nwith multiple mentions and opinions towards the same entity. This lack of\nresearch would be understandable if ELSA can be derived from existing tasks and\nmodels. To assess this, we annotate a set of professional reviews for their\noverall sentiment towards each volitional entity in the text. We sample from\ndata already annotated for document-level, sentence-level, and target-level\nsentiment in a multi-domain review corpus, and our results indicate that there\nis no single proxy task that provides this overall sentiment we seek for the\nentities at a satisfactory level of performance. We present a suite of\nexperiments aiming to assess the contribution towards ELSA provided by\ndocument-, sentence-, and target-level sentiment analysis, and provide a\ndiscussion of their shortcomings. We show that sentiment in our dataset is\nexpressed not only with an entity mention as target, but also towards targets\nwith a sentiment-relevant relation to a volitional entity. In our data, these\nrelations extend beyond anaphoric coreference resolution, and our findings call\nfor further research of the topic. Finally, we also present a survey of\nprevious relevant work.", "published": "2023-04-27 15:01:20", "link": "http://arxiv.org/abs/2304.14241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Frame Induction with Deep Metric Learning", "abstract": "Recent studies have demonstrated the usefulness of contextualized word\nembeddings in unsupervised semantic frame induction. However, they have also\nrevealed that generic contextualized embeddings are not always consistent with\nhuman intuitions about semantic frames, which causes unsatisfactory performance\nfor frame induction based on contextualized embeddings. In this paper, we\naddress supervised semantic frame induction, which assumes the existence of\nframe-annotated data for a subset of predicates in a corpus and aims to build a\nframe induction model that leverages the annotated data. We propose a model\nthat uses deep metric learning to fine-tune a contextualized embedding model,\nand we apply the fine-tuned contextualized embeddings to perform semantic frame\ninduction. Our experiments on FrameNet show that fine-tuning with deep metric\nlearning considerably improves the clustering evaluation scores, namely, the\nB-cubed F-score and Purity F-score, by about 8 points or more. We also\ndemonstrate that our approach is effective even when the number of training\ninstances is small.", "published": "2023-04-27 15:46:09", "link": "http://arxiv.org/abs/2304.14286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "q2d: Turning Questions into Dialogs to Teach Models How to Search", "abstract": "One of the exciting capabilities of recent language models for dialog is\ntheir ability to independently search for relevant information to ground a\ngiven dialog response. However, obtaining training data to teach models how to\nissue search queries is time and resource consuming. In this work, we propose\nq2d: an automatic data generation pipeline that generates information-seeking\ndialogs from questions. We prompt a large language model (PaLM) to create\nconversational versions of question answering datasets, and use it to improve\nquery generation models that communicate with external search APIs to ground\ndialog responses. Unlike previous approaches which relied on human written\ndialogs with search queries, our method allows to automatically generate\nquery-based grounded dialogs with better control and scale. Our experiments\ndemonstrate that: (1) For query generation on the QReCC dataset, models trained\non our synthetically-generated data achieve 90%--97% of the performance of\nmodels trained on the human-generated data; (2) We can successfully generate\ndata for training dialog models in new domains without any existing dialog data\nas demonstrated on the multi-hop MuSiQue and Bamboogle QA datasets. (3) We\nperform a thorough analysis of the generated dialogs showing that humans find\nthem of high quality and struggle to distinguish them from human-written\ndialogs.", "published": "2023-04-27 16:39:15", "link": "http://arxiv.org/abs/2304.14318v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Industrial Engineering with Large Language Models: A case study of\n  ChatGPT's performance on Oil & Gas problems", "abstract": "Large Language Models (LLMs) have shown great potential in solving complex\nproblems in various fields, including oil and gas engineering and other\nindustrial engineering disciplines like factory automation, PLC programming\netc. However, automatic identification of strong and weak solutions to\nfundamental physics equations governing several industrial processes remain a\nchallenging task. This paper identifies the limitation of current LLM\napproaches, particularly ChatGPT in selected practical problems native to oil\nand gas engineering but not exclusively. The performance of ChatGPT in solving\ncomplex problems in oil and gas engineering is discussed and the areas where\nLLMs are most effective are presented.", "published": "2023-04-27 17:33:49", "link": "http://arxiv.org/abs/2304.14354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "We're Afraid Language Models Aren't Modeling Ambiguity", "abstract": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is\na key part of human language understanding, allowing us to anticipate\nmisunderstanding as communicators and revise our interpretations as listeners.\nAs language models (LMs) are increasingly employed as dialogue interfaces and\nwriting aids, handling ambiguous language is critical to their success. We\ncharacterize ambiguity in a sentence by its effect on entailment relations with\nanother sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645\nexamples with diverse kinds of ambiguity. We design a suite of tests based on\nAmbiEnt, presenting the first evaluation of pretrained LMs to recognize\nambiguity and disentangle possible meanings. We find that the task remains\nextremely challenging, including for GPT-4, whose generated disambiguations are\nconsidered correct only 32% of the time in human evaluation, compared to 90%\nfor disambiguations in our dataset. Finally, to illustrate the value of\nambiguity-sensitive tools, we show that a multilabel NLI model can flag\npolitical claims in the wild that are misleading due to ambiguity. We encourage\nthe field to rediscover the importance of ambiguity for NLP.", "published": "2023-04-27 17:57:58", "link": "http://arxiv.org/abs/2304.14399v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale\n  Instructions", "abstract": "Large language models (LLMs) with instruction fine-tuning demonstrate\nsuperior generative capabilities. However, these models are resource-intensive.\nTo alleviate this issue, we explore distilling knowledge from instruction-tuned\nLLMs into much smaller ones. To this end, we carefully develop a large set of\n2.58M instructions based on both existing and newly-generated instructions. In\naddition to being sizable, we design our instructions to cover a broad set of\ntopics to ensure diversity. Extensive analysis of our instruction dataset\nconfirms its diversity, and we generate responses for these instructions using\ngpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of\nmodels, collectively referred to as LaMini-LM, which includes models from both\nthe encoder-decoder and decoder-only families, with varying sizes. We evaluate\nthe performance of our models using automatic metrics on 15 different natural\nlanguage processing (NLP) benchmarks, as well as through human assessment. The\nresults demonstrate that our proposed LaMini-LM models are comparable to\ncompetitive baselines, while being much smaller in size.", "published": "2023-04-27 17:58:49", "link": "http://arxiv.org/abs/2304.14402v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ViMQ: A Vietnamese Medical Question Dataset for Healthcare Dialogue\n  System Development", "abstract": "Existing medical text datasets usually take the form of question and answer\npairs that support the task of natural language generation, but lacking the\ncomposite annotations of the medical terms. In this study, we publish a\nVietnamese dataset of medical questions from patients with sentence-level and\nentity-level annotations for the Intent Classification and Named Entity\nRecognition tasks. The tag sets for two tasks are in medical domain and can\nfacilitate the development of task-oriented healthcare chatbots with better\ncomprehension of queries from patients. We train baseline models for the two\ntasks and propose a simple self-supervised training strategy with span-noise\nmodelling that substantially improves the performance. Dataset and code will be\npublished at https://github.com/tadeephuy/ViMQ", "published": "2023-04-27 17:59:53", "link": "http://arxiv.org/abs/2304.14405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Vietnamese Legal Questions Using Deep Neural Networks with\n  Biaffine Classifiers", "abstract": "In this paper, we propose using deep neural networks to extract important\ninformation from Vietnamese legal questions, a fundamental task towards\nbuilding a question answering system in the legal domain. Given a legal\nquestion in natural language, the goal is to extract all the segments that\ncontain the needed information to answer the question. We introduce a deep\nmodel that solves the task in three stages. First, our model leverages recent\nadvanced autoencoding language models to produce contextual word embeddings,\nwhich are then combined with character-level and POS-tag information to form\nword representations. Next, bidirectional long short-term memory networks are\nemployed to capture the relations among words and generate sentence-level\nrepresentations. At the third stage, borrowing ideas from graph-based\ndependency parsing methods which provide a global view on the input sentence,\nwe use biaffine classifiers to estimate the probability of each pair of\nstart-end words to be an important segment. Experimental results on a public\nVietnamese legal dataset show that our model outperforms the previous work by a\nlarge margin, achieving 94.79% in the F1 score. The results also prove the\neffectiveness of using contextual features extracted from pre-trained language\nmodels combined with other types of features such as character-level and\nPOS-tag features when training on a limited dataset.", "published": "2023-04-27 18:19:24", "link": "http://arxiv.org/abs/2304.14447v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PMC-LLaMA: Towards Building Open-source Language Models for Medicine", "abstract": "Recently, Large Language Models (LLMs) have showcased remarkable capabilities\nin natural language understanding. While demonstrating proficiency in everyday\nconversations and question-answering situations, these models frequently\nstruggle in domains that require precision, such as medical applications, due\nto their lack of domain-specific knowledge. In this paper, we describe the\nprocedure for building a powerful, open-source language model specifically\ndesigned for medicine applications, termed as PMC-LLaMA. Our contributions are\nthreefold: (i) we systematically investigate the process of adapting a\ngeneral-purpose foundation language model towards medical domain, this involves\ndata-centric knowledge injection through the integration of 4.8M biomedical\nacademic papers and 30K medical textbooks, as well as comprehensive fine-tuning\nfor alignment with domain-specific instructions; (ii) we contribute a\nlarge-scale, comprehensive dataset for instruction tuning. This dataset\nencompasses medical question-answering (QA), rationale for reasoning, and\nconversational dialogues, comprising a total of 202M tokens; (iii) we conduct\nthorough ablation studies to demonstrate the effectiveness of each proposed\ncomponent. While evaluating on various public medical question-answering\nbenchmarks, our lightweight PMCLLaMA, which consists of only 13 billion\nparameters, exhibits superior performance, even surpassing ChatGPT. All models,\ncodes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA.", "published": "2023-04-27 18:29:05", "link": "http://arxiv.org/abs/2304.14454v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visual Referential Games Further the Emergence of Disentangled\n  Representations", "abstract": "Natural languages are powerful tools wielded by human beings to communicate\ninformation. Among their desirable properties, compositionality has been the\nmain focus in the context of referential games and variants, as it promises to\nenable greater systematicity to the agents which would wield it. The concept of\ndisentanglement has been shown to be of paramount importance to learned\nrepresentations that generalise well in deep learning, and is thought to be a\nnecessary condition to enable systematicity. Thus, this paper investigates how\ndo compositionality at the level of the emerging languages, disentanglement at\nthe level of the learned representations, and systematicity relate to each\nother in the context of visual referential games. Firstly, we find that visual\nreferential games that are based on the Obverter architecture outperforms\nstate-of-the-art unsupervised learning approach in terms of many major\ndisentanglement metrics. Secondly, we expand the previously proposed Positional\nDisentanglement (PosDis) metric for compositionality to (re-)incorporate some\nconcerns pertaining to informativeness and completeness features found in the\nMutual Information Gap (MIG) disentanglement metric it stems from. This\nextension allows for further discrimination between the different kind of\ncompositional languages that emerge in the context of Obverter-based\nreferential games, in a way that neither the referential game accuracy nor\nprevious metrics were able to capture. Finally we investigate whether the\nresulting (emergent) systematicity, as measured by zero-shot compositional\nlearning tests, correlates with any of the disentanglement and compositionality\nmetrics proposed so far. Throughout the training process, statically\nsignificant correlation coefficients can be found both positive and negative\ndepending on the moment of the measure.", "published": "2023-04-27 20:00:51", "link": "http://arxiv.org/abs/2304.14511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse over Discourse: The Need for an Expanded Pragmatic Focus in\n  Conversational AI", "abstract": "The summarization of conversation, that is, discourse over discourse,\nelevates pragmatic considerations as a pervasive limitation of both\nsummarization and other applications of contemporary conversational AI.\nBuilding on impressive progress in both semantics and syntax, pragmatics\nconcerns meaning in the practical sense. In this paper, we discuss several\nchallenges in both summarization of conversations and other conversational AI\napplications, drawing on relevant theoretical work. We illustrate the\nimportance of pragmatics with so-called star sentences, syntactically\nacceptable propositions that are pragmatically inappropriate in conversation or\nits summary. Because the baseline for quality of AI is indistinguishability\nfrom human behavior, we draw heavily on the psycho-linguistics literature, and\nlabel our complaints as \"Turing Test Triggers\" (TTTs). We discuss implications\nfor the design and evaluation of conversation summarization methods and\nconversational AI applications like voice assistants and chatbots", "published": "2023-04-27 21:51:42", "link": "http://arxiv.org/abs/2304.14543v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Keyphrase Generation: Analysis and Evaluation", "abstract": "Keyphrase generation aims at generating topical phrases from a given text\neither by copying from the original text (present keyphrases) or by producing\nnew keyphrases (absent keyphrases) that capture the semantic meaning of the\ntext. Encoder-decoder models are most widely used for this task because of\ntheir capabilities for absent keyphrase generation. However, there has been\nlittle to no analysis on the performance and behavior of such models for\nkeyphrase generation. In this paper, we study various tendencies exhibited by\nthree strong models: T5 (based on a pre-trained transformer),\nCatSeq-Transformer (a non-pretrained Transformer), and ExHiRD (based on a\nrecurrent neural network). We analyze prediction confidence scores, model\ncalibration, and the effect of token position on keyphrases generation.\nMoreover, we motivate and propose a novel metric framework, SoftKeyScore, to\nevaluate the similarity between two sets of keyphrases by using softscores to\naccount for partial matching and semantic similarity. We find that SoftKeyScore\nis more suitable than the standard F1 metric for evaluating two sets of given\nkeyphrases.", "published": "2023-04-27 00:10:21", "link": "http://arxiv.org/abs/2304.13883v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ChatLog: Carefully Evaluating the Evolution of ChatGPT Across Time", "abstract": "ChatGPT has achieved great success and can be considered to have acquired an\ninfrastructural status. There are abundant works for evaluating ChatGPT on\nbenchmarks. However, existing benchmarks encounter two challenges: (1)\nDisregard for periodical evaluation and (2) Lack of fine-grained features. In\nthis paper, we construct ChatLog, an ever-updating dataset with large-scale\nrecords of diverse long-form ChatGPT responses for 21 NLP benchmarks from\nMarch, 2023 to now. We conduct a comprehensive performance evaluation to find\nthat most capabilities of ChatGPT improve over time except for some abilities,\nand there exists a step-wise evolving pattern of ChatGPT. We further analyze\nthe inherent characteristics of ChatGPT by extracting the knowledge and\nlinguistic features. We find some stable features that stay unchanged and apply\nthem on the detection of ChatGPT-generated texts to improve the robustness of\ncross-version detection. We will continuously maintain our project at\n\\url{https://github.com/THU-KEG/ChatLog/}.", "published": "2023-04-27 11:33:48", "link": "http://arxiv.org/abs/2304.14106v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT vs State-of-the-Art Models: A Benchmarking Study in Keyphrase\n  Generation Task", "abstract": "Transformer-based language models, including ChatGPT, have demonstrated\nexceptional performance in various natural language generation tasks. However,\nthere has been limited research evaluating ChatGPT's keyphrase generation\nability, which involves identifying informative phrases that accurately reflect\na document's content. This study seeks to address this gap by comparing\nChatGPT's keyphrase generation performance with state-of-the-art models, while\nalso testing its potential as a solution for two significant challenges in the\nfield: domain adaptation and keyphrase generation from long documents. We\nconducted experiments on six publicly available datasets from scientific\narticles and news domains, analyzing performance on both short and long\ndocuments. Our results show that ChatGPT outperforms current state-of-the-art\nmodels in all tested datasets and environments, generating high-quality\nkeyphrases that adapt well to diverse domains and document lengths.", "published": "2023-04-27 13:25:43", "link": "http://arxiv.org/abs/2304.14177v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Strong Zero-Shot Retriever", "abstract": "In this work, we propose a simple method that applies a large language model\n(LLM) to large-scale retrieval in zero-shot scenarios. Our method, the Language\nlanguage model as Retriever (LameR), is built upon no other neural models but\nan LLM, while breaking brute-force combinations of retrievers with LLMs and\nlifting the performance of zero-shot retrieval to be very competitive on\nbenchmark datasets. Essentially, we propose to augment a query with its\npotential answers by prompting LLMs with a composition of the query and the\nquery's in-domain candidates. The candidates, regardless of correct or wrong,\nare obtained by a vanilla retrieval procedure on the target collection. As a\npart of the prompts, they are likely to help LLM generate more precise answers\nby pattern imitation or candidate summarization. Even if all the candidates are\nwrong, the prompts at least make LLM aware of in-collection patterns and\ngenres. Moreover, due to the low performance of a self-supervised retriever,\nthe LLM-based query augmentation becomes less effective as the retriever\nbottlenecks the whole pipeline. Therefore, we propose to leverage a\nnon-parametric lexicon-based method (e.g., BM25) as the retrieval module to\ncapture query-document overlap in a literal fashion. As such, LameR makes the\nretrieval procedure transparent to the LLM, thus circumventing the performance\nbottleneck.", "published": "2023-04-27 14:45:55", "link": "http://arxiv.org/abs/2304.14233v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "string2string: A Modern Python Library for String-to-String Algorithms", "abstract": "We introduce string2string, an open-source library that offers a\ncomprehensive suite of efficient algorithms for a broad range of\nstring-to-string problems. It includes traditional algorithmic solutions as\nwell as recent advanced neural approaches to tackle various problems in string\nalignment, distance measurement, lexical and semantic search, and similarity\nanalysis -- along with several helpful visualization tools and metrics to\nfacilitate the interpretation and analysis of these methods. Notable algorithms\nfeatured in the library include the Smith-Waterman algorithm for pairwise local\nalignment, the Hirschberg algorithm for global alignment, the Wagner-Fisher\nalgorithm for edit distance, BARTScore and BERTScore for similarity analysis,\nthe Knuth-Morris-Pratt algorithm for lexical search, and Faiss for semantic\nsearch. Besides, it wraps existing efficient and widely-used implementations of\ncertain frameworks and metrics, such as sacreBLEU and ROUGE, whenever it is\nappropriate and suitable. Overall, the library aims to provide extensive\ncoverage and increased flexibility in comparison to existing libraries for\nstrings. It can be used for many downstream applications, tasks, and problems\nin natural-language processing, bioinformatics, and computational social\nsciences. It is implemented in Python, easily installable via pip, and\naccessible through a simple API. Source code, documentation, and tutorials are\nall available on our GitHub page: https://github.com/stanfordnlp/string2string.", "published": "2023-04-27 17:57:19", "link": "http://arxiv.org/abs/2304.14395v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Framing the News:From Human Perception to Large Language Model\n  Inferences", "abstract": "Identifying the frames of news is important to understand the articles'\nvision, intention, message to be conveyed, and which aspects of the news are\nemphasized. Framing is a widely studied concept in journalism, and has emerged\nas a new topic in computing, with the potential to automate processes and\nfacilitate the work of journalism professionals. In this paper, we study this\nissue with articles related to the Covid-19 anti-vaccine movement. First, to\nunderstand the perspectives used to treat this theme, we developed a protocol\nfor human labeling of frames for 1786 headlines of No-Vax movement articles of\nEuropean newspapers from 5 countries. Headlines are key units in the written\npress, and worth of analysis as many people only read headlines (or use them to\nguide their decision for further reading.) Second, considering advances in\nNatural Language Processing (NLP) with large language models, we investigated\ntwo approaches for frame inference of news headlines: first with a GPT-3.5\nfine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work\ncontributes to the study and analysis of the performance that these models have\nto facilitate journalistic tasks like classification of frames, while\nunderstanding whether the models are able to replicate human perception in the\nidentification of these frames.", "published": "2023-04-27 18:30:18", "link": "http://arxiv.org/abs/2304.14456v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Retrieval-based Knowledge Augmented Vision Language Pre-training", "abstract": "With the recent progress in large-scale vision and language representation\nlearning, Vision Language Pre-training (VLP) models have achieved promising\nimprovements on various multi-modal downstream tasks. Albeit powerful, these\nmodels have not fully leveraged world knowledge to their advantage. A key\nchallenge of knowledge-augmented VLP is the lack of clear connections between\nknowledge and multi-modal data. Moreover, not all knowledge present in\nimages/texts is useful, therefore prior approaches often struggle to\neffectively integrate knowledge, visual, and textual information. In this\nstudy, we propose REtrieval-based knowledge Augmented Vision Language (REAVL),\na novel knowledge-augmented pre-training framework to address the above issues.\nFor the first time, we introduce a knowledge-aware self-supervised learning\nscheme that efficiently establishes the correspondence between knowledge and\nmulti-modal data and identifies informative knowledge to improve the modeling\nof alignment and interactions between visual and textual modalities. By\nadaptively integrating informative knowledge with visual and textual\ninformation, REAVL achieves new state-of-the-art performance uniformly on\nknowledge-based vision-language understanding and multi-modal entity linking\ntasks, as well as competitive results on general vision-language tasks while\nonly using 0.2% pre-training data of the best models. Our model shows strong\nsample efficiency and effective knowledge utilization.", "published": "2023-04-27 02:23:47", "link": "http://arxiv.org/abs/2304.13923v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Learning and Reasoning Multifaceted and Longitudinal Data for Poverty\n  Estimates and Livelihood Capabilities of Lagged Regions in Rural India", "abstract": "Poverty is a multifaceted phenomenon linked to the lack of capabilities of\nhouseholds to earn a sustainable livelihood, increasingly being assessed using\nmultidimensional indicators. Its spatial pattern depends on social, economic,\npolitical, and regional variables. Artificial intelligence has shown immense\nscope in analyzing the complexities and nuances of poverty. The proposed\nproject aims to examine the poverty situation of rural India for the period of\n1990-2022 based on the quality of life and livelihood indicators. The districts\nwill be classified into `advanced', `catching up', `falling behind', and\n`lagged' regions. The project proposes to integrate multiple data sources,\nincluding conventional national-level large sample household surveys, census\nsurveys, and proxy variables like daytime, and nighttime data from satellite\nimages, and communication networks, to name a few, to provide a comprehensive\nview of poverty at the district level. The project also intends to examine\ncausation and longitudinal analysis to examine the reasons for poverty. Poverty\nand inequality could be widening in developing countries due to demographic and\ngrowth-agglomerating policies. Therefore, targeting the lagging regions and the\nvulnerable population is essential to eradicate poverty and improve the quality\nof life to achieve the goal of `zero poverty'. Thus, the study also focuses on\nthe districts with a higher share of the marginal section of the population\ncompared to the national average to trace the performance of development\nindicators and their association with poverty in these regions.", "published": "2023-04-27 05:33:08", "link": "http://arxiv.org/abs/2304.13958v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning Human-Human Interactions in Images from Weak Textual\n  Supervision", "abstract": "Interactions between humans are diverse and context-dependent, but previous\nworks have treated them as categorical, disregarding the heavy tail of possible\ninteractions. We propose a new paradigm of learning human-human interactions as\nfree text from a single still image, allowing for flexibility in modeling the\nunlimited space of situations and relationships between people. To overcome the\nabsence of data labelled specifically for this task, we use knowledge\ndistillation applied to synthetic caption data produced by a large language\nmodel without explicit supervision. We show that the pseudo-labels produced by\nthis procedure can be used to train a captioning model to effectively\nunderstand human-human interactions in images, as measured by a variety of\nmetrics that measure textual and semantic faithfulness and factual groundedness\nof our predictions. We further show that our approach outperforms SOTA image\ncaptioning and situation recognition models on this task. We will release our\ncode and pseudo-labels along with Waldo and Wenda, a manually-curated test set\nfor still image human-human interaction understanding.", "published": "2023-04-27 11:32:48", "link": "http://arxiv.org/abs/2304.14104v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DataComp: In search of the next generation of multimodal datasets", "abstract": "Multimodal datasets are a critical component in recent breakthroughs such as\nStable Diffusion and GPT-4, yet their design does not receive the same research\nattention as model architectures or training algorithms. To address this\nshortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset\nexperiments centered around a new candidate pool of 12.8 billion image-text\npairs from Common Crawl. Participants in our benchmark design new filtering\ntechniques or curate new data sources and then evaluate their new dataset by\nrunning our standardized CLIP training code and testing the resulting model on\n38 downstream test sets. Our benchmark consists of multiple compute scales\nspanning four orders of magnitude, which enables the study of scaling trends\nand makes the benchmark accessible to researchers with varying resources. Our\nbaseline experiments show that the DataComp workflow leads to better training\nsets. In particular, our best baseline, DataComp-1B, enables training a CLIP\nViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming\nOpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training\nprocedure and compute. We release DataComp and all accompanying code at\nwww.datacomp.ai.", "published": "2023-04-27 11:37:18", "link": "http://arxiv.org/abs/2304.14108v5", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality", "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.", "published": "2023-04-27 13:27:01", "link": "http://arxiv.org/abs/2304.14178v3", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controlled Text Generation with Natural Language Instructions", "abstract": "Large language models generate fluent texts and can follow natural language\ninstructions to solve a wide range of tasks without task-specific training.\nNevertheless, it is notoriously difficult to control their generation to\nsatisfy the various constraints required by different applications. In this\nwork, we present InstructCTG, a controlled text generation framework that\nincorporates different constraints by conditioning on natural language\ndescriptions and demonstrations of the constraints. In particular, we first\nextract the underlying constraints of natural texts through a combination of\noff-the-shelf NLP tools and simple heuristics. We then verbalize the\nconstraints into natural language instructions to form weakly supervised\ntraining data. By prepending natural language descriptions of the constraints\nand a few demonstrations, we fine-tune a pre-trained language model to\nincorporate various types of constraints. Compared to existing search-based or\nscore-based methods, InstructCTG is more flexible to different constraint types\nand has a much smaller impact on the generation quality and speed because it\ndoes not modify the decoding procedure. Additionally, InstructCTG allows the\nmodel to adapt to new constraints without re-training through the use of\nfew-shot task generalization and in-context learning abilities of\ninstruction-tuned language models.", "published": "2023-04-27 15:56:34", "link": "http://arxiv.org/abs/2304.14293v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ICE-Score: Instructing Large Language Models to Evaluate Code", "abstract": "Recent advancements in the field of natural language generation have\nfacilitated the use of large language models to assess the quality of generated\ntext. Although these models have shown promising results in tasks such as\nmachine translation and summarization, their applicability in code intelligence\ntasks remains limited without human involvement. The complexity of programming\nconcepts required for such tasks makes it difficult to develop evaluation\nmetrics that align with human judgment. Token-matching-based metrics, such as\nBLEU, have demonstrated weak correlations with human practitioners in code\nintelligence tasks. Moreover, utilizing human-written test suites to evaluate\nfunctional correctness can be challenging in domains with low resources. To\novercome these obstacles, we propose \\texttt{ICE-Score}, a new evaluation\nmetric via instructing large language models (LLMs) for code assessments. Our\nmetric addresses the limitations of existing approaches by achieving superior\ncorrelations with functional correctness and human preferences, without the\nneed for test oracles or references. We evaluate the efficacy of our metric on\ntwo different aspects (\\textit{human preference} and \\textit{execution\nsuccess}) and four programming languages. Our results demonstrate that our\nmetric surpasses state-of-the-art metrics for code generation, delivering high\nlevels of accuracy and consistency across various programming languages and\ntasks. We also make our evaluation metric and datasets available to the\npublic\\footnote{\\url{https://github.com/terryyz/ice-score}}, encouraging\nfurther research in evaluating code intelligence tasks.", "published": "2023-04-27 16:38:17", "link": "http://arxiv.org/abs/2304.14317v2", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Idioms, Probing and Dangerous Things: Towards Structural Probing for\n  Idiomaticity in Vector Space", "abstract": "The goal of this paper is to learn more about how idiomatic information is\nstructurally encoded in embeddings, using a structural probing method. We\nrepurpose an existing English verbal multi-word expression (MWE) dataset to\nsuit the probing framework and perform a comparative probing study of static\n(GloVe) and contextual (BERT) embeddings. Our experiments indicate that both\nencode some idiomatic information to varying degrees, but yield conflicting\nevidence as to whether idiomaticity is encoded in the vector norm, leaving this\nan open question. We also identify some limitations of the used dataset and\nhighlight important directions for future work in improving its suitability for\na probing analysis.", "published": "2023-04-27 17:06:20", "link": "http://arxiv.org/abs/2304.14333v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to\n  Guardrail Models for Virtual Assistants", "abstract": "A wave of new task-based virtual assistants has been fueled by increasingly\npowerful large language models (LLMs), such as GPT-4 (OpenAI, 2023). A major\nchallenge in deploying LLM-based virtual conversational assistants in real\nworld settings is ensuring they operate within what is admissible for the task.\nTo overcome this challenge, the designers of these virtual assistants rely on\nan independent guardrail system that verifies the virtual assistant's output\naligns with the constraints required for the task. However, relying on commonly\nused, prompt-based guardrails can be difficult to engineer correctly and\ncomprehensively. To address these challenges, we propose CONSCENDI. We use\nCONSCENDI to exhaustively generate training data with two key LLM-powered\ncomponents: scenario-augmented generation and contrastive training examples.\nWhen generating conversational data, we generate a set of rule-breaking\nscenarios, which enumerate a diverse set of high-level ways a rule can be\nviolated. This scenario-guided approach produces a diverse training set and\nprovides chatbot designers greater control. To generate contrastive examples,\nwe prompt the LLM to alter conversations with violations into acceptable\nconversations to enable fine-grained distinctions. We then use this data,\ngenerated by CONSCENDI, to train a smaller model. We find that CONSCENDI\nresults in guardrail models that improve over baselines in multiple dialogue\ndomains.", "published": "2023-04-27 17:39:11", "link": "http://arxiv.org/abs/2304.14364v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding Shared Speech-Text Representations", "abstract": "Recently, a number of approaches to train speech models by incorpo-rating\ntext into end-to-end models have been developed, with Mae-stro advancing\nstate-of-the-art automatic speech recognition (ASR)and Speech Translation (ST)\nperformance. In this paper, we expandour understanding of the resulting shared\nspeech-text representationswith two types of analyses. First we examine the\nlimits of speech-free domain adaptation, finding that a corpus-specific\nduration modelfor speech-text alignment is the most important component for\nlearn-ing a shared speech-text representation. Second, we inspect the\nsim-ilarities between activations of unimodal (speech or text) encodersas\ncompared to the activations of a shared encoder. We find that theshared encoder\nlearns a more compact and overlapping speech-textrepresentation than the\nuni-modal encoders. We hypothesize that thispartially explains the\neffectiveness of the Maestro shared speech-textrepresentations.", "published": "2023-04-27 20:05:36", "link": "http://arxiv.org/abs/2304.14514v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multivariate Representation Learning for Information Retrieval", "abstract": "Dense retrieval models use bi-encoder network architectures for learning\nquery and document representations. These representations are often in the form\nof a vector representation and their similarities are often computed using the\ndot product function. In this paper, we propose a new representation learning\nframework for dense retrieval. Instead of learning a vector for each query and\ndocument, our framework learns a multivariate distribution and uses negative\nmultivariate KL divergence to compute the similarity between distributions. For\nsimplicity and efficiency reasons, we assume that the distributions are\nmultivariate normals and then train large language models to produce mean and\nvariance vectors for these distributions. We provide a theoretical foundation\nfor the proposed framework and show that it can be seamlessly integrated into\nthe existing approximate nearest neighbor algorithms to perform retrieval\nefficiently. We conduct an extensive suite of experiments on a wide range of\ndatasets, and demonstrate significant improvements compared to competitive\ndense retrieval models.", "published": "2023-04-27 20:30:46", "link": "http://arxiv.org/abs/2304.14522v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Deep Transfer Learning for Automatic Speech Recognition: Towards Better\n  Generalization", "abstract": "Automatic speech recognition (ASR) has recently become an important challenge\nwhen using deep learning (DL). It requires large-scale training datasets and\nhigh computational and storage resources. Moreover, DL techniques and machine\nlearning (ML) approaches in general, hypothesize that training and testing data\ncome from the same domain, with the same input feature space and data\ndistribution characteristics. This assumption, however, is not applicable in\nsome real-world artificial intelligence (AI) applications. Moreover, there are\nsituations where gathering real data is challenging, expensive, or rarely\noccurring, which can not meet the data requirements of DL models. deep transfer\nlearning (DTL) has been introduced to overcome these issues, which helps\ndevelop high-performing models using real datasets that are small or slightly\ndifferent but related to the training data. This paper presents a comprehensive\nsurvey of DTL-based ASR frameworks to shed light on the latest developments and\nhelps academics and professionals understand current challenges. Specifically,\nafter presenting the DTL background, a well-designed taxonomy is adopted to\ninform the state-of-the-art. A critical analysis is then conducted to identify\nthe limitations and advantages of each framework. Moving on, a comparative\nstudy is introduced to highlight the current challenges before deriving\nopportunities for future research.", "published": "2023-04-27 21:08:05", "link": "http://arxiv.org/abs/2304.14535v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Appropriateness is all you need!", "abstract": "The strive to make AI applications \"safe\" has led to the development of\nsafety-measures as the main or even sole normative requirement of their\npermissible use. Similar can be attested to the latest version of chatbots,\nsuch as chatGPT. In this view, if they are \"safe\", they are supposed to be\npermissible to deploy. This approach, which we call \"safety-normativity\", is\nrather limited in solving the emerging issues that chatGPT and other chatbots\nhave caused thus far. In answering this limitation, in this paper we argue for\nlimiting chatbots in the range of topics they can chat about according to the\nnormative concept of appropriateness. We argue that rather than looking for\n\"safety\" in a chatbot's utterances to determine what they may and may not say,\nwe ought to assess those utterances according to three forms of\nappropriateness: technical-discursive, social, and moral. We then spell out\nwhat requirements for chatbots follow from these forms of appropriateness to\navoid the limits of previous accounts: positionality, acceptability, and value\nalignment (PAVA). With these in mind, we may be able to determine what a\nchatbot may and may not say. Lastly, one initial suggestion is to use challenge\nsets, specifically designed for appropriateness, as a validation method.", "published": "2023-04-27 22:21:52", "link": "http://arxiv.org/abs/2304.14553v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Understanding the Impact of Culture in Assessing Helpfulness of Online\n  Reviews", "abstract": "Online reviews have become essential for users to make informed decisions in\neveryday tasks ranging from planning summer vacations to purchasing groceries\nand making financial investments. A key problem in using online reviews is the\noverabundance of online that overwhelms the users. As a result, recommendation\nsystems for providing helpfulness of reviews are being developed. This paper\nargues that cultural background is an important feature that impacts the nature\nof a review written by the user, and must be considered as a feature in\nassessing the helpfulness of online reviews. The paper provides an in-depth\nstudy of differences in online reviews written by users from different cultural\nbackgrounds and how incorporating culture as a feature can lead to better\nreview helpfulness recommendations. In particular, we analyze online reviews\noriginating from two distinct cultural spheres, namely Arabic and Western\ncultures, for two different products, hotels and books. Our analysis\ndemonstrates that the nature of reviews written by users differs based on their\ncultural backgrounds and that this difference varies based on the specific\nproduct being reviewed. Finally, we have developed six different review\nhelpfulness recommendation models that demonstrate that taking culture into\naccount leads to better recommendations.", "published": "2023-04-27 07:20:55", "link": "http://arxiv.org/abs/2305.04836v1", "categories": ["cs.IR", "cs.CL", "cs.CY"], "primary_category": "cs.IR"}
{"title": "Energy-based Models are Zero-Shot Planners for Compositional Scene\n  Rearrangement", "abstract": "Language is compositional; an instruction can express multiple relation\nconstraints to hold among objects in a scene that a robot is tasked to\nrearrange. Our focus in this work is an instructable scene-rearranging\nframework that generalizes to longer instructions and to spatial concept\ncompositions never seen at training time. We propose to represent\nlanguage-instructed spatial concepts with energy functions over relative object\narrangements. A language parser maps instructions to corresponding energy\nfunctions and an open-vocabulary visual-language model grounds their arguments\nto relevant objects in the scene. We generate goal scene configurations by\ngradient descent on the sum of energy functions, one per language predicate in\nthe instruction. Local vision-based policies then re-locate objects to the\ninferred goal locations. We test our model on established instruction-guided\nmanipulation benchmarks, as well as benchmarks of compositional instructions we\nintroduce. We show our model can execute highly compositional instructions\nzero-shot in simulation and in the real world. It outperforms\nlanguage-to-action reactive policies and Large Language Model planners by a\nlarge margin, especially for long instructions that involve compositions of\nmultiple spatial concepts. Simulation and real-world robot execution videos, as\nwell as our code and datasets are publicly available on our website:\nhttps://ebmplanner.github.io.", "published": "2023-04-27 17:55:13", "link": "http://arxiv.org/abs/2304.14391v4", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "BactInt: A domain driven transfer learning approach and a corpus for\n  extracting inter-bacterial interactions from biomedical text", "abstract": "The community of different types of microbes present in a biological niche\nplays a very important role in functioning of the system. The crosstalk or\ninteractions among the different microbes contributes to the building blocks of\nsuch microbial community structures. Evidence reported in biomedical text\nserves as a reliable source for predicting such interactions. However, going\nthrough the vast and ever-increasing volume of biomedical literature is an\nintimidating and time consuming process. This necessitates development of\nautomated methods capable of accurately extracting bacterial relations reported\nin biomedical literature. In this paper, we introduce a method for automated\nextraction of microbial interactions (specifically between bacteria) from\nbiomedical literature along with ways of using transfer learning to improve its\naccuracy. We also describe a pipeline using which relations among specific\nbacteria groups can be mined. Additionally, we introduce the first publicly\navailable dataset which can be used to develop bacterial interaction extraction\nmethods.", "published": "2023-04-27 06:14:25", "link": "http://arxiv.org/abs/2305.07468v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "cs.IR"}
{"title": "XAI-based Comparison of Input Representations for Audio Event\n  Classification", "abstract": "Deep neural networks are a promising tool for Audio Event Classification. In\ncontrast to other data like natural images, there are many sensible and\nnon-obvious representations for audio data, which could serve as input to these\nmodels. Due to their black-box nature, the effect of different input\nrepresentations has so far mostly been investigated by measuring classification\nperformance. In this work, we leverage eXplainable AI (XAI), to understand the\nunderlying classification strategies of models trained on different input\nrepresentations. Specifically, we compare two model architectures with regard\nto relevant input features used for Audio Event Detection: one directly\nprocesses the signal as the raw waveform, and the other takes in its\ntime-frequency spectrogram representation. We show how relevance heatmaps\nobtained via \"Siren\"{Layer-wise Relevance Propagation} uncover\nrepresentation-dependent decision strategies. With these insights, we can make\na well-informed decision about the best input representation in terms of\nrobustness and representativity and confirm that the model's classification\nstrategies align with human requirements.", "published": "2023-04-27 08:30:07", "link": "http://arxiv.org/abs/2304.14019v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep sound-field denoiser: optically-measured sound-field denoising\n  using deep neural network", "abstract": "This paper proposes a deep sound-field denoiser, a deep neural network (DNN)\nbased denoising of optically measured sound-field images. Sound-field imaging\nusing optical methods has gained considerable attention due to its ability to\nachieve high-spatial-resolution imaging of acoustic phenomena that conventional\nacoustic sensors cannot accomplish. However, the optically measured sound-field\nimages are often heavily contaminated by noise because of the low sensitivity\nof optical interferometric measurements to airborne sound. Here, we propose a\nDNN-based sound-field denoising method. Time-varying sound-field image\nsequences are decomposed into harmonic complex-amplitude images by using a\ntime-directional Fourier transform. The complex images are converted into\ntwo-channel images consisting of real and imaginary parts and denoised by a\nnonlinear-activation-free network. The network is trained on a sound-field\ndataset obtained from numerical acoustic simulations with randomized\nparameters. We compared the method with conventional ones, such as image\nfilters, a spatiotemporal filter, and other DNN architectures, on numerical and\nexperimental data. The experimental data were measured by parallel\nphase-shifting interferometry and holographic speckle interferometry. The\nproposed deep sound-field denoiser significantly outperformed the conventional\nmethods on both the numerical and experimental data. Code is available on\nGitHub: https://github.com/nttcslab/deep-sound-field-denoiser.", "published": "2023-04-27 11:12:26", "link": "http://arxiv.org/abs/2304.14923v2", "categories": ["eess.SP", "cs.SD", "eess.AS", "eess.IV", "physics.optics"], "primary_category": "eess.SP"}
