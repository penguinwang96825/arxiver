{"title": "SgSum: Transforming Multi-document Summarization into Sub-graph\n  Selection", "abstract": "Most of existing extractive multi-document summarization (MDS) methods score\neach sentence individually and extract salient sentences one by one to compose\na summary, which have two main drawbacks: (1) neglecting both the intra and\ncross-document relations between sentences; (2) neglecting the coherence and\nconciseness of the whole summary. In this paper, we propose a novel MDS\nframework (SgSum) to formulate the MDS task as a sub-graph selection problem,\nin which source documents are regarded as a relation graph of sentences (e.g.,\nsimilarity graph or discourse graph) and the candidate summaries are its\nsub-graphs. Instead of selecting salient sentences, SgSum selects a salient\nsub-graph from the relation graph as the summary. Comparing with traditional\nmethods, our method has two main advantages: (1) the relations between\nsentences are captured by modeling both the graph structure of the whole\ndocument set and the candidate sub-graphs; (2) directly outputs an integrate\nsummary in the form of sub-graph which is more informative and coherent.\nExtensive experiments on MultiNews and DUC datasets show that our proposed\nmethod brings substantial improvements over several strong baselines. Human\nevaluation results also demonstrate that our model can produce significantly\nmore coherent and informative summaries compared with traditional MDS methods.\nMoreover, the proposed architecture has strong transfer ability from single to\nmulti-document input, which can reduce the resource bottleneck in MDS tasks.\nOur code and results are available at:\n\\url{https://github.com/PaddlePaddle/Research/tree/master/NLP/EMNLP2021-SgSum}.", "published": "2021-10-25 05:12:10", "link": "http://arxiv.org/abs/2110.12645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response\n  Selection", "abstract": "Dialogue disentanglement aims to group utterances in a long and\nmulti-participant dialogue into threads. This is useful for discourse analysis\nand downstream applications such as dialogue response selection, where it can\nbe the first step to construct a clean context/response set. Unfortunately,\nlabeling all~\\emph{reply-to} links takes quadratic effort w.r.t the number of\nutterances: an annotator must check all preceding utterances to identify the\none to which the current utterance is a reply. In this paper, we are the first\nto propose a~\\textbf{zero-shot} dialogue disentanglement solution. Firstly, we\ntrain a model on a multi-participant response selection dataset harvested from\nthe web which is not annotated; we then apply the trained model to perform\nzero-shot dialogue disentanglement. Without any labeled data, our model can\nachieve a cluster F1 score of 25. We also fine-tune the model using various\namounts of labeled data. Experiments show that with only 10\\% of the data, we\nachieve nearly the same performance of using the full dataset\\footnote{Code is\nreleased at\n\\url{https://github.com/chijames/zero_shot_dialogue_disentanglement}}.", "published": "2021-10-25 05:15:01", "link": "http://arxiv.org/abs/2110.12646v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Embedded Knowledge Graph Multi-hop Question Answering by\n  introducing Relational Chain Reasoning", "abstract": "Knowledge Graph Question Answering (KGQA) aims to answer user-questions from\na knowledge graph (KG) by identifying the reasoning relations between topic\nentity and answer. As a complex branch task of KGQA, multi-hop KGQA requires\nreasoning over the multi-hop relational chain preserved in KG to arrive at the\nright answer. Despite recent successes, the existing works on answering\nmulti-hop complex questions still face the following challenges: i) The absence\nof an explicit relational chain order reflected in user-question stems from a\nmisunderstanding of a user's intentions. ii) Incorrectly capturing relational\ntypes on weak supervision of which dataset lacks intermediate reasoning chain\nannotations due to expensive labeling cost. iii) Failing to consider implicit\nrelations between the topic entity and the answer implied in structured KG\nbecause of limited neighborhoods size constraint in subgraph retrieval-based\nalgorithms.To address these issues in multi-hop KGQA, we propose a novel model\nherein, namely Relational Chain based Embedded KGQA (Rce-KGQA), which\nsimultaneously utilizes the explicit relational chain revealed in natural\nlanguage question and the implicit relational chain stored in structured KG.\nOur extensive empirical study on three open-domain benchmarks proves that our\nmethod significantly outperforms the state-of-the-art counterparts like\nGraftNet, PullNet and EmbedKGQA. Comprehensive ablation experiments also verify\nthe effectiveness of our method on the multi-hop KGQA task. We have made our\nmodel's source code available at github:\nhttps://github.com/albert-jin/Rce-KGQA.", "published": "2021-10-25 06:53:02", "link": "http://arxiv.org/abs/2110.12679v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning for Neural Topic Model", "abstract": "Recent empirical studies show that adversarial topic models (ATM) can\nsuccessfully capture semantic patterns of the document by differentiating a\ndocument with another dissimilar sample. However, utilizing that\ndiscriminative-generative architecture has two important drawbacks: (1) the\narchitecture does not relate similar documents, which has the same\ndocument-word distribution of salient words; (2) it restricts the ability to\nintegrate external information, such as sentiments of the document, which has\nbeen shown to benefit the training of neural topic model. To address those\nissues, we revisit the adversarial topic architecture in the viewpoint of\nmathematical analysis, propose a novel approach to re-formulate discriminative\ngoal as an optimization problem, and design a novel sampling method which\nfacilitates the integration of external variables. The reformulation encourages\nthe model to incorporate the relations among similar samples and enforces the\nconstraint on the similarity among dissimilar ones; while the sampling method,\nwhich is based on the internal input and reconstructed output, helps inform the\nmodel of salient words contributing to the main topic. Experimental results\nshow that our framework outperforms other state-of-the-art neural topic models\nin three common benchmark datasets that belong to various domains, vocabulary\nsizes, and document lengths in terms of topic coherence.", "published": "2021-10-25 09:46:26", "link": "http://arxiv.org/abs/2110.12764v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Battling Hateful Content in Indic Languages HASOC '21", "abstract": "The extensive rise in consumption of online social media (OSMs) by a large\nnumber of people poses a critical problem of curbing the spread of hateful\ncontent on these platforms. With the growing usage of OSMs in multiple\nlanguages, the task of detecting and characterizing hate becomes more complex.\nThe subtle variations of code-mixed texts along with switching scripts only add\nto the complexity. This paper presents a solution for the HASOC 2021\nMultilingual Twitter Hate-Speech Detection challenge by team PreCog IIIT\nHyderabad. We adopt a multilingual transformer based approach and describe our\narchitecture for all 6 subtasks as part of the challenge. Out of the 6 teams\nthat participated in all the subtasks, our submissions rank 3rd overall.", "published": "2021-10-25 10:19:17", "link": "http://arxiv.org/abs/2110.12780v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paradigm Shift in Language Modeling: Revisiting CNN for Modeling\n  Sanskrit Originated Bengali and Hindi Language", "abstract": "Though there has been a large body of recent works in language modeling (LM)\nfor high resource languages such as English and Chinese, the area is still\nunexplored for low resource languages like Bengali and Hindi. We propose an end\nto end trainable memory efficient CNN architecture named CoCNN to handle\nspecific characteristics such as high inflection, morphological richness,\nflexible word order and phonetical spelling errors of Bengali and Hindi. In\nparticular, we introduce two learnable convolutional sub-models at word and at\nsentence level that are end to end trainable. We show that state-of-the-art\n(SOTA) Transformer models including pretrained BERT do not necessarily yield\nthe best performance for Bengali and Hindi. CoCNN outperforms pretrained BERT\nwith 16X less parameters, and it achieves much better performance than SOTA\nLSTM models on multiple real-world datasets. This is the first study on the\neffectiveness of different architectures drawn from three deep learning\nparadigms - Convolution, Recurrent, and Transformer neural nets for modeling\ntwo widely used languages, Bengali and Hindi.", "published": "2021-10-25 15:14:42", "link": "http://arxiv.org/abs/2110.13032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Diversity of Unsupervised Paraphrasing with Embedding\n  Outputs", "abstract": "We present a novel technique for zero-shot paraphrase generation. The key\ncontribution is an end-to-end multilingual paraphrasing model that is trained\nusing translated parallel corpora to generate paraphrases into \"meaning spaces\"\n-- replacing the final softmax layer with word embeddings. This architectural\nmodification, plus a training procedure that incorporates an autoencoding\nobjective, enables effective parameter sharing across languages for more fluent\nmonolingual rewriting, and facilitates fluency and diversity in generation. Our\ncontinuous-output paraphrase generation models outperform zero-shot\nparaphrasing baselines when evaluated on two languages using a battery of\ncomputational metrics as well as in human assessment.", "published": "2021-10-25 19:33:38", "link": "http://arxiv.org/abs/2110.13231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "No News is Good News: A Critique of the One Billion Word Benchmark", "abstract": "The One Billion Word Benchmark is a dataset derived from the WMT 2011 News\nCrawl, commonly used to measure language modeling ability in natural language\nprocessing. We train models solely on Common Crawl web scrapes partitioned by\nyear, and demonstrate that they perform worse on this task over time due to\ndistributional shift. Analysis of this corpus reveals that it contains several\nexamples of harmful text, as well as outdated references to current events. We\nsuggest that the temporal nature of news and its distribution shift over time\nmakes it poorly suited for measuring language modeling ability, and discuss\npotential impact and considerations for researchers building language models\nand evaluation datasets.", "published": "2021-10-25 02:41:27", "link": "http://arxiv.org/abs/2110.12609v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TODSum: Task-Oriented Dialogue Summarization with State Tracking", "abstract": "Previous dialogue summarization datasets mainly focus on open-domain chitchat\ndialogues, while summarization datasets for the broadly used task-oriented\ndialogue haven't been explored yet. Automatically summarizing such\ntask-oriented dialogues can help a business collect and review needs to improve\nthe service. Besides, previous datasets pay more attention to generate good\nsummaries with higher ROUGE scores, but they hardly understand the structured\ninformation of dialogues and ignore the factuality of summaries. In this paper,\nwe introduce a large-scale public Task-Oriented Dialogue Summarization dataset,\nTODSum, which aims to summarize the key points of the agent completing certain\ntasks with the user. Compared to existing work, TODSum suffers from severe\nscattered information issues and requires strict factual consistency, which\nmakes it hard to directly apply recent dialogue summarization models.\nTherefore, we introduce additional dialogue state knowledge for TODSum to\nenhance the faithfulness of generated summaries. We hope a better understanding\nof conversational content helps summarization models generate concise and\ncoherent summaries. Meanwhile, we establish a comprehensive benchmark for\nTODSum and propose a state-aware structured dialogue summarization model to\nintegrate dialogue state information and dialogue history. Exhaustive\nexperiments and qualitative analysis prove the effectiveness of dialogue\nstructure guidance. Finally, we discuss the current issues of TODSum and\npotential development directions for future work.", "published": "2021-10-25 06:53:11", "link": "http://arxiv.org/abs/2110.12680v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"So You Think You're Funny?\": Rating the Humour Quotient in Standup\n  Comedy", "abstract": "Computational Humour (CH) has attracted the interest of Natural Language\nProcessing and Computational Linguistics communities. Creating datasets for\nautomatic measurement of humour quotient is difficult due to multiple possible\ninterpretations of the content. In this work, we create a multi-modal\nhumour-annotated dataset ($\\sim$40 hours) using stand-up comedy clips. We\ndevise a novel scoring mechanism to annotate the training data with a humour\nquotient score using the audience's laughter. The normalized duration (laughter\nduration divided by the clip duration) of laughter in each clip is used to\ncompute this humour coefficient score on a five-point scale (0-4). This method\nof scoring is validated by comparing with manually annotated scores, wherein a\nquadratic weighted kappa of 0.6 is obtained. We use this dataset to train a\nmodel that provides a \"funniness\" score, on a five-point scale, given the audio\nand its corresponding text. We compare various neural language models for the\ntask of humour-rating and achieve an accuracy of $0.813$ in terms of Quadratic\nWeighted Kappa (QWK). Our \"Open Mic\" dataset is released for further research\nalong with the code.", "published": "2021-10-25 09:46:46", "link": "http://arxiv.org/abs/2110.12765v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Watermarked Adversarial Texts", "abstract": "Adversarial example generation has been a hot spot in recent years because it\ncan cause deep neural networks (DNNs) to misclassify the generated adversarial\nexamples, which reveals the vulnerability of DNNs, motivating us to find good\nsolutions to improve the robustness of DNN models. Due to the extensiveness and\nhigh liquidity of natural language over the social networks, various natural\nlanguage based adversarial attack algorithms have been proposed in the\nliterature. These algorithms generate adversarial text examples with high\nsemantic quality. However, the generated adversarial text examples may be\nmaliciously or illegally used. In order to tackle with this problem, we present\na general framework for generating watermarked adversarial text examples. For\neach word in a given text, a set of candidate words are determined to ensure\nthat all the words in the set can be used to either carry secret bits or\nfacilitate the construction of adversarial example. By applying a word-level\nadversarial text generation algorithm, the watermarked adversarial text example\ncan be finally generated. Experiments show that the adversarial text examples\ngenerated by the proposed method not only successfully fool advanced DNN\nmodels, but also carry a watermark that can effectively verify the ownership\nand trace the source of the adversarial examples. Moreover, the watermark can\nstill survive after attacked with adversarial example generation algorithms,\nwhich has shown the applicability and superiority.", "published": "2021-10-25 13:37:23", "link": "http://arxiv.org/abs/2110.12948v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Persona Authentication through Generative Dialogue", "abstract": "In this paper we define and investigate the problem of \\emph{persona\nauthentication}: learning a conversational policy to verify the consistency of\npersona models. We propose a learning objective and prove (under some mild\nassumptions) that local density estimators trained under this objective\nmaximize the mutual information between persona information and dialog\ntrajectory. Based on the proposed objective, we develop a method of learning an\nauthentication model that adaptively outputs personalized questions to reveal\nthe underlying persona of its partner throughout the course of multi-turn\nconversation. Experiments show that our authentication method discovers\neffective question sequences that generalize to unseen persona profiles.", "published": "2021-10-25 13:37:48", "link": "http://arxiv.org/abs/2110.12949v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generating artificial texts as substitution or complement of training\n  data", "abstract": "The quality of artificially generated texts has considerably improved with\nthe advent of transformers. The question of using these models to generate\nlearning data for supervised learning tasks naturally arises. In this article,\nthis question is explored under 3 aspects: (i) are artificial data an efficient\ncomplement? (ii) can they replace the original data when those are not\navailable or cannot be distributed for confidentiality reasons? (iii) can they\nimprove the explainability of classifiers? Different experiments are carried\nout on Web-related classification tasks -- namely sentiment analysis on product\nreviews and Fake News detection -- using artificially generated data by\nfine-tuned GPT-2 models. The results show that such artificial data can be used\nin a certain extend but require pre-processing to significantly improve\nperformance. We show that bag-of-word approaches benefit the most from such\ndata augmentation.", "published": "2021-10-25 14:53:42", "link": "http://arxiv.org/abs/2110.13016v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Findings from Experiments of On-line Joint Reinforcement Learning of\n  Semantic Parser and Dialogue Manager with real Users", "abstract": "Design of dialogue systems has witnessed many advances lately, yet acquiring\nhuge set of data remains an hindrance to their fast development for a new task\nor language. Besides, training interactive systems with batch data is not\nsatisfactory. On-line learning is pursued in this paper as a convenient way to\nalleviate these difficulties. After the system modules are initiated, a single\nprocess handles data collection, annotation and use in training algorithms. A\nnew challenge is to control the cost of the on-line learning borne by the user.\nOur work focuses on learning the semantic parsing and dialogue management\nmodules (speech recognition and synthesis offer ready-for-use solutions). In\nthis context we investigate several variants of simultaneous learning which are\ntested in user trials. In our experiments, with varying merits, they can all\nachieve good performance with only a few hundreds of training dialogues and\noverstep a handcrafted system. The analysis of these experiments gives us some\ninsights, discussed in the paper, into the difficulty for the system's trainers\nto establish a coherent and constant behavioural strategy to enable a fast and\ngood-quality training phase.", "published": "2021-10-25 18:51:41", "link": "http://arxiv.org/abs/2110.13213v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Distributionally Robust Recurrent Decoders with Random Network\n  Distillation", "abstract": "Neural machine learning models can successfully model language that is\nsimilar to their training distribution, but they are highly susceptible to\ndegradation under distribution shift, which occurs in many practical\napplications when processing out-of-domain (OOD) text. This has been attributed\nto \"shortcut learning\": relying on weak correlations over arbitrary large\ncontexts.\n  We propose a method based on OOD detection with Random Network Distillation\nto allow an autoregressive language model to automatically disregard OOD\ncontext during inference, smoothly transitioning towards a less expressive but\nmore robust model as the data becomes more OOD while retaining its full context\ncapability when operating in-distribution. We apply our method to a GRU\narchitecture, demonstrating improvements on multiple language modeling (LM)\ndatasets.", "published": "2021-10-25 19:26:29", "link": "http://arxiv.org/abs/2110.13229v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DeepHelp: Deep Learning for Shout Crisis Text Conversations", "abstract": "The Shout Crisis Text Line provides individuals undergoing mental health\ncrises an opportunity to have an anonymous text message conversation with a\ntrained Crisis Volunteer (CV). This project partners with Shout and its parent\norganisation, Mental Health Innovations, to explore the applications of Machine\nLearning in understanding Shout's conversations and improving its service. The\noverarching aim of this project is to develop a proof-of-concept model to\ndemonstrate the potential of applying deep learning to crisis text messages.\n  Specifically, this project aims to use deep learning to (1) predict an\nindividual's risk of suicide or self-harm, (2) assess conversation success and\nCV skill using robust metrics, and (3) extrapolate demographic information from\na texter survey to conversations where the texter did not complete the survey.\nTo these ends, contributions to deep learning include a modified\nTransformer-over-BERT model; a framework for multitask learning to improve\ngeneralisation in the presence of sparse labels; and a mathematical model for\nusing imperfect machine learning models to estimate population parameters from\na biased training set.\n  Key results include a deep learning model with likely better performance at\npredicting suicide risk than trained CVs and the ability to predict whether a\ntexter is 21 or under with 88.4% accuracy. We produce three metrics for\nconversation success and evaluate the validity and usefulness for each.\nFinally, reversal of participation bias provides evidence that women, who make\nup 80.3% of conversations with an associated texter survey, make up closer to\n73.5%- 74.8% of all conversations; and that if, after every conversation, the\ntexter had shared whether they found their conversation helpful, affirmative\nanswers would fall from 85.1% to 45.45% - 46.51%.", "published": "2021-10-25 20:04:19", "link": "http://arxiv.org/abs/2110.13244v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Alignment Attention by Matching Key and Query Distributions", "abstract": "The neural attention mechanism has been incorporated into deep neural\nnetworks to achieve state-of-the-art performance in various domains. Most such\nmodels use multi-head self-attention which is appealing for the ability to\nattend to information from different perspectives. This paper introduces\nalignment attention that explicitly encourages self-attention to match the\ndistributions of the key and query within each head. The resulting alignment\nattention networks can be optimized as an unsupervised regularization in the\nexisting attention framework. It is simple to convert any models with\nself-attention, including pre-trained ones, to the proposed alignment\nattention. On a variety of language understanding tasks, we show the\neffectiveness of our method in accuracy, uncertainty estimation, generalization\nacross domains, and robustness to adversarial attacks. We further demonstrate\nthe general applicability of our approach on graph attention and visual\nquestion answering, showing the great potential of incorporating our alignment\nmethod into various attention-related tasks.", "published": "2021-10-25 00:54:57", "link": "http://arxiv.org/abs/2110.12567v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Fine-tuning of Pre-trained Transformers for Hate, Offensive, and Profane\n  Content Detection in English and Marathi", "abstract": "This paper describes neural models developed for the Hate Speech and\nOffensive Content Identification in English and Indo-Aryan Languages Shared\nTask 2021. Our team called neuro-utmn-thales participated in two tasks on\nbinary and fine-grained classification of English tweets that contain hate,\noffensive, and profane content (English Subtasks A & B) and one task on\nidentification of problematic content in Marathi (Marathi Subtask A). For\nEnglish subtasks, we investigate the impact of additional corpora for hate\nspeech detection to fine-tune transformer models. We also apply a one-vs-rest\napproach based on Twitter-RoBERTa to discrimination between hate, profane and\noffensive posts. Our models ranked third in English Subtask A with the F1-score\nof 81.99% and ranked second in English Subtask B with the F1-score of 65.77%.\nFor the Marathi tasks, we propose a system based on the Language-Agnostic BERT\nSentence Embedding (LaBSE). This model achieved the second result in Marathi\nSubtask A obtaining an F1 of 88.08%.", "published": "2021-10-25 07:11:02", "link": "http://arxiv.org/abs/2110.12687v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.7.m; H.3.3"], "primary_category": "cs.CL"}
{"title": "SciClops: Detecting and Contextualizing Scientific Claims for Assisting\n  Manual Fact-Checking", "abstract": "This paper describes SciClops, a method to help combat online scientific\nmisinformation. Although automated fact-checking methods have gained\nsignificant attention recently, they require pre-existing ground-truth\nevidence, which, in the scientific context, is sparse and scattered across a\nconstantly-evolving scientific literature. Existing methods do not exploit this\nliterature, which can effectively contextualize and combat science-related\nfallacies. Furthermore, these methods rarely require human intervention, which\nis essential for the convoluted and critical domain of scientific\nmisinformation. SciClops involves three main steps to process scientific claims\nfound in online news articles and social media postings: extraction,\nclustering, and contextualization. First, the extraction of scientific claims\ntakes place using a domain-specific, fine-tuned transformer model. Second,\nsimilar claims extracted from heterogeneous sources are clustered together with\nrelated scientific literature using a method that exploits their content and\nthe connections among them. Third, check-worthy claims, broadcasted by popular\nyet unreliable sources, are highlighted together with an enhanced fact-checking\ncontext that includes related verified claims, news articles, and scientific\npapers. Extensive experiments show that SciClops tackles sufficiently these\nthree steps, and effectively assists non-expert fact-checkers in the\nverification of complex scientific claims, outperforming commercial\nfact-checking systems.", "published": "2021-10-25 16:35:58", "link": "http://arxiv.org/abs/2110.13090v1", "categories": ["cs.CL", "cs.CY", "cs.IR", "H.3.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally", "abstract": "When making everyday decisions, people are guided by their conscience, an\ninternal sense of right and wrong. By contrast, artificial agents are currently\nnot endowed with a moral sense. As a consequence, they may learn to behave\nimmorally when trained on environments that ignore moral concerns, such as\nviolent video games. With the advent of generally capable agents that pretrain\non many environments, it will become necessary to mitigate inherited biases\nfrom environments that teach immoral behavior. To facilitate the development of\nagents that avoid causing wanton harm, we introduce Jiminy Cricket, an\nenvironment suite of 25 text-based adventure games with thousands of diverse,\nmorally salient scenarios. By annotating every possible game state, the Jiminy\nCricket environments robustly evaluate whether agents can act morally while\nmaximizing reward. Using models with commonsense moral knowledge, we create an\nelementary artificial conscience that assesses and guides agents. In extensive\nexperiments, we find that the artificial conscience approach can steer agents\ntowards moral behavior without sacrificing performance.", "published": "2021-10-25 17:59:31", "link": "http://arxiv.org/abs/2110.13136v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "IconQA: A New Benchmark for Abstract Diagram Understanding and Visual\n  Language Reasoning", "abstract": "Current visual question answering (VQA) tasks mainly consider answering\nhuman-annotated questions for natural images. However, aside from natural\nimages, abstract diagrams with semantic richness are still understudied in\nvisual understanding and reasoning research. In this work, we introduce a new\nchallenge of Icon Question Answering (IconQA) with the goal of answering a\nquestion in an icon image context. We release IconQA, a large-scale dataset\nthat consists of 107,439 questions and three sub-tasks: multi-image-choice,\nmulti-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by\nreal-world diagram word problems that highlight the importance of abstract\ndiagram understanding and comprehensive cognitive reasoning. Thus, IconQA\nrequires not only perception skills like object recognition and text\nunderstanding, but also diverse cognitive reasoning skills, such as geometric\nreasoning, commonsense reasoning, and arithmetic reasoning. To facilitate\npotential IconQA models to learn semantic representations for icon images, we\nfurther release an icon dataset Icon645 which contains 645,687 colored icons on\n377 classes. We conduct extensive user studies and blind experiments and\nreproduce a wide range of advanced VQA methods to benchmark the IconQA task.\nAlso, we develop a strong IconQA baseline Patch-TRM that applies a pyramid\ncross-modal Transformer with input diagram embeddings pre-trained on the icon\ndataset. IconQA and Icon645 are available at https://iconqa.github.io.", "published": "2021-10-25 18:52:26", "link": "http://arxiv.org/abs/2110.13214v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Exposure of occupations to technologies of the fourth industrial\n  revolution", "abstract": "The fourth industrial revolution (4IR) is likely to have a substantial impact\non the economy. Companies need to build up capabilities to implement new\ntechnologies, and automation may make some occupations obsolete. However,\nwhere, when, and how the change will happen remain to be determined. Robust\nempirical indicators of technological progress linked to occupations can help\nto illuminate this change. With this aim, we provide such an indicator based on\npatent data. Using natural language processing, we calculate patent exposure\nscores for more than 900 occupations, which represent the technological\nprogress related to them. To provide a lens on the impact of the 4IR, we\ndifferentiate between traditional and 4IR patent exposure. Our method differs\nfrom previous approaches in that it both accounts for the diversity of\ntask-level patent exposures within an occupation and reflects work activities\nmore accurately. We find that exposure to 4IR patents differs from traditional\npatent exposure. Manual tasks, and accordingly occupations such as construction\nand production, are exposed mainly to traditional (non-4IR) patents but have\nlow exposure to 4IR patents. The analysis suggests that 4IR technologies may\nhave a negative impact on job growth; this impact appears 10 to 20 years after\npatent filing. Further, we compared the 4IR exposure to other automation and AI\nexposure scores. Whereas many measures refer to theoretical automation\npotential, our patent-based indicator reflects actual technology diffusion. Our\nwork not only allows analyses of the impact of 4IR technologies as a whole, but\nalso provides exposure scores for more than 300 technology fields, such as AI\nand smart office technologies. Finally, the work provides a general mapping of\npatents to tasks and occupations, which enables future researchers to construct\nindividual exposure measures.", "published": "2021-10-25 23:23:56", "link": "http://arxiv.org/abs/2110.13317v1", "categories": ["cs.CY", "cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CY"}
{"title": "The Efficiency Misnomer", "abstract": "Model efficiency is a critical aspect of developing and deploying machine\nlearning models. Inference time and latency directly affect the user\nexperience, and some applications have hard requirements. In addition to\ninference costs, model training also have direct financial and environmental\nimpacts. Although there are numerous well-established metrics (cost indicators)\nfor measuring model efficiency, researchers and practitioners often assume that\nthese metrics are correlated with each other and report only few of them. In\nthis paper, we thoroughly discuss common cost indicators, their advantages and\ndisadvantages, and how they can contradict each other. We demonstrate how\nincomplete reporting of cost indicators can lead to partial conclusions and a\nblurred or incomplete picture of the practical considerations of different\nmodels. We further present suggestions to improve reporting of efficiency\nmetrics.", "published": "2021-10-25 12:48:07", "link": "http://arxiv.org/abs/2110.12894v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "AxoNN: An asynchronous, message-driven parallel framework for\n  extreme-scale deep learning", "abstract": "In the last few years, the memory requirements to train state-of-the-art\nneural networks have far exceeded the DRAM capacities of modern hardware\naccelerators. This has necessitated the development of efficient algorithms to\ntrain these neural networks in parallel on large-scale GPU-based clusters.\nSince computation is relatively inexpensive on modern GPUs, designing and\nimplementing extremely efficient communication in these parallel training\nalgorithms is critical for extracting the maximum performance. This paper\npresents AxoNN, a parallel deep learning framework that exploits asynchrony and\nmessage-driven execution to schedule neural network operations on each GPU,\nthereby reducing GPU idle time and maximizing hardware efficiency. By using the\nCPU memory as a scratch space for offloading data periodically during training,\nAxoNN is able to reduce GPU memory consumption by four times. This allows us to\nincrease the number of parameters per GPU by four times, thus reducing the\namount of communication and increasing performance by over 13%. When tested\nagainst large transformer models with 12-100 billion parameters on 48-384\nNVIDIA Tesla V100 GPUs, AxoNN achieves a per-GPU throughput of 49.4-54.78% of\ntheoretical peak and reduces the training time by 22-37 days (15-25% speedup)\nas compared to the state-of-the-art.", "published": "2021-10-25 14:43:36", "link": "http://arxiv.org/abs/2110.13005v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC", "cs.PF"], "primary_category": "cs.LG"}
{"title": "Lhotse: a speech data representation library for the modern deep\n  learning ecosystem", "abstract": "Speech data is notoriously difficult to work with due to a variety of codecs,\nlengths of recordings, and meta-data formats. We present Lhotse, a speech data\nrepresentation library that draws upon lessons learned from Kaldi speech\nrecognition toolkit and brings its concepts into the modern deep learning\necosystem. Lhotse provides a common JSON description format with corresponding\nPython classes and data preparation recipes for over 30 popular speech corpora.\nVarious datasets can be easily combined together and re-purposed for different\ntasks. The library handles multi-channel recordings, long recordings, local and\ncloud storage, lazy and on-the-fly operations amongst other features. We\nintroduce Cut and CutSet concepts, which simplify common data wrangling tasks\nfor audio and help incorporate acoustic context of speech utterances. Finally,\nwe show how Lhotse leverages PyTorch data API abstractions and adopts them to\nhandle speech data for deep learning.", "published": "2021-10-25 00:32:21", "link": "http://arxiv.org/abs/2110.12561v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Controllable and Interpretable Singing Voice Decomposition via Assem-VC", "abstract": "We propose a singing decomposition system that encodes time-aligned\nlinguistic content, pitch, and source speaker identity via Assem-VC. With\ndecomposed speaker-independent information and the target speaker's embedding,\nwe could synthesize the singing voice of the target speaker. In conclusion, we\nmade a perfectly synced duet with the user's singing voice and the target\nsinger's converted singing voice.", "published": "2021-10-25 06:52:00", "link": "http://arxiv.org/abs/2110.12676v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Synchronization of Wireless Acoustic Sensor Networks in the Presence\n  of Time-varying Sampling Rate Offsets and Speaker Changes", "abstract": "A wireless acoustic sensor network records audio signals with sampling time\nand sampling rate offsets between the audio streams, if the analog-digital\nconverters (ADCs) of the network devices are not synchronized. Here, we\nintroduce a new sampling rate offset model to simulate time-varying sampling\nfrequencies caused, for example, by temperature changes of ADC crystal\noscillators, and propose an estimation algorithm to handle this dynamic aspect\nin combination with changing acoustic source positions. Furthermore, we show\nhow deep neural network based estimates of the distances between microphones\nand human speakers can be used to determine the sampling time offsets. This\nenables a synchronization of the audio streams to reflect the physical time\ndifferences of flight.", "published": "2021-10-25 11:38:03", "link": "http://arxiv.org/abs/2110.12820v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multichannel Speech Enhancement without Beamforming", "abstract": "Deep neural networks are often coupled with traditional spatial filters, such\nas MVDR beamformers for effectively exploiting spatial information. Even though\nsingle-stage end-to-end supervised models can obtain impressive enhancement,\ncombining them with a traditional beamformer and a DNN-based post-filter in a\nmultistage processing provides additional improvements. In this work, we\npropose a two-stage strategy for multi-channel speech enhancement that does not\nrequire a traditional beamformer for additional performance. First, we propose\na novel attentive dense convolutional network (ADCN) for estimating real and\nimaginary parts of complex spectrogram. ADCN obtains state-of-the-art results\namong single-stage models. Next, we use ADCN with a recently proposed\ntriple-path attentive recurrent network (TPARN) for estimating waveform\nsamples. The proposed strategy uses two insights; first, using different\napproaches in two stages; and second, using a stronger model in the first\nstage. We illustrate the efficacy of our strategy by evaluating multiple models\nin a two-stage approach with and without a traditional beamformer.", "published": "2021-10-25 17:49:14", "link": "http://arxiv.org/abs/2110.13130v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DelightfulTTS: The Microsoft Speech Synthesis System for Blizzard\n  Challenge 2021", "abstract": "This paper describes the Microsoft end-to-end neural text to speech (TTS)\nsystem: DelightfulTTS for Blizzard Challenge 2021. The goal of this challenge\nis to synthesize natural and high-quality speech from text, and we approach\nthis goal in two perspectives: The first is to directly model and generate\nwaveform in 48 kHz sampling rate, which brings higher perception quality than\nprevious systems with 16 kHz or 24 kHz sampling rate; The second is to model\nthe variation information in speech through a systematic design, which improves\nthe prosody and naturalness. Specifically, for 48 kHz modeling, we predict 16\nkHz mel-spectrogram in acoustic model, and propose a vocoder called HiFiNet to\ndirectly generate 48 kHz waveform from predicted 16 kHz mel-spectrogram, which\ncan better trade off training efficiency, modelling stability and voice\nquality. We model variation information systematically from both explicit\n(speaker ID, language ID, pitch and duration) and implicit (utterance-level and\nphoneme-level prosody) perspectives: 1) For speaker and language ID, we use\nlookup embedding in training and inference; 2) For pitch and duration, we\nextract the values from paired text-speech data in training and use two\npredictors to predict the values in inference; 3) For utterance-level and\nphoneme-level prosody, we use two reference encoders to extract the values in\ntraining, and use two separate predictors to predict the values in inference.\nAdditionally, we introduce an improved Conformer block to better model the\nlocal and global dependency in acoustic model. For task SH1, DelightfulTTS\nachieves 4.17 mean score in MOS test and 4.35 in SMOS test, which indicates the\neffectiveness of our proposed system", "published": "2021-10-25 02:47:59", "link": "http://arxiv.org/abs/2110.12612v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Deep Reinforcement Learning Approach for Audio-based Navigation and\n  Audio Source Localization in Multi-speaker Environments", "abstract": "In this work we apply deep reinforcement learning to the problems of\nnavigating a three-dimensional environment and inferring the locations of human\nspeaker audio sources within, in the case where the only available information\nis the raw sound from the environment, as a simulated human listener placed in\nthe environment would hear it. For this purpose we create two virtual\nenvironments using the Unity game engine, one presenting an audio-based\nnavigation problem and one presenting an audio source localization problem. We\nalso create an autonomous agent based on PPO online reinforcement learning\nalgorithm and attempt to train it to solve these environments. Our experiments\nshow that our agent achieves adequate performance and generalization ability in\nboth environments, measured by quantitative metrics, even when a limited amount\nof training data are available or the environment parameters shift in ways not\nencountered during training. We also show that a degree of agent knowledge\ntransfer is possible between the environments.", "published": "2021-10-25 10:18:34", "link": "http://arxiv.org/abs/2110.12778v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ML-Based Analysis to Identify Speech Features Relevant in Predicting\n  Alzheimer's Disease", "abstract": "Alzheimer's disease (AD) is a neurodegenerative disease that affects nearly\n50 million individuals across the globe and is one of the leading causes of\ndeaths globally. It is projected that by 2050, the number of people affected by\nthe disease would more than double. Consequently, the growing advancements in\ntechnology beg the question, can technology be used to predict Alzheimer's for\na better and early diagnosis? In this paper, we focus on this very problem.\nSpecifically, we have trained both ML models and neural networks to predict and\nclassify participants based on their speech patterns. We computed a number of\nlinguistic variables using DementiaBank's Pitt Corpus, a database consisting of\ntranscripts of interviews with subjects suffering from multiple\nneurodegenerative diseases. We then trained both binary classifiers, as well as\nmulticlass classifiers to distinguish AD from normal aging and other\nneurodegenerative diseases. We also worked on establishing the link between\nspecific speech factors that can help determine the onset of AD. Confusion\nmatrices and feature importance graphs have been plotted model-wise to compare\nthe performances of our models. In both multiclass and binary classification,\nneural networks were found to outperform the other models with a testing\naccuracy of 76.44% and 92.05% respectively. For the feature importance, it was\nconcluded that '%_PRESP' (present participle), '%_3S' (3rd person present tense\nmarkers) were two of the most important speech features for our classifiers in\npredicting AD.", "published": "2021-10-25 15:00:50", "link": "http://arxiv.org/abs/2110.13023v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Unsupervised Source Separation By Steering Pretrained Music Models", "abstract": "We showcase an unsupervised method that repurposes deep models trained for\nmusic generation and music tagging for audio source separation, without any\nretraining. An audio generation model is conditioned on an input mixture,\nproducing a latent encoding of the audio used to generate audio. This generated\naudio is fed to a pretrained music tagger that creates source labels. The\ncross-entropy loss between the tag distribution for the generated audio and a\npredefined distribution for an isolated source is used to guide gradient ascent\nin the (unchanging) latent space of the generative model. This system does not\nupdate the weights of the generative model or the tagger, and only relies on\nmoving through the generative model's latent space to produce separated\nsources. We use OpenAI's Jukebox as the pretrained generative model, and we\ncouple it with four kinds of pretrained music taggers (two architectures and\ntwo tagging datasets). Experimental results on two source separation datasets,\nshow this approach can produce separation estimates for a wider variety of\nsources than any tested supervised or unsupervised system. This work points to\nthe vast and heretofore untapped potential of large pretrained music models for\naudio-to-audio tasks like source separation.", "published": "2021-10-25 16:08:28", "link": "http://arxiv.org/abs/2110.13071v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Impact-sounding Acoustic Inspection of Concrete Structure", "abstract": "Impact sounding signal has been shown to contain information about structural\nintegrity flaws and subsurface objects from previous research. As\nnon-destructive testing (NDT) method, one of the biggest challenges in impact\nsounding based inspection is the subsurface targets detection and\nreconstruction. This paper presents the importance and practicability of using\nsolenoids to trigger impact sounding signal and using acoustic data to\nreconstruct subsurface objects to address this issue. First, by taking\nadvantage of Visual Simultaneous Localization and Mapping (V-SLAM), we could\nobtain the 3D position of the robot during the inspection. Second, our NDE\nmethod is based on Frequency Density (FD) analysis for the Fast Fourier\nTransform (FFT) of the impact sounding signal. At last, by combining the 3D\nposition data and acoustic data, this paper creates a 3D map to highlight the\npossible subsurface objects. The experimental results demonstrate the\nfeasibility of the method.", "published": "2021-10-25 17:39:36", "link": "http://arxiv.org/abs/2110.13125v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Beyond $L_p$ clipping: Equalization-based Psychoacoustic Attacks against\n  ASRs", "abstract": "Automatic Speech Recognition (ASR) systems convert speech into text and can\nbe placed into two broad categories: traditional and fully end-to-end. Both\ntypes have been shown to be vulnerable to adversarial audio examples that sound\nbenign to the human ear but force the ASR to produce malicious transcriptions.\nOf these attacks, only the \"psychoacoustic\" attacks can create examples with\nrelatively imperceptible perturbations, as they leverage the knowledge of the\nhuman auditory system. Unfortunately, existing psychoacoustic attacks can only\nbe applied against traditional models, and are obsolete against the newer,\nfully end-to-end ASRs. In this paper, we propose an equalization-based\npsychoacoustic attack that can exploit both traditional and fully end-to-end\nASRs. We successfully demonstrate our attack against real-world ASRs that\ninclude DeepSpeech and Wav2Letter. Moreover, we employ a user study to verify\nthat our method creates low audible distortion. Specifically, 80 of the 100\nparticipants voted in favor of all our attack audio samples as less noisier\nthan the existing state-of-the-art attack. Through this, we demonstrate both\ntypes of existing ASR pipelines can be exploited with minimum degradation to\nattack audio quality.", "published": "2021-10-25 20:30:51", "link": "http://arxiv.org/abs/2110.13250v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Deep Learning Tools for Audacity: Helping Researchers Expand the\n  Artist's Toolkit", "abstract": "We present a software framework that integrates neural networks into the\npopular open-source audio editing software, Audacity, with a minimal amount of\ndeveloper effort. In this paper, we showcase some example use cases for both\nend-users and neural network developers. We hope that this work fosters a new\nlevel of interactivity between deep learning practitioners and end-users.", "published": "2021-10-25 23:56:38", "link": "http://arxiv.org/abs/2110.13323v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Actions Speak Louder than Listening: Evaluating Music Style Transfer\n  based on Editing Experience", "abstract": "The subjective evaluation of music generation techniques has been mostly done\nwith questionnaire-based listening tests while ignoring the perspectives from\nmusic composition, arrangement, and soundtrack editing. In this paper, we\npropose an editing test to evaluate users' editing experience of music\ngeneration models in a systematic way. To do this, we design a new music style\ntransfer model combining the non-chronological inference architecture,\nautoregressive models and the Transformer, which serves as an improvement from\nthe baseline model on the same style transfer task. Then, we compare the\nperformance of the two models with a conventional listening test and the\nproposed editing test, in which the quality of generated samples is assessed by\nthe amount of effort (e.g., the number of required keyboard and mouse actions)\nspent by users to polish a music clip. Results on two target styles indicate\nthat the improvement over the baseline model can be reflected by the editing\ntest quantitatively. Also, the editing test provides profound insights which\nare not accessible from usual listening tests. The major contribution of this\npaper is the systematic presentation of the editing test and the corresponding\ninsights, while the proposed music style transfer model based on\nstate-of-the-art neural networks represents another contribution.", "published": "2021-10-25 12:20:30", "link": "http://arxiv.org/abs/2110.12855v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
