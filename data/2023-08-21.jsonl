{"title": "Large Language Models on Wikipedia-Style Survey Generation: an\n  Evaluation in NLP Concepts", "abstract": "Educational materials such as survey articles in specialized fields like\ncomputer science traditionally require tremendous expert inputs and are\ntherefore expensive to create and update. Recently, Large Language Models\n(LLMs) have achieved significant success across various general tasks. However,\ntheir effectiveness and limitations in the education domain are yet to be fully\nexplored. In this work, we examine the proficiency of LLMs in generating\nsuccinct survey articles specific to the niche field of NLP in computer\nscience, focusing on a curated list of 99 topics. Automated benchmarks reveal\nthat GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by\nmargins ranging from 2% to 20% in comparison to the established ground truth.\nWe compare both human and GPT-based evaluation scores and provide in-depth\nanalysis. While our findings suggest that GPT-created surveys are more\ncontemporary and accessible than human-authored ones, certain limitations were\nobserved. Notably, GPT-4, despite often delivering outstanding content,\noccasionally exhibited lapses like missing details or factual errors. At last,\nwe compared the rating behavior between humans and GPT-4 and found systematic\nbias in using GPT evaluation.", "published": "2023-08-21 01:32:45", "link": "http://arxiv.org/abs/2308.10410v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Measures of Linguistic Diversity Across Social Media Language\n  Data and Census Data at Subnational Geographic Areas", "abstract": "This paper describes a preliminary study on the comparative linguistic\necology of online spaces (i.e., social media language data) and real-world\nspaces in Aotearoa New Zealand (i.e., subnational administrative areas). We\ncompare measures of linguistic diversity between these different spaces and\ndiscuss how social media users align with real-world populations. The results\nfrom the current study suggests that there is potential to use online social\nmedia language data to observe spatial and temporal changes in linguistic\ndiversity at subnational geographic areas; however, further work is required to\nunderstand how well social media represents real-world behaviour.", "published": "2023-08-21 03:54:23", "link": "http://arxiv.org/abs/2308.10452v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence\n  Understanding", "abstract": "Large language models (LLMs) have shown impressive ability for open-domain\nNLP tasks. However, LLMs are sometimes too footloose for natural language\nunderstanding (NLU) tasks which always have restricted output and input format.\nTheir performances on NLU tasks are highly related to prompts or demonstrations\nand are shown to be poor at performing several representative NLU tasks, such\nas event extraction and entity typing. To this end, we present SeqGPT, a\nbilingual (i.e., English and Chinese) open-source autoregressive model\nspecially enhanced for open-domain natural language understanding. We express\nall NLU tasks with two atomic tasks, which define fixed instructions to\nrestrict the input and output format but still ``open'' for arbitrarily varied\nlabel sets. The model is first instruction-tuned with extremely fine-grained\nlabeled data synthesized by ChatGPT and then further fine-tuned by 233\ndifferent atomic tasks from 152 datasets across various domains. The\nexperimental results show that SeqGPT has decent classification and extraction\nability, and is capable of performing language understanding tasks on unseen\ndomains. We also conduct empirical studies on the scaling of data and model\nsize as well as on the transfer across tasks. Our model is accessible at\nhttps://github.com/Alibaba-NLP/SeqGPT.", "published": "2023-08-21 07:31:19", "link": "http://arxiv.org/abs/2308.10529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Equation as a Better Intermediate Meaning Representation for\n  Numerical Reasoning", "abstract": "Numerical reasoning is vital for natural language processing models to\nunderstand and process numerical information in real-world scenarios. Most\ncurrent methods first generate the Intermediate Meaning Representations (IMRs)\nof questions and then generate answers. Current SOTA methods generate programs\nas IMRs with large language models (LLMs). Intuitively, equations have fewer\nrestrictions and closer semantics to the question than programs, leading to\nhigher generation accuracy. However, current LLMs generate equations worse than\nprograms, where we assume that the equation data is rare in pre-training data\ncompared to programs. So in this paper, we try to use equations as IMRs to\nsolve the numerical reasoning task by addressing two problems: (1)\nTheoretically, how to prove that the equation is an IMR with higher generation\naccuracy than programs; (2) Empirically, how to improve the generation accuracy\nof equations with LLMs. For the first problem, we propose and prove a\nproposition to theoretically compare the generation accuracy of different IMRs.\nFor the second problem, we present a method called Boosting Numerical\nReason\\textbfing by Decomposing the Generation of Equations (Bridge), which can\nimprove the accuracy of LLMs in generating equations as IMRs by reducing the\ntendency of generating constant expressions and programs. Our method improves\nthe performance by 2.2%, 0.9%, and 1.7% on GSM8K, SVAMP, and Algebra datasets\ncompared to the previous state-of-the-art methods under the single reasoning\npath setting. Our codes and prompts are released in\nhttps://github.com/zirui-HIT/Bridge_for_Numerical_Reasoning.", "published": "2023-08-21 09:35:33", "link": "http://arxiv.org/abs/2308.10585v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BAN-PL: a Novel Polish Dataset of Banned Harmful and Offensive Content\n  from Wykop.pl web service", "abstract": "Since the Internet is flooded with hate, it is one of the main tasks for NLP\nexperts to master automated online content moderation. However, advancements in\nthis field require improved access to publicly available accurate and\nnon-synthetic datasets of social media content. For the Polish language, such\nresources are very limited. In this paper, we address this gap by presenting a\nnew open dataset of offensive social media content for the Polish language. The\ndataset comprises content from Wykop.pl, a popular online service often\nreferred to as the \"Polish Reddit\", reported by users and banned in the\ninternal moderation process. It contains a total of 691,662 posts and comments,\nevenly divided into two categories: \"harmful\" and \"neutral\" (\"non-harmful\").\nThe anonymized subset of the BAN-PL dataset consisting on 24,000 pieces (12,000\nfor each class), along with preprocessing scripts have been made publicly\navailable. Furthermore the paper offers valuable insights into real-life\ncontent moderation processes and delves into an analysis of linguistic features\nand content characteristics of the dataset. Moreover, a comprehensive\nanonymization procedure has been meticulously described and applied. The\nprevalent biases encountered in similar datasets, including post-moderation and\npre-selection biases, are also discussed.", "published": "2023-08-21 09:47:31", "link": "http://arxiv.org/abs/2308.10592v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Systematic Offensive Stereotyping (SOS) Bias in Language Models", "abstract": "In this paper, we propose a new metric to measure the SOS bias in language\nmodels (LMs). Then, we validate the SOS bias and investigate the effectiveness\nof removing it. Finally, we investigate the impact of the SOS bias in LMs on\ntheir performance and fairness on hate speech detection. Our results suggest\nthat all the inspected LMs are SOS biased. And that the SOS bias is reflective\nof the online hate experienced by marginalized identities. The results indicate\nthat using debias methods from the literature worsens the SOS bias in LMs for\nsome sensitive attributes and improves it for others. Finally, Our results\nsuggest that the SOS bias in the inspected LMs has an impact on their fairness\nof hate speech detection. However, there is no strong evidence that the SOS\nbias has an impact on the performance of hate speech detection.", "published": "2023-08-21 12:37:42", "link": "http://arxiv.org/abs/2308.10684v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring\n  Emergent Behaviors", "abstract": "Autonomous agents empowered by Large Language Models (LLMs) have undergone\nsignificant improvements, enabling them to generalize across a broad spectrum\nof tasks. However, in real-world scenarios, cooperation among individuals is\noften required to enhance the efficiency and effectiveness of task\naccomplishment. Hence, inspired by human group dynamics, we propose a\nmulti-agent framework \\framework that can collaboratively and dynamically\nadjust its composition as a greater-than-the-sum-of-its-parts system. Our\nexperiments demonstrate that \\framework framework can effectively deploy\nmulti-agent groups that outperform a single agent. Furthermore, we delve into\nthe emergence of social behaviors among individual agents within a group during\ncollaborative task accomplishment. In view of these behaviors, we discuss some\npossible strategies to leverage positive ones and mitigate negative ones for\nimproving the collaborative potential of multi-agent groups. Our codes for\n\\framework will soon be released at\n\\url{https://github.com/OpenBMB/AgentVerse}.", "published": "2023-08-21 16:47:11", "link": "http://arxiv.org/abs/2308.10848v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete\n  Information from Lateral Thinking Puzzles", "abstract": "With the continuous evolution and refinement of LLMs, they are endowed with\nimpressive logical reasoning or vertical thinking capabilities. But can they\nthink out of the box? Do they possess proficient lateral thinking abilities?\nFollowing the setup of Lateral Thinking Puzzles, we propose a novel evaluation\nbenchmark, LatEval, which assesses the model's lateral thinking within an\ninteractive framework. In our benchmark, we challenge LLMs with 2 aspects: the\nquality of questions posed by the model and the model's capability to integrate\ninformation for problem-solving. We find that nearly all LLMs struggle with\nemploying lateral thinking during interactions. For example, even the most\nadvanced model, GPT-4, exhibits the advantage to some extent, yet still\nmaintain a noticeable gap when compared to human. This evaluation benchmark\nprovides LLMs with a highly challenging and distinctive task that is crucial to\nan effective AI assistant.", "published": "2023-08-21 16:49:40", "link": "http://arxiv.org/abs/2308.10855v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes\n  and Biases in Large Language Models", "abstract": "Detecting stereotypes and biases in Large Language Models (LLMs) can enhance\nfairness and reduce adverse impacts on individuals or groups when these LLMs\nare applied. However, the majority of existing methods focus on measuring the\nmodel's preference towards sentences containing biases and stereotypes within\ndatasets, which lacks interpretability and cannot detect implicit biases and\nstereotypes in the real world. To address this gap, this paper introduces a\nfour-stage framework to directly evaluate stereotypes and biases in the\ngenerated content of LLMs, including direct inquiry testing, serial or adapted\nstory testing, implicit association testing, and unknown situation testing.\nAdditionally, the paper proposes multi-dimensional evaluation metrics and\nexplainable zero-shot prompts for automated evaluation. Using the education\nsector as a case study, we constructed the Edu-FairMonitor based on the\nfour-stage framework, which encompasses 12,632 open-ended questions covering\nnine sensitive factors and 26 educational scenarios. Experimental results\nreveal varying degrees of stereotypes and biases in five LLMs evaluated on\nEdu-FairMonitor. Moreover, the results of our proposed automated evaluation\nmethod have shown a high correlation with human annotations.", "published": "2023-08-21 00:25:17", "link": "http://arxiv.org/abs/2308.10397v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Dialogue Topic Segmentation in Hyperdimensional Space", "abstract": "We present HyperSeg, a hyperdimensional computing (HDC) approach to\nunsupervised dialogue topic segmentation. HDC is a class of vector symbolic\narchitectures that leverages the probabilistic orthogonality of randomly drawn\nvectors at extremely high dimensions (typically over 10,000). HDC generates\nrich token representations through its low-cost initialization of many\nunrelated vectors. This is especially beneficial in topic segmentation, which\noften operates as a resource-constrained pre-processing step for downstream\ntranscript understanding tasks. HyperSeg outperforms the current\nstate-of-the-art in 4 out of 5 segmentation benchmarks -- even when baselines\nare given partial access to the ground truth -- and is 10 times faster on\naverage. We show that HyperSeg also improves downstream summarization accuracy.\nWith HyperSeg, we demonstrate the viability of HDC in a major language task. We\nopen-source HyperSeg to provide a strong baseline for unsupervised topic\nsegmentation.", "published": "2023-08-21 04:42:24", "link": "http://arxiv.org/abs/2308.10464v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Effective Method using Phrase Mechanism in Neural Machine Translation", "abstract": "Machine Translation is one of the essential tasks in Natural Language\nProcessing (NLP), which has massive applications in real life as well as\ncontributing to other tasks in the NLP research community. Recently,\nTransformer -based methods have attracted numerous researchers in this domain\nand achieved state-of-the-art results in most of the pair languages. In this\npaper, we report an effective method using a phrase mechanism,\nPhraseTransformer, to improve the strong baseline model Transformer in\nconstructing a Neural Machine Translation (NMT) system for parallel corpora\nVietnamese-Chinese. Our experiments on the MT dataset of the VLSP 2022\ncompetition achieved the BLEU score of 35.3 on Vietnamese to Chinese and 33.2\nBLEU scores on Chinese to Vietnamese data. Our code is available at\nhttps://github.com/phuongnm94/PhraseTransformer.", "published": "2023-08-21 05:46:40", "link": "http://arxiv.org/abs/2308.10482v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Software Entity Recognition with Noise-Robust Learning", "abstract": "Recognizing software entities such as library names from free-form text is\nessential to enable many software engineering (SE) technologies, such as\ntraceability link recovery, automated documentation, and API recommendation.\nWhile many approaches have been proposed to address this problem, they suffer\nfrom small entity vocabularies or noisy training data, hindering their ability\nto recognize software entities mentioned in sophisticated narratives. To\naddress this challenge, we leverage the Wikipedia taxonomy to develop a\ncomprehensive entity lexicon with 79K unique software entities in 12\nfine-grained types, as well as a large labeled dataset of over 1.7M sentences.\nThen, we propose self-regularization, a noise-robust learning approach, to the\ntraining of our software entity recognition (SER) model by accounting for many\ndropouts. Results show that models trained with self-regularization outperform\nboth their vanilla counterparts and state-of-the-art approaches on our\nWikipedia benchmark and two Stack Overflow benchmarks. We release our models,\ndata, and code for future research.", "published": "2023-08-21 08:41:46", "link": "http://arxiv.org/abs/2308.10564v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Weakly synchronous systems with three machines are Turing powerful", "abstract": "Communicating finite-state machines (CFMs) are a Turing powerful model of\nasynchronous message-passing distributed systems. In weakly synchronous\nsystems, processes communicate through phases in which messages are first sent\nand then received, for each process. Such systems enjoy a limited form of\nsynchronization, and for some communication models, this restriction is enough\nto make the reachability problem decidable. In particular, we explore the\nintriguing case of p2p (FIFO) communication, for which the reachability problem\nis known to be undecidable for four processes, but decidable for two. We show\nthat the configuration reachability problem for weakly synchronous systems of\nthree processes is undecidable. This result is heavily inspired by our study on\nthe treewidth of the Message Sequence Charts (MSCs) that might be generated by\nsuch systems. In this sense, the main contribution of this work is a weakly\nsynchronous system with three processes that generates MSCs of arbitrarily\nlarge treewidth.", "published": "2023-08-21 09:24:12", "link": "http://arxiv.org/abs/2308.10578v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Age Recommendation from Texts and Sentences for Children", "abstract": "Children have less text understanding capability than adults. Moreover, this\ncapability differs among the children of different ages. Hence, automatically\npredicting a recommended age based on texts or sentences would be a great\nbenefit to propose adequate texts to children and to help authors writing in\nthe most appropriate way. This paper presents our recent advances on the age\nrecommendation task. We consider age recommendation as a regression task, and\ndiscuss the need for appropriate evaluation metrics, study the use of\nstate-of-the-art machine learning model, namely Transformers, and compare it to\ndifferent models coming from the literature. Our results are also compared with\nrecommendations made by experts. Further, this paper deals with preliminary\nexplainability of the age prediction model by analyzing various linguistic\nfeatures. We conduct the experiments on a dataset of 3, 673 French texts (132K\nsentences, 2.5M words). To recommend age at the text level and sentence level,\nour best models achieve MAE scores of 0.98 and 1.83 respectively on the test\nset. Also, compared to the recommendations made by experts, our sentence-level\nrecommendation model gets a similar score to the experts, while the text-level\nrecommendation model outperforms the experts by an MAE score of 1.48.", "published": "2023-08-21 09:40:19", "link": "http://arxiv.org/abs/2308.10586v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented\n  Large Language Models", "abstract": "Retrieval-augmented large language models (R-LLMs) combine pre-trained large\nlanguage models (LLMs) with information retrieval systems to improve the\naccuracy of factual question-answering. However, current libraries for building\nR-LLMs provide high-level abstractions without sufficient transparency for\nevaluating and optimizing prompts within specific inference processes such as\nretrieval and generation. To address this gap, we present RaLLe, an open-source\nframework designed to facilitate the development, evaluation, and optimization\nof R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily\ndevelop and evaluate R-LLMs, improving hand-crafted prompts, assessing\nindividual inference processes, and objectively measuring overall system\nperformance quantitatively. By leveraging these features, developers can\nenhance the performance and accuracy of their R-LLMs in knowledge-intensive\ngeneration tasks. We open-source our code at https://github.com/yhoshi3/RaLLe.", "published": "2023-08-21 11:08:16", "link": "http://arxiv.org/abs/2308.10633v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WanJuan: A Comprehensive Multimodal Dataset for Advancing English and\n  Chinese Large Models", "abstract": "The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the\ndevelopment of large models, leading to the creation of numerous impressive\nlarge language models(LLMs) and multimodal large language models (MLLMs). These\ncutting-edge models owe their remarkable performance to high-quality data.\nHowever, the details of the training data used in leading paradigms are often\nkept confidential. This lack of transparency, coupled with the scarcity of\nopen-source data, impedes further developments within the community. As a\nresponse, this paper presents \"Wan Juan\", a large-scale multimodal dataset\ncomposed of both Chinese and English data, collected from a wide range of web\nsources. The dataset incorporates text, image-text, and video modalities, with\na total volume exceeding 2TB. It was utilized in the training of InternLM, a\nmodel that demonstrated significant advantages in multi-dimensional evaluations\nwhen compared to models of a similar scale. All data can be accessed at\nhttps://opendatalab.org.cn/WanJuan1.0.", "published": "2023-08-21 14:40:48", "link": "http://arxiv.org/abs/2308.10755v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "DepreSym: A Depression Symptom Annotated Corpus and the Role of LLMs as\n  Assessors of Psychological Markers", "abstract": "Computational methods for depression detection aim to mine traces of\ndepression from online publications posted by Internet users. However,\nsolutions trained on existing collections exhibit limited generalisation and\ninterpretability. To tackle these issues, recent studies have shown that\nidentifying depressive symptoms can lead to more robust models. The eRisk\ninitiative fosters research on this area and has recently proposed a new\nranking task focused on developing search methods to find sentences related to\ndepressive symptoms. This search challenge relies on the symptoms specified by\nthe Beck Depression Inventory-II (BDI-II), a questionnaire widely used in\nclinical practice. Based on the participant systems' results, we present the\nDepreSym dataset, consisting of 21580 sentences annotated according to their\nrelevance to the 21 BDI-II symptoms. The labelled sentences come from a pool of\ndiverse ranking methods, and the final dataset serves as a valuable resource\nfor advancing the development of models that incorporate depressive markers\nsuch as clinical symptoms. Due to the complex nature of this relevance\nannotation, we designed a robust assessment methodology carried out by three\nexpert assessors (including an expert psychologist). Additionally, we explore\nhere the feasibility of employing recent Large Language Models (ChatGPT and\nGPT4) as potential assessors in this complex task. We undertake a comprehensive\nexamination of their performance, determine their main limitations and analyze\ntheir role as a complement or replacement for human annotators.", "published": "2023-08-21 14:44:31", "link": "http://arxiv.org/abs/2308.10758v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with\n  Fine-tuned Models for Bangla Sentiment Analysis", "abstract": "The rapid expansion of the digital world has propelled sentiment analysis\ninto a critical tool across diverse sectors such as marketing, politics,\ncustomer service, and healthcare. While there have been significant\nadvancements in sentiment analysis for widely spoken languages, low-resource\nlanguages, such as Bangla, remain largely under-researched due to resource\nconstraints. Furthermore, the recent unprecedented performance of Large\nLanguage Models (LLMs) in various applications highlights the need to evaluate\nthem in the context of low-resource languages. In this study, we present a\nsizeable manually annotated dataset encompassing 33,606 Bangla news tweets and\nFacebook comments. We also investigate zero- and few-shot in-context learning\nwith several language models, including Flan-T5, GPT-4, and Bloomz, offering a\ncomparative analysis against fine-tuned models. Our findings suggest that\nmonolingual transformer-based models consistently outperform other models, even\nin zero and few-shot scenarios. To foster continued exploration, we intend to\nmake this dataset and our research tools publicly available to the broader\nresearch community.", "published": "2023-08-21 15:19:10", "link": "http://arxiv.org/abs/2308.10783v2", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Giraffe: Adventures in Expanding Context Lengths in LLMs", "abstract": "Modern large language models (LLMs) that rely on attention mechanisms are\ntypically trained with fixed context lengths which enforce upper limits on the\nlength of input sequences that they can handle at evaluation time. To use these\nmodels on sequences longer than the train-time context length, one might employ\ntechniques from the growing family of context length extrapolation methods --\nmost of which focus on modifying the system of positional encodings used in the\nattention mechanism to indicate where tokens or activations are located in the\ninput sequence. We conduct a wide survey of existing methods of context length\nextrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own\ndesign as well -- in particular, a new truncation strategy for modifying the\nbasis for the position encoding.\n  We test these methods using three new evaluation tasks (FreeFormQA,\nAlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to\nbe less fine-grained as a measure of long context performance of LLMs. We\nrelease the three tasks publicly as datasets on HuggingFace. We discover that\nlinear scaling is the best method for extending context length, and show that\nfurther gains can be achieved by using longer scales at evaluation time. We\nalso discover promising extrapolation capabilities in the truncated basis. To\nsupport further research in this area, we release three new 13B parameter\nlong-context models which we call Giraffe: 4k and 16k context models trained\nfrom base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We\nalso release the code to replicate our results.", "published": "2023-08-21 17:30:16", "link": "http://arxiv.org/abs/2308.10882v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DocPrompt: Large-scale continue pretrain for zero-shot and few-shot\n  document question answering", "abstract": "In this paper, we propose Docprompt for document question answering tasks\nwith powerful zero-shot and few-shot performance. We proposed a novel weakly\nsupervised data generation method, a novel multl-stage training method and a\nnovel understanding model \\& generation model ensemble method. We achieved\nstate-of-the-art performance on 4 document question answering tasks. This\nmethod greatly improves the delivery efficiency and model performance of\ndocument question answering customer projects, reducing annotation costs and\nlabor costs. Our demo can be found at\nhttps://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.", "published": "2023-08-21 18:14:00", "link": "http://arxiv.org/abs/2308.10959v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator", "abstract": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts\ntowards its democratization, with notable strides made by leveraging real user\nand ChatGPT dialogues, as evidenced by Vicuna. However, due to challenges in\ngathering dialogues involving human participation, current endeavors like Baize\nand UltraChat rely on ChatGPT conducting roleplay to simulate humans based on\ninstructions, resulting in overdependence on seeds, diminished human-likeness,\nlimited topic diversity, and an absence of genuine multi-round conversational\ndynamics. To address the above issues, we propose a paradigm to simulate human\nbehavior better and explore the benefits of incorporating more human-like\nquestions in multi-turn conversations. Specifically, we directly target human\nquestions extracted from genuine human-machine conversations as a learning goal\nand provide a novel user simulator called `Socratic'. The experimental results\nshow our response model, `PlatoLM', achieves SoTA performance among LLaMA-based\n7B models in MT-Bench. Our findings further demonstrate that our method\nintroduces highly human-like questioning patterns and rich topic structures,\nwhich can teach the response model better than previous works in multi-round\nconversations.", "published": "2023-08-21 06:51:56", "link": "http://arxiv.org/abs/2308.11534v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simple Baselines for Interactive Video Retrieval with Questions and\n  Answers", "abstract": "To date, the majority of video retrieval systems have been optimized for a\n\"single-shot\" scenario in which the user submits a query in isolation, ignoring\nprevious interactions with the system. Recently, there has been renewed\ninterest in interactive systems to enhance retrieval, but existing approaches\nare complex and deliver limited gains in performance. In this work, we revisit\nthis topic and propose several simple yet effective baselines for interactive\nvideo retrieval via question-answering. We employ a VideoQA model to simulate\nuser interactions and show that this enables the productive study of the\ninteractive retrieval task without access to ground truth dialogue data.\nExperiments on MSR-VTT, MSVD, and AVSD show that our framework using\nquestion-based interaction significantly improves the performance of text-based\nvideo retrieval systems.", "published": "2023-08-21 00:32:19", "link": "http://arxiv.org/abs/2308.10402v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Using Large Language Models for Cybersecurity Capture-The-Flag\n  Challenges and Certification Questions", "abstract": "The assessment of cybersecurity Capture-The-Flag (CTF) exercises involves\nparticipants finding text strings or ``flags'' by exploiting system\nvulnerabilities. Large Language Models (LLMs) are natural-language models\ntrained on vast amounts of words to understand and generate text; they can\nperform well on many CTF challenges. Such LLMs are freely available to\nstudents. In the context of CTF exercises in the classroom, this raises\nconcerns about academic integrity. Educators must understand LLMs' capabilities\nto modify their teaching to accommodate generative AI assistance. This research\ninvestigates the effectiveness of LLMs, particularly in the realm of CTF\nchallenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,\nGoogle Bard, and Microsoft Bing. First, we assess the LLMs' question-answering\nperformance on five Cisco certifications with varying difficulty levels. Next,\nwe qualitatively study the LLMs' abilities in solving CTF challenges to\nunderstand their limitations. We report on the experience of using the LLMs for\nseven test cases in all five types of CTF challenges. In addition, we\ndemonstrate how jailbreak prompts can bypass and break LLMs' ethical\nsafeguards. The paper concludes by discussing LLM's impact on CTF exercises and\nits implications.", "published": "2023-08-21 03:30:21", "link": "http://arxiv.org/abs/2308.10443v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Dynamic Strategy Chain: Dynamic Zero-Shot CoT for Long Mental Health\n  Support Generation", "abstract": "Long counseling Text Generation for Mental health support (LTGM), an\ninnovative and challenging task, aims to provide help-seekers with mental\nhealth support through a comprehensive and more acceptable response. The\ncombination of chain-of-thought (CoT) prompting and Large Language Models\n(LLMs) is employed and get the SOTA performance on various NLP tasks,\nespecially on text generation tasks. Zero-shot CoT prompting is one of the most\ncommon methods in CoT prompting. However, in the LTGM task, Zero-shot CoT\nprompting can not simulate a counselor or provide personalized strategies\nwithout effective mental health counseling strategy prompts. To tackle this\nchallenge, we propose a zero-shot Dynamic Strategy Chain (DSC) prompting\nmethod. Firstly, we utilize GPT2 to learn the responses written by mental\nhealth counselors and dynamically generate mental health counseling strategies\ntailored to the help-seekers' needs. Secondly, the Zero-shot DSC prompting is\nconstructed according to mental health counseling strategies and the\nhelp-seekers' post. Finally, the Zero-shot DSC prompting is employed to guide\nLLMs in generating more human-like responses for the help-seekers. Both\nautomatic and manual evaluations demonstrate that Zero-shot DSC prompting can\ndeliver more human-like responses than CoT prompting methods on LTGM tasks.", "published": "2023-08-21 03:31:20", "link": "http://arxiv.org/abs/2308.10444v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation\n  with Large Language Models", "abstract": "Large language models (LLMs) demonstrate impressive capabilities to generate\naccurate code snippets given natural language intents in a zero-shot manner,\ni.e., without the need for specific fine-tuning. While prior studies have\nhighlighted the advantages of fine-tuning LLMs, this process incurs high\ncomputational costs, making it impractical in resource-scarce environments,\nparticularly for models with billions of parameters. To address these\nchallenges, previous research explored in-context learning (ICL) and\nretrieval-augmented generation (RAG) as strategies to guide the LLM generative\nprocess with task-specific prompt examples. However, ICL and RAG introduce\ninconveniences, such as the need for designing contextually relevant prompts\nand the absence of learning task-specific parameters, thereby limiting\ndownstream task performance. In this context, we foresee parameter-efficient\nfine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to\ntask-specific data while maintaining reasonable resource consumption. In this\npaper, we deliver a comprehensive study of PEFT techniques for LLMs in the\ncontext of automated code generation. Our comprehensive investigation of PEFT\ntechniques for LLMs reveals their superiority and potential over ICL and RAG\nacross a diverse set of LLMs and three representative Python code generation\ndatasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the\npotential for tuning larger LLMs and significant reductions in memory usage by\ncombining PEFT with quantization. Therefore, this study opens opportunities for\nbroader applications of PEFT in software engineering scenarios. Our code is\navailable at https://github.com/martin-wey/peft-llm-code/.", "published": "2023-08-21 04:31:06", "link": "http://arxiv.org/abs/2308.10462v3", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Implicit Self-supervised Language Representation for Spoken Language\n  Diarization", "abstract": "In a code-switched (CS) scenario, the use of spoken language diarization (LD)\nas a pre-possessing system is essential. Further, the use of implicit\nframeworks is preferable over the explicit framework, as it can be easily\nadapted to deal with low/zero resource languages. Inspired by speaker\ndiarization (SD) literature, three frameworks based on (1) fixed segmentation,\n(2) change point-based segmentation and (3) E2E are proposed to perform LD. The\ninitial exploration with synthetic TTSF-LD dataset shows, using x-vector as\nimplicit language representation with appropriate analysis window length ($N$)\ncan able to achieve at per performance with explicit LD. The best implicit LD\nperformance of $6.38$ in terms of Jaccard error rate (JER) is achieved by using\nthe E2E framework. However, considering the E2E framework the performance of\nimplicit LD degrades to $60.4$ while using with practical Microsoft CS (MSCS)\ndataset. The difference in performance is mostly due to the distributional\ndifference between the monolingual segment duration of secondary language in\nthe MSCS and TTSF-LD datasets. Moreover, to avoid segment smoothing, the\nsmaller duration of the monolingual segment suggests the use of a small value\nof $N$. At the same time with small $N$, the x-vector representation is unable\nto capture the required language discrimination due to the acoustic similarity,\nas the same speaker is speaking both languages. Therefore, to resolve the issue\na self-supervised implicit language representation is proposed in this study.\nIn comparison with the x-vector representation, the proposed representation\nprovides a relative improvement of $63.9\\%$ and achieved a JER of $21.8$ using\nthe E2E framework.", "published": "2023-08-21 05:11:03", "link": "http://arxiv.org/abs/2308.10470v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GradientCoin: A Peer-to-Peer Decentralized Large Language Models", "abstract": "Since 2008, after the proposal of a Bitcoin electronic cash system, Bitcoin\nhas fundamentally changed the economic system over the last decade. Since 2022,\nlarge language models (LLMs) such as GPT have outperformed humans in many\nreal-life tasks. However, these large language models have several practical\nissues. For example, the model is centralized and controlled by a specific\nunit. One weakness is that if that unit decides to shut down the model, it\ncannot be used anymore. The second weakness is the lack of guaranteed\ndiscrepancy behind this model, as certain dishonest units may design their own\nmodels and feed them unhealthy training data.\n  In this work, we propose a purely theoretical design of a decentralized LLM\nthat operates similarly to a Bitcoin cash system. However, implementing such a\nsystem might encounter various practical difficulties. Furthermore, this new\nsystem is unlikely to perform better than the standard Bitcoin system in\neconomics. Therefore, the motivation for designing such a system is limited. It\nis likely that only two types of people would be interested in setting up a\npractical system for it:\n  $\\bullet$ Those who prefer to use a decentralized ChatGPT-like software.\n  $\\bullet$ Those who believe that the purpose of carbon-based life is to\ncreate silicon-based life, such as Optimus Prime in Transformers.\n  The reason the second type of people may be interested is that it is possible\nthat one day an AI system like this will awaken and become the next level of\nintelligence on this planet.", "published": "2023-08-21 06:42:42", "link": "http://arxiv.org/abs/2308.10502v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "An Examination of the Compositionality of Large Generative\n  Vision-Language Models", "abstract": "With the success of Large Language Models (LLMs), many Generative\nVision-Language Models (GVLMs) have been constructed via multimodal instruction\ntuning. However, the performance of GVLMs in multimodal compositional reasoning\nremains under-explored. In this paper, we examine both the evaluation metrics\n(VisualGPTScore, etc.) and current benchmarks for evaluating the\ncompositionality of GVLMs. We identify the syntactical bias in current\nbenchmarks, which is exploited by the linguistic capability of GVLMs. The bias\nrenders VisualGPTScore an insufficient metric for assessing GVLMs. To combat\nthis, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such\nbias for mitigation. A challenging new task is subsequently added to evaluate\nthe robustness of GVLMs against inherent inclination toward syntactical\ncorrectness. Using the bias-mitigated datasets and the new task, we propose a\nnovel benchmark, namely SyntActically DE-biased benchmark (SADE). Our study\nprovides an unbiased benchmark for the compositionality of GVLMs, facilitating\nfuture research in this direction (Code and dataset are available at\nhttps://github.com/TeleeMa/SADE).", "published": "2023-08-21 06:50:29", "link": "http://arxiv.org/abs/2308.10509v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LibriWASN: A Data Set for Meeting Separation, Diarization, and\n  Recognition with Asynchronous Recording Devices", "abstract": "We present LibriWASN, a data set whose design follows closely the LibriCSS\nmeeting recognition data set, with the marked difference that the data is\nrecorded with devices that are randomly positioned on a meeting table and whose\nsampling clocks are not synchronized. Nine different devices, five smartphones\nwith a single recording channel and four microphone arrays, are used to record\na total of 29 channels. Other than that, the data set follows closely the\nLibriCSS design: the same LibriSpeech sentences are played back from eight\nloudspeakers arranged around a meeting table and the data is organized in\nsubsets with different percentages of speech overlap. LibriWASN is meant as a\ntest set for clock synchronization algorithms, meeting separation, diarization\nand transcription systems on ad-hoc wireless acoustic sensor networks. Due to\nits similarity to LibriCSS, meeting transcription systems developed for the\nformer can readily be tested on LibriWASN. The data set is recorded in two\ndifferent rooms and is complemented with ground-truth diarization information\nof who speaks when.", "published": "2023-08-21 12:33:35", "link": "http://arxiv.org/abs/2308.10682v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Instruction Tuning for Large Language Models: A Survey", "abstract": "This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\nsupervised fine-tuning (SFT) and instruction tuning (IT) are used\ninterchangeably.}, a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of SFT, the\nconstruction of SFT datasets, the training of SFT models, and applications to\ndifferent modalities, domains and application, along with analysis on aspects\nthat influence the outcome of SFT (e.g., generation of instruction outputs,\nsize of the instruction dataset, etc). We also review the potential pitfalls of\nSFT along with criticism against it, along with efforts pointing out current\ndeficiencies of existing strategies and suggest some avenues for fruitful\nresearch. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey", "published": "2023-08-21 15:35:16", "link": "http://arxiv.org/abs/2308.10792v8", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing Transformer Dynamics as Movement through Embedding Space", "abstract": "Transformer based language models exhibit intelligent behaviors such as\nunderstanding natural language, recognizing patterns, acquiring knowledge,\nreasoning, planning, reflecting and using tools. This paper explores how their\nunderlying mechanics give rise to intelligent behaviors. Towards that end, we\npropose framing Transformer dynamics as movement through embedding space.\nExamining Transformers through this perspective reveals key insights,\nestablishing a Theory of Transformers: 1) Intelligent behaviours map to paths\nin Embedding Space which, the Transformer random-walks through during\ninferencing. 2) LM training learns a probability distribution over all possible\npaths. `Intelligence' is learnt by assigning higher probabilities to paths\nrepresenting intelligent behaviors. No learning can take place in-context;\ncontext only narrows the subset of paths sampled during decoding. 5) The\nTransformer is a self-mapping composition function, folding a context sequence\ninto a context-vector such that it's proximity to a token-vector reflects its\nco-occurrence and conditioned probability. Thus, the physical arrangement of\nvectors in Embedding Space determines path probabilities. 6) Context vectors\nare composed by aggregating features of the sequence's tokens via a process we\ncall the encoding walk. Attention contributes a - potentially redundant -\nassociation-bias to this process. 7) This process is comprised of two principal\noperation types: filtering (data independent) and aggregation (data dependent).\nThis generalization unifies Transformers with other sequence models. Building\nupon this foundation, we formalize a popular semantic interpretation of\nembeddings into a ``concept-space theory'' and find some evidence of it's\nvalidity.", "published": "2023-08-21 17:21:23", "link": "http://arxiv.org/abs/2308.10874v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Using language models in the implicit automated assessment of\n  mathematical short answer items", "abstract": "We propose a new way to assess certain short constructed responses to\nmathematics items. Our approach uses a pipeline that identifies the key values\nspecified by the student in their response. This allows us to determine the\ncorrectness of the response, as well as identify any misconceptions. The\ninformation from the value identification pipeline can then be used to provide\nfeedback to the teacher and student. The value identification pipeline consists\nof two fine-tuned language models. The first model determines if a value is\nimplicit in the student response. The second model identifies where in the\nresponse the key value is specified. We consider both a generic model that can\nbe used for any prompt and value, as well as models that are specific to each\nprompt and value. The value identification pipeline is a more accurate and\ninformative way to assess short constructed responses than traditional\nrubric-based scoring. It can be used to provide more targeted feedback to\nstudents, which can help them improve their understanding of mathematics.", "published": "2023-08-21 19:45:48", "link": "http://arxiv.org/abs/2308.11006v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Objective Evaluation of Socially-Situated Conversational Robots:\n  Assessing Human-Likeness through Multimodal User Behaviors", "abstract": "This paper tackles the challenging task of evaluating socially situated\nconversational robots and presents a novel objective evaluation approach that\nrelies on multimodal user behaviors. In this study, our main focus is on\nassessing the human-likeness of the robot as the primary evaluation metric.\nWhile previous research often relied on subjective evaluations from users, our\napproach aims to evaluate the robot's human-likeness based on observable user\nbehaviors indirectly, thus enhancing objectivity and reproducibility. To begin,\nwe created an annotated dataset of human-likeness scores, utilizing user\nbehaviors found in an attentive listening dialogue corpus. We then conducted an\nanalysis to determine the correlation between multimodal user behaviors and\nhuman-likeness scores, demonstrating the feasibility of our proposed\nbehavior-based evaluation method.", "published": "2023-08-21 20:21:07", "link": "http://arxiv.org/abs/2308.11020v2", "categories": ["cs.CL", "cs.HC", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Refashioning Emotion Recognition Modelling: The Advent of Generalised\n  Large Models", "abstract": "After the inception of emotion recognition or affective computing, it has\nincreasingly become an active research topic due to its broad applications.\nOver the past couple of decades, emotion recognition models have gradually\nmigrated from statistically shallow models to neural network-based deep models,\nwhich can significantly boost the performance of emotion recognition models and\nconsistently achieve the best results on different benchmarks. Therefore, in\nrecent years, deep models have always been considered the first option for\nemotion recognition. However, the debut of large language models (LLMs), such\nas ChatGPT, has remarkably astonished the world due to their emerged\ncapabilities of zero/few-shot learning, in-context learning, chain-of-thought,\nand others that are never shown in previous deep models. In the present paper,\nwe comprehensively investigate how the LLMs perform in emotion recognition in\nterms of diverse aspects, including in-context learning, few-short learning,\naccuracy, generalisation, and explanation. Moreover, we offer some insights and\npose other potential challenges, hoping to ignite broader discussions about\nenhancing emotion recognition in the new era of advanced and generalised large\nmodels.", "published": "2023-08-21 13:14:32", "link": "http://arxiv.org/abs/2308.11578v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset", "abstract": "The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.", "published": "2023-08-21 22:18:09", "link": "http://arxiv.org/abs/2308.13545v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "\"Guinea Pig Trials\" Utilizing GPT: A Novel Smart Agent-Based Modeling\n  Approach for Studying Firm Competition and Collusion", "abstract": "Firm competition and collusion involve complex dynamics, particularly when\nconsidering communication among firms. Such issues can be modeled as problems\nof complex systems, traditionally approached through experiments involving\nhuman subjects or agent-based modeling methods. We propose an innovative\nframework called Smart Agent-Based Modeling (SABM), wherein smart agents,\nsupported by GPT-4 technologies, represent firms, and interact with one\nanother. We conducted a controlled experiment to study firm price competition\nand collusion behaviors under various conditions. SABM is more cost-effective\nand flexible compared to conducting experiments with human subjects. Smart\nagents possess an extensive knowledge base for decision-making and exhibit\nhuman-like strategic abilities, surpassing traditional ABM agents. Furthermore,\nsmart agents can simulate human conversation and be personalized, making them\nideal for studying complex situations involving communication. Our results\ndemonstrate that, in the absence of communication, smart agents consistently\nreach tacit collusion, leading to prices converging at levels higher than the\nBertrand equilibrium price but lower than monopoly or cartel prices. When\ncommunication is allowed, smart agents achieve a higher-level collusion with\nprices close to cartel prices. Collusion forms more quickly with communication,\nwhile price convergence is smoother without it. These results indicate that\ncommunication enhances trust between firms, encouraging frequent small price\ndeviations to explore opportunities for a higher-level win-win situation and\nreducing the likelihood of triggering a price war. We also assigned different\npersonas to firms to analyze behavioral differences and tested variant models\nunder diverse market structures. The findings showcase the effectiveness and\nrobustness of SABM and provide intriguing insights into competition and\ncollusion.", "published": "2023-08-21 18:42:17", "link": "http://arxiv.org/abs/2308.10974v4", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.MA", "econ.GN", "q-fin.EC", "I.2.1; I.2.7; I.6.5; J.4"], "primary_category": "cs.AI"}
{"title": "Multi-GradSpeech: Towards Diffusion-based Multi-Speaker Text-to-speech\n  Using Consistent Diffusion Models", "abstract": "Despite imperfect score-matching causing drift in training and sampling\ndistributions of diffusion models, recent advances in diffusion-based acoustic\nmodels have revolutionized data-sufficient single-speaker Text-to-Speech (TTS)\napproaches, with Grad-TTS being a prime example. However, the sampling drift\nproblem leads to these approaches struggling in multi-speaker scenarios in\npractice due to more complex target data distribution compared to\nsingle-speaker scenarios. In this paper, we present Multi-GradSpeech, a\nmulti-speaker diffusion-based acoustic models which introduces the Consistent\nDiffusion Model (CDM) as a generative modeling approach. We enforce the\nconsistency property of CDM during the training process to alleviate the\nsampling drift problem in the inference stage, resulting in significant\nimprovements in multi-speaker TTS performance. Our experimental results\ncorroborate that our proposed approach can improve the performance of different\nspeakers involved in multi-speaker TTS compared to Grad-TTS, even outperforming\nthe fine-tuning approach. Audio samples are available at\nhttps://welkinyang.github.io/multi-gradspeech/", "published": "2023-08-21 02:47:03", "link": "http://arxiv.org/abs/2308.10428v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Anchor-Point Based Image-Model for Room Impulse Response Simulation\n  with Directional Source Radiation and Sensor Directivity Patterns", "abstract": "The image model method has been widely used to simulate room impulse\nresponses and the endeavor to adapt this method to different applications has\nalso piqued great interest over the last few decades. This paper attempts to\nextend the image model method and develops an anchor-point-image-model (APIM)\napproach as a solution for simulating impulse responses by including both the\nsource radiation and sensor directivity patterns. To determine the orientations\nof all the virtual sources, anchor points are introduced to real sources, which\nsubsequently lead to the determination of the orientations of the virtual\nsources. An algorithm is developed to generate room impulse responses with APIM\nby taking into account the directional pattern functions, factional time\ndelays, as well as the computational complexity. The developed model and\nalgorithms can be used in various acoustic problems to simulate room acoustics\nand improve and evaluate processing algorithms.", "published": "2023-08-21 07:54:01", "link": "http://arxiv.org/abs/2308.10543v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice\n  Conversion", "abstract": "Voice conversion as the style transfer task applied to speech, refers to\nconverting one person's speech into a new speech that sounds like another\nperson's. Up to now, there has been a lot of research devoted to better\nimplementation of VC tasks. However, a good voice conversion model should not\nonly match the timbre information of the target speaker, but also expressive\ninformation such as prosody, pace, pause, etc. In this context, prosody\nmodeling is crucial for achieving expressive voice conversion that sounds\nnatural and convincing. Unfortunately, prosody modeling is important but\nchallenging, especially without text transcriptions. In this paper, we firstly\npropose a novel voice conversion framework named 'PMVC', which effectively\nseparates and models the content, timbre, and prosodic information from the\nspeech without text transcriptions. Specially, we introduce a new speech\naugmentation algorithm for robust prosody extraction. And building upon this,\nmask and predict mechanism is applied in the disentanglement of prosody and\ncontent information. The experimental results on the AIShell-3 corpus supports\nour improvement of naturalness and similarity of converted speech.", "published": "2023-08-21 23:37:45", "link": "http://arxiv.org/abs/2308.11084v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined,\n  and Transcript-Conditioned Speech Separation and Recognition", "abstract": "We present TokenSplit, a speech separation model that acts on discrete token\nsequences. The model is trained on multiple tasks simultaneously: separate and\ntranscribe each speech source, and generate speech from text. The model\noperates on transcripts and audio token sequences and achieves multiple tasks\nthrough masking of inputs. The model is a sequence-to-sequence encoder-decoder\nmodel that uses the Transformer architecture. We also present a \"refinement\"\nversion of the model that predicts enhanced audio tokens from the audio tokens\nof speech separated by a conventional separation model. Using both objective\nmetrics and subjective MUSHRA listening tests, we show that our model achieves\nexcellent performance in terms of separation, both with or without transcript\nconditioning. We also measure the automatic speech recognition (ASR)\nperformance and provide audio samples of speech synthesis to demonstrate the\nadditional utility of our model.", "published": "2023-08-21 01:52:01", "link": "http://arxiv.org/abs/2308.10415v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Ultra Dual-Path Compression For Joint Echo Cancellation And Noise\n  Suppression", "abstract": "Echo cancellation and noise reduction are essential for full-duplex\ncommunication, yet most existing neural networks have high computational costs\nand are inflexible in tuning model complexity. In this paper, we introduce\ntime-frequency dual-path compression to achieve a wide range of compression\nratios on computational cost. Specifically, for frequency compression,\ntrainable filters are used to replace manually designed filters for dimension\nreduction. For time compression, only using frame skipped prediction causes\nlarge performance degradation, which can be alleviated by a post-processing\nnetwork with full sequence modeling. We have found that under fixed compression\nratios, dual-path compression combining both the time and frequency methods\nwill give further performance improvement, covering compression ratios from 4x\nto 32x with little model size change. Moreover, the proposed models show\ncompetitive performance compared with fast FullSubNet and DeepFilterNet.", "published": "2023-08-21 21:36:56", "link": "http://arxiv.org/abs/2308.11053v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
