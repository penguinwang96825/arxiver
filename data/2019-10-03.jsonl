{"title": "Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge\n  Transfer Networks", "abstract": "Goal-oriented dialogue systems are now being widely adopted in industry where\nit is of key importance to maintain a rapid prototyping cycle for new products\nand domains. Data-driven dialogue system development has to be adapted to meet\nthis requirement --- therefore, reducing the amount of data and annotations\nnecessary for training such systems is a central research problem.\n  In this paper, we present the Dialogue Knowledge Transfer Network (DiKTNet),\na state-of-the-art approach to goal-oriented dialogue generation which only\nuses a few example dialogues (i.e. few-shot learning), none of which has to be\nannotated. We achieve this by performing a 2-stage training. Firstly, we\nperform unsupervised dialogue representation pre-training on a large source of\ngoal-oriented dialogues in multiple domains, the MetaLWOz corpus. Secondly, at\nthe transfer stage, we train DiKTNet using this representation together with 2\nother textual knowledge sources with different levels of generality: ELMo\nencoder and the main dataset's source domains.\n  Our main dataset is the Stanford Multi-Domain dialogue corpus. We evaluate\nour model on it in terms of BLEU and Entity F1 scores, and show that our\napproach significantly and consistently improves upon a series of baseline\nmodels as well as over the previous state-of-the-art dialogue generation model,\nZSDG. The improvement upon the latter --- up to 10% in Entity F1 and the\naverage of 3% in BLEU score --- is achieved using only the equivalent of 10% of\nZSDG's in-domain training data.", "published": "2019-10-03 05:08:15", "link": "http://arxiv.org/abs/1910.01302v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Topic-aware Pointer-Generator Networks for Summarizing Spoken\n  Conversations", "abstract": "Due to the lack of publicly available resources, conversation summarization\nhas received far less attention than text summarization. As the purpose of\nconversations is to exchange information between at least two interlocutors,\nkey information about a certain topic is often scattered and spanned across\nmultiple utterances and turns from different speakers. This phenomenon is more\npronounced during spoken conversations, where speech characteristics such as\nbackchanneling and false-starts might interrupt the topical flow. Moreover,\ntopic diffusion and (intra-utterance) topic drift are also more common in\nhuman-to-human conversations. Such linguistic characteristics of dialogue\ntopics make sentence-level extractive summarization approaches used in spoken\ndocuments ill-suited for summarizing conversations. Pointer-generator networks\nhave effectively demonstrated its strength at integrating extractive and\nabstractive capabilities through neural modeling in text summarization. To the\nbest of our knowledge, to date no one has adopted it for summarizing\nconversations. In this work, we propose a topic-aware architecture to exploit\nthe inherent hierarchical structure in conversations to further adapt the\npointer-generator model. Our approach significantly outperforms competitive\nbaselines, achieves more efficient learning outcomes, and attains more robust\nperformance.", "published": "2019-10-03 07:48:32", "link": "http://arxiv.org/abs/1910.01335v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Understanding of Medical Randomized Controlled Trials by\n  Conclusion Generation", "abstract": "Randomized controlled trials (RCTs) represent the paramount evidence of\nclinical medicine. Using machines to interpret the massive amount of RCTs has\nthe potential of aiding clinical decision-making. We propose a RCT conclusion\ngeneration task from the PubMed 200k RCT sentence classification dataset to\nexamine the effectiveness of sequence-to-sequence models on understanding RCTs.\nWe first build a pointer-generator baseline model for conclusion generation.\nThen we fine-tune the state-of-the-art GPT-2 language model, which is\npre-trained with general domain data, for this new medical domain task. Both\nautomatic and human evaluation show that our GPT-2 fine-tuned models achieve\nimproved quality and correctness in the generated conclusions compared to the\nbaseline pointer-generator model. Further inspection points out the limitations\nof this current approach and future directions to explore.", "published": "2019-10-03 13:35:00", "link": "http://arxiv.org/abs/1910.01462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Color Terminology Across Thousands of Languages", "abstract": "There is an extensive history of scholarship into what constitutes a \"basic\"\ncolor term, as well as a broadly attested acquisition sequence of basic color\nterms across many languages, as articulated in the seminal work of Berlin and\nKay (1969). This paper employs a set of diverse measures on massively\ncross-linguistic data to operationalize and critique the Berlin and Kay color\nterm hypotheses. Collectively, the 14 empirically-grounded computational\nlinguistic metrics we design---as well as their aggregation---correlate\nstrongly with both the Berlin and Kay basic/secondary color term partition\n(gamma=0.96) and their hypothesized universal acquisition sequence. The\nmeasures and result provide further empirical evidence from computational\nlinguistics in support of their claims, as well as additional nuance: they\nsuggest treating the partition as a spectrum instead of a dichotomy.", "published": "2019-10-03 14:46:22", "link": "http://arxiv.org/abs/1910.01531v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting UMLS Concepts from Medical Text Using General and\n  Domain-Specific Deep Learning Models", "abstract": "Entity recognition is a critical first step to a number of clinical NLP\napplications, such as entity linking and relation extraction. We present the\nfirst attempt to apply state-of-the-art entity recognition approaches on a\nnewly released dataset, MedMentions. This dataset contains over 4000 biomedical\nabstracts, annotated for UMLS semantic types. In comparison to existing\ndatasets, MedMentions contains a far greater number of entity types, and thus\nrepresents a more challenging but realistic scenario in a real-world setting.\nWe explore a number of relevant dimensions, including the use of contextual\nversus non-contextual word embeddings, general versus domain-specific\nunsupervised pre-training, and different deep learning architectures. We\ncontrast our results against the well-known i2b2 2010 entity recognition\ndataset, and propose a new method to combine general and domain-specific\ninformation. While producing a state-of-the-art result for the i2b2 2010 task\n(F1 = 0.90), our results on MedMentions are significantly lower (F1 = 0.63),\nsuggesting there is still plenty of opportunity for improvement on this new\ndata.", "published": "2019-10-03 01:51:17", "link": "http://arxiv.org/abs/1910.01274v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Hitachi at MRP 2019: Unified Encoder-to-Biaffine Network for\n  Cross-Framework Meaning Representation Parsing", "abstract": "This paper describes the proposed system of the Hitachi team for the\nCross-Framework Meaning Representation Parsing (MRP 2019) shared task. In this\nshared task, the participating systems were asked to predict nodes, edges and\ntheir attributes for five frameworks, each with different order of\n\"abstraction\" from input tokens. We proposed a unified encoder-to-biaffine\nnetwork for all five frameworks, which effectively incorporates a shared\nencoder to extract rich input features, decoder networks to generate anchorless\nnodes in UCCA and AMR, and biaffine networks to predict edges. Our system was\nranked fifth with the macro-averaged MRP F1 score of 0.7604, and outperformed\nthe baseline unified transition-based MRP. Furthermore, post-evaluation\nexperiments showed that we can boost the performance of the proposed system by\nincorporating multi-task learning, whereas the baseline could not. These imply\nefficacy of incorporating the biaffine network to the shared architecture for\nMRP and that learning heterogeneous meaning representations at once can boost\nthe system performance.", "published": "2019-10-03 04:27:49", "link": "http://arxiv.org/abs/1910.01299v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mapping (Dis-)Information Flow about the MH17 Plane Crash", "abstract": "Digital media enables not only fast sharing of information, but also\ndisinformation. One prominent case of an event leading to circulation of\ndisinformation on social media is the MH17 plane crash. Studies analysing the\nspread of information about this event on Twitter have focused on small,\nmanually annotated datasets, or used proxys for data annotation. In this work,\nwe examine to what extent text classifiers can be used to label data for\nsubsequent content analysis, in particular we focus on predicting pro-Russian\nand pro-Ukrainian Twitter content related to the MH17 plane crash. Even though\nwe find that a neural classifier improves over a hashtag based baseline,\nlabeling pro-Russian and pro-Ukrainian content with high precision remains a\nchallenging problem. We provide an error analysis underlining the difficulty of\nthe task and identify factors that might help improve classification in future\nwork. Finally, we show how the classifier can facilitate the annotation task\nfor human annotators.", "published": "2019-10-03 09:00:58", "link": "http://arxiv.org/abs/1910.01363v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Complex networks based word embeddings", "abstract": "Most of the time, the first step to learn word embeddings is to build a word\nco-occurrence matrix. As such matrices are equivalent to graphs, complex\nnetworks theory can naturally be used to deal with such data. In this paper, we\nconsider applying community detection, a main tool of this field, to the\nco-occurrence matrix corresponding to a huge corpus. Community structure is\nused as a way to reduce the dimensionality of the initial space. Using this\ncommunity structure, we propose a method to extract word embeddings that are\ncomparable to the state-of-the-art approaches.", "published": "2019-10-03 14:12:38", "link": "http://arxiv.org/abs/1910.01489v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Character Feature Engineering for Japanese Word Segmentation", "abstract": "On word segmentation problems, machine learning architecture engineering\noften draws attention. The problem representation itself, however, has remained\nalmost static as either word lattice ranking or character sequence tagging, for\nat least two decades. The latter of-ten shows stronger predictive power than\nthe former for out-of-vocabulary (OOV) issue. When the issue escalating to\nrapid adaptation, which is a common scenario for industrial applications,\nactive learning of partial annotations or re-training with additional lexical\nre-sources is usually applied, however, from a somewhat word-based perspective.\nNot only it is uneasy for end-users to comply with linguistically consistent\nword boundary decisions, but also the risk/cost of forking models permanently\nwith estimated weights is seldom affordable. To overcome the obstacle, this\nwork provides an alternative, which uses linguistic intuition about character\ncompositions, such that a sophisticated feature set and its derived scheme can\nenable dynamic lexicon expansion with the model remaining intact. Experiment\nresults suggest that the proposed solution, with or without external lexemes,\nperforms competitively in terms of F1 score and OOV recall across various\ndatasets.", "published": "2019-10-03 23:39:31", "link": "http://arxiv.org/abs/1910.01761v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Zero-Inflated Quality Estimation Model For Automatic Speech\n  Recognition System", "abstract": "The performances of automatic speech recognition (ASR) systems are usually\nevaluated by the metric word error rate (WER) when the manually transcribed\ndata are provided, which are, however, expensively available in the real\nscenario. In addition, the empirical distribution of WER for most ASR systems\nusually tends to put a significant mass near zero, making it difficult to\nsimulate with a single continuous distribution. In order to address the two\nissues of ASR quality estimation (QE), we propose a novel neural zero-inflated\nmodel to predict the WER of the ASR result without transcripts. We design a\nneural zero-inflated beta regression on top of a bidirectional transformer\nlanguage model conditional on speech features (speech-BERT). We adopt the\npre-training strategy of token level mask language modeling for speech-BERT as\nwell, and further fine-tune with our zero-inflated layer for the mixture of\ndiscrete and continuous outputs. The experimental results show that our\napproach achieves better performance on WER prediction in the metrics of\nPearson and MAE, compared with most existed quality estimation algorithms for\nASR or machine translation.", "published": "2019-10-03 03:19:55", "link": "http://arxiv.org/abs/1910.01289v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TexTrolls: Identifying Russian Trolls on Twitter from a Textual\n  Perspective", "abstract": "The online new emerging suspicious users, that usually are called trolls, are\none of the main sources of hate, fake, and deceptive online messages. Some\nagendas are utilizing these harmful users to spread incitement tweets, and as a\nconsequence, the audience get deceived. The challenge in detecting such\naccounts is that they conceal their identities which make them disguised in\nsocial media, adding more difficulty to identify them using just their social\nnetwork information. Therefore, in this paper, we propose a text-based approach\nto detect the online trolls such as those that were discovered during the US\n2016 presidential elections. Our approach is mainly based on textual features\nwhich utilize thematic information, and profiling features to identify the\naccounts from their way of writing tweets. We deduced the thematic information\nin a unsupervised way and we show that coupling them with the textual features\nenhanced the performance of the proposed model. In addition, we find that the\nproposed profiling features perform the best comparing to the textual features.", "published": "2019-10-03 07:56:52", "link": "http://arxiv.org/abs/1910.01340v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "CLEVRER: CoLlision Events for Video REpresentation and Reasoning", "abstract": "The ability to reason about temporal and causal events from videos lies at\nthe core of human intelligence. Most video reasoning benchmarks, however, focus\non pattern recognition from complex visual and language input, instead of on\ncausal structure. We study the complementary problem, exploring the temporal\nand causal structures behind videos of objects with simple visual appearance.\nTo this end, we introduce the CoLlision Events for Video REpresentation and\nReasoning (CLEVRER), a diagnostic video dataset for systematic evaluation of\ncomputational models on a wide range of reasoning tasks. Motivated by the\ntheory of human casual judgment, CLEVRER includes four types of questions:\ndescriptive (e.g., \"what color\"), explanatory (\"what is responsible for\"),\npredictive (\"what will happen next\"), and counterfactual (\"what if\"). We\nevaluate various state-of-the-art models for visual reasoning on our benchmark.\nWhile these models thrive on the perception-based task (descriptive), they\nperform poorly on the causal tasks (explanatory, predictive and\ncounterfactual), suggesting that a principled approach for causal reasoning\nshould incorporate the capability of both perceiving complex visual and\nlanguage inputs, and understanding the underlying dynamics and causal\nrelations. We also study an oracle model that explicitly combines these\ncomponents via symbolic representations.", "published": "2019-10-03 13:16:36", "link": "http://arxiv.org/abs/1910.01442v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Semi-Supervised Generative Modeling for Controllable Speech Synthesis", "abstract": "We present a novel generative model that combines state-of-the-art neural\ntext-to-speech (TTS) with semi-supervised probabilistic latent variable models.\nBy providing partial supervision to some of the latent variables, we are able\nto force them to take on consistent and interpretable purposes, which\npreviously hasn't been possible with purely unsupervised TTS models. We\ndemonstrate that our model is able to reliably discover and control important\nbut rarely labelled attributes of speech, such as affect and speaking rate,\nwith as little as 1% (30 minutes) supervision. Even at such low supervision\nlevels we do not observe a degradation of synthesis quality compared to a\nstate-of-the-art baseline. Audio samples are available on the web.", "published": "2019-10-03 20:18:45", "link": "http://arxiv.org/abs/1910.01709v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Types for Parallel Complexity in the Pi-calculus", "abstract": "Type systems as a way to control or analyze programs have been largely\nstudied in the context of functional programming languages. Some of those work\nallow to extract from a typing derivation for a program a complexity bound on\nthis program. We present how to adapt this result for parallel complexity in\nthe pi-calculus, as a model of concurrency and parallel communication. We study\ntwo notions of time complexity: the total computation time without parallelism\n(the work) and the computation time under maximal parallelism (the span). We\ndefine reduction relations in the pi-calculus to capture those two notions, and\nwe present two type systems from which one can extract a complexity bound on a\nprocess. The type systems are inspired by input/output types and size types,\nwith temporal information about communications.", "published": "2019-10-03 10:43:43", "link": "http://arxiv.org/abs/1910.02145v1", "categories": ["cs.LO", "cs.CC", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Midi Miner -- A Python library for tonal tension and track\n  classification", "abstract": "We present a Python library, called Midi Miner, that can calculate tonal\ntension and classify different tracks. MIDI (Music Instrument Digital\nInterface) is a hardware and software standard for communicating musical events\nbetween digital music devices. It is often used for tasks such as music\nrepresentation, communication between devices, and even music generation [5].\nTension is an essential element of the music listening experience, which can\ncome from a number of musical features including timbre, loudness and harmony\n[3]. Midi Miner provides a Python implementation for the tonal tension model\nbased on the spiral array [1] as presented by Herremans and Chew [4]. Midi\nMiner also performs key estimation and includes a track classifier that can\ndisentangle melody, bass, and harmony tracks. Even though tracks are often\nseparated in MIDI files, the musical function of each track is not always\nclear. The track classifier keeps the identified tracks and discards messy\ntracks, which can enable further analysis and training tasks.", "published": "2019-10-03 08:09:55", "link": "http://arxiv.org/abs/1910.02049v2", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Convolutional Neural Networks for Speech Controlled Prosthetic Hands", "abstract": "Speech recognition is one of the key topics in artificial intelligence, as it\nis one of the most common forms of communication in humans. Researchers have\ndeveloped many speech-controlled prosthetic hands in the past decades,\nutilizing conventional speech recognition systems that use a combination of\nneural network and hidden Markov model. Recent advancements in general-purpose\ngraphics processing units (GPGPUs) enable intelligent devices to run deep\nneural networks in real-time. Thus, state-of-the-art speech recognition systems\nhave rapidly shifted from the paradigm of composite subsystems optimization to\nthe paradigm of end-to-end optimization. However, a low-power embedded GPGPU\ncannot run these speech recognition systems in real-time. In this paper, we\nshow the development of deep convolutional neural networks (CNN) for speech\ncontrol of prosthetic hands that run in real-time on a NVIDIA Jetson TX2\ndeveloper kit. First, the device captures and converts speech into 2D features\n(like spectrogram). The CNN receives the 2D features and classifies the hand\ngestures. Finally, the hand gesture classes are sent to the prosthetic hand\nmotion control system. The whole system is written in Python with Keras, a deep\nlearning library that has a TensorFlow backend. Our experiments on the CNN\ndemonstrate the 91% accuracy and 2ms running time of hand gestures (text\noutput) from speech commands, which can be used to control the prosthetic hands\nin real-time.", "published": "2019-10-03 04:47:40", "link": "http://arxiv.org/abs/1910.01918v1", "categories": ["eess.SY", "cs.HC", "cs.LG", "cs.RO", "cs.SD", "cs.SY", "eess.AS", "68T40", "I.2"], "primary_category": "eess.SY"}
