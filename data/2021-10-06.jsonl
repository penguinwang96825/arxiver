{"title": "ABC: Attention with Bounded-memory Control", "abstract": "Transformer architectures have achieved state-of-the-art results on a variety\nof sequence modeling tasks. However, their attention mechanism comes with a\nquadratic complexity in sequence lengths, making the computational overhead\nprohibitive, especially for long sequences. Attention context can be seen as a\nrandom-access memory with each token taking a slot. Under this perspective, the\nmemory size grows linearly with the sequence length, and so does the overhead\nof reading from it. One way to improve the efficiency is to bound the memory\nsize. We show that disparate approaches can be subsumed into one abstraction,\nattention with bounded-memory control (ABC), and they vary in their\norganization of the memory. ABC reveals new, unexplored possibilities. First,\nit connects several efficient attention variants that would otherwise seem\napart. Second, this abstraction gives new insights--an established approach\n(Wang et al., 2020b) previously thought to be not applicable in causal\nattention, actually is. Last, we present a new instance of ABC, which draws\ninspiration from existing ABC approaches, but replaces their heuristic\nmemory-organizing functions with a learned, contextualized one. Our experiments\non language modeling, machine translation, and masked language model finetuning\nshow that our approach outperforms previous efficient attention models;\ncompared to the strong transformer baselines, it significantly improves the\ninference time and space efficiency with no or negligible accuracy loss.", "published": "2021-10-06 03:53:25", "link": "http://arxiv.org/abs/2110.02488v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KNN-BERT: Fine-Tuning Pre-Trained Models with KNN Classifier", "abstract": "Pre-trained models are widely used in fine-tuning downstream tasks with\nlinear classifiers optimized by the cross-entropy loss, which might face\nrobustness and stability problems. These problems can be improved by learning\nrepresentations that focus on similarities in the same class and contradictions\nin different classes when making predictions. In this paper, we utilize the\nK-Nearest Neighbors Classifier in pre-trained model fine-tuning. For this KNN\nclassifier, we introduce a supervised momentum contrastive learning framework\nto learn the clustered representations of the supervised downstream tasks.\nExtensive experiments on text classification tasks and robustness tests show\nthat by incorporating KNNs with the traditional fine-tuning process, we can\nobtain significant improvements on the clean accuracy in both rich-source and\nfew-shot settings and can improve the robustness against adversarial attacks.\n\\footnote{all codes is available at https://github.com/LinyangLee/KNN-BERT}", "published": "2021-10-06 06:17:05", "link": "http://arxiv.org/abs/2110.02523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weakly-supervised Text Classification Based on Keyword Graph", "abstract": "Weakly-supervised text classification has received much attention in recent\nyears for it can alleviate the heavy burden of annotating massive data. Among\nthem, keyword-driven methods are the mainstream where user-provided keywords\nare exploited to generate pseudo-labels for unlabeled texts. However, existing\nmethods treat keywords independently, thus ignore the correlation among them,\nwhich should be useful if properly exploited. In this paper, we propose a novel\nframework called ClassKG to explore keyword-keyword correlation on keyword\ngraph by GNN. Our framework is an iterative process. In each iteration, we\nfirst construct a keyword graph, so the task of assigning pseudo labels is\ntransformed to annotating keyword subgraphs. To improve the annotation quality,\nwe introduce a self-supervised task to pretrain a subgraph annotator, and then\nfinetune it. With the pseudo labels generated by the subgraph annotator, we\nthen train a text classifier to classify the unlabeled texts. Finally, we\nre-extract keywords from the classified texts. Extensive experiments on both\nlong-text and short-text datasets show that our method substantially\noutperforms the existing ones", "published": "2021-10-06 08:58:02", "link": "http://arxiv.org/abs/2110.02591v1", "categories": ["cs.CL", "I.7.0"], "primary_category": "cs.CL"}
{"title": "Sequential Reptile: Inter-Task Gradient Alignment for Multilingual\n  Learning", "abstract": "Multilingual models jointly pretrained on multiple languages have achieved\nremarkable performance on various multilingual downstream tasks. Moreover,\nmodels finetuned on a single monolingual downstream task have shown to\ngeneralize to unseen languages. In this paper, we first show that it is crucial\nfor those tasks to align gradients between them in order to maximize knowledge\ntransfer while minimizing negative transfer. Despite its importance, the\nexisting methods for gradient alignment either have a completely different\npurpose, ignore inter-task alignment, or aim to solve continual learning\nproblems in rather inefficient ways. As a result of the misaligned gradients\nbetween tasks, the model suffers from severe negative transfer in the form of\ncatastrophic forgetting of the knowledge acquired from the pretraining. To\novercome the limitations, we propose a simple yet effective method that can\nefficiently align gradients between tasks. Specifically, we perform each\ninner-optimization by sequentially sampling batches from all the tasks,\nfollowed by a Reptile outer update. Thanks to the gradients aligned between\ntasks by our method, the model becomes less vulnerable to negative transfer and\ncatastrophic forgetting. We extensively validate our method on various\nmulti-task learning and zero-shot cross-lingual transfer tasks, where our\nmethod largely outperforms all the relevant baselines we consider.", "published": "2021-10-06 09:10:10", "link": "http://arxiv.org/abs/2110.02600v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Application of the interactive Leipzig Corpus Miner as a generic\n  research platform for the use in the social sciences", "abstract": "This article introduces to the interactive Leipzig Corpus Miner (iLCM) - a\nnewly released, open-source software to perform automatic content analysis.\nSince the iLCM is based on the R-programming language, its generic text mining\nprocedures provided via a user-friendly graphical user interface (GUI) can\neasily be extended using the integrated IDE RStudio-Server or numerous other\ninterfaces in the tool. Furthermore, the iLCM offers various possibilities to\nuse quantitative and qualitative research approaches in combination. Some of\nthese possibilities will be presented in more detail in the following.", "published": "2021-10-06 12:53:00", "link": "http://arxiv.org/abs/2110.02708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How BPE Affects Memorization in Transformers", "abstract": "Training data memorization in NLP can both be beneficial (e.g., closed-book\nQA) and undesirable (personal data extraction). In any case, successful model\ntraining requires a non-trivial amount of memorization to store word spellings,\nvarious linguistic idiosyncrasies and common knowledge. However, little is\nknown about what affects the memorization behavior of NLP models, as the field\ntends to focus on the equally important question of generalization. In this\nwork, we demonstrate that the size of the subword vocabulary learned by\nByte-Pair Encoding (BPE) greatly affects both ability and tendency of standard\nTransformer models to memorize training data, even when we control for the\nnumber of learned parameters. We find that with a large subword vocabulary\nsize, Transformer models fit random mappings more easily and are more\nvulnerable to membership inference attacks. Similarly, given a prompt,\nTransformer-based language models with large subword vocabularies reproduce the\ntraining data more often. We conjecture this effect is caused by reduction in\nthe sequences' length that happens as the BPE vocabulary grows. Our findings\ncan allow a more informed choice of hyper-parameters, that is better tailored\nfor a particular use-case.", "published": "2021-10-06 14:01:56", "link": "http://arxiv.org/abs/2110.02782v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parallel Composition of Weighted Finite-State Transducers", "abstract": "Finite-state transducers (FSTs) are frequently used in speech recognition.\nTransducer composition is an essential operation for combining different\nsources of information at different granularities. However, composition is also\none of the more computationally expensive operations. Due to the heterogeneous\nstructure of FSTs, parallel algorithms for composition are suboptimal in\nefficiency, generality, or both. We propose an algorithm for parallel\ncomposition and implement it on graphics processing units. We benchmark our\nparallel algorithm on the composition of random graphs and the composition of\ngraphs commonly used in speech recognition. The parallel composition scales\nbetter with the size of the input graphs and for large graphs can be as much as\n10 to 30 times faster than a sequential CPU algorithm.", "published": "2021-10-06 15:19:00", "link": "http://arxiv.org/abs/2110.02848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pretrained Transformers for Offensive Language Identification in\n  Tanglish", "abstract": "This paper describes the system submitted to Dravidian-Codemix-HASOC2021:\nHate Speech and Offensive Language Identification in Dravidian Languages\n(Tamil-English and Malayalam-English). This task aims to identify offensive\ncontent in code-mixed comments/posts in Dravidian Languages collected from\nsocial media. Our approach utilizes pooling the last layers of pretrained\ntransformer multilingual BERT for this task which helped us achieve rank nine\non the leaderboard with a weighted average score of 0.61 for the Tamil-English\ndataset in subtask B. After the task deadline, we sampled the dataset uniformly\nand used the MuRIL pretrained model, which helped us achieve a weighted average\nscore of 0.67, the top score in the leaderboard. Furthermore, our approach to\nutilizing the pretrained models helps reuse our models for the same task with a\ndifferent dataset. Our code and models are available in\nhttps://github.com/seanbenhur/tanglish-offensive-language-identification", "published": "2021-10-06 15:23:40", "link": "http://arxiv.org/abs/2110.02852v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Lexical Normalization with Multilingual\n  Transformers", "abstract": "Current benchmark tasks for natural language processing contain text that is\nqualitatively different from the text used in informal day to day digital\ncommunication. This discrepancy has led to severe performance degradation of\nstate-of-the-art NLP models when fine-tuned on real-world data. One way to\nresolve this issue is through lexical normalization, which is the process of\ntransforming non-standard text, usually from social media, into a more\nstandardized form. In this work, we propose a sentence-level\nsequence-to-sequence model based on mBART, which frames the problem as a\nmachine translation problem. As the noisy text is a pervasive problem across\nlanguages, not just English, we leverage the multi-lingual pre-training of\nmBART to fine-tune it to our data. While current approaches mainly operate at\nthe word or subword level, we argue that this approach is straightforward from\na technical standpoint and builds upon existing pre-trained transformer\nnetworks. Our results show that while word-level, intrinsic, performance\nevaluation is behind other methods, our model improves performance on\nextrinsic, downstream tasks through normalization compared to models operating\non raw, unprocessed, social media text.", "published": "2021-10-06 15:53:20", "link": "http://arxiv.org/abs/2110.02869v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Optimal Transport as Alignment Objective for fine-tuning\n  Multilingual Contextualized Embeddings", "abstract": "Recent studies have proposed different methods to improve multilingual word\nrepresentations in contextualized settings including techniques that align\nbetween source and target embedding spaces. For contextualized embeddings,\nalignment becomes more complex as we additionally take context into\nconsideration. In this work, we propose using Optimal Transport (OT) as an\nalignment objective during fine-tuning to further improve multilingual\ncontextualized representations for downstream cross-lingual transfer. This\napproach does not require word-alignment pairs prior to fine-tuning that may\nlead to sub-optimal matching and instead learns the word alignments within\ncontext in an unsupervised manner. It also allows different types of mappings\ndue to soft matching between source and target sentences. We benchmark our\nproposed method on two tasks (XNLI and XQuAD) and achieve improvements over\nbaselines as well as competitive results compared to similar recent works.", "published": "2021-10-06 16:13:45", "link": "http://arxiv.org/abs/2110.02887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NUS-IDS at FinCausal 2021: Dependency Tree in Graph Neural Network for\n  Better Cause-Effect Span Detection", "abstract": "Automatic identification of cause-effect spans in financial documents is\nimportant for causality modelling and understanding reasons that lead to\nfinancial events. To exploit the observation that words are more connected to\nother words with the same cause-effect type in a dependency tree, we construct\nuseful graph embeddings by incorporating dependency relation features through a\ngraph neural network. Our model builds on a baseline BERT token classifier with\nViterbi decoding, and outperforms this baseline in cross-validation and during\nthe competition. In the official run of FinCausal 2021, we obtained Precision,\nRecall, and F1 scores of 95.56%, 95.56% and 95.57% that all ranked 1st place,\nand an Exact Match score of 86.05% which ranked 3rd place.", "published": "2021-10-06 18:05:59", "link": "http://arxiv.org/abs/2110.02991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Neurons Invariant to Sentence Structural Changes in Neural Machine\n  Translation", "abstract": "We present a methodology that explores how sentence structure is reflected in\nneural representations of machine translation systems. We demonstrate our\nmodel-agnostic approach with the Transformer English-German translation model.\nWe analyze neuron-level correlation of activations between paraphrases while\ndiscussing the methodology challenges and the need for confound analysis to\nisolate the effects of shallow cues. We find that similarity between activation\npatterns can be mostly accounted for by similarity in word choice and sentence\nlength. Following that, we manipulate neuron activations to control the\nsyntactic form of the output. We show this intervention to be somewhat\nsuccessful, indicating that deep models capture sentence-structure\ndistinctions, despite finding no such indication at the neuron level. To\nconduct our experiments, we develop a semi-automatic method to generate\nmeaning-preserving minimal pair paraphrases (active-passive voice and adverbial\nclause-noun phrase) and compile a corpus of such pairs.", "published": "2021-10-06 20:57:57", "link": "http://arxiv.org/abs/2110.03067v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cut the CARP: Fishing for zero-shot story evaluation", "abstract": "Recent advances in large-scale language models (Raffel et al., 2019; Brown et\nal., 2020) have brought significant qualitative and quantitative improvements\nin machine-driven text generation. Despite this, generation and evaluation of\nmachine-generated narrative text remains a challenging problem. Objective\nevaluation of computationally-generated stories may be prohibitively expensive,\nrequire meticulously annotated datasets, or may not adequately measure the\nlogical coherence of a generated story's narratological structure.\n  Informed by recent advances in contrastive learning (Radford et al., 2021),\nwe present Contrastive Authoring and Reviewing Pairing (CARP): a scalable,\nefficient method for performing qualitatively superior, zero-shot evaluation of\nstories. We show a strong correlation between human evaluation of stories and\nthose of CARP. Model outputs more significantly correlate with corresponding\nhuman input than those language-model based methods which utilize finetuning or\nprompt engineering approaches. We also present and analyze the Story-Critique\nDataset, a new corpora composed of 1.3 million aligned story-critique pairs\nderived from over 80,000 stories. We expect this corpus to be of interest to\nNLP researchers.", "published": "2021-10-06 23:50:46", "link": "http://arxiv.org/abs/2110.03111v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation\n  Models", "abstract": "Pre-trained Natural Language Processing (NLP) models can be easily adapted to\na variety of downstream language tasks. This significantly accelerates the\ndevelopment of language models. However, NLP models have been shown to be\nvulnerable to backdoor attacks, where a pre-defined trigger word in the input\ntext causes model misprediction. Previous NLP backdoor attacks mainly focus on\nsome specific tasks. This makes those attacks less general and applicable to\nother kinds of NLP models and tasks. In this work, we propose \\Name, the first\ntask-agnostic backdoor attack against the pre-trained NLP models. The key\nfeature of our attack is that the adversary does not need prior information\nabout the downstream tasks when implanting the backdoor to the pre-trained\nmodel. When this malicious model is released, any downstream models transferred\nfrom it will also inherit the backdoor, even after the extensive transfer\nlearning process. We further design a simple yet effective strategy to bypass a\nstate-of-the-art defense. Experimental results indicate that our approach can\ncompromise a wide range of downstream NLP tasks in an effective and stealthy\nway.", "published": "2021-10-06 02:48:58", "link": "http://arxiv.org/abs/2110.02467v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Multi-Modal Embeddings from Structured Data", "abstract": "Multi-modal word semantics aims to enhance embeddings with perceptual input,\nassuming that human meaning representation is grounded in sensory experience.\nMost research focuses on evaluation involving direct visual input, however,\nvisual grounding can contribute to linguistic applications as well. Another\nmotivation for this paper is the growing need for more interpretable models and\nfor evaluating model efficiency regarding size and performance. This work\nexplores the impact of visual information for semantics when the evaluation\ninvolves no direct visual input, specifically semantic similarity and\nrelatedness. We investigate a new embedding type in-between linguistic and\nvisual modalities, based on the structured annotations of Visual Genome. We\ncompare uni- and multi-modal models including structured, linguistic and image\nbased representations. We measure the efficiency of each model with regard to\ndata and model size, modality / data distribution and information gain. The\nanalysis includes an interpretation of embedding structures. We found that this\nnew embedding conveys complementary information for text based embeddings. It\nachieves comparable performance in an economic way, using orders of magnitude\nless resources than visual models.", "published": "2021-10-06 08:42:09", "link": "http://arxiv.org/abs/2110.02577v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Relation Prediction as an Auxiliary Training Objective for Improving\n  Multi-Relational Graph Representations", "abstract": "Learning good representations on multi-relational graphs is essential to\nknowledge base completion (KBC). In this paper, we propose a new\nself-supervised training objective for multi-relational graph representation\nlearning, via simply incorporating relation prediction into the commonly used\n1vsAll objective. The new training objective contains not only terms for\npredicting the subject and object of a given triple, but also a term for\npredicting the relation type. We analyse how this new objective impacts\nmulti-relational learning in KBC: experiments on a variety of datasets and\nmodels show that relation prediction can significantly improve entity ranking,\nthe most widely used evaluation task for KBC, yielding a 6.1% increase in MRR\nand 9.9% increase in Hits@1 on FB15k-237 as well as a 3.1% increase in MRR and\n3.4% in Hits@1 on Aristo-v4. Moreover, we observe that the proposed objective\nis especially effective on highly multi-relational datasets, i.e. datasets with\na large number of predicates, and generates better representations when larger\nembedding sizes are used.", "published": "2021-10-06 15:09:32", "link": "http://arxiv.org/abs/2110.02834v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Capturing Structural Locality in Non-parametric Language Models", "abstract": "Structural locality is a ubiquitous feature of real-world datasets, wherein\ndata points are organized into local hierarchies. Some examples include topical\nclusters in text or project hierarchies in source code repositories. In this\npaper, we explore utilizing this structural locality within non-parametric\nlanguage models, which generate sequences that reference retrieved examples\nfrom an external source. We propose a simple yet effective approach for adding\nlocality information into such models by adding learned parameters that improve\nthe likelihood of retrieving examples from local neighborhoods. Experiments on\ntwo different domains, Java source code and Wikipedia text, demonstrate that\nlocality features improve model efficacy over models without access to these\nfeatures, with interesting differences. We also perform an analysis of how and\nwhere locality features contribute to improved performance and why the\ntraditionally used contextual similarity metrics alone are not enough to grasp\nthe locality structure.", "published": "2021-10-06 15:53:38", "link": "http://arxiv.org/abs/2110.02870v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Human-in-the-Loop Refinement of Word Embeddings", "abstract": "Word embeddings are a fixed, distributional representation of the context of\nwords in a corpus learned from word co-occurrences. Despite their proven\nutility in machine learning tasks, word embedding models may capture uneven\nsemantic and syntactic representations, and can inadvertently reflect various\nkinds of bias present within corpora upon which they were trained. It has been\ndemonstrated that post-processing of word embeddings to apply information found\nin lexical dictionaries can improve the semantic associations, thus improving\ntheir quality. Building on this idea, we propose a system that incorporates an\nadaptation of word embedding post-processing, which we call \"interactive\nrefitting\", to address some of the most daunting qualitative problems found in\nword embeddings. Our approach allows a human to identify and address potential\nquality issues with word embeddings interactively. This has the advantage of\nnegating the question of who decides what constitutes bias or what other\nquality issues may affect downstream tasks. It allows each organization or\nentity to address concerns they may have at a fine grained level and to do so\nin an iterative and interactive fashion. It also allows for better insight into\nwhat effect word embeddings, and refinements to word embeddings, have on\nmachine learning pipelines.", "published": "2021-10-06 16:10:32", "link": "http://arxiv.org/abs/2110.02884v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical prosody modeling and control in non-autoregressive parallel\n  neural TTS", "abstract": "Neural text-to-speech (TTS) synthesis can generate speech that is\nindistinguishable from natural speech. However, the synthetic speech often\nrepresents the average prosodic style of the database instead of having more\nversatile prosodic variation. Moreover, many models lack the ability to control\nthe output prosody, which does not allow for different styles for the same text\ninput. In this work, we train a non-autoregressive parallel neural TTS\nfront-end model hierarchically conditioned on both coarse and fine-grained\nacoustic speech features to learn a latent prosody space with intuitive and\nmeaningful dimensions. Experiments show that a non-autoregressive TTS model\nhierarchically conditioned on utterance-wise pitch, pitch range, duration,\nenergy, and spectral tilt can effectively control each prosodic dimension,\ngenerate a wide variety of speaking styles, and provide word-wise emphasis\ncontrol, while maintaining equal or better quality to the baseline model.", "published": "2021-10-06 17:58:42", "link": "http://arxiv.org/abs/2110.02952v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Emphasis control for parallel neural TTS", "abstract": "Recent parallel neural text-to-speech (TTS) synthesis methods are able to\ngenerate speech with high fidelity while maintaining high performance. However,\nthese systems often lack control over the output prosody, thus restricting the\nsemantic information conveyable for a given text. This paper proposes a\nhierarchical parallel neural TTS system for prosodic emphasis control by\nlearning a latent space that directly corresponds to a change in emphasis.\nThree candidate features for the latent space are compared: 1) Variance of\npitch and duration within words in a sentence, 2) Wavelet-based feature\ncomputed from pitch, energy, and duration, and 3) Learned combination of the\ntwo aforementioned approaches. At inference time, word-level prosodic emphasis\nis achieved by increasing the feature values of the latent space for the given\nwords. Experiments show that all the proposed methods are able to achieve the\nperception of increased emphasis with little loss in overall quality. Moreover,\nemphasized utterances were preferred in a pairwise comparison test over the\nnon-emphasized utterances, indicating promise for real-world applications.", "published": "2021-10-06 18:45:39", "link": "http://arxiv.org/abs/2110.03012v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "The Low-Resource Double Bind: An Empirical Study of Pruning for\n  Low-Resource Machine Translation", "abstract": "A \"bigger is better\" explosion in the number of parameters in deep neural\nnetworks has made it increasingly challenging to make state-of-the-art networks\naccessible in compute-restricted environments. Compression techniques have\ntaken on renewed importance as a way to bridge the gap. However, evaluation of\nthe trade-offs incurred by popular compression techniques has been centered on\nhigh-resource datasets. In this work, we instead consider the impact of\ncompression in a data-limited regime. We introduce the term low-resource double\nbind to refer to the co-occurrence of data limitations and compute resource\nconstraints. This is a common setting for NLP for low-resource languages, yet\nthe trade-offs in performance are poorly studied. Our work offers surprising\ninsights into the relationship between capacity and generalization in\ndata-limited regimes for the task of machine translation. Our experiments on\nmagnitude pruning for translations from English into Yoruba, Hausa, Igbo and\nGerman show that in low-resource regimes, sparsity preserves performance on\nfrequent sentences but has a disparate impact on infrequent ones. However, it\nimproves robustness to out-of-distribution shifts, especially for datasets that\nare very distinct from the training distribution. Our findings suggest that\nsparsity can play a beneficial role at curbing memorization of low frequency\nattributes, and therefore offers a promising solution to the low-resource\ndouble bind.", "published": "2021-10-06 19:48:18", "link": "http://arxiv.org/abs/2110.03036v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LIDSNet: A Lightweight on-device Intent Detection model using Deep\n  Siamese Network", "abstract": "Intent detection is a crucial task in any Natural Language Understanding\n(NLU) system and forms the foundation of a task-oriented dialogue system. To\nbuild high-quality real-world conversational solutions for edge devices, there\nis a need for deploying intent detection model on device. This necessitates a\nlight-weight, fast, and accurate model that can perform efficiently in a\nresource-constrained environment. To this end, we propose LIDSNet, a novel\nlightweight on-device intent detection model, which accurately predicts the\nmessage intent by utilizing a Deep Siamese Network for learning better sentence\nrepresentations. We use character-level features to enrich the sentence-level\nrepresentations and empirically demonstrate the advantage of transfer learning\nby utilizing pre-trained embeddings. Furthermore, to investigate the efficacy\nof the modules in our architecture, we conduct an ablation study and arrive at\nour optimal model. Experimental results prove that LIDSNet achieves\nstate-of-the-art competitive accuracy of 98.00% and 95.97% on SNIPS and ATIS\npublic datasets respectively, with under 0.59M parameters. We further benchmark\nLIDSNet against fine-tuned BERTs and show that our model is at least 41x\nlighter and 30x faster during inference than MobileBERT on Samsung Galaxy S20\ndevice, justifying its efficiency on resource-constrained edge devices.", "published": "2021-10-06 18:20:37", "link": "http://arxiv.org/abs/2110.15717v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KNOT: Knowledge Distillation using Optimal Transport for Solving NLP\n  Tasks", "abstract": "We propose a new approach, Knowledge Distillation using Optimal Transport\n(KNOT), to distill the natural language semantic knowledge from multiple\nteacher networks to a student network. KNOT aims to train a (global) student\nmodel by learning to minimize the optimal transport cost of its assigned\nprobability distribution over the labels to the weighted sum of probabilities\npredicted by the (local) teacher models, under the constraints, that the\nstudent model does not have access to teacher models' parameters or training\ndata. To evaluate the quality of knowledge transfer, we introduce a new metric,\nSemantic Distance (SD), that measures semantic closeness between the predicted\nand ground truth label distributions. The proposed method shows improvements in\nthe global model's SD performance over the baseline across three NLP tasks\nwhile performing on par with Entropy-based distillation on standard accuracy\nand F1 metrics. The implementation pertaining to this work is publicly\navailable at: https://github.com/declare-lab/KNOT.", "published": "2021-10-06 00:44:00", "link": "http://arxiv.org/abs/2110.02432v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences", "abstract": "Transformer-based models have achieved great success in various NLP, vision,\nand speech tasks. However, the core of Transformer, the self-attention\nmechanism, has a quadratic time and memory complexity with respect to the\nsequence length, which hinders applications of Transformer-based models to long\nsequences. Many approaches have been proposed to mitigate this problem, such as\nsparse attention mechanisms, low-rank matrix approximations and scalable\nkernels, and token mixing alternatives to self-attention. We propose a novel\nPooling Network (PoNet) for token mixing in long sequences with linear\ncomplexity. We design multi-granularity pooling and pooling fusion to capture\ndifferent levels of contextual information and combine their interactions with\ntokens. On the Long Range Arena benchmark, PoNet significantly outperforms\nTransformer and achieves competitive accuracy, while being only slightly slower\nthan the fastest model, FNet, across all sequence lengths measured on GPUs. We\nalso conduct systematic studies on the transfer learning capability of PoNet\nand observe that PoNet achieves 95.7% of the accuracy of BERT on the GLUE\nbenchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis\ndemonstrates effectiveness of the designed multi-granularity pooling and\npooling fusion for token mixing in long sequences and efficacy of the designed\npre-training tasks for PoNet to learn transferable contextualized language\nrepresentations.", "published": "2021-10-06 01:07:54", "link": "http://arxiv.org/abs/2110.02442v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spell my name: keyword boosted speech recognition", "abstract": "Recognition of uncommon words such as names and technical terminology is\nimportant to understanding conversations in context. However, the ability to\nrecognise such words remains a challenge in modern automatic speech recognition\n(ASR) systems.\n  In this paper, we propose a simple but powerful ASR decoding method that can\nbetter recognise these uncommon keywords, which in turn enables better\nreadability of the results. The method boosts the probabilities of given\nkeywords in a beam search based on acoustic model predictions. The method does\nnot require any training in advance.\n  We demonstrate the effectiveness of our method on the LibriSpeeech test sets\nand also internal data of real-world conversations. Our method significantly\nboosts keyword accuracy on the test sets, while maintaining the accuracy of the\nother words, and as well as providing significant qualitative improvements.\nThis method is applicable to other tasks such as machine translation, or\nwherever unseen and difficult keywords need to be recognised in beam search.", "published": "2021-10-06 14:16:57", "link": "http://arxiv.org/abs/2110.02791v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Knowledge Assimilation for Expert-Layman Text Style\n  Transfer", "abstract": "Expert-layman text style transfer technologies have the potential to improve\ncommunication between members of scientific communities and the general public.\nHigh-quality information produced by experts is often filled with difficult\njargon laypeople struggle to understand. This is a particularly notable issue\nin the medical domain, where layman are often confused by medical text online.\nAt present, two bottlenecks interfere with the goal of building high-quality\nmedical expert-layman style transfer systems: a dearth of pretrained\nmedical-domain language models spanning both expert and layman terminologies\nand a lack of parallel corpora for training the transfer task itself. To\nmitigate the first issue, we propose a novel language model (LM) pretraining\ntask, Knowledge Base Assimilation, to synthesize pretraining data from the\nedges of a graph of expert- and layman-style medical terminology terms into an\nLM during self-supervised learning. To mitigate the second issue, we build a\nlarge-scale parallel corpus in the medical expert-layman domain using a\nmargin-based criterion. Our experiments show that transformer-based models\npretrained on knowledge base assimilation and other well-established\npretraining tasks fine-tuning on our new parallel corpus leads to considerable\nimprovement against expert-layman transfer benchmarks, gaining an average\nrelative improvement of our human evaluation, the Overall Success Rate (OSR),\nby 106%. We release our code and parallel corpus for future research.", "published": "2021-10-06 17:57:22", "link": "http://arxiv.org/abs/2110.02950v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Multimodal Language Representations using Convolutional\n  Autoencoders", "abstract": "Multimodal Language Analysis is a demanding area of research, since it is\nassociated with two requirements: combining different modalities and capturing\ntemporal information. During the last years, several works have been proposed\nin the area, mostly centered around supervised learning in downstream tasks. In\nthis paper we propose extracting unsupervised Multimodal Language\nrepresentations that are universal and can be applied to different tasks.\nTowards this end, we map the word-level aligned multimodal sequences to 2-D\nmatrices and then use Convolutional Autoencoders to learn embeddings by\ncombining multiple datasets. Extensive experimentation on Sentiment Analysis\n(MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned\nrepresentations can achieve near-state-of-the-art performance with just the use\nof a Logistic Regression algorithm for downstream classification. It is also\nshown that our method is extremely lightweight and can be easily generalized to\nother tasks and unseen data with small performance drop and almost the same\nnumber of parameters. The proposed multimodal representation models are\nopen-sourced and will help grow the applicability of Multimodal Language.", "published": "2021-10-06 18:28:07", "link": "http://arxiv.org/abs/2110.03007v2", "categories": ["cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Integrating Categorical Features in End-to-End ASR", "abstract": "All-neural, end-to-end ASR systems gained rapid interest from the speech\nrecognition community. Such systems convert speech input to text units using a\nsingle trainable neural network model. E2E models require large amounts of\npaired speech text data that is expensive to obtain. The amount of data\navailable varies across different languages and dialects. It is critical to\nmake use of all these data so that both low resource languages and high\nresource languages can be improved. When we want to deploy an ASR system for a\nnew application domain, the amount of domain specific training data is very\nlimited. To be able to leverage data from existing domains is important for ASR\naccuracy in the new domain. In this paper, we treat all these aspects as\ncategorical information in an ASR system, and propose a simple yet effective\nway to integrate categorical features into E2E model. We perform detailed\nanalysis on various training strategies, and find that building a joint model\nthat includes categorical features can be more accurate than multiple\nindependently trained models.", "published": "2021-10-06 20:07:53", "link": "http://arxiv.org/abs/2110.03047v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DRAFT-What you always wanted to know but could not find about\n  block-based environments", "abstract": "Block-based environments are visual programming environments, which are\nbecoming more and more popular because of their ease of use. The ease of use\ncomes thanks to their intuitive graphical representation and structural\nmetaphors (jigsaw-like puzzles) to display valid combinations of language\nconstructs to the users. Part of the current popularity of block-based\nenvironments is thanks to Scratch. As a result they are often associated with\ntools for children or young learners. However, it is unclear how these types of\nprogramming environments are developed and used in general. So we conducted a\nsystematic literature review on block-based environments by studying 152 papers\npublished between 2014 and 2020, and a non-systematic tool review of 32\nblock-based environments. In particular, we provide a helpful inventory of\nblock-based editors for end-users on different topics and domains. Likewise, we\nfocused on identifying the main components of block-based environments, how\nthey are engineered, and how they are used. This survey should be equally\nhelpful for language engineering researchers and language engineers alike.", "published": "2021-10-06 21:12:46", "link": "http://arxiv.org/abs/2110.03073v1", "categories": ["cs.SE", "cs.CL", "cs.HC"], "primary_category": "cs.SE"}
{"title": "CTC Variations Through New WFST Topologies", "abstract": "This paper presents novel Weighted Finite-State Transducer (WFST) topologies\nto implement Connectionist Temporal Classification (CTC)-like algorithms for\nautomatic speech recognition. Three new CTC variants are proposed: (1) the\n\"compact-CTC\", in which direct transitions between units are replaced with\n<epsilon> back-off transitions; (2) the \"minimal-CTC\", that only adds <blank>\nself-loops when used in WFST-composition; and (3) the \"selfless-CTC\" variants,\nwhich disallows self-loop for non-blank units. Compact-CTC allows for 1.5 times\nsmaller WFST decoding graphs and reduces memory consumption by two times when\ntraining CTC models with the LF-MMI objective without hurting the recognition\naccuracy. Minimal-CTC reduces graph size and memory consumption by two and four\ntimes for the cost of a small accuracy drop. Using selfless-CTC can improve the\naccuracy for wide context window models.", "published": "2021-10-06 23:00:15", "link": "http://arxiv.org/abs/2110.03098v3", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Internal Language Model Adaptation with Text-Only Data for End-to-End\n  Speech Recognition", "abstract": "Text-only adaptation of an end-to-end (E2E) model remains a challenging task\nfor automatic speech recognition (ASR). Language model (LM) fusion-based\napproaches require an additional external LM during inference, significantly\nincreasing the computation cost. To overcome this, we propose an internal LM\nadaptation (ILMA) of the E2E model using text-only data. Trained with\naudio-transcript pairs, an E2E model implicitly learns an internal LM that\ncharacterizes the token sequence probability which is approximated by the E2E\nmodel output after zeroing out the encoder contribution. During ILMA, we\nfine-tune the internal LM, i.e., the E2E components excluding the encoder, to\nminimize a cross-entropy loss. To make ILMA effective, it is essential to train\nthe E2E model with an internal LM loss besides the standard E2E loss.\nFurthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler\ndivergence between the output distributions of the adapted and unadapted\ninternal LMs. ILMA is the most effective when we update only the last linear\nlayer of the joint network. ILMA enables a fast text-only adaptation of the E2E\nmodel without increasing the run-time computational cost. Experimented with\n30K-hour trained transformer transducer models, ILMA achieves up to 34.9%\nrelative word error rate reduction from the unadapted baseline.", "published": "2021-10-06 23:03:29", "link": "http://arxiv.org/abs/2110.05354v5", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks", "abstract": "The advent of noisy intermediate-scale quantum (NISQ) computers raises a\ncrucial challenge to design quantum neural networks for fully quantum learning\ntasks. To bridge the gap, this work proposes an end-to-end learning framework\nnamed QTN-VQC, by introducing a trainable quantum tensor network (QTN) for\nquantum embedding on a variational quantum circuit (VQC). The architecture of\nQTN is composed of a parametric tensor-train network for feature extraction and\na tensor product encoding for quantum embedding. We highlight the QTN for\nquantum embedding in terms of two perspectives: (1) we theoretically\ncharacterize QTN by analyzing its representation power of input features; (2)\nQTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the\ngeneration of quantum embedding to the output measurement. Our experiments on\nthe MNIST dataset demonstrate the advantages of QTN for quantum embedding over\nother quantum embedding approaches.", "published": "2021-10-06 14:44:51", "link": "http://arxiv.org/abs/2110.03861v3", "categories": ["quant-ph", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.NE"], "primary_category": "quant-ph"}
{"title": "MediumVC: Any-to-any voice conversion using synthetic specific-speaker\n  speeches as intermedium features", "abstract": "To realize any-to-any (A2A) voice conversion (VC), most methods are to\nperform symmetric self-supervised reconstruction tasks (Xi to Xi), which\nusually results in inefficient performances due to inadequate feature\ndecoupling, especially for unseen speakers. We propose a two-stage\nreconstruction task (Xi to Yi to Xi) using synthetic specific-speaker speeches\nas intermedium features, where A2A VC is divided into two stages: any-to-one\n(A2O) and one-to-Any (O2A). In the A2O stage, we propose a new A2O method:\nSingleVC, by employing a noval data augment strategy(pitch-shifted and\nduration-remained, PSDR) to accomplish Xi to Yi. In the O2A stage, MediumVC is\nproposed based on pre-trained SingleVC to conduct Yi to Xi. Through such\nasymmetrical reconstruction tasks (Xi to Yi in SingleVC and Yi to Xi in\nMediumVC), the models are to capture robust disentangled features purposefully.\nExperiments indicate MediumVC can enhance the similarity of converted speeches\nwhile maintaining a high degree of naturalness.", "published": "2021-10-06 04:29:31", "link": "http://arxiv.org/abs/2110.02500v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems", "abstract": "Singing voice synthesis (SVS) is a task that aims to generate audio signals\naccording to musical scores and lyrics. With its multifaceted nature concerning\nmusic and language, producing singing voices indistinguishable from that of\nhuman singers has always remained an unfulfilled pursuit. Nonetheless, the\nadvancements of deep learning techniques have brought about a substantial leap\nin the quality and naturalness of synthesized singing voice. This paper aims to\nreview some of the state-of-the-art deep learning-driven SVS systems. We intend\nto summarize their deployed model architectures and identify the strengths and\nlimitations for each of the introduced systems. Thereby, we picture the recent\nadvancement trajectory of this field and conclude the challenges left to be\nresolved both in commercial applications and academic research.", "published": "2021-10-06 05:27:23", "link": "http://arxiv.org/abs/2110.02511v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Generalization Ability of MOS Prediction Networks", "abstract": "Automatic methods to predict listener opinions of synthesized speech remain\nelusive since listeners, systems being evaluated, characteristics of the\nspeech, and even the instructions given and the rating scale all vary from test\nto test. While automatic predictors for metrics such as mean opinion score\n(MOS) can achieve high prediction accuracy on samples from the same test, they\ntypically fail to generalize well to new listening test contexts. In this\npaper, using a variety of networks for MOS prediction including MOSNet and\nself-supervised speech models such as wav2vec2, we investigate their\nperformance on data from different listening tests in both zero-shot and\nfine-tuned settings. We find that wav2vec2 models fine-tuned for MOS prediction\nhave good generalization capability to out-of-domain data even for the most\nchallenging case of utterance-level predictions in the zero-shot setting, and\nthat fine-tuning to in-domain data can improve predictions. We also observe\nthat unseen systems are especially challenging for MOS prediction models.", "published": "2021-10-06 10:19:36", "link": "http://arxiv.org/abs/2110.02635v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Improving Real-time Score Following in Opera by Combining Music with\n  Lyrics Tracking", "abstract": "Fully automatic opera tracking is challenging because of the acoustic\ncomplexity of the genre, combining musical and linguistic information (singing,\nspeech) in complex ways. In this paper, we propose a new pipeline for complete\nopera tracking. The pipeline is based on two trackers. A music tracker that has\nproven to be effective at tracking orchestral parts, will lead the tracking\nprocess. In addition, a lyrics tracker, that has recently been shown to\nreliably track the lyrics of opera songs, will correct the music tracker when\ntracking parts that have a text dominance over the music. We will demonstrate\nthe efficiency of this method on the opera Don Giovanni, showing that this\ntechnique helps improving accuracy and robustness of a complete opera tracker.", "published": "2021-10-06 08:58:04", "link": "http://arxiv.org/abs/2110.02592v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Lower Interaural Coherence in Off-Signal Bands Impairs Binaural\n  Detection", "abstract": "Differences in interaural phase configuration between a target and a masker\ncan lead to substantial binaural unmasking. This effect is decreased for\nmasking noises with an interaural time difference (ITD). Adding a second noise\nwith an opposing ITD in most cases further reduces binaural unmasking. Thus\nfar, modeling of these detection thresholds required both a mechanism for\ninternal ITD compensation and an increased binaural bandwidth. An alternative\nexplanation for the reduction is that unmasking is impaired by the lower\ninteraural coherence in off-frequency regions caused by the second masker\n(Marquardt & McAlpine, 2009, JASA pp. EL177 - EL182). Based on this hypothesis,\nthe current work proposes a quantitative multi-channel model using monaurally\nderived peripheral filter bandwidths and an across-channel incoherence\ninterference mechanism. This mechanism differs from wider filters since it has\nno effect when the masker coherence is constant across frequency bands.\nCombined with a monaural energy discrimination pathway, the model predicts the\ndifferences between a single delayed noise and two opposingly delayed noises,\nas well as four other data sets. It helps resolve the inconsistency explaining\nsome data sets requires wide filters while others require narrow filters.", "published": "2021-10-06 12:32:08", "link": "http://arxiv.org/abs/2110.02695v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Prosody-TTS: An end-to-end speech synthesis system with prosody control", "abstract": "End-to-end text-to-speech synthesis systems achieved immense success in\nrecent times, with improved naturalness and intelligibility. However, the\nend-to-end models, which primarily depend on the attention-based alignment, do\nnot offer an explicit provision to modify/incorporate the desired prosody while\nsynthesizing the signal. Moreover, the state-of-the-art end-to-end systems use\nautoregressive models for synthesis, making the prediction sequential. Hence,\nthe inference time and the computational complexity are quite high. This paper\nproposes Prosody-TTS, an end-to-end speech synthesis model that combines the\nadvantages of statistical parametric models and end-to-end neural network\nmodels. It also has a provision to modify or incorporate the desired prosody by\ncontrolling the fundamental frequency (f0) and the phone duration. Generating\nspeech samples with appropriate prosody and rhythm helps in improving the\nnaturalness of the synthesized speech. We explicitly model the duration of the\nphoneme and the f0 to have control over them during the synthesis. The model is\ntrained in an end-to-end fashion to directly generate the speech waveform from\nthe input text, which in turn depends on the auxiliary subtasks of predicting\nthe phoneme duration, f0, and mel spectrogram. Experiments on the Telugu\nlanguage data of the IndicTTS database show that the proposed Prosody-TTS model\nachieves state-of-the-art performance with a mean opinion score of 4.08, with a\nvery low inference time.", "published": "2021-10-06 15:29:35", "link": "http://arxiv.org/abs/2110.02854v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Unsupervised Audio-Caption Aligning Learns Correspondences between\n  Individual Sound Events and Textual Phrases", "abstract": "We investigate unsupervised learning of correspondences between sound events\nand textual phrases through aligning audio clips with textual captions\ndescribing the content of a whole audio clip. We align originally unaligned and\nunannotated audio clips and their captions by scoring the similarities between\naudio frames and words, as encoded by modality-specific encoders and using a\nranking-loss criterion to optimize the model. After training, we obtain\nclip-caption similarity by averaging frame-word similarities and estimate\nevent-phrase correspondences by calculating frame-phrase similarities. We\nevaluate the method with two cross-modal tasks: audio-caption retrieval, and\nphrase-based sound event detection (SED). Experimental results show that the\nproposed method can globally associate audio clips with captions as well as\nlocally learn correspondences between individual sound events and textual\nphrases in an unsupervised manner.", "published": "2021-10-06 17:37:03", "link": "http://arxiv.org/abs/2110.02939v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "AECMOS: A speech quality assessment metric for echo impairment", "abstract": "Traditionally, the quality of acoustic echo cancellers is evaluated using\nintrusive speech quality assessment measures such as ERLE \\cite{g168} and PESQ\n\\cite{p862}, or by carrying out subjective laboratory tests. Unfortunately, the\nformer are not well correlated with human subjective measures, while the latter\nare time and resource consuming to carry out. We provide a new tool for speech\nquality assessment for echo impairment which can be used to evaluate the\nperformance of acoustic echo cancellers. More precisely, we develop a neural\nnetwork model to evaluate call quality degradations in two separate categories:\necho and degradations from other sources. We show that our model is accurate as\nmeasured by correlation with human subjective quality ratings. Our tool can be\nused effectively to stack rank echo cancellation models. AECMOS is being made\npublicly available as an Azure service.", "published": "2021-10-06 18:35:29", "link": "http://arxiv.org/abs/2110.03010v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adversarial Attacks on Machinery Fault Diagnosis", "abstract": "Despite the great progress of neural network-based (NN-based) machinery fault\ndiagnosis methods, their robustness has been largely neglected, for they can be\neasily fooled through adding imperceptible perturbation to the input. For fault\ndiagnosis problems, in this paper, we reformulate various adversarial attacks\nand intensively investigate them under untargeted and targeted conditions.\nExperimental results on six typical NN-based models show that accuracies of the\nmodels are greatly reduced by adding small perturbations. We further propose a\nsimple, efficient and universal scheme to protect the victim models. This work\nprovides an in-depth look at adversarial examples of machinery vibration\nsignals for developing protection methods against adversarial attack and\nimproving the robustness of NN-based models.", "published": "2021-10-06 04:26:30", "link": "http://arxiv.org/abs/2110.02498v2", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "EdiTTS: Score-based Editing for Controllable Text-to-Speech", "abstract": "We present EdiTTS, an off-the-shelf speech editing methodology based on\nscore-based generative modeling for text-to-speech synthesis. EdiTTS allows for\ntargeted, granular editing of audio, both in terms of content and pitch,\nwithout the need for any additional training, task-specific optimization, or\narchitectural modifications to the score-based model backbone. Specifically, we\napply coarse yet deliberate perturbations in the Gaussian prior space to induce\ndesired behavior from the diffusion model while applying masks and softening\nkernels to ensure that iterative edits are applied only to the target region.\nThrough listening tests and speech-to-text back transcription, we show that\nEdiTTS outperforms existing baselines and produces robust samples that satisfy\nuser-imposed requirements.", "published": "2021-10-06 08:51:10", "link": "http://arxiv.org/abs/2110.02584v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Investigation of the Effectiveness of Phase for Audio Classification", "abstract": "While log-amplitude mel-spectrogram has widely been used as the feature\nrepresentation for processing speech based on deep learning, the effectiveness\nof another aspect of speech spectrum, i.e., phase information, was shown\nrecently for tasks such as speech enhancement and source separation. In this\nstudy, we extensively investigated the effectiveness of including phase\ninformation of signals for eight audio classification tasks. We constructed a\nlearnable front-end that can compute the phase and its derivatives based on a\ntime-frequency representation with mel-like frequency axis. As a result,\nexperimental results showed significant performance improvement for musical\npitch detection, musical instrument detection, language identification, speaker\nidentification, and birdsong detection. On the other hand, overfitting to the\nrecording condition was observed for some tasks when the instantaneous\nfrequency was used. The results implied that the relationship between the phase\nvalues of adjacent elements is more important than the phase itself in audio\nclassification.", "published": "2021-10-06 16:07:06", "link": "http://arxiv.org/abs/2110.02878v3", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Style Equalization: Unsupervised Learning of Controllable Generative\n  Sequence Models", "abstract": "Controllable generative sequence models with the capability to extract and\nreplicate the style of specific examples enable many applications, including\nnarrating audiobooks in different voices, auto-completing and auto-correcting\nwritten handwriting, and generating missing training samples for downstream\nrecognition tasks. However, under an unsupervised-style setting, typical\ntraining algorithms for controllable sequence generative models suffer from the\ntraining-inference mismatch, where the same sample is used as content and style\ninput during training but unpaired samples are given during inference. In this\npaper, we tackle the training-inference mismatch encountered during\nunsupervised learning of controllable generative sequence models. The proposed\nmethod is simple yet effective, where we use a style transformation module to\ntransfer target style information into an unrelated style input. This method\nenables training using unpaired content and style samples and thereby mitigate\nthe training-inference mismatch. We apply style equalization to text-to-speech\nand text-to-handwriting synthesis on three datasets. We conduct thorough\nevaluation, including both quantitative and qualitative user studies. Our\nresults show that by mitigating the training-inference mismatch with the\nproposed style equalization, we achieve style replication scores comparable to\nreal data in our user studies.", "published": "2021-10-06 16:17:57", "link": "http://arxiv.org/abs/2110.02891v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Lightweight Speech Enhancement in Unseen Noisy and Reverberant\n  Conditions using KISS-GEV Beamforming", "abstract": "This paper introduces a new method referred to as KISS-GEV (for Keep It Super\nSimple Generalized eigenvalue) beamforming. While GEV beamforming usually\nrelies on deep neural network for estimating target and noise time-frequency\nmasks, this method uses a signal processing approach based on the direction of\narrival (DoA) of the target. This considerably reduces the amount of\ncomputations involved at test time, and works for speech enhancement in unseen\nconditions as there is no need to train a neural network with noisy speech. The\nproposed method can also be used to separate speech from a mixture, provided\nthe speech sources come from different directions. Results also show that the\nproposed method uses the same minimal DoA assumption as Delay-and-Sum\nbeamforming, yet outperforms this traditional approach.", "published": "2021-10-06 23:32:59", "link": "http://arxiv.org/abs/2110.03103v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "GANtron: Emotional Speech Synthesis with Generative Adversarial Networks", "abstract": "Speech synthesis is used in a wide variety of industries. Nonetheless, it\nalways sounds flat or robotic. The state of the art methods that allow for\nprosody control are very cumbersome to use and do not allow easy tuning. To\ntackle some of these drawbacks, in this work we target the implementation of a\ntext-to-speech model where the inferred speech can be tuned with the desired\nemotions. To do so, we use Generative Adversarial Networks (GANs) together with\na sequence-to-sequence model using an attention mechanism. We evaluate four\ndifferent configurations considering different inputs and training strategies,\nstudy them and prove how our best model can generate speech files that lie in\nthe same distribution as the initial training dataset. Additionally, a new\nstrategy to boost the training convergence by applying a guided attention loss\nis proposed.", "published": "2021-10-06 10:44:30", "link": "http://arxiv.org/abs/2110.03390v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "68-T07, 62H30", "I.2; I.6.2"], "primary_category": "cs.SD"}
