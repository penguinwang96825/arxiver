{"title": "Creating Arabic LLM Prompts at Scale", "abstract": "The debut of chatGPT and BARD has popularized instruction following text\ngeneration using LLMs, where a user can interrogate an LLM using natural\nlanguage requests and obtain natural language answers that matches their\nrequests. Training LLMs to respond in this manner requires a large number of\nworked out examples of user requests (aka prompts) with corresponding gold\nresponses. In this paper, we introduce two methods for creating such prompts\nfor Arabic cheaply and quickly. The first methods entails automatically\ntranslating existing prompt datasets from English, such as PromptSource and\nSuper-NaturalInstructions, and then using machine translation quality\nestimation to retain high quality translations only. The second method involves\ncreating natural language prompts on top of existing Arabic NLP datasets. Using\nthese two methods we were able to create more than 67.4 million Arabic prompts\nthat cover a variety of tasks including summarization, headline generation,\ngrammar checking, open/closed question answering, creative writing, etc. We\nshow that fine tuning an open 7 billion parameter large language model, namely\nbase Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter\ninstruction tuned model, namely Llama3 70B, in handling Arabic prompts.", "published": "2024-08-12 00:46:39", "link": "http://arxiv.org/abs/2408.05882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine\n  Advertising", "abstract": "With the increase in the more fluent ad texts automatically created by\nnatural language generation technology, it is in the high demand to verify the\nquality of these creatives in a real-world setting. We propose AdTEC, the first\npublic benchmark to evaluate ad texts in multiple aspects from the perspective\nof practical advertising operations. Our contributions are: (i) Defining five\ntasks for evaluating the quality of ad texts and building a dataset based on\nthe actual operational experience of advertising agencies, which is typically\nkept in-house. (ii) Validating the performance of existing pre-trained language\nmodels (PLMs) and human evaluators on the dataset. (iii) Analyzing the\ncharacteristics and providing challenges of the benchmark. The results show\nthat while PLMs have already reached the practical usage level in several\ntasks, human still outperforms in certain domains, implying that there is\nsignificant room for improvement in such area.", "published": "2024-08-12 03:32:53", "link": "http://arxiv.org/abs/2408.05906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DiagESC: Dialogue Synthesis for Integrating Depression Diagnosis into\n  Emotional Support Conversation", "abstract": "Dialogue systems for mental health care aim to provide appropriate support to\nindividuals experiencing mental distress. While extensive research has been\nconducted to deliver adequate emotional support, existing studies cannot\nidentify individuals who require professional medical intervention and cannot\noffer suitable guidance. We introduce the Diagnostic Emotional Support\nConversation task for an advanced mental health management system. We develop\nthe DESC dataset to assess depression symptoms while maintaining user\nexperience by utilizing task-specific utterance generation prompts and a strict\nfiltering algorithm. Evaluations by professional psychological counselors\nindicate that DESC has a superior ability to diagnose depression than existing\ndata. Additionally, conversational quality evaluation reveals that DESC\nmaintains fluent, consistent, and coherent dialogues.", "published": "2024-08-12 10:26:39", "link": "http://arxiv.org/abs/2408.06044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Tables with Numbers, with Numbers", "abstract": "This paper is a critical reflection on the epistemic culture of contemporary\ncomputational linguistics, framed in the context of its growing obsession with\ntables with numbers. We argue against tables with numbers on the basis of their\nepistemic irrelevance, their environmental impact, their role in enabling and\nexacerbating social inequalities, and their deep ties to commercial\napplications and profit-driven research. We substantiate our arguments with\nempirical evidence drawn from a meta-analysis of computational linguistics\nresearch over the last decade.", "published": "2024-08-12 11:23:24", "link": "http://arxiv.org/abs/2408.06062v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Utilize Transformers for translating Wikipedia category names", "abstract": "On Wikipedia, articles are categorized to aid readers in navigating content\nefficiently. The manual creation of new categories can be laborious and\ntime-intensive. To tackle this issue, we built language models to translate\nWikipedia categories from English to Vietnamese with a dataset containing\n15,000 English-Vietnamese category pairs. Subsequently, small to medium-scale\nTransformer pre-trained models with a sequence-to-sequence architecture were\nfine-tuned for category translation. The experiments revealed that\nOPUS-MT-en-vi surpassed other models, attaining the highest performance with a\nBLEU score of 0.73, despite its smaller model storage. We expect our paper to\nbe an alternative solution for translation tasks with limited computer\nresources.", "published": "2024-08-12 13:07:34", "link": "http://arxiv.org/abs/2408.06124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers", "abstract": "This paper introduces rStar, a self-play mutual reasoning approach that\nsignificantly improves reasoning capabilities of small language models (SLMs)\nwithout fine-tuning or superior models. rStar decouples reasoning into a\nself-play mutual generation-discrimination process. First, a target SLM\naugments the Monte Carlo Tree Search (MCTS) with a rich set of human-like\nreasoning actions to construct higher quality reasoning trajectories. Next,\nanother SLM, with capabilities similar to the target SLM, acts as a\ndiscriminator to verify each trajectory generated by the target SLM. The\nmutually agreed reasoning trajectories are considered mutual consistent, thus\nare more likely to be correct. Extensive experiments across five SLMs\ndemonstrate rStar can effectively solve diverse reasoning problems, including\nGSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K\naccuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for\nMistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be\navailable at https://github.com/zhentingqi/rStar.", "published": "2024-08-12 14:42:13", "link": "http://arxiv.org/abs/2408.06195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data", "abstract": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. The base model, FuxiTranyu-8B, features 8 billion parameters and\nis trained from scratch on meticulously balanced multilingual data that\ncontains 600 billion tokens covering 43 natural languages and 16 programming\nlanguages. We also develop two instruction-tuned models: FuxiTranyu-8B-SFT\nwhich is fine-tuned on a diverse multilingual instruction dataset, and\nFuxiTranyu-8B-DPO which is further refined with DPO on a preference dataset for\nenhanced alignment ability. Extensive experiments on a wide range of\nmultilingual benchmarks demonstrate the competitive performance of FuxiTranyu\nagainst existing multilingual LLMs, e.g., BLOOM-7B, PolyLM-13B, and\nMistral-7B-Instruct. Both neuron and representation interpretability analyses\nreveal that FuxiTranyu achieves consistent multilingual representations across\nlanguages. To promote further research into multilingual LLMs, we release both\nthe base and instruction-tuned FuxiTranyu models together with 58 pre-training\ncheckpoints at HuggingFace (see https://huggingface.co/TJUNLP/FuxiTranyu-8B)\nand Github (see https://github.com/tjunlp-lab/FuxiTranyu).", "published": "2024-08-12 16:34:56", "link": "http://arxiv.org/abs/2408.06273v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation", "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.", "published": "2024-08-12 16:39:03", "link": "http://arxiv.org/abs/2408.06276v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Animate, or Inanimate, That is the Question for Large Language Models", "abstract": "The cognitive essence of humans is deeply intertwined with the concept of\nanimacy, which plays an essential role in shaping their memory, vision, and\nmulti-layered language understanding. Although animacy appears in language via\nnuanced constraints on verbs and adjectives, it is also learned and refined\nthrough extralinguistic information. Similarly, we assume that the LLMs'\nlimited abilities to understand natural language when processing animacy are\nmotivated by the fact that these models are trained exclusively on text.\n  Hence, the question this paper aims to answer arises: can LLMs, in their\ndigital wisdom, process animacy in a similar way to what humans would do? We\nthen propose a systematic analysis via prompting approaches. In particular, we\nprobe different LLMs by prompting them using animate, inanimate, usual, and\nstranger contexts. Results reveal that, although LLMs have been trained\npredominantly on textual data, they exhibit human-like behavior when faced with\ntypical animate and inanimate entities in alignment with earlier studies.\nHence, LLMs can adapt to understand unconventional situations by recognizing\noddities as animated without needing to interface with unspoken cognitive\ntriggers humans rely on to break down animations.", "published": "2024-08-12 17:48:55", "link": "http://arxiv.org/abs/2408.06332v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FastFiD: Improve Inference Efficiency of Open Domain Question Answering\n  via Sentence Selection", "abstract": "Open Domain Question Answering (ODQA) has been advancing rapidly in recent\ntimes, driven by significant developments in dense passage retrieval and\npretrained language models. Current models typically incorporate the FiD\nframework, which is composed by a neural retriever alongside an encoder-decoder\nneural reader. In the answer generation process, the retriever will retrieve\nnumerous passages (around 100 for instance), each of which is then individually\nencoded by the encoder. Subsequently, the decoder makes predictions based on\nthese encoded passages. Nevertheless, this framework can be relatively\ntime-consuming, particularly due to the extensive length of the gathered\npassages. To address this, we introduce FastFiD in this paper, a novel approach\nthat executes sentence selection on the encoded passages. This aids in\nretaining valuable sentences while reducing the context length required for\ngenerating answers. Experiments on three commonly used datasets (Natural\nQuestions, TriviaQA and ASQA) demonstrate that our method can enhance the\ninference speed by 2.3X-5.7X, while simultaneously maintaining the model's\nperformance. Moreover, an in-depth analysis of the model's attention reveals\nthat the selected sentences indeed hold a substantial contribution towards the\nfinal answer. The codes are publicly available at\nhttps://github.com/thunlp/FastFiD.", "published": "2024-08-12 17:50:02", "link": "http://arxiv.org/abs/2408.06333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in\n  Language Models", "abstract": "Despite their wide adoption, the biases and unintended behaviors of language\nmodels remain poorly understood. In this paper, we identify and characterize a\nphenomenon never discussed before, which we call semantic leakage, where models\nleak irrelevant information from the prompt into the generation in unexpected\nways. We propose an evaluation setting to detect semantic leakage both by\nhumans and automatically, curate a diverse test suite for diagnosing this\nbehavior, and measure significant semantic leakage in 13 flagship models. We\nalso show that models exhibit semantic leakage in languages besides English and\nacross different settings and generation scenarios. This discovery highlights\nyet another type of bias in language models that affects their generation\npatterns and behavior.", "published": "2024-08-12 22:30:55", "link": "http://arxiv.org/abs/2408.06518v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Density Matrices for Metaphor Understanding", "abstract": "In physics, density matrices are used to represent mixed states, i.e.\nprobabilistic mixtures of pure states. This concept has previously been used to\nmodel lexical ambiguity. In this paper, we consider metaphor as a type of\nlexical ambiguity, and examine whether metaphorical meaning can be effectively\nmodelled using mixtures of word senses. We find that modelling metaphor is\nsignificantly more difficult than other kinds of lexical ambiguity, but that\nour best-performing density matrix method outperforms simple baselines as well\nas some neural language models.", "published": "2024-08-12 11:21:56", "link": "http://arxiv.org/abs/2408.11846v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompto: An open source library for asynchronous querying of LLM\n  endpoints", "abstract": "Recent surge in Large Language Model (LLM) availability has opened exciting\navenues for research. However, efficiently interacting with these models\npresents a significant hurdle since LLMs often reside on proprietary or\nself-hosted API endpoints, each requiring custom code for interaction.\nConducting comparative studies between different models can therefore be\ntime-consuming and necessitate significant engineering effort, hindering\nresearch efficiency and reproducibility. To address these challenges, we\npresent prompto, an open source Python library which facilitates asynchronous\nquerying of LLM endpoints enabling researchers to interact with multiple LLMs\nconcurrently, while maximising efficiency and utilising individual rate limits.\nOur library empowers researchers and developers to interact with LLMs more\neffectively and allowing faster experimentation, data generation and\nevaluation. prompto is released with an introductory video\n(https://youtu.be/lWN9hXBOLyQ) under MIT License and is available via GitHub\n(https://github.com/alan-turing-institute/prompto).", "published": "2024-08-12 15:19:59", "link": "http://arxiv.org/abs/2408.11847v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models", "abstract": "Vision-Language Models (VLMs) building upon the foundation of powerful large\nlanguage models have made rapid progress in reasoning across visual and textual\ndata. While VLMs perform well on vision tasks that they are trained on, our\nresults highlight key challenges in abstract pattern recognition. We present\nGlyphPattern, a 954 item dataset that pairs 318 human-written descriptions of\nvisual patterns from 40 writing systems with three visual presentation styles.\n  GlyphPattern evaluates abstract pattern recognition in VLMs, requiring models\nto understand and judge natural language descriptions of visual patterns.\nGlyphPattern patterns are drawn from a large-scale cognitive science\ninvestigation of human writing systems; as a result, they are rich in spatial\nreference and compositionality. Our experiments show that GlyphPattern is\nchallenging for state-of-the-art VLMs (GPT-4o achieves only 55% accuracy), with\nmarginal gains from few-shot prompting. Our detailed error analysis reveals\nchallenges at multiple levels, including visual processing, natural language\nunderstanding, and pattern generalization.", "published": "2024-08-12 02:16:47", "link": "http://arxiv.org/abs/2408.05894v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A New Pipeline For Generating Instruction Dataset via RAG and Self\n  Fine-Tuning", "abstract": "With the rapid development of large language models in recent years, there\nhas been an increasing demand for domain-specific Agents that can cater to the\nunique needs of enterprises and organizations. Unlike general models, which\nstrive for broad coverage, these specialized Agents rely on focused datasets\ntailored to their intended applications. This research proposes a pipeline that\nleverages the power of LLMs and the Retrieval-Augmented Generation related\nframework to construct high-quality instruction datasets for fine-tuning on\nspecific domains using custom document collections. By ingesting\ndomain-specific documents, the pipeline generates relevant and contextually\nappropriate instructions, thus effectively creating a comprehensive dataset for\nfine-tuning LLMs on the target domain. This approach overcomes the limitations\nof traditional dataset creation methods, which often rely on manual curation or\nweb-scraping techniques that may introduce noise and irrelevant data. Notably,\nour pipeline offers a dynamic solution that can quickly adapt to updates or\nmodifications in the domain-specific document collection, eliminating the need\nfor complete retraining. Additionally, it addresses the challenge of data\nscarcity by enabling the generation of instruction datasets from a limited set\nof initial documents, rendering it suitable for unpopular or specialized\ndomains where comprehensive datasets are scarce. As a case study, we apply this\napproach to the domain of psychiatry, a field requiring specialized knowledge\nand sensitive handling of patient information. The resulting fine-tuned LLM\ndemonstrates showcases the viability of the proposed approach and underscores\nits potential for widespread adoption across various industries and domains\nwhere tailored, accurate, and contextually relevant language models are\nindispensable.", "published": "2024-08-12 03:52:11", "link": "http://arxiv.org/abs/2408.05911v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Global-to-Local Support Spectrums for Language Model Explainability", "abstract": "Existing sample-based methods, like influence functions and representer\npoints, measure the importance of a training point by approximating the effect\nof its removal from training. As such, they are skewed towards outliers and\npoints that are very close to the decision boundaries. The explanations\nprovided by these methods are often static and not specific enough for\ndifferent test points. In this paper, we propose a method to generate an\nexplanation in the form of support spectrums which are based on two main ideas:\nthe support sets and a global-to-local importance measure. The support set is\nthe set of training points, in the predicted class, that ``lie in between'' the\ntest point and training points in the other classes. They indicate how well the\ntest point can be distinguished from the points not in the predicted class. The\nglobal-to-local importance measure is obtained by decoupling existing methods\ninto the global and local components which are then used to select the points\nin the support set. Using this method, we are able to generate explanations\nthat are tailored to specific test points. In the experiments, we show the\neffectiveness of the method in image classification and text generation tasks.", "published": "2024-08-12 08:05:30", "link": "http://arxiv.org/abs/2408.05976v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Language of Trauma: Modeling Traumatic Event Descriptions Across\n  Domains with Explainable AI", "abstract": "Psychological trauma can manifest following various distressing events and is\ncaptured in diverse online contexts. However, studies traditionally focus on a\nsingle aspect of trauma, often neglecting the transferability of findings\nacross different scenarios. We address this gap by training language models\nwith progressing complexity on trauma-related datasets, including\ngenocide-related court data, a Reddit dataset on post-traumatic stress disorder\n(PTSD), counseling conversations, and Incel forum posts. Our results show that\nthe fine-tuned RoBERTa model excels in predicting traumatic events across\ndomains, slightly outperforming large language models like GPT-4. Additionally,\nSLALOM-feature scores and conceptual explanations effectively differentiate and\ncluster trauma-related language, highlighting different trauma aspects and\nidentifying sexual abuse and experiences related to death as a common traumatic\nevent across all datasets. This transferability is crucial as it allows for the\ndevelopment of tools to enhance trauma detection and intervention in diverse\npopulations and settings.", "published": "2024-08-12 08:05:30", "link": "http://arxiv.org/abs/2408.05977v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation\n  Using Large Language Models and Transformers", "abstract": "In the rapidly evolving fields of natural language processing and computer\nvision, Visual Word Sense Disambiguation (VWSD) stands as a critical, yet\nchallenging task. The quest for models that can seamlessly integrate and\ninterpret multimodal data is more pressing than ever. Imagine a system that can\nunderstand language with the depth and nuance of human cognition, while\nsimultaneously interpreting the rich visual context of the world around it.\n  We present ARPA, an architecture that fuses the unparalleled contextual\nunderstanding of large language models with the advanced feature extraction\ncapabilities of transformers, which then pass through a custom Graph Neural\nNetwork (GNN) layer to learn intricate relationships and subtle nuances within\nthe data. This innovative architecture not only sets a new benchmark in visual\nword disambiguation but also introduces a versatile framework poised to\ntransform how linguistic and visual data interact by harnessing the synergistic\nstrengths of its components, ensuring robust performance even in the most\ncomplex disambiguation scenarios. Through a series of experiments and\ncomparative analysis, we reveal the substantial advantages of our model,\nunderscoring its potential to redefine standards in the field. Beyond its\narchitectural prowess, our architecture excels through experimental\nenrichments, including sophisticated data augmentation and multi-modal training\ntechniques.\n  ARPA's introduction marks a significant milestone in visual word\ndisambiguation, offering a compelling solution that bridges the gap between\nlinguistic and visual modalities. We invite researchers and practitioners to\nexplore the capabilities of our model, envisioning a future where such hybrid\nmodels drive unprecedented advancements in artificial intelligence.", "published": "2024-08-12 10:15:13", "link": "http://arxiv.org/abs/2408.06040v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Quantum Algorithms for Compositional Text Processing", "abstract": "Quantum computing and AI have found a fruitful intersection in the field of\nnatural language processing. We focus on the recently proposed DisCoCirc\nframework for natural language, and propose a quantum adaptation, QDisCoCirc.\nThis is motivated by a compositional approach to rendering AI interpretable:\nthe behavior of the whole can be understood in terms of the behavior of parts,\nand the way they are put together. For the model-native primitive operation of\ntext similarity, we derive quantum algorithms for fault-tolerant quantum\ncomputers to solve the task of question-answering within QDisCoCirc, and show\nthat this is BQP-hard; note that we do not consider the complexity of\nquestion-answering in other natural language processing models. Assuming\nwidely-held conjectures, implementing the proposed model classically would\nrequire super-polynomial resources. Therefore, it could provide a meaningful\ndemonstration of the power of practical quantum processors. The model\nconstruction builds on previous work in compositional quantum natural language\nprocessing. Word embeddings are encoded as parameterized quantum circuits, and\ncompositionality here means that the quantum circuits compose according to the\nlinguistic structure of the text. We outline a method for evaluating the model\non near-term quantum processors, and elsewhere we report on a recent\nimplementation of this on quantum hardware. In addition, we adapt a quantum\nalgorithm for the closest vector problem to obtain a Grover-like speedup in the\nfault-tolerant regime for our model. This provides an unconditional quadratic\nspeedup over any classical algorithm in certain circumstances, which we will\nverify empirically in future work.", "published": "2024-08-12 11:21:40", "link": "http://arxiv.org/abs/2408.06061v1", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "How ChatGPT Changed the Media's Narratives on AI: A Semi-Automated\n  Narrative Analysis Through Frame Semantics", "abstract": "We perform a mixed-method frame semantics-based analysis on a dataset of more\nthan 49,000 sentences collected from 5846 news articles that mention AI. The\ndataset covers the twelve-month period centred around the launch of OpenAI's\nchatbot ChatGPT and is collected from the most visited open-access\nEnglish-language news publishers. Our findings indicate that during the six\nmonths succeeding the launch, media attention rose tenfold$\\unicode{x2014}$from\nalready historically high levels. During this period, discourse has become\nincreasingly centred around experts and political leaders, and AI has become\nmore closely associated with dangers and risks. A deeper review of the data\nalso suggests a qualitative shift in the types of threat AI is thought to\nrepresent, as well as the anthropomorphic qualities ascribed to it.", "published": "2024-08-12 13:02:31", "link": "http://arxiv.org/abs/2408.06120v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Med42-v2: A Suite of Clinical LLMs", "abstract": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\n\\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.", "published": "2024-08-12 13:37:31", "link": "http://arxiv.org/abs/2408.06142v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Structural Diversity of Blackbox LLMs via\n  Chain-of-Specification Prompting", "abstract": "The capability to generate diverse text is a key challenge facing large\nlanguage models (LLMs). Thus far, diversity has been studied via metrics such\nas $n$-gram diversity or diversity of BERT embeddings. However, for these kinds\nof diversity, the user has little control over the dimensions along which\ndiversity is considered. For example, in the poetry domain, one might desire\ndiversity in terms of rhyme and meter, whereas in the code domain, one might\ndesire diversity in terms of the kinds of expressions used to solve a problem.\nWe propose a diversity metric called structural diversity, where the user\nprovides a mapping from generated text to features capturing the kinds of\ndiversity that they care about. In addition, we propose a novel strategy called\nchain-of-specification (CoS) prompting for improving diversity by first having\nthe LLM generate a specification encoding one instance of structural features,\nand then prompting the LLM to generate text that satisfies these features;\nnotably, our strategy works with blackbox LLMs. In our experiments, we show\nthat for structural diversity in the poetry and code domains, CoS significantly\nimproves diversity compared to several baselines.", "published": "2024-08-12 14:34:06", "link": "http://arxiv.org/abs/2408.06186v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Effects of Steering Latent Representation for Large Language Model\n  Unlearning", "abstract": "Representation Misdirection for Unlearning (RMU), which steers model\nrepresentation in the intermediate layer to a target random representation, is\nan effective method for large language model (LLM) unlearning. Despite its high\nperformance, the underlying cause and explanation remain underexplored. In this\npaper, we theoretically demonstrate that steering forget representations in the\nintermediate layer reduces token confidence, causing LLMs to generate wrong or\nnonsense responses. We investigate how the coefficient influences the alignment\nof forget-sample representations with the random direction and hint at the\noptimal coefficient values for effective unlearning across different network\nlayers. We show that RMU unlearned models are robust against adversarial\njailbreak attacks. Furthermore, our empirical analysis shows that RMU is less\neffective when applied to the middle and later layers in LLMs. To resolve this\ndrawback, we propose Adaptive RMU--a simple yet effective alternative method\nthat makes unlearning effective with most layers. Extensive experiments\ndemonstrate that Adaptive RMU significantly improves the unlearning performance\ncompared to prior art while incurring no additional computational cost.", "published": "2024-08-12 15:24:50", "link": "http://arxiv.org/abs/2408.06223v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Context-aware Visual Storytelling with Visual Prefix Tuning and\n  Contrastive Learning", "abstract": "Visual storytelling systems generate multi-sentence stories from image\nsequences. In this task, capturing contextual information and bridging visual\nvariation bring additional challenges. We propose a simple yet effective\nframework that leverages the generalization capabilities of pretrained\nfoundation models, only training a lightweight vision-language mapping network\nto connect modalities, while incorporating context to enhance coherence. We\nintroduce a multimodal contrastive objective that also improves visual\nrelevance and story informativeness. Extensive experimental results, across\nboth automatic metrics and human evaluations, demonstrate that the stories\ngenerated by our framework are diverse, coherent, informative, and interesting.", "published": "2024-08-12 16:15:32", "link": "http://arxiv.org/abs/2408.06259v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MovieSum: An Abstractive Summarization Dataset for Movie Screenplays", "abstract": "Movie screenplay summarization is challenging, as it requires an\nunderstanding of long input contexts and various elements unique to movies.\nLarge language models have shown significant advancements in document\nsummarization, but they often struggle with processing long input contexts.\nFurthermore, while television transcripts have received attention in recent\nstudies, movie screenplay summarization remains underexplored. To stimulate\nresearch in this area, we present a new dataset, MovieSum, for abstractive\nsummarization of movie screenplays. This dataset comprises 2200 movie\nscreenplays accompanied by their Wikipedia plot summaries. We manually\nformatted the movie screenplays to represent their structural elements.\nCompared to existing datasets, MovieSum possesses several distinctive features:\n(1) It includes movie screenplays, which are longer than scripts of TV\nepisodes. (2) It is twice the size of previous movie screenplay datasets. (3)\nIt provides metadata with IMDb IDs to facilitate access to additional external\nknowledge. We also show the results of recently released large language models\napplied to summarization on our dataset to provide a detailed baseline.", "published": "2024-08-12 16:43:09", "link": "http://arxiv.org/abs/2408.06281v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Long-Form Answers to Visual Questions from Blind and Low Vision People", "abstract": "Vision language models can now generate long-form answers to questions about\nimages - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a\ndataset of long-form answers to visual questions posed by blind and low vision\n(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,\ncollected from human expert describers and six VQA models. We develop and\nannotate functional roles of sentences of LFVQA and demonstrate that long-form\nanswers contain information beyond the question answer such as explanations and\nsuggestions. We further conduct automatic and human evaluations with BLV and\nsighted people to evaluate long-form answers. BLV people perceive both\nhuman-written and generated long-form answers to be plausible, but generated\nanswers often hallucinate incorrect visual details, especially for unanswerable\nvisual questions (e.g., blurry or irrelevant images). To reduce hallucinations,\nwe evaluate the ability of VQA models to abstain from answering unanswerable\nquestions across multiple prompting strategies.", "published": "2024-08-12 17:15:02", "link": "http://arxiv.org/abs/2408.06303v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Evaluating LLMs on Entity Disambiguation in Tables", "abstract": "Tables are crucial containers of information, but understanding their meaning\nmay be challenging. Over the years, there has been a surge in interest in\ndata-driven approaches based on deep learning that have increasingly been\ncombined with heuristic-based ones. In the last period, the advent of\n\\acf{llms} has led to a new category of approaches for table annotation.\nHowever, these approaches have not been consistently evaluated on a common\nground, making evaluation and comparison difficult. This work proposes an\nextensive evaluation of four STI SOTA approaches: Alligator (formerly s-elbat),\nDagobah, TURL, and TableLlama; the first two belong to the family of\nheuristic-based algorithms, while the others are respectively encoder-only and\ndecoder-only Large Language Models (LLMs). We also include in the evaluation\nboth GPT-4o and GPT-4o-mini, since they excel in various public benchmarks. The\nprimary objective is to measure the ability of these approaches to solve the\nentity disambiguation task with respect to both the performance achieved on a\ncommon-ground evaluation setting and the computational and cost requirements\ninvolved, with the ultimate aim of charting new research paths in the field.", "published": "2024-08-12 18:01:50", "link": "http://arxiv.org/abs/2408.06423v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in\n  Language Models", "abstract": "We propose a novel in-context learning algorithm for building autonomous\ndecision-making language agents. The language agent continuously attempts to\nsolve the same task by self-correcting each time the task fails. Our selected\nlanguage agent demonstrates the ability to solve tasks in a text-based game\nenvironment. Our results show that the gemma-2-9b-it language model, using our\nproposed method, can successfully complete two of six tasks that failed in the\nfirst attempt. This highlights the effectiveness of our approach in enhancing\nthe problem-solving capabilities of a single language model through\nself-correction, paving the way for more advanced autonomous agents. The code\nis publicly available at\nhttps://github.com/YenCheHsiao/AutonomousLLMAgentwithAdaptingPlanning.", "published": "2024-08-12 19:18:05", "link": "http://arxiv.org/abs/2408.06458v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Cross-Lingual Conversational Speech Summarization with Large Language\n  Models", "abstract": "Cross-lingual conversational speech summarization is an important problem,\nbut suffers from a dearth of resources. While transcriptions exist for a number\nof languages, translated conversational speech is rare and datasets containing\nsummaries are non-existent. We build upon the existing Fisher and Callhome\nSpanish-English Speech Translation corpus by supplementing the translations\nwith summaries. The summaries are generated using GPT-4 from the reference\ntranslations and are treated as ground truth. The task is to generate similar\nsummaries in the presence of transcription and translation errors. We build a\nbaseline cascade-based system using open-source speech recognition and machine\ntranslation models. We test a range of LLMs for summarization and analyze the\nimpact of transcription and translation errors. Adapting the Mistral-7B model\nfor this task performs significantly better than off-the-shelf models and\nmatches the performance of GPT-4.", "published": "2024-08-12 20:40:46", "link": "http://arxiv.org/abs/2408.06484v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Hierarchical in-Context Reinforcement Learning and\n  Hindsight Modular Reflections for Task Planning with LLMs", "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in\nvarious language tasks, making them promising candidates for decision-making in\nrobotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose\nRetrieval-Augmented in-context reinforcement Learning (RAHL), a novel framework\nthat decomposes complex tasks into sub-tasks using an LLM-based high-level\npolicy, in which a complex task is decomposed into sub-tasks by a high-level\npolicy on-the-fly. The sub-tasks, defined by goals, are assigned to the\nlow-level policy to complete. To improve the agent's performance in\nmulti-episode execution, we propose Hindsight Modular Reflection (HMR), where,\ninstead of reflecting on the full trajectory, we let the agent reflect on\nshorter sub-trajectories to improve reflection efficiency. We evaluated the\ndecision-making ability of the proposed RAHL in three benchmark\nenvironments--ALFWorld, Webshop, and HotpotQA. The results show that RAHL can\nachieve an improvement in performance in 9%, 42%, and 10% in 5 episodes of\nexecution in strong baselines. Furthermore, we also implemented RAHL on the\nBoston Dynamics SPOT robot. The experiment shows that the robot can scan the\nenvironment, find entrances, and navigate to new rooms controlled by the LLM\npolicy.", "published": "2024-08-12 22:40:01", "link": "http://arxiv.org/abs/2408.06520v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Rethinking the Alignment of Psychotherapy Dialogue Generation with\n  Motivational Interviewing Strategies", "abstract": "Recent advancements in large language models (LLMs) have shown promise in\ngenerating psychotherapeutic dialogues, particularly in the context of\nmotivational interviewing (MI). However, the inherent lack of transparency in\nLLM outputs presents significant challenges given the sensitive nature of\npsychotherapy. Applying MI strategies, a set of MI skills, to generate more\ncontrollable therapeutic-adherent conversations with explainability provides a\npossible solution. In this work, we explore the alignment of LLMs with MI\nstrategies by first prompting the LLMs to predict the appropriate strategies as\nreasoning and then utilizing these strategies to guide the subsequent dialogue\ngeneration. We seek to investigate whether such alignment leads to more\ncontrollable and explainable generations. Multiple experiments including\nautomatic and human evaluations are conducted to validate the effectiveness of\nMI strategies in aligning psychotherapy dialogue generation. Our findings\ndemonstrate the potential of LLMs in producing strategically aligned dialogues\nand suggest directions for practical applications in psychotherapeutic\nsettings.", "published": "2024-08-12 23:19:02", "link": "http://arxiv.org/abs/2408.06527v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge\n  Graph QA datasets with Large Language Models", "abstract": "The rapid advancement of Large Language Models (LLMs) and conversational\nassistants necessitates dynamic, scalable, and configurable conversational\ndatasets for training and evaluation. These datasets must accommodate diverse\nuser interaction modes, including text and voice, each presenting unique\nmodeling challenges. Knowledge Graphs (KGs), with their structured and evolving\nnature, offer an ideal foundation for current and precise knowledge. Although\nhuman-curated KG-based conversational datasets exist, they struggle to keep\npace with the rapidly changing user information needs. We present ConvKGYarn, a\nscalable method for generating up-to-date and configurable conversational KGQA\ndatasets. Qualitative psychometric analyses confirm our method can generate\nhigh-quality datasets rivaling a popular conversational KGQA dataset while\noffering it at scale and covering a wide range of human-interaction\nconfigurations. We showcase its utility by testing LLMs on diverse\nconversations - exploring model behavior on conversational KGQA sets with\ndifferent configurations grounded in the same KG fact set. Our results\nhighlight the ability of ConvKGYarn to improve KGQA foundations and evaluate\nparametric knowledge of LLMs, thus offering a robust solution to the constantly\nevolving landscape of conversational assistants.", "published": "2024-08-12 06:48:43", "link": "http://arxiv.org/abs/2408.05948v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controlling Surprisal in Music Generation via Information Content Curve\n  Matching", "abstract": "In recent years, the quality and public interest in music generation systems\nhave grown, encouraging research into various ways to control these systems. We\npropose a novel method for controlling surprisal in music generation using\nsequence models. To achieve this goal, we define a metric called Instantaneous\nInformation Content (IIC). The IIC serves as a proxy function for the perceived\nmusical surprisal (as estimated from a probabilistic model) and can be\ncalculated at any point within a music piece. This enables the comparison of\nsurprisal across different musical content even if the musical events occur in\nirregular time intervals. We use beam search to generate musical material whose\nIIC curve closely approximates a given target IIC. We experimentally show that\nthe IIC correlates with harmonic and rhythmic complexity and note density. The\ncorrelation decreases with the length of the musical context used for\nestimating the IIC. Finally, we conduct a qualitative user study to test if\nhuman listeners can identify the IIC curves that have been used as targets when\ngenerating the respective musical material. We provide code for creating IIC\ninterpolations and IIC visualizations on https://github.com/muthissar/iic.", "published": "2024-08-12 09:21:41", "link": "http://arxiv.org/abs/2408.06022v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Dialogue Speech Recognition with Robust Contextual Awareness\n  via Noise Representation Learning", "abstract": "Recent dialogue systems rely on turn-based spoken interactions, requiring\naccurate Automatic Speech Recognition (ASR). Errors in ASR can significantly\nimpact downstream dialogue tasks. To address this, using dialogue context from\nuser and agent interactions for transcribing subsequent utterances has been\nproposed. This method incorporates the transcription of the user's speech and\nthe agent's response as model input, using the accumulated context generated by\neach turn. However, this context is susceptible to ASR errors because it is\ngenerated by the ASR model in an auto-regressive fashion. Such noisy context\ncan further degrade the benefits of context input, resulting in suboptimal ASR\nperformance. In this paper, we introduce Context Noise Representation Learning\n(CNRL) to enhance robustness against noisy context, ultimately improving\ndialogue speech recognition accuracy. To maximize the advantage of context\nawareness, our approach includes decoder pre-training using text-based dialogue\ndata and noise representation learning for a context encoder. Based on the\nevaluation of speech dialogues, our method shows superior results compared to\nbaselines. Furthermore, the strength of our approach is highlighted in noisy\nenvironments where user speech is barely audible due to real-world noise,\nrelying on contextual information to transcribe the input accurately.", "published": "2024-08-12 10:21:09", "link": "http://arxiv.org/abs/2408.06043v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Investigation Into Explainable Audio Hate Speech Detection", "abstract": "Research on hate speech has predominantly revolved around detection and\ninterpretation from textual inputs, leaving verbal content largely unexplored.\nWhile there has been limited exploration into hate speech detection within\nverbal acoustic speech inputs, the aspect of interpretability has been\noverlooked. Therefore, we introduce a new task of explainable audio hate speech\ndetection. Specifically, we aim to identify the precise time intervals,\nreferred to as audio frame-level rationales, which serve as evidence for hate\nspeech classification. Towards this end, we propose two different approaches:\ncascading and End-to-End (E2E). The cascading approach initially converts audio\nto transcripts, identifies hate speech within these transcripts, and\nsubsequently locates the corresponding audio time frames. Conversely, the E2E\napproach processes audio utterances directly, which allows it to pinpoint hate\nspeech within specific time frames. Additionally, due to the lack of\nexplainable audio hate speech datasets that include audio frame-level\nrationales, we curated a synthetic audio dataset to train our models. We\nfurther validated these models on actual human speech utterances and found that\nthe E2E approach outperforms the cascading method in terms of the audio frame\nIntersection over Union (IoU) metric. Furthermore, we observed that including\nframe-level rationales significantly enhances hate speech detection accuracy\nfor the E2E approach.\n  \\textbf{Disclaimer} The reader may encounter content of an offensive or\nhateful nature. However, given the nature of the work, this cannot be avoided.", "published": "2024-08-12 11:32:34", "link": "http://arxiv.org/abs/2408.06065v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Building Decision Making Models Through Language Model Regime", "abstract": "We propose a novel approach for decision making problems leveraging the\ngeneralization capabilities of large language models (LLMs). Traditional\nmethods such as expert systems, planning algorithms, and reinforcement learning\noften exhibit limited generalization, typically requiring the training of new\nmodels for each unique task. In contrast, LLMs demonstrate remarkable success\nin generalizing across varied language tasks, inspiring a new strategy for\ntraining decision making models. Our approach, referred to as \"Learning then\nUsing\" (LTU), entails a two-stage process. Initially, the \\textit{learning}\nphase develops a robust foundational decision making model by integrating\ndiverse knowledge from various domains and decision making contexts. The\nsubsequent \\textit{using} phase refines this foundation model for specific\ndecision making scenarios. Distinct from other studies that employ LLMs for\ndecision making through supervised learning, our LTU method embraces a\nversatile training methodology that combines broad pre-training with targeted\nfine-tuning. Experiments in e-commerce domains such as advertising and search\noptimization have shown that LTU approach outperforms traditional supervised\nlearning regimes in decision making capabilities and generalization. The LTU\napproach is the first practical training architecture for both single-step and\nmulti-step decision making tasks combined with LLMs, which can be applied\nbeyond game and robot domains. It provides a robust and adaptable framework for\ndecision making, enhances the effectiveness and flexibility of various systems\nin tackling various challenges.", "published": "2024-08-12 12:04:14", "link": "http://arxiv.org/abs/2408.06087v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid\n  Library", "abstract": "In this study, we generate and maintain a database of 10 million virtual\nlipids through METiS's in-house de novo lipid generation algorithms and lipid\nvirtual screening techniques. These virtual lipids serve as a corpus for\npre-training, lipid representation learning, and downstream task knowledge\ntransfer, culminating in state-of-the-art LNP property prediction performance.\nWe propose LipidBERT, a BERT-like model pre-trained with the Masked Language\nModel (MLM) and various secondary tasks. Additionally, we compare the\nperformance of embeddings generated by LipidBERT and PhatGPT, our GPT-like\nlipid generation model, on downstream tasks. The proposed bilingual LipidBERT\nmodel operates in two languages: the language of ionizable lipid pre-training,\nusing in-house dry-lab lipid structures, and the language of LNP fine-tuning,\nutilizing in-house LNP wet-lab data. This dual capability positions LipidBERT\nas a key AI-based filter for future screening tasks, including new versions of\nMETiS de novo lipid libraries and, more importantly, candidates for in vivo\ntesting for orgran-targeting LNPs. To the best of our knowledge, this is the\nfirst successful demonstration of the capability of a pre-trained language\nmodel on virtual lipids and its effectiveness in downstream tasks using web-lab\ndata. This work showcases the clever utilization of METiS's in-house de novo\nlipid library as well as the power of dry-wet lab integration.", "published": "2024-08-12 13:44:24", "link": "http://arxiv.org/abs/2408.06150v2", "categories": ["cs.CL", "physics.chem-ph", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks", "abstract": "This paper introduces FLEURS-R, a speech restoration applied version of the\nFew-shot Learning Evaluation of Universal Representations of Speech (FLEURS)\ncorpus. FLEURS-R maintains an N-way parallel speech corpus in 102 languages as\nFLEURS, with improved audio quality and fidelity by applying the speech\nrestoration model Miipher. The aim of FLEURS-R is to advance speech technology\nin more languages and catalyze research including text-to-speech (TTS) and\nother speech generation tasks in low-resource languages. Comprehensive\nevaluations with the restored speech and TTS baseline models trained from the\nnew corpus show that the new corpus obtained significantly improved speech\nquality while maintaining the semantic contents of the speech. The corpus is\npublicly released via Hugging Face.", "published": "2024-08-12 15:28:51", "link": "http://arxiv.org/abs/2408.06227v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment", "abstract": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.", "published": "2024-08-12 16:24:51", "link": "http://arxiv.org/abs/2408.06266v5", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Synthetic Patient-Physician Dialogue Generation from Clinical Notes\n  Using LLM", "abstract": "Medical dialogue systems (MDS) enhance patient-physician communication,\nimprove healthcare accessibility, and reduce costs. However, acquiring suitable\ndata to train these systems poses significant challenges. Privacy concerns\nprevent the use of real conversations, necessitating synthetic alternatives.\nSynthetic dialogue generation from publicly available clinical notes offers a\npromising solution to this issue, providing realistic data while safeguarding\nprivacy. Our approach, SynDial, uses a single LLM iteratively with zero-shot\nprompting and a feedback loop to generate and refine high-quality synthetic\ndialogues. The feedback consists of weighted evaluation scores for similarity\nand extractiveness. The iterative process ensures dialogues meet predefined\nthresholds, achieving superior extractiveness as a result of the feedback loop.\nAdditionally, evaluation shows that the generated dialogues excel in factuality\nmetric compared to the baselines and has comparable diversity scores with GPT4.", "published": "2024-08-12 16:49:22", "link": "http://arxiv.org/abs/2408.06285v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific\n  Discovery", "abstract": "One of the grand challenges of artificial general intelligence is developing\nagents capable of conducting scientific research and discovering new knowledge.\nWhile frontier models have already been used as aides to human scientists, e.g.\nfor brainstorming ideas, writing code, or prediction tasks, they still conduct\nonly a small part of the scientific process. This paper presents the first\ncomprehensive framework for fully automatic scientific discovery, enabling\nfrontier large language models to perform research independently and\ncommunicate their findings. We introduce The AI Scientist, which generates\nnovel research ideas, writes code, executes experiments, visualizes results,\ndescribes its findings by writing a full scientific paper, and then runs a\nsimulated review process for evaluation. In principle, this process can be\nrepeated to iteratively develop ideas in an open-ended fashion, acting like the\nhuman scientific community. We demonstrate its versatility by applying it to\nthree distinct subfields of machine learning: diffusion modeling,\ntransformer-based language modeling, and learning dynamics. Each idea is\nimplemented and developed into a full paper at a cost of less than $15 per\npaper. To evaluate the generated papers, we design and validate an automated\nreviewer, which we show achieves near-human performance in evaluating paper\nscores. The AI Scientist can produce papers that exceed the acceptance\nthreshold at a top machine learning conference as judged by our automated\nreviewer. This approach signifies the beginning of a new era in scientific\ndiscovery in machine learning: bringing the transformative benefits of AI\nagents to the entire research process of AI itself, and taking us closer to a\nworld where endless affordable creativity and innovation can be unleashed on\nthe world's most challenging problems. Our code is open-sourced at\nhttps://github.com/SakanaAI/AI-Scientist", "published": "2024-08-12 16:58:11", "link": "http://arxiv.org/abs/2408.06292v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation\n  Agents", "abstract": "Large Multimodal Models (LMMs) have ushered in a new era in artificial\nintelligence, merging capabilities in both language and vision to form highly\ncapable Visual Foundation Agents. These agents are postulated to excel across a\nmyriad of tasks, potentially approaching general artificial intelligence.\nHowever, existing benchmarks fail to sufficiently challenge or showcase the\nfull potential of LMMs in complex, real-world environments. To address this\ngap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering\nbenchmark specifically designed to train and evaluate LMMs as visual foundation\nagents across diverse scenarios, including Embodied, Graphical User Interface,\nand Visual Design, with tasks formulated to probe the depth of LMMs'\nunderstanding and interaction capabilities. Through rigorous testing across\nnine proprietary LMM APIs and eight open models, we demonstrate the\nconsiderable yet still developing agent capabilities of these models.\nAdditionally, VAB constructs a trajectory training set constructed through\nhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and\nHuman Demonstrations, promoting substantial performance improvements in LMMs\nthrough behavior cloning. Our work not only aims to benchmark existing models\nbut also provides a solid foundation for future development into visual\nfoundation agents. Code, train \\& test data, and part of fine-tuned open LMMs\nare available at \\url{https://github.com/THUDM/VisualAgentBench}.", "published": "2024-08-12 17:44:17", "link": "http://arxiv.org/abs/2408.06327v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "LOLgorithm: Integrating Semantic,Syntactic and Contextual Elements for\n  Humor Classification", "abstract": "This paper explores humor detection through a linguistic lens, prioritizing\nsyntactic, semantic, and contextual features over computational methods in\nNatural Language Processing. We categorize features into syntactic, semantic,\nand contextual dimensions, including lexicons, structural statistics, Word2Vec,\nWordNet, and phonetic style. Our proposed model, Colbert, utilizes BERT\nembeddings and parallel hidden layers to capture sentence congruity. By\ncombining syntactic, semantic, and contextual features, we train Colbert for\nhumor detection. Feature engineering examines essential syntactic and semantic\nfeatures alongside BERT embeddings. SHAP interpretations and decision trees\nidentify influential features, revealing that a holistic approach improves\nhumor detection accuracy on unseen data. Integrating linguistic cues from\ndifferent dimensions enhances the model's ability to understand humor\ncomplexity beyond traditional computational methods.", "published": "2024-08-12 17:52:11", "link": "http://arxiv.org/abs/2408.06335v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Language Models for Efficient Code Generation", "abstract": "We introduce Differential Performance Evaluation (DPE), a framework designed\nto reliably evaluate Large Language Models (LLMs) for efficient code\ngeneration. Traditional coding benchmarks often fail to provide reliable\ninsights into code efficiency, due to their reliance on simplistic test inputs\nand the absence of effective compound metrics. DPE addresses these issues by\nfocusing on efficiency-demanding programming tasks and establishing an\ninsightful compound metric for performance evaluation. DPE operates in two\nphases: To curate efficiency datasets, it selects efficiency-demanding tasks\nfrom existing coding benchmarks and generates computationally expensive inputs\nto stress the efficiency of LLM solutions. To assess the code efficiency, DPE\nprofiles the new solution and compares it globally against a set of reference\nsolutions that exhibit distinct efficiency levels, where the matched level\ndefines its efficiency score. As a proof of concept, we use DPE to create\nEvalPerf, a benchmark with 121 performance-challenging coding tasks. Our\ncomprehensive evaluation draws interesting findings on the efficiency impact of\nmodel sizes, instruction tuning, and prompting. For example, while the scaling\nlaw fails to account for code efficiency, general instruction tuning benefits\nboth code correctness and efficiency. We also evaluate the evaluation by\nexamining the effectiveness of DPE, showing that EvalPerf is reliable and\nconvenient to use even across platforms.", "published": "2024-08-12 18:59:13", "link": "http://arxiv.org/abs/2408.06450v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "TOGGL: Transcribing Overlapping Speech with Staggered Labeling", "abstract": "Transcribing the speech of multiple overlapping speakers typically requires\nseparating the audio into multiple streams and recognizing each one\nindependently. More recent work jointly separates and transcribes, but requires\na separate decoding component for each speaker. We propose the TOGGL model to\nsimultaneously transcribe the speech of multiple speakers. The TOGGL model uses\nspecial output tokens to attribute the speech to each speaker with only a\nsingle decoder. Our approach generalizes beyond two speakers, even when trained\nonly on two-speaker data. We demonstrate superior performance compared to\ncompeting approaches on a conversational speech dataset. Our approach also\nimproves performance on single-speaker audio.", "published": "2024-08-12 20:19:27", "link": "http://arxiv.org/abs/2408.06474v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "What Color Scheme is More Effective in Assisting Readers to Locate\n  Information in a Color-Coded Article?", "abstract": "Color coding, a technique assigning specific colors to cluster information\ntypes, has proven advantages in aiding human cognitive activities, especially\nreading and comprehension. The rise of Large Language Models (LLMs) has\nstreamlined document coding, enabling simple automatic text labeling with\nvarious schemes. This has the potential to make color-coding more accessible\nand benefit more users. However, the impact of color choice on information\nseeking is understudied. We conducted a user study assessing various color\nschemes' effectiveness in LLM-coded text documents, standardizing contrast\nratios to approximately 5.55:1 across schemes. Participants performed timed\ninformation-seeking tasks in color-coded scholarly abstracts. Results showed\nnon-analogous and yellow-inclusive color schemes improved performance, with the\nlatter also being more preferred by participants. These findings can inform\nbetter color scheme choices for text annotation. As LLMs advance document\ncoding, we advocate for more research focusing on the \"color\" aspect of\ncolor-coding techniques.", "published": "2024-08-12 21:04:16", "link": "http://arxiv.org/abs/2408.06494v2", "categories": ["cs.HC", "cs.CL", "cs.CV"], "primary_category": "cs.HC"}
{"title": "Speech vs. Transcript: Does It Matter for Human Annotators in Speech\n  Summarization?", "abstract": "Reference summaries for abstractive speech summarization require human\nannotation, which can be performed by listening to an audio recording or by\nreading textual transcripts of the recording. In this paper, we examine whether\nsummaries based on annotators listening to the recordings differ from those\nbased on annotators reading transcripts. Using existing intrinsic evaluation\nbased on human evaluation, automatic metrics, LLM-based evaluation, and a\nretrieval-based reference-free method. We find that summaries are indeed\ndifferent based on the source modality, and that speech-based summaries are\nmore factually consistent and information-selective than transcript-based\nsummaries. Meanwhile, transcript-based summaries are impacted by recognition\nerrors in the source, and expert-written summaries are more informative and\nreliable. We make all the collected data and analysis code\npublic(https://github.com/cmu-mlsp/interview_humanssum) to facilitate the\nreproduction of our work and advance research in this area.", "published": "2024-08-12 13:25:53", "link": "http://arxiv.org/abs/2408.07277v1", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Statistical Patterns in the Equations of Physics and the Emergence of a\n  Meta-Law of Nature", "abstract": "Physics, as a fundamental science, aims to understand the laws of Nature and\ndescribe them in mathematical equations. While the physical reality manifests\nitself in a wide range of phenomena with varying levels of complexity, the\nequations that describe them display certain statistical regularities and\npatterns, which we begin to explore here. By drawing inspiration from\nlinguistics, where Zipf's law states that the frequency of any word in a large\ncorpus of text is roughly inversely proportional to its rank in the frequency\ntable, we investigate whether similar patterns for the distribution of\noperators emerge in the equations of physics. We analyse three corpora of\nformulae and find, using sophisticated implicit-likelihood methods, that the\nfrequency of operators as a function of their rank in the frequency table is\nbest described by an exponential law with a stable exponent, in contrast with\nZipf's inverse power-law. Understanding the underlying reasons behind this\nstatistical pattern may shed light on Nature's modus operandi or reveal\nrecurrent patterns in physicists' attempts to formalise the laws of Nature. It\nmay also provide crucial input for symbolic regression, potentially augmenting\nlanguage models to generate symbolic models for physical phenomena. By\npioneering the study of statistical regularities in the equations of physics,\nour results open the door for a meta-law of Nature, a (probabilistic) law that\nall physical laws obey.", "published": "2024-08-12 18:34:57", "link": "http://arxiv.org/abs/2408.11065v1", "categories": ["physics.soc-ph", "cs.CL", "hep-th", "physics.data-an", "physics.hist-ph"], "primary_category": "physics.soc-ph"}
{"title": "Adapting General Disentanglement-Based Speaker Anonymization for\n  Enhanced Emotion Preservation", "abstract": "A general disentanglement-based speaker anonymization system typically\nseparates speech into content, speaker, and prosody features using individual\nencoders. This paper explores how to adapt such a system when a new speech\nattribute, for example, emotion, needs to be preserved to a greater extent.\nWhile existing systems are good at anonymizing speaker embeddings, they are not\ndesigned to preserve emotion. Two strategies for this are examined. First, we\nshow that integrating emotion embeddings from a pre-trained emotion encoder can\nhelp preserve emotional cues, even though this approach slightly compromises\nprivacy protection. Alternatively, we propose an emotion compensation strategy\nas a post-processing step applied to anonymized speaker embeddings. This\nconceals the original speaker's identity and reintroduces the emotional traits\nlost during speaker embedding anonymization. Specifically, we model the emotion\nattribute using support vector machines to learn separate boundaries for each\nemotion. During inference, the original speaker embedding is processed in two\nways: one, by an emotion indicator to predict emotion and select the\nemotion-matched SVM accurately; and two, by a speaker anonymizer to conceal\nspeaker characteristics. The anonymized speaker embedding is then modified\nalong the corresponding SVM boundary towards an enhanced emotional direction to\nsave the emotional cues. The proposed strategies are also expected to be useful\nfor adapting a general disentanglement-based speaker anonymization system to\npreserve other target paralinguistic attributes, with potential for a range of\ndownstream tasks.", "published": "2024-08-12 05:40:21", "link": "http://arxiv.org/abs/2408.05928v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PyNeuralFx: A Python Package for Neural Audio Effect Modeling", "abstract": "We present PyNeuralFx, an open-source Python toolkit designed for research on\nneural audio effect modeling. The toolkit provides an intuitive framework and\noffers a comprehensive suite of features, including standardized implementation\nof well-established model architectures, loss functions, and easy-to-use\nvisualization tools. As such, it helps promote reproducibility for research on\nneural audio effect modeling, and enable in-depth performance comparison of\ndifferent models, offering insight into the behavior and operational\ncharacteristics of models through DSP methodology. The toolkit can be found at\nhttps://github.com/ytsrt66589/pyneuralfx.", "published": "2024-08-12 10:59:45", "link": "http://arxiv.org/abs/2408.06053v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust online reconstruction of continuous-time signals from a lean\n  spike train ensemble code", "abstract": "Sensory stimuli in animals are encoded into spike trains by neurons, offering\nadvantages such as sparsity, energy efficiency, and high temporal resolution.\nThis paper presents a signal processing framework that deterministically\nencodes continuous-time signals into biologically feasible spike trains, and\naddresses the questions about representable signal classes and reconstruction\nbounds. The framework considers encoding of a signal through spike trains\ngenerated by an ensemble of neurons using a convolve-then-threshold mechanism\nwith various convolution kernels. A closed-form solution to the inverse\nproblem, from spike trains to signal reconstruction, is derived in the Hilbert\nspace of shifted kernel functions, ensuring sparse representation of a\ngeneralized Finite Rate of Innovation (FRI) class of signals. Additionally,\ninspired by real-time processing in biological systems, an efficient iterative\nversion of the optimal reconstruction is formulated that considers only a\nfinite window of past spikes, ensuring robustness of the technique to\nill-conditioned encoding; convergence guarantees of the windowed reconstruction\nto the optimal solution are then provided. Experiments on a large audio dataset\ndemonstrate excellent reconstruction accuracy at spike rates as low as\none-fifth of the Nyquist rate, while showing clear competitive advantage in\ncomparison to state-of-the-art sparse coding techniques in the low spike rate\nregime.", "published": "2024-08-12 06:55:51", "link": "http://arxiv.org/abs/2408.05950v2", "categories": ["cs.NE", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.NE"}
{"title": "Audio Enhancement for Computer Audition -- An Iterative Training\n  Paradigm Using Sample Importance", "abstract": "Neural network models for audio tasks, such as automatic speech recognition\n(ASR) and acoustic scene classification (ASC), are susceptible to noise\ncontamination for real-life applications. To improve audio quality, an\nenhancement module, which can be developed independently, is explicitly used at\nthe front-end of the target audio applications. In this paper, we present an\nend-to-end learning solution to jointly optimise the models for audio\nenhancement (AE) and the subsequent applications. To guide the optimisation of\nthe AE module towards a target application, and especially to overcome\ndifficult samples, we make use of the sample-wise performance measure as an\nindication of sample importance. In experiments, we consider four\nrepresentative applications to evaluate our training paradigm, i.e., ASR,\nspeech command recognition (SCR), speech emotion recognition (SER), and ASC.\nThese applications are associated with speech and non-speech tasks concerning\nsemantic and non-semantic features, transient and global information, and the\nexperimental results indicate that our proposed approach can considerably boost\nthe noise robustness of the models, especially at low signal-to-noise ratios\n(SNRs), for a wide range of computer audition tasks in everyday-life noisy\nenvironments.", "published": "2024-08-12 16:23:58", "link": "http://arxiv.org/abs/2408.06264v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FoVNet: Configurable Field-of-View Speech Enhancement with Low\n  Computation and Distortion for Smart Glasses", "abstract": "This paper presents a novel multi-channel speech enhancement approach,\nFoVNet, that enables highly efficient speech enhancement within a configurable\nfield of view (FoV) of a smart-glasses user without needing specific\ntarget-talker(s) directions. It advances over prior works by enhancing all\nspeakers within any given FoV, with a hybrid signal processing and deep\nlearning approach designed with high computational efficiency. The neural\nnetwork component is designed with ultra-low computation (about 50 MMACS). A\nmulti-channel Wiener filter and a post-processing module are further used to\nimprove perceptual quality. We evaluate our algorithm with a microphone array\non smart glasses, providing a configurable, efficient solution for augmented\nhearing on energy-constrained devices. FoVNet excels in both computational\nefficiency and speech quality across multiple scenarios, making it a promising\nsolution for smart glasses applications.", "published": "2024-08-12 19:42:42", "link": "http://arxiv.org/abs/2408.06468v1", "categories": ["cs.SD", "cs.MM", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Music2Latent: Consistency Autoencoders for Latent Audio Compression", "abstract": "Efficient audio representations in a compressed continuous latent space are\ncritical for generative audio modeling and Music Information Retrieval (MIR)\ntasks. However, some existing audio autoencoders have limitations, such as\nmulti-stage training procedures, slow iterative sampling, or low reconstruction\nquality. We introduce Music2Latent, an audio autoencoder that overcomes these\nlimitations by leveraging consistency models. Music2Latent encodes samples into\na compressed continuous latent space in a single end-to-end training process\nwhile enabling high-fidelity single-step reconstruction. Key innovations\ninclude conditioning the consistency model on upsampled encoder outputs at all\nlevels through cross connections, using frequency-wise self-attention to\ncapture long-range frequency dependencies, and employing frequency-wise learned\nscaling to handle varying value distributions across frequencies at different\nnoise levels. We demonstrate that Music2Latent outperforms existing continuous\naudio autoencoders in sound quality and reconstruction accuracy while achieving\ncompetitive performance on downstream MIR tasks using its latent\nrepresentations. To our knowledge, this represents the first successful attempt\nat training an end-to-end consistency autoencoder model.", "published": "2024-08-12 21:25:19", "link": "http://arxiv.org/abs/2408.06500v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Source Separation of Multi-source Raw Music using a Residual Quantized\n  Variational Autoencoder", "abstract": "I developed a neural audio codec model based on the residual quantized\nvariational autoencoder architecture. I train the model on the Slakh2100\ndataset, a standard dataset for musical source separation, composed of\nmulti-track audio. The model can separate audio sources, achieving almost SoTA\nresults with much less computing power. The code is publicly available at\ngithub.com/LeonardoBerti00/Source-Separation-of-Multi-source-Music-using-Residual-Quantizad-Variational-Autoencoder", "published": "2024-08-12 17:30:17", "link": "http://arxiv.org/abs/2408.07020v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
