{"title": "Reducing the gap between streaming and non-streaming Transducer-based\n  ASR by adaptive two-stage knowledge distillation", "abstract": "Transducer is one of the mainstream frameworks for streaming speech\nrecognition. There is a performance gap between the streaming and non-streaming\ntransducer models due to limited context. To reduce this gap, an effective way\nis to ensure that their hidden and output distributions are consistent, which\ncan be achieved by hierarchical knowledge distillation. However, it is\ndifficult to ensure the distribution consistency simultaneously because the\nlearning of the output distribution depends on the hidden one. In this paper,\nwe propose an adaptive two-stage knowledge distillation method consisting of\nhidden layer learning and output layer learning. In the former stage, we learn\nhidden representation with full context by applying mean square error loss\nfunction. In the latter stage, we design a power transformation based adaptive\nsmoothness method to learn stable output distribution. It achieved 19\\%\nrelative reduction in word error rate, and a faster response for the first\ntoken compared with the original streaming model in LibriSpeech corpus.", "published": "2023-06-27 03:11:21", "link": "http://arxiv.org/abs/2306.15171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emulating Reader Behaviors for Fake News Detection", "abstract": "The wide dissemination of fake news has affected our lives in many aspects,\nmaking fake news detection important and attracting increasing attention.\nExisting approaches make substantial contributions in this field by modeling\nnews from a single-modal or multi-modal perspective. However, these modal-based\nmethods can result in sub-optimal outcomes as they ignore reader behaviors in\nnews consumption and authenticity verification. For instance, they haven't\ntaken into consideration the component-by-component reading process: from the\nheadline, images, comments, to the body, which is essential for modeling news\nwith more granularity. To this end, we propose an approach of Emulating the\nbehaviors of readers (Ember) for fake news detection on social media,\nincorporating readers' reading and verificating process to model news from the\ncomponent perspective thoroughly. Specifically, we first construct\nintra-component feature extractors to emulate the behaviors of semantic\nanalyzing on each component. Then, we design a module that comprises\ninter-component feature extractors and a sequence-based aggregator. This module\nmimics the process of verifying the correlation between components and the\noverall reading and verification sequence. Thus, Ember can handle the news with\nvarious components by emulating corresponding sequences. We conduct extensive\nexperiments on nine real-world datasets, and the results demonstrate the\nsuperiority of Ember.", "published": "2023-06-27 06:14:24", "link": "http://arxiv.org/abs/2306.15231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue\n  Evaluation", "abstract": "Existing reference-free turn-level evaluation metrics for chatbots\ninadequately capture the interaction between the user and the system.\nConsequently, they often correlate poorly with human evaluations. To address\nthis issue, we propose a novel model-agnostic approach that leverages\nConditional Pointwise Mutual Information (C-PMI) to measure the turn-level\ninteraction between the system and the user based on a given evaluation\ndimension. Experimental results on the widely used FED dialogue evaluation\ndataset demonstrate that our approach significantly improves the correlation\nwith human judgment compared with existing evaluation systems. By replacing the\nnegative log-likelihood-based scorer with our proposed C-PMI scorer, we achieve\na relative 62.6% higher Spearman correlation on average for the FED evaluation\nmetric. Our code is publicly available at https://github.com/renll/C-PMI.", "published": "2023-06-27 06:58:03", "link": "http://arxiv.org/abs/2306.15245v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for\n  Situated Neural Dialogue Generation", "abstract": "Humans talk in daily conversations while aligning and negotiating the\nexpressed meanings or common ground. Despite the impressive conversational\nabilities of the large generative language models, they do not consider the\nindividual differences in contextual understanding in a shared situated\nenvironment. In this work, we propose MindDial, a novel conversational\nframework that can generate situated free-form responses with theory-of-mind\nmodeling. We introduce an explicit mind module that can track the speaker's\nbelief and the speaker's prediction of the listener's belief. Then the next\nresponse is generated to resolve the belief difference and take task-related\naction. Our framework is applied to both prompting and fine-tuning-based\nmodels, and is evaluated across scenarios involving both common ground\nalignment and negotiation. Experiments show that models with mind modeling can\nachieve higher task outcomes when aligning and negotiating common ground. The\nablation study further validates the three-level belief design can aggregate\ninformation and improve task outcomes in both cooperative and negotiating\nsettings.", "published": "2023-06-27 07:24:32", "link": "http://arxiv.org/abs/2306.15253v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Out-of-Distribution Evaluation of Neural NLP Models", "abstract": "Adversarial robustness, domain generalization and dataset biases are three\nactive lines of research contributing to out-of-distribution (OOD) evaluation\non neural NLP models. However, a comprehensive, integrated discussion of the\nthree research lines is still lacking in the literature. In this survey, we 1)\ncompare the three lines of research under a unifying definition; 2) summarize\nthe data-generating processes and evaluation protocols for each line of\nresearch; and 3) emphasize the challenges and opportunities for future work.", "published": "2023-06-27 07:44:25", "link": "http://arxiv.org/abs/2306.15261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Pretrained Language Models Derive Correct Semantics from Corrupt\n  Subwords under Noise?", "abstract": "For Pretrained Language Models (PLMs), their susceptibility to noise has\nrecently been linked to subword segmentation. However, it is unclear which\naspects of segmentation affect their understanding. This study assesses the\nrobustness of PLMs against various disrupted segmentation caused by noise. An\nevaluation framework for subword segmentation, named Contrastive Lexical\nSemantic (CoLeS) probe, is proposed. It provides a systematic categorization of\nsegmentation corruption under noise and evaluation protocols by generating\ncontrastive datasets with canonical-noisy word pairs. Experimental results\nindicate that PLMs are unable to accurately compute word meanings if the noise\nintroduces completely different subwords, small subword fragments, or a large\nnumber of additional subwords, particularly when they are inserted within other\nsubwords.", "published": "2023-06-27 07:51:01", "link": "http://arxiv.org/abs/2306.15268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Client Reactions in Online Mental Health Counseling", "abstract": "Communication success relies heavily on reading participants' reactions. Such\nfeedback is especially important for mental health counselors, who must\ncarefully consider the client's progress and adjust their approach accordingly.\nHowever, previous NLP research on counseling has mainly focused on studying\ncounselors' intervention strategies rather than their clients' reactions to the\nintervention. This work aims to fill this gap by developing a theoretically\ngrounded annotation framework that encompasses counselors' strategies and\nclient reaction behaviors. The framework has been tested against a large-scale,\nhigh-quality text-based counseling dataset we collected over the past two years\nfrom an online welfare counseling platform. Our study shows how clients react\nto counselors' strategies, how such reactions affect the final counseling\noutcomes, and how counselors can adjust their strategies in response to these\nreactions. We also demonstrate that this study can help counselors\nautomatically predict their clients' states.", "published": "2023-06-27 09:39:54", "link": "http://arxiv.org/abs/2306.15334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Pseudo Future Contexts for Emotion Recognition in\n  Conversations", "abstract": "With the extensive accumulation of conversational data on the Internet,\nemotion recognition in conversations (ERC) has received increasing attention.\nPrevious efforts of this task mainly focus on leveraging contextual and\nspeaker-specific features, or integrating heterogeneous external commonsense\nknowledge. Among them, some heavily rely on future contexts, which, however,\nare not always available in real-life scenarios. This fact inspires us to\ngenerate pseudo future contexts to improve ERC. Specifically, for an utterance,\nwe generate its future context with pre-trained language models, potentially\ncontaining extra beneficial knowledge in a conversational form homogeneous with\nthe historical ones. These characteristics make pseudo future contexts easily\nfused with historical contexts and historical speaker-specific contexts,\nyielding a conceptually simple framework systematically integrating\nmulti-contexts. Experimental results on four ERC datasets demonstrate our\nmethod's superiority. Further in-depth analyses reveal that pseudo future\ncontexts can rival real ones to some extent, especially in relatively\ncontext-independent conversations.", "published": "2023-06-27 10:51:02", "link": "http://arxiv.org/abs/2306.15376v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quality Estimation of Machine Translated Texts based on Direct Evidence\n  from Training Data", "abstract": "Current Machine Translation systems achieve very good results on a growing\nvariety of language pairs and data sets. However, it is now well known that\nthey produce fluent translation outputs that often can contain important\nmeaning errors. Quality Estimation task deals with the estimation of quality of\ntranslations produced by a Machine Translation system without depending on\nReference Translations. A number of approaches have been suggested over the\nyears. In this paper we show that the parallel corpus used as training data for\ntraining the MT system holds direct clues for estimating the quality of\ntranslations produced by the MT system. Our experiments show that this simple\nand direct method holds promise for quality estimation of translations produced\nby any purely data driven machine translation system.", "published": "2023-06-27 11:52:28", "link": "http://arxiv.org/abs/2306.15399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KnowPrefix-Tuning: A Two-Stage Prefix-Tuning Framework for\n  Knowledge-Grounded Dialogue Generation", "abstract": "Existing knowledge-grounded conversation systems generate responses typically\nin a retrieve-then-generate manner. They require a large knowledge base and a\nstrong knowledge retrieval component, which is time- and resource-consuming. In\nthis paper, we address the challenge by leveraging the inherent knowledge\nencoded in the pre-trained language models (PLMs). We propose Knowledgeable\nPrefix Tuning (KnowPrefix-Tuning), a two-stage tuning framework, bypassing the\nretrieval process in a knowledge-grounded conversation system by injecting\nprior knowledge into the lightweight knowledge prefix. The knowledge prefix is\na sequence of continuous knowledge-specific vectors that can be learned during\ntraining. In addition, we propose a novel interactive re-parameterization\nmechanism that allows the prefix to interact fully with the PLM during the\noptimization of response generation. Experimental results demonstrate that\nKnowPrefix-Tuning outperforms fine-tuning and other lightweight tuning\napproaches, and performs comparably with strong retrieval-based baselines while\nbeing $3\\times$ faster during inference.", "published": "2023-06-27 12:38:49", "link": "http://arxiv.org/abs/2306.15430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paradigm Shift in Sustainability Disclosure Analysis: Empowering\n  Stakeholders with CHATREPORT, a Language Model-Based Tool", "abstract": "This paper introduces a novel approach to enhance Large Language Models\n(LLMs) with expert knowledge to automate the analysis of corporate\nsustainability reports by benchmarking them against the Task Force for\nClimate-Related Financial Disclosures (TCFD) recommendations. Corporate\nsustainability reports are crucial in assessing organizations' environmental\nand social risks and impacts. However, analyzing these reports' vast amounts of\ninformation makes human analysis often too costly. As a result, only a few\nentities worldwide have the resources to analyze these reports, which could\nlead to a lack of transparency. While AI-powered tools can automatically\nanalyze the data, they are prone to inaccuracies as they lack domain-specific\nexpertise. This paper introduces a novel approach to enhance LLMs with expert\nknowledge to automate the analysis of corporate sustainability reports. We\nchristen our tool CHATREPORT, and apply it in a first use case to assess\ncorporate climate risk disclosures following the TCFD recommendations.\nCHATREPORT results from collaborating with experts in climate science, finance,\neconomic policy, and computer science, demonstrating how domain experts can be\ninvolved in developing AI tools. We make our prompt templates, generated data,\nand scores available to the public to encourage transparency.", "published": "2023-06-27 14:46:47", "link": "http://arxiv.org/abs/2306.15518v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Annotation of Direct Speech in Written French Narratives", "abstract": "The automatic annotation of direct speech (AADS) in written text has been\noften used in computational narrative understanding. Methods based on either\nrules or deep neural networks have been explored, in particular for English or\nGerman languages. Yet, for French, our target language, not many works exist.\nOur goal is to create a unified framework to design and evaluate AADS models in\nFrench. For this, we consolidated the largest-to-date French narrative dataset\nannotated with DS per word; we adapted various baselines for sequence labelling\nor from AADS in other languages; and we designed and conducted an extensive\nevaluation focused on generalisation. Results show that the task still requires\nsubstantial efforts and emphasise characteristics of each baseline. Although\nthis framework could be improved, it is a step further to encourage more\nresearch on the topic.", "published": "2023-06-27 17:21:00", "link": "http://arxiv.org/abs/2306.15634v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style-transfer based Speech and Audio-visual Scene Understanding for\n  Robot Action Sequence Acquisition from Videos", "abstract": "To realize human-robot collaboration, robots need to execute actions for new\ntasks according to human instructions given finite prior knowledge. Human\nexperts can share their knowledge of how to perform a task with a robot through\nmulti-modal instructions in their demonstrations, showing a sequence of\nshort-horizon steps to achieve a long-horizon goal. This paper introduces a\nmethod for robot action sequence generation from instruction videos using (1)\nan audio-visual Transformer that converts audio-visual features and instruction\nspeech to a sequence of robot actions called dynamic movement primitives (DMPs)\nand (2) style-transfer-based training that employs multi-task learning with\nvideo captioning and weakly-supervised learning with a semantic classifier to\nexploit unpaired video-action data. We built a system that accomplishes various\ncooking actions, where an arm robot executes a DMP sequence acquired from a\ncooking video using the audio-visual Transformer. Experiments with\nEpic-Kitchen-100, YouCookII, QuerYD, and in-house instruction video datasets\nshow that the proposed method improves the quality of DMP sequences by 2.3\ntimes the METEOR score obtained with a baseline video-to-action Transformer.\nThe model achieved 32% of the task success rate with the task knowledge of the\nobject.", "published": "2023-06-27 17:37:53", "link": "http://arxiv.org/abs/2306.15644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Weakly Supervised Classifier and Dataset of White Supremacist Language", "abstract": "We present a dataset and classifier for detecting the language of white\nsupremacist extremism, a growing issue in online hate speech. Our weakly\nsupervised classifier is trained on large datasets of text from explicitly\nwhite supremacist domains paired with neutral and anti-racist data from similar\ndomains. We demonstrate that this approach improves generalization performance\nto new domains. Incorporating anti-racist texts as counterexamples to white\nsupremacist language mitigates bias.", "published": "2023-06-27 18:19:32", "link": "http://arxiv.org/abs/2306.15732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DMNER: Biomedical Entity Recognition by Detection and Matching", "abstract": "Biomedical named entity recognition (BNER) serves as the foundation for\nnumerous biomedical text mining tasks. Unlike general NER, BNER require a\ncomprehensive grasp of the domain, and incorporating external knowledge beyond\ntraining data poses a significant challenge. In this study, we propose a novel\nBNER framework called DMNER. By leveraging existing entity representation\nmodels SAPBERT, we tackle BNER as a two-step process: entity boundary detection\nand biomedical entity matching. DMNER exhibits applicability across multiple\nNER scenarios: 1) In supervised NER, we observe that DMNER effectively\nrectifies the output of baseline NER models, thereby further enhancing\nperformance. 2) In distantly supervised NER, combining MRC and AutoNER as span\nboundary detectors enables DMNER to achieve satisfactory results. 3) For\ntraining NER by merging multiple datasets, we adopt a framework similar to\nDS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the\ntraining. Through extensive experiments conducted on 10 benchmark datasets, we\ndemonstrate the versatility and effectiveness of DMNER.", "published": "2023-06-27 18:32:07", "link": "http://arxiv.org/abs/2306.15736v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identity Construction in a Misogynist Incels Forum", "abstract": "Online communities of involuntary celibates (incels) are a prominent source\nof misogynist hate speech. In this paper, we use quantitative text and network\nanalysis approaches to examine how identity groups are discussed on\nincels-dot-is, the largest black-pilled incels forum. We find that this\ncommunity produces a wide range of novel identity terms and, while terms for\nwomen are most common, mentions of other minoritized identities are increasing.\nAn analysis of the associations made with identity groups suggests an\nessentialist ideology where physical appearance, as well as gender and racial\nhierarchies, determine human value. We discuss implications for research into\nautomated misogynist hate speech detection.", "published": "2023-06-27 18:56:28", "link": "http://arxiv.org/abs/2306.15745v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SAHAAYAK 2023 -- the Multi Domain Bilingual Parallel Corpus of Sanskrit\n  to Hindi for Machine Translation", "abstract": "The data article presents the large bilingual parallel corpus of\nlow-resourced language pair Sanskrit-Hindi, named SAHAAYAK 2023. The corpus\ncontains total of 1.5M sentence pairs between Sanskrit and Hindi. To make the\nuniversal usability of the corpus and to make it balanced, data from multiple\ndomain has been incorporated into the corpus that includes, News, Daily\nconversations, Politics, History, Sport, and Ancient Indian Literature. The\nmultifaceted approach has been adapted to make a sizable multi-domain corpus of\nlow-resourced languages like Sanskrit. Our development approach is spanned from\ncreating a small hand-crafted dataset to applying a wide range of mining,\ncleaning, and verification. We have used the three-fold process of mining:\nmining from machine-readable sources, mining from non-machine readable sources,\nand collation from existing corpora sources. Post mining, the dedicated\npipeline for normalization, alignment, and corpus cleaning is developed and\napplied to the corpus to make it ready to use on machine translation\nalgorithms.", "published": "2023-06-27 11:06:44", "link": "http://arxiv.org/abs/2307.00021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Cross-Domain Behaviors of BERT in Review Understanding", "abstract": "Review score prediction requires review text understanding, a critical\nreal-world application of natural language processing. Due to dissimilar text\ndomains in product reviews, a common practice is fine-tuning BERT models upon\nreviews of differing domains. However, there has not yet been an empirical\nstudy of cross-domain behaviors of BERT models in the various tasks of product\nreview understanding. In this project, we investigate text classification BERT\nmodels fine-tuned on single-domain and multi-domain Amazon review data. In our\nfindings, though single-domain models achieved marginally improved performance\non their corresponding domain compared to multi-domain models, multi-domain\nmodels outperformed single-domain models when evaluated on multi-domain data,\nsingle-domain data the single-domain model was not fine-tuned on, and on\naverage when considering all tests. Though slight increases in accuracy can be\nachieved through single-domain model fine-tuning, computational resources and\ncosts can be reduced by utilizing multi-domain models that perform well across\ndomains.", "published": "2023-06-27 00:23:35", "link": "http://arxiv.org/abs/2306.15123v2", "categories": ["cs.CL", "cs.CE"], "primary_category": "cs.CL"}
{"title": "YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English\n  Parallel Corpus", "abstract": "Machine learning for sign languages is bottlenecked by data. In this paper,\nwe present YouTube-ASL, a large-scale, open-domain corpus of American Sign\nLanguage (ASL) videos and accompanying English captions drawn from YouTube.\nWith ~1000 hours of videos and >2500 unique signers, YouTube-ASL is ~3x as\nlarge and has ~10x as many unique signers as the largest prior ASL dataset. We\ntrain baseline models for ASL to English translation on YouTube-ASL and\nevaluate them on How2Sign, where we achieve a new finetuned state of the art of\n12.39 BLEU and, for the first time, report zero-shot results.", "published": "2023-06-27 02:44:07", "link": "http://arxiv.org/abs/2306.15162v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "DSRM: Boost Textual Adversarial Training with Distribution Shift Risk\n  Minimization", "abstract": "Adversarial training is one of the best-performing methods in improving the\nrobustness of deep language models. However, robust models come at the cost of\nhigh time consumption, as they require multi-step gradient ascents or word\nsubstitutions to obtain adversarial samples. In addition, these generated\nsamples are deficient in grammatical quality and semantic consistency, which\nimpairs the effectiveness of adversarial training. To address these problems,\nwe introduce a novel, effective procedure for instead adversarial training with\nonly clean data. Our procedure, distribution shift risk minimization (DSRM),\nestimates the adversarial loss by perturbing the input data's probability\ndistribution rather than their embeddings. This formulation results in a robust\nmodel that minimizes the expected global loss under adversarial attacks. Our\napproach requires zero adversarial samples for training and reduces time\nconsumption by up to 70\\% compared to current best-performing adversarial\ntraining methods. Experiments demonstrate that DSRM considerably improves\nBERT's resistance to textual adversarial attacks and achieves state-of-the-art\nrobust accuracy on various benchmarks.", "published": "2023-06-27 02:46:08", "link": "http://arxiv.org/abs/2306.15164v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GroundNLQ @ Ego4D Natural Language Queries Challenge 2023", "abstract": "In this report, we present our champion solution for Ego4D Natural Language\nQueries (NLQ) Challenge in CVPR 2023. Essentially, to accurately ground in a\nvideo, an effective egocentric feature extractor and a powerful grounding model\nare required. Motivated by this, we leverage a two-stage pre-training strategy\nto train egocentric feature extractors and the grounding model on video\nnarrations, and further fine-tune the model on annotated data. In addition, we\nintroduce a novel grounding model GroundNLQ, which employs a multi-modal\nmulti-scale grounding module for effective video and text fusion and various\ntemporal intervals, especially for long videos. On the blind test set,\nGroundNLQ achieves 25.67 and 18.18 for R1@IoU=0.3 and R1@IoU=0.5, respectively,\nand surpasses all other teams by a noticeable margin. Our code will be released\nat\\url{https://github.com/houzhijian/GroundNLQ}.", "published": "2023-06-27 07:27:52", "link": "http://arxiv.org/abs/2306.15255v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning", "abstract": "In the field of machine reading comprehension (MRC), existing systems have\nsurpassed the average performance of human beings in many tasks like SQuAD.\nHowever, there is still a long way to go when it comes to logical reasoning.\nAlthough some methods for it have been put forward, they either are designed in\na quite complicated way or rely too much on external structures. In this paper,\nwe proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand\nbut highly effective further pre-training task which logically strengthens the\npre-trained models with the help of 6 types of logical indicators and a\nlogically rich dataset LGP (LoGic Pre-training). IDOL achieves state-of-the-art\nperformance on ReClor and LogiQA, the two most representative benchmarks in\nlogical reasoning MRC, and is proven to be capable of generalizing to different\npre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0\nwhile keeping competitive general language understanding ability through\ntesting on tasks in GLUE. Besides, at the beginning of the era of large\nlanguage models, we take several of them like ChatGPT into comparison and find\nthat IDOL still shows its advantage.", "published": "2023-06-27 07:57:42", "link": "http://arxiv.org/abs/2306.15273v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Architecture of a Biologically Plausible Language Organ", "abstract": "We present a simulated biologically plausible language organ, made up of\nstylized but realistic neurons, synapses, brain areas, plasticity, and a\nsimplified model of sensory perception. We show through experiments that this\nmodel succeeds in an important early step in language acquisition: the learning\nof nouns, verbs, and their meanings, from the grounded input of only a modest\nnumber of sentences. Learning in this system is achieved through Hebbian\nplasticity, and without backpropagation. Our model goes beyond a parser\npreviously designed in a similar environment, with the critical addition of a\nbiologically plausible account for how language can be acquired in the infant's\nbrain, not just processed by a mature brain.", "published": "2023-06-27 10:25:22", "link": "http://arxiv.org/abs/2306.15364v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "CamemBERT-bio: Leveraging Continual Pre-training for Cost-Effective\n  Models on French Biomedical Data", "abstract": "Clinical data in hospitals are increasingly accessible for research through\nclinical data warehouses. However these documents are unstructured and it is\ntherefore necessary to extract information from medical reports to conduct\nclinical studies. Transfer learning with BERT-like models such as CamemBERT has\nallowed major advances for French, especially for named entity recognition.\nHowever, these models are trained for plain language and are less efficient on\nbiomedical data. Addressing this gap, we introduce CamemBERT-bio, a dedicated\nFrench biomedical model derived from a new public French biomedical dataset.\nThrough continual pre-training of the original CamemBERT, CamemBERT-bio\nachieves an improvement of 2.54 points of F1-score on average across various\nbiomedical named entity recognition tasks, reinforcing the potential of\ncontinual pre-training as an equally proficient yet less computationally\nintensive alternative to training from scratch. Additionally, we highlight the\nimportance of using a standard evaluation protocol that provides a clear view\nof the current state-of-the-art for French biomedical models.", "published": "2023-06-27 15:23:14", "link": "http://arxiv.org/abs/2306.15550v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Constructing Multilingual Code Search Dataset Using Neural Machine\n  Translation", "abstract": "Code search is a task to find programming codes that semantically match the\ngiven natural language queries. Even though some of the existing datasets for\nthis task are multilingual on the programming language side, their query data\nare only in English. In this research, we create a multilingual code search\ndataset in four natural and four programming languages using a neural machine\ntranslation model. Using our dataset, we pre-train and fine-tune the\nTransformer-based models and then evaluate them on multiple code search test\nsets. Our results show that the model pre-trained with all natural and\nprogramming language data has performed best in most cases. By applying\nback-translation data filtering to our dataset, we demonstrate that the\ntranslation quality affects the model's performance to a certain extent, but\nthe data size matters more.", "published": "2023-06-27 16:42:36", "link": "http://arxiv.org/abs/2306.15604v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "On the Universal Adversarial Perturbations for Efficient Data-free\n  Adversarial Detection", "abstract": "Detecting adversarial samples that are carefully crafted to fool the model is\na critical step to socially-secure applications. However, existing adversarial\ndetection methods require access to sufficient training data, which brings\nnoteworthy concerns regarding privacy leakage and generalizability. In this\nwork, we validate that the adversarial sample generated by attack algorithms is\nstrongly related to a specific vector in the high-dimensional inputs. Such\nvectors, namely UAPs (Universal Adversarial Perturbations), can be calculated\nwithout original training data. Based on this discovery, we propose a\ndata-agnostic adversarial detection framework, which induces different\nresponses between normal and adversarial samples to UAPs. Experimental results\nshow that our method achieves competitive detection performance on various text\nclassification tasks, and maintains an equivalent time consumption to normal\ninference.", "published": "2023-06-27 02:54:07", "link": "http://arxiv.org/abs/2306.15705v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Annotators: Enhancing Generalization of NLP\n  Models at Minimal Cost", "abstract": "State-of-the-art supervised NLP models achieve high accuracy but are also\nsusceptible to failures on inputs from low-data regimes, such as domains that\nare not represented in training data. As an approximation to collecting\nground-truth labels for the specific domain, we study the use of large language\nmodels (LLMs) for annotating inputs and improving the generalization of NLP\nmodels. Specifically, given a budget for LLM annotations, we present an\nalgorithm for sampling the most informative inputs to annotate and retrain the\nNLP model. We find that popular active learning strategies such as\nuncertainty-based sampling do not work well. Instead, we propose a sampling\nstrategy based on the difference in prediction scores between the base model\nand the finetuned NLP model, utilizing the fact that most NLP models are\nfinetuned from a base model. Experiments with classification (semantic\nsimilarity) and ranking (semantic search) tasks show that our sampling strategy\nleads to significant gains in accuracy for both the training and target\ndomains.", "published": "2023-06-27 19:29:55", "link": "http://arxiv.org/abs/2306.15766v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for\n  Brazilian Portuguese", "abstract": "We investigate the effectiveness of GPT-3.5 and GPT-4, two large language\nmodels, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese\nand compare their performance against Microsoft Word and Google Docs. We\nintroduce a GEC dataset for Brazilian Portuguese with four categories: Grammar,\nSpelling, Internet, and Fast typing. Our results show that while GPT-4 has\nhigher recall than other methods, LLMs tend to have lower precision, leading to\novercorrection. This study demonstrates the potential of LLMs as practical GEC\ntools for Brazilian Portuguese and encourages further exploration of LLMs for\nnon-English languages and other educational settings.", "published": "2023-06-27 20:37:54", "link": "http://arxiv.org/abs/2306.15788v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning", "abstract": "Fine-tuning large-scale pre-trained language models has been demonstrated\neffective for various natural language processing (NLP) tasks. Previous studies\nhave established that incorporating adversarial training during the fine-tuning\nstage can significantly enhance model generalization and robustness. However,\nfrom the perspective of game theory, such utilizations of adversarial training\ncorrespond to pure-strategy games, which are inherently limited in terms of the\nscope of their strategies, thereby still having room for improvement. In order\nto push the performance boundaries, we propose a novel Mixed-strategy\nAdversarial Training algorithm (MAT). Methodologically, we derive the Nash\nequilibrium of a mixed-strategy game for adversarial training using Entropy\nMirror Descent to establish MAT by sampling method. To verify the effectiveness\nof MAT, we conducted extensive benchmark experiments on large-scale pre-trained\nmodels, such as BERT and RoBERTa. MAT significantly outperforms the\nstate-of-the-art methods on both the GLUE and ANLI benchmarks in terms of\ngeneralization and robustness.", "published": "2023-06-27 23:19:53", "link": "http://arxiv.org/abs/2306.15826v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Symbol emergence as interpersonal cross-situational learning: the\n  emergence of lexical knowledge with combinatoriality", "abstract": "We present a computational model for a symbol emergence system that enables\nthe emergence of lexical knowledge with combinatoriality among agents through a\nMetropolis-Hastings naming game and cross-situational learning. Many\ncomputational models have been proposed to investigate combinatoriality in\nemergent communication and symbol emergence in cognitive and developmental\nrobotics. However, existing models do not sufficiently address category\nformation based on sensory-motor information and semiotic communication through\nthe exchange of word sequences within a single integrated model. Our proposed\nmodel facilitates the emergence of lexical knowledge with combinatoriality by\nperforming category formation using multimodal sensory-motor information and\nenabling semiotic communication through the exchange of word sequences among\nagents in a unified model. Furthermore, the model enables an agent to predict\nsensory-motor information for unobserved situations by combining words\nassociated with categories in each modality. We conducted two experiments with\ntwo humanoid robots in a simulated environment to evaluate our proposed model.\nThe results demonstrated that the agents can acquire lexical knowledge with\ncombinatoriality through interpersonal cross-situational learning based on the\nMetropolis-Hastings naming game and cross-situational learning. Furthermore,\nour results indicate that the lexical knowledge developed using our proposed\nmodel exhibits generalization performance for novel situations through\ninterpersonal cross-modal inference.", "published": "2023-06-27 23:55:33", "link": "http://arxiv.org/abs/2306.15837v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Rank in Generative Retrieval", "abstract": "Generative retrieval stands out as a promising new paradigm in text retrieval\nthat aims to generate identifier strings of relevant passages as the retrieval\ntarget. This generative paradigm taps into powerful generative language models,\ndistinct from traditional sparse or dense retrieval methods. However, only\nlearning to generate is insufficient for generative retrieval. Generative\nretrieval learns to generate identifiers of relevant passages as an\nintermediate goal and then converts predicted identifiers into the final\npassage rank list. The disconnect between the learning objective of\nautoregressive models and the desired passage ranking target leads to a\nlearning gap. To bridge this gap, we propose a learning-to-rank framework for\ngenerative retrieval, dubbed LTRGR. LTRGR enables generative retrieval to learn\nto rank passages directly, optimizing the autoregressive model toward the final\npassage ranking target via a rank loss. This framework only requires an\nadditional learning-to-rank training phase to enhance current generative\nretrieval systems and does not add any burden to the inference stage. We\nconducted experiments on three public benchmarks, and the results demonstrate\nthat LTRGR achieves state-of-the-art performance among generative retrieval\nmethods. The code and checkpoints are released at\nhttps://github.com/liyongqi67/LTRGR.", "published": "2023-06-27 05:48:14", "link": "http://arxiv.org/abs/2306.15222v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment\n  Rating in a Realistic Downstream Classification Task", "abstract": "Pretrained language models are publicly available and constantly finetuned\nfor various real-life applications. As they become capable of grasping complex\ncontextual information, harmful biases are likely increasingly intertwined with\nthose models. This paper analyses gender bias in BERT models with two main\ncontributions: First, a novel bias measure is introduced, defining biases as\nthe difference in sentiment valuation of female and male sample versions.\nSecond, we comprehensively analyse BERT's biases on the example of a realistic\nIMDB movie classifier. By systematically varying elements of the training\npipeline, we can conclude regarding their impact on the final model bias. Seven\ndifferent public BERT models in nine training conditions, i.e. 63 models in\ntotal, are compared. Almost all conditions yield significant gender biases.\nResults indicate that reflected biases stem from public BERT models rather than\ntask-specific data, emphasising the weight of responsible usage.", "published": "2023-06-27 08:36:35", "link": "http://arxiv.org/abs/2306.15298v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and\n  Multi-Dialect Corpus for Speech Representation Disentanglement", "abstract": "Disentangling uncorrelated information in speech utterances is a crucial\nresearch topic within speech community. Different speech-related tasks focus on\nextracting distinct speech representations while minimizing the affects of\nother uncorrelated information. We present a large-scale speech corpus to\nfacilitate the research of speech representation disentanglement. 3D-Speaker\ncontains over 10,000 speakers, each of whom are simultaneously recorded by\nmultiple Devices, locating at different Distances, and some speakers are\nspeaking multiple Dialects. The controlled combinations of multi-dimensional\naudio data yield a matrix of a diverse blend of speech representation\nentanglement, thereby motivating intriguing methods to untangle them. The\nmulti-domain nature of 3D-Speaker also makes it a suitable resource to evaluate\nlarge universal speech models and experiment methods of out-of-domain learning\nand self-supervised learning. https://3dspeaker.github.io/", "published": "2023-06-27 10:09:43", "link": "http://arxiv.org/abs/2306.15354v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Phase Space Analysis of Cardiac Spectra", "abstract": "Cardiac diseases are one of the main reasons of mortality in modern,\nindustrialized societies, and they cause high expenses in public health\nsystems. Therefore, it is important to develop analytical methods to improve\ncardiac diagnostics. Electric activity of heart was first modeled by using a\nset of nonlinear differential equations. Latter, variations of cardiac spectra\noriginated from deterministic dynamics are investigated. Analyzing the power\nspectra of a normal human heart presents His-Purkinje network, possessing a\nfractal like structure. Phase space trajectories are extracted from the time\nseries graph of ECG. Lower values of fractal dimension, D indicate dynamics\nthat are more coherent. If D has non-integer values greater than two when the\nsystem becomes chaotic or strange attractor. Recently, the development of a\nfast and robust method, which can be applied to multichannel physiologic\nsignals, was reported. This manuscript investigates two different ECG systems\nproduced from normal and abnormal human hearts to introduce an auxiliary phase\nspace method in conjunction with ECG signals for diagnoses of heart diseases.\nHere, the data for each person includes two signals based on V_4 and modified\nlead III (MLIII) respectively. Fractal analysis method is employed on the\ntrajectories constructed in phase space, from which the fractal dimension D is\nobtained using the box counting method. It is observed that, MLIII signals have\nlarger D values than the first signals (V_4), predicting more randomness yet\nmore information. The lowest value of D (1.708) indicates the perfect\noscillation of the normal heart and the highest value of D (1.863) presents the\nrandomness of the abnormal heart. Our significant finding is that the phase\nspace picture presents the distribution of the peak heights from the ECG\nspectra, giving valuable information about heart activities in conjunction with\nECG.", "published": "2023-06-27 12:37:21", "link": "http://arxiv.org/abs/2306.15425v1", "categories": ["physics.med-ph", "cs.CL", "cs.CV"], "primary_category": "physics.med-ph"}
{"title": "Using Large Language Models to Provide Explanatory Feedback to Human\n  Tutors", "abstract": "Research demonstrates learners engaging in the process of producing\nexplanations to support their reasoning, can have a positive impact on\nlearning. However, providing learners real-time explanatory feedback often\npresents challenges related to classification accuracy, particularly in\ndomain-specific environments, containing situationally complex and nuanced\nresponses. We present two approaches for supplying tutors real-time feedback\nwithin an online lesson on how to give students effective praise. This\nwork-in-progress demonstrates considerable accuracy in binary classification\nfor corrective feedback of effective, or effort-based (F1 score = 0.811), and\nineffective, or outcome-based (F1 score = 0.350), praise responses. More\nnotably, we introduce progress towards an enhanced approach of providing\nexplanatory feedback using large language model-facilitated named entity\nrecognition, which can provide tutors feedback, not only while engaging in\nlessons, but can potentially suggest real-time tutor moves. Future work\ninvolves leveraging large language models for data augmentation to improve\naccuracy, while also developing an explanatory feedback interface.", "published": "2023-06-27 14:19:12", "link": "http://arxiv.org/abs/2306.15498v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Unleashing the Power of User Reviews: Exploring Airline Choices at\n  Catania Airport, Italy", "abstract": "This study aims to investigate the possible relationship between the\nmechanisms of social influence and the choice of airline, through the use of\nnew tools, with the aim of understanding whether they can contribute to a\nbetter understanding of the factors influencing the decisions of consumers in\nthe aviation sector. We have chosen to extract user reviews from well-known\nplatforms: Trustpilot, Google, and Twitter. By combining web scraping\ntechniques, we have been able to collect a comprehensive dataset comprising a\nwide range of user opinions, feedback, and ratings. We then refined the BERT\nmodel to focus on insightful sentiment in the context of airline reviews.\nThrough our analysis, we observed an intriguing trend of average negative\nsentiment scores across various airlines, giving us deeper insight into the\ndynamics between airlines and helping us identify key partnerships, popular\nroutes, and airlines that play a central role in the aeronautical ecosystem of\nCatania airport during the specified period. Our investigation led us to find\nthat, despite an airline having received prestigious awards as a low-cost\nleader in Europe for two consecutive years 2021 and 2022, the \"Catanese\" user\ntends to suffer the dominant position of other companies. Understanding the\nimpact of positive reviews and leveraging sentiment analysis can help airlines\nimprove their reputation, attract more customers, and ultimately gain a\ncompetitive edge in the marketplace.", "published": "2023-06-27 15:10:57", "link": "http://arxiv.org/abs/2306.15541v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MyCrunchGPT: A chatGPT assisted framework for scientific machine\n  learning", "abstract": "Scientific Machine Learning (SciML) has advanced recently across many\ndifferent areas in computational science and engineering. The objective is to\nintegrate data and physics seamlessly without the need of employing elaborate\nand computationally taxing data assimilation schemes. However, preprocessing,\nproblem formulation, code generation, postprocessing and analysis are still\ntime consuming and may prevent SciML from wide applicability in industrial\napplications and in digital twin frameworks. Here, we integrate the various\nstages of SciML under the umbrella of ChatGPT, to formulate MyCrunchGPT, which\nplays the role of a conductor orchestrating the entire workflow of SciML based\non simple prompts by the user. Specifically, we present two examples that\ndemonstrate the potential use of MyCrunchGPT in optimizing airfoils in\naerodynamics, and in obtaining flow fields in various geometries in interactive\nmode, with emphasis on the validation stage. To demonstrate the flow of the\nMyCrunchGPT, and create an infrastructure that can facilitate a broader vision,\nwe built a webapp based guided user interface, that includes options for a\ncomprehensive summary report. The overall objective is to extend MyCrunchGPT to\nhandle diverse problems in computational mechanics, design, optimization and\ncontrols, and general scientific computing tasks involved in SciML, hence using\nit as a research assistant tool but also as an educational tool. While here the\nexamples focus in fluid mechanics, future versions will target solid mechanics\nand materials science, geophysics, systems biology and bioinformatics.", "published": "2023-06-27 15:23:42", "link": "http://arxiv.org/abs/2306.15551v2", "categories": ["cs.LG", "cs.CL", "physics.soc-ph"], "primary_category": "cs.LG"}
{"title": "Extending Context Window of Large Language Models via Positional\n  Interpolation", "abstract": "We present Position Interpolation (PI) that extends the context window sizes\nof RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal\nfine-tuning (within 1000 steps), while demonstrating strong empirical results\non various tasks that require long context, including passkey retrieval,\nlanguage modeling, and long document summarization from LLaMA 7B to 65B.\nMeanwhile, the extended model by Position Interpolation preserve quality\nrelatively well on tasks within its original context window. To achieve this\ngoal, Position Interpolation linearly down-scales the input position indices to\nmatch the original context window size, rather than extrapolating beyond the\ntrained context length which may lead to catastrophically high attention scores\nthat completely ruin the self-attention mechanism. Our theoretical study shows\nthat the upper bound of interpolation is at least $\\sim 600 \\times$ smaller\nthan that of extrapolation, further demonstrating its stability. Models\nextended via Position Interpolation retain its original architecture and can\nreuse most pre-existing optimization and infrastructure.", "published": "2023-06-27 16:26:26", "link": "http://arxiv.org/abs/2306.15595v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "REFLECT: Summarizing Robot Experiences for Failure Explanation and\n  Correction", "abstract": "The ability to detect and analyze failed executions automatically is crucial\nfor an explainable and robust robotic system. Recently, Large Language Models\n(LLMs) have demonstrated strong reasoning abilities on textual inputs. To\nleverage the power of LLMs for robot failure explanation, we introduce REFLECT,\na framework which queries LLM for failure reasoning based on a hierarchical\nsummary of robot past experiences generated from multisensory observations. The\nfailure explanation can further guide a language-based planner to correct the\nfailure and complete the task. To systematically evaluate the framework, we\ncreate the RoboFail dataset with a variety of tasks and failure scenarios. We\ndemonstrate that the LLM-based framework is able to generate informative\nfailure explanations that assist successful correction planning.", "published": "2023-06-27 18:03:15", "link": "http://arxiv.org/abs/2306.15724v4", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Next Steps for Human-Centered Generative AI: A Technical Perspective", "abstract": "Through iterative, cross-disciplinary discussions, we define and propose\nnext-steps for Human-centered Generative AI (HGAI). We contribute a\ncomprehensive research agenda that lays out future directions of Generative AI\nspanning three levels: aligning with human values; assimilating human intents;\nand augmenting human abilities. By identifying these next-steps, we intend to\ndraw interdisciplinary research teams to pursue a coherent set of emergent\nideas in HGAI, focusing on their interested topics while maintaining a coherent\nbig picture of the future work landscape.", "published": "2023-06-27 19:54:30", "link": "http://arxiv.org/abs/2306.15774v2", "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.HC"}
{"title": "FLuRKA: Fast and accurate unified Low-Rank & Kernel Attention", "abstract": "Many efficient $\\textit{approximate}$ self-attention techniques have become\nprevalent since the inception of the transformer architecture. Two popular\nclasses of these techniques are low-rank and kernel methods. Each of these\nmethods has its strengths. We observe these strengths synergistically\ncomplement each other and exploit them to fuse low-rank and kernel methods,\nproducing a new class of transformers: FLuRKA ($\\textbf{F}$ast\n$\\textbf{L}$ow-$\\textbf{R}$ank & $\\textbf{K}$ernel$ \\textbf{A}$ttention).\nFLuRKA are highly $\\textit{training-efficient}$ with faster model speeds\n$\\textit{and}$ similar model qualities compared to constituent low-rank and\nkernel methods. We theoretically and empirically evaluate the speed and quality\nof FLuRKA. Our model speed analysis posits a variety of parameter\nconfigurations where FLuRKA exhibit speedups over low-rank and kernel\napproximations and our model quality analysis bounds the error of FLuRKA with\nrespect to full-attention. Empirically, we instantiate three FLuRKA variants\nwhich experience speedups of up to 3.3x and 1.7x over low-rank and kernel\nmethods respectively. This translates to speedups of up to 20x over models with\nflash-attention. Across a diverse set of tasks spanning language modeling,\nlanguage understanding, long sequence modeling, machine translation, and image\nclassification, FLuRKA achieve comparable accuracy with underlying low-rank and\nkernel approximations, occasionally surpassing both.", "published": "2023-06-27 20:58:41", "link": "http://arxiv.org/abs/2306.15799v2", "categories": ["cs.LG", "cs.CL", "cs.PF"], "primary_category": "cs.LG"}
{"title": "Confidence-based Ensembles of End-to-End Speech Recognition Models", "abstract": "The number of end-to-end speech recognition models grows every year. These\nmodels are often adapted to new domains or languages resulting in a\nproliferation of expert systems that achieve great results on target data,\nwhile generally showing inferior performance outside of their domain of\nexpertise. We explore combination of such experts via confidence-based\nensembles: ensembles of models where only the output of the most-confident\nmodel is used. We assume that models' target data is not available except for a\nsmall validation set. We demonstrate effectiveness of our approach with two\napplications. First, we show that a confidence-based ensemble of 5 monolingual\nmodels outperforms a system where model selection is performed via a dedicated\nlanguage identification block. Second, we demonstrate that it is possible to\ncombine base and adapted models to achieve strong results on both original and\ntarget data. We validate all our results on multiple datasets and model\narchitectures.", "published": "2023-06-27 23:13:43", "link": "http://arxiv.org/abs/2306.15824v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "SparseOptimizer: Sparsify Language Models through Moreau-Yosida\n  Regularization and Accelerate via Compiler Co-design", "abstract": "This paper introduces SparseOptimizer, a novel deep learning optimizer that\nexploits Moreau-Yosida regularization to naturally induce sparsity in large\nlanguage models such as BERT, ALBERT and GPT. Key to the design of\nSparseOptimizer is an embedded shrinkage operator, which imparts sparsity\ndirectly within the optimization process. This operator, backed by a sound\ntheoretical framework, includes an analytical solution, thereby reinforcing the\noptimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play\nfunctionality eradicates the need for code modifications, making it a\nuniversally adaptable tool for a wide array of large language models. Empirical\nevaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2\nconfirm that SparseBERT and SparseALBERT, when sparsified using\nSparseOptimizer, achieve performance comparable to their dense counterparts,\nBERT and ALBERT, while significantly reducing their parameter count. Further,\nthis work proposes an innovative optimizer-compiler co-design strategy,\ndemonstrating the potential of inference acceleration (\\textbf{3.37x},\n\\textbf{6.30x}, and \\textbf{7.15x} in comparison with Pytorch, TensorFlow, and\nLLVM generic compile, respectively) in SparseBERT when paired with an\nappropriately designed compiler. This study represents a significant step\nforward in the evolution of efficient, scalable, and high-performing large\nlanguage models, setting a precedent for future exploration and optimization in\nthis domain. The SparseOptimizer code and SparseALBERT model will be publicly\navailable upon paper acceptance.", "published": "2023-06-27 17:50:26", "link": "http://arxiv.org/abs/2306.15656v3", "categories": ["cs.LG", "cs.AI", "cs.CC", "cs.CL", "cs.MS"], "primary_category": "cs.LG"}
{"title": "Scaling Laws for Discriminative Speech Recognition Rescoring Models", "abstract": "Recent studies have found that model performance has a smooth power-law\nrelationship, or scaling laws, with training data and model size, for a wide\nrange of problems. These scaling laws allow one to choose nearly optimal data\nand model sizes. We study whether this scaling property is also applicable to\nsecond-pass rescoring, which is an important component of speech recognition\nsystems. We focus on RescoreBERT as the rescoring model, which uses a\npre-trained Transformer-based architecture fined tuned with an ASR\ndiscriminative loss. Using such a rescoring model, we show that the word error\nrate (WER) follows a scaling law for over two orders of magnitude as training\ndata and model size increase. In addition, it is found that a pre-trained model\nwould require less data than a randomly initialized model of the same size,\nrepresenting effective data transferred from pre-training step. This effective\ndata transferred is found to also follow a scaling law with the data and model\nsize.", "published": "2023-06-27 22:19:44", "link": "http://arxiv.org/abs/2306.15815v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Wespeaker baselines for VoxSRC2023", "abstract": "This report showcases the results achieved using the wespeaker toolkit for\nthe VoxSRC2023 Challenge. Our aim is to provide participants, especially those\nwith limited experience, with clear and straightforward guidelines to develop\ntheir initial systems. Via well-structured recipes and strong results, we hope\nto offer an accessible and good enough start point for all interested\nindividuals. In this report, we describe the results achieved on the VoxSRC2023\ndev set using the pretrained models, you can check the CodaLab evaluation\nserver for the results on the evaluation set.", "published": "2023-06-27 02:44:06", "link": "http://arxiv.org/abs/2306.15161v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hyper-parameter Adaptation of Conformer ASR Systems for Elderly and\n  Dysarthric Speech Recognition", "abstract": "Automatic recognition of disordered and elderly speech remains highly\nchallenging tasks to date due to data scarcity. Parameter fine-tuning is often\nused to exploit the large quantities of non-aged and healthy speech pre-trained\nmodels, while neural architecture hyper-parameters are set using expert\nknowledge and remain unchanged. This paper investigates hyper-parameter\nadaptation for Conformer ASR systems that are pre-trained on the Librispeech\ncorpus before being domain adapted to the DementiaBank elderly and UASpeech\ndysarthric speech datasets. Experimental results suggest that hyper-parameter\nadaptation produced word error rate (WER) reductions of 0.45% and 0.67% over\nparameter-only fine-tuning on DBank and UASpeech tasks respectively. An\nintuitive correlation is found between the performance improvements by\nhyper-parameter domain adaptation and the relative utterance length ratio\nbetween the source and target domain data.", "published": "2023-06-27 07:49:35", "link": "http://arxiv.org/abs/2306.15265v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "GenerTTS: Pronunciation Disentanglement for Timbre and Style\n  Generalization in Cross-Lingual Text-to-Speech", "abstract": "Cross-lingual timbre and style generalizable text-to-speech (TTS) aims to\nsynthesize speech with a specific reference timbre or style that is never\ntrained in the target language. It encounters the following challenges: 1)\ntimbre and pronunciation are correlated since multilingual speech of a specific\nspeaker is usually hard to obtain; 2) style and pronunciation are mixed because\nthe speech style contains language-agnostic and language-specific parts. To\naddress these challenges, we propose GenerTTS, which mainly includes the\nfollowing works: 1) we elaborately design a HuBERT-based information bottleneck\nto disentangle timbre and pronunciation/style; 2) we minimize the mutual\ninformation between style and language to discard the language-specific\ninformation in the style embedding. The experiments indicate that GenerTTS\noutperforms baseline systems in terms of style similarity and pronunciation\naccuracy, and enables cross-lingual timbre and style generalization.", "published": "2023-06-27 08:47:48", "link": "http://arxiv.org/abs/2306.15304v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music", "abstract": "Vocal pitch is an important high-level feature in music audio processing.\nHowever, extracting vocal pitch in polyphonic music is more challenging due to\nthe presence of accompaniment. To eliminate the influence of the accompaniment,\nmost previous methods adopt music source separation models to obtain clean\nvocals from polyphonic music before predicting vocal pitches. As a result, the\nperformance of vocal pitch estimation is affected by the music source\nseparation models. To address this issue and directly extract vocal pitches\nfrom polyphonic music, we propose a robust model named RMVPE. This model can\nextract effective hidden features and accurately predict vocal pitches from\npolyphonic music. The experimental results demonstrate the superiority of RMVPE\nin terms of raw pitch accuracy (RPA) and raw chroma accuracy (RCA).\nAdditionally, experiments conducted with different types of noise show that\nRMVPE is robust across all signal-to-noise ratio (SNR) levels. The code of\nRMVPE is available at https://github.com/Dream-High/RMVPE.", "published": "2023-06-27 12:11:55", "link": "http://arxiv.org/abs/2306.15412v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Post-Processing Independent Evaluation of Sound Event Detection Systems", "abstract": "Due to the high variation in the application requirements of sound event\ndetection (SED) systems, it is not sufficient to evaluate systems only in a\nsingle operating mode. Therefore, the community recently adopted the polyphonic\nsound detection score (PSDS) as an evaluation metric, which is the normalized\narea under the PSD receiver operating characteristic (PSD-ROC). It summarizes\nthe system performance over a range of operating modes resulting from varying\nthe decision threshold that is used to translate the system output scores into\na binary detection output. Hence, it provides a more complete picture of the\noverall system behavior and is less biased by specific threshold tuning.\nHowever, besides the decision threshold there is also the post-processing that\ncan be changed to enter another operating mode. In this paper we propose the\npost-processing independent PSDS (piPSDS) as a generalization of the PSDS.\nHere, the post-processing independent PSD-ROC includes operating points from\nvarying post-processings with varying decision thresholds. Thus, it summarizes\neven more operating modes of an SED system and allows for system comparison\nwithout the need of implementing a post-processing and without a bias due to\ndifferent post-processings. While piPSDS can in principle combine different\ntypes of post-processing, we hear, as a first step, present median filter\nindependent PSDS (miPSDS) results for this year's DCASE Challenge Task4a\nsystems. Source code is publicly available in our sed_scores_eval package\n(https://github.com/fgnt/sed_scores_eval).", "published": "2023-06-27 12:54:06", "link": "http://arxiv.org/abs/2306.15440v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TranssionADD: A multi-frame reinforcement based sequence tagging model\n  for audio deepfake detection", "abstract": "Thanks to recent advancements in end-to-end speech modeling technology, it\nhas become increasingly feasible to imitate and clone a user`s voice. This\nleads to a significant challenge in differentiating between authentic and\nfabricated audio segments. To address the issue of user voice abuse and misuse,\nthe second Audio Deepfake Detection Challenge (ADD 2023) aims to detect and\nanalyze deepfake speech utterances. Specifically, Track 2, named the\nManipulation Region Location (RL), aims to pinpoint the location of manipulated\nregions in audio, which can be present in both real and generated audio\nsegments. We propose our novel TranssionADD system as a solution to the\nchallenging problem of model robustness and audio segment outliers in the trace\ncompetition. Our system provides three unique contributions: 1) we adapt\nsequence tagging task for audio deepfake detection; 2) we improve model\ngeneralization by various data augmentation techniques; 3) we incorporate\nmulti-frame detection (MFD) module to overcome limited representation provided\nby a single frame and use isolated-frame penalty (IFP) loss to handle outliers\nin segments. Our best submission achieved 2nd place in Track 2, demonstrating\nthe effectiveness and robustness of our proposed system.", "published": "2023-06-27 05:18:25", "link": "http://arxiv.org/abs/2306.15212v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-perspective Information Fusion Res2Net with RandomSpecmix for Fake\n  Speech Detection", "abstract": "In this paper, we propose the multi-perspective information fusion (MPIF)\nRes2Net with random Specmix for fake speech detection (FSD). The main purpose\nof this system is to improve the model's ability to learn precise forgery\ninformation for FSD task in low-quality scenarios. The task of random Specmix,\na data augmentation, is to improve the generalization ability of the model and\nenhance the model's ability to locate discriminative information. Specmix cuts\nand pastes the frequency dimension information of the spectrogram in the same\nbatch of samples without introducing other data, which helps the model to\nlocate the really useful information. At the same time, we randomly select\nsamples for augmentation to reduce the impact of data augmentation directly\nchanging all the data. Once the purpose of helping the model to locate\ninformation is achieved, it is also important to reduce unnecessary\ninformation. The role of MPIF-Res2Net is to reduce redundant interference\ninformation. Deceptive information from a single perspective is always similar,\nso the model learning this similar information will produce redundant spoofing\nclues and interfere with truly discriminative information. The proposed\nMPIF-Res2Net fuses information from different perspectives, making the\ninformation learned by the model more diverse, thereby reducing the redundancy\ncaused by similar information and avoiding interference with the learning of\ndiscriminative information. The results on the ASVspoof 2021 LA dataset\ndemonstrate the effectiveness of our proposed method, achieving EER and\nmin-tDCF of 3.29% and 0.2557, respectively.", "published": "2023-06-27 11:27:55", "link": "http://arxiv.org/abs/2306.15389v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Large-scale unsupervised audio pre-training for video-to-speech\n  synthesis", "abstract": "Video-to-speech synthesis is the task of reconstructing the speech signal\nfrom a silent video of a speaker. Most established approaches to date involve a\ntwo-step process, whereby an intermediate representation from the video, such\nas a spectrogram, is extracted first and then passed to a vocoder to produce\nthe raw audio. Some recent work has focused on end-to-end synthesis, whereby\nthe generation of raw audio and any intermediate representations is performed\njointly. All such approaches involve training on data from almost exclusively\naudio-visual datasets, i.e. every audio sample has a corresponding video\nsample. This precludes the use of abundant audio-only datasets which may not\nhave a corresponding visual modality (e.g. audiobooks, radio podcasts, speech\nrecognition datasets etc.), as well as audio-only architectures that have been\ndeveloped by the audio machine learning community over the years. In this paper\nwe propose to train encoder-decoder models on more than 3,500 hours of audio\ndata at 24kHz, and then use the pre-trained decoders to initialize the audio\ndecoders for the video-to-speech synthesis task. The pre-training step uses\naudio samples only and does not require labels or corresponding samples from\nother modalities (visual, text). We demonstrate that this pre-training step\nimproves the reconstructed speech and that it is an unexplored way to improve\nthe quality of the generator in a cross-modal task while only requiring samples\nfrom one of the modalities. We conduct experiments using both raw audio and mel\nspectrograms as target outputs and benchmark our models with existing work.", "published": "2023-06-27 13:31:33", "link": "http://arxiv.org/abs/2306.15464v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Classification of Infant Sleep/Wake States: Cross-Attention among Large\n  Scale Pretrained Transformer Networks using Audio, ECG, and IMU Data", "abstract": "Infant sleep is critical to brain and behavioral development. Prior studies\non infant sleep/wake classification have been largely limited to reliance on\nexpensive and burdensome polysomnography (PSG) tests in the laboratory or\nwearable devices that collect single-modality data. To facilitate data\ncollection and accuracy of detection, we aimed to advance this field of study\nby using a multi-modal wearable device, LittleBeats (LB), to collect audio,\nelectrocardiogram (ECG), and inertial measurement unit (IMU) data among a\ncohort of 28 infants. We employed a 3-branch (audio/ECG/IMU) large scale\ntransformer-based neural network (NN) to demonstrate the potential of such\nmulti-modal data. We pretrained each branch independently with its respective\nmodality, then finetuned the model by fusing the pretrained transformer layers\nwith cross-attention. We show that multi-modal data significantly improves\nsleep/wake classification (accuracy = 0.880), compared with use of a single\nmodality (accuracy = 0.732). Our approach to multi-modal mid-level fusion may\nbe adaptable to a diverse range of architectures and tasks, expanding future\ndirections of infant behavioral research.", "published": "2023-06-27 21:44:05", "link": "http://arxiv.org/abs/2306.15808v1", "categories": ["cs.MM", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.MM"}
{"title": "CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion\n  Intensity Regulation", "abstract": "Existing fine-grained intensity regulation methods rely on explicit control\nthrough predicted emotion probabilities. However, these high-level semantic\nprobabilities are often inaccurate and unsmooth at the phoneme level, leading\nto bias in learning. Especially when we attempt to mix multiple emotion\nintensities for specific phonemes, resulting in markedly reduced\ncontrollability and naturalness of the synthesis. To address this issue, we\npropose the CAScaded Explicit and Implicit coNtrol framework (CASEIN), which\nleverages accurate disentanglement of emotion manifolds from the reference\nspeech to learn the implicit representation at a lower semantic level. This\nrepresentation bridges the semantical gap between explicit probabilities and\nthe synthesis model, reducing bias in learning. In experiments, our CASEIN\nsurpasses existing methods in both controllability and naturalness. Notably, we\nare the first to achieve fine-grained control over the mixed intensity of\nmultiple emotions.", "published": "2023-06-27 09:45:43", "link": "http://arxiv.org/abs/2307.00020v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
