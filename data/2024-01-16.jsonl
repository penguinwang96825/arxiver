{"title": "JustiLM: Few-shot Justification Generation for Explainable Fact-Checking\n  of Real-world Claims", "abstract": "Justification is an explanation that supports the veracity assigned to a\nclaim in fact-checking. However, the task of justification generation is\npreviously oversimplified as summarization of fact-check article authored by\nfact-checkers. Therefore, we propose a realistic approach to generate\njustification based on retrieved evidence. We present a new benchmark dataset\ncalled ExClaim for \\underline{Ex}plainable fact-checking of real-world\n\\underline{Claim}s, and introduce JustiLM, a novel few-shot\n\\underline{Justi}fication generation based on retrieval-augmented\n\\underline{L}anguage \\underline{M}odel by using fact-check articles as\nauxiliary resource during training only. Experiments show that JustiLM achieves\npromising performance in justification generation compared to strong baselines,\nand can also enhance veracity classification with a straightforward extension.", "published": "2024-01-16 00:47:36", "link": "http://arxiv.org/abs/2401.08026v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Document-level Translation of Large Language Model via\n  Translation Mixed-instructions", "abstract": "Existing large language models (LLMs) for machine translation are typically\nfine-tuned on sentence-level translation instructions and achieve satisfactory\nperformance at the sentence level. However, when applied to document-level\ntranslation, these models face a significant challenge, particularly when\ndealing with documents containing over 512 tokens. This challenge arises from\nthe issue of sentence-level coverage, where subsequent sentences in the\ndocument remain untranslated. As a result, the document-level translation\ncapability of LLMs fine-tuned on sentence-level translation instructions is\nsignificantly limited. We conjecture that the primary cause of LLMs' weak\ndocument-level translation performance is the absence of document-to-document\nmapping ability. To address the issue, we propose an approach that combines\nsentence-level and document-level translation instructions of varying lengths\nto fine-tune LLMs. Our proposed translation mixed-instructions enable LLMs\n(Llama-2~7B and 13B) to maintain consistent translation performance from the\nsentence level to documents containing as many as 2048 tokens. Extensive\nexperimental results show that the proposed approach significantly enhances the\ndocument-level translation capabilities of LLMs on 10 language pairs,\neffectively mitigating the sentence-level coverage issue in document-level\ntranslation. Experimentation on discourse phenomena has demonstrated that our\ndocument-level translation approach significantly improves translation quality,\nboth in terms of BLEU score and discourse coherence.", "published": "2024-01-16 03:28:26", "link": "http://arxiv.org/abs/2401.08088v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible\n  Pipeline", "abstract": "Large language models (LLMs) have seen considerable advancements in natural\nlanguage understanding tasks, yet there remains a gap to bridge before\nattaining true artificial general intelligence, especially concerning\nshortcomings in mathematical reasoning capabilities. We postulate that the\ninherent nature of LLM training, which focuses on predicting probabilities of\nnext token, presents challenges in effectively modeling mathematical reasoning\nthat demands exact calculations, both from data-driven and theoretical\nstandpoints. In this paper, we address this challenge by enriching the data\nlandscape and introducing a novel math dataset, enhanced with a capability to\nutilize a Python code interpreter. This dataset is derived from GSM8K and MATH\nand has been further refined through a combination of GPT-4 annotations, human\nreview, and self-training processes, where the errors in the original GSM8K\ntraining set have been fixed. Additionally, we propose a tentative, easily\nreplicable protocol for the fine-tuning of math-specific LLMs, which has led to\na significant improvement in the performance of a 7B-parameter LLM on the GSM8K\nand MATH datasets. We are committed to advancing the field of mathematical\nreasoning in LLMs and, to that end, we have made source code for data\ngeneration / training / inference, and the model checkpoints publicly available\nat \\url{https://github.com/MARIO-Math-Reasoning/MARIO}. We hope this will\nfacilitate further research and development within the community.", "published": "2024-01-16 08:08:01", "link": "http://arxiv.org/abs/2401.08190v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inferflow: an Efficient and Highly Configurable Inference Engine for\n  Large Language Models", "abstract": "We present Inferflow, an efficient and highly configurable inference engine\nfor large language models (LLMs). With Inferflow, users can serve most of the\ncommon transformer models by simply modifying some lines in corresponding\nconfiguration files, without writing a single line of source code. Compared\nwith most existing inference engines, Inferflow has some key features. First,\nby implementing a modular framework of atomic build-blocks and technologies,\nInferflow is compositionally generalizable to new models. Second, 3.5-bit\nquantization is introduced in Inferflow as a tradeoff between 3-bit and 4-bit\nquantization. Third, hybrid model partitioning for multi-GPU inference is\nintroduced in Inferflow to better balance inference speed and throughput than\nthe existing partition-by-layer and partition-by-tensor strategies.", "published": "2024-01-16 11:39:09", "link": "http://arxiv.org/abs/2401.08294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SAPT: A Shared Attention Framework for Parameter-Efficient Continual\n  Learning of Large Language Models", "abstract": "The continual learning (CL) ability is vital for deploying large language\nmodels (LLMs) in the dynamic world. Existing methods devise the learning module\nto acquire task-specific knowledge with parameter-efficient tuning (PET) block\nand the selection module to pick out the corresponding one for the testing\ninput, aiming at handling the challenges of catastrophic forgetting and\nknowledge transfer in CL. However, these methods tend to address only one of\nthe challenges, ignoring the potential of aligning the two modules to\neffectively address catastrophic forgetting and knowledge transfer\nsimultaneously. To this end, we propose a novel Shared Attention Framework\n(SAPT), to align the PET learning and selection via the Shared Attentive\nLearning \\& Selection module. Extensive Experiments on two CL benchmarks\ndemonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates\nits superiority when we scale it to different model sizes (from 770M to 13B),\ndifferent model architectures (T5 and LLaMA-2) and unseen tasks.", "published": "2024-01-16 11:45:03", "link": "http://arxiv.org/abs/2401.08295v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Application of LLM Agents in Recruitment: A Novel Framework for Resume\n  Screening", "abstract": "The automation of resume screening is a crucial aspect of the recruitment\nprocess in organizations. Automated resume screening systems often encompass a\nrange of natural language processing (NLP) tasks. This paper introduces a novel\nLarge Language Models (LLMs) based agent framework for resume screening, aimed\nat enhancing efficiency and time management in recruitment processes. Our\nframework is distinct in its ability to efficiently summarize and grade each\nresume from a large dataset. Moreover, it utilizes LLM agents for\ndecision-making. To evaluate our framework, we constructed a dataset from\nactual resumes and simulated a resume screening process. Subsequently, the\noutcomes of the simulation experiment were compared and subjected to detailed\nanalysis. The results demonstrate that our automated resume screening framework\nis 11 times faster than traditional manual methods. Furthermore, by fine-tuning\nthe LLMs, we observed a significant improvement in the F1 score, reaching\n87.73\\%, during the resume sentence classification phase. In the resume\nsummarization and grading phase, our fine-tuned model surpassed the baseline\nperformance of the GPT-3.5 model. Analysis of the decision-making efficacy of\nthe LLM agents in the final offer stage further underscores the potential of\nLLM agents in transforming resume screening processes.", "published": "2024-01-16 12:30:56", "link": "http://arxiv.org/abs/2401.08315v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Salute the Classic: Revisiting Challenges of Machine Translation in the\n  Age of Large Language Models", "abstract": "The evolution of Neural Machine Translation (NMT) has been significantly\ninfluenced by six core challenges (Koehn and Knowles, 2017), which have acted\nas benchmarks for progress in this field. This study revisits these challenges,\noffering insights into their ongoing relevance in the context of advanced Large\nLanguage Models (LLMs): domain mismatch, amount of parallel data, rare word\nprediction, translation of long sentences, attention model as word alignment,\nand sub-optimal beam search. Our empirical findings indicate that LLMs\neffectively lessen the reliance on parallel data for major languages in the\npretraining phase. Additionally, the LLM-based translation system significantly\nenhances the translation of long sentences that contain approximately 80 words\nand shows the capability to translate documents of up to 512 words. However,\ndespite these significant improvements, the challenges of domain mismatch and\nprediction of rare words persist. While the challenges of word alignment and\nbeam search, specifically associated with NMT, may not apply to LLMs, we\nidentify three new challenges for LLMs in translation tasks: inference\nefficiency, translation of low-resource languages in the pretraining phase, and\nhuman-aligned evaluation. The datasets and models are released at\nhttps://github.com/pangjh3/LLM4MT.", "published": "2024-01-16 13:30:09", "link": "http://arxiv.org/abs/2401.08350v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphology and Syntax of the Tamil Language", "abstract": "This paper provides an overview of the morphology and syntax of the Tamil\nlanguage, focusing on its contemporary usage. The paper also highlights the\ncomplexity and richness of Tamil in terms of its morphological and syntactic\nfeatures, which will be useful for linguists analysing the language and\nconducting comparative studies. In addition, the paper will be useful for those\ndeveloping computational resources for the Tamil language. It is proven as a\nrule-based morphological analyser cum generator and a computational grammar for\nTamil have already been developed based on this paper. To enhance accessibility\nfor a broader audience, the analysis is conducted without relying on any\nspecific grammatical formalism.", "published": "2024-01-16 13:52:25", "link": "http://arxiv.org/abs/2401.08367v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual neural fuzzy matching for exploiting target-language\n  monolingual corpora in computer-aided translation", "abstract": "Computer-aided translation (CAT) tools based on translation memories (MT)\nplay a prominent role in the translation workflow of professional translators.\nHowever, the reduced availability of in-domain TMs, as compared to in-domain\nmonolingual corpora, limits its adoption for a number of translation tasks. In\nthis paper, we introduce a novel neural approach aimed at overcoming this\nlimitation by exploiting not only TMs, but also in-domain target-language (TL)\nmonolingual corpora, and still enabling a similar functionality to that offered\nby conventional TM-based CAT tools. Our approach relies on cross-lingual\nsentence embeddings to retrieve translation proposals from TL monolingual\ncorpora, and on a neural model to estimate their post-editing effort. The paper\npresents an automatic evaluation of these techniques on four language pairs\nthat shows that our approach can successfully exploit monolingual texts in a\nTM-based CAT environment, increasing the amount of useful translation\nproposals, and that our neural model for estimating the post-editing effort\nenables the combination of translation proposals obtained from monolingual\ncorpora and from TMs in the usual way. A human evaluation performed on a single\nlanguage pair confirms the results of the automatic evaluation and seems to\nindicate that the translation proposals retrieved with our approach are more\nuseful than what the automatic evaluation shows.", "published": "2024-01-16 14:00:28", "link": "http://arxiv.org/abs/2401.08374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM\n  Performance in Machine Translation", "abstract": "Moderate-sized large language models (LLMs) -- those with 7B or 13B\nparameters -- exhibit promising machine translation (MT) performance. However,\neven the top-performing 13B LLM-based translation models, like ALMA, does not\nmatch the performance of state-of-the-art conventional encoder-decoder\ntranslation models or larger-scale LLMs such as GPT-4. In this study, we bridge\nthis performance gap. We first assess the shortcomings of supervised\nfine-tuning for LLMs in the MT task, emphasizing the quality issues present in\nthe reference data, despite being human-generated. Then, in contrast to SFT\nwhich mimics reference translations, we introduce Contrastive Preference\nOptimization (CPO), a novel approach that trains models to avoid generating\nadequate but not perfect translations. Applying CPO to ALMA models with only\n22K parallel sentences and 12M parameters yields significant improvements. The\nresulting model, called ALMA-R, can match or exceed the performance of the WMT\ncompetition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.", "published": "2024-01-16 15:04:51", "link": "http://arxiv.org/abs/2401.08417v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ask the experts: sourcing high-quality datasets for nutritional\n  counselling through Human-AI collaboration", "abstract": "Large Language Models (LLMs), with their flexible generation abilities, can\nbe powerful data sources in domains with few or no available corpora. However,\nproblems like hallucinations and biases limit such applications. In this case\nstudy, we pick nutrition counselling, a domain lacking any public resource, and\nshow that high-quality datasets can be gathered by combining LLMs,\ncrowd-workers and nutrition experts. We first crowd-source and cluster a novel\ndataset of diet-related issues, then work with experts to prompt ChatGPT into\nproducing related supportive text. Finally, we let the experts evaluate the\nsafety of the generated text. We release HAI-coaching, the first\nexpert-annotated nutrition counselling dataset containing ~2.4K dietary\nstruggles from crowd workers, and ~97K related supportive texts generated by\nChatGPT. Extensive analysis shows that ChatGPT while producing highly fluent\nand human-like text, also manifests harmful behaviours, especially in sensitive\ntopics like mental health, making it unsuitable for unsupervised use.", "published": "2024-01-16 15:07:09", "link": "http://arxiv.org/abs/2401.08420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Portray Socially Subordinate Groups as More\n  Homogeneous, Consistent with a Bias Observed in Humans", "abstract": "Large language models (LLMs) are becoming pervasive in everyday life, yet\ntheir propensity to reproduce biases inherited from training data remains a\npressing concern. Prior investigations into bias in LLMs have focused on the\nassociation of social groups with stereotypical attributes. However, this is\nonly one form of human bias such systems may reproduce. We investigate a new\nform of bias in LLMs that resembles a social psychological phenomenon where\nsocially subordinate groups are perceived as more homogeneous than socially\ndominant groups. We had ChatGPT, a state-of-the-art LLM, generate texts about\nintersectional group identities and compared those texts on measures of\nhomogeneity. We consistently found that ChatGPT portrayed African, Asian, and\nHispanic Americans as more homogeneous than White Americans, indicating that\nthe model described racial minority groups with a narrower range of human\nexperience. ChatGPT also portrayed women as more homogeneous than men, but\nthese differences were small. Finally, we found that the effect of gender\ndiffered across racial/ethnic groups such that the effect of gender was\nconsistent within African and Hispanic Americans but not within Asian and White\nAmericans. We argue that the tendency of LLMs to describe groups as less\ndiverse risks perpetuating stereotypes and discriminatory behavior.", "published": "2024-01-16 16:52:00", "link": "http://arxiv.org/abs/2401.08495v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EmoLLMs: A Series of Emotional Large Language Models and Annotation\n  Tools for Comprehensive Affective Analysis", "abstract": "Sentiment analysis and emotion detection are important research topics in\nnatural language processing (NLP) and benefit many downstream tasks. With the\nwidespread application of LLMs, researchers have started exploring the\napplication of LLMs based on instruction-tuning in the field of sentiment\nanalysis. However, these models only focus on single aspects of affective\nclassification tasks (e.g. sentimental polarity or categorical emotions), and\noverlook the regression tasks (e.g. sentiment strength or emotion intensity),\nwhich leads to poor performance in downstream tasks. The main reason is the\nlack of comprehensive affective instruction tuning datasets and evaluation\nbenchmarks, which cover various affective classification and regression tasks.\nMoreover, although emotional information is useful for downstream tasks,\nexisting downstream datasets lack high-quality and comprehensive affective\nannotations. In this paper, we propose EmoLLMs, the first series of\nopen-sourced instruction-following LLMs for comprehensive affective analysis\nbased on fine-tuning various LLMs with instruction data, the first multi-task\naffective analysis instruction dataset (AAID) with 234K data samples based on\nvarious classification and regression tasks to support LLM instruction tuning,\nand a comprehensive affective evaluation benchmark (AEB) with 14 tasks from\nvarious sources and domains to test the generalization ability of LLMs. We\npropose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various\naffective instruction tasks. We compare our model with a variety of LLMs on\nAEB, where our models outperform all other open-sourced LLMs, and surpass\nChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve\nthe ChatGPT-level and GPT-4-level generalization capabilities on affective\nanalysis tasks, and demonstrates our models can be used as affective annotation\ntools.", "published": "2024-01-16 17:11:11", "link": "http://arxiv.org/abs/2401.08508v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Gaps between Pre-train and Downstream Settings in Bias Evaluation\n  and Debiasing", "abstract": "The output tendencies of Pre-trained Language Models (PLM) vary markedly\nbefore and after Fine-Tuning (FT) due to the updates to the model parameters.\nThese divergences in output tendencies result in a gap in the social biases of\nPLMs. For example, there exits a low correlation between intrinsic bias scores\nof a PLM and its extrinsic bias scores under FT-based debiasing methods.\nAdditionally, applying FT-based debiasing methods to a PLM leads to a decline\nin performance in downstream tasks. On the other hand, PLMs trained on large\ndatasets can learn without parameter updates via In-Context Learning (ICL)\nusing prompts. ICL induces smaller changes to PLMs compared to FT-based\ndebiasing methods. Therefore, we hypothesize that the gap observed in\npre-trained and FT models does not hold true for debiasing methods that use\nICL. In this study, we demonstrate that ICL-based debiasing methods show a\nhigher correlation between intrinsic and extrinsic bias scores compared to\nFT-based methods. Moreover, the performance degradation due to debiasing is\nalso lower in the ICL case compared to that in the FT case.", "published": "2024-01-16 17:15:08", "link": "http://arxiv.org/abs/2401.08511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spatial Entity Resolution between Restaurant Locations and\n  Transportation Destinations in Southeast Asia", "abstract": "As a tech company, Grab has expanded from transportation to food delivery,\naiming to serve Southeast Asia with hyperlocalized applications. Information\nabout places as transportation destinations can help to improve our knowledge\nabout places as restaurants, so long as the spatial entity resolution problem\nbetween these datasets can be solved. In this project, we attempted to\nrecognize identical place entities from databases of Points-of-Interest (POI)\nand GrabFood restaurants, using their spatial and textual attributes, i.e.,\nlatitude, longitude, place name, and street address.\n  Distance metrics were calculated for these attributes and fed to tree-based\nclassifiers. POI-restaurant matching was conducted separately for Singapore,\nPhilippines, Indonesia, and Malaysia. Experimental estimates demonstrate that a\nmatching POI can be found for over 35% of restaurants in these countries. As\npart of these estimates, test datasets were manually created, and RandomForest,\nAdaBoost, Gradient Boosting, and XGBoost perform well, with most accuracy,\nprecision, and recall scores close to or higher than 90% for matched vs.\nunmatched classification. To the authors' knowledge, there are no previous\npublished scientific papers devoted to matching of spatial entities for the\nSoutheast Asia region.", "published": "2024-01-16 17:59:54", "link": "http://arxiv.org/abs/2401.08537v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tuning Language Models by Proxy", "abstract": "Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. We then demonstrate the generality of\nproxy-tuning by applying it to domain adaptation on code, and task-specific\nfinetuning on question-answering and math problems. Finally, we show how to\nproxy-tune a truly black-box LM, GPT-3.5, for temporal adaptation, increasing\nits knowledge about recent events. Our work demonstrates the promise of using\nsmall tuned LMs to efficiently customize large, potentially proprietary LMs\nthrough decoding-time guidance.", "published": "2024-01-16 18:49:55", "link": "http://arxiv.org/abs/2401.08565v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deductive Closure Training of Language Models for Coherence, Accuracy,\n  and Updatability", "abstract": "While language models (LMs) can sometimes generate factually correct text and\nestimate truth values of individual claims, these generally do not reflect a\nglobally coherent, manipulable model of the world. As a consequence, current\nLMs also generate incorrect or nonsensical content, and are difficult to edit\nand bring up to date. We present a method called Deductive Closure Training\n(DCT) that uses LMs themselves to identify implications of (and contradictions\nwithin) the text that they generate, yielding an efficient self-supervised\nprocedure for improving LM factuality. Given a collection of seed documents,\nDCT prompts LMs to generate additional text implied by these documents, reason\nglobally about the correctness of this generated text, and finally fine-tune on\ntext inferred to be correct. Given seed documents from a trusted source, DCT\nprovides a tool for supervised model updating; if seed documents are sampled\nfrom the LM itself, DCT enables fully unsupervised fine-tuning for improved\ncoherence and accuracy. Across the CREAK, MQUaKE, and Reversal Curse datasets,\nsupervised DCT improves LM fact verification and text generation accuracy by\n3-26%; on CREAK fully unsupervised DCT improves verification accuracy by 12%.\nThese results show that LMs' reasoning capabilities during inference can be\nleveraged during training to improve their reliability.", "published": "2024-01-16 18:58:37", "link": "http://arxiv.org/abs/2401.08574v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical\n  Assistance", "abstract": "In this work, we present HuixiangDou, a technical assistant powered by Large\nLanguage Models (LLM). This system is designed to assist algorithm developers\nby providing insightful responses to questions related to open-source algorithm\nprojects, such as computer vision and deep learning projects from OpenMMLab. We\nfurther explore the integration of this assistant into the group chats of\ninstant messaging (IM) tools such as WeChat and Lark. Through several iterative\nimprovements and trials, we have developed a sophisticated technical chat\nassistant capable of effectively answering users' technical questions without\ncausing message flooding. This paper's contributions include: 1) Designing an\nalgorithm pipeline specifically for group chat scenarios; 2) Verifying the\nreliable performance of text2vec in task rejection; 3) Identifying three\ncritical requirements for LLMs in technical-assistant-like products, namely\nscoring ability, In-Context Learning (ICL), and Long Context. We have made the\nsource code, android app and web service available at Github\n(https://github.com/internlm/huixiangdou), OpenXLab\n(https://openxlab.org.cn/apps/detail/tpoisonooo/huixiangdou-web) and YouTube\n(https://youtu.be/ylXrT-Tei-Y) to aid in future research and application.\nHuixiangDou is applicable to any group chat within IM tools.", "published": "2024-01-16 19:00:10", "link": "http://arxiv.org/abs/2401.08772v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Robustness of LLM-Synthetic Text Detectors for Academic\n  Writing: A Comprehensive Analysis", "abstract": "The emergence of large language models (LLMs), such as Generative Pre-trained\nTransformer 4 (GPT-4) used by ChatGPT, has profoundly impacted the academic and\nbroader community. While these models offer numerous advantages in terms of\nrevolutionizing work and study methods, they have also garnered significant\nattention due to their potential negative consequences. One example is\ngenerating academic reports or papers with little to no human contribution.\nConsequently, researchers have focused on developing detectors to address the\nmisuse of LLMs. However, most existing methods prioritize achieving higher\naccuracy on restricted datasets, neglecting the crucial aspect of\ngeneralizability. This limitation hinders their practical application in\nreal-life scenarios where reliability is paramount. In this paper, we present a\ncomprehensive analysis of the impact of prompts on the text generated by LLMs\nand highlight the potential lack of robustness in one of the current\nstate-of-the-art GPT detectors. To mitigate these issues concerning the misuse\nof LLMs in academic writing, we propose a reference-based Siamese detector\nnamed Synthetic-Siamese which takes a pair of texts, one as the inquiry and the\nother as the reference. Our method effectively addresses the lack of robustness\nof previous detectors (OpenAI detector and DetectGPT) and significantly\nimproves the baseline performances in realistic academic writing scenarios by\napproximately 67% to 95%.", "published": "2024-01-16 01:58:36", "link": "http://arxiv.org/abs/2401.08046v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Incremental Extractive Opinion Summarization Using Cover Trees", "abstract": "Extractive opinion summarization involves automatically producing a summary\nof text about an entity (e.g., a product's reviews) by extracting\nrepresentative sentences that capture prevalent opinions in the review set.\nTypically, in online marketplaces user reviews accumulate over time, and\nopinion summaries need to be updated periodically to provide customers with\nup-to-date information. In this work, we study the task of extractive opinion\nsummarization in an incremental setting, where the underlying review set\nevolves over time. Many of the state-of-the-art extractive opinion\nsummarization approaches are centrality-based, such as CentroidRank (Radev et\nal., 2004; Chowdhury et al., 2022). CentroidRank performs extractive\nsummarization by selecting a subset of review sentences closest to the centroid\nin the representation space as the summary. However, these methods are not\ncapable of operating efficiently in an incremental setting, where reviews\narrive one at a time. In this paper, we present an efficient algorithm for\naccurately computing the CentroidRank summaries in an incremental setting. Our\napproach, CoverSumm, relies on indexing review representations in a cover tree\nand maintaining a reservoir of candidate summary review sentences. CoverSumm's\nefficacy is supported by a theoretical and empirical analysis of running time.\nEmpirically, on a diverse collection of data (both real and synthetically\ncreated to illustrate scaling considerations), we demonstrate that CoverSumm is\nup to 36x faster than baseline methods, and capable of adapting to nuanced\nchanges in data distribution. We also conduct human evaluations of the\ngenerated summaries and find that CoverSumm is capable of producing informative\nsummaries consistent with the underlying review set.", "published": "2024-01-16 02:00:17", "link": "http://arxiv.org/abs/2401.08047v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Multi-Modal Knowledge Retrieval with Large Language Models", "abstract": "Knowledge retrieval with multi-modal queries plays a crucial role in\nsupporting knowledge-intensive multi-modal applications. However, existing\nmethods face challenges in terms of their effectiveness and training\nefficiency, especially when it comes to training and integrating multiple\nretrievers to handle multi-modal queries. In this paper, we propose an\ninnovative end-to-end generative framework for multi-modal knowledge retrieval.\nOur framework takes advantage of the fact that large language models (LLMs) can\neffectively serve as virtual knowledge bases, even when trained with limited\ndata. We retrieve knowledge via a two-step process: 1) generating knowledge\nclues related to the queries, and 2) obtaining the relevant document by\nsearching databases using the knowledge clue. In particular, we first introduce\nan object-aware prefix-tuning technique to guide multi-grained visual learning.\nThen, we align multi-grained visual features into the textual feature space of\nthe LLM, employing the LLM to capture cross-modal interactions. Subsequently,\nwe construct instruction data with a unified format for model training.\nFinally, we propose the knowledge-guided generation strategy to impose prior\nconstraints in the decoding steps, thereby promoting the generation of\ndistinctive knowledge clues. Through experiments conducted on three benchmarks,\nwe demonstrate significant improvements ranging from 3.0% to 14.6% across all\nevaluation metrics when compared to strong baselines.", "published": "2024-01-16 08:44:29", "link": "http://arxiv.org/abs/2401.08206v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Generative Adversarial Attack for Multilingual Text Classifiers", "abstract": "Current adversarial attack algorithms, where an adversary changes a text to\nfool a victim model, have been repeatedly shown to be effective against text\nclassifiers. These attacks, however, generally assume that the victim model is\nmonolingual and cannot be used to target multilingual victim models, a\nsignificant limitation given the increased use of these models. For this\nreason, in this work we propose an approach to fine-tune a multilingual\nparaphrase model with an adversarial objective so that it becomes able to\ngenerate effective adversarial examples against multilingual classifiers. The\ntraining objective incorporates a set of pre-trained models to ensure text\nquality and language consistency of the generated text. In addition, all the\nmodels are suitably connected to the generator by vocabulary-mapping matrices,\nallowing for full end-to-end differentiability of the overall training\npipeline. The experimental validation over two multilingual datasets and five\nlanguages has shown the effectiveness of the proposed approach compared to\nexisting baselines, particularly in terms of query efficiency. We also provide\na detailed analysis of the generated attacks and discuss limitations and\nopportunities for future research.", "published": "2024-01-16 10:14:27", "link": "http://arxiv.org/abs/2401.08255v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AesBench: An Expert Benchmark for Multimodal Large Language Models on\n  Image Aesthetics Perception", "abstract": "With collective endeavors, multimodal large language models (MLLMs) are\nundergoing a flourishing development. However, their performances on image\naesthetics perception remain indeterminate, which is highly desired in\nreal-world applications. An obvious obstacle lies in the absence of a specific\nbenchmark to evaluate the effectiveness of MLLMs on aesthetic perception. This\nblind groping may impede the further development of more advanced MLLMs with\naesthetic perception capacity. To address this dilemma, we propose AesBench, an\nexpert benchmark aiming to comprehensively evaluate the aesthetic perception\ncapacities of MLLMs through elaborate design across dual facets. (1) We\nconstruct an Expert-labeled Aesthetics Perception Database (EAPD), which\nfeatures diversified image contents and high-quality annotations provided by\nprofessional aesthetic experts. (2) We propose a set of integrative criteria to\nmeasure the aesthetic perception abilities of MLLMs from four perspectives,\nincluding Perception (AesP), Empathy (AesE), Assessment (AesA) and\nInterpretation (AesI). Extensive experimental results underscore that the\ncurrent MLLMs only possess rudimentary aesthetic perception ability, and there\nis still a significant gap between MLLMs and humans. We hope this work can\ninspire the community to engage in deeper explorations on the aesthetic\npotentials of MLLMs. Source data will be available at\nhttps://github.com/yipoh/AesBench.", "published": "2024-01-16 10:58:07", "link": "http://arxiv.org/abs/2401.08276v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Anchor function: a type of benchmark functions for studying language\n  models", "abstract": "Understanding transformer-based language models is becoming increasingly\ncrucial, particularly as they play pivotal roles in advancing towards\nartificial general intelligence. However, language model research faces\nsignificant challenges, especially for academic research groups with\nconstrained resources. These challenges include complex data structures,\nunknown target functions, high computational costs and memory requirements, and\na lack of interpretability in the inference process, etc. Drawing a parallel to\nthe use of simple models in scientific research, we propose the concept of an\nanchor function. This is a type of benchmark function designed for studying\nlanguage models in learning tasks that follow an \"anchor-key\" pattern. By\nutilizing the concept of an anchor function, we can construct a series of\nfunctions to simulate various language tasks. The anchor function plays a role\nanalogous to that of mice in diabetes research, particularly suitable for\nacademic research. We demonstrate the utility of the anchor function with an\nexample, revealing two basic operations by attention structures in language\nmodels: shifting tokens and broadcasting one token from one position to many\npositions. These operations are also commonly observed in large language\nmodels. The anchor function framework, therefore, opens up a series of valuable\nand accessible research questions for further exploration, especially for\ntheoretical study.", "published": "2024-01-16 12:10:49", "link": "http://arxiv.org/abs/2401.08309v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large\n  Language Models in Tool Learning", "abstract": "Tool learning has generated widespread interest as a vital means of\ninteraction between Large Language Models (LLMs) and the physical world.\nCurrent research predominantly emphasizes LLMs' capacity to utilize tools in\nwell-structured environments while overlooking their stability when confronted\nwith the inevitable noise of the real world. To bridge this gap, we introduce\nRoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool\nlearning. Specifically, we establish five external environments, each featuring\nvarying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union),\nproviding an in-depth analysis of the model's resilience across three critical\nphases: tool selection, parameter identification, and content filling.\nExperiments involving six widely-used models underscore the urgent necessity\nfor enhancing the robustness of LLMs in tool learning. For instance, the\nperformance of GPT-4 even drops significantly from 80.00 to 58.10 when there is\nno substantial change in manual accuracy. More surprisingly, the noise\ncorrection capability inherent in the GPT family paradoxically impedes its\nadaptability in the face of mild noise. In light of these findings, we propose\nRoTTuning, a strategy that enriches the diversity of training environments to\nbolster the robustness of LLMs in tool learning. The code and data are\navailable at https://github.com/Junjie-Ye/RoTBench.", "published": "2024-01-16 12:45:15", "link": "http://arxiv.org/abs/2401.08326v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hallucination Detection and Hallucination Mitigation: An Investigation", "abstract": "Large language models (LLMs), including ChatGPT, Bard, and Llama, have\nachieved remarkable successes over the last two years in a range of different\napplications. In spite of these successes, there exist concerns that limit the\nwide application of LLMs. A key problem is the problem of hallucination.\nHallucination refers to the fact that in addition to correct responses, LLMs\ncan also generate seemingly correct but factually incorrect responses. This\nreport aims to present a comprehensive review of the current literature on both\nhallucination detection and hallucination mitigation. We hope that this report\ncan serve as a good reference for both engineers and researchers who are\ninterested in LLMs and applying them to real world tasks.", "published": "2024-01-16 13:36:07", "link": "http://arxiv.org/abs/2401.08358v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language\n  Models (Exemplified as A Video Agent)", "abstract": "Recent LLM-driven visual agents mainly focus on solving image-based tasks,\nwhich limits their ability to understand dynamic scenes, making it far from\nreal-life applications like guiding students in laboratory experiments and\nidentifying their mistakes. Hence, this paper explores DoraemonGPT, a\ncomprehensive and conceptually elegant system driven by LLMs to understand\ndynamic scenes. Considering the video modality better reflects the\never-changing nature of real-world scenarios, we exemplify DoraemonGPT as a\nvideo agent. Given a video with a question/task, DoraemonGPT begins by\nconverting the input video into a symbolic memory that stores task-related\nattributes. This structured representation allows for spatial-temporal querying\nand reasoning by well-designed sub-task tools, resulting in concise\nintermediate results. Recognizing that LLMs have limited internal knowledge\nwhen it comes to specialized domains (e.g., analyzing the scientific principles\nunderlying experiments), we incorporate plug-and-play tools to assess external\nknowledge and address tasks across different domains. Moreover, a novel\nLLM-driven planner based on Monte Carlo Tree Search is introduced to explore\nthe large planning space for scheduling various tools. The planner iteratively\nfinds feasible solutions by backpropagating the result's reward, and multiple\nsolutions can be summarized into an improved final answer. We extensively\nevaluate DoraemonGPT's effectiveness on three benchmarks and several\nin-the-wild scenarios. The code will be released at\nhttps://github.com/z-x-yang/DoraemonGPT.", "published": "2024-01-16 14:33:09", "link": "http://arxiv.org/abs/2401.08392v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on\n  Agriculture", "abstract": "There are two common ways in which developers are incorporating proprietary\nand domain-specific data when building applications of Large Language Models\n(LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the\nprompt with the external data, while fine-Tuning incorporates the additional\nknowledge into the model itself. However, the pros and cons of both approaches\nare not well understood. In this paper, we propose a pipeline for fine-tuning\nand RAG, and present the tradeoffs of both for multiple popular LLMs, including\nLlama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages,\nincluding extracting information from PDFs, generating questions and answers,\nusing them for fine-tuning, and leveraging GPT-4 for evaluating the results. We\npropose metrics to assess the performance of different stages of the RAG and\nfine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset.\nAgriculture as an industry has not seen much penetration of AI, and we study a\npotentially disruptive application - what if we could provide location-specific\ninsights to a farmer? Our results show the effectiveness of our dataset\ngeneration pipeline in capturing geographic-specific knowledge, and the\nquantitative and qualitative benefits of RAG and fine-tuning. We see an\naccuracy increase of over 6 p.p. when fine-tuning the model and this is\ncumulative with RAG, which increases accuracy by 5 p.p. further. In one\nparticular experiment, we also demonstrate that the fine-tuned model leverages\ninformation from across geographies to answer specific questions, increasing\nanswer similarity from 47% to 72%. Overall, the results point to how systems\nbuilt using LLMs can be adapted to respond and incorporate knowledge across a\ndimension that is critical for a specific industry, paving the way for further\napplications of LLMs in other industrial domains.", "published": "2024-01-16 14:44:47", "link": "http://arxiv.org/abs/2401.08406v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning for Conversational Question Answering over\n  Knowledge Graph", "abstract": "Conversational question answering (ConvQA) over law knowledge bases (KBs)\ninvolves answering multi-turn natural language questions about law and hope to\nfind answers in the law knowledge base. Despite many methods have been\nproposed. Existing law knowledge base ConvQA model assume that the input\nquestion is clear and can perfectly reflect user's intention. However, in real\nworld, the input questions are noisy and inexplict. This makes the model hard\nto find the correct answer in the law knowledge bases. In this paper, we try to\nuse reinforcement learning to solve this problem. The reinforcement learning\nagent can automatically learn how to find the answer based on the input\nquestion and the conversation history, even when the input question is\ninexplicit. We test the proposed method on several real world datasets and the\nresults show the effectivenss of the proposed model.", "published": "2024-01-16 16:09:56", "link": "http://arxiv.org/abs/2401.08460v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Contrastive Perplexity for Controlled Generation: An Application in\n  Detoxifying Large Language Models", "abstract": "The generation of undesirable and factually incorrect content of large\nlanguage models poses a significant challenge and remains largely an unsolved\nissue. This paper studies the integration of a contrastive learning objective\nfor fine-tuning LLMs for implicit knowledge editing and controlled text\ngeneration. Optimizing the training objective entails aligning text\nperplexities in a contrastive fashion. To facilitate training the model in a\nself-supervised fashion, we leverage an off-the-shelf LLM for training data\ngeneration. We showcase applicability in the domain of detoxification. Herein,\nthe proposed approach leads to a significant decrease in the generation of\ntoxic content while preserving general utility for downstream tasks such as\ncommonsense reasoning and reading comprehension. The proposed approach is\nconceptually simple but empirically powerful.", "published": "2024-01-16 16:49:39", "link": "http://arxiv.org/abs/2401.08491v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving ASR Contextual Biasing with Guided Attention", "abstract": "In this paper, we propose a Guided Attention (GA) auxiliary training loss,\nwhich improves the effectiveness and robustness of automatic speech recognition\n(ASR) contextual biasing without introducing additional parameters. A common\nchallenge in previous literature is that the word error rate (WER) reduction\nbrought by contextual biasing diminishes as the number of bias phrases\nincreases. To address this challenge, we employ a GA loss as an additional\ntraining objective besides the Transducer loss. The proposed GA loss aims to\nteach the cross attention how to align bias phrases with text tokens or audio\nframes. Compared to studies with similar motivations, the proposed loss\noperates directly on the cross attention weights and is easier to implement.\nThrough extensive experiments based on Conformer Transducer with Contextual\nAdapter, we demonstrate that the proposed method not only leads to a lower WER\nbut also retains its effectiveness as the number of bias phrases increases.\nSpecifically, the GA loss decreases the WER of rare vocabularies by up to 19.2%\non LibriSpeech compared to the contextual biasing baseline, and up to 49.3%\ncompared to a vanilla Transducer.", "published": "2024-01-16 21:16:12", "link": "http://arxiv.org/abs/2401.08835v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LoMA: Lossless Compressed Memory Attention", "abstract": "Large Language Models (LLMs) face limitations due to the high demand on GPU\nmemory and computational resources when handling long contexts. While sparsify\nthe Key-Value (KV) cache of transformer model is a typical strategy to\nalleviate resource usage, it unavoidably results in the loss of information. We\nintroduce Lossless Compressed Memory Attention (LoMA), a novel approach that\nenables lossless compression of the KV cache, thereby reducing the memory and\ncomputational demands during autoregressive generation. LoMA incorporates a\nspecialized training or fine-tuning precedure alongside an autoregressive\ngeneration algorithm optimized for the compressed context. Our method\ncompresses the KV cache after every $tc$ generated tokens with a compression\nratio of $c$ and a target compressed length $t$, and this process occurs within\na single inference pass without dependency on auxiliary models. We engineered\nan efficient training scheme involving specific inputs, attention masks, and\nposition identifiers to instill this compression capability. Experimental\nvalidation has demonstrated that LoMA significantly reducing computational\nconsumption and memory usage through achieving lossless KV cache compression.", "published": "2024-01-16 09:18:46", "link": "http://arxiv.org/abs/2401.09486v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Code-Based English Models Surprising Performance on Chinese QA Pair\n  Extraction Task", "abstract": "In previous studies, code-based models have consistently outperformed\ntext-based models in reasoning-intensive scenarios. When generating our\nknowledge base for Retrieval-Augmented Generation (RAG), we observed that\ncode-based models also perform exceptionally well in Chinese QA Pair Extraction\ntask. Further, our experiments and the metrics we designed discovered that\ncode-based models containing a certain amount of Chinese data achieve even\nbetter performance. Additionally, the capabilities of code-based English models\nin specified Chinese tasks offer a distinct perspective for discussion on the\nphilosophical \"Chinese Room\" thought experiment.", "published": "2024-01-16 02:11:35", "link": "http://arxiv.org/abs/2401.10286v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Few-Shot Learning for Mental Disorder Detection: A Continuous\n  Multi-Prompt Engineering Approach with Medical Knowledge Injection", "abstract": "This study harnesses state-of-the-art AI technology for detecting mental\ndisorders through user-generated textual content. Existing studies typically\nrely on fully supervised machine learning, which presents challenges such as\nthe labor-intensive manual process of annotating extensive training data for\neach research problem and the need to design specialized deep learning\narchitectures for each task. We propose a novel method to address these\nchallenges by leveraging large language models and continuous multi-prompt\nengineering, which offers two key advantages: (1) developing personalized\nprompts that capture each user's unique characteristics and (2) integrating\nstructured medical knowledge into prompts to provide context for disease\ndetection and facilitate predictive modeling. We evaluate our method using\nthree widely prevalent mental disorders as research cases. Our method\nsignificantly outperforms existing methods, including feature engineering,\narchitecture engineering, and discrete prompt engineering. Meanwhile, our\napproach demonstrates success in few-shot learning, i.e., requiring only a\nminimal number of training examples. Moreover, our method can be generalized to\nother rare mental disorder detection tasks with few positive labels. In\naddition to its technical contributions, our method has the potential to\nenhance the well-being of individuals with mental disorders and offer a\ncost-effective, accessible alternative for stakeholders beyond traditional\nmental disorder screening methods.", "published": "2024-01-16 13:54:43", "link": "http://arxiv.org/abs/2401.12988v2", "categories": ["cs.CL", "cs.AI", "K.5", "I.2.7; H.4.m"], "primary_category": "cs.CL"}
{"title": "Into the crossfire: evaluating the use of a language model to\n  crowdsource gun violence reports", "abstract": "Gun violence is a pressing and growing human rights issue that affects nearly\nevery dimension of the social fabric, from healthcare and education to\npsychology and the economy. Reliable data on firearm events is paramount to\ndeveloping more effective public policy and emergency responses. However, the\nlack of comprehensive databases and the risks of in-person surveys prevent\nhuman rights organizations from collecting needed data in most countries. Here,\nwe partner with a Brazilian human rights organization to conduct a systematic\nevaluation of language models to assist with monitoring real-world firearm\nevents from social media data. We propose a fine-tuned BERT-based model trained\non Twitter (now X) texts to distinguish gun violence reports from ordinary\nPortuguese texts. Our model achieves a high AUC score of 0.97. We then\nincorporate our model into a web application and test it in a live\nintervention. We study and interview Brazilian analysts who continuously\nfact-check social media texts to identify new gun violence events. Qualitative\nassessments show that our solution helped all analysts use their time more\nefficiently and expanded their search capacities. Quantitative assessments show\nthat the use of our model was associated with more analysts' interactions with\nonline users reporting gun violence. Taken together, our findings suggest that\nmodern Natural Language Processing techniques can help support the work of\nhuman rights organizations.", "published": "2024-01-16 14:40:54", "link": "http://arxiv.org/abs/2401.12989v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Topic Modelling: Going Beyond Token Outputs", "abstract": "Topic modelling is a text mining technique for identifying salient themes\nfrom a number of documents. The output is commonly a set of topics consisting\nof isolated tokens that often co-occur in such documents. Manual effort is\noften associated with interpreting a topic's description from such tokens.\nHowever, from a human's perspective, such outputs may not adequately provide\nenough information to infer the meaning of the topics; thus, their\ninterpretability is often inaccurately understood. Although several studies\nhave attempted to automatically extend topic descriptions as a means of\nenhancing the interpretation of topic models, they rely on external language\nsources that may become unavailable, must be kept up-to-date to generate\nrelevant results, and present privacy issues when training on or processing\ndata. This paper presents a novel approach towards extending the output of\ntraditional topic modelling methods beyond a list of isolated tokens. This\napproach removes the dependence on external sources by using the textual data\nitself by extracting high-scoring keywords and mapping them to the topic\nmodel's token outputs. To measure the interpretability of the proposed outputs\nagainst those of the traditional topic modelling approach, independent\nannotators manually scored each output based on their quality and usefulness,\nas well as the efficiency of the annotation task. The proposed approach\ndemonstrated higher quality and usefulness, as well as higher efficiency in the\nannotation task, in comparison to the outputs of a traditional topic modelling\nmethod, demonstrating an increase in their interpretability.", "published": "2024-01-16 16:05:54", "link": "http://arxiv.org/abs/2401.12990v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using\n  Self-Imagination", "abstract": "The potential of Vision-Language Models (VLMs) often remains underutilized in\nhandling complex text-based problems, particularly when these problems could\nbenefit from visual representation. Resonating with humans' ability to solve\ncomplex text-based problems by (1) creating a visual diagram from the problem\nand (2) deducing what steps they need to take to solve it, we propose\nSelf-Imagine. We leverage a single Vision-Language Model (VLM) to generate a\nstructured representation of the question using HTML, then render the HTML as\nan image, and finally use the same VLM to answer the question using both the\nquestion and the image. Our approach does not require any additional training\ndata or training. We evaluate our approach on three mathematics tasks and nine\ngeneral-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI\nPRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on\nall math tasks (on average GSM8K: +3.1%; ASDIV: +3.2%; SVAMP: +6.9%) and the\nmajority of the general-purpose reasoning tasks by 3.2% to 6.0% on average.", "published": "2024-01-16 00:46:29", "link": "http://arxiv.org/abs/2401.08025v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Calpric: Inclusive and Fine-grain Labeling of Privacy Policies with\n  Crowdsourcing and Active Learning", "abstract": "A significant challenge to training accurate deep learning models on privacy\npolicies is the cost and difficulty of obtaining a large and comprehensive set\nof training data. To address these challenges, we present Calpric , which\ncombines automatic text selection and segmentation, active learning and the use\nof crowdsourced annotators to generate a large, balanced training set for\nprivacy policies at low cost. Automated text selection and segmentation\nsimplifies the labeling task, enabling untrained annotators from crowdsourcing\nplatforms, like Amazon's Mechanical Turk, to be competitive with trained\nannotators, such as law students, and also reduces inter-annotator agreement,\nwhich decreases labeling cost. Having reliable labels for training enables the\nuse of active learning, which uses fewer training samples to efficiently cover\nthe input space, further reducing cost and improving class and data category\nbalance in the data set. The combination of these techniques allows Calpric to\nproduce models that are accurate over a wider range of data categories, and\nprovide more detailed, fine-grain labels than previous work. Our crowdsourcing\nprocess enables Calpric to attain reliable labeled data at a cost of roughly\n$0.92-$1.71 per labeled text segment. Calpric 's training process also\ngenerates a labeled data set of 16K privacy policy text segments across 9 Data\ncategories with balanced positive and negative samples.", "published": "2024-01-16 01:27:26", "link": "http://arxiv.org/abs/2401.08038v1", "categories": ["cs.CL", "cs.CR", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Study on Training and Developing Large Language Models for Behavior\n  Tree Generation", "abstract": "This paper presents an innovative exploration of the application potential of\nlarge language models (LLM) in addressing the challenging task of automatically\ngenerating behavior trees (BTs) for complex tasks. The conventional manual BT\ngeneration method is inefficient and heavily reliant on domain expertise. On\nthe other hand, existing automatic BT generation technologies encounter\nbottlenecks related to task complexity, model adaptability, and reliability. In\norder to overcome these challenges, we propose a novel methodology that\nleverages the robust representation and reasoning abilities of LLMs. The core\ncontribution of this paper lies in the design of a BT generation framework\nbased on LLM, which encompasses the entire process, from data synthesis and\nmodel training to application developing and data verification. Synthetic data\nis introduced to train the BT generation model (BTGen model), enhancing its\nunderstanding and adaptability to various complex tasks, thereby significantly\nimproving its overall performance. In order to ensure the effectiveness and\nexecutability of the generated BTs, we emphasize the importance of data\nverification and introduce a multilevel verification strategy. Additionally, we\nexplore a range of agent design and development schemes with LLM as the central\nelement. We hope that the work in this paper may provide a reference for the\nresearchers who are interested in BT generation based on LLMs.", "published": "2024-01-16 03:28:29", "link": "http://arxiv.org/abs/2401.08089v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "PRewrite: Prompt Rewriting with Reinforcement Learning", "abstract": "Prompt engineering is critical for the development of LLM-based applications.\nHowever, it is usually done manually in a \"trial and error\" fashion that can be\ntime consuming, ineffective, and sub-optimal. Even for the prompts which\nseemingly work well, there is always a lingering question: can the prompts be\nmade better with further modifications?\n  To address these problems, we investigate automated prompt engineering in\nthis paper. Specifically, we propose PRewrite, an automated method to rewrite\nan under-optimized prompt to a more effective prompt. We instantiate the prompt\nrewriter using a LLM. The rewriter LLM is trained using reinforcement learning\nto optimize the performance on a given downstream task. We conduct experiments\non diverse benchmark datasets, which demonstrates the effectiveness of\nPRewrite.", "published": "2024-01-16 08:04:50", "link": "http://arxiv.org/abs/2401.08189v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Large Language Models are Null-Shot Learners", "abstract": "This paper presents null-shot prompting. Null-shot prompting exploits\nhallucination in large language models (LLMs) by instructing LLMs to utilize\ninformation from the \"Examples\" section that never exists within the provided\ncontext to perform a task. While reducing hallucination is crucial and\nnon-negligible for daily and critical uses of LLMs, we propose that in the\ncurrent landscape in which these LLMs still hallucinate, it is possible, in\nfact, to exploit hallucination to increase performance in performing tasks\ncompared to standard zero-shot prompting. Experiments with eight LLMs show\nimprovements in performance across the majority of eight datasets, including\nreading comprehension, arithmetic reasoning, and closed-book question\nanswering. The observed inconsistency in increased relative performance across\nthe LLMs also potentially indicates a different degree of inherent\nhallucination in each model. These differences show that it is possible to\nutilize null-shot prompting as a way to detect degrees of hallucination in LLMs\nusing existing benchmarking datasets. We also perform ablation studies,\nincluding experimenting with a modified version of null-shot prompting that\nincorporates ideas from zero-shot chain-of-thought prompting, which shows\ndifferent trends of results.", "published": "2024-01-16 10:53:11", "link": "http://arxiv.org/abs/2401.08273v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hidden flaws behind expert-level accuracy of multimodal GPT-4 vision in\n  medicine", "abstract": "Recent studies indicate that Generative Pre-trained Transformer 4 with Vision\n(GPT-4V) outperforms human physicians in medical challenge tasks. However,\nthese evaluations primarily focused on the accuracy of multi-choice questions\nalone. Our study extends the current scope by conducting a comprehensive\nanalysis of GPT-4V's rationales of image comprehension, recall of medical\nknowledge, and step-by-step multimodal reasoning when solving New England\nJournal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test\nthe knowledge and diagnostic capabilities of medical professionals. Evaluation\nresults confirmed that GPT-4V performs comparatively to human physicians\nregarding multi-choice accuracy (81.6% vs. 77.8%). GPT-4V also performs well in\ncases where physicians incorrectly answer, with over 78% accuracy. However, we\ndiscovered that GPT-4V frequently presents flawed rationales in cases where it\nmakes the correct final choices (35.5%), most prominent in image comprehension\n(27.2%). Regardless of GPT-4V's high accuracy in multi-choice questions, our\nfindings emphasize the necessity for further in-depth evaluations of its\nrationales before integrating such multimodal AI models into clinical\nworkflows.", "published": "2024-01-16 14:41:20", "link": "http://arxiv.org/abs/2401.08396v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Machine Translation with Large Language Models: Prompt Engineering for\n  Persian, English, and Russian Directions", "abstract": "Generative large language models (LLMs) have demonstrated exceptional\nproficiency in various natural language processing (NLP) tasks, including\nmachine translation, question answering, text summarization, and natural\nlanguage understanding.\n  To further enhance the performance of LLMs in machine translation, we\nconducted an investigation into two popular prompting methods and their\ncombination, focusing on cross-language combinations of Persian, English, and\nRussian. We employed n-shot feeding and tailored prompting frameworks. Our\nfindings indicate that multilingual LLMs like PaLM exhibit human-like machine\ntranslation outputs, enabling superior fine-tuning of desired translation\nnuances in accordance with style guidelines and linguistic considerations.\nThese models also excel in processing and applying prompts. However, the choice\nof language model, machine translation task, and the specific source and target\nlanguages necessitate certain considerations when adopting prompting frameworks\nand utilizing n-shot in-context learning.\n  Furthermore, we identified errors and limitations inherent in popular LLMs as\nmachine translation tools and categorized them based on various linguistic\nmetrics. This typology of errors provides valuable insights for utilizing LLMs\neffectively and offers methods for designing prompts for in-context learning.\nOur report aims to contribute to the advancement of machine translation with\nLLMs by improving both the accuracy and reliability of evaluation metrics.", "published": "2024-01-16 15:16:34", "link": "http://arxiv.org/abs/2401.08429v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "ACM-class: I.2.2", "I.2.2"], "primary_category": "cs.CL"}
{"title": "Decentralised Emergence of Robust and Adaptive Linguistic Conventions in\n  Populations of Autonomous Agents Grounded in Continuous Worlds", "abstract": "This paper introduces a methodology through which a population of autonomous\nagents can establish a linguistic convention that enables them to refer to\narbitrary entities that they observe in their environment. The linguistic\nconvention emerges in a decentralised manner through local communicative\ninteractions between pairs of agents drawn from the population. The convention\nconsists of symbolic labels (word forms) associated to concept representations\n(word meanings) that are grounded in a continuous feature space. The concept\nrepresentations of each agent are individually constructed yet compatible on a\ncommunicative level. Through a range of experiments, we show (i) that the\nmethodology enables a population to converge on a communicatively effective,\ncoherent and human-interpretable linguistic convention, (ii) that it is\nnaturally robust against sensor defects in individual agents, (iii) that it can\neffectively deal with noisy observations, uncalibrated sensors and\nheteromorphic populations, (iv) that the method is adequate for continual\nlearning, and (v) that the convention self-adapts to changes in the environment\nand communicative needs of the agents.", "published": "2024-01-16 16:11:35", "link": "http://arxiv.org/abs/2401.08461v1", "categories": ["cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.AI"}
{"title": "Code Generation with AlphaCodium: From Prompt Engineering to Flow\n  Engineering", "abstract": "Code generation problems differ from common natural language problems - they\nrequire matching the exact syntax of the target language, identifying happy\npaths and edge cases, paying attention to numerous small details in the problem\nspec, and addressing other code-specific issues and requirements. Hence, many\nof the optimizations and tricks that have been successful in natural language\ngeneration may not be effective for code tasks. In this work, we propose a new\napproach to code generation by LLMs, which we call AlphaCodium - a test-based,\nmulti-stage, code-oriented iterative flow, that improves the performances of\nLLMs on code problems. We tested AlphaCodium on a challenging code generation\ndataset called CodeContests, which includes competitive programming problems\nfrom platforms such as Codeforces. The proposed flow consistently and\nsignificantly improves results. On the validation set, for example, GPT-4\naccuracy (pass@5) increased from 19% with a single well-designed direct prompt\nto 44% with the AlphaCodium flow. Many of the principles and best practices\nacquired in this work, we believe, are broadly applicable to general code\ngeneration tasks. Full implementation is available at:\nhttps://github.com/Codium-ai/AlphaCodium", "published": "2024-01-16 17:00:36", "link": "http://arxiv.org/abs/2401.08500v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Supporting Student Decisions on Learning Recommendations: An LLM-Based\n  Chatbot with Knowledge Graph Contextualization for Conversational\n  Explainability and Mentoring", "abstract": "Student commitment towards a learning recommendation is not separable from\ntheir understanding of the reasons it was recommended to them; and their\nability to modify it based on that understanding. Among explainability\napproaches, chatbots offer the potential to engage the student in a\nconversation, similar to a discussion with a peer or a mentor. The capabilities\nof chatbots, however, are still not sufficient to replace a human mentor,\ndespite the advancements of generative AI (GenAI) and large language models\n(LLM). Therefore, we propose an approach to utilize chatbots as mediators of\nthe conversation and sources of limited and controlled generation of\nexplanations, to harvest the potential of LLMs while reducing their potential\nrisks at the same time. The proposed LLM-based chatbot supports students in\nunderstanding learning-paths recommendations. We use a knowledge graph (KG) as\na human-curated source of information, to regulate the LLM's output through\ndefining its prompt's context. A group chat approach is developed to connect\nstudents with human mentors, either on demand or in cases that exceed the\nchatbot's pre-defined tasks. We evaluate the chatbot with a user study, to\nprovide a proof-of-concept and highlight the potential requirements and\nlimitations of utilizing chatbots in conversational explainability.", "published": "2024-01-16 17:31:35", "link": "http://arxiv.org/abs/2401.08517v3", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal\n  Data", "abstract": "Building cross-modal applications is challenging due to limited paired\nmulti-modal data. Recent works have shown that leveraging a pre-trained\nmulti-modal contrastive representation space enables cross-modal tasks to be\nlearned from uni-modal data. This is based on the assumption that contrastive\noptimization makes embeddings from different modalities interchangeable.\nHowever, this assumption is under-explored due to the poorly understood\ngeometry of the multi-modal contrastive space, where a modality gap exists. In\nour study, we provide a theoretical explanation of this space's geometry and\nintroduce a three-step method, $C^3$ (Connect, Collapse, Corrupt), to bridge\nthe modality gap, enhancing the interchangeability of embeddings. Our $C^3$\nmethod significantly improves cross-modal learning from uni-modal data,\nachieving state-of-the-art results on zero-shot image / audio / video\ncaptioning and text-to-image generation.", "published": "2024-01-16 18:52:27", "link": "http://arxiv.org/abs/2401.08567v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "MMToM-QA: Multimodal Theory of Mind Question Answering", "abstract": "Theory of Mind (ToM), the ability to understand people's mental states, is an\nessential ingredient for developing machines with human-level social\nintelligence. Recent machine learning models, particularly large language\nmodels, seem to show some aspects of ToM understanding. However, existing ToM\nbenchmarks use unimodal datasets - either video or text. Human ToM, on the\nother hand, is more than video or text understanding. People can flexibly\nreason about another person's mind based on conceptual representations (e.g.,\ngoals, beliefs, plans) extracted from any available data. To address this, we\nintroduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark.\nMMToM-QA comprehensively evaluates machine ToM both on multimodal data and on\ndifferent kinds of unimodal data about a person's activity in a household\nenvironment. To engineer multimodal ToM capacity, we propose a novel method,\nBIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM\nextracts unified representations from multimodal data and utilizes language\nmodels for scalable Bayesian inverse planning. We conducted a systematic\ncomparison of human performance, BIP-ALM, and state-of-the-art models,\nincluding GPT-4. The experiments demonstrate that large language models and\nlarge multimodal models still lack robust ToM capacity. BIP-ALM, on the other\nhand, shows promising results, by leveraging the power of both model-based\nmental inference and language models.", "published": "2024-01-16 18:59:24", "link": "http://arxiv.org/abs/2401.08743v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant\n  Reviews and Images on Social Media", "abstract": "Online reviews in the form of user-generated content (UGC) significantly\nimpact consumer decision-making. However, the pervasive issue of not only human\nfake content but also machine-generated content challenges UGC's reliability.\nRecent advances in Large Language Models (LLMs) may pave the way to fabricate\nindistinguishable fake generated content at a much lower cost. Leveraging\nOpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a\nmulti-modal dataset of 20,144 restaurant review-image pairs divided into\nauthentic and machine-generated. We explore unimodal and multimodal detection\nmodels, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from\nreadability and photographic theories to score reviews and images,\nrespectively, demonstrating their utility as hand-crafted features in scalable\nand interpretable detection models, with comparable performance. The paper\ncontributes by open-sourcing the dataset and releasing fake review detectors,\nrecommending its use in unimodal and multimodal fake review detection tasks,\nand evaluating linguistic and visual features in synthetic versus authentic\ndata.", "published": "2024-01-16 20:57:36", "link": "http://arxiv.org/abs/2401.08825v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Revisiting Self-supervised Learning of Speech Representation from a\n  Mutual Information Perspective", "abstract": "Existing studies on self-supervised speech representation learning have\nfocused on developing new training methods and applying pre-trained models for\ndifferent applications. However, the quality of these models is often measured\nby the performance of different downstream tasks. How well the representations\naccess the information of interest is less studied. In this work, we take a\ncloser look into existing self-supervised methods of speech from an\ninformation-theoretic perspective. We aim to develop metrics using mutual\ninformation to help practical problems such as model design and selection. We\nuse linear probes to estimate the mutual information between the target\ninformation and learned representations, showing another insight into the\naccessibility to the target information from speech representations. Further,\nwe explore the potential of evaluating representations in a self-supervised\nfashion, where we estimate the mutual information between different parts of\nthe data without using any labels. Finally, we show that both supervised and\nunsupervised measures echo the performance of the models on layer-wise linear\nprobing and speech recognition.", "published": "2024-01-16 21:13:22", "link": "http://arxiv.org/abs/2401.08833v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant\n  Meeting Transcription", "abstract": "We introduce the first Natural Office Talkers in Settings of Far-field Audio\nRecordings (``NOTSOFAR-1'') Challenge alongside datasets and baseline system.\nThe challenge focuses on distant speaker diarization and automatic speech\nrecognition (DASR) in far-field meeting scenarios, with single-channel and\nknown-geometry multi-channel tracks, and serves as a launch platform for two\nnew datasets: First, a benchmarking dataset of 315 meetings, averaging 6\nminutes each, capturing a broad spectrum of real-world acoustic conditions and\nconversational dynamics. It is recorded across 30 conference rooms, featuring\n4-8 attendees and a total of 35 unique speakers. Second, a 1000-hour simulated\ntraining dataset, synthesized with enhanced authenticity for real-world\ngeneralization, incorporating 15,000 real acoustic transfer functions. The\ntasks focus on single-device DASR, where multi-channel devices always share the\nsame known geometry. This is aligned with common setups in actual conference\nrooms, and avoids technical complexities associated with multi-device tasks. It\nalso allows for the development of geometry-specific solutions. The NOTSOFAR-1\nChallenge aims to advance research in the field of distant conversational\nspeech recognition, providing key resources to unlock the potential of\ndata-driven methods, which we believe are currently constrained by the absence\nof comprehensive high-quality training and benchmarking datasets.", "published": "2024-01-16 23:50:26", "link": "http://arxiv.org/abs/2401.08887v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Crowdsourced Adaptive Surveys", "abstract": "Public opinion surveys are vital for informing democratic decision-making,\nbut responding to rapidly evolving information environments and measuring\nbeliefs within niche communities can be challenging for traditional survey\nmethods. This paper introduces a crowdsourced adaptive survey methodology\n(CSAS) that unites advances in natural language processing and adaptive\nalgorithms to generate question banks that evolve with user input. The CSAS\nmethod converts open-ended text provided by participants into survey items and\napplies a multi-armed bandit algorithm to determine which questions should be\nprioritized in the survey. The method's adaptive nature allows for the\nexploration of new survey questions, while imposing minimal costs in survey\nlength. Applications in the domains of Latino information environments,\nnational issue importance, and local politics showcase CSAS's ability to\nidentify topics that might otherwise escape the notice of survey researchers. I\nconclude by highlighting CSAS's potential to bridge conceptual gaps between\nresearchers and participants in survey research.", "published": "2024-01-16 04:05:25", "link": "http://arxiv.org/abs/2401.12986v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "stat.AP"], "primary_category": "cs.CL"}
{"title": "TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition\n  in Conversation", "abstract": "Emotion Recognition in Conversation (ERC) plays a crucial role in enabling\ndialogue systems to effectively respond to user requests. The emotions in a\nconversation can be identified by the representations from various modalities,\nsuch as audio, visual, and text. However, due to the weak contribution of\nnon-verbal modalities to recognize emotions, multimodal ERC has always been\nconsidered a challenging task. In this paper, we propose Teacher-leading\nMultimodal fusion network for ERC (TelME). TelME incorporates cross-modal\nknowledge distillation to transfer information from a language model acting as\nthe teacher to the non-verbal students, thereby optimizing the efficacy of the\nweak modalities. We then combine multimodal features using a shifting fusion\napproach in which student networks support the teacher. TelME achieves\nstate-of-the-art performance in MELD, a multi-speaker conversation dataset for\nERC. Finally, we demonstrate the effectiveness of our components through\nadditional experiments.", "published": "2024-01-16 07:18:41", "link": "http://arxiv.org/abs/2401.12987v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in\n  3D World", "abstract": "Human beings possess the capability to multiply a melange of multisensory\ncues while actively exploring and interacting with the 3D world. Current\nmulti-modal large language models, however, passively absorb sensory data as\ninputs, lacking the capacity to actively interact with the objects in the 3D\nenvironment and dynamically collect their multisensory information. To usher in\nthe study of this area, we propose MultiPLY, a multisensory embodied large\nlanguage model that could incorporate multisensory interactive data, including\nvisual, audio, tactile, and thermal information into large language models,\nthereby establishing the correlation among words, actions, and percepts. To\nthis end, we first collect Multisensory Universe, a large-scale multisensory\ninteraction dataset comprising 500k data by deploying an LLM-powered embodied\nagent to engage with the 3D environment. To perform instruction tuning with\npre-trained LLM on such generated data, we first encode the 3D scene as\nabstracted object-centric representations and then introduce action tokens\ndenoting that the embodied agent takes certain actions within the environment,\nas well as state tokens that represent the multisensory state observations of\nthe agent at each time step. In the inference time, MultiPLY could generate\naction tokens, instructing the agent to take the action in the environment and\nobtain the next multisensory state observation. The observation is then\nappended back to the LLM via state tokens to generate subsequent text or action\ntokens. We demonstrate that MultiPLY outperforms baselines by a large margin\nthrough a diverse set of embodied tasks involving object retrieval, tool use,\nmultisensory captioning, and task decomposition.", "published": "2024-01-16 18:59:45", "link": "http://arxiv.org/abs/2401.08577v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Using i-vectors for subject-independent cross-session EEG transfer\n  learning", "abstract": "Cognitive load classification is the task of automatically determining an\nindividual's utilization of working memory resources during performance of a\ntask based on physiologic measures such as electroencephalography (EEG). In\nthis paper, we follow a cross-disciplinary approach, where tools and\nmethodologies from speech processing are used to tackle this problem. The\ncorpus we use was released publicly in 2021 as part of the first passive\nbrain-computer interface competition on cross-session workload estimation. We\npresent our approach which used i-vector-based neural network classifiers to\naccomplish inter-subject cross-session EEG transfer learning, achieving 18%\nrelative improvement over equivalent subject-dependent models. We also report\nexperiments showing how our subject-independent models perform competitively on\nheld-out subjects and improve with additional subject data, suggesting that\nsubject-dependent training is not required for effective cognitive load\ndetermination.", "published": "2024-01-16 21:56:27", "link": "http://arxiv.org/abs/2401.08851v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.LG"}
{"title": "Multi-Input Multi-Output Target-Speaker Voice Activity Detection For\n  Unified, Flexible, and Robust Audio-Visual Speaker Diarization", "abstract": "Audio-visual learning has demonstrated promising results in many classical\nspeech tasks (e.g., speech separation, automatic speech recognition, wake-word\nspotting). We believe that introducing visual modality will also benefit\nspeaker diarization. To date, Target-Speaker Voice Activity Detection (TS-VAD)\nplays an important role in highly accurate speaker diarization. However,\nprevious TS-VAD models take audio features and utilize the speaker's acoustic\nfootprint to distinguish his or her personal speech activities, which is easily\naffected by overlapped speech in multi-speaker scenarios. Although visual\ninformation naturally tolerates overlapped speech, it suffers from spatial\nocclusion, low resolution, etc. The potential modality-missing problem blocks\nTS-VAD towards an audio-visual approach.\n  This paper proposes a novel Multi-Input Multi-Output Target-Speaker Voice\nActivity Detection (MIMO-TSVAD) framework for speaker diarization. The proposed\nmethod can take audio-visual input and leverage the speaker's acoustic\nfootprint or lip track to flexibly conduct audio-based, video-based, and\naudio-visual speaker diarization in a unified sequence-to-sequence framework.\nExperimental results show that the MIMO-TSVAD framework demonstrates\nstate-of-the-art performance on the VoxConverse, DIHARD-III, and MISP 2022\ndatasets under corresponding evaluation metrics, obtaining the Diarization\nError Rates (DERs) of 4.18%, 10.10%, and 8.15%, respectively. In addition, it\ncan perform robustly in heavy lip-missing scenarios.", "published": "2024-01-16 02:06:28", "link": "http://arxiv.org/abs/2401.08052v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ECAPA2: A Hybrid Neural Network Architecture and Training Strategy for\n  Robust Speaker Embeddings", "abstract": "In this paper, we present ECAPA2, a novel hybrid neural network architecture\nand training strategy to produce robust speaker embeddings. Most speaker\nverification models are based on either the 1D- or 2D-convolutional operation,\noften manifested as Time Delay Neural Networks or ResNets, respectively. Hybrid\nmodels are relatively unexplored without an intuitive explanation what\nconstitutes best practices in regard to its architectural choices. We motivate\nthe proposed ECAPA2 model in this paper with an analysis of current speaker\nverification architectures. In addition, we propose a training strategy which\nmakes the speaker embeddings more robust against overlapping speech and short\nutterance lengths. The presented ECAPA2 architecture and training strategy\nattains state-of-the-art performance on the VoxCeleb1 test sets with\nsignificantly less parameters than current models. Finally, we make a\npre-trained model publicly available to promote research on downstream tasks.", "published": "2024-01-16 13:17:39", "link": "http://arxiv.org/abs/2401.08342v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Microphone Subset Selection for the Weighted Prediction Error Algorithm\n  using a Group Sparsity Penalty", "abstract": "Reverberation can severely degrade the quality of speech signals recorded\nusing microphones in an enclosure. In acoustic sensor networks with spatially\ndistributed microphones, a similar dereverberation performance may be achieved\nusing only a subset of all available microphones. Using the popular convex\nrelaxation method, in this paper we propose to perform microphone subset\nselection for the weighted prediction error (WPE) multi-channel dereverberation\nalgorithm by introducing a group sparsity penalty on the prediction filter\ncoefficients. The resulting problem is shown to be solved efficiently using the\naccelerated proximal gradient algorithm. Experimental evaluation using measured\nimpulse responses shows that the performance of the proposed method is close to\nthe optimal performance obtained by exhaustive search, both for\nfrequency-dependent as well as frequency-independent microphone subset\nselection. Furthermore, the performance using only a few microphones for\nfrequency-independent microphone subset selection is only marginally worse than\nusing all available microphones.", "published": "2024-01-16 16:45:12", "link": "http://arxiv.org/abs/2401.08486v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learning Disentangled Speech Representations with Contrastive Learning\n  and Time-Invariant Retrieval", "abstract": "Voice conversion refers to transferring speaker identity with well-preserved\ncontent. Better disentanglement of speech representations leads to better voice\nconversion. Recent studies have found that phonetic information from input\naudio has the potential ability to well represent content. Besides, the\nspeaker-style modeling with pre-trained models making the process more complex.\nTo tackle these issues, we introduce a new method named \"CTVC\" which utilizes\ndisentangled speech representations with contrastive learning and\ntime-invariant retrieval. Specifically, a similarity-based compression module\nis used to facilitate a more intimate connection between the frame-level hidden\nfeatures and linguistic information at phoneme-level. Additionally, a\ntime-invariant retrieval is proposed for timbre extraction based on multiple\nsegmentations and mutual information. Experimental results demonstrate that\n\"CTVC\" outperforms previous studies and improves the sound quality and\nsimilarity of converted results.", "published": "2024-01-16 03:40:13", "link": "http://arxiv.org/abs/2401.08096v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DIFFRENT: A Diffusion Model for Recording Environment Transfer of Speech", "abstract": "Properly setting up recording conditions, including microphone type and\nplacement, room acoustics, and ambient noise, is essential to obtaining the\ndesired acoustic characteristics of speech. In this paper, we propose\nDiff-R-EN-T, a Diffusion model for Recording ENvironment Transfer which\ntransforms the input speech to have the recording conditions of a reference\nspeech while preserving the speech content. Our model comprises the content\nenhancer, the recording environment encoder, and the diffusion decoder which\ngenerates the target mel-spectrogram by utilizing both enhancer and encoder as\ninput conditions. We evaluate DiffRENT in the speech enhancement and acoustic\nmatching scenarios. The results show that DiffRENT generalizes well to unseen\nenvironments and new speakers. Also, the proposed model achieves superior\nperformances in objective and subjective evaluation. Sound examples of our\nproposed model are available online.", "published": "2024-01-16 04:10:02", "link": "http://arxiv.org/abs/2401.08102v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ED-TTS: Multi-Scale Emotion Modeling using Cross-Domain Emotion\n  Diarization for Emotional Speech Synthesis", "abstract": "Existing emotional speech synthesis methods often utilize an utterance-level\nstyle embedding extracted from reference audio, neglecting the inherent\nmulti-scale property of speech prosody. We introduce ED-TTS, a multi-scale\nemotional speech synthesis model that leverages Speech Emotion Diarization\n(SED) and Speech Emotion Recognition (SER) to model emotions at different\nlevels. Specifically, our proposed approach integrates the utterance-level\nemotion embedding extracted by SER with fine-grained frame-level emotion\nembedding obtained from SED. These embeddings are used to condition the reverse\nprocess of the denoising diffusion probabilistic model (DDPM). Additionally, we\nemploy cross-domain SED to accurately predict soft labels, addressing the\nchallenge of a scarcity of fine-grained emotion-annotated datasets for\nsupervising emotional TTS training.", "published": "2024-01-16 07:13:16", "link": "http://arxiv.org/abs/2401.08166v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LiveScaler: Live control of the harmony of an electronic music track", "abstract": "In Electronic Dance Music (EDM), many artists use DJing techniques in order\nto perform their own productions live. As a consequence, they do not have\naccess during the performance to the internal structure of their tracks, and\nspecifically to their equivalent of a partition: MIDI files. On the other hand,\nif an artist attempts to remix or interpret their own production live, the\nnumber of tracks that they can simultaneously control is limited without\nsuitable software. This article introduces LiveScaler, a software that allows\nlive control of the harmony and pitch of electronic music. A set of pitch\ntransformations, termed affine transformations, is presented. These\ntransformations are applied to all MIDI streams of a prepared track. A MaxMSP\nimplementation, in conjunction with Ableton Live, is proposed. Special\nattention is given to control issues, mapping, and practical live\nexperimentation in the context of EDM.", "published": "2024-01-16 07:54:59", "link": "http://arxiv.org/abs/2401.08181v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Robust DOA estimation using deep acoustic imaging", "abstract": "Direction of arrival estimation (DoAE) aims at tracking a sound in azimuth\nand elevation. Recent advancements include data-driven models with inputs\nderived from ambisonics intensity vectors or correlations between channels in a\nmicrophone array. A spherical intensity map (SIM), or acoustic image, is an\nalternative input representation that remains underexplored. SIMs benefit from\nhigh-resolution microphone arrays, yet most DoAE datasets use low-resolution\nones. Therefore, we first propose a super-resolution method to upsample\nlow-resolution microphones. Next, we benchmark DoAE models that use SIMs as\ninput. We arrive to a model that uses SIMs for DoAE estimation and outperforms\na baseline and a state-of-the-art model. Our study highlights the relevance of\nacoustic imaging for DoAE tasks.", "published": "2024-01-16 04:40:39", "link": "http://arxiv.org/abs/2401.08717v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EmoTalker: Emotionally Editable Talking Face Generation via Diffusion\n  Model", "abstract": "In recent years, the field of talking faces generation has attracted\nconsiderable attention, with certain methods adept at generating virtual faces\nthat convincingly imitate human expressions. However, existing methods face\nchallenges related to limited generalization, particularly when dealing with\nchallenging identities. Furthermore, methods for editing expressions are often\nconfined to a singular emotion, failing to adapt to intricate emotions. To\novercome these challenges, this paper proposes EmoTalker, an emotionally\neditable portraits animation approach based on the diffusion model. EmoTalker\nmodifies the denoising process to ensure preservation of the original\nportrait's identity during inference. To enhance emotion comprehension from\ntext input, Emotion Intensity Block is introduced to analyze fine-grained\nemotions and strengths derived from prompts. Additionally, a crafted dataset is\nharnessed to enhance emotion comprehension within prompts. Experiments show the\neffectiveness of EmoTalker in generating high-quality, emotionally customizable\nfacial expressions.", "published": "2024-01-16 02:02:44", "link": "http://arxiv.org/abs/2401.08049v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "DurFlex-EVC: Duration-Flexible Emotional Voice Conversion Leveraging\n  Discrete Representations without Text Alignment", "abstract": "Emotional voice conversion (EVC) involves modifying various acoustic\ncharacteristics, such as pitch and spectral envelope, to match a desired\nemotional state while preserving the speaker's identity. Existing EVC methods\noften rely on text transcriptions or time-alignment information and struggle to\nhandle varying speech durations effectively. In this paper, we propose\nDurFlex-EVC, a duration-flexible EVC framework that operates without the need\nfor text or alignment information. We introduce a unit aligner that models\ncontextual information by aligning speech with discrete units representing\ncontent, eliminating the need for text or speech-text alignment. Additionally,\nwe design a style autoencoder that effectively disentangles content and\nemotional style, allowing precise manipulation of the emotional characteristics\nof the speech. We further enhance emotional expressiveness through a\nhierarchical stylize encoder that applies the target emotional style at\nmultiple hierarchical levels, refining the stylization process to improve the\nnaturalness and expressiveness of the converted speech. Experimental results\nfrom subjective and objective evaluations demonstrate that our approach\noutperforms baseline models, effectively handling duration variability and\nenhancing emotional expressiveness in the converted speech.", "published": "2024-01-16 03:39:35", "link": "http://arxiv.org/abs/2401.08095v4", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "From Coarse to Fine: Efficient Training for Audio Spectrogram\n  Transformers", "abstract": "Transformers have become central to recent advances in audio classification.\nHowever, training an audio spectrogram transformer, e.g. AST, from scratch can\nbe resource and time-intensive. Furthermore, the complexity of transformers\nheavily depends on the input audio spectrogram size. In this work, we aim to\noptimize AST training by linking to the resolution in the time-axis. We\nintroduce multi-phase training of audio spectrogram transformers by connecting\nthe seminal idea of coarse-to-fine with transformer models. To achieve this, we\npropose a set of methods for temporal compression. By employing one of these\nmethods, the transformer model learns from lower-resolution (coarse) data in\nthe initial phases, and then is fine-tuned with high-resolution data later in a\ncurriculum learning strategy. Experimental results demonstrate that the\nproposed training mechanism for AST leads to improved (or on-par) performance\nwith faster convergence, i.e. requiring fewer computational resources and less\ntime. This approach is also generalizable to other AST-based methods regardless\nof their learning paradigms.", "published": "2024-01-16 14:59:37", "link": "http://arxiv.org/abs/2401.08415v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Binaural Angular Separation Network", "abstract": "We propose a neural network model that can separate target speech sources\nfrom interfering sources at different angular regions using two microphones.\nThe model is trained with simulated room impulse responses (RIRs) using\nomni-directional microphones without needing to collect real RIRs. By relying\non specific angular regions and multiple room simulations, the model utilizes\nconsistent time difference of arrival (TDOA) cues, or what we call delay\ncontrast, to separate target and interference sources while remaining robust in\nvarious reverberation environments. We demonstrate the model is not only\ngeneralizable to a commercially available device with a slightly different\nmicrophone geometry, but also outperforms our previous work which uses one\nadditional microphone on the same device. The model runs in real-time on-device\nand is suitable for low-latency streaming applications such as telephony and\nvideo conferencing.", "published": "2024-01-16 22:36:12", "link": "http://arxiv.org/abs/2401.08864v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Explainable Proxy Model for Multiabel Audio Segmentation", "abstract": "Audio signal segmentation is a key task for automatic audio indexing. It\nconsists of detecting the boundaries of class-homogeneous segments in the\nsignal. In many applications, explainable AI is a vital process for\ntransparency of decision-making with machine learning. In this paper, we\npropose an explainable multilabel segmentation model that solves speech\nactivity (SAD), music (MD), noise (ND), and overlapped speech detection (OSD)\nsimultaneously. This proxy uses the non-negative matrix factorization (NMF) to\nmap the embedding used for the segmentation to the frequency domain.\nExperiments conducted on two datasets show similar performances as the\npre-trained black box model while showing strong explainability features.\nSpecifically, the frequency bins used for the decision can be easily identified\nat both the segment level (local explanations) and global level (class\nprototypes).", "published": "2024-01-16 10:41:33", "link": "http://arxiv.org/abs/2401.08268v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
