{"title": "Semi-interactive Attention Network for Answer Understanding in\n  Reverse-QA", "abstract": "Question answering (QA) is an important natural language processing (NLP)\ntask and has received much attention in academic research and industry\ncommunities. Existing QA studies assume that questions are raised by humans and\nanswers are generated by machines. Nevertheless, in many real applications,\nmachines are also required to determine human needs or perceive human states.\nIn such scenarios, machines may proactively raise questions and humans supply\nanswers. Subsequently, machines should attempt to understand the true meaning\nof these answers. This new QA approach is called reverse-QA (rQA) throughout\nthis paper. In this work, the human answer understanding problem is\ninvestigated and solved by classifying the answers into predefined answer-label\ncategories (e.g., True, False, Uncertain). To explore the relationships between\nquestions and answers, we use the interactive attention network (IAN) model and\npropose an improved structure called semi-interactive attention network\n(Semi-IAN). Two Chinese data sets for rQA are compiled. We evaluate several\nconventional text classification models for comparison, and experimental\nresults indicate the promising performance of our proposed models.", "published": "2019-01-12 02:50:40", "link": "http://arxiv.org/abs/1901.03788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What comes next? Extractive summarization by next-sentence prediction", "abstract": "Existing approaches to automatic summarization assume that a length limit for\nthe summary is given, and view content selection as an optimization problem to\nmaximize informativeness and minimize redundancy within this budget. This\nframework ignores the fact that human-written summaries have rich internal\nstructure which can be exploited to train a summarization system. We present\nNEXTSUM, a novel approach to summarization based on a model that predicts the\nnext sentence to include in the summary using not only the source article, but\nalso the summary produced so far. We show that such a model successfully\ncaptures summary-specific discourse moves, and leads to better content\nselection performance, in addition to automatically predicting how long the\ntarget summary should be. We perform experiments on the New York Times\nAnnotated Corpus of summaries, where NEXTSUM outperforms lead and content-model\nsummarization baselines by significant margins. We also show that the lengths\nof summaries produced by our system correlates with the lengths of the\nhuman-written gold standards.", "published": "2019-01-12 13:00:30", "link": "http://arxiv.org/abs/1901.03859v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HAS-QA: Hierarchical Answer Spans Model for Open-domain Question\n  Answering", "abstract": "This paper is concerned with open-domain question answering (i.e., OpenQA).\nRecently, some works have viewed this problem as a reading comprehension (RC)\ntask, and directly applied successful RC models to it. However, the\nperformances of such models are not so good as that in the RC task. In our\nopinion, the perspective of RC ignores three characteristics in OpenQA task: 1)\nmany paragraphs without the answer span are included in the data collection; 2)\nmultiple answer spans may exist within one given paragraph; 3) the end position\nof an answer span is dependent with the start position. In this paper, we first\npropose a new probabilistic formulation of OpenQA, based on a three-level\nhierarchical structure, i.e.,~the question level, the paragraph level and the\nanswer span level. Then a Hierarchical Answer Spans Model (HAS-QA) is designed\nto capture each probability. HAS-QA has the ability to tackle the above three\nproblems, and experiments on public OpenQA datasets show that it significantly\noutperforms traditional RC baselines and recent OpenQA baselines.", "published": "2019-01-12 14:29:35", "link": "http://arxiv.org/abs/1901.03866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Speech Act Classifier for Persian Texts and its Application in\n  Identifying Rumors", "abstract": "Speech Acts (SAs) are one of the important areas of pragmatics, which give us\na better understanding of the state of mind of the people and convey an\nintended language function. Knowledge of the SA of a text can be helpful in\nanalyzing that text in natural language processing applications. This study\npresents a dictionary-based statistical technique for Persian SA recognition.\nThe proposed technique classifies a text into seven classes of SA based on four\ncriteria: lexical, syntactic, semantic, and surface features. WordNet as the\ntool for extracting synonym and enriching features dictionary is utilized. To\nevaluate the proposed technique, we utilized four classification methods\nincluding Random Forest (RF), Support Vector Machine (SVM), Naive Bayes (NB),\nand K-Nearest Neighbors (KNN). The experimental results demonstrate that the\nproposed method using RF and SVM as the best classifiers achieved a\nstate-of-the-art performance with an accuracy of 0.95 for classification of\nPersian SAs. Our original vision of this work is introducing an application of\nSA recognition on social media content, especially the common SA in rumors.\nTherefore, the proposed system utilized to determine the common SAs in rumors.\nThe results showed that Persian rumors are often expressed in three SA classes\nincluding narrative, question, and threat, and in some cases with the request\nSA.", "published": "2019-01-12 21:54:23", "link": "http://arxiv.org/abs/1901.03904v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Prototypical Metric Transfer Learning for Continuous Speech Keyword\n  Spotting With Limited Training Data", "abstract": "Continuous Speech Keyword Spotting (CSKS) is the problem of spotting keywords\nin recorded conversations, when a small number of instances of keywords are\navailable in training data. Unlike the more common Keyword Spotting, where an\nalgorithm needs to detect lone keywords or short phrases like \"Alexa\",\n\"Cortana\", \"Hi Alexa!\", \"Whatsup Octavia?\" etc. in speech, CSKS needs to filter\nout embedded words from a continuous flow of speech, ie. spot \"Anna\" and\n\"github\" in \"I know a developer named Anna who can look into this github\nissue.\" Apart from the issue of limited training data availability, CSKS is an\nextremely imbalanced classification problem. We address the limitations of\nsimple keyword spotting baselines for both aforementioned challenges by using a\nnovel combination of loss functions (Prototypical networks' loss and metric\nloss) and transfer learning. Our method improves F1 score by over 10%.", "published": "2019-01-12 13:20:54", "link": "http://arxiv.org/abs/1901.03860v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
