{"title": "Homograph Disambiguation Through Selective Diacritic Restoration", "abstract": "Lexical ambiguity, a challenging phenomenon in all natural languages, is\nparticularly prevalent for languages with diacritics that tend to be omitted in\nwriting, such as Arabic. Omitting diacritics leads to an increase in the number\nof homographs: different words with the same spelling. Diacritic restoration\ncould theoretically help disambiguate these words, but in practice, the\nincrease in overall sparsity leads to performance degradation in NLP\napplications. In this paper, we propose approaches for automatically marking a\nsubset of words for diacritic restoration, which leads to selective homograph\ndisambiguation. Compared to full or no diacritic restoration, these approaches\nyield selectively-diacritized datasets that balance sparsity and lexical\ndisambiguation. We evaluate the various selection strategies extrinsically on\nseveral downstream applications: neural machine translation, part-of-speech\ntagging, and semantic textual similarity. Our experiments on Arabic show\npromising results, where our devised strategies on selective diacritization\nlead to a more balanced and consistent performance in downstream applications.", "published": "2019-12-10 03:45:42", "link": "http://arxiv.org/abs/1912.04479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Evaluate the Next System: Automatic Dialogue Evaluation from the\n  Perspective of Continual Learning", "abstract": "Automatic dialogue evaluation plays a crucial role in open-domain dialogue\nresearch. Previous works train neural networks with limited annotation for\nconducting automatic dialogue evaluation, which would naturally affect the\nevaluation fairness as dialogue systems close to the scope of training corpus\nwould have more preference than the other ones. In this paper, we study\nalleviating this problem from the perspective of continual learning: given an\nexisting neural dialogue evaluator and the next system to be evaluated, we\nfine-tune the learned neural evaluator by selectively forgetting/updating its\nparameters, to jointly fit dialogue systems have been and will be evaluated.\nOur motivation is to seek for a lifelong and low-cost automatic evaluation for\ndialogue systems, rather than to reconstruct the evaluator over and over again.\nExperimental results show that our continual evaluator achieves comparable\nperformance with reconstructing new evaluators, while requires significantly\nlower resources.", "published": "2019-12-10 12:27:45", "link": "http://arxiv.org/abs/1912.04664v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GeBioToolkit: Automatic Extraction of Gender-Balanced Multilingual\n  Corpus of Wikipedia Biographies", "abstract": "We introduce GeBioToolkit, a tool for extracting multilingual parallel\ncorpora at sentence level, with document and gender information from Wikipedia\nbiographies. Despite thegender inequalitiespresent in Wikipedia, the toolkit\nhas been designed to extract corpus balanced in gender. While our toolkit is\ncustomizable to any number of languages (and different domains), in this work\nwe present a corpus of 2,000 sentences in English, Spanish and Catalan, which\nhas been post-edited by native speakers to become a high-quality dataset for\nmachinetranslation evaluation. While GeBioCorpus aims at being one of the first\nnon-synthetic gender-balanced test datasets, GeBioToolkit aims at paving the\npath to standardize procedures to produce gender-balanced datasets", "published": "2019-12-10 15:50:50", "link": "http://arxiv.org/abs/1912.04778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Ensemble Method for Producing Word Representations focusing on the\n  Greek Language", "abstract": "In this paper we present a new ensemble method, Continuous Bag-of-Skip-grams\n(CBOS), that produces high-quality word representations putting emphasis on the\nmodern Greek language. The CBOS method combines the pioneering approaches for\nlearning word representations: Continuous Bag-of-Words (CBOW) and Continuous\nSkip-gram. These methods are compared through intrinsic and extrinsic\nevaluation tasks on three different sources of data: the English Wikipedia\ncorpus, the modern Greek Wikipedia corpus, and the modern Greek Web Content\ncorpus. By comparing these methods across different tasks and datasets, it is\nevident that the CBOS method achieves state-of-the-art performance.", "published": "2019-12-10 20:20:40", "link": "http://arxiv.org/abs/1912.04965v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Module Networks for Reasoning over Text", "abstract": "Answering compositional questions that require multiple steps of reasoning\nagainst text is challenging, especially when they involve discrete, symbolic\noperations. Neural module networks (NMNs) learn to parse such questions as\nexecutable programs composed of learnable modules, performing well on synthetic\nvisual QA domains. However, we find that it is challenging to learn these\nmodels for non-synthetic questions on open-domain text, where a model needs to\ndeal with the diversity of natural language and perform a broader range of\nreasoning. We extend NMNs by: (a) introducing modules that reason over a\nparagraph of text, performing symbolic reasoning (such as arithmetic, sorting,\ncounting) over numbers and dates in a probabilistic and differentiable manner;\nand (b) proposing an unsupervised auxiliary loss to help extract arguments\nassociated with the events in text. Additionally, we show that a limited amount\nof heuristically-obtained question program and intermediate module output\nsupervision provides sufficient inductive bias for accurate learning. Our\nproposed model significantly outperforms state-of-the-art models on a subset of\nthe DROP dataset that poses a variety of reasoning challenges that are covered\nby our modules.", "published": "2019-12-10 20:36:07", "link": "http://arxiv.org/abs/1912.04971v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Text Classification With Generative Language Models", "abstract": "This work investigates the use of natural language to enable zero-shot model\nadaptation to new tasks. We use text and metadata from social commenting\nplatforms as a source for a simple pretraining task. We then provide the\nlanguage model with natural language descriptions of classification tasks as\ninput and train it to generate the correct answer in natural language via a\nlanguage modeling objective. This allows the model to generalize to new\nclassification tasks without the need for multiple multitask classification\nheads. We show the zero-shot performance of these generative language models,\ntrained with weak supervision, on six benchmark text classification datasets\nfrom the torchtext library. Despite no access to training data, we achieve up\nto a 45% absolute improvement in classification accuracy over random or\nmajority class baselines. These results show that natural language can serve as\nsimple and powerful descriptors for task adaptation. We believe this points the\nway to new metalearning strategies for text problems.", "published": "2019-12-10 17:01:34", "link": "http://arxiv.org/abs/1912.10165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fraud detection in telephone conversations for financial services using\n  linguistic features", "abstract": "Detecting the elements of deception in a conversation is one of the most\nchallenging problems for the AI community. It becomes even more difficult to\ndesign a transparent system, which is fully explainable and satisfies the need\nfor financial and legal services to be deployed. This paper presents an\napproach for fraud detection in transcribed telephone conversations using\nlinguistic features. The proposed approach exploits the syntactic and semantic\ninformation of the transcription to extract both the linguistic markers and the\nsentiment of the customer's response. We demonstrate the results on real-world\nfinancial services data using simple, robust and explainable classifiers such\nas Naive Bayes, Decision Tree, Nearest Neighbours, and Support Vector Machines.", "published": "2019-12-10 15:07:48", "link": "http://arxiv.org/abs/1912.04748v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Machine Translation with Cross-lingual Word Embeddings", "abstract": "Learning word embeddings using distributional information is a task that has\nbeen studied by many researchers, and a lot of studies are reported in the\nliterature. On the contrary, less studies were done for the case of multiple\nlanguages. The idea is to focus on a single representation for a pair of\nlanguages such that semantically similar words are closer to one another in the\ninduced representation irrespective of the language. In this way, when data are\nmissing for a particular language, classifiers from another language can be\nused.", "published": "2019-12-10 19:50:35", "link": "http://arxiv.org/abs/1912.10167v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Duet at TREC 2019 Deep Learning Track", "abstract": "This report discusses three submissions based on the Duet architecture to the\nDeep Learning track at TREC 2019. For the document retrieval task, we adapt the\nDuet model to ingest a \"multiple field\" view of documents---we refer to the new\narchitecture as Duet with Multiple Fields (DuetMF). A second submission\ncombines the DuetMF model with other neural and traditional relevance\nestimators in a learning-to-rank framework and achieves improved performance\nover the DuetMF baseline. For the passage retrieval task, we submit a single\nrun based on an ensemble of eight Duet models.", "published": "2019-12-10 03:23:05", "link": "http://arxiv.org/abs/1912.04471v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Introducing MANtIS: a novel Multi-Domain Information Seeking Dialogues\n  Dataset", "abstract": "Conversational search is an approach to information retrieval (IR), where\nusers engage in a dialogue with an agent in order to satisfy their information\nneeds. Previous conceptual work described properties and actions a good agent\nshould exhibit. Unlike them, we present a novel conceptual model defined in\nterms of conversational goals, which enables us to reason about current\nresearch practices in conversational search. Based on the literature, we elicit\nhow existing tasks and test collections from the fields of IR, natural language\nprocessing (NLP) and dialogue systems (DS) fit into this model. We describe a\nset of characteristics that an ideal conversational search dataset should have.\nLastly, we introduce MANtIS (the code and dataset are available at\nhttps://guzpenha.github.io/MANtIS/), a large-scale dataset containing\nmulti-domain and grounded information seeking dialogues that fulfill all of our\ndataset desiderata. We provide baseline results for the conversation response\nranking and user intent prediction tasks.", "published": "2019-12-10 10:59:47", "link": "http://arxiv.org/abs/1912.04639v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Novel Topology for End-to-end Temporal Classification and Segmentation\n  with Recurrent Neural Network", "abstract": "Connectionist temporal classification (CTC) has matured as an alignment free\nto sequence transduction and shows competitive for end-to-end speech\nrecognition. In the CTC topology, the blank symbol occupies more than half of\nthe state trellis, which results the spike phenomenon of the non-blank symbols.\nFor classification task, the spikes work quite well, but as to the segmentation\ntask it does not provide boundaries information. In this paper, a novel\ntopology is introduced to combine the temporal classification and segmentation\nability in one framework.", "published": "2019-12-10 15:53:59", "link": "http://arxiv.org/abs/1912.04784v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Embedding Comparator: Visualizing Differences in Global Structure and\n  Local Neighborhoods via Small Multiples", "abstract": "Embeddings mapping high-dimensional discrete input to lower-dimensional\ncontinuous vector spaces have been widely adopted in machine learning\napplications as a way to capture domain semantics. Interviewing 13 embedding\nusers across disciplines, we find comparing embeddings is a key task for\ndeployment or downstream analysis but unfolds in a tedious fashion that poorly\nsupports systematic exploration. In response, we present the Embedding\nComparator, an interactive system that presents a global comparison of\nembedding spaces alongside fine-grained inspection of local neighborhoods. It\nsystematically surfaces points of comparison by computing the similarity of the\n$k$-nearest neighbors of every embedded object between a pair of spaces.\nThrough case studies across multiple modalities, we demonstrate our system\nrapidly reveals insights, such as semantic changes following fine-tuning,\nlanguage changes over time, and differences between seemingly similar models.\nIn evaluations with 15 participants, we find our system accelerates comparisons\nby shifting from laborious manual specification to browsing and manipulating\nvisualizations.", "published": "2019-12-10 17:46:43", "link": "http://arxiv.org/abs/1912.04853v3", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Medication Regimen Extraction From Medical Conversations", "abstract": "Extracting relevant information from medical conversations and providing it\nto doctors and patients might help in addressing doctor burnout and patient\nforgetfulness. In this paper, we focus on extracting the Medication Regimen\n(dosage and frequency for medications) discussed in a medical conversation. We\nframe the problem as a Question Answering (QA) task and perform comparative\nanalysis over: a QA approach, a new combined QA and Information Extraction\napproach, and other baselines. We use a small corpus of 6,692 annotated\ndoctor-patient conversations for the task. Clinical conversation corpora are\ncostly to create, difficult to handle (because of data privacy concerns), and\nthus scarce. We address this data scarcity challenge through data augmentation\nmethods, using publicly available embeddings and pretrain part of the network\non a related task (summarization) to improve the model's performance. Compared\nto the baseline, our best-performing models improve the dosage and frequency\nextractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94,\nrespectively. Using our best-performing model, we present the first fully\nautomated system that can extract Medication Regimen tags from spontaneous\ndoctor-patient conversations with about $\\approx$71% accuracy.", "published": "2019-12-10 20:18:39", "link": "http://arxiv.org/abs/1912.04961v3", "categories": ["cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unsupervised Transfer Learning via BERT Neuron Selection", "abstract": "Recent advancements in language representation models such as BERT have led\nto a rapid improvement in numerous natural language processing tasks. However,\nlanguage models usually consist of a few hundred million trainable parameters\nwith embedding space distributed across multiple layers, thus making them\nchallenging to be fine-tuned for a specific task or to be transferred to a new\ndomain. To determine whether there are task-specific neurons that can be\nexploited for unsupervised transfer learning, we introduce a method for\nselecting the most important neurons to solve a specific classification task.\nThis algorithm is further extended to multi-source transfer learning by\ncomputing the importance of neurons for several single-source transfer learning\nscenarios between different subsets of data sources. Besides, a task-specific\nfingerprint for each data source is obtained based on the percentage of the\nselected neurons in each layer. We perform extensive experiments in\nunsupervised transfer learning for sentiment analysis, natural language\ninference and sentence similarity, and compare our results with the existing\nliterature and baselines. Significantly, we found that the source and target\ndata sources with higher degrees of similarity between their task-specific\nfingerprints demonstrate a better transferability property. We conclude that\nour method can lead to better performance using just a few hundred\ntask-specific and interpretable neurons.", "published": "2019-12-10 16:08:26", "link": "http://arxiv.org/abs/1912.05308v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Advances in Online Audio-Visual Meeting Transcription", "abstract": "This paper describes a system that generates speaker-annotated transcripts of\nmeetings by using a microphone array and a 360-degree camera. The hallmark of\nthe system is its ability to handle overlapped speech, which has been an\nunsolved problem in realistic settings for over a decade. We show that this\nproblem can be addressed by using a continuous speech separation approach. In\naddition, we describe an online audio-visual speaker diarization method that\nleverages face tracking and identification, sound source localization, speaker\nidentification, and, if available, prior speaker information for robustness to\nvarious real world challenges. All components are integrated in a meeting\ntranscription framework called SRD, which stands for \"separate, recognize, and\ndiarize\". Experimental results using recordings of natural meetings involving\nup to 11 attendees are reported. The continuous speech separation improves a\nword error rate (WER) by 16.1% compared with a highly tuned beamformer. When a\ncomplete list of meeting attendees is available, the discrepancy between WER\nand speaker-attributed WER is only 1.0%, indicating accurate word-to-speaker\nassociation. This increases marginally to 1.6% when 50% of the attendees are\nunknown to the system.", "published": "2019-12-10 20:59:24", "link": "http://arxiv.org/abs/1912.04979v1", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Development and Evaluation of Video Recordings for the OLSA Matrix\n  Sentence Test", "abstract": "One of the established multi-lingual methods for testing speech\nintelligibility is the matrix sentence test (MST). Most versions of this test\nare designed with audio-only stimuli. Nevertheless, visual cues play an\nimportant role in speech intelligibility, mostly making it easier to understand\nspeech by speechreading. In this work we present the creation and evaluation of\ndubbed videos for the Oldenburger female MST (OLSA). 28 normal-hearing\nparticipants completed test and retest sessions with conditions including audio\nand visual modalities, speech in quiet and noise, and open and closed-set\nresponse formats. The levels to reach 80% sentence intelligibility were\nmeasured adaptively for the different conditions. In quiet, the audiovisual\nbenefit compared to audio-only was 7 dB in sound pressure level (SPL). In\nnoise, the audiovisual benefit was 5 dB in signal-to-noise ratio (SNR).\nSpeechreading scores ranged from 0% to 84% speech reception in visual-only\nsentences, with an average of 50% across participants. This large variability\nin speechreading abilities was reflected in the audiovisual speech reception\nthresholds (SRTs), which had a larger standard deviation than the audio-only\nSRTs. Training and learning effects in audiovisual sentences were found:\nparticipants improved their SRTs by approximately 3 dB SNR after 5 trials.\nParticipants retained their best scores on a separate retest session and\nfurther improved their SRTs by approx. -1.5 dB.", "published": "2019-12-10 14:15:42", "link": "http://arxiv.org/abs/1912.04700v3", "categories": ["eess.AS", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Sound Event Detection of Weakly Labelled Data with CNN-Transformer and\n  Automatic Threshold Optimization", "abstract": "Sound event detection (SED) is a task to detect sound events in an audio\nrecording. One challenge of the SED task is that many datasets such as the\nDetection and Classification of Acoustic Scenes and Events (DCASE) datasets are\nweakly labelled. That is, there are only audio tags for each audio clip without\nthe onset and offset times of sound events. \\qk{We compare segment-wise and\nclip-wise training for SED that is lacking in previous works. We propose a\nconvolutional neural network transformer (CNN-Transfomer) for audio tagging and\nSED, and show that CNN-Transformer performs similarly to a convolutional\nrecurrent neural network (CRNN)}. Another challenge of SED is that thresholds\nare required for detecting sound events. Previous works set thresholds\nempirically, and are not an optimal approaches. To solve this problem, we\npropose an automatic threshold optimization method. The first stage is to\noptimize the system with respect to metrics that do not depend on thresholds,\nsuch as mean average precision (mAP). The second stage is to optimize the\nthresholds with respect to metrics that depends on those thresholds. Our\nproposed automatic threshold optimization system achieves a state-of-the-art\naudio tagging F1 of 0.646, outperforming that without threshold optimization of\n0.629, and a sound event detection F1 of 0.584, outperforming that without\nthreshold optimization of 0.564.", "published": "2019-12-10 15:25:37", "link": "http://arxiv.org/abs/1912.04761v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quantifying the Chaos Level of Infants' Environment via Unsupervised\n  Learning", "abstract": "Acoustic environments vary dramatically within the home setting. They can be\na source of comfort and tranquility or chaos that can lead to less optimal\ncognitive development in children. Research to date has only subjectively\nmeasured household chaos. In this work, we use three unsupervised machine\nlearning techniques to quantify household chaos in infants' homes. These\nunsupervised techniques include hierarchical clustering using K-Means,\nclustering using self-organizing map (SOM) and deep learning. We evaluated\nthese techniques using data from 9 participants which is a total of 197 hours.\nResults show that these techniques are promising to quantify household chaos.", "published": "2019-12-10 17:34:31", "link": "http://arxiv.org/abs/1912.04844v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Cooperative Audio Source Separation and Enhancement Using Distributed\n  Microphone Arrays and Wearable Devices", "abstract": "Augmented listening devices such as hearing aids often perform poorly in\nnoisy and reverberant environments with many competing sound sources. Large\ndistributed microphone arrays can improve performance, but data from remote\nmicrophones often cannot be used for delay-constrained real-time processing. We\npresent a cooperative audio source separation and enhancement system that\nleverages wearable listening devices and other microphone arrays spread around\na room. The full distributed array is used to separate sound sources and\nestimate their statistics. Each listening device uses these statistics to\ndesign real-time binaural audio enhancement filters using its own local\nmicrophones. The system is demonstrated experimentally using 10 speech sources\nand 160 microphones in a large, reverberant room.", "published": "2019-12-10 22:55:21", "link": "http://arxiv.org/abs/1912.05038v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Motion-Tolerant Beamforming with Deformable Microphone Arrays", "abstract": "Microphone arrays are usually assumed to have rigid geometries: the\nmicrophones may move with respect to the sound field but remain fixed relative\nto each other. However, many useful arrays, such as those in wearable devices,\nhave sensors that can move relative to each other. We compare two approaches to\nbeamforming with deformable microphone arrays: first, by explicitly tracking\nthe geometry of the array as it changes over time, and second, by designing a\ntime-invariant beamformer based on the second-order statistics of the moving\narray. The time-invariant approach is shown to be appropriate when the motion\nof the array is small relative to the acoustic wavelengths of interest. The\nperformance of the proposed beamforming system is demonstrated using a wearable\nmicrophone array on a moving human listener in a cocktail-party scenario.", "published": "2019-12-10 23:17:47", "link": "http://arxiv.org/abs/1912.05043v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Listen to Look: Action Recognition by Previewing Audio", "abstract": "In the face of the video data deluge, today's expensive clip-level\nclassifiers are increasingly impractical. We propose a framework for efficient\naction recognition in untrimmed video that uses audio as a preview mechanism to\neliminate both short-term and long-term visual redundancies. First, we devise\nan ImgAud2Vid framework that hallucinates clip-level features by distilling\nfrom lighter modalities---a single frame and its accompanying audio---reducing\nshort-term temporal redundancy for efficient clip-level recognition. Second,\nbuilding on ImgAud2Vid, we further propose ImgAud-Skimming, an attention-based\nlong short-term memory network that iteratively selects useful moments in\nuntrimmed videos, reducing long-term temporal redundancy for efficient\nvideo-level recognition. Extensive experiments on four action recognition\ndatasets demonstrate that our method achieves the state-of-the-art in terms of\nboth recognition accuracy and speed.", "published": "2019-12-10 04:15:24", "link": "http://arxiv.org/abs/1912.04487v3", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Encoding Musical Style with Transformer Autoencoders", "abstract": "We consider the problem of learning high-level controls over the global\nstructure of generated sequences, particularly in the context of symbolic music\ngeneration with complex language models. In this work, we present the\nTransformer autoencoder, which aggregates encodings of the input data across\ntime to obtain a global representation of style from a given performance. We\nshow it is possible to combine this global representation with other temporally\ndistributed embeddings, enabling improved control over the separate aspects of\nperformance style and melody. Empirically, we demonstrate the effectiveness of\nour method on various music generation tasks on the MAESTRO dataset and a\nYouTube dataset with 10,000+ hours of piano performances, where we achieve\nimprovements in terms of log-likelihood and mean listening scores as compared\nto baselines.", "published": "2019-12-10 19:51:44", "link": "http://arxiv.org/abs/1912.05537v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Measuring Mother-Infant Emotions By Audio Sensing", "abstract": "It has been suggested in developmental psychology literature that the\ncommunication of affect between mothers and their infants correlates with the\nsocioemotional and cognitive development of infants. In this study, we obtained\nday-long audio recordings of 10 mother-infant pairs in order to study their\naffect communication in speech with a focus on mother's speech. In order to\nbuild a model for speech emotion detection, we used the Ryerson Audio-Visual\nDatabase of Emotional Speech and Song (RAVDESS) and trained a Convolutional\nNeural Nets model which is able to classify 6 different emotions at 70%\naccuracy. We applied our model to mother's speech and found the dominant\nemotions were angry and sad, which were not true. Based on our own\nobservations, we concluded that emotional speech databases made with the help\nof actors cannot generalize well to real-life settings, suggesting an active\nlearning or unsupervised approach in the future.", "published": "2019-12-10 19:49:35", "link": "http://arxiv.org/abs/1912.05920v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
