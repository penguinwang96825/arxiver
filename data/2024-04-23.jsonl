{"title": "MisgenderMender: A Community-Informed Approach to Interventions for\n  Misgendering", "abstract": "Content Warning: This paper contains examples of misgendering and erasure\nthat could be offensive and potentially triggering.\n  Misgendering, the act of incorrectly addressing someone's gender, inflicts\nserious harm and is pervasive in everyday technologies, yet there is a notable\nlack of research to combat it. We are the first to address this lack of\nresearch into interventions for misgendering by conducting a survey of\ngender-diverse individuals in the US to understand perspectives about automated\ninterventions for text-based misgendering. Based on survey insights on the\nprevalence of misgendering, desired solutions, and associated concerns, we\nintroduce a misgendering interventions task and evaluation dataset,\nMisgenderMender. We define the task with two sub-tasks: (i) detecting\nmisgendering, followed by (ii) correcting misgendering where misgendering is\npresent in domains where editing is appropriate. MisgenderMender comprises 3790\ninstances of social media content and LLM-generations about non-cisgender\npublic figures, annotated for the presence of misgendering, with additional\nannotations for correcting misgendering in LLM-generated text. Using this\ndataset, we set initial benchmarks by evaluating existing NLP systems and\nhighlighting challenges for future models to address. We release the full\ndataset, code, and demo at\nhttps://tamannahossainkay.github.io/misgendermender/.", "published": "2024-04-23 02:54:00", "link": "http://arxiv.org/abs/2404.14695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Insights into Alignment: Evaluating DPO and its Variants Across Multiple\n  Tasks", "abstract": "This study evaluates Direct Preference Optimization (DPO) and its variants\nfor aligning Large Language Models (LLMs) with human preferences, testing three\nconfigurations: (1) with Supervised Fine Tuning (SFT), (2) without SFT, and (3)\nwithout SFT but using an instruction tuned model. We further investigate how\ntraining set size influences model performance. Our evaluation spans 13\nbenchmarks covering dialogue, reasoning, mathematical problem-solving, question\nanswering, truthfulness, MT-Bench, Big Bench, and the Open LLM Leaderboard. We\nfind that: (1) alignment methods often achieve near optimal performance even\nwith smaller subsets of training data; (2) although they offer limited\nimprovements on complex reasoning tasks, they enhance mathematical\nproblem-solving; and (3) using an instruction tuned model improves\ntruthfulness. These insights highlight the conditions under which alignment\nmethods excel, as well as their limitations.", "published": "2024-04-23 03:55:01", "link": "http://arxiv.org/abs/2404.14723v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling the Sacred: Considerations when Using Religious Texts in\n  Natural Language Processing", "abstract": "This position paper concerns the use of religious texts in Natural Language\nProcessing (NLP), which is of special interest to the Ethics of NLP. Religious\ntexts are expressions of culturally important values, and machine learned\nmodels have a propensity to reproduce cultural values encoded in their training\ndata. Furthermore, translations of religious texts are frequently used by NLP\nresearchers when language data is scarce. This repurposes the translations from\ntheir original uses and motivations, which often involve attracting new\nfollowers. This paper argues that NLP's use of such texts raises considerations\nthat go beyond model biases, including data provenance, cultural contexts, and\ntheir use in proselytism. We argue for more consideration of researcher\npositionality, and of the perspectives of marginalized linguistic and religious\ncommunities.", "published": "2024-04-23 04:47:22", "link": "http://arxiv.org/abs/2404.14740v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simulating Task-Oriented Dialogues with State Transition Graphs and\n  Large Language Models", "abstract": "This paper explores SynTOD, a new synthetic data generation approach for\ndeveloping end-to-end Task-Oriented Dialogue (TOD) Systems capable of handling\ncomplex tasks such as intent classification, slot filling, conversational\nquestion-answering, and retrieval-augmented response generation, without\nrelying on crowdsourcing or real-world data. SynTOD utilizes a state transition\ngraph to define the desired behavior of a TOD system and generates diverse,\nstructured conversations through random walks and response simulation using\nlarge language models (LLMs). In our experiments, using graph-guided response\nsimulations leads to significant improvements in intent classification, slot\nfilling and response relevance compared to naive single-prompt simulated\nconversations. We also investigate the end-to-end TOD effectiveness of\ndifferent base and instruction-tuned LLMs, with and without the constructed\nsynthetic conversations. Finally, we explore how various LLMs can evaluate\nresponses in a TOD system and how well they are correlated with human\njudgments. Our findings pave the path towards quick development and evaluation\nof domain-specific TOD systems. We release our datasets, models, and code for\nresearch purposes.", "published": "2024-04-23 06:23:34", "link": "http://arxiv.org/abs/2404.14772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Med42 -- Evaluating Fine-Tuning Strategies for Medical LLMs:\n  Full-Parameter vs. Parameter-Efficient Approaches", "abstract": "This study presents a comprehensive analysis and comparison of two\npredominant fine-tuning methodologies - full-parameter fine-tuning and\nparameter-efficient tuning - within the context of medical Large Language\nModels (LLMs). We developed and refined a series of LLMs, based on the Llama-2\narchitecture, specifically designed to enhance medical knowledge retrieval,\nreasoning, and question-answering capabilities. Our experiments systematically\nevaluate the effectiveness of these tuning strategies across various well-known\nmedical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of\n72% on the US Medical Licensing Examination (USMLE) datasets, setting a new\nstandard in performance for openly available medical LLMs. Through this\ncomparative analysis, we aim to identify the most effective and efficient\nmethod for fine-tuning LLMs in the medical domain, thereby contributing\nsignificantly to the advancement of AI-driven healthcare applications.", "published": "2024-04-23 06:36:21", "link": "http://arxiv.org/abs/2404.14779v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Chain of Thought Prompting in Large Language Models via\n  Reasoning Patterns", "abstract": "Chain of Thought (CoT) prompting can encourage language models to engage in\nmulti-step logical reasoning. The quality of the provided demonstrations\nsignificantly influences the success of downstream inference tasks. Current\nunsupervised CoT methods primarily select examples based on the semantics of\nthe questions, which can introduce noise and lack interpretability. In this\npaper, we propose leveraging reasoning patterns to enhance CoT prompting\neffectiveness. Reasoning patterns represent the process by which language\nmodels arrive at their final results. By utilizing prior knowledge and\nprompt-based methods from large models, we first construct task-specific\npattern sets. We then select diverse demonstrations based on different\nreasoning patterns. This approach not only mitigates the impact of noise but\nalso provides explicit interpretability to help us understand the mechanisms of\nCoT. Extensive experiments demonstrate that our method is more robust and\nconsistently leads to improvements across various reasoning tasks.", "published": "2024-04-23 07:50:00", "link": "http://arxiv.org/abs/2404.14812v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-Level or Token-Level? A Comprehensive Study on Knowledge\n  Distillation", "abstract": "Knowledge distillation, transferring knowledge from a teacher model to a\nstudent model, has emerged as a powerful technique in neural machine\ntranslation for compressing models or simplifying training targets. Knowledge\ndistillation encompasses two primary methods: sentence-level distillation and\ntoken-level distillation. In sentence-level distillation, the student model is\ntrained to align with the output of the teacher model, which can alleviate the\ntraining difficulty and give student model a comprehensive understanding of\nglobal structure. Differently, token-level distillation requires the student\nmodel to learn the output distribution of the teacher model, facilitating a\nmore fine-grained transfer of knowledge. Studies have revealed divergent\nperformances between sentence-level and token-level distillation across\ndifferent scenarios, leading to the confusion on the empirical selection of\nknowledge distillation methods. In this study, we argue that token-level\ndistillation, with its more complex objective (i.e., distribution), is better\nsuited for ``simple'' scenarios, while sentence-level distillation excels in\n``complex'' scenarios. To substantiate our hypothesis, we systematically\nanalyze the performance of distillation methods by varying the model size of\nstudent models, the complexity of text, and the difficulty of decoding\nprocedure. While our experimental results validate our hypothesis, defining the\ncomplexity level of a given scenario remains a challenging task. So we further\nintroduce a novel hybrid method that combines token-level and sentence-level\ndistillation through a gating mechanism, aiming to leverage the advantages of\nboth individual methods. Experiments demonstrate that the hybrid method\nsurpasses the performance of token-level or sentence-level distillation methods\nand the previous works by a margin, demonstrating the effectiveness of the\nproposed hybrid method.", "published": "2024-04-23 08:29:56", "link": "http://arxiv.org/abs/2404.14827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language in Vivo vs. in Silico: Size Matters but Larger Language Models\n  Still Do Not Comprehend Language on a Par with Humans", "abstract": "Understanding the limits of language is a prerequisite for Large Language\nModels (LLMs) to act as theories of natural language. LLM performance in some\nlanguage tasks presents both quantitative and qualitative differences from that\nof humans, however it remains to be determined whether such differences are\namenable to model size. This work investigates the critical role of model\nscaling, determining whether increases in size make up for such differences\nbetween humans and models. We test three LLMs from different families (Bard,\n137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a\ngrammaticality judgment task featuring anaphora, center embedding,\ncomparatives, and negative polarity. N=1,200 judgments are collected and scored\nfor accuracy, stability, and improvements in accuracy upon repeated\npresentation of a prompt. Results of the best performing LLM, ChatGPT-4, are\ncompared to results of n=80 humans on the same stimuli. We find that humans are\noverall less accurate than ChatGPT-4 (76% vs. 80% accuracy, respectively), but\nthat this is due to ChatGPT-4 outperforming humans only in one task condition,\nnamely on grammatical sentences. Additionally, ChatGPT-4 wavers more than\nhumans in its answers (12.5% vs. 9.6% likelihood of an oscillating answer,\nrespectively). Thus, while increased model size may lead to better performance,\nLLMs are still not sensitive to (un)grammaticality the same way as humans are.\nIt seems possible but unlikely that scaling alone can fix this issue. We\ninterpret these results by comparing language learning in vivo and in silico,\nidentifying three critical differences concerning (i) the type of evidence,\n(ii) the poverty of the stimulus, and (iii) the occurrence of semantic\nhallucinations due to impenetrable linguistic reference.", "published": "2024-04-23 10:09:46", "link": "http://arxiv.org/abs/2404.14883v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pillars of Grammatical Error Correction: Comprehensive Inspection Of\n  Contemporary Approaches In The Era of Large Language Models", "abstract": "In this paper, we carry out experimental research on Grammatical Error\nCorrection, delving into the nuances of single-model systems, comparing the\nefficiency of ensembling and ranking methods, and exploring the application of\nlarge language models to GEC as single-model systems, as parts of ensembles,\nand as ranking methods. We set new state-of-the-art performance with F_0.5\nscores of 72.8 on CoNLL-2014-test and 81.4 on BEA-test, respectively. To\nsupport further advancements in GEC and ensure the reproducibility of our\nresearch, we make our code, trained models, and systems' outputs publicly\navailable.", "published": "2024-04-23 10:57:59", "link": "http://arxiv.org/abs/2404.14914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Current Approaches to Lemmatization: A Case Study in\n  Estonian", "abstract": "This study evaluates three different lemmatization approaches to Estonian --\nGenerative character-level models, Pattern-based word-level classification\nmodels, and rule-based morphological analysis. According to our experiments, a\nsignificantly smaller Generative model consistently outperforms the\nPattern-based classification model based on EstBERT. Additionally, we observe a\nrelatively small overlap in errors made by all three models, indicating that an\nensemble of different approaches could lead to improvements.", "published": "2024-04-23 13:06:32", "link": "http://arxiv.org/abs/2404.15003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TAXI: Evaluating Categorical Knowledge Editing for Language Models", "abstract": "Humans rarely learn one fact in isolation. Instead, learning a new fact\ninduces knowledge of other facts about the world. For example, in learning a\nkorat is a type of cat, you also infer it is a mammal and has claws, ensuring\nyour model of the world is consistent. Knowledge editing aims to inject new\nfacts into language models to improve their factuality, but current benchmarks\nfail to evaluate consistency, which is critical to ensure efficient, accurate,\nand generalizable edits. We manually create TAXI, a new benchmark dataset\nspecifically created to evaluate consistency in categorical knowledge edits.\nTAXI contains 11,120 multiple-choice queries for 976 edits spanning 41\ncategories (e.g., Dogs), 164 subjects (e.g., Labrador), and 183 properties\n(e.g., is a mammal). We then use TAXI to evaluate popular editors' categorical\nconsistency, measuring how often editing a subject's category appropriately\nedits its properties. We find that 1) the editors achieve marginal, yet\nnon-random consistency, 2) their consistency far underperforms human baselines,\nand 3) consistency is more achievable when editing atypical subjects Our code\nand data are available at https://github.com/derekpowell/taxi.", "published": "2024-04-23 13:09:11", "link": "http://arxiv.org/abs/2404.15004v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Textual Personality Detection toward Social Media: Integrating\n  Long-term and Short-term Perspectives", "abstract": "Textual personality detection aims to identify personality characteristics by\nanalyzing user-generated content toward social media platforms. Numerous\npsychological literature highlighted that personality encompasses both\nlong-term stable traits and short-term dynamic states. However, existing\nstudies often concentrate only on either long-term or short-term personality\nrepresentations, without effectively combining both aspects. This limitation\nhinders a comprehensive understanding of individuals' personalities, as both\nstable traits and dynamic states are vital. To bridge this gap, we propose a\nDual Enhanced Network(DEN) to jointly model users' long-term and short-term\npersonality for textual personality detection. In DEN, a Long-term Personality\nEncoding is devised to effectively model long-term stable personality traits.\nShort-term Personality Encoding is presented to capture short-term dynamic\npersonality states. The Bi-directional Interaction component facilitates the\nintegration of both personality aspects, allowing for a comprehensive\nrepresentation of the user's personality. Experimental results on two\npersonality detection datasets demonstrate the effectiveness of the DEN model\nand the benefits of considering both the dynamic and stable nature of\npersonality characteristics for textual personality detection.", "published": "2024-04-23 14:13:53", "link": "http://arxiv.org/abs/2404.15067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-view Content-aware Indexing for Long Document Retrieval", "abstract": "Long document question answering (DocQA) aims to answer questions from long\ndocuments over 10k words. They usually contain content structures such as\nsections, sub-sections, and paragraph demarcations. However, the indexing\nmethods of long documents remain under-explored, while existing systems\ngenerally employ fixed-length chunking. As they do not consider content\nstructures, the resultant chunks can exclude vital information or include\nirrelevant content. Motivated by this, we propose the Multi-view Content-aware\nindexing (MC-indexing) for more effective long DocQA via (i) segment structured\ndocument into content chunks, and (ii) represent each content chunk in\nraw-text, keywords, and summary views. We highlight that MC-indexing requires\nneither training nor fine-tuning. Having plug-and-play capability, it can be\nseamlessly integrated with any retrievers to boost their performance. Besides,\nwe propose a long DocQA dataset that includes not only question-answer pair,\nbut also document structure and answer scope. When compared to state-of-art\nchunking schemes, MC-indexing has significantly increased the recall by 42.8%,\n30.0%, 23.9%, and 16.3% via top k= 1.5, 3, 5, and 10 respectively. These\nimproved scores are the average of 8 widely used retrievers (2 sparse and 6\ndense) via extensive experiments.", "published": "2024-04-23 14:55:32", "link": "http://arxiv.org/abs/2404.15103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Fairness Issues in Automatically Generated Testing Content", "abstract": "Natural language generation tools are powerful and effective for generating\ncontent. However, language models are known to display bias and fairness\nissues, making them impractical to deploy for many use cases. We here focus on\nhow fairness issues impact automatically generated test content, which can have\nstringent requirements to ensure the test measures only what it was intended to\nmeasure. Specifically, we review test content generated for a large-scale\nstandardized English proficiency test with the goal of identifying content that\nonly pertains to a certain subset of the test population as well as content\nthat has the potential to be upsetting or distracting to some test takers.\nIssues like these could inadvertently impact a test taker's score and thus\nshould be avoided. This kind of content does not reflect the more\ncommonly-acknowledged biases, making it challenging even for modern models that\ncontain safeguards. We build a dataset of 601 generated texts annotated for\nfairness and explore a variety of methods for classification: fine-tuning,\ntopic-based classification, and prompting, including few-shot and\nself-correcting prompts. We find that combining prompt self-correction and\nfew-shot learning performs best, yielding an F1 score of 0.79 on our held-out\ntest set, while much smaller BERT- and topic-based models have competitive\nperformance on out-of-domain data.", "published": "2024-04-23 14:56:15", "link": "http://arxiv.org/abs/2404.15104v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Student Data Paradox and Curious Case of Single Student-Tutor Model:\n  Regressive Side Effects of Training LLMs for Personalized Learning", "abstract": "The pursuit of personalized education has led to the integration of Large\nLanguage Models (LLMs) in developing intelligent tutoring systems. To better\nunderstand and adapt to individual student needs, including their\nmisconceptions, LLMs need to be trained on extensive datasets of student-tutor\ndialogues. Our research uncovers a fundamental challenge in this approach: the\n``Student Data Paradox.'' This paradox emerges when LLMs, trained on student\ndata to understand learner behavior, inadvertently compromise their own factual\nknowledge and reasoning abilities. We investigate this paradox by training\nstate-of-the-art language models on student-tutor dialogue datasets and\nevaluating their performance across multiple benchmarks. These benchmarks\nassess various aspects of language model capabilities, including reasoning,\ntruthfulness, and common sense understanding. Our findings reveal significant\ndeclines in the models' performance across these diverse benchmarks, indicating\na broad impact on their capabilities when trained to model student behavior.\nOur research makes two primary contributions: (1) empirical demonstration of\nthe Student Data Paradox through quantitative analysis of model performance,\nand (2) introduction of ``hallucination tokens'' as a mitigation strategy.\nThese tokens, while improving performance, highlight the persistent challenge\nof balancing accurate student behavior modeling with maintaining the LLM's\nintegrity as an educational tool. This study emphasizes the need for innovative\nsolutions to reconcile the conflicting goals of faithfully understanding\ndiverse student cognition while preserving the model's ability to provide\naccurate information and guidance.", "published": "2024-04-23 15:57:55", "link": "http://arxiv.org/abs/2404.15156v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Setting up the Data Printer with Improved English to Ukrainian Machine\n  Translation", "abstract": "To build large language models for Ukrainian we need to expand our corpora\nwith large amounts of new algorithmic tasks expressed in natural language.\nExamples of task performance expressed in English are abundant, so with a\nhigh-quality translation system our community will be enabled to curate\ndatasets faster. To aid this goal, we introduce a recipe to build a translation\nsystem using supervised finetuning of a large pretrained language model with a\nnoisy parallel dataset of 3M pairs of Ukrainian and English sentences followed\nby a second phase of training using 17K examples selected by k-fold perplexity\nfiltering on another dataset of higher quality. Our decoder-only model named\nDragoman beats performance of previous state of the art encoder-decoder models\non the FLORES devtest set.", "published": "2024-04-23 16:34:34", "link": "http://arxiv.org/abs/2404.15196v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Instruction Tuning Make LLMs More Consistent?", "abstract": "The purpose of instruction tuning is enabling zero-shot performance, but\ninstruction tuning has also been shown to improve chain-of-thought reasoning\nand value alignment (Si et al., 2023). Here we consider the impact on\n$\\textit{consistency}$, i.e., the sensitivity of language models to small\nperturbations in the input. We compare 10 instruction-tuned LLaMA models to the\noriginal LLaMA-7b model and show that almost across-the-board they become more\nconsistent, both in terms of their representations and their predictions in\nzero-shot and downstream tasks. We explain these improvements through\nmechanistic analyses of factual recall.", "published": "2024-04-23 16:39:03", "link": "http://arxiv.org/abs/2404.15206v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of\n  the Noisy Channel", "abstract": "Training task-oriented dialogue systems typically requires turn-level\nannotations for interacting with their APIs: e.g. a dialogue state and the\nsystem actions taken at each step. These annotations can be costly to produce,\nerror-prone, and require both domain and annotation expertise. With advances in\nLLMs, we hypothesize that unlabeled data and a schema definition are sufficient\nfor building a working task-oriented dialogue system, completely unsupervised.\nWe consider a novel unsupervised setting of only (1) a well-defined API schema\n(2) a set of unlabeled dialogues between a user and agent. We propose an\ninnovative approach using expectation-maximization (EM) that infers turn-level\nannotations as latent variables using a noisy channel model to build an\nend-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark,\nour method more than doubles the dialogue success rate of a strong GPT-3.5\nbaseline.", "published": "2024-04-23 16:51:26", "link": "http://arxiv.org/abs/2404.15219v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CASPR: Automated Evaluation Metric for Contrastive Summarization", "abstract": "Summarizing comparative opinions about entities (e.g., hotels, phones) from a\nset of source reviews, often referred to as contrastive summarization, can\nconsiderably aid users in decision making. However, reliably measuring the\ncontrastiveness of the output summaries without relying on human evaluations\nremains an open problem. Prior work has proposed token-overlap based metrics,\nDistinctiveness Score, to measure contrast which does not take into account the\nsensitivity to meaning-preserving lexical variations. In this work, we propose\nan automated evaluation metric CASPR to better measure contrast between a pair\nof summaries. Our metric is based on a simple and light-weight method that\nleverages natural language inference (NLI) task to measure contrast by\nsegmenting reviews into single-claim sentences and carefully aggregating NLI\nscores between them to come up with a summary-level score. We compare CASPR\nwith Distinctiveness Score and a simple yet powerful baseline based on\nBERTScore. Our results on a prior dataset CoCoTRIP demonstrate that CASPR can\nmore reliably capture the contrastiveness of the summary pairs compared to the\nbaselines.", "published": "2024-04-23 23:27:29", "link": "http://arxiv.org/abs/2404.15565v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models for Material Selection", "abstract": "Material selection is a crucial step in conceptual design due to its\nsignificant impact on the functionality, aesthetics, manufacturability, and\nsustainability impact of the final product. This study investigates the use of\nLarge Language Models (LLMs) for material selection in the product design\nprocess and compares the performance of LLMs against expert choices for various\ndesign scenarios. By collecting a dataset of expert material preferences, the\nstudy provides a basis for evaluating how well LLMs can align with expert\nrecommendations through prompt engineering and hyperparameter tuning. The\ndivergence between LLM and expert recommendations is measured across different\nmodel configurations, prompt strategies, and temperature settings. This\napproach allows for a detailed analysis of factors influencing the LLMs'\neffectiveness in recommending materials. The results from this study highlight\ntwo failure modes, and identify parallel prompting as a useful\nprompt-engineering method when using LLMs for material selection. The findings\nfurther suggest that, while LLMs can provide valuable assistance, their\nrecommendations often vary significantly from those of human experts. This\ndiscrepancy underscores the need for further research into how LLMs can be\nbetter tailored to replicate expert decision-making in material selection. This\nwork contributes to the growing body of knowledge on how LLMs can be integrated\ninto the design process, offering insights into their current limitations and\npotential for future improvements.", "published": "2024-04-23 18:53:33", "link": "http://arxiv.org/abs/2405.03695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Word Embedding with Better Distance Weighting and Window Size\n  Scheduling", "abstract": "Distributed word representation (a.k.a. word embedding) is a key focus in\nnatural language processing (NLP). As a highly successful word embedding model,\nWord2Vec offers an efficient method for learning distributed word\nrepresentations on large datasets. However, Word2Vec lacks consideration for\ndistances between center and context words. We propose two novel methods,\nLearnable Formulated Weights (LFW) and Epoch-based Dynamic Window Size (EDWS),\nto incorporate distance information into two variants of Word2Vec, the\nContinuous Bag-of-Words (CBOW) model and the Continuous Skip-gram (Skip-gram)\nmodel. For CBOW, LFW uses a formula with learnable parameters that best\nreflects the relationship of influence and distance between words to calculate\ndistance-related weights for average pooling, providing insights for future NLP\ntext modeling research. For Skip-gram, we improve its dynamic window size\nstrategy to introduce distance information in a more balanced way. Experiments\nprove the effectiveness of LFW and EDWS in enhancing Word2Vec's performance,\nsurpassing previous state-of-the-art methods.", "published": "2024-04-23 00:05:48", "link": "http://arxiv.org/abs/2404.14631v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FINEMATCH: Aspect-based Fine-grained Image and Text Mismatch Detection\n  and Correction", "abstract": "Recent progress in large-scale pre-training has led to the development of\nadvanced vision-language models (VLMs) with remarkable proficiency in\ncomprehending and generating multimodal content. Despite the impressive ability\nto perform complex reasoning for VLMs, current models often struggle to\neffectively and precisely capture the compositional information on both the\nimage and text sides. To address this, we propose FineMatch, a new aspect-based\nfine-grained text and image matching benchmark, focusing on text and image\nmismatch detection and correction. This benchmark introduces a novel task for\nboosting and evaluating the VLMs' compositionality for aspect-based\nfine-grained text and image matching. In this task, models are required to\nidentify mismatched aspect phrases within a caption, determine the aspect's\nclass, and propose corrections for an image-text pair that may contain between\n0 and 3 mismatches. To evaluate the models' performance on this new task, we\npropose a new evaluation metric named ITM-IoU for which our experiments show a\nhigh correlation to human evaluation. In addition, we also provide a\ncomprehensive experimental analysis of existing mainstream VLMs, including\nfully supervised learning and in-context learning settings. We have found that\nmodels trained on FineMatch demonstrate enhanced proficiency in detecting\nfine-grained text and image mismatches. Moreover, models (e.g., GPT-4V, Gemini\nPro Vision) with strong abilities to perform multimodal in-context learning are\nnot as skilled at fine-grained compositional image and text matching analysis.\nWith FineMatch, we are able to build a system for text-to-image generation\nhallucination detection and correction.", "published": "2024-04-23 03:42:14", "link": "http://arxiv.org/abs/2404.14715v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete\n  Knowledge Graph Question Answering", "abstract": "To address the issues of insufficient knowledge and hallucination in Large\nLanguage Models (LLMs), numerous studies have explored integrating LLMs with\nKnowledge Graphs (KGs). However, these methods are typically evaluated on\nconventional Knowledge Graph Question Answering (KGQA) with complete KGs, where\nall factual triples required for each question are entirely covered by the\ngiven KG. In such cases, LLMs primarily act as an agent to find answer entities\nwithin the KG, rather than effectively integrating the internal knowledge of\nLLMs and external knowledge sources such as KGs. In fact, KGs are often\nincomplete to cover all the knowledge required to answer questions. To simulate\nthese real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, we propose leveraging LLMs for QA under\nIncomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the\nfactual triples for each question, and construct corresponding datasets. To\nhandle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),\nwhich can generate new factual triples while exploring KGs. Specifically, GoG\nperforms reasoning through a Thinking-Searching-Generating framework, which\ntreats LLM as both Agent and KG in IKGQA. Experimental results on two datasets\ndemonstrate that our GoG outperforms all previous methods.", "published": "2024-04-23 04:47:22", "link": "http://arxiv.org/abs/2404.14741v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Cells: Evolutional Process to Acquire Sense Diversity of Items", "abstract": "Previous models for learning the semantic vectors of items and their groups,\nsuch as words, sentences, nodes, and graphs, using distributed representation\nhave been based on the assumption that the basic sense of an item corresponds\nto one vector composed of dimensions corresponding to hidden contexts in the\ntarget real world, from which multiple senses of the item are obtained by\nconforming to lexical databases or adapting to the context. However, there may\nbe multiple senses of an item, which are hardly assimilated and change or\nevolve dynamically following the contextual shift even within a document or a\nrestricted period. This is a process similar to the evolution or adaptation of\na living entity with/to environmental shifts. Setting the scope of\ndisambiguation of items for sensemaking, the author presents a method in which\na word or item in the data embraces multiple semantic vectors that evolve via\ninteraction with others, similar to a cell embracing chromosomes crossing over\nwith each other. We obtained two preliminary results: (1) the role of a word\nthat evolves to acquire the largest or lower-middle variance of semantic\nvectors tends to be explainable by the author of the text; (2) the epicenters\nof earthquakes that acquire larger variance via crossover, corresponding to the\ninteraction with diverse areas of land crust, are likely to correspond to the\nepicenters of forthcoming large earthquakes.", "published": "2024-04-23 05:11:08", "link": "http://arxiv.org/abs/2404.14749v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ClinicalAgent: Clinical Trial Multi-Agent System with Large Language\n  Model-based Reasoning", "abstract": "Large Language Models (LLMs) and multi-agent systems have shown impressive\ncapabilities in natural language tasks but face challenges in clinical trial\napplications, primarily due to limited access to external knowledge.\nRecognizing the potential of advanced clinical trial tools that aggregate and\npredict based on the latest medical data, we propose an integrated solution to\nenhance their accessibility and utility. We introduce Clinical Agent System\n(ClinicalAgent), a clinical multi-agent system designed for clinical trial\ntasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct\nreasoning technology. This integration not only boosts LLM performance in\nclinical contexts but also introduces novel functionalities. The proposed\nmethod achieves competitive predictive performance in clinical trial outcome\nprediction (0.7908 PR-AUC), obtaining a 0.3326 improvement over the standard\nprompt Method. Publicly available code can be found at\nhttps://anonymous.4open.science/r/ClinicalAgent-6671.", "published": "2024-04-23 06:30:53", "link": "http://arxiv.org/abs/2404.14777v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond the Speculative Game: A Survey of Speculative Execution in Large\n  Language Models", "abstract": "With the increasingly giant scales of (causal) large language models (LLMs),\nthe inference efficiency comes as one of the core concerns along the improved\nperformance. In contrast to the memory footprint, the latency bottleneck seems\nto be of greater importance as there can be billions of requests to a LLM\n(e.g., GPT-4) per day. The bottleneck is mainly due to the autoregressive\ninnateness of LLMs, where tokens can only be generated sequentially during\ndecoding. To alleviate the bottleneck, the idea of speculative execution, which\noriginates from the field of computer architecture, is introduced to LLM\ndecoding in a \\textit{draft-then-verify} style. Under this regime, a sequence\nof tokens will be drafted in a fast pace by utilizing some heuristics, and then\nthe tokens shall be verified in parallel by the LLM. As the costly sequential\ninference is parallelized, LLM decoding speed can be significantly boosted.\nDriven by the success of LLMs in recent couple of years, a growing literature\nin this direction has emerged. Yet, there lacks a position survey to summarize\nthe current landscape and draw a roadmap for future development of this\npromising area. To meet this demand, we present the very first survey paper\nthat reviews and unifies literature of speculative execution in LLMs (e.g.,\nblockwise parallel decoding, speculative decoding, etc.) in a comprehensive\nframework and a systematic taxonomy. Based on the taxonomy, we present a\ncritical review and comparative analysis of the current arts. Finally we\nhighlight various key challenges and future directions to further develop the\narea.", "published": "2024-04-23 10:25:45", "link": "http://arxiv.org/abs/2404.14897v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does It Make Sense to Explain a Black Box With Another Black Box?", "abstract": "Although counterfactual explanations are a popular approach to explain ML\nblack-box classifiers, they are less widespread in NLP. Most methods find those\nexplanations by iteratively perturbing the target document until it is\nclassified differently by the black box. We identify two main families of\ncounterfactual explanation methods in the literature, namely, (a)\n\\emph{transparent} methods that perturb the target by adding, removing, or\nreplacing words, and (b) \\emph{opaque} approaches that project the target\ndocument into a latent, non-interpretable space where the perturbation is\ncarried out subsequently. This article offers a comparative study of the\nperformance of these two families of methods on three classical NLP tasks. Our\nempirical evidence shows that opaque approaches can be an overkill for\ndownstream applications such as fake news detection or sentiment analysis since\nthey add an additional level of complexity with no significant performance\ngain. These observations motivate our discussion, which raises the question of\nwhether it makes sense to explain a black box using another black box.", "published": "2024-04-23 11:40:30", "link": "http://arxiv.org/abs/2404.14943v1", "categories": ["cs.CL", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs\n  Better Solvers for Math Word Problems", "abstract": "Chain-of-Thought (CoT) prompting has enhanced the performance of Large\nLanguage Models (LLMs) across various reasoning tasks. However, CoT still falls\nshort in dealing with complex math word problems, as it usually suffers from\nthree pitfalls: semantic misunderstanding errors, calculation errors, and\nstep-missing errors. Prior studies involve addressing the calculation errors\nand step-missing errors, but neglect the semantic misunderstanding errors,\nwhich is the major factor limiting the reasoning performance of LLMs. To this\nend, we propose a simple-yet-effective method, namely Deeply Understanding the\nProblems (DUP), to improve the LLMs' math problem-solving ability by addressing\nsemantic misunderstanding errors. The core of our method is to encourage the\nLLMs to deeply understand the problems and extract the key problem-solving\ninformation used for better reasoning. Extensive experiments on 10 diverse\nreasoning benchmarks show that our DUP method consistently outperforms the\nother counterparts by a large margin. More encouragingly, DUP achieves a new\nSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under the\nzero-shot setting.", "published": "2024-04-23 12:16:05", "link": "http://arxiv.org/abs/2404.14963v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Social Media and Artificial Intelligence for Sustainable Cities and\n  Societies: A Water Quality Analysis Use-case", "abstract": "This paper focuses on a very important societal challenge of water quality\nanalysis. Being one of the key factors in the economic and social development\nof society, the provision of water and ensuring its quality has always remained\none of the top priorities of public authorities. To ensure the quality of\nwater, different methods for monitoring and assessing the water networks, such\nas offline and online surveys, are used. However, these surveys have several\nlimitations, such as the limited number of participants and low frequency due\nto the labor involved in conducting such surveys. In this paper, we propose a\nNatural Language Processing (NLP) framework to automatically collect and\nanalyze water-related posts from social media for data-driven decisions. The\nproposed framework is composed of two components, namely (i) text\nclassification, and (ii) topic modeling. For text classification, we propose a\nmerit-fusion-based framework incorporating several Large Language Models (LLMs)\nwhere different weight selection and optimization methods are employed to\nassign weights to the LLMs. In topic modeling, we employed the BERTopic library\nto discover the hidden topic patterns in the water-related tweets. We also\nanalyzed relevant tweets originating from different regions and countries to\nexplore global, regional, and country-specific issues and water-related\nconcerns. We also collected and manually annotated a large-scale dataset, which\nis expected to facilitate future research on the topic.", "published": "2024-04-23 12:33:14", "link": "http://arxiv.org/abs/2404.14977v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "A Reproducibility Study of PLAID", "abstract": "The PLAID (Performance-optimized Late Interaction Driver) algorithm for\nColBERTv2 uses clustered term representations to retrieve and progressively\nprune documents for final (exact) document scoring. In this paper, we reproduce\nand fill in missing gaps from the original work. By studying the parameters\nPLAID introduces, we find that its Pareto frontier is formed of a careful\nbalance among its three parameters; deviations beyond the suggested settings\ncan substantially increase latency without necessarily improving its\neffectiveness. We then compare PLAID with an important baseline missing from\nthe paper: re-ranking a lexical system. We find that applying ColBERTv2 as a\nre-ranker atop an initial pool of BM25 results provides better\nefficiency-effectiveness trade-offs in low-latency settings. However,\nre-ranking cannot reach peak effectiveness at higher latency settings due to\nlimitations in recall of lexical matching and provides a poor approximation of\nan exhaustive ColBERTv2 search. We find that recently proposed modifications to\nre-ranking that pull in the neighbors of top-scoring documents overcome this\nlimitation, providing a Pareto frontier across all operational points for\nColBERTv2 when evaluated using a well-annotated dataset. Curious about why\nre-ranking methods are highly competitive with PLAID, we analyze the token\nrepresentation clusters PLAID uses for retrieval and find that most clusters\nare predominantly aligned with a single token and vice versa. Given the\ncompetitive trade-offs that re-ranking baselines exhibit, this work highlights\nthe importance of carefully selecting pertinent baselines when evaluating the\nefficiency of retrieval engines.", "published": "2024-04-23 12:46:53", "link": "http://arxiv.org/abs/2404.14989v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "GSCo: Towards Generalizable AI in Medicine via Generalist-Specialist\n  Collaboration", "abstract": "Generalist foundation models (GFMs) are renowned for their exceptional\ncapability and flexibility in effectively generalizing across diverse tasks and\nmodalities. In the field of medicine, while GFMs exhibit superior\ngeneralizability based on their extensive intrinsic knowledge as well as\nproficiency in instruction following and in-context learning, specialist models\nexcel in precision due to their domain knowledge. In this work, for the first\ntime, we explore the synergy between the GFM and specialist models, to enable\nprecise medical image analysis on a broader scope. Specifically, we propose a\ncooperative framework, Generalist-Specialist Collaboration (GSCo), which\nconsists of two stages, namely the construction of GFM and specialists, and\ncollaborative inference on downstream tasks. In the construction stage, we\ndevelop MedDr, the largest open-source GFM tailored for medicine, showcasing\nexceptional instruction-following and in-context learning capabilities.\nMeanwhile, a series of lightweight specialists are crafted for downstream tasks\nwith low computational cost. In the collaborative inference stage, we introduce\ntwo cooperative mechanisms, Mixture-of-Expert Diagnosis and Retrieval-Augmented\nDiagnosis, to harvest the generalist's in-context learning abilities alongside\nthe specialists' domain expertise. For a comprehensive evaluation, we curate a\nlarge-scale benchmark featuring 28 datasets and about 250,000 images. Extensive\nresults demonstrate that MedDr consistently outperforms state-of-the-art GFMs\non downstream datasets. Furthermore, GSCo exceeds both GFMs and specialists\nacross all out-of-domain disease diagnosis datasets. These findings indicate a\nsignificant paradigm shift in the application of GFMs, transitioning from\nseparate models for specific tasks to a collaborative approach between GFMs and\nspecialists, thereby advancing the frontiers of generalizable AI in medicine.", "published": "2024-04-23 15:27:19", "link": "http://arxiv.org/abs/2404.15127v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Rethinking LLM Memorization through the Lens of Adversarial Compression", "abstract": "Large language models (LLMs) trained on web-scale datasets raise substantial\nconcerns regarding permissible data usage. One major question is whether these\nmodels \"memorize\" all their training data or they integrate many data sources\nin some way more akin to how a human would learn and synthesize information.\nThe answer hinges, to a large degree, on how we define memorization. In this\nwork, we propose the Adversarial Compression Ratio (ACR) as a metric for\nassessing memorization in LLMs. A given string from the training data is\nconsidered memorized if it can be elicited by a prompt (much) shorter than the\nstring itself -- in other words, if these strings can be \"compressed\" with the\nmodel by computing adversarial prompts of fewer tokens. The ACR overcomes the\nlimitations of existing notions of memorization by (i) offering an adversarial\nview of measuring memorization, especially for monitoring unlearning and\ncompliance; and (ii) allowing for the flexibility to measure memorization for\narbitrary strings at a reasonably low compute. Our definition serves as a\npractical tool for determining when model owners may be violating terms around\ndata usage, providing a potential legal tool and a critical lens through which\nto address such scenarios.", "published": "2024-04-23 15:49:37", "link": "http://arxiv.org/abs/2404.15146v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Bias patterns in the application of LLMs for clinical decision support:\n  A comprehensive study", "abstract": "Large Language Models (LLMs) have emerged as powerful candidates to inform\nclinical decision-making processes. While these models play an increasingly\nprominent role in shaping the digital landscape, two growing concerns emerge in\nhealthcare applications: 1) to what extent do LLMs exhibit social bias based on\npatients' protected attributes (like race), and 2) how do design choices (like\narchitecture design and prompting strategies) influence the observed biases? To\nanswer these questions rigorously, we evaluated eight popular LLMs across three\nquestion-answering (QA) datasets using clinical vignettes (patient\ndescriptions) standardized for bias evaluations. We employ red-teaming\nstrategies to analyze how demographics affect LLM outputs, comparing both\ngeneral-purpose and clinically-trained models. Our extensive experiments reveal\nvarious disparities (some significant) across protected groups. We also observe\nseveral counter-intuitive patterns such as larger models not being necessarily\nless biased and fined-tuned models on medical data not being necessarily better\nthan the general-purpose models. Furthermore, our study demonstrates the impact\nof prompt design on bias patterns and shows that specific phrasing can\ninfluence bias patterns and reflection-type approaches (like Chain of Thought)\ncan reduce biased outcomes effectively. Consistent with prior studies, we call\non additional evaluations, scrutiny, and enhancement of LLMs used in clinical\ndecision support applications.", "published": "2024-04-23 15:52:52", "link": "http://arxiv.org/abs/2404.15149v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Re-Thinking Inverse Graphics With Large Language Models", "abstract": "Inverse graphics -- the task of inverting an image into physical variables\nthat, when rendered, enable reproduction of the observed scene -- is a\nfundamental challenge in computer vision and graphics. Successfully\ndisentangling an image into its constituent elements, such as the shape, color,\nand material properties of the objects of the 3D scene that produced it,\nrequires a comprehensive understanding of the environment. This complexity\nlimits the ability of existing carefully engineered approaches to generalize\nacross domains. Inspired by the zero-shot ability of large language models\n(LLMs) to generalize to novel contexts, we investigate the possibility of\nleveraging the broad world knowledge encoded in such models to solve\ninverse-graphics problems. To this end, we propose the Inverse-Graphics Large\nLanguage Model (IG-LLM), an inverse-graphics framework centered around an LLM,\nthat autoregressively decodes a visual embedding into a structured,\ncompositional 3D-scene representation. We incorporate a frozen pre-trained\nvisual encoder and a continuous numeric head to enable end-to-end training.\nThrough our investigation, we demonstrate the potential of LLMs to facilitate\ninverse graphics through next-token prediction, without the application of\nimage-space supervision. Our analysis enables new possibilities for precise\nspatial reasoning about images that exploit the visual knowledge of LLMs. We\nrelease our code and data at https://ig-llm.is.tue.mpg.de/ to ensure the\nreproducibility of our investigation and to facilitate future research.", "published": "2024-04-23 16:59:02", "link": "http://arxiv.org/abs/2404.15228v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CultureBank: An Online Community-Driven Knowledge Base Towards\n  Culturally Aware Language Technologies", "abstract": "To enhance language models' cultural awareness, we design a generalizable\npipeline to construct cultural knowledge bases from different online\ncommunities on a massive scale. With the pipeline, we construct CultureBank, a\nknowledge base built upon users' self-narratives with 12K cultural descriptors\nsourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge\nresources, CultureBank contains diverse views on cultural descriptors to allow\nflexible interpretation of cultural knowledge, and contextualized cultural\nscenarios to help grounded evaluation. With CultureBank, we evaluate different\nLLMs' cultural awareness, and identify areas for improvement. We also fine-tune\na language model on CultureBank: experiments show that it achieves better\nperformances on two downstream cultural tasks in a zero-shot setting. Finally,\nwe offer recommendations based on our findings for future culturally aware\nlanguage technologies. The project page is https://culturebank.github.io . The\ncode and model is at https://github.com/SALT-NLP/CultureBank . The released\nCultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .", "published": "2024-04-23 17:16:08", "link": "http://arxiv.org/abs/2404.15238v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference", "abstract": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.", "published": "2024-04-23 18:10:42", "link": "http://arxiv.org/abs/2404.15420v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating the Efficacy of Large Language Models in Identifying Phishing\n  Attempts", "abstract": "Phishing, a prevalent cybercrime tactic for decades, remains a significant\nthreat in today's digital world. By leveraging clever social engineering\nelements and modern technology, cybercrime targets many individuals,\nbusinesses, and organizations to exploit trust and security. These\ncyber-attackers are often disguised in many trustworthy forms to appear as\nlegitimate sources. By cleverly using psychological elements like urgency,\nfear, social proof, and other manipulative strategies, phishers can lure\nindividuals into revealing sensitive and personalized information. Building on\nthis pervasive issue within modern technology, this paper aims to analyze the\neffectiveness of 15 Large Language Models (LLMs) in detecting phishing\nattempts, specifically focusing on a randomized set of \"419 Scam\" emails. The\nobjective is to determine which LLMs can accurately detect phishing emails by\nanalyzing a text file containing email metadata based on predefined criteria.\nThe experiment concluded that the following models, ChatGPT 3.5,\nGPT-3.5-Turbo-Instruct, and ChatGPT, were the most effective in detecting\nphishing emails.", "published": "2024-04-23 19:55:18", "link": "http://arxiv.org/abs/2404.15485v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Killkan: The Automatic Speech Recognition Dataset for Kichwa with\n  Morphosyntactic Information", "abstract": "This paper presents Killkan, the first dataset for automatic speech\nrecognition (ASR) in the Kichwa language, an indigenous language of Ecuador.\nKichwa is an extremely low-resource endangered language, and there have been no\nresources before Killkan for Kichwa to be incorporated in applications of\nnatural language processing. The dataset contains approximately 4 hours of\naudio with transcription, translation into Spanish, and morphosyntactic\nannotation in the format of Universal Dependencies. The audio data was\nretrieved from a publicly available radio program in Kichwa. This paper also\nprovides corpus-linguistic analyses of the dataset with a special focus on the\nagglutinative morphology of Kichwa and frequent code-switching with Spanish.\nThe experiments show that the dataset makes it possible to develop the first\nASR system for Kichwa with reliable quality despite its small dataset size.\nThis dataset, the ASR model, and the code used to develop them will be publicly\navailable. Thus, our study positively showcases resource building and its\napplications for low-resource languages and their community.", "published": "2024-04-23 20:26:07", "link": "http://arxiv.org/abs/2404.15501v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToM-LM: Delegating Theory of Mind Reasoning to External Symbolic\n  Executors in Large Language Models", "abstract": "Theory of Mind (ToM) refers to the ability of individuals to attribute mental\nstates to others. While Large Language Models (LLMs) have shown some promise\nwith ToM ability, they still struggle with complex ToM reasoning. Our approach\nleverages an external symbolic executor, specifically the SMCDEL model checker,\nand fine-tuning to improve the ToM reasoning ability of LLMs. In our approach,\nan LLM is first fine-tuned through pairs of natural language and symbolic\nformulation representation of ToM problems and is then instructed to generate\nthe symbolic formulation with a one-shot in-context example. The generated\nsymbolic formulation is then executed by the SMCDEL model checker to perform\ntransparent and verifiable ToM reasoning and give the final result. We\ndemonstrate that our approach, ToM-LM, shows a significant improvement over all\nthe constructed baselines. Our study proposes a novel view about externalizing\na particular component of ToM reasoning, mainly reasoning about beliefs, and\nsuggests generalizing it to other aspects of ToM reasoning.", "published": "2024-04-23 20:59:03", "link": "http://arxiv.org/abs/2404.15515v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability\n  of Large Language Models", "abstract": "Recently developed large language models (LLMs) have been shown to perform\nremarkably well on a wide range of language understanding tasks. But, can they\nreally \"reason\" over the natural language? This question has been receiving\nsignificant research attention and many reasoning skills such as commonsense,\nnumerical, and qualitative have been studied. However, the crucial skill\npertaining to 'logical reasoning' has remained underexplored. Existing work\ninvestigating this reasoning ability of LLMs has focused only on a couple of\ninference rules (such as modus ponens and modus tollens) of propositional and\nfirst-order logic. Addressing the above limitation, we comprehensively evaluate\nthe logical reasoning ability of LLMs on 25 different reasoning patterns\nspanning over propositional, first-order, and non-monotonic logics. To enable\nsystematic evaluation, we introduce LogicBench, a natural language\nquestion-answering dataset focusing on the use of a single inference rule. We\nconduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini,\nLlama-2, and Mistral using chain-of-thought prompting. Experimental results\nshow that existing LLMs do not fare well on LogicBench; especially, they\nstruggle with instances involving complex reasoning and negations. Furthermore,\nthey sometimes overlook contextual information necessary for reasoning to\narrive at the correct conclusion. We believe that our work and findings\nfacilitate future research for evaluating and enhancing the logical reasoning\nability of LLMs. Data and code are available at\nhttps://github.com/Mihir3009/LogicBench.", "published": "2024-04-23 21:08:49", "link": "http://arxiv.org/abs/2404.15522v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PRISM: Patient Records Interpretation for Semantic Clinical Trial\n  Matching using Large Language Models", "abstract": "Clinical trial matching is the task of identifying trials for which patients\nmay be potentially eligible. Typically, this task is labor-intensive and\nrequires detailed verification of patient electronic health records (EHRs)\nagainst the stringent inclusion and exclusion criteria of clinical trials. This\nprocess is manual, time-intensive, and challenging to scale up, resulting in\nmany patients missing out on potential therapeutic options. Recent advancements\nin Large Language Models (LLMs) have made automating patient-trial matching\npossible, as shown in multiple concurrent research studies. However, the\ncurrent approaches are confined to constrained, often synthetic datasets that\ndo not adequately mirror the complexities encountered in real-world medical\ndata. In this study, we present the first, end-to-end large-scale empirical\nevaluation of clinical trial matching using real-world EHRs. Our study\nshowcases the capability of LLMs to accurately match patients with appropriate\nclinical trials. We perform experiments with proprietary LLMs, including GPT-4\nand GPT-3.5, as well as our custom fine-tuned model called OncoLLM and show\nthat OncoLLM, despite its significantly smaller size, not only outperforms\nGPT-3.5 but also matches the performance of qualified medical doctors. All\nexperiments were carried out on real-world EHRs that include clinical notes and\navailable clinical trials from a single cancer center in the United States.", "published": "2024-04-23 22:33:19", "link": "http://arxiv.org/abs/2404.15549v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SHED: Shapley-Based Automated Dataset Refinement for Instruction\n  Fine-Tuning", "abstract": "The pre-trained Large Language Models (LLMs) can be adapted for many\ndownstream tasks and tailored to align with human preferences through\nfine-tuning. Recent studies have discovered that LLMs can achieve desirable\nperformance with only a small amount of high-quality data, suggesting that a\nlarge amount of the data in these extensive datasets is redundant or even\nharmful. Identifying high-quality data from vast datasets to curate small yet\neffective datasets has emerged as a critical challenge. In this paper, we\nintroduce SHED, an automated dataset refinement framework based on Shapley\nvalue for instruction fine-tuning. SHED eliminates the need for human\nintervention or the use of commercial LLMs. Moreover, the datasets curated\nthrough SHED exhibit transferability, indicating they can be reused across\ndifferent LLMs with consistently high performance. We conduct extensive\nexperiments to evaluate the datasets curated by SHED. The results demonstrate\nSHED's superiority over state-of-the-art methods across various tasks and LLMs;\nnotably, datasets comprising only 10% of the original data selected by SHED\nachieve performance comparable to or surpassing that of the full datasets.", "published": "2024-04-23 04:56:48", "link": "http://arxiv.org/abs/2405.00705v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NExT: Teaching Large Language Models to Reason about Code Execution", "abstract": "A fundamental skill among human developers is the ability to understand and\nreason about program execution. As an example, a programmer can mentally\nsimulate code execution in natural language to debug and repair code (aka.\nrubber duck debugging). However, large language models (LLMs) of code are\ntypically trained on the surface textual form of programs, thus may lack a\nsemantic understanding of how programs execute at run-time. To address this\nissue, we propose NExT, a method to teach LLMs to inspect the execution traces\nof programs (variable states of executed lines) and reason about their run-time\nbehavior through chain-of-thought (CoT) rationales. Specifically, NExT uses\nself-training to bootstrap a synthetic training set of execution-aware\nrationales that lead to correct task solutions (e.g., fixed programs) without\nlaborious manual annotation. Experiments on program repair tasks based on MBPP\nand HumanEval demonstrate that NExT improves the fix rate of a PaLM 2 model, by\n26.1% and 14.3% absolute, respectively, with significantly improved rationale\nquality as verified by automated metrics and human raters. Our model can also\ngeneralize to scenarios where program traces are absent at test-time.", "published": "2024-04-23 01:46:32", "link": "http://arxiv.org/abs/2404.14662v1", "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Automated Multi-Language to English Machine Translation Using Generative\n  Pre-Trained Transformers", "abstract": "The task of accurate and efficient language translation is an extremely\nimportant information processing task. Machine learning enabled and automated\ntranslation that is accurate and fast is often a large topic of interest in the\nmachine learning and data science communities. In this study, we examine using\nlocal Generative Pretrained Transformer (GPT) models to perform automated zero\nshot black-box, sentence wise, multi-natural-language translation into English\ntext. We benchmark 16 different open-source GPT models, with no custom\nfine-tuning, from the Huggingface LLM repository for translating 50 different\nnon-English languages into English using translated TED Talk transcripts as the\nreference dataset. These GPT model inference calls are performed strictly\nlocally, on single A100 Nvidia GPUs. Benchmark metrics that are reported are\nlanguage translation accuracy, using BLEU, GLEU, METEOR, and chrF text overlap\nmeasures, and wall-clock time for each sentence translation. The best overall\nperforming GPT model for translating into English text for the BLEU metric is\nReMM-v2-L2-13B with a mean score across all tested languages of $0.152$, for\nthe GLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages\nof $0.256$, for the chrF metric is Llama2-chat-AYT-13B with a mean score across\nall tested languages of $0.448$, and for the METEOR metric is ReMM-v2-L2-13B\nwith a mean score across all tested languages of $0.438$.", "published": "2024-04-23 02:19:35", "link": "http://arxiv.org/abs/2404.14680v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pegasus-v1 Technical Report", "abstract": "This technical report introduces Pegasus-1, a multimodal language model\nspecialized in video content understanding and interaction through natural\nlanguage. Pegasus-1 is designed to address the unique challenges posed by video\ndata, such as interpreting spatiotemporal information, to offer nuanced video\ncontent comprehension across various lengths. This technical report overviews\nPegasus-1's architecture, training strategies, and its performance in\nbenchmarks on video conversation, zero-shot video question answering, and video\nsummarization. We also explore qualitative characteristics of Pegasus-1 ,\ndemonstrating its capabilities as well as its limitations, in order to provide\nreaders a balanced view of its current state and its future direction.", "published": "2024-04-23 02:32:57", "link": "http://arxiv.org/abs/2404.14687v1", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Retrieval Augmented Generation for Domain-specific Question Answering", "abstract": "Question answering (QA) has become an important application in the advanced\ndevelopment of large language models. General pre-trained large language models\nfor question-answering are not trained to properly understand the knowledge or\nterminology for a specific domain, such as finance, healthcare, education, and\ncustomer service for a product. To better cater to domain-specific\nunderstanding, we build an in-house question-answering system for Adobe\nproducts. We propose a novel framework to compile a large question-answer\ndatabase and develop the approach for retrieval-aware finetuning of a Large\nLanguage model. We showcase that fine-tuning the retriever leads to major\nimprovements in the final generation. Our overall approach reduces\nhallucinations during generation while keeping in context the latest retrieval\ninformation for contextual grounding.", "published": "2024-04-23 05:51:45", "link": "http://arxiv.org/abs/2404.14760v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Watch Out for Your Guidance on Generation! Exploring Conditional\n  Backdoor Attacks against Large Language Models", "abstract": "Mainstream backdoor attacks on large language models (LLMs) typically set a\nfixed trigger in the input instance and specific responses for triggered\nqueries. However, the fixed trigger setting (e.g., unusual words) may be easily\ndetected by human detection, limiting the effectiveness and practicality in\nreal-world scenarios. To enhance the stealthiness of backdoor activation, we\npresent a new poisoning paradigm against LLMs triggered by specifying\ngeneration conditions, which are commonly adopted strategies by users during\nmodel inference. The poisoned model performs normally for output under\nnormal/other generation conditions, while becomes harmful for output under\ntarget generation conditions. To achieve this objective, we introduce BrieFool,\nan efficient attack framework. It leverages the characteristics of generation\nconditions by efficient instruction sampling and poisoning data generation,\nthereby influencing the behavior of LLMs under target conditions. Our attack\ncan be generally divided into two types with different targets: Safety\nunalignment attack and Ability degradation attack. Our extensive experiments\ndemonstrate that BrieFool is effective across safety domains and ability\ndomains, achieving higher success rates than baseline methods, with 94.3 % on\nGPT-3.5-turbo", "published": "2024-04-23 07:19:20", "link": "http://arxiv.org/abs/2404.14795v5", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Large Language Models on Generative Graph Analytics: Query,\n  Learning, and Applications", "abstract": "A graph is a fundamental data model to represent various entities and their\ncomplex relationships in society and nature, such as social networks,\ntransportation networks, financial networks, and biomedical systems. Recently,\nlarge language models (LLMs) have showcased a strong generalization ability to\nhandle various NLP and multi-mode tasks to answer users' arbitrary questions\nand specific-domain content generation. Compared with graph learning models,\nLLMs enjoy superior advantages in addressing the challenges of generalizing\ngraph tasks by eliminating the need for training graph learning models and\nreducing the cost of manual annotation. In this survey, we conduct a\ncomprehensive investigation of existing LLM studies on graph data, which\nsummarizes the relevant graph analytics tasks solved by advanced LLM models and\npoints out the existing remaining challenges and future directions.\nSpecifically, we study the key problems of LLM-based generative graph analytics\n(LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP),\nLLM-based graph inference and learning (LLM-GIL), and graph-LLM-based\napplications. LLM-GQP focuses on an integration of graph analytics techniques\nand LLM prompts, including graph understanding and knowledge graph (KG) based\naugmented retrieval, while LLM-GIL focuses on learning and reasoning over\ngraphs, including graph learning, graph-formed reasoning and graph\nrepresentation. We summarize the useful prompts incorporated into LLM to handle\ndifferent graph downstream tasks. Moreover, we give a summary of LLM model\nevaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM\nmodels. We also explore open problems and future directions in this exciting\ninterdisciplinary research area of LLMs and graph analytics.", "published": "2024-04-23 07:39:24", "link": "http://arxiv.org/abs/2404.14809v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Towards Universal Dense Blocking for Entity Resolution", "abstract": "Blocking is a critical step in entity resolution, and the emergence of neural\nnetwork-based representation models has led to the development of dense\nblocking as a promising approach for exploring deep semantics in blocking.\nHowever, previous advanced self-supervised dense blocking approaches require\ndomain-specific training on the target domain, which limits the benefits and\nrapid adaptation of these methods. To address this issue, we propose\nUniBlocker, a dense blocker that is pre-trained on a domain-independent,\neasily-obtainable tabular corpus using self-supervised contrastive learning. By\nconducting domain-independent pre-training, UniBlocker can be adapted to\nvarious downstream blocking scenarios without requiring domain-specific\nfine-tuning. To evaluate the universality of our entity blocker, we also\nconstruct a new benchmark covering a wide range of blocking tasks from multiple\ndomains and scenarios. Our experiments show that the proposed UniBlocker,\nwithout any domain-specific learning, significantly outperforms previous self-\nand unsupervised dense blocking methods and is comparable and complementary to\nthe state-of-the-art sparse blocking methods.", "published": "2024-04-23 08:39:29", "link": "http://arxiv.org/abs/2404.14831v2", "categories": ["cs.DB", "cs.CL", "cs.IR"], "primary_category": "cs.DB"}
{"title": "Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein\n  Language Models", "abstract": "Fine-tuning Pre-trained protein language models (PLMs) has emerged as a\nprominent strategy for enhancing downstream prediction tasks, often\noutperforming traditional supervised learning approaches. As a widely applied\npowerful technique in natural language processing, employing\nParameter-Efficient Fine-Tuning techniques could potentially enhance the\nperformance of PLMs. However, the direct transfer to life science tasks is\nnon-trivial due to the different training strategies and data forms. To address\nthis gap, we introduce SES-Adapter, a simple, efficient, and scalable adapter\nmethod for enhancing the representation learning of PLMs. SES-Adapter\nincorporates PLM embeddings with structural sequence embeddings to create\nstructure-aware representations. We show that the proposed method is compatible\nwith different PLM architectures and across diverse tasks. Extensive\nevaluations are conducted on 2 types of folding structures with notable quality\ndifferences, 9 state-of-the-art baselines, and 9 benchmark datasets across\ndistinct downstream tasks. Results show that compared to vanilla PLMs,\nSES-Adapter improves downstream task performance by a maximum of 11% and an\naverage of 3%, with significantly accelerated training speed by a maximum of\n1034% and an average of 362%, the convergence rate is also improved by\napproximately 2 times. Moreover, positive optimization is observed even with\nlow-quality predicted structures. The source code for SES-Adapter is available\nat https://github.com/tyang816/SES-Adapter.", "published": "2024-04-23 09:05:09", "link": "http://arxiv.org/abs/2404.14850v1", "categories": ["cs.CL", "cs.LG", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "From Matching to Generation: A Survey on Generative Information\n  Retrieval", "abstract": "Information Retrieval (IR) systems are crucial tools for users to access\ninformation, which have long been dominated by traditional methods relying on\nsimilarity matching. With the advancement of pre-trained language models,\ngenerative information retrieval (GenIR) emerges as a novel paradigm,\nattracting increasing attention. Based on the form of information provided to\nusers, current research in GenIR can be categorized into two aspects:\n\\textbf{(1) Generative Document Retrieval} (GR) leverages the generative\nmodel's parameters for memorizing documents, enabling retrieval by directly\ngenerating relevant document identifiers without explicit indexing. \\textbf{(2)\nReliable Response Generation} employs language models to directly generate\ninformation users seek, breaking the limitations of traditional IR in terms of\ndocument granularity and relevance matching while offering flexibility,\nefficiency, and creativity to meet practical needs. This paper aims to\nsystematically review the latest research progress in GenIR. We will summarize\nthe advancements in GR regarding model training and structure, document\nidentifier, incremental learning, etc., as well as progress in reliable\nresponse generation in aspects of internal knowledge memorization, external\nknowledge augmentation, etc. We also review the evaluation, challenges and\nfuture developments in GenIR systems. This review aims to offer a comprehensive\nreference for researchers, encouraging further development in the GenIR field.\nGithub Repository: https://github.com/RUC-NLPIR/GenIR-Survey", "published": "2024-04-23 09:05:37", "link": "http://arxiv.org/abs/2404.14851v4", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Graph Machine Learning in the Era of Large Language Models (LLMs)", "abstract": "Graphs play an important role in representing complex relationships in\nvarious domains like social networks, knowledge graphs, and molecular\ndiscovery. With the advent of deep learning, Graph Neural Networks (GNNs) have\nemerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the\nrepresentation and processing of graph structures. Recently, LLMs have\ndemonstrated unprecedented capabilities in language tasks and are widely\nadopted in a variety of applications such as computer vision and recommender\nsystems. This remarkable success has also attracted interest in applying LLMs\nto the graph domain. Increasing efforts have been made to explore the potential\nof LLMs in advancing Graph ML's generalization, transferability, and few-shot\nlearning ability. Meanwhile, graphs, especially knowledge graphs, are rich in\nreliable factual knowledge, which can be utilized to enhance the reasoning\ncapabilities of LLMs and potentially alleviate their limitations such as\nhallucinations and the lack of explainability. Given the rapid progress of this\nresearch direction, a systematic review summarizing the latest advancements for\nGraph ML in the era of LLMs is necessary to provide an in-depth understanding\nto researchers and practitioners. Therefore, in this survey, we first review\nthe recent developments in Graph ML. We then explore how LLMs can be utilized\nto enhance the quality of graph features, alleviate the reliance on labeled\ndata, and address challenges such as graph heterogeneity and\nout-of-distribution (OOD) generalization. Afterward, we delve into how graphs\ncan enhance LLMs, highlighting their abilities to enhance LLM pre-training and\ninference. Furthermore, we investigate various applications and discuss the\npotential future directions in this promising field.", "published": "2024-04-23 11:13:39", "link": "http://arxiv.org/abs/2404.14928v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "StoryTTS: A Highly Expressive Text-to-Speech Dataset with Rich Textual\n  Expressiveness Annotations", "abstract": "While acoustic expressiveness has long been studied in expressive\ntext-to-speech (ETTS), the inherent expressiveness in text lacks sufficient\nattention, especially for ETTS of artistic works. In this paper, we introduce\nStoryTTS, a highly ETTS dataset that contains rich expressiveness both in\nacoustic and textual perspective, from the recording of a Mandarin storytelling\nshow. A systematic and comprehensive labeling framework is proposed for textual\nexpressiveness. We analyze and define speech-related textual expressiveness in\nStoryTTS to include five distinct dimensions through linguistics, rhetoric,\netc. Then we employ large language models and prompt them with a few manual\nannotation examples for batch annotation. The resulting corpus contains 61\nhours of consecutive and highly prosodic speech equipped with accurate text\ntranscriptions and rich textual expressiveness annotations. Therefore, StoryTTS\ncan aid future ETTS research to fully mine the abundant intrinsic textual and\nacoustic features. Experiments are conducted to validate that TTS models can\ngenerate speech with improved expressiveness when integrating with the\nannotated textual labels in StoryTTS.", "published": "2024-04-23 11:41:35", "link": "http://arxiv.org/abs/2404.14946v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Head Mixture-of-Experts", "abstract": "Sparse Mixtures of Experts (SMoE) scales model capacity without significant\nincreases in training and inference costs, but exhibits the following two\nissues: (1) Low expert activation, where only a small subset of experts are\nactivated for optimization. (2) Lacking fine-grained analytical capabilities\nfor multiple semantic concepts within individual tokens. We propose Multi-Head\nMixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each\ntoken into multiple sub-tokens. These sub-tokens are then assigned to and\nprocessed by a diverse set of experts in parallel, and seamlessly reintegrated\ninto the original token form. The multi-head mechanism enables the model to\ncollectively attend to information from various representation spaces within\ndifferent experts, while significantly enhances expert activation, thus deepens\ncontext understanding and alleviate overfitting. Moreover, our MH-MoE is\nstraightforward to implement and decouples from other SMoE optimization\nmethods, making it easy to integrate with other SMoE models for enhanced\nperformance. Extensive experimental results across three tasks: English-focused\nlanguage modeling, Multi-lingual language modeling and Masked multi-modality\nmodeling tasks, demonstrate the effectiveness of MH-MoE.", "published": "2024-04-23 13:47:09", "link": "http://arxiv.org/abs/2404.15045v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging\n  Upcycled Mixture-of-Experts", "abstract": "We introduce XFT, a simple yet powerful training scheme, by simply merging\nupcycled Mixture-of-Experts (MoE) to unleash the performance limit of\ninstruction-tuned code Large Language Models (LLMs). While vanilla sparse\nupcycling fails to improve instruction tuning, XFT introduces a shared expert\nmechanism with a novel routing weight normalization strategy into sparse\nupcycling, which significantly boosts instruction tuning. After fine-tuning the\nupcycled MoE model, XFT introduces a learnable model merging mechanism to\ncompile the upcycled MoE model back to a dense model, achieving upcycled\nMoE-level performance with only dense-model compute. By applying XFT to a 1.3B\nmodel, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6\npass@1 on HumanEval and HumanEval+ respectively. With the same data and model\narchitecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+,\nalong with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and\nDS-1000, demonstrating its generalizability. XFT is fully orthogonal to\nexisting techniques such as Evol-Instruct and OSS-Instruct, opening a new\ndimension for improving code instruction tuning. Codes are available at\nhttps://github.com/ise-uiuc/xft.", "published": "2024-04-23 17:32:24", "link": "http://arxiv.org/abs/2404.15247v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Aligning LLM Agents by Learning Latent Preference from User Edits", "abstract": "We study interactive learning of LLM-based language agents based on user\nedits made to the agent's output. In a typical setting such as writing\nassistants, the user interacts with a language agent to generate a response\ngiven a context, and may optionally edit the agent response to personalize it\nbased on their latent preference, in addition to improving the correctness. The\nedit feedback is naturally generated, making it a suitable candidate for\nimproving the agent's alignment with the user's preference, and for reducing\nthe cost of user edits over time. We propose a learning framework, PRELUDE that\ninfers a description of the user's latent preference based on historic edit\ndata. The inferred user preference descriptions are used to define prompts for\ngenerating responses in the future. This avoids fine-tuning the agent, which is\ncostly, challenging to scale with the number of users, and may even degrade its\nperformance on other tasks. Furthermore, learning descriptive preference\nimproves interpretability, allowing the user to view and modify the learned\npreference. However, user preference can be complex, subtle, and vary based on\ncontext, making it challenging to learn. To address this, we propose a simple\nyet effective algorithm named CIPHER that leverages the LLM to infer the user\npreference for a given context based on user edits. In the future, CIPHER\nretrieves inferred preferences from the k-closest contexts in the history, and\nforms an aggregate preference for response generation. We introduce two\ninteractive environments -- summarization and email writing, and use a GPT-4\nsimulated user for evaluation. On both tasks, CIPHER outperforms several\nbaselines by achieving the lowest edit distance cost while only having a small\noverhead in LLM query cost. Our analysis reports that user preferences learned\nby CIPHER show significant similarity to the ground truth latent preferences.", "published": "2024-04-23 17:57:47", "link": "http://arxiv.org/abs/2404.15269v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Layout Planning for Visually-Rich Documents with\n  Instruction-Following Models", "abstract": "Recent advancements in instruction-following models have made user\ninteractions with models more user-friendly and efficient, broadening their\napplicability. In graphic design, non-professional users often struggle to\ncreate visually appealing layouts due to limited skills and resources. In this\nwork, we introduce a novel multimodal instruction-following framework for\nlayout planning, allowing users to easily arrange visual elements into tailored\nlayouts by specifying canvas size and design purpose, such as for book covers,\nposters, brochures, or menus. We developed three layout reasoning tasks to\ntrain the model in understanding and executing layout instructions. Experiments\non two benchmarks show that our method not only simplifies the design process\nfor non-professionals but also surpasses the performance of few-shot GPT-4V\nmodels, with mIoU higher by 12% on Crello. This progress highlights the\npotential of multimodal instruction-following models to automate and simplify\nthe design process, providing an approachable solution for a wide range of\ndesign tasks on visually-rich documents.", "published": "2024-04-23 17:58:33", "link": "http://arxiv.org/abs/2404.15271v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and\n  Radiology Reports for Full-Body Scenarios", "abstract": "Medical Vision-Language Pretraining (Med-VLP) establishes a connection\nbetween visual content from medical images and the relevant textual\ndescriptions. Existing Med-VLP methods primarily focus on 2D images depicting a\nsingle body part, notably chest X-rays. In this paper, we extend the scope of\nMed-VLP to encompass 3D images, specifically targeting full-body scenarios, by\nusing a multimodal dataset of CT images and reports. Compared with the 2D\ncounterpart, 3D VLP is required to effectively capture essential semantics from\nsignificantly sparser representation in 3D imaging. In this paper, we introduce\nCT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method\nthat constructs organ-level image-text pairs to enhance multimodal contrastive\nlearning, aligning grounded visual features with precise diagnostic text.\nAdditionally, we developed an abnormality dictionary to augment contrastive\nlearning with diverse contrastive pairs. Our method, trained on a multimodal CT\ndataset comprising 44,011 organ-level vision-text pairs from 17,702 patients\nacross 104 organs, demonstrates it can identify organs and abnormalities in a\nzero-shot manner using natural languages. The performance of CT-GLIP is\nvalidated on a separate test set of 1,130 patients, focusing on the 16 most\nfrequent abnormalities across 7 organs. The experimental results show our\nmodel's superior performance over the standard CLIP framework across zero-shot\nand fine-tuning scenarios, using both CNN and ViT architectures.", "published": "2024-04-23 17:59:01", "link": "http://arxiv.org/abs/2404.15272v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal\n  LLMs", "abstract": "Multimodal LLMs are the natural evolution of LLMs, and enlarge their\ncapabilities so as to work beyond the pure textual modality. As research is\nbeing carried out to design novel architectures and vision-and-language\nadapters, in this paper we concentrate on endowing such models with the\ncapability of answering questions that require external knowledge. Our\napproach, termed Wiki-LLaVA, aims at integrating an external knowledge source\nof multimodal documents, which is accessed through a hierarchical retrieval\npipeline. Relevant passages, using this approach, are retrieved from the\nexternal knowledge source and employed as additional context for the LLM,\naugmenting the effectiveness and precision of generated dialogues. We conduct\nextensive experiments on datasets tailored for visual question answering with\nexternal data and demonstrate the appropriateness of our approach.", "published": "2024-04-23 18:00:09", "link": "http://arxiv.org/abs/2404.15406v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection &\n  Correction Task On the Shoulders of Medical Agents", "abstract": "In natural language processing applied to the clinical domain, utilizing\nlarge language models has emerged as a promising avenue for error detection and\ncorrection on clinical notes, a knowledge-intensive task for which annotated\ndata is scarce. This paper presents MedReAct'N'MedReFlex, which leverages a\nsuite of four LLM-based medical agents. The MedReAct agent initiates the\nprocess by observing, analyzing, and taking action, generating trajectories to\nguide the search to target a potential error in the clinical notes.\nSubsequently, the MedEval agent employs five evaluators to assess the targeted\nerror and the proposed correction. In cases where MedReAct's actions prove\ninsufficient, the MedReFlex agent intervenes, engaging in reflective analysis\nand proposing alternative strategies. Finally, the MedFinalParser agent formats\nthe final output, preserving the original style while ensuring the integrity of\nthe error correction process. One core component of our method is our RAG\npipeline based on our ClinicalCorp corpora. Among other well-known sources\ncontaining clinical guidelines and information, we preprocess and release the\nopen-source MedWiki dataset for clinical RAG application. Our results\ndemonstrate the central role of our RAG approach with ClinicalCorp leveraged\nthrough the MedReAct'N'MedReFlex framework. It achieved the ninth rank on the\nMEDIQA-CORR 2024 final leaderboard.", "published": "2024-04-23 20:00:37", "link": "http://arxiv.org/abs/2404.15488v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "GeoLLM-Engine: A Realistic Environment for Building Geospatial Copilots", "abstract": "Geospatial Copilots unlock unprecedented potential for performing Earth\nObservation (EO) applications through natural language instructions. However,\nexisting agents rely on overly simplified single tasks and template-based\nprompts, creating a disconnect with real-world scenarios. In this work, we\npresent GeoLLM-Engine, an environment for tool-augmented agents with intricate\ntasks routinely executed by analysts on remote sensing platforms. We enrich our\nenvironment with geospatial API tools, dynamic maps/UIs, and external\nmultimodal knowledge bases to properly gauge an agent's proficiency in\ninterpreting realistic high-level natural language commands and its functional\ncorrectness in task completions. By alleviating overheads typically associated\nwith human-in-the-loop benchmark curation, we harness our massively parallel\nengine across 100 GPT-4-Turbo nodes, scaling to over half a million diverse\nmulti-tool tasks and across 1.1 million satellite images. By moving beyond\ntraditional single-task image-caption paradigms, we investigate\nstate-of-the-art agents and prompting techniques against long-horizon prompts.", "published": "2024-04-23 20:23:37", "link": "http://arxiv.org/abs/2404.15500v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "DreamCraft: Text-Guided Generation of Functional 3D Environments in\n  Minecraft", "abstract": "Procedural Content Generation (PCG) algorithms enable the automatic\ngeneration of complex and diverse artifacts. However, they don't provide\nhigh-level control over the generated content and typically require domain\nexpertise. In contrast, text-to-3D methods allow users to specify desired\ncharacteristics in natural language, offering a high amount of flexibility and\nexpressivity. But unlike PCG, such approaches cannot guarantee functionality,\nwhich is crucial for certain applications like game design. In this paper, we\npresent a method for generating functional 3D artifacts from free-form text\nprompts in the open-world game Minecraft. Our method, DreamCraft, trains\nquantized Neural Radiance Fields (NeRFs) to represent artifacts that, when\nviewed in-game, match given text descriptions. We find that DreamCraft produces\nmore aligned in-game artifacts than a baseline that post-processes the output\nof an unconstrained NeRF. Thanks to the quantized representation of the\nenvironment, functional constraints can be integrated using specialized loss\nterms. We show how this can be leveraged to generate 3D structures that match a\ntarget distribution or obey certain adjacency rules over the block types.\nDreamCraft inherits a high degree of expressivity and controllability from the\nNeRF, while still being able to incorporate functional constraints through\ndomain-specific objectives.", "published": "2024-04-23 21:57:14", "link": "http://arxiv.org/abs/2404.15538v1", "categories": ["cs.GR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.GR"}
{"title": "Atomas: Hierarchical Alignment on Molecule-Text for Unified Molecule\n  Understanding and Generation", "abstract": "Molecule-and-text cross-modal representation learning has emerged as a\npromising direction for enhancing the quality of molecular representation,\nthereby improving performance in various scientific fields. However, most\napproaches employ a global alignment approach to learn the knowledge from\ndifferent modalities that may fail to capture fine-grained information, such as\nmolecule-and-text fragments and stereoisomeric nuances, which is crucial for\ndownstream tasks. Furthermore, it is incapable of modeling such information\nusing a similar global alignment strategy due to the lack of annotations about\nthe fine-grained fragments in the existing dataset. In this paper, we propose\nAtomas, a hierarchical molecular representation learning framework that jointly\nlearns representations from SMILES strings and text. We design a Hierarchical\nAdaptive Alignment model to automatically learn the fine-grained fragment\ncorrespondence between two modalities and align these representations at three\nsemantic levels. Atomas's end-to-end training framework supports understanding\nand generating molecules, enabling a wider range of downstream tasks. Atomas\nachieves superior performance across 12 tasks on 11 datasets, outperforming 11\nbaseline models thus highlighting the effectiveness and versatility of our\nmethod. Scaling experiments further demonstrate Atomas's robustness and\nscalability. Moreover, visualization and qualitative analysis, validated by\nhuman experts, confirm the chemical relevance of our approach. Codes are\nreleased on https://github.com/yikunpku/Atomas.", "published": "2024-04-23 12:35:44", "link": "http://arxiv.org/abs/2404.16880v3", "categories": ["q-bio.QM", "cs.AI", "cs.CL"], "primary_category": "q-bio.QM"}
{"title": "From Complexity to Clarity: How AI Enhances Perceptions of Scientists\n  and the Public's Understanding of Science", "abstract": "This paper evaluated the effectiveness of using generative AI to simplify\nscience communication and enhance the public's understanding of science. By\ncomparing lay summaries of journal articles from PNAS, yoked to those generated\nby AI, this work first assessed linguistic simplicity differences across such\nsummaries and public perceptions in follow-up experiments. Specifically, Study\n1a analyzed simplicity features of PNAS abstracts (scientific summaries) and\nsignificance statements (lay summaries), observing that lay summaries were\nindeed linguistically simpler, but effect size differences were small. Study 1b\nused a large language model, GPT-4, to create significance statements based on\npaper abstracts and this more than doubled the average effect size without\nfine-tuning. Study 2 experimentally demonstrated that simply-written GPT\nsummaries facilitated more favorable perceptions of scientists (they were\nperceived as more credible and trustworthy, but less intelligent) than more\ncomplexly-written human PNAS summaries. Crucially, Study 3 experimentally\ndemonstrated that participants comprehended scientific writing better after\nreading simple GPT summaries compared to complex PNAS summaries. In their own\nwords, participants also summarized scientific papers in a more detailed and\nconcrete manner after reading GPT summaries compared to PNAS summaries of the\nsame article. AI has the potential to engage scientific communities and the\npublic via a simple language heuristic, advocating for its integration into\nscientific dissemination for a more informed society.", "published": "2024-04-23 14:43:35", "link": "http://arxiv.org/abs/2405.00706v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Interactive Analysis of LLMs using Meaningful Counterfactuals", "abstract": "Counterfactual examples are useful for exploring the decision boundaries of\nmachine learning models and determining feature attributions. How can we apply\ncounterfactual-based methods to analyze and explain LLMs? We identify the\nfollowing key challenges. First, the generated textual counterfactuals should\nbe meaningful and readable to users and thus can be mentally compared to draw\nconclusions. Second, to make the solution scalable to long-form text, users\nshould be equipped with tools to create batches of counterfactuals from\nperturbations at various granularity levels and interactively analyze the\nresults. In this paper, we tackle the above challenges and contribute 1) a\nnovel algorithm for generating batches of complete and meaningful textual\ncounterfactuals by removing and replacing text segments in different\ngranularities, and 2) LLM Analyzer, an interactive visualization tool to help\nusers understand an LLM's behaviors by interactively inspecting and aggregating\nmeaningful counterfactuals. We evaluate the proposed algorithm by the\ngrammatical correctness of its generated counterfactuals using 1,000 samples\nfrom medical, legal, finance, education, and news datasets. In our experiments,\n97.2% of the counterfactuals are grammatically correct. Through a use case,\nuser studies, and feedback from experts, we demonstrate the usefulness and\nusability of the proposed interactive visualization tool.", "published": "2024-04-23 19:57:03", "link": "http://arxiv.org/abs/2405.00708v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "I.2.7; H.5.2"], "primary_category": "cs.CL"}
{"title": "Evaluating Tool-Augmented Agents in Remote Sensing Platforms", "abstract": "Tool-augmented Large Language Models (LLMs) have shown impressive\ncapabilities in remote sensing (RS) applications. However, existing benchmarks\nassume question-answering input templates over predefined image-text data\npairs. These standalone instructions neglect the intricacies of realistic\nuser-grounded tasks. Consider a geospatial analyst: they zoom in a map area,\nthey draw a region over which to collect satellite imagery, and they succinctly\nask \"Detect all objects here\". Where is `here`, if it is not explicitly\nhardcoded in the image-text template, but instead is implied by the system\nstate, e.g., the live map positioning? To bridge this gap, we present\nGeoLLM-QA, a benchmark designed to capture long sequences of verbal, visual,\nand click-based actions on a real UI platform. Through in-depth evaluation of\nstate-of-the-art LLMs over a diverse set of 1,000 tasks, we offer insights\ntowards stronger agents for RS applications.", "published": "2024-04-23 20:37:24", "link": "http://arxiv.org/abs/2405.00709v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Software Mention Recognition with a Three-Stage Framework Based on\n  BERTology Models at SOMD 2024", "abstract": "This paper describes our systems for the sub-task I in the Software Mention\nDetection in Scholarly Publications shared-task. We propose three approaches\nleveraging different pre-trained language models (BERT, SciBERT, and XLM-R) to\ntackle this challenge. Our bestperforming system addresses the named entity\nrecognition (NER) problem through a three-stage framework. (1) Entity Sentence\nClassification - classifies sentences containing potential software mentions;\n(2) Entity Extraction - detects mentions within classified sentences; (3)\nEntity Type Classification - categorizes detected mentions into specific\nsoftware types. Experiments on the official dataset demonstrate that our\nthree-stage framework achieves competitive performance, surpassing both other\nparticipating teams and our alternative approaches. As a result, our framework\nbased on the XLM-R-based model achieves a weighted F1-score of 67.80%,\ndelivering our team the 3rd rank in Sub-task I for the Software Mention\nRecognition task.", "published": "2024-04-23 17:06:24", "link": "http://arxiv.org/abs/2405.01575v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "FlashSpeech: Efficient Zero-Shot Speech Synthesis", "abstract": "Recent progress in large-scale zero-shot speech synthesis has been\nsignificantly advanced by language models and diffusion models. However, the\ngeneration process of both methods is slow and computationally intensive.\nEfficient speech synthesis using a lower computing budget to achieve quality on\npar with previous work remains a significant challenge. In this paper, we\npresent FlashSpeech, a large-scale zero-shot speech synthesis system with\napproximately 5\\% of the inference time compared with previous work.\nFlashSpeech is built on the latent consistency model and applies a novel\nadversarial consistency training approach that can train from scratch without\nthe need for a pre-trained diffusion model as the teacher. Furthermore, a new\nprosody generator module enhances the diversity of prosody, making the rhythm\nof the speech sound more natural. The generation processes of FlashSpeech can\nbe achieved efficiently with one or two sampling steps while maintaining high\naudio quality and high similarity to the audio prompt for zero-shot speech\ngeneration. Our experimental results demonstrate the superior performance of\nFlashSpeech. Notably, FlashSpeech can be about 20 times faster than other\nzero-shot speech synthesis systems while maintaining comparable performance in\nterms of voice quality and similarity. Furthermore, FlashSpeech demonstrates\nits versatility by efficiently performing tasks like voice conversion, speech\nediting, and diverse speech sampling. Audio samples can be found in\nhttps://flashspeech.github.io/.", "published": "2024-04-23 02:57:46", "link": "http://arxiv.org/abs/2404.14700v4", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bayesian Example Selection Improves In-Context Learning for Speech,\n  Text, and Visual Modalities", "abstract": "Large language models (LLMs) can adapt to new tasks through in-context\nlearning (ICL) based on a few examples presented in dialogue history without\nany model parameter update. Despite such convenience, the performance of ICL\nheavily depends on the quality of the in-context examples presented, which\nmakes the in-context example selection approach a critical choice. This paper\nproposes a novel Bayesian in-Context example Selection method (ByCS) for ICL.\nExtending the inference probability conditioned on in-context examples based on\nBayes' theorem, ByCS focuses on the inverse inference conditioned on test\ninput. Following the assumption that accurate inverse inference probability\n(likelihood) will result in accurate inference probability (posterior),\nin-context examples are selected based on their inverse inference results.\nDiverse and extensive cross-tasking and cross-modality experiments are\nperformed with speech, text, and image examples. Experimental results show the\nefficacy and robustness of our ByCS method on various models, tasks and\nmodalities.", "published": "2024-04-23 03:42:48", "link": "http://arxiv.org/abs/2404.14716v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Beyond Code Generation: An Observational Study of ChatGPT Usage in\n  Software Engineering Practice", "abstract": "Large Language Models (LLMs) are frequently discussed in academia and the\ngeneral public as support tools for virtually any use case that relies on the\nproduction of text, including software engineering. Currently there is much\ndebate, but little empirical evidence, regarding the practical usefulness of\nLLM-based tools such as ChatGPT for engineers in industry. We conduct an\nobservational study of 24 professional software engineers who have been using\nChatGPT over a period of one week in their jobs, and qualitatively analyse\ntheir dialogues with the chatbot as well as their overall experience (as\ncaptured by an exit survey). We find that, rather than expecting ChatGPT to\ngenerate ready-to-use software artifacts (e.g., code), practitioners more often\nuse ChatGPT to receive guidance on how to solve their tasks or learn about a\ntopic in more abstract terms. We also propose a theoretical framework for how\n(i) purpose of the interaction, (ii) internal factors (e.g., the user's\npersonality), and (iii) external factors (e.g., company policy) together shape\nthe experience (in terms of perceived usefulness and trust). We envision that\nour framework can be used by future research to further the academic discussion\non LLM usage by software engineering practitioners, and to serve as a reference\npoint for the design of future empirical LLM research in this domain.", "published": "2024-04-23 10:34:16", "link": "http://arxiv.org/abs/2404.14901v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Transformers Can Represent $n$-gram Language Models", "abstract": "Existing work has analyzed the representational capacity of the transformer\narchitecture by means of formal models of computation. However, the focus so\nfar has been on analyzing the architecture in terms of language\n\\emph{acceptance}. We contend that this is an ill-suited problem in the study\nof \\emph{language models} (LMs), which are definitionally \\emph{probability\ndistributions} over strings. In this paper, we focus on the relationship\nbetween transformer LMs and $n$-gram LMs, a simple and historically relevant\nclass of language models. We show that transformer LMs using the hard or sparse\nattention mechanisms can exactly represent any $n$-gram LM, giving us a\nconcrete lower bound on their probabilistic representational capacity. This\nprovides a first step towards understanding the mechanisms that transformer LMs\ncan use to represent probability distributions over strings.", "published": "2024-04-23 12:51:37", "link": "http://arxiv.org/abs/2404.14994v3", "categories": ["cs.CL", "cs.AI", "cs.CC", "cs.FL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to\n  Complement Historical Analysis", "abstract": "This paper presents BattleAgent, an emulation system that combines the Large\nVision-Language Model and Multi-agent System. This novel system aims to\nsimulate complex dynamic interactions among multiple agents, as well as between\nagents and their environments, over a period of time. It emulates both the\ndecision-making processes of leaders and the viewpoints of ordinary\nparticipants, such as soldiers. The emulation showcases the current\ncapabilities of agents, featuring fine-grained multi-modal interactions between\nagents and landscapes. It develops customizable agent structures to meet\nspecific situational requirements, for example, a variety of battle-related\nactivities like scouting and trench digging. These components collaborate to\nrecreate historical events in a lively and comprehensive manner while offering\ninsights into the thoughts and feelings of individuals from diverse viewpoints.\nThe technological foundations of BattleAgent establish detailed and immersive\nsettings for historical battles, enabling individual agents to partake in,\nobserve, and dynamically respond to evolving battle scenarios. This methodology\nholds the potential to substantially deepen our understanding of historical\nevents, particularly through individual accounts. Such initiatives can also aid\nhistorical research, as conventional historical narratives often lack\ndocumentation and prioritize the perspectives of decision-makers, thereby\noverlooking the experiences of ordinary individuals. BattelAgent illustrates\nAI's potential to revitalize the human aspect in crucial social events, thereby\nfostering a more nuanced collective understanding and driving the progressive\ndevelopment of human society.", "published": "2024-04-23 21:37:22", "link": "http://arxiv.org/abs/2404.15532v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "primary_category": "cs.HC"}
{"title": "Evaluating LLMs for Hardware Design and Test", "abstract": "Large Language Models (LLMs) have demonstrated capabilities for producing\ncode in Hardware Description Languages (HDLs). However, most of the focus\nremains on their abilities to write functional code, not test code. The\nhardware design process consists of both design and test, and so eschewing\nvalidation and verification leaves considerable potential benefit unexplored,\ngiven that a design and test framework may allow for progress towards full\nautomation of the digital design pipeline. In this work, we perform one of the\nfirst studies exploring how a LLM can both design and test hardware modules\nfrom provided specifications. Using a suite of 8 representative benchmarks, we\nexamined the capabilities and limitations of the state-of-the-art\nconversational LLMs when producing Verilog for functional and verification\npurposes. We taped out the benchmarks on a Skywater 130nm shuttle and received\nthe functional chip.", "published": "2024-04-23 18:55:49", "link": "http://arxiv.org/abs/2405.02326v2", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.AR"}
{"title": "Qualitative Approaches to Voice UX", "abstract": "Voice is a natural mode of expression offered by modern computer-based\nsystems. Qualitative perspectives on voice-based user experiences (voice UX)\noffer rich descriptions of complex interactions that numbers alone cannot fully\nrepresent. We conducted a systematic review of the literature on qualitative\napproaches to voice UX, capturing the nature of this body of work in a\nsystematic map and offering a qualitative synthesis of findings. We highlight\nthe benefits of qualitative methods for voice UX research, identify\nopportunities for increasing rigour in methods and outcomes, and distill\npatterns of experience across a diversity of devices and modes of qualitative\npraxis.", "published": "2024-04-23 04:33:49", "link": "http://arxiv.org/abs/2404.14736v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Breaking Walls: Pioneering Automatic Speech Recognition for Central\n  Kurdish: End-to-End Transformer Paradigm", "abstract": "End-to-end transformer-based models epitomize the cutting-edge in Automatic\nSpeech Recognition (ASR) systems. Despite their substantial benefits, these\nmodels demand extensive training data to perform optimally, presenting a\nsignificant challenge for low-resource languages such as Central Kurdish.\nAddressing this issue requires innovative methods and techniques. This paper\naims to develop an ASR system for Intermediate Kurdish by collecting a robust\ncorpus of speech, using the N-GRAM language model, and utilizing an external\nKurdish tokenizer for refinement and integration techniques to enhance the\nmodel's performance. We collect a comprehensive 100-hour speech corpus from\ndiverse sources. Additionally, applied fine-tuning techniques to our speech\ncorpus on Persian, English, and Arabic pre-trained models, specifically\nutilizing the xls-r-300m, xls-r-1b, and xls-r-2b Wav2vec 2.0 models. And\nutilized language models trained by 3-gram and 4-gram from a large text corpus\nof 300 million tokens. The fine-tuned xls-r-2b model, combined with a 3-gram\nlanguage model and included external Kurdish tokenizer, achieved the best\nperformance, yielding a Word Error Rate (WER) of 10.0% on the validation set\nand 11.8% on the Asosoft test set. The ASR model has demonstrated the\nadvantages of having a large vocabulary compared to the existing Kurdish ASR\nmodels. Compared to other models, it produced more accurate and higher\nperformance outcomes by working with a lower error rate.", "published": "2024-04-23 10:47:56", "link": "http://arxiv.org/abs/2406.02561v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Rethinking Processing Distortions: Disentangling the Impact of Speech\n  Enhancement Errors on Speech Recognition Performance", "abstract": "It is challenging to improve automatic speech recognition (ASR) performance\nin noisy conditions with a single-channel speech enhancement (SE) front-end.\nThis is generally attributed to the processing distortions caused by the\nnonlinear processing of single-channel SE front-ends. However, the causes of\nsuch degraded ASR performance have not been fully investigated. How to design\nsingle-channel SE front-ends in a way that significantly improves ASR\nperformance remains an open research question. In this study, we investigate a\nsignal-level numerical metric that can explain the cause of degradation in ASR\nperformance. To this end, we propose a novel analysis scheme based on the\northogonal projection-based decomposition of SE errors. This scheme manually\nmodifies the ratio of the decomposed interference, noise, and artifact errors,\nand it enables us to directly evaluate the impact of each error type on ASR\nperformance. Our analysis reveals the particularly detrimental effect of\nartifact errors on ASR performance compared to the other types of errors. This\nprovides us with a more principled definition of processing distortions that\ncause the ASR performance degradation. Then, we study two practical approaches\nfor reducing the impact of artifact errors. First, we prove that the simple\nobservation adding (OA) post-processing (i.e., interpolating the enhanced and\nobserved signals) can monotonically improve the signal-to-artifact ratio.\nSecond, we propose a novel training objective, called artifact-boosted\nsignal-to-distortion ratio (AB-SDR), which forces the model to estimate the\nenhanced signals with fewer artifact errors. Through experiments, we confirm\nthat both the OA and AB-SDR approaches are effective in decreasing artifact\nerrors caused by single-channel SE front-ends, allowing them to significantly\nimprove ASR performance.", "published": "2024-04-23 09:30:23", "link": "http://arxiv.org/abs/2404.14860v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Sample Dynamic Time Warping for Few-Shot Keyword Spotting", "abstract": "In multi-sample keyword spotting, each keyword class is represented by\nmultiple spoken instances, called samples. A na\\\"ive approach to detect\nkeywords in a target sequence consists of querying all samples of all classes\nusing sub-sequence dynamic time warping. However, the resulting processing time\nincreases linearly with respect to the number of samples belonging to each\nclass. Alternatively, only a single Fr\\'echet mean can be queried for each\nclass, resulting in reduced processing time but usually also in worse detection\nperformance as the variability of the query samples is not captured\nsufficiently well. In this work, multi-sample dynamic time warping is proposed\nto compute class-specific cost-tensors that include the variability of all\nquery samples. To significantly reduce the computational complexity during\ninference, these cost tensors are converted to cost matrices before applying\ndynamic time warping. In experimental evaluations for few-shot keyword\nspotting, it is shown that this method yields a very similar performance as\nusing all individual query samples as templates while having a runtime that is\nonly slightly slower than when using Fr\\'echet means.", "published": "2024-04-23 10:36:23", "link": "http://arxiv.org/abs/2404.14903v2", "categories": ["eess.AS", "cs.IR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Additive Margin in Contrastive Self-Supervised Frameworks to Learn\n  Discriminative Speaker Representations", "abstract": "Self-Supervised Learning (SSL) frameworks became the standard for learning\nrobust class representations by benefiting from large unlabeled datasets. For\nSpeaker Verification (SV), most SSL systems rely on contrastive-based loss\nfunctions. We explore different ways to improve the performance of these\ntechniques by revisiting the NT-Xent contrastive loss. Our main contribution is\nthe definition of the NT-Xent-AM loss and the study of the importance of\nAdditive Margin (AM) in SimCLR and MoCo SSL methods to further separate\npositive from negative pairs. Despite class collisions, we show that AM\nenhances the compactness of same-speaker embeddings and reduces the number of\nfalse negatives and false positives on SV. Additionally, we demonstrate the\neffectiveness of the symmetric contrastive loss, which provides more\nsupervision for the SSL task. Implementing these two modifications to SimCLR\nimproves performance and results in 7.85% EER on VoxCeleb1-O, outperforming\nother equivalent methods.", "published": "2024-04-23 10:56:58", "link": "http://arxiv.org/abs/2404.14913v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Every Breath You Don't Take: Deepfake Speech Detection Using Breath", "abstract": "Deepfake speech represents a real and growing threat to systems and society.\nMany detectors have been created to aid in defense against speech deepfakes.\nWhile these detectors implement myriad methodologies, many rely on low-level\nfragments of the speech generation process. We hypothesize that breath, a\nhigher-level part of speech, is a key component of natural speech and thus\nimproper generation in deepfake speech is a performant discriminator. To\nevaluate this, we create a breath detector and leverage this against a custom\ndataset of online news article audio to discriminate between real/deepfake\nspeech. Additionally, we make this custom dataset publicly available to\nfacilitate comparison for future work. Applying our simple breath detector as a\ndeepfake speech discriminator on in-the-wild samples allows for accurate\nclassification (perfect 1.0 AUPRC and 0.0 EER on test data) across 33.6 hours\nof audio. We compare our model with the state-of-the-art SSL-wav2vec model and\nshow that this complex deep learning model completely fails to classify the\nsame in-the-wild samples (0.72 AUPRC and 0.99 EER).", "published": "2024-04-23 15:48:51", "link": "http://arxiv.org/abs/2404.15143v2", "categories": ["cs.SD", "cs.CR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Passing : a Non-Binary Voice Gender Prediction System for\n  evaluating Transgender voice transition", "abstract": "This paper presents a software allowing to describe voices using a continuous\nVoice Femininity Percentage (VFP). This system is intended for transgender\nspeakers during their voice transition and for voice therapists supporting them\nin this process. A corpus of 41 French cis- and transgender speakers was\nrecorded. A perceptual evaluation allowed 57 participants to estimate the VFP\nfor each voice. Binary gender classification models were trained on external\ngender-balanced data and used on overlapping windows to obtain average gender\nprediction estimates, which were calibrated to predict VFP and obtained higher\naccuracy than $F_0$ or vocal track length-based models. Training data speaking\nstyle and DNN architecture were shown to impact VFP estimation. Accuracy of the\nmodels was affected by speakers' age. This highlights the importance of style,\nage, and the conception of gender as binary or not, to build adequate\nstatistical representations of cultural concepts.", "published": "2024-04-23 16:15:39", "link": "http://arxiv.org/abs/2404.15176v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
