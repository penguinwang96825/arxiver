{"title": "Improving Entity Linking by Modeling Latent Entity Type Information", "abstract": "Existing state of the art neural entity linking models employ attention-based\nbag-of-words context model and pre-trained entity embeddings bootstrapped from\nword embeddings to assess topic level context compatibility. However, the\nlatent entity type information in the immediate context of the mention is\nneglected, which causes the models often link mentions to incorrect entities\nwith incorrect type. To tackle this problem, we propose to inject latent entity\ntype information into the entity embeddings based on pre-trained BERT. In\naddition, we integrate a BERT-based entity similarity score into the local\ncontext model of a state-of-the-art model to better capture latent entity type\ninformation. Our model significantly outperforms the state-of-the-art entity\nlinking models on standard benchmark (AIDA-CoNLL). Detailed experiment analysis\ndemonstrates that our model corrects most of the type errors produced by the\ndirect baseline.", "published": "2020-01-06 09:18:29", "link": "http://arxiv.org/abs/2001.01447v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stance Detection Benchmark: How Robust Is Your Stance Detection?", "abstract": "Stance Detection (StD) aims to detect an author's stance towards a certain\ntopic or claim and has become a key component in applications like fake news\ndetection, claim validation, and argument search. However, while stance is\neasily detected by humans, machine learning models are clearly falling short of\nthis task. Given the major differences in dataset sizes and framing of StD\n(e.g. number of classes and inputs), we introduce a StD benchmark that learns\nfrom ten StD datasets of various domains in a multi-dataset learning (MDL)\nsetting, as well as from related tasks via transfer learning. Within this\nbenchmark setup, we are able to present new state-of-the-art results on five of\nthe datasets. Yet, the models still perform well below human capabilities and\neven simple adversarial attacks severely hurt the performance of MDL models.\nDeeper investigation into this phenomenon suggests the existence of biases\ninherited from multiple datasets by design. Our analysis emphasizes the need of\nfocus on robustness and de-biasing strategies in multi-task learning\napproaches. The benchmark dataset and code is made available.", "published": "2020-01-06 13:37:51", "link": "http://arxiv.org/abs/2001.01565v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Machine Reading Comprehension Systems", "abstract": "Machine reading comprehension is a challenging task and hot topic in natural\nlanguage processing. Its goal is to develop systems to answer the questions\nregarding a given context. In this paper, we present a comprehensive survey on\ndifferent aspects of machine reading comprehension systems, including their\napproaches, structures, input/outputs, and research novelties. We illustrate\nthe recent trends in this field based on 241 reviewed papers from 2016 to 2020.\nOur investigations demonstrate that the focus of research has changed in recent\nyears from answer extraction to answer generation, from single to\nmulti-document reading comprehension, and from learning from scratch to using\npre-trained embeddings. We also discuss the popular datasets and the evaluation\nmetrics in this field. The paper ends with investigating the most cited papers\nand their contributions.", "published": "2020-01-06 13:54:06", "link": "http://arxiv.org/abs/2001.01582v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Benefits of Transfer Learning in Neural Machine Translation", "abstract": "Neural machine translation is known to require large numbers of parallel\ntraining sentences, which generally prevent it from excelling on low-resource\nlanguage pairs. This thesis explores the use of cross-lingual transfer learning\non neural networks as a way of solving the problem with the lack of resources.\nWe propose several transfer learning approaches to reuse a model pretrained on\na high-resource language pair. We pay particular attention to the simplicity of\nthe techniques. We study two scenarios: (a) when we reuse the high-resource\nmodel without any prior modifications to its training process and (b) when we\ncan prepare the first-stage high-resource model for transfer learning in\nadvance. For the former scenario, we present a proof-of-concept method by\nreusing a model trained by other researchers. In the latter scenario, we\npresent a method which reaches even larger improvements in translation\nperformance. Apart from proposed techniques, we focus on an in-depth analysis\nof transfer learning techniques and try to shed some light on transfer learning\nimprovements. We show how our techniques address specific problems of\nlow-resource languages and are suitable even in high-resource transfer\nlearning. We evaluate the potential drawbacks and behavior by studying transfer\nlearning in various situations, for example, under artificially damaged\ntraining corpora, or with fixed various model parts.", "published": "2020-01-06 15:11:59", "link": "http://arxiv.org/abs/2001.01622v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Topic Extraction of Crawled Documents Collection using Correlated Topic\n  Model in MapReduce Framework", "abstract": "The tremendous increase in the amount of available research documents impels\nresearchers to propose topic models to extract the latent semantic themes of a\ndocuments collection. However, how to extract the hidden topics of the\ndocuments collection has become a crucial task for many topic model\napplications. Moreover, conventional topic modeling approaches suffer from the\nscalability problem when the size of documents collection increases. In this\npaper, the Correlated Topic Model with variational Expectation-Maximization\nalgorithm is implemented in MapReduce framework to solve the scalability\nproblem. The proposed approach utilizes the dataset crawled from the public\ndigital library. In addition, the full-texts of the crawled documents are\nanalysed to enhance the accuracy of MapReduce CTM. The experiments are\nconducted to demonstrate the performance of the proposed algorithm. From the\nevaluation, the proposed approach has a comparable performance in terms of\ntopic coherences with LDA implemented in MapReduce framework.", "published": "2020-01-06 17:09:21", "link": "http://arxiv.org/abs/2001.01669v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Language Models Are An Effective Patient Representation Learning\n  Technique For Electronic Health Record Data", "abstract": "Widespread adoption of electronic health records (EHRs) has fueled the\ndevelopment of using machine learning to build prediction models for various\nclinical outcomes. This process is often constrained by having a relatively\nsmall number of patient records for training the model. We demonstrate that\nusing patient representation schemes inspired from techniques in natural\nlanguage processing can increase the accuracy of clinical prediction models by\ntransferring information learned from the entire patient population to the task\nof training a specific model, where only a subset of the population is\nrelevant. Such patient representation schemes enable a 3.5% mean improvement in\nAUROC on five prediction tasks compared to standard baselines, with the average\nimprovement rising to 19% when only a small number of patient records are\navailable for training the clinical prediction model.", "published": "2020-01-06 22:24:59", "link": "http://arxiv.org/abs/2001.05295v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Semantic Sensitive TF-IDF to Determine Word Relevance in Documents", "abstract": "Keyword extraction has received an increasing attention as an important\nresearch topic which can lead to have advancements in diverse applications such\nas document context categorization, text indexing and document classification.\nIn this paper we propose STF-IDF, a novel semantic method based on TF-IDF, for\nscoring word importance of informal documents in a corpus. A set of nearly four\nmillion documents from health-care social media was collected and was trained\nin order to draw semantic model and to find the word embeddings. Then, the\nfeatures of semantic space were utilized to rearrange the original TF-IDF\nscores through an iterative solution so as to improve the moderate performance\nof this algorithm on informal texts. After testing the proposed method with 200\nrandomly chosen documents, our method managed to decrease the TF-IDF mean error\nrate by a factor of 50% and reaching the mean error of 13.7%, as opposed to\n27.2% of the original TF-IDF.", "published": "2020-01-06 00:23:11", "link": "http://arxiv.org/abs/2001.09896v2", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Character-Aware Attention-Based End-to-End Speech Recognition", "abstract": "Predicting words and subword units (WSUs) as the output has shown to be\neffective for the attention-based encoder-decoder (AED) model in end-to-end\nspeech recognition. However, as one input to the decoder recurrent neural\nnetwork (RNN), each WSU embedding is learned independently through context and\nacoustic information in a purely data-driven fashion. Little effort has been\nmade to explicitly model the morphological relationships among WSUs. In this\nwork, we propose a novel character-aware (CA) AED model in which each WSU\nembedding is computed by summarizing the embeddings of its constituent\ncharacters using a CA-RNN. This WSU-independent CA-RNN is jointly trained with\nthe encoder, the decoder and the attention network of a conventional AED to\npredict WSUs. With CA-AED, the embeddings of morphologically similar WSUs are\nnaturally and directly correlated through the CA-RNN in addition to the\nsemantic and acoustic relations modeled by a traditional AED. Moreover, CA-AED\nsignificantly reduces the model parameters in a traditional AED by replacing\nthe large pool of WSU embeddings with a much smaller set of character\nembeddings. On a 3400 hours Microsoft Cortana dataset, CA-AED achieves up to\n11.9% relative WER improvement over a strong AED baseline with 27.1% fewer\nmodel parameters.", "published": "2020-01-06 22:19:17", "link": "http://arxiv.org/abs/2001.01795v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domain Adaptation via Teacher-Student Learning for End-to-End Speech\n  Recognition", "abstract": "Teacher-student (T/S) has shown to be effective for domain adaptation of deep\nneural network acoustic models in hybrid speech recognition systems. In this\nwork, we extend the T/S learning to large-scale unsupervised domain adaptation\nof an attention-based end-to-end (E2E) model through two levels of knowledge\ntransfer: teacher's token posteriors as soft labels and one-best predictions as\ndecoder guidance. To further improve T/S learning with the help of ground-truth\nlabels, we propose adaptive T/S (AT/S) learning. Instead of conditionally\nchoosing from either the teacher's soft token posteriors or the one-hot\nground-truth label, in AT/S, the student always learns from both the teacher\nand the ground truth with a pair of adaptive weights assigned to the soft and\none-hot labels quantifying the confidence on each of the knowledge sources. The\nconfidence scores are dynamically estimated at each decoder step as a function\nof the soft and one-hot labels. With 3400 hours parallel close-talk and\nfar-field Microsoft Cortana data for domain adaptation, T/S and AT/S achieve\n6.3% and 10.3% relative word error rate improvement over a strong E2E model\ntrained with the same amount of far-field data.", "published": "2020-01-06 22:30:33", "link": "http://arxiv.org/abs/2001.01798v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement based on Denoising Autoencoder with Multi-branched\n  Encoders", "abstract": "Deep learning-based models have greatly advanced the performance of speech\nenhancement (SE) systems. However, two problems remain unsolved, which are\nclosely related to model generalizability to noisy conditions: (1) mismatched\nnoisy condition during testing, i.e., the performance is generally sub-optimal\nwhen models are tested with unseen noise types that are not involved in the\ntraining data; (2) local focus on specific noisy conditions, i.e., models\ntrained using multiple types of noises cannot optimally remove a specific noise\ntype even though the noise type has been involved in the training data. These\nproblems are common in real applications. In this paper, we propose a novel\ndenoising autoencoder with a multi-branched encoder (termed DAEME) model to\ndeal with these two problems. In the DAEME model, two stages are involved:\ntraining and testing. In the training stage, we build multiple component models\nto form a multi-branched encoder based on a decision tree (DSDT). The DSDT is\nbuilt based on prior knowledge of speech and noisy conditions (the speaker,\nenvironment, and signal factors are considered in this paper), where each\ncomponent of the multi-branched encoder performs a particular mapping from\nnoisy to clean speech along the branch in the DSDT. Finally, a decoder is\ntrained on top of the multi-branched encoder. In the testing stage, noisy\nspeech is first processed by each component model. The multiple outputs from\nthese models are then integrated into the decoder to determine the final\nenhanced speech. Experimental results show that DAEME is superior to several\nbaseline models in terms of objective evaluation metrics, automatic speech\nrecognition results, and quality in subjective human listening tests.", "published": "2020-01-06 13:03:58", "link": "http://arxiv.org/abs/2001.01538v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-visual Recognition of Overlapped speech for the LRS2 dataset", "abstract": "Automatic recognition of overlapped speech remains a highly challenging task\nto date. Motivated by the bimodal nature of human speech perception, this paper\ninvestigates the use of audio-visual technologies for overlapped speech\nrecognition. Three issues associated with the construction of audio-visual\nspeech recognition (AVSR) systems are addressed. First, the basic architecture\ndesigns i.e. end-to-end and hybrid of AVSR systems are investigated. Second,\npurposefully designed modality fusion gates are used to robustly integrate the\naudio and visual features. Third, in contrast to a traditional pipelined\narchitecture containing explicit speech separation and recognition components,\na streamlined and integrated AVSR system optimized consistently using the\nlattice-free MMI (LF-MMI) discriminative criterion is also proposed. The\nproposed LF-MMI time-delay neural network (TDNN) system establishes the\nstate-of-the-art for the LRS2 dataset. Experiments on overlapped speech\nsimulated from the LRS2 dataset suggest the proposed AVSR system outperformed\nthe audio only baseline LF-MMI DNN system by up to 29.98\\% absolute in word\nerror rate (WER) reduction, and produced recognition performance comparable to\na more complex pipelined system. Consistent performance improvements of 4.89\\%\nabsolute in WER reduction over the baseline AVSR system using feature fusion\nare also obtained.", "published": "2020-01-06 16:43:05", "link": "http://arxiv.org/abs/2001.01656v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modeling Musical Structure with Artificial Neural Networks", "abstract": "In recent years, artificial neural networks (ANNs) have become a universal\ntool for tackling real-world problems. ANNs have also shown great success in\nmusic-related tasks including music summarization and classification,\nsimilarity estimation, computer-aided or autonomous composition, and automatic\nmusic analysis. As structure is a fundamental characteristic of Western music,\nit plays a role in all these tasks. Some structural aspects are particularly\nchallenging to learn with current ANN architectures. This is especially true\nfor mid- and high-level self-similarity, tonal and rhythmic relationships. In\nthis thesis, I explore the application of ANNs to different aspects of musical\nstructure modeling, identify some challenges involved and propose strategies to\naddress them. First, using probability estimations of a Restricted Boltzmann\nMachine (RBM), a probabilistic bottom-up approach to melody segmentation is\nstudied. Then, a top-down method for imposing a high-level structural template\nin music generation is presented, which combines Gibbs sampling using a\nconvolutional RBM with gradient-descent optimization on the intermediate\nsolutions. Furthermore, I motivate the relevance of musical transformations in\nstructure modeling and show how a connectionist model, the Gated Autoencoder\n(GAE), can be employed to learn transformations between musical fragments. For\nlearning transformations in sequences, I propose a special predictive training\nof the GAE, which yields a representation of polyphonic music as a sequence of\nintervals. Furthermore, the applicability of these interval representations to\na top-down discovery of repeated musical sections is shown. Finally, a\nrecurrent variant of the GAE is proposed, and its efficacy in music prediction\nand modeling of low-level repetition structure is demonstrated.", "published": "2020-01-06 18:35:57", "link": "http://arxiv.org/abs/2001.01720v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigation and Analysis of Hyper and Hypo neuron pruning to\n  selectively update neurons during Unsupervised Adaptation", "abstract": "Unseen or out-of-domain data can seriously degrade the performance of a\nneural network model, indicating the model's failure to generalize to unseen\ndata. Neural net pruning can not only help to reduce a model's size but can\nimprove the model's generalization capacity as well. Pruning approaches look\nfor low-salient neurons that are less contributive to a model's decision and\nhence can be removed from the model. This work investigates if pruning\napproaches are successful in detecting neurons that are either high-salient\n(mostly active or hyper) or low-salient (barely active or hypo), and whether\nremoval of such neurons can help to improve the model's generalization\ncapacity. Traditional blind adaptation techniques update either the whole or a\nsubset of layers, but have never explored selectively updating individual\nneurons across one or more layers. Focusing on the fully connected layers of a\nconvolutional neural network (CNN), this work shows that it may be possible to\nselectively adapt certain neurons (consisting of the hyper and the hypo\nneurons) first, followed by a full-network fine tuning. Using the task of\nautomatic speech recognition, this work demonstrates how the removal of hyper\nand hypo neurons from a model can improve the model's performance on\nout-of-domain speech data and how selective neuron adaptation can ensure\nimproved performance when compared to traditional blind model adaptation.", "published": "2020-01-06 19:46:57", "link": "http://arxiv.org/abs/2001.01755v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
