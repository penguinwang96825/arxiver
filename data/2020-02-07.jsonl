{"title": "Multimodal Matching Transformer for Live Commenting", "abstract": "Automatic live commenting aims to provide real-time comments on videos for\nviewers. It encourages users engagement on online video sites, and is also a\ngood benchmark for video-to-text generation. Recent work on this task adopts\nencoder-decoder models to generate comments. However, these methods do not\nmodel the interaction between videos and comments explicitly, so they tend to\ngenerate popular comments that are often irrelevant to the videos. In this\nwork, we aim to improve the relevance between live comments and videos by\nmodeling the cross-modal interactions among different modalities. To this end,\nwe propose a multimodal matching transformer to capture the relationships among\ncomments, vision, and audio. The proposed model is based on the transformer\nframework and can iteratively learn the attention-aware representations for\neach modality. We evaluate the model on a publicly available live commenting\ndataset. Experiments show that the multimodal matching transformer model\noutperforms the state-of-the-art methods.", "published": "2020-02-07 07:19:15", "link": "http://arxiv.org/abs/2002.02649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Visual Semantics into Sentence Representations within a\n  Grounded Space", "abstract": "Language grounding is an active field aiming at enriching textual\nrepresentations with visual information. Generally, textual and visual elements\nare embedded in the same representation space, which implicitly assumes a\none-to-one correspondence between modalities. This hypothesis does not hold\nwhen representing words, and becomes problematic when used to learn sentence\nrepresentations --- the focus of this paper --- as a visual scene can be\ndescribed by a wide variety of sentences. To overcome this limitation, we\npropose to transfer visual information to textual representations by learning\nan intermediate representation space: the grounded space. We further propose\ntwo new complementary objectives ensuring that (1) sentences associated with\nthe same visual content are close in the grounded space and (2) similarities\nbetween related elements are preserved across modalities. We show that this\nmodel outperforms the previous state-of-the-art on classification and semantic\nrelatedness tasks.", "published": "2020-02-07 12:26:41", "link": "http://arxiv.org/abs/2002.02734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multilingual View of Unsupervised Machine Translation", "abstract": "We present a probabilistic framework for multilingual neural machine\ntranslation that encompasses supervised and unsupervised setups, focusing on\nunsupervised translation. In addition to studying the vanilla case where there\nis only monolingual data available, we propose a novel setup where one language\nin the (source, target) pair is not associated with any parallel data, but\nthere may exist auxiliary parallel data that contains the other. This auxiliary\ndata can naturally be utilized in our probabilistic framework via a novel\ncross-translation loss term. Empirically, we show that our approach results in\nhigher BLEU scores over state-of-the-art unsupervised models on the WMT'14\nEnglish-French, WMT'16 English-German, and WMT'16 English-Romanian datasets in\nmost directions. In particular, we obtain a +1.65 BLEU advantage over the\nbest-performing unsupervised model in the Romanian-English direction.", "published": "2020-02-07 18:50:21", "link": "http://arxiv.org/abs/2002.02955v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translating Web Search Queries into Natural Language Questions", "abstract": "Users often query a search engine with a specific question in mind and often\nthese queries are keywords or sub-sentential fragments. For example, if the\nusers want to know the answer for \"What's the capital of USA\", they will most\nprobably query \"capital of USA\" or \"USA capital\" or some keyword-based\nvariation of this. For example, for the user entered query \"capital of USA\",\nthe most probable question intent is \"What's the capital of USA?\". In this\npaper, we are proposing a method to generate well-formed natural language\nquestion from a given keyword-based query, which has the same question intent\nas the query. Conversion of keyword-based web query into a well-formed question\nhas lots of applications, with some of them being in search engines, Community\nQuestion Answering (CQA) website and bots communication. We found a synergy\nbetween query-to-question problem with standard machine translation(MT) task.\nWe have used both Statistical MT (SMT) and Neural MT (NMT) models to generate\nthe questions from the query. We have observed that MT models perform well in\nterms of both automatic and human evaluation.", "published": "2020-02-07 05:52:06", "link": "http://arxiv.org/abs/2002.02631v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Depressed individuals express more distorted thinking on social media", "abstract": "Depression is a leading cause of disability worldwide, but is often\nunder-diagnosed and under-treated. One of the tenets of cognitive-behavioral\ntherapy (CBT) is that individuals who are depressed exhibit distorted modes of\nthinking, so-called cognitive distortions, which can negatively affect their\nemotions and motivation. Here, we show that individuals with a self-reported\ndiagnosis of depression on social media express higher levels of distorted\nthinking than a random sample. Some types of distorted thinking were found to\nbe more than twice as prevalent in our depressed cohort, in particular\nPersonalizing and Emotional Reasoning. This effect is specific to the distorted\ncontent of the expression and can not be explained by the presence of specific\ntopics, sentiment, or first-person pronouns. Our results point towards the\ndetection, and possibly mitigation, of patterns of online language that are\ngenerally deemed depressogenic. They may also provide insight into recent\nobservations that social media usage can have a negative impact on mental\nhealth.", "published": "2020-02-07 14:18:53", "link": "http://arxiv.org/abs/2002.02800v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing", "abstract": "In this paper, we propose a novel model compression approach to effectively\ncompress BERT by progressive module replacing. Our approach first divides the\noriginal BERT into several modules and builds their compact substitutes. Then,\nwe randomly replace the original modules with their substitutes to train the\ncompact modules to mimic the behavior of the original modules. We progressively\nincrease the probability of replacement through the training. In this way, our\napproach brings a deeper level of interaction between the original and compact\nmodels. Compared to the previous knowledge distillation approaches for BERT\ncompression, our approach does not introduce any additional loss function. Our\napproach outperforms existing knowledge distillation approaches on GLUE\nbenchmark, showing a new perspective of model compression.", "published": "2020-02-07 17:52:16", "link": "http://arxiv.org/abs/2002.02925v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Snippext: Semi-supervised Opinion Mining with Augmented Data", "abstract": "Online services are interested in solutions to opinion mining, which is the\nproblem of extracting aspects, opinions, and sentiments from text. One method\nto mine opinions is to leverage the recent success of pre-trained language\nmodels which can be fine-tuned to obtain high-quality extractions from reviews.\nHowever, fine-tuning language models still requires a non-trivial amount of\ntraining data. In this paper, we study the problem of how to significantly\nreduce the amount of labeled training data required in fine-tuning language\nmodels for opinion mining. We describe Snippext, an opinion mining system\ndeveloped over a language model that is fine-tuned through semi-supervised\nlearning with augmented data. A novelty of Snippext is its clever use of a\ntwo-prong approach to achieve state-of-the-art (SOTA) performance with little\nlabeled training data through: (1) data augmentation to automatically generate\nmore labeled training data from existing ones, and (2) a semi-supervised\nlearning technique to leverage the massive amount of unlabeled data in addition\nto the (limited amount of) labeled data. We show with extensive experiments\nthat Snippext performs comparably and can even exceed previous SOTA results on\nseveral opinion mining tasks with only half the training data required.\nFurthermore, it achieves new SOTA results when all training data are leveraged.\nBy comparison to a baseline pipeline, we found that Snippext extracts\nsignificantly more fine-grained opinions which enable new opportunities of\ndownstream applications.", "published": "2020-02-07 23:54:23", "link": "http://arxiv.org/abs/2002.03049v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Identifying physical health comorbidities in a cohort of individuals\n  with severe mental illness: An application of SemEHR", "abstract": "Multimorbidity research in mental health services requires data from physical\nhealth conditions which is traditionally limited in mental health care\nelectronic health records. In this study, we aimed to extract data from\nphysical health conditions from clinical notes using SemEHR. Data was extracted\nfrom Clinical Record Interactive Search (CRIS) system at South London and\nMaudsley Biomedical Research Centre (SLaM BRC) and the cohort consisted of all\nindividuals who had received a primary or secondary diagnosis of severe mental\nillness between 2007 and 2018. Three pairs of annotators annotated 2403\ndocuments with an average Cohen's Kappa of 0.757. Results show that the NLP\nperformance varies across different diseases areas (F1 0.601 - 0.954)\nsuggesting that the language patterns or terminologies of different condition\ngroups entail different technical challenges to the same NLP task.", "published": "2020-02-07 13:14:58", "link": "http://arxiv.org/abs/2002.08901v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer Transducer: A Streamable Speech Recognition Model with\n  Transformer Encoders and RNN-T Loss", "abstract": "In this paper we present an end-to-end speech recognition model with\nTransformer encoders that can be used in a streaming speech recognition system.\nTransformer computation blocks based on self-attention are used to encode both\naudio and label sequences independently. The activations from both audio and\nlabel encoders are combined with a feed-forward layer to compute a probability\ndistribution over the label space for every combination of acoustic frame\nposition and label history. This is similar to the Recurrent Neural Network\nTransducer (RNN-T) model, which uses RNNs for information encoding instead of\nTransformer encoders. The model is trained with the RNN-T loss well-suited to\nstreaming decoding. We present results on the LibriSpeech dataset showing that\nlimiting the left context for self-attention in the Transformer layers makes\ndecoding computationally tractable for streaming, with only a slight\ndegradation in accuracy. We also show that the full attention version of our\nmodel beats the-state-of-the art accuracy on the LibriSpeech benchmarks. Our\nresults also show that we can bridge the gap between full attention and limited\nattention versions of our model by attending to a limited number of future\nframes.", "published": "2020-02-07 00:04:04", "link": "http://arxiv.org/abs/2002.02562v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LEAP System for SRE19 CTS Challenge -- Improvements and Error Analysis", "abstract": "The NIST Speaker Recognition Evaluation - Conversational Telephone Speech\n(CTS) challenge 2019 was an open evaluation for the task of speaker\nverification in challenging conditions. In this paper, we provide a detailed\naccount of the LEAP SRE system submitted to the CTS challenge focusing on the\nnovel components in the back-end system modeling. All the systems used the\ntime-delay neural network (TDNN) based x-vector embeddings. The x-vector system\nin our SRE19 submission used a large pool of training speakers (about 14k\nspeakers). Following the x-vector extraction, we explored a neural network\napproach to backend score computation that was optimized for a speaker\nverification cost. The system combination of generative and neural PLDA models\nresulted in significant improvements for the SRE evaluation dataset. We also\nfound additional gains for the SRE systems based on score normalization and\ncalibration. Subsequent to the evaluations, we have performed a detailed\nanalysis of the submitted systems. The analysis revealed the incremental gains\nobtained for different training dataset combinations as well as the modeling\nmethods.", "published": "2020-02-07 12:28:56", "link": "http://arxiv.org/abs/2002.02735v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised pretraining transfers well across languages", "abstract": "Cross-lingual and multi-lingual training of Automatic Speech Recognition\n(ASR) has been extensively investigated in the supervised setting. This assumes\nthe existence of a parallel corpus of speech and orthographic transcriptions.\nRecently, contrastive predictive coding (CPC) algorithms have been proposed to\npretrain ASR systems with unlabelled data. In this work, we investigate whether\nunsupervised pretraining transfers well across languages. We show that a slight\nmodification of the CPC pretraining extracts features that transfer well to\nother languages, being on par or even outperforming supervised pretraining.\nThis shows the potential of unsupervised methods for languages with few\nlinguistic resources.", "published": "2020-02-07 15:34:53", "link": "http://arxiv.org/abs/2002.02848v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "I love your chain mail! Making knights smile in a fantasy game world:\n  Open-domain goal-oriented dialogue agents", "abstract": "Dialogue research tends to distinguish between chit-chat and goal-oriented\ntasks. While the former is arguably more naturalistic and has a wider use of\nlanguage, the latter has clearer metrics and a straightforward learning signal.\nHumans effortlessly combine the two, for example engaging in chit-chat with the\ngoal of exchanging information or eliciting a specific response. Here, we\nbridge the divide between these two domains in the setting of a rich\nmulti-player text-based fantasy environment where agents and humans engage in\nboth actions and dialogue. Specifically, we train a goal-oriented model with\nreinforcement learning against an imitation-learned ``chit-chat'' model with\ntwo approaches: the policy either learns to pick a topic or learns to pick an\nutterance given the top-K utterances from the chit-chat model. We show that\nboth models outperform an inverse model baseline and can converse naturally\nwith their dialogue partner in order to achieve goals.", "published": "2020-02-07 16:22:36", "link": "http://arxiv.org/abs/2002.02878v2", "categories": ["cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.AI"}
{"title": "MA-DST: Multi-Attention Based Scalable Dialog State Tracking", "abstract": "Task oriented dialog agents provide a natural language interface for users to\ncomplete their goal. Dialog State Tracking (DST), which is often a core\ncomponent of these systems, tracks the system's understanding of the user's\ngoal throughout the conversation. To enable accurate multi-domain DST, the\nmodel needs to encode dependencies between past utterances and slot semantics\nand understand the dialog context, including long-range cross-domain\nreferences. We introduce a novel architecture for this task to encode the\nconversation history and slot semantics more robustly by using attention\nmechanisms at multiple granularities. In particular, we use cross-attention to\nmodel relationships between the context and slots at different semantic levels\nand self-attention to resolve cross-domain coreferences. In addition, our\nproposed architecture does not rely on knowing the domain ontologies beforehand\nand can also be used in a zero-shot setting for new domains or unseen slot\nvalues. Our model improves the joint goal accuracy by 5% (absolute) in the\nfull-data setting and by up to 2% (absolute) in the zero-shot setting over the\npresent state-of-the-art on the MultiWoZ 2.1 dataset.", "published": "2020-02-07 05:34:58", "link": "http://arxiv.org/abs/2002.08898v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "What Would You Ask the Machine Learning Model? Identification of User\n  Needs for Model Explanations Based on Human-Model Conversations", "abstract": "Recently we see a rising number of methods in the field of eXplainable\nArtificial Intelligence. To our surprise, their development is driven by model\ndevelopers rather than a study of needs for human end users. The analysis of\nneeds, if done, takes the form of an A/B test rather than a study of open\nquestions. To answer the question \"What would a human operator like to ask the\nML model?\" we propose a conversational system explaining decisions of the\npredictive model. In this experiment, we developed a chatbot called dr_ant to\ntalk about machine learning model trained to predict survival odds on Titanic.\nPeople can talk with dr_ant about different aspects of the model to understand\nthe rationale behind its predictions. Having collected a corpus of 1000+\ndialogues, we analyse the most common types of questions that users would like\nto ask. To our knowledge, it is the first study which uses a conversational\nsystem to collect the needs of human operators from the interactive and\niterative dialogue explorations of a predictive model.", "published": "2020-02-07 15:59:49", "link": "http://arxiv.org/abs/2002.05674v3", "categories": ["cs.CY", "cs.CL", "cs.HC", "cs.LG", "stat.ML"], "primary_category": "cs.CY"}
{"title": "$M^3$T: Multi-Modal Continuous Valence-Arousal Estimation in the Wild", "abstract": "This report describes a multi-modal multi-task ($M^3$T) approach underlying\nour submission to the valence-arousal estimation track of the Affective\nBehavior Analysis in-the-wild (ABAW) Challenge, held in conjunction with the\nIEEE International Conference on Automatic Face and Gesture Recognition (FG)\n2020. In the proposed $M^3$T framework, we fuse both visual features from\nvideos and acoustic features from the audio tracks to estimate the valence and\narousal. The spatio-temporal visual features are extracted with a 3D\nconvolutional network and a bidirectional recurrent neural network. Considering\nthe correlations between valence / arousal, emotions, and facial actions, we\nalso explores mechanisms to benefit from other tasks. We evaluated the $M^3$T\nframework on the validation set provided by ABAW and it significantly\noutperforms the baseline method.", "published": "2020-02-07 18:53:13", "link": "http://arxiv.org/abs/2002.02957v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
