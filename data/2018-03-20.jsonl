{"title": "Why not be Versatile? Applications of the SGNMT Decoder for Machine\n  Translation", "abstract": "SGNMT is a decoding platform for machine translation which allows paring\nvarious modern neural models of translation with different kinds of constraints\nand symbolic models. In this paper, we describe three use cases in which SGNMT\nis currently playing an active role: (1) teaching as SGNMT is being used for\ncourse work and student theses in the MPhil in Machine Learning, Speech and\nLanguage Technology at the University of Cambridge, (2) research as most of the\nresearch work of the Cambridge MT group is based on SGNMT, and (3) technology\ntransfer as we show how SGNMT is helping to transfer research findings from the\nlaboratory to the industry, eg. into a product of SDL plc.", "published": "2018-03-20 00:44:18", "link": "http://arxiv.org/abs/1803.07204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "eSCAPE: a Large-scale Synthetic Corpus for Automatic Post-Editing", "abstract": "Training models for the automatic correction of machine-translated text\nusually relies on data consisting of (source, MT, human post- edit) triplets\nproviding, for each source sentence, examples of translation errors with the\ncorresponding corrections made by a human post-editor. Ideally, a large amount\nof data of this kind should allow the model to learn reliable correction\npatterns and effectively apply them at test stage on unseen (source, MT) pairs.\nIn practice, however, their limited availability calls for solutions that also\nintegrate in the training process other sources of knowledge. Along this\ndirection, state-of-the-art results have been recently achieved by systems\nthat, in addition to a limited amount of available training data, exploit\nartificial corpora that approximate elements of the \"gold\" training instances\nwith automatic translations. Following this idea, we present eSCAPE, the\nlargest freely-available Synthetic Corpus for Automatic Post-Editing released\nso far. eSCAPE consists of millions of entries in which the MT element of the\ntraining triplets has been obtained by translating the source side of\npublicly-available parallel corpora, and using the target side as an artificial\nhuman post-edit. Translations are obtained both with phrase-based and neural\nmodels. For each MT paradigm, eSCAPE contains 7.2 million triplets for\nEnglish-German and 3.3 millions for English-Italian, resulting in a total of\n14,4 and 6,6 million instances respectively. The usefulness of eSCAPE is proved\nthrough experiments in a general-domain scenario, the most challenging one for\nautomatic post-editing. For both language directions, the models trained on our\nartificial data always improve MT quality with statistically significant gains.\nThe current version of eSCAPE can be freely downloaded from:\nhttp://hltshare.fbk.eu/QT21/eSCAPE.html.", "published": "2018-03-20 06:59:27", "link": "http://arxiv.org/abs/1803.07274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expressivity in TTS from Semantics and Pragmatics", "abstract": "In this paper we present ongoing work to produce an expressive TTS reader\nthat can be used both in text and dialogue applications. The system called\nSPARSAR has been used to read (English) poetry so far but it can now be applied\nto any text. The text is fully analyzed both at phonetic and phonological\nlevel, and at syntactic and semantic level. In addition, the system has access\nto a restricted list of typical pragmatically marked phrases and expressions\nthat are used to convey specific discourse function and speech acts and need\nspecialized intonational contours. The text is transformed into a poem-like\nstructures, where each line corresponds to a Breath Group, semantically and\nsyntactically consistent. Stanzas correspond to paragraph boundaries.\nAnalogical parameters are related to ToBI theoretical indices but their number\nis doubled. In this paper, we concentrate on short stories and fables.", "published": "2018-03-20 08:35:16", "link": "http://arxiv.org/abs/1803.07295v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UnibucKernel: A kernel-based learning method for complex word\n  identification", "abstract": "In this paper, we present a kernel-based learning approach for the 2018\nComplex Word Identification (CWI) Shared Task. Our approach is based on\ncombining multiple low-level features, such as character n-grams, with\nhigh-level semantic features that are either automatically learned using word\nembeddings or extracted from a lexical knowledge base, namely WordNet. After\nfeature extraction, we employ a kernel method for the learning phase. The\nfeature matrix is first transformed into a normalized kernel matrix. For the\nbinary classification task (simple versus complex), we employ Support Vector\nMachines. For the regression task, in which we have to predict the complexity\nlevel of a word (a word is more complex if it is labeled as complex by more\nannotators), we employ v-Support Vector Regression. We applied our approach\nonly on the three English data sets containing documents from Wikipedia,\nWikiNews and News domains. Our best result during the competition was the third\nplace on the English Wikipedia data set. However, in this paper, we also report\nbetter post-competition results.", "published": "2018-03-20 18:47:54", "link": "http://arxiv.org/abs/1803.07602v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AllenNLP: A Deep Semantic Natural Language Processing Platform", "abstract": "This paper describes AllenNLP, a platform for research on deep learning\nmethods in natural language understanding. AllenNLP is designed to support\nresearchers who want to build novel language understanding models quickly and\neasily. It is built on top of PyTorch, allowing for dynamic computation graphs,\nand provides (1) a flexible data API that handles intelligent batching and\npadding, (2) high-level abstractions for common operations in working with\ntext, and (3) a modular and extensible experiment framework that makes doing\ngood science easy. It also includes reference implementations of high quality\napproaches for both core semantic problems (e.g. semantic role labeling (Palmer\net al., 2005)) and language understanding applications (e.g. machine\ncomprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source\neffort maintained by engineers and researchers at the Allen Institute for\nArtificial Intelligence.", "published": "2018-03-20 20:32:07", "link": "http://arxiv.org/abs/1803.07640v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Product Characterisation towards Personalisation: Learning Attributes\n  from Unstructured Data to Recommend Fashion Products", "abstract": "In this paper, we describe a solution to tackle a common set of challenges in\ne-commerce, which arise from the fact that new products are continually being\nadded to the catalogue. The challenges involve properly personalising the\ncustomer experience, forecasting demand and planning the product range. We\nargue that the foundational piece to solve all of these problems is having\nconsistent and detailed information about each product, information that is\nrarely available or consistent given the multitude of suppliers and types of\nproducts. We describe in detail the architecture and methodology implemented at\nASOS, one of the world's largest fashion e-commerce retailers, to tackle this\nproblem. We then show how this quantitative understanding of the products can\nbe leveraged to improve recommendations in a hybrid recommender system\napproach.", "published": "2018-03-20 22:25:29", "link": "http://arxiv.org/abs/1803.07679v1", "categories": ["stat.ML", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "stat.ML"}
