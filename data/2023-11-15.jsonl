{"title": "Formal Proofs as Structured Explanations: Proposing Several Tasks on\n  Explainable Natural Language Inference", "abstract": "In this position paper, we propose a reasoning framework that can model the\nreasoning process underlying natural language inferences. The framework is\nbased on the semantic tableau method, a well-studied proof system in formal\nlogic. Like the semantic tableau, the framework is driven by refutation --\nsomething is proved if and only if its counterexample was not refuted. Despite\nbeing rooted in formal logic, the framework shares similarities with the mental\nmodels, a theory on the psychology of reasoning. We will show how the reasoning\nframework can facilitate the collection of comprehensive and structured\nexplanations for existing naturalistic inference problems. To make the\nsuggestion more concrete, we propose a method of semi-automatically obtaining\nstructured explanations from the formal proofs of a reliable and\nhigh-performing logic-based inference system. Taking advantage of the in-depth\ninformation available in the generated formal proofs, we show how it can be\nused to define natural language reasoning tasks with structured explanations.\nThe proposed tasks can be ordered according to difficulty defined in terms of\nthe granularity of explanations. We argue that the tasks that contain a natural\nsketch of the proofs will suffer from substantially fewer shortcomings than the\nexisting explainable reasoning tasks (or datasets).", "published": "2023-11-15 01:24:09", "link": "http://arxiv.org/abs/2311.08637v2", "categories": ["cs.CL", "03B65, 68T50", "F.4.1; I.2.3; K.3.2; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Evaluating Robustness of Dialogue Summarization Models in the Presence\n  of Naturally Occurring Variations", "abstract": "Dialogue summarization task involves summarizing long conversations while\npreserving the most salient information. Real-life dialogues often involve\nnaturally occurring variations (e.g., repetitions, hesitations) and existing\ndialogue summarization models suffer from performance drop on such\nconversations. In this study, we systematically investigate the impact of such\nvariations on state-of-the-art dialogue summarization models using publicly\navailable datasets. To simulate real-life variations, we introduce two types of\nperturbations: utterance-level perturbations that modify individual utterances\nwith errors and language variations, and dialogue-level perturbations that add\nnon-informative exchanges (e.g., repetitions, greetings). We conduct our\nanalysis along three dimensions of robustness: consistency, saliency, and\nfaithfulness, which capture different aspects of the summarization model's\nperformance. We find that both fine-tuned and instruction-tuned models are\naffected by input variations, with the latter being more susceptible,\nparticularly to dialogue-level perturbations. We also validate our findings via\nhuman evaluation. Finally, we investigate if the robustness of fine-tuned\nmodels can be improved by training them with a fraction of perturbed data and\nobserve that this approach is insufficient to address robustness challenges\nwith current models and thus warrants a more thorough investigation to identify\nbetter solutions. Overall, our work highlights robustness challenges in\ndialogue summarization and provides insights for future research.", "published": "2023-11-15 05:11:43", "link": "http://arxiv.org/abs/2311.08705v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning", "abstract": "Instruction tuning has remarkably advanced large language models (LLMs) in\nunderstanding and responding to diverse human instructions. Despite the success\nin high-resource languages, its application in lower-resource ones faces\nchallenges due to the imbalanced foundational abilities of LLMs across\ndifferent languages, stemming from the uneven language distribution in their\npre-training data. To tackle this issue, we propose pivot language guided\ngeneration (PLUG), an approach that utilizes a high-resource language,\nprimarily English, as the pivot to enhance instruction tuning in lower-resource\nlanguages. It trains the model to first process instructions in the pivot\nlanguage, and then produce responses in the target language. To evaluate our\napproach, we introduce a benchmark, X-AlpacaEval, of instructions in 4\nlanguages (Chinese, Korean, Italian, and Spanish), each annotated by\nprofessional translators. Our approach demonstrates a significant improvement\nin the instruction-following abilities of LLMs by 29% on average, compared to\ndirectly responding in the target language alone. Further experiments validate\nthe versatility of our approach by employing alternative pivot languages beyond\nEnglish to assist languages where LLMs exhibit lower proficiency. Our code and\ndata are available at https://github.com/ytyz1307zzh/PLUG.", "published": "2023-11-15 05:28:07", "link": "http://arxiv.org/abs/2311.08711v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decomposing Uncertainty for Large Language Models through Input\n  Clarification Ensembling", "abstract": "Uncertainty decomposition refers to the task of decomposing the total\nuncertainty of a predictive model into aleatoric (data) uncertainty, resulting\nfrom inherent randomness in the data-generating process, and epistemic (model)\nuncertainty, resulting from missing information in the model's training data.\nIn large language models (LLMs) specifically, identifying sources of\nuncertainty is an important step toward improving reliability, trustworthiness,\nand interpretability, but remains an important open research question. In this\npaper, we introduce an uncertainty decomposition framework for LLMs, called\ninput clarification ensembling, which can be applied to any pre-trained LLM.\nOur approach generates a set of clarifications for the input, feeds them into\nan LLM, and ensembles the corresponding predictions. We show that, when\naleatoric uncertainty arises from ambiguity or under-specification in LLM\ninputs, this approach makes it possible to factor an (unclarified) LLM's\npredictions into separate aleatoric and epistemic terms, using a decomposition\nsimilar to the one employed by Bayesian neural networks. Empirical evaluations\ndemonstrate that input clarification ensembling provides accurate and reliable\nuncertainty quantification on several language processing tasks. Code and data\nare available at https://github.com/UCSB-NLP-Chang/llm_uncertainty.", "published": "2023-11-15 05:58:35", "link": "http://arxiv.org/abs/2311.08718v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term\n  Memory", "abstract": "Memory-augmented Large Language Models (LLMs) have demonstrated remarkable\nperformance in long-term human-machine interactions, which basically relies on\niterative recalling and reasoning of history to generate high-quality\nresponses. However, such repeated recall-reason steps easily produce biased\nthoughts, \\textit{i.e.}, inconsistent reasoning results when recalling the same\nhistory for different questions. On the contrary, humans can keep thoughts in\nthe memory and recall them without repeated reasoning. Motivated by this human\ncapability, we propose a novel memory mechanism called TiM (Think-in-Memory)\nthat enables LLMs to maintain an evolved memory for storing historical thoughts\nalong the conversation stream. The TiM framework consists of two crucial\nstages: (1) before generating a response, a LLM agent recalls relevant thoughts\nfrom memory, and (2) after generating a response, the LLM agent post-thinks and\nincorporates both historical and new thoughts to update the memory. Thus, TiM\ncan eliminate the issue of repeated reasoning by saving the post-thinking\nthoughts as the history. Besides, we formulate the basic principles to organize\nthe thoughts in memory based on the well-established operations,\n(\\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic\nupdates and evolution of the thoughts. Furthermore, we introduce\nLocality-Sensitive Hashing into TiM to achieve efficient retrieval for the\nlong-term conversations. We conduct qualitative and quantitative experiments on\nreal-world and simulated dialogues covering a wide range of topics,\ndemonstrating that equipping existing LLMs with TiM significantly enhances\ntheir performance in generating responses for long-term interactions.", "published": "2023-11-15 06:08:35", "link": "http://arxiv.org/abs/2311.08719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Token Prediction as Implicit Classification to Identify LLM-Generated\n  Text", "abstract": "This paper introduces a novel approach for identifying the possible large\nlanguage models (LLMs) involved in text generation. Instead of adding an\nadditional classification layer to a base LM, we reframe the classification\ntask as a next-token prediction task and directly fine-tune the base LM to\nperform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the\nbackbone for our experiments. We compared our approach to the more direct\napproach of utilizing hidden states for classification. Evaluation shows the\nexceptional performance of our method in the text classification task,\nhighlighting its simplicity and efficiency. Furthermore, interpretability\nstudies on the features extracted by our model reveal its ability to\ndifferentiate distinctive writing styles among various LLMs even in the absence\nof an explicit classifier. We also collected a dataset named OpenLLMText,\ncontaining approximately 340k text samples from human and LLMs, including\nGPT3.5, PaLM, LLaMA, and GPT2.", "published": "2023-11-15 06:33:52", "link": "http://arxiv.org/abs/2311.08723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty Estimation on Sequential Labeling via Uncertainty\n  Transmission", "abstract": "Sequential labeling is a task predicting labels for each token in a sequence,\nsuch as Named Entity Recognition (NER). NER tasks aim to extract entities and\npredict their labels given a text, which is important in information\nextraction. Although previous works have shown great progress in improving NER\nperformance, uncertainty estimation on NER (UE-NER) is still underexplored but\nessential. This work focuses on UE-NER, which aims to estimate uncertainty\nscores for the NER predictions. Previous uncertainty estimation models often\noverlook two unique characteristics of NER: the connection between entities\n(i.e., one entity embedding is learned based on the other ones) and wrong span\ncases in the entity extraction subtask. Therefore, we propose a Sequential\nLabeling Posterior Network (SLPN) to estimate uncertainty scores for the\nextracted entities, considering uncertainty transmitted from other tokens.\nMoreover, we have defined an evaluation strategy to address the specificity of\nwrong-span cases. Our SLPN has achieved significant improvements on three\ndatasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant\ndataset. Our code is available at\n\\url{https://github.com/he159ok/UncSeqLabeling_SLPN}.", "published": "2023-11-15 06:36:29", "link": "http://arxiv.org/abs/2311.08726v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Emergency Decision-making with Knowledge Graphs and Large\n  Language Models", "abstract": "Emergency management urgently requires comprehensive knowledge while having a\nhigh possibility to go beyond individuals' cognitive scope. Therefore,\nartificial intelligence(AI) supported decision-making under that circumstance\nis of vital importance. Recent emerging large language models (LLM) provide a\nnew direction for enhancing targeted machine intelligence. However, the\nutilization of LLM directly would inevitably introduce unreliable output for\nits inherent issue of hallucination and poor reasoning skills. In this work, we\ndevelop a system called Enhancing Emergency decision-making with Knowledge\nGraph and LLM (E-KELL), which provides evidence-based decision-making in\nvarious emergency stages. The study constructs a structured emergency knowledge\ngraph and guides LLMs to reason over it via a prompt chain. In real-world\nevaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in\ncomprehensibility, accuracy, conciseness, and instructiveness from a group of\nemergency commanders and firefighters, demonstrating a significant improvement\nacross various situations compared to baseline models. This work introduces a\nnovel approach to providing reliable emergency decision support.", "published": "2023-11-15 06:48:50", "link": "http://arxiv.org/abs/2311.08732v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thread of Thought Unraveling Chaotic Contexts", "abstract": "Large Language Models (LLMs) have ushered in a transformative era in the\nfield of natural language processing, excelling in tasks related to text\ncomprehension and generation. Nevertheless, they encounter difficulties when\nconfronted with chaotic contexts (e.g., distractors rather than long irrelevant\ncontext), leading to the inadvertent omission of certain details within the\nchaotic context. In response to these challenges, we introduce the \"Thread of\nThought\" (ThoT) strategy, which draws inspiration from human cognitive\nprocesses. ThoT systematically segments and analyzes extended contexts while\nadeptly selecting pertinent information. This strategy serves as a versatile\n\"plug-and-play\" module, seamlessly integrating with various LLMs and prompting\ntechniques. In the experiments, we utilize the PopQA and EntityQ datasets, as\nwell as a Multi-Turn Conversation Response dataset (MTCR) we collected, to\nillustrate that ThoT significantly improves reasoning performance compared to\nother prompting techniques.", "published": "2023-11-15 06:54:44", "link": "http://arxiv.org/abs/2311.08734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating Toeplitz Neural Network with Constant-time Inference\n  Complexity", "abstract": "Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in\nvarious sequence modeling tasks. They outperform commonly used\nTransformer-based models while benefiting from log-linear space-time\ncomplexities. On the other hand, State Space Models (SSMs) achieve lower\nperformance than TNNs in language modeling but offer the advantage of constant\ninference complexity. In this paper, we aim to combine the strengths of TNNs\nand SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to\nachieve the same constant inference complexities as SSMs. To accomplish this,\nwe formulate the conversion process as an optimization problem and provide a\nclosed-form solution. We demonstrate how to transform the target equation into\na Vandermonde linear system problem, which can be efficiently solved using the\nDiscrete Fourier Transform (DFT). Notably, our method requires no training and\nmaintains numerical stability. It can be also applied to any LongConv-based\nmodel. To assess its effectiveness, we conduct extensive experiments on\nlanguage modeling tasks across various settings. Additionally, we compare our\nmethod to other gradient-descent solutions, highlighting the superior numerical\nstability of our approach. The source code is available at\nhttps://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.", "published": "2023-11-15 07:50:57", "link": "http://arxiv.org/abs/2311.08756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StrategyLLM: Large Language Models as Strategy Generators, Executors,\n  Optimizers, and Evaluators for Problem Solving", "abstract": "Most existing prompting methods suffer from the issues of generalizability\nand consistency, as they often rely on instance-specific solutions that may not\nbe applicable to other instances and lack task-level consistency across the\nselected few-shot examples. To address these limitations, we propose a\ncomprehensive framework, StrategyLLM, allowing LLMs to perform inductive\nreasoning, deriving general strategies from specific task instances, and\ndeductive reasoning, applying these general strategies to particular task\nexamples, for constructing generalizable and consistent few-shot prompts. It\nemploys four LLM-based agents: strategy generator, executor, optimizer, and\nevaluator, working together to generate, evaluate, and select promising\nstrategies for a given task. Experimental results demonstrate that StrategyLLM\noutperforms the competitive baseline CoT-SC that requires human-annotated\nsolutions on 13 datasets across 4 challenging tasks without human involvement,\nincluding math reasoning (34.2\\% $\\rightarrow$ 38.8\\%), commonsense reasoning\n(70.3\\% $\\rightarrow$ 72.5\\%), algorithmic reasoning (73.7\\% $\\rightarrow$\n85.0\\%), and symbolic reasoning (30.0\\% $\\rightarrow$ 79.2\\%). Further analysis\nreveals that StrategyLLM is applicable to various LLMs and demonstrates\nadvantages across numerous scenarios.", "published": "2023-11-15 09:18:09", "link": "http://arxiv.org/abs/2311.08803v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disinformation Capabilities of Large Language Models", "abstract": "Automated disinformation generation is often listed as an important risk\nassociated with large language models (LLMs). The theoretical ability to flood\nthe information space with disinformation content might have dramatic\nconsequences for societies around the world. This paper presents a\ncomprehensive study of the disinformation capabilities of the current\ngeneration of LLMs to generate false news articles in the English language. In\nour study, we evaluated the capabilities of 10 LLMs using 20 disinformation\nnarratives. We evaluated several aspects of the LLMs: how good they are at\ngenerating news articles, how strongly they tend to agree or disagree with the\ndisinformation narratives, how often they generate safety warnings, etc. We\nalso evaluated the abilities of detection models to detect these articles as\nLLM-generated. We conclude that LLMs are able to generate convincing news\narticles that agree with dangerous disinformation narratives.", "published": "2023-11-15 10:25:30", "link": "http://arxiv.org/abs/2311.08838v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient\n  Large-scale Multilingual Continued Pretraining", "abstract": "Instead of pretraining multilingual language models from scratch, a more\nefficient method is to adapt existing pretrained language models (PLMs) to new\nlanguages via vocabulary extension and continued pretraining. However, this\nmethod usually randomly initializes the embeddings of new subwords and\nintroduces substantially more embedding parameters to the model, thus weakening\nthe efficiency. To address these issues, we propose a novel framework:\n$\\textbf{O}$ne $\\textbf{F}$or $\\textbf{A}$ll ($\\textbf{OFA}$), which wisely\ninitializes the embeddings of unseen subwords and thus can adapt a PLM to\nmultiple languages efficiently and effectively. OFA takes advantage of external\nwell-aligned multilingual static word vectors and injects the alignment\nknowledge into the subword embeddings. In addition, OFA applies matrix\nfactorization and replaces the cumbersome embeddings with two lower-dimensional\nmatrices, which largely reduces the number of parameters. We show OFA\naccelerates the convergence of continued pretraining, which is environmentally\nfriendly as much fewer carbon footprints are generated. Through extensive\nexperiments, we demonstrate OFA can achieve competitive or better performance\nthan default continued pretraining baselines on a wide range of crosslingual\ndownstream tasks. We make our code and models publicly available.", "published": "2023-11-15 10:40:45", "link": "http://arxiv.org/abs/2311.08849v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Rule-based Knowledge into Large Language Models", "abstract": "Large language models (LLMs) have shown incredible performance in completing\nvarious real-world tasks. The current paradigm of knowledge learning for LLMs\nis mainly based on learning from examples, in which LLMs learn the internal\nrule implicitly from a certain number of supervised examples. However, this\nlearning paradigm may not well learn those complicated rules, especially when\nthe training examples are limited. We are inspired that humans can learn the\nnew tasks or knowledge in another way by learning from rules. That is, humans\ncan learn new tasks or grasp new knowledge quickly and generalize well given\nonly a detailed rule and a few optional examples. Therefore, in this paper, we\naim to explore the feasibility of this new learning paradigm, which targets on\nencoding rule-based knowledge into LLMs. We further propose rule distillation,\nwhich first uses the strong in-context abilities of LLMs to extract the\nknowledge from the textual rules, and then explicitly encode the knowledge into\nthe parameters of LLMs by learning from the above in-context signals produced\ninside the model. Our experiments show that making LLMs learn from rules by our\nmethod is much more efficient than example-based learning in both the sample\nsize and generalization ability. Warning: This paper may contain examples with\noffensive content.", "published": "2023-11-15 11:42:41", "link": "http://arxiv.org/abs/2311.08883v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLIMB: Curriculum Learning for Infant-inspired Model Building", "abstract": "We describe our team's contribution to the STRICT-SMALL track of the BabyLM\nChallenge. The challenge requires training a language model from scratch using\nonly a relatively small training dataset of ten million words. We experiment\nwith three variants of cognitively-motivated curriculum learning and analyze\ntheir effect on the performance of the model on linguistic evaluation tasks. In\nthe vocabulary curriculum, we analyze methods for constraining the vocabulary\nin the early stages of training to simulate cognitively more plausible learning\ncurves. In the data curriculum experiments, we vary the order of the training\ninstances based on i) infant-inspired expectations and ii) the learning\nbehavior of the model. In the objective curriculum, we explore different\nvariations of combining the conventional masked language modeling task with a\nmore coarse-grained word class prediction task to reinforce linguistic\ngeneralization capabilities. Our results did not yield consistent improvements\nover our own non-curriculum learning baseline across a range of linguistic\nbenchmarks; however, we do find marginal gains on select tasks. Our analysis\nhighlights key takeaways for specific combinations of tasks and settings which\nbenefit from our proposed curricula. We moreover determine that careful\nselection of model architecture, and training hyper-parameters yield\nsubstantial improvements over the default baselines provided by the BabyLM\nchallenge.", "published": "2023-11-15 11:48:16", "link": "http://arxiv.org/abs/2311.08886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are legal but they are not: Making the case for a\n  powerful LegalLLM", "abstract": "Realizing the recent advances in Natural Language Processing (NLP) to the\nlegal sector poses challenging problems such as extremely long sequence\nlengths, specialized vocabulary that is usually only understood by legal\nprofessionals, and high amounts of data imbalance. The recent surge of Large\nLanguage Models (LLMs) has begun to provide new opportunities to apply NLP in\nthe legal domain due to their ability to handle lengthy, complex sequences.\nMoreover, the emergence of domain-specific LLMs has displayed extremely\npromising results on various tasks. In this study, we aim to quantify how\ngeneral LLMs perform in comparison to legal-domain models (be it an LLM or\notherwise). Specifically, we compare the zero-shot performance of three\ngeneral-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR\nsubset of the LexGLUE benchmark for contract provision classification. Although\nthe LLMs were not explicitly trained on legal data, we observe that they are\nstill able to classify the theme correctly in most cases. However, we find that\ntheir mic-F1/mac-F1 performance is up to 19.2/26.8\\% lesser than smaller models\nfine-tuned on the legal domain, thus underscoring the need for more powerful\nlegal-domain LLMs.", "published": "2023-11-15 11:50:10", "link": "http://arxiv.org/abs/2311.08890v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HeLM: Highlighted Evidence augmented Language Model for Enhanced\n  Table-to-Text Generation", "abstract": "Large models have demonstrated significant progress across various domains,\nparticularly in tasks related to text generation. In the domain of Table to\nText, many Large Language Model (LLM)-based methods currently resort to\nmodifying prompts to invoke public APIs, incurring potential costs and\ninformation leaks. With the advent of open-source large models, fine-tuning\nLLMs has become feasible. In this study, we conducted parameter-efficient\nfine-tuning on the LLaMA2 model. Distinguishing itself from previous\nfine-tuning-based table-to-text methods, our approach involves injecting\nreasoning information into the input by emphasizing table-specific row data.\nOur model consists of two modules: 1) a table reasoner that identifies relevant\nrow evidence, and 2) a table summarizer that generates sentences based on the\nhighlighted table. To facilitate this, we propose a search strategy to\nconstruct reasoning labels for training the table reasoner. On both the FetaQA\nand QTSumm datasets, our approach achieved state-of-the-art results.\nAdditionally, we observed that highlighting input tables significantly enhances\nthe model's performance and provides valuable interpretability.", "published": "2023-11-15 12:02:52", "link": "http://arxiv.org/abs/2311.08896v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Improving for Zero-Shot Named Entity Recognition with Large\n  Language Models", "abstract": "Exploring the application of powerful large language models (LLMs) on the\nnamed entity recognition (NER) task has drawn much attention recently. This\nwork pushes the performance boundary of zero-shot NER with LLMs by proposing a\ntraining-free self-improving framework, which utilizes an unlabeled corpus to\nstimulate the self-learning ability of LLMs. First, we use the LLM to make\npredictions on the unlabeled corpus using self-consistency and obtain a\nself-annotated dataset. Second, we explore various strategies to select\nreliable annotations to form a reliable self-annotated dataset. Finally, for\neach test input, we retrieve demonstrations from the reliable self-annotated\ndataset and perform inference via in-context learning. Experiments on four\nbenchmarks show substantial performance improvements achieved by our framework.\nThrough comprehensive experimental analysis, we find that increasing the size\nof unlabeled corpus or iterations of self-improving does not guarantee further\nimprovement, but the performance might be boosted via more advanced strategies\nfor reliable annotation selection. Code and data are publicly available at\nhttps://github.com/Emma1066/Self-Improve-Zero-Shot-NER", "published": "2023-11-15 12:47:52", "link": "http://arxiv.org/abs/2311.08921v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speculative Contrastive Decoding", "abstract": "Large language models~(LLMs) exhibit exceptional performance in language\ntasks, yet their auto-regressive inference is limited due to high computational\nrequirements and is sub-optimal due to the exposure bias. Inspired by\nspeculative decoding and contrastive decoding, we introduce Speculative\nContrastive Decoding~(SCD), a straightforward yet powerful decoding approach\nthat leverages predictions from smaller language models~(LMs) to achieve both\ndecoding acceleration and quality improvement. Extensive evaluations and\nanalyses on four diverse language tasks demonstrate the effectiveness of SCD,\nshowing that decoding efficiency and quality can compatibly benefit from one\nsmaller LM.", "published": "2023-11-15 14:15:30", "link": "http://arxiv.org/abs/2311.08981v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentAlign: Accurate and Scalable Sentence Alignment", "abstract": "We present SentAlign, an accurate sentence alignment tool designed to handle\nvery large parallel document pairs. Given user-defined parameters, the\nalignment algorithm evaluates all possible alignment paths in fairly large\ndocuments of thousands of sentences and uses a divide-and-conquer approach to\nalign documents containing tens of thousands of sentences. The scoring function\nis based on LaBSE bilingual sentence representations. SentAlign outperforms\nfive other sentence alignment tools when evaluated on two different evaluation\nsets, German-French and English-Icelandic, and on a downstream machine\ntranslation task.", "published": "2023-11-15 14:15:41", "link": "http://arxiv.org/abs/2311.08982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic\n  Fact-checkers", "abstract": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. In this work, we present a holistic end-to-end solution for\nannotating the factuality of LLM-generated responses, which encompasses a\nmulti-stage annotation scheme designed to yield detailed labels concerning the\nverifiability and factual inconsistencies found in LLM outputs. We further\nconstruct an open-domain document-level factuality benchmark in three-level\ngranularity: claim, sentence and document, aiming to facilitate the evaluation\nof automatic fact-checking systems. Preliminary experiments show that FacTool,\nFactScore and Perplexity.ai are struggling to identify false claims, with the\nbest F1=0.63 by this annotation solution based on GPT-4. Annotation tool,\nbenchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.", "published": "2023-11-15 14:41:57", "link": "http://arxiv.org/abs/2311.09000v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and\n  Future Directions", "abstract": "End-to-end task-oriented dialogue (EToD) can directly generate responses in\nan end-to-end fashion without modular training, which attracts escalating\npopularity. The advancement of deep neural networks, especially the successful\nuse of large pre-trained models, has further led to significant progress in\nEToD research in recent years. In this paper, we present a thorough review and\nprovide a unified perspective to summarize existing approaches as well as\nrecent trends to advance the development of EToD research. The contributions of\nthis paper can be summarized: (1) \\textbf{\\textit{First survey}}: to our\nknowledge, we take the first step to present a thorough survey of this research\nfield; (2) \\textbf{\\textit{New taxonomy}}: we first introduce a unified\nperspective for EToD, including (i) \\textit{Modularly EToD} and (ii)\n\\textit{Fully EToD}; (3) \\textbf{\\textit{New Frontiers}}: we discuss some\npotential frontier areas as well as the corresponding challenges, hoping to\nspur breakthrough research in EToD field; (4) \\textbf{\\textit{Abundant\nresources}}: we build a public website\\footnote{We collect the related papers,\nbaseline projects, and leaderboards for the community at\n\\url{https://etods.net/}.}, where EToD researchers could directly access the\nrecent progress. We hope this work can serve as a thorough reference for the\nEToD research community.", "published": "2023-11-15 14:50:16", "link": "http://arxiv.org/abs/2311.09008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Potential of Large Language Models in Computational\n  Argumentation", "abstract": "Computational argumentation has become an essential tool in various domains,\nincluding law, public policy, and artificial intelligence. It is an emerging\nresearch field in natural language processing that attracts increasing\nattention. Research on computational argumentation mainly involves two types of\ntasks: argument mining and argument generation. As large language models (LLMs)\nhave demonstrated impressive capabilities in understanding context and\ngenerating natural language, it is worthwhile to evaluate the performance of\nLLMs on diverse computational argumentation tasks. This work aims to embark on\nan assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both\nzero-shot and few-shot settings. We organize existing tasks into six main\ncategories and standardize the format of fourteen openly available datasets. In\naddition, we present a new benchmark dataset on counter speech generation that\naims to holistically evaluate the end-to-end performance of LLMs on argument\nmining and argument generation. Extensive experiments show that LLMs exhibit\ncommendable performance across most of the datasets, demonstrating their\ncapabilities in the field of argumentation. Our analysis offers valuable\nsuggestions for evaluating computational argumentation and its integration with\nLLMs in future research endeavors.", "published": "2023-11-15 15:12:15", "link": "http://arxiv.org/abs/2311.09022v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRASP: A novel benchmark for evaluating language GRounding And Situated\n  Physics understanding in multimodal language models", "abstract": "This paper presents GRASP, a novel benchmark to evaluate the language\ngrounding and physical understanding capabilities of video-based multimodal\nlarge language models (LLMs). This evaluation is accomplished via a two-tier\napproach leveraging Unity simulations. The first level tests for language\ngrounding by assessing a model's ability to relate simple textual descriptions\nwith visual information. The second level evaluates the model's understanding\nof \"Intuitive Physics\" principles, such as object permanence and continuity. In\naddition to releasing the benchmark, we use it to evaluate several\nstate-of-the-art multimodal LLMs. Our evaluation reveals significant\nshortcomings in the language grounding and intuitive physics capabilities of\nthese models. Although they exhibit at least some grounding capabilities,\nparticularly for colors and shapes, these capabilities depend heavily on the\nprompting strategy. At the same time, all models perform below or at the chance\nlevel of 50% in the Intuitive Physics tests, while human subjects are on\naverage 80% correct. These identified limitations underline the importance of\nusing benchmarks like GRASP to monitor the progress of future models in\ndeveloping these competencies.", "published": "2023-11-15 15:38:28", "link": "http://arxiv.org/abs/2311.09048v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale\n  of Two Benchmarks", "abstract": "The concept of localization in LLMs is often mentioned in prior work;\nhowever, methods for localization have never been systematically and directly\nevaluated. We propose two complementary benchmarks that evaluate the ability of\nlocalization methods to pinpoint LLM components responsible for memorized data.\nIn our INJ benchmark, we actively inject a piece of new information into a\nsmall subset of LLM weights, enabling us to directly evaluate whether\nlocalization methods can identify these \"ground truth\" weights. In our DEL\nbenchmark, we evaluate localization by measuring how much dropping out\nidentified neurons deletes a memorized pretrained sequence. Despite their\ndifferent perspectives, our two benchmarks yield consistent rankings of five\nlocalization methods. Methods adapted from network pruning perform well on both\nbenchmarks, and all evaluated methods show promising localization ability. On\nthe other hand, even successful methods identify neurons that are not specific\nto a single memorized sequence.", "published": "2023-11-15 15:52:40", "link": "http://arxiv.org/abs/2311.09060v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Self-Disclosures of Use, Misuse and Addiction in\n  Community-based Social Media Posts", "abstract": "In the last decade, the United States has lost more than 500,000 people from\nan overdose involving prescription and illicit opioids making it a national\npublic health emergency (USDHHS, 2017). Medical practitioners require robust\nand timely tools that can effectively identify at-risk patients.\nCommunity-based social media platforms such as Reddit allow self-disclosure for\nusers to discuss otherwise sensitive drug-related behaviors. We present a\nmoderate size corpus of 2500 opioid-related posts from various subreddits\nlabeled with six different phases of opioid use: Medical Use, Misuse,\nAddiction, Recovery, Relapse, Not Using. For every post, we annotate span-level\nextractive explanations and crucially study their role both in annotation\nquality and model development. We evaluate several state-of-the-art models in a\nsupervised, few-shot, or zero-shot setting. Experimental results and error\nanalysis show that identifying the phases of opioid use disorder is highly\ncontextual and challenging. However, we find that using explanations during\nmodeling leads to a significant boost in classification accuracy demonstrating\ntheir beneficial role in a high-stakes domain such as studying the opioid use\ndisorder continuum.", "published": "2023-11-15 16:05:55", "link": "http://arxiv.org/abs/2311.09066v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Bias Probing: Fairness Benchmarking for Language Models", "abstract": "While the impact of social biases in language models has been recognized,\nprior methods for bias evaluation have been limited to binary association tests\non small datasets, limiting our understanding of bias complexities. This paper\nproposes a novel framework for probing language models for social biases by\nassessing disparate treatment, which involves treating individuals differently\naccording to their affiliation with a sensitive demographic group. We curate\nSoFa, a large-scale benchmark designed to address the limitations of existing\nfairness collections. SoFa expands the analysis beyond the binary comparison of\nstereotypical versus anti-stereotypical identities to include a diverse range\nof identities and stereotypes. Comparing our methodology with existing\nbenchmarks, we reveal that biases within language models are more nuanced than\nacknowledged, indicating a broader scope of encoded biases than previously\nrecognized. Benchmarking LMs on SoFa, we expose how identities expressing\ndifferent religions lead to the most pronounced disparate treatments across all\nmodels. Finally, our findings indicate that real-life adversities faced by\nvarious groups such as women and people with disabilities are mirrored in the\nbehavior of these models.", "published": "2023-11-15 16:35:59", "link": "http://arxiv.org/abs/2311.09090v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defending Large Language Models Against Jailbreaking Attacks Through\n  Goal Prioritization", "abstract": "While significant attention has been dedicated to exploiting weaknesses in\nLLMs through jailbreaking attacks, there remains a paucity of effort in\ndefending against these attacks. We point out a pivotal factor contributing to\nthe success of jailbreaks: the intrinsic conflict between the goals of being\nhelpful and ensuring safety. Accordingly, we propose to integrate goal\nprioritization at both training and inference stages to counteract.\nImplementing goal prioritization during inference substantially diminishes the\nAttack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And\nintegrating goal prioritization into model training reduces the ASR from 71.0%\nto 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking\nsamples are included during training, our approach slashes the ASR by half.\nAdditionally, our findings reveal that while stronger LLMs face greater safety\nrisks, they also possess a greater capacity to be steered towards defending\nagainst such attacks, both because of their stronger ability in instruction\nfollowing. Our work thus contributes to the comprehension of jailbreaking\nattacks and defenses, and sheds light on the relationship between LLMs'\ncapability and safety. Our code is available at\n\\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.", "published": "2023-11-15 16:42:29", "link": "http://arxiv.org/abs/2311.09096v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding\n  Dataset with Event Argument Annotation", "abstract": "Understanding events in texts is a core objective of natural language\nunderstanding, which requires detecting event occurrences, extracting event\narguments, and analyzing inter-event relationships. However, due to the\nannotation challenges brought by task complexity, a large-scale dataset\ncovering the full process of event understanding has long been absent. In this\npaper, we introduce MAVEN-Arg, which augments MAVEN datasets with event\nargument annotations, making the first all-in-one dataset supporting event\ndetection, event argument extraction (EAE), and event relation extraction. As\nan EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive\nschema covering 162 event types and 612 argument roles, all with expert-written\ndefinitions and examples; (2) a large data scale, containing 98,591 events and\n290,613 arguments obtained with laborious human annotation; (3) the exhaustive\nannotation supporting all task variants of EAE, which annotates both entity and\nnon-entity event arguments in document level. Experiments indicate that\nMAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary\nlarge language models (LLMs). Furthermore, to demonstrate the benefits of an\nall-in-one dataset, we preliminarily explore a potential application, future\nevent prediction, with LLMs. MAVEN-Arg and codes can be obtained from\nhttps://github.com/THU-KEG/MAVEN-Argument.", "published": "2023-11-15 16:52:14", "link": "http://arxiv.org/abs/2311.09105v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"We Demand Justice!\": Towards Social Context Grounding of Political\n  Texts", "abstract": "Social media discourse frequently consists of 'seemingly similar language\nused by opposing sides of the political spectrum', often translating to starkly\ncontrasting perspectives. E.g., 'thoughts and prayers', could express sympathy\nfor mass-shooting victims, or criticize the lack of legislative action on the\nissue. This paper defines the context required to fully understand such\nambiguous statements in a computational setting and ground them in real-world\nentities, actions, and attitudes. We propose two challenging datasets that\nrequire an understanding of the real-world context of the text. We benchmark\nthese datasets against models built upon large pre-trained models, such as\nRoBERTa and GPT-3. Additionally, we develop and benchmark more structured\nmodels building upon existing Discourse Contextualization Framework and\nPolitical Actor Representation models. We analyze the datasets and the\npredictions to obtain further insights into the pragmatic language\nunderstanding challenges posed by the proposed social grounding tasks.", "published": "2023-11-15 16:53:35", "link": "http://arxiv.org/abs/2311.09106v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal NER: A Gold-Standard Multilingual Named Entity Recognition\n  Benchmark", "abstract": "We introduce Universal NER (UNER), an open, community-driven project to\ndevelop gold-standard NER benchmarks in many languages. The overarching goal of\nUNER is to provide high-quality, cross-lingually consistent annotations to\nfacilitate and standardize multilingual NER research. UNER v1 contains 18\ndatasets annotated with named entities in a cross-lingual consistent schema\nacross 12 diverse languages. In this paper, we detail the dataset creation and\ncomposition of UNER; we also provide initial modeling baselines on both\nin-language and cross-lingual learning settings. We release the data, code, and\nfitted models to the public.", "published": "2023-11-15 17:09:54", "link": "http://arxiv.org/abs/2311.09122v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Social Meme-ing: Measuring Linguistic Variation in Memes", "abstract": "Much work in the space of NLP has used computational methods to explore\nsociolinguistic variation in text. In this paper, we argue that memes, as\nmultimodal forms of language comprised of visual templates and text, also\nexhibit meaningful social variation. We construct a computational pipeline to\ncluster individual instances of memes into templates and semantic variables,\ntaking advantage of their multimodal structure in doing so. We apply this\nmethod to a large collection of meme images from Reddit and make available the\nresulting \\textsc{SemanticMemes} dataset of 3.8M images clustered by their\nsemantic function. We use these clusters to analyze linguistic variation in\nmemes, discovering not only that socially meaningful variation in meme usage\nexists between subreddits, but that patterns of meme innovation and\nacculturation within these communities align with previous findings on written\nlanguage.", "published": "2023-11-15 17:20:20", "link": "http://arxiv.org/abs/2311.09130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Neural Machine Translation Models: Human Feedback in Training\n  and Inference", "abstract": "Reinforcement learning from human feedback (RLHF) is a recent technique to\nimprove the quality of the text generated by a language model, making it closer\nto what humans would generate. A core ingredient in RLHF's success in aligning\nand improving large language models (LLMs) is its reward model, trained using\nhuman feedback on model outputs. In machine translation (MT), where metrics\ntrained from human annotations can readily be used as reward models, recent\nmethods using minimum Bayes risk decoding and reranking have succeeded in\nimproving the final quality of translation. In this study, we comprehensively\nexplore and compare techniques for integrating quality metrics as reward models\ninto the MT pipeline. This includes using the reward model for data filtering,\nduring the training phase through RL, and at inference time by employing\nreranking techniques, and we assess the effects of combining these in a unified\napproach. Our experimental results, conducted across multiple translation\ntasks, underscore the crucial role of effective data filtering, based on\nestimated quality, in harnessing the full potential of RL in enhancing MT\nquality. Furthermore, our findings demonstrate the effectiveness of combining\nRL training with reranking techniques, showcasing substantial improvements in\ntranslation quality.", "published": "2023-11-15 17:21:58", "link": "http://arxiv.org/abs/2311.09132v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rescue: Ranking LLM Responses with Partial Ordering to Improve Response\n  Generation", "abstract": "Customizing LLMs for a specific task involves separating high-quality\nresponses from lower-quality ones. This skill can be developed using supervised\nfine-tuning with extensive human preference data. However, obtaining a large\nvolume of expert-annotated data is costly for most tasks. In this paper, we\nexplore a novel method to optimize LLMs using ranking metrics. This method\ntrains the model to prioritize the best responses from a pool of candidates\ncreated for a particular task. Rather than a traditional full ordering, we\nadvocate for a partial ordering, as achieving consensus on the perfect order of\ncandidate responses can be challenging. Our partial ordering is more robust,\nless sensitive to noise, and can be achieved with limited human annotations or\nthrough heuristic methods. We test our system's improved response generation\nability using benchmark datasets, including textual entailment and\nmulti-document question answering. We conduct ablation studies to understand\ncrucial factors, such as how to gather candidate responses for a specific task,\ndetermine their most suitable order, and balance supervised fine-tuning with\nranking metrics. Our approach, named Rescue, offers a promising avenue for\nenhancing the response generation and task accuracy of LLMs.", "published": "2023-11-15 17:27:14", "link": "http://arxiv.org/abs/2311.09136v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models", "abstract": "We are currently in an era of fierce competition among various large language\nmodels (LLMs) continuously pushing the boundaries of benchmark performance.\nHowever, genuinely assessing the capabilities of these LLMs has become a\nchallenging and critical issue due to potential data contamination, and it\nwastes dozens of time and effort for researchers and engineers to download and\ntry those contaminated models. To save our precious time, we propose a novel\nand useful method, Clean-Eval, which mitigates the issue of data contamination\nand evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to\nparaphrase and back-translate the contaminated data into a candidate set,\ngenerating expressions with the same meaning but in different surface forms. A\nsemantic detector is then used to filter the generated low-quality samples to\nnarrow down this candidate set. The best candidate is finally selected from\nthis set based on the BLEURT score. According to human assessment, this best\ncandidate is semantically similar to the original contamination data but\nexpressed differently. All candidates can form a new benchmark to evaluate the\nmodel. Our experiments illustrate that Clean-Eval substantially restores the\nactual evaluation results on contaminated LLMs under both few-shot learning and\nfine-tuning scenarios.", "published": "2023-11-15 17:50:30", "link": "http://arxiv.org/abs/2311.09154v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SiRA: Sparse Mixture of Low Rank Adaptation", "abstract": "Parameter Efficient Tuning has been an prominent approach to adapt the Large\nLanguage Model to downstream tasks. Most previous works considers adding the\ndense trainable parameters, where all parameters are used to adapt certain\ntask. We found this less effective empirically using the example of LoRA that\nintroducing more trainable parameters does not help. Motivated by this we\ninvestigate the importance of leveraging \"sparse\" computation and propose SiRA:\nsparse mixture of low rank adaption. SiRA leverages the Sparse Mixture of\nExpert(SMoE) to boost the performance of LoRA. Specifically it enforces the top\n$k$ experts routing with a capacity limit restricting the maximum number of\ntokens each expert can process. We propose a novel and simple expert dropout on\ntop of gating network to reduce the over-fitting issue. Through extensive\nexperiments, we verify SiRA performs better than LoRA and other mixture of\nexpert approaches across different single tasks and multitask settings.", "published": "2023-11-15 18:15:37", "link": "http://arxiv.org/abs/2311.09179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ContraDoc: Understanding Self-Contradictions in Documents with Large\n  Language Models", "abstract": "In recent times, large language models (LLMs) have shown impressive\nperformance on various document-level tasks such as document classification,\nsummarization, and question-answering. However, research on understanding their\ncapabilities on the task of self-contradictions in long documents has been very\nlimited. In this work, we introduce ContraDoc, the first human-annotated\ndataset to study self-contradictions in long documents across multiple domains,\nvarying document lengths, self-contradictions types, and scope. We then analyze\nthe current capabilities of four state-of-the-art open-source and commercially\navailable LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4\nperforms the best and can outperform humans on this task, we find that it is\nstill unreliable and struggles with self-contradictions that require more\nnuance and context. We release the dataset and all the code associated with the\nexperiments (https://github.com/ddhruvkr/CONTRADOC).", "published": "2023-11-15 18:23:17", "link": "http://arxiv.org/abs/2311.09182v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PsyEval: A Suite of Mental Health Related Tasks for Evaluating Large\n  Language Models", "abstract": "Evaluating Large Language Models (LLMs) in the mental health domain poses\ndistinct challenged from other domains, given the subtle and highly subjective\nnature of symptoms that exhibit significant variability among individuals. This\npaper presents PsyEval, the first comprehensive suite of mental health-related\ntasks for evaluating LLMs. PsyEval encompasses five sub-tasks that evaluate\nthree critical dimensions of mental health. This comprehensive framework is\ndesigned to thoroughly assess the unique challenges and intricacies of mental\nhealth-related tasks, making PsyEval a highly specialized and valuable tool for\nevaluating LLM performance in this domain. We evaluate twelve advanced LLMs\nusing PsyEval. Experiment results not only demonstrate significant room for\nimprovement in current LLMs concerning mental health but also unveil potential\ndirections for future model optimization.", "published": "2023-11-15 18:32:27", "link": "http://arxiv.org/abs/2311.09189v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Priming Demonstrates Abstract Grammatical Representations in\n  Multilingual Language Models", "abstract": "Abstract grammatical knowledge - of parts of speech and grammatical patterns\n- is key to the capacity for linguistic generalization in humans. But how\nabstract is grammatical knowledge in large language models? In the human\nliterature, compelling evidence for grammatical abstraction comes from\nstructural priming. A sentence that shares the same grammatical structure as a\npreceding sentence is processed and produced more readily. Because confounds\nexist when using stimuli in a single language, evidence of abstraction is even\nmore compelling from crosslingual structural priming, where use of a syntactic\nstructure in one language primes an analogous structure in another language. We\nmeasure crosslingual structural priming in large language models, comparing\nmodel behavior to human experimental results from eight crosslingual\nexperiments covering six languages, and four monolingual structural priming\nexperiments in three non-English languages. We find evidence for abstract\nmonolingual and crosslingual grammatical representations in the models that\nfunction similarly to those found in humans. These results demonstrate that\ngrammatical representations in multilingual language models are not only\nsimilar across languages, but they can causally influence text produced in\ndifferent languages.", "published": "2023-11-15 18:39:56", "link": "http://arxiv.org/abs/2311.09194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Is Multilinguality a Curse? Language Modeling for 250 High- and\n  Low-Resource Languages", "abstract": "Multilingual language models are widely used to extend NLP systems to\nlow-resource languages. However, concrete evidence for the effects of\nmultilinguality on language modeling performance in individual languages\nremains scarce. Here, we pre-train over 10,000 monolingual and multilingual\nlanguage models for over 250 languages, including multiple language families\nthat are under-studied in NLP. We assess how language modeling performance in\neach language varies as a function of (1) monolingual dataset size, (2) added\nmultilingual dataset size, (3) linguistic similarity of the added languages,\nand (4) model size (up to 45M parameters). We find that in moderation, adding\nmultilingual data improves low-resource language modeling performance, similar\nto increasing low-resource dataset sizes by up to 33%. Improvements depend on\nthe syntactic similarity of the added multilingual data, with marginal\nadditional effects of vocabulary overlap. However, high-resource languages\nconsistently perform worse in multilingual pre-training scenarios. As dataset\nsizes increase, adding multilingual data begins to hurt performance for both\nlow-resource and high-resource languages, likely due to limited model capacity\n(the \"curse of multilinguality\"). These results suggest that massively\nmultilingual pre-training may not be optimal for any languages involved, but\nthat more targeted models can significantly improve performance.", "published": "2023-11-15 18:47:42", "link": "http://arxiv.org/abs/2311.09205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GENEVA: GENErating and Visualizing branching narratives using LLMs", "abstract": "Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The\nnarratives of these may take years to write and typically involve a large\ncreative team. In this work, we demonstrate the potential of large generative\ntext models to assist this process. \\textbf{GENEVA}, a prototype tool,\ngenerates a rich narrative graph with branching and reconverging storylines\nthat match a high-level narrative description and constraints provided by the\ndesigner. A large language model (LLM), GPT-4, is used to generate the\nbranching narrative and to render it in a graph format in a two-step process.\nWe illustrate the use of GENEVA in generating new branching narratives for four\nwell-known stories under different contextual constraints. This tool has the\npotential to assist in game development, simulations, and other applications\nwith game-like properties.", "published": "2023-11-15 18:55:45", "link": "http://arxiv.org/abs/2311.09213v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive\n  Thinking from Large Language Models", "abstract": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing. However, the massive scale and computational demands of\nthese models present formidable challenges when considering their practical\ndeployment in resource-constrained environments. While techniques such as\nchain-of-thought (CoT) distillation have displayed promise in distilling LLMs\ninto small language models (SLMs), there is a risk that distilled SLMs may\nstill inherit flawed reasoning and hallucinations from LLMs. To address these\nissues, we propose a twofold methodology: First, we introduce a novel method\nfor distilling the self-evaluation capability from LLMs into SLMs, aiming to\nmitigate the adverse effects of flawed reasoning and hallucinations inherited\nfrom LLMs. Second, we advocate for distilling more comprehensive thinking by\nincorporating multiple distinct CoTs and self-evaluation outputs, to ensure a\nmore thorough and robust knowledge transfer into SLMs. Experiments on three NLP\nbenchmarks demonstrate that our method significantly improves the performance\nof distilled SLMs, offering a new perspective for developing more effective and\nefficient SLMs in resource-constrained environments.", "published": "2023-11-15 18:56:23", "link": "http://arxiv.org/abs/2311.09214v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Chain-of-Thought Prompting", "abstract": "Despite the success of chain of thought in enhancing language model\nreasoning, the underlying process remains less well understood. Although\nlogically sound reasoning appears inherently crucial for chain of thought,\nprior studies surprisingly reveal minimal impact when using invalid\ndemonstrations instead. Furthermore, the conventional chain of thought does not\ninform language models on what mistakes to avoid, which potentially leads to\nmore errors. Hence, inspired by how humans can learn from both positive and\nnegative examples, we propose contrastive chain of thought to enhance language\nmodel reasoning. Compared to the conventional chain of thought, our approach\nprovides both valid and invalid reasoning demonstrations, to guide the model to\nreason step-by-step while reducing reasoning mistakes. To improve\ngeneralization, we introduce an automatic method to construct contrastive\ndemonstrations. Our experiments on reasoning benchmarks demonstrate that\ncontrastive chain of thought can serve as a general enhancement of\nchain-of-thought prompting.", "published": "2023-11-15 18:54:01", "link": "http://arxiv.org/abs/2311.09277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMRefine: Pinpointing and Refining Large Language Models via\n  Fine-Grained Actionable Feedback", "abstract": "Recent large language models (LLM) are leveraging human feedback to improve\ntheir generation quality. However, human feedback is costly to obtain,\nespecially during inference. In this work, we propose LLMRefine, an inference\ntime optimization method to refine LLM's output. The core idea is to use a\nlearned fine-grained feedback model to pinpoint defects and guide LLM to refine\nthem iteratively. Using original LLM as a proposal of edits, LLMRefine searches\nfor defect-less text via simulated annealing, trading off the exploration and\nexploitation. We conduct experiments on three text generation tasks, including\nmachine translation, long-form question answering (QA), and topical\nsummarization. LLMRefine consistently outperforms all baseline approaches,\nachieving improvements up to 1.7 MetricX points on translation tasks, 8.1\nROUGE-L on ASQA, 2.2 ROUGE-L on topical summarization.", "published": "2023-11-15 19:52:11", "link": "http://arxiv.org/abs/2311.09336v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language and Task Arithmetic with Parameter-Efficient Layers for\n  Zero-Shot Summarization", "abstract": "Parameter-efficient fine-tuning (PEFT) using labeled task data can\nsignificantly improve the performance of large language models (LLMs) on the\ndownstream task. However, there are 7000 languages in the world and many of\nthese languages lack labeled data for real-world language generation tasks. In\nthis paper, we propose to improve zero-shot cross-lingual transfer by composing\nlanguage or task specialized parameters. Our method composes language and task\nPEFT modules via element-wise arithmetic operations to leverage unlabeled data\nand English labeled data. We extend our approach to cases where labeled data\nfrom more languages is available and propose to arithmetically compose PEFT\nmodules trained on languages related to the target. Empirical results on\nsummarization demonstrate that our method is an effective strategy that obtains\nconsistent gains using minimal training of PEFT modules.", "published": "2023-11-15 20:04:58", "link": "http://arxiv.org/abs/2311.09344v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LePaRD: A Large-Scale Dataset of Judges Citing Precedents", "abstract": "We present the Legal Passage Retrieval Dataset LePaRD. LePaRD is a massive\ncollection of U.S. federal judicial citations to precedent in context. The\ndataset aims to facilitate work on legal passage prediction, a challenging\npractice-oriented legal retrieval and reasoning task. Legal passage prediction\nseeks to predict relevant passages from precedential court decisions given the\ncontext of a legal argument. We extensively evaluate various retrieval\napproaches on LePaRD, and find that classification appears to work best.\nHowever, we note that legal precedent prediction is a difficult task, and there\nremains significant room for improvement. We hope that by publishing LePaRD, we\nwill encourage others to engage with a legal NLP task that promises to help\nexpand access to justice by reducing the burden associated with legal research.\nA subset of the LePaRD dataset is freely available and the whole dataset will\nbe released upon publication.", "published": "2023-11-15 20:33:27", "link": "http://arxiv.org/abs/2311.09356v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Emergent Audio Classification Ability of ASR\n  Foundation Models", "abstract": "Text and vision foundation models can perform many tasks in a zero-shot\nsetting, a desirable property that enables these systems to be applied in\ngeneral and low-resource settings. There has been far less work, however, on\nthe zero-shot abilities of ASR foundation models, with these systems typically\nfine-tuned to specific tasks or constrained to applications that match their\ntraining criterion and data annotation. In this work we investigate the ability\nof Whisper and MMS, ASR foundation models trained primarily for speech\nrecognition, to perform zero-shot audio classification. We use simple\ntemplate-based text prompts at the decoder and use the resulting decoding\nprobabilities to generate zero-shot predictions. Without training the model on\nextra data or adding any new parameters, we demonstrate that Whisper shows\npromising zero-shot classification performance on a range of 8\naudio-classification datasets, outperforming the accuracy of existing\nstate-of-the-art zero-shot baselines by an average of 9%. One important step to\nunlock the emergent ability is debiasing, where a simple unsupervised\nreweighting method of the class probabilities yields consistent significant\nperformance gains. We further show that performance increases with model size,\nimplying that as ASR foundation models scale up, they may exhibit improved\nzero-shot performance.", "published": "2023-11-15 20:52:56", "link": "http://arxiv.org/abs/2311.09363v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Online User Aggression: Content Detection and Behavioral\n  Analysis on Social Media", "abstract": "The rise of social media platforms has led to an increase in cyber-aggressive\nbehavior, encompassing a broad spectrum of hostile behavior, including\ncyberbullying, online harassment, and the dissemination of offensive and hate\nspeech. These behaviors have been associated with significant societal\nconsequences, ranging from online anonymity to real-world outcomes such as\ndepression, suicidal tendencies, and, in some instances, offline violence.\nRecognizing the societal risks associated with unchecked aggressive content,\nthis paper delves into the field of Aggression Content Detection and Behavioral\nAnalysis of Aggressive Users, aiming to bridge the gap between disparate\nstudies. In this paper, we analyzed the diversity of definitions and proposed a\nunified cyber-aggression definition. We examine the comprehensive process of\nAggression Content Detection, spanning from dataset creation, feature selection\nand extraction, and detection algorithm development. Further, we review studies\non Behavioral Analysis of Aggression that explore the influencing factors,\nconsequences, and patterns associated with cyber-aggressive behavior. This\nsystematic literature review is a cross-examination of content detection and\nbehavioral analysis in the realm of cyber-aggression. The integrated\ninvestigation reveals the effectiveness of incorporating sociological insights\ninto computational techniques for preventing cyber-aggressive behavior.\nFinally, the paper concludes by identifying research gaps and encouraging\nfurther progress in the unified domain of socio-computational aggressive\nbehavior analysis.", "published": "2023-11-15 20:59:13", "link": "http://arxiv.org/abs/2311.09367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue\n  systems", "abstract": "Linguistic entrainment, or alignment, represents a phenomenon where\nlinguistic patterns employed by conversational participants converge to one\nanother. While entrainment has been shown to produce a more natural user\nexperience, most dialogue systems do not have any provisions for it. In this\nwork, we introduce methods for achieving dialogue entrainment in a GPT-2-based\nend-to-end task-oriented dialogue system through the utilization of shared\nvocabulary. We experiment with training instance weighting,\nentrainment-specific loss, and additional conditioning to generate responses\nthat align with the user. We demonstrate that all three approaches produce\nsignificantly better entrainment than the base, non-entrainment-optimized\nmodel, as confirmed by both automated and manual evaluation metrics.", "published": "2023-11-15 21:35:25", "link": "http://arxiv.org/abs/2311.09390v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Translate or Not to Translate: A Systematic Investigation of\n  Translation-Based Cross-Lingual Transfer to Low-Resource Languages", "abstract": "Perfect machine translation (MT) would render cross-lingual transfer (XLT) by\nmeans of multilingual language models (mLMs) superfluous. Given, on the one\nhand, the large body of work on improving XLT with mLMs and, on the other hand,\nrecent advances in massively multilingual MT, in this work, we systematically\nevaluate existing and propose new translation-based XLT approaches for transfer\nto low-resource languages. We show that all translation-based approaches\ndramatically outperform zero-shot XLT with mLMs -- with the combination of\nround-trip translation of the source-language training data and the translation\nof the target-language test instances at inference -- being generally the most\neffective. We next show that one can obtain further empirical gains by adding\nreliable translations to other high-resource languages to the training data.\nMoreover, we propose an effective translation-based XLT strategy even for\nlanguages not supported by the MT system. Finally, we show that model selection\nfor XLT based on target-language validation data obtained with MT outperforms\nmodel selection based on the source-language data. We believe our findings\nwarrant a broader inclusion of more robust translation-based baselines in XLT\nresearch.", "published": "2023-11-15 22:03:28", "link": "http://arxiv.org/abs/2311.09404v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting generalization performance with correctness discriminators", "abstract": "The ability to predict an NLP model's accuracy on unseen, potentially\nout-of-distribution data is a prerequisite for trustworthiness. We present a\nnovel model that establishes upper and lower bounds on the accuracy, without\nrequiring gold labels for the unseen data. We achieve this by training a\ndiscriminator which predicts whether the output of a given sequence-to-sequence\nmodel is correct or not. We show across a variety of tagging, parsing, and\nsemantic parsing tasks that the gold accuracy is reliably between the predicted\nupper and lower bounds, and that these bounds are remarkably close together.", "published": "2023-11-15 22:43:42", "link": "http://arxiv.org/abs/2311.09422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset", "abstract": "Using novel approaches to dataset development, the Biasly dataset captures\nthe nuance and subtlety of misogyny in ways that are unique within the\nliterature. Built in collaboration with multi-disciplinary experts and\nannotators themselves, the dataset contains annotations of movie subtitles,\ncapturing colloquial expressions of misogyny in North American film. The\ndataset can be used for a range of NLP tasks, including classification,\nseverity score regression, and text generation for rewrites. In this paper, we\ndiscuss the methodology used, analyze the annotations obtained, and provide\nbaselines using common NLP algorithms in the context of misogyny detection and\nmitigation. We hope this work will promote AI for social good in NLP for bias\ndetection, explanation, and removal.", "published": "2023-11-15 23:27:19", "link": "http://arxiv.org/abs/2311.09443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of\n  Lexical Overlap in Train and Test Reference Summaries", "abstract": "Ideal summarization models should generalize to novel summary-worthy content\nwithout remembering reference training summaries by rote. However, a single\naverage performance score on the entire test set is inadequate in determining\nsuch model competencies. We propose a fine-grained evaluation protocol by\npartitioning a test set based on the lexical similarity of reference test\nsummaries with training summaries. We observe up to a 5x (1.2x) difference in\nROUGE-2 (entity recall) scores between the subsets with the lowest and highest\nsimilarity. Next, we show that such training repetitions also make a model\nvulnerable to rote learning, reproducing data artifacts such as factual errors,\nespecially when reference test summaries are lexically close to training\nsummaries. Consequently, we propose to limit lexical repetitions in training\nsummaries during both supervised fine-tuning and likelihood calibration stages\nto improve the performance on novel test cases while retaining average\nperformance. Our automatic and human evaluations on novel test subsets and\nrecent news articles show that limiting lexical repetitions in training\nsummaries can prevent rote learning and improve generalization.", "published": "2023-11-15 23:47:53", "link": "http://arxiv.org/abs/2311.09458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Machine Translation through Advanced In-Context Learning: A\n  Methodological Strategy for GPT-4 Improvement", "abstract": "The challenge of improving translation accuracy in GPT-4 is being addressed\nby harnessing a method known as in-context learning. This paper introduces a\nstrategic approach to utilize in-context learning specifically for machine\ntranslation, aiming to significantly boost accuracy. The crux of this method\nlies in the judicious selection of demonstrations that are most effective for\nin-context learning. By selecting these examples carefully, GPT-4 can utilize\nthem to achieve remarkably accurate machine translations, eliminating the need\nfor task-specific fine-tuning. This technique is anchored in the semantic\nsimilarities between the user's prompt and the chosen dataset. Sentences from\nthis dataset, carefully picked for their relevance and clarity, serve as potent\ndemonstrations for in-context learning. This approach not only enhances\ntranslation accuracy but also enriches the understanding of nuanced linguistic\nstructures. It represents a significant step forward in machine learning,\nleveraging the inherent capabilities of GPT-4 to provide translations that are\nnot only accurate but also contextually rich and linguistically sophisticated.\nThis method demonstrates the potential of in-context learning in overcoming\nlanguage barriers, opening new avenues for cross-cultural communication and\nglobal collaboration.", "published": "2023-11-15 10:28:28", "link": "http://arxiv.org/abs/2311.10765v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memory Augmented Language Models through Mixture of Word Experts", "abstract": "Scaling up the number of parameters of language models has proven to be an\neffective approach to improve performance. For dense models, increasing model\nsize proportionally increases the model's computation footprint. In this work,\nwe seek to aggressively decouple learning capacity and FLOPs through\nMixture-of-Experts (MoE) style models with large knowledge-rich vocabulary\nbased routing functions and experts. Our proposed approach, dubbed Mixture of\nWord Experts (MoWE), can be seen as a memory augmented model, where a large set\nof word-specific experts play the role of a sparse memory. We demonstrate that\nMoWE performs significantly better than the T5 family of models with similar\nnumber of FLOPs in a variety of NLP tasks. Additionally, MoWE outperforms\nregular MoE models on knowledge intensive tasks and has similar performance to\nmore complex memory augmented approaches that often require to invoke custom\nmechanisms to search the sparse memory.", "published": "2023-11-15 18:19:56", "link": "http://arxiv.org/abs/2311.10768v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded\n  Explanations in LLMs", "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage tasks, yet understanding their reasoning processes remains a\nsignificant challenge. We address this by introducing XplainLLM, a dataset\naccompanying an explanation framework designed to enhance LLM transparency and\nreliability. Our dataset comprises 24,204 instances where each instance\ninterprets the LLM's reasoning behavior using knowledge graphs (KGs) and graph\nattention networks (GAT), and includes explanations of LLMs such as the\ndecoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a\nframework for generating grounded explanations and the debugger-scores for\nmultidimensional quality analysis. Our explanations include why-choose and\nwhy-not-choose components, reason-elements, and debugger-scores that\ncollectively illuminate the LLM's reasoning behavior. Our evaluations\ndemonstrate XplainLLM's potential to reduce hallucinations and improve grounded\nexplanation generation in LLMs. XplainLLM is a resource for researchers and\npractitioners to build trust and verify the reliability of LLM outputs.", "published": "2023-11-15 00:34:28", "link": "http://arxiv.org/abs/2311.08614v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toucan: Token-Aware Character Level Language Modeling", "abstract": "Character-level language models obviate the need for separately trained\ntokenizers, but efficiency suffers from longer sequence lengths. Learning to\ncombine character representations into tokens has made training these models\nmore efficient, but they still require decoding characters individually. We\npropose Toucan, an augmentation to character-level models to make them\n\"token-aware\". Comparing our method to prior work, we demonstrate significant\nspeed-ups in character generation without a loss in language modeling\nperformance. We then explore differences between our learned dynamic\ntokenization of character sequences with popular fixed vocabulary solutions\nsuch as Byte-Pair Encoding and WordPiece, finding our approach leads to a\ngreater amount of longer sequences tokenized as single items. Our project and\ncode are available at https://nlp.jhu.edu/nuggets/.", "published": "2023-11-15 00:57:51", "link": "http://arxiv.org/abs/2311.08620v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multistage Collaborative Knowledge Distillation from a Large Language\n  Model for Semi-Supervised Sequence Generation", "abstract": "We study semi-supervised sequence generation tasks, where the few labeled\nexamples are too scarce to finetune a model, and meanwhile, few-shot prompted\nlarge language models (LLMs) exhibit room for improvement. In this paper, we\npresent the discovery that a student model distilled from a few-shot prompted\nLLM can commonly generalize better than its teacher to unseen examples on such\ntasks. We find that the student is able to learn a general pattern from the\nhigh-quality pseudolabels produced by the teacher during knowledge distillation\n(KD), and favorably not a general pattern from the low-quality pseudolables.\nLeveraging this discovery, we propose a new method, Multistage Collaborative\nKnowledge Distillation from an LLM (MCKD), for these tasks. MCKD first few-shot\nprompts an LLM to produce pseudolabels for unlabeled data. Then at each stage\nof an iterative KD process, a new pair of students is trained on disjoint\npartitions of the pseudolabeled data, and produces new and improved\npseudolabels for their unseen partitions. We conduct extensive experiments on\nfour syntactic and semantic parsing datasets and show the effectiveness of MCKD\nfor low-resource semi-supervised sequence generation. On CRAFT biomedical\nparsing, for example, 3-stage MCKD with 50 labeled examples outperforms an LLM\nteacher and vanilla KD by 7.5% and 3.7% parsing F1, respectively, and matches\nthe performance of supervised finetuning with 500 labeled examples.", "published": "2023-11-15 01:28:28", "link": "http://arxiv.org/abs/2311.08640v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explore Spurious Correlations at the Concept Level in Language Models\n  for Text Classification", "abstract": "Language models (LMs) have achieved notable success in numerous NLP tasks,\nemploying both fine-tuning and in-context learning (ICL) methods. While\nlanguage models demonstrate exceptional performance, they face robustness\nchallenges due to spurious correlations arising from imbalanced label\ndistributions in training data or ICL exemplars. Previous research has\nprimarily concentrated on word, phrase, and syntax features, neglecting the\nconcept level, often due to the absence of concept labels and difficulty in\nidentifying conceptual content in input texts. This paper introduces two main\ncontributions. First, we employ ChatGPT to assign concept labels to texts,\nassessing concept bias in models during fine-tuning or ICL on test data. We\nfind that LMs, when encountering spurious correlations between a concept and a\nlabel in training or prompts, resort to shortcuts for predictions. Second, we\nintroduce a data rebalancing technique that incorporates ChatGPT-generated\ncounterfactual data, thereby balancing label distribution and mitigating\nspurious correlations. Our method's efficacy, surpassing traditional token\nremoval approaches, is validated through extensive testing.", "published": "2023-11-15 01:58:54", "link": "http://arxiv.org/abs/2311.08648v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Calibration of Multilingual Question Answering LLMs", "abstract": "Multilingual pre-trained Large Language Models (LLMs) are incredibly\neffective at Question Answering (QA), a core task in Natural Language\nUnderstanding, achieving high accuracies on several multilingual benchmarks.\nHowever, little is known about how well their confidences are calibrated. In\nthis paper, we comprehensively benchmark the calibration of several\nmultilingual LLMs (MLLMs) on a variety of QA tasks. We perform extensive\nexperiments, spanning encoder-only, encoder-decoder, and decoder-only QA models\n(size varying from 110M to 7B parameters) and diverse languages, including both\nhigh- and low-resource ones. We study different dimensions of calibration in\nin-distribution, out-of-distribution, and cross-lingual transfer settings, and\ninvestigate strategies to improve it, including post-hoc methods and\nregularized fine-tuning. For decoder-only LLMs such as LlaMa2, we additionally\nfind that in-context learning improves confidence calibration on multilingual\ndata. We also conduct several ablation experiments to study the effect of\nlanguage distances, language corpus size, and model size on calibration, and\nhow multilingual models compare with their monolingual counterparts for diverse\ntasks and languages. Our experiments suggest that the multilingual QA models\nare poorly calibrated for languages other than English and incorporating a\nsmall set of cheaply translated multilingual samples during\nfine-tuning/calibration effectively enhances the calibration performance.", "published": "2023-11-15 03:29:02", "link": "http://arxiv.org/abs/2311.08669v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Safer-Instruct: Aligning Language Models with Automated Preference Data", "abstract": "Reinforcement learning from human feedback (RLHF) is a vital strategy for\nenhancing model capability in language models. However, annotating preference\ndata for RLHF is a resource-intensive and creativity-demanding process, while\nexisting automatic generation methods face limitations in data diversity and\nquality. In response, we present Safer-Instruct, a novel pipeline for\nautomatically constructing large-scale preference data. Our approach leverages\nreversed instruction tuning, instruction induction, and expert model evaluation\nto efficiently generate high-quality preference data without human annotators.\nTo verify the effectiveness of Safer-Instruct, we apply the pipeline to\nconstruct a safety preference dataset as a case study. Finetuning an Alpaca\nmodel on this synthetic dataset not only demonstrates improved harmlessness but\nalso outperforms models fine-tuned on human-annotated safety preference data,\nall the while maintaining a competitive edge in downstream tasks. Importantly,\nour Safer-Instruct framework is versatile and can be applied to generate\npreference data across various domains, extending its utility beyond safety\npreferences. It addresses the challenges in preference data acquisition and\nadvances the development of more capable and responsible AI systems. For\ndataset and code implementation, see\nhttps://github.com/uscnlp-lime/safer-instruct", "published": "2023-11-15 04:22:22", "link": "http://arxiv.org/abs/2311.08685v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Routing to the Expert: Efficient Reward-guided Ensemble of Large\n  Language Models", "abstract": "The complementary potential of Large Language Models (LLM) assumes\noff-the-shelf LLMs have heterogeneous expertise in a wide range of domains and\ntasks so that an ensemble of LLMs can achieve consistently better performance.\nExisting ensemble methods for LLMs mainly focus on reward model ranking of\noutputs, leading to significant computation overhead. To combat this issue, we\nrevisit the complementary potential of LLMs and further elaborate it by mining\nlatent expertise with off-the-shelf reward models. We propose Zooter, a\nreward-guided routing method distilling rewards on training queries to train a\nrouting function, which can precisely distribute each query to the LLM with\nexpertise about it. We also integrate a tag-based label enhancement to mitigate\nnoise from uncertainty when using rewards as silver supervision. Zooter shows\ncomputation efficiency in inference as it introduces only a minor computation\noverhead of a routing function compared with reward model ranking methods. We\nevaluate Zooter on a comprehensive benchmark collection with 26 subsets on\ndifferent domains and tasks. Zooter outperforms the best single model on\naverage and ranks first on 44% of tasks, even surpassing multiple reward model\nranking methods.", "published": "2023-11-15 04:40:43", "link": "http://arxiv.org/abs/2311.08692v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Debate Helps Supervise Unreliable Experts", "abstract": "As AI systems are used to answer more difficult questions and potentially\nhelp create new knowledge, judging the truthfulness of their outputs becomes\nmore difficult and more important. How can we supervise unreliable experts,\nwhich have access to the truth but may not accurately report it, to give\nanswers that are systematically true and don't just superficially seem true,\nwhen the supervisor can't tell the difference between the two on their own? In\nthis work, we show that debate between two unreliable experts can help a\nnon-expert judge more reliably identify the truth. We collect a dataset of\nhuman-written debates on hard reading comprehension questions where the judge\nhas not read the source passage, only ever seeing expert arguments and short\nquotes selectively revealed by 'expert' debaters who have access to the\npassage. In our debates, one expert argues for the correct answer, and the\nother for an incorrect answer. Comparing debate to a baseline we call\nconsultancy, where a single expert argues for only one answer which is correct\nhalf of the time, we find that debate performs significantly better, with 84%\njudge accuracy compared to consultancy's 74%. Debates are also more efficient,\nbeing 68% of the length of consultancies. By comparing human to AI debaters, we\nfind evidence that with more skilled (in this case, human) debaters, the\nperformance of debate goes up but the performance of consultancy goes down. Our\nerror analysis also supports this trend, with 46% of errors in human debate\nattributable to mistakes by the honest debater (which should go away with\nincreased skill); whereas 52% of errors in human consultancy are due to\ndebaters obfuscating the relevant evidence from the judge (which should become\nworse with increased skill). Overall, these results show that debate is a\npromising approach for supervising increasingly capable but potentially\nunreliable AI systems.", "published": "2023-11-15 05:05:40", "link": "http://arxiv.org/abs/2311.08702v1", "categories": ["cs.AI", "cs.CL", "I.2.0"], "primary_category": "cs.AI"}
{"title": "Can Large Language Models Follow Concept Annotation Guidelines? A Case\n  Study on Scientific and Financial Domains", "abstract": "Although large language models (LLMs) exhibit remarkable capacity to leverage\nin-context demonstrations, it is still unclear to what extent they can learn\nnew concepts or facts from ground-truth labels. To address this question, we\nexamine the capacity of instruction-tuned LLMs to follow in-context concept\nguidelines for sentence labeling tasks. We design guidelines that present\ndifferent types of factual and counterfactual concept definitions, which are\nused as prompts for zero-shot sentence classification tasks. Our results show\nthat although concept definitions consistently help in task performance, only\nthe larger models (with 70B parameters or more) have limited ability to work\nunder counterfactual contexts. Importantly, only proprietary models such as\nGPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is\ndue to more sophisticated alignment methods. Finally, we find that\nFalcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which\nindicates that careful fine-tuning is more effective than increasing model\nscale. Altogether, our simple evaluation method reveals significant gaps in\nconcept understanding between the most capable open-source language models and\nthe leading proprietary APIs.", "published": "2023-11-15 05:11:26", "link": "http://arxiv.org/abs/2311.08704v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Construction in Power Distribution Networks", "abstract": "In this paper, we propose a method for knowledge graph construction in power\ndistribution networks. This method leverages entity features, which involve\ntheir semantic, phonetic, and syntactic characteristics, in both the knowledge\ngraph of distribution network and the dispatching texts. An enhanced model\nbased on Convolutional Neural Network, is utilized for effectively matching\ndispatch text entities with those in the knowledge graph. The effectiveness of\nthis model is evaluated through experiments in real-world power distribution\ndispatch scenarios. The results indicate that, compared with the baselines, the\nproposed model excels in linking a variety of entity types, demonstrating high\noverall accuracy in power distribution knowledge graph construction task.", "published": "2023-11-15 06:35:01", "link": "http://arxiv.org/abs/2311.08724v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "German FinBERT: A German Pre-trained Language Model", "abstract": "This study presents German FinBERT, a novel pre-trained German language model\ntailored for financial textual data. The model is trained through a\ncomprehensive pre-training process, leveraging a substantial corpus comprising\nfinancial reports, ad-hoc announcements and news related to German companies.\nThe corpus size is comparable to the data sets commonly used for training\nstandard BERT models. I evaluate the performance of German FinBERT on\ndownstream tasks, specifically sentiment prediction, topic recognition and\nquestion answering against generic German language models. My results\ndemonstrate improved performance on finance-specific data, indicating the\nefficacy of German FinBERT in capturing domain-specific nuances. The presented\nfindings suggest that German FinBERT holds promise as a valuable tool for\nfinancial text analysis, potentially benefiting various applications in the\nfinancial domain.", "published": "2023-11-15 09:07:29", "link": "http://arxiv.org/abs/2311.08793v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Evaluating Gender Bias in the Translation of Gender-Neutral Languages\n  into English", "abstract": "Machine Translation (MT) continues to improve in quality and adoption, yet\nthe inadvertent perpetuation of gender bias remains a significant concern.\nDespite numerous studies into gender bias in translations from gender-neutral\nlanguages such as Turkish into more strongly gendered languages like English,\nthere are no benchmarks for evaluating this phenomenon or for assessing\nmitigation strategies. To address this gap, we introduce GATE X-E, an extension\nto the GATE (Rarrick et al., 2023) corpus, that consists of human translations\nfrom Turkish, Hungarian, Finnish, and Persian into English. Each translation is\naccompanied by feminine, masculine, and neutral variants for each possible\ngender interpretation. The dataset, which contains between 1250 and 1850\ninstances for each of the four language pairs, features natural sentences with\na wide range of sentence lengths and domains, challenging translation rewriters\non various linguistic phenomena. Additionally, we present an English gender\nrewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We\nopen source our contributions to encourage further research on gender\ndebiasing.", "published": "2023-11-15 10:25:14", "link": "http://arxiv.org/abs/2311.08836v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Violet: A Vision-Language Model for Arabic Image Captioning with Gemini\n  Decoder", "abstract": "Although image captioning has a vast array of applications, it has not\nreached its full potential in languages other than English. Arabic, for\ninstance, although the native language of more than 400 million people, remains\nlargely underrepresented in this area. This is due to the lack of labeled data\nand powerful Arabic generative models. We alleviate this issue by presenting a\nnovel vision-language model dedicated to Arabic, dubbed \\textit{Violet}. Our\nmodel is based on a vision encoder and a Gemini text decoder that maintains\ngeneration fluency while allowing fusion between the vision and language\ncomponents. To train our model, we introduce a new method for automatically\nacquiring data from available English datasets. We also manually prepare a new\ndataset for evaluation. \\textit{Violet} performs sizeably better than our\nbaselines on all of our evaluation datasets. For example, it reaches a CIDEr\nscore of $61.2$ on our manually annotated dataset and achieves an improvement\nof $13$ points on Flickr8k.", "published": "2023-11-15 10:34:14", "link": "http://arxiv.org/abs/2311.08844v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Llamas Know What GPTs Don't Show: Surrogate Models for Confidence\n  Estimation", "abstract": "To maintain user trust, large language models (LLMs) should signal low\nconfidence on examples where they are incorrect, instead of misleading the\nuser. The standard approach of estimating confidence is to use the softmax\nprobabilities of these models, but as of November 2023, state-of-the-art LLMs\nsuch as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We\nfirst study eliciting confidence linguistically -- asking an LLM for its\nconfidence in its answer -- which performs reasonably (80.5% AUC on GPT-4\naveraged across 12 question-answering datasets -- 7% above a random baseline)\nbut leaves room for improvement. We then explore using a surrogate confidence\nmodel -- using a model where we do have probabilities to evaluate the original\nmodel's confidence in a given question. Surprisingly, even though these\nprobabilities come from a different and often weaker model, this method leads\nto higher AUC than linguistic confidences on 9 out of 12 datasets. Our best\nmethod composing linguistic confidences and surrogate model probabilities gives\nstate-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on\nGPT-4).", "published": "2023-11-15 11:27:44", "link": "http://arxiv.org/abs/2311.08877v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing\n  Supervised Models with In-Context Learning", "abstract": "Existing Knowledge Base Question Answering (KBQA) architectures are hungry\nfor annotated data, which make them costly and time-consuming to deploy. We\nintroduce the problem of few-shot transfer learning for KBQA, where the target\ndomain offers only a few labeled examples, but a large labeled training dataset\nis available in a source domain. We propose a novel KBQA architecture called\nFuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers,\nre-ranks using an LLM and uses this as input for LLM few-shot in-context\nlearning to generate logical forms. These are further refined using\nexecution-guided feedback. Experiments over multiple source-target KBQA pairs\nof varying complexity show that FuSIC-KBQA significantly outperforms\nadaptations of SoTA KBQA models for this setting. Additional experiments show\nthat FuSIC-KBQA also outperforms SoTA KBQA models in the in-domain setting when\ntraining data is limited.", "published": "2023-11-15 11:56:56", "link": "http://arxiv.org/abs/2311.08894v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformers in the Service of Description Logic-based Contexts", "abstract": "Recent advancements in transformer-based models have initiated research\ninterests in investigating their ability to learn to perform reasoning tasks.\nHowever, most of the contexts used for this purpose are in practice very\nsimple: generated from short (fragments of) first-order logic sentences with\nonly a few logical operators and quantifiers. In this work, we construct the\nnatural language dataset, DELTA$_D$, using the description logic language\n$\\mathcal{ALCQ}$. DELTA$_D$ contains 384K examples, and increases in two\ndimensions: i) reasoning depth, and ii) linguistic complexity. In this way, we\nsystematically investigate the reasoning ability of a supervised fine-tuned\nDeBERTa-based model and of two large language models (GPT-3.5, GPT-4) with\nfew-shot prompting. Our results demonstrate that the DeBERTa-based model can\nmaster the reasoning task and that the performance of GPTs can improve\nsignificantly even when a small number of samples is provided (9 shots). We\nopen-source our code and datasets.", "published": "2023-11-15 13:23:24", "link": "http://arxiv.org/abs/2311.08941v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying Linear Relational Concepts in Large Language Models", "abstract": "Transformer language models (LMs) have been shown to represent concepts as\ndirections in the latent space of hidden activations. However, for any\nhuman-interpretable concept, how can we find its direction in the latent space?\nWe present a technique called linear relational concepts (LRC) for finding\nconcept directions corresponding to human-interpretable concepts by first\nmodeling the relation between subject and object as a linear relational\nembedding (LRE). We find that inverting the LRE and using earlier object layers\nresults in a powerful technique for finding concept directions that outperforms\nstandard black-box probing classifiers. We evaluate LRCs on their performance\nas concept classifiers as well as their ability to causally change model\noutput.", "published": "2023-11-15 14:01:41", "link": "http://arxiv.org/abs/2311.08968v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When does In-context Learning Fall Short and Why? A Study on\n  Specification-Heavy Tasks", "abstract": "In-context learning (ICL) has become the default method for using large\nlanguage models (LLMs), making the exploration of its limitations and\nunderstanding the underlying causes crucial. In this paper, we find that ICL\nfalls short of handling specification-heavy tasks, which are tasks with\ncomplicated and extensive task specifications, requiring several hours for\nordinary humans to master, such as traditional information extraction tasks.\nThe performance of ICL on these tasks mostly cannot reach half of the\nstate-of-the-art results. To explore the reasons behind this failure, we\nconduct comprehensive experiments on 18 specification-heavy tasks with various\nLLMs and identify three primary reasons: inability to specifically understand\ncontext, misalignment in task schema comprehension with humans, and inadequate\nlong-text understanding ability. Furthermore, we demonstrate that through\nfine-tuning, LLMs can achieve decent performance on these tasks, indicating\nthat the failure of ICL is not an inherent flaw of LLMs, but rather a drawback\nof existing alignment methods that renders LLMs incapable of handling\ncomplicated specification-heavy tasks via ICL. To substantiate this, we perform\ndedicated instruction tuning on LLMs for these tasks and observe a notable\nimprovement. We hope the analyses in this paper could facilitate advancements\nin alignment methods enabling LLMs to meet more sophisticated human demands.", "published": "2023-11-15 14:26:30", "link": "http://arxiv.org/abs/2311.08993v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Similarity is Not Enough to Explain Language Model Performance", "abstract": "Large language models achieve high performance on many but not all downstream\ntasks. The interaction between pretraining data and task data is commonly\nassumed to determine this variance: a task with data that is more similar to a\nmodel's pretraining data is assumed to be easier for that model. We test\nwhether distributional and example-specific similarity measures (embedding-,\ntoken- and model-based) correlate with language model performance through a\nlarge-scale comparison of the Pile and C4 pretraining datasets with downstream\nbenchmarks. Similarity correlates with performance for multilingual datasets,\nbut in other benchmarks, we surprisingly find that similarity metrics are not\ncorrelated with accuracy or even each other. This suggests that the\nrelationship between pretraining data and downstream tasks is more complex than\noften assumed.", "published": "2023-11-15 14:48:08", "link": "http://arxiv.org/abs/2311.09006v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MELA: Multilingual Evaluation of Linguistic Acceptability", "abstract": "In this work, we present the largest benchmark to date on linguistic\nacceptability: Multilingual Evaluation of Linguistic Acceptability -- MELA,\nwith 46K samples covering 10 languages from a diverse set of language families.\nWe establish LLM baselines on this benchmark, and investigate cross-lingual\ntransfer in acceptability judgements with XLM-R. In pursuit of multilingual\ninterpretability, we conduct probing experiments with fine-tuned XLM-R to\nexplore the process of syntax capability acquisition. Our results show that\nGPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R,\nwhile open-source multilingual models lag behind by a noticeable gap.\nCross-lingual transfer experiments show that transfer in acceptability judgment\nis non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance\nin a completely unrelated language -- Chinese. Results of our probing\nexperiments indicate that training on MELA improves the performance of XLM-R on\nsyntax-related tasks. Our data is available at\nhttps://github.com/sjtu-compling/MELA.", "published": "2023-11-15 15:25:28", "link": "http://arxiv.org/abs/2311.09033v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing Knowledge Editing in Language Models via Relation Perspective", "abstract": "Knowledge Editing (KE) for modifying factual knowledge in Large Language\nModels (LLMs) has been receiving increasing attention. However, existing\nknowledge editing methods are entity-centric, and it is unclear whether this\napproach is suitable for a relation-centric perspective. To address this gap,\nthis paper constructs a new benchmark named RaKE, which focuses on Relation\nbased Knowledge Editing. In this paper, we establish a suite of innovative\nmetrics for evaluation and conduct comprehensive experiments involving various\nknowledge editing baselines. We notice that existing knowledge editing methods\nexhibit the potential difficulty in their ability to edit relations. Therefore,\nwe further explore the role of relations in factual triplets within the\ntransformer. Our research results confirm that knowledge related to relations\nis not only stored in the FFN network but also in the attention layers. This\nprovides experimental support for future relation-based knowledge editing\nmethods.", "published": "2023-11-15 15:44:42", "link": "http://arxiv.org/abs/2311.09053v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Well Do Large Language Models Truly Ground?", "abstract": "To reduce issues like hallucinations and lack of control in Large Language\nModels (LLMs), a common method is to generate responses by grounding on\nexternal contexts given as input, known as knowledge-augmented models. However,\nprevious research often narrowly defines \"grounding\" as just having the correct\nanswer, which does not ensure the reliability of the entire response. To\novercome this, we propose a stricter definition of grounding: a model is truly\ngrounded if it (1) fully utilizes the necessary knowledge from the provided\ncontext, and (2) stays within the limits of that knowledge. We introduce a new\ndataset and a grounding metric to evaluate model capability under the\ndefinition. We perform experiments across 25 LLMs of different sizes and\ntraining methods and provide insights into factors that influence grounding\nperformance. Our findings contribute to a better understanding of how to\nimprove grounding capabilities and suggest an area of improvement toward more\nreliable and controllable LLM applications.", "published": "2023-11-15 16:11:27", "link": "http://arxiv.org/abs/2311.09069v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Vocabulary Sharing Facilitates Multilingualism in LLaMA?", "abstract": "Large Language Models (LLMs), often show strong performance on English tasks,\nwhile exhibiting limitations on other languages. What is an LLM's multilingual\ncapability when it is trained only on certain languages? The underlying\nmechanism remains unclear. This study endeavors to examine the multilingual\ncapability of LLMs from the vocabulary sharing perspective by conducting an\nexhaustive analysis across 101 languages. Through the investigation of the\nperformance gap before and after embedding fine-tuning, we discovered four\ndistinct quadrants. By delving into each quadrant we provide actionable and\nefficient guidelines for tuning these languages. Extensive experiments reveal\nthat existing LLMs possess multilingual capabilities that surpass our\nexpectations, and we can significantly improve the multilingual performance of\nLLMs based on these attributes of each\nquadrant~\\footnote{\\url{https://github.com/CONE-MT/Vocabulary-Sharing-Facilitates-Multilingualism}.}.", "published": "2023-11-15 16:13:14", "link": "http://arxiv.org/abs/2311.09071v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Grounding Gaps in Language Model Generations", "abstract": "Effective conversation requires common ground: a shared understanding between\nthe participants. Common ground, however, does not emerge spontaneously in\nconversation. Speakers and listeners work together to both identify and\nconstruct a shared basis while avoiding misunderstanding. To accomplish\ngrounding, humans rely on a range of dialogue acts, like clarification (What do\nyou mean?) and acknowledgment (I understand.). However, it is unclear whether\nlarge language models (LLMs) generate text that reflects human grounding. To\nthis end, we curate a set of grounding acts and propose corresponding metrics\nthat quantify attempted grounding. We study whether LLM generations contain\ngrounding acts, simulating turn-taking from several dialogue datasets and\ncomparing results to humans. We find that -- compared to humans -- LLMs\ngenerate language with less conversational grounding, instead generating text\nthat appears to simply presume common ground. To understand the roots of the\nidentified grounding gap, we examine the role of instruction tuning and\npreference optimization, finding that training on contemporary preference data\nleads to a reduction in generated grounding acts. Altogether, we highlight the\nneed for more research investigating conversational grounding in human-AI\ninteraction.", "published": "2023-11-15 17:40:27", "link": "http://arxiv.org/abs/2311.09144v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Temporal Knowledge Question Answering via Abstract Reasoning Induction", "abstract": "In this study, we address the challenge of enhancing temporal knowledge\nreasoning in Large Language Models (LLMs). LLMs often struggle with this task,\nleading to the generation of inaccurate or misleading responses. This issue\nmainly arises from their limited ability to handle evolving factual knowledge\nand complex temporal logic. To overcome these limitations, we propose Abstract\nReasoning Induction (ARI) framework, which divides temporal reasoning into two\ndistinct phases: Knowledge-agnostic and Knowledge-based. This framework offers\nfactual knowledge support to LLMs while minimizing the incorporation of\nextraneous noisy data. Concurrently, informed by the principles of\nconstructivism, ARI provides LLMs the capability to engage in proactive,\nself-directed learning from both correct and incorrect historical reasoning\nsamples. By teaching LLMs to actively construct knowledge and methods, it can\nsignificantly boosting their temporal reasoning abilities. Our approach\nachieves remarkable improvements, with relative gains of 29.7% and 9.27% on two\ntemporal QA datasets, underscoring its efficacy in advancing temporal reasoning\nin LLMs. The code can be found at https://github.com/czy1999/ARI-QA", "published": "2023-11-15 17:46:39", "link": "http://arxiv.org/abs/2311.09149v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "AbsPyramid: Benchmarking the Abstraction Ability of Language Models with\n  a Unified Entailment Graph", "abstract": "Cognitive research indicates that abstraction ability is essential in human\nintelligence, which remains under-explored in language models. In this paper,\nwe present AbsPyramid, a unified entailment graph of 221K textual descriptions\nof abstraction knowledge. While existing resources only touch nouns or verbs\nwithin simplified events or specific domains, AbsPyramid collects abstract\nknowledge for three components of diverse events to comprehensively evaluate\nthe abstraction ability of language models in the open domain. Experimental\nresults demonstrate that current LLMs face challenges comprehending abstraction\nknowledge in zero-shot and few-shot settings. By training on our rich\nabstraction knowledge, we find LLMs can acquire basic abstraction abilities and\ngeneralize to unseen events. In the meantime, we empirically show that our\nbenchmark is comprehensive to enhance LLMs across two previous abstraction\ntasks.", "published": "2023-11-15 18:11:23", "link": "http://arxiv.org/abs/2311.09174v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Generation and Evaluation Capabilities of Large Language\n  Models for Instruction Controllable Summarization", "abstract": "While large language models (LLMs) can already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for desired summary\ncharacteristics. To this end, we curate an evaluation-only dataset for this\ntask setting and conduct human evaluations of five LLM-based systems to assess\ntheir instruction-following capabilities in controllable summarization. We then\nbenchmark LLM-based automatic evaluation for this task with 4 different\nevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study\nreveals that instruction controllable text summarization remains a challenging\ntask for LLMs, since (1) all LLMs evaluated still make factual and other types\nof errors in their summaries; (2) no LLM-based evaluation methods can achieve a\nstrong alignment with human annotators when judging the quality of candidate\nsummaries; (3) different LLMs show large performance gaps in summary generation\nand evaluation capabilities. We make our collected benchmark InstruSum publicly\navailable to facilitate future research in this direction.", "published": "2023-11-15 18:25:26", "link": "http://arxiv.org/abs/2311.09184v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Never Lost in the Middle: Mastering Long-Context Question Answering with\n  Position-Agnostic Decompositional Training", "abstract": "While large language models (LLMs) are equipped with longer text input\ncapabilities than before, they are struggling to seek correct information in\nlong contexts. The \"lost in the middle\" problem challenges most LLMs, referring\nto the dramatic decline in accuracy when correct information is located in the\nmiddle. To overcome this crucial issue, this paper proposes to enhance the\ninformation searching and reflection ability of LLMs in long contexts via\nspecially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).\nFollowing these tasks, our model excels in focusing more precisely on the\ndesired information. Experimental results show substantial improvement in\nMulti-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%\nabsolute gain in shuffled settings, by 21.5% in passage retrieval task. We\nrelease our model, Ziya-Reader to promote related research in the community.", "published": "2023-11-15 18:42:44", "link": "http://arxiv.org/abs/2311.09198v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fusion-Eval: Integrating Assistant Evaluators with LLMs", "abstract": "Evaluating natural language systems poses significant challenges,\nparticularly in the realms of natural language understanding and high-level\nreasoning. In this paper, we introduce 'Fusion-Eval', an innovative approach\nthat leverages Large Language Models (LLMs) to integrate insights from various\nassistant evaluators. The LLM is given the example to evaluate along with\nscores from the assistant evaluators. Each of these evaluators specializes in\nassessing distinct aspects of responses. Fusion-Eval achieves a 0.962\nsystem-level Kendall-Tau correlation with humans on SummEval and a 0.744\nturn-level Spearman correlation on TopicalChat, which is significantly higher\nthan baseline methods. These results highlight Fusion-Eval's significant\npotential in the realm of natural language system evaluation.", "published": "2023-11-15 18:46:56", "link": "http://arxiv.org/abs/2311.09204v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language\n  Models", "abstract": "Retrieval-augmented language models (RALMs) represent a substantial\nadvancement in the capabilities of large language models, notably in reducing\nfactual hallucination by leveraging external knowledge sources. However, the\nreliability of the retrieved information is not always guaranteed. The\nretrieval of irrelevant data can lead to misguided responses, and potentially\ncausing the model to overlook its inherent knowledge, even when it possesses\nadequate information to address the query. Moreover, standard RALMs often\nstruggle to assess whether they possess adequate knowledge, both intrinsic and\nretrieved, to provide an accurate answer. In situations where knowledge is\nlacking, these systems should ideally respond with \"unknown\" when the answer is\nunattainable. In response to these challenges, we introduces Chain-of-Noting\n(CoN), a novel approach aimed at improving the robustness of RALMs in facing\nnoisy, irrelevant documents and in handling unknown scenarios. The core idea of\nCoN is to generate sequential reading notes for retrieved documents, enabling a\nthorough evaluation of their relevance to the given question and integrating\nthis information to formulate the final answer. We employed ChatGPT to create\ntraining data for CoN, which was subsequently trained on an LLaMa-2 7B model.\nOur experiments across four open-domain QA benchmarks show that RALMs equipped\nwith CoN significantly outperform standard RALMs. Notably, CoN achieves an\naverage improvement of +7.9 in EM score given entirely noisy retrieved\ndocuments and +10.5 in rejection rates for real-time questions that fall\noutside the pre-training knowledge scope.", "published": "2023-11-15 18:54:53", "link": "http://arxiv.org/abs/2311.09210v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Controllable Text Summarization: Unraveling Challenges, Approaches, and\n  Prospects -- A Survey", "abstract": "Generic text summarization approaches often fail to address the specific\nintent and needs of individual users. Recently, scholarly attention has turned\nto the development of summarization methods that are more closely tailored and\ncontrolled to align with specific objectives and user needs. Despite a growing\ncorpus of controllable summarization research, there is no comprehensive survey\navailable that thoroughly explores the diverse controllable attributes employed\nin this context, delves into the associated challenges, and investigates the\nexisting solutions. In this survey, we formalize the Controllable Text\nSummarization (CTS) task, categorize controllable attributes according to their\nshared characteristics and objectives, and present a thorough examination of\nexisting datasets and methods within each category. Moreover, based on our\nfindings, we uncover limitations and research gaps, while also exploring\npotential solutions and future directions for CTS. We release our detailed\nanalysis of CTS papers at\nhttps://github.com/ashokurlana/controllable_text_summarization_survey.", "published": "2023-11-15 18:55:43", "link": "http://arxiv.org/abs/2311.09212v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Assessing Translation capabilities of Large Language Models involving\n  English and Indian Languages", "abstract": "Generative Large Language Models (LLMs) have achieved remarkable advancements\nin various NLP tasks. In this work, our aim is to explore the multilingual\ncapabilities of large language models by using machine translation as a task\ninvolving English and 22 Indian languages. We first investigate the translation\ncapabilities of raw large language models, followed by exploring the in-context\nlearning capabilities of the same raw models. We fine-tune these large language\nmodels using parameter efficient fine-tuning methods such as LoRA and\nadditionally with full fine-tuning. Through our study, we have identified the\nbest performing large language model for the translation task involving LLMs,\nwhich is based on LLaMA.\n  Our results demonstrate significant progress, with average BLEU scores of\n13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99,\n42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for\nEnglish to Indian languages on IN22 (conversational), IN22 (general),\nflores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, for\nIndian languages to English, we achieved average BLEU scores of 14.03, 16.65,\n16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51,\nand 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational),\nIN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets.\nOverall, our findings highlight the potential and strength of large language\nmodels for machine translation capabilities, including for languages that are\ncurrently underrepresented in LLMs.", "published": "2023-11-15 18:58:19", "link": "http://arxiv.org/abs/2311.09216v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large\n  Language Models", "abstract": "Although Large Language Models (LLMs) demonstrate remarkable ability in\nprocessing and generating human-like text, they do have limitations when it\ncomes to comprehending and expressing world knowledge that extends beyond the\nboundaries of natural language(e.g., chemical molecular formula). Injecting a\ncollection of symbolic data directly into the training of LLMs can be\nproblematic, as it disregards the synergies among different symbolic families\nand overlooks the need for a balanced mixture of natural and symbolic data. In\nthis work, we tackle these challenges from both a data and framework\nperspective and introduce Symbol-LLM series models. First, we curated a data\ncollection consisting of 34 tasks and incorporating approximately 20 distinct\nsymbolic families, intending to capture the interrelations and foster synergies\nbetween symbols. Then, a two-stage tuning framework succeeds in injecting\nsymbolic knowledge without loss of the generality ability. Extensive\nexperiments on both symbol- and NL-centric tasks demonstrate the balanced and\nsuperior performances of Symbol-LLM series models. The project page is\nhttps://xufangzhi.github.io/symbol-llm-page/.", "published": "2023-11-15 18:59:56", "link": "http://arxiv.org/abs/2311.09278v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spoken Word2Vec: Learning Skipgram Embeddings from Speech", "abstract": "Text word embeddings that encode distributional semantics work by modeling\ncontextual similarities of frequently occurring words. Acoustic word\nembeddings, on the other hand, typically encode low-level phonetic\nsimilarities. Semantic embeddings for spoken words have been previously\nexplored using analogous algorithms to Word2Vec, but the resulting vectors\nstill mainly encoded phonetic rather than semantic features. In this paper, we\nexamine the assumptions and architectures used in previous works and show\nexperimentally how shallow skipgram-like algorithms fail to encode\ndistributional semantics when the input units are acoustically correlated. We\nillustrate the potential of an alternative deep end-to-end variant of the model\nand examine the effects on the resulting embeddings, showing positive results\nof semantic relatedness in the embedding space.", "published": "2023-11-15 19:25:29", "link": "http://arxiv.org/abs/2311.09319v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Temperature-scaling surprisal estimates improve fit to human reading\n  times -- but does it do so for the \"right reasons\"?", "abstract": "A wide body of evidence shows that human language processing difficulty is\npredicted by the information-theoretic measure surprisal, a word's negative log\nprobability in context. However, it is still unclear how to best estimate these\nprobabilities needed for predicting human processing difficulty -- while a\nlong-standing belief held that models with lower perplexity would provide more\naccurate estimates of word predictability, and therefore lead to better reading\ntime predictions, recent work has shown that for very large models,\npsycholinguistic predictive power decreases. One reason could be that language\nmodels might be more confident of their predictions than humans, because they\nhave had exposure to several magnitudes more data. In this paper, we test what\neffect temperature-scaling of large language model (LLM) predictions has on\nsurprisal estimates and their predictive power of reading times of English\ntexts. Firstly, we show that calibration of large language models typically\nimproves with model size, i.e. poorer calibration cannot account for poorer fit\nto reading times. Secondly, we find that temperature-scaling probabilities lead\nto a systematically better fit to reading times (up to 89% improvement in delta\nlog likelihood), across several reading time corpora. Finally, we show that\nthis improvement in fit is chiefly driven by words that are composed of\nmultiple subword tokens.", "published": "2023-11-15 19:34:06", "link": "http://arxiv.org/abs/2311.09325v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating Hallucinations in Pruned Large Language Models for\n  Abstractive Summarization", "abstract": "Despite the remarkable performance of generative large language models (LLMs)\non abstractive summarization, they face two significant challenges: their\nconsiderable size and tendency to hallucinate. Hallucinations are concerning\nbecause they erode reliability and raise safety issues. Pruning is a technique\nthat reduces model size by removing redundant weights, enabling more efficient\nsparse inference. Pruned models yield downstream task performance comparable to\nthe original, making them ideal alternatives when operating on a limited\nbudget. However, the effect that pruning has upon hallucinations in abstractive\nsummarization with LLMs has yet to be explored. In this paper, we provide an\nextensive empirical study across five summarization datasets, two\nstate-of-the-art pruning methods, and five instruction-tuned LLMs.\nSurprisingly, we find that hallucinations are less prevalent from pruned LLMs\nthan the original models. Our analysis suggests that pruned models tend to\ndepend more on the source document for summary generation. This leads to a\nhigher lexical overlap between the generated summary and the source document,\nwhich could be a reason for the reduction in hallucination risk.", "published": "2023-11-15 19:49:24", "link": "http://arxiv.org/abs/2311.09335v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empirical evaluation of Uncertainty Quantification in\n  Retrieval-Augmented Language Models for Science", "abstract": "Large language models (LLMs) have shown remarkable achievements in natural\nlanguage processing tasks, producing high-quality outputs. However, LLMs still\nexhibit limitations, including the generation of factually incorrect\ninformation. In safety-critical applications, it is important to assess the\nconfidence of LLM-generated content to make informed decisions. Retrieval\nAugmented Language Models (RALMs) is relatively a new area of research in NLP.\nRALMs offer potential benefits for scientific NLP tasks, as retrieved\ndocuments, can serve as evidence to support model-generated content. This\ninclusion of evidence enhances trustworthiness, as users can verify and explore\nthe retrieved documents to validate model outputs. Quantifying uncertainty in\nRALM generations further improves trustworthiness, with retrieved text and\nconfidence scores contributing to a comprehensive and reliable model for\nscientific applications. However, there is limited to no research on UQ for\nRALMs, particularly in scientific contexts. This study aims to address this gap\nby conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific\ntasks. This research investigates how uncertainty scores vary when scientific\nknowledge is incorporated as pretraining and retrieval data and explores the\nrelationship between uncertainty scores and the accuracy of model-generated\noutputs. We observe that an existing RALM finetuned with scientific knowledge\nas the retrieval data tends to be more confident in generating predictions\ncompared to the model pretrained only with scientific knowledge. We also found\nthat RALMs are overconfident in their predictions, making inaccurate\npredictions more confidently than accurate ones. Scientific knowledge provided\neither as pretraining or retrieval corpus does not help alleviate this issue.\nWe released our code, data and dashboards at https://github.com/pnnl/EXPERT2.", "published": "2023-11-15 20:42:11", "link": "http://arxiv.org/abs/2311.09358v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph\n  Construction", "abstract": "While the potential of Open Information Extraction (Open IE) for Knowledge\nGraph Construction (KGC) may seem promising, we find that the alignment of Open\nIE extraction results with existing knowledge graphs to be inadequate. The\nadvent of Large Language Models (LLMs), especially the commercially available\nOpenAI models, have reset expectations for what is possible with deep learning\nmodels and have created a new field called prompt engineering. We investigate\nthe use of GPT models and prompt engineering for knowledge graph construction\nwith the Wikidata knowledge graph to address a similar problem to Open IE,\nwhich we call Open Knowledge Extraction (OKE) using an approach we call the\nLinked Open Knowledge Extractor (LOKE, pronounced like \"Loki\"). We consider the\nentity linking task essential to construction of real world knowledge graphs.\nWe merge the CaRB benchmark scoring approach with data from the TekGen dataset\nfor the LOKE task. We then show that a well engineered prompt, paired with a\nnaive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's\nOpenIE 4 implementation on the OKE task, although it over-generates triples\ncompared to the reference set due to overall triple scarcity in the TekGen set.\nThrough an analysis of entity linkability in the CaRB dataset, as well as\noutputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the \"silver\"\nTekGen triples show that the task is significantly different in content from\nOIE, if not structure. Through this analysis and a qualitative analysis of\nsentence extractions via all methods, we found that LOKE-GPT extractions are of\nhigh utility for the KGC task and suitable for use in semi-automated extraction\nsettings.", "published": "2023-11-15 20:57:44", "link": "http://arxiv.org/abs/2311.09366v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Long-form Question Answering: An Iterative Planning-Retrieval-Generation\n  Approach", "abstract": "Long-form question answering (LFQA) poses a challenge as it involves\ngenerating detailed answers in the form of paragraphs, which go beyond simple\nyes/no responses or short factual answers. While existing QA models excel in\nquestions with concise answers, LFQA requires handling multiple topics and\ntheir intricate relationships, demanding comprehensive explanations. Previous\nattempts at LFQA focused on generating long-form answers by utilizing relevant\ncontexts from a corpus, relying solely on the question itself. However, they\noverlooked the possibility that the question alone might not provide sufficient\ninformation to identify the relevant contexts. Additionally, generating\ndetailed long-form answers often entails aggregating knowledge from diverse\nsources. To address these limitations, we propose an LFQA model with iterative\nPlanning, Retrieval, and Generation. This iterative process continues until a\ncomplete answer is generated for the given question. From an extensive\nexperiment on both an open domain and a technical domain QA dataset, we find\nthat our model outperforms the state-of-the-art models on various textual and\nfactual metrics for the LFQA task.", "published": "2023-11-15 21:22:27", "link": "http://arxiv.org/abs/2311.09383v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural machine translation for automated feedback on children's\n  early-stage writing", "abstract": "In this work, we address the problem of assessing and constructing feedback\nfor early-stage writing automatically using machine learning. Early-stage\nwriting is typically vastly different from conventional writing due to phonetic\nspelling and lack of proper grammar, punctuation, spacing etc. Consequently,\nearly-stage writing is highly non-trivial to analyze using common linguistic\nmetrics. We propose to use sequence-to-sequence models for \"translating\"\nearly-stage writing by students into \"conventional\" writing, which allows the\ntranslated text to be analyzed using linguistic metrics. Furthermore, we\npropose a novel robust likelihood to mitigate the effect of noise in the\ndataset. We investigate the proposed methods using a set of numerical\nexperiments and demonstrate that the conventional text can be predicted with\nhigh accuracy.", "published": "2023-11-15 21:32:44", "link": "http://arxiv.org/abs/2311.09389v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "When Large Language Models contradict humans? Large Language Models'\n  Sycophantic Behaviour", "abstract": "Large Language Models have been demonstrating the ability to solve complex\ntasks by delivering answers that are positively evaluated by humans due in part\nto the intensive use of human feedback that refines responses. However, the\nsuggestibility transmitted through human feedback increases the inclination to\nproduce responses that correspond to the users' beliefs or misleading prompts\nas opposed to true facts, a behaviour known as sycophancy. This phenomenon\ndecreases the bias, robustness, and, consequently, their reliability. In this\npaper, we shed light on the suggestibility of Large Language Models (LLMs) to\nsycophantic behaviour, demonstrating these tendencies via human-influenced\nprompts over different tasks. Our investigation reveals that LLMs show\nsycophantic tendencies when responding to queries involving subjective opinions\nand statements that should elicit a contrary response based on facts. In\ncontrast, when confronted with mathematical tasks or queries that have an\nobjective answer, these models at various scales seem not to follow the users'\nhints by demonstrating confidence in delivering the correct answers.", "published": "2023-11-15 22:18:33", "link": "http://arxiv.org/abs/2311.09410v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Striped Attention: Faster Ring Attention for Causal Transformers", "abstract": "To help address the growing demand for ever-longer sequence lengths in\ntransformer models, Liu et al. recently proposed Ring Attention, an exact\nattention algorithm capable of overcoming per-device memory bottle- necks by\ndistributing self-attention across multiple devices. In this paper, we study\nthe performance characteristics of Ring Attention in the important special case\nof causal transformer models, and identify a key workload imbal- ance due to\ntriangular structure of causal attention computations. We propose a simple\nextension to Ring Attention, which we call Striped Attention to fix this\nimbalance. Instead of devices having contiguous subsequences, each device has a\nsubset of tokens distributed uniformly throughout the sequence, which we\ndemonstrate leads to more even workloads. In experiments running Striped\nAttention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x\nend-to-end throughput improvements over the original Ring Attention algorithm\non causal transformer training at a sequence length of 256k. Furthermore, on 16\nTPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of\n786k. We release the code for our experiments as open source", "published": "2023-11-15 23:01:02", "link": "http://arxiv.org/abs/2311.09431v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious\n  Demonstrations Shows their Vulnerabilities", "abstract": "The rapid progress in open-source Large Language Models (LLMs) is\nsignificantly driving AI development forward. However, there is still a limited\nunderstanding of their trustworthiness. Deploying these models at scale without\nsufficient trustworthiness can pose significant risks, highlighting the need to\nuncover these issues promptly. In this work, we conduct an adversarial\nassessment of open-source LLMs on trustworthiness, scrutinizing them across\neight different aspects including toxicity, stereotypes, ethics, hallucination,\nfairness, sycophancy, privacy, and robustness against adversarial\ndemonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU)\nprompting strategy by incorporating carefully crafted malicious demonstrations\nfor trustworthiness attack. Our extensive experiments encompass recent and\nrepresentative series of open-source LLMs, including Vicuna, MPT, Falcon,\nMistral, and Llama 2. The empirical outcomes underscore the efficacy of our\nattack strategy across diverse aspects. More interestingly, our result analysis\nreveals that models with superior performance in general NLP tasks do not\nalways have greater trustworthiness; in fact, larger models can be more\nvulnerable to attacks. Additionally, models that have undergone instruction\ntuning, focusing on instruction following, tend to be more susceptible,\nalthough fine-tuning LLMs for safety alignment proves effective in mitigating\nadversarial trustworthiness attacks.", "published": "2023-11-15 23:33:07", "link": "http://arxiv.org/abs/2311.09447v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparing Generalization in Learning with Limited Numbers of Exemplars:\n  Transformer vs. RNN in Attractor Dynamics", "abstract": "ChatGPT, a widely-recognized large language model (LLM), has recently gained\nsubstantial attention for its performance scaling, attributed to the billions\nof web-sourced natural language sentences used for training. Its underlying\narchitecture, Transformer, has found applications across diverse fields,\nincluding video, audio signals, and robotic movement. %The crucial question\nthis raises concerns the Transformer's generalization-in-learning (GIL)\ncapacity. However, this raises a crucial question about Transformer's\ngeneralization in learning (GIL) capacity. Is ChatGPT's success chiefly due to\nthe vast dataset used for training, or is there more to the story? To\ninvestigate this, we compared Transformer's GIL capabilities with those of a\ntraditional Recurrent Neural Network (RNN) in tasks involving attractor\ndynamics learning. For performance evaluation, the Dynamic Time Warping (DTW)\nmethod has been employed. Our simulation results suggest that under conditions\nof limited data availability, Transformer's GIL abilities are markedly inferior\nto those of RNN.", "published": "2023-11-15 00:37:49", "link": "http://arxiv.org/abs/2311.10763v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Value FULCRA: Mapping Large Language Models to the Multidimensional\n  Spectrum of Basic Human Values", "abstract": "The rapid advancement of Large Language Models (LLMs) has attracted much\nattention to value alignment for their responsible development. However, how to\ndefine values in this context remains a largely unexplored question. Existing\nwork mainly follows the Helpful, Honest, Harmless principle and specifies\nvalues as risk criteria formulated in the AI community, e.g., fairness and\nprivacy protection, suffering from poor clarity, adaptability and transparency.\nInspired by basic values in humanity and social science across cultures, this\nwork proposes a novel basic value alignment paradigm and introduces a value\nspace spanned by basic value dimensions. All LLMs' behaviors can be mapped into\nthe space by identifying the underlying values, possessing the potential to\naddress the three challenges. To foster future research, we apply the\nrepresentative Schwartz's Theory of Basic Values as an initialized example and\nconstruct FULCRA, a dataset consisting of 5k (LLM output, value vector) pairs.\nOur extensive analysis of FULCRA reveals the underlying relation between basic\nvalues and LLMs' behaviors, demonstrating that our approach not only covers\nexisting mainstream risks but also anticipates possibly unidentified ones.\nAdditionally, we present an initial implementation of the basic value\nevaluation and alignment, paving the way for future research in this line.", "published": "2023-11-15 10:29:28", "link": "http://arxiv.org/abs/2311.10766v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Restoration of Diacritics for Speech Data Sets", "abstract": "Automatic text-based diacritic restoration models generally have high\ndiacritic error rates when applied to speech transcripts as a result of domain\nand style shifts in spoken language. In this work, we explore the possibility\nof improving the performance of automatic diacritic restoration when applied to\nspeech data by utilizing parallel spoken utterances. In particular, we use the\npre-trained Whisper ASR model fine-tuned on relatively small amounts of\ndiacritized Arabic speech data to produce rough diacritized transcripts for the\nspeech utterances, which we then use as an additional input for diacritic\nrestoration models. The proposed framework consistently improves diacritic\nrestoration performance compared to text-only baselines. Our results highlight\nthe inadequacy of current text-based diacritic restoration models for speech\ndata sets and provide a new baseline for speech-based diacritic restoration.", "published": "2023-11-15 19:24:54", "link": "http://arxiv.org/abs/2311.10771v2", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "MMC: Advancing Multimodal Chart Understanding with Large-scale\n  Instruction Tuning", "abstract": "With the rapid development of large language models (LLMs) and their\nintegration into large multimodal models (LMMs), there has been impressive\nprogress in zero-shot completion of user-oriented vision-language tasks.\nHowever, a gap remains in the domain of chart image understanding due to the\ndistinct abstract components in charts. To address this, we introduce a\nlarge-scale MultiModal Chart Instruction (\\textbf{MMC-Instruction}) dataset\ncomprising 600k instances supporting diverse tasks and chart types. Leveraging\nthis data, we develop MultiModal Chart Assistant (\\textbf{MMCA}), an LMM that\nachieves state-of-the-art performance on existing chart QA benchmarks.\nRecognizing the need for a comprehensive evaluation of LMM chart understanding,\nwe also propose a MultiModal Chart Benchmark (\\textbf{MMC-Benchmark}), a\ncomprehensive human-annotated benchmark with nine distinct tasks evaluating\nreasoning capabilities over charts. Extensive experiments on MMC-Benchmark\nreveal the limitations of existing LMMs on correctly interpreting charts, even\nfor the most recent GPT-4V model. Our work provides an instruction-tuning\nmethodology and benchmark to advance multimodal understanding of charts. Code\nand data are available at https://github.com/FuxiaoLiu/MMC.", "published": "2023-11-15 23:36:42", "link": "http://arxiv.org/abs/2311.10774v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Jungle of Bias: Political Bias Attribution in Language\n  Models via Dependency Analysis", "abstract": "The rapid advancement of Large Language Models (LLMs) has sparked intense\ndebate regarding the prevalence of bias in these models and its mitigation.\nYet, as exemplified by both results on debiasing methods in the literature and\nreports of alignment-related defects from the wider community, bias remains a\npoorly understood topic despite its practical relevance. To enhance the\nunderstanding of the internal causes of bias, we analyse LLM bias through the\nlens of causal fairness analysis, which enables us to both comprehend the\norigins of bias and reason about its downstream consequences and mitigation. To\noperationalize this framework, we propose a prompt-based method for the\nextraction of confounding and mediating attributes which contribute to the LLM\ndecision process. By applying Activity Dependency Networks (ADNs), we then\nanalyse how these attributes influence an LLM's decision process. We apply our\nmethod to LLM ratings of argument quality in political debates. We find that\nthe observed disparate treatment can at least in part be attributed to\nconfounding and mitigating attributes and model misalignment, and discuss the\nconsequences of our findings for human-AI alignment and bias mitigation. Our\ncode and data are at https://github.com/david-jenny/LLM-Political-Study.", "published": "2023-11-15 00:02:25", "link": "http://arxiv.org/abs/2311.08605v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Towards Generalizable SER: Soft Labeling and Data Augmentation for\n  Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech", "abstract": "Recognizing emotions in spoken communication is crucial for advanced\nhuman-machine interaction. Current emotion detection methodologies often\ndisplay biases when applied cross-corpus. To address this, our study\namalgamates 16 diverse datasets, resulting in 375 hours of data across\nlanguages like English, Chinese, and Japanese. We propose a soft labeling\nsystem to capture gradational emotional intensities. Using the Whisper encoder\nand data augmentation methods inspired by contrastive learning, our method\nemphasizes the temporal dynamics of emotions. Our validation on four\nmultilingual datasets demonstrates notable zero-shot generalization. We publish\nour open source model weights and initial promising results after fine-tuning\non Hume-Prosody.", "published": "2023-11-15 00:09:21", "link": "http://arxiv.org/abs/2311.08607v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multiple-Question Multiple-Answer Text-VQA", "abstract": "We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do\ntext-VQA in encoder-decoder transformer models. The text-VQA task requires a\nmodel to answer a question by understanding multi-modal content: text\n(typically from OCR) and an associated image. To the best of our knowledge,\nalmost all previous approaches for text-VQA process a single question and its\nassociated content to predict a single answer. In order to answer multiple\nquestions from the same image, each question and content are fed into the model\nmultiple times. In contrast, our proposed MQMA approach takes multiple\nquestions and content as input at the encoder and predicts multiple answers at\nthe decoder in an auto-regressive manner at the same time. We make several\nnovel architectural modifications to standard encoder-decoder transformers to\nsupport MQMA. We also propose a novel MQMA denoising pre-training task which is\ndesigned to teach the model to align and delineate multiple questions and\ncontent with associated answers. MQMA pre-trained model achieves\nstate-of-the-art results on multiple text-VQA datasets, each with strong\nbaselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%),\nDocVQA (+1.1%) absolute improvements over the previous state-of-the-art\napproaches.", "published": "2023-11-15 01:00:02", "link": "http://arxiv.org/abs/2311.08622v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder\n  Transformer Models", "abstract": "Encoder-decoder transformer models have achieved great success on various\nvision-language (VL) tasks, but they suffer from high inference latency.\nTypically, the decoder takes up most of the latency because of the\nauto-regressive decoding. To accelerate the inference, we propose an approach\nof performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit\nencoder-decoder transformer model which is trained with deep supervision so\nthat each of its decoder layers is capable of generating plausible predictions.\nIn addition, we leverage simple yet practical techniques, including shared\ngeneration head and adaptation modules, to keep accuracy when exiting at\nshallow decoder layers. Based on the multi-exit model, we perform step-level\ndynamic early exit during inference, where the model may decide to use fewer\ndecoder layers based on its confidence of the current layer at each individual\ndecoding step. Considering different number of decoder layers may be used at\ndifferent decoding steps, we compute deeper-layer decoder features of previous\ndecoding steps just-in-time, which ensures the features from different decoding\nsteps are semantically aligned. We evaluate our approach with two\nstate-of-the-art encoder-decoder transformer models on various VL tasks. We\nshow our approach can reduce overall inference latency by 30%-60% with\ncomparable or even higher accuracy compared to baselines.", "published": "2023-11-15 01:01:02", "link": "http://arxiv.org/abs/2311.08623v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Evaluating Concurrent Robustness of Language Models Across Diverse\n  Challenge Sets", "abstract": "Language models, characterized by their black-box nature, often hallucinate\nand display sensitivity to input perturbations, causing concerns about trust.\nTo enhance trust, it is imperative to gain a comprehensive understanding of the\nmodel's failure modes and develop effective strategies to improve their\nperformance. In this study, we introduce a methodology designed to examine how\ninput perturbations affect language models across various scales, including\npre-trained models and large language models (LLMs). Utilizing fine-tuning, we\nenhance the model's robustness to input perturbations. Additionally, we\ninvestigate whether exposure to one perturbation enhances or diminishes the\nmodel's performance with respect to other perturbations. To address robustness\nagainst multiple perturbations, we present three distinct fine-tuning\nstrategies. Furthermore, we broaden the scope of our methodology to encompass\nlarge language models (LLMs) by leveraging a chain of thought (CoT) prompting\napproach augmented with exemplars. We employ the Tabular-NLI task to showcase\nhow our proposed strategies adeptly train a robust model, enabling it to\naddress diverse perturbations while maintaining accuracy on the original\ndataset. https://msin-infotabs.github.io/", "published": "2023-11-15 02:59:10", "link": "http://arxiv.org/abs/2311.08662v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "It Takes Two to Negotiate: Modeling Social Exchange in Online\n  Multiplayer Games", "abstract": "Online games are dynamic environments where players interact with each other,\nwhich offers a rich setting for understanding how players negotiate their way\nthrough the game to an ultimate victory. This work studies online player\ninteractions during the turn-based strategy game, Diplomacy. We annotated a\ndataset of over 10,000 chat messages for different negotiation strategies and\nempirically examined their importance in predicting long- and short-term game\noutcomes. Although negotiation strategies can be predicted reasonably\naccurately through the linguistic modeling of the chat messages, more is needed\nfor predicting short-term outcomes such as trustworthiness. On the other hand,\nthey are essential in graph-aware reinforcement learning approaches to predict\nlong-term outcomes, such as a player's success, based on their prior\nnegotiation history. We close with a discussion of the implications and impact\nof our work. The dataset is available at\nhttps://github.com/kj2013/claff-diplomacy.", "published": "2023-11-15 03:21:04", "link": "http://arxiv.org/abs/2311.08666v1", "categories": ["cs.CL", "cs.GT", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Eye on Clinical BERT: Investigating Language Model Generalization for\n  Diabetic Eye Disease Phenotyping", "abstract": "Diabetic eye disease is a major cause of blindness worldwide. The ability to\nmonitor relevant clinical trajectories and detect lapses in care is critical to\nmanaging the disease and preventing blindness. Alas, much of the information\nnecessary to support these goals is found only in the free text of the\nelectronic medical record. To fill this information gap, we introduce a system\nfor extracting evidence from clinical text of 19 clinical concepts related to\ndiabetic eye disease and inferring relevant attributes for each. In developing\nthis ophthalmology phenotyping system, we are also afforded a unique\nopportunity to evaluate the effectiveness of clinical language models at\nadapting to new clinical domains. Across multiple training paradigms, we find\nthat BERT language models pretrained on out-of-distribution clinical data offer\nno significant improvement over BERT language models pretrained on non-clinical\ndata for our domain. Our study tempers recent claims that language models\npretrained on clinical data are necessary for clinical NLP tasks and highlights\nthe importance of not treating clinical language data as a single homogeneous\ndomain.", "published": "2023-11-15 04:30:20", "link": "http://arxiv.org/abs/2311.08687v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attribute Diversity Determines the Systematicity Gap in VQA", "abstract": "Although modern neural networks often generalize to new combinations of\nfamiliar concepts, the conditions that enable such compositionality have long\nbeen an open question. In this work, we study the systematicity gap in visual\nquestion answering: the performance difference between reasoning on previously\nseen and unseen combinations of object attributes. To test, we introduce a\nnovel diagnostic dataset, CLEVR-HOPE. We find that the systematicity gap is not\nreduced by increasing the quantity of training data, but is reduced by\nincreasing the diversity of training data. In particular, our experiments\nsuggest that the more distinct attribute type combinations are seen during\ntraining, the more systematic we can expect the resulting model to be.", "published": "2023-11-15 04:50:30", "link": "http://arxiv.org/abs/2311.08695v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented\n  Instruction Tuning with Auxiliary Evaluation Aspects", "abstract": "Natural Language Generation (NLG) typically involves evaluating the generated\ntext in various aspects (e.g., consistency and naturalness) to obtain a\ncomprehensive assessment. However, multi-aspect evaluation remains challenging\nas it may require the evaluator to generalize to any given evaluation aspect\neven if it's absent during training. In this paper, we introduce X-Eval, a\ntwo-stage instruction tuning framework to evaluate the text in both seen and\nunseen aspects customized by end users. X-Eval consists of two learning stages:\nthe vanilla instruction tuning stage that improves the model's ability to\nfollow evaluation instructions, and an enhanced instruction tuning stage that\nexploits the connections between fine-grained evaluation aspects to better\nassess text quality. To support the training of X-Eval, we collect\nAspectInstruct, the first instruction tuning dataset tailored for multi-aspect\nNLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance\ntask diversity, we devise an augmentation strategy that converts human rating\nannotations into diverse forms of NLG evaluation tasks, including scoring,\ncomparison, ranking, and Boolean question answering. Extensive experiments\nacross three essential categories of NLG tasks: dialogue generation,\nsummarization, and data-to-text coupled with 21 aspects in meta-evaluation,\ndemonstrate that our X-Eval enables even a lightweight language model to\nachieve a comparable if not higher correlation with human judgments compared to\nthe state-of-the-art NLG evaluators, such as GPT-4.", "published": "2023-11-15 09:01:55", "link": "http://arxiv.org/abs/2311.08788v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MAP's not dead yet: Uncovering true language model modes by conditioning\n  away degeneracy", "abstract": "It has been widely observed that exact or approximate MAP (mode-seeking)\ndecoding from natural language generation (NLG) models consistently leads to\ndegenerate outputs (Holtzman et al., 2019; Stahlberg and Byrne, 2019). Prior\nwork has attributed this behavior to either a fundamental and unavoidable\ninadequacy of modes in probabilistic models or weaknesses in language modeling.\nContrastingly, we argue that degenerate modes can even occur in the absence of\nany modeling error, due to contamination of the training data. Specifically, we\nargue that mixing even a tiny amount of low-entropy noise with a population\ntext distribution can cause the data distribution's mode to become degenerate.\nWe therefore propose to apply MAP decoding to the model's true conditional\ndistribution where the conditioning variable explicitly avoids specific\ndegenerate behavior. Using exact search, we empirically verify that the\nlength-conditional modes of machine translation models and language models are\nindeed more fluent and topical than their unconditional modes. For the first\ntime, we also share many examples of exact modal sequences from these models,\nand from several variants of the LLaMA-7B model. Notably, we observe that\nvarious kinds of degenerate modes persist, even at the scale of LLaMA-7B.\nAlthough we cannot tractably address these degeneracies with exact search, we\nperform a classifier-based approximate search on LLaMA-7B, a model which was\nnot trained for instruction following, and find that we are able to elicit\nreasonable outputs without any finetuning.", "published": "2023-11-15 09:38:53", "link": "http://arxiv.org/abs/2311.08817v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Large-scale Deep Biasing with Phoneme Features and Text-only\n  Data in Streaming Transducer", "abstract": "Deep biasing for the Transducer can improve the recognition performance of\nrare words or contextual entities, which is essential in practical\napplications, especially for streaming Automatic Speech Recognition (ASR).\nHowever, deep biasing with large-scale rare words remains challenging, as the\nperformance drops significantly when more distractors exist and there are words\nwith similar grapheme sequences in the bias list. In this paper, we combine the\nphoneme and textual information of rare words in Transducers to distinguish\nwords with similar pronunciation or spelling. Moreover, the introduction of\ntraining with text-only data containing more rare words benefits large-scale\ndeep biasing. The experiments on the LibriSpeech corpus demonstrate that the\nproposed method achieves state-of-the-art performance on rare word error rate\nfor different scales and levels of bias lists.", "published": "2023-11-15 13:53:28", "link": "http://arxiv.org/abs/2311.08966v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Uli Dataset: An Exercise in Experience Led Annotation of oGBV", "abstract": "Online gender based violence has grown concomitantly with adoption of the\ninternet and social media. Its effects are worse in the Global majority where\nmany users use social media in languages other than English. The scale and\nvolume of conversations on the internet has necessitated the need for automated\ndetection of hate speech, and more specifically gendered abuse. There is,\nhowever, a lack of language specific and contextual data to build such\nautomated tools. In this paper we present a dataset on gendered abuse in three\nlanguages- Hindi, Tamil and Indian English. The dataset comprises of tweets\nannotated along three questions pertaining to the experience of gender abuse,\nby experts who identify as women or a member of the LGBTQIA community in South\nAsia. Through this dataset we demonstrate a participatory approach to creating\ndatasets that drive AI systems.", "published": "2023-11-15 16:30:44", "link": "http://arxiv.org/abs/2311.09086v3", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Towards A Unified View of Answer Calibration for Multi-Step Reasoning", "abstract": "Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have\nbroadened the scope for improving multi-step reasoning capabilities. We\ngenerally divide multi-step reasoning into two phases: path generation to\ngenerate the reasoning path(s); and answer calibration post-processing the\nreasoning path(s) to obtain a final answer. However, the existing literature\nlacks systematic analysis on different answer calibration approaches. In this\npaper, we summarize the taxonomy of recent answer calibration techniques and\nbreak them down into step-level and path-level strategies. We then conduct a\nthorough evaluation on these strategies from a unified view, systematically\nscrutinizing step-level and path-level answer calibration across multiple\npaths. Experimental results reveal that integrating the dominance of both\nstrategies tends to derive optimal outcomes. Our study holds the potential to\nilluminate key insights for optimizing multi-step reasoning with answer\ncalibration.", "published": "2023-11-15 16:47:57", "link": "http://arxiv.org/abs/2311.09101v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge\n  Graph Completion?", "abstract": "Knowledge graphs (KGs) consist of links that describe relationships between\nentities. Due to the difficulty of manually enumerating all relationships\nbetween entities, automatically completing them is essential for KGs. Knowledge\nGraph Completion (KGC) is a task that infers unseen relationships between\nentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\nTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\nonly the knowledge from training data. In contrast, the recent Pre-trained\nLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\nTherefore, PLM-based KGC can estimate missing links between entities by reusing\nmemorized knowledge from pre-training without inference. This approach is\nproblematic because building KGC models aims to infer unseen links between\nentities. However, conventional evaluations in KGC do not consider inference\nand memorization abilities separately. Thus, a PLM-based KGC method, which\nachieves high performance in current KGC evaluations, may be ineffective in\npractical applications. To address this issue, we analyze whether PLM-based KGC\nmethods make inferences or merely access memorized knowledge. For this purpose,\nwe propose a method for constructing synthetic datasets specified in this\nanalysis and conclude that PLMs acquire the inference abilities required for\nKGC through pre-training, even though the performance improvements mostly come\nfrom textual information of entities and relations.", "published": "2023-11-15 16:56:49", "link": "http://arxiv.org/abs/2311.09109v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ever: Mitigating Hallucination in Large Language Models through\n  Real-Time Verification and Rectification", "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating fluent text. However, they often encounter the challenge of\ngenerating inaccurate or hallucinated content. This issue is common in both\nnon-retrieval-based generation and retrieval-augmented generation approaches,\nand existing post-hoc rectification methods may not address the accumulated\nhallucination errors that may be caused by the \"snowballing\" issue, especially\nin reasoning tasks. To tackle these challenges, we introduce a novel approach\ncalled Real-time Verification and Rectification (Ever). Instead of waiting\nuntil the end of the generation process to rectify hallucinations, Ever employs\na real-time, step-wise generation and hallucination rectification strategy. The\nprimary objective is to detect and rectify hallucinations as they occur during\nthe text generation process. When compared to both retrieval-based and\nnon-retrieval-based baselines, Ever demonstrates a significant improvement in\ngenerating trustworthy and factually accurate text across a diverse range of\ntasks, including short-form QA, biography generation, and multi-hop reasoning.", "published": "2023-11-15 17:04:56", "link": "http://arxiv.org/abs/2311.09114v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "R-Spin: Efficient Speaker and Noise-invariant Representation Learning\n  with Acoustic Pieces", "abstract": "This paper introduces Robust Spin (R-Spin), a data-efficient domain-specific\nself-supervision method for speaker and noise-invariant speech representations\nby learning discrete acoustic units with speaker-invariant clustering (Spin).\nR-Spin resolves Spin's issues and enhances content representations by learning\nto predict acoustic pieces. R-Spin offers a 12X reduction in computational\nresources compared to previous state-of-the-art methods while outperforming\nthem in severely distorted speech scenarios. This paper provides detailed\nanalyses to show how discrete units contribute to speech encoder training and\nimproving robustness in diverse acoustic environments.", "published": "2023-11-15 17:07:44", "link": "http://arxiv.org/abs/2311.09117v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Pearl: Personalizing Large Language Model Writing Assistants with\n  Generation-Calibrated Retrievers", "abstract": "Powerful large language models have facilitated the development of writing\nassistants that promise to significantly improve the quality and efficiency of\ncomposition and communication. However, a barrier to effective assistance is\nthe lack of personalization in LLM outputs to the author's communication style,\nspecialized knowledge, and values. In this paper, we address this challenge by\nproposing Pearl, a LLM writing assistant personalized with a retriever that is\ntrained to be generation-calibrated for personalization. Generation calibration\nensures that our retriever selects historic user authored documents to augment\nan LLM prompt such that they are likely to help an LLM generation better adhere\nto a users' preferences. We propose two key novelties for training such a\nretriever: (1) A training data selection method that identifies user requests\nlikely to benefit from personalization and documents that provide that benefit;\nand (2) A scale-calibrating KL-divergence objective that ensures that our\nretriever scores remain proportional to the downstream generation quality from\nusing the document for personalized generation. In a series of holistic\nevaluations, we demonstrate the effectiveness of Pearl in generating long-form\ntexts on multiple social media datasets. Finally, we demonstrate how a\ngeneration-calibrated retriever can double as a performance predictor --\ndetecting low quality retrieval, and improving potentially under-performing\noutputs via revision with LLMs.", "published": "2023-11-15 18:19:58", "link": "http://arxiv.org/abs/2311.09180v2", "categories": ["cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Verifiable Text Generation with Symbolic References", "abstract": "LLMs are vulnerable to hallucinations, and thus their outputs generally\nrequire laborious human verification for high-stakes applications. To this end,\nwe propose symbolically grounded generation (SymGen) as a simple approach for\nenabling easier manual validation of an LLM's output. SymGen prompts an LLM to\ninterleave its regular output text with explicit symbolic references to fields\npresent in some conditioning data (e.g., a table in JSON format). The\nreferences can be used to display the provenance of different spans of text in\nthe generation, reducing the effort required for manual verification. Across a\nrange of data-to-text and question-answering experiments, we find that LLMs are\nable to directly output text that makes use of accurate symbolic references\nwhile maintaining fluency and factuality. In a human study we further find that\nsuch annotations can streamline human verification of machine-generated text.\nOur code will be available at http://symgen.github.io.", "published": "2023-11-15 18:28:29", "link": "http://arxiv.org/abs/2311.09188v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task", "abstract": "The study explores the effectiveness of the Chain-of-Thought approach, known\nfor its proficiency in language tasks by breaking them down into sub-tasks and\nintermediate steps, in improving vision-language tasks that demand\nsophisticated perception and reasoning. We present the \"Description then\nDecision\" strategy, which is inspired by how humans process signals. This\nstrategy significantly improves probing task performance by 50%, establishing\nthe groundwork for future research on reasoning paradigms in complex\nvision-language tasks.", "published": "2023-11-15 18:39:21", "link": "http://arxiv.org/abs/2311.09193v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "TableLlama: Towards Open Large Generalist Models for Tables", "abstract": "Semi-structured tables are ubiquitous. There has been a variety of tasks that\naim to automatically interpret, augment, and query tables. Current methods\noften require pretraining on tables or special model architecture design, are\nrestricted to specific table types, or have simplifying assumptions about\ntables and tasks. This paper makes the first step towards developing\nopen-source large language models (LLMs) as generalists for a diversity of\ntable-based tasks. Towards that end, we construct TableInstruct, a new dataset\nwith a variety of realistic tables and tasks, for instruction tuning and\nevaluating LLMs. We further develop the first open-source generalist model for\ntables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the\nlong context challenge. We experiment under both in-domain setting and\nout-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves\ncomparable or better performance than the SOTA for each task, despite the\nlatter often has task-specific design. On 6 out-of-domain datasets, it achieves\n5-44 absolute point gains compared with the base model, showing that training\non TableInstruct enhances the model's generalizability. We open-source our\ndataset and trained model to boost future work on developing open generalist\nmodels for tables.", "published": "2023-11-15 18:47:52", "link": "http://arxiv.org/abs/2311.09206v3", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Auto-ICL: In-Context Learning without Human Supervision", "abstract": "With in-context learning ability, the performance of large language models\ncan be significantly boosted when provided with appropriate context. However,\nexisting in-context learning methods mainly rely on human-provided contexts,\nsuch as labeled examples and explicit instructions. Writing context by humans\nis labor-intensive on various tasks and limits the model to tasks manageable by\nhumans. To overcome these limitations, we propose Automatic In-Context Learning\nframework that enables the model to autonomously generate examples and\ninstructions for problem-solving. With experiments across various models and\ndatasets, results show that model-generated contexts outperform human-annotated\ncontexts, including Few-Shot and Few-Shot-CoT methods, and surpass existing\nself-generated context methods like Zero-CoT and Auto-CoT.", "published": "2023-11-15 07:37:28", "link": "http://arxiv.org/abs/2311.09263v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Divergences between Language Models and Human Brains", "abstract": "Do machines and humans process language in similar ways? Recent research has\nhinted at the affirmative, showing that human neural activity can be\neffectively predicted using the internal representations of language models\n(LMs). Although such results are thought to reflect shared computational\nprinciples between LMs and human brains, there are also clear differences in\nhow LMs and humans represent and use language. In this work, we systematically\nexplore the divergences between human and machine language processing by\nexamining the differences between LM representations and human brain responses\nto language as measured by Magnetoencephalography (MEG) across two datasets in\nwhich subjects read and listened to narrative stories. Using an LLM-based\ndata-driven approach, we identify two domains that LMs do not capture well:\nsocial/emotional intelligence and physical commonsense. We validate these\nfindings with human behavioral experiments and hypothesize that the gap is due\nto insufficient representations of social/emotional and physical knowledge in\nLMs. Our results show that fine-tuning LMs on these domains can improve their\nalignment with human brain responses.", "published": "2023-11-15 19:02:40", "link": "http://arxiv.org/abs/2311.09308v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Banach-Tarski Embeddings and Transformers", "abstract": "We introduce a new construction of embeddings of arbitrary recursive data\nstructures into high dimensional vectors. These embeddings provide an\ninterpretable model for the latent state vectors of transformers. We\ndemonstrate that these embeddings can be decoded to the original data structure\nwhen the embedding dimension is sufficiently large. This decoding algorithm has\na natural implementation as a transformer. We also show that these embedding\nvectors can be manipulated directly to perform computations on the underlying\ndata without decoding. As an example we present an algorithm that constructs\nthe embedded parse tree of an embedded token sequence using only vector\noperations in embedding space.", "published": "2023-11-15 21:30:26", "link": "http://arxiv.org/abs/2311.09387v2", "categories": ["cs.LG", "cs.CL", "cs.DS"], "primary_category": "cs.LG"}
{"title": "Alternatives to the Scaled Dot Product for Attention in the Transformer\n  Neural Network Architecture", "abstract": "The transformer neural network architecture uses a form of attention in which\nthe dot product of query and key is divided by the square root of the key\ndimension before applying softmax. This scaling of the dot product is designed\nto avoid the absolute value of the dot products becoming so large that applying\nsoftmax leads to vanishing gradients. In this paper, we propose some\nalternative scalings, including dividing the dot product instead by the sum of\nthe key lengths before applying softmax. We use simulated keys and queries to\nshow that in many situations this appears to be more effective at avoiding\nregions where applying softmax leads to vanishing gradients.", "published": "2023-11-15 22:10:42", "link": "http://arxiv.org/abs/2311.09406v1", "categories": ["cs.LG", "cs.CL", "cs.NE", "I.2.0; I.2.7"], "primary_category": "cs.LG"}
{"title": "Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language\n  Models", "abstract": "This work investigates the potential of undermining both fairness and\ndetection performance in abusive language detection. In a dynamic and complex\ndigital world, it is crucial to investigate the vulnerabilities of these\ndetection models to adversarial fairness attacks to improve their fairness\nrobustness. We propose a simple yet effective framework FABLE that leverages\nbackdoor attacks as they allow targeted control over the fairness and detection\nperformance. FABLE explores three types of trigger designs (i.e., rare,\nartificial, and natural triggers) and novel sampling strategies. Specifically,\nthe adversary can inject triggers into samples in the minority group with the\nfavored outcome (i.e., \"non-abusive\") and flip their labels to the unfavored\noutcome, i.e., \"abusive\". Experiments on benchmark datasets demonstrate the\neffectiveness of FABLE attacking fairness and utility in abusive language\ndetection.", "published": "2023-11-15 22:57:13", "link": "http://arxiv.org/abs/2311.09428v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Trojan Activation Attack: Red-Teaming Large Language Models using\n  Activation Steering for Safety-Alignment", "abstract": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are\nspecifically trained to ensure alignment, which refers to making models behave\nin accordance with human intentions. While these models have demonstrated\ncommendable results on various safety benchmarks, the vulnerability of their\nsafety alignment has not been extensively studied. This is particularly\ntroubling given the potential harm that LLMs can inflict. Existing attack\nmethods on LLMs often rely on poisoned training data or the injection of\nmalicious prompts. These approaches compromise the stealthiness and\ngeneralizability of the attacks, making them susceptible to detection.\nAdditionally, these models often demand substantial computational resources for\nimplementation, making them less practical for real-world applications. In this\nwork, we study a different attack scenario, called Trojan Activation Attack\n(TA^2), which injects trojan steering vectors into the activation layers of\nLLMs. These malicious steering vectors can be triggered at inference time to\nsteer the models toward attacker-desired behaviors by manipulating their\nactivations. Our experiment results on four primary alignment tasks show that\nTA^2 is highly effective and adds little or no overhead to attack efficiency.\nAdditionally, we discuss potential countermeasures against such activation\nattacks.", "published": "2023-11-15 23:07:40", "link": "http://arxiv.org/abs/2311.09433v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Labeled Interactive Topic Models", "abstract": "Topic models are valuable for understanding extensive document collections,\nbut they don't always identify the most relevant topics. Classical\nprobabilistic and anchor-based topic models offer interactive versions that\nallow users to guide the models towards more pertinent topics. However, such\ninteractive features have been lacking in neural topic models. To correct this\nlacuna, we introduce a user-friendly interaction for neural topic models. This\ninteraction permits users to assign a word label to a topic, leading to an\nupdate in the topic model where the words in the topic become closely aligned\nwith the given label. Our approach encompasses two distinct kinds of neural\ntopic models. The first includes models where topic embeddings are trainable\nand evolve during the training process. The second kind involves models where\ntopic embeddings are integrated post-training, offering a different approach to\ntopic refinement. To facilitate user interaction with these neural topic\nmodels, we have developed an interactive interface. This interface enables\nusers to engage with and re-label topics as desired. We evaluate our method\nthrough a human study, where users can relabel topics to find relevant\ndocuments. Using our method, user labeling improves document rank scores,\nhelping to find more relevant documents to a given query when compared to no\nuser labeling.", "published": "2023-11-15 23:18:01", "link": "http://arxiv.org/abs/2311.09438v2", "categories": ["cs.LG", "cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.LG"}
{"title": "VideoCon: Robust Video-Language Alignment via Contrast Captions", "abstract": "Despite being (pre)trained on a massive amount of data, state-of-the-art\nvideo-language alignment models are not robust to semantically-plausible\ncontrastive changes in the video captions. Our work addresses this by\nidentifying a broad spectrum of contrast misalignments, such as replacing\nentities, actions, and flipping event order, which alignment models should be\nrobust against. To this end, we introduce the VideoCon, a video-language\nalignment dataset constructed by a large language model that generates\nplausible contrast video captions and explanations for differences between\noriginal and contrast video captions. Then, a generative video-language model\nis finetuned with VideoCon to assess video-language entailment and generate\nexplanations. Our VideoCon-based alignment model significantly outperforms\ncurrent models. It exhibits a 12-point increase in AUC for the video-language\nalignment task on human-generated contrast captions. Finally, our model sets\nnew state of the art zero-shot performance in temporally-extensive\nvideo-language tasks such as text-to-video retrieval (SSv2-Temporal) and video\nquestion answering (ATP-Hard). Moreover, our model shows superior performance\non novel videos and human-crafted captions and explanations. Our code and data\nare available at https://github.com/Hritikbansal/videocon.", "published": "2023-11-15 19:51:57", "link": "http://arxiv.org/abs/2311.10111v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with\n  Large Language Models", "abstract": "Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become\na heated topic. Various methods have been proposed to forecast links on TKGs.\nMost of them are embedding-based, where hidden representations are learned to\nrepresent knowledge graph (KG) entities and relations based on the observed\ngraph contexts. Although these methods show strong performance on traditional\nTKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the\nunseen zero-shot relations that have no prior graph context. In this paper, we\ntry to mitigate this problem as follows. We first input the text descriptions\nof KG relations into large language models (LLMs) for generating relation\nrepresentations, and then introduce them into embedding-based TKGF methods.\nLLM-empowered representations can capture the semantic information in the\nrelation descriptions. This makes the relations, whether seen or unseen, with\nsimilar semantic meanings stay close in the embedding space, enabling TKGF\nmodels to recognize zero-shot relations even without any observed graph\ncontext. Experimental results show that our approach helps TKGF models to\nachieve much better performance in forecasting the facts with previously unseen\nrelations, while still maintaining their ability in link forecasting regarding\nseen relations.", "published": "2023-11-15 21:25:15", "link": "http://arxiv.org/abs/2311.10112v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Exponentially Faster Language Modelling", "abstract": "Language models only really need to use an exponential fraction of their\nneurons for individual inferences. As proof, we present UltraFastBERT, a BERT\nvariant that uses 0.3% of its neurons during inference while performing on par\nwith similar BERT models. UltraFastBERT selectively engages just 12 out of 4095\nneurons for each layer inference. This is achieved by replacing feedforward\nnetworks with fast feedforward networks (FFFs). While no truly efficient\nimplementation currently exists to unlock the full acceleration potential of\nconditional neural execution, we provide high-level CPU code achieving 78x\nspeedup over the optimized baseline feedforward implementation, and a PyTorch\nimplementation delivering 40x speedup over the equivalent batched feedforward\ninference. We publish our training code, benchmarking setup, and model weights.", "published": "2023-11-15 18:42:50", "link": "http://arxiv.org/abs/2311.10770v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "ToolTalk: Evaluating Tool-Usage in a Conversational Setting", "abstract": "Large language models (LLMs) have displayed massive improvements in reasoning\nand decision-making skills and can hold natural conversations with users. Many\nrecent works seek to augment LLM-based assistants with external tools so they\ncan access private or up-to-date information and carry out actions on behalf of\nusers. To better measure the performance of these assistants, this paper\nintroduces ToolTalk, a benchmark consisting of complex user intents requiring\nmulti-step tool usage specified through dialogue. ToolTalk contains 28 tools\ngrouped into 7 plugins, and includes a complete simulated implementation of\neach tool, allowing for fully automated evaluation of assistants that rely on\nexecution feedback. ToolTalk also emphasizes tools that externally affect the\nworld rather than only tools for referencing or searching information. We\nevaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and\n50% respectively. Our analysis of the errors reveals three major categories and\nsuggests some future directions for improvement. We release ToolTalk at\nhttps://github.com/microsoft/ToolTalk.", "published": "2023-11-15 23:50:31", "link": "http://arxiv.org/abs/2311.10775v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stealthy and Persistent Unalignment on Large Language Models via\n  Backdoor Injections", "abstract": "Recent developments in Large Language Models (LLMs) have manifested\nsignificant advancements. To facilitate safeguards against malicious\nexploitation, a body of research has concentrated on aligning LLMs with human\npreferences and inhibiting their generation of inappropriate content.\nUnfortunately, such alignments are often vulnerable: fine-tuning with a minimal\namount of harmful data can easily unalign the target LLM. While being\neffective, such fine-tuning-based unalignment approaches also have their own\nlimitations: (1) non-stealthiness, after fine-tuning, safety audits or\nred-teaming can easily expose the potential weaknesses of the unaligned models,\nthereby precluding their release/use. (2) non-persistence, the unaligned LLMs\ncan be easily repaired through re-alignment, i.e., fine-tuning again with\naligned data points. In this work, we show that it is possible to conduct\nstealthy and persistent unalignment on large language models via backdoor\ninjections. We also provide a novel understanding on the relationship between\nthe backdoor persistence and the activation pattern and further provide\nguidelines for potential trigger design. Through extensive experiments, we\ndemonstrate that our proposed stealthy and persistent unalignment can\nsuccessfully pass the safety evaluation while maintaining strong persistence\nagainst re-alignment defense.", "published": "2023-11-15 23:52:05", "link": "http://arxiv.org/abs/2312.00027v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Deep Representation Learning for Open Vocabulary\n  Electroencephalography-to-Text Decoding", "abstract": "Previous research has demonstrated the potential of using pre-trained\nlanguage models for decoding open vocabulary Electroencephalography (EEG)\nsignals captured through a non-invasive Brain-Computer Interface (BCI).\nHowever, the impact of embedding EEG signals in the context of language models\nand the effect of subjectivity, remain unexplored, leading to uncertainty about\nthe best approach to enhance decoding performance. Additionally, current\nevaluation metrics used to assess decoding effectiveness are predominantly\nsyntactic and do not provide insights into the comprehensibility of the decoded\noutput for human understanding. We present an end-to-end deep learning\nframework for non-invasive brain recordings that brings modern representational\nlearning approaches to neuroscience. Our proposal introduces the following\ninnovations: 1) an end-to-end deep learning architecture for open vocabulary\nEEG decoding, incorporating a subject-dependent representation learning module\nfor raw EEG encoding, a BART language model, and a GPT-4 sentence refinement\nmodule; 2) a more comprehensive sentence-level evaluation metric based on the\nBERTScore; 3) an ablation study that analyses the contributions of each module\nwithin our proposal, providing valuable insights for future research. We\nevaluate our approach on two publicly available datasets, ZuCo v1.0 and v2.0,\ncomprising EEG recordings of 30 subjects engaged in natural reading tasks. Our\nmodel achieves a BLEU-1 score of 42.75%, a ROUGE-1-F of 33.28%, and a\nBERTScore-F of 53.86%, outperforming the previous state-of-the-art methods by\n3.38%, 8.43%, and 6.31%, respectively.", "published": "2023-11-15 08:03:09", "link": "http://arxiv.org/abs/2312.09430v1", "categories": ["eess.SP", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Three Conjectures on Unexpectedeness", "abstract": "Unexpectedness is a central concept in Simplicity Theory, a theory of\ncognition relating various inferential processes to the computation of\nKolmogorov complexities, rather than probabilities. Its predictive power has\nbeen confirmed by several experiments with human subjects, yet its theoretical\nbasis remains largely unexplored: why does it work? This paper lays the\ngroundwork for three theoretical conjectures. First, unexpectedness can be seen\nas a generalization of Bayes' rule. Second, the frequentist core of\nunexpectedness can be connected to the function of tracking ergodic properties\nof the world. Third, unexpectedness can be seen as constituent of various\nmeasures of divergence between the entropy of the world (environment) and the\nvariety of the observer (system). The resulting framework hints to research\ndirections that go beyond the division between probabilistic and logical\napproaches, potentially bringing new insights into the extraction of causal\nrelations, and into the role of descriptive mechanisms in learning.", "published": "2023-11-15 08:24:41", "link": "http://arxiv.org/abs/2311.08768v1", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "primary_category": "cs.AI"}
{"title": "Multi-channel Conversational Speaker Separation via Neural Diarization", "abstract": "When dealing with overlapped speech, the performance of automatic speech\nrecognition (ASR) systems substantially degrades as they are designed for\nsingle-talker speech. To enhance ASR performance in conversational or meeting\nenvironments, continuous speaker separation (CSS) is commonly employed.\nHowever, CSS requires a short separation window to avoid many speakers inside\nthe window and sequential grouping of discontinuous speech segments. To address\nthese limitations, we introduce a new multi-channel framework called \"speaker\nseparation via neural diarization\" (SSND) for meeting environments. Our\napproach utilizes an end-to-end diarization system to identify the speech\nactivity of each individual speaker. By leveraging estimated speaker\nboundaries, we generate a sequence of embeddings, which in turn facilitate the\nassignment of speakers to the outputs of a multi-talker separation model. SSND\naddresses the permutation ambiguity issue of talker-independent speaker\nseparation during the diarization phase through location-based training, rather\nthan during the separation process. This unique approach allows multiple\nnon-overlapped speakers to be assigned to the same output stream, making it\npossible to efficiently process long segments-a task impossible with CSS.\nAdditionally, SSND is naturally suitable for speaker-attributed ASR. We\nevaluate our proposed diarization and separation methods on the open LibriCSS\ndataset, advancing state-of-the-art diarization and ASR results by a large\nmargin.", "published": "2023-11-15 01:09:16", "link": "http://arxiv.org/abs/2311.08630v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EDMSound: Spectrogram Based Diffusion Models for Efficient and\n  High-Quality Audio Synthesis", "abstract": "Audio diffusion models can synthesize a wide variety of sounds. Existing\nmodels often operate on the latent domain with cascaded phase recovery modules\nto reconstruct waveform. This poses challenges when generating high-fidelity\naudio. In this paper, we propose EDMSound, a diffusion-based generative model\nin spectrogram domain under the framework of elucidated diffusion models (EDM).\nCombining with efficient deterministic sampler, we achieved similar Fr\\'echet\naudio distance (FAD) score as top-ranked baseline with only 10 steps and\nreached state-of-the-art performance with 50 steps on the DCASE2023 foley sound\ngeneration benchmark. We also revealed a potential concern regarding diffusion\nbased audio generation models that they tend to generate samples with high\nperceptual similarity to the data from training data. Project page:\nhttps://agentcooper2002.github.io/EDMSound/", "published": "2023-11-15 03:27:35", "link": "http://arxiv.org/abs/2311.08667v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control\n  and Contrastive Learning with Negative Samples Augmentation", "abstract": "Better disentanglement of speech representation is essential to improve the\nquality of voice conversion. Recently contrastive learning is applied to voice\nconversion successfully based on speaker labels. However, the performance of\nmodel will reduce in conversion between similar speakers. Hence, we propose an\naugmented negative sample selection to address the issue. Specifically, we\ncreate hard negative samples based on the proposed speaker fusion module to\nimprove learning ability of speaker encoder. Furthermore, considering the\nfine-grain modeling of speaker style, we employ a reference encoder to extract\nfine-grained style and conduct the augmented contrastive learning on global\nstyle. The experimental results show that the proposed method outperforms\nprevious work in voice conversion tasks.", "published": "2023-11-15 03:29:31", "link": "http://arxiv.org/abs/2311.08670v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Autoencoder with Group-based Decoder and Multi-task Optimization for\n  Anomalous Sound Detection", "abstract": "In industry, machine anomalous sound detection (ASD) is in great demand.\nHowever, collecting enough abnormal samples is difficult due to the high cost,\nwhich boosts the rapid development of unsupervised ASD algorithms. Autoencoder\n(AE) based methods have been widely used for unsupervised ASD, but suffer from\nproblems including 'shortcut', poor anti-noise ability and sub-optimal quality\nof features. To address these challenges, we propose a new AE-based framework\ntermed AEGM. Specifically, we first insert an auxiliary classifier into AE to\nenhance ASD in a multi-task learning manner. Then, we design a group-based\ndecoder structure, accompanied by an adaptive loss function, to endow the model\nwith domain-specific knowledge. Results on the DCASE 2021 Task 2 development\nset show that our methods achieve a relative improvement of 13.11% and 15.20%\nrespectively in average AUC over the official AE and MobileNetV2 across test\nsets of seven machines.", "published": "2023-11-15 10:15:37", "link": "http://arxiv.org/abs/2311.08829v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-objective Non-intrusive Hearing-aid Speech Assessment Model", "abstract": "Without the need for a clean reference, non-intrusive speech assessment\nmethods have caught great attention for objective evaluations. While deep\nlearning models have been used to develop non-intrusive speech assessment\nmethods with promising results, there is limited research on hearing-impaired\nsubjects. This study proposes a multi-objective non-intrusive hearing-aid\nspeech assessment model, called HASA-Net Large, which predicts speech quality\nand intelligibility scores based on input speech signals and specified\nhearing-loss patterns. Our experiments showed the utilization of pre-trained\nSSL models leads to a significant boost in speech quality and intelligibility\npredictions compared to using spectrograms as input. Additionally, we examined\nthree distinct fine-tuning approaches that resulted in further performance\nimprovements. Furthermore, we demonstrated that incorporating SSL models\nresulted in greater transferability to OOD dataset. Finally, this study\nintroduces HASA-Net Large, which is a non-invasive approach for evaluating\nspeech quality and intelligibility. HASA-Net Large utilizes raw waveforms and\nhearing-loss patterns to accurately predict speech quality and intelligibility\nlevels for individuals with normal and impaired hearing and demonstrates\nsuperior prediction performance and transferability.", "published": "2023-11-15 11:32:50", "link": "http://arxiv.org/abs/2311.08878v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AI-based soundscape analysis: Jointly identifying sound sources and\n  predicting annoyance", "abstract": "Soundscape studies typically attempt to capture the perception and\nunderstanding of sonic environments by surveying users. However, for long-term\nmonitoring or assessing interventions, sound-signal-based approaches are\nrequired. To this end, most previous research focused on psycho-acoustic\nquantities or automatic sound recognition. Few attempts were made to include\nappraisal (e.g., in circumplex frameworks). This paper proposes an artificial\nintelligence (AI)-based dual-branch convolutional neural network with\ncross-attention-based fusion (DCNN-CaF) to analyze automatic soundscape\ncharacterization, including sound recognition and appraisal. Using the DeLTA\ndataset containing human-annotated sound source labels and perceived annoyance,\nthe DCNN-CaF is proposed to perform sound source classification (SSC) and\nhuman-perceived annoyance rating prediction (ARP). Experimental findings\nindicate that (1) the proposed DCNN-CaF using loudness and Mel features\noutperforms the DCNN-CaF using only one of them. (2) The proposed DCNN-CaF with\ncross-attention fusion outperforms other typical AI-based models and\nsoundscape-related traditional machine learning methods on the SSC and ARP\ntasks. (3) Correlation analysis reveals that the relationship between sound\nsources and annoyance is similar for humans and the proposed AI-based DCNN-CaF\nmodel. (4) Generalization tests show that the proposed model's ARP in the\npresence of model-unknown sound sources is consistent with expert expectations\nand can explain previous findings from the literature on sound-scape\naugmentation.", "published": "2023-11-15 15:23:33", "link": "http://arxiv.org/abs/2311.09030v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CREPE Notes: A new method for segmenting pitch contours into discrete\n  notes", "abstract": "Tracking the fundamental frequency (f0) of a monophonic instrumental\nperformance is effectively a solved problem with several solutions achieving\n99% accuracy. However, the related task of automatic music transcription\nrequires a further processing step to segment an f0 contour into discrete\nnotes. This sub-task of note segmentation is necessary to enable a range of\napplications including musicological analysis and symbolic music generation.\nBuilding on CREPE, a state-of-the-art monophonic pitch tracking solution based\non a simple neural network, we propose a simple and effective method for\npost-processing CREPE's output to achieve monophonic note segmentation. The\nproposed method demonstrates state-of-the-art results on two challenging\ndatasets of monophonic instrumental music. Our approach also gives a 97%\nreduction in the total number of parameters used when compared with other deep\nlearning based methods.", "published": "2023-11-15 11:43:48", "link": "http://arxiv.org/abs/2311.08884v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can MusicGen Create Training Data for MIR Tasks?", "abstract": "We are investigating the broader concept of using AI-based generative music\nsystems to generate training data for Music Information Retrieval (MIR) tasks.\nTo kick off this line of work, we ran an initial experiment in which we trained\na genre classifier on a fully artificial music dataset created with MusicGen.\nWe constructed over 50 000 genre- conditioned textual descriptions and\ngenerated a collection of music excerpts that covers five musical genres. Our\npreliminary results show that the proposed model can learn genre-specific\ncharacteristics from artificial music tracks that generalise well to real-world\nmusic recordings.", "published": "2023-11-15 16:41:56", "link": "http://arxiv.org/abs/2311.09094v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
