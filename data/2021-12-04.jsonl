{"title": "Hierarchical Neural Data Synthesis for Semantic Parsing", "abstract": "Semantic parsing datasets are expensive to collect. Moreover, even the\nquestions pertinent to a given domain, which are the input of a semantic\nparsing system, might not be readily available, especially in cross-domain\nsemantic parsing. This makes data augmentation even more challenging. Existing\nmethods to synthesize new data use hand-crafted or induced rules, requiring\nsubstantial engineering effort and linguistic expertise to achieve good\ncoverage and precision, which limits the scalability. In this work, we propose\na purely neural approach of data augmentation for semantic parsing that\ncompletely removes the need for grammar engineering while achieving higher\nsemantic parsing accuracy. Furthermore, our method can synthesize in the\nzero-shot setting, where only a new domain schema is available without any\ninput-output examples of the new domain. On the Spider cross-domain text-to-SQL\nsemantic parsing benchmark, we achieve the state-of-the-art performance on the\ndevelopment set (77.2% accuracy) using our zero-shot augmentation.", "published": "2021-12-04 01:33:08", "link": "http://arxiv.org/abs/2112.02212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Response Generation for Assistive Use-cases", "abstract": "Conversational agents have become an integral part of the general population\nfor simple task enabling situations. However, these systems are yet to have any\nsocial impact on the diverse and minority population, for example, helping\npeople with neurological disorders, for example ALS, and people with speech,\nlanguage and social communication disorders. Language model technology can play\na huge role to help these users carry out daily communication and social\ninteractions. To enable this population, we build a dialog system that can be\ncontrolled by users using cues or keywords. We build models that can suggest\nrelevant cues in the dialog response context which is used to control response\ngeneration and can speed up communication. We also introduce a keyword loss to\nlexically constrain the model output. We show both qualitatively and\nquantitatively that our models can effectively induce the keyword into the\nmodel response without degrading the quality of response. In the context of\nusage of such systems for people with degenerative disorders, we present human\nevaluation of our cue or keyword predictor and the controllable dialog system\nand show that our models perform significantly better than models without\ncontrol. Our study shows that keyword control on end to end response generation\nmodels is powerful and can enable and empower users with degenerative disorders\nto carry out their day to day communication.", "published": "2021-12-04 05:13:29", "link": "http://arxiv.org/abs/2112.02246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Russian Jeopardy! Data Set for Question-Answering Systems", "abstract": "Question answering (QA) is one of the most common NLP tasks that relates to\nnamed entity recognition, fact extraction, semantic search and some other\nfields. In industry, it is much appreciated in chatbots and corporate\ninformation systems. It is also a challenging task that attracted the attention\nof a very general audience at the quiz show Jeopardy! In this article we\ndescribe a Jeopardy!-like Russian QA data set collected from the official\nRussian quiz database Chgk (che ge ka). The data set includes 379,284 quiz-like\nquestions with 29,375 from the Russian analogue of Jeopardy! - \"Own Game\". We\nobserve its linguistic features and the related QA-task. We conclude about\nperspectives of a QA competition based on the data set collected from this\ndatabase.", "published": "2021-12-04 13:18:12", "link": "http://arxiv.org/abs/2112.02325v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representation Learning for Conversational Data using Discourse Mutual\n  Information Maximization", "abstract": "Although many pretrained models exist for text or images, there have been\nrelatively fewer attempts to train representations specifically for dialog\nunderstanding. Prior works usually relied on finetuned representations based on\ngeneric text representation models like BERT or GPT-2. But such language\nmodeling pretraining objectives do not take the structural information of\nconversational text into consideration. Although generative dialog models can\nlearn structural features too, we argue that the structure-unaware word-by-word\ngeneration is not suitable for effective conversation modeling. We empirically\ndemonstrate that such representations do not perform consistently across\nvarious dialog understanding tasks. Hence, we propose a structure-aware Mutual\nInformation based loss-function DMI (Discourse Mutual Information) for training\ndialog-representation models, that additionally captures the inherent\nuncertainty in response prediction. Extensive evaluation on nine diverse dialog\nmodeling tasks shows that our proposed DMI-based models outperform strong\nbaselines by significant margins.", "published": "2021-12-04 13:17:07", "link": "http://arxiv.org/abs/2112.05787v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LAVT: Language-Aware Vision Transformer for Referring Image Segmentation", "abstract": "Referring image segmentation is a fundamental vision-language task that aims\nto segment out an object referred to by a natural language expression from an\nimage. One of the key challenges behind this task is leveraging the referring\nexpression for highlighting relevant positions in the image. A paradigm for\ntackling this problem is to leverage a powerful vision-language (\"cross-modal\")\ndecoder to fuse features independently extracted from a vision encoder and a\nlanguage encoder. Recent methods have made remarkable advancements in this\nparadigm by exploiting Transformers as cross-modal decoders, concurrent to the\nTransformer's overwhelming success in many other vision-language tasks.\nAdopting a different approach in this work, we show that significantly better\ncross-modal alignments can be achieved through the early fusion of linguistic\nand visual features in intermediate layers of a vision Transformer encoder\nnetwork. By conducting cross-modal feature fusion in the visual feature\nencoding stage, we can leverage the well-proven correlation modeling power of a\nTransformer encoder for excavating helpful multi-modal context. This way,\naccurate segmentation results are readily harvested with a light-weight mask\npredictor. Without bells and whistles, our method surpasses the previous\nstate-of-the-art methods on RefCOCO, RefCOCO+, and G-Ref by large margins.", "published": "2021-12-04 04:53:35", "link": "http://arxiv.org/abs/2112.02244v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "\"Stop Asian Hate!\" : Refining Detection of Anti-Asian Hate Speech During\n  the COVID-19 Pandemic", "abstract": "Content warning: This work displays examples of explicit and/or strongly\noffensive language. Fueled by a surge of anti-Asian xenophobia and prejudice\nduring the COVID-19 pandemic, many have taken to social media to express these\nnegative sentiments. Identifying these posts is crucial for moderation and\nunderstanding the nature of hate in online spaces. In this paper, we create and\nannotate a corpus of tweets to explore anti-Asian hate speech with a finer\nlevel of granularity. Our analysis reveals that this emergent form of hate\nspeech often eludes established approaches. To address this challenge, we\ndevelop a model and an accompanied efficient training regimen that incorporates\nagreement between annotators. Our approach produces up to 8.8% improvement in\nmacro F1 scores over a strong established baseline, indicating its\neffectiveness even in settings where consensus among annotators is low. We\ndemonstrate that we are able to identify hate speech that is systematically\nmissed by established hate speech detectors.", "published": "2021-12-04 06:55:19", "link": "http://arxiv.org/abs/2112.02265v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts", "abstract": "Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention\nrecently for its transferable visual representation learning. However, due to\nthe semantic gap within datasets, CLIP's pre-trained image-text alignment\nbecomes sub-optimal on downstream tasks, which severely harms its transferring\nperformance. To better adapt the cross-modality embedding space, we propose to\nenhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide\ntextual features of different categories to adaptively explore informative\nregions on the image and aggregate visual features by attention mechanisms. In\nthis way, the texts become visual-guided, namely, more semantically correlated\nwith downstream images, which greatly benefits the category-wise matching\nprocess. In few-shot settings, we evaluate our VT-CLIP on 11 well-known\nclassification datasets to demonstrate its effectiveness.", "published": "2021-12-04 18:34:24", "link": "http://arxiv.org/abs/2112.02399v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice\n  Conversion for everyone", "abstract": "YourTTS brings the power of a multilingual approach to the task of zero-shot\nmulti-speaker TTS. Our method builds upon the VITS model and adds several novel\nmodifications for zero-shot multi-speaker and multilingual training. We\nachieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and\nresults comparable to SOTA in zero-shot voice conversion on the VCTK dataset.\nAdditionally, our approach achieves promising results in a target language with\na single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS\nand zero-shot voice conversion systems in low-resource languages. Finally, it\nis possible to fine-tune the YourTTS model with less than 1 minute of speech\nand achieve state-of-the-art results in voice similarity and with reasonable\nquality. This is important to allow synthesis for speakers with a very\ndifferent voice or recording characteristics from those seen during training.", "published": "2021-12-04 19:50:29", "link": "http://arxiv.org/abs/2112.02418v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emojich -- zero-shot emoji generation using Russian language: a\n  technical report", "abstract": "This technical report presents a text-to-image neural network \"Emojich\" that\ngenerates emojis using captions in Russian language as a condition. We aim to\nkeep the generalization ability of a pretrained big model ruDALL-E Malevich\n(XL) 1.3B parameters at the fine-tuning stage, while giving special style to\nthe images generated. Here are presented some engineering methods, code\nrealization, all hyper-parameters for reproducing results and a Telegram bot\nwhere everyone can create their own customized sets of stickers. Also, some\nnewly generated emojis obtained by \"Emojich\" model are demonstrated.", "published": "2021-12-04 23:37:32", "link": "http://arxiv.org/abs/2112.02448v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unraveling Social Perceptions & Behaviors towards Migrants on Twitter", "abstract": "We draw insights from the social psychology literature to identify two facets\nof Twitter deliberations about migrants, i.e., perceptions about migrants and\nbehaviors towards mi-grants. Our theoretical anchoring helped us in identifying\ntwo prevailing perceptions (i.e., sympathy and antipathy) and two dominant\nbehaviors (i.e., solidarity and animosity) of social media users towards\nmigrants. We have employed unsuper-vised and supervised approaches to identify\nthese perceptions and behaviors. In the domain of applied NLP, our study\nof-fers a nuanced understanding of migrant-related Twitter de-liberations. Our\nproposed transformer-based model, i.e., BERT + CNN, has reported an F1-score of\n0.76 and outper-formed other models. Additionally, we argue that tweets\ncon-veying antipathy or animosity can be broadly considered hate speech towards\nmigrants, but they are not the same. Thus, our approach has fine-tuned the\nbinary hate speech detection task by highlighting the granular differences\nbetween perceptual and behavioral aspects of hate speeches.", "published": "2021-12-04 20:45:26", "link": "http://arxiv.org/abs/2112.06642v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Speech Separation Using an Asynchronous Fully Recurrent Convolutional\n  Neural Network", "abstract": "Recent advances in the design of neural network architectures, in particular\nthose specialized in modeling sequences, have provided significant improvements\nin speech separation performance. In this work, we propose to use a\nbio-inspired architecture called Fully Recurrent Convolutional Neural Network\n(FRCNN) to solve the separation task. This model contains bottom-up, top-down\nand lateral connections to fuse information processed at various time-scales\nrepresented by \\textit{stages}. In contrast to the traditional approach\nupdating stages in parallel, we propose to first update the stages one by one\nin the bottom-up direction, then fuse information from adjacent stages\nsimultaneously and finally fuse information from all stages to the bottom stage\ntogether. Experiments showed that this asynchronous updating scheme achieved\nsignificantly better results with much fewer parameters than the traditional\nsynchronous updating scheme. In addition, the proposed model achieved good\nbalance between speech separation accuracy and computational efficiency as\ncompared to other state-of-the-art models on three benchmark datasets.", "published": "2021-12-04 12:44:11", "link": "http://arxiv.org/abs/2112.02321v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards the One Learning Algorithm Hypothesis: A System-theoretic\n  Approach", "abstract": "The existence of a universal learning architecture in human cognition is a\nwidely spread conjecture supported by experimental findings from neuroscience.\nWhile no low-level implementation can be specified yet, an abstract outline of\nhuman perception and learning is believed to entail three basic properties: (a)\nhierarchical attention and processing, (b) memory-based knowledge\nrepresentation, and (c) progressive learning and knowledge compaction. We\napproach the design of such a learning architecture from a system-theoretic\nviewpoint, developing a closed-loop system with three main components: (i) a\nmulti-resolution analysis pre-processor, (ii) a group-invariant feature\nextractor, and (iii) a progressive knowledge-based learning module.\nMulti-resolution feedback loops are used for learning, i.e., for adapting the\nsystem parameters to online observations. To design (i) and (ii), we build upon\nthe established theory of wavelet-based multi-resolution analysis and the\nproperties of group convolution operators. Regarding (iii), we introduce a\nnovel learning algorithm that constructs progressively growing knowledge\nrepresentations in multiple resolutions. The proposed algorithm is an extension\nof the Online Deterministic Annealing (ODA) algorithm based on annealing\noptimization, solved using gradient-free stochastic approximation. ODA has\ninherent robustness and regularization properties and provides a means to\nprogressively increase the complexity of the learning model i.e. the number of\nthe neurons, as needed, through an intuitive bifurcation phenomenon. The\nproposed multi-resolution approach is hierarchical, progressive,\nknowledge-based, and interpretable. We illustrate the properties of the\nproposed architecture in the context of the state-of-the-art learning\nalgorithms and deep learning methods.", "published": "2021-12-04 05:54:33", "link": "http://arxiv.org/abs/2112.02256v1", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.AS", "eess.IV", "eess.SY"], "primary_category": "cs.LG"}
