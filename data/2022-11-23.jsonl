{"title": "Word-Level Representation From Bytes For Language Modeling", "abstract": "Modern language models mostly take sub-words as input, a design that balances\nthe trade-off between vocabulary size, number of parameters, and performance.\nHowever, sub-word tokenization still has disadvantages like not being robust to\nnoise and difficult to generalize to new languages. Also, the current trend of\nscaling up models reveals that larger models require larger embeddings but that\nmakes parallelization hard. Previous work on image classification proves\nsplitting raw input into a sequence of chucks is a strong, model-agnostic\ninductive bias. Based on this observation, we rethink the existing\ncharacter-aware method that takes character-level inputs but makes word-level\nsequence modeling and prediction. We overhaul this method by introducing a\ncross-attention network that builds word-level representation directly from\nbytes, and a sub-word level prediction based on word-level hidden states to\navoid the time and space requirement of word-level prediction. With these two\nimprovements combined, we have a token free model with slim input embeddings\nfor downstream tasks. We name our method Byte2Word and perform evaluations on\nlanguage modeling and text classification. Experiments show that Byte2Word is\non par with the strong sub-word baseline BERT but only takes up 10\\% of\nembedding size. We further test our method on synthetic noise and cross-lingual\ntransfer and find it competitive to baseline methods on both settings.", "published": "2022-11-23 03:11:13", "link": "http://arxiv.org/abs/2211.12677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding Compression for Text Classification Using Dictionary Screening", "abstract": "In this paper, we propose a dictionary screening method for embedding\ncompression in text classification tasks. The key purpose of this method is to\nevaluate the importance of each keyword in the dictionary. To this end, we\nfirst train a pre-specified recurrent neural network-based model using a full\ndictionary. This leads to a benchmark model, which we then use to obtain the\npredicted class probabilities for each sample in a dataset. Next, to evaluate\nthe impact of each keyword in affecting the predicted class probabilities, we\ndevelop a novel method for assessing the importance of each keyword in a\ndictionary. Consequently, each keyword can be screened, and only the most\nimportant keywords are reserved. With these screened keywords, a new dictionary\nwith a considerably reduced size can be constructed. Accordingly, the original\ntext sequence can be substantially compressed. The proposed method leads to\nsignificant reductions in terms of parameters, average text sequence, and\ndictionary size. Meanwhile, the prediction power remains very competitive\ncompared to the benchmark model. Extensive numerical studies are presented to\ndemonstrate the empirical performance of the proposed method.", "published": "2022-11-23 05:32:13", "link": "http://arxiv.org/abs/2211.12715v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Agent-Specific Deontic Modality Detection in Legal Language", "abstract": "Legal documents are typically long and written in legalese, which makes it\nparticularly difficult for laypeople to understand their rights and duties.\nWhile natural language understanding technologies can be valuable in supporting\nsuch understanding in the legal domain, the limited availability of datasets\nannotated for deontic modalities in the legal domain, due to the cost of hiring\nexperts and privacy issues, is a bottleneck. To this end, we introduce,\nLEXDEMOD, a corpus of English contracts annotated with deontic modality\nexpressed with respect to a contracting party or agent along with the modal\ntriggers. We benchmark this dataset on two tasks: (i) agent-specific\nmulti-label deontic modality classification, and (ii) agent-specific deontic\nmodality and trigger span detection using Transformer-based (Vaswani et al.,\n2017) language models. Transfer learning experiments show that the linguistic\ndiversity of modal expressions in LEXDEMOD generalizes reasonably from lease to\nemployment and rental agreements. A small case study indicates that a model\ntrained on LEXDEMOD can detect red flags with high recall. We believe our work\noffers a new research direction for deontic modality detection in the legal\ndomain.", "published": "2022-11-23 07:32:23", "link": "http://arxiv.org/abs/2211.12752v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Data Sparsity for Short Text Topic Modeling by Topic-Semantic\n  Contrastive Learning", "abstract": "To overcome the data sparsity issue in short text topic modeling, existing\nmethods commonly rely on data augmentation or the data characteristic of short\ntexts to introduce more word co-occurrence information. However, most of them\ndo not make full use of the augmented data or the data characteristic: they\ninsufficiently learn the relations among samples in data, leading to dissimilar\ntopic distributions of semantically similar text pairs. To better address data\nsparsity, in this paper we propose a novel short text topic modeling framework,\nTopic-Semantic Contrastive Topic Model (TSCTM). To sufficiently model the\nrelations among samples, we employ a new contrastive learning method with\nefficient positive and negative sampling strategies based on topic semantics.\nThis contrastive learning method refines the representations, enriches the\nlearning signals, and thus mitigates the sparsity issue. Extensive experimental\nresults show that our TSCTM outperforms state-of-the-art baselines regardless\nof the data augmentation availability, producing high-quality topics and topic\ndistributions.", "published": "2022-11-23 11:33:43", "link": "http://arxiv.org/abs/2211.12878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Lifelong Language Learning", "abstract": "Lifelong learning aims to accumulate knowledge and alleviate catastrophic\nforgetting when learning tasks sequentially. However, existing lifelong\nlanguage learning methods only focus on the supervised learning setting.\nUnlabeled data, which can be easily accessed in real-world scenarios, are\nunderexplored. In this paper, we explore a novel setting, semi-supervised\nlifelong language learning (SSLL), where a model learns sequentially arriving\nlanguage tasks with both labeled and unlabeled data. We propose an unlabeled\ndata enhanced lifelong learner to explore SSLL. Specially, we dedicate\ntask-specific modules to alleviate catastrophic forgetting and design two\nmodules to exploit unlabeled data: (1) a virtual supervision enhanced task\nsolver is constructed on a teacher-student framework to mine the underlying\nknowledge from unlabeled data; and (2) a backward augmented learner is built to\nencourage knowledge transfer from newly arrived unlabeled data to previous\ntasks. Experimental results on various language tasks demonstrate our model's\neffectiveness and superiority over competitive baselines under the new setting\nSSLL.", "published": "2022-11-23 15:51:33", "link": "http://arxiv.org/abs/2211.13050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Schr\u00f6dinger's Bat: Diffusion Models Sometimes Generate Polysemous\n  Words in Superposition", "abstract": "Recent work has shown that despite their impressive capabilities,\ntext-to-image diffusion models such as DALL-E 2 (Ramesh et al., 2022) can\ndisplay strange behaviours when a prompt contains a word with multiple possible\nmeanings, often generating images containing both senses of the word (Rassin et\nal., 2022). In this work we seek to put forward a possible explanation of this\nphenomenon. Using the similar Stable Diffusion model (Rombach et al., 2022), we\nfirst show that when given an input that is the sum of encodings of two\ndistinct words, the model can produce an image containing both concepts\nrepresented in the sum. We then demonstrate that the CLIP encoder used to\nencode prompts (Radford et al., 2021) encodes polysemous words as a\nsuperposition of meanings, and that using linear algebraic techniques we can\nedit these representations to influence the senses represented in the generated\nimages. Combining these two findings, we suggest that the homonym duplication\nphenomenon described by Rassin et al. (2022) is caused by diffusion models\nproducing images representing both of the meanings that are present in\nsuperposition in the encoding of a polysemous word.", "published": "2022-11-23 16:26:49", "link": "http://arxiv.org/abs/2211.13095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Open-Domain QA Reader Utilize External Knowledge Efficiently like\n  Humans?", "abstract": "Recent state-of-the-art open-domain QA models are typically based on a two\nstage retriever-reader approach in which the retriever first finds the relevant\nknowledge/passages and the reader then leverages that to predict the answer.\nPrior work has shown that the performance of the reader usually tends to\nimprove with the increase in the number of these passages. Thus,\nstate-of-the-art models use a large number of passages (e.g. 100) for\ninference. While the reader in this approach achieves high prediction\nperformance, its inference is computationally very expensive. We humans, on the\nother hand, use a more efficient strategy while answering: firstly, if we can\nconfidently answer the question using our already acquired knowledge then we do\nnot even use the external knowledge, and in the case when we do require\nexternal knowledge, we don't read the entire knowledge at once, instead, we\nonly read that much knowledge that is sufficient to find the answer. Motivated\nby this procedure, we ask a research question \"Can the open-domain QA reader\nutilize external knowledge efficiently like humans without sacrificing the\nprediction performance?\"\n  Driven by this question, we explore an approach that utilizes both\n'closed-book' (leveraging knowledge already present in the model parameters)\nand 'open-book' inference (leveraging external knowledge). Furthermore, instead\nof using a large fixed number of passages for open-book inference, we\ndynamically read the external knowledge in multiple 'knowledge iterations'.\nThrough comprehensive experiments on NQ and TriviaQA datasets, we demonstrate\nthat this dynamic reading approach improves both the 'inference efficiency' and\nthe 'prediction accuracy' of the reader. Comparing with the FiD reader, this\napproach matches its accuracy by utilizing just 18.32% of its reader inference\ncost and also outperforms it by achieving up to 55.10% accuracy on NQ Open.", "published": "2022-11-23 05:04:56", "link": "http://arxiv.org/abs/2211.12707v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Breaking the Representation Bottleneck of Chinese Characters: Neural\n  Machine Translation with Stroke Sequence Modeling", "abstract": "Existing research generally treats Chinese character as a minimum unit for\nrepresentation. However, such Chinese character representation will suffer two\nbottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich\ninternal features (e.g., radicals and strokes); and 2) Parameter bottleneck,\neach individual character has to be represented by a unique vector. In this\npaper, we introduce a novel representation method for Chinese characters to\nbreak the bottlenecks, namely StrokeNet, which represents a Chinese character\nby a Latinized stroke sequence (e.g., \"ao1 (concave)\" to \"ajaie\" and \"tu1\n(convex)\" to \"aeaqe\"). Specifically, StrokeNet maps each stroke to a specific\nLatin character, thus allowing similar Chinese characters to have similar Latin\nrepresentations. With the introduction of StrokeNet to neural machine\ntranslation (NMT), many powerful but not applicable techniques to non-Latin\nlanguages (e.g., shared subword vocabulary learning and ciphertext-based data\naugmentation) can now be perfectly implemented. Experiments on the widely-used\nNIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT\ntasks show that StrokeNet can provide a significant performance boost over the\nstrong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17\nChinese-English task which is better than any previously reported results\nwithout using monolingual data. Code and scripts are freely available at\nhttps://github.com/zjwang21/StrokeNet.", "published": "2022-11-23 08:49:43", "link": "http://arxiv.org/abs/2211.12781v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tell Me What Happened: Unifying Text-guided Video Completion via\n  Multimodal Masked Video Generation", "abstract": "Generating a video given the first several static frames is challenging as it\nanticipates reasonable future frames with temporal coherence. Besides video\nprediction, the ability to rewind from the last frame or infilling between the\nhead and tail is also crucial, but they have rarely been explored for video\ncompletion. Since there could be different outcomes from the hints of just a\nfew frames, a system that can follow natural language to perform video\ncompletion may significantly improve controllability. Inspired by this, we\nintroduce a novel task, text-guided video completion (TVC), which requests the\nmodel to generate a video from partial frames guided by an instruction. We then\npropose Multimodal Masked Video Generation (MMVG) to address this TVC task.\nDuring training, MMVG discretizes the video frames into visual tokens and masks\nmost of them to perform video completion from any time point. At inference\ntime, a single MMVG model can address all 3 cases of TVC, including video\nprediction, rewind, and infilling, by applying corresponding masking\nconditions. We evaluate MMVG in various video scenarios, including egocentric,\nanimation, and gaming. Extensive experimental results indicate that MMVG is\neffective in generating high-quality visual appearances with text guidance for\nTVC.", "published": "2022-11-23 10:14:12", "link": "http://arxiv.org/abs/2211.12824v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GraphWOZ: Dialogue Management with Conversational Knowledge Graphs", "abstract": "We present a new approach to dialogue management using conversational\nknowledge graphs as core representation of the dialogue state. To this end, we\nintroduce a new dataset, GraphWOZ, which comprises Wizard-of-Oz dialogues in\nwhich human participants interact with a robot acting as a receptionist. In\ncontrast to most existing work on dialogue management, GraphWOZ relies on a\ndialogue state explicitly represented as a dynamic knowledge graph instead of a\nfixed set of slots. This graph is composed of a varying number of entities\n(such as individuals, places, events, utterances and mentions) and relations\nbetween them (such as persons being part of a group or attending an event). The\ngraph is then regularly updated on the basis of new observations and system\nactions. GraphWOZ is released along with detailed manual annotations related to\nthe user intents, system responses, and reference relations occurring in both\nuser and system turns. Based on GraphWOZ, we present experimental results for\ntwo dialogue management tasks, namely conversational entity linking and\nresponse ranking. For conversational entity linking, we show how to connect\nutterance mentions to their corresponding entity in the knowledge graph with a\nneural model relying on a combination of both string and graph-based features.\nResponse ranking is then performed by summarizing the relevant content of the\ngraph into a text, which is concatenated with the dialogue history and employed\nas input to score possible responses to a given dialogue state.", "published": "2022-11-23 10:53:21", "link": "http://arxiv.org/abs/2211.12852v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sarcasm Detection Framework Using Context, Emotion and Sentiment\n  Features", "abstract": "Sarcasm detection is an essential task that can help identify the actual\nsentiment in user-generated data, such as discussion forums or tweets. Sarcasm\nis a sophisticated form of linguistic expression because its surface meaning\nusually contradicts its inner, deeper meaning. Such incongruity is the\nessential component of sarcasm, however, it makes sarcasm detection quite a\nchallenging task. In this paper, we propose a model, that incorporates\ndifferent features to capture the incongruity intrinsic to sarcasm. We use a\npre-trained transformer and CNN to capture context features, and we use\ntransformers pre-trained on emotions detection and sentiment analysis tasks.\nOur approach outperformed previous state-of-the-art results on four datasets\nfrom social networking platforms and online media.", "published": "2022-11-23 15:14:44", "link": "http://arxiv.org/abs/2211.13014v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TorchScale: Transformers at Scale", "abstract": "Large Transformers have achieved state-of-the-art performance across many\ntasks. Most open-source libraries on scaling Transformers focus on improving\ntraining or inference with better parallelization. In this work, we present\nTorchScale, an open-source toolkit that allows researchers and developers to\nscale up Transformers efficiently and effectively. TorchScale has the\nimplementation of several modeling techniques, which can improve modeling\ngenerality and capability, as well as training stability and efficiency.\nExperimental results on language modeling and neural machine translation\ndemonstrate that TorchScale can successfully scale Transformers to different\nsizes without tears. The library is available at https://aka.ms/torchscale.", "published": "2022-11-23 17:58:51", "link": "http://arxiv.org/abs/2211.13184v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SeedBERT: Recovering Annotator Rating Distributions from an Aggregated\n  Label", "abstract": "Many machine learning tasks -- particularly those in affective computing --\nare inherently subjective. When asked to classify facial expressions or to rate\nan individual's attractiveness, humans may disagree with one another, and no\nsingle answer may be objectively correct. However, machine learning datasets\ncommonly have just one \"ground truth\" label for each sample, so models trained\non these labels may not perform well on tasks that are subjective in nature.\nThough allowing models to learn from the individual annotators' ratings may\nhelp, most datasets do not provide annotator-specific labels for each sample.\nTo address this issue, we propose SeedBERT, a method for recovering annotator\nrating distributions from a single label by inducing pre-trained models to\nattend to different portions of the input. Our human evaluations indicate that\nSeedBERT's attention mechanism is consistent with human sources of annotator\ndisagreement. Moreover, in our empirical evaluations using large language\nmodels, SeedBERT demonstrates substantial gains in performance on downstream\nsubjective tasks compared both to standard deep learning models and to other\ncurrent models that account explicitly for annotator disagreement.", "published": "2022-11-23 18:35:15", "link": "http://arxiv.org/abs/2211.13196v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mask the Correct Tokens: An Embarrassingly Simple Approach for Error\n  Correction", "abstract": "Text error correction aims to correct the errors in text sequences such as\nthose typed by humans or generated by speech recognition models. Previous error\ncorrection methods usually take the source (incorrect) sentence as encoder\ninput and generate the target (correct) sentence through the decoder. Since the\nerror rate of the incorrect sentence is usually low (e.g., 10\\%), the\ncorrection model can only learn to correct on limited error tokens but\ntrivially copy on most tokens (correct tokens), which harms the effective\ntraining of error correction. In this paper, we argue that the correct tokens\nshould be better utilized to facilitate effective training and then propose a\nsimple yet effective masking strategy to achieve this goal. Specifically, we\nrandomly mask out a part of the correct tokens in the source sentence and let\nthe model learn to not only correct the original error tokens but also predict\nthe masked tokens based on their context information. Our method enjoys several\nadvantages: 1) it alleviates trivial copy; 2) it leverages effective training\nsignals from correct tokens; 3) it is a plug-and-play module and can be applied\nto different models and tasks. Experiments on spelling error correction and\nspeech recognition error correction on Mandarin datasets and grammar error\ncorrection on English datasets with both autoregressive and non-autoregressive\ngeneration models show that our method improves the correction accuracy\nconsistently.", "published": "2022-11-23 19:05:48", "link": "http://arxiv.org/abs/2211.13252v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rank-One Editing of Encoder-Decoder Models", "abstract": "Large sequence to sequence models for tasks such as Neural Machine\nTranslation (NMT) are usually trained over hundreds of millions of samples.\nHowever, training is just the origin of a model's life-cycle. Real-world\ndeployments of models require further behavioral adaptations as new\nrequirements emerge or shortcomings become known. Typically, in the space of\nmodel behaviors, behavior deletion requests are addressed through model\nretrainings whereas model finetuning is done to address behavior addition\nrequests, both procedures being instances of data-based model intervention. In\nthis work, we present a preliminary study investigating rank-one editing as a\ndirect intervention method for behavior deletion requests in encoder-decoder\ntransformer models. We propose four editing tasks for NMT and show that the\nproposed editing algorithm achieves high efficacy, while requiring only a\nsingle instance of positive example to fix an erroneous (negative) model\nbehavior.", "published": "2022-11-23 21:34:57", "link": "http://arxiv.org/abs/2211.13317v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Report on the Euphemisms Detection Shared Task", "abstract": "This paper presents The Shared Task on Euphemism Detection for the Third\nWorkshop on Figurative Language Processing (FigLang 2022) held in conjunction\nwith EMNLP 2022. Participants were invited to investigate the euphemism\ndetection task: given input text, identify whether it contains a euphemism. The\ninput data is a corpus of sentences containing potentially euphemistic terms\n(PETs) collected from the GloWbE corpus (Davies and Fuchs, 2015), and are\nhuman-annotated as containing either a euphemistic or literal usage of a PET.\nIn this paper, we present the results and analyze the common themes, methods\nand findings of the participating teams", "published": "2022-11-23 22:06:35", "link": "http://arxiv.org/abs/2211.13327v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Focal Loss to Fight Shallow Heuristics: An Empirical Analysis of\n  Modulated Cross-Entropy in Natural Language Inference", "abstract": "There is no such thing as a perfect dataset. In some datasets, deep neural\nnetworks discover underlying heuristics that allow them to take shortcuts in\nthe learning process, resulting in poor generalization capability. Instead of\nusing standard cross-entropy, we explore whether a modulated version of\ncross-entropy called focal loss can constrain the model so as not to use\nheuristics and improve generalization performance. Our experiments in natural\nlanguage inference show that focal loss has a regularizing impact on the\nlearning process, increasing accuracy on out-of-distribution data, but slightly\ndecreasing performance on in-distribution data. Despite the improved\nout-of-distribution performance, we demonstrate the shortcomings of focal loss\nand its inferiority in comparison to the performance of methods such as\nunbiased focal loss and self-debiasing ensembles.", "published": "2022-11-23 22:19:00", "link": "http://arxiv.org/abs/2211.13331v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Data Recasting to Enhance Tabular Reasoning", "abstract": "Creating challenging tabular inference data is essential for learning complex\nreasoning. Prior work has mostly relied on two data generation strategies. The\nfirst is human annotation, which yields linguistically diverse data but is\ndifficult to scale. The second category for creation is synthetic generation,\nwhich is scalable and cost effective but lacks inventiveness. In this research,\nwe present a framework for semi-automatically recasting existing tabular data\nto make use of the benefits of both approaches. We utilize our framework to\nbuild tabular NLI instances from five datasets that were initially intended for\ntasks like table2text creation, tabular Q/A, and semantic parsing. We\ndemonstrate that recasted data could be used as evaluation benchmarks as well\nas augmentation data to enhance performance on tabular NLI tasks. Furthermore,\nwe investigate the effectiveness of models trained on recasted data in the\nzero-shot scenario, and analyse trends in performance across different recasted\ndatasets types.", "published": "2022-11-23 00:04:57", "link": "http://arxiv.org/abs/2211.12641v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical\n  Reasoning over Tabular and Textual Data", "abstract": "Numerical reasoning over hybrid data containing tables and long texts has\nrecently received research attention from the AI community. To generate an\nexecutable reasoning program consisting of math and table operations to answer\na question, state-of-the-art methods use a retriever-generator pipeline.\nHowever, their retrieval results are static, while different generation steps\nmay rely on different sentences. To attend to the retrieved information that is\nrelevant to each generation step, in this paper, we propose DyRRen, an extended\nretriever-reranker-generator framework where each generation step is enhanced\nby a dynamic reranking of retrieved sentences. It outperforms existing\nbaselines on the FinQA dataset.", "published": "2022-11-23 02:41:50", "link": "http://arxiv.org/abs/2211.12668v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Continual Learning of Natural Language Processing Tasks: A Survey", "abstract": "Continual learning (CL) is a learning paradigm that emulates the human\ncapability of learning and accumulating knowledge continually without\nforgetting the previously learned knowledge and also transferring the learned\nknowledge to help learn new tasks better. This survey presents a comprehensive\nreview and analysis of the recent progress of CL in NLP, which has significant\ndifferences from CL in computer vision and machine learning. It covers (1) all\nCL settings with a taxonomy of existing techniques; (2) catastrophic forgetting\n(CF) prevention, (3) knowledge transfer (KT), which is particularly important\nfor NLP tasks; and (4) some theory and the hidden challenge of inter-task class\nseparation (ICS). (1), (3) and (4) have not been included in the existing\nsurvey. Finally, a list of future directions is discussed.", "published": "2022-11-23 04:46:28", "link": "http://arxiv.org/abs/2211.12701v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "RoentGen: Vision-Language Foundation Model for Chest X-ray Generation", "abstract": "Multimodal models trained on large natural image-text pair datasets have\nexhibited astounding abilities in generating high-quality images. Medical\nimaging data is fundamentally different to natural images, and the language\nused to succinctly capture relevant details in medical data uses a different,\nnarrow but semantically rich, domain-specific vocabulary. Not surprisingly,\nmulti-modal models trained on natural image-text pairs do not tend to\ngeneralize well to the medical domain. Developing generative imaging models\nfaithfully representing medical concepts while providing compositional\ndiversity could mitigate the existing paucity of high-quality, annotated\nmedical imaging datasets. In this work, we develop a strategy to overcome the\nlarge natural-medical distributional shift by adapting a pre-trained latent\ndiffusion model on a corpus of publicly available chest x-rays (CXR) and their\ncorresponding radiology (text) reports. We investigate the model's ability to\ngenerate high-fidelity, diverse synthetic CXR conditioned on text prompts. We\nassess the model outputs quantitatively using image quality metrics, and\nevaluate image quality and text-image alignment by human domain experts. We\npresent evidence that the resulting model (RoentGen) is able to create visually\nconvincing, diverse synthetic CXR images, and that the output can be controlled\nto a new extent by using free-form text prompts including radiology-specific\nlanguage. Fine-tuning this model on a fixed training set and using it as a data\naugmentation method, we measure a 5% improvement of a classifier trained\njointly on synthetic and real images, and a 3% improvement when trained on a\nlarger but purely synthetic training set. Finally, we observe that this\nfine-tuning distills in-domain knowledge in the text-encoder and can improve\nits representation capabilities of certain diseases like pneumothorax by 25%.", "published": "2022-11-23 06:58:09", "link": "http://arxiv.org/abs/2211.12737v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval", "abstract": "Many recent studies leverage the pre-trained CLIP for text-video cross-modal\nretrieval by tuning the backbone with additional heavy modules, which not only\nbrings huge computational burdens with much more parameters, but also leads to\nthe knowledge forgetting from upstream models. In this work, we propose the\nVoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the\ntext-video retrieval task. The proposed VoP is an end-to-end framework with\nboth video & text prompts introducing, which can be regarded as a powerful\nbaseline with only 0.1% trainable parameters. Further, based on the\nspatio-temporal characteristics of videos, we develop three novel video prompt\nmechanisms to improve the performance with different scales of trainable\nparameters. The basic idea of the VoP enhancement is to model the frame\nposition, frame context, and layer function with specific trainable prompts,\nrespectively. Extensive experiments show that compared to full fine-tuning, the\nenhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval\nbenchmarks with 6x less parameter overhead. The code will be available at\nhttps://github.com/bighuang624/VoP.", "published": "2022-11-23 08:20:29", "link": "http://arxiv.org/abs/2211.12764v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "IMaSC -- ICFOSS Malayalam Speech Corpus", "abstract": "Modern text-to-speech (TTS) systems use deep learning to synthesize speech\nincreasingly approaching human quality, but they require a database of high\nquality audio-text sentence pairs for training. Malayalam, the official\nlanguage of the Indian state of Kerala and spoken by 35+ million people, is a\nlow resource language in terms of available corpora for TTS systems. In this\npaper, we present IMaSC, a Malayalam text and speech corpora containing\napproximately 50 hours of recorded speech. With 8 speakers and a total of\n34,473 text-audio pairs, IMaSC is larger than every other publicly available\nalternative. We evaluated the database by using it to train TTS models for each\nspeaker based on a modern deep learning architecture. Via subjective\nevaluation, we show that our models perform significantly better in terms of\nnaturalness compared to previous studies and publicly available models, with an\naverage mean opinion score of 4.50, indicating that the synthesized speech is\nclose to human quality.", "published": "2022-11-23 09:21:01", "link": "http://arxiv.org/abs/2211.12796v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Generation of Socratic Subquestions for Teaching Math Word\n  Problems", "abstract": "Socratic questioning is an educational method that allows students to\ndiscover answers to complex problems by asking them a series of thoughtful\nquestions. Generation of didactically sound questions is challenging, requiring\nunderstanding of the reasoning process involved in the problem. We hypothesize\nthat such questioning strategy can not only enhance the human performance, but\nalso assist the math word problem (MWP) solvers. In this work, we explore the\nability of large language models (LMs) in generating sequential questions for\nguiding math word problem-solving. We propose various guided question\ngeneration schemes based on input conditioning and reinforcement learning. On\nboth automatic and human quality evaluations, we find that LMs constrained with\ndesirable question properties generate superior questions and improve the\noverall performance of a math word problem solver. We conduct a preliminary\nuser study to examine the potential value of such question generation models in\nthe education domain. Results suggest that the difficulty level of problems\nplays an important role in determining whether questioning improves or hinders\nhuman performance. We discuss the future of using such questioning strategies\nin education.", "published": "2022-11-23 10:40:22", "link": "http://arxiv.org/abs/2211.12835v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "This is the way: designing and compiling LEPISZCZE, a comprehensive NLP\n  benchmark for Polish", "abstract": "The availability of compute and data to train larger and larger language\nmodels increases the demand for robust methods of benchmarking the true\nprogress of LM training. Recent years witnessed significant progress in\nstandardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or\nKILT have become de facto standard tools to compare large language models.\nFollowing the trend to replicate GLUE for other languages, the KLEJ benchmark\nhas been released for Polish. In this paper, we evaluate the progress in\nbenchmarking for low-resourced languages. We note that only a handful of\nlanguages have such comprehensive benchmarks. We also note the gap in the\nnumber of tasks being evaluated by benchmarks for resource-rich English/Chinese\nand the rest of the world. In this paper, we introduce LEPISZCZE (the Polish\nword for glew, the Middle English predecessor of glue), a new, comprehensive\nbenchmark for Polish NLP with a large variety of tasks and high-quality\noperationalization of the benchmark. We design LEPISZCZE with flexibility in\nmind. Including new models, datasets, and tasks is as simple as possible while\nstill offering data versioning and model tracking. In the first run of the\nbenchmark, we test 13 experiments (task and dataset pairs) based on the five\nmost recent LMs for Polish. We use five datasets from the Polish benchmark and\nadd eight novel datasets. As the paper's main contribution, apart from\nLEPISZCZE, we provide insights and experiences learned while creating the\nbenchmark for Polish as the blueprint to design similar benchmarks for other\nlow-resourced languages.", "published": "2022-11-23 16:51:09", "link": "http://arxiv.org/abs/2211.13112v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors", "abstract": "Recently, text-to-image diffusion models have shown remarkable capabilities\nin creating realistic images from natural language prompts. However, few works\nhave explored using these models for semantic localization or grounding. In\nthis work, we explore how an off-the-shelf text-to-image diffusion model,\ntrained without exposure to localization information, can ground various\nsemantic phrases without segmentation-specific re-training. We introduce an\ninference time optimization process capable of generating segmentation masks\nconditioned on natural language prompts. Our proposal, Peekaboo, is a\nfirst-of-its-kind zero-shot, open-vocabulary, unsupervised semantic grounding\ntechnique leveraging diffusion models without any training. We evaluate\nPeekaboo on the Pascal VOC dataset for unsupervised semantic segmentation and\nthe RefCOCO dataset for referring segmentation, showing results competitive\nwith promising results. We also demonstrate how Peekaboo can be used to\ngenerate images with transparency, even though the underlying diffusion model\nwas only trained on RGB images - which to our knowledge we are the first to\nattempt. Please see our project page, including our code:\nhttps://ryanndagreat.github.io/peekaboo", "published": "2022-11-23 18:59:05", "link": "http://arxiv.org/abs/2211.13224v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Unified Multimodal Model with Unlikelihood Training for Visual Dialog", "abstract": "The task of visual dialog requires a multimodal chatbot to answer sequential\nquestions from humans about image content. Prior work performs the standard\nlikelihood training for answer generation on the positive instances (involving\ncorrect answers). However, the likelihood objective often leads to frequent and\ndull outputs and fails to exploit the useful knowledge from negative instances\n(involving incorrect answers). In this paper, we propose a Unified Multimodal\nModel with UnLikelihood Training, named UniMM-UL, to tackle this problem.\nFirst, to improve visual dialog understanding and generation by multi-task\nlearning, our model extends ViLBERT from only supporting answer discrimination\nto holding both answer discrimination and answer generation seamlessly by\ndifferent attention masks. Specifically, in order to make the original\ndiscriminative model compatible with answer generation, we design novel\ngenerative attention masks to implement the autoregressive Masked Language\nModeling (autoregressive MLM) task. And to attenuate the adverse effects of the\nlikelihood objective, we exploit unlikelihood training on negative instances to\nmake the model less likely to generate incorrect answers. Then, to utilize\ndense annotations, we adopt different fine-tuning methods for both generating\nand discriminating answers, rather than just for discriminating answers as in\nthe prior work. Finally, on the VisDial dataset, our model achieves the best\ngenerative results (69.23 NDCG score). And our model also yields comparable\ndiscriminative results with the state-of-the-art in both single-model and\nensemble settings (75.92 and 76.17 NDCG scores).", "published": "2022-11-23 13:47:42", "link": "http://arxiv.org/abs/2211.13235v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Device Directedness with Contextual Cues for Spoken Dialog Systems", "abstract": "In this work, we define barge-in verification as a supervised learning task\nwhere audio-only information is used to classify user spoken dialogue into true\nand false barge-ins. Following the success of pre-trained models, we use\nlow-level speech representations from a self-supervised representation learning\nmodel for our downstream classification task. Further, we propose a novel\ntechnique to infuse lexical information directly into speech representations to\nimprove the domain-specific language information implicitly learned during\npre-training. Experiments conducted on spoken dialog data show that our\nproposed model trained to validate barge-in entirely from speech\nrepresentations is faster by 38% relative and achieves 4.5% relative F1 score\nimprovement over a baseline LSTM model that uses both audio and Automatic\nSpeech Recognition (ASR) 1-best hypotheses. On top of this, our best proposed\nmodel with lexically infused representations along with contextual features\nprovides a further relative improvement of 5.7% in the F1 score but only 22%\nfaster than the baseline.", "published": "2022-11-23 19:49:11", "link": "http://arxiv.org/abs/2211.13280v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SEAT: Stable and Explainable Attention", "abstract": "Currently, attention mechanism becomes a standard fixture in most\nstate-of-the-art natural language processing (NLP) models, not only due to\noutstanding performance it could gain, but also due to plausible innate\nexplanation for the behaviors of neural architectures it provides, which is\nnotoriously difficult to analyze. However, recent studies show that attention\nis unstable against randomness and perturbations during training or testing,\nsuch as random seeds and slight perturbation of embedding vectors, which\nimpedes it from becoming a faithful explanation tool. Thus, a natural question\nis whether we can find some substitute of the current attention which is more\nstable and could keep the most important characteristics on explanation and\nprediction of attention. In this paper, to resolve the problem, we provide a\nfirst rigorous definition of such alternate namely SEAT (Stable and Explainable\nAttention). Specifically, a SEAT should has the following three properties: (1)\nIts prediction distribution is enforced to be close to the distribution based\non the vanilla attention; (2) Its top-k indices have large overlaps with those\nof the vanilla attention; (3) It is robust w.r.t perturbations, i.e., any\nslight perturbation on SEAT will not change the prediction distribution too\nmuch, which implicitly indicates that it is stable to randomness and\nperturbations. Finally, through intensive experiments on various datasets, we\ncompare our SEAT with other baseline methods using RNN, BiLSTM and BERT\narchitectures via six different evaluation metrics for model interpretation,\nstability and accuracy. Results show that SEAT is more stable against different\nperturbations and randomness while also keeps the explainability of attention,\nwhich indicates it is a more faithful explanation. Moreover, compared with\nvanilla attention, there is almost no utility (accuracy) degradation for SEAT.", "published": "2022-11-23 20:33:30", "link": "http://arxiv.org/abs/2211.13290v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SciRepEval: A Multi-Format Benchmark for Scientific Document\n  Representations", "abstract": "Learned representations of scientific documents can serve as valuable input\nfeatures for downstream tasks without further fine-tuning. However, existing\nbenchmarks for evaluating these representations fail to capture the diversity\nof relevant tasks. In response, we introduce SciRepEval, the first\ncomprehensive benchmark for training and evaluating scientific document\nrepresentations. It includes 24 challenging and realistic tasks, 8 of which are\nnew, across four formats: classification, regression, ranking and search. We\nthen use this benchmark to study and improve the generalization ability of\nscientific document representation models. We show how state-of-the-art models\nlike SPECTER and SciNCL struggle to generalize across the task formats, and\nthat simple multi-task training fails to improve them. However, a new approach\nthat learns multiple embeddings per document, each tailored to a different\nformat, can improve performance. We experiment with task-format-specific\ncontrol codes and adapters and find they outperform the existing\nsingle-embedding state-of-the-art by over 2 points absolute. We release the\nresulting family of multi-format models, called SPECTER2, for the community to\nuse and build on.", "published": "2022-11-23 21:25:39", "link": "http://arxiv.org/abs/2211.13308v4", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Typicality of Musical Sequences", "abstract": "It has been shown in a recent publication that words in human-produced\nEnglish language tend to have an information content close to the conditional\nentropy. In this paper, we show that the same is true for events in\nhuman-produced monophonic musical sequences. We also show how \"typical\nsampling\" influences the distribution of information around the entropy for\nsingle events and sequences.", "published": "2022-11-23 16:05:28", "link": "http://arxiv.org/abs/2211.13016v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASiT: Local-Global Audio Spectrogram vIsion Transformer for Event\n  Classification", "abstract": "Transformers, which were originally developed for natural language\nprocessing, have recently generated significant interest in the computer vision\nand audio communities due to their flexibility in learning long-range\nrelationships. Constrained by the data hungry nature of transformers and the\nlimited amount of labelled data, most transformer-based models for audio tasks\nare finetuned from ImageNet pretrained models, despite the huge gap between the\ndomain of natural images and audio. This has motivated the research in\nself-supervised pretraining of audio transformers, which reduces the dependency\non large amounts of labeled data and focuses on extracting concise\nrepresentations of audio spectrograms. In this paper, we propose\n\\textbf{L}ocal-\\textbf{G}lobal \\textbf{A}udio \\textbf{S}pectrogram\nv\\textbf{I}sion \\textbf{T}ransformer, namely ASiT, a novel self-supervised\nlearning framework that captures local and global contextual information by\nemploying group masked model learning and self-distillation. We evaluate our\npretrained models on both audio and speech classification tasks, including\naudio event classification, keyword spotting, and speaker identification. We\nfurther conduct comprehensive ablation studies, including evaluations of\ndifferent pretraining strategies. The proposed ASiT framework significantly\nboosts the performance on all tasks and sets a new state-of-the-art performance\nin five audio and speech classification tasks, outperforming recent methods,\nincluding the approaches that use additional datasets for pretraining.", "published": "2022-11-23 18:21:09", "link": "http://arxiv.org/abs/2211.13189v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice-preserving Zero-shot Multiple Accent Conversion", "abstract": "Most people who have tried to learn a foreign language would have experienced\ndifficulties understanding or speaking with a native speaker's accent. For\nnative speakers, understanding or speaking a new accent is likewise a difficult\ntask. An accent conversion system that changes a speaker's accent but preserves\nthat speaker's voice identity, such as timbre and pitch, has the potential for\na range of applications, such as communication, language learning, and\nentertainment. Existing accent conversion models tend to change the speaker\nidentity and accent at the same time. Here, we use adversarial learning to\ndisentangle accent dependent features while retaining other acoustic\ncharacteristics. What sets our work apart from existing accent conversion\nmodels is the capability to convert an unseen speaker's utterance to multiple\naccents while preserving its original voice identity. Subjective evaluations\nshow that our model generates audio that sound closer to the target accent and\nlike the original speaker.", "published": "2022-11-23 19:51:16", "link": "http://arxiv.org/abs/2211.13282v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Proceedings of the 4th International Workshop on Reading Music Systems", "abstract": "The International Workshop on Reading Music Systems (WoRMS) is a workshop\nthat tries to connect researchers who develop systems for reading music, such\nas in the field of Optical Music Recognition, with other researchers and\npractitioners that could benefit from such systems, like librarians or\nmusicologists.\n  The relevant topics of interest for the workshop include, but are not limited\nto: Music reading systems; Optical music recognition; Datasets and performance\nevaluation; Image processing on music scores; Writer identification; Authoring,\nediting, storing and presentation systems for music scores; Multi-modal\nsystems; Novel input-methods for music to produce written music; Web-based\nMusic Information Retrieval services; Applications and projects; Use-cases\nrelated to written music.\n  These are the proceedings of the 4th International Workshop on Reading Music\nSystems, held online on Nov. 18th 2022.", "published": "2022-11-23 20:16:45", "link": "http://arxiv.org/abs/2211.13285v1", "categories": ["cs.CV", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Whose Emotion Matters? Speaking Activity Localisation without Prior\n  Knowledge", "abstract": "The task of emotion recognition in conversations (ERC) benefits from the\navailability of multiple modalities, as provided, for example, in the\nvideo-based Multimodal EmotionLines Dataset (MELD). However, only a few\nresearch approaches use both acoustic and visual information from the MELD\nvideos. There are two reasons for this: First, label-to-video alignments in\nMELD are noisy, making those videos an unreliable source of emotional speech\ndata. Second, conversations can involve several people in the same scene, which\nrequires the localisation of the utterance source. In this paper, we introduce\nMELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using\nrecent active speaker detection and automatic speech recognition models, we are\nable to realign the videos of MELD and capture the facial expressions from\nspeakers in 96.92% of the utterances provided in MELD. Experiments with a\nself-supervised voice recognition model indicate that the realigned MELD-FAIR\nvideos more closely match the transcribed utterances given in the MELD dataset.\nFinally, we devise a model for emotion recognition in conversations trained on\nthe realigned MELD-FAIR videos, which outperforms state-of-the-art models for\nERC based on vision alone. This indicates that localising the source of\nspeaking activities is indeed effective for extracting facial expressions from\nthe uttering speakers and that faces provide more informative visual cues than\nthe visual features state-of-the-art models have been using so far. The\nMELD-FAIR realignment data, and the code of the realignment procedure and of\nthe emotional recognition, are available at\nhttps://github.com/knowledgetechnologyuhh/MELD-FAIR.", "published": "2022-11-23 09:57:17", "link": "http://arxiv.org/abs/2211.15377v4", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.NE", "cs.SD", "68T20", "I.2.0"], "primary_category": "eess.AS"}
