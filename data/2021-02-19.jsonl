{"title": "Back Translation Survey for Improving Text Augmentation", "abstract": "Natural Language Processing (NLP) relies heavily on training data.\nTransformers, as they have gotten bigger, have required massive amounts of\ntraining data. To satisfy this requirement, text augmentation should be looked\nat as a way to expand your current dataset and to generalize your models. One\ntext augmentation we will look at is translation augmentation. We take an\nEnglish sentence and translate it to another language before translating it\nback to English. In this paper, we look at the effect of 108 different language\nback translations on various metrics and text embeddings.", "published": "2021-02-19 02:08:26", "link": "http://arxiv.org/abs/2102.09708v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialect Identification in Nuanced Arabic Tweets Using Farasa\n  Segmentation and AraBERT", "abstract": "This paper presents our approach to address the EACL WANLP-2021 Shared Task\n1: Nuanced Arabic Dialect Identification (NADI). The task is aimed at\ndeveloping a system that identifies the geographical location(country/province)\nfrom where an Arabic tweet in the form of modern standard Arabic or dialect\ncomes from. We solve the task in two parts. The first part involves\npre-processing the provided dataset by cleaning, adding and segmenting various\nparts of the text. This is followed by carrying out experiments with different\nversions of two Transformer based models, AraBERT and AraELECTRA. Our final\napproach achieved macro F1-scores of 0.216, 0.235, 0.054, and 0.043 in the four\nsubtasks, and we were ranked second in MSA identification subtasks and fourth\nin DA identification subtasks.", "published": "2021-02-19 05:39:21", "link": "http://arxiv.org/abs/2102.09749v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progressive Transformer-Based Generation of Radiology Reports", "abstract": "Inspired by Curriculum Learning, we propose a consecutive (i.e.,\nimage-to-text-to-text) generation framework where we divide the problem of\nradiology report generation into two steps. Contrary to generating the full\nradiology report from the image at once, the model generates global concepts\nfrom the image in the first step and then reforms them into finer and coherent\ntexts using a transformer architecture. We follow the transformer-based\nsequence-to-sequence paradigm at each step. We improve upon the\nstate-of-the-art on two benchmark datasets.", "published": "2021-02-19 07:42:13", "link": "http://arxiv.org/abs/2102.09777v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on Measuring the Similarity of Sentential Arguments\n  with Language Model Domain Adaptation", "abstract": "Measuring the similarity between two different sentential arguments is an\nimportant task in argument mining. However, one of the challenges in this field\nis that the dataset must be annotated using expertise in a variety of topics,\nmaking supervised learning with labeled data expensive. In this paper, we\ninvestigated whether this problem could be alleviated through transfer\nlearning. We first adapted a pretrained language model to a domain of interest\nusing self-supervised learning. Then, we fine-tuned the model to a task of\nmeasuring the similarity between sentences taken from different domains. Our\napproach improves a correlation with human-annotated similarity scores compared\nto competitive baseline models on the Argument Facet Similarity dataset in an\nunsupervised setting. Moreover, we achieve comparable performance to a fully\nsupervised baseline model by using only about 60% of the labeled data samples.\nWe believe that our work suggests the possibility of a generalized argument\nclustering model for various argumentative topics.", "published": "2021-02-19 08:05:46", "link": "http://arxiv.org/abs/2102.09786v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Emotion Recognition in Hindi-English Code-Mixed Data: A\n  Transformer Based Approach", "abstract": "In the last few years, emotion detection in social-media text has become a\npopular problem due to its wide ranging application in better understanding the\nconsumers, in psychology, in aiding human interaction with computers, designing\nsmart systems etc. Because of the availability of huge amounts of data from\nsocial-media, which is regularly used for expressing sentiments and opinions,\nthis problem has garnered great attention. In this paper, we present a Hinglish\ndataset labelled for emotion detection. We highlight a deep learning based\napproach for detecting emotions in Hindi-English code mixed tweets, using\nbilingual word embeddings derived from FastText and Word2Vec approaches, as\nwell as transformer based models. We experiment with various deep learning\nmodels, including CNNs, LSTMs, Bi-directional LSTMs (with and without\nattention), along with transformers like BERT, RoBERTa, and ALBERT. The\ntransformer based BERT model outperforms all other models giving the best\nperformance with an accuracy of 71.43%.", "published": "2021-02-19 14:07:20", "link": "http://arxiv.org/abs/2102.09943v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Curriculum Learning for Sentiment Analysis along Task\n  Difficulty, Pacing and Visualization Axes", "abstract": "While Curriculum Learning (CL) has recently gained traction in Natural\nlanguage Processing Tasks, it is still not adequately analyzed. Previous works\nonly show their effectiveness but fail short to explain and interpret the\ninternal workings fully. In this paper, we analyze curriculum learning in\nsentiment analysis along multiple axes. Some of these axes have been proposed\nby earlier works that need more in-depth study. Such analysis requires\nunderstanding where curriculum learning works and where it does not. Our axes\nof analysis include Task difficulty on CL, comparing CL pacing techniques, and\nqualitative analysis by visualizing the movement of attention scores in the\nmodel as curriculum phases progress. We find that curriculum learning works\nbest for difficult tasks and may even lead to a decrement in performance for\ntasks with higher performance without curriculum learning. We see that One-Pass\ncurriculum strategies suffer from catastrophic forgetting and attention\nmovement visualization within curriculum pacing. This shows that curriculum\nlearning breaks down the challenging main task into easier sub-tasks solved\nsequentially.", "published": "2021-02-19 15:42:14", "link": "http://arxiv.org/abs/2102.09990v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Transformer based Ensemble Learning to classify Scientific\n  Articles", "abstract": "Many time reviewers fail to appreciate novel ideas of a researcher and\nprovide generic feedback. Thus, proper assignment of reviewers based on their\narea of expertise is necessary. Moreover, reading each and every paper from\nend-to-end for assigning it to a reviewer is a tedious task. In this paper, we\ndescribe a system which our team FideLIPI submitted in the shared task of\nSDPRA-2021 [14]. It comprises four independent sub-systems capable of\nclassifying abstracts of scientific literature to one of the given seven\nclasses. The first one is a RoBERTa [10] based model built over these\nabstracts. Adding topic models / Latent dirichlet allocation (LDA) [2] based\nfeatures to the first model results in the second sub-system. The third one is\na sentence level RoBERTa [10] model. The fourth one is a Logistic Regression\nmodel built using Term Frequency Inverse Document Frequency (TF-IDF) features.\nWe ensemble predictions of these four sub-systems using majority voting to\ndevelop the final system which gives a F1 score of 0.93 on the test and\nvalidation set. This outperforms the existing State Of The Art (SOTA) model\nSciBERT's [1] in terms of F1 score on the validation set.Our codebase is\navailable at https://github.com/SDPRA-2021/shared-task/tree/main/FideLIPI", "published": "2021-02-19 15:42:26", "link": "http://arxiv.org/abs/2102.09991v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Formal Language Theory Meets Modern NLP", "abstract": "NLP is deeply intertwined with the formal study of language, both\nconceptually and historically. Arguably, this connection goes all the way back\nto Chomsky's Syntactic Structures in 1957. It also still holds true today, with\na strand of recent works building formal analysis of modern neural networks\nmethods in terms of formal languages. In this document, I aim to explain\nbackground about formal languages as they relate to this recent work. I will by\nnecessity ignore large parts of the rich history of this field, instead\nfocusing on concepts connecting to modern deep learning-based NLP.", "published": "2021-02-19 18:51:10", "link": "http://arxiv.org/abs/2102.10094v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Domain Adaptation in Neural Machine Translation Through\n  Multidimensional Tagging", "abstract": "While NMT has achieved remarkable results in the last 5 years, production\nsystems come with strict quality requirements in arbitrarily niche domains that\nare not always adequately covered by readily available parallel corpora. This\nis typically addressed by training domain specific models, using fine-tuning\nmethods and some variation of back-translation on top of in-domain monolingual\ncorpora. However, industrial practitioners can rarely afford to focus on a\nsingle domain. A far more typical scenario includes a set of closely related,\nyet succinctly different sub-domains. At Booking.com, we need to translate\nproperty descriptions, user reviews, as well as messages, (for example those\nsent between a customer and an agent or property manager). An editor might need\nto translate articles across a set of different topics. An e-commerce platform\nwould typically need to translate both the description of each item and the\nuser generated content related to them. To this end, we propose MDT: a novel\nmethod to simultaneously fine-tune on several sub-domains by passing\nmultidimensional sentence-level information to the model during training and\ninference. We show that MDT achieves results competitive to N specialist models\neach fine-tuned on a single constituent domain, while effectively serving all N\nsub-domains, therefore cutting development and maintenance costs by the same\nfactor. Besides BLEU (industry standard automatic evaluation metric known to\nonly weakly correlate with human judgement) we also report rigorous human\nevaluation results for all models and sub-domains as well as specific examples\nthat better contextualise the performance of each model in terms of adequacy\nand fluency. To facilitate further research, we plan to make the code available\nupon acceptance.", "published": "2021-02-19 21:19:42", "link": "http://arxiv.org/abs/2102.10160v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "abstract": "GPT-3 can perform numerous tasks when provided a natural language prompt that\ncontains a few training examples. We show that this type of few-shot learning\ncan be unstable: the choice of prompt format, training examples, and even the\norder of the training examples can cause accuracy to vary from near chance to\nnear state-of-the-art. We demonstrate that this instability arises from the\nbias of language models towards predicting certain answers, e.g., those that\nare placed near the end of the prompt or are common in the pre-training data.\nTo mitigate this, we first estimate the model's bias towards each answer by\nasking for its prediction when given the training prompt and a content-free\ntest input such as \"N/A\". We then fit calibration parameters that cause the\nprediction for this input to be uniform across answers. On a diverse set of\ntasks, this contextual calibration procedure substantially improves GPT-3 and\nGPT-2's average accuracy (up to 30.0% absolute) and reduces variance across\ndifferent choices of the prompt.", "published": "2021-02-19 00:23:59", "link": "http://arxiv.org/abs/2102.09690v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Dynamic BERT via Trainable Gate Variables and a Bi-modal\n  Regularizer", "abstract": "The BERT model has shown significant success on various natural language\nprocessing tasks. However, due to the heavy model size and high computational\ncost, the model suffers from high latency, which is fatal to its deployments on\nresource-limited devices. To tackle this problem, we propose a dynamic\ninference method on BERT via trainable gate variables applied on input tokens\nand a regularizer that has a bi-modal property. Our method shows reduced\ncomputational cost on the GLUE dataset with a minimal performance drop.\nMoreover, the model adjusts with a trade-off between performance and\ncomputational cost with the user-specified hyperparameter.", "published": "2021-02-19 03:59:23", "link": "http://arxiv.org/abs/2102.09727v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KBCNMUJAL@HASOC-Dravidian-CodeMix-FIRE2020: Using Machine Learning for\n  Detection of Hate Speech and Offensive Code-Mixed Social Media text", "abstract": "This paper describes the system submitted by our team, KBCNMUJAL, for Task 2\nof the shared task Hate Speech and Offensive Content Identification in\nIndo-European Languages (HASOC), at Forum for Information Retrieval Evaluation,\nDecember 16-20, 2020, Hyderabad, India. The datasets of two Dravidian languages\nViz. Malayalam and Tamil of size 4000 observations, each were shared by the\nHASOC organizers. These datasets are used to train the machine using different\nmachine learning algorithms, based on classification and regression models. The\ndatasets consist of tweets or YouTube comments with two class labels offensive\nand not offensive. The machine is trained to classify such social media\nmessages in these two categories. Appropriate n-gram feature sets are extracted\nto learn the specific characteristics of the Hate Speech text messages. These\nfeature models are based on TFIDF weights of n-gram. The referred work and\nrespective experiments show that the features such as word, character and\ncombined model of word and character n-grams could be used to identify the term\npatterns of offensive text contents. As a part of the HASOC shared task, the\ntest data sets are made available by the HASOC track organizers. The best\nperforming classification models developed for both languages are applied on\ntest datasets. The model which gives the highest accuracy result on training\ndataset for Malayalam language was experimented to predict the categories of\nrespective test data. This system has obtained an F1 score of 0.77. Similarly\nthe best performing model for Tamil language has obtained an F1 score of 0.87.\nThis work has received 2nd and 3rd rank in this shared Task 2 for Malayalam and\nTamil language respectively. The proposed system is named HASOC_kbcnmujal.", "published": "2021-02-19 11:08:02", "link": "http://arxiv.org/abs/2102.09866v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Alternate Endings: Improving Prosody for Incremental Neural TTS with\n  Predicted Future Text Input", "abstract": "The prosody of a spoken word is determined by its surrounding context. In\nincremental text-to-speech synthesis, where the synthesizer produces an output\nbefore it has access to the complete input, the full context is often unknown\nwhich can result in a loss of naturalness in the synthesized speech. In this\npaper, we investigate whether the use of predicted future text can attenuate\nthis loss. We compare several test conditions of next future word: (a) unknown\n(zero-word), (b) language model predicted, (c) randomly predicted and (d)\nground-truth. We measure the prosodic features (pitch, energy and duration) and\nfind that predicted text provides significant improvements over a zero-word\nlookahead, but only slight gains over random-word lookahead. We confirm these\nresults with a perceptive test.", "published": "2021-02-19 13:11:34", "link": "http://arxiv.org/abs/2102.09914v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Back to Prior Knowledge: Joint Event Causality Extraction via\n  Convolutional Semantic Infusion", "abstract": "Joint event and causality extraction is a challenging yet essential task in\ninformation retrieval and data mining. Recently, pre-trained language models\n(e.g., BERT) yield state-of-the-art results and dominate in a variety of NLP\ntasks. However, these models are incapable of imposing external knowledge in\ndomain-specific extraction. Considering the prior knowledge of frequent n-grams\nthat represent cause/effect events may benefit both event and causality\nextraction, in this paper, we propose convolutional knowledge infusion for\nfrequent n-grams with different windows of length within a joint extraction\nframework. Knowledge infusion during convolutional filter initialization not\nonly helps the model capture both intra-event (i.e., features in an event\ncluster) and inter-event (i.e., associations across event clusters) features\nbut also boosts training convergence. Experimental results on the benchmark\ndatasets show that our model significantly outperforms the strong BERT+CSNN\nbaseline.", "published": "2021-02-19 13:31:46", "link": "http://arxiv.org/abs/2102.09923v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis for YouTube Comments in Roman Urdu", "abstract": "Sentiment analysis is a vast area in the Machine learning domain. A lot of\nwork is done on datasets and their analysis of the English Language. In\nPakistan, a huge amount of data is in roman Urdu language, it is scattered all\nover the social sites including Twitter, YouTube, Facebook and similar\napplications. In this study the focus domain of dataset gathering is YouTube\ncomments. The Dataset contains the comments of people over different Pakistani\ndramas and TV shows. The Dataset contains multi-class classification that is\ngrouped The comments into positive, negative and neutral sentiment. In this\nStudy comparative analysis is done for five supervised learning Algorithms\nincluding linear regression, SVM, KNN, Multi layer Perceptron and Na\\\"ive Bayes\nclassifier. Accuracy, recall, precision and F-measure are used for measuring\nperformance. Results show that accuracy of SVM is 64 percent, which is better\nthan the rest of the list.", "published": "2021-02-19 18:15:52", "link": "http://arxiv.org/abs/2102.10075v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hate-Alert@DravidianLangTech-EACL2021: Ensembling strategies for\n  Transformer-based Offensive language Detection", "abstract": "Social media often acts as breeding grounds for different forms of offensive\ncontent. For low resource languages like Tamil, the situation is more complex\ndue to the poor performance of multilingual or language-specific models and\nlack of proper benchmark datasets. Based on this shared task, Offensive\nLanguage Identification in Dravidian Languages at EACL 2021, we present an\nexhaustive exploration of different transformer models, We also provide a\ngenetic algorithm technique for ensembling different models. Our ensembled\nmodels trained separately for each language secured the first position in\nTamil, the second position in Kannada, and the first position in Malayalam\nsub-tasks. The models and codes are provided.", "published": "2021-02-19 18:35:38", "link": "http://arxiv.org/abs/2102.10084v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conditional Adversarial Networks for Multi-Domain Text Classification", "abstract": "In this paper, we propose conditional adversarial networks (CANs), a\nframework that explores the relationship between the shared features and the\nlabel predictions to impose more discriminability to the shared features, for\nmulti-domain text classification (MDTC). The proposed CAN introduces a\nconditional domain discriminator to model the domain variance in both shared\nfeature representations and class-aware information simultaneously and adopts\nentropy conditioning to guarantee the transferability of the shared features.\nWe provide theoretical analysis for the CAN framework, showing that CAN's\nobjective is equivalent to minimizing the total divergence among multiple joint\ndistributions of shared features and label predictions. Therefore, CAN is a\ntheoretically sound adversarial network that discriminates over multiple\ndistributions. Evaluation results on two MDTC benchmarks show that CAN\noutperforms prior methods. Further experiments demonstrate that CAN has a good\nability to generalize learned knowledge to unseen domains.", "published": "2021-02-19 21:59:03", "link": "http://arxiv.org/abs/2102.10176v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Keyboards as a new model of computation", "abstract": "We introduce a new formalisation of languages, called keyboards. We consider\na set of elementary operations (writing/erasing a letter, going to the right or\nto the left,...) and we define a keyboard as a set of finite sequences of such\noperations, called keys. The corresponding language is the set of words\nobtained by applying some sequence of those keys. Unlike classical models of\ncomputation, every key can be applied anytime. We define various classes of\nlanguages based on different sets of elementary operations, and compare their\nexpressive powers. We also compare them to well-known classes of languages\n(Chomsky hierarchy). We obtain a strict hierarchy of languages, whose\nexpressivity is orthogonal to the one of the aforementionned classical models.\n  --\n  Nous introduisons une nouvelle repr\\'esentation de langages, les claviers. On\nse munit d'un ensemble d'op\\'erations \\'el\\'ementaires (ajout, effacement d'une\nlettre, d\\'eplacement \\`a droite, \\`a gauche, ...), et on d\\'efinit un clavier\ncomme un ensemble de suites finies d'op\\'erations \\'el\\'ementaires, appel\\'ees\ntouches. Son langage sera l'ensemble des mots obtenus en appliquant une suite\nquelconque de touches. Contrairement \\`a des mod\\`eles de calcul classiques,\ntoutes les touches peuvent \\^etre appliqu\\'ees \\`a tout moment. En premier lieu\nnous d\\'efinissons diff\\'erentes classes de claviers en faisant varier\nl'ensemble des op\\'erations \\'el\\'ementaires autoris\\'ees, et nous comparons\nl'expressivit\\'e des classes de langages obtenues. Nous comparons \\'egalement\nces classes \\`a la hi\\'erarchie de Chomsky. Nous obtenons que toutes les\nclasses \\'etudi\\'ees sont diff\\'erentes, et nous caract\\'erisons les classes\ninclues dans les rationnels et les alg\\'ebriques. L'expressivit\\'e des claviers\nsemble orthogonale \\`a celle des mod\\`eles \\'evoqu\\'es pr\\'ec\\'edemment.", "published": "2021-02-19 22:17:18", "link": "http://arxiv.org/abs/2102.10182v3", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Scaling Creative Inspiration with Fine-Grained Functional Aspects of\n  Ideas", "abstract": "Large repositories of products, patents and scientific papers offer an\nopportunity for building systems that scour millions of ideas and help users\ndiscover inspirations. However, idea descriptions are typically in the form of\nunstructured text, lacking key structure that is required for supporting\ncreative innovation interactions. Prior work has explored idea representations\nthat were either limited in expressivity, required significant manual effort\nfrom users, or dependent on curated knowledge bases with poor coverage. We\nexplore a novel representation that automatically breaks up products into\nfine-grained functional aspects capturing the purposes and mechanisms of ideas,\nand use it to support important creative innovation interactions: functional\nsearch for ideas, and exploration of the design space around a focal problem by\nviewing related problem perspectives pooled from across many products. In user\nstudies, our approach boosts the quality of creative search and inspirations,\nsubstantially outperforming strong baselines by 50-60%.", "published": "2021-02-19 06:30:41", "link": "http://arxiv.org/abs/2102.09761v3", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Direction of Arrival Estimation of Noisy Speech Using Convolutional\n  Recurrent Neural Networks with Higher-Order Ambisonics Signals", "abstract": "Training convolutional recurrent neural networks on first-order Ambisonics\nsignals is a well-known approach when estimating the direction of arrival for\nspeech/sound signals. In this work, we investigate whether increasing the order\nof Ambisonics up to the fourth order further improves the estimation\nperformance of convolutional recurrent neural networks. While our results on\ndata based on simulated spatial room impulse responses show that the use of\nhigher Ambisonics orders does have the potential to provide better localization\nresults, no further improvement was shown on data based on real spatial room\nimpulse responses from order two onwards. Rather, it seems to be crucial to\nextract meaningful features from the raw data. First order features derived\nfrom the acoustic intensity vector were superior to pure higher-order magnitude\nand phase features in almost all scenarios.", "published": "2021-02-19 10:42:02", "link": "http://arxiv.org/abs/2102.09853v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Frequency-Temporal Attention Network for Singing Melody Extraction", "abstract": "Musical audio is generally composed of three physical properties: frequency,\ntime and magnitude. Interestingly, human auditory periphery also provides\nneural codes for each of these dimensions to perceive music. Inspired by these\nintrinsic characteristics, a frequency-temporal attention network is proposed\nto mimic human auditory for singing melody extraction. In particular, the\nproposed model contains frequency-temporal attention modules and a selective\nfusion module corresponding to these three physical properties. The frequency\nattention module is used to select the same activation frequency bands as did\nin cochlear and the temporal attention module is responsible for analyzing\ntemporal patterns. Finally, the selective fusion module is suggested to\nrecalibrate magnitudes and fuse the raw information for prediction. In\naddition, we propose to use another branch to simultaneously predict the\npresence of singing voice melody. The experimental results show that the\nproposed model outperforms existing state-of-the-art methods.", "published": "2021-02-19 06:43:55", "link": "http://arxiv.org/abs/2102.09763v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unit selection synthesis based data augmentation for fixed phrase\n  speaker verification", "abstract": "Data augmentation is commonly used to help build a robust speaker\nverification system, especially in limited-resource case. However, conventional\ndata augmentation methods usually focus on the diversity of acoustic\nenvironment, leaving the lexicon variation neglected. For text dependent\nspeaker verification tasks, it's well-known that preparing training data with\nthe target transcript is the most effectual approach to build a well-performing\nsystem, however collecting such data is time-consuming and expensive. In this\nwork, we propose a unit selection synthesis based data augmentation method to\nleverage the abundant text-independent data resources. In this approach\ntext-independent speeches of each speaker are firstly broke up to speech\nsegments each contains one phone unit. Then segments that contain phonetics in\nthe target transcript are selected to produce a speech with the target\ntranscript by concatenating them in turn. Experiments are carried out on the\nAISHELL Speaker Verification Challenge 2019 database, the results and analysis\nshows that our proposed method can boost the system performance significantly.", "published": "2021-02-19 09:14:23", "link": "http://arxiv.org/abs/2102.09817v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AISPEECH-SJTU accent identification system for the Accented English\n  Speech Recognition Challenge", "abstract": "This paper describes the AISpeech-SJTU system for the accent identification\ntrack of the Interspeech-2020 Accented English Speech Recognition Challenge. In\nthis challenge track, only 160-hour accented English data collected from 8\ncountries and the auxiliary Librispeech dataset are provided for training. To\nbuild an accurate and robust accent identification system, we explore the whole\nsystem pipeline in detail. First, we introduce the ASR based phone\nposteriorgram (PPG) feature to accent identification and verify its efficacy.\nThen, a novel TTS based approach is carefully designed to augment the very\nlimited accent training data for the first time. Finally, we propose the test\ntime augmentation and embedding fusion schemes to further improve the system\nperformance. Our final system is ranked first in the challenge and outperforms\nall the other participants by a large margin. The submitted system achieves\n83.63\\% average accuracy on the challenge evaluation data, ahead of the others\nby more than 10\\% in absolute terms.", "published": "2021-02-19 09:40:13", "link": "http://arxiv.org/abs/2102.09828v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Robust Maximum Likelihood Distortionless Response Beamformer based on\n  a Complex Generalized Gaussian Distribution", "abstract": "For multichannel speech enhancement, this letter derives a robust maximum\nlikelihood distortionless response beamformer by modeling speech sparse priors\nwith a complex generalized Gaussian distribution, where we refer to as the\nCGGD-MLDR beamformer. The proposed beamformer can be regarded as a\ngeneralization of the minimum power distortionless response beamformer and its\nimproved variations. For narrowband applications, we also reveal that the\nproposed beamformer reduces to the minimum dispersion distortionless response\nbeamformer, which has been derived with the ${{\\ell}_{p}}$-norm minimization.\nThe mechanisms of the proposed beamformer in improving the robustness are\nclearly pointed out and experimental results show its better performance in\nPESQ improvement.", "published": "2021-02-19 10:12:47", "link": "http://arxiv.org/abs/2102.09838v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "End-to-End Neural Systems for Automatic Children Speech Recognition: An\n  Empirical Study", "abstract": "A key desiderata for inclusive and accessible speech recognition technology\nis ensuring its robust performance to children's speech. Notably, this includes\nthe rapidly advancing neural network based end-to-end speech recognition\nsystems. Children speech recognition is more challenging due to the larger\nintra-inter speaker variability in terms of acoustic and linguistic\ncharacteristics compared to adult speech. Furthermore, the lack of adequate and\nappropriate children speech resources adds to the challenge of designing robust\nend-to-end neural architectures. This study provides a critical assessment of\nautomatic children speech recognition through an empirical study of\ncontemporary state-of-the-art end-to-end speech recognition systems. Insights\nare provided on the aspects of training data requirements, adaptation on\nchildren data, and the effect of children age, utterance lengths, different\narchitectures and loss functions for end-to-end systems and role of language\nmodels on the speech recognition performance.", "published": "2021-02-19 13:18:08", "link": "http://arxiv.org/abs/2102.09918v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CatNet: music source separation system with mix-audio augmentation", "abstract": "Music source separation (MSS) is the task of separating a music piece into\nindividual sources, such as vocals and accompaniment. Recently, neural network\nbased methods have been applied to address the MSS problem, and can be\ncategorized into spectrogram and time-domain based methods. However, there is a\nlack of research of using complementary information of spectrogram and\ntime-domain inputs for MSS. In this article, we propose a CatNet framework that\nconcatenates a UNet separation branch using spectrogram as input and a WavUNet\nseparation branch using time-domain waveform as input for MSS. We propose an\nend-to-end and fully differentiable system that incorporate spectrogram\ncalculation into CatNet. In addition, we propose a novel mix-audio data\naugmentation method that randomly mix audio segments from the same source as\naugmented audio segments for training. Our proposed CatNet MSS system achieves\na state-of-the-art vocals separation source distortion ratio (SDR) of 7.54 dB,\noutperforming MMDenseNet of 6.57 dB evaluated on the MUSDB18 dataset.", "published": "2021-02-19 15:01:24", "link": "http://arxiv.org/abs/2102.09966v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech enhancement with weakly labelled data from AudioSet", "abstract": "Speech enhancement is a task to improve the intelligibility and perceptual\nquality of degraded speech signal. Recently, neural networks based methods have\nbeen applied to speech enhancement. However, many neural network based methods\nrequire noisy and clean speech pairs for training. We propose a speech\nenhancement framework that can be trained with large-scale weakly labelled\nAudioSet dataset. Weakly labelled data only contain audio tags of audio clips,\nbut not the onset or offset times of speech. We first apply pretrained audio\nneural networks (PANNs) to detect anchor segments that contain speech or sound\nevents in audio clips. Then, we randomly mix two detected anchor segments\ncontaining speech and sound events as a mixture, and build a conditional source\nseparation network using PANNs predictions as soft conditions for speech\nenhancement. In inference, we input a noisy speech signal with the one-hot\nencoding of \"Speech\" as a condition to the trained system to predict enhanced\nspeech. Our system achieves a PESQ of 2.28 and an SSNR of 8.75 dB on the\nVoiceBank-DEMAND dataset, outperforming the previous SEGAN system of 2.16 and\n7.73 dB respectively.", "published": "2021-02-19 15:08:42", "link": "http://arxiv.org/abs/2102.09971v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TransMask: A Compact and Fast Speech Separation Model Based on\n  Transformer", "abstract": "Speech separation is an important problem in speech processing, which targets\nto separate and generate clean speech from a mixed audio containing speech from\ndifferent speakers. Empowered by the deep learning technologies over\nsequence-to-sequence domain, recent neural speech separation models are now\ncapable of generating highly clean speech audios. To make these models more\npractical by reducing the model size and inference time while maintaining high\nseparation quality, we propose a new transformer-based speech separation\napproach, called TransMask. By fully un-leashing the power of self-attention on\nlong-term dependency exception, we demonstrate the size of TransMask is more\nthan 60% smaller and the inference is more than 2 times faster than\nstate-of-the-art solutions. TransMask fully utilizes the parallelism during\ninference, and achieves nearly linear inference time within reasonable input\naudio lengths. It also outperforms existing solutions on output speech audio\nquality, achieving SDR above 16 over Librimix benchmark.", "published": "2021-02-19 15:19:24", "link": "http://arxiv.org/abs/2102.09978v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "One Shot Audio to Animated Video Generation", "abstract": "We consider the challenging problem of audio to animated video generation. We\npropose a novel method OneShotAu2AV to generate an animated video of arbitrary\nlength using an audio clip and a single unseen image of a person as an input.\nThe proposed method consists of two stages. In the first stage, OneShotAu2AV\ngenerates the talking-head video in the human domain given an audio and a\nperson's image. In the second stage, the talking-head video from the human\ndomain is converted to the animated domain. The model architecture of the first\nstage consists of spatially adaptive normalization based multi-level generator\nand multiple multilevel discriminators along with multiple adversarial and\nnon-adversarial losses. The second stage leverages attention based\nnormalization driven GAN architecture along with temporal predictor based\nrecycle loss and blink loss coupled with lipsync loss, for unsupervised\ngeneration of animated video. In our approach, the input audio clip is not\nrestricted to any specific language, which gives the method multilingual\napplicability. OneShotAu2AV can generate animated videos that have: (a) lip\nmovements that are in sync with the audio, (b) natural facial expressions such\nas blinks and eyebrow movements, (c) head movements. Experimental evaluation\ndemonstrates superior performance of OneShotAu2AV as compared to U-GAT-IT and\nRecycleGan on multiple quantitative metrics including KID(Kernel Inception\nDistance), Word error rate, blinks/sec", "published": "2021-02-19 04:29:17", "link": "http://arxiv.org/abs/2102.09737v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Hierarchical Recurrent Neural Networks for Conditional Melody Generation\n  with Long-term Structure", "abstract": "The rise of deep learning technologies has quickly advanced many fields,\nincluding that of generative music systems. There exist a number of systems\nthat allow for the generation of good sounding short snippets, yet, these\ngenerated snippets often lack an overarching, longer-term structure. In this\nwork, we propose CM-HRNN: a conditional melody generation model based on a\nhierarchical recurrent neural network. This model allows us to generate\nmelodies with long-term structures based on given chord accompaniments. We also\npropose a novel, concise event-based representation to encode musical lead\nsheets while retaining the notes' relative position within the bar with respect\nto the musical meter. With this new data representation, the proposed\narchitecture can simultaneously model the rhythmic, as well as the pitch\nstructures in an effective way. Melodies generated by the proposed model were\nextensively evaluated in quantitative experiments as well as a user study to\nensure the musical quality of the output as well as to evaluate if they contain\nrepeating patterns. We also compared the system with the state-of-the-art\nAttentionRNN. This comparison shows that melodies generated by CM-HRNN contain\nmore repeated patterns (i.e., higher compression ratio) and a lower tonal\ntension (i.e., more tonally concise). Results from our listening test indicate\nthat CM-HRNN outperforms AttentionRNN in terms of long-term structure and\noverall rating.", "published": "2021-02-19 08:22:26", "link": "http://arxiv.org/abs/2102.09794v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Artificially Synthesising Data for Audio Classification and Segmentation\n  to Improve Speech and Music Detection in Radio Broadcast", "abstract": "Segmenting audio into homogeneous sections such as music and speech helps us\nunderstand the content of audio. It is useful as a pre-processing step to\nindex, store, and modify audio recordings, radio broadcasts and TV programmes.\nDeep learning models for segmentation are generally trained on copyrighted\nmaterial, which cannot be shared. Annotating these datasets is time-consuming\nand expensive and therefore, it significantly slows down research progress. In\nthis study, we present a novel procedure that artificially synthesises data\nthat resembles radio signals. We replicate the workflow of a radio DJ in mixing\naudio and investigate parameters like fade curves and audio ducking. We trained\na Convolutional Recurrent Neural Network (CRNN) on this synthesised data and\noutperformed state-of-the-art algorithms for music-speech detection. This paper\ndemonstrates the data synthesis procedure as a highly effective technique to\ngenerate large datasets to train deep neural networks for audio segmentation.", "published": "2021-02-19 14:47:05", "link": "http://arxiv.org/abs/2102.09959v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "I.5.4; I.2.m"], "primary_category": "eess.AS"}
