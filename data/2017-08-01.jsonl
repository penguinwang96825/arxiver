{"title": "Using Linguistic Features to Improve the Generalization Capability of\n  Neural Coreference Resolvers", "abstract": "Coreference resolution is an intermediate step for text understanding. It is\nused in tasks and domains for which we do not necessarily have coreference\nannotated corpora. Therefore, generalization is of special importance for\ncoreference resolution. However, while recent coreference resolvers have\nnotable improvements on the CoNLL dataset, they struggle to generalize properly\nto new domains or datasets. In this paper, we investigate the role of\nlinguistic features in building more generalizable coreference resolvers. We\nshow that generalization improves only slightly by merely using a set of\nadditional linguistic features. However, employing features and subsets of\ntheir values that are informative for coreference resolution, considerably\nimproves generalization. Thanks to better generalization, our system achieves\nstate-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our\nsystem, which is trained on CoNLL, achieves on-par performance with a system\ndesigned for this dataset.", "published": "2017-08-01 05:09:34", "link": "http://arxiv.org/abs/1708.00160v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Investigation into the Pedagogical Features of Documents", "abstract": "Characterizing the content of a technical document in terms of its learning\nutility can be useful for applications related to education, such as generating\nreading lists from large collections of documents. We refer to this learning\nutility as the \"pedagogical value\" of the document to the learner. While\npedagogical value is an important concept that has been studied extensively\nwithin the education domain, there has been little work exploring it from a\ncomputational, i.e., natural language processing (NLP), perspective. To allow a\ncomputational exploration of this concept, we introduce the notion of\n\"pedagogical roles\" of documents (e.g., Tutorial and Survey) as an intermediary\ncomponent for the study of pedagogical value. Given the lack of available\ncorpora for our exploration, we create the first annotated corpus of\npedagogical roles and use it to test baseline techniques for automatic\nprediction of such roles.", "published": "2017-08-01 06:35:24", "link": "http://arxiv.org/abs/1708.00179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Part-of-Speech Tagging for NLP Pipelines", "abstract": "This paper outlines the results of sentence level linguistics based rules for\nimproving part-of-speech tagging. It is well known that the performance of\ncomplex NLP systems is negatively affected if one of the preliminary stages is\nless than perfect. Errors in the initial stages in the pipeline have a\nsnowballing effect on the pipeline's end performance. We have created a set of\nlinguistics based rules at the sentence level which adjust part-of-speech tags\nfrom state-of-the-art taggers. Comparison with state-of-the-art taggers on\nwidely used benchmarks demonstrate significant improvements in tagging accuracy\nand consequently in the quality and accuracy of NLP systems.", "published": "2017-08-01 10:56:17", "link": "http://arxiv.org/abs/1708.00241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Continuously Growing Dataset of Sentential Paraphrases", "abstract": "A major challenge in paraphrase research is the lack of parallel corpora. In\nthis paper, we present a new method to collect large-scale sentential\nparaphrases from Twitter by linking tweets through shared URLs. The main\nadvantage of our method is its simplicity, as it gets rid of the classifier or\nhuman in the loop needed to select data before annotation and subsequent\napplication of paraphrase identification algorithms in the previous work. We\npresent the largest human-labeled paraphrase corpus to date of 51,524 sentence\npairs and the first cross-domain benchmarking for automatic paraphrase\nidentification. In addition, we show that more than 30,000 new sentential\nparaphrases can be easily and continuously captured every month at ~70%\nprecision, and demonstrate their utility for downstream NLP tasks through\nphrasal paraphrase extraction. We make our code and data freely available.", "published": "2017-08-01 15:41:51", "link": "http://arxiv.org/abs/1708.00391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Generative Parser with a Discriminative Recognition Algorithm", "abstract": "Generative models defining joint distributions over parse trees and sentences\nare useful for parsing and language modeling, but impose restrictions on the\nscope of features and are often outperformed by discriminative models. We\npropose a framework for parsing and language modeling which marries a\ngenerative model with a discriminative recognition model in an encoder-decoder\nsetting. We provide interpretations of the framework based on expectation\nmaximization and variational inference, and show that it enables parsing and\nlanguage modeling within a single implementation. On the English Penn\nTreen-bank, our framework obtains competitive performance on constituency\nparsing while matching the state-of-the-art single-model language modeling\nscore.", "published": "2017-08-01 17:02:45", "link": "http://arxiv.org/abs/1708.00415v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deriving Verb Predicates By Clustering Verbs with Arguments", "abstract": "Hand-built verb clusters such as the widely used Levin classes (Levin, 1993)\nhave proved useful, but have limited coverage. Verb classes automatically\ninduced from corpus data such as those from VerbKB (Wijaya, 2016), on the other\nhand, can give clusters with much larger coverage, and can be adapted to\nspecific corpora such as Twitter. We present a method for clustering the\noutputs of VerbKB: verbs with their multiple argument types, e.g.\n\"marry(person, person)\", \"feel(person, emotion).\" We make use of a novel\nlow-dimensional embedding of verbs and their arguments to produce high quality\nclusters in which the same verb can be in different clusters depending on its\nargument type. The resulting verb clusters do a better job than hand-built\nclusters of predicting sarcasm, sentiment, and locus of control in tweets.", "published": "2017-08-01 17:05:32", "link": "http://arxiv.org/abs/1708.00416v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing with Small Feed-Forward Networks", "abstract": "We show that small and shallow feed-forward neural networks can achieve near\nstate-of-the-art results on a range of unstructured and structured language\nprocessing tasks while being considerably cheaper in memory and computational\nrequirements than deep recurrent models. Motivated by resource-constrained\nenvironments like mobile phones, we showcase simple techniques for obtaining\nsuch small neural network models, and investigate different tradeoffs when\ndeciding how to allocate a small memory budget.", "published": "2017-08-01 09:13:44", "link": "http://arxiv.org/abs/1708.00214v1", "categories": ["cs.CL", "cs.NE", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Lightweight Front-end Tool for Interactive Entity Population", "abstract": "Entity population, a task of collecting entities that belong to a particular\ncategory, has attracted attention from vertical domains. There is still a high\ndemand for creating entity dictionaries in vertical domains, which are not\ncovered by existing knowledge bases. We develop a lightweight front-end tool\nfor facilitating interactive entity population. We implement key components\nnecessary for effective interactive entity population: 1) GUI-based dashboards\nto quickly modify an entity dictionary, and 2) entity highlighting on documents\nfor quickly viewing the current progress. We aim to reduce user cost from\nbeginning to end, including package installation and maintenance. The\nimplementation enables users to use this tool on their web browsers without any\nadditional packages --- users can focus on their missions to create entity\ndictionaries. Moreover, an entity expansion module is implemented as external\nAPIs. This design makes it easy to continuously improve interactive entity\npopulation pipelines. We are making our demo publicly available\n(http://bit.ly/luwak-demo).", "published": "2017-08-01 19:27:02", "link": "http://arxiv.org/abs/1708.00481v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improved Representation Learning for Predicting Commonsense Ontologies", "abstract": "Recent work in learning ontologies (hierarchical and partially-ordered\nstructures) has leveraged the intrinsic geometry of spaces of learned\nrepresentations to make predictions that automatically obey complex structural\nconstraints. We explore two extensions of one such model, the order-embedding\nmodel for hierarchical relation learning, with an aim towards improved\nperformance on text data for commonsense knowledge representation. Our first\nmodel jointly learns ordering relations and non-hierarchical knowledge in the\nform of raw text. Our second extension exploits the partial order structure of\nthe training data to find long-distance triplet constraints among embeddings\nwhich are poorly enforced by the pairwise training procedure. We find that both\nincorporating free text and augmented training constraints improve over the\noriginal order-embedding model and other strong baselines.", "published": "2017-08-01 23:05:57", "link": "http://arxiv.org/abs/1708.00549v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learned in Translation: Contextualized Word Vectors", "abstract": "Computer vision has benefited from initializing multiple deep layers with\nweights pretrained on large supervised training sets like ImageNet. Natural\nlanguage processing (NLP) typically sees initialization of only the lowest\nlayer of deep models with pretrained word vectors. In this paper, we use a deep\nLSTM encoder from an attentional sequence-to-sequence model trained for machine\ntranslation (MT) to contextualize word vectors. We show that adding these\ncontext vectors (CoVe) improves performance over using only unsupervised word\nand character vectors on a wide variety of common NLP tasks: sentiment analysis\n(SST, IMDb), question classification (TREC), entailment (SNLI), and question\nanswering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe\nimproves performance of our baseline models to the state of the art.", "published": "2017-08-01 00:05:34", "link": "http://arxiv.org/abs/1708.00107v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Continuous Relaxation of Beam Search for End-to-end Training of Neural\n  Sequence Models", "abstract": "Beam search is a desirable choice of test-time decoding algorithm for neural\nsequence models because it potentially avoids search errors made by simpler\ngreedy methods. However, typical cross entropy training procedures for these\nmodels do not directly consider the behaviour of the final decoding method. As\na result, for cross-entropy trained models, beam decoding can sometimes yield\nreduced test performance when compared with greedy decoding. In order to train\nmodels that can more effectively make use of beam search, we propose a new\ntraining procedure that focuses on the final loss metric (e.g. Hamming loss)\nevaluated on the output of beam search. While well-defined, this \"direct loss\"\nobjective is itself discontinuous and thus difficult to optimize. Hence, in our\napproach, we form a sub-differentiable surrogate objective by introducing a\nnovel continuous approximation of the beam search decoding procedure. In\nexperiments, we show that optimizing this new training objective yields\nsubstantially better results on two sequence tasks (Named Entity Recognition\nand CCG Supertagging) when compared with both cross entropy trained greedy\ndecoding and cross entropy trained beam decoding baselines.", "published": "2017-08-01 00:16:08", "link": "http://arxiv.org/abs/1708.00111v2", "categories": ["cs.LG", "cs.CL", "cs.NE", "I.2.7; I.2.6"], "primary_category": "cs.LG"}
{"title": "Retrofitting Distributional Embeddings to Knowledge Graphs with\n  Functional Relations", "abstract": "Knowledge graphs are a versatile framework to encode richly structured data\nrelationships, but it can be challenging to combine these graphs with\nunstructured data. Methods for retrofitting pre-trained entity representations\nto the structure of a knowledge graph typically assume that entities are\nembedded in a connected space and that relations imply similarity. However,\nuseful knowledge graphs often contain diverse entities and relations (with\npotentially disjoint underlying corpora) which do not accord with these\nassumptions. To overcome these limitations, we present Functional Retrofitting,\na framework that generalizes current retrofitting methods by explicitly\nmodeling pairwise relations. Our framework can directly incorporate a variety\nof pairwise penalty functions previously developed for knowledge graph\ncompletion. Further, it allows users to encode, learn, and extract information\nabout relation semantics. We present both linear and neural instantiations of\nthe framework. Functional Retrofitting significantly outperforms existing\nretrofitting methods on complex knowledge graphs and loses no accuracy on\nsimpler graphs (in which relations do imply similarity). Finally, we\ndemonstrate the utility of the framework by predicting new drug--disease\ntreatment pairs in a large, complex health knowledge graph.", "published": "2017-08-01 00:23:03", "link": "http://arxiv.org/abs/1708.00112v3", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Grounding Language for Transfer in Deep Reinforcement Learning", "abstract": "In this paper, we explore the utilization of natural language to drive\ntransfer for reinforcement learning (RL). Despite the wide-spread application\nof deep RL techniques, learning generalized policy representations that work\nacross domains remains a challenging problem. We demonstrate that textual\ndescriptions of environments provide a compact intermediate channel to\nfacilitate effective policy transfer. Specifically, by learning to ground the\nmeaning of text to the dynamics of the environment such as transitions and\nrewards, an autonomous agent can effectively bootstrap policy learning on a new\ndomain given its description. We employ a model-based RL approach consisting of\na differentiable planning module, a model-free component and a factorized state\nrepresentation to effectively use entity descriptions. Our model outperforms\nprior work on both transfer and multi-task scenarios in a variety of different\nenvironments. For instance, we achieve up to 14% and 11.5% absolute improvement\nover previously existing models in terms of average and initial rewards,\nrespectively.", "published": "2017-08-01 02:20:00", "link": "http://arxiv.org/abs/1708.00133v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Rating Regression with Abstractive Tips Generation for\n  Recommendation", "abstract": "Recently, some E-commerce sites launch a new interaction box called Tips on\ntheir mobile apps. Users can express their experience and feelings or provide\nsuggestions using short texts typically several words or one sentence. In\nessence, writing some tips and giving a numerical rating are two facets of a\nuser's product assessment action, expressing the user experience and feelings.\nJointly modeling these two facets is helpful for designing a better\nrecommendation system. While some existing models integrate text information\nsuch as item specifications or user reviews into user and item latent factors\nfor improving the rating prediction, no existing works consider tips for\nimproving recommendation quality. We propose a deep learning based framework\nnamed NRT which can simultaneously predict precise ratings and generate\nabstractive tips with good linguistic quality simulating user experience and\nfeelings. For abstractive tips generation, gated recurrent neural networks are\nemployed to \"translate\" user and item latent representations into a concise\nsentence. Extensive experiments on benchmark datasets from different domains\nshow that NRT achieves significant improvements over the state-of-the-art\nmethods. Moreover, the generated tips can vividly predict the user experience\nand feelings.", "published": "2017-08-01 04:25:17", "link": "http://arxiv.org/abs/1708.00154v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SenGen: Sentence Generating Neural Variational Topic Model", "abstract": "We present a new topic model that generates documents by sampling a topic for\none whole sentence at a time, and generating the words in the sentence using an\nRNN decoder that is conditioned on the topic of the sentence. We argue that\nthis novel formalism will help us not only visualize and model the topical\ndiscourse structure in a document better, but also potentially lead to more\ninterpretable topics since we can now illustrate topics by sampling\nrepresentative sentences instead of bag of words or phrases. We present a\nvariational auto-encoder approach for learning in which we use a factorized\nvariational encoder that independently models the posterior over topical\nmixture vectors of documents using a feed-forward network, and the posterior\nover topic assignments to sentences using an RNN. Our preliminary experiments\non two different datasets indicate early promise, but also expose many\nchallenges that remain to be addressed.", "published": "2017-08-01 13:31:24", "link": "http://arxiv.org/abs/1708.00308v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "End-to-End Neural Segmental Models for Speech Recognition", "abstract": "Segmental models are an alternative to frame-based models for sequence\nprediction, where hypothesized path weights are based on entire segment scores\nrather than a single frame at a time. Neural segmental models are segmental\nmodels that use neural network-based weight functions. Neural segmental models\nhave achieved competitive results for speech recognition, and their end-to-end\ntraining has been explored in several studies. In this work, we review neural\nsegmental models, which can be viewed as consisting of a neural network-based\nacoustic encoder and a finite-state transducer decoder. We study end-to-end\nsegmental models with different weight functions, including ones based on\nframe-level neural classifiers and on segmental recurrent neural networks. We\nstudy how reducing the search space size impacts performance under different\nweight functions. We also compare several loss functions for end-to-end\ntraining. Finally, we explore training approaches, including multi-stage vs.\nend-to-end training and multitask training that combines segmental and\nframe-level losses.", "published": "2017-08-01 21:53:56", "link": "http://arxiv.org/abs/1708.00531v2", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
