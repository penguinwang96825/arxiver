{"title": "NC-DRE: Leveraging Non-entity Clue Information for Document-level\n  Relation Extraction", "abstract": "Document-level relation extraction (RE), which requires reasoning on multiple\nentities in different sentences to identify complex inter-sentence relations,\nis more challenging than sentence-level RE. To extract the complex\ninter-sentence relations, previous studies usually employ graph neural networks\n(GNN) to perform inference upon heterogeneous document-graphs. Despite their\ngreat successes, these graph-based methods, which normally only consider the\nwords within the mentions in the process of building graphs and reasoning, tend\nto ignore the non-entity clue words that are not in the mentions but provide\nimportant clue information for relation reasoning. To alleviate this problem,\nwe treat graph-based document-level RE models as an encoder-decoder framework,\nwhich typically uses a pre-trained language model as the encoder and a GNN\nmodel as the decoder, and propose a novel graph-based model NC-DRE that\nintroduces decoder-to-encoder attention mechanism to leverage Non-entity Clue\ninformation for Document-level Relation Extraction.", "published": "2022-04-01 07:30:26", "link": "http://arxiv.org/abs/2204.00255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PriMock57: A Dataset Of Primary Care Mock Consultations", "abstract": "Recent advances in Automatic Speech Recognition (ASR) have made it possible\nto reliably produce automatic transcripts of clinician-patient conversations.\nHowever, access to clinical datasets is heavily restricted due to patient\nprivacy, thus slowing down normal research practices. We detail the development\nof a public access, high quality dataset comprising of57 mocked primary care\nconsultations, including audio recordings, their manual utterance-level\ntranscriptions, and the associated consultation notes. Our work illustrates how\nthe dataset can be used as a benchmark for conversational medical ASR as well\nas consultation note generation from transcripts.", "published": "2022-04-01 10:18:28", "link": "http://arxiv.org/abs/2204.00333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cyberbullying detection across social media platforms via platform-aware\n  adversarial encoding", "abstract": "Despite the increasing interest in cyberbullying detection, existing efforts\nhave largely been limited to experiments on a single platform and their\ngeneralisability across different social media platforms have received less\nattention. We propose XP-CB, a novel cross-platform framework based on\nTransformers and adversarial learning. XP-CB can enhance a Transformer\nleveraging unlabelled data from the source and target platforms to come up with\na common representation while preventing platform-specific training. To\nvalidate our proposed framework, we experiment on cyberbullying datasets from\nthree different platforms through six cross-platform configurations, showing\nits effectiveness with both BERT and RoBERTa as the underlying Transformer\nmodels.", "published": "2022-04-01 10:25:46", "link": "http://arxiv.org/abs/2204.00334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Shallow Discourse Parsing in the PDTB-3: Handling\n  Intra-sentential Implicits", "abstract": "In the PDTB-3, several thousand implicit discourse relations were newly\nannotated \\textit{within} individual sentences, adding to the over 15,000\nimplicit relations annotated \\textit{across} adjacent sentences in the PDTB-2.\nGiven that the position of the arguments to these \\textit{intra-sentential\nimplicits} is no longer as well-defined as with \\textit{inter-sentential\nimplicits}, a discourse parser must identify both their location and their\nsense. That is the focus of the current work. The paper provides a\ncomprehensive analysis of our results, showcasing model performance under\ndifferent scenarios, pointing out limitations and noting future directions.", "published": "2022-04-01 10:56:25", "link": "http://arxiv.org/abs/2204.00350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Biomedical Term Clustering by Learning Fine-grained Term\n  Representations", "abstract": "Term clustering is important in biomedical knowledge graph construction.\nUsing similarities between terms embedding is helpful for term clustering.\nState-of-the-art term embeddings leverage pretrained language models to encode\nterms, and use synonyms and relation knowledge from knowledge graphs to guide\ncontrastive learning. These embeddings provide close embeddings for terms\nbelonging to the same concept. However, from our probing experiments, these\nembeddings are not sensitive to minor textual differences which leads to\nfailure for biomedical term clustering. To alleviate this problem, we adjust\nthe sampling strategy in pretraining term embeddings by providing dynamic hard\npositive and negative samples during contrastive learning to learn fine-grained\nrepresentations which result in better biomedical term clustering. We name our\nproposed method as CODER++, and it has been applied in clustering biomedical\nconcepts in the newly released Biomedical Knowledge Graph named BIOS.", "published": "2022-04-01 12:30:58", "link": "http://arxiv.org/abs/2204.00391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sense disambiguation of compound constituents", "abstract": "In distributional semantic accounts of the meaning of noun-noun compounds\n(e.g. starfish, bank account, houseboat) the important role of constituent\npolysemy remains largely unaddressed(cf. the meaning of star in starfish vs.\nstar cluster vs. star athlete). Instead of semantic vectors that average over\nthe different meanings of a constituent, disambiguated vectors of the\nconstituents would be needed in order to see what these more specific\nconstituent meanings contribute to the meaning of the compound as a whole. This\npaper presents a novel approach to this specific problem of word sense\ndisambiguation: set expansion. We build on the approach developed by Mahabal et\nal. (2018) which was originally designed to solve the analogy problem. We\nmodified their method in such a way that it can address the problem of sense\ndisambiguation of compound constituents. The results of experiments with a data\nset of almost 9000 compounds (LADEC, Gagn\\'e et al. 2019) suggest that this\napproach is successful, yet the success is sensitive to the frequency with\nwhich the compounds are attested.", "published": "2022-04-01 13:33:50", "link": "http://arxiv.org/abs/2204.00429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Evaluation and Correlation with Automatic Metrics in Consultation\n  Note Generation", "abstract": "In recent years, machine learning models have rapidly become better at\ngenerating clinical consultation notes; yet, there is little work on how to\nproperly evaluate the generated consultation notes to understand the impact\nthey may have on both the clinician using them and the patient's clinical\nsafety. To address this we present an extensive human evaluation study of\nconsultation notes where 5 clinicians (i) listen to 57 mock consultations, (ii)\nwrite their own notes, (iii) post-edit a number of automatically generated\nnotes, and (iv) extract all the errors, both quantitative and qualitative. We\nthen carry out a correlation study with 18 automatic quality metrics and the\nhuman judgements. We find that a simple, character-based Levenshtein distance\nmetric performs on par if not better than common model-based metrics like\nBertScore. All our findings and annotations are open-sourced.", "published": "2022-04-01 14:04:16", "link": "http://arxiv.org/abs/2204.00447v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Fake News Detection with Knowledge-Enhanced Language\n  Models", "abstract": "Recent advances in fake news detection have exploited the success of\nlarge-scale pre-trained language models (PLMs). The predominant\nstate-of-the-art approaches are based on fine-tuning PLMs on labelled fake news\ndatasets. However, large-scale PLMs are generally not trained on structured\nfactual data and hence may not possess priors that are grounded in factually\naccurate knowledge. The use of existing knowledge bases (KBs) with rich\nhuman-curated factual information has thus the potential to make fake news\ndetection more effective and robust. In this paper, we investigate the impact\nof knowledge integration into PLMs for fake news detection. We study several\nstate-of-the-art approaches for knowledge integration, mostly using Wikidata as\nKB, on two popular fake news datasets - LIAR, a politics-based dataset, and\nCOVID-19, a dataset of messages posted on social media relating to the COVID-19\npandemic. Our experiments show that knowledge-enhanced models can significantly\nimprove fake news detection on LIAR where the KB is relevant and up-to-date.\nThe mixed results on COVID-19 highlight the reliance on stylistic features and\nthe importance of domain-specific and current KBs.", "published": "2022-04-01 14:14:46", "link": "http://arxiv.org/abs/2204.00458v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty Determines the Adequacy of the Mode and the Tractability of\n  Decoding in Sequence-to-Sequence Models", "abstract": "In many natural language processing (NLP) tasks the same input (e.g. source\nsentence) can have multiple possible outputs (e.g. translations). To analyze\nhow this ambiguity (also known as intrinsic uncertainty) shapes the\ndistribution learned by neural sequence models we measure sentence-level\nuncertainty by computing the degree of overlap between references in\nmulti-reference test sets from two different NLP tasks: machine translation\n(MT) and grammatical error correction (GEC). At both the sentence- and the\ntask-level, intrinsic uncertainty has major implications for various aspects of\nsearch such as the inductive biases in beam search and the complexity of exact\nsearch. In particular, we show that well-known pathologies such as a high\nnumber of beam search errors, the inadequacy of the mode, and the drop in\nsystem performance with large beam sizes apply to tasks with high level of\nambiguity such as MT but not to less uncertain tasks such as GEC. Furthermore,\nwe propose a novel exact $n$-best search algorithm for neural sequence models,\nand show that intrinsic uncertainty affects model uncertainty as the model\ntends to overly spread out the probability mass for uncertain tasks and\nsentences.", "published": "2022-04-01 14:30:19", "link": "http://arxiv.org/abs/2204.00471v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CipherDAug: Ciphertext based Data Augmentation for Neural Machine\n  Translation", "abstract": "We propose a novel data-augmentation technique for neural machine translation\nbased on ROT-$k$ ciphertexts. ROT-$k$ is a simple letter substitution cipher\nthat replaces a letter in the plaintext with the $k$th letter after it in the\nalphabet. We first generate multiple ROT-$k$ ciphertexts using different values\nof $k$ for the plaintext which is the source side of the parallel data. We then\nleverage this enciphered training data along with the original parallel data\nvia multi-source training to improve neural machine translation. Our method,\nCipherDAug, uses a co-regularization-inspired training procedure, requires no\nexternal data sources other than the original training data, and uses a\nstandard Transformer to outperform strong data augmentation techniques on\nseveral datasets by a significant margin. This technique combines easily with\nexisting approaches to data augmentation, and yields particularly strong\nresults in low-resource settings.", "published": "2022-04-01 19:02:14", "link": "http://arxiv.org/abs/2204.00665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Argument Structure Extraction with Transfer Learning and\n  Active Learning", "abstract": "The automation of extracting argument structures faces a pair of challenges\non (1) encoding long-term contexts to facilitate comprehensive understanding,\nand (2) improving data efficiency since constructing high-quality argument\nstructures is time-consuming. In this work, we propose a novel context-aware\nTransformer-based argument structure prediction model which, on five different\ndomains, significantly outperforms models that rely on features or only encode\nlimited contexts. To tackle the difficulty of data annotation, we examine two\ncomplementary methods: (i) transfer learning to leverage existing annotated\ndata to boost model performance in a new target domain, and (ii) active\nlearning to strategically identify a small amount of samples for annotation. We\nfurther propose model-independent sample acquisition strategies, which can be\ngeneralized to diverse domains. With extensive experiments, we show that our\nsimple-yet-effective acquisition strategies yield competitive results against\nthree strong comparisons. Combined with transfer learning, substantial F1 score\nboost (5-25) can be further achieved during the early iterations of active\nlearning across domains.", "published": "2022-04-01 22:08:17", "link": "http://arxiv.org/abs/2204.00707v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Pre-trained Language Models End-to-end Few-shot Learners with\n  Contrastive Prompt Tuning", "abstract": "Pre-trained Language Models (PLMs) have achieved remarkable performance for\nvarious language understanding tasks in IR systems, which require the\nfine-tuning process based on labeled training data. For low-resource scenarios,\nprompt-based learning for PLMs exploits prompts as task guidance and turns\ndownstream tasks into masked language problems for effective few-shot\nfine-tuning. In most existing approaches, the high performance of prompt-based\nlearning heavily relies on handcrafted prompts and verbalizers, which may limit\nthe application of such approaches in real-world scenarios. To solve this\nissue, we present CP-Tuning, the first end-to-end Contrastive Prompt Tuning\nframework for fine-tuning PLMs without any manual engineering of task-specific\nprompts and verbalizers. It is integrated with the task-invariant continuous\nprompt encoding technique with fully trainable prompt parameters. We further\npropose the pair-wise cost-sensitive contrastive learning procedure to optimize\nthe model in order to achieve verbalizer-free class mapping and enhance the\ntask-invariance of prompts. It explicitly learns to distinguish different\nclasses and makes the decision boundary smoother by assigning different costs\nto easy and hard cases. Experiments over a variety of language understanding\ntasks used in IR systems and different PLMs show that CP-Tuning outperforms\nstate-of-the-art methods.", "published": "2022-04-01 02:24:24", "link": "http://arxiv.org/abs/2204.00166v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graph Enhanced Contrastive Learning for Radiology Findings Summarization", "abstract": "The impression section of a radiology report summarizes the most prominent\nobservation from the findings section and is the most important section for\nradiologists to communicate to physicians. Summarizing findings is\ntime-consuming and can be prone to error for inexperienced radiologists, and\nthus automatic impression generation has attracted substantial attention. With\nthe encoder-decoder framework, most previous studies explore incorporating\nextra knowledge (e.g., static pre-defined clinical ontologies or extra\nbackground information). Yet, they encode such knowledge by a separate encoder\nto treat it as an extra input to their models, which is limited in leveraging\ntheir relations with the original findings. To address the limitation, we\npropose a unified framework for exploiting both extra knowledge and the\noriginal findings in an integrated way so that the critical information (i.e.,\nkey words and their relations) can be extracted in an appropriate way to\nfacilitate impression generation. In detail, for each input findings, it is\nencoded by a text encoder, and a graph is constructed through its entities and\ndependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is\nadopted to model relation information in the constructed graph. Finally, to\nemphasize the key words in the findings, contrastive learning is introduced to\nmap positive samples (constructed by masking non-key words) closer and push\napart negative ones (constructed by masking key words). The experimental\nresults on OpenI and MIMIC-CXR confirm the effectiveness of our proposed\nmethod.", "published": "2022-04-01 04:39:44", "link": "http://arxiv.org/abs/2204.00203v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multifaceted Improvements for Conversational Open-Domain Question\n  Answering", "abstract": "Open-domain question answering (OpenQA) is an important branch of textual QA\nwhich discovers answers for the given questions based on a large number of\nunstructured documents. Effectively mining correct answers from the open-domain\nsources still has a fair way to go. Existing OpenQA systems might suffer from\nthe issues of question complexity and ambiguity, as well as insufficient\nbackground knowledge. Recently, conversational OpenQA is proposed to address\nthese issues with the abundant contextual information in the conversation.\nPromising as it might be, there exist several fundamental limitations including\nthe inaccurate question understanding, the coarse ranking for passage\nselection, and the inconsistent usage of golden passage in the training and\ninference phases. To alleviate these limitations, in this paper, we propose a\nframework with Multifaceted Improvements for Conversational open-domain\nQuestion Answering (MICQA). Specifically, MICQA has three significant\nadvantages. First, the proposed KL-divergence based regularization is able to\nlead to a better question understanding for retrieval and answer reading.\nSecond, the added post-ranker module can push more relevant passages to the top\nplacements and be selected for reader with a two-aspect constrains. Third, the\nwell designed curriculum learning strategy effectively narrows the gap between\nthe golden passage settings of training and inference, and encourages the\nreader to find true answer without the golden passage assistance. Extensive\nexperiments conducted on the publicly available dataset OR-QuAC demonstrate the\nsuperiority of MICQA over the state-of-the-art model in conversational OpenQA\ntask.", "published": "2022-04-01 07:54:27", "link": "http://arxiv.org/abs/2204.00266v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Predicting Intervention Approval in Clinical Trials through\n  Multi-Document Summarization", "abstract": "Clinical trials offer a fundamental opportunity to discover new treatments\nand advance the medical knowledge. However, the uncertainty of the outcome of a\ntrial can lead to unforeseen costs and setbacks. In this study, we propose a\nnew method to predict the effectiveness of an intervention in a clinical trial.\nOur method relies on generating an informative summary from multiple documents\navailable in the literature about the intervention under study. Specifically,\nour method first gathers all the abstracts of PubMed articles related to the\nintervention. Then, an evidence sentence, which conveys information about the\neffectiveness of the intervention, is extracted automatically from each\nabstract. Based on the set of evidence sentences extracted from the abstracts,\na short summary about the intervention is constructed. Finally, the produced\nsummaries are used to train a BERT-based classifier, in order to infer the\neffectiveness of an intervention. To evaluate our proposed method, we introduce\na new dataset which is a collection of clinical trials together with their\nassociated PubMed articles. Our experiments, demonstrate the effectiveness of\nproducing short informative summaries and using them to predict the\neffectiveness of an intervention.", "published": "2022-04-01 08:45:39", "link": "http://arxiv.org/abs/2204.00290v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing Speech Emotion Recognition Transformers for Linguistic Knowledge", "abstract": "Large, pre-trained neural networks consisting of self-attention layers\n(transformers) have recently achieved state-of-the-art results on several\nspeech emotion recognition (SER) datasets. These models are typically\npre-trained in self-supervised manner with the goal to improve automatic speech\nrecognition performance -- and thus, to understand linguistic information. In\nthis work, we investigate the extent in which this information is exploited\nduring SER fine-tuning. Using a reproducible methodology based on open-source\ntools, we synthesise prosodically neutral speech utterances while varying the\nsentiment of the text. Valence predictions of the transformer model are very\nreactive to positive and negative sentiment content, as well as negations, but\nnot to intensifiers or reducers, while none of those linguistic features impact\narousal or dominance. These findings show that transformers can successfully\nleverage linguistic information to improve their valence predictions, and that\nlinguistic analysis should be included in their testing.", "published": "2022-04-01 12:47:45", "link": "http://arxiv.org/abs/2204.00400v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structured Pruning Learns Compact and Accurate Models", "abstract": "The growing size of neural language models has led to increased attention in\nmodel compression. The two predominant approaches are pruning, which gradually\nremoves weights from a pre-trained model, and distillation, which trains a\nsmaller compact model to match a larger one. Pruning methods can significantly\nreduce the model size but hardly achieve large speedups as distillation.\nHowever, distillation methods require large amounts of unlabeled data and are\nexpensive to train. In this work, we propose a task-specific structured pruning\nmethod CoFi (Coarse- and Fine-grained Pruning), which delivers highly\nparallelizable subnetworks and matches the distillation methods in both\naccuracy and latency, without resorting to any unlabeled data. Our key insight\nis to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads\nand hidden units) modules, which controls the pruning decision of each\nparameter with masks of different granularity. We also devise a layerwise\ndistillation strategy to transfer knowledge from unpruned to pruned models\nduring optimization. Our experiments on GLUE and SQuAD datasets show that CoFi\nyields models with over 10x speedups with a small accuracy drop, showing its\neffectiveness and efficiency compared to previous pruning and distillation\napproaches.", "published": "2022-04-01 13:09:56", "link": "http://arxiv.org/abs/2204.00408v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Disentangled Representations of Negation and Uncertainty", "abstract": "Negation and uncertainty modeling are long-standing tasks in natural language\nprocessing. Linguistic theory postulates that expressions of negation and\nuncertainty are semantically independent from each other and the content they\nmodify. However, previous works on representation learning do not explicitly\nmodel this independence. We therefore attempt to disentangle the\nrepresentations of negation, uncertainty, and content using a Variational\nAutoencoder. We find that simply supervising the latent representations results\nin good disentanglement, but auxiliary objectives based on adversarial learning\nand mutual information minimization can provide additional disentanglement\ngains.", "published": "2022-04-01 15:12:05", "link": "http://arxiv.org/abs/2204.00511v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unified and Effective Ensemble Knowledge Distillation", "abstract": "Ensemble knowledge distillation can extract knowledge from multiple teacher\nmodels and encode it into a single student model. Many existing methods learn\nand distill the student model on labeled data only. However, the teacher models\nare usually learned on the same labeled data, and their predictions have high\ncorrelations with groudtruth labels. Thus, they cannot provide sufficient\nknowledge complementary to task labels for student teaching. Distilling on\nunseen unlabeled data has the potential to enhance the knowledge transfer from\nthe teachers to the student. In this paper, we propose a unified and effective\nensemble knowledge distillation method that distills a single student model\nfrom an ensemble of teacher models on both labeled and unlabeled data. Since\ndifferent teachers may have diverse prediction correctness on the same sample,\non labeled data we weight the predictions of different teachers according to\ntheir correctness. In addition, we weight the distillation loss based on the\noverall prediction correctness of the teacher ensemble to distill high-quality\nknowledge. On unlabeled data, there is no groundtruth to evaluate prediction\ncorrectness. Fortunately, the disagreement among teachers is an indication of\nsample hardness, and thereby we weight the distillation loss based on teachers'\ndisagreement to emphasize knowledge distillation on important samples.\nExtensive experiments on four datasets show the effectiveness of our proposed\nensemble distillation method.", "published": "2022-04-01 16:15:39", "link": "http://arxiv.org/abs/2204.00548v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Nowruz at SemEval-2022 Task 7: Tackling Cloze Tests with Transformers\n  and Ordinal Regression", "abstract": "This paper outlines the system using which team Nowruz participated in\nSemEval 2022 Task 7 Identifying Plausible Clarifications of Implicit and\nUnderspecified Phrases for both subtasks A and B. Using a pre-trained\ntransformer as a backbone, the model targeted the task of multi-task\nclassification and ranking in the context of finding the best fillers for a\ncloze task related to instructional texts on the website Wikihow.\n  The system employed a combination of two ordinal regression components to\ntackle this task in a multi-task learning scenario. According to the official\nleaderboard of the shared task, this system was ranked 5th in the ranking and\n7th in the classification subtasks out of 21 participating teams. With\nadditional experiments, the models have since been further optimised.", "published": "2022-04-01 16:36:10", "link": "http://arxiv.org/abs/2204.00556v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CharacterBERT and Self-Teaching for Improving the Robustness of Dense\n  Retrievers on Queries with Typos", "abstract": "Current dense retrievers are not robust to out-of-domain and outlier queries,\ni.e. their effectiveness on these queries is much poorer than what one would\nexpect. In this paper, we consider a specific instance of such queries: queries\nthat contain typos. We show that a small character level perturbation in\nqueries (as caused by typos) highly impacts the effectiveness of dense\nretrievers. We then demonstrate that the root cause of this resides in the\ninput tokenization strategy employed by BERT. In BERT, tokenization is\nperformed using the BERT's WordPiece tokenizer and we show that a token with a\ntypo will significantly change the token distributions obtained after\ntokenization. This distribution change translates to changes in the input\nembeddings passed to the BERT-based query encoder of dense retrievers. We then\nturn our attention to devising dense retriever methods that are robust to such\nqueries with typos, while still being as performant as previous methods on\nqueries without typos. For this, we use CharacterBERT as the backbone encoder\nand an efficient yet effective training method, called Self-Teaching (ST), that\ndistills knowledge from queries without typos into the queries with typos.\nExperimental results show that CharacterBERT in combination with ST achieves\nsignificantly higher effectiveness on queries with typos compared to previous\nmethods. Along with these results and the open-sourced implementation of the\nmethods, we also provide a new passage retrieval dataset consisting of\nreal-world queries with typos and associated relevance assessments on the MS\nMARCO corpus, thus supporting the research community in the investigation of\neffective and robust dense retrievers. Code, experimental results and dataset\nare made available at https://github.com/ielab/CharacterBERT-DR.", "published": "2022-04-01 23:02:50", "link": "http://arxiv.org/abs/2204.00716v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "COOL, a Context Outlooker, and its Application to Question Answering and\n  other Natural Language Processing Tasks", "abstract": "Vision outlooker improves the performance of vision transformers, which\nimplements a self-attention mechanism by adding an outlook attention, a form of\nlocal attention.\n  In natural language processing, as has been the case in computer vision and\nother domains, transformer-based models constitute the state-of-the-art for\nmost processing tasks. In this domain, too, many authors have argued and\ndemonstrated the importance of local context.\n  We present an outlook attention mechanism, COOL, for natural language\nprocessing. COOL, added on top of the self-attention layers of a\ntransformer-based model, encodes local syntactic context considering word\nproximity and more pair-wise constraints than dynamic convolution used by\nexisting approaches.\n  A comparative empirical performance evaluation of an implementation of COOL\nwith different transformer-based models confirms the opportunity for\nimprovement over a baseline using the original models alone for various natural\nlanguage processing tasks, including question answering. The proposed approach\nachieves competitive performance with existing state-of-the-art methods on some\ntasks.", "published": "2022-04-01 07:03:40", "link": "http://arxiv.org/abs/2204.09593v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Syntax-informed Question Answering with Heterogeneous Graph Transformer", "abstract": "Large neural language models are steadily contributing state-of-the-art\nperformance to question answering and other natural language and information\nprocessing tasks. These models are expensive to train. We propose to evaluate\nwhether such pre-trained models can benefit from the addition of explicit\nlinguistics information without requiring retraining from scratch.\n  We present a linguistics-informed question answering approach that extends\nand fine-tunes a pre-trained transformer-based neural language model with\nsymbolic knowledge encoded with a heterogeneous graph transformer. We\nillustrate the approach by the addition of syntactic information in the form of\ndependency and constituency graphic structures connecting tokens and virtual\nvertices.\n  A comparative empirical performance evaluation with BERT as its baseline and\nwith Stanford Question Answering Dataset demonstrates the competitiveness of\nthe proposed approach. We argue, in conclusion and in the light of further\nresults of preliminary experiments, that the approach is extensible to further\nlinguistics information including semantics and pragmatics.", "published": "2022-04-01 07:48:03", "link": "http://arxiv.org/abs/2204.09655v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Filter-based Discriminative Autoencoders for Children Speech Recognition", "abstract": "Children speech recognition is indispensable but challenging due to the\ndiversity of children's speech. In this paper, we propose a filter-based\ndiscriminative autoencoder for acoustic modeling. To filter out the influence\nof various speaker types and pitches, auxiliary information of the speaker and\npitch features is input into the encoder together with the acoustic features to\ngenerate phonetic embeddings. In the training phase, the decoder uses the\nauxiliary information and the phonetic embedding extracted by the encoder to\nreconstruct the input acoustic features. The autoencoder is trained by\nsimultaneously minimizing the ASR loss and feature reconstruction error. The\nframework can make the phonetic embedding purer, resulting in more accurate\nsenone (triphone-state) scores. Evaluated on the test set of the CMU Kids\ncorpus, our system achieves a 7.8% relative WER reduction compared to the\nbaseline system. In the domain adaptation experiment, our system also\noutperforms the baseline system on the British-accent PF-STAR task.", "published": "2022-04-01 02:18:57", "link": "http://arxiv.org/abs/2204.00164v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "InterAug: Augmenting Noisy Intermediate Predictions for CTC-based ASR", "abstract": "This paper proposes InterAug: a novel training method for CTC-based ASR using\naugmented intermediate representations for conditioning. The proposed method\nexploits the conditioning framework of self-conditioned CTC to train robust\nmodels by conditioning with \"noisy\" intermediate predictions. During the\ntraining, intermediate predictions are changed to incorrect intermediate\npredictions, and fed into the next layer for conditioning. The subsequent\nlayers are trained to correct the incorrect intermediate predictions with the\nintermediate losses. By repeating the augmentation and the correction,\niterative refinements, which generally require a special decoder, can be\nrealized only with the audio encoder. To produce noisy intermediate\npredictions, we also introduce new augmentation: intermediate feature space\naugmentation and intermediate token space augmentation that are designed to\nsimulate typical errors. The combination of the proposed InterAug framework\nwith new augmentation allows explicit training of the robust audio encoders. In\nexperiments using augmentations simulating deletion, insertion, and\nsubstitution error, we confirmed that the trained model acquires robustness to\neach error, boosting the speech recognition performance of the strong\nself-conditioned CTC baseline.", "published": "2022-04-01 02:51:21", "link": "http://arxiv.org/abs/2204.00174v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Alternate Intermediate Conditioning with Syllable-level and\n  Character-level Targets for Japanese ASR", "abstract": "End-to-end automatic speech recognition directly maps input speech to\ncharacters. However, the mapping can be problematic when several different\npronunciations should be mapped into one character or when one pronunciation is\nshared among many different characters. Japanese ASR suffers the most from such\nmany-to-one and one-to-many mapping problems due to Japanese kanji characters.\nTo alleviate the problems, we introduce explicit interaction between characters\nand syllables using Self-conditioned connectionist temporal classification\n(CTC), in which the upper layers are ``self-conditioned'' on the intermediate\npredictions from the lower layers. The proposed method utilizes character-level\nand syllable-level intermediate predictions as conditioning features to deal\nwith mutual dependency between characters and syllables. Experimental results\non Corpus of Spontaneous Japanese show that the proposed method outperformed\nthe conventional multi-task and Self-conditioned CTC methods.", "published": "2022-04-01 02:51:22", "link": "http://arxiv.org/abs/2204.00175v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Better Intermediates Improve CTC Inference", "abstract": "This paper proposes a method for improved CTC inference with searched\nintermediates and multi-pass conditioning. The paper first formulates\nself-conditioned CTC as a probabilistic model with an intermediate prediction\nas a latent representation and provides a tractable conditioning framework. We\nthen propose two new conditioning methods based on the new formulation: (1)\nSearched intermediate conditioning that refines intermediate predictions with\nbeam-search, (2) Multi-pass conditioning that uses predictions of previous\ninference for conditioning the next inference. These new approaches enable\nbetter conditioning than the original self-conditioned CTC during inference and\nimprove the final performance. Experiments with the LibriSpeech dataset show\nrelative 3%/12% performance improvement at the maximum in test clean/other sets\ncompared to the original self-conditioned CTC.", "published": "2022-04-01 02:51:23", "link": "http://arxiv.org/abs/2204.00176v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Effect and Analysis of Large-scale Language Model Rescoring on\n  Competitive ASR Systems", "abstract": "Large-scale language models (LLMs) such as GPT-2, BERT and RoBERTa have been\nsuccessfully applied to ASR N-best rescoring. However, whether or how they can\nbenefit competitive, near state-of-the-art ASR systems remains unexplored. In\nthis study, we incorporate LLM rescoring into one of the most competitive ASR\nbaselines: the Conformer-Transducer model. We demonstrate that consistent\nimprovement is achieved by the LLM's bidirectionality, pretraining, in-domain\nfinetuning and context augmentation. Furthermore, our lexical analysis sheds\nlight on how each of these components may be contributing to the ASR\nperformance.", "published": "2022-04-01 05:20:55", "link": "http://arxiv.org/abs/2204.00212v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "End-to-End Multi-speaker ASR with Independent Vector Analysis", "abstract": "We develop an end-to-end system for multi-channel, multi-speaker automatic\nspeech recognition. We propose a frontend for joint source separation and\ndereverberation based on the independent vector analysis (IVA) paradigm. It\nuses the fast and stable iterative source steering algorithm together with a\nneural source model. The parameters from the ASR module and the neural source\nmodel are optimized jointly from the ASR loss itself. We demonstrate\ncompetitive performance with previous systems using neural beamforming\nfrontends. First, we explore the trade-offs when using various number of\nchannels for training and testing. Second, we demonstrate that the proposed IVA\nfrontend performs well on noisy data, even when trained on clean mixtures only.\nFurthermore, it extends without retraining to the separation of more speakers,\nwhich is demonstrated on mixtures of three and four speakers.", "published": "2022-04-01 05:45:33", "link": "http://arxiv.org/abs/2204.00218v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text-To-Speech Data Augmentation for Low Resource Speech Recognition", "abstract": "Nowadays, the main problem of deep learning techniques used in the\ndevelopment of automatic speech recognition (ASR) models is the lack of\ntranscribed data. The goal of this research is to propose a new data\naugmentation method to improve ASR models for agglutinative and low-resource\nlanguages. This novel data augmentation method generates both synthetic text\nand synthetic audio. Some experiments were conducted using the corpus of the\nQuechua language, which is an agglutinative and low-resource language. In this\nstudy, a sequence-to-sequence (seq2seq) model was applied to generate synthetic\ntext, in addition to generating synthetic speech using a text-to-speech (TTS)\nmodel for Quechua. The results show that the new data augmentation method works\nwell to improve the ASR model for Quechua. In this research, an 8.73%\nimprovement in the word-error-rate (WER) of the ASR model is obtained using a\ncombination of synthetic text and synthetic speech.", "published": "2022-04-01 08:53:44", "link": "http://arxiv.org/abs/2204.00291v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "WavFT: Acoustic model finetuning with labelled and unlabelled data", "abstract": "Unsupervised and self-supervised learning methods have leveraged unlabelled\ndata to improve the pretrained models. However, these methods need\nsignificantly large amount of unlabelled data and the computational cost of\ntraining models with such large amount of data can be prohibitively high. We\naddress this issue by using unlabelled data during finetuning, instead of\npretraining. We propose acoustic model finetuning (FT) using labelled and\nunlabelled data. The model is jointly trained to learn representations to\nclassify senones, as well as learn contextual acoustic representations. Our\ntraining objective is a combination of cross entropy loss, suitable for\nclassification task, and contrastive loss, suitable to learn acoustic\nrepresentations. The proposed approach outperforms conventional finetuning with\n11.2% and 9.19% word error rate relative (WERR) reduction on Gujarati and\nBengali languages respectively.", "published": "2022-04-01 10:52:28", "link": "http://arxiv.org/abs/2204.00348v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios", "abstract": "Adaptive text to speech (TTS) can synthesize new voices in zero-shot\nscenarios efficiently, by using a well-trained source TTS model without\nadapting it on the speech data of new speakers. Considering seen and unseen\nspeakers have diverse characteristics, zero-shot adaptive TTS requires strong\ngeneralization ability on speaker characteristics, which brings modeling\nchallenges. In this paper, we develop AdaSpeech 4, a zero-shot adaptive TTS\nsystem for high-quality speech synthesis. We model the speaker characteristics\nsystematically to improve the generalization on new speakers. Generally, the\nmodeling of speaker characteristics can be categorized into three steps:\nextracting speaker representation, taking this speaker representation as\ncondition, and synthesizing speech/mel-spectrogram given this speaker\nrepresentation. Accordingly, we improve the modeling in three steps: 1) To\nextract speaker representation with better generalization, we factorize the\nspeaker characteristics into basis vectors and extract speaker representation\nby weighted combining of these basis vectors through attention. 2) We leverage\nconditional layer normalization to integrate the extracted speaker\nrepresentation to TTS model. 3) We propose a novel supervision loss based on\nthe distribution of basis vectors to maintain the corresponding speaker\ncharacteristics in generated mel-spectrograms. Without any fine-tuning,\nAdaSpeech 4 achieves better voice quality and similarity than baselines in\nmultiple datasets.", "published": "2022-04-01 13:47:44", "link": "http://arxiv.org/abs/2204.00436v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Zero-Shot Cross-lingual Aphasia Detection using Automatic Speech\n  Recognition", "abstract": "Aphasia is a common speech and language disorder, typically caused by a brain\ninjury or a stroke, that affects millions of people worldwide. Detecting and\nassessing Aphasia in patients is a difficult, time-consuming process, and\nnumerous attempts to automate it have been made, the most successful using\nmachine learning models trained on aphasic speech data. Like in many medical\napplications, aphasic speech data is scarce and the problem is exacerbated in\nso-called \"low resource\" languages, which are, for this task, most languages\nexcluding English. We attempt to leverage available data in English and achieve\nzero-shot aphasia detection in low-resource languages such as Greek and French,\nby using language-agnostic linguistic features. Current cross-lingual aphasia\ndetection approaches rely on manually extracted transcripts. We propose an\nend-to-end pipeline using pre-trained Automatic Speech Recognition (ASR) models\nthat share cross-lingual speech representations and are fine-tuned for our\ndesired low-resource languages. To further boost our ASR model's performance,\nwe also combine it with a language model. We show that our ASR-based end-to-end\npipeline offers comparable results to previous setups using human-annotated\ntranscripts.", "published": "2022-04-01 14:05:02", "link": "http://arxiv.org/abs/2204.00448v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "End-to-End Integration of Speech Recognition, Speech Enhancement, and\n  Self-Supervised Learning Representation", "abstract": "This work presents our end-to-end (E2E) automatic speech recognition (ASR)\nmodel targetting at robust speech recognition, called Integraded speech\nRecognition with enhanced speech Input for Self-supervised learning\nrepresentation (IRIS). Compared with conventional E2E ASR models, the proposed\nE2E model integrates two important modules including a speech enhancement (SE)\nmodule and a self-supervised learning representation (SSLR) module. The SE\nmodule enhances the noisy speech. Then the SSLR module extracts features from\nenhanced speech to be used for speech recognition (ASR). To train the proposed\nmodel, we establish an efficient learning scheme. Evaluation results on the\nmonaural CHiME-4 task show that the IRIS model achieves the best performance\nreported in the literature for the single-channel CHiME-4 benchmark (2.0% for\nthe real development and 3.9% for the real test) thanks to the powerful\npre-trained SSLR module and the fine-tuned SE module.", "published": "2022-04-01 16:02:31", "link": "http://arxiv.org/abs/2204.00540v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Novel Multimodal Approach for Studying the Dynamics of Curiosity in\n  Small Group Learning", "abstract": "Curiosity is a vital metacognitive skill in educational contexts, leading to\ncreativity, and a love of learning. And while many school systems increasingly\nundercut curiosity by teaching to the test, teachers are increasingly\ninterested in how to evoke curiosity in their students to prepare them for a\nworld in which lifelong learning and reskilling will be more and more\nimportant. One aspect of curiosity that has received little attention, however,\nis the role of peers in eliciting curiosity. We present what we believe to be\nthe first theoretical framework that articulates an integrated socio-cognitive\naccount of curiosity that ties observable behaviors in peers to underlying\ncuriosity states. We make a bipartite distinction between individual and\ninterpersonal functions that contribute to curiosity, and multimodal behaviors\nthat fulfill these functions. We validate the proposed framework by leveraging\na longitudinal latent variable modeling approach. Findings confirm a positive\npredictive relationship between the latent variables of individual and\ninterpersonal functions and curiosity, with the interpersonal functions\nexercising a comparatively stronger influence. Prominent behavioral\nrealizations of these functions are also discovered in a data-driven manner. We\ninstantiate the proposed theoretical framework in a set of strategies and\ntactics that can be incorporated into learning technologies to indicate, evoke,\nand scaffold curiosity. This work is a step towards designing learning\ntechnologies that can recognize and evoke moment-by-moment curiosity during\nlearning in social contexts and towards a more complete multimodal learning\nanalytics. The underlying rationale is applicable more generally for developing\ncomputer support for other metacognitive and socio-emotional skills.", "published": "2022-04-01 16:12:40", "link": "http://arxiv.org/abs/2204.00545v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Multi-task RNN-T with Semantic Decoder for Streamable Spoken Language\n  Understanding", "abstract": "End-to-end Spoken Language Understanding (E2E SLU) has attracted increasing\ninterest due to its advantages of joint optimization and low latency when\ncompared to traditionally cascaded pipelines. Existing E2E SLU models usually\nfollow a two-stage configuration where an Automatic Speech Recognition (ASR)\nnetwork first predicts a transcript which is then passed to a Natural Language\nUnderstanding (NLU) module through an interface to infer semantic labels, such\nas intent and slot tags. This design, however, does not consider the NLU\nposterior while making transcript predictions, nor correct the NLU prediction\nerror immediately by considering the previously predicted word-pieces. In\naddition, the NLU model in the two-stage system is not streamable, as it must\nwait for the audio segments to complete processing, which ultimately impacts\nthe latency of the SLU system. In this work, we propose a streamable multi-task\nsemantic transducer model to address these considerations. Our proposed\narchitecture predicts ASR and NLU labels auto-regressively and uses a semantic\ndecoder to ingest both previously predicted word-pieces and slot tags while\naggregating them through a fusion network. Using an industry scale SLU and a\npublic FSC dataset, we show the proposed model outperforms the two-stage E2E\nSLU model for both ASR and NLU metrics.", "published": "2022-04-01 16:38:56", "link": "http://arxiv.org/abs/2204.00558v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "abstract": "Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities\ndepending on the domain of data they are trained on. While these domains are\ngeneric, they may only barely overlap. For example, visual-language models\n(VLMs) are trained on Internet-scale image captions, but large language models\n(LMs) are further trained on Internet-scale text with no images (e.g.,\nspreadsheets, SAT questions, code). As a result, these models store different\nforms of commonsense knowledge across different domains. In this work, we show\nthat this diversity is symbiotic, and can be leveraged through Socratic Models\n(SMs): a modular framework in which multiple pretrained models may be composed\nzero-shot i.e., via multimodal-informed prompting, to exchange information with\neach other and capture new multimodal capabilities, without requiring\nfinetuning. With minimal engineering, SMs are not only competitive with\nstate-of-the-art zero-shot image captioning and video-to-text retrieval, but\nalso enable new applications such as (i) answering free-form questions about\negocentric video, (ii) engaging in multimodal assistive dialogue with people\n(e.g., for cooking recipes) by interfacing with external APIs and databases\n(e.g., web search), and (iii) robot perception and planning.", "published": "2022-04-01 17:43:13", "link": "http://arxiv.org/abs/2204.00598v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "End-to-end multi-talker audio-visual ASR using an active speaker\n  attention module", "abstract": "This paper presents a new approach for end-to-end audio-visual multi-talker\nspeech recognition. The approach, referred to here as the visual context\nattention model (VCAM), is important because it uses the available video\ninformation to assign decoded text to one of multiple visible faces. This\nessentially resolves the label ambiguity issue associated with most\nmulti-talker modeling approaches which can decode multiple label strings but\ncannot assign the label strings to the correct speakers. This is implemented as\na transformer-transducer based end-to-end model and evaluated using a two\nspeaker audio-visual overlapping speech dataset created from YouTube videos. It\nis shown in the paper that the VCAM model improves performance with respect to\npreviously reported audio-only and audio-visual multi-talker ASR systems.", "published": "2022-04-01 18:42:14", "link": "http://arxiv.org/abs/2204.00652v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Feature Structure Distillation with Centered Kernel Alignment in BERT\n  Transferring", "abstract": "Knowledge distillation is an approach to transfer information on\nrepresentations from a teacher to a student by reducing their difference. A\nchallenge of this approach is to reduce the flexibility of the student's\nrepresentations inducing inaccurate learning of the teacher's knowledge. To\nresolve it in transferring, we investigate distillation of structures of\nrepresentations specified to three types: intra-feature, local inter-feature,\nglobal inter-feature structures. To transfer them, we introduce feature\nstructure distillation methods based on the Centered Kernel Alignment, which\nassigns a consistent value to similar features structures and reveals more\ninformative relations. In particular, a memory-augmented transfer method with\nclustering is implemented for the global structures. The methods are\nempirically analyzed on the nine tasks for language understanding of the GLUE\ndataset with Bidirectional Encoder Representations from Transformers (BERT),\nwhich is a representative neural language model. In the results, the proposed\nmethods effectively transfer the three types of structures and improve\nperformance compared to state-of-the-art distillation methods. Indeed, the code\nfor the methods is available in https://github.com/maroo-sky/FSD.", "published": "2022-04-01 10:10:27", "link": "http://arxiv.org/abs/2204.08922v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spatial Loss for Unsupervised Multi-channel Source Separation", "abstract": "We propose a spatial loss for unsupervised multi-channel source separation.\nThe proposed loss exploits the duality of direction of arrival (DOA) and\nbeamforming: the steering and beamforming vectors should be aligned for the\ntarget source, but orthogonal for interfering ones. The spatial loss encourages\nconsistency between the mixing and demixing systems from a classic DOA\nestimator and a neural separator, respectively. With the proposed loss, we\ntrain the neural separators based on minimum variance distortionless response\n(MVDR) beamforming and independent vector analysis (IVA). We also investigate\nthe effectiveness of combining our spatial loss and a signal loss, which uses\nthe outputs of blind source separation as the reference. We evaluate our\nproposed method on synthetic and recorded (LibriCSS) mixtures. We find that the\nspatial loss is most effective to train IVA-based separators. For the neural\nMVDR beamformer, it performs best when combined with a signal loss. On\nsynthetic mixtures, the proposed unsupervised loss leads to the same\nperformance as a supervised loss in terms of word error rate. On LibriCSS, we\nobtain close to state-of-the-art performance without any labeled training data.", "published": "2022-04-01 05:13:17", "link": "http://arxiv.org/abs/2204.00210v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multiple Confidence Gates For Joint Training Of SE And ASR", "abstract": "Joint training of speech enhancement model (SE) and speech recognition model\n(ASR) is a common solution for robust ASR in noisy environments. SE focuses on\nimproving the auditory quality of speech, but the enhanced feature distribution\nis changed, which is uncertain and detrimental to the ASR. To tackle this\nchallenge, an approach with multiple confidence gates for jointly training of\nSE and ASR is proposed. A speech confidence gates prediction module is designed\nto replace the former SE module in joint training. The noisy speech is filtered\nby gates to obtain features that are easier to be fitting by the ASR network.\nThe experimental results show that the proposed method has better performance\nthan the traditional robust speech recognition system on test sets of clean\nspeech, synthesized noisy speech, and real noisy speech.", "published": "2022-04-01 06:19:24", "link": "http://arxiv.org/abs/2204.00226v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Universal Adaptor: Converting Mel-Spectrograms Between Different\n  Configurations for Speech Synthesis", "abstract": "Most recent speech synthesis systems are composed of a synthesizer and a\nvocoder. However, the existing synthesizers and vocoders can only be matched to\nacoustic features extracted with a specific configuration. Hence, we can't\ncombine arbitrary synthesizers and vocoders together to form a complete system,\nnot to mention apply to a newly developed model. In this paper, we proposed\nUniversal Adaptor, which takes a Mel-spectrogram parametrized by the source\nconfiguration and converts it into a Mel-spectrogram parametrized by the target\nconfiguration, as long as we feed in the source and the target configurations.\nExperiments show that the quality of speeches synthesized from our output of\nUniversal Adaptor is comparable to those synthesized from ground truth\nMel-spectrogram no matter in single-speaker or multi-speaker scenarios.\nMoreover, Universal Adaptor can be applied in the recent TTS systems and voice\nconversion systems without dropping quality.", "published": "2022-04-01 02:43:13", "link": "http://arxiv.org/abs/2204.00170v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker verification in mismatch training and testing conditions", "abstract": "This paper presents an exhaustive study about the robustness of several\nparameterizations, with a new database specially acquired for the purpose of a\nspeaker recognition application. This database includes the following\nvariations: different recording sessions (including telephonic and microphonic\nrecordings), recording rooms, and languages (it has been obtained from a\nbilingual set of speakers). This study has been performed with covariance\nmatrices in a text independent speaker verification application. It reveals\nthat the combination of several parameterizations can improve the robustness in\nall the scenarios.", "published": "2022-04-01 09:45:59", "link": "http://arxiv.org/abs/2204.00311v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Using segment-based features of jaw movements to recognize foraging\n  activities in grazing cattle", "abstract": "Precision livestock farming optimizes livestock production through the use of\nsensor information and communication technologies to support decision making,\nproactively and near real-time. Among available technologies to monitor\nforaging behavior, the acoustic method has been highly reliable and repeatable,\nbut can be subject to further computational improvements to increase precision\nand specificity of recognition of foraging activities. In this study, an\nalgorithm called Jaw Movement segment-based Foraging Activity Recognizer\n(JMFAR) is proposed. The method is based on the computation and analysis of\ntemporal, statistical and spectral features of jaw movement sounds for\ndetection of rumination and grazing bouts. They are called JM-segment features\nbecause they are extracted from a sound segment and expect to capture JM\ninformation of the whole segment rather than individual JMs. Two variants of\nthe method are proposed and tested: (i) the temporal and statistical features\nonly JMFAR-ns; and (ii) a feature selection process (JMFAR-sel). The JMFAR was\ntested on signals registered in a free grazing environment, achieving an\naverage weighted F1-score of 93%. Then, it was compared with a state-of-the-art\nalgorithm, showing improved performance for estimation of grazing bouts (+19%).\nThe JMFAR-ns variant reduced the computational cost by 25.4%, but achieved a\nslightly lower performance than the JMFAR. The good performance and low\ncomputational cost of JMFAR-ns supports the feasibility of using this algorithm\nvariant for real-time implementation in low-cost embedded systems. The method\npresented within this publication is protected by a pending patent application:\nAR P20220100910.", "published": "2022-04-01 10:15:00", "link": "http://arxiv.org/abs/2204.00331v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Efficiency of Integrating Self-supervised Learning and\n  Meta-learning for User-defined Few-shot Keyword Spotting", "abstract": "User-defined keyword spotting is a task to detect new spoken terms defined by\nusers. This can be viewed as a few-shot learning problem since it is\nunreasonable for users to define their desired keywords by providing many\nexamples. To solve this problem, previous works try to incorporate\nself-supervised learning models or apply meta-learning algorithms. But it is\nunclear whether self-supervised learning and meta-learning are complementary\nand which combination of the two types of approaches is most effective for\nfew-shot keyword discovery. In this work, we systematically study these\nquestions by utilizing various self-supervised learning models and combining\nthem with a wide variety of meta-learning algorithms. Our result shows that\nHuBERT combined with Matching network achieves the best result and is robust to\nthe changes of few-shot examples.", "published": "2022-04-01 10:59:39", "link": "http://arxiv.org/abs/2204.00352v3", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Multimodal Clustering with Role Induced Constraints for Speaker\n  Diarization", "abstract": "Speaker clustering is an essential step in conventional speaker diarization\nsystems and is typically addressed as an audio-only speech processing task. The\nlanguage used by the participants in a conversation, however, carries\nadditional information that can help improve the clustering performance. This\nis especially true in conversational interactions, such as business meetings,\ninterviews, and lectures, where specific roles assumed by interlocutors\n(manager, client, teacher, etc.) are often associated with distinguishable\nlinguistic patterns. In this paper we propose to employ a supervised text-based\nmodel to extract speaker roles and then use this information to guide an\naudio-based spectral clustering step by imposing must-link and cannot-link\nconstraints between segments. The proposed method is applied on two different\ndomains, namely on medical interactions and on podcast episodes, and is shown\nto yield improved results when compared to the audio-only approach.", "published": "2022-04-01 18:53:13", "link": "http://arxiv.org/abs/2204.00657v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Neural Convolutive Matrix Factorization for Articulatory\n  Representation Decomposition", "abstract": "Most of the research on data-driven speech representation learning has\nfocused on raw audios in an end-to-end manner, paying little attention to their\ninternal phonological or gestural structure. This work, investigating the\nspeech representations derived from articulatory kinematics signals, uses a\nneural implementation of convolutive sparse matrix factorization to decompose\nthe articulatory data into interpretable gestures and gestural scores. By\napplying sparse constraints, the gestural scores leverage the discrete\ncombinatorial properties of phonological gestures. Phoneme recognition\nexperiments were additionally performed to show that gestural scores indeed\ncode phonological information successfully. The proposed work thus makes a\nbridge between articulatory phonology and deep neural networks to leverage\ninformative, intelligible, interpretable,and efficient speech representations.", "published": "2022-04-01 14:25:19", "link": "http://arxiv.org/abs/2204.00465v3", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Quantized GAN for Complex Music Generation from Dance Videos", "abstract": "We present Dance2Music-GAN (D2M-GAN), a novel adversarial multi-modal\nframework that generates complex musical samples conditioned on dance videos.\nOur proposed framework takes dance video frames and human body motions as\ninput, and learns to generate music samples that plausibly accompany the\ncorresponding input. Unlike most existing conditional music generation works\nthat generate specific types of mono-instrumental sounds using symbolic audio\nrepresentations (e.g., MIDI), and that usually rely on pre-defined musical\nsynthesizers, in this work we generate dance music in complex styles (e.g.,\npop, breaking, etc.) by employing a Vector Quantized (VQ) audio representation,\nand leverage both its generality and high abstraction capacity of its symbolic\nand continuous counterparts. By performing an extensive set of experiments on\nmultiple datasets, and following a comprehensive evaluation protocol, we assess\nthe generative qualities of our proposal against alternatives. The attained\nquantitative results, which measure the music consistency, beats\ncorrespondence, and music diversity, demonstrate the effectiveness of our\nproposed method. Last but not least, we curate a challenging dance-music\ndataset of in-the-wild TikTok videos, which we use to further demonstrate the\nefficacy of our approach in real-world applications -- and which we hope to\nserve as a starting point for relevant future research.", "published": "2022-04-01 17:53:39", "link": "http://arxiv.org/abs/2204.00604v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Learning Audio-Video Modalities from Image Captions", "abstract": "A major challenge in text-video and text-audio retrieval is the lack of\nlarge-scale training data. This is unlike image-captioning, where datasets are\nin the order of millions of samples. To close this gap we propose a new video\nmining pipeline which involves transferring captions from image captioning\ndatasets to video clips with no additional manual effort. Using this pipeline,\nwe create a new large-scale, weakly labelled audio-video captioning dataset\nconsisting of millions of paired clips and captions. We show that training a\nmultimodal transformed based model on this data achieves competitive\nperformance on video retrieval and video captioning, matching or even\noutperforming HowTo100M pretraining with 20x fewer clips. We also show that our\nmined clips are suitable for text-audio pretraining, and achieve state of the\nart results for the task of audio retrieval.", "published": "2022-04-01 19:48:18", "link": "http://arxiv.org/abs/2204.00679v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Residual-guided Personalized Speech Synthesis based on Face Image", "abstract": "Previous works derive personalized speech features by training the model on a\nlarge dataset composed of his/her audio sounds. It was reported that face\ninformation has a strong link with the speech sound. Thus in this work, we\ninnovatively extract personalized speech features from human faces to\nsynthesize personalized speech using neural vocoder. A Face-based Residual\nPersonalized Speech Synthesis Model (FR-PSS) containing a speech encoder, a\nspeech synthesizer and a face encoder is designed for PSS. In this model, by\ndesigning two speech priors, a residual-guided strategy is introduced to guide\nthe face feature to approach the true speech feature in the training. Moreover,\nconsidering the error of feature's absolute values and their directional bias,\nwe formulate a novel tri-item loss function for face encoder. Experimental\nresults show that the speech synthesized by our model is comparable to the\npersonalized speech synthesized by training a large amount of audio data in\nprevious works.", "published": "2022-04-01 15:27:14", "link": "http://arxiv.org/abs/2204.01672v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
