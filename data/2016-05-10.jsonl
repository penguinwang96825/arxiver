{"title": "Coverage Embedding Models for Neural Machine Translation", "abstract": "In this paper, we enhance the attention-based neural machine translation\n(NMT) by adding explicit coverage embedding models to alleviate issues of\nrepeating and dropping translations in NMT. For each source word, our model\nstarts with a full coverage embedding vector to track the coverage status, and\nthen keeps updating it with neural networks as the translation goes.\nExperiments on the large-scale Chinese-to-English task show that our enhanced\nmodel improves the translation quality significantly on various test sets over\nthe strong large vocabulary NMT system.", "published": "2016-05-10 18:44:34", "link": "http://arxiv.org/abs/1605.03148v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vocabulary Manipulation for Neural Machine Translation", "abstract": "In order to capture rich language phenomena, neural machine translation\nmodels have to use a large vocabulary size, which requires high computing time\nand large memory usage. In this paper, we alleviate this issue by introducing a\nsentence-level or batch-level vocabulary, which is only a very small sub-set of\nthe full output vocabulary. For each sentence or batch, we only predict the\ntarget words in its sentence-level or batch-level vocabulary. Thus, we reduce\nboth the computing time and the memory usage. Our method simply takes into\naccount the translation options of each word or phrase in the source sentence,\nand picks a very small target vocabulary for each sentence based on a\nword-to-word translation model or a bilingual phrase library learned from a\ntraditional machine translation model. Experimental results on the large-scale\nEnglish-to-French task show that our method achieves better translation\nperformance by 1 BLEU point over the large vocabulary neural machine\ntranslation system of Jean et al. (2015).", "published": "2016-05-10 20:50:56", "link": "http://arxiv.org/abs/1605.03209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical Case Based IS-A Relation Extraction with Boosting for Polish", "abstract": "Pattern-based methods of IS-A relation extraction rely heavily on so called\nHearst patterns. These are ways of expressing instance enumerations of a class\nin natural language. While these lexico-syntactic patterns prove quite useful,\nthey may not capture all taxonomical relations expressed in text. Therefore in\nthis paper we describe a novel method of IS-A relation extraction from\npatterns, which uses morpho-syntactical annotations along with grammatical case\nof noun phrases that constitute entities participating in IS-A relation. We\nalso describe a method for increasing the number of extracted relations that we\ncall pseudo-subclass boosting which has potential application in any\npattern-based relation extraction method. Experiments were conducted on a\ncorpus of about 0.5 billion web documents in Polish language.", "published": "2016-05-10 10:03:48", "link": "http://arxiv.org/abs/1605.02916v1", "categories": ["cs.CL", "cs.IR", "H.3.1"], "primary_category": "cs.CL"}
{"title": "The Yahoo Query Treebank, V. 1.0", "abstract": "A description and annotation guidelines for the Yahoo Webscope release of\nQuery Treebank, Version 1.0, May 2016.", "published": "2016-05-10 11:29:28", "link": "http://arxiv.org/abs/1605.02945v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Different approaches for identifying important concepts in probabilistic\n  biomedical text summarization", "abstract": "Automatic text summarization tools help users in biomedical domain to acquire\ntheir intended information from various textual resources more efficiently.\nSome of the biomedical text summarization systems put the basis of their\nsentence selection approach on the frequency of concepts extracted from the\ninput text. However, it seems that exploring other measures rather than the\nfrequency for identifying the valuable content of the input document, and\nconsidering the correlations existing between concepts may be more useful for\nthis type of summarization. In this paper, we describe a Bayesian summarizer\nfor biomedical text documents. The Bayesian summarizer initially maps the input\ntext to the Unified Medical Language System (UMLS) concepts, then it selects\nthe important ones to be used as classification features. We introduce\ndifferent feature selection approaches to identify the most important concepts\nof the text and to select the most informative content according to the\ndistribution of these concepts. We show that with the use of an appropriate\nfeature selection approach, the Bayesian biomedical summarizer can improve the\nperformance of summarization. We perform extensive evaluations on a corpus of\nscientific papers in biomedical domain. The results show that the Bayesian\nsummarizer outperforms the biomedical summarizers that rely on the frequency of\nconcepts, the domain-independent and baseline methods based on the\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. Moreover,\nthe results suggest that using the meaningfulness measure and considering the\ncorrelations of concepts in the feature selection step lead to a significant\nincrease in the performance of summarization.", "published": "2016-05-10 11:33:33", "link": "http://arxiv.org/abs/1605.02948v3", "categories": ["cs.CL", "cs.IR", "I.2.7; J.3"], "primary_category": "cs.CL"}
