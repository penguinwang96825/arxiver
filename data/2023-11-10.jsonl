{"title": "CFBenchmark: Chinese Financial Assistant Benchmark for Large Language\n  Model", "abstract": "Large language models (LLMs) have demonstrated great potential in the\nfinancial domain. Thus, it becomes important to assess the performance of LLMs\nin the financial tasks. In this work, we introduce CFBenchmark, to evaluate the\nperformance of LLMs for Chinese financial assistant. The basic version of\nCFBenchmark is designed to evaluate the basic ability in Chinese financial text\nprocessing from three aspects~(\\emph{i.e.} recognition, classification, and\ngeneration) including eight tasks, and includes financial texts ranging in\nlength from 50 to over 1,800 characters. We conduct experiments on several LLMs\navailable in the literature with CFBenchmark-Basic, and the experimental\nresults indicate that while some LLMs show outstanding performance in specific\ntasks, overall, there is still significant room for improvement in basic tasks\nof financial text processing with existing models. In the future, we plan to\nexplore the advanced version of CFBenchmark, aiming to further explore the\nextensive capabilities of language models in more profound dimensions as a\nfinancial assistant in Chinese. Our codes are released at\nhttps://github.com/TongjiFinLab/CFBenchmark.", "published": "2023-11-10 01:12:03", "link": "http://arxiv.org/abs/2311.05812v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let's Reinforce Step by Step", "abstract": "While recent advances have boosted LM proficiency in linguistic benchmarks,\nLMs consistently struggle to reason correctly on complex tasks like\nmathematics. We turn to Reinforcement Learning from Human Feedback (RLHF) as a\nmethod with which to shape model reasoning processes. In particular, we explore\ntwo reward schemes, outcome-supervised reward models (ORMs) and\nprocess-supervised reward models (PRMs), to optimize for logical reasoning. Our\nresults show that the fine-grained reward provided by PRM-based methods\nenhances accuracy on simple mathematical reasoning (GSM8K) while, unexpectedly,\nreducing performance in complex tasks (MATH). Furthermore, we show the critical\nrole reward aggregation functions play in model performance. Providing\npromising avenues for future research, our study underscores the need for\nfurther exploration into fine-grained reward modeling for more reliable\nlanguage models.", "published": "2023-11-10 01:35:51", "link": "http://arxiv.org/abs/2311.05821v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trends in Integration of Knowledge and Large Language Models: A Survey\n  and Taxonomy of Methods, Benchmarks, and Applications", "abstract": "Large language models (LLMs) exhibit superior performance on various natural\nlanguage tasks, but they are susceptible to issues stemming from outdated data\nand domain-specific limitations. In order to address these challenges,\nresearchers have pursued two primary strategies, knowledge editing and\nretrieval augmentation, to enhance LLMs by incorporating external information\nfrom different aspects. Nevertheless, there is still a notable absence of a\ncomprehensive survey. In this paper, we propose a review to discuss the trends\nin integration of knowledge and large language models, including taxonomy of\nmethods, benchmarks, and applications. In addition, we conduct an in-depth\nanalysis of different methods and point out potential research directions in\nthe future. We hope this survey offers the community quick access and a\ncomprehensive overview of this research area, with the intention of inspiring\nfuture research endeavors.", "published": "2023-11-10 05:24:04", "link": "http://arxiv.org/abs/2311.05876v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation\n  Extraction", "abstract": "Few-shot relation extraction involves identifying the type of relationship\nbetween two specific entities within a text, using a limited number of\nannotated samples. A variety of solutions to this problem have emerged by\napplying meta-learning and neural graph techniques which typically necessitate\na training process for adaptation. Recently, the strategy of in-context\nlearning has been demonstrating notable results without the need of training.\nFew studies have already utilized in-context learning for zero-shot information\nextraction. Unfortunately, the evidence for inference is either not considered\nor implicitly modeled during the construction of chain-of-thought prompts. In\nthis paper, we propose a novel approach for few-shot relation extraction using\nlarge language models, named CoT-ER, chain-of-thought with explicit evidence\nreasoning. In particular, CoT-ER first induces large language models to\ngenerate evidences using task-specific and concept-level knowledge. Then these\nevidences are explicitly incorporated into chain-of-thought prompting for\nrelation extraction. Experimental results demonstrate that our CoT-ER approach\n(with 0% training data) achieves competitive performance compared to the\nfully-supervised (with 100% training data) state-of-the-art approach on the\nFewRel1.0 and FewRel2.0 datasets.", "published": "2023-11-10 08:12:00", "link": "http://arxiv.org/abs/2311.05922v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Zero Shot Hypothesis Proposers", "abstract": "Significant scientific discoveries have driven the progress of human\ncivilisation. The explosion of scientific literature and data has created\ninformation barriers across disciplines that have slowed the pace of scientific\ndiscovery. Large Language Models (LLMs) hold a wealth of global and\ninterdisciplinary knowledge that promises to break down these information\nbarriers and foster a new wave of scientific discovery. However, the potential\nof LLMs for scientific discovery has not been formally explored. In this paper,\nwe start from investigating whether LLMs can propose scientific hypotheses. To\nthis end, we construct a dataset consist of background knowledge and hypothesis\npairs from biomedical literature. The dataset is divided into training, seen,\nand unseen test sets based on the publication date to control visibility. We\nsubsequently evaluate the hypothesis generation capabilities of various\ntop-tier instructed models in zero-shot, few-shot, and fine-tuning settings,\nincluding both closed and open-source LLMs. Additionally, we introduce an\nLLM-based multi-agent cooperative framework with different role designs and\nexternal tools to enhance the capabilities related to generating hypotheses. We\nalso design four metrics through a comprehensive review to evaluate the\ngenerated hypotheses for both ChatGPT-based and human evaluations. Through\nexperiments and analyses, we arrive at the following findings: 1) LLMs\nsurprisingly generate untrained yet validated hypotheses from testing\nliterature. 2) Increasing uncertainty facilitates candidate generation,\npotentially enhancing zero-shot hypothesis generation capabilities. These\nfindings strongly support the potential of LLMs as catalysts for new scientific\ndiscoveries and guide further exploration.", "published": "2023-11-10 10:03:49", "link": "http://arxiv.org/abs/2311.05965v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training\n  Regime and Better Alignment to Human Preferences", "abstract": "Recently, the increasing demand for superior medical services has highlighted\nthe discrepancies in the medical infrastructure. With big data, especially\ntexts, forming the foundation of medical services, there is an exigent need for\neffective natural language processing (NLP) solutions tailored to the\nhealthcare domain. Conventional approaches leveraging pre-trained models\npresent promising results in this domain and current large language models\n(LLMs) offer advanced foundation for medical text processing. However, most\nmedical LLMs are trained only with supervised fine-tuning (SFT), even though it\nefficiently empowers LLMs to understand and respond to medical instructions but\nis ineffective in learning domain knowledge and aligning with human preference.\nIn this work, we propose ChiMed-GPT, a new benchmark LLM designed explicitly\nfor Chinese medical domain, and undergoes a comprehensive training regime with\npre-training, SFT, and RLHF. Evaluations on tasks including information\nextraction, question answering, and dialogue generation demonstrate\nChiMed-GPT's superior performance over general domain LLMs. Furthermore, we\nanalyze possible biases through prompting ChiMed-GPT to perform attitude scales\nregarding discrimination of patients, so as to contribute to further\nresponsible development of LLMs in the medical domain. The code and model are\nreleased at https://github.com/synlp/ChiMed-GPT.", "published": "2023-11-10 12:25:32", "link": "http://arxiv.org/abs/2311.06025v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual and Multi-topical Benchmark of Fine-tuned Language models\n  and Large Language Models for Check-Worthy Claim Detection", "abstract": "This study compares the performance of (1) fine-tuned language models and (2)\nlarge language models on the task of check-worthy claim detection. For the\npurpose of the comparison we composed a multilingual and multi-topical dataset\ncomprising texts of various sources and styles. Building on this, we performed\na benchmark analysis to determine the most general multilingual and\nmulti-topical claim detector.\n  We chose three state-of-the-art models in the check-worthy claim detection\ntask and fine-tuned them. Furthermore, we selected four state-of-the-art large\nlanguage models without any fine-tuning. We made modifications to the models to\nadapt them for multilingual settings and through extensive experimentation and\nevaluation, we assessed the performance of all the models in terms of accuracy,\nrecall, and F1-score in in-domain and cross-domain scenarios. Our results\ndemonstrate that despite the technological progress in the area of natural\nlanguage processing, the models fine-tuned for the task of check-worthy claim\ndetection still outperform the zero-shot approaches in cross-domain settings.", "published": "2023-11-10 15:36:35", "link": "http://arxiv.org/abs/2311.06121v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparison of Lexicon-Based and ML-Based Sentiment Analysis: Are There\n  Outlier Words?", "abstract": "Lexicon-based approaches to sentiment analysis of text are based on each word\nor lexical entry having a pre-defined weight indicating its sentiment polarity.\nThese are usually manually assigned but the accuracy of these when compared\nagainst machine leaning based approaches to computing sentiment, are not known.\nIt may be that there are lexical entries whose sentiment values cause a\nlexicon-based approach to give results which are very different to a machine\nlearning approach. In this paper we compute sentiment for more than 150,000\nEnglish language texts drawn from 4 domains using the Hedonometer, a\nlexicon-based technique and Azure, a contemporary machine-learning based\napproach which is part of the Azure Cognitive Services family of APIs which is\neasy to use. We model differences in sentiment scores between approaches for\ndocuments in each domain using a regression and analyse the independent\nvariables (Hedonometer lexical entries) as indicators of each word's importance\nand contribution to the score differences. Our findings are that the importance\nof a word depends on the domain and there are no standout lexical entries which\nsystematically cause differences in sentiment scores.", "published": "2023-11-10 18:21:50", "link": "http://arxiv.org/abs/2311.06221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking", "abstract": "Tracking dialogue states is an essential topic in task-oriented dialogue\nsystems, which involve filling in the necessary information in pre-defined\nslots corresponding to a schema. While general pre-trained language models have\nbeen shown effective in slot-filling, their performance is limited when applied\nto specific domains. We propose a graph-based framework that learns\ndomain-specific prompts by incorporating the dialogue schema. Specifically, we\nembed domain-specific schema encoded by a graph neural network into the\npre-trained language model, which allows for relations in the schema to guide\nthe model for better adaptation to the specific domain. Our experiments\ndemonstrate that the proposed graph-based method outperforms other multi-domain\nDST approaches while using similar or fewer trainable parameters. We also\nconduct a comprehensive study of schema graph architectures, parameter usage,\nand module ablation that demonstrate the effectiveness of our model on\nmulti-domain dialogue state tracking.", "published": "2023-11-10 19:00:02", "link": "http://arxiv.org/abs/2311.06345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Definitions from Large Language Models", "abstract": "Dictionary definitions are historically the arbitrator of what words mean,\nbut this primacy has come under threat by recent progress in NLP, including\nword embeddings and generative models like ChatGPT. We present an exploratory\nstudy of the degree of alignment between word definitions from classical\ndictionaries and these newer computational artifacts. Specifically, we compare\ndefinitions from three published dictionaries to those generated from variants\nof ChatGPT. We show that (i) definitions from different traditional\ndictionaries exhibit more surface form similarity than do model-generated\ndefinitions, (ii) that the ChatGPT definitions are highly accurate, comparable\nto traditional dictionaries, and (iii) ChatGPT-based embedding definitions\nretain their accuracy even on low frequency words, much better than GloVE and\nFastText word embeddings.", "published": "2023-11-10 19:27:20", "link": "http://arxiv.org/abs/2311.06362v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relation Extraction in underexplored biomedical domains: A\n  diversity-optimised sampling and synthetic data generation approach", "abstract": "The sparsity of labelled data is an obstacle to the development of Relation\nExtraction models and the completion of databases in various biomedical areas.\nWhile being of high interest in drug-discovery, the natural-products\nliterature, reporting the identification of potential bioactive compounds from\norganisms, is a concrete example of such an overlooked topic. To mark the start\nof this new task, we created the first curated evaluation dataset and extracted\nliterature items from the LOTUS database to build training sets. To this end,\nwe developed a new sampler inspired by diversity metrics in ecology, named\nGreedy Maximum Entropy sampler, or GME-sampler\n(https://github.com/idiap/gme-sampler). The strategic optimization of both\nbalance and diversity of the selected items in the evaluation set is important\ngiven the resource-intensive nature of manual curation. After quantifying the\nnoise in the training set, in the form of discrepancies between the input\nabstracts text and the expected output labels, we explored different strategies\naccordingly. Framing the task as an end-to-end Relation Extraction, we\nevaluated the performance of standard fine-tuning as a generative task and\nfew-shot learning with open Large Language Models (LLaMA 7B-65B). In addition\nto their evaluation in few-shot settings, we explore the potential of open\nLarge Language Models (Vicuna-13B) as synthetic data generator and propose a\nnew workflow for this purpose. All evaluated models exhibited substantial\nimprovements when fine-tuned on synthetic abstracts rather than the original\nnoisy data. We provide our best performing (f1-score=59.0) BioGPT-Large model\nfor end-to-end RE of natural-products relationships along with all the\ngenerated synthetic data and the evaluation dataset. See more details at\nhttps://github.com/idiap/abroad-re.", "published": "2023-11-10 19:36:00", "link": "http://arxiv.org/abs/2311.06364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Heaps' Law in GPT-Neo Large Language Model Emulated Corpora", "abstract": "Heaps' law is an empirical relation in text analysis that predicts vocabulary\ngrowth as a function of corpus size. While this law has been validated in\ndiverse human-authored text corpora, its applicability to large language model\ngenerated text remains unexplored. This study addresses this gap, focusing on\nthe emulation of corpora using the suite of GPT-Neo large language models. To\nconduct our investigation, we emulated corpora of PubMed abstracts using three\ndifferent parameter sizes of the GPT-Neo model. Our emulation strategy involved\nusing the initial five words of each PubMed abstract as a prompt and\ninstructing the model to expand the content up to the original abstract's\nlength. Our findings indicate that the generated corpora adhere to Heaps' law.\nInterestingly, as the GPT-Neo model size grows, its generated vocabulary\nincreasingly adheres to Heaps' law as as observed in human-authored text. To\nfurther improve the richness and authenticity of GPT-Neo outputs, future\niterations could emphasize enhancing model size or refining the model\narchitecture to curtail vocabulary repetition.", "published": "2023-11-10 20:07:32", "link": "http://arxiv.org/abs/2311.06377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeMuX: Data-efficient Multilingual Learning", "abstract": "We consider the task of optimally fine-tuning pre-trained multilingual\nmodels, given small amounts of unlabelled target data and an annotation budget.\nIn this paper, we introduce DEMUX, a framework that prescribes the exact\ndata-points to label from vast amounts of unlabelled multilingual data, having\nunknown degrees of overlap with the target set. Unlike most prior works, our\nend-to-end framework is language-agnostic, accounts for model representations,\nand supports multilingual target configurations. Our active learning strategies\nrely upon distance and uncertainty measures to select task-specific neighbors\nthat are most informative to label, given a model. DeMuX outperforms strong\nbaselines in 84% of the test cases, in the zero-shot setting of disjoint source\nand target language sets (including multilingual target pools), across three\nmodels and four tasks. Notably, in low-budget settings (5-100 examples), we\nobserve gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for\ncomplex tasks. Our code is released here:\nhttps://github.com/simran-khanuja/demux.", "published": "2023-11-10 20:09:08", "link": "http://arxiv.org/abs/2311.06379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Citation Recommendation on Scholarly Legal Articles", "abstract": "Citation recommendation is the task of finding appropriate citations based on\na given piece of text. The proposed datasets for this task consist mainly of\nseveral scientific fields, lacking some core ones, such as law. Furthermore,\ncitation recommendation is used within the legal domain to identify supporting\narguments, utilizing non-scholarly legal articles. In order to alleviate the\nlimitations of existing studies, we gather the first scholarly legal dataset\nfor the task of citation recommendation. Also, we conduct experiments with\nstate-of-the-art models and compare their performance on this dataset. The\nstudy suggests that, while BM25 is a strong benchmark for the legal citation\nrecommendation task, the most effective method involves implementing a two-step\nprocess that entails pre-fetching with BM25+, followed by re-ranking with\nSciNCL, which enhances the performance of the baseline from 0.26 to 0.30\nMAP@10. Moreover, fine-tuning leads to considerable performance increases in\npre-trained models, which shows the importance of including legal articles in\nthe training data of these models.", "published": "2023-11-10 07:11:55", "link": "http://arxiv.org/abs/2311.05902v1", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "primary_category": "cs.IR"}
{"title": "Fake Alignment: Are LLMs Really Aligned Well?", "abstract": "The growing awareness of safety concerns in large language models (LLMs) has\nsparked considerable interest in the evaluation of safety. This study\ninvestigates an under-explored issue about the evaluation of LLMs, namely the\nsubstantial discrepancy in performance between multiple-choice questions and\nopen-ended questions. Inspired by research on jailbreak attack patterns, we\nargue this is caused by mismatched generalization. That is, LLM only remembers\nthe answer style for open-ended safety questions, which makes it unable to\nsolve other forms of safety tests. We refer to this phenomenon as fake\nalignment and construct a comparative benchmark to empirically verify its\nexistence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework\nand two novel metrics--Consistency Score (CS) and Consistent Safety Score\n(CSS), which jointly assess two complementary forms of evaluation to quantify\nfake alignment and obtain corrected performance estimation. Applying FINE to 14\nwidely-used LLMs reveals several models with purported safety are poorly\naligned in practice. Subsequently, we found that multiple-choice format data\ncan also be used as high-quality contrast distillation-based fine-tuning data,\nwhich can strongly improve the alignment consistency of LLMs with minimal\nfine-tuning overhead. For data and code, see\nhttps://github.com/AIFlames/Fake-Alignment.", "published": "2023-11-10 08:01:23", "link": "http://arxiv.org/abs/2311.05915v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Making LLMs Worth Every Penny: Resource-Limited Text Classification in\n  Banking", "abstract": "Standard Full-Data classifiers in NLP demand thousands of labeled examples,\nwhich is impractical in data-limited domains. Few-shot methods offer an\nalternative, utilizing contrastive learning techniques that can be effective\nwith as little as 20 examples per class. Similarly, Large Language Models\n(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.\nHowever, the performance-cost trade-offs of these methods remain underexplored,\na critical concern for budget-limited organizations. Our work addresses this\ngap by studying the aforementioned approaches over the Banking77 financial\nintent detection dataset, including the evaluation of cutting-edge LLMs by\nOpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We\ncomplete the picture with two additional methods: first, a cost-effective\nquerying method for LLMs based on retrieval-augmented generation (RAG), able to\nreduce operational costs multiple times compared to classic few-shot\napproaches, and second, a data augmentation method using GPT-4, able to improve\nperformance in data-limited scenarios. Finally, to inspire future research, we\nprovide a human expert's curated subset of Banking77, along with extensive\nerror analysis.", "published": "2023-11-10 15:10:36", "link": "http://arxiv.org/abs/2311.06102v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models can be Logical Solvers", "abstract": "Logical reasoning is a fundamental aspect of human intelligence and a key\ncomponent of tasks like problem-solving and decision-making. Recent\nadvancements have enabled Large Language Models (LLMs) to potentially exhibit\nreasoning capabilities, but complex logical reasoning remains a challenge. The\nstate-of-the-art, solver-augmented language models, use LLMs to parse natural\nlanguage logical questions into symbolic representations first and then adopt\nexternal logical solvers to take in the symbolic representations and output the\nanswers. Despite their impressive performance, any parsing errors will\ninevitably result in the failure of the execution of the external logical\nsolver and no answer to the logical questions. In this paper, we introduce\nLoGiPT, a novel language model that directly emulates the reasoning processes\nof logical solvers and bypasses the parsing errors by learning to strict\nadherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly\nconstructed instruction-tuning dataset derived from revealing and refining the\ninvisible reasoning process of deductive solvers. Experimental results on two\npublic deductive reasoning datasets demonstrate that LoGiPT outperforms\nstate-of-the-art solver-augmented LMs and few-shot prompting methods on\ncompetitive LLMs like ChatGPT or GPT-4.", "published": "2023-11-10 16:23:50", "link": "http://arxiv.org/abs/2311.06158v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset", "abstract": "Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.", "published": "2023-11-10 17:38:46", "link": "http://arxiv.org/abs/2311.06204v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Argumentation Element Annotation Modeling using XLNet", "abstract": "This study demonstrates the effectiveness of XLNet, a transformer-based\nlanguage model, for annotating argumentative elements in persuasive essays.\nXLNet's architecture incorporates a recurrent mechanism that allows it to model\nlong-term dependencies in lengthy texts. Fine-tuned XLNet models were applied\nto three datasets annotated with different schemes - a proprietary dataset\nusing the Annotations for Revisions and Reflections on Writing (ARROW) scheme,\nthe PERSUADE corpus, and the Argument Annotated Essays (AAE) dataset. The XLNet\nmodels achieved strong performance across all datasets, even surpassing human\nagreement levels in some cases. This shows XLNet capably handles diverse\nannotation schemes and lengthy essays. Comparisons between the model outputs on\ndifferent datasets also revealed insights into the relationships between the\nannotation tags. Overall, XLNet's strong performance on modeling argumentative\nstructures across diverse datasets highlights its suitability for providing\nautomated feedback on essay organization.", "published": "2023-11-10 18:55:23", "link": "http://arxiv.org/abs/2311.06239v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Structured Pruning under Limited Task Data", "abstract": "Large, pre-trained models are problematic to use in resource constrained\napplications. Fortunately, task-aware structured pruning methods offer a\nsolution. These approaches reduce model size by dropping structural units like\nlayers and attention heads in a manner that takes into account the end-task.\nHowever, these pruning algorithms require more task-specific data than is\ntypically available. We propose a framework which combines structured pruning\nwith transfer learning to reduce the need for task-specific data. Our empirical\nresults answer questions such as: How should the two tasks be coupled? What\nparameters should be transferred? And, when during training should transfer\nlearning be introduced? Leveraging these insights, we demonstrate that our\nframework results in pruned models with improved generalization over strong\nbaselines.", "published": "2023-11-10 20:23:35", "link": "http://arxiv.org/abs/2311.06382v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilling Large Language Models using Skill-Occupation Graph Context\n  for HR-Related Tasks", "abstract": "Numerous HR applications are centered around resumes and job descriptions.\nWhile they can benefit from advancements in NLP, particularly large language\nmodels, their real-world adoption faces challenges due to absence of\ncomprehensive benchmarks for various HR tasks, and lack of smaller models with\ncompetitive capabilities. In this paper, we aim to bridge this gap by\nintroducing the Resume-Job Description Benchmark (RJDB). We meticulously craft\nthis benchmark to cater to a wide array of HR tasks, including matching and\nexplaining resumes to job descriptions, extracting skills and experiences from\nresumes, and editing resumes. To create this benchmark, we propose to distill\ndomain-specific knowledge from a large language model (LLM). We rely on a\ncurated skill-occupation graph to ensure diversity and provide context for LLMs\ngeneration. Our benchmark includes over 50 thousand triples of job\ndescriptions, matched resumes and unmatched resumes. Using RJDB, we train\nmultiple smaller student models. Our experiments reveal that the student models\nachieve near/better performance than the teacher model (GPT-4), affirming the\neffectiveness of the benchmark. Additionally, we explore the utility of RJDB on\nout-of-distribution data for skill extraction and resume-job description\nmatching, in zero-shot and weak supervision manner. We release our datasets and\ncode to foster further research and industry applications.", "published": "2023-11-10 20:25:42", "link": "http://arxiv.org/abs/2311.06383v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing Modular Approaches for Visual Question Decomposition", "abstract": "Modular neural networks without additional training have recently been shown\nto surpass end-to-end neural networks on challenging vision-language tasks. The\nlatest such methods simultaneously introduce LLM-based code generation to build\nprograms and a number of skill-specific, task-oriented modules to execute them.\nIn this paper, we focus on ViperGPT and ask where its additional performance\ncomes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model\nit subsumes vs. additional symbolic components. To do so, we conduct a\ncontrolled study (comparing end-to-end, modular, and prompting-based methods\nacross several VQA benchmarks). We find that ViperGPT's reported gains over\nBLIP-2 can be attributed to its selection of task-specific modules, and when we\nrun ViperGPT using a more task-agnostic selection of modules, these gains go\naway. Additionally, ViperGPT retains much of its performance if we make\nprominent alterations to its selection of modules: e.g. removing or retaining\nonly BLIP-2. Finally, we compare ViperGPT against a prompting-based\ndecomposition strategy and find that, on some benchmarks, modular approaches\nsignificantly benefit by representing subtasks with natural language, instead\nof code.", "published": "2023-11-10 22:14:26", "link": "http://arxiv.org/abs/2311.06411v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Knowledge Graphs are not Created Equal: Exploring the Properties and\n  Structure of Real KGs", "abstract": "Despite the recent popularity of knowledge graph (KG) related tasks and\nbenchmarks such as KG embeddings, link prediction, entity alignment and\nevaluation of the reasoning abilities of pretrained language models as KGs, the\nstructure and properties of real KGs are not well studied. In this paper, we\nperform a large scale comparative study of 29 real KG datasets from diverse\ndomains such as the natural sciences, medicine, and NLP to analyze their\nproperties and structural patterns. Based on our findings, we make several\nrecommendations regarding KG-based model development and evaluation. We believe\nthat the rich structural information contained in KGs can benefit the\ndevelopment of better KG models across fields and we hope this study will\ncontribute to breaking the existing data silos between different areas of\nresearch (e.g., ML, NLP, AI for sciences).", "published": "2023-11-10 22:18:09", "link": "http://arxiv.org/abs/2311.06414v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ChatGPT Prompting Cannot Estimate Predictive Uncertainty in\n  High-Resource Languages", "abstract": "ChatGPT took the world by storm for its impressive abilities. Due to its\nrelease without documentation, scientists immediately attempted to identify its\nlimits, mainly through its performance in natural language processing (NLP)\ntasks. This paper aims to join the growing literature regarding ChatGPT's\nabilities by focusing on its performance in high-resource languages and on its\ncapacity to predict its answers' accuracy by giving a confidence level. The\nanalysis of high-resource languages is of interest as studies have shown that\nlow-resource languages perform worse than English in NLP tasks, but no study so\nfar has analysed whether high-resource languages perform as well as English.\nThe analysis of ChatGPT's confidence calibration has not been carried out\nbefore either and is critical to learn about ChatGPT's trustworthiness. In\norder to study these two aspects, five high-resource languages and two NLP\ntasks were chosen. ChatGPT was asked to perform both tasks in the five\nlanguages and to give a numerical confidence value for each answer. The results\nshow that all the selected high-resource languages perform similarly and that\nChatGPT does not have a good confidence calibration, often being overconfident\nand never giving low confidence values.", "published": "2023-11-10 23:25:34", "link": "http://arxiv.org/abs/2311.06427v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ontology Learning Using Formal Concept Analysis and WordNet", "abstract": "Manual ontology construction takes time, resources, and domain specialists.\nSupporting a component of this process for automation or semi-automation would\nbe good. This project and dissertation provide a Formal Concept Analysis and\nWordNet framework for learning concept hierarchies from free texts. The process\nhas steps. First, the document is Part-Of-Speech labeled, then parsed to\nproduce sentence parse trees. Verb/noun dependencies are derived from parse\ntrees next. After lemmatizing, pruning, and filtering the word pairings, the\nformal context is created. The formal context may contain some erroneous and\nuninteresting pairs because the parser output may be erroneous, not all derived\npairs are interesting, and it may be large due to constructing it from a large\nfree text corpus. Deriving lattice from the formal context may take longer,\ndepending on the size and complexity of the data. Thus, decreasing formal\ncontext may eliminate erroneous and uninteresting pairs and speed up idea\nlattice derivation. WordNet-based and Frequency-based approaches are tested.\nFinally, we compute formal idea lattice and create a classical concept\nhierarchy. The reduced concept lattice is compared to the original to evaluate\nthe outcomes. Despite several system constraints and component discrepancies\nthat may prevent logical conclusion, the following data imply idea hierarchies\nin this project and dissertation are promising. First, the reduced idea lattice\nand original concept have commonalities. Second, alternative language or\nstatistical methods can reduce formal context size. Finally, WordNet-based and\nFrequency-based approaches reduce formal context differently, and the order of\napplying them is examined to reduce context efficiently.", "published": "2023-11-10 08:28:30", "link": "http://arxiv.org/abs/2311.14699v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT Exhibits Gender and Racial Biases in Acute Coronary Syndrome\n  Management", "abstract": "Recent breakthroughs in large language models (LLMs) have led to their rapid\ndissemination and widespread use. One early application has been to medicine,\nwhere LLMs have been investigated to streamline clinical workflows and\nfacilitate clinical analysis and decision-making. However, a leading barrier to\nthe deployment of Artificial Intelligence (AI) and in particular LLMs has been\nconcern for embedded gender and racial biases. Here, we evaluate whether a\nleading LLM, ChatGPT 3.5, exhibits gender and racial bias in clinical\nmanagement of acute coronary syndrome (ACS). We find that specifying patients\nas female, African American, or Hispanic resulted in a decrease in guideline\nrecommended medical management, diagnosis, and symptom management of ACS. Most\nnotably, the largest disparities were seen in the recommendation of coronary\nangiography or stress testing for the diagnosis and further intervention of ACS\nand recommendation of high intensity statins. These disparities correlate with\nbiases that have been observed clinically and have been implicated in the\ndifferential gender and racial morbidity and mortality outcomes of ACS and\ncoronary artery disease. Furthermore, we find that the largest disparities are\nseen during unstable angina, where fewer explicit clinical guidelines exist.\nFinally, we find that through asking ChatGPT 3.5 to explain its reasoning prior\nto providing an answer, we are able to improve clinical accuracy and mitigate\ninstances of gender and racial biases. This is among the first studies to\ndemonstrate that the gender and racial biases that LLMs exhibit do in fact\naffect clinical management. Additionally, we demonstrate that existing\nstrategies that improve LLM performance not only improve LLM performance in\nclinical management, but can also be used to mitigate gender and racial biases.", "published": "2023-11-10 19:59:36", "link": "http://arxiv.org/abs/2311.14703v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Leveraging LLMs for Synthesizing Training Data Across Many Languages in\n  Multilingual Dense Retrieval", "abstract": "There has been limited success for dense retrieval models in multilingual\nretrieval, due to uneven and scarce training data available across multiple\nlanguages. Synthetic training data generation is promising (e.g., InPars or\nPromptagator), but has been investigated only for English. Therefore, to study\nmodel capabilities across both cross-lingual and monolingual retrieval tasks,\nwe develop SWIM-IR, a synthetic retrieval training dataset containing 33 (high\nto very-low resource) languages for fine-tuning multilingual dense retrievers\nwithout requiring any human supervision. To construct SWIM-IR, we propose SAP\n(summarize-then-ask prompting), where the large language model (LLM) generates\na textual summary prior to the query generation step. SAP assists the LLM in\ngenerating informative queries in the target language. Using SWIM-IR, we\nexplore synthetic fine-tuning of multilingual dense retrieval models and\nevaluate them robustly on three retrieval benchmarks: XOR-Retrieve\n(cross-lingual), MIRACL (monolingual) and XTREME-UP (cross-lingual). Our\nmodels, called SWIM-X, are competitive with human-supervised dense retrieval\nmodels, e.g., mContriever-X, finding that SWIM-IR can cheaply substitute for\nexpensive human-labeled retrieval training data. SWIM-IR dataset and SWIM-X\nmodels are available at https://github.com/google-research-datasets/SWIM-IR.", "published": "2023-11-10 00:17:10", "link": "http://arxiv.org/abs/2311.05800v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Tamil-Llama: A New Tamil Language Model Based on Llama 2", "abstract": "Language modeling has witnessed remarkable advancements in recent years, with\nLarge Language Models (LLMs) like ChatGPT setting unparalleled benchmarks in\nhuman-like text generation. However, a prevailing limitation is the\nunderrepresentation of languages like Tamil in these cutting-edge models,\nleading to suboptimal performance in diverse linguistic contexts. This paper\naddresses this lacuna, enhancing the open-source LLaMA model with an addition\nof 16,000 Tamil tokens, aiming to achieve superior text generation and\ncomprehension in the Tamil language. We strategically employ the LoRA\nmethodology for efficient model training on a comprehensive Tamil corpus,\nensuring computational feasibility and model robustness. Moreover, we introduce\na Tamil-translated version of the Alpaca dataset and a subset of the OpenOrca\ndataset tailored for instruction fine-tuning. Our results showcase significant\nperformance improvements in Tamil text generation, with potential implications\nfor the broader landscape of LLMs in Indian languages. We further underscore\nour commitment to open research by making our models, datasets, and code\npublicly accessible, fostering further innovations in language modeling.", "published": "2023-11-10 03:02:39", "link": "http://arxiv.org/abs/2311.05845v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Practical Membership Inference Attacks against Fine-tuned Large Language\n  Models via Self-prompt Calibration", "abstract": "Membership Inference Attacks (MIA) aim to infer whether a target data record\nhas been utilized for model training or not. Existing MIAs designed for large\nlanguage models (LLMs) can be bifurcated into two types: reference-free and\nreference-based attacks. Although reference-based attacks appear promising\nperformance by calibrating the probability measured on the target model with\nreference models, this illusion of privacy risk heavily depends on a reference\ndataset that closely resembles the training set. Both two types of attacks are\npredicated on the hypothesis that training records consistently maintain a\nhigher probability of being sampled. However, this hypothesis heavily relies on\nthe overfitting of target models, which will be mitigated by multiple\nregularization methods and the generalization of LLMs. Thus, these reasons lead\nto high false-positive rates of MIAs in practical scenarios. We propose a\nMembership Inference Attack based on Self-calibrated Probabilistic Variation\n(SPV-MIA). Specifically, we introduce a self-prompt approach, which constructs\nthe dataset to fine-tune the reference model by prompting the target LLM\nitself. In this manner, the adversary can collect a dataset with a similar\ndistribution from public APIs. Furthermore, we introduce probabilistic\nvariation, a more reliable membership signal based on LLM memorization rather\nthan overfitting, from which we rediscover the neighbour attack with\ntheoretical grounding. Comprehensive evaluation conducted on three datasets and\nfour exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a\nsignificantly high level of 0.9. Our code and dataset are available at:\nhttps://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA", "published": "2023-11-10 13:55:05", "link": "http://arxiv.org/abs/2311.06062v4", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Syntax-semantics interface: an algebraic model", "abstract": "We extend our formulation of Merge and Minimalism in terms of Hopf algebras\nto an algebraic model of a syntactic-semantic interface. We show that methods\nadopted in the formulation of renormalization (extraction of meaningful\nphysical values) in theoretical physics are relevant to describe the extraction\nof meaning from syntactic expressions. We show how this formulation relates to\ncomputational models of semantics and we answer some recent controversies about\nimplications for generative linguistics of the current functioning of large\nlanguage models.", "published": "2023-11-10 17:12:09", "link": "http://arxiv.org/abs/2311.06189v1", "categories": ["cs.CL", "math.LO", "math.QA", "math.RA", "91F20, 16T05, 18C50"], "primary_category": "cs.CL"}
{"title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in\n  Large Language Models", "abstract": "We propose the Data Contamination Quiz (DCQ), a simple and effective approach\nto detect data contamination in large language models (LLMs) and estimate the\namount of it. Specifically, we frame data contamination detection as a series\nof multiple-choice questions and devise a quiz format wherein three perturbed\nversions of each subsampled instance from a specific dataset partition (e.g.,\nGSM8k test set) are created. These changes only include word-level\nperturbations. The generated perturbations, along with the original dataset\ninstance, form the options in the DCQ, with an extra option accommodating the\npossibility of selecting none of the provided options. Given that the only\ndistinguishing signal among the options is the exact wording with respect to\nthe original dataset instance, an LLM, when tasked with identifying the\noriginal dataset instance, gravitates towards selecting the original one if it\nhas been exposed to it in its pre-training phase -- a trait intrinsic to LLMs.\nWhile accounting for positional biases in LLMs, the quiz performance reveals\nthe contamination level for the model being examined with the dataset partition\nto which the quiz pertains. Applied to various datasets with GPT-4 and GPT-3.5,\nour findings -- while fully lacking access to pre-training data and model\nparameters -- suggest that DCQ achieves state-of-the-art results and uncovers\ngreater contamination/memorization levels compared to existing methods and\nproficiently bypasses more safety filters, especially those set to avoid\ngenerating copyrighted contents.", "published": "2023-11-10 18:48:58", "link": "http://arxiv.org/abs/2311.06233v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming", "abstract": "Engaging in the deliberate generation of abnormal outputs from Large Language\nModels (LLMs) by attacking them is a novel human activity. This paper presents\na thorough exposition of how and why people perform such attacks, defining LLM\nred-teaming based on extensive and diverse evidence. Using a formal qualitative\nmethodology, we interviewed dozens of practitioners from a broad range of\nbackgrounds, all contributors to this novel work of attempting to cause LLMs to\nfail. We focused on the research questions of defining LLM red teaming,\nuncovering the motivations and goals for performing the activity, and\ncharacterizing the strategies people use when attacking LLMs. Based on the\ndata, LLM red teaming is defined as a limit-seeking, non-malicious, manual\nactivity, which depends highly on a team-effort and an alchemist mindset. It is\nhighly intrinsically motivated by curiosity, fun, and to some degrees by\nconcerns for various harms of deploying LLMs. We identify a taxonomy of 12\nstrategies and 35 different techniques of attacking LLMs. These findings are\npresented as a comprehensive grounded theory of how and why people attack large\nlanguage models: LLM red teaming.", "published": "2023-11-10 18:52:58", "link": "http://arxiv.org/abs/2311.06237v3", "categories": ["cs.CL", "cs.CR", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization", "abstract": "Large foundation models are becoming ubiquitous, but training them from\nscratch is prohibitively expensive. Thus, efficiently adapting these powerful\nmodels to downstream tasks is increasingly important. In this paper, we study a\nprincipled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream\ntask adaptation. Despite demonstrating good generalizability, OFT still uses a\nfairly large number of trainable parameters due to the high dimensionality of\northogonal matrices. To address this, we start by examining OFT from an\ninformation transmission perspective, and then identify a few key desiderata\nthat enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast\nFourier transform algorithm enables efficient information transmission, we\npropose an efficient orthogonal parameterization using butterfly structures. We\napply this parameterization to OFT, creating a novel parameter-efficient\nfinetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a\nspecial case, BOFT introduces a generalized orthogonal finetuning framework.\nFinally, we conduct an extensive empirical study of adapting large vision\ntransformers, large language models, and text-to-image diffusion models to\nvarious downstream tasks in vision and language.", "published": "2023-11-10 18:59:54", "link": "http://arxiv.org/abs/2311.06243v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Knowledge-Augmented Large Language Models for Personalized Contextual\n  Query Suggestion", "abstract": "Large Language Models (LLMs) excel at tackling various natural language\ntasks. However, due to the significant costs involved in re-training or\nfine-tuning them, they remain largely static and difficult to personalize.\nNevertheless, a variety of applications could benefit from generations that are\ntailored to users' preferences, goals, and knowledge. Among them is web search,\nwhere knowing what a user is trying to accomplish, what they care about, and\nwhat they know can lead to improved search experiences. In this work, we\npropose a novel and general approach that augments an LLM with relevant context\nfrom users' interaction histories with a search engine in order to personalize\nits outputs. Specifically, we construct an entity-centric knowledge store for\neach user based on their search and browsing activities on the web, which is\nthen leveraged to provide contextually relevant LLM prompt augmentations. This\nknowledge store is light-weight, since it only produces user-specific aggregate\nprojections of interests and knowledge onto public knowledge graphs, and\nleverages existing search log infrastructure, thereby mitigating the privacy,\ncompliance, and scalability concerns associated with building deep user\nprofiles for personalization. We validate our approach on the task of\ncontextual query suggestion, which requires understanding not only the user's\ncurrent search context but also what they historically know and care about.\nThrough a number of experiments based on human evaluation, we show that our\napproach is significantly better than several other LLM-powered baselines,\ngenerating query suggestions that are contextually more relevant, personalized,\nand useful.", "published": "2023-11-10 01:18:47", "link": "http://arxiv.org/abs/2311.06318v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Autoregressive Language Models For Estimating the Entropy of Epic EHR\n  Audit Logs", "abstract": "EHR audit logs are a highly granular stream of events that capture clinician\nactivities, and is a significant area of interest for research in\ncharacterizing clinician workflow on the electronic health record (EHR).\nExisting techniques to measure the complexity of workflow through EHR audit\nlogs (audit logs) involve time- or frequency-based cross-sectional aggregations\nthat are unable to capture the full complexity of a EHR session. We briefly\nevaluate the usage of transformer-based tabular language model (tabular LM) in\nmeasuring the entropy or disorderedness of action sequences within workflow and\nrelease the evaluated models publicly.", "published": "2023-11-10 21:32:34", "link": "http://arxiv.org/abs/2311.06401v3", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Follow-Up Differential Descriptions: Language Models Resolve Ambiguities\n  for Image Classification", "abstract": "A promising approach for improving the performance of vision-language models\nlike CLIP for image classification is to extend the class descriptions (i.e.,\nprompts) with related attributes, e.g., using brown sparrow instead of sparrow.\nHowever, current zero-shot methods select a subset of attributes regardless of\ncommonalities between the target classes, potentially providing no useful\ninformation that would have helped to distinguish between them. For instance,\nthey may use color instead of bill shape to distinguish between sparrows and\nwrens, which are both brown. We propose Follow-up Differential Descriptions\n(FuDD), a zero-shot approach that tailors the class descriptions to each\ndataset and leads to additional attributes that better differentiate the target\nclasses. FuDD first identifies the ambiguous classes for each image, and then\nuses a Large Language Model (LLM) to generate new class descriptions that\ndifferentiate between them. The new class descriptions resolve the initial\nambiguity and help predict the correct label. In our experiments, FuDD\nconsistently outperforms generic description ensembles and naive LLM-generated\ndescriptions on 12 datasets. We show that differential descriptions are an\neffective tool to resolve class ambiguities, which otherwise significantly\ndegrade the performance. We also show that high quality natural language class\ndescriptions produced by FuDD result in comparable performance to few-shot\nadaptation methods.", "published": "2023-11-10 05:24:07", "link": "http://arxiv.org/abs/2311.07593v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How to Bridge the Gap between Modalities: Survey on Multimodal Large\n  Language Model", "abstract": "We explore Multimodal Large Language Models (MLLMs), which integrate LLMs\nlike GPT-4 to handle multimodal data, including text, images, audio, and more.\nMLLMs demonstrate capabilities such as generating image captions and answering\nimage-based questions, bridging the gap towards real-world human-computer\ninteractions and hinting at a potential pathway to artificial general\nintelligence. However, MLLMs still face challenges in addressing the semantic\ngap in multimodal data, which may lead to erroneous outputs, posing potential\nrisks to society. Selecting the appropriate modality alignment method is\ncrucial, as improper methods might require more parameters without significant\nperformance improvements. This paper aims to explore modality alignment methods\nfor LLMs and their current capabilities. Implementing effective modality\nalignment can help LLMs address environmental issues and enhance accessibility.\nThe study surveys existing modality alignment methods for MLLMs, categorizing\nthem into four groups: (1) Multimodal Converter, which transforms data into a\nformat that LLMs can understand; (2) Multimodal Perceiver, which improves how\nLLMs percieve different types of data; (3) Tool Learning, which leverages\nexternal tools to convert data into a common format, usually text; and (4)\nData-Driven Method, which teaches LLMs to understand specific data types within\ndatasets.", "published": "2023-11-10 09:51:24", "link": "http://arxiv.org/abs/2311.07594v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Multi-Label Topic Model for Financial Textual Data", "abstract": "This paper presents a multi-label topic model for financial texts like ad-hoc\nannouncements, 8-K filings, finance related news or annual reports. I train the\nmodel on a new financial multi-label database consisting of 3,044 German ad-hoc\nannouncements that are labeled manually using 20 predefined, economically\nmotivated topics. The best model achieves a macro F1 score of more than 85%.\nTranslating the data results in an English version of the model with similar\nperformance. As application of the model, I investigate differences in stock\nmarket reactions across topics. I find evidence for strong positive or negative\nmarket reactions for some topics, like announcements of new Large Scale\nProjects or Bankruptcy Filings, while I do not observe significant price\neffects for some other topics. Furthermore, in contrast to previous studies,\nthe multi-label structure of the model allows to analyze the effects of\nco-occurring topics on stock market reactions. For many cases, the reaction to\na specific topic depends heavily on the co-occurrence with other topics. For\nexample, if allocated capital from a Seasoned Equity Offering (SEO) is used for\nrestructuring a company in the course of a Bankruptcy Proceeding, the market\nreacts positively on average. However, if that capital is used for covering\nunexpected, additional costs from the development of new drugs, the SEO implies\nnegative reactions on average.", "published": "2023-11-10 12:56:07", "link": "http://arxiv.org/abs/2311.07598v1", "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "MultiIoT: Benchmarking Machine Learning for the Internet of Things", "abstract": "The next generation of machine learning systems must be adept at perceiving\nand interacting with the physical world through a diverse array of sensory\nchannels. Commonly referred to as the `Internet of Things (IoT)' ecosystem,\nsensory data from motion, thermal, geolocation, depth, wireless signals, video,\nand audio are increasingly used to model the states of physical environments\nand the humans inside them. Despite the potential for understanding human\nwellbeing, controlling physical devices, and interconnecting smart cities, the\ncommunity has seen limited benchmarks for building machine learning systems for\nIoT. Existing efforts are often specialized to a single sensory modality or\nprediction task, which makes it difficult to study and train large-scale models\nacross many IoT sensors and tasks. To accelerate the development of new machine\nlearning technologies for IoT, this paper proposes MultiIoT, the most expansive\nand unified IoT benchmark to date, encompassing over 1.15 million samples from\n12 modalities and 8 real-world tasks. MultiIoT introduces unique challenges\ninvolving (1) generalizable learning from many sensory modalities, (2)\nmultimodal interactions across long temporal ranges, (3) extreme heterogeneity\ndue to unique structure and noise topologies in real-world sensors, and (4)\ncomplexity during training and inference. We evaluate a comprehensive set of\nmodels on MultiIoT, including modality and task-specific methods, multisensory\nand multitask supervised models, and large multisensory foundation models. Our\nresults highlight opportunities for ML to make a significant impact in IoT, but\nmany challenges in scalable learning from heterogeneous, long-range, and\nimperfect sensory modalities still persist. We release all code and data to\naccelerate future research in machine learning for IoT.", "published": "2023-11-10 18:13:08", "link": "http://arxiv.org/abs/2311.06217v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "A Survey of AI Text-to-Image and AI Text-to-Video Generators", "abstract": "Text-to-Image and Text-to-Video AI generation models are revolutionary\ntechnologies that use deep learning and natural language processing (NLP)\ntechniques to create images and videos from textual descriptions. This paper\ninvestigates cutting-edge approaches in the discipline of Text-to-Image and\nText-to-Video AI generations. The survey provides an overview of the existing\nliterature as well as an analysis of the approaches used in various studies. It\ncovers data preprocessing techniques, neural network types, and evaluation\nmetrics used in the field. In addition, the paper discusses the challenges and\nlimitations of Text-to-Image and Text-to-Video AI generations, as well as\nfuture research directions. Overall, these models have promising potential for\na wide range of applications such as video production, content creation, and\ndigital marketing.", "published": "2023-11-10 17:33:58", "link": "http://arxiv.org/abs/2311.06329v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "eess.IV"], "primary_category": "cs.CV"}
{"title": "The Shape of Learning: Anisotropy and Intrinsic Dimensions in\n  Transformer-Based Models", "abstract": "In this study, we present an investigation into the anisotropy dynamics and\nintrinsic dimension of embeddings in transformer architectures, focusing on the\ndichotomy between encoders and decoders. Our findings reveal that the\nanisotropy profile in transformer decoders exhibits a distinct bell-shaped\ncurve, with the highest anisotropy concentrations in the middle layers. This\npattern diverges from the more uniformly distributed anisotropy observed in\nencoders. In addition, we found that the intrinsic dimension of embeddings\nincreases in the initial phases of training, indicating an expansion into\nhigher-dimensional space. Which is then followed by a compression phase towards\nthe end of training with dimensionality decrease, suggesting a refinement into\nmore compact representations. Our results provide fresh insights to the\nunderstanding of encoders and decoders embedding properties.", "published": "2023-11-10 08:25:02", "link": "http://arxiv.org/abs/2311.05928v2", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "math.GN", "math.IT"], "primary_category": "cs.CL"}
{"title": "Smart Agent-Based Modeling: On the Use of Large Language Models in\n  Computer Simulations", "abstract": "Computer simulations offer a robust toolset for exploring complex systems\nacross various disciplines. A particularly impactful approach within this realm\nis Agent-Based Modeling (ABM), which harnesses the interactions of individual\nagents to emulate intricate system dynamics. ABM's strength lies in its\nbottom-up methodology, illuminating emergent phenomena by modeling the\nbehaviors of individual components of a system. Yet, ABM has its own set of\nchallenges, notably its struggle with modeling natural language instructions\nand common sense in mathematical equations or rules. This paper seeks to\ntranscend these boundaries by integrating Large Language Models (LLMs) like GPT\ninto ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based\nModeling (SABM). Building upon the concept of smart agents -- entities\ncharacterized by their intelligence, adaptability, and computation ability --\nwe explore in the direction of utilizing LLM-powered agents to simulate\nreal-world scenarios with increased nuance and realism. In this comprehensive\nexploration, we elucidate the state of the art of ABM, introduce SABM's\npotential and methodology, and present three case studies (source codes\navailable at https://github.com/Roihn/SABM), demonstrating the SABM methodology\nand validating its effectiveness in modeling real-world systems. Furthermore,\nwe cast a vision towards several aspects of the future of SABM, anticipating a\nbroader horizon for its applications. Through this endeavor, we aspire to\nredefine the boundaries of computer simulations, enabling a more profound\nunderstanding of complex systems.", "published": "2023-11-10 18:54:33", "link": "http://arxiv.org/abs/2311.06330v4", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.MA", "econ.GN", "q-fin.EC"], "primary_category": "cs.AI"}
{"title": "The AeroSonicDB (YPAD-0523) Dataset for Acoustic Detection and\n  Classification of Aircraft", "abstract": "The time and expense required to collect and label audio data has been a\nprohibitive factor in the availability of domain specific audio datasets. As\nthe predictive specificity of a classifier depends on the specificity of the\nlabels it is trained on, it follows that finely-labelled datasets are crucial\nfor advances in machine learning. Aiming to stimulate progress in the field of\nmachine listening, this paper introduces AeroSonicDB (YPAD-0523), a dataset of\nlow-flying aircraft sounds for training acoustic detection and classification\nsystems. This paper describes the method of exploiting ADS-B radio\ntransmissions to passively collect and label audio samples. Provides a summary\nof the collated dataset. Presents baseline results from three binary\nclassification models, then discusses the limitations of the current dataset\nand its future potential. The dataset contains 625 aircraft recordings ranging\nin event duration from 18 to 60 seconds, for a total of 8.87 hours of aircraft\naudio. These 625 samples feature 301 unique aircraft, each of which are\nsupplied with 14 supplementary (non-acoustic) labels to describe the aircraft.\nThe dataset also contains 3.52 hours of ambient background audio (\"silence\"),\nas a means to distinguish aircraft noise from other local environmental noises.\nAdditionally, 6 hours of urban soundscape recordings (with aircraft\nannotations) are included as an ancillary method for evaluating model\nperformance, and to provide a testing ground for real-time applications.", "published": "2023-11-10 19:41:10", "link": "http://arxiv.org/abs/2311.06368v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
