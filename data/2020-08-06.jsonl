{"title": "ConvBERT: Improving BERT with Span-based Dynamic Convolution", "abstract": "Pre-trained language models like BERT and its variants have recently achieved\nimpressive performance in various natural language understanding tasks.\nHowever, BERT heavily relies on the global self-attention block and thus\nsuffers large memory footprint and computation cost. Although all its attention\nheads query on the whole input sequence for generating the attention map from a\nglobal perspective, we observe some heads only need to learn local\ndependencies, which means the existence of computation redundancy. We therefore\npropose a novel span-based dynamic convolution to replace these self-attention\nheads to directly model local dependencies. The novel convolution heads,\ntogether with the rest self-attention heads, form a new mixed attention block\nthat is more efficient at both global and local context learning. We equip BERT\nwith this mixed attention design and build a ConvBERT model. Experiments have\nshown that ConvBERT significantly outperforms BERT and its variants in various\ndownstream tasks, with lower training cost and fewer model parameters.\nRemarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than\nELECTRAbase, while using less than 1/4 training cost. Code and pre-trained\nmodels will be released.", "published": "2020-08-06 07:43:19", "link": "http://arxiv.org/abs/2008.02496v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Neural Query Auto Completion", "abstract": "Query Auto Completion (QAC), as the starting point of information retrieval\ntasks, is critical to user experience. Generally it has two steps: generating\ncompleted query candidates according to query prefixes, and ranking them based\non extracted features. Three major challenges are observed for a query auto\ncompletion system: (1) QAC has a strict online latency requirement. For each\nkeystroke, results must be returned within tens of milliseconds, which poses a\nsignificant challenge in designing sophisticated language models for it. (2)\nFor unseen queries, generated candidates are of poor quality as contextual\ninformation is not fully utilized. (3) Traditional QAC systems heavily rely on\nhandcrafted features such as the query candidate frequency in search logs,\nlacking sufficient semantic understanding of the candidate.\n  In this paper, we propose an efficient neural QAC system with effective\ncontext modeling to overcome these challenges. On the candidate generation\nside, this system uses as much information as possible in unseen prefixes to\ngenerate relevant candidates, increasing the recall by a large margin. On the\ncandidate ranking side, an unnormalized language model is proposed, which\neffectively captures deep semantics of queries. This approach presents better\nranking performance over state-of-the-art neural ranking methods and reduces\n$\\sim$95\\% latency compared to neural language modeling methods. The empirical\nresults on public datasets show that our model achieves a good balance between\naccuracy and efficiency. This system is served in LinkedIn job search with\nsignificant product impact observed.", "published": "2020-08-06 21:28:36", "link": "http://arxiv.org/abs/2008.02879v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Sentiment Analysis Based on Probabilistic Graphical Models and\n  Recurrent Neural Network", "abstract": "Sentiment Analysis is the task of classifying documents based on the\nsentiments expressed in textual form, this can be achieved by using lexical and\nsemantic methods. The purpose of this study is to investigate the use of\nsemantics to perform sentiment analysis based on probabilistic graphical models\nand recurrent neural networks. In the empirical evaluation, the classification\nperformance of the graphical models was compared with some traditional machine\nlearning classifiers and a recurrent neural network. The datasets used for the\nexperiments were IMDB movie reviews, Amazon Consumer Product reviews, and\nTwitter Review datasets. After this empirical study, we conclude that the\ninclusion of semantics for sentiment analysis tasks can greatly improve the\nperformance of a classifier, as the semantic feature extraction methods reduce\nuncertainties in classification resulting in more accurate predictions.", "published": "2020-08-06 11:59:00", "link": "http://arxiv.org/abs/2009.00234v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "DeText: A Deep Text Ranking Framework with BERT", "abstract": "Ranking is the most important component in a search system. Mostsearch\nsystems deal with large amounts of natural language data,hence an effective\nranking system requires a deep understandingof text semantics. Recently, deep\nlearning based natural languageprocessing (deep NLP) models have generated\npromising results onranking systems. BERT is one of the most successful models\nthatlearn contextual embedding, which has been applied to capturecomplex\nquery-document relations for search ranking. However,this is generally done by\nexhaustively interacting each query wordwith each document word, which is\ninefficient for online servingin search product systems. In this paper, we\ninvestigate how tobuild an efficient BERT-based ranking model for industry use\ncases.The solution is further extended to a general ranking framework,DeText,\nthat is open sourced and can be applied to various rankingproductions. Offline\nand online experiments of DeText on threereal-world search systems present\nsignificant improvement overstate-of-the-art approaches.", "published": "2020-08-06 05:12:11", "link": "http://arxiv.org/abs/2008.02460v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Question and Answer Test-Train Overlap in Open-Domain Question Answering\n  Datasets", "abstract": "Ideally Open-Domain Question Answering models should exhibit a number of\ncompetencies, ranging from simply memorizing questions seen at training time,\nto answering novel question formulations with answers seen during training, to\ngeneralizing to completely novel questions with novel answers. However, single\naggregated test set scores do not show the full picture of what capabilities\nmodels truly have. In this work, we perform a detailed study of the test sets\nof three popular open-domain benchmark datasets with respect to these\ncompetencies. We find that 60-70% of test-time answers are also present\nsomewhere in the training sets. We also find that 30% of test-set questions\nhave a near-duplicate paraphrase in their corresponding training sets. Using\nthese findings, we evaluate a variety of popular open-domain models to obtain\ngreater insight into what extent they can actually generalize, and what drives\ntheir overall performance. We find that all models perform dramatically worse\non questions that cannot be memorized from training sets, with a mean absolute\nperformance difference of 63% between repeated and non-repeated data. Finally\nwe show that simple nearest-neighbor models out-perform a BART closed-book QA\nmodel, further highlighting the role that training set memorization plays in\nthese benchmarks", "published": "2020-08-06 13:17:43", "link": "http://arxiv.org/abs/2008.02637v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multilingual Neural Machine Translation Model for Biomedical Data", "abstract": "We release a multilingual neural machine translation model, which can be used\nto translate text in the biomedical domain. The model can translate from 5\nlanguages (French, German, Italian, Korean and Spanish) into English. It is\ntrained with large amounts of generic and biomedical data, using domain tags.\nOur benchmarks show that it performs near state-of-the-art both on news\n(generic domain) and biomedical test sets, and that it outperforms the existing\npublicly released models. We believe that this release will help the\nlarge-scale multilingual analysis of the digital content of the COVID-19 crisis\nand of its effects on society, economy, and healthcare policies.\n  We also release a test set of biomedical text for Korean-English. It consists\nof 758 sentences from official guidelines and recent papers, all about\nCOVID-19.", "published": "2020-08-06 21:26:43", "link": "http://arxiv.org/abs/2008.02878v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data balancing for boosting performance of low-frequency classes in\n  Spoken Language Understanding", "abstract": "Despite the fact that data imbalance is becoming more and more common in\nreal-world Spoken Language Understanding (SLU) applications, it has not been\nstudied extensively in the literature. To the best of our knowledge, this paper\npresents the first systematic study on handling data imbalance for SLU. In\nparticular, we discuss the application of existing data balancing techniques\nfor SLU and propose a multi-task SLU model for intent classification and slot\nfilling. Aiming to avoid over-fitting, in our model methods for data balancing\nare leveraged indirectly via an auxiliary task which makes use of a\nclass-balanced batch generator and (possibly) synthetic data. Our results on a\nreal-world dataset indicate that i) our proposed model can boost performance on\nlow frequency intents significantly while avoiding a potential performance\ndecrease on the head intents, ii) synthetic data are beneficial for\nbootstrapping new intents when realistic data are not available, but iii) once\na certain amount of realistic data becomes available, using synthetic data in\nthe auxiliary task only yields better performance than adding them to the\nprimary task training data, and iv) in a joint training scenario, balancing the\nintent distribution individually improves not only intent classification but\nalso slot filling performance.", "published": "2020-08-06 12:23:11", "link": "http://arxiv.org/abs/2008.02603v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Compositional Networks Enable Systematic Generalization for Grounded\n  Language Understanding", "abstract": "Humans are remarkably flexible when understanding new sentences that include\ncombinations of concepts they have never encountered before. Recent work has\nshown that while deep networks can mimic some human language abilities when\npresented with novel sentences, systematic variation uncovers the limitations\nin the language-understanding abilities of networks. We demonstrate that these\nlimitations can be overcome by addressing the generalization challenges in the\ngSCAN dataset, which explicitly measures how well an agent is able to interpret\nnovel linguistic commands grounded in vision, e.g., novel pairings of\nadjectives and nouns. The key principle we employ is compositionality: that the\ncompositional structure of networks should reflect the compositional structure\nof the problem domain they address, while allowing other parameters to be\nlearned end-to-end. We build a general-purpose mechanism that enables agents to\ngeneralize their language understanding to compositional domains. Crucially,\nour network has the same state-of-the-art performance as prior work while\ngeneralizing its knowledge when prior work does not. Our network also provides\na level of interpretability that enables users to inspect what each part of\nnetworks learns. Robust grounded language understanding without dramatic\nfailures and without corner cases is critical to building safe and fair robots;\nwe demonstrate the significant role that compositionality can play in achieving\nthat goal.", "published": "2020-08-06 16:17:35", "link": "http://arxiv.org/abs/2008.02742v3", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Semantic Complexity in End-to-End Spoken Language Understanding", "abstract": "End-to-end spoken language understanding (SLU) models are a class of model\narchitectures that predict semantics directly from speech. Because of their\ninput and output types, we refer to them as speech-to-interpretation (STI)\nmodels. Previous works have successfully applied STI models to targeted use\ncases, such as recognizing home automation commands, however no study has yet\naddressed how these models generalize to broader use cases. In this work, we\nanalyze the relationship between the performance of STI models and the\ndifficulty of the use case to which they are applied. We introduce empirical\nmeasures of dataset semantic complexity to quantify the difficulty of the SLU\ntasks. We show that near-perfect performance metrics for STI models reported in\nthe literature were obtained with datasets that have low semantic complexity\nvalues. We perform experiments where we vary the semantic complexity of a\nlarge, proprietary dataset and show that STI model performance correlates with\nour semantic complexity measures, such that performance increases as complexity\nvalues decrease. Our results show that it is important to contextualize an STI\nmodel's performance with the complexity values of its training dataset to\nreveal the scope of its applicability.", "published": "2020-08-06 20:18:53", "link": "http://arxiv.org/abs/2008.02858v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A general solution to the preferential selection model", "abstract": "We provide a general analytic solution to Herbert Simon's 1955 model for\ntime-evolving novelty functions. This has far-reaching consequences: Simon's is\na pre-cursor model for Barabasi's 1999 preferential attachment model for\ngrowing social networks, and our general abstraction of it more considers\nattachment to be a form of link selection. We show that any system which can be\nmodeled as instances of types---i.e., occurrence data (frequencies)---can be\ngeneratively modeled (and simulated) from a distributional perspective with an\nexceptionally high-degree of accuracy.", "published": "2020-08-06 21:51:00", "link": "http://arxiv.org/abs/2008.02885v1", "categories": ["physics.soc-ph", "cs.CL", "cs.CY"], "primary_category": "physics.soc-ph"}
{"title": "Evaluating computational models of infant phonetic learning across\n  languages", "abstract": "In the first year of life, infants' speech perception becomes attuned to the\nsounds of their native language. Many accounts of this early phonetic learning\nexist, but computational models predicting the attunement patterns observed in\ninfants from the speech input they hear have been lacking. A recent study\npresented the first such model, drawing on algorithms proposed for unsupervised\nlearning from naturalistic speech, and tested it on a single phone contrast.\nHere we study five such algorithms, selected for their potential cognitive\nrelevance. We simulate phonetic learning with each algorithm and perform tests\non three phone contrasts from different languages, comparing the results to\ninfants' discrimination patterns. The five models display varying degrees of\nagreement with empirical observations, showing that our approach can help\ndecide between candidate mechanisms for early phonetic learning, and providing\ninsight into which aspects of the models are critical for capturing infants'\nperceptual development.", "published": "2020-08-06 22:07:45", "link": "http://arxiv.org/abs/2008.02888v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Better Fine-Tuning by Reducing Representational Collapse", "abstract": "Although widely adopted, existing approaches for fine-tuning pre-trained\nlanguage models have been shown to be unstable across hyper-parameter settings,\nmotivating recent work on trust region methods. In this paper, we present a\nsimplified and efficient method rooted in trust region theory that replaces\npreviously used adversarial objectives with parametric noise (sampling from\neither a normal or uniform distribution), thereby discouraging representation\nchange during fine-tuning when possible without hurting performance. We also\nintroduce a new analysis to motivate the use of trust region methods more\ngenerally, by studying representational collapse; the degradation of\ngeneralizable representations from pre-trained models as they are fine-tuned\nfor a specific end task. Extensive experiments show that our fine-tuning method\nmatches or exceeds the performance of previous trust region methods on a range\nof understanding and generation tasks (including DailyMail/CNN, Gigaword,\nReddit TIFU, and the GLUE benchmark), while also being much faster. We also\nshow that it is less prone to representation collapse; the pre-trained models\nmaintain more generalizable representations every time they are fine-tuned.", "published": "2020-08-06 02:13:16", "link": "http://arxiv.org/abs/2008.03156v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire", "abstract": "Lipreading is an impressive technique and there has been a definite\nimprovement of accuracy in recent years. However, existing methods for\nlipreading mainly build on autoregressive (AR) model, which generate target\ntokens one by one and suffer from high inference latency. To breakthrough this\nconstraint, we propose FastLR, a non-autoregressive (NAR) lipreading model\nwhich generates all target tokens simultaneously. NAR lipreading is a\nchallenging task that has many difficulties: 1) the discrepancy of sequence\nlengths between source and target makes it difficult to estimate the length of\nthe output sequence; 2) the conditionally independent behavior of NAR\ngeneration lacks the correlation across time which leads to a poor\napproximation of target distribution; 3) the feature representation ability of\nencoder can be weak due to lack of effective alignment mechanism; and 4) the\nremoval of AR language model exacerbates the inherent ambiguity problem of\nlipreading. Thus, in this paper, we introduce three methods to reduce the gap\nbetween FastLR and AR model: 1) to address challenges 1 and 2, we leverage\nintegrate-and-fire (I\\&F) module to model the correspondence between source\nvideo frames and output text sequence. 2) To tackle challenge 3, we add an\nauxiliary connectionist temporal classification (CTC) decoder to the top of the\nencoder and optimize it with extra CTC loss. We also add an auxiliary\nautoregressive decoder to help the feature extraction of encoder. 3) To\novercome challenge 4, we propose a novel Noisy Parallel Decoding (NPD) for I\\&F\nand bring Byte-Pair Encoding (BPE) into lipreading. Our experiments exhibit\nthat FastLR achieves the speedup up to 10.97$\\times$ comparing with\nstate-of-the-art lipreading model with slight WER absolute increase of 1.5\\%\nand 5.5\\% on GRID and LRS2 lipreading datasets respectively, which demonstrates\nthe effectiveness of our proposed method.", "published": "2020-08-06 08:28:56", "link": "http://arxiv.org/abs/2008.02516v4", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Discovering and Categorising Language Biases in Reddit", "abstract": "We present a data-driven approach using word embeddings to discover and\ncategorise language biases on the discussion platform Reddit. As spaces for\nisolated user communities, platforms such as Reddit are increasingly connected\nto issues of racism, sexism and other forms of discrimination. Hence, there is\na need to monitor the language of these groups. One of the most promising AI\napproaches to trace linguistic biases in large textual datasets involves word\nembeddings, which transform text into high-dimensional dense vectors and\ncapture semantic relations between words. Yet, previous studies require\npredefined sets of potential biases to study, e.g., whether gender is more or\nless associated with particular types of jobs. This makes these approaches\nunfit to deal with smaller and community-centric datasets such as those on\nReddit, which contain smaller vocabularies and slang, as well as biases that\nmay be particular to that community. This paper proposes a data-driven approach\nto automatically discover language biases encoded in the vocabulary of online\ndiscourse communities on Reddit. In our approach, protected attributes are\nconnected to evaluative words found in the data, which are then categorised\nthrough a semantic analysis system. We verify the effectiveness of our method\nby comparing the biases we discover in the Google News dataset with those found\nin previous literature. We then successfully discover gender bias, religion\nbias, and ethnic bias in different Reddit communities. We conclude by\ndiscussing potential application scenarios and limitations of this data-driven\nbias discovery method.", "published": "2020-08-06 16:42:10", "link": "http://arxiv.org/abs/2008.02754v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI", "68T50, 68T09, 91D30"], "primary_category": "cs.CL"}
{"title": "aschern at SemEval-2020 Task 11: It Takes Three to Tango: RoBERTa, CRF,\n  and Transfer Learning", "abstract": "We describe our system for SemEval-2020 Task 11 on Detection of Propaganda\nTechniques in News Articles. We developed ensemble models using RoBERTa-based\nneural architectures, additional CRF layers, transfer learning between the two\nsubtasks, and advanced post-processing to handle the multi-label nature of the\ntask, the consistency between nested spans, repetitions, and labels from\nsimilar spans in training. We achieved sizable improvements over baseline\nfine-tuned RoBERTa models, and the official evaluation ranked our system 3rd\n(almost tied with the 2nd) out of 36 teams on the span identification subtask\nwith an F1 score of 0.491, and 2nd (almost tied with the 1st) out of 31 teams\non the technique classification subtask with an F1 score of 0.62.", "published": "2020-08-06 18:45:25", "link": "http://arxiv.org/abs/2008.02837v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.NE", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Simultaneous measurement of time-invariant linear and nonlinear, and\n  random and extra responses using frequency domain variant of velvet noise", "abstract": "We introduce a new acoustic measurement method that can measure the linear\ntime-invariant response, the nonlinear time-invariant response, and random and\ntime-varying responses simultaneously. The method uses a set of orthogonal\nsequences made from a set of unit FVNs (Frequency domain variant of Velvet\nNoise), a new member of the TSP (Time Stretched Pulse). FVN has a unique\nfeature that other TSP members do not. It is a high degree of design freedom\nthat makes the proposed method possible without introducing extra equipment. We\nintroduce two useful cases using two and four orthogonal sequences and\nillustrates their use using simulations and acoustic measurement examples. We\ndeveloped an interactive and realtime acoustic analysis tool based on the\nproposed method. We made it available in an open-source repository. The\nproposed response analysis method is general and applies to other fields, such\nas auditory-feedback research and assessment of sound recording and coding.", "published": "2020-08-06 03:15:15", "link": "http://arxiv.org/abs/2008.02439v2", "categories": ["eess.AS", "cs.SD", "94A12", "J.m"], "primary_category": "eess.AS"}
{"title": "Quantification of Transducer Misalignment in Ultrasound Tongue Imaging", "abstract": "In speech production research, different imaging modalities have been\nemployed to obtain accurate information about the movement and shaping of the\nvocal tract. Ultrasound is an affordable and non-invasive imaging modality with\nrelatively high temporal and spatial resolution to study the dynamic behavior\nof tongue during speech production. However, a long-standing problem for\nultrasound tongue imaging is the transducer misalignment during longer data\nrecording sessions. In this paper, we propose a simple, yet effective,\nmisalignment quantification approach. The analysis employs MSE distance and two\nsimilarity measurement metrics to identify the relative displacement between\nthe chin and the transducer. We visualize these measures as a function of the\ntimestamp of the utterances. Extensive experiments are conducted on a Hungarian\nand Scottish English child dataset. The results suggest that large values of\nMean Square Error (MSE) and small values of Structural Similarity Index (SSIM)\nand Complex Wavelet SSIM indicate corruptions or issues during the data\nrecordings, which can either be caused by transducer misalignment or lack of\ngel.", "published": "2020-08-06 06:11:17", "link": "http://arxiv.org/abs/2008.02470v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "PPSpeech: Phrase based Parallel End-to-End TTS System", "abstract": "Current end-to-end autoregressive TTS systems (e.g. Tacotron 2) have\noutperformed traditional parallel approaches on the quality of synthesized\nspeech. However, they introduce new problems at the same time. Due to the\nautoregressive nature, the time cost of inference has to be proportional to the\nlength of text, which pose a great challenge for online serving. On the other\nhand, the style of synthetic speech becomes unstable and may change obviously\namong sentences. In this paper, we propose a Phrase based Parallel End-to-End\nTTS System (PPSpeech) to address these issues. PPSpeech uses autoregression\napproach within a phrase and executes parallel strategies for different\nphrases. By this method, we can achieve both high quality and high efficiency.\nIn addition, we propose acoustic embedding and text context embedding as the\nconditions of encoder to keep successive and prevent from abrupt style or\ntimbre change. Experiments show that, the synthesis speed of PPSpeech is much\nfaster than sentence level autoregressive Tacotron 2 when a sentence has more\nthan 5 phrases. The speed advantage increases with the growth of sentence\nlength. Subjective experiments show that the proposed system with acoustic\nembedding and context embedding as conditions can make the style transition\nacross sentences gradient and natural, defeating Global Style Token (GST)\nobviously in MOS.", "published": "2020-08-06 07:32:34", "link": "http://arxiv.org/abs/2008.02490v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HooliGAN: Robust, High Quality Neural Vocoding", "abstract": "Recent developments in generative models have shown that deep learning\ncombined with traditional digital signal processing (DSP) techniques could\nsuccessfully generate convincing violin samples [1], that source-excitation\ncombined with WaveNet yields high-quality vocoders [2, 3] and that generative\nadversarial network (GAN) training can improve naturalness [4, 5]. By combining\nthe ideas in these models we introduce HooliGAN, a robust vocoder that has\nstate of the art results, finetunes very well to smaller datasets (<30 minutes\nof speechdata) and generates audio at 2.2MHz on GPU and 35kHz on CPU. We also\nshow a simple modification to Tacotron-basedmodels that allows seamless\nintegration with HooliGAN. Results from our listening tests show the proposed\nmodel's ability to consistently output high-quality audio with a variety of\ndatasets, big and small. We provide samples at the following demo page:\nhttps://resemble-ai.github.io/hooligan_demo/", "published": "2020-08-06 07:37:32", "link": "http://arxiv.org/abs/2008.02493v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spectral-change enhancement with prior SNR for the hearing impaired", "abstract": "A previous signal processing algorithm that aimed to enhance spectral changes\n(SCE) over time showed benefit for hearing-impaired (HI) listeners to recognize\nspeech in background noise. In this work, the previous SCE was manipulated to\nperform on target-dominant segments, rather than treating all frames equally.\nInstantaneous signal-to-noise ratios (SNRs) were calculated to determine\nwhether the segments should be processed. Initially, the ideal SNR calculated\nby the knowledge of premixed signals was introduced to the previous SCE\nalgorithm (SCE-iSNR). Speech intelligibility (SI) and clarity preference were\nmeasured for 12 HI listeners in steady speech-spectrum noise (SSN) and six-talk\nspeech (STS) maskers, respectively. The results showed the SCE-iSNR algorithm\nimproved SI significantly for both maskers at high signal-to-masker ratios\n(SMRs) and for STS masker at low SMRs, while processing effect on speech\nquality was small. Secondly, the estimated SNR obtained from real mixtures was\nused, resulting in another SCE-eSNR. SI and subjective rating on naturalness\nand speech quality were tested for 7 HI subjects. The SCE-eSNR algorithm showed\nimproved SI for SSN masker at high SMRs and for STS masker at low SMRs, as well\nas better naturalness and speech quality for STS masker. The limitations of\napplying the algorithms are discussed.", "published": "2020-08-06 08:33:37", "link": "http://arxiv.org/abs/2008.02519v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Few-Shot Drum Transcription in Polyphonic Music", "abstract": "Data-driven approaches to automatic drum transcription (ADT) are often\nlimited to a predefined, small vocabulary of percussion instrument classes.\nSuch models cannot recognize out-of-vocabulary classes nor are they able to\nadapt to finer-grained vocabularies. In this work, we address open vocabulary\nADT by introducing few-shot learning to the task. We train a Prototypical\nNetwork on a synthetic dataset and evaluate the model on multiple real-world\nADT datasets with polyphonic accompaniment. We show that, given just a handful\nof selected examples at inference time, we can match and in some cases\noutperform a state-of-the-art supervised ADT approach under a fixed vocabulary\nsetting. At the same time, we show that our model can successfully generalize\nto finer-grained or extended vocabularies unseen during training, a scenario\nwhere supervised approaches cannot operate at all. We provide a detailed\nanalysis of our experimental results, including a breakdown of performance by\nsound class and by polyphony.", "published": "2020-08-06 17:58:14", "link": "http://arxiv.org/abs/2008.02791v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Separation Based on Multi-Stage Elaborated Dual-Path Deep BiLSTM\n  with Auxiliary Identity Loss", "abstract": "Deep neural network with dual-path bi-directional long short-term memory\n(BiLSTM) block has been proved to be very effective in sequence modeling,\nespecially in speech separation. This work investigates how to extend dual-path\nBiLSTM to result in a new state-of-the-art approach, called TasTas, for\nmulti-talker monaural speech separation (a.k.a cocktail party problem). TasTas\nintroduces two simple but effective improvements, one is an iterative\nmulti-stage refinement scheme, and the other is to correct the speech with\nimperfect separation through a loss of speaker identity consistency between the\nseparated speech and original speech, to boost the performance of dual-path\nBiLSTM based networks. TasTas takes the mixed utterance of two speakers and\nmaps it to two separated utterances, where each utterance contains only one\nspeaker's voice. Our experiments on the notable benchmark WSJ0-2mix data corpus\nresult in 20.55dB SDR improvement, 20.35dB SI-SDR improvement, 3.69 of PESQ,\nand 94.86\\% of ESTOI, which shows that our proposed networks can lead to big\nperformance improvement on the speaker separation task. We have open sourced\nour re-implementation of the DPRNN-TasNet here\n(https://github.com/ShiZiqiang/dual-path-RNNs-DPRNNs-based-speech-separation),\nand our TasTas is realized based on this implementation of DPRNN-TasNet, it is\nbelieved that the results in this paper can be reproduced with ease.", "published": "2020-08-06 07:36:50", "link": "http://arxiv.org/abs/2008.03149v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ultrasound-based Articulatory-to-Acoustic Mapping with WaveGlow Speech\n  Synthesis", "abstract": "For articulatory-to-acoustic mapping using deep neural networks, typically\nspectral and excitation parameters of vocoders have been used as the training\ntargets. However, vocoding often results in buzzy and muffled final speech\nquality. Therefore, in this paper on ultrasound-based articulatory-to-acoustic\nconversion, we use a flow-based neural vocoder (WaveGlow) pre-trained on a\nlarge amount of English and Hungarian speech data. The inputs of the\nconvolutional neural network are ultrasound tongue images. The training target\nis the 80-dimensional mel-spectrogram, which results in a finer detailed\nspectral representation than the previously used 25-dimensional Mel-Generalized\nCepstrum. From the output of the ultrasound-to-mel-spectrogram prediction,\nWaveGlow inference results in synthesized speech. We compare the proposed\nWaveGlow-based system with a continuous vocoder which does not use strict\nvoiced/unvoiced decision when predicting F0. The results demonstrate that\nduring the articulatory-to-acoustic mapping experiments, the WaveGlow neural\nvocoder produces significantly more natural synthesized speech than the\nbaseline system. Besides, the advantage of WaveGlow is that F0 is included in\nthe mel-spectrogram representation, and it is not necessary to predict the\nexcitation separately.", "published": "2020-08-06 04:55:44", "link": "http://arxiv.org/abs/2008.03152v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mixing-Specific Data Augmentation Techniques for Improved Blind\n  Violin/Piano Source Separation", "abstract": "Blind music source separation has been a popular and active subject of\nresearch in both the music information retrieval and signal processing\ncommunities. To counter the lack of available multi-track data for supervised\nmodel training, a data augmentation method that creates artificial mixtures by\ncombining tracks from different songs has been shown useful in recent works.\nFollowing this light, we examine further in this paper extended data\naugmentation methods that consider more sophisticated mixing settings employed\nin the modern music production routine, the relationship between the tracks to\nbe combined, and factors of silence. As a case study, we consider the\nseparation of violin and piano tracks in a violin piano ensemble, evaluating\nthe performance in terms of common metrics, namely SDR, SIR, and SAR. In\naddition to examining the effectiveness of these new data augmentation methods,\nwe also study the influence of the amount of training data. Our evaluation\nshows that the proposed mixing-specific data augmentation methods can help\nimprove the performance of a deep learning-based model for source separation,\nespecially in the case of small training data.", "published": "2020-08-06 07:02:24", "link": "http://arxiv.org/abs/2008.02480v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Shouted Speech Compensation for Speaker Verification Robust to Vocal\n  Effort Conditions", "abstract": "The performance of speaker verification systems degrades when vocal effort\nconditions between enrollment and test (e.g., shouted vs. normal speech) are\ndifferent. This is a potential situation in non-cooperative speaker\nverification tasks. In this paper, we present a study on different methods for\nlinear compensation of embeddings making use of Gaussian mixture models to\ncluster shouted and normal speech domains. These compensation techniques are\nborrowed from the area of robustness for automatic speech recognition and, in\nthis work, we apply them to compensate the mismatch between shouted and normal\nconditions in speaker verification. Before compensation, shouted condition is\nautomatically detected by means of logistic regression. The process is\ncomputationally light and it is performed in the back-end of an x-vector\nsystem. Experimental results show that applying the proposed approach in the\npresence of vocal effort mismatch yields up to 13.8% equal error rate relative\nimprovement with respect to a system that applies neither shouted speech\ndetection nor compensation.", "published": "2020-08-06 07:25:57", "link": "http://arxiv.org/abs/2008.02487v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving on-device speaker verification using federated learning with\n  privacy", "abstract": "Information on speaker characteristics can be useful as side information in\nimproving speaker recognition accuracy. However, such information is often\nprivate. This paper investigates how privacy-preserving learning can improve a\nspeaker verification system, by enabling the use of privacy-sensitive speaker\ndata to train an auxiliary classification model that predicts vocal\ncharacteristics of speakers. In particular, this paper explores the utility\nachieved by approaches which combine different federated learning and\ndifferential privacy mechanisms. These approaches make it possible to train a\ncentral model while protecting user privacy, with users' data remaining on\ntheir devices. Furthermore, they make learning on a large population of\nspeakers possible, ensuring good coverage of speaker characteristics when\ntraining a model. The auxiliary model described here uses features extracted\nfrom phrases which trigger a speaker verification system. From these features,\nthe model predicts speaker characteristic labels considered useful as side\ninformation. The knowledge of the auxiliary model is distilled into a speaker\nverification system using multi-task learning, with the side information labels\npredicted by this auxiliary model being the additional task. This approach\nresults in a 6% relative improvement in equal error rate over a baseline\nsystem.", "published": "2020-08-06 13:37:14", "link": "http://arxiv.org/abs/2008.02651v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Dynamic Emotion Modeling with Learnable Graphs and Graph Inception\n  Network", "abstract": "Human emotion is expressed, perceived and captured using a variety of dynamic\ndata modalities, such as speech (verbal), videos (facial expressions) and\nmotion sensors (body gestures). We propose a generalized approach to emotion\nrecognition that can adapt across modalities by modeling dynamic data as\nstructured graphs. The motivation behind the graph approach is to build compact\nmodels without compromising on performance. To alleviate the problem of optimal\ngraph construction, we cast this as a joint graph learning and classification\ntask. To this end, we present the Learnable Graph Inception Network (L-GrIN)\nthat jointly learns to recognize emotion and to identify the underlying graph\nstructure in the dynamic data. Our architecture comprises multiple novel\ncomponents: a new graph convolution operation, a graph inception layer,\nlearnable adjacency, and a learnable pooling function that yields a graph-level\nembedding. We evaluate the proposed architecture on five benchmark emotion\nrecognition databases spanning three different modalities (video, audio, motion\ncapture), where each database captures one of the following emotional cues:\nfacial expressions, speech and body gestures. We achieve state-of-the-art\nperformance on all five databases outperforming several competitive baselines\nand relevant existing methods. Our graph architecture shows superior\nperformance with significantly fewer parameters (compared to convolutional or\nrecurrent neural networks) promising its applicability to resource-constrained\ndevices.", "published": "2020-08-06 13:51:31", "link": "http://arxiv.org/abs/2008.02661v2", "categories": ["cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Attentive Fusion Enhanced Audio-Visual Encoding for Transformer Based\n  Robust Speech Recognition", "abstract": "Audio-visual information fusion enables a performance improvement in speech\nrecognition performed in complex acoustic scenarios, e.g., noisy environments.\nIt is required to explore an effective audio-visual fusion strategy for\naudiovisual alignment and modality reliability. Different from the previous\nend-to-end approaches where the audio-visual fusion is performed after encoding\neach modality, in this paper we propose to integrate an attentive fusion block\ninto the encoding process. It is shown that the proposed audio-visual fusion\nmethod in the encoder module can enrich audio-visual representations, as the\nrelevance between the two modalities is leveraged. In line with the\ntransformer-based architecture, we implement the embedded fusion block using a\nmulti-head attention based audiovisual fusion with one-way or two-way\ninteractions. The proposed method can sufficiently combine the two streams and\nweaken the over-reliance on the audio modality. Experiments on the LRS3-TED\ndataset demonstrate that the proposed method can increase the recognition rate\nby 0.55%, 4.51% and 4.61% on average under the clean, seen and unseen noise\nconditions, respectively, compared to the state-of-the-art approach.", "published": "2020-08-06 14:39:07", "link": "http://arxiv.org/abs/2008.02686v1", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Aalto's End-to-End DNN systems for the INTERSPEECH 2020 Computational\n  Paralinguistics Challenge", "abstract": "End-to-end neural network models (E2E) have shown significant performance\nbenefits on different INTERSPEECH ComParE tasks. Prior work has applied either\na single instance of an E2E model for a task or the same E2E architecture for\ndifferent tasks. However, applying a single model is unstable or using the same\narchitecture under-utilizes task-specific information. On ComParE 2020 tasks,\nwe investigate applying an ensemble of E2E models for robust performance and\ndeveloping task-specific modifications for each task. ComParE 2020 introduces\nthree sub-challenges: the breathing sub-challenge to predict the output of a\nrespiratory belt worn by a patient while speaking, the elderly sub-challenge to\nestimate the elderly speaker's arousal and valence levels and the mask\nsub-challenge to classify if the speaker is wearing a mask or not. On each of\nthese tasks, an ensemble outperforms the single E2E model. On the breathing\nsub-challenge, we study the impact of multi-loss strategies on task\nperformance. On the elderly sub-challenge, predicting the valence and arousal\nlevels prompts us to investigate multi-task training and implement data\nsampling strategies to handle class imbalance. On the mask sub-challenge, using\nan E2E system without feature engineering is competitive to feature-engineered\nbaselines and provides substantial gains when combined with feature-engineered\nbaselines.", "published": "2020-08-06 14:45:10", "link": "http://arxiv.org/abs/2008.02689v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Cross-Domain Singing Voice Conversion", "abstract": "We present a wav-to-wav generative model for the task of singing voice\nconversion from any identity. Our method utilizes both an acoustic model,\ntrained for the task of automatic speech recognition, together with melody\nextracted features to drive a waveform-based generator. The proposed generative\narchitecture is invariant to the speaker's identity and can be trained to\ngenerate target singers from unlabeled training data, using either speech or\nsinging sources. The model is optimized in an end-to-end fashion without any\nmanual supervision, such as lyrics, musical notes or parallel samples. The\nproposed approach is fully-convolutional and can generate audio in real-time.\nExperiments show that our method significantly outperforms the baseline methods\nwhile generating convincingly better audio samples than alternative attempts.", "published": "2020-08-06 18:29:11", "link": "http://arxiv.org/abs/2008.02830v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Respiratory Sound Classification Using Long-Short Term Memory", "abstract": "Developing a reliable sound detection and recognition system offers many\nbenefits and has many useful applications in different industries. This paper\nexamines the difficulties that exist when attempting to perform sound\nclassification as it relates to respiratory disease classification. Some\nmethods which have been employed such as independent component analysis and\nblind source separation are examined. Finally, an examination on the use of\ndeep learning and long short-term memory networks is performed in order to\nidentify how such a task can be implemented.", "published": "2020-08-06 23:11:57", "link": "http://arxiv.org/abs/2008.02900v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Phonological Features for 0-shot Multilingual Speech Synthesis", "abstract": "Code-switching---the intra-utterance use of multiple languages---is prevalent\nacross the world. Within text-to-speech (TTS), multilingual models have been\nfound to enable code-switching. By modifying the linguistic input to\nsequence-to-sequence TTS, we show that code-switching is possible for languages\nunseen during training, even within monolingual models. We use a small set of\nphonological features derived from the International Phonetic Alphabet (IPA),\nsuch as vowel height and frontness, consonant place and manner. This allows the\nmodel topology to stay unchanged for different languages, and enables new,\npreviously unseen feature combinations to be interpreted by the model. We show\nthat this allows us to generate intelligible, code-switched speech in a new\nlanguage at test time, including the approximation of sounds never seen in\ntraining.", "published": "2020-08-06 18:25:18", "link": "http://arxiv.org/abs/2008.04107v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Transfer Learning Method for Speech Emotion Recognition from Automatic\n  Speech Recognition", "abstract": "This paper presents a transfer learning method in speech emotion recognition\nbased on a Time-Delay Neural Network (TDNN) architecture. A major challenge in\nthe current speech-based emotion detection research is data scarcity. The\nproposed method resolves this problem by applying transfer learning techniques\nin order to leverage data from the automatic speech recognition (ASR) task for\nwhich ample data is available. Our experiments also show the advantage of\nspeaker-class adaptation modeling techniques by adopting identity-vector\n(i-vector) based features in addition to standard Mel-Frequency Cepstral\nCoefficient (MFCC) features.[1] We show the transfer learning models\nsignificantly outperform the other methods without pretraining on ASR. The\nexperiments performed on the publicly available IEMOCAP dataset which provides\n12 hours of motional speech data. The transfer learning was initialized by\nusing the Ted-Lium v.2 speech dataset providing 207 hours of audio with the\ncorresponding transcripts. We achieve the highest significantly higher accuracy\nwhen compared to state-of-the-art, using five-fold cross validation. Using only\nspeech, we obtain an accuracy 71.7% for anger, excitement, sadness, and\nneutrality emotion content.", "published": "2020-08-06 20:37:22", "link": "http://arxiv.org/abs/2008.02863v2", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
