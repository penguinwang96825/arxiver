{"title": "Web-scale Surface and Syntactic n-gram Features for Dependency Parsing", "abstract": "We develop novel first- and second-order features for dependency parsing\nbased on the Google Syntactic Ngrams corpus, a collection of subtree counts of\nparsed sentences from scanned books. We also extend previous work on surface\n$n$-gram features from Web1T to the Google Books corpus and from first-order to\nsecond-order, comparing and analysing performance over newswire and web\ntreebanks.\n  Surface and syntactic $n$-grams both produce substantial and complementary\ngains in parsing accuracy across domains. Our best system combines the two\nfeature sets, achieving up to 0.8% absolute UAS improvements on newswire and\n1.4% on web text.", "published": "2015-02-25 03:27:38", "link": "http://arxiv.org/abs/1502.07038v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking Sticks and Ambiguities with Adaptive Skip-gram", "abstract": "Recently proposed Skip-gram model is a powerful method for learning\nhigh-dimensional word representations that capture rich semantic relationships\nbetween words. However, Skip-gram as well as most prior work on learning word\nrepresentations does not take into account word ambiguity and maintain only\nsingle representation per word. Although a number of Skip-gram modifications\nwere proposed to overcome this limitation and learn multi-prototype word\nrepresentations, they either require a known number of word meanings or learn\nthem using greedy heuristic approaches. In this paper we propose the Adaptive\nSkip-gram model which is a nonparametric Bayesian extension of Skip-gram\ncapable to automatically learn the required number of representations for all\nwords at desired semantic resolution. We derive efficient online variational\nlearning algorithm for the model and empirically demonstrate its efficiency on\nword-sense induction task.", "published": "2015-02-25 17:15:56", "link": "http://arxiv.org/abs/1502.07257v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting a comparability mapping to improve bi-lingual data\n  categorization: a three-mode data analysis perspective", "abstract": "We address in this paper the co-clustering and co-classification of bilingual\ndata laying in two linguistic similarity spaces when a comparability measure\ndefining a mapping between these two spaces is available. A new approach that\nwe can characterized as a three-mode analysis scheme, is proposed to mix the\ncomparability measure with the two similarity measures. Our aim is to improve\njointly the accuracy of classification and clustering tasks performed in each\nof the two linguistic spaces, as well as the quality of the final alignment of\ncomparable clusters that can be obtained. We used first some purely synthetic\nrandom data sets to assess our formal similarity-comparability mixing model. We\nthen propose two variants of the comparability measure that has been defined by\n(Li and Gaussier 2010) in the context of bilingual lexicon extraction to adapt\nit to clustering or categorizing tasks. These two variant measures are\nsubsequently used to evaluate our similarity-comparability mixing model in the\ncontext of the co-classification and co-clustering of comparable textual data\nsets collected from Wikipedia categories for the English and French languages.\nOur experiments show clear improvements in clustering and classification\naccuracies when mixing comparability with similarity measures, with, as\nexpected, a higher robustness obtained when the two comparability variant\nmeasures that we propose are used. We believe that this approach is\nparticularly well suited for the construction of thematic comparable corpora of\ncontrollable quality.", "published": "2015-02-25 13:07:41", "link": "http://arxiv.org/abs/1502.07157v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
