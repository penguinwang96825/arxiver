{"title": "Limit Order Book Simulation and Trade Evaluation with $K$-Nearest-Neighbor Resampling", "abstract": "In this paper, we show how $K$-nearest neighbor ($K$-NN) resampling, an\noff-policy evaluation method proposed in \\cite{giegrich2023k}, can be applied\nto simulate limit order book (LOB) markets and how it can be used to evaluate\nand calibrate trading strategies. Using historical LOB data, we demonstrate\nthat our simulation method is capable of recreating realistic LOB dynamics and\nthat synthetic trading within the simulation leads to a market impact in line\nwith the corresponding literature. Compared to other statistical LOB simulation\nmethods, our algorithm has theoretical convergence guarantees under general\nconditions, does not require optimization, is easy to implement and\ncomputationally efficient. Furthermore, we show that in a benchmark comparison\nour method outperforms a deep learning-based algorithm for several key\nstatistics. In the context of a LOB with pro-rata type matching, we demonstrate\nhow our algorithm can calibrate the size of limit orders for a liquidation\nstrategy. Finally, we describe how $K$-NN resampling can be modified for\nchoices of higher dimensional state spaces.", "published": "2024-09-10 13:50:53", "link": "http://arxiv.org/abs/2409.06514v1", "categories": ["q-fin.TR", "cs.LG", "math.OC", "q-fin.ST", "stat.ML"], "primary_category": "q-fin.TR"}
{"title": "Deep Learning and Large Language Models for Audio and Text Analysis in\n  Predicting Suicidal Acts in Chinese Psychological Support Hotlines", "abstract": "Suicide is a pressing global issue, demanding urgent and effective preventive\ninterventions. Among the various strategies in place, psychological support\nhotlines had proved as a potent intervention method. Approximately two million\npeople in China attempt suicide annually, with many individuals making multiple\nattempts. Prompt identification and intervention for high-risk individuals are\ncrucial to preventing tragedies. With the rapid advancement of artificial\nintelligence (AI), especially the development of large-scale language models\n(LLMs), new technological tools have been introduced to the field of mental\nhealth. This study included 1284 subjects, and was designed to validate whether\ndeep learning models and LLMs, using audio and transcribed text from support\nhotlines, can effectively predict suicide risk. We proposed a simple LLM-based\npipeline that first summarizes transcribed text from approximately one hour of\nspeech to extract key features, and then predict suicidial bahaviours in the\nfuture. We compared our LLM-based method with the traditional manual scale\napproach in a clinical setting and with five advanced deep learning models.\nSurprisingly, the proposed simple LLM pipeline achieved strong performance on a\ntest set of 46 subjects, with an F1 score of 76\\% when combined with manual\nscale rating. This is 7\\% higher than the best speech-based deep learning\nmodels and represents a 27.82\\% point improvement in F1 score compared to using\nthe manual scale apporach alone. Our study explores new applications of LLMs\nand demonstrates their potential for future use in suicide prevention efforts.", "published": "2024-09-10 02:22:50", "link": "http://arxiv.org/abs/2409.06164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SubRegWeigh: Effective and Efficient Annotation Weighing with Subword\n  Regularization", "abstract": "NLP datasets may still contain annotation errors, even when they are manually\nannotated. Researchers have attempted to develop methods to automatically\nreduce the adverse effect of errors in datasets. However, existing methods are\ntime-consuming because they require many trained models to detect errors. This\npaper proposes a time-saving method that utilizes a tokenization technique\ncalled subword regularization to simulate multiple error detection models for\ndetecting errors. Our proposed method, SubRegWeigh, can perform annotation\nweighting four to five times faster than the existing method. Additionally,\nSubRegWeigh improved performance in document classification and named entity\nrecognition tasks. In experiments with pseudo-incorrect labels, SubRegWeigh\nclearly identifies pseudo-incorrect labels as annotation errors. Our code is\navailable at https://github.com/4ldk/SubRegWeigh .", "published": "2024-09-10 04:48:36", "link": "http://arxiv.org/abs/2409.06216v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inference is All You Need: Self Example Retriever for Cross-domain\n  Dialogue State Tracking with ChatGPT", "abstract": "Traditional dialogue state tracking approaches heavily rely on extensive\ntraining data and handcrafted features, limiting their scalability and\nadaptability to new domains. In this paper, we propose a novel method that\nleverages inference and in-context learning with ChatGPT for domain transfer in\ndialogue state tracking, without any parameter updates. By guiding ChatGPT's\nchain of thought, we enable it to retrieve relevant examples and generalize\nknowledge to accurately infer dialogue states, solely through inference.\nExperimental results on the MultiWOZ dataset demonstrate competitive\nperformance and promising generalization across domains. Our parameter-free\napproach offers a scalable and adaptable solution, opening new research\ndirections in domain transfer learning.", "published": "2024-09-10 06:24:46", "link": "http://arxiv.org/abs/2409.06243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Paragraphs from LLM Token Activations", "abstract": "Generative large language models (LLMs) excel in natural language processing\ntasks, yet their inner workings remain underexplored beyond token-level\npredictions. This study investigates the degree to which these models decide\nthe content of a paragraph at its onset, shedding light on their contextual\nunderstanding. By examining the information encoded in single-token\nactivations, specifically the \"\\textbackslash n\\textbackslash n\" double newline\ntoken, we demonstrate that patching these activations can transfer significant\ninformation about the context of the following paragraph, providing further\ninsights into the model's capacity to plan ahead.", "published": "2024-09-10 08:33:31", "link": "http://arxiv.org/abs/2409.06328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval Or Holistic Understanding? Dolce: Differentiate Our Long\n  Context Evaluation Tasks", "abstract": "We argue that there are two major distinct capabilities in long context\nunderstanding: retrieval and holistic understanding. Understanding and further\nimproving LLMs' long context capabilities would not be possible without knowing\nthe tasks' focus categories. We aim to automatically identify retrieval focused\nand holistic understanding focused problems from suites of benchmarks and\nquantitatively measure the difficulty within each focus. In this paper, we\npresent the Dolce framework, which parameterizes each problem by $\\lambda$\n(complexity) and $k$ (redundancy) and assigns to one of five predefined focus\ncategories. We propose to sample short contexts from the full context and\nestimate the probability an LLM solves the problem using the sampled spans. To\nfind the $\\lambda$ and $k$ for each problem, we further propose a mixture model\nof a non-parametric background noise component and a parametric/non-parametric\nhybrid oracle component, where we derive the probability functions\nparameterized by $\\lambda$ and $k$ for both the correct-or-wrong (COW) scenario\nand the partial-point-in-grading (PIG) scenario. Our proposed methods can\nidentify 0% to 67% of the problems are retrieval focused and 0% to 90% of the\nproblems are holistic understanding focused across 44 existing long context\nevaluation tasks.", "published": "2024-09-10 08:48:05", "link": "http://arxiv.org/abs/2409.06338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coarse-Grained Sense Inventories Based on Semantic Matching between\n  English Dictionaries", "abstract": "WordNet is one of the largest handcrafted concept dictionaries visualizing\nword connections through semantic relationships. It is widely used as a word\nsense inventory in natural language processing tasks. However, WordNet's\nfine-grained senses have been criticized for limiting its usability. In this\npaper, we semantically match sense definitions from Cambridge dictionaries and\nWordNet and develop new coarse-grained sense inventories. We verify the\neffectiveness of our inventories by comparing their semantic coherences with\nthat of Coarse Sense Inventory. The advantages of the proposed inventories\ninclude their low dependency on large-scale resources, better aggregation of\nclosely related senses, CEFR-level assignments, and ease of expansion and\nimprovement.", "published": "2024-09-10 10:08:58", "link": "http://arxiv.org/abs/2409.06386v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping News Narratives Using LLMs and Narrative-Structured Text\n  Embeddings", "abstract": "Given the profound impact of narratives across various societal levels, from\npersonal identities to international politics, it is crucial to understand\ntheir distribution and development over time. This is particularly important in\nonline spaces. On the Web, narratives can spread rapidly and intensify societal\ndivides and conflicts. While many qualitative approaches exist, quantifying\nnarratives remains a significant challenge. Computational narrative analysis\nlacks frameworks that are both comprehensive and generalizable. To address this\ngap, we introduce a numerical narrative representation grounded in\nstructuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a\nnarrative through a constellation of six functional character roles. These\nso-called actants are genre-agnostic, making the model highly generalizable. We\nextract the actants using an open-source LLM and integrate them into a\nNarrative-Structured Text Embedding that captures both the semantics and\nnarrative structure of a text. We demonstrate the analytical insights of the\nmethod on the example of 5000 full-text news articles from Al Jazeera and The\nWashington Post on the Israel-Palestine conflict. Our method successfully\ndistinguishes articles that cover the same topics but differ in narrative\nstructure.", "published": "2024-09-10 14:15:30", "link": "http://arxiv.org/abs/2409.06540v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From LIMA to DeepLIMA: following a new path of interoperability", "abstract": "In this article, we describe the architecture of the LIMA (Libre Multilingual\nAnalyzer) framework and its recent evolution with the addition of new text\nanalysis modules based on deep neural networks. We extended the functionality\nof LIMA in terms of the number of supported languages while preserving existing\nconfigurable architecture and the availability of previously developed\nrule-based and statistical analysis components. Models were trained for more\nthan 60 languages on the Universal Dependencies 2.5 corpora, WikiNer corpora,\nand CoNLL-03 dataset. Universal Dependencies allowed us to increase the number\nof supported languages and to generate models that could be integrated into\nother platforms. This integration of ubiquitous Deep Learning Natural Language\nProcessing models and the use of standard annotated collections using Universal\nDependencies can be viewed as a new path of interoperability, through the\nnormalization of models and data, that are complementary to a more standard\ntechnical interoperability, implemented in LIMA through services available in\nDocker containers on Docker Hub.", "published": "2024-09-10 14:26:12", "link": "http://arxiv.org/abs/2409.06550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring syntactic information in sentence embeddings through\n  multilingual subject-verb agreement", "abstract": "In this paper, our goal is to investigate to what degree multilingual\npretrained language models capture cross-linguistically valid abstract\nlinguistic representations. We take the approach of developing curated\nsynthetic data on a large scale, with specific properties, and using them to\nstudy sentence representations built using pretrained language models. We use a\nnew multiple-choice task and datasets, Blackbird Language Matrices (BLMs), to\nfocus on a specific grammatical structural phenomenon -- subject-verb agreement\nacross a variety of sentence structures -- in several languages. Finding a\nsolution to this task requires a system detecting complex linguistic patterns\nand paradigms in text representations. Using a two-level architecture that\nsolves the problem in two steps -- detect syntactic objects and their\nproperties in individual sentences, and find patterns across an input sequence\nof sentences -- we show that despite having been trained on multilingual texts\nin a consistent manner, multilingual pretrained language models have\nlanguage-specific differences, and syntactic structure is not shared, even\nacross closely related languages.", "published": "2024-09-10 14:58:55", "link": "http://arxiv.org/abs/2409.06567v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations.", "published": "2024-09-10 15:39:32", "link": "http://arxiv.org/abs/2409.06595v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploring Italian sentence embeddings properties through multi-tasking", "abstract": "We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings.", "published": "2024-09-10 16:22:18", "link": "http://arxiv.org/abs/2409.06622v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TeXBLEU: Automatic Metric for Evaluate LaTeX Format", "abstract": "LaTeX is suitable for creating specially formatted documents in science,\ntechnology, mathematics, and computer science. Although the use of mathematical\nexpressions in LaTeX format along with language models is increasing, there are\nno proper evaluation matrices to evaluate them. In this study, we propose\nTeXBLEU, a metric for evaluating mathematical expressions in the LaTeX format\nbuilt on the n-gram-based BLEU metric widely used in translation tasks. The\nproposed TeXBLEU consists of a predefined tokenizer trained on the arXiv paper\ndataset and a fine-tuned embedding model with positional encoding. The TeXBLEU\nscore was calculated by replacing BLUE's modified precision score with the\nsimilarity of n-gram-based tokens. TeXBLEU showed improvements of 86\\%, 121\\%,\nand 610\\% over traditional evaluation metrics, such as BLEU, sacreBLEU, and\nRouge, respectively, on the MathBridge dataset with 1,000 data points. The code\nis available at https://github.com/KyuDan1/TeXBLEU.", "published": "2024-09-10 16:54:32", "link": "http://arxiv.org/abs/2409.06639v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "E2LLM: Encoder Elongated Large Language Models for Long-Context\n  Understanding and Reasoning", "abstract": "In the realm of Large Language Models (LLMs), the ability to process long\ncontexts is increasingly crucial for tasks such as multi-round dialogues, code\ngeneration, and document summarization. This paper addresses the challenges of\nenhancing the long-context performance, reducing computational complexity, and\nleveraging pretrained models collectively termed the \"impossible triangle.\" We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. The method involves splitting long\ncontexts into chunks, compressing each into embedding vectors via a pretrained\ntext encoder, and utilizing an adapter to align these representations with a\ndecoder-only LLM. Two training objectives, focusing on reconstruction of the\nencoder output and long-context instruction fine-tuning, are employed to\nfacilitate the understanding of soft prompts by the LLM. Experimental results\ndemonstrate that E2LLM achieves superior performance in long-context scenarios\nwhile balancing efficiency, performance, and compatibility with pretrained\nmodels. Our framework thus represents a significant advancement in the field,\ncontributing to effective long-text modeling.", "published": "2024-09-10 17:44:35", "link": "http://arxiv.org/abs/2409.06679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translating Step-by-Step: Decomposing the Translation Process for\n  Improved Translation Quality of Long-Form Texts", "abstract": "In this paper we present a step-by-step approach to long-form text\ntranslation, drawing on established processes in translation studies. Instead\nof viewing machine translation as a single, monolithic task, we propose a\nframework that engages language models in a multi-turn interaction,\nencompassing pre-translation research, drafting, refining, and proofreading,\nresulting in progressively improved translations. Extensive automatic\nevaluations using Gemini 1.5 Pro across ten language pairs show that\ntranslating step-by-step yields large translation quality improvements over\nconventional zero-shot prompting approaches and earlier human-like baseline\nstrategies, resulting in state-of-the-art results on WMT2024.", "published": "2024-09-10 18:02:21", "link": "http://arxiv.org/abs/2409.06790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PingPong: A Benchmark for Role-Playing Language Models with User\n  Emulation and Multi-Model Evaluation", "abstract": "We introduce a benchmark for evaluating the role-playing capabilities of\nlanguage models. Our approach leverages different language models to simulate\nusers in dynamic, multi-turn conversations and assess the resulting dialogues.\nOur methodology involves three main components: a player model that adopts a\nspecific character role, an interrogator model that simulates user behavior in\na specific situation, and a judge model ensemble that evaluates conversation\nquality with 3 metrics: character consistency, entertainment value, and\nlanguage fluency. We evaluated more than 40 models in both English and Russian,\nwith each model participating in 64 conversations with 8 characters and 8\nsituations. We conducted experiments comparing automated evaluations with human\nannotations to validate our approach, demonstrating strong correlations across\nmultiple criteria. This work provides a foundation for a robust and dynamic\nevaluation of different model capabilities in interactive scenarios.", "published": "2024-09-10 19:00:44", "link": "http://arxiv.org/abs/2409.06820v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What is the Role of Small Models in the LLM Era: A Survey", "abstract": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models", "published": "2024-09-10 20:45:43", "link": "http://arxiv.org/abs/2409.06857v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Socially Unacceptable Discourse with Zero-shot Learning", "abstract": "Socially Unacceptable Discourse (SUD) analysis is crucial for maintaining\nonline positive environments. We investigate the effectiveness of\nEntailment-based zero-shot text classification (unsupervised method) for SUD\ndetection and characterization by leveraging pre-trained transformer models and\nprompting techniques. The results demonstrate good generalization capabilities\nof these models to unseen data and highlight the promising nature of this\napproach for generating labeled datasets for the analysis and characterization\nof extremist narratives. The findings of this research contribute to the\ndevelopment of robust tools for studying SUD and promoting responsible\ncommunication online.", "published": "2024-09-10 07:32:00", "link": "http://arxiv.org/abs/2409.13735v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP4PBM: A Systematic Review on Process Extraction using Natural\n  Language Processing with Rule-based, Machine and Deep Learning Methods", "abstract": "This literature review studies the field of automated process extraction,\ni.e., transforming textual descriptions into structured processes using Natural\nLanguage Processing (NLP). We found that Machine Learning (ML) / Deep Learning\n(DL) methods are being increasingly used for the NLP component. In some cases,\nthey were chosen for their suitability towards process extraction, and results\nshow that they can outperform classic rule-based methods. We also found a\npaucity of gold-standard, scalable annotated datasets, which currently hinders\nobjective evaluations as well as the training or fine-tuning of ML / DL\nmethods. Finally, we discuss preliminary work on the application of LLMs for\nautomated process extraction, as well as promising developments in this field.", "published": "2024-09-10 15:16:02", "link": "http://arxiv.org/abs/2409.13738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Table-to-Text Generation with Pretrained Diffusion Models", "abstract": "Diffusion models have demonstrated significant potential in achieving\nstate-of-the-art performance across various text generation tasks. In this\nsystematic study, we investigate their application to the table-to-text problem\nby adapting the diffusion model to the task and conducting an in-depth\nanalysis. Our experiments cover multiple aspects of diffusion models training.\nWe explore sampling strategy influence by inducing recent diffusion model\naccelerator DPM-Solver++ into our core model. We have tested different\nprediction aggregation methods, like ROVER and Minimum Bayes-Risk (MBR). Our\nstudies cover the impact of the pre-training phase in diffusion models and the\ngeneration length constraints influence. We also have compared diffusion model\ngeneration with auto-regressive text-to-text models with different temperature\nsettings for diversity evaluation. Our key observation is that diffusion models\ndemonstrate the balance between quality and diversity while auto-regressive\ntext-to-text models are not successful at handling both at the same time.\nFurthermore, we found out that to achieve the highest quality possible, it is\npreferable to use a regular sampler with the strictest length constraint to\ncreate multiple samples, and then use MBR to aggregate the predictions.\nHowever, if you are prepared to give up high level of diversity and to\naccelerate the process, you can also utilize a fast sampler DPM-Solver++. Our\nfindings reveal that diffusion models achieve comparable results in the\ntable-to-text domain, highlighting their viability in the table-to-text\nchallenge as a promising research direction.", "published": "2024-09-10 15:36:53", "link": "http://arxiv.org/abs/2409.13739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTScoreVisualizer: A Web Tool for Understanding Simplified Text\n  Evaluation with BERTScore", "abstract": "The BERTScore metric is commonly used to evaluate automatic text\nsimplification systems. However, current implementations of the metric fail to\nprovide complete visibility into all information the metric can produce.\nNotably, the specific token matchings can be incredibly useful in generating\nclause-level insight into the quality of simplified text. We address this by\nintroducing BERTScoreVisualizer, a web application that goes beyond reporting\nprecision, recall, and F1 score and provides a visualization of the matching\nbetween tokens. We believe that our software can help improve the analysis of\ntext simplification systems by specifically showing where generated, simplified\ntext deviates from reference text. We host our code and demo on GitHub.", "published": "2024-09-10 22:15:50", "link": "http://arxiv.org/abs/2409.17160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn,\n  Focus, and Review", "abstract": "Traditional Large Language Model (LLM) pretraining relies on autoregressive\nlanguage modeling with randomly sampled data from web-scale datasets. Inspired\nby human learning techniques like spaced repetition, we hypothesize that random\nsampling leads to high training costs, lower-quality models, and significant\ndata forgetting. To address these inefficiencies, we propose the\nLearn-Focus-Review (LFR) paradigm -- a dynamic training approach that adapts to\nthe model's learning progress. LFR tracks the model's learning performance\nacross data blocks (sequences of tokens) and prioritizes revisiting challenging\nregions of the dataset that are more prone to being forgotten, enabling better\nretention and more efficient learning. Using the LFR paradigm, we pretrained\nLlama and GPT models on the SlimPajama and OpenWebText datasets, respectively.\nThese models were evaluated on downstream tasks across various domains,\nincluding question answering, problem-solving, commonsense reasoning, language\nmodeling, and translation. Compared to baseline models trained on the full\ndatasets, LFR consistently achieved lower perplexity and higher accuracy, while\nusing only 5%--19% of the training tokens. Furthermore, LFR matched the\nperformance of industry-standard Pythia models with up to 2$\\times$ the\nparameter count, using just 3.2% of the training tokens, demonstrating its\neffectiveness and efficiency.", "published": "2024-09-10 00:59:18", "link": "http://arxiv.org/abs/2409.06131v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks", "abstract": "In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.", "published": "2024-09-10 03:06:17", "link": "http://arxiv.org/abs/2409.06173v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SQLucid: Grounding Natural Language Database Queries with Interactive\n  Explanations", "abstract": "Though recent advances in machine learning have led to significant\nimprovements in natural language interfaces for databases, the accuracy and\nreliability of these systems remain limited, especially in high-stakes domains.\nThis paper introduces SQLucid, a novel user interface that bridges the gap\nbetween non-expert users and complex database querying processes. SQLucid\naddresses existing limitations by integrating visual correspondence,\nintermediate query results, and editable step-by-step SQL explanations in\nnatural language to facilitate user understanding and engagement. This unique\nblend of features empowers users to understand and refine SQL queries easily\nand precisely. Two user studies and one quantitative experiment were conducted\nto validate SQLucid's effectiveness, showing significant improvement in task\ncompletion accuracy and user confidence compared to existing interfaces. Our\ncode is available at https://github.com/magic-YuanTian/SQLucid.", "published": "2024-09-10 03:14:09", "link": "http://arxiv.org/abs/2409.06178v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "NOVI : Chatbot System for University Novice with BERT and LLMs", "abstract": "To mitigate the difficulties of university freshmen in adapting to university\nlife, we developed NOVI, a chatbot system based on GPT-4o. This system utilizes\npost and comment data from SKKU 'Everytime', a university community site.\nDeveloped using LangChain, NOVI's performance has been evaluated with a BLEU\nscore, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR\nscore. This approach is not only limited to help university freshmen but is\nalso expected to help various people adapting to new environments with\ndifferent data. This research explores the development and potential\napplication of new educational technology tools, contributing to easier social\nadaptation for beginners and settling a foundation for future advancement in\nLLM studies.", "published": "2024-09-10 03:43:26", "link": "http://arxiv.org/abs/2409.06192v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing\n  Behaviors with LLMs", "abstract": "This paper introduces text-to-shape-display, a novel approach to generating\ndynamic shape changes in pin-based shape displays through natural language\ncommands. By leveraging large language models (LLMs) and AI-chaining, our\napproach allows users to author shape-changing behaviors on demand through text\nprompts without programming. We describe the foundational aspects necessary for\nsuch a system, including the identification of key generative elements\n(primitive, animation, and interaction) and design requirements to enhance user\ninteraction, based on formative exploration and iterative design processes.\nBased on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a\n24 x 24 shape display, which translates the user's textual command into\nexecutable code and allows for quick exploration through a web-based control\ninterface. We evaluate the effectiveness of SHAPE-IT in two ways: 1)\nperformance evaluation and 2) user evaluation (N= 10). The study conclusions\nhighlight the ability to facilitate rapid ideation of a wide range of\nshape-changing behaviors with AI. However, the findings also expose\naccuracy-related challenges and limitations, prompting further exploration into\nrefining the framework for leveraging AI to better suit the unique requirements\nof shape-changing systems.", "published": "2024-09-10 04:18:49", "link": "http://arxiv.org/abs/2409.06205v1", "categories": ["cs.HC", "cs.CL", "H.5.2"], "primary_category": "cs.HC"}
{"title": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning", "abstract": "Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available.", "published": "2024-09-10 04:34:42", "link": "http://arxiv.org/abs/2409.06211v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Keyword-Aware ASR Error Augmentation for Robust Dialogue State Tracking", "abstract": "Dialogue State Tracking (DST) is a key part of task-oriented dialogue\nsystems, identifying important information in conversations. However, its\naccuracy drops significantly in spoken dialogue environments due to named\nentity errors from Automatic Speech Recognition (ASR) systems. We introduce a\nsimple yet effective data augmentation method that targets those entities to\nimprove the robustness of DST model. Our novel method can control the placement\nof errors using keyword-highlighted prompts while introducing phonetically\nsimilar errors. As a result, our method generated sufficient error patterns on\nkeywords, leading to improved accuracy in noised and low-accuracy ASR\nenvironments.", "published": "2024-09-10 07:06:40", "link": "http://arxiv.org/abs/2409.06263v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Sequential Recommendations through Multi-Perspective\n  Reflections and Iteration", "abstract": "Sequence recommendation (SeqRec) aims to predict the next item a user will\ninteract with by understanding user intentions and leveraging collaborative\nfiltering information. Large language models (LLMs) have shown great promise in\nrecommendation tasks through prompt-based, fixed reflection libraries, and\nfine-tuning techniques. However, these methods face challenges, including lack\nof supervision, inability to optimize reflection sources, inflexibility to\ndiverse user needs, and high computational costs. Despite promising results,\ncurrent studies primarily focus on reflections of users' explicit preferences\n(e.g., item titles) while neglecting implicit preferences (e.g., brands) and\ncollaborative filtering information. This oversight hinders the capture of\npreference shifts and dynamic user behaviors. Additionally, existing approaches\nlack mechanisms for reflection evaluation and iteration, often leading to\nsuboptimal recommendations. To address these issues, we propose the Mixture of\nREflectors (MoRE) framework, designed to model and learn dynamic user\npreferences in SeqRec. Specifically, MoRE introduces three reflectors for\ngenerating LLM-based reflections on explicit preferences, implicit preferences,\nand collaborative signals. Each reflector incorporates a self-improving\nstrategy, termed refining-and-iteration, to evaluate and iteratively update\nreflections. Furthermore, a meta-reflector employs a contextual bandit\nalgorithm to select the most suitable expert and corresponding reflections for\neach user's recommendation, effectively capturing dynamic preferences.\nExtensive experiments on three real-world datasets demonstrate that MoRE\nconsistently outperforms state-of-the-art methods, requiring less training time\nand GPU memory compared to other LLM-based approaches in SeqRec.", "published": "2024-09-10 09:58:55", "link": "http://arxiv.org/abs/2409.06377v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Length Desensitization in Direct Preference Optimization", "abstract": "Direct Preference Optimization (DPO) is widely utilized in the Reinforcement\nLearning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)\nwith human preferences, thereby enhancing both their harmlessness and efficacy.\nHowever, it has been observed that DPO tends to over-optimize for verbosity,\nwhich can detrimentally affect both performance and user experience. In this\npaper, we conduct an in-depth theoretical analysis of DPO's optimization\nobjective and reveal a strong correlation between its implicit reward and data\nlength. This correlation misguides the optimization direction, resulting in\nlength sensitivity during the DPO training and leading to verbosity. To address\nthis issue, we propose a length-desensitization improvement method for DPO,\ntermed LD-DPO. The proposed method aims to desensitize DPO to data length by\ndecoupling explicit length preference, which is relatively insignificant, from\nthe other implicit preferences, thereby enabling more effective learning of the\nintrinsic preferences. We utilized two settings (Base and Instruct) of\nLlama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various\nbenchmarks including MT-Bench and AlpacaEval 2. The experimental results\nindicate that LD-DPO consistently outperforms DPO and other baseline methods,\nachieving more concise responses with a 10-40% reduction in length compared to\nDPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can\nindeed achieve length desensitization and align the model more closely with\nhuman-like preferences.", "published": "2024-09-10 10:49:38", "link": "http://arxiv.org/abs/2409.06411v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Questioning Internal Knowledge Structure of Large Language Models\n  Through the Lens of the Olympic Games", "abstract": "Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs.", "published": "2024-09-10 13:54:04", "link": "http://arxiv.org/abs/2409.06518v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Alleviating Hallucinations in Large Language Models with Scepticism\n  Modeling", "abstract": "Hallucinations is a major challenge for large language models (LLMs),\nprevents adoption in diverse fields. Uncertainty estimation could be used for\nalleviating the damages of hallucinations. The skeptical emotion of human could\nbe useful for enhancing the ability of self estimation. Inspirited by this\nobservation, we proposed a new approach called Skepticism Modeling (SM). This\napproach is formalized by combining the information of token and logits for\nself estimation. We construct the doubt emotion aware data, perform continual\npre-training, and then fine-tune the LLMs, improve their ability of self\nestimation. Experimental results demonstrate this new approach effectively\nenhances a model's ability to estimate their uncertainty, and validate its\ngeneralization ability of other tasks by out-of-domain experiments.", "published": "2024-09-10 15:51:15", "link": "http://arxiv.org/abs/2409.06601v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large\n  Language Model", "abstract": "Large language models (LLMs) have demonstrated significant capabilities in\nmathematical reasoning, particularly with text-based mathematical problems.\nHowever, current multi-modal large language models (MLLMs), especially those\nspecialized in mathematics, tend to focus predominantly on solving geometric\nproblems but ignore the diversity of visual information available in other\nareas of mathematics. Moreover, the geometric information for these specialized\nmathematical MLLMs is derived from several public datasets, which are typically\nlimited in diversity and complexity. To address these limitations, we aim to\nconstruct a fine-tuning dataset named MathVL, and develop a series of\nspecialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised\nFine-Tuning (SFT) on MathVL with various parameter-scale backbones. To\nextensively evaluate the effectiveness of MathGLM-Vision, we conduct\nexperiments on several public benchmarks and our curated MathVL-test consisting\nof 2,000 problems. Experimental results demonstrate that MathGLM-Vision\nachieves significant improvements compared with some existing models, including\nbackbone models and open-source mathematical MLLMs. These findings indicate the\nimportance of diversity dataset in enhancing the mathematical reasoning\nabilities of MLLMs.", "published": "2024-09-10 01:20:22", "link": "http://arxiv.org/abs/2409.13729v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VisScience: An Extensive Benchmark for Evaluating K12 Educational\n  Multi-modal Scientific Reasoning", "abstract": "Multi-modal large language models (MLLMs) have demonstrated promising\ncapabilities across various tasks by integrating textual and visual information\nto achieve visual understanding in complex scenarios. Despite the availability\nof several benchmarks aims to evaluating MLLMs in tasks from visual question\nanswering to complex problem-solving, most focus predominantly on mathematics\nor general visual understanding tasks. This reveals a critical gap in current\nbenchmarks, which often overlook the inclusion of other key scientific\ndisciplines such as physics and chemistry. To address this gap, we meticulously\nconstruct a comprehensive benchmark, named VisScience, which is utilized to\nassess the multi-modal scientific reasoning across the three disciplines of\nmathematics, physics, and chemistry. This benchmark comprises 3,000 questions\ndrawn from K12 education - spanning elementary school through high school -\nequally distributed across three disciplines, with 1,000 questions per\ndiscipline. The questions within VisScience span 21 distinct subjects and are\ncategorized into five difficulty levels, offering a broad spectrum of topics\nwithin each discipline. With VisScience, we present a detailed evaluation of\nthe performance of 25 representative MLLMs in scientific reasoning.\nExperimental results demonstrate that closed-source MLLMs generally outperform\nopen-source models. The best performance observed include a 53.4\\% accuracy in\nmathematics by Claude3.5-Sonnet, 38.2\\% in physics by GPT-4o, and 47.0\\% in\nchemistry by Gemini-1.5-Pro. These results underscore the strengths and\nlimitations of MLLMs, suggesting areas for future improvement and highlighting\nthe importance of developing models that can effectively handle the diverse\ndemands of multi-modal scientific reasoning.", "published": "2024-09-10 01:20:26", "link": "http://arxiv.org/abs/2409.13730v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation", "abstract": "The recently developed retrieval-augmented generation (RAG) technology has\nenabled the efficient construction of domain-specific applications. However, it\nalso has limitations, including the gap between vector similarity and the\nrelevance of knowledge reasoning, as well as insensitivity to knowledge logic,\nsuch as numerical values, temporal relations, expert rules, and others, which\nhinder the effectiveness of professional knowledge services. In this work, we\nintroduce a professional domain knowledge service framework called Knowledge\nAugmented Generation (KAG). KAG is designed to address the aforementioned\nchallenges with the motivation of making full use of the advantages of\nknowledge graph(KG) and vector retrieval, and to improve generation and\nreasoning performance by bidirectionally enhancing large language models (LLMs)\nand KGs through five key aspects: (1) LLM-friendly knowledge representation,\n(2) mutual-indexing between knowledge graphs and original chunks, (3)\nlogical-form-guided hybrid reasoning engine, (4) knowledge alignment with\nsemantic reasoning, and (5) model capability enhancement for KAG. We compared\nKAG with existing RAG methods in multihop question answering and found that it\nsignificantly outperforms state-of-theart methods, achieving a relative\nimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We\nhave successfully applied KAG to two professional knowledge Q&A tasks of Ant\nGroup, including E-Government Q&A and E-Health Q&A, achieving significant\nimprovement in professionalism compared to RAG methods.", "published": "2024-09-10 02:00:28", "link": "http://arxiv.org/abs/2409.13731v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowing When to Ask -- Bridging Large Language Models and Data", "abstract": "Large Language Models (LLMs) are prone to generating factually incorrect\ninformation when responding to queries that involve numerical and statistical\ndata or other timely facts. In this paper, we present an approach for enhancing\nthe accuracy of LLMs by integrating them with Data Commons, a vast, open-source\nrepository of public statistics from trusted organizations like the United\nNations (UN), Center for Disease Control and Prevention (CDC) and global census\nbureaus. We explore two primary methods: Retrieval Interleaved Generation\n(RIG), where the LLM is trained to produce natural language queries to retrieve\ndata from Data Commons, and Retrieval Augmented Generation (RAG), where\nrelevant data tables are fetched from Data Commons and used to augment the\nLLM's prompt. We evaluate these methods on a diverse set of queries,\ndemonstrating their effectiveness in improving the factual accuracy of LLM\noutputs. Our work represents an early step towards building more trustworthy\nand reliable LLMs that are grounded in verifiable statistical data and capable\nof complex factual reasoning.", "published": "2024-09-10 17:51:21", "link": "http://arxiv.org/abs/2409.13741v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Large Dataset of Spontaneous Speech with the Accent Spoken in S\u00e3o\n  Paulo for Automatic Speech Recognition Evaluation", "abstract": "We present a freely available spontaneous speech corpus for the Brazilian\nPortuguese language and report preliminary automatic speech recognition (ASR)\nresults, using both the Wav2Vec2-XLSR-53 and Distil-Whisper models fine-tuned\nand trained on our corpus. The NURC-SP Audio Corpus comprises 401 different\nspeakers (204 females, 197 males) with a total of 239.30 hours of transcribed\naudio recordings. To the best of our knowledge, this is the first large\nPaulistano accented spontaneous speech corpus dedicated to the ASR task in\nPortuguese. We first present the design and development procedures of the\nNURC-SP Audio Corpus, and then describe four ASR experiments in detail. The\nexperiments demonstrated promising results for the applicability of the corpus\nfor ASR. Specifically, we fine-tuned two versions of Wav2Vec2-XLSR-53 model,\ntrained a Distil-Whisper model using our dataset with labels determined by\nWhisper Large-V3 model, and fine-tuned this Distil-Whisper model with our\ncorpus. Our best results were the Distil-Whisper fine-tuned over NURC-SP Audio\nCorpus with a WER of 24.22% followed by a fine-tuned versions of\nWav2Vec2-XLSR-53 model with a WER of 33.73%, that is almost 10% point worse\nthan Distil-Whisper's. To enable experiment reproducibility, we share the\nNURC-SP Audio Corpus dataset, pre-trained models, and training recipes in\nHugging-Face and Github repositories.", "published": "2024-09-10 21:45:06", "link": "http://arxiv.org/abs/2409.15350v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Advancing Topic Segmentation of Broadcasted Speech with Multilingual\n  Semantic Embeddings", "abstract": "Recent advancements in speech-based topic segmentation have highlighted the\npotential of pretrained speech encoders to capture semantic representations\ndirectly from speech. Traditionally, topic segmentation has relied on a\npipeline approach in which transcripts of the automatic speech recognition\nsystems are generated, followed by text-based segmentation algorithms. In this\npaper, we introduce an end-to-end scheme that bypasses this conventional\ntwo-step process by directly employing semantic speech encoders for\nsegmentation. Focused on the broadcasted news domain, which poses unique\nchallenges due to the diversity of speakers and topics within single\nrecordings, we address the challenge of accessing topic change points\nefficiently in an end-to-end manner. Furthermore, we propose a new benchmark\nfor spoken news topic segmentation by utilizing a dataset featuring\napproximately 1000 hours of publicly available recordings across six European\nlanguages and including an evaluation set in Hindi to test the model's\ncross-domain performance in a cross-lingual, zero-shot scenario. This setup\nreflects real-world diversity and the need for models adapting to various\nlinguistic settings. Our results demonstrate that while the traditional\npipeline approach achieves a state-of-the-art $P_k$ score of 0.2431 for\nEnglish, our end-to-end model delivers a competitive $P_k$ score of 0.2564.\nWhen trained multilingually, these scores further improve to 0.1988 and 0.2370,\nrespectively. To support further research, we release our model along with data\npreparation scripts, facilitating open research on multilingual spoken news\ntopic segmentation.", "published": "2024-09-10 05:24:36", "link": "http://arxiv.org/abs/2409.06222v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models", "abstract": "The Audio Question Answering (AQA) task includes audio event classification,\naudio captioning, and open-ended reasoning. Recently, AQA has garnered\nattention due to the advent of Large Audio Language Models (LALMs). Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext-only Large Language Models (LLMs) through a projection module. While LALMs\nexcel in general audio understanding, they are limited in temporal reasoning,\nwhich may hinder their commercial applications and on-device deployment. This\npaper addresses these challenges and limitations in audio temporal reasoning.\nFirst, we introduce a data augmentation technique for generating reliable audio\ntemporal questions and answers using an LLM. Second, we perform a further\nfine-tuning of an existing baseline using curriculum learning strategy to\nspecialize in temporal reasoning without compromising performance on fine-tuned\ntasks. We demonstrate the performance of our model using state-of-the-art LALMs\non public audio benchmark datasets. Third, we implement our AQA model on-device\nlocally and investigate its CPU inference for edge applications.", "published": "2024-09-10 05:26:53", "link": "http://arxiv.org/abs/2409.06223v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NLP-Powered Repository and Search Engine for Academic Papers: A Case\n  Study on Cyber Risk Literature with CyLit", "abstract": "As the body of academic literature continues to grow, researchers face\nincreasing difficulties in effectively searching for relevant resources.\nExisting databases and search engines often fall short of providing a\ncomprehensive and contextually relevant collection of academic literature. To\naddress this issue, we propose a novel framework that leverages Natural\nLanguage Processing (NLP) techniques. This framework automates the retrieval,\nsummarization, and clustering of academic literature within a specific research\ndomain. To demonstrate the effectiveness of our approach, we introduce CyLit,\nan NLP-powered repository specifically designed for the cyber risk literature.\nCyLit empowers researchers by providing access to context-specific resources\nand enabling the tracking of trends in the dynamic and rapidly evolving field\nof cyber risk. Through the automatic processing of large volumes of data, our\nNLP-powered solution significantly enhances the efficiency and specificity of\nacademic literature searches. We compare the literature categorization results\nof CyLit to those presented in survey papers or generated by ChatGPT,\nhighlighting the distinctive insights this tool provides into cyber risk\nresearch literature. Using NLP techniques, we aim to revolutionize the way\nresearchers discover, analyze, and utilize academic resources, ultimately\nfostering advancements in various domains of knowledge.", "published": "2024-09-10 05:41:40", "link": "http://arxiv.org/abs/2409.06226v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "SpeechTaxi: On Multilingual Semantic Speech Classification", "abstract": "Recent advancements in multilingual speech encoding as well as transcription\nraise the question of the most effective approach to semantic speech\nclassification. Concretely, can (1) end-to-end (E2E) classifiers obtained by\nfine-tuning state-of-the-art multilingual speech encoders (MSEs) match or\nsurpass the performance of (2) cascading (CA), where speech is first\ntranscribed into text and classification is delegated to a text-based\nclassifier. To answer this, we first construct SpeechTaxi, an 80-hour\nmultilingual dataset for semantic speech classification of Bible verses,\ncovering 28 diverse languages. We then leverage SpeechTaxi to conduct a wide\nrange of experiments comparing E2E and CA in monolingual semantic speech\nclassification as well as in cross-lingual transfer. We find that E2E based on\nMSEs outperforms CA in monolingual setups, i.e., when trained on in-language\ndata. However, MSEs seem to have poor cross-lingual transfer abilities, with\nE2E substantially lagging CA both in (1) zero-shot transfer to languages unseen\nin training and (2) multilingual training, i.e., joint training on multiple\nlanguages. Finally, we devise a novel CA approach based on transcription to\nRomanized text as a language-agnostic intermediate representation and show that\nit represents a robust solution for languages without native ASR support. Our\nSpeechTaxi dataset is publicly available at: https://huggingface.co/\ndatasets/LennartKeller/SpeechTaxi/.", "published": "2024-09-10 09:56:15", "link": "http://arxiv.org/abs/2409.06372v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for\n  Scholarly Knowledge Organization", "abstract": "The increasing amount of published scholarly articles, exceeding 2.5 million\nyearly, raises the challenge for researchers in following scientific progress.\nIntegrating the contributions from scholarly articles into a novel type of\ncognitive knowledge graph (CKG) will be a crucial element for accessing and\norganizing scholarly knowledge, surpassing the insights provided by titles and\nabstracts. This research focuses on effectively conveying structured scholarly\nknowledge by utilizing large language models (LLMs) to categorize scholarly\narticles and describe their contributions in a structured and comparable\nmanner. While previous studies explored language models within specific\nresearch domains, the extensive domain-independent knowledge captured by LLMs\noffers a substantial opportunity for generating structured contribution\ndescriptions as CKGs. Additionally, LLMs offer customizable pathways through\nprompt engineering or fine-tuning, thus facilitating to leveraging of smaller\nLLMs known for their efficiency, cost-effectiveness, and environmental\nconsiderations. Our methodology involves harnessing LLM knowledge, and\ncomplementing it with domain expert-verified scholarly data sourced from a CKG.\nThis strategic fusion significantly enhances LLM performance, especially in\ntasks like scholarly article categorization and predicate recommendation. Our\nmethod involves fine-tuning LLMs with CKG knowledge and additionally injecting\nknowledge from a CKG with a novel prompting technique significantly increasing\nthe accuracy of scholarly knowledge extraction. We integrated our approach in\nthe Open Research Knowledge Graph (ORKG), thus enabling precise access to\norganized scholarly knowledge, crucially benefiting domain-independent\nscholarly knowledge exchange and dissemination among policymakers, industrial\npractitioners, and the general public.", "published": "2024-09-10 11:31:02", "link": "http://arxiv.org/abs/2409.06433v1", "categories": ["cs.DL", "cs.CL", "cs.LG"], "primary_category": "cs.DL"}
{"title": "An Effective Context-Balanced Adaptation Approach for Long-Tailed Speech\n  Recognition", "abstract": "End-to-end (E2E) automatic speech recognition (ASR) models have become\nstandard practice for various commercial applications. However, in real-world\nscenarios, the long-tailed nature of word distribution often leads E2E ASR\nmodels to perform well on common words but fall short in recognizing uncommon\nones. Recently, the notion of a contextual adapter (CA) was proposed to infuse\nexternal knowledge represented by a context word list into E2E ASR models.\nAlthough CA can improve recognition performance on rare words, two crucial data\nimbalance problems remain. First, when using low-frequency words as context\nwords during training, since these words rarely occur in the utterance, CA\nbecomes prone to overfit on attending to the <no-context> token due to\nhigher-frequency words not being present in the context list. Second, the\nlong-tailed distribution within the context list itself still causes the model\nto perform poorly on low-frequency context words. In light of this, we explore\nin-depth the impact of altering the context list to have words with different\nfrequency distributions on model performance, and meanwhile extend CA with a\nsimple yet effective context-balanced learning objective. A series of\nexperiments conducted on the AISHELL-1 benchmark dataset suggests that using\nall vocabulary words from the training corpus as the context list and pairing\nthem with our balanced objective yields the best performance, demonstrating a\nsignificant reduction in character error rate (CER) by up to 1.21% and a more\npronounced 9.44% reduction in the error rate of zero-shot words.", "published": "2024-09-10 12:52:36", "link": "http://arxiv.org/abs/2409.06468v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of\n  Additional Language Mixture Ratio", "abstract": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance.", "published": "2024-09-10 16:26:43", "link": "http://arxiv.org/abs/2409.06624v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders", "abstract": "The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.", "published": "2024-09-10 16:46:18", "link": "http://arxiv.org/abs/2409.06635v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sortformer: Seamless Integration of Speaker Diarization and ASR by\n  Bridging Timestamps and Tokens", "abstract": "We propose Sortformer, a novel neural model for speaker diarization, trained\nwith unconventional objectives compared to existing end-to-end diarization\nmodels. The permutation problem in speaker diarization has long been regarded\nas a critical challenge. Most prior end-to-end diarization systems employ\npermutation invariant loss (PIL), which optimizes for the permutation that\nyields the lowest error. In contrast, we introduce Sort Loss, which enables a\ndiarization model to autonomously resolve permutation, with or without PIL. We\ndemonstrate that combining Sort Loss and PIL achieves performance competitive\nwith state-of-the-art end-to-end diarization models trained exclusively with\nPIL. Crucially, we present a streamlined multispeaker ASR architecture that\nleverages Sortformer as a speaker supervision model, embedding speaker label\nestimation within the ASR encoder state using a sinusoidal kernel function.\nThis approach resolves the speaker permutation problem through sorted\nobjectives, effectively bridging speaker-label timestamps and speaker tokens.\nIn our experiments, we show that the proposed multispeaker ASR architecture,\nenhanced with speaker supervision, improves performance via adapter techniques.\nCode and trained models will be made publicly available via the NVIDIA NeMo\nframework.", "published": "2024-09-10 17:20:11", "link": "http://arxiv.org/abs/2409.06656v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models", "abstract": "Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.", "published": "2024-09-10 17:34:34", "link": "http://arxiv.org/abs/2409.06666v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Geometric-Averaged Preference Optimization for Soft Preference Labels", "abstract": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, human preferences can vary\nacross individuals, and therefore should be represented distributionally. In\nthis work, we introduce the distributional soft preference labels and improve\nDirect Preference Optimization (DPO) with a weighted geometric average of the\nLLM output likelihood in the loss function. This approach adjusts the scale of\nlearning loss based on the soft labels such that the loss would approach zero\nwhen the responses are closer to equally preferred. This simple modification\ncan be easily applied to any DPO-based methods and mitigate over-optimization\nand objective mismatch, which prior works suffer from. Our experiments simulate\nthe soft preference labels with AI feedback from LLMs and demonstrate that\ngeometric averaging consistently improves performance on standard benchmarks\nfor alignment research. In particular, we observe more preferable responses\nthan binary labels and significant improvements where modestly-confident labels\nare in the majority.", "published": "2024-09-10 17:54:28", "link": "http://arxiv.org/abs/2409.06691v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Decomposition of surprisal: Unified computational model of ERP\n  components in language processing", "abstract": "The functional interpretation of language-related ERP components has been a\ncentral debate in psycholinguistics for decades. We advance an\ninformation-theoretic model of human language processing in the brain in which\nincoming linguistic input is processed at first shallowly and later with more\ndepth, with these two kinds of information processing corresponding to distinct\nelectroencephalographic signatures. Formally, we show that the information\ncontent (surprisal) of a word in context can be decomposed into two quantities:\n(A) shallow surprisal, which signals shallow processing difficulty for a word,\nand corresponds with the N400 signal; and (B) deep surprisal, which reflects\nthe discrepancy between shallow and deep representations, and corresponds to\nthe P600 signal and other late positivities. Both of these quantities can be\nestimated straightforwardly using modern NLP models. We validate our theory by\nsuccessfully simulating ERP patterns elicited by a variety of linguistic\nmanipulations in previously-reported experimental data from six experiments,\nwith successful novel qualitative and quantitative predictions. Our theory is\ncompatible with traditional cognitive theories assuming a `good-enough' shallow\nrepresentation stage, but with a precise information-theoretic formulation. The\nmodel provides an information-theoretic model of ERP components grounded on\ncognitive processes, and brings us closer to a fully-specified\nneuro-computational model of language processing.", "published": "2024-09-10 18:14:02", "link": "http://arxiv.org/abs/2409.06803v2", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "NSP: A Neuro-Symbolic Natural Language Navigational Planner", "abstract": "Path planners that can interpret free-form natural language instructions hold\npromise to automate a wide range of robotics applications. These planners\nsimplify user interactions and enable intuitive control over complex\nsemi-autonomous systems. While existing symbolic approaches offer guarantees on\nthe correctness and efficiency, they struggle to parse free-form natural\nlanguage inputs. Conversely, neural approaches based on pre-trained Large\nLanguage Models (LLMs) can manage natural language inputs but lack performance\nguarantees. In this paper, we propose a neuro-symbolic framework for path\nplanning from natural language inputs called NSP. The framework leverages the\nneural reasoning abilities of LLMs to i) craft symbolic representations of the\nenvironment and ii) a symbolic path planning algorithm. Next, a solution to the\npath planning problem is obtained by executing the algorithm on the environment\nrepresentation. The framework uses a feedback loop from the symbolic execution\nenvironment to the neural generation process to self-correct syntax errors and\nsatisfy execution time constraints. We evaluate our neuro-symbolic approach\nusing a benchmark suite with 1500 path-planning problems. The experimental\nevaluation shows that our neuro-symbolic approach produces 90.1% valid paths\nthat are on average 19-77% shorter than state-of-the-art neural approaches.", "published": "2024-09-10 20:49:05", "link": "http://arxiv.org/abs/2409.06859v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "A Dataset for Evaluating LLM-based Evaluation Functions for Research\n  Question Extraction Task", "abstract": "The progress in text summarization techniques has been remarkable. However\nthe task of accurately extracting and summarizing necessary information from\nhighly specialized documents such as research papers has not been sufficiently\ninvestigated. We are focusing on the task of extracting research questions (RQ)\nfrom research papers and construct a new dataset consisting of machine learning\npapers, RQ extracted from these papers by GPT-4, and human evaluations of the\nextracted RQ from multiple perspectives. Using this dataset, we systematically\ncompared recently proposed LLM-based evaluation functions for summarizations,\nand found that none of the functions showed sufficiently high correlations with\nhuman evaluations. We expect our dataset provides a foundation for further\nresearch on developing better evaluation functions tailored to the RQ\nextraction task, and contribute to enhance the performance of the task. The\ndataset is available at https://github.com/auto-res/PaperRQ-HumanAnno-Dataset.", "published": "2024-09-10 21:54:46", "link": "http://arxiv.org/abs/2409.06883v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Large Language Models with Domain-Specific Knowledge: The Case\n  in Topological Materials", "abstract": "Large language models (LLMs), such as ChatGPT, have demonstrated impressive\nperformance in the text generation task, showing the ability to understand and\nrespond to complex instructions. However, the performance of naive LLMs in\nspeciffc domains is limited due to the scarcity of domain-speciffc corpora and\nspecialized training. Moreover, training a specialized large-scale model\nnecessitates signiffcant hardware resources, which restricts researchers from\nleveraging such models to drive advances. Hence, it is crucial to further\nimprove and optimize LLMs to meet speciffc domain demands and enhance their\nscalability. Based on the condensed matter data center, we establish a material\nknowledge graph (MaterialsKG) and integrate it with literature. Using large\nlanguage models and prompt learning, we develop a specialized dialogue system\nfor topological materials called TopoChat. Compared to naive LLMs, TopoChat\nexhibits superior performance in structural and property querying, material\nrecommendation, and complex relational reasoning. This system enables efffcient\nand precise retrieval of information and facilitates knowledge interaction,\nthereby encouraging the advancement on the ffeld of condensed matter materials.", "published": "2024-09-10 06:01:16", "link": "http://arxiv.org/abs/2409.13732v2", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RNR: Teaching Large Language Models to Follow Roles and Rules", "abstract": "Instruction fine-tuning (IFT) elicits instruction following capabilities and\nsteers the behavior of large language models (LLMs) via supervised learning.\nHowever, existing models trained on open-source IFT datasets only have the\nability to follow instructions from users, and often fail to follow complex\nrole and rules specified by developers, a.k.a. system prompts. The ability to\nfollow these roles and rules is essential for deployment, as it ensures that\nthe model safely interacts with users within developer defined guidelines. To\nimprove such role and rule following ability, we propose \\model, an automated\ndata generation pipeline that generates diverse roles and rules from existing\nIFT instructions, along with corresponding responses. This data can then be\nused to train models that follow complex system prompts. The models are\nevaluated on our newly created benchmarks for role and rule following ability,\nas well as standard instruction-following benchmarks and general NLP tasks. Our\nframework significantly improves role and rule following capability in LLMs, as\nevidenced by over 25% increase in pass-rate on rule adherence, i.e. following\nall requirements, in our experiments with the Alpaca and Ultrachat datasets.\nMoreover, our models achieves this increase without any regression on popular\ninstruction following benchmarks.", "published": "2024-09-10 06:07:32", "link": "http://arxiv.org/abs/2409.13733v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Enhancing Kurdish Text-to-Speech with Native Corpus Training: A\n  High-Quality WaveGlow Vocoder Approach", "abstract": "The ability to synthesize spoken language from text has greatly facilitated\naccess to digital content with the advances in text-to-speech technology.\nHowever, effective TTS development for low-resource languages, such as Central\nKurdish (CKB), still faces many challenges due mainly to the lack of linguistic\ninformation and dedicated resources. In this paper, we improve the Kurdish TTS\nsystem based on Tacotron by training the Kurdish WaveGlow vocoder on a 21-hour\ncentral Kurdish speech corpus instead of using a pre-trained English vocoder\nWaveGlow. Vocoder training on the target language corpus is required to\naccurately and fluently adapt phonetic and prosodic changes in Kurdish\nlanguage. The effectiveness of these enhancements is that our model is\nsignificantly better than the baseline system with English pretrained models.\nIn particular, our adaptive WaveGlow model achieves an impressive MOS of 4.91,\nwhich sets a new benchmark for Kurdish speech synthesis. On one hand, this\nstudy empowers the advanced features of the TTS system for Central Kurdish, and\non the other hand, it opens the doors for other dialects in Kurdish and other\nrelated languages to further develop.", "published": "2024-09-10 06:23:52", "link": "http://arxiv.org/abs/2409.13734v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language agents achieve superhuman synthesis of scientific knowledge", "abstract": "Language models are known to hallucinate incorrect information, and it is\nunclear if they are sufficiently accurate and reliable for use in scientific\nresearch. We developed a rigorous human-AI comparison methodology to evaluate\nlanguage model agents on real-world literature search tasks covering\ninformation retrieval, summarization, and contradiction detection tasks. We\nshow that PaperQA2, a frontier language model agent optimized for improved\nfactuality, matches or exceeds subject matter expert performance on three\nrealistic literature research tasks without any restrictions on humans (i.e.,\nfull access to internet, search tools, and time). PaperQA2 writes cited,\nWikipedia-style summaries of scientific topics that are significantly more\naccurate than existing, human-written Wikipedia articles. We also introduce a\nhard benchmark for scientific literature research called LitQA2 that guided\ndesign of PaperQA2, leading to it exceeding human performance. Finally, we\napply PaperQA2 to identify contradictions within the scientific literature, an\nimportant scientific task that is challenging for humans. PaperQA2 identifies\n2.34 +/- 1.99 contradictions per paper in a random subset of biology papers, of\nwhich 70% are validated by human experts. These results demonstrate that\nlanguage model agents are now capable of exceeding domain experts across\nmeaningful tasks on scientific literature.", "published": "2024-09-10 16:37:58", "link": "http://arxiv.org/abs/2409.13740v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "How Redundant Is the Transformer Stack in Speech Representation Models?", "abstract": "Self-supervised speech representation models, particularly those leveraging\ntransformer architectures, have demonstrated remarkable performance across\nvarious tasks such as speech recognition, speaker identification, and emotion\ndetection. Recent studies on transformer models revealed a high redundancy\nbetween layers and the potential for significant pruning, which we will\ninvestigate here for transformer-based speech representation models. We perform\na detailed analysis of layer similarity in speech representation models using\nthree similarity metrics: cosine similarity, centered kernel alignment, and\nmutual nearest-neighbor alignment. Our findings reveal a block-like structure\nof high similarity, suggesting two main processing steps and significant\nredundancy of layers. We demonstrate the effectiveness of pruning\ntransformer-based speech representation models without the need for\npost-training, achieving up to 40% reduction in transformer layers while\nmaintaining over 95% of the model's predictive capacity. Furthermore, we employ\na knowledge distillation method to substitute the entire transformer stack with\nmimicking layers, reducing the network size 95-98% and the inference time by up\nto 94%. This substantial decrease in computational load occurs without\nconsiderable performance loss, suggesting that the transformer stack is almost\ncompletely redundant for downstream applications of speech representation\nmodels.", "published": "2024-09-10 11:00:24", "link": "http://arxiv.org/abs/2409.16302v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DeepScore: A Comprehensive Approach to Measuring Quality in AI-Generated\n  Clinical Documentation", "abstract": "Medical practitioners are rapidly adopting generative AI solutions for\nclinical documentation, leading to significant time savings and reduced stress.\nHowever, evaluating the quality of AI-generated documentation is a complex and\nongoing challenge. This paper presents an overview of DeepScribe's\nmethodologies for assessing and managing note quality, focusing on various\nmetrics and the composite \"DeepScore\", an overall index of quality and\naccuracy. These methodologies aim to enhance the quality of patient care\ndocumentation through accountability and continuous improvement.", "published": "2024-09-10 23:06:48", "link": "http://arxiv.org/abs/2409.16307v1", "categories": ["cs.CL", "cs.AI", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Unlock Novel Scientific Research Ideas?", "abstract": "\"An idea is nothing more nor less than a new combination of old elements\"\n(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and\npublicly available ChatGPT have marked a significant turning point in the\nintegration of Artificial Intelligence (AI) into people's everyday lives. This\nstudy explores the capability of LLMs in generating novel research ideas based\non information from research papers. We conduct a thorough examination of 4\nLLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and\nPhysics). We found that the future research ideas generated by Claude-2 and\nGPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.\nWe also found that Claude-2 generates more diverse future research ideas than\nGPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the\nnovelty, relevancy, and feasibility of the generated future research ideas.\nThis investigation offers insights into the evolving role of LLMs in idea\ngeneration, highlighting both its capability and limitations. Our work\ncontributes to the ongoing efforts in evaluating and utilizing language models\nfor generating future research ideas. We make our datasets and codes publicly\navailable.", "published": "2024-09-10 03:26:42", "link": "http://arxiv.org/abs/2409.06185v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training\n  Data", "abstract": "Large language models (LLMs) have shown great potential for automatic code\ngeneration and form the basis for various tools such as GitHub Copilot.\nHowever, recent studies highlight that many LLM-generated code contains serious\nsecurity vulnerabilities. While previous work tries to address this by training\nmodels that generate secure code, these attempts remain constrained by limited\naccess to training data and labor-intensive data preparation.\n  In this paper, we introduce HexaCoder, a novel approach to enhance the\nability of LLMs to generate secure codes by automatically synthesizing secure\ncodes, which reduces the effort of finding suitable training data. HexaCoder\ncomprises two key components: an oracle-guided data synthesis pipeline and a\ntwo-step process for secure code generation. The data synthesis pipeline\ngenerates pairs of vulnerable and fixed codes for specific Common Weakness\nEnumeration (CWE) types by utilizing a state-of-the-art LLM for repairing\nvulnerable code. A security oracle identifies vulnerabilities, and a\nstate-of-the-art LLM repairs them by extending and/or editing the codes,\ncreating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA)\nmethod. Each example of our fine-tuning dataset includes the necessary\nsecurity-related libraries and code that form the basis of our novel two-step\ngeneration approach. This allows the model to integrate security-relevant\nlibraries before generating the main code, significantly reducing the number of\ngenerated vulnerable codes by up to 85% compared to the baseline methods. We\nperform extensive evaluations on three different benchmarks for four LLMs,\ndemonstrating that HexaCoder not only improves the security of the generated\ncode but also maintains a high level of functional correctness.", "published": "2024-09-10 12:01:43", "link": "http://arxiv.org/abs/2409.06446v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CR"}
{"title": "VC-ENHANCE: Speech Restoration with Integrated Noise Suppression and\n  Voice Conversion", "abstract": "Noise suppression (NS) algorithms are effective in improving speech quality\nin many cases. However, aggressive noise suppression can damage the target\nspeech, reducing both speech intelligibility and quality despite removing the\nnoise. This study proposes an explicit speech restoration method using a voice\nconversion (VC) technique for restoration after noise suppression. We observed\nthat high-quality speech can be restored through a diffusion-based voice\nconversion stage, conditioned on the target speaker embedding and speech\ncontent information extracted from the de-noised speech. This speech\nrestoration can achieve enhancement effects such as bandwidth extension,\nde-reverberation, and in-painting. Our experimental results demonstrate that\nthis two-stage NS+VC framework outperforms single-stage enhancement models in\nterms of output speech quality, as measured by objective metrics, while scoring\nslightly lower in speech intelligibility. To further improve the\nintelligibility of the combined system, we propose a content encoder adaptation\nmethod for robust content extraction in noisy conditions.", "published": "2024-09-10 00:38:08", "link": "http://arxiv.org/abs/2409.06126v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DENSE: Dynamic Embedding Causal Target Speech Extraction", "abstract": "Target speech extraction (TSE) focuses on extracting the speech of a specific\ntarget speaker from a mixture of signals. Existing TSE models typically utilize\nstatic embeddings as conditions for extracting the target speaker's voice.\nHowever, the static embeddings often fail to capture the contextual information\nof the extracted speech signal, which may limit the model's performance. We\npropose a novel dynamic embedding causal target speech extraction model to\naddress this limitation. Our approach incorporates an autoregressive mechanism\nto generate context-dependent embeddings based on the extracted speech,\nenabling real-time, frame-level extraction. Experimental results demonstrate\nthat the proposed model enhances short-time objective intelligibility (STOI)\nand signal-to-distortion ratio (SDR), offering a promising solution for target\nspeech extraction in challenging scenarios.", "published": "2024-09-10 01:14:57", "link": "http://arxiv.org/abs/2409.06136v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for\n  Robust Singing Voice Conversion", "abstract": "Singing voice conversion (SVC) is hindered by noise sensitivity due to the\nuse of non-robust methods for extracting pitch and energy during the inference.\nAs clean signals are key for the source audio in SVC, music source separation\npreprocessing offers a viable solution for handling noisy audio, like singing\nwith background music (BGM). However, current separating methods struggle to\nfully remove noise or excessively suppress signal components, affecting the\nnaturalness and similarity of the processed audio. To tackle this, our study\nintroduces RobustSVC, a novel any-to-one SVC framework that converts noisy\nvocals into clean vocals sung by the target singer. We replace the non-robust\nfeature with a HuBERT-based melody extractor and use adversarial training\nmechanisms with three discriminators to reduce information leakage in\nself-supervised representations. Experimental results show that RobustSVC is\nnoise-robust and achieves higher similarity and naturalness than baseline\nmethods in both noisy and clean vocal conditions.", "published": "2024-09-10 06:10:33", "link": "http://arxiv.org/abs/2409.06237v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Two-Stage Band-Split Mamba-2 Network For Music Separation", "abstract": "Music source separation (MSS) aims to separate mixed music into its distinct\ntracks, such as vocals, bass, drums, and more. MSS is considered to be a\nchallenging audio separation task due to the complexity of music signals.\nAlthough the RNN and Transformer architecture are not perfect, they are\ncommonly used to model the music sequence for MSS. Recently, Mamba-2 has\nalready demonstrated high efficiency in various sequential modeling tasks, but\nits superiority has not been investigated in MSS. This paper applies Mamba-2\nwith a two-stage strategy, which introduces residual mapping based on the mask\nmethod, effectively compensating for the details absent in the mask and further\nimproving separation performance. Experiments confirm the superiority of\nbidirectional Mamba-2 and the effectiveness of the two-stage network in MSS.\nThe source code is publicly accessible at\nhttps://github.com/baijinglin/TS-BSmamba2.", "published": "2024-09-10 06:25:53", "link": "http://arxiv.org/abs/2409.06245v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spoofing-Aware Speaker Verification Robust Against Domain and Channel\n  Mismatches", "abstract": "In real-world applications, it is challenging to build a speaker verification\nsystem that is simultaneously robust against common threats, including spoofing\nattacks, channel mismatch, and domain mismatch. Traditional automatic speaker\nverification (ASV) systems often tackle these issues separately, leading to\nsuboptimal performance when faced with simultaneous challenges. In this paper,\nwe propose an integrated framework that incorporates pair-wise learning and\nspoofing attack simulation into the meta-learning paradigm to enhance\nrobustness against these multifaceted threats. This novel approach employs an\nasymmetric dual-path model and a multi-task learning strategy to handle ASV,\nanti-spoofing, and spoofing-aware ASV tasks concurrently. A new testing\ndataset, CNComplex, is introduced to evaluate system performance under these\ncombined threats. Experimental results demonstrate that our integrated model\nsignificantly improves performance over traditional ASV systems across various\nscenarios, showcasing its potential for real-world deployment. Additionally,\nthe proposed framework's ability to generalize across different conditions\nhighlights its robustness and reliability, making it a promising solution for\npractical ASV applications.", "published": "2024-09-10 08:32:29", "link": "http://arxiv.org/abs/2409.06327v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "InstructSing: High-Fidelity Singing Voice Generation via Instructing\n  Yourself", "abstract": "It is challenging to accelerate the training process while ensuring both\nhigh-quality generated voices and acceptable inference speed. In this paper, we\npropose a novel neural vocoder called InstructSing, which can converge much\nfaster compared with other neural vocoders while maintaining good performance\nby integrating differentiable digital signal processing and adversarial\ntraining. It includes one generator and two discriminators. Specifically, the\ngenerator incorporates a harmonic-plus-noise (HN) module to produce 8kHz audio\nas an instructive signal. Subsequently, the HN module is connected with an\nextended WaveNet by an UNet-based module, which transforms the output of the HN\nmodule to a latent variable sequence containing essential periodic and\naperiodic information. In addition to the latent sequence, the extended WaveNet\nalso takes the mel-spectrogram as input to generate 48kHz high-fidelity singing\nvoices. In terms of discriminators, we combine a multi-period discriminator, as\noriginally proposed in HiFiGAN, with a multi-resolution multi-band STFT\ndiscriminator. Notably, InstructSing achieves comparable voice quality to other\nneural vocoders but with only one-tenth of the training steps on a 4 NVIDIA\nV100 GPU machine\\footnote{{Demo page:\n\\href{https://wavelandspeech.github.io/instructsing/}{\\texttt{https://wavelandspeech.github.io/inst\\\\ructsing/}}}}.\nWe plan to open-source our code and pretrained model once the paper get\naccepted.", "published": "2024-09-10 08:38:11", "link": "http://arxiv.org/abs/2409.06330v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Janssen 2.0: Audio Inpainting in the Time-frequency Domain", "abstract": "The paper focuses on inpainting missing parts of an audio signal spectrogram.\nThe autoregression-based Janssen algorithm, the state-of-the-art for the\ntime-domain audio inpainting, is adapted for the time-frequency setting. This\nnovel method, termed Janssen-TF, is compared to the deep-prior neural network\napproach using both objective metrics and a~subjective listening test, proving\nJanssen-TF to be superior in all the considered measures.", "published": "2024-09-10 10:16:59", "link": "http://arxiv.org/abs/2409.06392v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Emotional Text-to-Speech Controllability with Natural Language\n  Guidance through Contrastive Learning and Diffusion Models", "abstract": "While current emotional text-to-speech (TTS) systems can generate highly\nintelligible emotional speech, achieving fine control over emotion rendering of\nthe output speech still remains a significant challenge. In this paper, we\nintroduce ParaEVITS, a novel emotional TTS framework that leverages the\ncompositionality of natural language to enhance control over emotional\nrendering. By incorporating a text-audio encoder inspired by ParaCLAP, a\ncontrastive language-audio pretraining (CLAP) model for computational\nparalinguistics, the diffusion model is trained to generate emotional\nembeddings based on textual emotional style descriptions. Our framework first\ntrains on reference audio using the audio encoder, then fine-tunes a diffusion\nmodel to process textual inputs from ParaCLAP's text encoder. During inference,\nspeech attributes such as pitch, jitter, and loudness are manipulated using\nonly textual conditioning. Our experiments demonstrate that ParaEVITS\neffectively control emotion rendering without compromising speech quality.\nSpeech demos are publicly available.", "published": "2024-09-10 12:13:24", "link": "http://arxiv.org/abs/2409.06451v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention-Based Beamformer For Multi-Channel Speech Enhancement", "abstract": "Minimum Variance Distortionless Response (MVDR) is a classical adaptive\nbeamformer that theoretically ensures the distortionless transmission of\nsignals in the target direction, which makes it popular in real applications.\nIts noise reduction performance actually depends on the accuracy of the noise\nand speech spatial covariance matrices (SCMs) estimation. Time-frequency masks\nare often used to compute these SCMs. However, most mask-based beamforming\nmethods typically assume that the sources are stationary, ignoring the case of\nmoving sources, which leads to performance degradation. In this paper, we\npropose an attention-based mechanism to calculate the speech and noise SCMs and\nthen apply MVDR to obtain the enhanced speech. To fully incorporate spatial\ninformation, the inplace convolution operator and frequency-independent LSTM\nare applied to facilitate SCMs estimation. The model is optimized in an\nend-to-end manner. Experiments demonstrate that the proposed method outperforms\nbaselines with reduced computation and fewer parameters under various\nconditions.", "published": "2024-09-10 12:23:51", "link": "http://arxiv.org/abs/2409.06456v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Differences between Human Perception and Model Inference in\n  Audio Event Recognition", "abstract": "Audio Event Recognition (AER) traditionally focuses on detecting and\nidentifying audio events. Most existing AER models tend to detect all potential\nevents without considering their varying significance across different\ncontexts. This makes the AER results detected by existing models often have a\nlarge discrepancy with human auditory perception. Although this is a critical\nand significant issue, it has not been extensively studied by the Detection and\nClassification of Sound Scenes and Events (DCASE) community because solving it\nis time-consuming and labour-intensive. To address this issue, this paper\nintroduces the concept of semantic importance in AER, focusing on exploring the\ndifferences between human perception and model inference. This paper constructs\na Multi-Annotated Foreground Audio Event Recognition (MAFAR) dataset, which\ncomprises audio recordings labelled by 10 professional annotators. Through\nlabelling frequency and variance, the MAFAR dataset facilitates the\nquantification of semantic importance and analysis of human perception. By\ncomparing human annotations with the predictions of ensemble pre-trained\nmodels, this paper uncovers a significant gap between human perception and\nmodel inference in both semantic identification and existence detection of\naudio events. Experimental results reveal that human perception tends to ignore\nsubtle or trivial events in the event semantic identification, while model\ninference is easily affected by events with noises. Meanwhile, in event\nexistence detection, models are usually more sensitive than humans.", "published": "2024-09-10 15:19:50", "link": "http://arxiv.org/abs/2409.06580v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Prosodic Parameter Manipulation in TTS generated speech for Controlled\n  Speech Generation", "abstract": "This paper explores the manipulation of prosodic parameters in Text-to-Speech\n(TTS) systems to achieve controlled speech generation. By leveraging advanced\nspeech processing techniques, we compare TTS-generated audio with\nhuman-recorded speech to analyze differences in pitch, duration, and energy.\nKey features are extracted using tools like PyWorld and Librosa, which are then\nadjusted to align with the prosodic characteristics of natural human speech.\nThe modified features undergo synthesis, producing enhanced TTS outputs that\nmore closely mirror the natural prosody of human speech. This approach aims to\nenhance the naturalness and expressiveness of TTS systems by providing a\nframework for precise prosodic parameter adjustments. Our methodology involves\nfeature extraction, prosodic manipulation, and synthesis, followed by\ncomprehensive evaluations to ensure consistency with human speech patterns. The\nfindings demonstrate the feasibility and effectiveness of prosodic parameter\nmanipulation for controlled speech generation, highlighting its potential to\nsignificantly improve TTS applications.", "published": "2024-09-10 11:50:35", "link": "http://arxiv.org/abs/2409.12176v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis", "abstract": "Foley is a term commonly used in filmmaking, referring to the addition of\ndaily sound effects to silent films or videos to enhance the auditory\nexperience. Video-to-Audio (V2A), as a particular type of automatic foley task,\npresents inherent challenges related to audio-visual synchronization. These\nchallenges encompass maintaining the content consistency between the input\nvideo and the generated audio, as well as the alignment of temporal and\nloudness properties within the video. To address these issues, we construct a\ncontrollable video-to-audio synthesis model, termed Draw an Audio, which\nsupports multiple input instructions through drawn masks and loudness signals.\nTo ensure content consistency between the synthesized audio and target video,\nwe introduce the Mask-Attention Module (MAM), which employs masked video\ninstruction to enable the model to focus on regions of interest. Additionally,\nwe implement the Time-Loudness Module (TLM), which uses an auxiliary loudness\nsignal to ensure the synthesis of sound that aligns with the video in both\nloudness and temporal dimensions. Furthermore, we have extended a large-scale\nV2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive\nexperiments on challenging benchmarks across two large-scale V2A datasets\nverify Draw an Audio achieves the state-of-the-art. Project page:\nhttps://yannqi.github.io/Draw-an-Audio/.", "published": "2024-09-10 01:07:20", "link": "http://arxiv.org/abs/2409.06135v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DeWinder: Single-Channel Wind Noise Reduction using Ultrasound Sensing", "abstract": "The quality of audio recordings in outdoor environments is often degraded by\nthe presence of wind. Mitigating the impact of wind noise on the perceptual\nquality of single-channel speech remains a significant challenge due to its\nnon-stationary characteristics. Prior work in noise suppression treats wind\nnoise as a general background noise without explicit modeling of its\ncharacteristics. In this paper, we leverage ultrasound as an auxiliary modality\nto explicitly sense the airflow and characterize the wind noise. We propose a\nmulti-modal deep-learning framework to fuse the ultrasonic Doppler features and\nspeech signals for wind noise reduction. Our results show that DeWinder can\nsignificantly improve the noise reduction capabilities of state-of-the-art\nspeech enhancement models.", "published": "2024-09-10 01:20:13", "link": "http://arxiv.org/abs/2409.06137v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Multi-Source Music Generation with Latent Diffusion", "abstract": "Most music generation models directly generate a single music mixture. To\nallow for more flexible and controllable generation, the Multi-Source Diffusion\nModel (MSDM) has been proposed to model music as a mixture of multiple\ninstrumental sources (e.g. piano, drums, bass, and guitar). Its goal is to use\none single diffusion model to generate mutually-coherent music sources, that\nare then mixed to form the music. Despite its capabilities, MSDM is unable to\ngenerate music with rich melodies and often generates empty sounds. Its\nwaveform diffusion approach also introduces significant Gaussian noise\nartifacts that compromise audio quality. In response, we introduce a\nMulti-Source Latent Diffusion Model (MSLDM) that employs Variational\nAutoencoders (VAEs) to encode each instrumental source into a distinct latent\nrepresentation. By training a VAE on all music sources, we efficiently capture\neach source's unique characteristics in a \"source latent.\" The source latents\nare concatenated and our diffusion model learns this joint latent space. This\napproach significantly enhances the total and partial generation of music by\nleveraging the VAE's latent compression and noise-robustness. The compressed\nsource latent also facilitates more efficient generation. Subjective listening\ntests and Frechet Audio Distance (FAD) scores confirm that our model\noutperforms MSDM, showcasing its practical and enhanced applicability in music\ngeneration systems. We also emphasize that modeling sources is more effective\nthan direct music mixture modeling. Codes and models are available at\nhttps://github.com/XZWY/MSLDM. Demos are available at\nhttps://xzwy.github.io/MSLDMDemo/.", "published": "2024-09-10 03:41:10", "link": "http://arxiv.org/abs/2409.06190v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MTDA-HSED: Mutual-Assistance Tuning and Dual-Branch Aggregating for\n  Heterogeneous Sound Event Detection", "abstract": "Sound Event Detection (SED) plays a vital role in comprehending and\nperceiving acoustic scenes. Previous methods have demonstrated impressive\ncapabilities. However, they are deficient in learning features of complex\nscenes from heterogeneous dataset. In this paper, we introduce a novel\ndual-branch architecture named Mutual-Assistance Tuning and Dual-Branch\nAggregating for Heterogeneous Sound Event Detection (MTDA-HSED). The MTDA-HSED\narchitecture employs the Mutual-Assistance Audio Adapter (M3A) to effectively\ntackle the multi-scenario problem and uses the Dual-Branch Mid-Fusion (DBMF)\nmodule to tackle the multi-granularity problem. Specifically, M3A is integrated\ninto the BEATs block as an adapter to improve the BEATs' performance by\nfine-tuning it on the multi-scenario dataset. The DBMF module connects BEATs\nand CNN branches, which facilitates the deep fusion of information from the\nBEATs and the CNN branches. Experimental results show that the proposed methods\nexceed the baseline of mpAUC by \\textbf{$5\\%$} on the DESED and MAESTRO Real\ndatasets. Code is available at https://github.com/Visitor-W/MTDA.", "published": "2024-09-10 03:57:21", "link": "http://arxiv.org/abs/2409.06196v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spectral oversubtraction? An approach for speech enhancement after robot\n  ego speech filtering in semi-real-time", "abstract": "Spectral subtraction, widely used for its simplicity, has been employed to\naddress the Robot Ego Speech Filtering (RESF) problem for detecting speech\ncontents of human interruption from robot's single-channel microphone\nrecordings when it is speaking. However, this approach suffers from\noversubtraction in the fundamental frequency range (FFR), leading to degraded\nspeech content recognition. To address this, we propose a Two-Mask\nConformer-based Metric Generative Adversarial Network (CMGAN) to enhance the\ndetected speech and improve recognition results. Our model compensates for\noversubtracted FFR values with high-frequency information and long-term\nfeatures and then de-noises the new spectrogram. In addition, we introduce an\nincremental processing method that allows semi-real-time audio processing with\nstreaming input on a network trained on long fixed-length input. Evaluations of\ntwo datasets, including one with unseen noise, demonstrate significant\nimprovements in recognition accuracy and the effectiveness of the proposed\ntwo-mask approach and incremental processing, enhancing the robustness of the\nproposed RESF pipeline in real-world HRI scenarios.", "published": "2024-09-10 07:24:35", "link": "http://arxiv.org/abs/2409.06274v1", "categories": ["cs.RO", "cs.SD", "eess.AS", "68T50"], "primary_category": "cs.RO"}
{"title": "An End-to-End Approach for Chord-Conditioned Song Generation", "abstract": "The Song Generation task aims to synthesize music composed of vocals and\naccompaniment from given lyrics. While the existing method, Jukebox, has\nexplored this task, its constrained control over the generations often leads to\ndeficiency in music performance. To mitigate the issue, we introduce an\nimportant concept from music composition, namely chords, to song generation\nnetworks. Chords form the foundation of accompaniment and provide vocal melody\nwith associated harmony. Given the inaccuracy of automatic chord extractors, we\ndevise a robust cross-attention mechanism augmented with dynamic weight\nsequence to integrate extracted chord information into song generations and\nreduce frame-level flaws, and propose a novel model termed Chord-Conditioned\nSong Generator (CSG) based on it. Experimental evidence demonstrates our\nproposed method outperforms other approaches in terms of musical performance\nand control precision of generated songs.", "published": "2024-09-10 08:07:43", "link": "http://arxiv.org/abs/2409.06307v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VoiceWukong: Benchmarking Deepfake Voice Detection", "abstract": "With the rapid advancement of technologies like text-to-speech (TTS) and\nvoice conversion (VC), detecting deepfake voices has become increasingly\ncrucial. However, both academia and industry lack a comprehensive and intuitive\nbenchmark for evaluating detectors. Existing datasets are limited in language\ndiversity and lack many manipulations encountered in real-world production\nenvironments.\n  To fill this gap, we propose VoiceWukong, a benchmark designed to evaluate\nthe performance of deepfake voice detectors. To build the dataset, we first\ncollected deepfake voices generated by 19 advanced and widely recognized\ncommercial tools and 15 open-source tools. We then created 38 data variants\ncovering six types of manipulations, constructing the evaluation dataset for\ndeepfake voice detection. VoiceWukong thus includes 265,200 English and 148,200\nChinese deepfake voice samples. Using VoiceWukong, we evaluated 12\nstate-of-the-art detectors. AASIST2 achieved the best equal error rate (EER) of\n13.50%, while all others exceeded 20%. Our findings reveal that these detectors\nface significant challenges in real-world applications, with dramatically\ndeclining performance. In addition, we conducted a user study with more than\n300 participants. The results are compared with the performance of the 12\ndetectors and a multimodel large language model (MLLM), i.e., Qwen2-Audio,\nwhere different detectors and humans exhibit varying identification\ncapabilities for deepfake voices at different deception levels, while the LALM\ndemonstrates no detection ability at all. Furthermore, we provide a leaderboard\nfor deepfake voice detection, publicly available at\n{https://voicewukong.github.io}.", "published": "2024-09-10 09:07:12", "link": "http://arxiv.org/abs/2409.06348v1", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Soft Acoustic Curvature Sensor: Design and Development", "abstract": "This paper introduces a novel Soft Acoustic Curvature (SAC) sensor. SAC\nincorporates integrated audio components and features an acoustic channel\nwithin a flexible structure. A reference acoustic wave, generated by a speaker\nat one end of the channel, propagates and is received by a microphone at the\nother channel's end. Our previous study revealed that acoustic wave energy\ndissipation varies with acoustic channel deformation, leading us to design a\nnovel channel capable of large deformation due to bending. We then use Machine\nLearning (ML) models to establish a complex mapping between channel\ndeformations and sound modulation. Various sound frequencies and ML models were\nevaluated to enhance curvature detection accuracy. The sensor, constructed\nusing soft material and 3D printing, was validated experimentally, with\ncurvature measurement errors remaining within 3.5 m-1 for a range of 0 to 60\nm-1 curvatures. These results demonstrate the effectiveness of the proposed\nmethod for estimating curvatures. With its flexible structure, the SAC sensor\nholds potential for applications in soft robotics, including shape measurement\nfor continuum manipulators, soft grippers, and wearable devices.", "published": "2024-09-10 10:22:46", "link": "http://arxiv.org/abs/2409.06395v3", "categories": ["cs.SD", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Human-mimetic binaural ear design and sound source direction estimation\n  for task realization of musculoskeletal humanoids", "abstract": "Human-like environment recognition by musculoskeletal humanoids is important\nfor task realization in real complex environments and for use as dummies for\ntest subjects. Humans integrate various sensory information to perceive their\nsurroundings, and hearing is particularly useful for recognizing objects out of\nview or out of touch. In this research, we aim to realize human-like auditory\nenvironmental recognition and task realization for musculoskeletal humanoids by\nequipping them with a human-like auditory processing system. Humans realize\nsound-based environmental recognition by estimating directions of the sound\nsources and detecting environmental sounds based on changes in the time and\nfrequency domain of incoming sounds and the integration of auditory information\nin the central nervous system. We propose a human mimetic auditory information\nprocessing system, which consists of three components: the human mimetic\nbinaural ear unit, which mimics human ear structure and characteristics, the\nsound source direction estimation system, and the environmental sound detection\nsystem, which mimics processing in the central nervous system. We apply it to\nMusashi, a human mimetic musculoskeletal humanoid, and have it perform tasks\nthat require sound information outside of view in real noisy environments to\nconfirm the usefulness of the proposed methods.", "published": "2024-09-10 11:20:41", "link": "http://arxiv.org/abs/2409.06429v1", "categories": ["cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Sines, Transient, Noise Neural Modeling of Piano Notes", "abstract": "This paper introduces a novel method for emulating piano sounds. We propose\nto exploit the sines, transient, and noise decomposition to design a\ndifferentiable spectral modeling synthesizer replicating piano notes. Three\nsub-modules learn these components from piano recordings and generate the\ncorresponding harmonic, transient, and noise signals. Splitting the emulation\ninto three independently trainable models reduces the modeling tasks'\ncomplexity. The quasi-harmonic content is produced using a differentiable\nsinusoidal model guided by physics-derived formulas, whose parameters are\nautomatically estimated from audio recordings. The noise sub-module uses a\nlearnable time-varying filter, and the transients are generated using a deep\nconvolutional network. From singular notes, we emulate the coupling between\ndifferent keys in trichords with a convolutional-based network. Results show\nthe model matches the partial distribution of the target while predicting the\nenergy in the higher part of the spectrum presents more challenges. The energy\ndistribution in the spectra of the transient and noise components is accurate\noverall. While the model is more computationally and memory efficient,\nperceptual tests reveal limitations in accurately modeling the attack phase of\nnotes. Despite this, it generally achieves perceptual accuracy in emulating\nsingle notes and trichords.", "published": "2024-09-10 13:48:18", "link": "http://arxiv.org/abs/2409.06513v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
